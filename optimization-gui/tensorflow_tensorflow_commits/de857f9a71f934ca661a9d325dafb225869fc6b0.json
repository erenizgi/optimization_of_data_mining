{
    "author": "MedoX71T",
    "message": "Merge branch 'tensorflow:master' into master",
    "sha": "de857f9a71f934ca661a9d325dafb225869fc6b0",
    "files": [
        {
            "sha": "dcc171ac5af01914e34b9bf02739927c956cc28d",
            "filename": "ci/official/containers/ml_build/cuda13.0_cudnn9.15.packages.txt",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/ci%2Fofficial%2Fcontainers%2Fml_build%2Fcuda13.0_cudnn9.15.packages.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/ci%2Fofficial%2Fcontainers%2Fml_build%2Fcuda13.0_cudnn9.15.packages.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Fcontainers%2Fml_build%2Fcuda13.0_cudnn9.15.packages.txt?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,23 @@\n+# All required CUDA packages\n+cuda-compat-13-0\n+cuda-command-line-tools-13-0\n+cuda-cudart-dev-13-0\n+cuda-nvcc-13-0\n+cuda-cupti-13-0\n+cuda-nvprune-13-0\n+cuda-libraries-13-0\n+cuda-libraries-dev-13-0\n+cuda-nvml-dev-13-0\n+libcufft-13-0\n+libcurand-13-0\n+libcusolver-dev-13-0\n+libcusparse-dev-13-0\n+libcublas-13-0\n+libcublas-dev-13-0\n+libnccl-dev=2.27.7-1+cuda13.0\n+libnccl2=2.27.7-1+cuda13.0\n+# CuDNN: https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#ubuntu-network-installation\n+libcudnn9-headers-cuda-13=9.15.1.9-1\n+libcudnn9-static-cuda-13=9.15.1.9-1\n+libcudnn9-dev-cuda-13=9.15.1.9-1\n+libcudnn9-cuda-13=9.15.1.9-1\n\\ No newline at end of file"
        },
        {
            "sha": "03f49d85797225e4b46f8d6f56a5f68032f6dde5",
            "filename": "ci/official/utilities/setup_docker.sh",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/ci%2Fofficial%2Futilities%2Fsetup_docker.sh",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/ci%2Fofficial%2Futilities%2Fsetup_docker.sh",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Futilities%2Fsetup_docker.sh?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -62,6 +62,12 @@ if ! docker container inspect tf >/dev/null 2>&1 ; then\n     # Additional setup is contained in ci/official/envs/rbe.\n     CONTAINER_IP_ADDR=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' tf)\n     netsh advfirewall firewall add rule name=\"Allow Metadata Proxy\" dir=in action=allow protocol=TCP localport=80 remoteip=\"$CONTAINER_IP_ADDR\"\n+\n+    # Stop non-essential indexing and link tracking services that\n+    # may lock new files or symlinks.\n+    # They may be causing sporadic \"Permission denied\" errors during Bazel builds.\n+    # b/461500885\n+    docker exec tf powershell -NoProfile -Command 'Stop-Service -Name SysMain,DiagTrack -Force -ErrorAction SilentlyContinue'\n   fi\n \n fi"
        },
        {
            "sha": "4dd78e4cd7bbb14ab569dc35c78333f5fd39af1b",
            "filename": "tensorflow/c/c_api_function_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fc_api_function_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fc_api_function_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fc_api_function_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1171,7 +1171,7 @@ TEST_F(CApiFunctionTest, InvalidOutputTensor_BadNodePtr) {\n   EXPECT_EQ(TF_INVALID_ARGUMENT, TF_GetCode(s_));\n   EXPECT_EQ(string(\"Node is null\\n\\tEncountered while processing output 0 \"\n                    \"from function 'MyFunc'\"),\n-            string(TF_Message(s_)));\n+            std::string(TF_Message(s_)));\n }\n \n TEST_F(CApiFunctionTest, NodeMissingInput) {"
        },
        {
            "sha": "f59a73a08719454a56a9fb5a01bd7489e7cd194c",
            "filename": "tensorflow/c/c_api_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fc_api_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fc_api_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fc_api_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -2478,7 +2478,7 @@ TEST_F(CApiAttributesTest, Names) {\n \n   TF_OperationGetAttrName(oper, 0, value.get(), s_);\n   EXPECT_EQ(TF_OK, TF_GetCode(s_)) << TF_Message(s_);\n-  EXPECT_EQ(\"v\", string(static_cast<const char*>(value.get()), 1));\n+  EXPECT_EQ(\"v\", std::string(static_cast<const char*>(value.get()), 1));\n }\n \n TEST_F(CApiAttributesTest, Errors) {"
        },
        {
            "sha": "ec446fd8389687f42b30875d6e8dc93d4fd36d3f",
            "filename": "tensorflow/c/experimental/filesystem/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Ffilesystem%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Ffilesystem%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fexperimental%2Ffilesystem%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -49,6 +49,7 @@ cc_library(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@local_xla//xla/tsl/platform:env\",\n         \"@local_xla//xla/tsl/platform:errors\",\n     ],"
        },
        {
            "sha": "5a8c4ba3ccb56c01a36af53c7fb56866afac687f",
            "filename": "tensorflow/c/experimental/filesystem/modular_filesystem.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Ffilesystem%2Fmodular_filesystem.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Ffilesystem%2Fmodular_filesystem.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fexperimental%2Ffilesystem%2Fmodular_filesystem.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/c/experimental/filesystem/filesystem_interface.h\"\n #include \"xla/tsl/platform/file_system.h\"\n #include \"tensorflow/core/platform/file_statistics.h\""
        },
        {
            "sha": "c0ae70b64abec7cbf2a8a7c0d351317ec59c2ec8",
            "filename": "tensorflow/c/experimental/gradients/tape/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fgradients%2Ftape%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fgradients%2Ftape%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fexperimental%2Fgradients%2Ftape%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -50,6 +50,7 @@ cc_library(\n         \"//tensorflow/core/platform:strcat\",\n         \"//tensorflow/core/platform:stringpiece\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n         \"@local_xla//xla/tsl/platform:errors\",\n     ],"
        },
        {
            "sha": "2839616c63991b604a2dff4ecad0f8cfb883ef7a",
            "filename": "tensorflow/c/experimental/gradients/tape/tape_operation.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fgradients%2Ftape%2Ftape_operation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fgradients%2Ftape%2Ftape_operation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fexperimental%2Fgradients%2Ftape%2Ftape_operation.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"tensorflow/c/eager/abstract_operation.h\"\n #include \"tensorflow/c/eager/abstract_tensor_handle.h\""
        },
        {
            "sha": "de027662df30cf0cfb2511e9d1f6c96a9e78c4aa",
            "filename": "tensorflow/c/experimental/saved_model/core/ops/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fsaved_model%2Fcore%2Fops%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fsaved_model%2Fcore%2Fops%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fexperimental%2Fsaved_model%2Fcore%2Fops%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -82,6 +82,7 @@ tf_cc_test(\n         \"//tensorflow/core/common_runtime/eager:context\",\n         \"//tensorflow/core/common_runtime/eager:core\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n "
        },
        {
            "sha": "866dbaa94895d01d1ef360119e6092f9f92aefe6",
            "filename": "tensorflow/c/experimental/saved_model/core/ops/restore_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fsaved_model%2Fcore%2Fops%2Frestore_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fc%2Fexperimental%2Fsaved_model%2Fcore%2Fops%2Frestore_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fc%2Fexperimental%2Fsaved_model%2Fcore%2Fops%2Frestore_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <string>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/c/eager/immediate_execution_tensor_handle.h\"\n #include \"tensorflow/c/experimental/saved_model/core/test_utils.h\"\n #include \"tensorflow/c/tensor_interface.h\""
        },
        {
            "sha": "f776fdd9612ecdee7000e26cb2b1f9bb7b528be9",
            "filename": "tensorflow/cc/client/client_session.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcc%2Fclient%2Fclient_session.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcc%2Fclient%2Fclient_session.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcc%2Fclient%2Fclient_session.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -34,7 +34,7 @@ class ClientSession::Impl {\n   Impl(Session* session, std::shared_ptr<Graph> graph)\n       : session_(session), graph_(std::move(graph)) {}\n \n-  static SessionOptions MakeDefaultSessionOptions(const string& target);\n+  static SessionOptions MakeDefaultSessionOptions(const std::string& target);\n   absl::Status MaybeExtendGraph() const;\n \n   std::unique_ptr<Session> session_;\n@@ -44,7 +44,7 @@ class ClientSession::Impl {\n   mutable int last_num_graph_nodes_ TF_GUARDED_BY(mu_) = 0;\n };\n \n-ClientSession::ClientSession(const Scope& scope, const string& target)\n+ClientSession::ClientSession(const Scope& scope, const std::string& target)\n     : ClientSession(scope, Impl::MakeDefaultSessionOptions(target)) {}\n \n ClientSession::ClientSession(const Scope& scope) : ClientSession(scope, \"\") {}\n@@ -64,7 +64,7 @@ ClientSession::ClientSession(const Scope& scope,\n ClientSession::~ClientSession() {}\n \n SessionOptions ClientSession::Impl::MakeDefaultSessionOptions(\n-    const string& target) {\n+    const std::string& target) {\n   SessionOptions options;\n   options.env = Env::Default();\n   options.target = target;\n@@ -108,7 +108,7 @@ absl::Status ClientSession::Run(const RunOptions& run_options,\n                                 const std::vector<Operation>& run_outputs,\n                                 std::vector<Tensor>* outputs,\n                                 RunMetadata* run_metadata) const {\n-  std::vector<std::pair<string, Tensor>> feeds;\n+  std::vector<std::pair<std::string, Tensor>> feeds;\n   feeds.reserve(inputs.size());\n   for (auto const& feed : inputs) {\n     TF_RETURN_IF_ERROR(feed.second.status);\n@@ -117,12 +117,12 @@ absl::Status ClientSession::Run(const RunOptions& run_options,\n                        std::forward_as_tuple(feed.second.tensor));\n   }\n \n-  std::vector<string> output_tensor_names;\n+  std::vector<std::string> output_tensor_names;\n   output_tensor_names.reserve(fetch_outputs.size());\n   for (auto const& output : fetch_outputs) {\n     output_tensor_names.push_back(output.name());\n   }\n-  std::vector<string> target_node_names;\n+  std::vector<std::string> target_node_names;\n   target_node_names.reserve(run_outputs.size());\n   for (auto const& output : run_outputs) {\n     target_node_names.push_back(output.node()->name());\n@@ -138,17 +138,17 @@ absl::Status ClientSession::Run(\n     const std::vector<Operation>& run_outputs, std::vector<Tensor>* outputs,\n     RunMetadata* run_metadata,\n     const thread::ThreadPoolOptions& threadpool_options) const {\n-  std::vector<std::pair<string, Tensor>> feeds;\n+  std::vector<std::pair<std::string, Tensor>> feeds;\n   for (auto const& feed : inputs) {\n     TF_RETURN_IF_ERROR(feed.second.status);\n     feeds.emplace_back(feed.first.name(), feed.second.tensor);\n   }\n-  std::vector<string> output_tensor_names;\n+  std::vector<std::string> output_tensor_names;\n   output_tensor_names.reserve(fetch_outputs.size());\n   for (auto const& output : fetch_outputs) {\n     output_tensor_names.push_back(output.name());\n   }\n-  std::vector<string> target_node_names;\n+  std::vector<std::string> target_node_names;\n   target_node_names.reserve(run_outputs.size());\n   for (auto const& output : run_outputs) {\n     target_node_names.push_back(output.node()->name());"
        },
        {
            "sha": "bf5cf8b2c6c371b224264ab1a6fcbd729c542572",
            "filename": "tensorflow/cc/client/client_session.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcc%2Fclient%2Fclient_session.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcc%2Fclient%2Fclient_session.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcc%2Fclient%2Fclient_session.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -65,7 +65,7 @@ class ClientSession {\n \n   /// Create a new session to evaluate the graph contained in `scope` by\n   /// connecting to the TensorFlow runtime specified by `target`.\n-  ClientSession(const Scope& scope, const string& target);\n+  ClientSession(const Scope& scope, const std::string& target);\n \n   /// Same as above, but use the empty string (\"\") as the target specification.\n   explicit ClientSession(const Scope& scope);"
        },
        {
            "sha": "87cb051b75df6316d419b66a38ce5cf7e5323c1e",
            "filename": "tensorflow/compiler/aot/codegen.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Fcodegen.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Fcodegen.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Faot%2Fcodegen.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -670,33 +670,24 @@ absl::Status ExtendRewrites(\n         \" is outside the range of temp sizes: [0,\", buffer_infos_size, \")\"));\n   }\n \n-  const bool xla_cpu_multi_thread_eigen =\n-      xla::GetDebugOptionsFromFlags().xla_cpu_multi_thread_eigen();\n-\n   std::vector<std::string> runtime_specific_includes = {R\"(\n #include \"absl/log/check.h\"\n+#include \"absl/synchronization/blocking_counter.h\"\n #include \"xla/backends/cpu/runtime/kernel_c_api.h\"\n #include \"xla/types.h\")\"};\n \n   if (HasThunkKind(aot_thunks->proto().thunk_sequence(),\n                    xla::cpu::ThunkProto::kDotThunk)) {\n-    if (xla_cpu_multi_thread_eigen) {\n-      runtime_specific_includes.push_back(\n-          R\"(#include \"xla/service/cpu/runtime_matmul.h\")\");\n-    }\n     runtime_specific_includes.push_back(\n-        R\"(#include \"xla/service/cpu/runtime_single_threaded_matmul.h\")\");\n+        R\"(#include \"xla/backends/cpu/runtime/dot_lib.h\")\");\n   }\n \n   if (HasThunkKind(aot_thunks->proto().thunk_sequence(),\n                    xla::cpu::ThunkProto::kConvolutionThunk)) {\n-    if (xla_cpu_multi_thread_eigen) {\n-      runtime_specific_includes.push_back(\n-          R\"(#include \"xla/service/cpu/runtime_conv2d.h\")\");\n-    }\n-\n     runtime_specific_includes.push_back(\n-        R\"(#include \"xla/service/cpu/runtime_single_threaded_conv2d.h\")\");\n+        R\"(#include \"absl/synchronization/notification.h\")\");\n+    runtime_specific_includes.push_back(\n+        R\"(#include \"xla/backends/cpu/runtime/convolution_lib.h\")\");\n   }\n \n   if (HasThunkKind(aot_thunks->proto().thunk_sequence(),\n@@ -708,7 +699,7 @@ absl::Status ExtendRewrites(\n   if (HasThunkKind(aot_thunks->proto().thunk_sequence(),\n                    xla::cpu::ThunkProto::kTopKThunk)) {\n     runtime_specific_includes.push_back(\n-        R\"(#include \"xla/service/cpu/runtime_topk.h\")\");\n+        R\"(#include \"xla/backends/cpu/runtime/topk_lib.h\")\");\n   }\n \n   TF_ASSIGN_OR_RETURN("
        },
        {
            "sha": "67fea2e6a022c1fc2a336693c5c8428283141b6d",
            "filename": "tensorflow/compiler/aot/tfcompile.bzl",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Ftfcompile.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Ftfcompile.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Faot%2Ftfcompile.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -322,10 +322,11 @@ def _tf_library(\n             # include_standard_runtime_deps is False.  Without them, the\n             # generated code will fail to compile.\n             \"//third_party/absl/log:check\",\n+            \"//third_party/absl/synchronization\",\n+            \"//tensorflow/core:framework_lite\",\n             \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\n             \"@local_xla//xla:types\",\n             \"@local_xla//xla/backends/cpu/runtime:kernel_c_api\",\n-            \"//tensorflow/core:framework_lite\",\n             \"@local_xla//xla/backends/cpu/runtime:rng_state_lib\",\n         ] + (need_xla_data_proto and [\n             # If we're generating the program shape, we must depend on the\n@@ -336,11 +337,11 @@ def _tf_library(\n         ] or []) + (include_standard_runtime_deps and [\n             # TODO(cwhipkey): only depend on kernel code that the model actually\n             # needed.\n+            \"@local_xla//xla/backends/cpu/runtime:dot_lib\",\n             \"@local_xla//xla/backends/cpu/runtime:sort_lib\",\n-            \"@local_xla//xla/service/cpu:runtime_conv2d\",\n+            \"@local_xla//xla/backends/cpu/runtime:topk_lib\",\n+            \"@local_xla//xla/backends/cpu/runtime:convolution_lib\",\n             \"@local_xla//xla/service/cpu:runtime_matmul\",\n-            \"@local_xla//xla/service/cpu:runtime_topk\",\n-            \"@local_xla//xla/service/cpu:runtime_single_threaded_conv2d\",\n             \"@local_xla//xla/service/cpu:runtime_single_threaded_matmul\",\n             \"@eigen_archive//:eigen3\",\n         ] or []) + (use_xla_nanort_runtime and ["
        },
        {
            "sha": "485bfd36dfa0a53cfb163584dde07d32bfb2e5b6",
            "filename": "tensorflow/compiler/aot/thunk_proto_execution_deserializer.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 127,
            "changes": 173,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Fthunk_proto_execution_deserializer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Fthunk_proto_execution_deserializer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Faot%2Fthunk_proto_execution_deserializer.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -127,32 +127,23 @@ ThunkProtoExecutionDeserializer::ThunkSpecificRunImplFromThunkSequence(\n }\n \n absl::StatusOr<std::string> ThunkProtoExecutionDeserializer::GetMatmulFunction(\n-    xla::PrimitiveType xla_type, bool is_single_threaded) {\n+    xla::PrimitiveType xla_type) {\n   switch (xla_type) {\n     case xla::F16:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedMatMulF16\"\n-                 : \"__xla_cpu_runtime_EigenMatMulF16\";\n+      return \"::xla::cpu::internal::TypedMatMul<Eigen::half, Eigen::half, \"\n+             \"Eigen::half>\";\n     case xla::F32:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedMatMulF32\"\n-                 : \"__xla_cpu_runtime_EigenMatMulF32\";\n+      return \"::xla::cpu::internal::TypedMatMul<float, float, float>\";\n     case xla::F64:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedMatMulF64\"\n-                 : \"__xla_cpu_runtime_EigenMatMulF64\";\n+      return \"::xla::cpu::internal::TypedMatMul<double, double, double>\";\n     case xla::C64:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedMatMulC64\"\n-                 : \"__xla_cpu_runtime_EigenMatMulC64\";\n+      return \"::xla::cpu::internal::TypedMatMul<std::complex<float>, \"\n+             \"std::complex<float>, std::complex<float>\";\n     case xla::C128:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedMatMulC128\"\n-                 : \"__xla_cpu_runtime_EigenMatMulC128\";\n+      return \"::xla::cpu::internal::TypedMatMul<std::complex<double>, \"\n+             \"std::complex<double>, std::complex<double>\";\n     case xla::S32:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedMatMulS32\"\n-                 : \"__xla_cpu_runtime_EigenMatMulS32\";\n+      return \"::xla::cpu::internal::TypedMatMul<int32_t, int32_t, int32_t>\";\n     default:\n       return xla::Internal(\"Unsupported xla type: %d\", xla_type);\n   }\n@@ -166,43 +157,23 @@ absl::StatusOr<std::string> ThunkProtoExecutionDeserializer::GetDotThunkRunImpl(\n   }\n   const xla::cpu::DotThunkProto& dot_thunk = thunk.dot_thunk();\n \n-  absl::string_view dot_thunk_invocation_format = xla_cpu_multi_thread_eigen_\n-                                                      ? R\"(\n+  absl::string_view dot_thunk_invocation_format = R\"(\n      // Dot Thunk\n      {\n+        absl::BlockingCounter done({{BATCH_SIZE}});\n         for (int64_t i = 0; i < {{BATCH_SIZE}}; ++i) {\n-          if (run_options->intra_op_thread_pool() != nullptr) {\n-            {{MATMUL_FUNCTION}}(\n-              run_options,\n-              {{OUTPUT_PTR}} + {{OUTPUT_STRIDE}} * i,\n-              {{LHS_PTR}} + {{LHS_STRIDE}} * i,\n-              {{RHS_PTR}} + {{RHS_STRIDE}} * i,\n-              {{M}}, {{N}}, {{K}}, {{TRANSPOSE_LHS}}, {{TRANSPOSE_RHS}});\n-          } else {\n-            {{SINGLE_THREADED_MATMUL_FUNCTION}}(\n-                nullptr,\n-                {{OUTPUT_PTR}} + {{OUTPUT_STRIDE}} * i,\n-                {{LHS_PTR}} + {{LHS_STRIDE}} * i,\n-                {{RHS_PTR}} + {{RHS_STRIDE}} * i,\n-                {{M}}, {{N}}, {{K}}, {{TRANSPOSE_LHS}}, {{TRANSPOSE_RHS}});\n-          }\n+          {{MATMUL_FUNCTION}}(\n+            run_options->intra_op_thread_pool(),\n+            {{OUTPUT_PTR}} + {{OUTPUT_STRIDE}} * i,\n+            {{LHS_PTR}} + {{LHS_STRIDE}} * i,\n+            {{RHS_PTR}} + {{RHS_STRIDE}} * i,\n+            {{M}}, {{N}}, {{K}}, {{TRANSPOSE_LHS}}, {{TRANSPOSE_RHS}},\n+            [&done] { done.DecrementCount(); }\n+          );\n         }\n+        done.Wait();\n      }\n-     )\"\n-                                                      :\n-                                                      R\"(\n-      // Dot Thunk\n-      {\n-         for (int64_t i = 0; i < {{BATCH_SIZE}}; ++i) {\n-          {{SINGLE_THREADED_MATMUL_FUNCTION}}(\n-                nullptr,\n-                {{OUTPUT_PTR}} + {{OUTPUT_STRIDE}} * i,\n-                {{LHS_PTR}} + {{LHS_STRIDE}} * i,\n-                {{RHS_PTR}} + {{RHS_STRIDE}} * i,\n-                {{M}}, {{N}}, {{K}}, {{TRANSPOSE_LHS}}, {{TRANSPOSE_RHS}});\n-         }\n-      }\n-      )\";\n+     )\";\n \n   if (!(dot_thunk.lhs_buffer_shape().shape().element_type() ==\n             dot_thunk.rhs_buffer_shape().shape().element_type() &&\n@@ -214,13 +185,7 @@ absl::StatusOr<std::string> ThunkProtoExecutionDeserializer::GetDotThunkRunImpl(\n \n   TF_ASSIGN_OR_RETURN(\n       std::string matmul_function,\n-      GetMatmulFunction(dot_thunk.lhs_buffer_shape().shape().element_type(),\n-                        /*is_single_threaded=*/false));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::string single_threaded_matmul_function,\n-      GetMatmulFunction(dot_thunk.lhs_buffer_shape().shape().element_type(),\n-                        /*is_single_threaded=*/true));\n+      GetMatmulFunction(dot_thunk.lhs_buffer_shape().shape().element_type()));\n \n   TF_ASSIGN_OR_RETURN(std::string data_type,\n                       CppDataTypeFromXlaType(\n@@ -280,7 +245,7 @@ absl::StatusOr<std::string> ThunkProtoExecutionDeserializer::GetDotThunkRunImpl(\n   int64_t out_stride = m * n;\n \n   std::vector<std::pair<std::string, std::string>> rewrites = {\n-      {\"{{SINGLE_THREADED_MATMUL_FUNCTION}}\", single_threaded_matmul_function},\n+      {\"{{MATMUL_FUNCTION}}\", matmul_function},\n       {\"{{OUTPUT_PTR}}\", output_ptr},\n       {\"{{OUTPUT_STRIDE}}\", absl::StrCat(out_stride)},\n       {\"{{LHS_PTR}}\", lhs_ptr},\n@@ -294,25 +259,17 @@ absl::StatusOr<std::string> ThunkProtoExecutionDeserializer::GetDotThunkRunImpl(\n       {\"{{TRANSPOSE_RHS}}\", transpose_rhs ? \"true\" : \"false\"},\n       {\"{{BATCH_SIZE}}\", absl::StrCat(dot_shape.batch_size)}};\n \n-  if (xla_cpu_multi_thread_eigen_) {\n-    rewrites.push_back({\"{{MATMUL_FUNCTION}}\", matmul_function});\n-  }\n-\n   return absl::StrReplaceAll(dot_thunk_invocation_format, rewrites);\n };\n \n absl::StatusOr<std::string>\n ThunkProtoExecutionDeserializer::GetConvolutionFunction(\n-    xla::PrimitiveType xla_type, bool is_single_threaded) {\n+    xla::PrimitiveType xla_type) {\n   switch (xla_type) {\n     case xla::F16:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedConv2DF16\"\n-                 : \"__xla_cpu_runtime_EigenConv2DF16\";\n+      return \"xla::cpu::internal::EigenConv2D<Eigen::half>\";\n     case xla::F32:\n-      return is_single_threaded\n-                 ? \"__xla_cpu_runtime_EigenSingleThreadedConv2DF32\"\n-                 : \"__xla_cpu_runtime_EigenConv2DF32\";\n+      return \"xla::cpu::internal::EigenConv2D<float>\";\n     default:\n       return xla::Internal(\"Unsupported xla type: %d\", xla_type);\n   }\n@@ -345,63 +302,28 @@ ThunkProtoExecutionDeserializer::GetConvolution2DRunImpl(\n   TF_ASSIGN_OR_RETURN(\n       std::string convolution_function,\n       GetConvolutionFunction(\n-          convolution_thunk.input_buffer_shape().shape().element_type(),\n-          /*is_single_threaded=*/false));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::string single_threaded_convolution_function,\n-      GetConvolutionFunction(\n-          convolution_thunk.input_buffer_shape().shape().element_type(),\n-          /*is_single_threaded=*/true));\n+          convolution_thunk.input_buffer_shape().shape().element_type()));\n \n-  absl::string_view convolution_thunk_invocation_format =\n-      xla_cpu_multi_thread_eigen_ ? R\"(\n+  absl::string_view convolution_thunk_invocation_format = R\"(\n      // Convolution Thunk\n      {\n-         if (run_options->intra_op_thread_pool() != nullptr) {\n-           {{CONVOLUTION_FUNCTION}}(\n-             run_options,\n-             {{OUTPUT_PTR}}, {{LHS_PTR}}, {{RHS_PTR}}, {{INPUT_BATCH}},\n-             {{INPUT_ROWS}}, {{INPUT_COLS}}, {{INPUT_CHANNELS}}, {{KERNEL_ROWS}},\n-             {{KERNEL_COLS}}, {{KERNEL_CHANNELS}}, {{KERNEL_FILTERS}},\n-             {{OUTPUT_ROWS}}, {{OUTPUT_COLS}}, {{ROW_STRIDE}}, {{COL_STRIDE}},\n-             {{PADDING_TOP}}, {{PADDING_BOTTOM}}, {{PADDING_LEFT}},\n-             {{PADDING_RIGHT}}, {{LHS_ROW_DILATION}}, {{LHS_COL_DILATION}},\n-             {{RHS_ROW_DILATION}}, {{RHS_COL_DILATION}}, {{FEATURE_GROUP_COUNT}}\n-           );\n-         } else {\n-           {{SINGLE_THREADED_CONVOLUTION_FUNCTION}}(\n-             nullptr,\n-             {{OUTPUT_PTR}}, {{LHS_PTR}}, {{RHS_PTR}}, {{INPUT_BATCH}},\n-             {{INPUT_ROWS}}, {{INPUT_COLS}}, {{INPUT_CHANNELS}}, {{KERNEL_ROWS}},\n-             {{KERNEL_COLS}}, {{KERNEL_CHANNELS}}, {{KERNEL_FILTERS}},\n-             {{OUTPUT_ROWS}}, {{OUTPUT_COLS}}, {{ROW_STRIDE}}, {{COL_STRIDE}},\n-             {{PADDING_TOP}}, {{PADDING_BOTTOM}}, {{PADDING_LEFT}},\n-             {{PADDING_RIGHT}}, {{LHS_ROW_DILATION}}, {{LHS_COL_DILATION}},\n-             {{RHS_ROW_DILATION}}, {{RHS_COL_DILATION}}, {{FEATURE_GROUP_COUNT}}\n-           );\n-         }\n-     })\"\n-                                  :\n-                                  R\"(\n-      // Convolution Thunk\n-      {\n-        {{SINGLE_THREADED_CONVOLUTION_FUNCTION}}(\n-              nullptr,\n-              {{OUTPUT_PTR}}, {{LHS_PTR}}, {{RHS_PTR}}, {{INPUT_BATCH}},\n-              {{INPUT_ROWS}}, {{INPUT_COLS}}, {{INPUT_CHANNELS}}, {{KERNEL_ROWS}},\n-              {{KERNEL_COLS}}, {{KERNEL_CHANNELS}}, {{KERNEL_FILTERS}},\n-              {{OUTPUT_ROWS}}, {{OUTPUT_COLS}}, {{ROW_STRIDE}}, {{COL_STRIDE}},\n-              {{PADDING_TOP}}, {{PADDING_BOTTOM}}, {{PADDING_LEFT}},\n-              {{PADDING_RIGHT}}, {{LHS_ROW_DILATION}}, {{LHS_COL_DILATION}},\n-              {{RHS_ROW_DILATION}}, {{RHS_COL_DILATION}}, {{FEATURE_GROUP_COUNT}}\n-            );\n-      }\n-      )\";\n+        absl::Notification done;\n+        {{CONVOLUTION_FUNCTION}}(\n+          run_options->intra_op_thread_pool(),\n+          {{OUTPUT_PTR}}, {{LHS_PTR}}, {{RHS_PTR}}, {{INPUT_BATCH}},\n+          {{INPUT_ROWS}}, {{INPUT_COLS}}, {{INPUT_CHANNELS}}, {{KERNEL_ROWS}},\n+          {{KERNEL_COLS}}, {{KERNEL_CHANNELS}}, {{KERNEL_FILTERS}},\n+          {{OUTPUT_ROWS}}, {{OUTPUT_COLS}}, {{ROW_STRIDE}}, {{COL_STRIDE}},\n+          {{PADDING_TOP}}, {{PADDING_BOTTOM}}, {{PADDING_LEFT}},\n+          {{PADDING_RIGHT}}, {{LHS_ROW_DILATION}}, {{LHS_COL_DILATION}},\n+          {{RHS_ROW_DILATION}}, {{RHS_COL_DILATION}}, {{FEATURE_GROUP_COUNT}},\n+          [&done] { done.Notify(); }\n+        );\n+        done.WaitForNotification();\n+     })\";\n \n   std::vector<std::pair<std::string, std::string>> rewrites = {\n-      {\"{{SINGLE_THREADED_CONVOLUTION_FUNCTION}}\",\n-       single_threaded_convolution_function},\n+      {\"{{CONVOLUTION_FUNCTION}}\", convolution_function},\n       {\"{{OUTPUT_PTR}}\", output_ptr},\n       {\"{{LHS_PTR}}\", lhs_ptr},\n       {\"{{RHS_PTR}}\", rhs_ptr},\n@@ -428,10 +350,6 @@ ThunkProtoExecutionDeserializer::GetConvolution2DRunImpl(\n       {\"{{FEATURE_GROUP_COUNT}}\",\n        absl::StrCat(canonical_dims.feature_group_count)}};\n \n-  if (xla_cpu_multi_thread_eigen_) {\n-    rewrites.push_back({\"{{CONVOLUTION_FUNCTION}}\", convolution_function});\n-  }\n-\n   return absl::StrReplaceAll(convolution_thunk_invocation_format, rewrites);\n }\n \n@@ -624,6 +542,7 @@ ThunkProtoExecutionDeserializer::GetSortThunkRunImpl(\n            [comparator](const void** data) {\n              bool result;\n              (*comparator)(&result, nullptr, data, nullptr, nullptr, nullptr);\n+             ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(&result, sizeof(result));\n              return result;\n            };\n \n@@ -688,7 +607,7 @@ ThunkProtoExecutionDeserializer::GetTopKThunkRunImpl(\n   absl::string_view topk_thunk_invocation_format = R\"(\n      // TopK Thunk\n      {\n-    __xla_cpu_runtime_TopKF32({{BATCH_SIZE}}, {{INPUT_SIZE}}, {{K}},\n+    ::xla::cpu::internal::TopK({{BATCH_SIZE}}, {{INPUT_SIZE}}, {{K}},\n                               reinterpret_cast<const float*>({{VALUES_PTR}}),\n                               reinterpret_cast<float*>({{OUTPUT_PTR}}),\n                               reinterpret_cast<int32_t*>({{INDICES_PTR}}));"
        },
        {
            "sha": "a5f7ddcd5fa13b8008a1785f7089ea66836dc2ef",
            "filename": "tensorflow/compiler/aot/thunk_proto_execution_deserializer.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Fthunk_proto_execution_deserializer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Faot%2Fthunk_proto_execution_deserializer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Faot%2Fthunk_proto_execution_deserializer.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -44,14 +44,13 @@ class ThunkProtoExecutionDeserializer {\n       const xla::cpu::ThunkSequenceProto& thunk_sequence_proto);\n \n  protected:\n-  absl::StatusOr<std::string> GetMatmulFunction(xla::PrimitiveType xla_type,\n-                                                bool is_single_threaded);\n+  absl::StatusOr<std::string> GetMatmulFunction(xla::PrimitiveType xla_type);\n \n   absl::StatusOr<std::string> GetDotThunkRunImpl(\n       const xla::cpu::ThunkProto& thunk);\n \n   absl::StatusOr<std::string> GetConvolutionFunction(\n-      xla::PrimitiveType xla_type, bool is_single_threaded);\n+      xla::PrimitiveType xla_type);\n \n   absl::StatusOr<std::string> GetConvolution2DRunImpl(\n       const xla::cpu::ConvolutionThunkProto& convolution_thunk,"
        },
        {
            "sha": "b5a3319ba13362f5dcabc9288ad8256b4b941d78",
            "filename": "tensorflow/compiler/mlir/lite/kernels/internal/runtime_shape_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fkernels%2Finternal%2Fruntime_shape_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fkernels%2Finternal%2Fruntime_shape_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fkernels%2Finternal%2Fruntime_shape_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -19,9 +19,7 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/lite/kernels/internal/runtime_shape.h\"\n \n #include <cstdint>\n-#include <functional>\n #include <initializer_list>\n-#include <numeric>\n #include <vector>\n \n #include <gmock/gmock.h>"
        },
        {
            "sha": "d6a4b0c3fcbf97ee482b4e4f965b3fa30cb12744",
            "filename": "tensorflow/compiler/mlir/tfrt/tests/sink_in_invariant_ops.mlir",
            "status": "modified",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -195,6 +195,28 @@ func.func @sink_in_stateful_call(%arg0: tensor<i32> {tf_saved_model.index_path =\n   func.return %2 : tensor<i32>\n }\n \n+// Test VarHandleOp getting sinked when it is used by the called function and returned by the called function.\n+\n+// CHECK: func private @func_use_and_return_varhandle([[arg0:.+]]: tensor<!tf_type.resource<tensor<i32>>>)\n+func.func private @func_use_and_return_varhandle(%arg0: tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) {\n+  // CHECK: tf.VarHandleOp\n+  // CHECK-NEXT: tf.ReadVariableOp\n+  %0 = \"tf.ReadVariableOp\"(%arg0) {device = \"cpu\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+\n+  func.return %0, %arg0 : tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>\n+}\n+\n+// CHECK-LABEL: func @sink_in_stateful_call_varhandle_return\n+func.func @sink_in_stateful_call_varhandle_return(%arg0: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n+  attributes {tf_saved_model.exported_names = [\"test_sink_in_stateful_call_varhandle_return\"]} {\n+  // CHECK: tf.VarHandleOp\n+  %0 = \"tf.VarHandleOp\"() {container = \"\", shared_name = \"x\"} : () -> tensor<!tf_type.resource<tensor<i32>>>\n+  // CHECK: \"tf.StatefulPartitionedCall\"(%0)\n+  %1:2 = \"tf.StatefulPartitionedCall\"(%0) {device = \"/CPU:0\", config = \"\", config_proto = \"\", executor_type = \"\", f = @func_use_and_return_varhandle} : (tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>)\n+  %2 = \"tf.AddV2\"(%arg0, %1#0) {device = \"/CPU:0\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\n+  func.return %2 : tensor<i32>\n+}\n+\n // CHECK-LABEL: func @sink_in_if\n func.func @sink_in_if(%arg0: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n   attributes {tf_saved_model.exported_names = [\"test_sink_in_if\"]} {\n@@ -374,3 +396,54 @@ func.func @nested_sink_in_if(%arg: tensor<i32> {tf_saved_model.index_path = [\"in\n }\n \n }\n+\n+// -----\n+\n+module attributes {tf_saved_model.semantics} {\n+\n+// Test sinks crossing nested tf.While and BatchFunction, while the sinkable ops are only copied at the target.\n+\n+// CHECK-LABEL: func private @batched_function\n+func.func private @batched_function(%arg0: tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  attributes {tf._input_shapes = [#tf_type.shape<1x3>, #tf_type.shape<*>], tf.signature.is_stateful} {\n+  // CHECK: tf.VarHandleOp\n+  // CHECK-NEXT: tf.ReadVariableOp\n+  %1 = \"tf.ReadVariableOp\"(%arg0) {device = \"/device:CPU:0\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  %2 = \"tf.Identity\"(%1) {device = \"/device:CPU:0\"} : (tensor<i32>) -> tensor<i32>\n+  func.return %2 : tensor<i32>\n+}\n+\n+// CHECK-LABEL: func private @while_cond_func\n+func.func private @while_cond_func(\n+    %arg0: tensor<i32>,\n+    %arg1: tensor<i32>,\n+    %arg: tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32> {\n+  // CHECK: [[handle:%.*]] = \"tf.VarHandleOp\"()\n+  // CHECK: \"tf.ReadVariableOp\"([[handle]])\n+  %0 = \"tf.ReadVariableOp\"(%arg) {device = \"cpu\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  func.return %0 : tensor<i32>\n+}\n+\n+// CHECK-LABEL: func private @while_body_func\n+func.func private @while_body_func(\n+    %arg0: tensor<i32>,\n+    %arg1: tensor<i32>,\n+    %arg2: tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) {\n+  // CHECK: \"tf.BatchFunction\"(%arg2)\n+  %0 = \"tf.BatchFunction\"(%arg2) {allowed_batch_sizes = [6], batch_timeout_micros = 100000 : i64, batching_queue = \"\", container = \"\", device = \"/device:CPU:0\", enable_large_batch_splitting = false, f = @batched_function, max_batch_size = 6 : i64, max_enqueued_batches = 10 : i64, num_batch_threads = 1 : i64, operandSegmentSizes = array<i32: 1, 0>, shared_name = \"batch/\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  func.return %0, %arg0, %arg2 : tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>\n+}\n+\n+// CHECK-LABEL: func @nested_sink_in_while_and_batch_functions\n+func.func @nested_sink_in_while_and_batch_functions(%arg: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n+  attributes {tf_saved_model.exported_names = [\"test_sink_in_while_and_batch_functions\"]} {\n+  // CHECK: [[handle:%.*]] = \"tf.VarHandleOp\"()\n+  %handle = \"tf.VarHandleOp\"() {container = \"\", shared_name = \"x\"} : () -> tensor<!tf_type.resource<tensor<i32>>>\n+  // CHECK: [[cond:%.*]] = \"tf.Const\"()\n+  %cond = \"tf.Const\"() {device = \"/CPU:0\", value = dense<0> : tensor<i32>} : () -> tensor<i32>\n+  // CHECK: \"tf.While\"([[cond]], [[cond]], [[handle]])\n+  %x:3 = \"tf.While\"(%cond, %cond, %handle) {body = @while_body_func, cond = @while_cond_func, is_stateless = false, parallel_iterations = 10 : i64, shape_invariant} : (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>)\n+  func.return %x#0 : tensor<i32>\n+}\n+\n+}"
        },
        {
            "sha": "fddb217d4c57ee1d7c8e6bc0ad208ae0f12bf6b7",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/sink_in_invariant_ops.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 4,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -49,15 +49,28 @@ bool IsSinkCandidate(mlir::Operation *op) {\n // Check if the op is allowed to be sinked. We are being conservative here to\n // whilelist very limited set of ops here.\n struct AllowSinkHelper {\n-  explicit AllowSinkHelper(mlir::Operation *op, int arg_index) {\n+  explicit AllowSinkHelper(mlir::Operation* sinked_op, mlir::Operation* user,\n+                           int arg_index) {\n     if (llvm::isa<mlir::TF::BatchFunctionOp,\n-                  mlir::TF::StatefulPartitionedCallOp>(op)) {\n+                  mlir::TF::StatefulPartitionedCallOp>(user)) {\n       allow_sink_to = true;\n       callee_arg_index = arg_index;\n       return;\n     }\n \n-    if (llvm::isa<mlir::TF::IfOp>(op) && arg_index > 0) {\n+    // We tend to limit this support on WhileOp to only VarHandleOp to satisfy\n+    // IFRT lowering requirements.\n+    // Sinking other invariants like ConstOp is error-prone because it requires\n+    // non-trivial effort to avoid sinking Consts when they are used by cond\n+    // function and we don't need such support.\n+    if (llvm::isa<mlir::TF::VarHandleOp>(sinked_op) &&\n+        llvm::isa<mlir::TF::WhileOp>(user)) {\n+      allow_sink_to = true;\n+      callee_arg_index = arg_index;\n+      return;\n+    }\n+\n+    if (llvm::isa<mlir::TF::IfOp>(user) && arg_index > 0) {\n       allow_sink_to = true;\n       callee_arg_index = arg_index - 1;\n       return;\n@@ -107,7 +120,8 @@ void FindSinkTarget(\n   for (mlir::OpOperand &use : value.getUses()) {\n     auto *user = use.getOwner();\n \n-    AllowSinkHelper helper(user, use.getOperandNumber());\n+    AllowSinkHelper helper(original.getDefiningOp(), user,\n+                           use.getOperandNumber());\n \n     if (helper.allow_sink_to) {\n       auto values = FindValueInCallees(symbol_table, symbol_users, user,\n@@ -116,6 +130,14 @@ void FindSinkTarget(\n         FindSinkTarget(symbol_table, symbol_users, original, value, targets);\n       }\n     } else if (value != original) {\n+      // If the sinked op is directly used by ReturnOp, we don't sink it.\n+      // One example is for tf.WhileOp, the input and output of the cond\n+      // function and the body function must be the same. If the cond function\n+      // has an input of type tf.VarHandleOp and it just return the VarHandleOp,\n+      // we don't need to sink it.\n+      if (llvm::isa<mlir::func::ReturnOp>(user)) {\n+        continue;\n+      }\n       targets[&use].insert(original);\n     }\n   }"
        },
        {
            "sha": "527a724c491b968d76708f0522614eb5fec2ae3e",
            "filename": "tensorflow/compiler/mlir/tfrt/translate/mlrt/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -19,6 +19,7 @@ cc_library(\n     srcs = [\"mlir_to_bytecode.cc\"],\n     hdrs = [\"mlir_to_bytecode.h\"],\n     deps = [\n+        \"//tensorflow/compiler/mlir/tfrt/ir/mlrt:mlrt_ops\",\n         \"//tensorflow/core/tfrt/mlrt/bytecode\",\n         \"//tensorflow/core/tfrt/mlrt/bytecode:executable\",\n         \"//tensorflow/core/tfrt/mlrt/bytecode:function\",\n@@ -43,6 +44,7 @@ tf_cc_test(\n     data = glob([\"testdata/**\"]),\n     deps = [\n         \":mlir_to_bytecode\",\n+        \"//tensorflow/compiler/mlir/tfrt/ir/mlrt:mlrt_ops\",\n         \"//tensorflow/core/tfrt/mlrt/bytecode\",\n         \"//tensorflow/core/tfrt/mlrt/bytecode:executable\",\n         \"//tensorflow/core/tfrt/mlrt/interpreter:attribute_span\","
        },
        {
            "sha": "2324f958f1926602836534563eed238d2d49e694",
            "filename": "tensorflow/compiler/mlir/tfrt/translate/mlrt/mlir_to_bytecode.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 13,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Fmlir_to_bytecode.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Fmlir_to_bytecode.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Fmlir_to_bytecode.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"mlir/IR/Types.h\"  // from @llvm-project\n #include \"mlir/IR/Value.h\"  // from @llvm-project\n #include \"mlir/Support/LLVM.h\"  // from @llvm-project\n+#include \"tensorflow/compiler/mlir/tfrt/ir/mlrt/mlrt_dialect.h\"\n #include \"tensorflow/core/tfrt/mlrt/bytecode/bytecode.h\"\n #include \"tensorflow/core/tfrt/mlrt/bytecode/executable.h\"\n #include \"tensorflow/core/tfrt/mlrt/bytecode/function.h\"\n@@ -169,19 +170,26 @@ struct FunctionEmitterContext {\n   struct RegInfo {\n     int num_uses = 0;\n     int id = -1;\n+    bool persistent = false;  // True if the register should not be freed\n   };\n \n   int next_reg_id = 0;\n   llvm::DenseMap<mlir::Value, RegInfo> register_table;\n   std::vector<int> free_regs;\n \n-  int AssignRegId() {\n-    if (free_regs.empty()) {\n+  int AssignRegId(bool is_persistent) {\n+    if (is_persistent) {\n+      // Persistent types ALWAYS get a brand new ID.\n       return next_reg_id++;\n     }\n-    int id = free_regs.back();\n-    free_regs.pop_back();\n-    return id;\n+\n+    // Non-persistent types can reuse from free_regs.\n+    if (!free_regs.empty()) {\n+      int id = free_regs.back();\n+      free_regs.pop_back();\n+      return id;\n+    }\n+    return next_reg_id++;\n   }\n \n   void FreeRegId(int id) { free_regs.push_back(id); }\n@@ -202,7 +210,7 @@ void EmitKernel(FunctionEmitterContext& function_context,\n     auto iter = function_context.register_table.find(result);\n     CHECK(iter != function_context.register_table.end());  // Crash Ok\n     CHECK_EQ(iter->second.id, -1);                         // Crash Ok\n-    iter->second.id = function_context.AssignRegId();\n+    iter->second.id = function_context.AssignRegId(iter->second.persistent);\n     results.push_back(iter->second.id);\n   }\n   constructor.construct_results(results.size())\n@@ -218,9 +226,12 @@ void EmitKernel(FunctionEmitterContext& function_context,\n     int id = iter->second.id;\n     CHECK_NE(id, -1);  // Crash Ok\n     last_uses.push_back(0);\n-    if (--iter->second.num_uses == 0) {\n-      function_context.FreeRegId(id);\n-      last_uses.back() = 1;\n+    auto& reg_info = iter->second;\n+    if (!reg_info.persistent) {\n+      if (--reg_info.num_uses == 0) {\n+        function_context.FreeRegId(id);\n+        last_uses.back() = 1;\n+      }\n     }\n     arguments.push_back(id);\n   }\n@@ -282,18 +293,23 @@ void EmitFunction(const ModuleEmitterContext& module_context,\n   std::vector<uint32_t> input_regs;\n   input_regs.reserve(block.getNumArguments());\n   for (auto arg : block.getArguments()) {\n-    int id = function_context.AssignRegId();\n+    bool persistent = mlir::isa<mlrt::compiler::AsyncHandleType>(arg.getType());\n+    int id = function_context.AssignRegId(persistent);\n     input_regs.push_back(id);\n     register_table[arg] = {static_cast<int>(std::distance(arg.getUses().begin(),\n                                                           arg.getUses().end())),\n-                           id};\n+                           id, persistent};\n   }\n   constructor.construct_input_regs(input_regs);\n \n   for (auto& op : block) {\n     for (auto result : op.getResults()) {\n-      register_table[result] = {static_cast<int>(\n-          std::distance(result.getUses().begin(), result.getUses().end()))};\n+      bool persistent =\n+          mlir::isa<mlrt::compiler::AsyncHandleType>(result.getType());\n+      register_table[result] = {\n+          static_cast<int>(\n+              std::distance(result.getUses().begin(), result.getUses().end())),\n+          -1, persistent};\n     }\n   }\n "
        },
        {
            "sha": "53f2e7591c8a9a6fbefc935b77940146f114234a",
            "filename": "tensorflow/compiler/mlir/tfrt/translate/mlrt/mlir_to_bytecode_test.cc",
            "status": "modified",
            "additions": 125,
            "deletions": 0,
            "changes": 125,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Fmlir_to_bytecode_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Fmlir_to_bytecode_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Fmlir_to_bytecode_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n #include \"mlir/Parser/Parser.h\"  // from @llvm-project\n #include \"mlir/Support/LLVM.h\"  // from @llvm-project\n+#include \"tensorflow/compiler/mlir/tfrt/ir/mlrt/mlrt_dialect.h\"\n #include \"xla/tsl/platform/resource_loader.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n #include \"tensorflow/core/tfrt/mlrt/bytecode/bytecode.h\"\n@@ -382,5 +383,129 @@ TEST(MlirToByteCodeTest, CustomDense) {\n   }\n }\n \n+TEST(MlirToByteCodeTest, AsyncNotFreed) {\n+  constexpr char kAsyncMlir[] =\n+      \"tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata/async.mlir\";\n+\n+  mlir::DialectRegistry registry;\n+  registry.insert<mlir::func::FuncDialect, mlrt::compiler::MlrtDialect>();\n+  mlir::MLIRContext mlir_context(registry);\n+  mlir_context.allowUnregisteredDialects();\n+  auto mlir_module = mlir::parseSourceFile<mlir::ModuleOp>(\n+      tsl::GetDataDependencyFilepath(kAsyncMlir), &mlir_context);\n+\n+  AttributeEncoderRegistry attribute_encoder_registry;\n+  bc::Buffer buffer =\n+      EmitExecutable(attribute_encoder_registry, mlir_module.get()).value();\n+\n+  bc::Executable executable(buffer.data());\n+\n+  auto kernel_names = executable.kernel_names();\n+  EXPECT_THAT(kernel_names,\n+              ElementsAreArray({\"test_mlbc.add.i32\", \"return\", \"mlrt.async\",\n+                                \"mlrt.await_handle\"}));\n+\n+  auto functions = executable.functions();\n+  ASSERT_EQ(functions.size(), 2);\n+\n+  auto function = functions[1];\n+  EXPECT_EQ(function.name().str(), \"main\");\n+  EXPECT_EQ(function.num_regs(), 4);\n+  EXPECT_THAT(function.input_regs(), ElementsAreArray({0, 1}));\n+  EXPECT_THAT(function.output_regs(), ElementsAreArray({1}));\n+  EXPECT_THAT(function.output_last_uses(), ElementsAreArray({true}));\n+\n+  auto kernels = function.kernels();\n+  ASSERT_EQ(kernels.size(), 5);\n+\n+  EXPECT_EQ(kernels[0].code(), 2);  // mlrt.async\n+  EXPECT_THAT(kernels[0].arguments(), ElementsAreArray({0, 1}));\n+  // The returned handle is in register 2, which is never used by other kernels.\n+  EXPECT_THAT(kernels[0].results(), ElementsAreArray({2}));\n+  EXPECT_THAT(kernels[0].last_uses(), ElementsAreArray({false, false}));\n+\n+  EXPECT_EQ(kernels[1].code(), 3);  // mlrt.await_handle\n+  EXPECT_THAT(kernels[1].arguments(), ElementsAreArray({2}));\n+  EXPECT_THAT(kernels[1].results(), IsEmpty());\n+\n+  EXPECT_EQ(kernels[2].code(), 0);  // test_mlbc.add.i32\n+  EXPECT_THAT(kernels[2].arguments(), ElementsAreArray({0, 1}));\n+  EXPECT_THAT(kernels[2].results(), ElementsAreArray({3}));\n+  EXPECT_THAT(kernels[2].last_uses(), ElementsAreArray({true, true}));\n+\n+  EXPECT_EQ(kernels[3].code(), 0);  // test_mlbc.add.i32\n+  EXPECT_THAT(kernels[3].arguments(), ElementsAreArray({3, 3}));\n+  EXPECT_THAT(kernels[3].results(), ElementsAreArray({1}));\n+  EXPECT_THAT(kernels[3].last_uses(), ElementsAreArray({false, true}));\n+\n+  EXPECT_EQ(kernels[4].code(), 1);  // return\n+  EXPECT_THAT(kernels[4].arguments(), ElementsAreArray({1}));\n+  EXPECT_THAT(kernels[4].results(), IsEmpty());\n+  EXPECT_THAT(kernels[4].last_uses(), ElementsAreArray({true}));\n+}\n+\n+TEST(MlirToByteCodeTest, AsyncUseNewId) {\n+  constexpr char kAsyncMlir[] =\n+      \"tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata/async2.mlir\";\n+\n+  mlir::DialectRegistry registry;\n+  registry.insert<mlir::func::FuncDialect, mlrt::compiler::MlrtDialect>();\n+  mlir::MLIRContext mlir_context(registry);\n+  mlir_context.allowUnregisteredDialects();\n+  auto mlir_module = mlir::parseSourceFile<mlir::ModuleOp>(\n+      tsl::GetDataDependencyFilepath(kAsyncMlir), &mlir_context);\n+\n+  AttributeEncoderRegistry attribute_encoder_registry;\n+  bc::Buffer buffer =\n+      EmitExecutable(attribute_encoder_registry, mlir_module.get()).value();\n+\n+  bc::Executable executable(buffer.data());\n+\n+  auto kernel_names = executable.kernel_names();\n+  EXPECT_THAT(kernel_names,\n+              ElementsAreArray({\"test_mlbc.add.i32\", \"return\", \"mlrt.async\",\n+                                \"mlrt.await_handle\"}));\n+\n+  auto functions = executable.functions();\n+  ASSERT_EQ(functions.size(), 2);\n+\n+  auto function = functions[1];\n+  EXPECT_EQ(function.name().str(), \"main\");\n+  EXPECT_EQ(function.num_regs(), 4);\n+  EXPECT_THAT(function.input_regs(), ElementsAreArray({0, 1}));\n+  EXPECT_THAT(function.output_regs(), ElementsAreArray({1}));\n+  EXPECT_THAT(function.output_last_uses(), ElementsAreArray({true}));\n+\n+  auto kernels = function.kernels();\n+  ASSERT_EQ(kernels.size(), 5);\n+\n+  EXPECT_EQ(kernels[0].code(), 0);  // test_mlbc.add.i32\n+  EXPECT_THAT(kernels[0].arguments(), ElementsAreArray({0, 1}));\n+  EXPECT_THAT(kernels[0].results(), ElementsAreArray({2}));\n+  EXPECT_THAT(kernels[0].last_uses(), ElementsAreArray({true, true}));\n+\n+  EXPECT_EQ(kernels[1].code(), 2);  // mlrt.async\n+  EXPECT_THAT(kernels[1].arguments(), ElementsAreArray({2, 2}));\n+  // The returned handle is in register 3, which is never used by other kernels.\n+  EXPECT_THAT(kernels[1].results(), ElementsAreArray({3}));\n+  EXPECT_THAT(kernels[1].last_uses(), ElementsAreArray({false, false}));\n+\n+  EXPECT_EQ(kernels[2].code(), 3);  // mlrt.await_handle\n+  EXPECT_THAT(kernels[2].arguments(), ElementsAreArray({3}));\n+  EXPECT_THAT(kernels[2].results(), IsEmpty());\n+  EXPECT_THAT(kernels[2].last_uses(), ElementsAreArray({false}));\n+\n+  EXPECT_EQ(kernels[3].code(), 0);  // test_mlbc.add.i32\n+  EXPECT_THAT(kernels[3].arguments(), ElementsAreArray({2, 2}));\n+  // AsyncHandle does not free its register. So this can only use 1.\n+  EXPECT_THAT(kernels[3].results(), ElementsAreArray({1}));\n+  EXPECT_THAT(kernels[3].last_uses(), ElementsAreArray({false, true}));\n+\n+  EXPECT_EQ(kernels[4].code(), 1);  // return\n+  EXPECT_THAT(kernels[4].arguments(), ElementsAreArray({1}));\n+  EXPECT_THAT(kernels[4].results(), IsEmpty());\n+  EXPECT_THAT(kernels[4].last_uses(), ElementsAreArray({true}));\n+}\n+\n }  // namespace\n }  // namespace mlrt"
        },
        {
            "sha": "f3816531218c815e47459a83f6cecfea8557bc3d",
            "filename": "tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata/async.mlir",
            "status": "added",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Ftestdata%2Fasync.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Ftestdata%2Fasync.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Ftestdata%2Fasync.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,15 @@\n+func.func @add_i32(%arg0: i32, %arg1: i32) -> i32 {\n+  %0 = \"test_mlbc.add.i32\"(%arg0, %arg1) : (i32, i32) -> i32\n+  func.return %0 : i32\n+}\n+\n+func.func @main(%arg0: i32, %arg1: i32) -> i32 {\n+  %handle = \"mlrt.async\"(%arg0, %arg1) {callee = @add_i32} : (i32, i32) -> !mlrt.async_handle\n+\n+  \"mlrt.await_handle\"(%handle) : (!mlrt.async_handle) -> () \n+\n+  %c1 = \"test_mlbc.add.i32\"(%arg0, %arg1) : (i32, i32) -> i32\n+  %c2 = \"test_mlbc.add.i32\"(%c1, %c1) : (i32, i32) -> i32\n+\n+  func.return %c2 : i32\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "c960fedd2adc25de18348c8162f8351ebe805085",
            "filename": "tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata/async2.mlir",
            "status": "added",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Ftestdata%2Fasync2.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Ftestdata%2Fasync2.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fmlrt%2Ftestdata%2Fasync2.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,16 @@\n+func.func @add_i32(%arg0: i32, %arg1: i32) -> i32 {\n+  %0 = \"test_mlbc.add.i32\"(%arg0, %arg1) : (i32, i32) -> i32\n+  func.return %0 : i32\n+}\n+\n+func.func @main(%arg0: i32, %arg1: i32) -> i32 {\n+  %c1 = \"test_mlbc.add.i32\"(%arg0, %arg1) : (i32, i32) -> i32\n+ \n+  %handle = \"mlrt.async\"(%c1, %c1) {callee = @add_i32} : (i32, i32) -> !mlrt.async_handle\n+\n+  \"mlrt.await_handle\"(%handle) : (!mlrt.async_handle) -> () \n+\n+  %c2 = \"test_mlbc.add.i32\"(%c1, %c1) : (i32, i32) -> i32\n+\n+  func.return %c2 : i32\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "a6f717a4168f2ad1a3c7cc4b6f0e2a1c1c18de07",
            "filename": "tensorflow/core/common_runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1350,10 +1350,7 @@ cc_library(\n     srcs = [\"lower_functional_ops.cc\"],\n     hdrs = [\"lower_functional_ops.h\"],\n     copts = tf_copts(),\n-    visibility = default_package_visibility + [\n-        \"//platforms/performance/autograppler:__subpackages__\",\n-        \"//platforms/performance/tf_sim:__subpackages__\",\n-    ],\n+    visibility = default_package_visibility + [\"//platforms/performance/tf_sim:__subpackages__\"],\n     deps = [\n         \":device_propagation\",\n         \":function_utils\","
        },
        {
            "sha": "506edb7c1523563179fbea9b688df1cca6167c33",
            "filename": "tensorflow/core/common_runtime/pluggable_device/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -109,6 +109,7 @@ cc_library(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_xla//xla/pjrt:pjrt_api\",\n         \"@local_xla//xla/pjrt/c:pjrt_c_api_hdrs\",\n@@ -137,6 +138,7 @@ cc_library(\n         \"//tensorflow/core/platform:stream_executor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_xla//xla/stream_executor:stream_executor_h\",\n         \"@local_xla//xla/stream_executor/integrations:device_mem_allocator\","
        },
        {
            "sha": "8ec86ab19d035cd2773d5c4d6e8f451259d664d1",
            "filename": "tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_bfc_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_bfc_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_bfc_allocator.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <cstdlib>\n #include <cstring>\n+#include <string>\n \n #include \"absl/log/log.h\"\n #include \"absl/memory/memory.h\""
        },
        {
            "sha": "924e0ed2cb6066ea317cd3024c9e5e6ff11c79a5",
            "filename": "tensorflow/core/common_runtime/pluggable_device/pluggable_device_context.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_context.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_context.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_context.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <functional>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/core/common_runtime/device/device_event_mgr.h\"\n #include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n #include \"tensorflow/core/framework/device.h\""
        },
        {
            "sha": "6e62aefa1707f37c6660134b4710050177fb0f68",
            "filename": "tensorflow/core/common_runtime/pluggable_device/pluggable_device_context.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_context.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <functional>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/core/common_runtime/device.h\"\n #include \"tensorflow/core/framework/device_base.h\"\n #include \"tensorflow/core/framework/tensor.h\""
        },
        {
            "sha": "6f735b70695b4f0bf820d5cec67e46780816c02f",
            "filename": "tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_init.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_init.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_init.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -15,6 +15,8 @@ limitations under the License.\n \n #include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n \n+#include <string>\n+\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"xla/stream_executor/platform_manager.h\""
        },
        {
            "sha": "e428437c8d029fe2854aa0e36943111a530e7e9a",
            "filename": "tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_process_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_process_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpluggable_device%2Fpluggable_device_process_state.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <cstdint>\n #include <cstring>\n #include <memory>\n+#include <string>\n #include <unordered_map>\n #include <utility>\n #include <vector>"
        },
        {
            "sha": "c23039c252c67f6d7a9ec227dc07c5f4f4825876",
            "filename": "tensorflow/core/framework/register_types.h",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Fregister_types.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Fregister_types.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fregister_types.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,6 +17,8 @@ limitations under the License.\n #define TENSORFLOW_CORE_FRAMEWORK_REGISTER_TYPES_H_\n // This file is used by cuda code and must remain compilable by nvcc.\n \n+#include <cstdint>  // IWYU pragma: export\n+\n #include \"tensorflow/core/framework/numeric_types.h\"\n #include \"tensorflow/core/framework/resource_handle.h\"\n #include \"tensorflow/core/framework/variant.h\"\n@@ -61,19 +63,19 @@ limitations under the License.\n // readability.\n #define TF_CALL_float(m) m(float)\n #define TF_CALL_double(m) m(double)\n-#define TF_CALL_int32(m) m(::tensorflow::int32)\n-#define TF_CALL_uint32(m) m(::tensorflow::uint32)\n-#define TF_CALL_uint8(m) m(::tensorflow::uint8)\n-#define TF_CALL_int16(m) m(::tensorflow::int16)\n+#define TF_CALL_int32(m) m(::int32_t)\n+#define TF_CALL_uint32(m) m(::uint32_t)\n+#define TF_CALL_uint8(m) m(::uint8_t)\n+#define TF_CALL_int16(m) m(::int16_t)\n \n-#define TF_CALL_int8(m) m(::tensorflow::int8)\n+#define TF_CALL_int8(m) m(::int8_t)\n #define TF_CALL_string(m) m(::tensorflow::tstring)\n #define TF_CALL_tstring(m) m(::tensorflow::tstring)\n #define TF_CALL_resource(m) m(::tensorflow::ResourceHandle)\n #define TF_CALL_variant(m) m(::tensorflow::Variant)\n #define TF_CALL_complex64(m) m(::tensorflow::complex64)\n #define TF_CALL_int64(m) m(::int64_t)\n-#define TF_CALL_uint64(m) m(::tensorflow::uint64)\n+#define TF_CALL_uint64(m) m(::uint64_t)\n #define TF_CALL_bool(m) m(bool)\n \n #define TF_CALL_qint8(m) m(::tensorflow::qint8)\n@@ -83,7 +85,7 @@ limitations under the License.\n #define TF_CALL_qint16(m) m(::tensorflow::qint16)\n \n #define TF_CALL_quint16(m) m(::tensorflow::quint16)\n-#define TF_CALL_uint16(m) m(::tensorflow::uint16)\n+#define TF_CALL_uint16(m) m(::uint16_t)\n #define TF_CALL_complex128(m) m(::tensorflow::complex128)\n #define TF_CALL_half(m) m(Eigen::half)\n \n@@ -105,7 +107,7 @@ limitations under the License.\n // supported.\n #define TF_CALL_float(m) m(float)\n #define TF_CALL_double(m)\n-#define TF_CALL_int32(m) m(::tensorflow::int32)\n+#define TF_CALL_int32(m) m(::int32_t)\n #define TF_CALL_uint32(m)\n #define TF_CALL_uint8(m)\n #define TF_CALL_int16(m)\n@@ -148,7 +150,7 @@ limitations under the License.\n // Only float, int32, and bool are supported.\n #define TF_CALL_float(m) m(float)\n #define TF_CALL_double(m)\n-#define TF_CALL_int32(m) m(::tensorflow::int32)\n+#define TF_CALL_int32(m) m(::int32_t)\n #define TF_CALL_uint32(m)\n #define TF_CALL_uint8(m)\n #define TF_CALL_int16(m)"
        },
        {
            "sha": "970577a1df33769f881c7bb75dd488f089dabeeb",
            "filename": "tensorflow/core/framework/resource_handle.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Fresource_handle.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Fresource_handle.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fresource_handle.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -99,26 +99,26 @@ absl::Status ResourceHandle::FromProto(const ResourceHandleProto& proto) {\n   return absl::OkStatus();\n }\n \n-string ResourceHandle::SerializeAsString() const {\n+std::string ResourceHandle::SerializeAsString() const {\n   ResourceHandleProto proto;\n   AsProto(&proto);\n   return proto.SerializeAsString();\n }\n \n-bool ResourceHandle::ParseFromString(const string& s) {\n+bool ResourceHandle::ParseFromString(const std::string& s) {\n   ResourceHandleProto proto;\n   return proto.ParseFromString(s) && FromProto(proto).ok();\n }\n \n-string ResourceHandle::DebugString() const {\n+std::string ResourceHandle::DebugString() const {\n   return absl::StrFormat(\n       \"device: %s container: %s name: %s hash_code: 0x%X maybe_type_name %s, \"\n       \"dtype and shapes : %s\",\n       device(), container(), name(), hash_code(),\n       port::Demangle(maybe_type_name()),\n       DtypeAndShapesToString(dtypes_and_shapes()));\n }\n-string ResourceHandle::SummarizeValue() const {\n+std::string ResourceHandle::SummarizeValue() const {\n   return absl::StrFormat(\n       \"ResourceHandle(name=\\\"%s\\\", device=\\\"%s\\\", container=\\\"%s\\\", \"\n       \"type=\\\"%s\\\", dtype and shapes : \\\"%s\\\")\",\n@@ -127,7 +127,7 @@ string ResourceHandle::SummarizeValue() const {\n }\n \n ResourceHandle ResourceHandle::MakeRefCountingHandle(\n-    ResourceBase* resource, const string& device_name,\n+    ResourceBase* resource, const std::string& device_name,\n     const TypeIndex& type_index,\n     const std::vector<DtypeAndPartialTensorShape>& dtypes_and_shapes,\n     const absl::optional<ManagedStackTrace>& definition_stack_trace) {\n@@ -164,7 +164,7 @@ std::atomic<int64_t> ResourceHandle::current_id_;\n \n int64_t ResourceHandle::GenerateUniqueId() { return current_id_.fetch_add(1); }\n \n-string ProtoDebugString(const ResourceHandle& handle) {\n+std::string ProtoDebugString(const ResourceHandle& handle) {\n   return handle.DebugString();\n }\n \n@@ -180,7 +180,7 @@ void EncodeResourceHandleList(const ResourceHandle* p, int64_t n,\n \n bool DecodeResourceHandleList(std::unique_ptr<port::StringListDecoder> d,\n                               ResourceHandle* ps, int64_t n) {\n-  std::vector<uint32> sizes(n);\n+  std::vector<uint32_t> sizes(n);\n   if (!d->ReadSizes(&sizes)) return false;\n \n   ResourceHandleProto proto;"
        },
        {
            "sha": "a201638e797306d6dd0e1cb13457c0f285fc7cf2",
            "filename": "tensorflow/core/framework/shape_inference.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Fshape_inference.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Fshape_inference.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fshape_inference.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -703,7 +703,8 @@ ShapeHandle InferenceContext::UnknownShape() {\n }\n \n ShapeHandle InferenceContext::UnknownShapeOfRank(int64_t rank) {\n-  CHECK_LE(rank, kint32max) << \"rank must be less than kint32max\";\n+  CHECK_LE(rank, std::numeric_limits<int32_t>::max())\n+      << \"rank must be less than kint32max\";\n   if (rank == kUnknownRank) {\n     return UnknownShape();\n   }"
        },
        {
            "sha": "819b628f13795586b9c53ffad12c4b3d37cc29f1",
            "filename": "tensorflow/core/framework/tensor.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Ftensor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fframework%2Ftensor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -963,13 +963,13 @@ absl::Status Tensor::BitcastFrom(const Tensor& other, DataType dtype,\n   switch (TYPE_ENUM) {                                         \\\n     CASE(float, SINGLE_ARG(STMTS))                             \\\n     CASE(double, SINGLE_ARG(STMTS))                            \\\n-    CASE(int32, SINGLE_ARG(STMTS))                             \\\n-    CASE(uint8, SINGLE_ARG(STMTS))                             \\\n-    CASE(uint16, SINGLE_ARG(STMTS))                            \\\n-    CASE(uint32, SINGLE_ARG(STMTS))                            \\\n-    CASE(uint64, SINGLE_ARG(STMTS))                            \\\n-    CASE(int16, SINGLE_ARG(STMTS))                             \\\n-    CASE(int8, SINGLE_ARG(STMTS))                              \\\n+    CASE(int32_t, SINGLE_ARG(STMTS))                           \\\n+    CASE(uint8_t, SINGLE_ARG(STMTS))                           \\\n+    CASE(uint16_t, SINGLE_ARG(STMTS))                          \\\n+    CASE(uint32_t, SINGLE_ARG(STMTS))                          \\\n+    CASE(uint64_t, SINGLE_ARG(STMTS))                          \\\n+    CASE(int16_t, SINGLE_ARG(STMTS))                           \\\n+    CASE(int8_t, SINGLE_ARG(STMTS))                            \\\n     CASE(tstring, SINGLE_ARG(STMTS))                           \\\n     CASE(complex64, SINGLE_ARG(STMTS))                         \\\n     CASE(complex128, SINGLE_ARG(STMTS))                        \\"
        },
        {
            "sha": "bc08f46c579c3baabf5e2adbe1c5d90a0e4d9591",
            "filename": "tensorflow/core/kernels/cast_op_impl.h",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fcast_op_impl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fcast_op_impl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcast_op_impl.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -61,10 +61,10 @@ struct OutOfRange {\n \n // Add additional logging for out of range inputs when running in debug mode.\n #ifndef NDEBUG\n-VALIDATE_CAST(int32, float);\n-VALIDATE_CAST(int64, float);\n-VALIDATE_CAST(int32, double);\n-VALIDATE_CAST(int64, double);\n+VALIDATE_CAST(int32_t, float);\n+VALIDATE_CAST(int64_t, float);\n+VALIDATE_CAST(int32_t, double);\n+VALIDATE_CAST(int64_t, double);\n #endif\n \n CAST_FUNCTORS(Eigen::ThreadPoolDevice);\n@@ -74,13 +74,13 @@ CAST_FUNCTORS(Eigen::ThreadPoolDevice);\n \n #define CURRY_TYPES3(FN, arg0, arg1)   \\\n   FN(arg0, arg1, bool);                \\\n-  FN(arg0, arg1, uint8);               \\\n-  FN(arg0, arg1, uint16);              \\\n-  FN(arg0, arg1, uint32);              \\\n-  FN(arg0, arg1, uint64);              \\\n-  FN(arg0, arg1, int8);                \\\n-  FN(arg0, arg1, int16);               \\\n-  FN(arg0, arg1, int32);               \\\n+  FN(arg0, arg1, uint8_t);             \\\n+  FN(arg0, arg1, uint16_t);            \\\n+  FN(arg0, arg1, uint32_t);            \\\n+  FN(arg0, arg1, uint64_t);            \\\n+  FN(arg0, arg1, int8_t);              \\\n+  FN(arg0, arg1, int16_t);             \\\n+  FN(arg0, arg1, int32_t);             \\\n   FN(arg0, arg1, int64_t);             \\\n   FN(arg0, arg1, float);               \\\n   FN(arg0, arg1, double);              \\"
        },
        {
            "sha": "12f22ca8bb107fede56a0481df18514d6db67c94",
            "filename": "tensorflow/core/kernels/data/experimental/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -875,6 +875,7 @@ tf_kernel_library(\n         \"//tensorflow/core/kernels/data/experimental/sql\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n \n@@ -888,6 +889,7 @@ tf_kernel_library(\n         \"//tensorflow/core:protos_all_cc\",\n         \"//tensorflow/core/kernels:summary_interface\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n     ],\n )\n@@ -902,6 +904,7 @@ tf_kernel_library(\n         \"//tensorflow/core:lib_internal\",\n         \"//tensorflow/core:protos_all_cc\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n \n@@ -936,6 +939,7 @@ tf_kernel_library(\n         \"//tensorflow/core/framework:dataset_options_proto_cc\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n         \"@eigen_archive//:eigen3\",\n     ],\n )\n@@ -988,6 +992,7 @@ tf_kernel_library(\n         \"//tensorflow/core/framework:types_proto_cc\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n         \"@eigen_archive//:eigen3\",\n     ],\n )"
        },
        {
            "sha": "cab138c9903c4266bc3d1b7e60dd99f5682186c5",
            "filename": "tensorflow/core/kernels/data/experimental/sql_dataset_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsql_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsql_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsql_dataset_op.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -14,11 +14,13 @@ limitations under the License.\n ==============================================================================*/\n #include <cstdint>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/dataset.h\"\n #include \"tensorflow/core/framework/partial_tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor.h\""
        },
        {
            "sha": "5e9d433b2e1785a1fa92df5adf453095cca0f749",
            "filename": "tensorflow/core/kernels/data/experimental/stats_aggregator_ops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_aggregator_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_aggregator_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_aggregator_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -14,10 +14,12 @@ limitations under the License.\n ==============================================================================*/\n #include <cstdint>\n #include <memory>\n+#include <string>\n #include <unordered_map>\n #include <utility>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/op_requires.h\""
        },
        {
            "sha": "3961ae52923e7e6aaf9fa60a69ef1393c90532e4",
            "filename": "tensorflow/core/kernels/data/experimental/stats_dataset_ops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_dataset_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_dataset_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_dataset_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -15,10 +15,12 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/dataset.h\"\n #include \"tensorflow/core/framework/dataset_options.pb.h\"\n #include \"tensorflow/core/framework/partial_tensor_shape.h\""
        },
        {
            "sha": "285144ce9448fcf57b3d31aaa5a771788c5aefe0",
            "filename": "tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fthreadpool_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fthreadpool_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fthreadpool_dataset_op.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,11 +17,13 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/data/dataset_utils.h\"\n #include \"tensorflow/core/framework/dataset.h\"\n #include \"tensorflow/core/framework/dataset_options.pb.h\""
        },
        {
            "sha": "92f504347a4dae9e82a2e6f1468b758718b76910",
            "filename": "tensorflow/core/kernels/data/experimental/unique_dataset_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,12 +17,14 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n+#include <string>\n #include <unordered_set>\n #include <utility>\n #include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/partial_tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/types.pb.h\""
        },
        {
            "sha": "3692d02cdbdd1d0e78894ebdd8d5201e5829a6c8",
            "filename": "tensorflow/core/kernels/data/experimental/unique_dataset_op_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -12,6 +12,7 @@ limitations under the License.\n #include \"tensorflow/core/kernels/data/experimental/unique_dataset_op.h\"\n \n #include <cstdint>\n+#include <string>\n #include <utility>\n #include <vector>\n "
        },
        {
            "sha": "f4991bc1fe252a5a225412877b6b5f3cd4cd0565",
            "filename": "tensorflow/core/kernels/matmul_op_impl.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -51,7 +51,6 @@ limitations under the License.\n #endif\n \n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n-#include \"xla/stream_executor/host_or_device_scalar.h\"\n #include \"tensorflow/core/kernels/gpu_utils.h\"\n #include \"tensorflow/core/kernels/matmul_util.h\"\n #include \"tensorflow/core/kernels/numeric_options_utils.h\""
        },
        {
            "sha": "1144ddb78c7cdfa1473062a69717faea4c70b9af",
            "filename": "tensorflow/core/ops/compat/op_compatibility_lib.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcompat%2Fop_compatibility_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcompat%2Fop_compatibility_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fcompat%2Fop_compatibility_lib.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -27,19 +27,19 @@ limitations under the License.\n \n namespace tensorflow {\n \n-static string OpsHistoryDirectory(const string& ops_prefix,\n-                                  const string& history_version) {\n+static std::string OpsHistoryDirectory(const std::string& ops_prefix,\n+                                       const std::string& history_version) {\n   return io::JoinPath(ops_prefix,\n                       absl::StrCat(\"compat/ops_history_\", history_version));\n }\n \n-static string OpsHistoryFile(const string& ops_prefix,\n-                             const string& history_version) {\n+static std::string OpsHistoryFile(const std::string& ops_prefix,\n+                                  const std::string& history_version) {\n   return io::JoinPath(ops_prefix, absl::StrCat(\"compat/ops_history.\",\n                                                history_version, \".pbtxt\"));\n }\n \n-static string FileNameFromOpName(const string& op_name) {\n+static std::string FileNameFromOpName(const std::string& op_name) {\n   return absl::StrCat(op_name, \".pbtxt\");\n }\n \n@@ -51,23 +51,24 @@ static void AddNewOpToHistory(const OpDef& op,\n   }\n }\n \n-static absl::Status ReadOpHistory(Env* env, const string& file,\n-                                  const string& directory,\n+static absl::Status ReadOpHistory(Env* env, const std::string& file,\n+                                  const std::string& directory,\n                                   OpCompatibilityLib::OpHistory* out) {\n   // Read op history form `directory` if it exists there.\n-  std::vector<string> matching_files;\n+  std::vector<std::string> matching_files;\n   absl::Status status = env->GetMatchingPaths(\n       io::JoinPath(directory, \"*.pbtxt\"), &matching_files);\n   if (status.ok() && !matching_files.empty()) {\n     printf(\"Reading op history from %s/*.pbtxt...\\n\", directory.c_str());\n     std::sort(matching_files.begin(), matching_files.end());\n-    for (const string& full_file : matching_files) {\n-      string op_history_str;\n+    for (const std::string& full_file : matching_files) {\n+      std::string op_history_str;\n       TF_RETURN_IF_ERROR(ReadFileToString(env, full_file, &op_history_str));\n       OpList in_op_history;\n       protobuf::TextFormat::ParseFromString(op_history_str, &in_op_history);\n-      const string file_tail = FileNameFromOpName(in_op_history.op(0).name());\n-      const string expected = io::JoinPath(directory, file_tail);\n+      const std::string file_tail =\n+          FileNameFromOpName(in_op_history.op(0).name());\n+      const std::string expected = io::JoinPath(directory, file_tail);\n       if (full_file != expected) {\n         return errors::Internal(\"Expected file paths to match but '\", full_file,\n                                 \"' != '\", expected, \"'\");\n@@ -76,7 +77,7 @@ static absl::Status ReadOpHistory(Env* env, const string& file,\n     }\n   } else {  // Otherwise, fall back to reading op history from `file`.\n     printf(\"Reading op history from %s...\\n\", file.c_str());\n-    string op_history_str;\n+    std::string op_history_str;\n     TF_RETURN_IF_ERROR(ReadFileToString(env, file, &op_history_str));\n     OpList in_op_history;\n     protobuf::TextFormat::ParseFromString(op_history_str, &in_op_history);\n@@ -98,9 +99,9 @@ static absl::Status ReadOpHistory(Env* env, const string& file,\n   return absl::OkStatus();\n }\n \n-OpCompatibilityLib::OpCompatibilityLib(const string& ops_prefix,\n-                                       const string& history_version,\n-                                       const std::set<string>* stable_ops)\n+OpCompatibilityLib::OpCompatibilityLib(const std::string& ops_prefix,\n+                                       const std::string& history_version,\n+                                       const std::set<std::string>* stable_ops)\n     : ops_file_(io::JoinPath(ops_prefix, \"ops.pbtxt\")),\n       op_history_file_(OpsHistoryFile(ops_prefix, history_version)),\n       op_history_directory_(OpsHistoryDirectory(ops_prefix, history_version)),\n@@ -121,12 +122,12 @@ absl::Status OpCompatibilityLib::ValidateCompatible(Env* env, int* changed_ops,\n \n   if (stable_ops_ != nullptr) {\n     printf(\"Verifying no stable ops have been removed...\\n\");\n-    std::vector<string> removed;\n+    std::vector<std::string> removed;\n     // We rely on stable_ops_ and op_list_ being in sorted order.\n     auto iter = stable_ops_->begin();\n     for (int cur = 0; iter != stable_ops_->end() && cur < op_list_.op_size();\n          ++cur) {\n-      const string& op_name = op_list_.op(cur).name();\n+      const std::string& op_name = op_list_.op(cur).name();\n       while (op_name > *iter) {\n         removed.push_back(*iter);\n         ++iter;\n@@ -156,9 +157,9 @@ absl::Status OpCompatibilityLib::ValidateCompatible(Env* env, int* changed_ops,\n   // Within the OplList it has versions in oldest-first order.\n   while (cur < op_list_.op_size() && hist < in_op_history.size()) {\n     const OpDef& cur_op = op_list_.op(cur);\n-    const string& cur_op_name = cur_op.name();\n+    const std::string& cur_op_name = cur_op.name();\n     const OpList& history_op_list = in_op_history[hist].second;\n-    const string& history_op_name = history_op_list.op(0).name();\n+    const std::string& history_op_name = history_op_list.op(0).name();\n     if (stable_ops_ != nullptr && stable_ops_->count(cur_op_name) == 0) {\n       // Ignore unstable op.\n       for (++cur; cur < op_list_.op_size(); ++cur) {\n@@ -188,7 +189,7 @@ absl::Status OpCompatibilityLib::ValidateCompatible(Env* env, int* changed_ops,\n       const int end = history_op_list.op_size();\n       // Is the last op in the history the same as the current op?\n       // Compare using their serialized representations.\n-      string history_str, cur_str;\n+      std::string history_str, cur_str;\n       history_op_list.op(end - 1).SerializeToString(&history_str);\n       cur_op.SerializeToString(&cur_str);\n \n@@ -232,7 +233,7 @@ absl::Status OpCompatibilityLib::ValidateCompatible(Env* env, int* changed_ops,\n \n   // Add remaining new ops.\n   for (; cur < op_list_.op_size(); ++cur) {\n-    const string& op_name = op_list_.op(cur).name();\n+    const std::string& op_name = op_list_.op(cur).name();\n     if (stable_ops_ != nullptr && stable_ops_->count(op_name) == 0) {\n       // Ignore unstable op.\n     } else {"
        },
        {
            "sha": "829c152a8cb5c5bf06dc29529747b454e685d846",
            "filename": "tensorflow/core/ops/compat/op_compatibility_lib.h",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcompat%2Fop_compatibility_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcompat%2Fop_compatibility_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fcompat%2Fop_compatibility_lib.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -33,27 +33,30 @@ class OpCompatibilityLib {\n   //   If stable_ops == nullptr, we use all registered ops.\n   //   Otherwise ValidateCompatible() ignores ops not in *stable_ops\n   //   and require all ops in *stable_ops to exist.\n-  OpCompatibilityLib(const string& ops_prefix, const string& history_version,\n-                     const std::set<string>* stable_ops);\n+  OpCompatibilityLib(const std::string& ops_prefix,\n+                     const std::string& history_version,\n+                     const std::set<std::string>* stable_ops);\n \n   // Name of the file that contains the checked-in versions of *all*\n   // ops, with docs.\n-  const string& ops_file() const { return ops_file_; }\n+  const std::string& ops_file() const { return ops_file_; }\n \n   // Name of the file that contains all versions of *stable* ops,\n   // without docs.  Op history is in (alphabetical, oldest-first)\n   // order.\n-  const string& op_history_file() const { return op_history_file_; }\n+  const std::string& op_history_file() const { return op_history_file_; }\n \n   // Name of the directory that contains all versions of *stable* ops,\n   // without docs.  Op history is one file per op, in oldest-first\n   // order within the file.\n-  const string& op_history_directory() const { return op_history_directory_; }\n+  const std::string& op_history_directory() const {\n+    return op_history_directory_;\n+  }\n \n   // Should match the contents of ops_file().  Run before calling\n   // ValidateCompatible().\n-  string OpsString() const {\n-    string result;\n+  std::string OpsString() const {\n+    std::string result;\n     google::protobuf::TextFormat::PrintToString(op_list_, &result);\n     return result;\n   }\n@@ -63,7 +66,7 @@ class OpCompatibilityLib {\n   int num_all_ops() const { return op_list_.op_size(); }\n \n   // <file name, file contents> pairs representing op history.\n-  typedef std::vector<std::pair<string, OpList>> OpHistory;\n+  typedef std::vector<std::pair<std::string, OpList>> OpHistory;\n \n   // Make sure the current version of the *stable* ops are compatible\n   // with the historical versions, and if out_op_history != nullptr,\n@@ -74,10 +77,10 @@ class OpCompatibilityLib {\n                                   OpHistory* out_op_history);\n \n  private:\n-  const string ops_file_;\n-  const string op_history_file_;\n-  const string op_history_directory_;\n-  const std::set<string>* stable_ops_;\n+  const std::string ops_file_;\n+  const std::string op_history_file_;\n+  const std::string op_history_directory_;\n+  const std::set<std::string>* stable_ops_;\n   OpList op_list_;\n };\n "
        },
        {
            "sha": "b1a1e93e8fa9255be67c5b41a336a652b7e0b6f8",
            "filename": "tensorflow/core/ops/compat/update_ops_main.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcompat%2Fupdate_ops_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcompat%2Fupdate_ops_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fcompat%2Fupdate_ops_main.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -30,14 +30,14 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-void WriteUpdateTo(const string& directory) {\n+void WriteUpdateTo(const std::string& directory) {\n   OpCompatibilityLib compatibility(\n       directory, absl::StrCat(\"v\", TF_MAJOR_VERSION), nullptr);\n \n   // Write full copy of all ops to ops.pbtxt.\n   Env* env = Env::Default();\n   {\n-    const string& ops_file = compatibility.ops_file();\n+    const std::string& ops_file = compatibility.ops_file();\n     printf(\"Writing ops to %s...\\n\", ops_file.c_str());\n     TF_QCHECK_OK(WriteStringToFile(env, ops_file, compatibility.OpsString()));\n   }\n@@ -52,7 +52,7 @@ void WriteUpdateTo(const string& directory) {\n                                                 &out_op_history));\n   printf(\"%d changed ops\\n%d added ops\\n\", changed_ops, added_ops);\n \n-  const string& history_dir = compatibility.op_history_directory();\n+  const std::string& history_dir = compatibility.op_history_directory();\n   absl::Status status = env->CreateDir(history_dir);\n   if (!absl::IsAlreadyExists(status)) {\n     TF_QCHECK_OK(status);"
        },
        {
            "sha": "8efefb7245fc0aa4bde1faaf085d8325ff6387d9",
            "filename": "tensorflow/core/ops/cudnn_rnn_ops.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -92,9 +92,9 @@ REGISTER_OP(\"CudnnRNN\")\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n \n-      string direction;\n+      std::string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n-      string rnn_mode;\n+      std::string rnn_mode;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n       int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n       DimensionHandle output_size;\n@@ -140,9 +140,9 @@ REGISTER_OP(\"CudnnRNNV2\")\n       auto seq_length = c->Dim(input_shape, 0);\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n-      string direction;\n+      std::string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n-      string rnn_mode;\n+      std::string rnn_mode;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n       int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n       DimensionHandle output_size;\n@@ -195,9 +195,9 @@ REGISTER_OP(\"CudnnRNNV3\")\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n \n-      string direction;\n+      std::string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n-      string rnn_mode;\n+      std::string rnn_mode;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n       if (rnn_mode == \"lstm\") {\n         TF_RETURN_IF_ERROR(c->WithRank(input_c_shape, 3, &unused));"
        },
        {
            "sha": "5bc206c1392496cc42b2513983065701351efe78",
            "filename": "tensorflow/core/ops/cudnn_rnn_ops_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -24,10 +24,10 @@ limitations under the License.\n \n namespace tensorflow {\n \n-static string JoinedCopies(const string& s, int copies) {\n-  string res;\n+static std::string JoinedCopies(const std::string& s, int copies) {\n+  std::string res;\n   for (int i = 0; i < copies; ++i) {\n-    strings::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n+    absl::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n   }\n   return res;\n }\n@@ -58,12 +58,12 @@ TEST(CudnnRNNOpsTest, ForwardLstm_ShapeFn) {\n   std::vector<int> output_shape = {seq_length, batch_size,\n                                    num_units * dir_count};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_h_shape), \";\", \"[?]\");\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNN\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNN\")\n@@ -95,12 +95,12 @@ TEST(CudnnRNNOpsTest, ForwardV2Lstm_ShapeFn) {\n   std::vector<int> output_shape = {seq_length, batch_size,\n                                    num_units * dir_count};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_h_shape), \";\", \"[?]\");\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNNV2\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV2\")\n@@ -135,13 +135,13 @@ TEST(CudnnRNNOpsTest, ForwardV3Lstm_ShapeFn) {\n                                    num_units * dir_count};\n   std::vector<int> seq_lengths_shape = {batch_size};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n       shape_to_str(seq_lengths_shape));\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in2;?;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in2;?;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNNV3\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n@@ -177,13 +177,13 @@ TEST(CudnnRNNOpsTest, ForwardV3Gru) {\n                                    num_units * dir_count};\n   std::vector<int> seq_lengths_shape = {batch_size};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n       shape_to_str(seq_lengths_shape));\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;[];?;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;[];?;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNNV3\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n@@ -207,7 +207,7 @@ TEST(CudnnRNNOpsTest, LSTMBlockCell_ShapeFn) {\n   ShapeInferenceTestOp op(\"LSTMBlockCell\");\n \n   // Last 6 inputs don't affect shape inference.\n-  string input_suffix = strings::StrCat(\";\", JoinedCopies(\"?\", 6));\n+  std::string input_suffix = absl::StrCat(\";\", JoinedCopies(\"?\", 6));\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 2\", op, \"[?];?\" + input_suffix);\n@@ -234,7 +234,7 @@ TEST(CudnnRNNOpsTest, BlockLSTM_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Middle inputs don't affect shape inference.\n-  string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n+  std::string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?]\" + infix + \"?\");\n@@ -266,7 +266,7 @@ TEST(CudnnRNNOpsTest, BlockLSTMV2_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Middle inputs don't affect shape inference.\n-  string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n+  std::string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?]\" + infix + \"?\");"
        },
        {
            "sha": "e85532695ffeb4b3ffa21d2ae540aa3881a143a6",
            "filename": "tensorflow/core/ops/data_flow_ops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fdata_flow_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fdata_flow_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fdata_flow_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -120,7 +120,7 @@ absl::Status DynamicStitchShapeFunction(InferenceContext* c) {\n \n     if (indices_t != nullptr) {\n       // The length is based on the highest index from flattened indices.\n-      const int32* indices = indices_t->flat<int32>().data();\n+      const int32_t* indices = indices_t->flat<int32_t>().data();\n       int64_t count = indices_t->NumElements();\n       for (int64_t i = 0; i < count; ++i) {\n         if (indices[i] > max_index) {\n@@ -340,7 +340,7 @@ REGISTER_OP(\"QueueDequeueManyV2\")\n       if (c->input_tensor(1) == nullptr) {\n         n_shape = c->Vector(InferenceContext::kUnknownDim);\n       } else {\n-        const int32_t n = c->input_tensor(1)->scalar<int32>()();\n+        const int32_t n = c->input_tensor(1)->scalar<int32_t>()();\n         if (n < 0) {\n           return errors::InvalidArgument(\"Input 'n' must be >= 0, but is \", n);\n         }"
        },
        {
            "sha": "30165ffe914129e1ad412a987262811c6558bb55",
            "filename": "tensorflow/core/ops/data_flow_ops_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fdata_flow_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fdata_flow_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fdata_flow_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -138,12 +138,12 @@ TEST(DataFlowOpsTest, DynamicStitch) {\n   INFER_OK(op, \"[2,3];[5,6];[2,3,4,5];[5,6,4,5]\", \"[?,d2_2,d2_3]\");\n \n   // 1 known input tensors, not enough to change answer.\n-  Tensor tensor_2 = test::AsTensor<int32>(\n-      std::vector<int32>{2, 4, 6, 0, 10, 11}, TensorShape({2, 3}));\n-  Tensor tensor_5 = test::AsTensor<int32>(\n-      std::vector<int32>{0,    1,  2,  3,  4,  5,  6,  7,  8,  9,\n-                         10,   11, 12, 13, 14, 15, 16, 17, 18, 19,\n-                         1000, 21, 22, 23, 24, 25, 26, 27, 28, 29},\n+  Tensor tensor_2 = test::AsTensor<int32_t>(\n+      std::vector<int32_t>{2, 4, 6, 0, 10, 11}, TensorShape({2, 3}));\n+  Tensor tensor_5 = test::AsTensor<int32_t>(\n+      std::vector<int32_t>{0,    1,  2,  3,  4,  5,  6,  7,  8,  9,\n+                           10,   11, 12, 13, 14, 15, 16, 17, 18, 19,\n+                           1000, 21, 22, 23, 24, 25, 26, 27, 28, 29},\n       TensorShape({5, 6}));\n   op.input_tensors.push_back(nullptr);\n   op.input_tensors.push_back(&tensor_5);\n@@ -157,7 +157,7 @@ TEST(DataFlowOpsTest, DynamicStitch) {\n   op.input_tensors[1] = &tensor_5;\n   INFER_OK(op, \"[2,3];[5,6];[2,3,4,5];[5,6,4,5]\", \"[1001,d2_2,d2_3]\");\n \n-  tensor_2.flat<int32>()(3) = 10000;\n+  tensor_2.flat<int32_t>()(3) = 10000;\n   INFER_OK(op, \"[2,3];[5,6];[2,3,4,5];[5,6,4,5]\", \"[10001,d2_2,d2_3]\");\n }\n \n@@ -254,7 +254,7 @@ TEST(DataFlowOpsTest, QueueDequeueManyV2ShapeFn) {\n   shapes_and_types.emplace_back(\"[?,2]\", DT_FLOAT);\n   INFER_OK(op, \"?;?\", \"[12,1,?,3];[12,?,2]\");\n \n-  n_tensor = test::AsScalar<int32>(-1);  // invalid value of n.\n+  n_tensor = test::AsScalar<int32_t>(-1);  // invalid value of n.\n   INFER_ERROR(\"must be >= 0\", op, \"?;?\");\n }\n "
        },
        {
            "sha": "5d36e15f0f24f6ca1d3a7f2078a1b26502be2cf5",
            "filename": "tensorflow/core/ops/image_ops.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fimage_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fimage_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fimage_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -57,7 +57,7 @@ absl::Status SetOutputToSizedImage(InferenceContext* c,\n           DataTypeString(size_tensor->dtype()), \" for input #\", size_input_idx,\n           \" in \", c->DebugString());\n     }\n-    auto vec = size_tensor->vec<int32>();\n+    auto vec = size_tensor->vec<int32_t>();\n     height = c->MakeDim(vec(0));\n     width = c->MakeDim(vec(1));\n   }\n@@ -415,7 +415,7 @@ REGISTER_OP(\"ResizeNearestNeighborGrad\")\n         TF_RETURN_IF_ERROR(c->ReplaceDim(input, 1, c->UnknownDim(), &input));\n         TF_RETURN_IF_ERROR(c->ReplaceDim(input, 2, c->UnknownDim(), &input));\n       } else {\n-        auto size_vec = size->vec<int32>();\n+        auto size_vec = size->vec<int32_t>();\n         TF_RETURN_IF_ERROR(\n             c->ReplaceDim(input, 1, c->MakeDim(size_vec(0)), &input));\n         TF_RETURN_IF_ERROR(\n@@ -516,7 +516,7 @@ REGISTER_OP(\"DecodeAndCropJpeg\")\n \n       const Tensor* crop_window = c->input_tensor(1);\n       if (crop_window != nullptr) {\n-        auto crop_window_vec = crop_window->vec<int32>();\n+        auto crop_window_vec = crop_window->vec<int32_t>();\n         h = c->MakeDim(crop_window_vec(2));\n         w = c->MakeDim(crop_window_vec(3));\n       }\n@@ -861,7 +861,7 @@ REGISTER_OP(\"ExtractGlimpse\")\n \n       bool uniform_noise = false;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"uniform_noise\", &uniform_noise));\n-      string noise;\n+      std::string noise;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"noise\", &noise));\n       if (uniform_noise && (!noise.empty() && noise != \"uniform\")) {\n         return errors::InvalidArgument(\n@@ -895,7 +895,7 @@ REGISTER_OP(\"ExtractGlimpseV2\")\n \n       bool uniform_noise = false;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"uniform_noise\", &uniform_noise));\n-      string noise;\n+      std::string noise;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"noise\", &noise));\n       if (uniform_noise && (!noise.empty() && noise != \"uniform\")) {\n         return errors::InvalidArgument("
        },
        {
            "sha": "d11204c47d437c9781e0aa95b718fb0a99fc76ed",
            "filename": "tensorflow/core/ops/image_ops_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fimage_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fimage_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fimage_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -45,7 +45,7 @@ TEST(ImageOpsTest, Resize_ShapeFn) {\n     // When the size tensor is not a constant, the middle dims are unknown.\n     INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,?,?,d0_3]\");\n \n-    Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+    Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n     op.input_tensors[1] = &size_tensor;\n     INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,20,30,d0_3]\");\n   }\n@@ -244,7 +244,7 @@ TEST(ImageOpsTest, ExtractGlimpse_ShapeFn) {\n   // When the size tensor is not a constant, the middle dims are unknown.\n   INFER_OK(op, \"[1,?,3,?];[2];?\", \"[d0_0,?,?,d0_3]\");\n \n-  Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors[1] = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];[2];?\", \"[d0_0,20,30,d0_3]\");\n \n@@ -272,7 +272,7 @@ TEST(ImageOpsTest, CropAndResize_ShapeFn) {\n   // When the size tensor is not a constant, the middle dims are unknown.\n   INFER_OK(op, \"[1,?,3,?];?;?;[2]\", \"[?,?,?,d0_3]\");\n \n-  Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors[3] = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];?;?;[2]\", \"[?,20,30,d0_3]\");\n \n@@ -298,7 +298,7 @@ TEST(ImageOpsTest, ResizeNearestNeighborGrad_ShapeFn) {\n   // When the size tensor is not a constant, the middle dims are unknown.\n   INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,?,?,d0_3]\");\n \n-  Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors[1] = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,20,30,d0_3]\");\n }\n@@ -314,7 +314,7 @@ TEST(ImageOpsTest, CropAndResizeGradImage_ShapeFn) {\n   INFER_OK(op, \"?;?;?;?\", \"[?,?,?,?]\");\n \n   // Known image_size should result in full shape information.\n-  Tensor image_size = test::AsTensor<int32>({10, 20, 30, 40});\n+  Tensor image_size = test::AsTensor<int32_t>({10, 20, 30, 40});\n   op.input_tensors[3] = &image_size;\n   INFER_OK(op, \"?;?;?;[1]\", \"[10, 20, 30, 40]\");\n }\n@@ -357,7 +357,7 @@ TEST(ImageOpsTest, QuantizedResizeBilinear_ShapeFn) {\n   INFER_ERROR(\"must be rank 0\", op, \"[1,?,3,?];[2];[?];[]\");\n   INFER_ERROR(\"must be rank 0\", op, \"[1,?,3,?];[2];[];[?]\");\n \n-  const Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  const Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors.at(1) = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];[2];[];[]\", \"[d0_0,20,30,d0_3];[];[]\");\n }"
        },
        {
            "sha": "63fa08218976333bfee63d430bfbe9c952322583",
            "filename": "tensorflow/core/ops/io_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fio_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fio_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fio_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -112,7 +112,7 @@ REGISTER_OP(\"RestoreV2\")\n               \"The number of shape_and_slice doesn't match tensor outputs.\");\n         }\n         for (int i = 0; i < shape_and_slices_flat.size(); ++i) {\n-          const string& shape_and_slice = shape_and_slices_flat(i);\n+          const std::string& shape_and_slice = shape_and_slices_flat(i);\n           if (shape_and_slice.empty()) {\n             c->set_output(i, c->UnknownShape());\n             continue;"
        },
        {
            "sha": "19abdfd6779d41ea66313d571f13059af91113d5",
            "filename": "tensorflow/core/ops/linalg_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Flinalg_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Flinalg_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Flinalg_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -41,7 +41,7 @@ TEST(LinalgOpsTest, UnchangedSquare_ShapeFn) {\n   for (const char* op_name : {\"Cholesky\", \"CholeskyGrad\", \"MatrixInverse\"}) {\n     ShapeInferenceTestOp op(op_name);\n \n-    const string extra_shape = (op.name == \"CholeskyGrad\" ? \";?\" : \"\");\n+    const std::string extra_shape = (op.name == \"CholeskyGrad\" ? \";?\" : \"\");\n \n     INFER_OK(op, \"?\" + extra_shape, \"?\");\n     INFER_ERROR(\"Shape must be at least rank 2 but is rank 1\", op,"
        },
        {
            "sha": "75f6e9f8b53721f46118bb1e73de34c917693996",
            "filename": "tensorflow/core/ops/lookup_ops.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Flookup_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Flookup_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Flookup_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -90,9 +90,9 @@ REGISTER_OP(\"LookupTableFind\")\n \n absl::Status ValidateTableType(InferenceContext* c,\n                                const ShapeAndType& key_shape_and_type,\n-                               const string& key_dtype_attr,\n+                               const std::string& key_dtype_attr,\n                                const ShapeAndType& value_shape_and_type,\n-                               const string& value_dtype_attr) {\n+                               const std::string& value_dtype_attr) {\n   DataType key_dtype;\n   TF_RETURN_IF_ERROR(c->GetAttr(key_dtype_attr, &key_dtype));\n   if (key_shape_and_type.dtype != key_dtype) {\n@@ -115,8 +115,8 @@ absl::Status ValidateTableType(InferenceContext* c,\n }\n \n absl::Status ValidateTableResourceHandle(InferenceContext* c, ShapeHandle keys,\n-                                         const string& key_dtype_attr,\n-                                         const string& value_dtype_attr,\n+                                         const std::string& key_dtype_attr,\n+                                         const std::string& value_dtype_attr,\n                                          ShapeAndType* output_shape_and_type) {\n   auto* handle_data = c->input_handle_shapes_and_types(0);\n   if (handle_data == nullptr || handle_data->size() != 2) {"
        },
        {
            "sha": "cecfa31bace6116716f2a48b720b8f4820321c8f",
            "filename": "tensorflow/core/ops/math_grad.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_grad.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -630,7 +630,7 @@ absl::Status SquaredDifferenceGrad(const AttrSlice& attrs, FunctionDef* g) {\n }\n REGISTER_OP_GRADIENT(\"SquaredDifference\", SquaredDifferenceGrad);\n \n-absl::Status MaximumMinimumGradHelper(const string& comparator,\n+absl::Status MaximumMinimumGradHelper(const std::string& comparator,\n                                       const AttrSlice& attrs, FunctionDef* g) {\n   // clang-format off\n   return GradForBinaryCwise(g, {\n@@ -770,7 +770,7 @@ REGISTER_OP_GRADIENT(\"Mean\", MeanGrad);\n // REGISTER_OP_GRADIENT(\"UnsortedSegmentSum\", UnsortedSegmentSumGrad);\n // REGISTER_OP_GRADIENT(\"UnsortedSegmentMax\", UnsortedSegmentMaxGrad);\n \n-absl::Status MinMaxGradHelper(const string& op, const AttrSlice& attrs,\n+absl::Status MinMaxGradHelper(const std::string& op, const AttrSlice& attrs,\n                               FunctionDef* g) {\n   // clang-format off\n   *g = FDH::Define(\n@@ -807,13 +807,11 @@ absl::Status MinGrad(const AttrSlice& attrs, FunctionDef* g) {\n }\n REGISTER_OP_GRADIENT(\"Min\", MinGrad);\n \n-static absl::Status MatMulGradHelper(FunctionDef* g, const string& opname,\n-                                     const string& attr_adj_x,\n-                                     const string& attr_adj_y, const string& x0,\n-                                     bool ax0, const string& x1, bool ax1,\n-                                     const string& y0, bool ay0,\n-                                     const string& y1, bool ay1,\n-                                     bool enable_broadcasting) {\n+static absl::Status MatMulGradHelper(\n+    FunctionDef* g, const std::string& opname, const std::string& attr_adj_x,\n+    const std::string& attr_adj_y, const std::string& x0, bool ax0,\n+    const std::string& x1, bool ax1, const std::string& y0, bool ay0,\n+    const std::string& y1, bool ay1, bool enable_broadcasting) {\n   // The final outputs are \"dx\" and \"dy\". If we're broadcasting compute\n   // intermediate nodes for now.\n   std::vector<FDH::Node> nodes = {\n@@ -831,9 +829,9 @@ static absl::Status MatMulGradHelper(FunctionDef* g, const string& opname,\n   // broadcasting-specific ops.\n   if (enable_broadcasting) {\n     std::vector<FDH::Node> unbroadcast_gradients = {\n-        FDH::Const<int32>(\"zero\", absl::Span<const int32>{0}),\n-        FDH::Const<int32>(\"one\", absl::Span<const int32>{1}),\n-        FDH::Const<int32>(\"minustwo\", absl::Span<const int32>{-2}),\n+        FDH::Const<int32_t>(\"zero\", absl::Span<const int32_t>{0}),\n+        FDH::Const<int32_t>(\"one\", absl::Span<const int32_t>{1}),\n+        FDH::Const<int32_t>(\"minustwo\", absl::Span<const int32_t>{-2}),\n         // Compute the batch shapes of the inputs (all but last two dims).\n         {{\"sx\"}, \"Shape\", {\"x\"}, {{\"T\", \"$T\"}}},\n         {{\"sy\"}, \"Shape\", {\"y\"}, {{\"T\", \"$T\"}}},\n@@ -866,9 +864,11 @@ static absl::Status MatMulGradHelper(FunctionDef* g, const string& opname,\n   return absl::OkStatus();\n }\n \n-absl::Status MatMulGradCommon(const string& opname, const string& attr_adj_x,\n-                              const string& attr_adj_y, const AttrSlice& attrs,\n-                              FunctionDef* g, bool enable_broadcasting) {\n+absl::Status MatMulGradCommon(const std::string& opname,\n+                              const std::string& attr_adj_x,\n+                              const std::string& attr_adj_y,\n+                              const AttrSlice& attrs, FunctionDef* g,\n+                              bool enable_broadcasting) {\n   DataType T;\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"T\", &T));\n   if (T == DT_COMPLEX64 || T == DT_COMPLEX128) {"
        },
        {
            "sha": "5ef72e958292eab50c2b21e9140911a8ca088e8f",
            "filename": "tensorflow/core/ops/math_grad_test.cc",
            "status": "modified",
            "additions": 60,
            "deletions": 59,
            "changes": 119,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_grad_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_grad_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_grad_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -43,9 +43,9 @@ class MathGradTest : public ::testing::Test {\n   absl::Status Unary(const FDH::Node& op_node, const Tensor& x,\n                      const DataType dst, Tensor* y) {\n     const DataType src = x.dtype();\n-    auto adef = [](const string& name,\n+    auto adef = [](const std::string& name,\n                    const DataType type) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(type));\n+      return absl::StrCat(name, \":\", DataTypeString(type));\n     };\n     // Sum(op(x)), sum all output of op(x).\n     auto test = FDH::Define(\"Test\", {adef(\"x\", src)}, {adef(\"l\", dst)}, {},\n@@ -94,13 +94,13 @@ class MathGradTest : public ::testing::Test {\n     return s;\n   }\n \n-  absl::Status Unary(const string& op, const Tensor& x, Tensor* y) {\n+  absl::Status Unary(const std::string& op, const Tensor& x, Tensor* y) {\n     const FDH::Node op_node = {{\"y\"}, op, {\"x\"}, {{\"T\", x.dtype()}}};\n     return Unary(op_node, x, x.dtype(), y);\n   }\n \n   // Unary op expecting OK.\n-  Tensor SymGrad(const string& op, const Tensor& x) {\n+  Tensor SymGrad(const std::string& op, const Tensor& x) {\n     Tensor ret;\n     TF_CHECK_OK(Unary(op, x, &ret));\n     return ret;\n@@ -115,11 +115,11 @@ class MathGradTest : public ::testing::Test {\n   }\n \n   // Binary\n-  void SymGrad(const string& op, const Tensor& x, const Tensor& y, Tensor* dx,\n-               Tensor* dy) {\n+  void SymGrad(const std::string& op, const Tensor& x, const Tensor& y,\n+               Tensor* dx, Tensor* dy) {\n     const DataType T = x.dtype();\n-    auto adef = [T](const string& name) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(T));\n+    auto adef = [T](const std::string& name) {  // E.g., x:float, dy:double\n+      return absl::StrCat(name, \":\", DataTypeString(T));\n     };\n     // Sum(op(x)), sum all output of op(x).\n     auto test = FDH::Define(\"Test\", {adef(\"x\"), adef(\"y\")}, {adef(\"l\")}, {},\n@@ -171,11 +171,11 @@ class MathGradTest : public ::testing::Test {\n   }\n \n   // Reduction grad\n-  void ReductionGrad(const string& op, const Tensor& x, const Tensor& idx,\n+  void ReductionGrad(const std::string& op, const Tensor& x, const Tensor& idx,\n                      Tensor* dx, Tensor* di) {\n     const DataType T = x.dtype();\n-    auto adef = [T](const string& name) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(T));\n+    auto adef = [T](const std::string& name) {  // E.g., x:float, dy:double\n+      return absl::StrCat(name, \":\", DataTypeString(T));\n     };\n     // Sum(op(x, idx)), sum all output of op(x, idx).\n     auto test = FDH::Define(\"Test\", {adef(\"x\"), \"i:int32\"}, {adef(\"l\")}, {},\n@@ -225,11 +225,11 @@ class MathGradTest : public ::testing::Test {\n     *di = outputs[1];\n   }\n \n-  Tensor ReduceSum(const Tensor& x, absl::Span<const int32> axes) {\n+  Tensor ReduceSum(const Tensor& x, absl::Span<const int32_t> axes) {\n     int num_axes = axes.length();\n     Tensor y(DT_INT32, TensorShape({num_axes}));\n     for (size_t i = 0; i < axes.size(); ++i) {\n-      y.flat<int32>()(i) = axes[i];\n+      y.flat<int32_t>()(i) = axes[i];\n     }\n     auto T = x.dtype();\n     auto gdef = test::function::GDef(\n@@ -248,8 +248,8 @@ class MathGradTest : public ::testing::Test {\n     return outputs[0];\n   }\n \n-  Tensor MatMulCommon(const string& opname, const string& attr_adj_x,\n-                      const string& attr_adj_y, const Tensor& x, bool ax,\n+  Tensor MatMulCommon(const std::string& opname, const std::string& attr_adj_x,\n+                      const std::string& attr_adj_y, const Tensor& x, bool ax,\n                       const Tensor& y, bool ay) {\n     auto T = x.dtype();\n     auto gdef = test::function::GDef(\n@@ -281,12 +281,13 @@ class MathGradTest : public ::testing::Test {\n     return MatMulCommon(\"BatchMatMulV2\", \"adj_x\", \"adj_y\", x, ax, y, ay);\n   }\n \n-  void MatMulGradCommon(const string& opname, const string& attr_adj_x,\n-                        const string& attr_adj_y, const Tensor& x, bool ax,\n+  void MatMulGradCommon(const std::string& opname,\n+                        const std::string& attr_adj_x,\n+                        const std::string& attr_adj_y, const Tensor& x, bool ax,\n                         const Tensor& y, bool ay, Tensor* dx, Tensor* dy) {\n     const DataType T = x.dtype();\n-    auto adef = [T](const string& name) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(T));\n+    auto adef = [T](const std::string& name) {  // E.g., x:float, dy:double\n+      return absl::StrCat(name, \":\", DataTypeString(T));\n     };\n     // Sum(op(x)), sum all output of op(x).\n     auto test =\n@@ -412,7 +413,7 @@ class MathGradTest : public ::testing::Test {\n   }\n };\n \n-void HasError(const absl::Status& s, const string& substr) {\n+void HasError(const absl::Status& s, const std::string& substr) {\n   EXPECT_TRUE(absl::StrContains(s.ToString(), substr))\n       << s << \", expected substring \" << substr;\n }\n@@ -1363,187 +1364,187 @@ TEST_F(MathGradTest, BatchMatMulV2_BroadcastWhileAdjointed) {\n TEST_F(MathGradTest, Sum_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Sum\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 1.f, 1.f, 1.f, 1.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Sum_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Sum\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 1.f, 1.f, 1.f, 1.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Mean_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Mean\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>(\n               {1.f / 2, 1.f / 2, 1.f / 2, 1.f / 2, 1.f / 2, 1.f / 2},\n               TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Mean_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Mean\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>(\n               {1.f / 3, 1.f / 3, 1.f / 3, 1.f / 3, 1.f / 3, 1.f / 3},\n               TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Mean_dim0_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Mean\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>(\n               {1.f / 6, 1.f / 6, 1.f / 6, 1.f / 6, 1.f / 6, 1.f / 6},\n               TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Min_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 1.f, 1.f, 0.f, 0.f, 0.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Min_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 0.f, 0.f, 1.f, 0.f, 0.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Min_dim0_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 0.f, 0.f, 0.f, 0.f, 0.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Min_dim0_dim1_Dups) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, -3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({.5f, 0.f, 0.f, 0.f, 0.f, .5f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Max_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   LOG(INFO) << dx.SummarizeValue(6);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({0.f, 0.f, 0.f, 1.f, 1.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Max_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({0.f, 0.f, 1.f, 0.f, 0.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Max_dim0_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({0.f, 0.f, 0.f, 0.f, 0.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Max_dim0_dim1_Dups) {\n   auto x = test::AsTensor<float>({3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({.5f, 0.f, 0.f, 0.f, 0.f, .5f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n }  // namespace"
        },
        {
            "sha": "e594e3d5c4aca01312a6610c27cf9d028b96b243",
            "filename": "tensorflow/core/ops/math_ops.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1029,7 +1029,7 @@ absl::Status ArgOpShape(shape_inference::InferenceContext* c) {\n \n   int64_t dimension_val;\n   if (dim_t->dtype() == DT_INT32) {\n-    dimension_val = dim_t->scalar<int32>()();\n+    dimension_val = dim_t->scalar<int32_t>()();\n   } else {\n     dimension_val = dim_t->scalar<int64_t>()();\n   }\n@@ -1142,7 +1142,7 @@ absl::Status SparseSegmentReductionGradShapeFnImpl(\n     // shape is unknown.\n     dim0_shape = c->Vector(InferenceContext::kUnknownDim);\n   } else {\n-    auto dim0_value = dim0->scalar<int32>()();\n+    auto dim0_value = dim0->scalar<int32_t>()();\n     if (dim0_value < 0) {\n       return errors::InvalidArgument(\n           \"Cannot specify a negative value for output_dim0\");\n@@ -1198,7 +1198,7 @@ absl::Status SparseSegmentReductionWithNumSegmentsShapeFn(InferenceContext* c) {\n     TF_RETURN_IF_ERROR(c->Concatenate(c->Vector(InferenceContext::kUnknownDim),\n                                       subshape, &out));\n   } else {\n-    auto dim0_value = dim0->scalar<int32>()();\n+    auto dim0_value = dim0->scalar<int32_t>()();\n     if (dim0_value < 0) {\n       return errors::InvalidArgument(\n           \"Cannot specify a negative value for num_segments\");\n@@ -1573,19 +1573,19 @@ REGISTER_OP(\"Range\")\n         return absl::OkStatus();\n       }\n       if (dtype == DT_INT32) {\n-        return RangeSize<int32>(start_t, limit_t, delta_t, c);\n+        return RangeSize<int32_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_INT16) {\n-        return RangeSize<int16>(start_t, limit_t, delta_t, c);\n+        return RangeSize<int16_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_INT8) {\n-        return RangeSize<int8>(start_t, limit_t, delta_t, c);\n+        return RangeSize<int8_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_INT64) {\n         return RangeSize<int64_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_UINT16) {\n-        return RangeSize<uint16>(start_t, limit_t, delta_t, c);\n+        return RangeSize<uint16_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_UINT32) {\n-        return RangeSize<uint32>(start_t, limit_t, delta_t, c);\n+        return RangeSize<uint32_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_UINT64) {\n-        return RangeSize<uint64>(start_t, limit_t, delta_t, c);\n+        return RangeSize<uint64_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_FLOAT) {\n         return RangeSize<float>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_DOUBLE) {\n@@ -1621,7 +1621,7 @@ REGISTER_OP(\"LinSpace\")\n \n       int64_t num;\n       if (num_t->dtype() == DT_INT32) {\n-        num = num_t->scalar<int32>()();\n+        num = num_t->scalar<int32_t>()();\n       } else {\n         num = num_t->scalar<int64_t>()();\n       }\n@@ -1760,7 +1760,7 @@ REGISTER_OP(\"Bincount\")\n       }\n \n       // Return `[size]` shape if size is known.\n-      int32_t size_val = size_tensor->scalar<int32>()();\n+      int32_t size_val = size_tensor->scalar<int32_t>()();\n       if (size_val < 0) {\n         return errors::InvalidArgument(\"size (\", size_val,\n                                        \") must be non-negative\");\n@@ -1801,7 +1801,7 @@ REGISTER_OP(\"DenseBincount\")\n       DataType dtype;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n       if (dtype == DT_INT32) {\n-        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n+        size_val = static_cast<int64_t>(size_tensor->scalar<int32_t>()());\n       } else if (dtype == DT_INT64) {\n         size_val = size_tensor->scalar<int64_t>()();\n       } else {\n@@ -1846,7 +1846,7 @@ REGISTER_OP(\"SparseBincount\")\n       DataType dtype;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n       if (dtype == DT_INT32) {\n-        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n+        size_val = static_cast<int64_t>(size_tensor->scalar<int32_t>()());\n       } else if (dtype == DT_INT64) {\n         size_val = size_tensor->scalar<int64_t>()();\n       } else {\n@@ -2136,11 +2136,11 @@ REGISTER_OP(\"SobolSample\")\n       const Tensor* num_results_t = c->input_tensor(1);\n \n       int32_t dim = dim_t == nullptr ? InferenceContext::kUnknownDim\n-                                     : dim_t->scalar<int32>()();\n+                                     : dim_t->scalar<int32_t>()();\n \n       int32_t num_results = num_results_t == nullptr\n                                 ? InferenceContext::kUnknownDim\n-                                : num_results_t->scalar<int32>()();\n+                                : num_results_t->scalar<int32_t>()();\n \n       c->set_output(0, c->Matrix(num_results, dim));\n       return absl::OkStatus();"
        },
        {
            "sha": "b4392dbe439bf56315db203db78526e4f9f3a190",
            "filename": "tensorflow/core/ops/math_ops_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fmath_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -160,18 +160,18 @@ TEST(MathOpsTest, BroadcastBinaryOps_ShapeFn) {\n     }\n   };\n \n-  for (string op_name : {\"Add\",        \"Complex\",\n-                         \"Div\",        \"Equal\",\n-                         \"Greater\",    \"GreaterEqual\",\n-                         \"Igamma\",     \"Igammac\",\n-                         \"Zeta\",       \"Polygamma\",\n-                         \"Less\",       \"LessEqual\",\n-                         \"LogicalAnd\", \"LogicalOr\",\n-                         \"Maximum\",    \"Minimum\",\n-                         \"Mod\",        \"Mul\",\n-                         \"NotEqual\",   \"Pow\",\n-                         \"Sub\",        \"SquaredDifference\",\n-                         \"DivNoNan\"}) {\n+  for (std::string op_name : {\"Add\",        \"Complex\",\n+                              \"Div\",        \"Equal\",\n+                              \"Greater\",    \"GreaterEqual\",\n+                              \"Igamma\",     \"Igammac\",\n+                              \"Zeta\",       \"Polygamma\",\n+                              \"Less\",       \"LessEqual\",\n+                              \"LogicalAnd\", \"LogicalOr\",\n+                              \"Maximum\",    \"Minimum\",\n+                              \"Mod\",        \"Mul\",\n+                              \"NotEqual\",   \"Pow\",\n+                              \"Sub\",        \"SquaredDifference\",\n+                              \"DivNoNan\"}) {\n     ShapeInferenceTestOp op(op_name);\n     AddNodeAttr(\"incompatible_shape_error\", true, &op.node_def);\n     test_shapes(op, true);"
        },
        {
            "sha": "b0da798463cc38529604eee20fd20317bf2f051a",
            "filename": "tensorflow/core/ops/nn_grad.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fnn_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fnn_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fnn_grad.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -37,7 +37,7 @@ absl::Status SoftmaxGrad(const AttrSlice& attrs, FunctionDef* g) {\n       {\n         {{\"softmax\"}, \"Softmax\", {\"x\"}, {{\"T\", \"$T\"}}},\n         {{\"n0\"}, \"Mul\", {\"grad_softmax\", \"softmax\"}, {{\"T\", \"$T\"}}},\n-        FDH::Const<int32>(\"indices\", {-1}),\n+        FDH::Const<int32_t>(\"indices\", {-1}),\n         {{\"n1\"}, \"Sum\", {\"n0\", \"indices\"}, {{\"keep_dims\", true}, {\"T\", \"$T\"}}},\n         {{\"n2\"}, \"Sub\", {\"grad_softmax\", \"n1\"}, {{\"T\", \"$T\"}}},\n         {{\"grad_x\"}, \"Mul\", {\"n2\", \"softmax\"}, {{\"T\", \"$T\"}}}\n@@ -61,7 +61,7 @@ absl::Status LogSoftmaxGrad(const AttrSlice& attrs, FunctionDef* g) {\n       // Based on _LogSoftmaxGrad in nn_grad.py.\n       {\n         {{\"softmax\"}, \"Softmax\", {\"x\"}, {{\"T\", \"$T\"}}},\n-        FDH::Const<int32>(\"indices\", {-1}),\n+        FDH::Const<int32_t>(\"indices\", {-1}),\n         {{\"n0\"}, \"Sum\", {\"grad_logsoftmax\", \"indices\"},\n          {{\"keep_dims\", true}, {\"T\", \"$T\"}}},\n         {{\"n1\"}, \"Mul\", {\"n0\", \"softmax\"}, {{\"T\", \"$T\"}}},"
        },
        {
            "sha": "c9a70cc3622cf126c7d35d6575f898ceb5e935eb",
            "filename": "tensorflow/core/ops/nn_ops.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fnn_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fnn_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fnn_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -539,8 +539,8 @@ absl::Status CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n     DimensionHandle new_height = c->UnknownDim();\n     DimensionHandle new_width = c->UnknownDim();\n     if (size != nullptr) {\n-      new_height = c->MakeDim(size->flat<int32>()(0));\n-      new_width = c->MakeDim(size->flat<int32>()(1));\n+      new_height = c->MakeDim(size->flat<int32_t>()(0));\n+      new_width = c->MakeDim(size->flat<int32_t>()(1));\n     }\n     TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 1, new_height, &resized));\n     TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 2, new_width, &resized));\n@@ -559,8 +559,8 @@ absl::Status CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n     std::vector<DimensionHandle> output_dims;\n     for (int i = 0; i < 4; ++i) {\n       DimensionHandle dim = c->Dim(resized, i);\n-      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 0));\n-      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 1));\n+      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32_t>()(i, 0));\n+      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32_t>()(i, 1));\n       if (p0 < 0 || p1 < 0) {\n         return errors::InvalidArgument(\"Paddings must be non-negative\");\n       }\n@@ -576,7 +576,7 @@ absl::Status CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n   // Work out the convolution's effect with 'padded' as the input.\n   ShapeHandle filter;\n   TF_RETURN_IF_ERROR(c->WithRank(c->input(filter_index), 4, &filter));\n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n   if (strides.size() != 4) {\n     return errors::InvalidArgument(\n@@ -1026,7 +1026,7 @@ REGISTER_OP(\"MaxPoolWithArgmax\")\n     .Output(\"argmax: Targmax\")\n     .Attr(\"T: realnumbertype\")\n     .SetShapeFn([](InferenceContext* c) {\n-      std::vector<int32> ksize;\n+      std::vector<int32_t> ksize;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"ksize\", &ksize));\n       for (int i = 0; i < ksize.size(); ++i) {\n         if (ksize[i] <= 0) {\n@@ -1091,7 +1091,7 @@ REGISTER_OP(\"Dilation2D\")\n       ShapeHandle filter_shape;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &filter_shape));\n \n-      std::vector<int32> strides;\n+      std::vector<int32_t> strides;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n       if (strides.size() != 4) {\n         return errors::InvalidArgument(\n@@ -1100,7 +1100,7 @@ REGISTER_OP(\"Dilation2D\")\n             strides.size());\n       }\n \n-      std::vector<int32> rates;\n+      std::vector<int32_t> rates;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n       if (rates.size() != 4) {\n         return errors::InvalidArgument("
        },
        {
            "sha": "23ddf12e08305e393475458fe437d71edac39423",
            "filename": "tensorflow/core/ops/nn_ops_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fnn_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fnn_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fnn_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -59,7 +59,7 @@ TEST(NNOpsTest, TopKV2_ShapeFn) {\n   Tensor k_t;\n   op.input_tensors[1] = &k_t;\n \n-  k_t = test::AsScalar<int32>(20);\n+  k_t = test::AsScalar<int32_t>(20);\n   // With known input, each output is an unknown shape.\n   INFER_OK(op, \"?;[]\", \"?;?\");\n   // With vector input, each output is [k].\n@@ -75,7 +75,7 @@ TEST(NNOpsTest, TopKV2_ShapeFn) {\n               \"[1];[]\");\n   INFER_ERROR(\"input must have last dimension >= k = 20 but is 4\", op,\n               \"[1,2,3,4];[]\");\n-  k_t = test::AsScalar<int32>(-1);\n+  k_t = test::AsScalar<int32_t>(-1);\n   INFER_ERROR(\n       \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n       op, \"[1,2,3,4];[]\");\n@@ -87,7 +87,7 @@ TEST(NNOpsTest, NthElement_ShapeFn) {\n \n   Tensor n_t;\n   op.input_tensors[1] = &n_t;\n-  n_t = test::AsScalar<int32>(20);\n+  n_t = test::AsScalar<int32_t>(20);\n \n   INFER_OK(op, \"?;[]\", \"?\");\n   INFER_OK(op, \"[21];[]\", \"[]\");\n@@ -98,7 +98,7 @@ TEST(NNOpsTest, NthElement_ShapeFn) {\n   INFER_ERROR(\"Input must have last dimension > n = 20 but is 1\", op, \"[1];[]\");\n   INFER_ERROR(\"Input must have last dimension > n = 20 but is 20\", op,\n               \"[1,2,3,20];[]\");\n-  n_t = test::AsScalar<int32>(-1);\n+  n_t = test::AsScalar<int32_t>(-1);\n   INFER_ERROR(\n       \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n       op, \"[1,2,3,4];[]\");\n@@ -182,7 +182,7 @@ TEST(NNOpsTest, FusedBatchNorm_ShapeFn) {\n   ShapeInferenceTestOp op(\"FusedBatchNorm\");\n \n   auto set_op = [&op](bool is_training, float exponential_avg_factor,\n-                      string data_format) {\n+                      std::string data_format) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNorm\")\n                      .Input(FakeInput(DT_FLOAT))\n                      .Input(FakeInput(DT_FLOAT))\n@@ -276,7 +276,7 @@ TEST(NNOpsTest, FusedBatchNorm_ShapeFn) {\n \n TEST(NNOpsTest, FusedBatchNormGrad_ShapeFn) {\n   ShapeInferenceTestOp op(\"FusedBatchNormGrad\");\n-  auto set_op = [&op](string data_format) {\n+  auto set_op = [&op](std::string data_format) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNormGrad\")\n                      .Input(FakeInput(DT_FLOAT))\n                      .Input(FakeInput(DT_FLOAT))\n@@ -490,8 +490,9 @@ TEST(NNOpsTest, InTopK_ShapeFn) {\n \n TEST(NNOpsTest, Dilation2DShapeTest) {\n   ShapeInferenceTestOp op(\"Dilation2D\");\n-  auto set_op = [&op](const std::vector<int32>& strides,\n-                      const std::vector<int32>& rates, const string& padding) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::vector<int32_t>& rates,\n+                      const std::string& padding) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"Dilation2D\")\n                      .Input(\"input\", 0, DT_FLOAT)\n                      .Input(\"filter\", 0, DT_FLOAT)\n@@ -568,8 +569,8 @@ TEST(NNOpsTest, FractionalAvgPoolGrad) {\n   INFER_OK(op, \"?;?;?;?\", \"[?,?,?,?]\");\n \n   // When input tensor is known, its values determine output shape.\n-  std::vector<int32> shape{1, 2, 3, 4};\n-  Tensor shape_t = test::AsTensor<int32>(shape);\n+  std::vector<int32_t> shape{1, 2, 3, 4};\n+  Tensor shape_t = test::AsTensor<int32_t>(shape);\n   op.input_tensors[0] = &shape_t;\n   INFER_OK(op, \"[5];?;?;?\", \"[1,2,3,4]\");\n }"
        },
        {
            "sha": "e433ac7ae3e5e106b20f0968f83a8959e4451bb8",
            "filename": "tensorflow/core/ops/parsing_ops_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fparsing_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fparsing_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fparsing_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -157,7 +157,7 @@ TEST(ParsingOpsTest, ParseSequenceExample_ShapeFn) {\n                            bool add_extra_shape = false) {\n     using NodeOutList = std::vector<NodeDefBuilder::NodeOut>;\n     using DataTypeList = std::vector<DataType>;\n-    string string_in(\"test\");\n+    std::string string_in(\"test\");\n     NodeDefBuilder::NodeOut node_in{\"a\", 0, DT_STRING};\n     TF_ASSERT_OK(\n         NodeDefBuilder(\"test\", \"ParseSequenceExample\")\n@@ -169,15 +169,15 @@ TEST(ParsingOpsTest, ParseSequenceExample_ShapeFn) {\n             .Attr(\"Nfeature_list_sparse\", num_feature_list_sparse)\n             .Attr(\"Nfeature_list_dense\", num_feature_list_dense)\n             .Attr(\"feature_list_dense_missing_assumed_empty\",\n-                  std::vector<string>(num_feature_list_dense, string_in))\n+                  std::vector<std::string>(num_feature_list_dense, string_in))\n             .Attr(\"context_sparse_keys\",\n-                  std::vector<string>(num_context_sparse, string_in))\n+                  std::vector<std::string>(num_context_sparse, string_in))\n             .Attr(\"context_dense_keys\",\n-                  std::vector<string>(num_context_dense, string_in))\n+                  std::vector<std::string>(num_context_dense, string_in))\n             .Attr(\"feature_list_sparse_keys\",\n-                  std::vector<string>(num_feature_list_sparse, string_in))\n+                  std::vector<std::string>(num_feature_list_sparse, string_in))\n             .Attr(\"feature_list_dense_keys\",\n-                  std::vector<string>(num_feature_list_dense, string_in))\n+                  std::vector<std::string>(num_feature_list_dense, string_in))\n             .Attr(\"context_sparse_types\",\n                   DataTypeList(num_context_sparse, DT_FLOAT))\n             .Attr(\"context_dense_types\",\n@@ -395,7 +395,7 @@ TEST(ParsingOpsTest, ParseSequenceExampleV2_ShapeFn) {\n                            bool add_extra_shape = false) {\n     using NodeOutList = std::vector<NodeDefBuilder::NodeOut>;\n     using DataTypeList = std::vector<DataType>;\n-    string string_in(\"test\");\n+    std::string string_in(\"test\");\n     NodeDefBuilder::NodeOut node_in{\"a\", 0, DT_STRING};\n     TF_ASSERT_OK(\n         NodeDefBuilder(\"test\", \"ParseSequenceExampleV2\")"
        },
        {
            "sha": "7d9e2dc7b4745de5851e42b6d60fc93141c3d75a",
            "filename": "tensorflow/core/ops/ragged_array_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fragged_array_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fragged_array_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fragged_array_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -106,7 +106,7 @@ REGISTER_OP(\"RaggedCross\")\n       int dense_start = num_ragged * 2 + num_sparse * 3;\n       for (int i = 0; i < dense_types.size(); ++i) {\n         ShapeHandle dense_input = c->input(i + dense_start);\n-        int32 rank = c->Rank(dense_input);\n+        int32_t rank = c->Rank(dense_input);\n         if (rank == InferenceContext::kUnknownRank) {\n           continue;\n         } else if (rank != 2) {"
        },
        {
            "sha": "016b35539805de0ea0ddf2abb0f7d88b595c9989",
            "filename": "tensorflow/core/ops/random_index_shuffle_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Frandom_index_shuffle_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Frandom_index_shuffle_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Frandom_index_shuffle_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -38,9 +38,9 @@ static absl::Status StatelessRandomPermuteShape(InferenceContext* c) {\n   TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &max_index_shape));\n \n   // Figure out if the output is a scalar or tensor.\n-  const int32 index_rank = c->Rank(index_shape);\n-  const int32 seed_rank = c->Rank(seed_shape);\n-  const int32 max_index_rank = c->Rank(max_index_shape);\n+  const int32_t index_rank = c->Rank(index_shape);\n+  const int32_t seed_rank = c->Rank(seed_shape);\n+  const int32_t max_index_rank = c->Rank(max_index_shape);\n \n   // Check that last dimension of seed is 3.\n   if (seed_rank == 1 && c->Value(c->Dim(seed_shape, 0)) != 3) {"
        },
        {
            "sha": "1f2cbafcea4c7d1ec59e2756e2a47e9f641216d8",
            "filename": "tensorflow/core/ops/random_ops_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Frandom_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Frandom_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Frandom_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -30,10 +30,10 @@ TEST(RandomOpsTest, Multinomial_ShapeFn) {\n   INFER_OK(op, \"[?,?];?\", \"[d0_0,?]\");\n   INFER_OK(op, \"[2,?];?\", \"[d0_0,?]\");\n   INFER_OK(op, \"[2,1];?\", \"[d0_0,?]\");\n-  Tensor num_samples = test::AsScalar<int32>(3);\n+  Tensor num_samples = test::AsScalar<int32_t>(3);\n   op.input_tensors[1] = &num_samples;\n   INFER_OK(op, \"[2,1];[]\", \"[d0_0,3]\");\n-  num_samples = test::AsTensor<int32>({1, 2, 3});\n+  num_samples = test::AsTensor<int32_t>({1, 2, 3});\n   INFER_ERROR(\"Shape must be rank 0 but is rank 1\", op, \"[2,1];[3]\");\n }\n \n@@ -45,7 +45,7 @@ TEST(RandomOpsTest, RandomGamma_ShapeFn) {\n   INFER_OK(op, \"?;[3]\", \"?\");\n   INFER_OK(op, \"[1];?\", \"?\");\n   INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"[1,2];[3,4]\");\n-  Tensor shape = test::AsTensor<int32>({1, 2, 3});\n+  Tensor shape = test::AsTensor<int32_t>({1, 2, 3});\n   op.input_tensors[0] = &shape;\n   INFER_OK(op, \"[3];[4,?]\", \"[1,2,3,d1_0,d1_1]\");\n   INFER_OK(op, \"[3];[4,5]\", \"[1,2,3,d1_0,d1_1]\");\n@@ -60,7 +60,7 @@ TEST(RandomOpsTest, RandomPoisson_ShapeFn) {\n   INFER_OK(op, \"?;[3]\", \"?\");\n   INFER_OK(op, \"[1];?\", \"?\");\n   INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"[1,2];[3,4]\");\n-  Tensor shape = test::AsTensor<int32>({1, 2, 3});\n+  Tensor shape = test::AsTensor<int32_t>({1, 2, 3});\n   op.input_tensors[0] = &shape;\n   INFER_OK(op, \"[3];[4,?]\", \"[1,2,3,d1_0,d1_1]\");\n   INFER_OK(op, \"[3];[4,5]\", \"[1,2,3,d1_0,d1_1]\");"
        },
        {
            "sha": "157d1f3dafb8e7739c4e8c6f7ba666f9867fe640",
            "filename": "tensorflow/core/ops/rnn_ops_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Frnn_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Frnn_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Frnn_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -23,10 +23,10 @@ limitations under the License.\n \n namespace tensorflow {\n \n-static string JoinedCopies(const string& s, int copies) {\n-  string res;\n+static std::string JoinedCopies(const std::string& s, int copies) {\n+  std::string res;\n   for (int i = 0; i < copies; ++i) {\n-    strings::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n+    absl::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n   }\n   return res;\n }\n@@ -62,7 +62,7 @@ TEST(RnnOpsTest, LSTMBlockCell_ShapeFn) {\n   ShapeInferenceTestOp op(\"LSTMBlockCell\");\n \n   // Last 6 inputs don't affect shape inference.\n-  string input_suffix = strings::StrCat(\";\", JoinedCopies(\"?\", 6));\n+  std::string input_suffix = absl::StrCat(\";\", JoinedCopies(\"?\", 6));\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 2\", op, \"[?];?\" + input_suffix);\n@@ -77,7 +77,7 @@ TEST(RnnOpsTest, LSTMBlockCellGrad_ShapeFn) {\n   ShapeInferenceTestOp op(\"LSTMBlockCellGrad\");\n \n   // Last 14 inputs don't affect shape inference.\n-  string input_suffix = strings::StrCat(\";\", JoinedCopies(\"?\", 14));\n+  std::string input_suffix = absl::StrCat(\";\", JoinedCopies(\"?\", 14));\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 2\", op, \"[?];?\" + input_suffix);\n@@ -107,7 +107,7 @@ TEST(RnnOpsTest, BlockLSTM_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Middle inputs don't affect shape inference.\n-  string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n+  std::string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?]\" + infix + \"?\");\n@@ -147,7 +147,7 @@ TEST(RnnOpsTest, BlockLSTMGrad_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Last inputs don't affect shape inference.\n-  string suffix = \";\" + JoinedCopies(\"?\", 9);\n+  std::string suffix = \";\" + JoinedCopies(\"?\", 9);\n \n   // Rank check for x\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?];?;?;?;?;?;?;?\" + suffix);\n@@ -167,11 +167,11 @@ TEST(RnnOpsTest, BlockLSTMGrad_ShapeFn) {\n       \"[?,?,?];\" + JoinedCopies(\"[?,?]\", 3) + \";\" + JoinedCopies(\"[?]\", 4));\n \n   // Output with copies input shapes to output.\n-  string input = strings::StrCat(\"?;[?,?,?];\", JoinedCopies(\"[?,?]\", 3), \";\",\n-                                 JoinedCopies(\"[?]\", 4), suffix);\n-  string expected = \"in1\";\n+  std::string input = strings::StrCat(\"?;[?,?,?];\", JoinedCopies(\"[?,?]\", 3),\n+                                      \";\", JoinedCopies(\"[?]\", 4), suffix);\n+  std::string expected = \"in1\";\n   for (int i = 1; i < 8; ++i) {\n-    strings::StrAppend(&expected, \";in\", (i + 1));\n+    absl::StrAppend(&expected, \";in\", i + 1);\n   }\n   INFER_OK(op, input, expected);\n }"
        },
        {
            "sha": "927e74a89f6db3fa0cede880f38f22dd3dd373fc",
            "filename": "tensorflow/core/ops/sparse_csr_matrix_ops_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fsparse_csr_matrix_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fsparse_csr_matrix_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fsparse_csr_matrix_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -189,7 +189,8 @@ TEST(SparseMatrixOpsTest, SparseMatrixAdd_ShapeFn) {\n   op.input_resource_handle_shapes_and_types.push_back(nullptr);\n   op.input_resource_handle_shapes_and_types.push_back(nullptr);\n   auto set_shapes = [&a_shapes_and_types, &b_shapes_and_types](\n-                        const string& a_shape, const string& b_shape) {\n+                        const std::string& a_shape,\n+                        const std::string& b_shape) {\n     a_shapes_and_types[0].first = a_shape;\n     b_shapes_and_types[0].first = b_shape;\n   };\n@@ -225,7 +226,8 @@ TEST(SparseMatrixOpsTest, SparseMatrixSparseMatMul_ShapeFn) {\n   op.input_resource_handle_shapes_and_types.push_back(&a_shapes_and_types);\n   op.input_resource_handle_shapes_and_types.push_back(&b_shapes_and_types);\n   auto set_shapes = [&a_shapes_and_types, &b_shapes_and_types](\n-                        const string& a_shape, const string& b_shape) {\n+                        const std::string& a_shape,\n+                        const std::string& b_shape) {\n     a_shapes_and_types[0].first = a_shape;\n     b_shapes_and_types[0].first = b_shape;\n   };\n@@ -323,7 +325,8 @@ TEST(SparseMatrixOpsTest, SparseMatrixSoftmaxGrad_ShapeFn) {\n   op.input_resource_handle_shapes_and_types.push_back(&a_shapes_and_types);\n   op.input_resource_handle_shapes_and_types.push_back(&b_shapes_and_types);\n   auto set_shapes = [&a_shapes_and_types, &b_shapes_and_types](\n-                        const string& a_shape, const string& b_shape) {\n+                        const std::string& a_shape,\n+                        const std::string& b_shape) {\n     a_shapes_and_types[0].first = a_shape;\n     b_shapes_and_types[0].first = b_shape;\n   };"
        },
        {
            "sha": "3ca5baa9351894e3f1655e535f9e82c5408e5e08",
            "filename": "tensorflow/core/ops/sparse_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fsparse_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fsparse_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fsparse_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -126,7 +126,7 @@ TEST(SparseOpsTest, SparseToDense_ShapeFn) {\n   INFER_OK(op, \"?;?;?;?\", \"?\");\n   INFER_OK(op, \"?;[?];?;?\", \"?\");\n   INFER_OK(op, \"?;[4];?;?\", \"[?,?,?,?]\");\n-  Tensor in_t = test::AsTensor<int32>({1, 2, 3, 4});\n+  Tensor in_t = test::AsTensor<int32_t>({1, 2, 3, 4});\n   op.input_tensors[1] = &in_t;\n   INFER_OK(op, \"?;[4];?;?\", \"[1,2,3,4]\");\n }"
        },
        {
            "sha": "f0e9b434c38196c6182657ac4f6537269d6beede",
            "filename": "tensorflow/core/ops/spectral_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fspectral_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fspectral_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fspectral_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -115,7 +115,7 @@ absl::Status RFFTShape(InferenceContext* c, const bool forward,\n       TF_RETURN_IF_ERROR(c->ReplaceDim(out, -rank + i, c->UnknownDim(), &out));\n     }\n   } else {\n-    auto fft_length_as_vec = fft_length_tensor->vec<int32>();\n+    auto fft_length_as_vec = fft_length_tensor->vec<int32_t>();\n     for (int i = 0; i < rank; ++i) {\n       // For RFFT, replace the last dimension with fft_length/2 + 1.\n       auto dim = forward && i == rank - 1 && fft_length_as_vec(i) != 0"
        },
        {
            "sha": "49de445d57fd0760129595b143569d442d41a2ba",
            "filename": "tensorflow/core/ops/spectral_ops_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fspectral_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fspectral_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fspectral_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -72,7 +72,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n \n     // Tests with known values for fft_length input.\n     op.input_tensors.resize(2);\n-    Tensor fft_length = test::AsTensor<int32>({10});\n+    Tensor fft_length = test::AsTensor<int32_t>({10});\n     op.input_tensors[1] = &fft_length;\n \n     // The inner-most dimension of the RFFT is n/2+1 while for IRFFT it's n.\n@@ -86,7 +86,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1];[1]\", \"[d0_0,10]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({11});\n+    fft_length = test::AsTensor<int32_t>({11});\n     if (forward) {\n       INFER_OK(op, \"[?];[1]\", \"[6]\");\n       INFER_OK(op, \"[1];[1]\", \"[6]\");\n@@ -97,7 +97,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1];[1]\", \"[d0_0,11]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({12});\n+    fft_length = test::AsTensor<int32_t>({12});\n     if (forward) {\n       INFER_OK(op, \"[?];[1]\", \"[7]\");\n       INFER_OK(op, \"[1];[1]\", \"[7]\");\n@@ -132,7 +132,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n \n     // Tests with known values for fft_length input.\n     op.input_tensors.resize(2);\n-    Tensor fft_length = test::AsTensor<int32>({9, 10});\n+    Tensor fft_length = test::AsTensor<int32_t>({9, 10});\n     op.input_tensors[1] = &fft_length;\n \n     // The inner-most dimension of the RFFT is n/2+1 while for IRFFT it's n.\n@@ -146,7 +146,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1];[2]\", \"[d0_0,9,10]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({10, 11});\n+    fft_length = test::AsTensor<int32_t>({10, 11});\n     if (forward) {\n       INFER_OK(op, \"[?,?];[2]\", \"[10,6]\");\n       INFER_OK(op, \"[1,1];[2]\", \"[10,6]\");\n@@ -157,7 +157,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1];[2]\", \"[d0_0,10,11]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({11, 12});\n+    fft_length = test::AsTensor<int32_t>({11, 12});\n     if (forward) {\n       INFER_OK(op, \"[?,?];[2]\", \"[11,7]\");\n       INFER_OK(op, \"[1,1];[2]\", \"[11,7]\");\n@@ -192,7 +192,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n \n     // Tests with known values for fft_length input.\n     op.input_tensors.resize(2);\n-    Tensor fft_length = test::AsTensor<int32>({10, 11, 12});\n+    Tensor fft_length = test::AsTensor<int32_t>({10, 11, 12});\n     op.input_tensors[1] = &fft_length;\n \n     // The inner-most dimension of the RFFT is n/2+1 while for IRFFT it's n.\n@@ -206,7 +206,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1,1];[3]\", \"[d0_0,10,11,12]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({11, 12, 13});\n+    fft_length = test::AsTensor<int32_t>({11, 12, 13});\n     if (forward) {\n       INFER_OK(op, \"[?,?,?];[3]\", \"[11,12,7]\");\n       INFER_OK(op, \"[1,1,1];[3]\", \"[11,12,7]\");\n@@ -217,7 +217,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1,1];[3]\", \"[d0_0,11,12,13]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({12, 13, 14});\n+    fft_length = test::AsTensor<int32_t>({12, 13, 14});\n     if (forward) {\n       INFER_OK(op, \"[?,?,?];[3]\", \"[12,13,8]\");\n       INFER_OK(op, \"[1,1,1];[3]\", \"[12,13,8]\");"
        },
        {
            "sha": "702791f04ef090b095a5bd3fe0da4e4b1168d41b",
            "filename": "tensorflow/core/ops/string_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fstring_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Fstring_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fstring_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -145,16 +145,16 @@ REGISTER_OP(\"StringFormat\")\n     .Attr(\"placeholder: string = '%s'\")\n     .Attr(\"summarize: int = 3\")\n     .SetShapeFn([](InferenceContext* c) {\n-      string template_;\n-      string placeholder;\n+      std::string template_;\n+      std::string placeholder;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"template\", &template_));\n       TF_RETURN_IF_ERROR(c->GetAttr(\"placeholder\", &placeholder));\n \n       std::vector<std::string> split_template;\n       split_template = absl::StrSplit(template_, placeholder);\n       int64_t num_placeholders = split_template.size() - 1;\n       if (c->num_inputs() != num_placeholders) {\n-        return errors::InvalidArgument(strings::StrCat(\n+        return errors::InvalidArgument(absl::StrCat(\n             \"num placeholders in template and num inputs must match: \",\n             num_placeholders, \" vs. \", c->num_inputs()));\n       }"
        },
        {
            "sha": "1f966749f8a3107afc8752285e3e4497d8f8534e",
            "filename": "tensorflow/core/ops/tpu_embedding_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Ftpu_embedding_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Ftpu_embedding_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Ftpu_embedding_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -137,7 +137,7 @@ REGISTER_OP(\"EnqueueTPUEmbeddingSparseBatch\")\n     .Attr(\"combiners: list(string) = []\")\n     .SetIsStateful()\n     .SetShapeFn([](shape_inference::InferenceContext* c) -> absl::Status {\n-      std::vector<string> combiners;\n+      std::vector<std::string> combiners;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"combiners\", &combiners));\n       int n;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"N\", &n));"
        },
        {
            "sha": "b92f897d346946d32100ef7ac34c66e9e6150d2a",
            "filename": "tensorflow/core/ops/training_ops_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Ftraining_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fops%2Ftraining_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Ftraining_ops_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,9 +20,9 @@ limitations under the License.\n namespace tensorflow {\n \n // Used for testing the grad+indices handling for SparseApplyXYZ tests.\n-static void TestGradAndIndicesErrorHandling(const ShapeInferenceTestOp& op,\n-                                            string shape_spec_middle,\n-                                            const string& shape_spec_end = \"\") {\n+static void TestGradAndIndicesErrorHandling(\n+    const ShapeInferenceTestOp& op, std::string shape_spec_middle,\n+    const std::string& shape_spec_end = \"\") {\n   auto shape_spec = [&shape_spec_middle, shape_spec_end](\n                         const char* var_spec, const char* grad_indices_spec) {\n     return strings::StrCat(var_spec, \";\", shape_spec_middle, \";\","
        },
        {
            "sha": "312e9286e3d19d062bb2132fd0c28c738268e467",
            "filename": "tensorflow/core/public/version.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fpublic%2Fversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fpublic%2Fversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fpublic%2Fversion.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -93,7 +93,7 @@ limitations under the License.\n \n #define TF_GRAPH_DEF_VERSION_MIN_PRODUCER 0\n #define TF_GRAPH_DEF_VERSION_MIN_CONSUMER 0\n-#define TF_GRAPH_DEF_VERSION 2413  // Updated: 2025/11/16\n+#define TF_GRAPH_DEF_VERSION 2418  // Updated: 2025/11/21\n \n // Checkpoint compatibility versions (the versions field in SavedSliceMeta).\n //"
        },
        {
            "sha": "1443cffc4c6e6a95cb64d53e63b7959a66fcf26f",
            "filename": "tensorflow/core/summary/loader.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Floader.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Floader.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Floader.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -33,9 +33,9 @@ namespace tensorflow {\n namespace {\n \n template <typename T>\n-string AddCommas(T n) {\n+std::string AddCommas(T n) {\n   static_assert(std::is_integral<T>::value, \"is_integral\");\n-  string s = strings::StrCat(n);\n+  std::string s = strings::StrCat(n);\n   if (s.size() > 3) {\n     int extra = s.size() / 3 - (s.size() % 3 == 0 ? 1 : 0);\n     s.append(extra, 'X');\n@@ -52,19 +52,19 @@ string AddCommas(T n) {\n }\n \n int main(int argc, char* argv[]) {\n-  string path;\n-  string events;\n-  string experiment_name;\n-  string run_name;\n-  string user_name;\n+  std::string path;\n+  std::string events;\n+  std::string experiment_name;\n+  std::string run_name;\n+  std::string user_name;\n   std::vector<Flag> flag_list = {\n       Flag(\"db\", &path, \"Path of SQLite DB file\"),\n       Flag(\"events\", &events, \"TensorFlow record proto event log file\"),\n       Flag(\"experiment_name\", &experiment_name, \"The DB experiment_name value\"),\n       Flag(\"run_name\", &run_name, \"The DB run_name value\"),\n       Flag(\"user_name\", &user_name, \"The DB user_name value\"),\n   };\n-  string usage = Flags::Usage(argv[0], flag_list);\n+  std::string usage = Flags::Usage(argv[0], flag_list);\n   bool parse_result = Flags::Parse(&argc, argv, flag_list);\n   if (!parse_result || path.empty()) {\n     std::cerr << \"The loader tool imports tf.Event record files, created by\\n\"\n@@ -99,9 +99,9 @@ int main(int argc, char* argv[]) {\n   TF_CHECK_OK(env->NewRandomAccessFile(events, &file));\n   io::RecordReader reader(file.get());\n \n-  uint64 start = env->NowMicros();\n-  uint64 records = 0;\n-  uint64 offset = 0;\n+  uint64_t start = env->NowMicros();\n+  uint64_t records = 0;\n+  uint64_t offset = 0;\n   tstring record;\n   while (true) {\n     std::unique_ptr<Event> event = std::unique_ptr<Event>(new Event);\n@@ -116,9 +116,10 @@ int main(int argc, char* argv[]) {\n     TF_CHECK_OK(db_writer->WriteEvent(std::move(event)));\n     ++records;\n   }\n-  uint64 elapsed = env->NowMicros() - start;\n-  uint64 bps = (elapsed == 0 ? offset : static_cast<uint64>(\n-                                            offset / (elapsed / 1000000.0)));\n+  uint64_t elapsed = env->NowMicros() - start;\n+  uint64_t bps =\n+      (elapsed == 0 ? offset\n+                    : static_cast<uint64_t>(offset / (elapsed / 1000000.0)));\n   LOG(INFO) << \"Loaded \" << AddCommas(offset) << \" bytes with \"\n             << AddCommas(records) << \" records at \" << AddCommas(bps) << \" bps\";\n   return 0;"
        },
        {
            "sha": "d39fd74812491f25101026af1c16ea91ad770a35",
            "filename": "tensorflow/core/summary/schema.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fschema.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fschema.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fschema.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -21,7 +21,7 @@ limitations under the License.\n \n namespace tensorflow {\n \n-constexpr uint32 kTensorboardSqliteApplicationId = 0xfeedabee;\n+constexpr uint32_t kTensorboardSqliteApplicationId = 0xfeedabee;\n \n /// \\brief Creates TensorBoard SQLite tables and indexes.\n ///"
        },
        {
            "sha": "a5e3695e42010302e36f0699ebde6fe432c29510",
            "filename": "tensorflow/core/summary/summary_converter.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -72,16 +72,16 @@ absl::Status TensorValueAt(Tensor t, int64_t i, T* out) {\n #undef COMPLEX_CASE\n }\n \n-typedef Eigen::Tensor<uint8, 2, Eigen::RowMajor> Uint8Image;\n+typedef Eigen::Tensor<uint8_t, 2, Eigen::RowMajor> Uint8Image;\n \n // Add the sequence of images specified by ith_image to the summary.\n //\n // Factoring this loop out into a helper function lets ith_image behave\n // differently in the float and uint8 cases: the float case needs a temporary\n // buffer which can be shared across calls to ith_image, but the uint8 case\n // does not.\n-absl::Status AddImages(const string& tag, int max_images, int batch_size, int w,\n-                       int h, int depth,\n+absl::Status AddImages(const std::string& tag, int max_images, int batch_size,\n+                       int w, int h, int depth,\n                        const std::function<Uint8Image(int)>& ith_image,\n                        Summary* s) {\n   const int N = std::min<int>(max_images, batch_size);\n@@ -118,7 +118,7 @@ absl::Status AddImages(const string& tag, int max_images, int batch_size, int w,\n template <class T>\n void NormalizeFloatImage(int hw, int depth,\n                          typename TTypes<T>::ConstMatrix values,\n-                         typename TTypes<uint8>::ConstVec bad_color,\n+                         typename TTypes<uint8_t>::ConstVec bad_color,\n                          Uint8Image* image) {\n   if (!image->size()) return;  // Nothing to do for empty images\n \n@@ -178,8 +178,8 @@ void NormalizeFloatImage(int hw, int depth,\n       }\n     }\n     if (finite) {\n-      image->chip<0>(i) =\n-          (values.template chip<0>(i) * scale + offset).template cast<uint8>();\n+      image->chip<0>(i) = (values.template chip<0>(i) * scale + offset)\n+                              .template cast<uint8_t>();\n     } else {\n       image->chip<0>(i) = bad_color;\n     }\n@@ -189,16 +189,16 @@ void NormalizeFloatImage(int hw, int depth,\n template <class T>\n absl::Status NormalizeAndAddImages(const Tensor& tensor, int max_images, int h,\n                                    int w, int hw, int depth, int batch_size,\n-                                   const string& base_tag,\n+                                   const std::string& base_tag,\n                                    Tensor bad_color_tensor, Summary* s) {\n   // For float and half images, nans and infs are replaced with bad_color.\n   if (bad_color_tensor.dim_size(0) < depth) {\n     return errors::InvalidArgument(\n         \"expected depth <= bad_color.size, got depth = \", depth,\n         \", bad_color.size = \", bad_color_tensor.dim_size(0));\n   }\n-  auto bad_color_full = bad_color_tensor.vec<uint8>();\n-  typename TTypes<uint8>::ConstVec bad_color(bad_color_full.data(), depth);\n+  auto bad_color_full = bad_color_tensor.vec<uint8_t>();\n+  typename TTypes<uint8_t>::ConstVec bad_color(bad_color_full.data(), depth);\n \n   // Float images must be scaled and translated.\n   Uint8Image image(hw, depth);\n@@ -214,7 +214,7 @@ absl::Status NormalizeAndAddImages(const Tensor& tensor, int max_images, int h,\n \n }  // namespace\n \n-absl::Status AddTensorAsScalarToSummary(const Tensor& t, const string& tag,\n+absl::Status AddTensorAsScalarToSummary(const Tensor& t, const std::string& tag,\n                                         Summary* s) {\n   Summary::Value* v = s->add_value();\n   v->set_tag(tag);\n@@ -224,8 +224,8 @@ absl::Status AddTensorAsScalarToSummary(const Tensor& t, const string& tag,\n   return absl::OkStatus();\n }\n \n-absl::Status AddTensorAsHistogramToSummary(const Tensor& t, const string& tag,\n-                                           Summary* s) {\n+absl::Status AddTensorAsHistogramToSummary(const Tensor& t,\n+                                           const std::string& tag, Summary* s) {\n   Summary::Value* v = s->add_value();\n   v->set_tag(tag);\n   histogram::Histogram histo;\n@@ -244,9 +244,9 @@ absl::Status AddTensorAsHistogramToSummary(const Tensor& t, const string& tag,\n   return absl::OkStatus();\n }\n \n-absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_images, const Tensor& bad_color,\n-                                       Summary* s) {\n+absl::Status AddTensorAsImageToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_images,\n+                                       const Tensor& bad_color, Summary* s) {\n   if (!(tensor.dims() == 4 &&\n         (tensor.dim_size(3) == 1 || tensor.dim_size(3) == 3 ||\n          tensor.dim_size(3) == 4))) {\n@@ -269,8 +269,8 @@ absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n   if (tensor.dtype() == DT_UINT8) {\n     // For uint8 input, no normalization is necessary\n     auto ith_image = [&tensor, batch_size, hw, depth](int i) {\n-      auto values = tensor.shaped<uint8, 3>({batch_size, hw, depth});\n-      return typename TTypes<uint8>::ConstMatrix(\n+      auto values = tensor.shaped<uint8_t, 3>({batch_size, hw, depth});\n+      return typename TTypes<uint8_t>::ConstMatrix(\n           &values(i, 0, 0), Eigen::DSizes<Eigen::DenseIndex, 2>(hw, depth));\n     };\n     TF_RETURN_IF_ERROR(\n@@ -293,9 +293,9 @@ absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n   return absl::OkStatus();\n }\n \n-absl::Status AddTensorAsAudioToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_outputs, float sample_rate,\n-                                       Summary* s) {\n+absl::Status AddTensorAsAudioToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_outputs,\n+                                       float sample_rate, Summary* s) {\n   if (sample_rate <= 0.0f) {\n     return errors::InvalidArgument(\"sample_rate must be > 0\");\n   }"
        },
        {
            "sha": "650958341e9d37098af163681a9ebc021200b19d",
            "filename": "tensorflow/core/summary/summary_converter.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -23,16 +23,16 @@ limitations under the License.\n namespace tensorflow {\n \n // TODO(jart): Delete these methods in favor of new Python implementation.\n-absl::Status AddTensorAsScalarToSummary(const Tensor& t, const string& tag,\n+absl::Status AddTensorAsScalarToSummary(const Tensor& t, const std::string& tag,\n                                         Summary* s);\n-absl::Status AddTensorAsHistogramToSummary(const Tensor& t, const string& tag,\n-                                           Summary* s);\n-absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_images, const Tensor& bad_color,\n-                                       Summary* s);\n-absl::Status AddTensorAsAudioToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_outputs, float sample_rate,\n-                                       Summary* s);\n+absl::Status AddTensorAsHistogramToSummary(const Tensor& t,\n+                                           const std::string& tag, Summary* s);\n+absl::Status AddTensorAsImageToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_images,\n+                                       const Tensor& bad_color, Summary* s);\n+absl::Status AddTensorAsAudioToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_outputs,\n+                                       float sample_rate, Summary* s);\n \n }  // namespace tensorflow\n "
        },
        {
            "sha": "849fc9a6954c7e96ec77d5e7b8e3e91e104f6bc9",
            "filename": "tensorflow/core/summary/summary_db_writer.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 59,
            "changes": 121,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -67,13 +67,13 @@ namespace tensorflow {\n namespace {\n \n // https://www.sqlite.org/fileformat.html#record_format\n-const uint64 kIdTiers[] = {\n+const uint64_t kIdTiers[] = {\n     0x7fffffULL,        // 23-bit (3 bytes on disk)\n     0x7fffffffULL,      // 31-bit (4 bytes on disk)\n     0x7fffffffffffULL,  // 47-bit (5 bytes on disk)\n                         // remaining bits for future use\n };\n-const int kMaxIdTier = sizeof(kIdTiers) / sizeof(uint64) - 1;\n+const int kMaxIdTier = sizeof(kIdTiers) / sizeof(uint64_t) - 1;\n const int kIdCollisionDelayMicros = 10;\n const int kMaxIdCollisions = 21;  // sum(2**i*10s for i in range(21))~=21s\n const int64_t kAbsent = 0LL;\n@@ -92,16 +92,16 @@ const int64_t kPreallocateRows = 1000;\n // hundreds of megs but doesn't need the transaction to maintain its\n // invariants. This ensures the WAL read penalty is small and might\n // allow writers in other processes a chance to schedule.\n-const uint64 kFlushBytes = 1024 * 1024;\n+const uint64_t kFlushBytes = 1024 * 1024;\n \n-double DoubleTime(uint64 micros) {\n+double DoubleTime(uint64_t micros) {\n   // TODO(@jart): Follow precise definitions for time laid out in schema.\n   // TODO(@jart): Use monotonic clock from gRPC codebase.\n   return static_cast<double>(micros) / 1.0e6;\n }\n \n-string StringifyShape(const TensorShape& shape) {\n-  string result;\n+std::string StringifyShape(const TensorShape& shape) {\n+  std::string result;\n   bool first = true;\n   for (const auto& dim : shape) {\n     if (first) {\n@@ -233,7 +233,7 @@ class IdAllocator {\n class GraphWriter {\n  public:\n   static absl::Status Save(Sqlite* db, SqliteTransaction* txn, IdAllocator* ids,\n-                           GraphDef* graph, uint64 now, int64_t run_id,\n+                           GraphDef* graph, uint64_t now, int64_t run_id,\n                            int64_t* graph_id)\n       SQLITE_EXCLUSIVE_TRANSACTIONS_REQUIRED(*db) {\n     TF_RETURN_IF_ERROR(ids->CreateNewId(graph_id));\n@@ -246,7 +246,7 @@ class GraphWriter {\n   }\n \n  private:\n-  GraphWriter(Sqlite* db, SqliteTransaction* txn, GraphDef* graph, uint64 now,\n+  GraphWriter(Sqlite* db, SqliteTransaction* txn, GraphDef* graph, uint64_t now,\n               int64_t graph_id)\n       : db_(db), txn_(txn), graph_(graph), now_(now), graph_id_(graph_id) {}\n \n@@ -338,7 +338,7 @@ class GraphWriter {\n       node->clear_op();\n       node->clear_device();\n       node->clear_input();\n-      string node_def;\n+      std::string node_def;\n       if (node->SerializeToString(&node_def)) {\n         insert.BindBlobUnsafe(6, node_def);\n       }\n@@ -364,7 +364,7 @@ class GraphWriter {\n     insert.BindInt(2, graph_id_);\n     insert.BindDouble(3, DoubleTime(now_));\n     graph_->clear_node();\n-    string graph_def;\n+    std::string graph_def;\n     if (graph_->SerializeToString(&graph_def)) {\n       insert.BindBlobUnsafe(4, graph_def);\n     }\n@@ -382,11 +382,11 @@ class GraphWriter {\n \n   Sqlite* const db_;\n   SqliteTransaction* const txn_;\n-  uint64 unflushed_bytes_ = 0;\n+  uint64_t unflushed_bytes_ = 0;\n   GraphDef* const graph_;\n-  const uint64 now_;\n+  const uint64_t now_;\n   const int64_t graph_id_;\n-  std::vector<string> name_copies_;\n+  std::vector<std::string> name_copies_;\n   std::unordered_map<absl::string_view, int64_t, StringPieceHasher>\n       name_to_node_id_;\n \n@@ -403,25 +403,25 @@ class GraphWriter {\n /// This class is thread safe.\n class RunMetadata {\n  public:\n-  RunMetadata(IdAllocator* ids, const string& experiment_name,\n-              const string& run_name, const string& user_name)\n+  RunMetadata(IdAllocator* ids, const std::string& experiment_name,\n+              const std::string& run_name, const std::string& user_name)\n       : ids_{ids},\n         experiment_name_{experiment_name},\n         run_name_{run_name},\n         user_name_{user_name} {\n     DCHECK(ids_ != nullptr);\n   }\n \n-  const string& experiment_name() { return experiment_name_; }\n-  const string& run_name() { return run_name_; }\n-  const string& user_name() { return user_name_; }\n+  const std::string& experiment_name() { return experiment_name_; }\n+  const std::string& run_name() { return run_name_; }\n+  const std::string& user_name() { return user_name_; }\n \n   int64_t run_id() TF_LOCKS_EXCLUDED(mu_) {\n     mutex_lock lock(mu_);\n     return run_id_;\n   }\n \n-  absl::Status SetGraph(Sqlite* db, uint64 now, double computed_time,\n+  absl::Status SetGraph(Sqlite* db, uint64_t now, double computed_time,\n                         std::unique_ptr<GraphDef> g)\n       SQLITE_TRANSACTIONS_EXCLUDED(*db) TF_LOCKS_EXCLUDED(mu_) {\n     int64_t run_id;\n@@ -437,8 +437,8 @@ class RunMetadata {\n     return txn.Commit();\n   }\n \n-  absl::Status GetTagId(Sqlite* db, uint64 now, double computed_time,\n-                        const string& tag_name, int64_t* tag_id,\n+  absl::Status GetTagId(Sqlite* db, uint64_t now, double computed_time,\n+                        const std::string& tag_name, int64_t* tag_id,\n                         const SummaryMetadata& metadata)\n       TF_LOCKS_EXCLUDED(mu_) {\n     mutex_lock lock(mu_);\n@@ -484,7 +484,7 @@ class RunMetadata {\n   }\n \n  private:\n-  absl::Status InitializeUser(Sqlite* db, uint64 now)\n+  absl::Status InitializeUser(Sqlite* db, uint64_t now)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (user_id_ != kAbsent || user_name_.empty()) return absl::OkStatus();\n     const char* get_sql = R\"sql(\n@@ -516,7 +516,7 @@ class RunMetadata {\n     return absl::OkStatus();\n   }\n \n-  absl::Status InitializeExperiment(Sqlite* db, uint64 now,\n+  absl::Status InitializeExperiment(Sqlite* db, uint64_t now,\n                                     double computed_time)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (experiment_name_.empty()) return absl::OkStatus();\n@@ -584,7 +584,7 @@ class RunMetadata {\n     return absl::OkStatus();\n   }\n \n-  absl::Status InitializeRun(Sqlite* db, uint64 now, double computed_time)\n+  absl::Status InitializeRun(Sqlite* db, uint64_t now, double computed_time)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (run_name_.empty()) return absl::OkStatus();\n     TF_RETURN_IF_ERROR(InitializeExperiment(db, now, computed_time));\n@@ -630,15 +630,15 @@ class RunMetadata {\n \n   mutex mu_;\n   IdAllocator* const ids_;\n-  const string experiment_name_;\n-  const string run_name_;\n-  const string user_name_;\n+  const std::string experiment_name_;\n+  const std::string run_name_;\n+  const std::string user_name_;\n   int64_t experiment_id_ TF_GUARDED_BY(mu_) = kAbsent;\n   int64_t run_id_ TF_GUARDED_BY(mu_) = kAbsent;\n   int64_t user_id_ TF_GUARDED_BY(mu_) = kAbsent;\n   double experiment_started_time_ TF_GUARDED_BY(mu_) = 0.0;\n   double run_started_time_ TF_GUARDED_BY(mu_) = 0.0;\n-  std::unordered_map<string, int64_t> tag_ids_ TF_GUARDED_BY(mu_);\n+  std::unordered_map<std::string, int64_t> tag_ids_ TF_GUARDED_BY(mu_);\n \n   RunMetadata(const RunMetadata&) = delete;\n   void operator=(const RunMetadata&) = delete;\n@@ -654,7 +654,7 @@ class SeriesWriter {\n     DCHECK(series_ > 0);\n   }\n \n-  absl::Status Append(Sqlite* db, int64_t step, uint64 now,\n+  absl::Status Append(Sqlite* db, int64_t step, uint64_t now,\n                       double computed_time, const Tensor& t)\n       SQLITE_TRANSACTIONS_EXCLUDED(*db) TF_LOCKS_EXCLUDED(mu_) {\n     mutex_lock lock(mu_);\n@@ -837,9 +837,9 @@ class SeriesWriter {\n   mutex mu_;\n   const int64_t series_;\n   RunMetadata* const meta_;\n-  uint64 count_ TF_GUARDED_BY(mu_) = 0;\n+  uint64_t count_ TF_GUARDED_BY(mu_) = 0;\n   std::deque<int64_t> rowids_ TF_GUARDED_BY(mu_);\n-  uint64 unflushed_bytes_ TF_GUARDED_BY(mu_) = 0;\n+  uint64_t unflushed_bytes_ TF_GUARDED_BY(mu_) = 0;\n \n   SeriesWriter(const SeriesWriter&) = delete;\n   void operator=(const SeriesWriter&) = delete;\n@@ -856,7 +856,7 @@ class RunWriter {\n  public:\n   explicit RunWriter(RunMetadata* meta) : meta_{meta} {}\n \n-  absl::Status Append(Sqlite* db, int64_t tag_id, int64_t step, uint64 now,\n+  absl::Status Append(Sqlite* db, int64_t tag_id, int64_t step, uint64_t now,\n                       double computed_time, const Tensor& t)\n       SQLITE_TRANSACTIONS_EXCLUDED(*db) TF_LOCKS_EXCLUDED(mu_) {\n     SeriesWriter* writer = GetSeriesWriter(tag_id);\n@@ -903,8 +903,8 @@ class RunWriter {\n /// This class is thread safe.\n class SummaryDbWriter : public SummaryWriterInterface {\n  public:\n-  SummaryDbWriter(Env* env, Sqlite* db, const string& experiment_name,\n-                  const string& run_name, const string& user_name)\n+  SummaryDbWriter(Env* env, Sqlite* db, const std::string& experiment_name,\n+                  const std::string& run_name, const std::string& user_name)\n       : SummaryWriterInterface(),\n         env_{env},\n         db_{db},\n@@ -941,8 +941,9 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n   absl::Status Flush() override { return absl::OkStatus(); }\n \n-  absl::Status WriteTensor(int64_t global_step, Tensor t, const string& tag,\n-                           const string& serialized_metadata) override {\n+  absl::Status WriteTensor(int64_t global_step, Tensor t,\n+                           const std::string& tag,\n+                           const std::string& serialized_metadata) override {\n     TF_RETURN_IF_ERROR(CheckSupportedType(t));\n     SummaryMetadata metadata;\n     if (!metadata.ParseFromString(serialized_metadata)) {\n@@ -952,7 +953,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteScalar(int64_t global_step, Tensor t,\n-                           const string& tag) override {\n+                           const std::string& tag) override {\n     TF_RETURN_IF_ERROR(CheckSupportedType(t));\n     SummaryMetadata metadata;\n     PatchPluginName(&metadata, kScalarPluginName);\n@@ -961,7 +962,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n   absl::Status WriteGraph(int64_t global_step,\n                           std::unique_ptr<GraphDef> g) override {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     return meta_.SetGraph(db_, now, DoubleTime(now), std::move(g));\n   }\n \n@@ -970,8 +971,8 @@ class SummaryDbWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteHistogram(int64_t global_step, Tensor t,\n-                              const string& tag) override {\n-    uint64 now = env_->NowMicros();\n+                              const std::string& tag) override {\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(DoubleTime(now));\n@@ -980,9 +981,9 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return MigrateEvent(std::move(e));\n   }\n \n-  absl::Status WriteImage(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteImage(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_images, Tensor bad_color) override {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(DoubleTime(now));\n@@ -991,9 +992,9 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return MigrateEvent(std::move(e));\n   }\n \n-  absl::Status WriteAudio(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteAudio(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_outputs, float sample_rate) override {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(DoubleTime(now));\n@@ -1002,12 +1003,12 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return MigrateEvent(std::move(e));\n   }\n \n-  string DebugString() const override { return \"SummaryDbWriter\"; }\n+  std::string DebugString() const override { return \"SummaryDbWriter\"; }\n \n  private:\n-  absl::Status Write(int64_t step, const Tensor& t, const string& tag,\n+  absl::Status Write(int64_t step, const Tensor& t, const std::string& tag,\n                      const SummaryMetadata& metadata) {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     double computed_time = DoubleTime(now);\n     int64_t tag_id;\n     TF_RETURN_IF_ERROR(\n@@ -1022,7 +1023,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n   absl::Status MigrateEvent(std::unique_ptr<Event> e) {\n     switch (e->what_case()) {\n       case Event::WhatCase::kSummary: {\n-        uint64 now = env_->NowMicros();\n+        uint64_t now = env_->NowMicros();\n         auto summaries = e->mutable_summary();\n         for (int i = 0; i < summaries->value_size(); ++i) {\n           Summary::Value* value = summaries->mutable_value(i);\n@@ -1046,16 +1047,16 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return absl::OkStatus();\n   }\n \n-  absl::Status MigrateGraph(const Event* e, const string& graph_def) {\n-    uint64 now = env_->NowMicros();\n+  absl::Status MigrateGraph(const Event* e, const std::string& graph_def) {\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<GraphDef> graph{new GraphDef};\n     if (!ParseProtoUnlimited(graph.get(), graph_def)) {\n       return errors::InvalidArgument(\"bad proto\");\n     }\n     return meta_.SetGraph(db_, now, e->wall_time(), std::move(graph));\n   }\n \n-  absl::Status MigrateSummary(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateSummary(const Event* e, Summary::Value* s, uint64_t now) {\n     switch (s->value_case()) {\n       case Summary::Value::ValueCase::kTensor:\n         TF_RETURN_WITH_CONTEXT_IF_ERROR(MigrateTensor(e, s, now), \"tensor\");\n@@ -1078,7 +1079,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return absl::OkStatus();\n   }\n \n-  absl::Status MigrateTensor(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateTensor(const Event* e, Summary::Value* s, uint64_t now) {\n     Tensor t;\n     if (!t.FromProto(s->tensor())) return errors::InvalidArgument(\"bad proto\");\n     TF_RETURN_IF_ERROR(CheckSupportedType(t));\n@@ -1090,7 +1091,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n   // TODO(jart): Refactor Summary -> Tensor logic into separate file.\n \n-  absl::Status MigrateScalar(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateScalar(const Event* e, Summary::Value* s, uint64_t now) {\n     // See tensorboard/plugins/scalar/summary.py and data_compat.py\n     Tensor t{DT_FLOAT, {}};\n     t.scalar<float>()() = s->simple_value();\n@@ -1101,7 +1102,8 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return run_.Append(db_, tag_id, e->step(), now, e->wall_time(), t);\n   }\n \n-  absl::Status MigrateHistogram(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateHistogram(const Event* e, Summary::Value* s,\n+                                uint64_t now) {\n     const HistogramProto& histo = s->histo();\n     int k = histo.bucket_size();\n     if (k != histo.bucket_limit_size()) {\n@@ -1132,7 +1134,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return run_.Append(db_, tag_id, e->step(), now, e->wall_time(), t);\n   }\n \n-  absl::Status MigrateImage(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateImage(const Event* e, Summary::Value* s, uint64_t now) {\n     // See tensorboard/plugins/image/summary.py and data_compat.py\n     Tensor t{DT_STRING, {3}};\n     auto img = s->mutable_image();\n@@ -1146,7 +1148,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return run_.Append(db_, tag_id, e->step(), now, e->wall_time(), t);\n   }\n \n-  absl::Status MigrateAudio(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateAudio(const Event* e, Summary::Value* s, uint64_t now) {\n     // See tensorboard/plugins/audio/summary.py and data_compat.py\n     Tensor t{DT_STRING, {1, 2}};\n     auto wav = s->mutable_audio();\n@@ -1168,9 +1170,10 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n }  // namespace\n \n-absl::Status CreateSummaryDbWriter(Sqlite* db, const string& experiment_name,\n-                                   const string& run_name,\n-                                   const string& user_name, Env* env,\n+absl::Status CreateSummaryDbWriter(Sqlite* db,\n+                                   const std::string& experiment_name,\n+                                   const std::string& run_name,\n+                                   const std::string& user_name, Env* env,\n                                    SummaryWriterInterface** result) {\n   *result = new SummaryDbWriter(env, db, experiment_name, run_name, user_name);\n   return absl::OkStatus();"
        },
        {
            "sha": "05900fe8ce0ca6747ee7f5975d7683ec578b37cd",
            "filename": "tensorflow/core/summary/summary_db_writer.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -34,9 +34,10 @@ namespace tensorflow {\n /// the future if support for other DBs is added to core.\n ///\n /// The result holds a new reference to db.\n-absl::Status CreateSummaryDbWriter(Sqlite* db, const string& experiment_name,\n-                                   const string& run_name,\n-                                   const string& user_name, Env* env,\n+absl::Status CreateSummaryDbWriter(Sqlite* db,\n+                                   const std::string& experiment_name,\n+                                   const std::string& run_name,\n+                                   const std::string& user_name, Env* env,\n                                    SummaryWriterInterface** result);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "8c25da1823f0576d47db58976c5eea03c817a695",
            "filename": "tensorflow/core/summary/summary_db_writer_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -47,12 +47,12 @@ Tensor MakeScalarInt64(int64_t x) {\n class FakeClockEnv : public EnvWrapper {\n  public:\n   FakeClockEnv() : EnvWrapper(Env::Default()), current_millis_(0) {}\n-  void AdvanceByMillis(const uint64 millis) { current_millis_ += millis; }\n-  uint64 NowMicros() const override { return current_millis_ * 1000; }\n-  uint64 NowSeconds() const override { return current_millis_ * 1000; }\n+  void AdvanceByMillis(const uint64_t millis) { current_millis_ += millis; }\n+  uint64_t NowMicros() const override { return current_millis_ * 1000; }\n+  uint64_t NowSeconds() const override { return current_millis_ * 1000; }\n \n  private:\n-  uint64 current_millis_;\n+  uint64_t current_millis_;\n };\n \n class SummaryDbWriterTest : public ::testing::Test {\n@@ -71,7 +71,7 @@ class SummaryDbWriterTest : public ::testing::Test {\n     db_ = nullptr;\n   }\n \n-  int64_t QueryInt(const string& sql) {\n+  int64_t QueryInt(const std::string& sql) {\n     SqliteStatement stmt = db_->PrepareOrDie(sql);\n     bool is_done;\n     absl::Status s = stmt.Step(&is_done);\n@@ -82,7 +82,7 @@ class SummaryDbWriterTest : public ::testing::Test {\n     return stmt.ColumnInt(0);\n   }\n \n-  double QueryDouble(const string& sql) {\n+  double QueryDouble(const std::string& sql) {\n     SqliteStatement stmt = db_->PrepareOrDie(sql);\n     bool is_done;\n     absl::Status s = stmt.Step(&is_done);\n@@ -93,7 +93,7 @@ class SummaryDbWriterTest : public ::testing::Test {\n     return stmt.ColumnDouble(0);\n   }\n \n-  string QueryString(const string& sql) {\n+  std::string QueryString(const std::string& sql) {\n     SqliteStatement stmt = db_->PrepareOrDie(sql);\n     bool is_done;\n     absl::Status s = stmt.Step(&is_done);\n@@ -142,7 +142,7 @@ TEST_F(SummaryDbWriterTest, WriteHistogram_VerifyTensorValues) {\n \n   // TODO(nickfelt): implement QueryTensor() to encapsulate this\n   // Verify the data\n-  string result = QueryString(\"SELECT data FROM Tensors\");\n+  std::string result = QueryString(\"SELECT data FROM Tensors\");\n   const double* val = reinterpret_cast<const double*>(result.data());\n   double histarray[] = {std::numeric_limits<double>::min(),\n                         -30.5,"
        },
        {
            "sha": "dfb1bba4aecbe59c6055e18a9487c83956efaf80",
            "filename": "tensorflow/core/summary/summary_file_writer.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -47,7 +47,8 @@ class SummaryFileWriter : public SummaryWriterInterface {\n         flush_millis_(flush_millis),\n         env_(env) {}\n \n-  absl::Status Initialize(const string& logdir, const string& filename_suffix) {\n+  absl::Status Initialize(const std::string& logdir,\n+                          const std::string& filename_suffix) {\n     const absl::Status is_dir = env_->IsDirectory(logdir);\n     if (!is_dir.ok()) {\n       if (is_dir.code() != tensorflow::error::NOT_FOUND) {\n@@ -60,8 +61,8 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     int32_t pid = env_->GetProcessId();\n     static std::atomic<int64_t> file_id_counter(0);\n     // Precede filename_suffix with \".\" if it doesn't already start with one.\n-    string sep = absl::StartsWith(filename_suffix, \".\") ? \"\" : \".\";\n-    const string uniquified_filename_suffix = absl::StrCat(\n+    std::string sep = absl::StartsWith(filename_suffix, \".\") ? \"\" : \".\";\n+    const std::string uniquified_filename_suffix = absl::StrCat(\n         \".\", pid, \".\", file_id_counter.fetch_add(1), sep, filename_suffix);\n     mutex_lock ml(mu_);\n     events_writer_ =\n@@ -86,8 +87,9 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     (void)Flush();  // Ignore errors.\n   }\n \n-  absl::Status WriteTensor(int64_t global_step, Tensor t, const string& tag,\n-                           const string& serialized_metadata) override {\n+  absl::Status WriteTensor(int64_t global_step, Tensor t,\n+                           const std::string& tag,\n+                           const std::string& serialized_metadata) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(GetWallTime());\n@@ -110,7 +112,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteScalar(int64_t global_step, Tensor t,\n-                           const string& tag) override {\n+                           const std::string& tag) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(GetWallTime());\n@@ -120,7 +122,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteHistogram(int64_t global_step, Tensor t,\n-                              const string& tag) override {\n+                              const std::string& tag) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(GetWallTime());\n@@ -129,7 +131,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     return WriteEvent(std::move(e));\n   }\n \n-  absl::Status WriteImage(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteImage(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_images, Tensor bad_color) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n@@ -139,7 +141,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     return WriteEvent(std::move(e));\n   }\n \n-  absl::Status WriteAudio(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteAudio(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_outputs, float sample_rate) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n@@ -168,7 +170,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     return absl::OkStatus();\n   }\n \n-  string DebugString() const override { return \"SummaryFileWriter\"; }\n+  std::string DebugString() const override { return \"SummaryFileWriter\"; }\n \n  private:\n   double GetWallTime() {\n@@ -189,21 +191,22 @@ class SummaryFileWriter : public SummaryWriterInterface {\n   bool is_initialized_;\n   const int max_queue_;\n   const int flush_millis_;\n-  uint64 last_flush_;\n+  uint64_t last_flush_;\n   Env* env_;\n   mutex mu_;\n   std::vector<std::unique_ptr<Event>> queue_ TF_GUARDED_BY(mu_);\n   // A pointer to allow deferred construction.\n   std::unique_ptr<EventsWriter> events_writer_ TF_GUARDED_BY(mu_);\n-  std::vector<std::pair<string, SummaryMetadata>> registered_summaries_\n+  std::vector<std::pair<std::string, SummaryMetadata>> registered_summaries_\n       TF_GUARDED_BY(mu_);\n };\n \n }  // namespace\n \n absl::Status CreateSummaryFileWriter(int max_queue, int flush_millis,\n-                                     const string& logdir,\n-                                     const string& filename_suffix, Env* env,\n+                                     const std::string& logdir,\n+                                     const std::string& filename_suffix,\n+                                     Env* env,\n                                      SummaryWriterInterface** result) {\n   SummaryFileWriter* w = new SummaryFileWriter(max_queue, flush_millis, env);\n   const absl::Status s = w->Initialize(logdir, filename_suffix);"
        },
        {
            "sha": "a3ba40bf8a4db38631f9d08e3d58d224f9dd9754",
            "filename": "tensorflow/core/summary/summary_file_writer.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -35,9 +35,9 @@ namespace tensorflow {\n /// returned status is ok. The Env object must not be destroyed until\n /// after the returned writer.\n absl::Status CreateSummaryFileWriter(int max_queue, int flush_millis,\n-                                     const string& logdir,\n-                                     const string& filename_suffix, Env* env,\n-                                     SummaryWriterInterface** result);\n+                                     const std::string& logdir,\n+                                     const std::string& filename_suffix,\n+                                     Env* env, SummaryWriterInterface** result);\n \n }  // namespace tensorflow\n "
        },
        {
            "sha": "94ca029774f40d52a9b0c518e9113318423841d1",
            "filename": "tensorflow/core/summary/summary_file_writer_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -43,21 +43,21 @@ namespace {\n class FakeClockEnv : public EnvWrapper {\n  public:\n   FakeClockEnv() : EnvWrapper(Env::Default()), current_millis_(0) {}\n-  void AdvanceByMillis(const uint64 millis) { current_millis_ += millis; }\n-  uint64 NowMicros() const override { return current_millis_ * 1000; }\n-  uint64 NowSeconds() const override { return current_millis_ * 1000; }\n+  void AdvanceByMillis(const uint64_t millis) { current_millis_ += millis; }\n+  uint64_t NowMicros() const override { return current_millis_ * 1000; }\n+  uint64_t NowSeconds() const override { return current_millis_ * 1000; }\n \n  private:\n-  uint64 current_millis_;\n+  uint64_t current_millis_;\n };\n \n class SummaryFileWriterTest : public ::testing::Test {\n  protected:\n   absl::Status SummaryTestHelper(\n-      const string& test_name,\n+      const std::string& test_name,\n       const std::function<absl::Status(SummaryWriterInterface*)>& writer_fn,\n       const std::function<void(const Event&)>& test_fn) {\n-    static std::set<string>* tests = new std::set<string>();\n+    static std::set<std::string>* tests = new std::set<std::string>();\n     CHECK(tests->insert(test_name).second) << \": \" << test_name;\n \n     SummaryWriterInterface* writer;\n@@ -68,10 +68,10 @@ class SummaryFileWriterTest : public ::testing::Test {\n     TF_CHECK_OK(writer_fn(writer));\n     TF_CHECK_OK(writer->Flush());\n \n-    std::vector<string> files;\n+    std::vector<std::string> files;\n     TF_CHECK_OK(env_.GetChildren(testing::TmpDir(), &files));\n     bool found = false;\n-    for (const string& f : files) {\n+    for (const std::string& f : files) {\n       if (absl::StrContains(f, test_name)) {\n         if (found) {\n           return errors::Unknown(\"Found more than one file for \", test_name);\n@@ -82,7 +82,7 @@ class SummaryFileWriterTest : public ::testing::Test {\n                                              &read_file));\n         io::RecordReader reader(read_file.get(), io::RecordReaderOptions());\n         tstring record;\n-        uint64 offset = 0;\n+        uint64_t offset = 0;\n         TF_CHECK_OK(\n             reader.ReadRecord(&offset,\n                               &record));  // The first event is irrelevant\n@@ -179,7 +179,7 @@ namespace {\n template <typename T>\n static absl::Status CreateImage(SummaryWriterInterface* writer) {\n   Tensor bad_color(DT_UINT8, TensorShape({1}));\n-  bad_color.scalar<uint8>()() = 0;\n+  bad_color.scalar<uint8_t>()() = 0;\n   Tensor one(DataTypeToEnum<T>::v(), TensorShape({1, 1, 1, 1}));\n   one.scalar<T>()() = T(1);\n   TF_RETURN_IF_ERROR(writer->WriteImage(2, one, \"name\", 1, bad_color));\n@@ -202,7 +202,7 @@ static void CheckImage(const Event& e) {\n \n TEST_F(SummaryFileWriterTest, WriteImageUInt8) {\n   TF_CHECK_OK(\n-      SummaryTestHelper(\"image_test_uint8\", CreateImage<uint8>, CheckImage));\n+      SummaryTestHelper(\"image_test_uint8\", CreateImage<uint8_t>, CheckImage));\n }\n \n TEST_F(SummaryFileWriterTest, WriteImageFloat) {\n@@ -272,19 +272,19 @@ TEST_F(SummaryFileWriterTest, WallTime) {\n \n TEST_F(SummaryFileWriterTest, AvoidFilenameCollision) {\n   // Keep unique with all other test names in this file.\n-  string test_name = \"avoid_filename_collision_test\";\n+  std::string test_name = \"avoid_filename_collision_test\";\n   int num_files = 10;\n   for (int i = 0; i < num_files; i++) {\n     SummaryWriterInterface* writer;\n     TF_CHECK_OK(CreateSummaryFileWriter(1, 1, testing::TmpDir(), test_name,\n                                         &env_, &writer));\n     core::ScopedUnref deleter(writer);\n   }\n-  std::vector<string> files;\n+  std::vector<std::string> files;\n   TF_CHECK_OK(env_.GetChildren(testing::TmpDir(), &files));\n   // Filter `files` down to just those generated in this test.\n   files.erase(std::remove_if(files.begin(), files.end(),\n-                             [test_name](string f) {\n+                             [test_name](std::string f) {\n                                return !absl::StrContains(f, test_name);\n                              }),\n               files.end());"
        },
        {
            "sha": "29c459cca89f13b0e67e6b69dcde74cfb6fc2f69",
            "filename": "tensorflow/core/summary/vacuum.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fvacuum.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Fsummary%2Fvacuum.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fvacuum.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -110,7 +110,7 @@ void Vacuum(const char* path) {\n }\n \n int main(int argc, char* argv[]) {\n-  string usage = Flags::Usage(argv[0], {});\n+  std::string usage = Flags::Usage(argv[0], {});\n   bool parse_result = Flags::Parse(&argc, argv, {});\n   if (!parse_result) {\n     std::cerr << \"The vacuum tool rebuilds SQLite database files created by\\n\""
        },
        {
            "sha": "ef8e371457400ba8a234dc5c27c598e2132d72e9",
            "filename": "tensorflow/core/tfrt/saved_model/saved_model_aot_compile.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model_aot_compile.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model_aot_compile.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model_aot_compile.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -308,15 +308,15 @@ AotCompileToGpuPjRtExecutable(\n       may_alias_resource_update, &options, compilation_result));\n \n   TF_ASSIGN_OR_RETURN(\n-      xla::Compiler::TargetConfig gpu_config,\n-      xla::Compiler::TargetConfig::FromProto(gpu_target_config));\n+      xla::Compiler::GpuTargetConfig gpu_config,\n+      xla::Compiler::GpuTargetConfig::FromProto(gpu_target_config));\n   xla::StreamExecutorGpuCompiler pjrt_gpu_compiler;\n   // Create a trivial topology, which won't be used.\n   xla::StreamExecutorGpuTopologyDescription topology(xla::CudaId(),\n                                                      xla::CudaName(), nullptr);\n   xla::CompileOptions pjrt_options =\n       GetPjRtCompileOptions(options, **compilation_result);\n-  pjrt_options.target_config = gpu_config;\n+  pjrt_options.gpu_target_config = gpu_config;\n   return pjrt_gpu_compiler.Compile(\n       pjrt_options, *((*compilation_result)->computation), topology, nullptr);\n }"
        },
        {
            "sha": "eb7913a44deb800d1e2f1f02ccb4b01dc79c2078",
            "filename": "tensorflow/core/tfrt/tfrt_session/tfrt_session.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Ftfrt%2Ftfrt_session%2Ftfrt_session.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fcore%2Ftfrt%2Ftfrt_session%2Ftfrt_session.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Ftfrt_session%2Ftfrt_session.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -106,7 +106,8 @@ class TfrtSessionFactory : public tensorflow::SessionFactory {\n   bool use_gpu_ TF_GUARDED_BY(mutex_) = false;\n   std::unique_ptr<ThreadPoolManager> thread_pool_manager_ TF_GUARDED_BY(mutex_);\n   bool enable_mlrt_ TF_GUARDED_BY(mutex_) = false;\n-  tensorflow::BackendCompiler* backend_compiler_ TF_GUARDED_BY(mutex_);\n+  tensorflow::BackendCompiler* backend_compiler_ TF_GUARDED_BY(mutex_) =\n+      nullptr;\n   std::unique_ptr<StaticDeviceMgr> device_manager_;\n };\n "
        },
        {
            "sha": "09e4db2bc04bfa5f2ae48e8556262b7176957327",
            "filename": "tensorflow/examples/wav_to_spectrogram/main.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fmain.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fmain.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fmain.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n \n #include <cstdint>\n+#include <string>\n #include <vector>\n \n #include \"absl/log/log.h\""
        },
        {
            "sha": "6536ef720e58ea82929eacf8876fc202a4af22ab",
            "filename": "tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fwav_to_spectrogram.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fwav_to_spectrogram.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fwav_to_spectrogram.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n+#include <string>\n #include <vector>\n \n #include \"absl/status/status.h\""
        },
        {
            "sha": "019741f49a93f6c17d7f12cb42b428ef65eb6c77",
            "filename": "tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fwav_to_spectrogram_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fwav_to_spectrogram_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fexamples%2Fwav_to_spectrogram%2Fwav_to_spectrogram_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -15,6 +15,8 @@ limitations under the License.\n \n #include \"tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram.h\"\n \n+#include <string>\n+\n #include \"tensorflow/core/lib/core/status_test_util.h\"\n #include \"tensorflow/core/lib/io/path.h\"\n #include \"tensorflow/core/lib/wav/wav_io.h\""
        },
        {
            "sha": "fd1cffa0d561cf8641f25837c6b3f5c2e7299d2f",
            "filename": "tensorflow/lite/core/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -342,6 +342,8 @@ cc_test(\n     srcs = [\"signature_runner_test.cc\"],\n     data = [\n         \"//tensorflow/lite:testdata/multi_signatures.bin\",\n+        \"//tensorflow/lite:testdata/no_signatures.bin\",\n+        \"//tensorflow/lite:testdata/no_signatures_no_tensor_names.bin\",\n         \"//tensorflow/lite:testdata/reverse_signature_model.bin\",\n     ],\n     deps = ["
        },
        {
            "sha": "c7d3ebb4c0801f8592f47c1952c6402526010766",
            "filename": "tensorflow/lite/core/interpreter.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2Finterpreter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2Finterpreter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Finterpreter.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -568,10 +568,18 @@ Interpreter::CreatePlaceholderSignatureDef() {\n   auto placeholder_signature_def = std::make_unique<internal::SignatureDef>();\n   for (auto i = 0; i < inputs().size(); ++i) {\n     auto* name = GetInputName(i);\n+    if (*name == 0) {\n+      placeholder_input_names_.push_back(\"input\" + std::to_string(i));\n+      name = placeholder_input_names_.back().c_str();\n+    }\n     placeholder_signature_def->inputs[name] = inputs()[i];\n   }\n   for (auto i = 0; i < outputs().size(); ++i) {\n     auto* name = GetOutputName(i);\n+    if (*name == 0) {\n+      placeholder_output_names_.push_back(\"output\" + std::to_string(i));\n+      name = placeholder_output_names_.back().c_str();\n+    }\n     placeholder_signature_def->outputs[name] = outputs()[i];\n   }\n   placeholder_signature_def->signature_key = kPlaceholderSignatureDefKey;"
        },
        {
            "sha": "8fef5727a011968e500dc8ff5dce7f37e344ce3c",
            "filename": "tensorflow/lite/core/interpreter.h",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2Finterpreter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2Finterpreter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Finterpreter.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1069,6 +1069,14 @@ class Interpreter {\n   static constexpr char kPlaceholderSignatureDefKey[] =\n       \"<placeholder signature>\";\n \n+  // Placeholder input names to use when the model has with no signatures & no\n+  // tensor names.\n+  std::vector<std::string> placeholder_input_names_;\n+\n+  // Placeholder output names to use when the model has with no signatures & no\n+  // tensor names.\n+  std::vector<std::string> placeholder_output_names_;\n+\n   // Placeholder SignatureDef for legacy models with no signatures.\n   std::unique_ptr<internal::SignatureDef> placeholder_signature_def_;\n "
        },
        {
            "sha": "d18affa28a037297f0f983280d6157dcb5cfb5fe",
            "filename": "tensorflow/lite/core/signature_runner_test.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2Fsignature_runner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Fcore%2Fsignature_runner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Fsignature_runner_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -134,6 +134,65 @@ TEST(SignatureRunnerTest, ReverseSignatureModel) {\n   EXPECT_STREQ(subgraph_output_names[1], \"prod\");\n }\n \n+TEST(SignatureRunnerTest, TestPlaceholderSignatures) {\n+  TestErrorReporter reporter;\n+  auto model = FlatBufferModel::BuildFromFile(\n+      \"tensorflow/lite/testdata/no_signatures.bin\", &reporter);\n+  ASSERT_TRUE(model);\n+  ops::builtin::BuiltinOpResolver resolver;\n+  InterpreterBuilder builder(*model, resolver);\n+\n+  std::unique_ptr<Interpreter> interpreter;\n+  ASSERT_EQ(builder(&interpreter), kTfLiteOk);\n+  ASSERT_NE(interpreter, nullptr);\n+\n+  std::vector<const std::string*> signature_defs =\n+      interpreter->signature_keys();\n+  ASSERT_EQ(signature_defs.size(), 0);\n+\n+  SignatureRunner* default_runner =\n+      interpreter->GetSignatureRunner(/*signature_key=*/nullptr);\n+  ASSERT_NE(default_runner, nullptr);\n+  EXPECT_EQ(default_runner->signature_key(), \"<placeholder signature>\");\n+  const std::vector<const char*>& input_names = default_runner->input_names();\n+  const std::vector<const char*>& output_names = default_runner->output_names();\n+  ASSERT_EQ(input_names.size(), 2);\n+  EXPECT_EQ(std::string(input_names[0]), \"x1\");\n+  EXPECT_EQ(std::string(input_names[1]), \"x2\");\n+  ASSERT_EQ(output_names.size(), 1);\n+  EXPECT_EQ(std::string(output_names[0]), \"Identity\");\n+}\n+\n+TEST(SignatureRunnerTest, TestPlaceholderSignaturesDefaultNames) {\n+  TestErrorReporter reporter;\n+  auto model = FlatBufferModel::BuildFromFile(\n+      \"tensorflow/lite/testdata/no_signatures_no_tensor_names.bin\",\n+      &reporter);\n+  ASSERT_TRUE(model);\n+  ops::builtin::BuiltinOpResolver resolver;\n+  InterpreterBuilder builder(*model, resolver);\n+\n+  std::unique_ptr<Interpreter> interpreter;\n+  ASSERT_EQ(builder(&interpreter), kTfLiteOk);\n+  ASSERT_NE(interpreter, nullptr);\n+\n+  std::vector<const std::string*> signature_defs =\n+      interpreter->signature_keys();\n+  ASSERT_EQ(signature_defs.size(), 0);\n+\n+  SignatureRunner* default_runner =\n+      interpreter->GetSignatureRunner(/*signature_key=*/nullptr);\n+  ASSERT_NE(default_runner, nullptr);\n+  EXPECT_EQ(default_runner->signature_key(), \"<placeholder signature>\");\n+  const std::vector<const char*>& input_names = default_runner->input_names();\n+  const std::vector<const char*>& output_names = default_runner->output_names();\n+  ASSERT_EQ(input_names.size(), 2);\n+  EXPECT_EQ(std::string(input_names[0]), \"input0\");\n+  EXPECT_EQ(std::string(input_names[1]), \"input1\");\n+  ASSERT_EQ(output_names.size(), 1);\n+  EXPECT_EQ(std::string(output_names[0]), \"output0\");\n+}\n+\n }  // namespace\n }  // namespace impl\n }  // namespace tflite"
        },
        {
            "sha": "c5d285995a637cc7f063e82c1cf494801b516426",
            "filename": "tensorflow/lite/testdata/no_signatures_no_tensor_names.bin",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Ftestdata%2Fno_signatures_no_tensor_names.bin",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Ftestdata%2Fno_signatures_no_tensor_names.bin",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftestdata%2Fno_signatures_no_tensor_names.bin?ref=de857f9a71f934ca661a9d325dafb225869fc6b0"
        },
        {
            "sha": "f03ae364d385e7f198657ab77a5f897d109ab321",
            "filename": "tensorflow/lite/tools/cmake/modules/eigen.cmake",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Feigen.cmake",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Feigen.cmake",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Feigen.cmake?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -23,7 +23,7 @@ OverridableFetchContent_Declare(\n   eigen\n   GIT_REPOSITORY https://gitlab.com/libeigen/eigen.git\n   # Sync with tensorflow/third_party/eigen3/workspace.bzl\n-  GIT_TAG 70d8d99d0df9fd967b135efd8d12ed20fc48d007\n+  GIT_TAG dcbaf2d608f306450f1e74949eb87e9a22a7ef4b\n   # It's not currently (cmake 3.17) possible to shallow clone with a GIT TAG\n   # as cmake attempts to git checkout the commit hash after the clone\n   # which doesn't work as it's a shallow clone hence a different commit hash."
        },
        {
            "sha": "f863e7cbb3b6320dbd7c1f0d98acaab975694874",
            "filename": "tensorflow/lite/tools/cmake/modules/xnnpack.cmake",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Fxnnpack.cmake",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Fxnnpack.cmake",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Fxnnpack.cmake?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -23,7 +23,7 @@ OverridableFetchContent_Declare(\n   xnnpack\n   GIT_REPOSITORY https://github.com/google/XNNPACK\n   # Sync with tensorflow/workspace2.bzl\n-  GIT_TAG decc685b0ecfd00da5a2168eb03b0c795678f084\n+  GIT_TAG fa0fd6471a39a5d66a59d4cd8f8cc4a93a4bd470\n   GIT_PROGRESS TRUE\n   PREFIX \"${CMAKE_BINARY_DIR}\"\n   SOURCE_DIR \"${CMAKE_BINARY_DIR}/xnnpack\""
        },
        {
            "sha": "92d1e99849806df85b36fd6a869d4c33f6e4a8dc",
            "filename": "tensorflow/python/compat/compat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fcompat%2Fcompat.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fcompat%2Fcompat.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fcompat%2Fcompat.py?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -29,7 +29,7 @@\n # This value changes every day with an automatic CL. It can be modified in code\n # via `forward_compatibility_horizon()` or with the environment variable\n # TF_FORWARD_COMPATIBILITY_DELTA_DAYS, which is added to the compatibility date.\n-_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 11, 16)\n+_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 11, 21)\n _FORWARD_COMPATIBILITY_DELTA_DAYS_VAR_NAME = \"TF_FORWARD_COMPATIBILITY_DELTA_DAYS\"\n _FORWARD_COMPATIBILITY_DATE_NUMBER = None\n "
        },
        {
            "sha": "b42abafddaa7dc44570789d836415254407684c5",
            "filename": "tensorflow/python/framework/python_op_gen_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fframework%2Fpython_op_gen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fframework%2Fpython_op_gen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fframework%2Fpython_op_gen_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -530,7 +530,7 @@ TEST(PythonOpGen, GenerateMetadataWhenOpRegOffsetsIsPresent) {\n   int target_end = target_begin + 3;\n \n   std::vector<string> sp = absl::StrSplit(code, '\\n');\n-  string last_line = sp.back();\n+  std::string last_line = sp.back();\n   ASSERT_TRUE(absl::StrContains(last_line,\n                                 \"# kythe.proto.metadata.GeneratedCodeInfo:\"));\n   GeneratedCodeInfo gci = DecodeAnnotation(last_line);"
        },
        {
            "sha": "6da554f9882dd10db81b91f71cb909d702ed9d26",
            "filename": "tensorflow/python/lib/io/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Flib%2Fio%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Flib%2Fio%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Flib%2Fio%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -36,6 +36,7 @@ tf_python_pybind_extension(\n         \"//tensorflow/python/lib/core:pybind11_absl\",\n         \"//tensorflow/python/lib/core:pybind11_status\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@pybind11\",\n     ],\n )\n@@ -58,6 +59,8 @@ tf_python_pybind_extension(\n         \"//tensorflow/python/lib/core:pybind11_status\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@pybind11\",\n     ],\n )"
        },
        {
            "sha": "f0c2a99c58ba5e6756d2a7de3568047695aef3fc",
            "filename": "tensorflow/python/lib/io/file_io_wrapper.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Flib%2Fio%2Ffile_io_wrapper.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Flib%2Fio%2Ffile_io_wrapper.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Flib%2Fio%2Ffile_io_wrapper.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"pybind11/pybind11.h\"  // from @pybind11\n #include \"pybind11/stl.h\"  // from @pybind11\n #include \"tensorflow/core/lib/core/error_codes.pb.h\""
        },
        {
            "sha": "060eb973fd50ca9bc929273e2b26ea5fdbcc8fca",
            "filename": "tensorflow/python/lib/io/record_io_wrapper.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Flib%2Fio%2Frecord_io_wrapper.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Flib%2Fio%2Frecord_io_wrapper.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Flib%2Fio%2Frecord_io_wrapper.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -18,6 +18,8 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"pybind11/pybind11.h\"  // from @pybind11\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/core/stringpiece.h\""
        },
        {
            "sha": "0d68e5aa1906542fe782c67ca4ef42d24d1b9c16",
            "filename": "tensorflow/python/ops/structured/structured_tensor.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fops%2Fstructured%2Fstructured_tensor.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fops%2Fstructured%2Fstructured_tensor.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fops%2Fstructured%2Fstructured_tensor.py?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"Structured Tensors.\"\"\"\n \n+from __future__ import annotations\n+\n import re\n from typing import Callable, Dict, List, Mapping, Optional, Sequence, Tuple, Union\n \n@@ -49,6 +51,10 @@\n # FieldValue.\n _FieldFn = Callable[[_FieldValue], _FieldValue]\n \n+# Field names work as key, and they can be a sequence to refer to the\n+# sub-levels (embedded) StructuredTensor's.\n+FieldName = Union[str, Sequence[str]]\n+\n \n @tf_export('experimental.StructuredTensor')\n class StructuredTensor(extension_type.BatchableExtensionType):\n@@ -99,15 +105,6 @@ class StructuredTensor(extension_type.BatchableExtensionType):\n   _ragged_shape: dynamic_ragged_shape.DynamicRaggedShape\n \n   __name__ = 'tf.StructuredTensor'\n-  #=============================================================================\n-  # Common Types\n-  #=============================================================================\n-  # pylint: disable=invalid-name\n-  # Field names work as key, and they can be a sequence to refer to the\n-  # sub-levels (embedded) StructuredTensor's.\n-  FieldName = Union[str, Sequence[str]]\n-\n-  # pylint: enable=invalid-name\n \n   #=============================================================================\n   # Constructor & Factory Methods\n@@ -1190,6 +1187,11 @@ def rank(self):\n       return self._ragged_shape.rank\n \n \n+# We cannot define this inside the class in the usual way because under Python\n+# 3.14 we attempt to interpret it as an ExtensionType field. It would be cleaner\n+# not to define it inside the class at all, but we want to avoid an API break.\n+StructuredTensor.FieldName = FieldName\n+\n # Regular expression used to determine whether a string is a valid field name.\n # Note: we plan to relax (or possibly eliminate) this in the future; you\n # should not rely on the fact that some field names are currently disallowed."
        },
        {
            "sha": "77c77ff6692b8a8721821d88d65ded2ea4b989f3",
            "filename": "tensorflow/python/profiler/internal/profiler_pywrap_impl.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_pywrap_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_pywrap_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_pywrap_impl.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -45,7 +45,7 @@ absl::Status ProfilerSessionWrapper::Start(\n   return session_->Status();\n }\n \n-absl::Status ProfilerSessionWrapper::Stop(tensorflow::string* result) {\n+absl::Status ProfilerSessionWrapper::Stop(std::string* result) {\n   if (session_ != nullptr) {\n     tensorflow::profiler::XSpace xspace;\n     absl::Status status = session_->CollectData(&xspace);"
        },
        {
            "sha": "a788e78b15365a6ed3521c4c7bff7e9337035365",
            "filename": "tensorflow/python/profiler/internal/profiler_pywrap_impl.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_pywrap_impl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_pywrap_impl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_pywrap_impl.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -35,12 +35,12 @@ class ProfilerSessionWrapper {\n       const char* logdir,\n       const absl::flat_hash_map<std::string,\n                                 std::variant<bool, int, std::string>>& options);\n-  absl::Status Stop(tensorflow::string* result);\n+  absl::Status Stop(std::string* result);\n   absl::Status ExportToTensorBoard();\n \n  private:\n   std::unique_ptr<tsl::ProfilerSession> session_;\n-  tensorflow::string logdir_;\n+  std::string logdir_;\n };\n \n }  // namespace pywrap"
        },
        {
            "sha": "d48ddabf15b1913dd6badf5aa0da2d8eda00833b",
            "filename": "tensorflow/python/profiler/internal/profiler_wrapper.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_wrapper.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_wrapper.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fprofiler_wrapper.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -80,7 +80,7 @@ PYBIND11_MODULE(_pywrap_profiler, m) {\n            })\n       .def(\"stop\",\n            [](ProfilerSessionWrapper& wrapper) {\n-             tensorflow::string content;\n+             std::string content;\n              absl::Status status;\n              {\n                py::gil_scoped_release release;"
        },
        {
            "sha": "7123e2539a17fbcd7491d77cbd2f204a0f00cf5d",
            "filename": "tensorflow/python/profiler/internal/pywrap_profiler_plugin.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fpywrap_profiler_plugin.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fpywrap_profiler_plugin.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fprofiler%2Finternal%2Fpywrap_profiler_plugin.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -80,7 +80,7 @@ PYBIND11_MODULE(_pywrap_profiler_plugin, m) {\n \n   m.def(\"monitor\", [](const char* service_addr, int duration_ms,\n                       int monitoring_level, bool display_timestamp) {\n-    tsl::string content;\n+    std::string content;\n     absl::Status status;\n     {\n       py::gil_scoped_release release;"
        },
        {
            "sha": "257e11b1caa99d72d0b1274e13ac335f0ed41bcb",
            "filename": "tensorflow/python/tools/saved_model_aot_compile.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftools%2Fsaved_model_aot_compile.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftools%2Fsaved_model_aot_compile.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftools%2Fsaved_model_aot_compile.py?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -367,6 +367,7 @@ def aot_compile_cpu_meta_graph_def(checkpoint_path,\n     else:\n       xla_flags += ' --xla_cpu_multi_thread_eigen={}'.format(\n           'true' if multithreading else 'false')\n+    xla_flags += ' --xla_cpu_experimental_ynn_fusion_type= '\n     os.environ['XLA_FLAGS'] = xla_flags\n \n   temp_dir = test.get_temp_dir()"
        },
        {
            "sha": "d6762a7b15300234c40c4a927648ce8c300b6df1",
            "filename": "tensorflow/python/tpu/tpu_embedding_v3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3.py?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1388,18 +1388,18 @@ def compute_sparse_core_stats(\n   ) -> Tuple[Any, Any]:\n     \"\"\"Computes the max_ids/unique ids settings from the input features.\"\"\"\n     copy_feature_config = _clone_feature_config(feature_config)\n-    table_config = []\n-    for feature in nest.flatten(copy_feature_config):\n-      table_config.append(feature.table)\n+    table_config_list = list(\n+        {feature.table for feature in nest.flatten(copy_feature_config)}\n+    )\n \n-    for table in table_config:\n+    for table in table_config_list:\n       if table.optimizer is None:\n         table.optimizer = optimizer\n \n     flat_features = nest.flatten_with_joined_string_paths(copy_feature_config)\n \n     s = _stack_tables_with_same_table_dim_and_optimizer(\n-        table_config,\n+        table_config_list,\n         flat_features,\n         num_tpu_chips,\n         num_sc_per_chip,"
        },
        {
            "sha": "67ca374d0ff7e50b9291697a91461a5cef12ad56",
            "filename": "tensorflow/python/tpu/tpu_embedding_v3_test.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_test.py?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -171,6 +171,7 @@ def setUp(self):\n         combiner='sum',\n         name='user',\n     )\n+    self.rng = np.random.default_rng(0)\n \n   def test_single_feature_single_table_lookup_with_static_buffer_size(self):\n     feature_config = tpu_embedding_v2_utils.FeatureConfig(\n@@ -864,6 +865,46 @@ def step(data):\n     ):\n       self.assertAllEqual(per_feature_result, per_feature_result_cpu)\n \n+  def test_compute_sparse_core_stats_shared_table(self):\n+    resolver = tpu_cluster_resolver.TPUClusterResolver(tpu='')\n+    remote.connect_to_cluster(resolver)\n+    tpu_cluster_resolver.initialize_tpu_system(resolver)\n+    strategy = tpu_strategy.TPUStrategy(resolver)\n+\n+    batch_size = 16\n+    num_tpu_chips = strategy.num_replicas_in_sync\n+\n+    shared_table = tpu_embedding_v2_utils.TableConfig(\n+        vocabulary_size=self.vocabulary_size,\n+        dim=self.embedding_dim,\n+        initializer=init_ops_v2.Constant(1.0),\n+        combiner='sum',\n+        name='shared_table',\n+    )\n+\n+    features = ['feature_a', 'feature_b']\n+    feature_config = {\n+        feature: tpu_embedding_v2_utils.FeatureConfig(\n+            table=shared_table, name=feature, output_shape=[batch_size]\n+        )\n+        for feature in features\n+    }\n+\n+    data = {\n+        feature: math_ops.cast(\n+            math_ops.range(batch_size * num_tpu_chips) % self.vocabulary_size,\n+            dtypes.float32,\n+        )\n+        for feature in features\n+    }\n+\n+    tpu_embedding_v3.TPUEmbeddingV2.compute_sparse_core_stats(\n+        features=data,\n+        feature_config=feature_config,\n+        num_tpu_chips=num_tpu_chips,\n+        optimizer=tpu_embedding_v2_utils.SGD(learning_rate=1.0),\n+    )\n+\n   def test_raise_error_when_weight_decay_is_set(self):\n     feature_config = tpu_embedding_v2_utils.FeatureConfig(\n         table=self.table_video, name='watched', output_shape=[16]"
        },
        {
            "sha": "c1c60df8672beceb1cbab593bc8726d2e9778ab8",
            "filename": "tensorflow/python/training/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftraining%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fpython%2Ftraining%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftraining%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -75,7 +75,6 @@ py_strict_library(\n         \"//tensorflow_minigo:__subpackages__\",\n         \"//tensorflow_models:__subpackages__\",\n         \"//third_party/cloud_tpu/convergence_tools:__subpackages__\",\n-        \"//third_party/mlperf:__subpackages__\",\n         \"//third_party/py/tf_slim:__subpackages__\",\n     ],\n     deps = ["
        },
        {
            "sha": "6251e2a39a2fb4dc2bc4d6f0eb2b6bd10b1368aa",
            "filename": "tensorflow/security/fuzzing/cc/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -105,6 +105,7 @@ tf_cc_fuzz_test(\n         \"//tensorflow/core:portable_gif_internal\",\n         \"//tensorflow/core/platform:str_util\",\n         \"//tensorflow/core/platform:stringpiece\",\n+        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n \n@@ -156,6 +157,7 @@ tf_cc_fuzz_test(\n     deps = [\n         \"//tensorflow/core/platform:str_util\",\n         \"//tensorflow/core/platform:stringpiece\",\n+        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n "
        },
        {
            "sha": "0f064a8828d3c1270ef9c8295b35d63c2bc8b7c4",
            "filename": "tensorflow/security/fuzzing/cc/consume_leading_digits_fuzz.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2Fconsume_leading_digits_fuzz.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2Fconsume_leading_digits_fuzz.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2Fconsume_leading_digits_fuzz.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -13,9 +13,11 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n #include <cassert>\n+#include <cstdint>\n #include <string>\n \n #include \"fuzztest/fuzztest.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/core/platform/str_util.h\"\n #include \"tensorflow/core/platform/stringpiece.h\"\n #include \"tensorflow/core/platform/types.h\""
        },
        {
            "sha": "f8ddc2c9f7256404f4477c6cdd340eaa8e719d0e",
            "filename": "tensorflow/security/fuzzing/cc/parseURI_fuzz.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2FparseURI_fuzz.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2FparseURI_fuzz.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2FparseURI_fuzz.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include \"fuzztest/fuzztest.h\"\n #include \"absl/strings/match.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/core/platform/path.h\"\n #include \"tensorflow/core/platform/stringpiece.h\"\n "
        },
        {
            "sha": "da60f6d40db0154342fde3edbadc129002081a88",
            "filename": "tensorflow/security/fuzzing/cc/string_replace_fuzz.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2Fstring_replace_fuzz.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2Fstring_replace_fuzz.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fsecurity%2Ffuzzing%2Fcc%2Fstring_replace_fuzz.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n #include <string>\n \n #include \"fuzztest/fuzztest.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/core/platform/str_util.h\"\n #include \"tensorflow/core/platform/stringpiece.h\"\n "
        },
        {
            "sha": "ea218e74cc4ca078102dd3669a8dd36e9b934092",
            "filename": "tensorflow/tools/graph_transforms/transform_utils_test.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 34,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Fgraph_transforms%2Ftransform_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Fgraph_transforms%2Ftransform_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fgraph_transforms%2Ftransform_utils_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -51,7 +51,7 @@ class TransformUtilsTest : public ::testing::Test {\n \n     GraphDef graph_def;\n     TF_ASSERT_OK(root.ToGraphDef(&graph_def));\n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(graph_def, &node_map);\n \n     EXPECT_EQ(1, node_map.count(\"a\"));\n@@ -85,7 +85,7 @@ class TransformUtilsTest : public ::testing::Test {\n     GraphDef graph_def;\n     TF_ASSERT_OK(root.ToGraphDef(&graph_def));\n \n-    std::map<string, std::vector<const NodeDef*>> outputs_map;\n+    std::map<std::string, std::vector<const NodeDef*>> outputs_map;\n     MapNodesToOutputs(graph_def, &outputs_map);\n \n     EXPECT_EQ(1, outputs_map.count(\"a\"));\n@@ -109,9 +109,9 @@ class TransformUtilsTest : public ::testing::Test {\n   }\n \n   void TestNodeNamePartsFromInput() {\n-    string prefix;\n-    string node_name;\n-    string suffix;\n+    std::string prefix;\n+    std::string node_name;\n+    std::string suffix;\n \n     NodeNamePartsFromInput(\"some_node_name\", &prefix, &node_name, &suffix);\n     EXPECT_EQ(\"\", prefix);\n@@ -175,45 +175,45 @@ class TransformUtilsTest : public ::testing::Test {\n     int32_t value_i = 32;\n     SetNodeAttr(\"foo\", value_i, &node);\n     EXPECT_EQ(32, node.attr().at(\"foo\").i());\n-    string value_s = \"some_value\";\n+    std::string value_s = \"some_value\";\n     SetNodeAttr(\"bar\", value_s, &node);\n     EXPECT_EQ(\"some_value\", node.attr().at(\"bar\").s());\n   }\n \n   void TestSetNodeTensorAttr() {\n     NodeDef node;\n-    SetNodeTensorAttr<int32>(\"foo\", {3, 1}, {1, 2, 3}, &node);\n+    SetNodeTensorAttr<int32_t>(\"foo\", {3, 1}, {1, 2, 3}, &node);\n     TensorProto tensor_proto = node.attr().at(\"foo\").tensor();\n     Tensor tensor;\n     CHECK(tensor.FromProto(tensor_proto));\n     EXPECT_EQ(DT_INT32, tensor.dtype());\n     EXPECT_EQ(3, tensor.shape().dim_size(0));\n     EXPECT_EQ(1, tensor.shape().dim_size(1));\n-    EXPECT_EQ(1, tensor.flat<int32>()(0));\n-    EXPECT_EQ(2, tensor.flat<int32>()(1));\n-    EXPECT_EQ(3, tensor.flat<int32>()(2));\n+    EXPECT_EQ(1, tensor.flat<int32_t>()(0));\n+    EXPECT_EQ(2, tensor.flat<int32_t>()(1));\n+    EXPECT_EQ(3, tensor.flat<int32_t>()(2));\n   }\n \n   void TestSetNodeTensorAttrWithTensor() {\n     NodeDef node;\n     Tensor input_tensor(DT_INT32, {4, 5});\n-    test::FillIota<int32>(&input_tensor, 1);\n-    SetNodeTensorAttr<int32>(\"foo\", input_tensor, &node);\n+    test::FillIota<int32_t>(&input_tensor, 1);\n+    SetNodeTensorAttr<int32_t>(\"foo\", input_tensor, &node);\n     TensorProto tensor_proto = node.attr().at(\"foo\").tensor();\n     Tensor tensor;\n     CHECK(tensor.FromProto(tensor_proto));\n-    test::ExpectTensorEqual<int32>(input_tensor, tensor);\n+    test::ExpectTensorEqual<int32_t>(input_tensor, tensor);\n   }\n \n   void TestGetNodeTensorAttr() {\n     NodeDef node;\n     Tensor input_tensor(DT_INT32, {4, 5});\n-    test::FillIota<int32>(&input_tensor, 1);\n+    test::FillIota<int32_t>(&input_tensor, 1);\n     TensorProto tensor_proto;\n     input_tensor.AsProtoTensorContent(&tensor_proto);\n     SetNodeAttr(\"foo\", tensor_proto, &node);\n     Tensor result = GetNodeTensorAttr(node, \"foo\");\n-    test::ExpectTensorEqual<int32>(input_tensor, result);\n+    test::ExpectTensorEqual<int32_t>(input_tensor, result);\n   }\n \n   void TestFilterGraphDef() {\n@@ -247,7 +247,7 @@ class TransformUtilsTest : public ::testing::Test {\n         [](const NodeDef& node) { return (node.name() != \"remove_me\"); },\n         &result_graph_def);\n \n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(result_graph_def, &node_map);\n     EXPECT_EQ(1, node_map.count(\"a\"));\n     EXPECT_EQ(1, node_map.count(\"b\"));\n@@ -269,7 +269,7 @@ class TransformUtilsTest : public ::testing::Test {\n     GraphDef result_graph_def;\n     RemoveAttributes(graph_def, {\"dtype\"}, &result_graph_def);\n \n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(result_graph_def, &node_map);\n     const NodeDef* removed_placeholder = node_map[\"placeholder\"];\n     EXPECT_EQ(nullptr,\n@@ -432,12 +432,12 @@ class TransformUtilsTest : public ::testing::Test {\n     GraphDef replaced_graph_def;\n     TF_ASSERT_OK(ReplaceMatchingOpTypes(\n         graph_def, {\"*\"},\n-        [](const NodeMatch& match, const std::set<string>& input_nodes,\n-           const std::set<string>& output_nodes,\n+        [](const NodeMatch& match, const std::set<std::string>& input_nodes,\n+           const std::set<std::string>& output_nodes,\n            std::vector<NodeDef>* new_nodes) {\n           NodeDef original_copy;\n           original_copy = match.node;\n-          const string original_name = match.node.name();\n+          const std::string original_name = match.node.name();\n           original_copy.set_name(original_name + \"_before_identity\");\n           new_nodes->push_back(original_copy);\n \n@@ -540,10 +540,10 @@ class TransformUtilsTest : public ::testing::Test {\n \n     GraphDef renamed_graph_def;\n     TF_ASSERT_OK(RenameNodeInputs(graph_def, {{\"a\", \"b\"}},\n-                                  std::unordered_set<string>(),\n+                                  std::unordered_set<std::string>(),\n                                   &renamed_graph_def));\n \n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(renamed_graph_def, &node_map);\n     EXPECT_EQ(\"b\", node_map.at(\"add\")->input(0));\n     EXPECT_EQ(\"b\", node_map.at(\"add\")->input(1));\n@@ -579,9 +579,9 @@ class TransformUtilsTest : public ::testing::Test {\n     GraphDef renamed_graph_def;\n     TF_ASSERT_OK(RenameNodeInputs(\n         graph_def, {{\"a\", \"f\"}, {\"f\", \"e\"}, {\"e\", \"d\"}, {\"d\", \"c\"}},\n-        std::unordered_set<string>(), &renamed_graph_def));\n+        std::unordered_set<std::string>(), &renamed_graph_def));\n \n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(renamed_graph_def, &node_map);\n     EXPECT_EQ(\"c\", node_map.at(\"add\")->input(0));\n     EXPECT_EQ(\"b\", node_map.at(\"add\")->input(1));\n@@ -617,7 +617,7 @@ class TransformUtilsTest : public ::testing::Test {\n     GraphDef renamed_graph_def;\n     absl::Status rename_status =\n         RenameNodeInputs(graph_def, {{\"a\", \"d\"}, {\"d\", \"a\"}},\n-                         std::unordered_set<string>(), &renamed_graph_def);\n+                         std::unordered_set<std::string>(), &renamed_graph_def);\n     EXPECT_FALSE(rename_status.ok());\n   }\n \n@@ -651,10 +651,10 @@ class TransformUtilsTest : public ::testing::Test {\n \n     GraphDef renamed_graph_def;\n     TF_ASSERT_OK(RenameNodeInputs(graph_def, {{\"quantize_a:*\", \"quantize_b\"}},\n-                                  std::unordered_set<string>(),\n+                                  std::unordered_set<std::string>(),\n                                   &renamed_graph_def));\n \n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(renamed_graph_def, &node_map);\n     EXPECT_EQ(\"quantize_b:1\", node_map.at(\"add\")->input(0));\n     EXPECT_EQ(\"quantize_b:2\", node_map.at(\"add\")->input(1));\n@@ -691,7 +691,7 @@ class TransformUtilsTest : public ::testing::Test {\n     TF_ASSERT_OK(RenameNodeInputs(graph_def, {{\"a\", \"b\"}}, {\"add2\"},\n                                   &renamed_graph_def));\n \n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(renamed_graph_def, &node_map);\n     EXPECT_EQ(\"b\", node_map.at(\"add\")->input(0));\n     EXPECT_EQ(\"b\", node_map.at(\"add\")->input(1));\n@@ -731,10 +731,11 @@ class TransformUtilsTest : public ::testing::Test {\n     const_node2->set_op(\"Const\");\n     const_node2->set_name(\"const_node2\");\n \n-    std::vector<std::pair<string, string>> invalid_inputs;\n+    std::vector<std::pair<std::string, std::string>> invalid_inputs;\n     FindInvalidInputs(graph_def, &invalid_inputs);\n     EXPECT_EQ(3, invalid_inputs.size());\n-    for (const std::pair<string, string>& invalid_input : invalid_inputs) {\n+    for (const std::pair<std::string, std::string>& invalid_input :\n+         invalid_inputs) {\n       EXPECT_TRUE((invalid_input.first == \"add_node1\") ||\n                   (invalid_input.first == \"add_node2\"));\n       if (invalid_input.first == \"add_node1\") {\n@@ -802,7 +803,7 @@ class TransformUtilsTest : public ::testing::Test {\n         Const(root.WithOpName(\"float_const\"), Input::Initializer(float_data));\n \n     Tensor int_data(DT_INT32, TensorShape({width}));\n-    test::FillIota<int32>(&int_data, 1);\n+    test::FillIota<int32_t>(&int_data, 1);\n     Output int_const =\n         Const(root.WithOpName(\"int_const\"), Input::Initializer(int_data));\n \n@@ -813,7 +814,7 @@ class TransformUtilsTest : public ::testing::Test {\n     GraphDef graph_def;\n     TF_ASSERT_OK(root.ToGraphDef(&graph_def));\n \n-    std::map<string, const NodeDef*> node_map;\n+    std::map<std::string, const NodeDef*> node_map;\n     MapNamesToNodes(graph_def, &node_map);\n \n     const NodeDef* float_const_def = node_map.at(\"float_const\");\n@@ -920,7 +921,7 @@ class TransformUtilsTest : public ::testing::Test {\n \n     auto e_root = tensorflow::Scope::NewRootScope();\n     Tensor e_data(DT_INT32, TensorShape({width}));\n-    test::FillIota<int32>(&e_data, 1);\n+    test::FillIota<int32_t>(&e_data, 1);\n     Output e_const = Const(e_root.WithOpName(\"a\"), Input::Initializer(e_data));\n     GraphDef e_graph_def;\n     TF_ASSERT_OK(e_root.ToGraphDef(&e_graph_def));\n@@ -976,7 +977,7 @@ class TransformUtilsTest : public ::testing::Test {\n     TransformFuncContext context;\n     context.params.insert({\"foo\", {\"a\", \"b\"}});\n     context.params.insert({\"bar\", {\"c\"}});\n-    string value;\n+    std::string value;\n     TF_EXPECT_OK(context.GetOneStringParameter(\"bar\", \"d\", &value));\n     EXPECT_EQ(\"c\", value);\n     EXPECT_FALSE(context.GetOneStringParameter(\"foo\", \"d\", &value).ok());"
        },
        {
            "sha": "4c695c7a0d850aaa7bf320452b8c694a9a3c0db4",
            "filename": "tensorflow/tools/tfg_graph_transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Ftfg_graph_transforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Ftfg_graph_transforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftfg_graph_transforms%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -29,6 +29,7 @@ cc_library(\n         \"//tensorflow/core/protobuf:for_core_protos_cc\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n "
        },
        {
            "sha": "77cb33fd6a778f627e2f63e02790419a3a1675c2",
            "filename": "tensorflow/tools/tfg_graph_transforms/utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Ftfg_graph_transforms%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Ftfg_graph_transforms%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftfg_graph_transforms%2Futils.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <string>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"tensorflow/cc/saved_model/image_format/internal_api.h\"\n #include \"tensorflow/core/platform/path.h\"\n #include \"tensorflow/core/platform/status.h\""
        },
        {
            "sha": "1c72364a2a0d3815b627d8f9f9e28e9024a4d630",
            "filename": "tensorflow/tools/tfg_graph_transforms/utils.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Ftfg_graph_transforms%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Ftools%2Ftfg_graph_transforms%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftfg_graph_transforms%2Futils.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/errors.h\""
        },
        {
            "sha": "fedccde267230688a1178c3b4ab6aa92437590ed",
            "filename": "tensorflow/workspace0.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fworkspace0.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fworkspace0.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fworkspace0.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -108,10 +108,10 @@ def workspace():\n     # Details: https://github.com/google-ml-infra/rules_ml_toolchain\n     http_archive(\n         name = \"rules_ml_toolchain\",\n-        sha256 = \"a7951a86c4e9783302230b859237d953a6c8c301b219d344e05d70496eeefa52\",\n-        strip_prefix = \"rules_ml_toolchain-0d383c69076f637d55eaae0b6e0ee2980b1345a9\",\n+        sha256 = \"5f17275397752b666adbf8f0a81a3ebfb1e26a970b459cac33a06a8f03caa537\",\n+        strip_prefix = \"rules_ml_toolchain-a2626615e1277a635b43dd268e1d4bc892afea10\",\n         urls = [\n-            \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/0d383c69076f637d55eaae0b6e0ee2980b1345a9.tar.gz\",\n+            \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/a2626615e1277a635b43dd268e1d4bc892afea10.tar.gz\",\n         ],\n     )\n "
        },
        {
            "sha": "8c26d8a53ad74271be5b4c024593239e683c1ce1",
            "filename": "tensorflow/workspace2.bzl",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fworkspace2.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/tensorflow%2Fworkspace2.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fworkspace2.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -168,18 +168,18 @@ def _tf_repositories():\n     # LINT.IfChange(xnnpack)\n     tf_http_archive(\n         name = \"XNNPACK\",\n-        sha256 = \"027376a71384311a0ddca0fc986dea621bac0f8b30c96365bf4d2937b627226f\",\n-        strip_prefix = \"XNNPACK-decc685b0ecfd00da5a2168eb03b0c795678f084\",\n-        urls = tf_mirror_urls(\"https://github.com/google/XNNPACK/archive/decc685b0ecfd00da5a2168eb03b0c795678f084.zip\"),\n+        sha256 = \"a633a48ba393211771204d25ebc5f35359b71bfbefaa6e955aa92570caede727\",\n+        strip_prefix = \"XNNPACK-fa0fd6471a39a5d66a59d4cd8f8cc4a93a4bd470\",\n+        urls = tf_mirror_urls(\"https://github.com/google/XNNPACK/archive/fa0fd6471a39a5d66a59d4cd8f8cc4a93a4bd470.zip\"),\n     )\n     # LINT.ThenChange(//tensorflow/lite/tools/cmake/modules/xnnpack.cmake)\n \n     # XNNPack dependency.\n     tf_http_archive(\n         name = \"KleidiAI\",\n-        sha256 = \"42155cfc084bf1f80e9ef486470f949502ea8d1b845b2f1bebd58978a1b540aa\",\n-        strip_prefix = \"kleidiai-8ca226712975f24f13f71d04cda039a0ee9f9e2f\",\n-        urls = tf_mirror_urls(\"https://github.com/ARM-software/kleidiai/archive/8ca226712975f24f13f71d04cda039a0ee9f9e2f.zip\"),\n+        sha256 = \"fb4f8180171d035a08432b086194121f627d00a76d58cebaad57d7a87ad40dbd\",\n+        strip_prefix = \"kleidiai-7a3a609a3278106df7157bdd27b8f0e75ab00b60\",\n+        urls = tf_mirror_urls(\"https://github.com/ARM-software/kleidiai/archive/7a3a609a3278106df7157bdd27b8f0e75ab00b60.zip\"),\n     )\n \n     tf_http_archive("
        },
        {
            "sha": "f0df7e75993c4542bde995391684345e6c7fbe34",
            "filename": "third_party/xla/.github/workflows/ci.yml",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2F.github%2Fworkflows%2Fci.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2F.github%2Fworkflows%2Fci.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2F.github%2Fworkflows%2Fci.yml?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -78,6 +78,12 @@ jobs:\n             name: \"JAX Linux x86 CPU\",\n             repo: \"jax-ml/jax\",\n           },\n+          {\n+            pool: \"windows-x86-n2-16\",\n+            container: null,\n+            name: \"JAX Windows x86 CPU\",\n+            repo: \"jax-ml/jax\",\n+          },\n           {\n             pool: \"linux-x86-g2-16-l4-1gpu\",\n             container: \"gcr.io/tensorflow-sigs/build:latest-python3.11\",\n@@ -121,4 +127,9 @@ jobs:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: \"Run build.py\"\n         working-directory: ${{ matrix.job_info.repo }}\n-        run: $GITHUB_WORKSPACE/openxla/xla/build_tools/ci/build.py --build=\"${{ matrix.job_info.name }}_github_actions\"\n+        run: |\n+          if [[ \"${{ matrix.job_info.pool }}\" == *windows* ]]; then\n+            python $GITHUB_WORKSPACE\\\\openxla\\\\xla\\\\build_tools\\\\ci\\\\build.py --build=\"${{ matrix.job_info.name }}_github_actions\"\n+          else\n+            $GITHUB_WORKSPACE/openxla/xla/build_tools/ci/build.py --build=\"${{ matrix.job_info.name }}_github_actions\"\n+          fi"
        },
        {
            "sha": "cdfd5e8784e21337da69fb647ad88ddd4ba4a874",
            "filename": "third_party/xla/MODULE.bazel",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2FMODULE.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2FMODULE.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2FMODULE.bazel?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -40,9 +40,9 @@ bazel_dep(name = \"rules_ml_toolchain\")\n # echo \"sha256-${HASH}\"\n archive_override(\n     module_name = \"rules_ml_toolchain\",\n-    integrity = \"sha256-p5UahsTpeDMCIwuFkjfZU6bIwwGyGdNE4F1wSW7u+lI=\",\n-    strip_prefix = \"rules_ml_toolchain-0d383c69076f637d55eaae0b6e0ee2980b1345a9\",\n-    urls = [\"https://github.com/google-ml-infra/rules_ml_toolchain/archive/0d383c69076f637d55eaae0b6e0ee2980b1345a9.tar.gz\"],\n+    integrity = \"sha256-XxcnU5d1K2Zq2/jwqBo+v7HiapcLRZysM6BqjwPKpTc=\",\n+    strip_prefix = \"rules_ml_toolchain-a2626615e1277a635b43dd268e1d4bc892afea10\",\n+    urls = [\"https://github.com/google-ml-infra/rules_ml_toolchain/archive/a2626615e1277a635b43dd268e1d4bc892afea10.tar.gz\"],\n )\n \n # TODO: Upstream the patch?"
        },
        {
            "sha": "f591654b7047e08e88b1e7053d9e37db97acb565",
            "filename": "third_party/xla/WORKSPACE",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2FWORKSPACE",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2FWORKSPACE",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2FWORKSPACE?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -9,10 +9,10 @@ load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n # Details: https://github.com/google-ml-infra/rules_ml_toolchain\n http_archive(\n     name = \"rules_ml_toolchain\",\n-    sha256 = \"a7951a86c4e9783302230b859237d953a6c8c301b219d344e05d70496eeefa52\",\n-    strip_prefix = \"rules_ml_toolchain-0d383c69076f637d55eaae0b6e0ee2980b1345a9\",\n+    sha256 = \"5f17275397752b666adbf8f0a81a3ebfb1e26a970b459cac33a06a8f03caa537\",\n+    strip_prefix = \"rules_ml_toolchain-a2626615e1277a635b43dd268e1d4bc892afea10\",\n     urls = [\n-        \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/0d383c69076f637d55eaae0b6e0ee2980b1345a9.tar.gz\",\n+        \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/a2626615e1277a635b43dd268e1d4bc892afea10.tar.gz\",\n     ],\n )\n "
        },
        {
            "sha": "17af3bf0d9280ecee30493c3f6270e484bb49b82",
            "filename": "third_party/xla/build_tools/ci/build.py",
            "status": "modified",
            "additions": 33,
            "deletions": 2,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -126,6 +126,7 @@ class BuildType(enum.Enum):\n   XLA_MACOS_ARM64_CPU_KOKORO = enum.auto()\n \n   JAX_LINUX_X86_CPU_GITHUB_ACTIONS = enum.auto()\n+  JAX_WINDOWS_X86_CPU_GITHUB_ACTIONS = enum.auto()\n   JAX_LINUX_X86_GPU_L4_GITHUB_ACTIONS = enum.auto()\n \n   TENSORFLOW_LINUX_X86_CPU_GITHUB_ACTIONS = enum.auto()\n@@ -159,6 +160,7 @@ class Build:\n   repo_env: Dict[str, Any] = dataclasses.field(default_factory=dict)\n   override_repository: Dict[str, str] = dataclasses.field(default_factory=dict)\n   options: Dict[str, Any] = dataclasses.field(default_factory=dict)\n+  startup_options: Dict[str, Any] = dataclasses.field(default_factory=dict)\n   extra_setup_commands: Tuple[List[str], ...] = ()\n \n   def __post_init__(self):\n@@ -187,6 +189,7 @@ def bazel_command(\n     Returns: List of command line arguments\n     \"\"\"\n     options = _dict_to_cli_options(self.options)\n+    startup_options = _dict_to_cli_options(self.startup_options)\n     configs = [f\"--config={config}\" for config in self.configs]\n     build_tag_filters = (\n         f\"--build_tag_filters={','.join(self.build_tag_filters)}\"\n@@ -211,7 +214,14 @@ def bazel_command(\n         + options\n         + list(extra_options)\n     )\n-    return [\"bazel\", subcommand, *all_options, \"--\", *self.target_patterns]\n+    return [\n+        \"bazel\",\n+        *startup_options,\n+        subcommand,\n+        *all_options,\n+        \"--\",\n+        *self.target_patterns,\n+    ]\n \n   def commands(self) -> List[List[str]]:\n     \"\"\"Returns list of commands for a build.\"\"\"\n@@ -228,7 +238,8 @@ def commands(self) -> List[List[str]]:\n         self.type_ == BuildType.XLA_MACOS_X86_CPU_KOKORO\n         or self.type_ == BuildType.XLA_MACOS_ARM64_CPU_KOKORO\n     )\n-    if not macos_build:\n+    windows_build = (self.type_ == BuildType.JAX_WINDOWS_X86_CPU_GITHUB_ACTIONS)\n+    if not (macos_build or windows_build):\n       cmds.append(\n           retry(\n               self.bazel_command(\n@@ -707,6 +718,26 @@ def nvidia_gpu_build_with_compute_capability(\n     repo_env={\"HERMETIC_PYTHON_VERSION\": \"3.12\"},\n )\n \n+Build(\n+    type_=BuildType.JAX_WINDOWS_X86_CPU_GITHUB_ACTIONS,\n+    repo=\"google/jax\",\n+    configs=(\"rbe_windows_amd64\",),\n+    target_patterns=(\"//tests:cpu_tests\", \"//tests:backend_independent_tests\"),\n+    test_env=dict(\n+        JAX_NUM_GENERATED_CASES=25,\n+        JAX_SKIP_SLOW_TESTS=1,\n+    ),\n+    override_repository=dict(\n+        xla=f\"{_GITHUB_WORKSPACE}\\\\openxla\\\\xla\",\n+    ),\n+    options={**_DEFAULT_BAZEL_OPTIONS, \"build_runfile_links\": False},\n+    repo_env={\"HERMETIC_PYTHON_VERSION\": \"3.12\"},\n+    subcommand=\"build\",\n+    startup_options={\n+        \"output_base\": f\"{_GITHUB_WORKSPACE}\\\\bazel_output_base\",\n+    },\n+)\n+\n Build(\n     type_=BuildType.JAX_LINUX_X86_GPU_L4_GITHUB_ACTIONS,\n     repo=\"google/jax\","
        },
        {
            "sha": "0b160bfa3ef2e8fe818b3f7f1ab09d5e7fe8d26c",
            "filename": "third_party/xla/build_tools/ci/golden_commands.txt",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -9,6 +9,10 @@ parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_fi\n bazel test --build_tag_filters=-multiaccelerator --test_tag_filters=-multiaccelerator --config=rbe_linux_x86_64_cuda --test_env=JAX_SKIP_SLOW_TESTS=1 --test_env=TF_CPP_MIN_LOG_LEVEL=0 --test_env=JAX_EXCLUDE_TEST_TARGETS=PmapTest.testSizeOverflow --repo_env=HERMETIC_PYTHON_VERSION=3.11 --override_repository=xla=$GITHUB_WORKSPACE/openxla/xla --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async -- //tests:gpu_tests //tests:backend_independent_tests\n bazel analyze-profile profile.json.gz\n # END BuildType.JAX_LINUX_X86_GPU_L4_GITHUB_ACTIONS\n+# BEGIN BuildType.JAX_WINDOWS_X86_CPU_GITHUB_ACTIONS\n+bazel --output_base=$GITHUB_WORKSPACE\\bazel_output_base build --build_tag_filters= --test_tag_filters= --config=rbe_windows_amd64 --test_env=JAX_NUM_GENERATED_CASES=25 --test_env=JAX_SKIP_SLOW_TESTS=1 --repo_env=HERMETIC_PYTHON_VERSION=3.12 --override_repository=xla=$GITHUB_WORKSPACE\\openxla\\xla --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --build_runfile_links=False -- //tests:cpu_tests //tests:backend_independent_tests\n+bazel analyze-profile profile.json.gz\n+# END BuildType.JAX_WINDOWS_X86_CPU_GITHUB_ACTIONS\n # BEGIN BuildType.TENSORFLOW_LINUX_X86_CPU_GITHUB_ACTIONS\n find $GITHUB_WORKSPACE/openxla/xla -type f -exec sed -i s/@local_xla/@local_xla/g {} +\n find $GITHUB_WORKSPACE/openxla/xla -type f -exec sed -i s/@local_tsl/@local_tsl/g {} +"
        },
        {
            "sha": "9ac9c1114f15899ac8eaaa1429933e3a46f29074",
            "filename": "third_party/xla/build_tools/pjrt_wheels/BUILD.bazel",
            "status": "modified",
            "additions": 56,
            "deletions": 1,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2FBUILD.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2FBUILD.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2FBUILD.bazel?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -35,6 +35,21 @@ genrule(\n     cmd = \"cp $< $@\",\n )\n \n+# The wheels have the same C API header, but we need to give each their own copy\n+genrule(\n+    name = \"pjrt_c_api_hdr_cuda\",\n+    srcs = [\"//xla/pjrt/c:pjrt_c_api.h\"],\n+    outs = [\"xla_plugins/xla_\" + cuda_label + \"_pjrt/include/pjrt_c_api.h\"],\n+    cmd = \"cp $< $@\",\n+)\n+\n+genrule(\n+    name = \"pjrt_c_api_hdr_cpu\",\n+    srcs = [\"//xla/pjrt/c:pjrt_c_api.h\"],\n+    outs = [\"xla_plugins/xla_cpu_pjrt/include/pjrt_c_api.h\"],\n+    cmd = \"cp $< $@\",\n+)\n+\n # GPU-specific files\n cc_binary(\n     name = \"xla_plugins/xla_\" + cuda_label + \"_pjrt/xla_gpu_pjrt.so\",\n@@ -65,8 +80,9 @@ py_wheel(\n     summary = \"XLA PJRT Plugin\",\n     version = wheel_version,\n     deps = [\n-        \":xla_plugins/xla_\" + cuda_label + \"_pjrt/xla_gpu_pjrt.so\",\n         \":init_file_\" + cuda_label,\n+        \":pjrt_c_api_hdr_cuda\",\n+        \":xla_plugins/xla_\" + cuda_label + \"_pjrt/xla_gpu_pjrt.so\",\n     ],\n )\n \n@@ -106,6 +122,45 @@ py_wheel(\n     version = wheel_version,\n     deps = [\n         \":init_file_cpu\",\n+        \":pjrt_c_api_hdr_cpu\",\n         \":xla_plugins/xla_cpu_pjrt/xla_cpu_pjrt.so\",\n     ],\n )\n+\n+# Tests\n+cc_test(\n+    name = \"cpu_smoke_test\",\n+    srcs = [\"smoke_test.cc\"],\n+    data = [\":xla_plugins/xla_cpu_pjrt/xla_cpu_pjrt.so\"],\n+    env = {\n+        \"PJRT_PLUGIN_PATH\": \"build_tools/pjrt_wheels/xla_plugins/xla_cpu_pjrt/xla_cpu_pjrt.so\",\n+    },\n+    linkopts = [\"-ldl\"],\n+    deps = [\"//xla/pjrt/c:pjrt_c_api_hdrs\"],\n+)\n+\n+cc_test(\n+    name = cuda_label + \"_smoke_test\",\n+    srcs = [\"smoke_test.cc\"],\n+    data = [\":xla_plugins/xla_\" + cuda_label + \"_pjrt/xla_gpu_pjrt.so\"],\n+    env = {\n+        \"PJRT_PLUGIN_PATH\": \"build_tools/pjrt_wheels/xla_plugins/xla_\" + cuda_label + \"_pjrt/xla_gpu_pjrt.so\",\n+    },\n+    linkopts = [\"-ldl\"],\n+    deps = [\"//xla/pjrt/c:pjrt_c_api_hdrs\"],\n+)\n+\n+# The CPU and CUDA test suites are run in CI's build script\n+test_suite(\n+    name = \"cpu_test_suite\",\n+    tests = [\n+        \":cpu_smoke_test\",\n+    ],\n+)\n+\n+test_suite(\n+    name = \"cuda_test_suite\",\n+    tests = [\n+        \":\" + cuda_label + \"_smoke_test\",\n+    ],\n+)"
        },
        {
            "sha": "e00f439a363d6d5c0595b9b5e849798f02befb1e",
            "filename": "third_party/xla/build_tools/pjrt_wheels/smoke_test.cc",
            "status": "added",
            "additions": 57,
            "deletions": 0,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2Fsmoke_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2Fsmoke_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2Fsmoke_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,57 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <dlfcn.h>\n+\n+#include <iostream>\n+\n+#include \"xla/pjrt/c/pjrt_c_api.h\"\n+\n+typedef const PJRT_Api* (*GetPjrtApi_Func)();\n+\n+int main() {\n+  // 1. Open the shared object\n+  const char* so_path = std::getenv(\"PJRT_PLUGIN_PATH\");\n+  std::cout << \"so_path: \" << so_path << std::endl;\n+  void* handle = dlopen(so_path, RTLD_LAZY);\n+  if (!handle) {\n+    std::cerr << \"Error: Could not open shared object.\" << std::endl;\n+    std::cerr << \"Reason: \" << dlerror() << std::endl;\n+    return 1;\n+  }\n+\n+  // 2. Load the symbol (the function)\n+  GetPjrtApi_Func get_pjrt_api = (GetPjrtApi_Func)dlsym(handle, \"GetPjrtApi\");\n+  const char* dlsym_error = dlerror();\n+  if (dlsym_error) {\n+    std::cerr << \"Error: Could not find symbol 'GetPjrtApi'.\" << std::endl;\n+    std::cerr << \"Reason: \" << dlsym_error << std::endl;\n+    dlclose(handle);\n+    return 1;\n+  }\n+\n+  // 3. Call the function\n+  std::cout << \"Successfully loaded symbol. Calling GetPjrtApi()...\"\n+            << std::endl;\n+  const PJRT_Api* api = get_pjrt_api();\n+  if (api) {\n+    std::cout << \"Success! Received PjrtApi struct pointer.\" << std::endl;\n+  } else {\n+    std::cerr << \"Error: GetPjrtApi() returned a null pointer.\" << std::endl;\n+    return 1;\n+  }\n+\n+  return 0;\n+}"
        },
        {
            "sha": "eeaccd6f9f873c50b756ce653de7a55a44ded205",
            "filename": "third_party/xla/build_tools/rocm/rocm_xla.bazelrc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Frocm%2Frocm_xla.bazelrc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Frocm%2Frocm_xla.bazelrc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Frocm%2Frocm_xla.bazelrc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -26,10 +26,13 @@ build:tsan --copt -fno-omit-frame-pointer\n build:tsan --linkopt -fsanitize=thread\n build:tsan --linkopt -g\n build:tsan --//build_tools/rocm:sanitizer=tsan\n+build:tsan --test_env=TSAN_OPTIONS=suppressions=build_tools/rocm/tsan_ignore_list.txt::history_size=7:ignore_noninstrumented_modules=1\n+build:tsan --run_under=//build_tools/rocm:sanitizer_wrapper\n \n build:asan --test_env=ASAN_OPTIONS=suppressions=build_tools/rocm/asan_ignore_list.txt:use_sigaltstack=0\n build:asan --test_env=LSAN_OPTIONS=suppressions=build_tools/rocm/lsan_ignore_list.txt:use_sigaltstack=0\n build:asan --//build_tools/rocm:sanitizer=asan\n+build:asan --run_under=//build_tools/rocm:sanitizer_wrapper\n \n test:xla_sgpu -- \\\n //xla/... \\"
        },
        {
            "sha": "9f88753e2fcc5a2395e2eaa565d5e689f458aedc",
            "filename": "third_party/xla/build_tools/rocm/tsan_ignore_list.txt",
            "status": "added",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Frocm%2Ftsan_ignore_list.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fbuild_tools%2Frocm%2Ftsan_ignore_list.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Frocm%2Ftsan_ignore_list.txt?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,35 @@\n+race:libhsa-runtime64.so\n+race:libamdhip64.so\n+race:hipStreamSynchronize\n+race:libhipblaslt.so\n+race:libamd_comgr.so\n+race:librccl.so\n+\n+# Abseil reference counting (DropRef / RefCount init)\n+race:tsl::ReferenceCounted\n+race:absl::lts_*::Mutex\n+race:absl::lts_*::CondVar\n+\n+# XLA GPU RawSEDeviceMemory RCReference reuse\n+race:xla::RawSEDeviceMemory\n+race:xla::gpu::AllocateDestinationBuffer\n+race:xla::LocalDeviceState::ThenRelease\n+\n+# To be fixed\n+race:xla::GpuAsyncHostToDeviceTransferManager::TransferRawDataToSubBuffer\n+race:xla::LiteralBase::Piece::DeallocateBuffers\n+race:xla::PjRtStreamExecutorLoadedExecutable::ExecuteHelper\n+race:xla::PjRtStreamExecutorClient::BufferFromHostBufferInternal\n+race:xla::PjRtStreamExecutorClient::AllocateAndRecordEvent\n+race:xla::HloRunnerPjRt::TransferLiteralsFromDevice\n+race:xla::MutableLiteralBase::~MutableLiteralBase\n+race:xla::MutableLiteralBase::PopulateR1<int>\n+race:xla::gpu::GpuCompiler::CompileSingleModule\n+race:xla::LiteralBase::Piece::Storage::Storage\n+race:xla::LocalClient::TransferFromOutfeedLocal\n+race:llvm::cl::opt_storage<bool, false, false>::setValue<int>\n+\n+race:xla::gpu::(anonymous namespace)::RecoverExp2Pattern::initStaticsIfNeeded*\n+race:lld::lldMain\n+race:llvm::*\n+race:xla::gpu::GpuExecutable::ExecuteAsyncOnStream"
        },
        {
            "sha": "c196c9024507dacf262fe1d8a82d12f1bd54898f",
            "filename": "third_party/xla/docs/error_codes.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferror_codes.md?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -2,4 +2,6 @@\n \n This page is a list of all error codes emitted by the XLA compiler.\n \n+-   [E0100](./errors/error_0100.md)\n+-   [E0101](./errors/error_0101.md)\n -   [E0102](./errors/error_0102.md)"
        },
        {
            "sha": "36a7d62aae95ba90aa8fb98769bdb9314ad8f946",
            "filename": "third_party/xla/docs/errors/error_0100.md",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0100.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0100.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0100.md?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,68 @@\n+# Error code: 0100\n+\n+**Category:** Buffer allocation failure - TPU\n+\n+**Type:** Runtime\n+\n+## Error log example\n+\n+```\n+ValueError: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 8.00M. That was not possible. There are 6.43M free.; (0x0x1_HBM0)\n+```\n+\n+## Why do these happen?\n+\n+XLA:TPU runtimes memory allocator failed to find a suitable block of memory on\n+the accelerators HBM for the requested operation. These operations are\n+typically user initiated buffer allocations via\n+[`jax.device_put`](https://docs.jax.dev/en/latest/_autosummary/jax.device_put.html)\n+or allocations for program outputs. These failures stem from a couple of\n+reasons: - Out of Memory (OOM) - The user is trying to allocate a chunk of\n+memory that is larger than the total amount of free memory available on the\n+TPUs HBM. - Memory Fragmentation - The allocation fails because **no single\n+contiguous free block** in the memory space is large enough to satisfy the\n+requested size. The total amount of free memory is sufficient for the\n+allocation, but it is scattered across the memory space in small, non-contiguous\n+blocks.\n+\n+The TPU runtime has a number of mechanisms in-place to retry allocation failures\n+including: - If there are queued deallocations, the runtime retries failed\n+allocations, - On OOMs caused by a fragmentation the runtime can automatically\n+trigger a defragmentation and a retry. - The TPU runtime prioritizes buffer\n+allocations over keeping programs loaded. If a buffer allocation fails due to\n+insufficient HBM, the system will evict loaded TPU programs until enough memory\n+is available for the buffer.\n+\n+So an error encountered after the above mitigations typically require user\n+action.\n+\n+## How can a user fix their program when they do happen?\n+\n+-   Reduce your model's memory footprint:\n+    -   Decrease Batch Size: Reducing the batch size directly lowers memory\n+        usage.\n+    -   Parameter Sharding: For very large models, use techniques like model\n+        parallelism or sharding to distribute parameters across the HBM of\n+        multiple TPU cores or hosts.\n+    -   Shorten Sequence/Context Length: For models that operate on sequences\n+        (like language models), reducing the input sequence length can\n+        significantly decrease the memory footprint.\n+    -   Buffer Donation: Utilize framework features (such as: `jax.jit(...,\n+        donate_argnums=...)`) to signal to XLA that certain input buffers can be\n+        overwritten and reused for outputs.\n+    -   Optimize Checkpoint Strategy: Instead of saving the entire model state\n+        at once, consider saving only the model weights or using a sharded\n+        checkpointing strategy.\n+-   Address Memory Layout and Padding:\n+    -   TPU memory is allocated in chunks, and padding can increase the actual\n+        size of tensors.\n+-   Ensure no memory leaks:\n+    -   Ensure references to `jax.Array` objects are not being held longer than\n+        intended. Holding on to `jax.Array` objects might prevent automatic\n+        de-allocation even after program compilation is completed.\n+\n+## How can a user debug these failures?\n+\n+Enable the `tpu_log_allocations_on_oom` flag for which the allocator will dump a\n+detailed report of all current allocations when an OOM occurs, which can be\n+invaluable for debugging."
        },
        {
            "sha": "aa239d4cfce7e3152640c7cdbbbf19690b825a1e",
            "filename": "third_party/xla/docs/errors/error_0101.md",
            "status": "added",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0101.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0101.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0101.md?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,66 @@\n+# Error code: 0101\n+\n+**Category:** Program allocation failure\n+\n+**Type:** Runtime\n+\n+## Error log example\n+\n+```\n+XlaRuntimeError: RESOURCE_EXHAUSTED: Error loading program 'jit_embedding_pipeline_step_fn': Attempting to reserve 29.49G at the bottom of memory. That was not possible. There are 147.64M free, 0B reserved, and 147.64M reservable. Scope: unknown..: while running replica 0 and partition 34 of a replicated computation (other replicas may have failed as well).\n+```\n+\n+## Why do these happen?\n+\n+This error indicates that the XLA runtime on a TPU device failed to load a\n+compiled XLA program executable into the TPU's HBM. It typically occurs for one\n+of the following reasons: - Program Size Exceeds Available HBM: The compiled XLA\n+program, including its instructions, static data, and any embedded constants, is\n+larger than the total amount of free HBM currently available on the specific TPU\n+core(s) where the program is being loaded. - HBM Fragmentation: While the total\n+free HBM on the device might be sufficient in aggregate, it is not available in\n+a single, contiguous block large enough to fit the entire program.\n+\n+It's important to understand how the TPU runtime prioritizes memory. Buffer\n+allocations are privileged over loaded programs. If a buffer allocation fails,\n+the runtime will evict already loaded programs from HBM to free up space. This\n+can lead to a situation where a program that loaded successfully before now\n+fails with an OOM error, because the HBM is now occupied with more data buffers.\n+\n+## How can a user fix their program when they do happen?\n+\n+-   Reduce Buffer Memory Footprint: Freeing up memory used by data buffers will\n+    leave more room for the program itself:\n+    -   Decrease Batch Size: This is one of the most effective ways to reduce\n+        the amount of memory used for activations.\n+    -   Parameter Sharding: For very large models, use model parallelism or\n+        sharding techniques (like FSDP or Megascale) to distribute the model's\n+        parameters and computation across multiple TPU cores or hosts.\n+    -   Shorten Sequence/Context Length: For models processing sequential data\n+        (e.g., NLP models), reducing the sequence length can significantly\n+        decrease memory usage.\n+    -   Buffer Donation: Use framework features (e.g., `jax.jit(...,\n+        donate_argnums=...)`) to allow XLA to reuse the memory of input buffers\n+        for storing output, reducing peak memory usage.\n+-   Reduce programs memory requirements for temporaries:\n+    -   Reduce programs memory usage for temporaries by using the\n+        `tpu_shared_memory_percent` flag. Note that this might negatively affect\n+        performance.\n+-   Optimize Execution Strategy/Reduce Serving load:\n+    -   Manage Program Loading: If you are JIT-compiling multiple functions, be\n+        aware that each function can result in a program being loaded. Try to\n+        structure your workload to minimize the number of concurrently loaded\n+        programs.\n+-   Ensure no memory leaks:\n+    -   Ensure references to `jax.Array` objects are not being held longer than\n+        intended. Holding on to `jax.Array` objects might prevent automatic\n+        de-allocation even after program compilation is completed.\n+\n+## How can a user debug these failures?\n+\n+-   Enable the `tpu_log_allocations_on_oom` flag for which the allocator will\n+    dump a detailed report of all current allocations when an OOM occurs, which\n+    can be invaluable for debugging.\n+-   Profile Your Program: Use the JAX memory profiler or the TensorFlow profiler\n+    to get a detailed view of your program's memory usage over time. This can\n+    help identify unexpected peaks in memory consumption."
        },
        {
            "sha": "da201691d155f9839b678832607b523fe07461e8",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1206,8 +1206,8 @@ replicas.\n | `channel_id`          | optional `ChannelHandle` | Optional channel ID for   |\n :                       :                          : cross-module              :\n :                       :                          : communication             :\n-| `inpace`              | optional `bool`          | flag whether permutation  |\n-:                       :                          : should be done inplace    :\n+| `inplace`             | optional `bool`          | flag whether permutation  |\n+:                       :                          : should be done in place   :\n \n Note that there are the following restrictions on the `source_target_pairs`:\n "
        },
        {
            "sha": "2ab7c777c2409119935e8c5de0da5bf6f821f289",
            "filename": "third_party/xla/third_party/eigen3/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Feigen3%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Feigen3%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Feigen3%2Fworkspace.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -7,8 +7,8 @@ def repo():\n \n     # Attention: tools parse and update these lines.\n     # LINT.IfChange\n-    EIGEN_COMMIT = \"70d8d99d0df9fd967b135efd8d12ed20fc48d007\"\n-    EIGEN_SHA256 = \"78d1158871b8d3663cead3fb3c482721155df9a331d94cfcc60bcdf5cdbf18e1\"\n+    EIGEN_COMMIT = \"dcbaf2d608f306450f1e74949eb87e9a22a7ef4b\"\n+    EIGEN_SHA256 = \"a71517b3815984c1a8174db1ebc58a17d4f5c23c06e377bbc4a5dfc85855a516\"\n     # LINT.ThenChange(//tensorflow/lite/tools/cmake/modules/eigen.cmake)\n \n     tf_http_archive("
        },
        {
            "sha": "3fcb99ad26eec19e3bbdec3afee2d1c8a23bce03",
            "filename": "third_party/xla/third_party/llvm/generated.patch",
            "status": "modified",
            "additions": 179,
            "deletions": 124,
            "changes": 303,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1,53 +1,144 @@\n Auto generated patch. Do not edit or delete it, even if empty.\n-diff -ruN --strip-trailing-cr a/libc/src/stdio/printf_core/vfprintf_internal.h b/libc/src/stdio/printf_core/vfprintf_internal.h\n---- a/libc/src/stdio/printf_core/vfprintf_internal.h\n-+++ b/libc/src/stdio/printf_core/vfprintf_internal.h\n-@@ -51,8 +51,11 @@\n- LIBC_INLINE FileIOResult fwrite_unlocked(const void *ptr, size_t size,\n-                                          size_t nmemb, ::FILE *f) {\n-   // Need to use system errno in this case, as system write will set this errno\n--  // which we need to propagate back into our code.\n--  return {::fwrite_unlocked(ptr, size, nmemb, f), errno};\n-+  // which we need to propagate back into our code. fwrite only modifies errno\n-+  // if there was an error, and errno may have previously been nonzero. Only\n-+  // return errno if there was an error.\n-+  size_t members_written = ::fwrite_unlocked(ptr, size, nmemb, f);\n-+  return {members_written, members_written == nmemb ? 0 : errno};\n- }\n- #endif // LIBC_COPT_STDIO_USE_SYSTEM_FILE\n- } // namespace internal\n-diff -ruN --strip-trailing-cr a/libcxx/include/fstream b/libcxx/include/fstream\n---- a/libcxx/include/fstream\n-+++ b/libcxx/include/fstream\n-@@ -315,8 +315,14 @@\n-         traits_type::copy(__str, this->gptr(), __n);\n-         this->__gbump_ptrdiff(__n);\n+diff -ruN --strip-trailing-cr a/libcxx/include/__hash_table b/libcxx/include/__hash_table\n+--- a/libcxx/include/__hash_table\n++++ b/libcxx/include/__hash_table\n+@@ -1910,6 +1910,8 @@\n+         __bucket_list_[__next_chash] = __before_first;\n+         __chash                      = __next_chash;\n        }\n--      if (__len - __n >= this->egptr() - this->eback())\n--        return std::fread(__str + __n, sizeof(char_type), __len - __n, __file_);\n-+      const streamsize __remainder    = __len - __n;\n-+      const streamsize __buffer_space = this->egptr() - this->eback();\n-+\n-+      if (__remainder >= __buffer_space)\n-+        return std::fread(__str + __n, sizeof(char_type), __remainder, __file_) + __n;\n-+      else if (__remainder > 0)\n-+        return basic_streambuf<_CharT, _Traits>::xsgetn(__str + __n, __remainder) + __n;\n-+      return __n;\n++    } else { // When __next is a nullptr we've fully erased the last bucket. Update the bucket list accordingly.\n++      __bucket_list_[__chash] = nullptr;\n      }\n-     return basic_streambuf<_CharT, _Traits>::xsgetn(__str, __len);\n    }\n-diff -ruN --strip-trailing-cr a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp\n---- a/llvm/lib/Passes/PassBuilderPipelines.cpp\n-+++ b/llvm/lib/Passes/PassBuilderPipelines.cpp\n-@@ -228,7 +228,7 @@\n- static cl::opt<bool>\n-     EnableDFAJumpThreading(\"enable-dfa-jump-thread\",\n-                            cl::desc(\"Enable DFA jump threading\"),\n--                           cl::init(true), cl::Hidden);\n-+                           cl::init(false), cl::Hidden);\n  \n- static cl::opt<bool>\n-     EnableHotColdSplit(\"hot-cold-split\",\n+diff -ruN --strip-trailing-cr a/libcxx/test/std/containers/unord/unord.map/unord.map.modifiers/erase_range.pass.cpp b/libcxx/test/std/containers/unord/unord.map/unord.map.modifiers/erase_range.pass.cpp\n+--- a/libcxx/test/std/containers/unord/unord.map/unord.map.modifiers/erase_range.pass.cpp\n++++ b/libcxx/test/std/containers/unord/unord.map/unord.map.modifiers/erase_range.pass.cpp\n+@@ -57,6 +57,28 @@\n+     assert(c.size() == 0);\n+     assert(k == c.end());\n+   }\n++  { // Make sure that we're properly updating the bucket list when we're erasing to the end\n++    std::unordered_map<int, int> m;\n++    m.insert(std::make_pair(1, 1));\n++    m.insert(std::make_pair(2, 2));\n++\n++    {\n++      auto pair = m.equal_range(1);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    {\n++      auto pair = m.equal_range(2);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    m.insert(std::make_pair(3, 3));\n++    assert(m.size() == 1);\n++    assert(*m.begin() == std::make_pair(3, 3));\n++    assert(++m.begin() == m.end());\n++  }\n+ #if TEST_STD_VER >= 11\n+   {\n+     typedef std::unordered_map<int,\n+diff -ruN --strip-trailing-cr a/libcxx/test/std/containers/unord/unord.multimap/unord.multimap.modifiers/erase_range.pass.cpp b/libcxx/test/std/containers/unord/unord.multimap/unord.multimap.modifiers/erase_range.pass.cpp\n+--- a/libcxx/test/std/containers/unord/unord.multimap/unord.multimap.modifiers/erase_range.pass.cpp\n++++ b/libcxx/test/std/containers/unord/unord.multimap/unord.multimap.modifiers/erase_range.pass.cpp\n+@@ -122,6 +122,28 @@\n+     for (const auto& v : map)\n+       assert(v.first == 1 || v.first == collision_val);\n+   }\n++  { // Make sure that we're properly updating the bucket list when we're erasing to the end\n++    std::unordered_multimap<int, int> m;\n++    m.insert(std::make_pair(1, 1));\n++    m.insert(std::make_pair(2, 2));\n++\n++    {\n++      auto pair = m.equal_range(1);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    {\n++      auto pair = m.equal_range(2);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    m.insert(std::make_pair(3, 3));\n++    assert(m.size() == 1);\n++    assert(*m.begin() == std::make_pair(3, 3));\n++    assert(++m.begin() == m.end());\n++  }\n+ #if TEST_STD_VER >= 11\n+   {\n+     typedef std::unordered_multimap<int,\n+diff -ruN --strip-trailing-cr a/libcxx/test/std/containers/unord/unord.multiset/erase_range.pass.cpp b/libcxx/test/std/containers/unord/unord.multiset/erase_range.pass.cpp\n+--- a/libcxx/test/std/containers/unord/unord.multiset/erase_range.pass.cpp\n++++ b/libcxx/test/std/containers/unord/unord.multiset/erase_range.pass.cpp\n+@@ -64,6 +64,28 @@\n+     for (const auto& v : map)\n+       assert(v == 1 || v == collision_val);\n+   }\n++  { // Make sure that we're properly updating the bucket list when we're erasing to the end\n++    std::unordered_multiset<int> m;\n++    m.insert(1);\n++    m.insert(2);\n++\n++    {\n++      auto pair = m.equal_range(1);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    {\n++      auto pair = m.equal_range(2);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    m.insert(3);\n++    assert(m.size() == 1);\n++    assert(*m.begin() == 3);\n++    assert(++m.begin() == m.end());\n++  }\n+ #if TEST_STD_VER >= 11\n+   {\n+     typedef std::unordered_multiset<int, std::hash<int>, std::equal_to<int>, min_allocator<int>> C;\n+diff -ruN --strip-trailing-cr a/libcxx/test/std/containers/unord/unord.set/erase_range.pass.cpp b/libcxx/test/std/containers/unord/unord.set/erase_range.pass.cpp\n+--- a/libcxx/test/std/containers/unord/unord.set/erase_range.pass.cpp\n++++ b/libcxx/test/std/containers/unord/unord.set/erase_range.pass.cpp\n+@@ -47,6 +47,28 @@\n+     assert(c.size() == 0);\n+     assert(k == c.end());\n+   }\n++  { // Make sure that we're properly updating the bucket list when we're erasing to the end\n++    std::unordered_set<int> m;\n++    m.insert(1);\n++    m.insert(2);\n++\n++    {\n++      auto pair = m.equal_range(1);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    {\n++      auto pair = m.equal_range(2);\n++      assert(pair.first != pair.second);\n++      m.erase(pair.first, pair.second);\n++    }\n++\n++    m.insert(3);\n++    assert(m.size() == 1);\n++    assert(*m.begin() == 3);\n++    assert(++m.begin() == m.end());\n++  }\n+ #if TEST_STD_VER >= 11\n+   {\n+     typedef std::unordered_set<int, std::hash<int>, std::equal_to<int>, min_allocator<int>> C;\n diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp b/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp\n --- a/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp\n +++ b/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp\n@@ -60,7 +151,7 @@ diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp b/\n  public:\n    IndVarSimplify(LoopInfo *LI, ScalarEvolution *SE, DominatorTree *DT,\n                   const DataLayout &DL, TargetLibraryInfo *TLI,\n-@@ -1077,6 +1079,85 @@\n+@@ -1091,6 +1093,85 @@\n    return true;\n  }\n  \n@@ -146,7 +237,7 @@ diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp b/\n  static void replaceExitCond(BranchInst *BI, Value *NewCond,\n                              SmallVectorImpl<WeakTrackingVH> &DeadInsts) {\n    auto *OldCond = BI->getCondition();\n-@@ -1984,6 +2065,10 @@\n+@@ -1998,6 +2079,10 @@\n  \n    // The Rewriter may not be used from this point on.\n  \n@@ -493,83 +584,6 @@ diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/PowerPC/combine-sext-and-shl-a\n  ; CHECK-P9-BE-NEXT:    #\n  ; CHECK-P9-BE-NEXT:    ld r4, 0(r3)\n  ; CHECK-P9-BE-NEXT:    extswsli r5, r29, 2\n-diff -ruN --strip-trailing-cr a/llvm/test/Other/new-pm-defaults.ll b/llvm/test/Other/new-pm-defaults.ll\n---- a/llvm/test/Other/new-pm-defaults.ll\n-+++ b/llvm/test/Other/new-pm-defaults.ll\n-@@ -208,7 +208,6 @@\n- ; CHECK-O-NEXT: Running analysis: DemandedBitsAnalysis\n- ; CHECK-O-NEXT: Running pass: InstCombinePass\n- ; CHECK-EP-PEEPHOLE-NEXT: Running pass: NoOpFunctionPass\n--; CHECK-O23SZ-NEXT: Running pass: DFAJumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running pass: JumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running analysis: LazyValueAnalysis\n- ; CHECK-O23SZ-NEXT: Running pass: CorrelatedValuePropagationPass\n-diff -ruN --strip-trailing-cr a/llvm/test/Other/new-pm-thinlto-postlink-defaults.ll b/llvm/test/Other/new-pm-thinlto-postlink-defaults.ll\n---- a/llvm/test/Other/new-pm-thinlto-postlink-defaults.ll\n-+++ b/llvm/test/Other/new-pm-thinlto-postlink-defaults.ll\n-@@ -133,7 +133,6 @@\n- ; CHECK-O-NEXT: Running pass: BDCEPass\n- ; CHECK-O-NEXT: Running analysis: DemandedBitsAnalysis\n- ; CHECK-O-NEXT: Running pass: InstCombinePass\n--; CHECK-O23SZ-NEXT: Running pass: DFAJumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running pass: JumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running analysis: LazyValueAnalysis\n- ; CHECK-O23SZ-NEXT: Running pass: CorrelatedValuePropagationPass\n-diff -ruN --strip-trailing-cr a/llvm/test/Other/new-pm-thinlto-postlink-pgo-defaults.ll b/llvm/test/Other/new-pm-thinlto-postlink-pgo-defaults.ll\n---- a/llvm/test/Other/new-pm-thinlto-postlink-pgo-defaults.ll\n-+++ b/llvm/test/Other/new-pm-thinlto-postlink-pgo-defaults.ll\n-@@ -118,7 +118,6 @@\n- ; CHECK-O-NEXT: Running pass: BDCEPass\n- ; CHECK-O-NEXT: Running analysis: DemandedBitsAnalysis\n- ; CHECK-O-NEXT: Running pass: InstCombinePass\n--; CHECK-O23SZ-NEXT: Running pass: DFAJumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running pass: JumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running analysis: LazyValueAnalysis\n- ; CHECK-O23SZ-NEXT: Running pass: CorrelatedValuePropagationPass\n-diff -ruN --strip-trailing-cr a/llvm/test/Other/new-pm-thinlto-postlink-samplepgo-defaults.ll b/llvm/test/Other/new-pm-thinlto-postlink-samplepgo-defaults.ll\n---- a/llvm/test/Other/new-pm-thinlto-postlink-samplepgo-defaults.ll\n-+++ b/llvm/test/Other/new-pm-thinlto-postlink-samplepgo-defaults.ll\n-@@ -127,7 +127,6 @@\n- ; CHECK-O-NEXT: Running pass: BDCEPass\n- ; CHECK-O-NEXT: Running analysis: DemandedBitsAnalysis\n- ; CHECK-O-NEXT: Running pass: InstCombinePass\n--; CHECK-O23SZ-NEXT: Running pass: DFAJumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running pass: JumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running analysis: LazyValueAnalysis\n- ; CHECK-O23SZ-NEXT: Running pass: CorrelatedValuePropagationPass\n-diff -ruN --strip-trailing-cr a/llvm/test/Other/new-pm-thinlto-prelink-defaults.ll b/llvm/test/Other/new-pm-thinlto-prelink-defaults.ll\n---- a/llvm/test/Other/new-pm-thinlto-prelink-defaults.ll\n-+++ b/llvm/test/Other/new-pm-thinlto-prelink-defaults.ll\n-@@ -165,7 +165,6 @@\n- ; CHECK-O-NEXT: Running pass: BDCEPass\n- ; CHECK-O-NEXT: Running analysis: DemandedBitsAnalysis\n- ; CHECK-O-NEXT: Running pass: InstCombinePass\n--; CHECK-O23SZ-NEXT: Running pass: DFAJumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running pass: JumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running analysis: LazyValueAnalysis\n- ; CHECK-O23SZ-NEXT: Running pass: CorrelatedValuePropagationPass\n-diff -ruN --strip-trailing-cr a/llvm/test/Other/new-pm-thinlto-prelink-pgo-defaults.ll b/llvm/test/Other/new-pm-thinlto-prelink-pgo-defaults.ll\n---- a/llvm/test/Other/new-pm-thinlto-prelink-pgo-defaults.ll\n-+++ b/llvm/test/Other/new-pm-thinlto-prelink-pgo-defaults.ll\n-@@ -167,7 +167,6 @@\n- ; CHECK-O-NEXT: Running pass: BDCEPass\n- ; CHECK-O-NEXT: Running analysis: DemandedBitsAnalysis\n- ; CHECK-O-NEXT: Running pass: InstCombinePass\n--; CHECK-O23SZ-NEXT: Running pass: DFAJumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running pass: JumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running analysis: LazyValueAnalysis\n- ; CHECK-O23SZ-NEXT: Running pass: CorrelatedValuePropagationPass\n-diff -ruN --strip-trailing-cr a/llvm/test/Other/new-pm-thinlto-prelink-samplepgo-defaults.ll b/llvm/test/Other/new-pm-thinlto-prelink-samplepgo-defaults.ll\n---- a/llvm/test/Other/new-pm-thinlto-prelink-samplepgo-defaults.ll\n-+++ b/llvm/test/Other/new-pm-thinlto-prelink-samplepgo-defaults.ll\n-@@ -131,7 +131,6 @@\n- ; CHECK-O-NEXT: Running pass: BDCEPass\n- ; CHECK-O-NEXT: Running analysis: DemandedBitsAnalysis\n- ; CHECK-O-NEXT: Running pass: InstCombinePass\n--; CHECK-O23SZ-NEXT: Running pass: DFAJumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running pass: JumpThreadingPass\n- ; CHECK-O23SZ-NEXT: Running analysis: LazyValueAnalysis\n- ; CHECK-O23SZ-NEXT: Running pass: CorrelatedValuePropagationPass\n diff -ruN --strip-trailing-cr a/llvm/test/Transforms/IndVarSimplify/AMDGPU/addrspace-7-doesnt-crash.ll b/llvm/test/Transforms/IndVarSimplify/AMDGPU/addrspace-7-doesnt-crash.ll\n --- a/llvm/test/Transforms/IndVarSimplify/AMDGPU/addrspace-7-doesnt-crash.ll\n +++ b/llvm/test/Transforms/IndVarSimplify/AMDGPU/addrspace-7-doesnt-crash.ll\n@@ -1776,3 +1790,44 @@ diff -ruN --strip-trailing-cr a/llvm/test/Transforms/PhaseOrdering/X86/vdiv.ll b\n +; CHECK: [[META12]] = !{!\"llvm.loop.unroll.disable\"}\n +; CHECK: [[LOOP13]] = distinct !{[[LOOP13]], [[META8]]}\n  ;.\n+diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel\n+--- a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel\n++++ b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel\n+@@ -9794,6 +9794,29 @@\n+     deps = [\":mlir_float16_utils\"],\n+ )\n+ \n++cc_library(\n++    name = \"_mlir_apfloat_utils\",\n++    srcs = [\"lib/ExecutionEngine/APFloatWrappers.cpp\"],\n++    defines = [\"mlir_apfloat_wrappers_EXPORTS\"],\n++    includes = [\"include\"],\n++    deps = [\n++        \"//llvm:Support\",\n++    ],\n++)\n++\n++# Indirection to avoid 'libmlir_apfloat_utils.so' filename clash.\n++alias(\n++    name = \"mlir_apfloat_utils\",\n++    actual = \"_mlir_apfloat_utils\",\n++)\n++\n++cc_binary(\n++    name = \"libmlir_apfloat_utils.so\",\n++    linkshared = True,\n++    linkstatic = False,\n++    deps = [\":mlir_apfloat_utils\"],\n++)\n++\n+ # Unlike mlir_float16_utils, mlir_c_runner_utils, etc, we do *not* make\n+ # this a shared library: because on the CMake side, doing so causes issues\n+ # when building on Windows.  In particular, various functions take/return\n+@@ -9837,6 +9860,7 @@\n+     deps = [\n+         \":SparseTensorEnums\",\n+         \":SparseTensorRuntime\",\n++        \":mlir_apfloat_utils\",\n+         \":mlir_float16_utils\",\n+         \"//llvm:Support\",\n+     ],"
        },
        {
            "sha": "855355c45f663912bea203efefe024986796f56d",
            "filename": "third_party/xla/third_party/llvm/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n \n def repo(name):\n     \"\"\"Imports LLVM.\"\"\"\n-    LLVM_COMMIT = \"741ba8209c1f9bd5b1a145d9c137f5e18bfffb84\"\n-    LLVM_SHA256 = \"45f2faa4d50e9f333c4130c23f3a712cf42f8f8fec4520207f6514344f8b32b0\"\n+    LLVM_COMMIT = \"355e0f94af5adabe90ac57110ce1b47596afd4cd\"\n+    LLVM_SHA256 = \"70762c09d25dc7aaa23856338c5dd14a72faf85ecd4362d3267ec99427db95f8\"\n \n     tf_http_archive(\n         name = name,"
        },
        {
            "sha": "5b9aee4f085e34b913d1463f202a47e59144b917",
            "filename": "third_party/xla/third_party/rocm_device_libs/build_defs.bzl",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Fbuild_defs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Fbuild_defs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Fbuild_defs.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -2,8 +2,6 @@\n \n load(\"@bazel_skylib//lib:paths.bzl\", \"paths\")\n \n-BitcodeLibraryInfo = provider(fields = [\"bc_file\"])\n-\n def _bitcode_library_impl(ctx):\n     \"\"\"Implements a bitcode library rule.\"\"\"\n     srcs = ctx.files.srcs\n@@ -19,20 +17,21 @@ def _bitcode_library_impl(ctx):\n             out = ctx.actions.declare_file(src.basename + \".bc\")\n             bc_outputs.append(out)\n \n-            extra_flags = ctx.attr.file_specific_flags.get(src.basename, \"\")\n+            extra_flags = ctx.attr.file_specific_flags.get(src.basename, [])\n             include_flags = [\"-I{}\".format(dir) for dir in include_dirs]\n             include_flags += [\"-I{}\".format(ctx.files._clang_header[0].dirname)]\n             include_flags += [\"-I{}\".format(ctx.files._clang_includes[0].dirname)]\n+\n+            # https://github.com/ROCm/llvm-project/blob/679865ee84553d564ad0551d878196e58c9d03f3/amd/device-libs/cmake/OCL.cmake#L33\n             args = [\n-                \"-x\",\n-                \"cl\",\n-                \"--target=amdgcn-amd-amdhsa\",\n-                \"-emit-llvm\",\n                 \"-fcolor-diagnostics\",\n                 \"-Werror\",\n                 \"-Wno-error=atomic-alignment\",\n+                \"-x\",\n+                \"cl\",\n                 \"-Xclang\",\n                 \"-cl-std=CL2.0\",\n+                \"--target=amdgcn-amd-amdhsa\",\n                 \"-fvisibility=hidden\",\n                 \"-fomit-frame-pointer\",\n                 \"-Xclang\",\n@@ -47,16 +46,17 @@ def _bitcode_library_impl(ctx):\n                 \"-cl-no-stdinc\",\n                 \"-Xclang\",\n                 \"-mcode-object-version=none\",\n+                \"-emit-llvm\",\n                 \"-c\",\n-            ] + include_flags + [src.path, \"-o\", out.path] + extra_flags.split(\" \")\n+            ] + include_flags + [src.path, \"-o\", out.path] + extra_flags\n \n             ctx.actions.run(\n                 executable = ctx.executable._clang,\n                 inputs = [src] + hdrs + ctx.files._clang_includes + ctx.files._clang_header,\n                 outputs = [out],\n                 arguments = args,\n-                progress_message = \"Compiling {}  bitcode\".format(src.basename),\n-                mnemonic = \"RocmBitCodeCompile\",\n+                progress_message = \"Compiling {} to bitcode\".format(src.basename),\n+                mnemonic = \"BitcodeCompile\",\n             )\n \n         elif src.path.endswith(\".ll\"):\n@@ -71,7 +71,7 @@ def _bitcode_library_impl(ctx):\n         outputs = [prelink_out],\n         arguments = [f.path for f in bc_outputs] + [\"-o\", prelink_out.path],\n         progress_message = \"Linking {} bitcode files\".format(ctx.label.name),\n-        mnemonic = \"RocmBitCodeLink\",\n+        mnemonic = \"BitcodeLink\",\n     )\n \n     # Internalize symbols (llvm-link + -internalize)\n@@ -82,7 +82,7 @@ def _bitcode_library_impl(ctx):\n         outputs = [internalize_out],\n         arguments = [\"-internalize\", \"-only-needed\", prelink_out.path, \"-o\", internalize_out.path],\n         progress_message = \"Internalizing symbols for {}\".format(ctx.label.name),\n-        mnemonic = \"RocmBitCodeInternalizingSymbols\",\n+        mnemonic = \"BitcodeInternalizeSymbols\",\n     )\n \n     # Strip unnecessary metadata\n@@ -93,7 +93,7 @@ def _bitcode_library_impl(ctx):\n         outputs = [strip_out],\n         arguments = [\"-passes=strip\", \"-o\", strip_out.path, internalize_out.path],\n         progress_message = \"Stripping {}\".format(ctx.label.name),\n-        mnemonic = \"RocmBitCodeStripping\",\n+        mnemonic = \"BitcodeStrip\",\n     )\n \n     # Final preparation of bitcode (custom prepare_builtins tool)\n@@ -104,20 +104,19 @@ def _bitcode_library_impl(ctx):\n         outputs = [final_bc],\n         arguments = [strip_out.path, \"-o\", final_bc.path],\n         progress_message = \"Preparing final bitcode for {}\".format(ctx.label.name),\n-        mnemonic = \"RocmBitCodeFinalize\",\n+        mnemonic = \"BitcodeFinalize\",\n     )\n \n     return [\n         DefaultInfo(files = depset([final_bc])),\n-        BitcodeLibraryInfo(bc_file = final_bc),\n     ]\n \n bitcode_library = rule(\n     implementation = _bitcode_library_impl,\n     attrs = {\n         \"srcs\": attr.label_list(allow_files = [\".cl\", \".ll\"]),\n         \"hdrs\": attr.label_list(allow_files = [\".h\"]),\n-        \"file_specific_flags\": attr.string_dict(),\n+        \"file_specific_flags\": attr.string_list_dict(),\n         \"_clang\": attr.label(\n             default = Label(\"@llvm-project//clang:clang\"),\n             executable = True,"
        },
        {
            "sha": "11795b3537e7a974f34041ccce9f06e310716be6",
            "filename": "third_party/xla/third_party/rocm_device_libs/rocm_device_libs.BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Frocm_device_libs.BUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Frocm_device_libs.BUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Frocm_device_libs.BUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -39,9 +39,9 @@ bitcode_library(\n         \"oclc/inc/*.h\",\n     ]),\n     file_specific_flags = {\n-        \"native_logF.cl\": \"-fapprox-func\",\n-        \"native_expF.cl\": \"-fapprox-func\",\n-        \"sqrtF.cl\": \"-cl-fp32-correctly-rounded-divide-sqrt\",\n+        \"native_logF.cl\": [\"-fapprox-func\"],\n+        \"native_expF.cl\": [\"-fapprox-func\"],\n+        \"sqrtF.cl\": [\"-cl-fp32-correctly-rounded-divide-sqrt\"],\n     },\n )\n \n@@ -57,6 +57,6 @@ bitcode_library(\n         \"oclc/inc/*.h\",\n     ]),\n     file_specific_flags = {\n-        \"gaaf.cl\": \"-munsafe-fp-atomics\",\n+        \"gaaf.cl\": [\"-munsafe-fp-atomics\"],\n     },\n )"
        },
        {
            "sha": "2958b33b670cf8044cf417c0544e59c04832a605",
            "filename": "third_party/xla/third_party/rocm_device_libs/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Frocm_device_libs%2Fworkspace.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n     \"\"\"Imports Rocm-Device-Libs.\"\"\"\n-    LLVM_COMMIT = \"c93c6e5451544e9ead12f2d2b15e1969b9a1bd04\"\n-    LLVM_SHA256 = \"f715a0a9c3c1a2b09a79939016ed53a0cbd454f7b0ea4ef32878433275c7b16c\"\n+    LLVM_COMMIT = \"fcc50fb091b7c75d8f6c9a6554d0b004bc0cd474\"\n+    LLVM_SHA256 = \"fa9089d3134bd32d2b05a141006b9261e441c1d80b75782db0dcb154b6a60561\"\n \n     tf_http_archive(\n         name = \"rocm_device_libs\","
        },
        {
            "sha": "7f92eb5c2a5118f0486f8c2d912ad95462f4c71d",
            "filename": "third_party/xla/third_party/shardy/temporary.patch",
            "status": "modified",
            "additions": 324,
            "deletions": 1649,
            "changes": 1973,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch?ref=de857f9a71f934ca661a9d325dafb225869fc6b0"
        },
        {
            "sha": "655f504b33bf913ca597490fe09e3950cf00cbfb",
            "filename": "third_party/xla/third_party/shardy/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -3,8 +3,8 @@\n load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n-    SHARDY_COMMIT = \"c898ed01ccd83579bdb8e9cdba685ed5f188066e\"\n-    SHARDY_SHA256 = \"6a39d72a85322e333f834aae9df3731cd9e594fc07fd360985a4076c9bff7bf0\"\n+    SHARDY_COMMIT = \"44f89b834dc884b4988565e541cf072ee340f000\"\n+    SHARDY_SHA256 = \"e103bbc826604f966959fc48c25713425452d3eb83b548d592e784f794ebc8ab\"\n \n     tf_http_archive(\n         name = \"shardy\","
        },
        {
            "sha": "d5f42e92a6049e8025f0c7a05c3dd97b915650bc",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 570,
            "deletions": 1143,
            "changes": 1713,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1,50 +1,6 @@\n-diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel\n---- stablehlo/BUILD.bazel\n-+++ stablehlo/BUILD.bazel\n-@@ -1105,6 +1105,24 @@\n-     tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n-     td_file = \"stablehlo/transforms/Passes.td\",\n-     deps = [\"@llvm-project//mlir:PassBaseTdFiles\"],\n-+)\n-+\n-+cc_library(\n-+    name = \"stablehlo_broadcast_lowering\",\n-+    srcs = [\n-+        \"stablehlo/transforms/StablehloBroadcastLowering.cpp\",\n-+    ],\n-+    hdrs = [\n-+        \"stablehlo/transforms/StablehloBroadcastLowering.h\",\n-+    ],\n-+    strip_include_prefix = \".\",\n-+    deps = [\n-+        \":stablehlo_ops\",\n-+        \"@llvm-project//llvm:Support\",\n-+        \"@llvm-project//mlir:IR\",\n-+        \"@llvm-project//mlir:ShapeDialect\",\n-+        \"@llvm-project//mlir:Support\",\n-+    ],\n- )\n- \n- cc_library(\n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp\n --- stablehlo/stablehlo/dialect/StablehloOps.cpp\n +++ stablehlo/stablehlo/dialect/StablehloOps.cpp\n-@@ -3275,12 +3275,12 @@\n- // Entry point for Attribute printing, TableGen generated code will handle the\n- // dispatch to the individual classes.\n- void StablehloDialect::printAttribute(Attribute attr,\n--                                      DialectAsmPrinter& os) const {\n-+                                      DialectAsmPrinter& printer) const {\n-   if (auto type_extensions = dyn_cast<TypeExtensionsAttr>(attr)) {\n--    hlo::printTypeExtensions(cast<hlo::BoundedAttrInterface>(attr), os);\n-+    hlo::printTypeExtensions(cast<hlo::BoundedAttrInterface>(attr), printer);\n-     return;\n-   }\n--  LogicalResult result = generatedAttributePrinter(attr, os);\n-+  LogicalResult result = generatedAttributePrinter(attr, printer);\n-   (void)result;\n-   assert(succeeded(result));\n- }\n @@ -4024,6 +4024,61 @@\n    ReturnOp::create(*builder, loc, compare);\n  }\n@@ -110,24 +66,7 @@ diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/\n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h\n --- stablehlo/stablehlo/dialect/StablehloOps.h\n +++ stablehlo/stablehlo/dialect/StablehloOps.h\n-@@ -93,13 +93,14 @@\n-   Type parseType(DialectAsmParser& parser) const override;\n- \n-   // Prints a type registered to this dialect.\n--  void printType(Type type, DialectAsmPrinter& os) const override;\n-+  void printType(Type type, DialectAsmPrinter& printer) const override;\n- \n-   // Parses an attribute registered to this dialect.\n-   Attribute parseAttribute(DialectAsmParser& parser, Type type) const override;\n- \n-   // Prints an attribute registered to this dialect.\n--  void printAttribute(Attribute attr, DialectAsmPrinter& os) const override;\n-+  void printAttribute(Attribute attr,\n-+                      DialectAsmPrinter& printer) const override;\n- \n-   // Get the set dialect version.\n-   std::optional<StablehloDialectVersion> getVersion() const;\n-@@ -203,6 +204,16 @@\n+@@ -204,6 +204,16 @@\n    stablehlo::ReturnOp::create(builder, loc, reducer.getResult());\n  }\n  \n@@ -144,563 +83,243 @@ diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/di\n  // PrecisionConfigAttr is a constraint attribute on ArrayAttrs.\n  // Create this class to allow for building this attr similar to other\n  // attributes.\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n-@@ -20,7 +20,7 @@\n- #include <utility>\n- #include <vector>\n- \n--#include \"gtest/gtest.h\"\n-+#include \"testing/base/public/gunit.h\"\n- #include \"llvm/ADT/DenseMap.h\"\n- #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n- #include \"mlir/IR/BuiltinTypes.h\"\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp\n-@@ -15,7 +15,7 @@\n- \n- #include <string>\n+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n+--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n++++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n+@@ -203,6 +203,9 @@\n+   // If the op does not support type inference, return a default output shape\n+   // parameter that must be injected.\n+   MethodParameter getDefaultOutputShape() {\n++    if (hasSingleVariadicResult(getOp()) || getOp().getNumResults() > 1) {\n++      return MethodParameter(\"TypeRange\", \"resultTypes\");\n++    }\n+     return MethodParameter(\"Type\", \"resultType\");\n+   }\n  \n--#include \"gtest/gtest.h\"\n-+#include \"testing/base/public/gunit.h\"\n- #include \"llvm/Support/raw_ostream.h\"\n- #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n- #include \"mlir/IR/BuiltinOps.h\"\n+@@ -276,7 +279,7 @@\n+     BuilderParams params = getOpBuilderParameters();\n+     SmallVector<MethodParameter> parameters;\n+     if (params.outputShape.has_value()) {\n+-      parameters.push_back(getDefaultOutputShape());\n++      parameters.push_back(params.outputShape.value());\n+     }\n+     for (auto& operand : params.operands) {\n+       parameters.push_back(\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n --- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n +++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n-@@ -17,7 +17,7 @@\n+@@ -17,12 +17,12 @@\n  #include <cstdint>\n  #include <string>\n  \n -#include \"gtest/gtest.h\"\n-+#include \"testing/base/public/gunit.h\"\n  #include \"mlir/IR/BuiltinAttributes.h\"\n  #include \"mlir/IR/BuiltinOps.h\"\n  #include \"mlir/IR/DialectRegistry.h\"\n-diff --ruN a/stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td b/stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td\n---- stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td\n-+++ stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td\n-@@ -17,6 +17,6 @@\n- #ifndef STABLEHLO_INTEGRATIONS_PYTHON_INTERPRETER_OPS\n- #define STABLEHLO_INTEGRATIONS_PYTHON_INTERPRETER_OPS\n- \n--include \"third_party/stablehlo/stablehlo/reference/InterpreterOps.h\"\n-+include \"stablehlo/reference/InterpreterOps.h\"\n- \n- #endif\n-diff --ruN a/stablehlo/stablehlo/tests/BUILD.bazel b/stablehlo/stablehlo/tests/BUILD.bazel\n---- stablehlo/stablehlo/tests/BUILD.bazel\n-+++ stablehlo/stablehlo/tests/BUILD.bazel\n-@@ -102,6 +102,8 @@\n-     deps = [\n-         \":test_utils_inc_gen\",\n-         \"//:stablehlo_assembly_format\",\n-+        \"//:stablehlo_broadcast_lowering\",\n-+        \"//:stablehlo_ops\",\n-         \"@llvm-project//llvm:Support\",\n-         \"@llvm-project//mlir:FuncDialect\",\n-         \"@llvm-project//mlir:IR\",\n-diff --ruN a/stablehlo/stablehlo/tests/TestUtils.cpp b/stablehlo/stablehlo/tests/TestUtils.cpp\n---- stablehlo/stablehlo/tests/TestUtils.cpp\n-+++ stablehlo/stablehlo/tests/TestUtils.cpp\n-@@ -19,6 +19,7 @@\n- #include <utility>\n- \n- #include \"llvm/ADT/STLExtras.h\"\n-+#include \"llvm/ADT/SmallVector.h\"\n- #include \"llvm/Support/Casting.h\"\n- #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n- #include \"mlir/Dialect/Shape/IR/Shape.h\"\n-@@ -28,6 +29,7 @@\n- #include \"mlir/IR/Operation.h\"\n- #include \"mlir/IR/OperationSupport.h\"\n- #include \"mlir/IR/PatternMatch.h\"\n-+#include \"mlir/IR/TypeRange.h\"\n- #include \"mlir/Interfaces/InferTypeOpInterface.h\"\n- #include \"mlir/Interfaces/SideEffectInterfaces.h\"\n- #include \"mlir/Pass/Pass.h\"\n-@@ -35,11 +37,34 @@\n+ #include \"mlir/IR/MLIRContext.h\"\n+ #include \"mlir/IR/OwningOpRef.h\"\n++#include \"mlir/IR/Types.h\"\n+ #include \"mlir/IR/Verifier.h\"\n+ #include \"mlir/Support/DebugStringHelper.h\"\n  #include \"mlir/Support/LLVM.h\"\n- #include \"mlir/Support/LogicalResult.h\"\n- #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-+#include \"stablehlo/dialect/StablehloOps.h\"\n-+#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n+@@ -32,6 +32,7 @@\n+ #include \"stablehlo/integrations/cpp/builder/FuncBuilder.h\"\n+ #include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n+ #include \"stablehlo/integrations/cpp/builder/StablehloBuilder.h\"\n++#include \"gtest/gtest.h\"\n  \n  namespace mlir {\n- namespace hlo {\n- \n- namespace {\n-+\n-+struct BroadcastValuesPattern : public RewritePattern {\n-+  explicit BroadcastValuesPattern(MLIRContext* context)\n-+      : RewritePattern(\"hlo_test_broadcast.numpy_broadcast\", 1, context) {}\n-+  LogicalResult matchAndRewrite(Operation* op,\n-+                                PatternRewriter& rewriter) const override {\n-+    // Process all operands\n-+    SmallVector<Value> operands = llvm::to_vector(op->getOperands());\n-+    auto broadcastedOperands =\n-+        stablehlo::numpyBroadcastIfNeeded(rewriter, operands);\n-+    if (failed(broadcastedOperands)) return failure();\n-+\n-+    // Replace with custom call to avoid pattern reapplication\n-+    auto customCall = stablehlo::CustomCallOp::create(\n-+        rewriter, op->getLoc(), op->getResultTypes(), *broadcastedOperands);\n-+    customCall.setCallTargetName(\"numpy_broadcasted\");\n-+    customCall.setHasSideEffect(true);\n-+    rewriter.replaceOp(op, customCall);\n-+    return success();\n-+  }\n-+};\n+ namespace stablehlo {\n+@@ -1517,6 +1518,29 @@\n+   EXPECT_EQ(expected, debugString(*module));\n+ }\n  \n- struct InferReturnTypesPattern : public RewritePattern {\n-   explicit InferReturnTypesPattern(MLIRContext *context)\n-@@ -137,36 +162,55 @@\n-   }\n- };\n- \n-+#define GEN_PASS_DEF_HLOTESTBROADCASTPASS\n- #define GEN_PASS_DEF_HLOTESTINFERPASS\n- #define GEN_PASS_DEF_HLOTESTSPECULATABILITYPASS\n- #include \"stablehlo/tests/TestUtils.h.inc\"\n- \n-+struct HloTestBroadcastPass\n-+    : public impl::HloTestBroadcastPassBase<HloTestBroadcastPass> {\n-+  LogicalResult initialize(MLIRContext* context) override {\n-+    RewritePatternSet patterns(context);\n-+    patterns.add<BroadcastValuesPattern>(context);\n-+    patterns_ = std::move(patterns);\n-+    return success();\n++TEST(MlirBuilderTest, VariadicResult) {\n++  std::string expected = R\"mlir(module {\n++  func.func @main() -> (tensor<f64>, tensor<f64>) {\n++    %0:2 = stablehlo.custom_call @two_outs() : () -> (tensor<f64>, tensor<f64>)\n++    return %0#0, %0#1 : tensor<f64>, tensor<f64>\n +  }\n-+\n-+  void runOnOperation() override {\n-+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns_))))\n-+      return signalPassFailure();\n++})mlir\";\n++\n++  StablehloModuleBuilder mb;\n++  {\n++    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n++    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n++    auto type = makeTensorType(fb.getContext(), {}, ElementType::F64);\n++    SmallVector<Type> resultTypes = {type, type};\n++    // Pass double data with i64 type.\n++    auto cc = stablehlo::CustomCall(fb, resultTypes, {}, \"two_outs\");\n++    func::Return(fb, {cc});\n +  }\n +\n-+ private:\n-+  FrozenRewritePatternSet patterns_;\n-+};\n-+\n- struct HloTestInferPass : public impl::HloTestInferPassBase<HloTestInferPass> {\n-   LogicalResult initialize(MLIRContext *context) override {\n--    RewritePatternSet patterns_(context);\n--    patterns_.add<InferReturnTypesPattern>(context);\n--    patterns_.add<ReifyReturnTypeShapesPattern>(context);\n--    patterns = std::move(patterns_);\n-+    RewritePatternSet patterns(context);\n-+    patterns.add<InferReturnTypesPattern>(context);\n-+    patterns.add<ReifyReturnTypeShapesPattern>(context);\n-+    patterns_ = std::move(patterns);\n-     return success();\n-   }\n- \n-   void runOnOperation() override {\n--    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns))))\n-+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns_))))\n-       return signalPassFailure();\n-   }\n- \n-  private:\n--  FrozenRewritePatternSet patterns;\n-+  FrozenRewritePatternSet patterns_;\n- };\n- \n- struct HloTestSpeculatabilityPass\n-     : public impl::HloTestSpeculatabilityPassBase<HloTestSpeculatabilityPass> {\n-   LogicalResult initialize(MLIRContext *context) override {\n--    RewritePatternSet patterns_(context);\n--    patterns_.add<IsSpeculatablePattern>(context);\n--    patterns_.add<IsNotSpeculatablePattern>(context);\n--    patterns_.add<IsRecursivelySpeculatablePattern>(context);\n--    patterns = std::move(patterns_);\n-+    RewritePatternSet patterns(context);\n-+    patterns.add<IsSpeculatablePattern>(context);\n-+    patterns.add<IsNotSpeculatablePattern>(context);\n-+    patterns.add<IsRecursivelySpeculatablePattern>(context);\n-+    patterns_ = std::move(patterns);\n-     return success();\n-   }\n- \n-@@ -175,11 +219,11 @@\n-     config.setMaxIterations(1)\n-         .setUseTopDownTraversal(true)\n-         .setRegionSimplificationLevel(GreedySimplifyRegionLevel::Disabled);\n--    (void)applyPatternsGreedily(getOperation(), std::move(patterns));\n-+    (void)applyPatternsGreedily(getOperation(), std::move(patterns_));\n-   }\n- \n-  private:\n--  FrozenRewritePatternSet patterns;\n-+  FrozenRewritePatternSet patterns_;\n- };\n- \n- #define GEN_PASS_REGISTRATION\n-diff --ruN a/stablehlo/stablehlo/tests/TestUtils.td b/stablehlo/stablehlo/tests/TestUtils.td\n---- stablehlo/stablehlo/tests/TestUtils.td\n-+++ stablehlo/stablehlo/tests/TestUtils.td\n-@@ -16,6 +16,11 @@\n- \n- include \"mlir/Pass/PassBase.td\"\n- \n-+def HloTestBroadcastPass : Pass<\"hlo-test-broadcast\", \"func::FuncOp\"> {\n-+  let summary = \"Uses test ops to invoke BroadcastUtils methods.\";\n-+  let dependentDialects = [\"stablehlo::StablehloDialect\"];\n++  OwningOpRef<ModuleOp> module = mb->build();\n++  EXPECT_EQ(expected, debugString(*module));\n +}\n +\n- def HloTestInferPass : Pass<\"hlo-test-infer\", \"func::FuncOp\"> {\n-   let summary = \"Uses test ops to invoke InferShapedTypeOpInterface methods.\";\n-   let dependentDialects = [\"shape::ShapeDialect\"];\n+ ////////\n+ // Custom Attribute Tests\n+ ////////\n diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stablehlo/tests/ops_broadcasting.mlir\n --- stablehlo/stablehlo/tests/ops_broadcasting.mlir\n +++ stablehlo/stablehlo/tests/ops_broadcasting.mlir\n-@@ -0,0 +1,249 @@\n-+// RUN: stablehlo-opt %s --hlo-test-broadcast --split-input-file --allow-unregistered-dialect | FileCheck %s\n-+\n-+/////////\n-+// Scalar broadcast tests.\n-+\n-+// [] x [1] => [1]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_1\n-+func.func @scalar_broadcast_scalar_x_1(%arg0: tensor<f64>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<1xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [1] x [] => [1]\n-+// CHECK-LABEL: func @scalar_broadcast_1_x_scalar\n-+func.func @scalar_broadcast_1_x_scalar(%arg0: tensor<1xf64>, %arg1: tensor<f64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<1xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<f64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [] x [10] => [10]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_10\n-+func.func @scalar_broadcast_scalar_x_10(%arg0: tensor<f64>, %arg1: tensor<10xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<10xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<10xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [<=10] x [] => [<=10]\n-+// CHECK-LABEL: func @scalar_broadcast_b10_x_scalar\n-+func.func @scalar_broadcast_b10_x_scalar(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<f64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<f64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [] x [<=10] => [<=10]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_b10\n-+func.func @scalar_broadcast_scalar_x_b10(%arg0: tensor<f64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 0\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [] x [1, <=10, 1] => [1, <=10, 1]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_1_b10_1\n-+func.func @scalar_broadcast_scalar_x_1_b10_1(%arg0: tensor<f64>, %arg1: tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<1x10x1xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 1\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// [10, 1, <=5] x [] => [10, 1, <=5]\n-+// CHECK-LABEL: func @scalar_broadcast_10_1_b5_x_scalar\n-+func.func @scalar_broadcast_10_1_b5_x_scalar(%arg0: tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, %arg1: tensor<f64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10x1x5xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 2\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 2\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, tensor<f64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+//////\n-+// 1-D SCALAR TESTS\n-+\n-+// [1] x [1] => [1]\n-+// [1] x [10] => [1]\n-+// [<=10] x [1] => [<=10]\n-+// [1] x [<=10] => [<=10]\n-+// [1] x [1, <=10, 1] => [1, <=10, 1]\n-+\n-+\n-+// [1] x [1] => [1]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_1\n-+func.func @single_dim_scalar_1_x_1(%arg0: tensor<1xf64>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [1] x [10] => [10]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_10\n-+func.func @single_dim_scalar_1_x_10(%arg0: tensor<1xf64>, %arg1: tensor<10xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<10xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [<=10] x [1] => [<=10]\n-+// CHECK-LABEL: func @single_dim_scalar_b10_x_1\n-+func.func @single_dim_scalar_b10_x_1(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<1xf64>) -> !stablehlo.token\n+@@ -92,6 +92,8 @@\n+ // [<=10] x [1] => [<=10]\n+ // [1] x [<=10] => [<=10]\n+ // [1] x [1, <=10, 1] => [1, <=10, 1]\n++// [5] x [10, 1] => [10, 5]\n++// [5] x [<=10, 1] => [<=10, 5]\n+ \n+ \n+ // [1] x [1] => [1]\n+@@ -232,6 +234,38 @@\n+ \n+ // -----\n+ \n++// [5] x [10, 1] => [10, 5]\n++// CHECK-LABEL: func @tensor_broadcast_5_x_10_1\n++func.func @tensor_broadcast_5_x_10_1(%arg0: tensor<5xf64>, %arg1: tensor<10x1xf64>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf64>) -> tensor<10x5xf64>\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<10x1xf64>) -> tensor<10x5xf64>\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<5xf64>, tensor<10x1xf64>) -> !stablehlo.token\n +  return %0 : !stablehlo.token\n +}\n +\n +// -----\n +\n-+// [1] x [<=10] => [<=10]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_b10\n-+func.func @single_dim_scalar_1_x_b10(%arg0: tensor<1xf64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 0\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// [<=10] x [<=10] => [<=10] // PT layer must ensure these are identical!\n-+// CHECK-LABEL: func @single_dim_scalar_b10_x_b10\n-+func.func @single_dim_scalar_b10_x_b10(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n++// [<=10, 1] x [5] => [<=10, 5]\n++// CHECK-LABEL: func @tensor_broadcast_b5_1_x_5\n++func.func @tensor_broadcast_b5_1_x_5(\n++  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n++  %arg1: tensor<5xf64>\n++) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x5xf64, #stablehlo.bounds<10, ?>>\n++  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf64>) -> tensor<10x5xf64>\n++  // CHECK: %[[ARG0_DIM0_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[ARG0_DIM0_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (\n++    tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n++    tensor<5xf64>\n++  ) -> !stablehlo.token\n +  return %0 : !stablehlo.token\n +}\n +\n +// -----\n +\n-+// [1] x [1, <=10, 1] => [1, <=10, 1]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_1_b10_1\n-+func.func @single_dim_scalar_1_x_1_b10_1(%arg0: tensor<1xf64>, %arg1: tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<1xf64>) -> tensor<1x10x1xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 1\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n+ //////\n+ // N-ary broadcast tests.\n+ \n+@@ -247,3 +281,42 @@\n+   return %0 : !stablehlo.token\n+ }\n+ \n +// -----\n +\n-+// [10, 1, <=5] x [1] => [10, 1, <=5]\n-+// CHECK-LABEL: func @single_dim_scalar_10_1_b5_x_1\n-+func.func @single_dim_scalar_10_1_b5_x_1(%arg0: tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1xf64>) -> tensor<10x1x5xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 2\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 2\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+\n-+//////\n-+// N-D Tests\n-+\n-+// [1, 2] x [1, 2] => [1, 2]\n-+// CHECK-LABEL: func @tensor_no_broadcast_match\n-+func.func @tensor_no_broadcast_match(%arg0: tensor<1x2xf64>, %arg1: tensor<1x2xf64>) -> !stablehlo.token {\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1x2xf64>, tensor<1x2xf64>) ->  !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n++/////\n++// Broadcast errors\n +\n-+// [10, 1] x [1, 1] => [10, 1]\n-+// CHECK-LABEL: func @tensor_broadcast_10_1_x_1_1\n-+func.func @tensor_broadcast_10_1_x_1_1(%arg0: tensor<10x1xf64>, %arg1: tensor<1x1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x1xf64>) -> tensor<10x1xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1xf64>, tensor<1x1xf64>) -> !stablehlo.token\n++// [10] x [5] => error\n++// expected-error @+1 {{incompatible shapes for broadcasting 10 and 5}}\n++func.func @broadcast_error_10_x_5(%arg0: tensor<10xf64>, %arg1: tensor<5xf64>) -> !stablehlo.token {\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, tensor<5xf64>) -> !stablehlo.token\n +  return %0 : !stablehlo.token\n +}\n +\n +// -----\n +\n-+// [<=10, 1] x [1, 10] => [<=10, 10]\n-+// CHECK-LABEL: func @tensor_broadcast_b10_1_x_1_10\n-+func.func @tensor_broadcast_b10_1_x_1_10(%arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>, %arg1: tensor<1x10xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x10xf64, #stablehlo.bounds<10, ?>>\n-+  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x10xf64>) -> tensor<10x10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>, tensor<1x10xf64>) -> !stablehlo.token\n++// [10] x [<=10] => error\n++// expected-error @+1 {{cannot mix bounded and static dimensions in broadcast}}\n++func.func @broadcast_error_10_x_b10(%arg0: tensor<10xf64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n +  return %0 : !stablehlo.token\n +}\n +\n +// -----\n +\n-+// [<=10, 1] x [1, <=10] => [<=10, <=10]\n-+// CHECK-LABEL: func @tensor_broadcast_b10_1_x_1_b10\n-+func.func @tensor_broadcast_b10_1_x_1_b10(\n-+  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+  %arg1: tensor<1x?xf64, #stablehlo.bounds<?, 10>>\n-+) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x10xf64, #stablehlo.bounds<10, ?>>\n-+  // CHECK: %[[ARG1_DIM1_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST_STATIC]], %[[ARG1_DIM1_SIZE]], dim = 1\n-+  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x?xf64, #stablehlo.bounds<?, 10>>) -> tensor<10x?xf64, #stablehlo.bounds<?, 10>>\n-+  // CHECK: %[[ARG0_DIM0_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[ARG0_DIM0_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (\n-+    tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+    tensor<1x?xf64, #stablehlo.bounds<?, 10>>\n-+  ) -> !stablehlo.token\n++// [10] x not_tensor => error\n++func.func @broadcast_error_not_tensor(%arg0: tensor<10xf64>, %arg1: !stablehlo.token) -> !stablehlo.token {\n++  // expected-error @+1 {{expected ranked tensor type for broadcast inputs}}\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, !stablehlo.token) -> !stablehlo.token\n +  return %0 : !stablehlo.token\n +}\n +\n +// -----\n +\n-+//////\n-+// N-ary broadcast tests.\n-+\n-+\n-+// [<=10, 1] x [1, <=10] x [1] => [<=10, <=10]\n-+// CHECK-LABEL: func @nary_broadcast_b10_1_x_1_b10_x_1\n-+func.func @nary_broadcast_b10_1_x_1_b10_x_1(\n-+  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+  %arg1: tensor<1x?xf64, #stablehlo.bounds<?, 10>>,\n-+  %arg2: tensor<1xf64>\n-+) -> !stablehlo.token {\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1, %arg2) : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>, tensor<1x?xf64, #stablehlo.bounds<?, 10>>, tensor<1xf64>) -> !stablehlo.token\n++// [] => error\n++func.func @broadcast_error_empty() -> !stablehlo.token {\n++  // expected-error @+1 {{requires at least one operand to broadcast}}\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"() : () -> !stablehlo.token\n +  return %0 : !stablehlo.token\n +}\n +\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -473,6 +473,44 @@\n-   return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xi32>, tensor<3x3xi32>, tensor<2x5xi32>\n- }\n+@@ -47,8 +47,8 @@\n+ ////////\n+ // CaseOp\n  \n-+// CHECK-LABEL: func.func @fold_concatenate_splat_leading\n-+func.func @fold_concatenate_splat_leading(%arg0: tensor<1xi32>) -> tensor<3xi32> {\n-+  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate [[CST0]], %arg0, dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n-+  %0 = stablehlo.concatenate %cst0, %cst0, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>\n-+  return %0 : tensor<3xi32>\n-+}\n-+\n-+// CHECK-LABEL: func.func @fold_concatenate_splat_trailing\n-+func.func @fold_concatenate_splat_trailing(%arg0: tensor<2xi32>) -> tensor<6xi32> {\n-+  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<4xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate %arg0, [[CST0]], dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<2xi32>\n-+  %0 = stablehlo.concatenate %arg0, %cst0, %cst0, dim = 0 : (tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<6xi32>\n-+  return %0 : tensor<6xi32>\n+-// CHECK-LABEL: func.func @case_fold_constant_branch_index\n+-func.func @case_fold_constant_branch_index(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {\n++// CHECK-LABEL: func.func @case_fold_constant_branch_index_int_result\n++func.func @case_fold_constant_branch_index_int_result(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {\n+   // CHECK-NEXT: {{(^ *|func\\.)}}return %arg1\n+   // CHECK-NOT:  stablehlo.case\n+   %branch_index = stablehlo.constant dense<1> : tensor<i32>\n+@@ -60,6 +60,47 @@\n+     stablehlo.return %arg2 : tensor<i32>\n+   }) : (tensor<i32>) -> tensor<i32>\n+   func.return %result: tensor<i32>\n +}\n +\n-+// CHECK-LABEL: func.func @fold_concatenate_splat_middle\n-+func.func @fold_concatenate_splat_middle(%arg0: tensor<1xi32>) -> tensor<4xi32> {\n-+  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate %arg0, [[CST0]], %arg0, dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n-+  %0 = stablehlo.concatenate %arg0, %cst0, %cst0, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>\n-+  return %0 : tensor<4xi32>\n-+}\n++// -----\n +\n-+// CHECK-LABEL: func.func @fold_concatenate_splat_multiple\n-+func.func @fold_concatenate_splat_multiple(%arg0: tensor<1xi32>) -> tensor<5xi32> {\n-+  // CHECK-DAG: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n-+  // CHECK-DAG: [[CST1:%.+]] = stablehlo.constant dense<1> : tensor<2xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate [[CST0]], [[CST1]], %arg0, dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n-+  %cst1 = stablehlo.constant dense<1> : tensor<1xi32>\n-+  %0 = stablehlo.concatenate %cst0, %cst0, %cst1, %cst1, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<5xi32>\n-+  return %0 : tensor<5xi32>\n++// CHECK-LABEL: func.func @case_fold_constant_branch_index_complex_result\n++func.func @case_fold_constant_branch_index_complex_result(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>, %arg2: tensor<complex<f32>>) -> tensor<complex<f32>> {\n++  // CHECK-NEXT: {{(^ *|func\\.)}}return %arg1\n++  // CHECK-NOT:  stablehlo.case\n++  %branch_index = stablehlo.constant dense<1> : tensor<i32>\n++  %result = \"stablehlo.case\"(%branch_index) ({\n++    stablehlo.return %arg0 : tensor<complex<f32>>\n++  }, {\n++    stablehlo.return %arg1 : tensor<complex<f32>>\n++  }, {\n++    stablehlo.return %arg2 : tensor<complex<f32>>\n++  }) : (tensor<i32>) -> tensor<complex<f32>>\n++  func.return %result: tensor<complex<f32>>\n +}\n +\n- // -----\n- \n- ////////\n-@@ -576,16 +614,19 @@\n- // ReshapeOp\n- \n- // CHECK-LABEL: func @reshape_fold\n--func.func @reshape_fold() -> (tensor<1xi32>, tensor<2x2xi32>) {\n--  %c0 = stablehlo.constant dense<2> : tensor<i32>\n-+func.func @reshape_fold() -> (tensor<1xf32>, tensor<2x2xi32>, tensor<3x2xcomplex<f32>>) {\n-+  %c0 = stablehlo.constant dense<2.0> : tensor<f32>\n-   %c1 = stablehlo.constant dense<[1, 2, 3, 4]> : tensor<4xi32>\n--  %0 = stablehlo.reshape %c0 : (tensor<i32>) -> tensor<1xi32>\n-+  %c2 = stablehlo.constant dense<(1.0,2.0)> : tensor<2x3xcomplex<f32>>\n-+  %0 = stablehlo.reshape %c0 : (tensor<f32>) -> tensor<1xf32>\n-   %1 = stablehlo.reshape %c1 : (tensor<4xi32>) -> tensor<2x2xi32>\n--\n--  // CHECK-DAG:  [[CST1:%.+]] = stablehlo.constant dense<2> : tensor<1xi32>\n--  // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\\[\\[1, 2\\], \\[3, 4\\]\\]}}> : tensor<2x2xi32>\n--  // CHECK-NEXT: return [[CST1]], [[CST2]]\n--  return %0, %1 : tensor<1xi32>, tensor<2x2xi32>\n-+  %2 = stablehlo.reshape %c2 : (tensor<2x3xcomplex<f32>>) -> tensor<3x2xcomplex<f32>>\n++// -----\n +\n-+  // CHECK-DAG:  [[RESULT0:%.+]] = stablehlo.constant dense<2.0{{.*}}> : tensor<1xf32>\n-+  // CHECK-DAG:  [[RESULT1:%.+]] = stablehlo.constant dense<{{\\[\\[1, 2\\], \\[3, 4\\]\\]}}> : tensor<2x2xi32>\n-+  // CHECK-DAG:  [[RESULT2:%.+]] = stablehlo.constant dense<(1.0{{.*}},2.0{{.*}})> : tensor<3x2xcomplex<f32>>\n-+  // CHECK-NEXT: return [[RESULT0]], [[RESULT1]], [[RESULT2]]\n-+  return %0, %1, %2 : tensor<1xf32>, tensor<2x2xi32>, tensor<3x2xcomplex<f32>>\n++// CHECK-LABEL: func.func @case_fold_inline_call_tf_function\n++func.func @case_fold_inline_call_tf_function(%arg0: !stablehlo.token {jax.token = true}, %arg1: tensor<16xi32>, %arg2: tensor<16xi64>) -> (!stablehlo.token {jax.token = true}, tensor<16xi32> {jax.result_info = \"result\"}) {\n++  // CHECK: [[RESULT_TOKEN:%.+]] = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2)\n++  // CHECK: [[UNUSED_TOKEN:%.+]] = {{\"?}}stablehlo.case{{\"?}}(\n++  // CHECK: return [[RESULT_TOKEN]], %arg1\n++  %c = stablehlo.constant dense<1> : tensor<i32>\n++  %c_0 = stablehlo.constant dense<0> : tensor<i32>\n++  %0 = \"stablehlo.case\"(%c_0) ({\n++    stablehlo.return %c_0 : tensor<i32>\n++  }, {\n++    stablehlo.return %c : tensor<i32>\n++  }) : (tensor<i32>) -> tensor<i32>\n++  %1 = \"stablehlo.case\"(%0) ({\n++    %2 = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_index = 0 : i64, has_token_input_output = true}} : (!stablehlo.token, tensor<16xi32>, tensor<16xi64>) -> !stablehlo.token\n++    stablehlo.return %2 : !stablehlo.token\n++  }, {\n++    %2 = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_index = 1 : i64, has_token_input_output = true}} : (!stablehlo.token, tensor<16xi32>, tensor<16xi64>) -> !stablehlo.token\n++    stablehlo.return %2 : !stablehlo.token\n++  }) : (tensor<i32>) -> !stablehlo.token\n++  return %1, %arg1 : !stablehlo.token, tensor<16xi32>\n  }\n  \n  // -----\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n-@@ -27,6 +27,14 @@\n-   // CHECK-NOT: stablehlo.constant\n-   // CHECK: return %arg0\n-   return %1 : tensor<f32>\n-+}\n-+\n-+// CHECK-LABEL: @add_cst_on_rhs_with_attrs\n-+func.func @add_cst_on_rhs_with_attrs(%arg0: tensor<f32>) -> tensor<f32> {\n-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n-+  // CHECK: stablehlo.add %arg0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  %0 = stablehlo.add %cst, %arg0 {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  return %0 : tensor<f32>\n- }\n- \n- // -----\n-@@ -120,6 +128,16 @@\n+@@ -128,6 +128,16 @@\n    return %7 : tensor<3x2x3x3xi32>\n  }\n  \n@@ -717,7 +336,7 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplific\n  // CHECK-LABEL: func.func @broadcast_in_dim_reshape\n  // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x6xi32>)\n  func.func @broadcast_in_dim_reshape(%arg0: tensor<3x6xi32>)\n-@@ -132,6 +150,15 @@\n+@@ -140,6 +150,15 @@\n  \n    // CHECK-NEXT: return [[R0]], [[R5]]\n    return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>\n@@ -733,580 +352,359 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplific\n  }\n  \n  // CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape\n-@@ -976,6 +1003,26 @@\n-   // CHECK-NOT: stablehlo.constant\n-   // CHECK: return %arg0 : tensor<f32>\n-   return %0 : tensor<f32>\n-+}\n-+\n-+// CHECK-LABEL: @multiply_by_one_merge_attrs\n-+func.func @multiply_by_one_merge_attrs(%arg0: tensor<f32>) -> tensor<f32> {\n-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n-+  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\"}} : tensor<f32>\n-+  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"1\"}} : tensor<f32>\n-+  // CHECK: return %[[ADD]] : tensor<f32>\n-+  return %1 : tensor<f32>\n-+}\n-+\n-+// CHECK-LABEL: @multiply_by_one_merge_attrs_conflict\n-+func.func @multiply_by_one_merge_attrs_conflict(%arg0: tensor<f32>) -> tensor<f32> {\n-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n-+  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"0\"}} : tensor<f32>\n-+  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"1\"}} : tensor<f32>\n-+  // CHECK: return %[[ADD]] : tensor<f32>\n-+  return %1 : tensor<f32>\n+diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n+--- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n++++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n+@@ -63,7 +63,8 @@\n+   // Get tensor type\n+   mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n+   if (!tensor_type)\n+-    return emitError(op.getLoc(), \"expected ranked tensor type\");\n++    return emitError(op.getLoc(),\n++                     \"expected ranked tensor type for broadcast inputs\");\n+ \n+   auto encoding =\n+       mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n+@@ -78,10 +79,11 @@\n+   return dimensions;\n  }\n  \n- // -----\n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n-@@ -9,6 +9,32 @@\n-   // CHECK: stablehlo.add %arg0, %cst : tensor<f32>\n-   %1 = stablehlo.add %0, %arg0 : tensor<f32>\n-   return %1 : tensor<f32>\n-+}\n-+\n-+// -----\n-+\n-+func.func @concatenate_fold_splat_flatten_integ(%arg0: tensor<8xf32>) -> tensor<64xf32> {\n-+  // CHECK-DAG: [[CST0:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<8xf32>\n-+  // CHECK-DAG: [[CST1:%.+]] = stablehlo.constant dense<1.000000e+00> : tensor<8xf32>\n-+  // CHECK-DAG: [[CST2:%.+]] = stablehlo.constant dense<2.000000e+00> : tensor<8xf32>\n-+  // CHECK-DAG: [[CST3:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<8xf32>\n-+  // CHECK: stablehlo.concatenate [[CST0]], [[CST1]], [[CST2]], [[CST3]], %arg0, %arg0, %arg0, %arg0,\n-+  %cst0 = stablehlo.constant dense<0.0> : tensor<f32>\n-+  %cst1 = stablehlo.constant dense<1.0> : tensor<f32>\n-+  %cst2 = stablehlo.constant dense<2.0> : tensor<f32>\n-+  %cst3 = stablehlo.constant dense<3.0> : tensor<f32>\n-+  %0 = stablehlo.reshape %cst0 : (tensor<f32>) -> tensor<1xf32>\n-+  %1 = stablehlo.reshape %cst1 : (tensor<f32>) -> tensor<1xf32>\n-+  %2 = stablehlo.reshape %cst2 : (tensor<f32>) -> tensor<1xf32>\n-+  %3 = stablehlo.reshape %cst3 : (tensor<f32>) -> tensor<1xf32>\n-+  %4 = stablehlo.concatenate %0, %0, %0, %0, %0, %0, %0, %0, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %5 = stablehlo.concatenate %1, %1, %1, %1, %1, %1, %1, %1, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %6 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %7 = stablehlo.concatenate %3, %3, %3, %3, %3, %3, %3, %3, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %8 = stablehlo.concatenate %4, %5, %6, %7, dim = 0 : (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> tensor<32xf32>\n-+  %9 = stablehlo.concatenate %arg0, %arg0, %arg0, %arg0, dim = 0 : (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> tensor<32xf32>\n-+  %10 = stablehlo.concatenate %8, %9, dim = 0 : (tensor<32xf32>, tensor<32xf32>) -> tensor<64xf32>\n-+  return %10 : tensor<64xf32>\n+-FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(const Dimensions& a,\n++FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(Value op,\n++                                                       const Dimensions& a,\n+                                                        const Dimensions& b) {\n+   LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] inputs: \"\n+-                          << toString(a) << \" * \" << toString(b));\n++                          << toString(a) << \" * \" << toString(b) << \"\\n\");\n+   size_t max_rank = std::max(a.size(), b.size());\n+   Dimensions result(max_rank);\n+ \n+@@ -110,14 +112,14 @@\n+ \n+     // If both LHS and RHS are not 1, dim size must match.\n+     if (dim_a.size != dim_b.size) {\n+-      return emitError(a[a_idx].boundOp.value().getLoc(),\n+-                       \"incompatible shapes for broadcasting \")\n++      // FIXME\n++      return emitError(op.getLoc(), \"incompatible shapes for broadcasting \")\n+              << dim_a.size << \" and \" << dim_b.size;\n+     }\n+ \n+     // If bounded both must be bounded\n+     if (dim_a.boundOp.has_value() != dim_b.boundOp.has_value()) {\n+-      return emitError(a[a_idx].boundOp.value().getLoc(),\n++      return emitError(op.getLoc(),\n+                        \"cannot mix bounded and static dimensions in broadcast\");\n+     }\n+ \n+@@ -126,7 +128,7 @@\n+   }\n+ \n+   LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n+-                          << toString(result));\n++                          << toString(result) << \"\\n\");\n+   return result;\n  }\n  \n- // -----\n-diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n---- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-+++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-@@ -0,0 +1,293 @@\n-+/* Copyright 2025 The StableHLO Authors.\n-+\n-+Licensed under the Apache License, Version 2.0 (the \"License\");\n-+you may not use this file except in compliance with the License.\n-+You may obtain a copy of the License at\n-+\n-+    http://www.apache.org/licenses/LICENSE-2.0\n-+\n-+Unless required by applicable law or agreed to in writing, software\n-+distributed under the License is distributed on an \"AS IS\" BASIS,\n-+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-+See the License for the specific language governing permissions and\n-+limitations under the License.\n-+==============================================================================*/\n-+\n-+#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n-+\n-+#include <algorithm>\n-+#include <cassert>\n-+#include <cstddef>\n-+#include <cstdint>\n-+#include <string>\n-+#include <utility>\n-+\n-+#include \"llvm/ADT/STLExtras.h\"\n-+#include \"llvm/ADT/Sequence.h\"\n-+#include \"llvm/ADT/SmallVector.h\"\n-+#include \"llvm/Support/Debug.h\"\n-+#include \"llvm/Support/raw_ostream.h\"\n-+#include \"mlir/IR/Builders.h\"\n-+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n-+#include \"mlir/IR/BuiltinTypes.h\"\n-+#include \"mlir/IR/Diagnostics.h\"\n-+#include \"mlir/IR/Location.h\"\n-+#include \"mlir/IR/Types.h\"\n-+#include \"mlir/IR/Value.h\"\n-+#include \"mlir/Support/LLVM.h\"\n-+#include \"stablehlo/dialect/StablehloOps.h\"\n-+\n-+#define DEBUG_TYPE \"stablehlo-broadcast-lowering\"\n-+\n-+namespace mlir {\n-+namespace stablehlo {\n-+\n-+/////\n-+// Bounded dynamism broadcasting\n-+\n-+namespace {\n-+\n-+DimensionInfo getDimensionInfo(Value op, mlir::RankedTensorType tensorType,\n-+                               TypeExtensionsAttr encoding,\n-+                               int64_t dim) {\n-+  if (!encoding || !mlir::ShapedType::isDynamic(tensorType.getDimSize(dim)))\n-+    return DimensionInfo{tensorType.getDimSize(dim)};\n-+\n-+  return DimensionInfo{\n-+      encoding.getBounds()[dim],\n-+      op,\n-+      dim,\n-+  };\n-+}\n-+\n-+FailureOr<Dimensions> getDimensions(Value op) {\n-+  // Get tensor type\n-+  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n-+  if (!tensor_type)\n-+    return emitError(op.getLoc(), \"expected ranked tensor type\");\n-+\n-+  auto encoding =\n-+      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n-+          tensor_type.getEncoding());\n-+\n-+  Dimensions dimensions;\n-+  dimensions.reserve(tensor_type.getRank());\n-+  for (size_t idx = 0; idx < tensor_type.getRank(); ++idx) {\n-+    auto dimInfo = getDimensionInfo(op, tensor_type, encoding, idx);\n-+    dimensions.push_back(dimInfo);\n-+  }\n-+  return dimensions;\n-+}\n-+\n-+FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(\n-+    const Dimensions& a, const Dimensions& b) {\n-+  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] inputs: \"\n-+                          << toString(a) << \" * \" << toString(b));\n-+  size_t max_rank = std::max(a.size(), b.size());\n-+  Dimensions result(max_rank);\n-+\n-+  // Iterate from right to left (NumPy-style broadcasting)\n-+  for (int i = 1; i <= max_rank; ++i) {\n-+    size_t a_idx = a.size() - i;\n-+    size_t b_idx = b.size() - i;\n-+    size_t res_idx = max_rank - i;\n-+\n-+    // Get DimensionInfo for the current index, padding with size 1 if out of\n-+    // bounds.\n-+    DimensionInfo dim_a =\n-+        (a_idx >= 0 && a_idx < a.size()) ? a[a_idx] : DimensionInfo{1};\n-+    DimensionInfo dim_b =\n-+        (b_idx >= 0 && b_idx < b.size()) ? b[b_idx] : DimensionInfo{1};\n-+\n-+    // Short circuit on size 1 dimensions.\n-+    if (dim_a.size == 1) {\n-+      result[res_idx] = dim_b;\n-+      continue;\n-+    }\n-+    if (dim_b.size == 1) {\n-+      result[res_idx] = dim_a;\n-+      continue;\n-+    }\n-+\n-+    // If both LHS and RHS are not 1, dim size must match.\n-+    if (dim_a.size != dim_b.size) {\n-+      return emitError(a[a_idx].boundOp.value().getLoc(),\n-+                       \"incompatible shapes for broadcasting \")\n-+             << dim_a.size << \" and \" << dim_b.size;\n-+    }\n-+\n-+    // If bounded both must be bounded\n-+    if (dim_a.boundOp.has_value() != dim_b.boundOp.has_value()) {\n-+      return emitError(a[a_idx].boundOp.value().getLoc(),\n-+                       \"cannot mix bounded and static dimensions in broadcast\");\n-+    }\n-+\n-+    // LHS and RHS match, populate with one of the dimensions.\n-+    result[res_idx] = dim_a;\n-+  }\n-+\n-+  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n-+                          << toString(result));\n-+  return result;\n-+}\n-+\n-+mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n-+                                           mlir::Type element_type) {\n-+  mlir::SmallVector<int64_t> shape;\n-+  mlir::SmallVector<int64_t> bounds;\n-+  shape.reserve(dims.size());\n-+  for (const DimensionInfo& dim : dims) {\n-+    if (dim.boundOp.has_value()) {\n-+      shape.push_back(mlir::ShapedType::kDynamic);\n-+      bounds.push_back(dim.size);\n-+    } else {\n-+      shape.push_back(dim.size);\n-+      bounds.push_back(mlir::ShapedType::kDynamic);\n-+    }\n-+  }\n-+  mlir::stablehlo::TypeExtensionsAttr encoding;\n-+  if (!llvm::all_of(\n-+          bounds, [](int64_t b) { return b == mlir::ShapedType::kDynamic; })) {\n-+    encoding = mlir::stablehlo::TypeExtensionsAttr::get(\n-+        element_type.getContext(), bounds);\n-+  }\n-+  return mlir::RankedTensorType::get(shape, element_type, encoding);\n-+}\n-+\n-+}  // namespace\n-+\n-+\n-+FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops) {\n-+  if (ops.empty()) return failure();\n-+\n-+  Value first = ops[0];\n-+  auto bcastShapeOrFail = getDimensions(first);\n-+  if (failed(bcastShapeOrFail)) return failure();\n-+  Dimensions bcastShape = std::move(*bcastShapeOrFail);\n-+\n-+  for (int i = 1; i < ops.size(); ++i) {\n-+    Value currOp = ops[i];\n-+    auto dims = getDimensions(currOp);\n-+    if (failed(dims)) return failure();\n-+    auto currBcastShapeOrFail =\n-+        getNumpyBroadcastShapeWithBounds(bcastShape, *dims);\n-+    if (failed(currBcastShapeOrFail)) return failure();\n-+    bcastShape = std::move(*currBcastShapeOrFail);\n-+  }\n-+  return std::move(bcastShape);\n-+}\n-+\n-+std::string toString(const Dimensions& dims) {\n-+  std::string result;\n-+  llvm::raw_string_ostream os(result);\n-+  os << \"tensor<\";\n-+  llvm::interleave(\n-+      dims, os,\n-+      [&](const DimensionInfo& dim) {\n-+        os << (dim.boundOp.has_value() ? \"b\" : \"\") << dim.size;\n-+      },\n-+      \"x\");\n-+  os << \">\";\n-+  return result;\n-+}\n-+\n-+FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n-+                                                     ArrayRef<Value> operands) {\n-+  // Figure out the broadcast shape\n-+  auto bcastShapeOrFail = getNumpyBroadcastShape(operands);\n-+  if (failed(bcastShapeOrFail)) return failure();\n-+  Dimensions bcastShape = std::move(*bcastShapeOrFail);\n-+\n-+  // Apply to all operands\n-+  SmallVector<Value> broadcastedOperands;\n-+  for (auto operand : operands) {\n-+    auto bcastOperand = numpyBroadcastIfNeeded(builder, operand, bcastShape);\n-+    if (failed(bcastOperand)) return failure();\n-+    broadcastedOperands.push_back(*bcastOperand);\n-+  }\n-+  return std::move(broadcastedOperands);\n-+}\n-+\n-+FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n-+                                        const Dimensions& shape) {\n-+  LLVM_DEBUG(llvm::dbgs() << \"[BroadcastIfNeeded] input: \" << input\n-+                          << \" shape: \" << toString(shape));\n-+  auto loc = input.getLoc();\n-+  mlir::RankedTensorType input_type =\n-+      dyn_cast<RankedTensorType>(input.getType());\n-+  if (!input_type) return emitError(input.getLoc(), \"expected tensor type\");\n-+  mlir::RankedTensorType output_type =\n-+      getRankedTensorType(shape, input_type.getElementType());\n-+\n-+  // Short circuit if no broadcasting is needed.\n-+  if (input_type == output_type) return input;\n-+\n-+  int64_t input_rank = input_type.getRank();\n-+  int64_t output_rank = output_type.getRank();\n-+  if (input_rank > output_rank)\n-+    return emitError(loc, \"input rank must be <= output rank, got \")\n-+           << input_rank << \" vs \" << output_rank;\n-+\n-+  size_t rank_diff = output_rank - input_rank;\n-+  SmallVector<int64_t> bcast_dims;\n-+  bcast_dims.reserve(input_rank);\n-+\n-+  auto inputShapeOrFail = getDimensions(input);\n-+  if (failed(inputShapeOrFail)) return failure();\n-+  Dimensions inputShape = std::move(*inputShapeOrFail);\n-+\n-+  // Construct broadcast dimensions.\n-+  auto broadcastDimensions = llvm::to_vector(\n-+      llvm::seq<int64_t>(output_rank - input_rank, output_rank));\n-+\n-+  // Construct the result type of the broadcast\n-+  //  - If input is static and target shape is static, use static shape.\n-+  //  - If input has bounded dim, target shape must be bounded, use bounded dim.\n-+  //  - If input is not bounded, but target shape is bounded, broadcast to\n-+  //    the padded shape then call SetDimensionSize to make dynamic.\n-+  auto bcastShape = shape;\n-+  for (size_t i = 0; i < input_rank; ++i) {\n-+    int64_t input_dim_size = inputShape[i].size;\n-+    int64_t result_idx = i + rank_diff;\n-+    int64_t result_dim_size = shape[result_idx].size;\n-+    if (input_dim_size != 1 && input_dim_size != result_dim_size)\n-+      return emitError(loc, \"Cannot broadcast input: \")\n-+             << input_type << \" to target shape \" << toString(shape);\n-+\n-+    if (!inputShape[i].boundOp.has_value() &&\n-+        shape[result_idx].boundOp.has_value()) {\n-+      // Use padded shape in broadcast.\n-+      bcastShape[result_idx] = DimensionInfo{shape[result_idx].size};\n+@@ -155,8 +157,11 @@\n+ \n+ }  // namespace\n+ \n+-FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops) {\n+-  if (ops.empty()) return failure();\n++FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n++                                             ArrayRef<Value> ops) {\n++  if (ops.empty())\n++    return emitError(builder.getInsertionPoint()->getLoc(),\n++                     \"requires at least one operand to broadcast\");\n+ \n+   Value first = ops[0];\n+   auto bcastShapeOrFail = getDimensions(first);\n+@@ -168,7 +173,7 @@\n+     auto dims = getDimensions(currOp);\n+     if (failed(dims)) return failure();\n+     auto currBcastShapeOrFail =\n+-        getNumpyBroadcastShapeWithBounds(bcastShape, *dims);\n++        getNumpyBroadcastShapeWithBounds(currOp, bcastShape, *dims);\n+     if (failed(currBcastShapeOrFail)) return failure();\n+     bcastShape = std::move(*currBcastShapeOrFail);\n+   }\n+@@ -192,7 +197,7 @@\n+ FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n+                                                      ArrayRef<Value> operands) {\n+   // Figure out the broadcast shape\n+-  auto bcastShapeOrFail = getNumpyBroadcastShape(operands);\n++  auto bcastShapeOrFail = getNumpyBroadcastShape(builder, operands);\n+   if (failed(bcastShapeOrFail)) return failure();\n+   Dimensions bcastShape = std::move(*bcastShapeOrFail);\n+ \n+@@ -208,35 +213,34 @@\n+ \n+ FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n+                                         const Dimensions& shape) {\n+-  LLVM_DEBUG(llvm::dbgs() << \"[BroadcastIfNeeded] input: \" << input\n+-                          << \" shape: \" << toString(shape));\n++  LLVM_DEBUG(llvm::dbgs() << \"[numpyBroadcastIfNeeded] Broadcasting input \"\n++                          << input.getType() << \" => \" << toString(shape)\n++                          << \"\\n\");\n+   auto loc = input.getLoc();\n+-  mlir::RankedTensorType input_type =\n++  mlir::RankedTensorType inputType =\n+       dyn_cast<RankedTensorType>(input.getType());\n+-  if (!input_type) return emitError(input.getLoc(), \"expected tensor type\");\n+-  mlir::RankedTensorType output_type =\n+-      getRankedTensorType(shape, input_type.getElementType());\n++  if (!inputType)\n++    return emitError(loc, \"expected ranked tensor type for broadcast inputs\");\n++  mlir::RankedTensorType outputType =\n++      getRankedTensorType(shape, inputType.getElementType());\n+ \n+   // Short circuit if no broadcasting is needed.\n+-  if (input_type == output_type) return input;\n+-\n+-  int64_t input_rank = input_type.getRank();\n+-  int64_t output_rank = output_type.getRank();\n+-  if (input_rank > output_rank)\n++  if (inputType == outputType) return input;\n++\n++  int64_t inputRank = inputType.getRank();\n++  int64_t outputRank = outputType.getRank();\n++  if (inputRank > outputRank)\n+     return emitError(loc, \"input rank must be <= output rank, got \")\n+-           << input_rank << \" vs \" << output_rank;\n+-\n+-  size_t rank_diff = output_rank - input_rank;\n+-  SmallVector<int64_t> bcast_dims;\n+-  bcast_dims.reserve(input_rank);\n+-\n++           << inputRank << \" vs \" << outputRank;\n++\n++  size_t rankDiff = outputRank - inputRank;\n+   auto inputShapeOrFail = getDimensions(input);\n+   if (failed(inputShapeOrFail)) return failure();\n+   Dimensions inputShape = std::move(*inputShapeOrFail);\n+ \n+   // Construct broadcast dimensions.\n+   auto broadcastDimensions = llvm::to_vector(\n+-      llvm::seq<int64_t>(output_rank - input_rank, output_rank));\n++      llvm::seq<int64_t>(outputRank - inputRank, outputRank));\n+ \n+   // Construct the result type of the broadcast\n+   //  - If input is static and target shape is static, use static shape.\n+@@ -244,33 +248,35 @@\n+   //  - If input is not bounded, but target shape is bounded, broadcast to\n+   //    the padded shape then call SetDimensionSize to make dynamic.\n+   auto bcastShape = shape;\n+-  for (int64_t i = 0; i < input_rank; ++i) {\n+-    int64_t input_dim_size = inputShape[i].size;\n+-    int64_t result_idx = i + rank_diff;\n+-    int64_t result_dim_size = shape[result_idx].size;\n+-    if (input_dim_size != 1 && input_dim_size != result_dim_size)\n++  for (int64_t i = 0; i < inputRank; ++i) {\n++    int64_t inputDimSize = inputShape[i].size;\n++    int64_t resultIdx = i + rankDiff;\n++    int64_t resultDimSize = shape[resultIdx].size;\n++    if (inputDimSize != 1 && inputDimSize != resultDimSize)\n+       return emitError(loc, \"Cannot broadcast input: \")\n+-             << input_type << \" to target shape \" << toString(shape);\n++             << inputType << \" to target shape \" << toString(shape);\n+ \n+     if (!inputShape[i].boundOp.has_value() &&\n+-        shape[result_idx].boundOp.has_value()) {\n++        shape[resultIdx].boundOp.has_value()) {\n+       // Use padded shape in broadcast.\n+-      bcastShape[result_idx] = DimensionInfo{shape[result_idx].size};\n+-    }\n+-    bcast_dims.push_back(result_idx);\n++      bcastShape[resultIdx] = DimensionInfo{shape[resultIdx].size};\n +    }\n-+    bcast_dims.push_back(result_idx);\n-+  }\n-+\n-+  // Broadcast to padded size for remaining dimensions.\n-+  for (size_t i = input_rank; i < shape.size(); ++i) {\n-+    bcastShape[i] = DimensionInfo{shape[i].size};\n-+  }\n-+\n-+  // Insert broadcast ops\n-+  mlir::RankedTensorType bcast_type =\n-+      getRankedTensorType(bcastShape, input_type.getElementType());\n-+  Value bcast_op = stablehlo::BroadcastInDimOp::create(\n-+      builder, loc, bcast_type, input, broadcastDimensions);\n-+  if (bcast_op.getType() == output_type) return bcast_op;\n-+\n-+  // Mark the padded broadcast as dynamic where the result is bounded.\n-+  // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n-+  // dimensions that required broadcasting.\n-+  for (size_t i = 0; i < shape.size(); ++i) {\n-+    if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n-+      Value boundOp = shape[i].boundOp.value();\n-+      auto dim_size = stablehlo::GetDimensionSizeOp::create(\n-+          builder, loc, boundOp, shape[i].boundOpDim);\n-+      bcast_op = stablehlo::SetDimensionSizeOp::create(builder, loc, bcast_op,\n-+                                                       dim_size, i);\n+   }\n+ \n+   // Broadcast to padded size for remaining dimensions.\n+-  for (size_t i = input_rank; i < shape.size(); ++i) {\n++  for (size_t i = 0; i < rankDiff; ++i) {\n+     bcastShape[i] = DimensionInfo{shape[i].size};\n+   }\n+ \n+   // Insert broadcast ops\n+-  mlir::RankedTensorType bcast_type =\n+-      getRankedTensorType(bcastShape, input_type.getElementType());\n+-  Value bcast_op = stablehlo::BroadcastInDimOp::create(\n+-      builder, loc, bcast_type, input, broadcastDimensions);\n+-  if (bcast_op.getType() == output_type) return bcast_op;\n++  mlir::RankedTensorType bcastType =\n++      getRankedTensorType(bcastShape, inputType.getElementType());\n++  LLVM_DEBUG(\n++      llvm::dbgs() << \"[numpyBroadcastIfNeeded] Broadcast to padded type \"\n++                   << bcastType << \"\\n\");\n++  Value bcastOp = stablehlo::BroadcastInDimOp::create(\n++      builder, loc, bcastType, input, broadcastDimensions);\n++  if (bcastOp.getType() == outputType) return bcastOp;\n+ \n+   // Mark the padded broadcast as dynamic where the result is bounded.\n+   // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n+@@ -278,13 +284,13 @@\n+   for (size_t i = 0; i < shape.size(); ++i) {\n+     if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n+       Value boundOp = shape[i].boundOp.value();\n+-      auto dim_size = stablehlo::GetDimensionSizeOp::create(\n++      auto dimSize = stablehlo::GetDimensionSizeOp::create(\n+           builder, loc, boundOp, shape[i].boundOpDim);\n+-      bcast_op = stablehlo::SetDimensionSizeOp::create(builder, loc, bcast_op,\n+-                                                       dim_size, i);\n+-    }\n+-  }\n+-  return bcast_op;\n++      bcastOp = stablehlo::SetDimensionSizeOp::create(builder, loc, bcastOp,\n++                                                       dimSize, i);\n +    }\n +  }\n-+  return bcast_op;\n-+}\n-+\n-+}  // namespace stablehlo\n-+}  // namespace mlir\n++  return bcastOp;\n+ }\n+ \n+ }  // namespace stablehlo\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n-@@ -0,0 +1,68 @@\n-+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-+   Copyright 2022 The StableHLO Authors.\n-+\n-+Licensed under the Apache License, Version 2.0 (the \"License\");\n-+you may not use this file except in compliance with the License.\n-+You may obtain a copy of the License at\n-+\n-+    http://www.apache.org/licenses/LICENSE-2.0\n-+\n-+Unless required by applicable law or agreed to in writing, software\n-+distributed under the License is distributed on an \"AS IS\" BASIS,\n-+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-+See the License for the specific language governing permissions and\n-+limitations under the License.\n-+==============================================================================*/\n-+\n-+\n-+#ifndef STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n-+#define STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n-+\n-+#include <cstdint>\n-+#include <optional>\n-+#include <string>\n-+\n-+#include \"mlir/IR/Builders.h\"\n-+#include \"mlir/IR/Value.h\"\n-+#include \"mlir/Support/LLVM.h\"\n-+\n-+namespace mlir {\n-+namespace stablehlo {\n-+\n-+///////\n-+// Numpy broadcasting with support for bounded dynamism.\n-+\n-+// Struct that represents a dim size of a tensor and possible dynamic value to\n-+// match. If dimension is not dynamic, bound_op is set to std::nullopt. If\n-+// dimension is bounded, the resulting dimension should be padded to `size` then\n-+// marked dynamic using:\n-+//   runtime_size = get_dimension_size(bound_op, dim=bound_op_dim)\n-+//   T = set_dimension_size(T, dim=bound_op_dim, runtime_size)\n-+//\n-+struct DimensionInfo {\n-+  int64_t size;\n-+  std::optional<Value> boundOp = std::nullopt;\n-+  int64_t boundOpDim = -1;\n-+};\n-+\n-+using Dimensions = SmallVector<DimensionInfo>;\n-+std::string toString(const Dimensions& dims);\n-+\n-+// Returns the common shape these ops would broadcast to, or an error if the\n-+// ops are not broadcastable.\n-+FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops);\n-+\n-+// Apply numpy broadcasting to the given operands, returning an error if any\n-+// operands are not broadcastable.\n-+FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n-+                                                     ArrayRef<Value> operands);\n-+\n-+// Apply numpy broadcasting to the given operand, returning an error if the\n-+// operand is not broadcastable.\n-+FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n-+                                        const Dimensions& shape);\n-+\n-+}  // namespace stablehlo\n-+}  // namespace mlir\n-+\n-+#endif  // STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n+@@ -49,7 +49,8 @@\n+ \n+ // Returns the common shape these ops would broadcast to, or an error if the\n+ // ops are not broadcastable.\n+-FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops);\n++FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n++                                             ArrayRef<Value> ops);\n+ \n+ // Apply numpy broadcasting to the given operands, returning an error if any\n+ // operands are not broadcastable.\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -822,6 +822,61 @@\n-   int64_t foldOpElementLimit;\n- };\n- \n-+// Pattern: concat(splat_a, splat_a, X) -> concat(splat_a_resize, X)\n-+struct FoldConcatenateAdjacentSplatsOpPattern final\n-+    : ShapeOpRewritePattern<mlir::stablehlo::ConcatenateOp> {\n-+  using ShapeOpRewritePattern::ShapeOpRewritePattern;\n-+\n-+  LogicalResult matchAndRewrite(ConcatenateOp op,\n-+                                PatternRewriter& rewriter) const override {\n-+    SmallVector<Value> newOperands;\n-+    SplatElementsAttr currSplat;\n-+    for (size_t i = 0; i < op.getNumOperands(); ++i) {\n-+      Value operand = op.getOperand(i);\n-+      // Match a splat and look ahead for adjacent identical splats.\n-+      if (matchPattern(operand, m_Constant(&currSplat)) && currSplat) {\n-+        size_t j = i+1;\n-+        SplatElementsAttr lookaheadSplat;\n-+        int64_t nOccurrences = 1;\n-+        for (; j < op.getNumOperands(); ++j) {\n-+          if (matchPattern(op.getOperand(j), m_Constant(&lookaheadSplat)) &&\n-+              lookaheadSplat && lookaheadSplat == currSplat) {\n-+            ++nOccurrences;\n-+            continue;\n-+          }\n-+          break;\n-+        }\n-+\n-+        // Special case for a single occurrence, no new constants\n-+        if (nOccurrences == 1) {\n-+          newOperands.push_back(operand);\n-+          continue;\n-+        }\n-+\n-+        // Resize the splat and append it to the new operands.\n-+        SmallVector<int64_t> newShape =\n-+            llvm::to_vector(currSplat.getType().getShape());\n-+        newShape[op.getDimension()] *= nOccurrences;\n-+        newOperands.push_back(ConstantOp::create(\n-+            rewriter, op.getLoc(),\n-+            currSplat.resizeSplat(currSplat.getType().clone(newShape))));\n+@@ -14,6 +14,7 @@\n+ \n+ #include <cassert>\n+ #include <cmath>\n++#include <complex>\n+ #include <cstddef>\n+ #include <cstdint>\n+ #include <functional>\n+@@ -38,6 +39,7 @@\n+ #include \"mlir/Dialect/CommonFolders.h\"\n+ #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+ #include \"mlir/Dialect/Utils/IndexingUtils.h\"\n++#include \"mlir/IR/Builders.h\"\n+ #include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n+ #include \"mlir/IR/BuiltinAttributes.h\"\n+ #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+@@ -82,6 +84,71 @@\n+                 /*isUnsigned=*/!isSigned);\n+ }\n+ \n++class LazyPlaceholderValue {\n++ public:\n++  static FailureOr<LazyPlaceholderValue> preparePlaceholderFor(\n++      PatternRewriter& rewriter, Value likeValue) {\n++    Type valueType = likeValue.getType();\n++\n++    // If `getZeroAttr(valueType)` returns a valid attribute, simply wrap the\n++    // result in a `stablehlo.constant` op.\n++    if (TypedAttr placeholderAttr = rewriter.getZeroAttr(valueType)) {\n++      return LazyPlaceholderValue([&rewriter, placeholderAttr](Location loc) {\n++        return ConstantOp::create(rewriter, loc, placeholderAttr);\n++      });\n++    }\n +\n-+        // Set `i` to j-1 so that next iteration processes the next operand.\n-+        i = j - 1;\n-+        continue;\n++    // `getZeroAttr` doesn't support complex types, so we handle that case here.\n++    if (auto shapedType = dyn_cast<ShapedType>(valueType)) {\n++      if (auto complexElementType =\n++              dyn_cast<ComplexType>(shapedType.getElementType())) {\n++        if (!isa<FloatType>(complexElementType.getElementType()))\n++          return rewriter.notifyMatchFailure(\n++              likeValue.getLoc(),\n++              \"unexpected real component type for complex element type\");\n++        auto realImagComponentFloatType =\n++            cast<FloatType>(complexElementType.getElementType());\n++        APFloat apFloatZero(0.0);\n++        bool losesInfo;\n++        apFloatZero.convert(realImagComponentFloatType.getFloatSemantics(),\n++                            llvm::RoundingMode::NearestTiesToEven, &losesInfo);\n++        std::complex<APFloat> complexZeroScalar(apFloatZero, apFloatZero);\n++        auto complexZeroSplat =\n++            SplatElementsAttr::get(shapedType, complexZeroScalar);\n++        return LazyPlaceholderValue(\n++            [&rewriter, complexZeroSplat](Location loc) {\n++              return ConstantOp::create(rewriter, loc, complexZeroSplat);\n++            });\n +      }\n-+      // Not splat, append the operand.\n-+      newOperands.push_back(operand);\n +    }\n-+    if (newOperands.size() == op.getNumOperands()) {\n-+      return rewriter.notifyMatchFailure(op, \"No splats to fold\");\n-+    }\n-+    rewriter.replaceOpWithNewOp<ConcatenateOp>(op, op.getType(), newOperands,\n-+                                               op.getDimension());\n-+    return success();\n-+  }\n-+};\n +\n- struct FoldConvertOpPattern : public ShapeOpRewritePattern<ConvertOp> {\n-   using ShapeOpRewritePattern::ShapeOpRewritePattern;\n- \n-@@ -1108,7 +1163,8 @@\n-                                 PatternRewriter& rewriter) const override {\n-     auto resultType = op.getType();\n-     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||\n--        failed(validateShapeFoldDtype(rewriter, op, resultType)))\n-+        failed(validateShapeFoldDtype(rewriter, op, resultType,\n-+                                      /*allowComplex=*/true)))\n-       return failure();\n- \n-     DenseElementsAttr attr;\n-@@ -1923,6 +1979,8 @@\n-   patterns->add<FoldClampOpPattern>(context, options, benefit);\n-   patterns->add<FoldCompareOpPattern>(context, options, benefit);\n-   patterns->add<FoldConcatenateOpPattern>(context, options, benefit);\n-+  patterns->add<FoldConcatenateAdjacentSplatsOpPattern>(context, options,\n-+                                                        benefit);\n-   patterns->add<FoldConvertOpPattern>(context, options, benefit);\n-   patterns->add<FoldDivOpPattern>(context, options, benefit);\n-   patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n-@@ -69,6 +69,54 @@\n-   });\n- }\n- \n-+bool mergeDiscardableAttributes(ValueRange sourceValues,\n-+                                ValueRange destValues) {\n-+  if (sourceValues.size() != destValues.size()) return false;\n-+  bool changed = false;\n-+  for (auto [source, dest] : llvm::zip(sourceValues, destValues)) {\n-+    if (mergeDiscardableAttributes(source, dest)) changed = true;\n-+  }\n-+  return changed;\n-+}\n-+\n-+bool mergeDiscardableAttributes(Value sourceValue, Value destValue) {\n-+  Operation* sourceOp = sourceValue.getDefiningOp();\n-+  Operation* destOp = destValue.getDefiningOp();\n-+  if (!sourceOp || !destOp) return false;\n++    // If `valueType` is a token type, use `stablehlo.after_all` with no\n++    // arguments to create a placeholder token.\n++    if (isa<TokenType>(valueType)) {\n++      return LazyPlaceholderValue([&rewriter](Location loc) {  //\n++        return AfterAllOp::create(rewriter, loc, {});\n++      });\n++    }\n +\n-+  auto sourceAttrs = sourceOp->getDiscardableAttrDictionary();\n-+  if (!sourceAttrs) return true;\n++    // TODO: Support quantized and buffer types.\n +\n-+  auto destAttrs = destOp->getDiscardableAttrDictionary();\n-+  if (!destAttrs) {\n-+    destOp->setDiscardableAttrs(sourceAttrs);\n-+    return true;\n++    return rewriter.notifyMatchFailure(\n++        likeValue.getLoc(), \"unable to create placeholder value for type\");\n +  }\n +\n-+  NamedAttrList mergedAttrs(destAttrs);\n-+  for (auto attr : sourceAttrs.getValue()) {\n-+    if (attr.getName() == \"mhlo.frontend_attributes\" &&\n-+        mergedAttrs.get(\"mhlo.frontend_attributes\")) {\n-+      // Merge frontend attributes, prioritizing source attributes.\n-+      auto destFrontendAttrs =\n-+          cast<DictionaryAttr>(mergedAttrs.get(\"mhlo.frontend_attributes\"));\n-+      auto sourceFrontendAttrs = cast<DictionaryAttr>(attr.getValue());\n-+      NamedAttrList frontendAttrs(destFrontendAttrs);\n-+      for (auto sourceAttr : sourceFrontendAttrs) {\n-+        frontendAttrs.set(sourceAttr.getName(), sourceAttr.getValue());\n-+      }\n-+      mergedAttrs.set(\"mhlo.frontend_attributes\",\n-+                      frontendAttrs.getDictionary(destOp->getContext()));\n-+    } else {\n-+      // Otherwise prioritize source attributes\n-+      mergedAttrs.set(attr.getName(), attr.getValue());\n-+    }\n++  Value createAt(Location loc) const {\n++    if (!lazyInitializer)\n++      llvm::report_fatal_error(\"No lazy initializer for this value type.\");\n++    return lazyInitializer(loc);\n +  }\n +\n-+  destOp->setDiscardableAttrs(mergedAttrs.getDictionary(destOp->getContext()));\n-+  return true;\n-+}\n++ private:\n++  LazyPlaceholderValue(std::function<Value(Location)> lazyInitializer)\n++      : lazyInitializer(std::move(lazyInitializer)) {}\n +\n- template <typename OpType>\n- struct SimplifyOpRewritePattern : OpRewritePattern<OpType> {\n-   SimplifyOpRewritePattern(\n++  std::function<Value(Location)> lazyInitializer;\n++};\n++\n+ LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,\n+                                         Operation* op, ShapedType resultType) {\n+   if (!resultType.hasStaticShape())\n+@@ -737,18 +804,14 @@\n+     Operation* terminator = blockToInline->getTerminator();\n+     ValueRange results = terminator->getOperands();\n+ \n+-    // TODO: Add support for complex, quantized, and token return types.\n+-    // Currently, this pattern only supports int and float return types. We'll\n+-    // need a more general equivalent of `getZeroAttr` to support other types.\n+-    SmallVector<TypedAttr> placeholderAttrs;\n++    SmallVector<LazyPlaceholderValue> lazyPlaceholderResults;\n+     for (auto result : op.getResults()) {\n+-      TypedAttr placeholderAttr = rewriter.getZeroAttr(result.getType());\n+-      if (!placeholderAttr)\n+-        return rewriter.notifyMatchFailure(\n+-            op,\n+-            \"The case op's return type isn't currently supported by this \"\n+-            \"optimization pattern.\");\n+-      placeholderAttrs.push_back(placeholderAttr);\n++      auto placeholder =\n++          LazyPlaceholderValue::preparePlaceholderFor(rewriter, result);\n++\n++      if (failed(placeholder)) return failure();\n++\n++      lazyPlaceholderResults.push_back(std::move(placeholder.value()));\n+     }\n+ \n+     // Inline the active branch of the `case` op.\n+@@ -763,9 +826,9 @@\n+     Block& noopBlock = region.emplaceBlock();\n+     SmallVector<Value> placeholderResults;\n+     rewriter.setInsertionPointToEnd(&noopBlock);\n+-    for (auto placeholderAttr : placeholderAttrs) {\n++    for (const auto& lazyPlaceholderResult : lazyPlaceholderResults) {\n+       placeholderResults.push_back(\n+-          ConstantOp::create(rewriter, region.getLoc(), placeholderAttr));\n++          lazyPlaceholderResult.createAt(region.getLoc()));\n+     }\n+     stablehlo::ReturnOp::create(rewriter, region.getLoc(), placeholderResults);\n+ \n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n@@ -1320,70 +718,83 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n            \"!(\"\n              \"llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && \"\n              \"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\"\n-@@ -134,6 +135,8 @@\n+@@ -134,8 +135,7 @@\n  \n  def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n  \n+-def MergeDiscardableAttributes\n+-    : NativeCodeCall<\"mergeDiscardableAttributes($0, $1)\">;\n +def MergeDiscardableAttributes : NativeCodeCall<\"mergeDiscardableAttributes($0, $1)\">;\n-+\n+ \n  def StableHLO_ConvertOpWithShape : NativeCodeCall<\n      \"stablehlo::ConvertOp::create($_builder, $_loc, $0.getType(), $1)\">;\n+@@ -151,10 +151,10 @@\n  \n-@@ -149,8 +152,9 @@\n  // op(cst, X) -> op(X, cst)\n  class CanonicalizeConstantToRhs<Op StableHLO_OpType>\n-   : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n--        (StableHLO_OpType $rhs, $lhs),\n--        [(NotConstantOp $rhs), (CommutativeOp $op)]>;\n+-    : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n+-          (StableHLO_OpType:$new_op $rhs, $lhs),\n+-          [(NotConstantOp $rhs), (CommutativeOp $op)],\n+-          [(MergeDiscardableAttributes $op, $new_op)]>;\n++  : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n +        (StableHLO_OpType:$new_op $rhs, $lhs),\n +        [(NotConstantOp $rhs), (CommutativeOp $op)],\n +        [(MergeDiscardableAttributes $op, $new_op)]>;\n  \n  ////////\n  // AddOp\n-@@ -161,8 +165,9 @@\n+@@ -165,9 +165,9 @@\n  \n  // Pattern: add(X, 0) -> X\n  def AddOp_RemoveNoop\n--  : Pat<(StableHLO_AddOp $lhs, (ConstantLikeMatcher AnyZero:$value)),\n--        (replaceWithValue $lhs)>;\n+-    : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),\n+-          (replaceWithValue $lhs), [],\n+-          [(MergeDiscardableAttributes $op, $lhs)]>;\n +  : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;\n  \n  ////////\n  // AndOp\n-@@ -173,13 +178,15 @@\n+@@ -177,25 +177,26 @@\n+   : CanonicalizeConstantToRhs<StableHLO_AndOp>;\n  \n  // Pattern: and(X, 0) -> 0\n- def AndOp_FoldToZero\n--  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n--        (replaceWithValue $zero)>;\n+-def AndOp_FoldToZero : Pat<(StableHLO_AndOp:$op $lhs,\n+-                               (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-                           (replaceWithValue $zero), [],\n+-                           [(MergeDiscardableAttributes $op, $zero)]>;\n++def AndOp_FoldToZero\n +  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n +        (replaceWithValue $zero), [],\n +        [(MergeDiscardableAttributes $op, $zero)]>;\n  \n  // Pattern: and(X, 1) -> X\n- def AndOp_RemoveNoop\n--  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n--        (replaceWithValue $lhs)>;\n+-def AndOp_RemoveNoop : Pat<(StableHLO_AndOp:$op $lhs,\n+-                               (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n+-                           (replaceWithValue $lhs), [],\n+-                           [(MergeDiscardableAttributes $op, $lhs)]>;\n++def AndOp_RemoveNoop\n +  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;\n  \n  ////////\n  // BroadcastInDimOp\n-@@ -188,7 +195,8 @@\n+ \n+ // Pattern: broadcast_in_dim(X, [iota...]) -> X\n  def BroadcastInDimOp_RemoveNoop\n-   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n-         (replaceWithValue $operand),\n--        [(TypesEqual $op, $operand)]>;\n+-    : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n+-          (replaceWithValue $operand), [(TypesEqual $op, $operand)],\n+-          [(MergeDiscardableAttributes $op, $operand)]>;\n++  : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n++        (replaceWithValue $operand),\n +        [(TypesEqual $op, $operand)],\n +        [(MergeDiscardableAttributes $op, $operand)]>;\n  \n  // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])\n  //       -> broadcast_in_dim(X, merge(dimsA, dimsB))\n-@@ -203,8 +211,10 @@\n+@@ -210,8 +211,10 @@\n  \n  // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])\n  //          [if same numel]\n@@ -1395,7 +806,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n          (StableHLO_ReshapeOpWithShape $op, $operand),\n          [(NumberOfElementsEqual $op, $operand)],\n          [],\n-@@ -213,7 +223,7 @@\n+@@ -220,7 +223,7 @@\n  // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])\n  //          [if same numel & rank]\n  def BroadcastInDimOp_ReplaceWithTranspose\n@@ -1404,62 +815,78 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n          (StableHLO_TransposeOp $operand, (InvertBroadcastDims $dims)),\n          [(NumberOfElementsEqual $op, $operand), (RankEqual $op, $operand)]>;\n  \n-@@ -254,7 +264,8 @@\n+@@ -259,9 +262,10 @@\n+ \n+ // Pattern: convert(X, [X.type]) -> X\n  def ConvertOp_RemoveNoop\n-   : Pat<(StableHLO_ConvertOp:$convert $operand),\n-         (replaceWithValue $operand),\n--        [(TypesEqual $convert, $operand)]>;\n+-    : Pat<(StableHLO_ConvertOp:$convert $operand),\n+-          (replaceWithValue $operand), [(TypesEqual $convert, $operand)],\n+-          [(MergeDiscardableAttributes $convert, $operand)]>;\n++  : Pat<(StableHLO_ConvertOp:$convert $operand),\n++        (replaceWithValue $operand),\n +        [(TypesEqual $convert, $operand)],\n +        [(MergeDiscardableAttributes $convert, $operand)]>;\n  \n  ////////\n  // DynamicBroadcastInDimOp\n-@@ -441,13 +452,15 @@\n+@@ -447,16 +451,16 @@\n+ //\n  // Multiplication by 0. This fold is not trivial for floats in presence of NaNs,\n  // so we currently only enable it for ints.\n- def MulOp_FoldToZero\n--  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n--        (replaceWithValue $zero)>;\n+-def MulOp_FoldToZero : Pat<(StableHLO_MulOp:$mul_op $lhs,\n+-                               (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-                           (replaceWithValue $zero), [],\n+-                           [(MergeDiscardableAttributes $mul_op, $zero)]>;\n++def MulOp_FoldToZero\n +  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n +        (replaceWithValue $zero), [],\n +        [(MergeDiscardableAttributes $mul_op, $zero)]>;\n  \n  // Pattern: multiply(X, 1i) -> X\n  def MulOp_RemoveNoop\n--  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n--        (replaceWithValue $lhs)>;\n+-    : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n+-          (replaceWithValue $lhs), [],\n+-          [(MergeDiscardableAttributes $mul_op, $lhs)]>;\n +  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $mul_op, $lhs)]>;\n  \n  ////////\n  // OrOp\n-@@ -457,13 +470,15 @@\n+@@ -465,16 +469,16 @@\n+ def OrOp_CanonicalizeConstantToRhs : CanonicalizeConstantToRhs<StableHLO_OrOp>;\n  \n  // Pattern: or(X, 1) -> 1\n- def OrOp_FoldToOne\n--  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n--        (replaceWithValue $one)>;\n+-def OrOp_FoldToOne : Pat<(StableHLO_OrOp:$op $lhs,\n+-                             (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n+-                         (replaceWithValue $one), [],\n+-                         [(MergeDiscardableAttributes $op, $one)]>;\n++def OrOp_FoldToOne\n +  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n +        (replaceWithValue $one), [],\n +        [(MergeDiscardableAttributes $op, $one)]>;\n  \n  // Pattern: or(X, 0) -> X\n- def OrOp_RemoveNoop\n--  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n--        (replaceWithValue $lhs)>;\n+-def OrOp_RemoveNoop : Pat<(StableHLO_OrOp:$op $lhs,\n+-                              (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-                          (replaceWithValue $lhs), [],\n+-                          [(MergeDiscardableAttributes $op, $lhs)]>;\n++def OrOp_RemoveNoop\n +  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;\n  \n  ////////\n  // PadOp\n-@@ -564,8 +579,9 @@\n+@@ -574,10 +578,10 @@\n+         (StableHLO_ConstantLike<\"0\"> $operand)>;\n  \n  // Pattern: subtract(X, 0) -> X\n- def SubtractOp_RemoveNoop\n--  : Pat<(StableHLO_SubtractOp $lhs, (StableHLO_ConstantOp AnyZero:$value)),\n--        (replaceWithValue $lhs)>;\n+-def SubtractOp_RemoveNoop : Pat<(StableHLO_SubtractOp:$op $lhs,\n+-                                    (StableHLO_ConstantOp AnyZero:$value)),\n+-                                (replaceWithValue $lhs), [],\n+-                                [(MergeDiscardableAttributes $op, $lhs)]>;\n++def SubtractOp_RemoveNoop\n +  : Pat<(StableHLO_SubtractOp:$op $lhs, (StableHLO_ConstantOp AnyZero:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;"
        },
        {
            "sha": "6012798b53e02ef962736f4d5a1a359494640a2e",
            "filename": "third_party/xla/third_party/stablehlo/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n     # LINT.IfChange\n-    STABLEHLO_COMMIT = \"3f27c53c20b9021ccab8b5f673e2c72e5b9cd6aa\"\n-    STABLEHLO_SHA256 = \"915e05e79d9764c048557a929c64e090ab58a5c7334da2c2650cd6378aa4d166\"\n+    STABLEHLO_COMMIT = \"96acdcb7724f4a9eec6d2e5af2597b0750c13948\"\n+    STABLEHLO_SHA256 = \"68e068a78d71f0764d5dd385ef434df922050530de99001969493298a00d64a0\"\n     # LINT.ThenChange(Google-internal path)\n \n     tf_http_archive("
        },
        {
            "sha": "f2a151c730ca5667a756dfd603059c827cac669f",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl833447018.patch",
            "status": "added",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl833447018.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl833447018.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl833447018.patch?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,22 @@\n+\n+--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertWarpSpecializeToLLVM.cpp\t2025-10-15 10:11:13.000000000 -0700\n++++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertWarpSpecializeToLLVM.cpp\t2025-11-20 11:24:12.000000000 -0800\n+@@ -7,6 +7,7 @@\n+ #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+ #include \"mlir/IR/BuiltinOps.h\"\n+ #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n++#include \"mlir/IR/TypeRange.h\"\n+ #include \"mlir/Pass/PassManager.h\"\n+ #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+ #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n+@@ -91,7 +92,9 @@\n+   if (numThreads == 32)\n+     LLVM::NVIDIA::createSyncWarp(b.getLoc(), b);\n+   else\n+-    b.create<NVVM::BarrierOp>(b.i32_val(barIdx), b.i32_val(numThreads));\n++    mlir::NVVM::BarrierOp::create(b, b.getLoc(), mlir::TypeRange(),\n++                                  b.i32_val(barIdx), b.i32_val(numThreads), {},\n++                                  {});\n+ }\n+ \n+ static void createAllBarrier(TritonLLVMIRRewriter &b, unsigned barIdx) {"
        },
        {
            "sha": "c2834d8afc853cd2d93763fd0a48a3403287284d",
            "filename": "third_party/xla/third_party/triton/llvm_integration/series.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -9,5 +9,6 @@ LLVM nor MLIR integrator, please do not add any patches to this list.\n \n llvm_patch_list = [\n     \"//third_party/triton:llvm_integration/cl831451347.patch\",\n+    \"//third_party/triton:llvm_integration/cl833447018.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "3809738d7bde2db9a0a7a49ba21b32768d52f32f",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -42,6 +42,7 @@ cc_library(\n     deps = [\n         \":stringpiece\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@local_xla//xla/tsl/platform:errors\",\n         \"@local_xla//xla/tsl/platform:macros\",\n         \"@local_xla//xla/tsl/platform:status\",\n@@ -66,6 +67,7 @@ cc_library(\n     deps = [\n         \":stringpiece\",\n         \":tstring\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@local_xla//xla/tsl/platform:byte_order\",\n         \"@local_xla//xla/tsl/platform:types\",\n     ],\n@@ -908,6 +910,7 @@ cc_library(\n     deps = [\n         \":raw_coding\",\n         \":stringpiece\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@local_xla//xla/tsl/platform:macros\",\n         \"@local_xla//xla/tsl/platform:types\",\n     ],"
        },
        {
            "sha": "e02f77aa9e53383a2768b35b81625a527295f92d",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/base64.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fbase64.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fbase64.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fbase64.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <string>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/tsl/platform/status.h\"\n #include \"tsl/platform/stringpiece.h\"\n "
        },
        {
            "sha": "ddd372b1ce3c697b021b34cc42ccc684801d2259",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/coding.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fcoding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fcoding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fcoding.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #ifndef TENSORFLOW_TSL_PLATFORM_CODING_H_\n #define TENSORFLOW_TSL_PLATFORM_CODING_H_\n \n+#include \"absl/strings/string_view.h\"\n #include \"xla/tsl/platform/types.h\"\n #include \"tsl/platform/stringpiece.h\"\n #include \"tsl/platform/tstring.h\""
        },
        {
            "sha": "0945bf071d5e6ded620941d7de290bacf393b0df",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/hash.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fhash.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fhash.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fhash.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <functional>\n #include <string>\n \n+#include \"absl/strings/string_view.h\"\n #include \"xla/tsl/platform/types.h\"\n #include \"tsl/platform/stringpiece.h\"\n "
        },
        {
            "sha": "9a275de74d2a996df9d14fe31bcf2e9499a40588",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/str_util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstr_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstr_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstr_util.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n \n #include \"absl/strings/ascii.h\"\n #include \"absl/strings/match.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"tsl/platform/stringpiece.h\"\n "
        },
        {
            "sha": "0242ff350d1f3f312928b60f8a5450d634b4d5e9",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/str_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstr_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstr_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstr_util.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/strings/match.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/str_split.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/strings/strip.h\"\n #include \"xla/tsl/platform/macros.h\"\n #include \"xla/tsl/platform/types.h\""
        },
        {
            "sha": "2ecbc74047b9314516683978e42fada8cc5e8c03",
            "filename": "third_party/xla/third_party/xnnpack/workspace.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fxnnpack%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fthird_party%2Fxnnpack%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fxnnpack%2Fworkspace.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -6,8 +6,8 @@ def repo():\n     # LINT.IfChange\n     tf_http_archive(\n         name = \"XNNPACK\",\n-        sha256 = \"027376a71384311a0ddca0fc986dea621bac0f8b30c96365bf4d2937b627226f\",\n-        strip_prefix = \"XNNPACK-decc685b0ecfd00da5a2168eb03b0c795678f084\",\n-        urls = tf_mirror_urls(\"https://github.com/google/XNNPACK/archive/decc685b0ecfd00da5a2168eb03b0c795678f084.zip\"),\n+        sha256 = \"a633a48ba393211771204d25ebc5f35359b71bfbefaa6e955aa92570caede727\",\n+        strip_prefix = \"XNNPACK-fa0fd6471a39a5d66a59d4cd8f8cc4a93a4bd470\",\n+        urls = tf_mirror_urls(\"https://github.com/google/XNNPACK/archive/fa0fd6471a39a5d66a59d4cd8f8cc4a93a4bd470.zip\"),\n     )\n     # LINT.ThenChange(//tensorflow/lite/tools/cmake/modules/xnnpack.cmake)"
        },
        {
            "sha": "dbeb3d5b0894bb78ff4c7fe45cfb84a33d77c108",
            "filename": "third_party/xla/workspace0.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fworkspace0.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fworkspace0.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fworkspace0.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -140,10 +140,10 @@ def workspace():\n     if \"rules_ml_toolchain\" not in native.existing_rules():\n         http_archive(\n             name = \"rules_ml_toolchain\",\n-            sha256 = \"a7951a86c4e9783302230b859237d953a6c8c301b219d344e05d70496eeefa52\",\n-            strip_prefix = \"rules_ml_toolchain-0d383c69076f637d55eaae0b6e0ee2980b1345a9\",\n+            sha256 = \"5f17275397752b666adbf8f0a81a3ebfb1e26a970b459cac33a06a8f03caa537\",\n+            strip_prefix = \"rules_ml_toolchain-a2626615e1277a635b43dd268e1d4bc892afea10\",\n             urls = [\n-                \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/0d383c69076f637d55eaae0b6e0ee2980b1345a9.tar.gz\",\n+                \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/a2626615e1277a635b43dd268e1d4bc892afea10.tar.gz\",\n             ],\n         )\n "
        },
        {
            "sha": "058c6865081a307e12c05ca3e6147c876fa19900",
            "filename": "third_party/xla/workspace2.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fworkspace2.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fworkspace2.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fworkspace2.bzl?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -161,9 +161,9 @@ def _tf_repositories():\n \n     tf_http_archive(\n         name = \"KleidiAI\",\n-        sha256 = \"ecd433a4f7446f7f02a04e458989de8253f19187aa85e3b81b19e0b60f0bf859\",\n-        strip_prefix = \"kleidiai-d7770c89632329a9914ef1a90289917597639cbe\",\n-        urls = tf_mirror_urls(\"https://github.com/ARM-software/kleidiai/archive/d7770c89632329a9914ef1a90289917597639cbe.zip\"),\n+        sha256 = \"fb4f8180171d035a08432b086194121f627d00a76d58cebaad57d7a87ad40dbd\",\n+        strip_prefix = \"kleidiai-7a3a609a3278106df7157bdd27b8f0e75ab00b60\",\n+        urls = tf_mirror_urls(\"https://github.com/ARM-software/kleidiai/archive/7a3a609a3278106df7157bdd27b8f0e75ab00b60.zip\"),\n     )\n \n     tf_http_archive("
        },
        {
            "sha": "2c2dcb14750ef75203252ea13520b636ccb08b9b",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -458,8 +458,11 @@ absl::StatusOr<std::vector<Autotuner::ConfigResult>> Autotuner::ProfileAll(\n \n   std::optional<ScopedShapedBuffer> reference_output;\n   if (autotune_config_.check_buffers) {\n-    TF_ASSIGN_OR_RETURN(reference_output,\n-                        GetReferenceOutput(candidates, *input_buffers));\n+    reference_output = GetReferenceOutput(candidates, *input_buffers);\n+    if (!reference_output.has_value()) {\n+      LOG(WARNING) << \"No reference output found even though buffer checking \"\n+                      \"was requested while autotuning\";\n+    }\n   }\n \n   for (int i = 0; i < candidates.size(); ++i) {\n@@ -475,8 +478,7 @@ absl::StatusOr<std::vector<Autotuner::ConfigResult>> Autotuner::ProfileAll(\n     } else {\n       duration = profile_result->duration;\n       scratch_bytes = profile_result->scratch_bytes;\n-      if (autotune_config_.check_buffers) {\n-        CHECK(reference_output.has_value());\n+      if (autotune_config_.check_buffers && reference_output.has_value()) {\n         CHECK(profile_result->output_buffer.has_value());\n         failure =\n             CheckBuffers(*input_buffers, profile_result->output_buffer.value(),\n@@ -499,7 +501,7 @@ absl::StatusOr<Autotuner::ConfigResult> Autotuner::PickBestConfig(\n         std::remove_if(results.begin(), results.end(),\n                        [](const ConfigResult& result) {\n                          return result.config.codegen_backend->name() ==\n-                                \"cublas\";\n+                                \"Cublas_fission\";\n                        }),\n         results.end());\n   }\n@@ -558,7 +560,7 @@ absl::Status Autotuner::DumpHlo(HloInstruction* instr, const Config& config) {\n   return absl::OkStatus();\n }\n \n-absl::StatusOr<ScopedShapedBuffer> Autotuner::GetReferenceOutput(\n+std::optional<ScopedShapedBuffer> Autotuner::GetReferenceOutput(\n     std::vector<ExecutableCandidate>& candidates, InputBuffers& input_buffers) {\n   for (auto& candidate : candidates) {\n     if (candidate.config.codegen_backend->CanProduceWrongResults()) {\n@@ -574,8 +576,7 @@ absl::StatusOr<ScopedShapedBuffer> Autotuner::GetReferenceOutput(\n       return std::move(profile_result.value().output_buffer.value());\n     }\n   }\n-  return absl::NotFoundError(\n-      \"No reference output found but correctness checking is enabled!\");\n+  return std::nullopt;\n }\n \n std::optional<Autotuner::Failure> Autotuner::CheckBuffers("
        },
        {
            "sha": "eb15e516070b22344ad8f89b9f91e143acd921db",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.h",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -46,6 +46,8 @@ namespace xla {\n struct AutotuneConfig {\n   // Whether to check the correctness of the output buffers and OOM reads on\n   // Input Buffers.\n+  // Correctness check is only performed when a trustable reference output is\n+  // available.\n   bool check_buffers = true;\n   // Relative tolerance for correctness check.\n   float relative_tolerance = 1e-6;\n@@ -67,9 +69,9 @@ struct AutotuneConfig {\n   std::string dump_logs_to = \"\";\n   // TODO b/446618161 - Remove this when old triton emitter is\n   // deprecated.\n-  // If true, autotuner will not select cublas configs. We still try cublas\n-  // configs as they can be used to check numerical issues with triton but they\n-  // are not considered for selection, unless there are no other options.\n+  // If true, autotuner will not select cublas configs for fusions. We still try\n+  // the configs as they can be used to check numerical issues with triton but\n+  // they are not considered for selection, unless there are no other options.\n   bool exclude_cublas_config = false;\n   // TODO b/446870267- Remove this option and use default configs rather than\n   // the first config.\n@@ -197,7 +199,7 @@ class Autotuner {\n   absl::StatusOr<ConfigResult> PickBestConfig(\n       std::vector<ConfigResult>& results);\n \n-  absl::StatusOr<ScopedShapedBuffer> GetReferenceOutput(\n+  std::optional<ScopedShapedBuffer> GetReferenceOutput(\n       std::vector<ExecutableCandidate>& candidates,\n       InputBuffers& input_buffers);\n "
        },
        {
            "sha": "1fc74269cead16684a267c2bc1951ec7fcd7b7ff",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 2,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -467,7 +467,7 @@ TEST_F(AutotunerTest, CacheHit) {\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), IsOk());\n }\n \n-TEST_F(AutotunerTest, AutotuneWithBufferCheck) {\n+TEST_F(AutotunerTest, AutotuneWithBufferCheckFiltersWrongResults) {\n   config_.check_buffers = true;\n \n   std::vector<std::unique_ptr<BackendConfig>> configs_1;\n@@ -514,6 +514,43 @@ TEST_F(AutotunerTest, AutotuneWithBufferCheck) {\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), IsOk());\n }\n \n+TEST_F(AutotunerTest, AutotuneSkipsBufferCheckWhenNoReferenceOutput) {\n+  config_.check_buffers = true;\n+\n+  std::vector<std::unique_ptr<BackendConfig>> configs;\n+  configs.push_back(GetTestConfig(\"test_config_1\"));\n+  configs.push_back(GetTestConfig(\"test_config_2\"));\n+  auto backend = std::make_unique<MockCodegenBackendWithWrongResults>();\n+  EXPECT_CALL(*backend, GetSupportedConfigs)\n+      .WillOnce(Return(std::move(configs)));\n+  EXPECT_CALL(*backend, Compile(_, _))\n+      .WillOnce(Return(std::unique_ptr<Executable>()))\n+      .WillOnce(Return(std::unique_ptr<Executable>()));\n+\n+  EXPECT_CALL(*backend, ApplyConfig(_, ConfigMatcher(\"test_config_1\")))\n+      .Times(1)\n+      .WillRepeatedly(Return(absl::OkStatus()));\n+\n+  auto profiler = std::make_unique<MockProfiler>();\n+  ScopedShapedBuffer output_1(Shape(), nullptr, 0),\n+      output_2(Shape(), nullptr, 0), output_3(Shape(), nullptr, 0);\n+  EXPECT_CALL(*profiler, CreateInputBuffers(_))\n+      .WillOnce(Return(std::make_unique<InputBuffers>()));\n+  EXPECT_CALL(*profiler, Profile(_, _))\n+      .WillOnce(Return(ProfileResult({absl::Seconds(1), std::move(output_1)})))\n+      .WillOnce(Return(ProfileResult({absl::Seconds(2), std::nullopt})));\n+  EXPECT_CALL(*profiler, CheckOutputBuffer(_, _, _)).Times(0);\n+\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::move(backend));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto autotuner,\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n+                        std::make_unique<MockAutotunerCache>()));\n+  auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n+  EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), IsOk());\n+}\n+\n TEST_F(AutotunerTest, AutotuneWithScratchBytesOptimization) {\n   std::vector<std::unique_ptr<BackendConfig>> configs;\n   configs.push_back(GetTestConfig(\"config_most_time_less_scratch\"));\n@@ -667,7 +704,7 @@ TEST_F(AutotunerTest, ExcludeCublasConfig) {\n   EXPECT_CALL(*backend, Compile(_, _))\n       .WillOnce(Return(std::unique_ptr<Executable>()))\n       .WillOnce(Return(std::unique_ptr<Executable>()));\n-  EXPECT_CALL(*backend, name()).WillRepeatedly(Return(\"cublas\"));\n+  EXPECT_CALL(*backend, name()).WillRepeatedly(Return(\"Cublas_fission\"));\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::move(backend));\n "
        },
        {
            "sha": "97f9e847f5eb25d5bdb6ca1ca408f5475ef71530",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -249,6 +249,38 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"target_machine_options\",\n+    srcs = [\"target_machine_options.cc\"],\n+    hdrs = [\"target_machine_options.h\"],\n+    deps = [\n+        \"//xla:util\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/cpu/codegen:cpu_features\",\n+        \"//xla/service/cpu:executable_proto_cc\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:TargetParser\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"target_machine_options_test\",\n+    srcs = [\"target_machine_options_test.cc\"],\n+    deps = [\n+        \":target_machine_options\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/service/cpu:executable_proto_cc\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"constant_allocation\",\n     srcs = [\"constant_allocation.cc\"],"
        },
        {
            "sha": "201124b2c66190bfa4319c8ff55324117b685613",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -101,12 +101,14 @@ cc_library(\n         \":polynomial_approximations\",\n         \"//xla:util\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/cpu:target_machine_options\",\n         \"//xla/codegen:intrinsic_lib\",\n         \"//xla/codegen/intrinsic\",\n         \"//xla/codegen/intrinsic:intrinsic_compiler_lib\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/cpu:backend_config_proto_cc\",\n         \"//xla/service/cpu:cpu_options\",\n+        \"//xla/service/cpu:executable_proto_cc\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -143,8 +145,12 @@ xla_cc_test(\n     deps = [\n         \":ir_compiler\",\n         \":kernel_api_ir_builder\",\n+        \"//xla:debug_options_flags\",\n         \"//xla:util\",\n+        \"//xla/backends/cpu:target_machine_options\",\n         \"//xla/service/cpu:backend_config_proto_cc\",\n+        \"//xla/service/cpu:cpu_compiler_pure\",\n+        \"//xla/service/cpu:test_header_helper\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n@@ -315,7 +321,9 @@ xla_cc_test(\n         \":ir_compiler\",\n         \":jit_compiler\",\n         \":kernel_api_ir_builder\",\n+        \"//xla:debug_options_flags\",\n         \"//xla:util\",\n+        \"//xla/backends/cpu:target_machine_options\",\n         \"//xla/backends/cpu/runtime:function_library\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n@@ -632,10 +640,12 @@ xla_cc_test(\n         \":execution_engine\",\n         \":ir_compiler\",\n         \":jit_compiler\",\n+        \":kernel_api_ir_builder\",\n         \":object_loader\",\n+        \"//xla:debug_options_flags\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/codegen:kernel_api_ir_builder\",\n+        \"//xla/backends/cpu:target_machine_options\",\n         \"//xla/backends/cpu/runtime:function_library\",\n         \"//xla/service:cpu_plugin\",\n         \"//xla/service/cpu:executable_proto_cc\","
        },
        {
            "sha": "cab42ce16aab098981283455a58deb4da2a0e320",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -43,15 +43,10 @@ cc_library(\n         \":passes_inc_gen\",\n         \":xla_cpu_rewrite_patterns\",\n         \"//xla/backends/cpu/codegen/emitters/ir:xla_cpu\",\n-        \"//xla/codegen/emitters:implicit_arith_op_builder\",\n         \"//xla/codegen/emitters/ir:xla\",\n-        \"//xla/codegen/intrinsic:fptrunc\",\n-        \"//xla/codegen/intrinsic:log1p\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n-        \"//xla/mlir/utils:type_util\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n-        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:ArithDialect\","
        },
        {
            "sha": "b14e7c172a37e391f8e93326ed06155835df2142",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/expand_float_ops.cc",
            "status": "modified",
            "additions": 57,
            "deletions": 56,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fexpand_float_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fexpand_float_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fexpand_float_ops.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,26 +17,24 @@ limitations under the License.\n #include <memory>\n #include <utility>\n \n-#include \"absl/strings/string_view.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Diagnostics.h\"\n-#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n #include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"xla/codegen/emitters/implicit_arith_op_builder.h\"\n-#include \"xla/mlir/utils/type_util.h\"\n \n namespace xla::cpu {\n \n@@ -48,33 +46,37 @@ namespace {\n \n namespace ma = ::mlir::arith;\n \n-mlir::func::FuncOp GetOrInsertDeclaration(mlir::PatternRewriter& rewriter,\n-                                          mlir::ModuleOp& module_op,\n-                                          absl::string_view name,\n-                                          mlir::FunctionType func_type) {\n-  // Check if the function already exists\n-  if (auto func = module_op.lookupSymbol<mlir::func::FuncOp>(name)) {\n-    // Ensure the existing function has the correct type\n-    if (func.getFunctionType() == func_type) {\n-      return func;\n-    }\n+// Get a constant value, if the type is a vector, splat the value to the vector\n+// type.\n+mlir::Value GetConst(mlir::ImplicitLocOpBuilder& b, mlir::Type type,\n+                     mlir::TypedAttr value) {\n+  if (auto vector_type = mlir::dyn_cast<mlir::VectorType>(type)) {\n+    value =\n+        mlir::SplatElementsAttr::get(mlir::cast<mlir::ShapedType>(type), value);\n   }\n+  return mlir::arith::ConstantOp::create(b, type, value);\n+}\n \n-  // If not found or type mismatch, create the declaration\n-  mlir::PatternRewriter::InsertionGuard insertGuard(rewriter);\n-  rewriter.setInsertionPointToStart(module_op.getBody());\n+mlir::Value EmitBF16ToF32(mlir::Type dst_ty, mlir::Value in,\n+                          mlir::ImplicitLocOpBuilder& b) {\n+  auto get_type = [&](mlir::Type element_type) -> mlir::Type {\n+    if (auto vector_type = mlir::dyn_cast<mlir::VectorType>(in.getType())) {\n+      return vector_type.clone(element_type);\n+    }\n+    return element_type;\n+  };\n \n-  auto func_decl =\n-      rewriter.create<mlir::func::FuncOp>(module_op.getLoc(), name, func_type);\n-  func_decl.setPrivate();\n-  return func_decl;\n-}\n+  mlir::Type i16_type = get_type(b.getI16Type());\n+  mlir::Type i32_type = get_type(b.getI32Type());\n \n-mlir::Value EmitBF16ToF32(mlir::Value in, mlir::ImplicitLocOpBuilder& b) {\n-  mlir::Value i16 = b.create<ma::BitcastOp>(b.getI16Type(), in);\n-  emitters::ImplicitArithOpBuilder i32(\n-      b.create<ma::ExtUIOp>(b.getI32Type(), i16), &b);\n-  return b.create<ma::BitcastOp>(b.getType<mlir::Float32Type>(), i32 << 16);\n+  mlir::Value i16 = ma::BitcastOp::create(b, i16_type, in);\n+  mlir::Value i32 = ma::ExtUIOp::create(b, i32_type, i16);\n+\n+  mlir::TypedAttr shift_value = b.getI32IntegerAttr(16);\n+  mlir::Value shift_const = GetConst(b, i32_type, shift_value);\n+\n+  mlir::Value i32_shl = mlir::arith::ShLIOp::create(b, i32, shift_const);\n+  return ma::BitcastOp::create(b, dst_ty, i32_shl);\n }\n \n struct RewriteExtFPattern : public mlir::OpRewritePattern<ma::ExtFOp> {\n@@ -83,13 +85,14 @@ struct RewriteExtFPattern : public mlir::OpRewritePattern<ma::ExtFOp> {\n   mlir::LogicalResult matchAndRewrite(\n       ma::ExtFOp op, mlir::PatternRewriter& rewriter) const override {\n     auto src = op.getOperand();\n-    auto dst_ty = mlir::cast<mlir::FloatType>(op.getType());\n+    auto dst_ty = op.getType();\n \n     mlir::ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n \n-    if (mlir::isa<mlir::BFloat16Type>(src.getType()) &&\n-        mlir::isa<mlir::Float32Type>(dst_ty)) {\n-      rewriter.replaceOp(op, EmitBF16ToF32(src, builder));\n+    if (mlir::isa<mlir::BFloat16Type>(\n+            mlir::getElementTypeOrSelf(src.getType())) &&\n+        mlir::isa<mlir::Float32Type>(mlir::getElementTypeOrSelf(dst_ty))) {\n+      rewriter.replaceOp(op, EmitBF16ToF32(dst_ty, src, builder));\n       return mlir::success();\n     }\n \n@@ -104,19 +107,19 @@ class RewriteCbrtPattern : public mlir::OpRewritePattern<mlir::math::CbrtOp> {\n   mlir::LogicalResult matchAndRewrite(\n       mlir::math::CbrtOp op, mlir::PatternRewriter& rewriter) const override {\n     mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n+    mlir::arith::FastMathFlagsAttr fastmath = op.getFastmathAttr();\n \n     mlir::Value input_abs =\n-        b.create<mlir::math::AbsFOp>(op.getOperand(), op.getFastmathAttr())\n-            .getResult();\n+        b.create<mlir::math::AbsFOp>(op.getOperand(), fastmath).getResult();\n \n-    mlir::Value one_third = b.create<mlir::arith::ConstantOp>(\n-        b.getFloatAttr(op.getType(), 1.0 / 3.0));\n-    mlir::Value cbrt_abs = b.create<mlir::math::PowFOp>(input_abs, one_third,\n-                                                        op.getFastmathAttr());\n+    mlir::TypedAttr third_attr =\n+        b.getFloatAttr(mlir::getElementTypeOrSelf(op.getType()), 1.0 / 3.0);\n+    mlir::Value third_value = GetConst(b, op.getType(), third_attr);\n+    mlir::Value cbrt_abs =\n+        b.create<mlir::math::PowFOp>(input_abs, third_value, fastmath);\n \n     mlir::Value cbrt_signed =\n-        b.create<mlir::math::CopySignOp>(cbrt_abs, op.getOperand(),\n-                                         op.getFastmathAttr())\n+        b.create<mlir::math::CopySignOp>(cbrt_abs, op.getOperand(), fastmath)\n             .getResult();\n \n     rewriter.replaceOp(op, cbrt_signed);\n@@ -136,29 +139,27 @@ class RewriteExpm1Pattern : public mlir::OpRewritePattern<mlir::math::ExpM1Op> {\n     mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n \n     mlir::Type type = op.getType();\n-    mlir::Value one =\n-        b.create<mlir::arith::ConstantOp>(b.getFloatAttr(type, 1.0));\n-    mlir::Value half =\n-        b.create<mlir::arith::ConstantOp>(b.getFloatAttr(type, 0.5));\n-    mlir::Value zero =\n-        b.create<mlir::arith::ConstantOp>(b.getFloatAttr(type, 0.0));\n+    mlir::Type element_type = mlir::getElementTypeOrSelf(type);\n+    mlir::Value one = GetConst(b, type, b.getFloatAttr(element_type, 1.0));\n+    mlir::Value half = GetConst(b, type, b.getFloatAttr(element_type, 0.5));\n+    mlir::Value zero = GetConst(b, type, b.getFloatAttr(element_type, 0.0));\n     mlir::Value x = op.getOperand();\n \n-    mlir::Value exp_x = b.create<mlir::math::ExpOp>(x, op.getFastmathAttr());\n+    mlir::arith::FastMathFlagsAttr fastmath = op.getFastmathAttr();\n+\n+    mlir::Value exp_x = b.create<mlir::math::ExpOp>(x, fastmath);\n \n     mlir::Value exp_x_minus_1 =\n-        b.create<mlir::arith::SubFOp>(exp_x, one, op.getFastmathAttr());\n+        b.create<mlir::arith::SubFOp>(exp_x, one, fastmath);\n \n-    mlir::Value half_x =\n-        b.create<mlir::arith::MulFOp>(x, half, op.getFastmathAttr());\n-    mlir::Value tanh_half_x =\n-        b.create<mlir::math::TanhOp>(half_x, op.getFastmathAttr());\n+    mlir::Value half_x = b.create<mlir::arith::MulFOp>(x, half, fastmath);\n+    mlir::Value tanh_half_x = b.create<mlir::math::TanhOp>(half_x, fastmath);\n     mlir::Value exp_x_plus_1 =\n-        b.create<mlir::arith::AddFOp>(exp_x, one, op.getFastmathAttr());\n-    mlir::Value small_result = b.create<mlir::arith::MulFOp>(\n-        tanh_half_x, exp_x_plus_1, op.getFastmathAttr());\n+        b.create<mlir::arith::AddFOp>(exp_x, one, fastmath);\n+    mlir::Value small_result =\n+        b.create<mlir::arith::MulFOp>(tanh_half_x, exp_x_plus_1, fastmath);\n \n-    mlir::Value abs_x = b.create<mlir::math::AbsFOp>(x, op.getFastmathAttr());\n+    mlir::Value abs_x = b.create<mlir::math::AbsFOp>(x, fastmath);\n     mlir::Value x_is_large = b.create<mlir::arith::CmpFOp>(\n         mlir::arith::CmpFPredicate::OGT, abs_x, half);\n     mlir::Value normal_result = b.create<mlir::arith::SelectOp>("
        },
        {
            "sha": "5c2cab2fec7b4630ecc13efceb0dd81c3a645649",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/tests/expand_float_ops.mlir",
            "status": "modified",
            "additions": 33,
            "deletions": 3,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fexpand_float_ops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fexpand_float_ops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fexpand_float_ops.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1,12 +1,18 @@\n // RUN: emitters_opt %s -split-input-file -xla-cpu-expand-float-ops | FileCheck %s\n \n-\n func.func @extend(%input: bf16) -> f32 {\n+  // CHECK-NOT: arith.extf\n   %truncated = arith.extf %input : bf16 to f32\n   func.return %truncated : f32\n }\n \n-// CHECK-NOT: arith.extf\n+// -----\n+\n+func.func @extend_vector(%input: vector<8xbf16>) -> vector<8xf32> {\n+  // CHECK-NOT: arith.extf\n+  %truncated = arith.extf %input : vector<8xbf16> to vector<8xf32>\n+  func.return %truncated : vector<8xf32>\n+}\n \n // -----\n \n@@ -15,7 +21,6 @@ func.func @cbrt(%arg0: f64) -> f64 {\n   return %ret : f64\n }\n \n-\n // CHECK: @cbrt(%[[ARG:.*]]: f64) -> f64\n // CHECK-NOT: math.cbrt\n // CHECK-DAG: %[[CONSTANT:.*]] = arith.constant 0.3333333\n@@ -26,10 +31,35 @@ func.func @cbrt(%arg0: f64) -> f64 {\n \n // -----\n \n+func.func @cbrt_vector(%arg0: vector<8xf64>) -> vector<8xf64> {\n+  %ret = math.cbrt %arg0 fastmath<reassoc> : vector<8xf64>\n+  return %ret : vector<8xf64>\n+}\n+\n+// CHECK: @cbrt_vector(%[[ARG:.*]]: vector<8xf64>) -> vector<8xf64>\n+// CHECK-NOT: math.cbrt\n+// CHECK-DAG: %[[CONSTANT:.*]] = arith.constant dense<0.33333333333333331> : vector<8xf64>\n+// CHECK: %[[ABS:.*]] = math.absf %[[ARG]] fastmath<reassoc> : vector<8xf64>\n+// CHECK: %[[CBRT_ABS:.*]] = math.powf %[[ABS]], %[[CONSTANT]] fastmath<reassoc> : vector<8xf64>\n+// CHECK: %[[CBRT_SIGNED:.*]] = math.copysign %[[CBRT_ABS]], %[[ARG]] fastmath<reassoc> : vector<8xf64>\n+// CHECK: return %[[CBRT_SIGNED]]\n+\n+// -----\n+\n func.func @expm1(%arg0: f64) -> f64 {\n   %ret = math.expm1 %arg0 : f64\n   return %ret : f64\n }\n \n // CHECK-LABEL: @expm1\n // CHECK-NOT: math.expm1\n+\n+// -----\n+\n+func.func @expm1_vector(%arg0: vector<4xf64>) -> vector<4xf64> {\n+  %ret = math.expm1 %arg0 : vector<4xf64>\n+  return %ret : vector<4xf64>\n+}\n+\n+// CHECK-LABEL: @expm1_vector\n+// CHECK-NOT: math.expm1"
        },
        {
            "sha": "80d6f4444b536a10b7e0e40ab417157f79e91079",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 13,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -188,7 +188,10 @@ static std::unique_ptr<::mlir::Pass> CreateConvertMathToLLVMPass() {\n // The final lowering passes common to both scalar and tiled kernels.\n // These passes are primarily responsible for lowering individual ops to\n // their LLVM equivalent.\n-static void AddGenericLoweringPasses(mlir::OpPassManager& pm) {\n+static void AddGenericLoweringPasses(mlir::OpPassManager& pm,\n+                                     bool fast_min_max) {\n+  pm.addNestedPass<mlir::func::FuncOp>(\n+      emitters::CreateSimplifyArithPass(fast_min_max));\n   pm.addPass(emitters::CreateSimplifyAffinePass());\n   pm.addPass(mlir::createCanonicalizerPass());\n \n@@ -275,9 +278,8 @@ static void AddScalarLoweringPasses(mlir::OpPassManager& pm,\n   // simplify-affine has maximally folded expressions to work with.\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(mlir::createCSEPass());\n-  pm.addNestedPass<mlir::func::FuncOp>(\n-      emitters::CreateSimplifyArithPass(fast_min_max));\n-  AddGenericLoweringPasses(pm);\n+\n+  AddGenericLoweringPasses(pm, fast_min_max);\n }\n \n static void AddBufferizationPasses(mlir::OpPassManager& pm) {\n@@ -310,16 +312,21 @@ static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n   emitters::RegisterOptimizationPasses(pm);\n \n   pm.addPass(CreateShloToVectorPass());\n-  pm.addPass(CreateXTileToVectorPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(CreateLowerXTileEntryPass());\n   pm.addNestedPass<mlir::func::FuncOp>(\n       mlir::vector::createLowerVectorMultiReductionPass(\n           mlir::vector::VectorMultiReductionLowering::InnerParallel));\n-  pm.addPass(CreateTensorOpsToVectorPass());\n+  pm.addPass(CreateTensorOpsToBufferizablePass());\n+\n+  mlir::stablehlo::StablehloLegalizeToLinalgPassOptions\n+      stablehlo_to_linalg_options;\n+  stablehlo_to_linalg_options.enablePrimitiveOps = true;\n+  pm.addPass(mlir::stablehlo::createStablehloLegalizeToLinalgPass());\n+  pm.addPass(xtile::createConvertElementwise0DTensorToScalarPass());\n \n   pm.addPass(mlir::createConvertElementwiseToLinalgPass());\n-  pm.addPass(mlir::createLinalgElementwiseOpFusionPass());\n+  pm.addPass(CreateFuseElementwisePass());\n \n   AddBufferizationPasses(pm);\n \n@@ -329,7 +336,8 @@ static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n // Lowering passes for the tiled emitter.\n // The input IR is from the xtile dialect which uses tensors that are converted\n // first to the vector dialect and then to LLVM.\n-static void AddTiledLoweringPasses(mlir::OpPassManager& pm) {\n+static void AddTiledLoweringPasses(mlir::OpPassManager& pm, bool fast_min_max) {\n+  pm.addPass(CreateVectorToScalarPass());\n   pm.addPass(cpu::CreateMemrefCopyToLoopsPass());\n   pm.addPass(cpu::createLowerToLLVMPass());\n   pm.addPass(mlir::createConvertVectorToSCFPass(\n@@ -342,7 +350,7 @@ static void AddTiledLoweringPasses(mlir::OpPassManager& pm) {\n   pm.addPass(mlir::createConvertComplexToStandardPass());\n   pm.addPass(mlir::memref::createExpandStridedMetadataPass());\n \n-  AddGenericLoweringPasses(pm);\n+  AddGenericLoweringPasses(pm, fast_min_max);\n }\n \n static int GetLlvmFunctionDefCount(mlir::ModuleOp m) {\n@@ -392,7 +400,7 @@ FusionCompiler::FusionCompiler(mlir::MLIRContext* context, Options options,\n     tiled_pass_manager_.addPass(\n         std::make_unique<ModuleCallbackPass>(hooks_.post_optimization));\n   }\n-  AddTiledLoweringPasses(tiled_pass_manager_);\n+  AddTiledLoweringPasses(tiled_pass_manager_, options_.fast_min_max);\n \n   scalar_pass_manager_.addInstrumentation(\n       std::make_unique<TraceInstrumentation>());\n@@ -452,6 +460,9 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> FusionCompiler::Compile(\n       mlir_module, llvm_context,\n       absl::StrCat(kXlaModuleIdentifier, \"_\", module_name));\n \n+  TF_RET_CHECK(llvm_module != nullptr)\n+      << \"Failed to translate module to LLVM IR.\";\n+\n   if (mlir::Attribute options =\n           mlir_module->getAttr(xla::ExtraBackendOptionsAttr::name)) {\n     const auto formatter = [](std::string* out, const mlir::StringAttr& attr) {\n@@ -471,9 +482,6 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> FusionCompiler::Compile(\n                               mlir::cast<mlir::StringAttr>(options).str());\n   }\n \n-  TF_RET_CHECK(llvm_module != nullptr)\n-      << \"Failed to translate module to LLVM IR.\";\n-\n   llvm_module->setDataLayout(llvm_module->getDataLayout());\n \n   if (options_.fast_math_flags.any()) {"
        },
        {
            "sha": "a532b353ce2f2b5aa5d694db6be0cb1d657690ac",
            "filename": "third_party/xla/xla/backends/cpu/codegen/ir_compiler.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 47,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n@@ -63,9 +62,9 @@ limitations under the License.\n #include \"llvm/TargetParser/Triple.h\"\n #include \"llvm/Transforms/IPO/AlwaysInliner.h\"\n #include \"llvm/Transforms/Instrumentation/DataFlowSanitizer.h\"\n-#include \"xla/backends/cpu/codegen/cpu_features.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n #include \"xla/backends/cpu/codegen/polynomial_approximations.h\"\n+#include \"xla/backends/cpu/target_machine_options.h\"\n #include \"xla/codegen/intrinsic/intrinsic.h\"\n #include \"xla/codegen/intrinsic/intrinsic_compiler_lib.h\"\n #include \"xla/codegen/intrinsic_lib.h\"\n@@ -76,7 +75,6 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n-#include \"tsl/platform/cpu_info.h\"\n \n namespace xla::cpu {\n \n@@ -194,7 +192,7 @@ std::unique_ptr<IrCompiler> IrCompiler::Create(\n   TargetMachineBuilder target_machine_builder =\n       IrCompiler::InferTargetMachineBuilder(std::move(target_options),\n                                             options.opt_level,\n-                                            options.max_cpu_feature);\n+                                            options.target_machine_options);\n \n   return std::make_unique<IrCompiler>(target_machine_builder,\n                                       std::move(options), std::move(hooks));\n@@ -218,41 +216,35 @@ absl::once_flag initialize_llvm_flag;\n absl::StatusOr<std::unique_ptr<llvm::TargetMachine>>\n IrCompiler::InferTargetMachine(\n     const llvm::TargetOptions& target_options, llvm::CodeGenOptLevel opt_level,\n-    std::optional<tsl::port::CPUFeature> max_cpu_feature) {\n-  // Detect machine attributes for the target CPU.\n-  auto result = DetectMachineAttributes(max_cpu_feature);\n-  llvm::SmallVector<std::string> attrs(result.features.begin(),\n-                                       result.features.end());\n-\n-  // If `max_cpu_feature` is newer than the host CPU, we should keep the host\n-  // CPU name, e.g., we don't want to set the target CPU to Skylake when we are\n-  // on a Broadwell host.\n-  absl::string_view cpu = result.num_filtered_features\n-                              ? CpuTargetFromMaxFeature(*max_cpu_feature)\n-                              : absl::string_view(llvm::sys::getHostCPUName());\n+    const TargetMachineOptions& target_machine_options) {\n+  auto attrs_vec = target_machine_options.GetTargetMachineFeaturesVector();\n+  llvm::SmallVector<std::string> attrs(attrs_vec.begin(), attrs_vec.end());\n \n   absl::call_once(initialize_llvm_flag, InitializeLLVMTarget);\n   std::unique_ptr<llvm::TargetMachine> target_machine(\n       llvm::EngineBuilder()\n           .setTargetOptions(target_options)\n           .setOptLevel(opt_level)\n           .selectTarget(\n-              /*TargetTriple=*/llvm::Triple(), /*MArch=*/\"\",\n-              /*MCPU=*/cpu,\n+              /*TargetTriple=*/llvm::Triple(target_machine_options.triple()),\n+              /*MArch=*/\"\",\n+              /*MCPU=*/target_machine_options.cpu(),\n               /*MAttrs=*/attrs));\n \n   if (target_machine == nullptr) {\n-    return Internal(\"Failed to create target machine for CPU %s\", cpu);\n+    return Internal(\"Failed to create target machine for CPU %s\",\n+                    target_machine_options.cpu());\n   }\n \n   return std::move(target_machine);\n }\n \n IrCompiler::TargetMachineBuilder IrCompiler::InferTargetMachineBuilder(\n     const llvm::TargetOptions& target_options, llvm::CodeGenOptLevel opt_level,\n-    std::optional<tsl::port::CPUFeature> max_cpu_feature) {\n-  return [target_options, opt_level, max_cpu_feature] {\n-    return InferTargetMachine(target_options, opt_level, max_cpu_feature);\n+    const TargetMachineOptions& target_machine_options) {\n+  return [target_options, opt_level, target_machine_options] {\n+    return InferTargetMachine(target_options, opt_level,\n+                              target_machine_options);\n   };\n }\n \n@@ -474,31 +466,7 @@ llvm::CodeGenOptLevel IrCompiler::GetCodeGenOptLevel(\n \n absl::StatusOr<std::unique_ptr<llvm::TargetMachine>>\n IrCompiler::build_target_machine() const {\n-  TF_ASSIGN_OR_RETURN(auto target_machine, target_machine_builder_());\n-\n-  absl::string_view current_features(target_machine->getTargetFeatureString());\n-\n-  std::vector<std::string> additional_features;\n-  for (absl::string_view feature : absl::StrSplit(current_features, ',')) {\n-    // Scatter & gather can result in very poor performance.\n-    if (absl::StartsWith(feature, \"+avx512\")) {\n-      additional_features.push_back(\"+prefer-no-scatter\");\n-      additional_features.push_back(\"+prefer-no-gather\");\n-    }\n-  }\n-\n-  if (additional_features.empty()) {\n-    return target_machine;\n-  }\n-  std::string additional_features_str = absl::StrJoin(additional_features, \",\");\n-  if (current_features.empty()) {\n-    target_machine->setTargetFeatureString(additional_features_str);\n-  } else {\n-    target_machine->setTargetFeatureString(\n-        absl::StrCat(current_features, \",\", additional_features_str));\n-  }\n-\n-  return target_machine;\n+  return target_machine_builder_();\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "c170bc0755f9c7f081ab72199fb0963c27bab2b7",
            "filename": "third_party/xla/xla/backends/cpu/codegen/ir_compiler.h",
            "status": "modified",
            "additions": 6,
            "deletions": 12,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -35,9 +35,10 @@ limitations under the License.\n #include \"llvm/Support/Error.h\"\n #include \"llvm/Target/TargetMachine.h\"\n #include \"llvm/Target/TargetOptions.h\"\n+#include \"xla/backends/cpu/target_machine_options.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n+#include \"xla/service/cpu/executable.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n-#include \"tsl/platform/cpu_info.h\"\n \n namespace xla::cpu {\n \n@@ -64,10 +65,7 @@ class IrCompiler : public llvm::orc::IRCompileLayer::IRCompiler {\n     llvm::CodeGenOptLevel opt_level = llvm::CodeGenOptLevel::None;\n     bool optimize_for_size = false;\n \n-    // Maximum CPU instruction set for wich the compiler should generate code.\n-    // If instruction set is empty, compiler will generate code for all ISA\n-    // extensions detected on the current machine.\n-    std::optional<tsl::port::CPUFeature> max_cpu_feature;\n+    TargetMachineOptions target_machine_options;\n \n     llvm::FastMathFlags fast_math_flags;\n \n@@ -96,22 +94,18 @@ class IrCompiler : public llvm::orc::IRCompileLayer::IRCompiler {\n   IrCompiler(TargetMachineBuilder target_machine_builder, Options options,\n              CompilationHooks hooks);\n \n-  // Infers the `llvm::TargetMachine` for the current host. If `max_cpu_feature`\n-  // is provided, it will be used to constrain the set of features that LLVM\n-  // codegen (instruction selection) is allowed to use, e.g. it can be used to\n-  // explicitly disable certain AVX512 extensions, in case the compiled\n-  // executable will be serialized and later loaded on a different machine.\n+  // Infers the `llvm::TargetMachine` for the targeted host.\n   static absl::StatusOr<std::unique_ptr<llvm::TargetMachine>>\n   InferTargetMachine(const llvm::TargetOptions& target_options,\n                      llvm::CodeGenOptLevel opt_level,\n-                     std::optional<tsl::port::CPUFeature> max_cpu_feature);\n+                     const TargetMachineOptions& target_machine_options);\n \n   // Returns a target machine builder that uses `InferTargetMachine` defined\n   // above to infer the target machine for the given options.\n   static TargetMachineBuilder InferTargetMachineBuilder(\n       const llvm::TargetOptions& target_options,\n       llvm::CodeGenOptLevel opt_level,\n-      std::optional<tsl::port::CPUFeature> max_cpu_feature);\n+      const TargetMachineOptions& target_machine_options);\n \n   // Compiles a `module` to an ObjectFile.\n   llvm::Expected<std::unique_ptr<llvm::MemoryBuffer>> operator()("
        },
        {
            "sha": "4cd49d46fce01db71e67a8abd00bcf06c9a8453a",
            "filename": "third_party/xla/xla/backends/cpu/codegen/ir_compiler_test.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 4,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -39,7 +39,11 @@ limitations under the License.\n #include \"llvm/Target/TargetMachine.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n+#include \"xla/backends/cpu/target_machine_options.h\"\n+#include \"xla/debug_options_flags.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n+#include \"xla/service/cpu/cpu_compiler.h\"\n+#include \"xla/service/cpu/test_target_triple_helper.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -105,7 +109,9 @@ TEST(IrCompilerTest, OverrideIrCompilerCompileOptions) {\n \n   std::unique_ptr<IrCompiler> ir_compiler = IrCompiler::Create(\n       llvm::TargetOptions(),\n-      IrCompiler::Options{/*opt_level=*/llvm::CodeGenOptLevel::Aggressive},\n+      IrCompiler::Options{/*opt_level=*/llvm::CodeGenOptLevel::Aggressive,\n+                          /*optimize_for_size=*/false,\n+                          TargetMachineOptions(GetDebugOptionsFromFlags())},\n       compilation_hooks);\n \n   std::vector<std::unique_ptr<llvm::Module>> modules;\n@@ -189,14 +195,22 @@ TEST(IrCompilerTest, TestAdditionalFeatures) {\n       return absl::InternalError(\"Failed to lookup target: \" + error);\n     }\n \n+    TargetMachineOptions target_machine_options(triple, cpu_name, features);\n+\n     llvm::TargetOptions target_options;\n     return absl::WrapUnique(target->createTargetMachine(\n-        target_triple, cpu_name, features, target_options,\n+        llvm::Triple(target_machine_options.triple()),\n+        target_machine_options.cpu(),\n+        target_machine_options.GetTargetMachineFeatures(), target_options,\n         /*RM=*/std::nullopt));\n   };\n \n-  IrCompiler ir_compiler(std::move(builder), IrCompiler::Options(),\n-                         IrCompiler::CompilationHooks());\n+  IrCompiler ir_compiler(\n+      std::move(builder),\n+      IrCompiler::Options{/*opt_level=*/llvm::CodeGenOptLevel::None,\n+                          /*optimize_for_size=*/false,\n+                          TargetMachineOptions(GetDebugOptionsFromFlags())},\n+      IrCompiler::CompilationHooks());\n \n   {\n     has_avx512 = true;\n@@ -219,6 +233,29 @@ TEST(IrCompilerTest, TestAdditionalFeatures) {\n   }\n }\n \n+TEST(IrCompilerTest, TargetMachineOptionsAreCorrectlySet) {\n+  auto context = std::make_unique<llvm::LLVMContext>();\n+  IrCompiler::CompilationHooks compilation_hooks;\n+\n+  TargetMachineOptions target_machine_options(\n+      kTargetTripleForHost, kTargetCpuForHost, \"+foo-feature,-bar-feature\");\n+\n+  std::unique_ptr<IrCompiler> ir_compiler = IrCompiler::Create(\n+      llvm::TargetOptions(),\n+      IrCompiler::Options{/*opt_level=*/llvm::CodeGenOptLevel::Aggressive,\n+                          /*optimize_for_size=*/false, target_machine_options},\n+      compilation_hooks);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto target_machine,\n+                          ir_compiler->build_target_machine());\n+\n+  EXPECT_EQ(target_machine->getTargetCPU(), kTargetCpuForHost);\n+  EXPECT_EQ(target_machine->getTargetTriple().getTriple(),\n+            kTargetTripleForHost);\n+  EXPECT_EQ(target_machine->getTargetFeatureString(),\n+            \"+foo-feature,-bar-feature\");\n+}\n+\n }  // namespace\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "dfb4110e82f839c9861730e410b31910a216e311",
            "filename": "third_party/xla/xla/backends/cpu/codegen/jit_compiler_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 6,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fjit_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fjit_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fjit_compiler_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -38,13 +38,16 @@ limitations under the License.\n #include \"llvm/ExecutionEngine/Orc/ThreadSafeModule.h\"\n #include \"llvm/IR/DataLayout.h\"\n #include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/Support/CodeGen.h\"\n #include \"llvm/Support/Error.h\"\n #include \"llvm/Support/SourceMgr.h\"\n #include \"llvm/Target/TargetMachine.h\"\n #include \"llvm/Target/TargetOptions.h\"\n #include \"xla/backends/cpu/codegen/ir_compiler.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n #include \"xla/backends/cpu/runtime/function_library.h\"\n+#include \"xla/backends/cpu/target_machine_options.h\"\n+#include \"xla/debug_options_flags.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -97,9 +100,12 @@ TEST(JitCompilerTest, Compile) {\n     thread_pool.Schedule(std::move(task));\n   };\n \n-  std::unique_ptr<IrCompiler> ir_compiler =\n-      IrCompiler::Create(llvm::TargetOptions(), IrCompiler::Options(),\n-                         IrCompiler::CompilationHooks());\n+  std::unique_ptr<IrCompiler> ir_compiler = IrCompiler::Create(\n+      llvm::TargetOptions(),\n+      IrCompiler::Options{/*opt_level=*/llvm::CodeGenOptLevel::None,\n+                          /*optimize_for_size=*/false,\n+                          TargetMachineOptions(GetDebugOptionsFromFlags())},\n+      IrCompiler::CompilationHooks());\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto compiler,\n@@ -193,9 +199,12 @@ TEST(JitCompilerTest, ExternalDefinitionGenerator) {\n     return std::make_unique<ExternalDefinitionGenerator>();\n   };\n \n-  std::unique_ptr<IrCompiler> ir_compiler =\n-      IrCompiler::Create(llvm::TargetOptions(), IrCompiler::Options(),\n-                         IrCompiler::CompilationHooks());\n+  std::unique_ptr<IrCompiler> ir_compiler = IrCompiler::Create(\n+      llvm::TargetOptions(),\n+      IrCompiler::Options{/*opt_level=*/llvm::CodeGenOptLevel::None,\n+                          /*optimize_for_size=*/false,\n+                          TargetMachineOptions(GetDebugOptionsFromFlags())},\n+      IrCompiler::CompilationHooks());\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto compiler,"
        },
        {
            "sha": "5c86bdffd7de5f0ebed585460929a41af5b1f682",
            "filename": "third_party/xla/xla/backends/cpu/codegen/object_loader_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fobject_loader_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fobject_loader_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fobject_loader_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"llvm/IR/DataLayout.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/Object/ObjectFile.h\"\n+#include \"llvm/Support/CodeGen.h\"\n #include \"llvm/Support/Error.h\"\n #include \"llvm/Support/SourceMgr.h\"\n #include \"llvm/Target/TargetMachine.h\"\n@@ -51,6 +52,8 @@ limitations under the License.\n #include \"xla/backends/cpu/codegen/jit_compiler.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n #include \"xla/backends/cpu/runtime/function_library.h\"\n+#include \"xla/backends/cpu/target_machine_options.h\"\n+#include \"xla/debug_options_flags.h\"\n #include \"xla/service/cpu/executable.pb.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n@@ -114,7 +117,11 @@ TEST_P(ObjectLoaderTest, Load) {\n   ir_compiler_hooks.post_codegen = object_files_saver;\n \n   std::unique_ptr<IrCompiler> ir_compiler = IrCompiler::Create(\n-      llvm::TargetOptions(), IrCompiler::Options(), ir_compiler_hooks);\n+      llvm::TargetOptions(),\n+      IrCompiler::Options{/*opt_level=*/llvm::CodeGenOptLevel::None,\n+                          /*optimize_for_size=*/false,\n+                          TargetMachineOptions(GetDebugOptionsFromFlags())},\n+      ir_compiler_hooks);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto compiler,"
        },
        {
            "sha": "6aaea220050ae5311cb772b0532f57a79d2f10bf",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -48,12 +48,13 @@ cc_library(\n cc_library(\n     name = \"passes\",\n     srcs = [\n+        \"fuse_elementwise_pass.cc\",\n         \"linalg_elementwise_to_vector_pass.cc\",\n         \"lower_xtile_entry.cc\",\n         \"memref_copy_to_loops.cc\",\n         \"shlo_to_vector.cc\",\n-        \"tensor_ops_to_vector.cc\",\n-        \"xtile_to_vector.cc\",\n+        \"tensor_ops_to_bufferizable.cc\",\n+        \"vector_to_scalar_pass.cc\",\n     ],\n     hdrs = [\"passes.h\"],\n     deps = ["
        },
        {
            "sha": "d8248ebea93845a413569674478722472a048cd6",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/fuse_elementwise_pass.cc",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ffuse_elementwise_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ffuse_elementwise_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ffuse_elementwise_pass.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,73 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <memory>\n+#include <utility>\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Linalg/Transforms/Transforms.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/AffineExpr.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DEF_FUSEELEMENTWISEPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+class FuseElementwisePass\n+    : public impl::FuseElementwisePassBase<FuseElementwisePass> {\n+ public:\n+  using FuseElementwisePassBase::FuseElementwisePassBase;\n+\n+  void runOnOperation() override {\n+    mlir::MLIRContext* context = &getContext();\n+    mlir::RewritePatternSet patterns(context);\n+\n+    // Only fuse op with one use.\n+    mlir::linalg::ControlFusionFn fuse_control_fn =\n+        [](mlir::OpOperand* fused_operand) {\n+          mlir::Operation* producer = fused_operand->get().getDefiningOp();\n+          return producer && producer->hasOneUse();\n+        };\n+\n+    mlir::linalg::populateElementwiseOpsFusionPatterns(patterns,\n+                                                       fuse_control_fn);\n+\n+    if (mlir::failed(\n+            mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateFuseElementwisePass() {\n+  return std::make_unique<FuseElementwisePass>();\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "6ae31ae3594c00c525f39c2846885f685e1743ad",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/lower_xtile_entry.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 6,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -183,15 +183,24 @@ class LowerXTileEntryPass\n       mlir::Value workgroup_id = builder.create<ExtractWorkgroupIdOp>(\n           builder.getIndexType(), call_frame, WorkGroupDimension::x);\n \n+      auto flags = mlir::arith::IntegerOverflowFlags::nsw |\n+                   mlir::arith::IntegerOverflowFlags::nuw;\n+\n+      // This isn't needed for correctness as the workgroup id passed from the\n+      // runtime will always be in bounds but it constrains the range which LLVM\n+      // can then take advantage of.\n+      mlir::Value bounded_workgroup_id = builder.create<mlir::arith::MaxSIOp>(\n+          workgroup_id, builder.create<mlir::arith::ConstantIndexOp>(0));\n+\n       mlir::Value start_tile_id = builder.create<mlir::arith::MulIOp>(\n-          builder.getIndexType(), workgroup_id, tiles_per_workgroup_value);\n-      mlir::Value bounded_start_tile_id = builder.create<mlir::arith::MinSIOp>(\n-          builder.getIndexType(), start_tile_id, tile_count_value);\n+          bounded_workgroup_id, tiles_per_workgroup_value, flags);\n+      mlir::Value bounded_start_tile_id =\n+          builder.create<mlir::arith::MinSIOp>(start_tile_id, tile_count_value);\n \n       mlir::Value end_tile_id = builder.create<mlir::arith::AddIOp>(\n-          builder.getIndexType(), start_tile_id, tiles_per_workgroup_value);\n-      mlir::Value bounded_end_tile_id = builder.create<mlir::arith::MinSIOp>(\n-          builder.getIndexType(), end_tile_id, tile_count_value);\n+          start_tile_id, tiles_per_workgroup_value, flags);\n+      mlir::Value bounded_end_tile_id =\n+          builder.create<mlir::arith::MinSIOp>(end_tile_id, tile_count_value);\n \n       mlir::Value step = builder.create<mlir::arith::ConstantIndexOp>(1);\n "
        },
        {
            "sha": "b086a33d018ae4ffeba098ebc9ccd1b13c1e70e1",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -37,9 +37,10 @@ namespace xla::cpu {\n std::unique_ptr<mlir::Pass> CreateLinalgElementwiseToVectorPass();\n std::unique_ptr<mlir::Pass> CreateLowerXTileEntryPass();\n std::unique_ptr<mlir::Pass> CreateShloToVectorPass();\n-std::unique_ptr<mlir::Pass> CreateXTileToVectorPass();\n-std::unique_ptr<mlir::Pass> CreateTensorOpsToVectorPass();\n+std::unique_ptr<mlir::Pass> CreateTensorOpsToBufferizablePass();\n std::unique_ptr<mlir::Pass> CreateMemrefCopyToLoopsPass();\n+std::unique_ptr<mlir::Pass> CreateFuseElementwisePass();\n+std::unique_ptr<mlir::Pass> CreateVectorToScalarPass();\n \n #define GEN_PASS_REGISTRATION\n #include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\""
        },
        {
            "sha": "522094c7f10d7c93699d3712ca346a186ec641f9",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "modified",
            "additions": 32,
            "deletions": 15,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -15,17 +15,6 @@ limitations under the License.\n \n include \"mlir/Pass/PassBase.td\"\n \n-def XTileToVectorPass : Pass<\"xtile-cpu-xtile-to-vector\", \"mlir::ModuleOp\"> {\n-  let summary = \"Lowering xtile ops to vector ops\";\n-\n-  let constructor = \"CreateXTileToVectorPass()\";\n-\n-  let dependentDialects = [\n-    \"mlir::vector::VectorDialect\",\n-    \"xla::xtile::XTileDialect\",\n-  ];\n-}\n-\n def LowerXTileEntryPass : Pass<\"xtile-cpu-lower-xtile-entry\", \"mlir::ModuleOp\"> {\n   let summary = \"Lowers the entry function into the form required by the CPU runtime\";\n \n@@ -67,14 +56,19 @@ def LinalgElementwiseToVectorPass : Pass<\"xtile-cpu-linalg-elementwise-to-vector\n   ];\n }\n \n-def TensorOpsToVectorPass : Pass<\"xtile-cpu-tensor-ops-to-vector\",\n+def TensorOpsToBufferizablePass : Pass<\"xtile-cpu-tensor-ops-to-bufferizable\",\n                                  \"mlir::ModuleOp\"> {\n-  let summary = \"Lowering tensor dialect ops to vector ops\";\n+  let summary = \"Lowering tensor dialect ops to bufferizable ops\";\n+\n+  let description = [{\n+    Some tensor ops such as bitcast are not directly bufferizable. This pass\n+    lowers such ops to ops that have bufferization support.\n+  }];\n \n-  let constructor = \"CreateTensorOpsToVectorPass()\";\n+  let constructor = \"CreateTensorOpsToBufferizablePass()\";\n \n   let dependentDialects = [\n-    \"mlir::vector::VectorDialect\",\n+    \"mlir::arith::ArithDialect\",\n   ];\n }\n \n@@ -88,3 +82,26 @@ def MemrefCopyToLoopsPass : Pass<\"xtile-cpu-memref-copy-to-loops\",\n     \"::mlir::memref::MemRefDialect\",\n   ];\n }\n+\n+def FuseElementwisePass : Pass<\"xtile-cpu-fuse-elementwise\"> {\n+  let summary = \"Fuse linalg elementwise ops.\";\n+\n+  let description = [{\n+    This pass fuses multiple linalg elementwise ops into linalg elementwise ops\n+    that contain multiple instructions. This allows for fewer matrializations\n+    and fewer temporary allocations in bufferization.\n+  }];\n+}\n+\n+def VectorToScalarPass : Pass<\"xtile-cpu-vector-to-scalar\"> {\n+  let summary = \"Convert vector ops to scalar ops where possible.\";\n+\n+  let description = [{\n+    This pass converts elementwise vector ops to scalar ops if the operation\n+    acts on a single element.\n+  }];\n+\n+  let dependentDialects = [\n+    \"::mlir::vector::VectorDialect\",\n+  ];\n+}"
        },
        {
            "sha": "4949d2e4edd1cc67156ce3743abc229115427a1b",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/shlo_to_vector.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 23,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -296,23 +296,6 @@ struct LowerBroadcastInDim\n   }\n };\n \n-struct LowerReshape : mlir::OpRewritePattern<mlir::stablehlo::ReshapeOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      mlir::stablehlo::ReshapeOp op,\n-      mlir::PatternRewriter& rewriter) const override {\n-    auto source_vector = ReadTensorToVector(rewriter, op.getOperand());\n-    auto result_vector_type = GetVectorType(op.getType());\n-\n-    mlir::Value reshaped_vector = mlir::vector::ShapeCastOp::create(\n-        rewriter, op->getLoc(), result_vector_type, source_vector);\n-\n-    rewriter.replaceOp(op, WriteVectorToTensor(rewriter, reshaped_vector));\n-    return mlir::success();\n-  }\n-};\n-\n struct LowerIota : mlir::OpRewritePattern<mlir::stablehlo::IotaOp> {\n   using OpRewritePattern::OpRewritePattern;\n \n@@ -324,9 +307,9 @@ struct LowerIota : mlir::OpRewritePattern<mlir::stablehlo::IotaOp> {\n           op, \"iota op with rank != 1 is not supported\");\n     }\n \n-    auto result_vector_type = GetVectorType(op.getType());\n-    auto element_type = result_vector_type.getElementType();\n-    int64_t iota_size = result_vector_type.getNumElements();\n+    auto result_type = op.getType();\n+    auto element_type = result_type.getElementType();\n+    int64_t iota_size = result_type.getNumElements();\n \n     llvm::SmallVector<mlir::Attribute> iota_values(iota_size);\n     for (int idx = 0; idx != iota_size; ++idx) {\n@@ -335,9 +318,9 @@ struct LowerIota : mlir::OpRewritePattern<mlir::stablehlo::IotaOp> {\n \n     mlir::Value iota_const = mlir::arith::ConstantOp::create(\n         rewriter, op->getLoc(),\n-        mlir::DenseElementsAttr::get(result_vector_type, iota_values));\n+        mlir::DenseElementsAttr::get(result_type, iota_values));\n \n-    rewriter.replaceOp(op, WriteVectorToTensor(rewriter, iota_const));\n+    rewriter.replaceOp(op, iota_const);\n     return mlir::success();\n   }\n };\n@@ -350,7 +333,7 @@ class ShloToVectorPass : public impl::ShloToVectorPassBase<ShloToVectorPass> {\n     mlir::MLIRContext* context = &getContext();\n     mlir::RewritePatternSet patterns(context);\n     patterns.add<LowerTranspose, LowerDotGeneral, LowerReduce,\n-                 LowerBroadcastInDim, LowerReshape, LowerIota>(context);\n+                 LowerBroadcastInDim, LowerIota>(context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       signalPassFailure();"
        },
        {
            "sha": "69c45ca69964d9c7a87ab901498092b6ae08fcd1",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tensor_ops_to_bufferizable.cc",
            "status": "renamed",
            "additions": 13,
            "deletions": 39,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftensor_ops_to_bufferizable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftensor_ops_to_bufferizable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftensor_ops_to_bufferizable.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,74 +17,48 @@ limitations under the License.\n #include <memory>\n #include <utility>\n \n-#include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/IR/AffineExpr.h\"\n-#include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/MLIRContext.h\"\n-#include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/PatternMatch.h\"\n-#include \"mlir/IR/Value.h\"\n-#include \"mlir/IR/Visitors.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"xla/backends/cpu/codegen/tiled/transforms/lowering_utils.h\"\n #include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n \n namespace xla::cpu {\n \n-#define GEN_PASS_DECL_TENSOROPSTOVECTORPASS\n-#define GEN_PASS_DEF_TENSOROPSTOVECTORPASS\n+#define GEN_PASS_DECL_TENSOROPSTOBUFFERIZABLEPASS\n+#define GEN_PASS_DEF_TENSOROPSTOBUFFERIZABLEPASS\n #include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n \n namespace {\n \n-struct LowerFromElements\n-    : mlir::OpRewritePattern<mlir::tensor::FromElementsOp> {\n+struct TensorToArithBitcast : mlir::OpRewritePattern<mlir::tensor::BitcastOp> {\n   using OpRewritePattern::OpRewritePattern;\n \n   mlir::LogicalResult matchAndRewrite(\n-      mlir::tensor::FromElementsOp op,\n+      mlir::tensor::BitcastOp op,\n       mlir::PatternRewriter& rewriter) const override {\n-    mlir::VectorType vector_type = GetVectorType(op.getType());\n-    mlir::Value vector_from_elements =\n-        rewriter.create<mlir::vector::FromElementsOp>(op.getLoc(), vector_type,\n-                                                      op->getOperands());\n-    rewriter.replaceOp(op, WriteVectorToTensor(rewriter, vector_from_elements));\n+    rewriter.replaceOpWithNewOp<mlir::arith::BitcastOp>(op, op.getType(),\n+                                                        op.getOperand());\n     return mlir::success();\n   }\n };\n \n-struct LowerExtract : mlir::OpRewritePattern<mlir::tensor::ExtractOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      mlir::tensor::ExtractOp op,\n-      mlir::PatternRewriter& rewriter) const override {\n-    mlir::Value vector_input = ReadTensorToVector(rewriter, op.getTensor());\n-    llvm::SmallVector<mlir::OpFoldResult> indices(op.getIndices());\n-    rewriter.replaceOpWithNewOp<mlir::vector::ExtractOp>(op, vector_input,\n-                                                         indices);\n-    return mlir::success();\n-  }\n-};\n-\n-class TensorOpsToVectorPass\n-    : public impl::TensorOpsToVectorPassBase<TensorOpsToVectorPass> {\n+class TensorOpsToBufferizablePass\n+    : public impl::TensorOpsToBufferizablePassBase<\n+          TensorOpsToBufferizablePass> {\n  public:\n-  using TensorOpsToVectorPassBase::TensorOpsToVectorPassBase;\n+  using TensorOpsToBufferizablePassBase::TensorOpsToBufferizablePassBase;\n \n   void runOnOperation() override {\n     mlir::MLIRContext* context = &getContext();\n     mlir::RewritePatternSet patterns(context);\n-    patterns.add<LowerFromElements, LowerExtract>(context);\n+    patterns.add<TensorToArithBitcast>(context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       signalPassFailure();\n@@ -95,8 +69,8 @@ class TensorOpsToVectorPass\n \n }  // namespace\n \n-std::unique_ptr<mlir::Pass> CreateTensorOpsToVectorPass() {\n-  return std::make_unique<TensorOpsToVectorPass>();\n+std::unique_ptr<mlir::Pass> CreateTensorOpsToBufferizablePass() {\n+  return std::make_unique<TensorOpsToBufferizablePass>();\n }\n \n }  // namespace xla::cpu",
            "previous_filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tensor_ops_to_vector.cc"
        },
        {
            "sha": "b9adea8741e8cc4d3a215caaa3da576d0b1ed8ea",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/fuse_elementwise.mlir",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ffuse_elementwise.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ffuse_elementwise.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ffuse_elementwise.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,23 @@\n+// RUN: fusion_compiler_opt %s -split-input-file  \\\n+// RUN:   -linalg-generalize-named-ops -xtile-cpu-fuse-elementwise | FileCheck %s\n+\n+func.func @elementwise_add_to_vector(\n+    %lhs : tensor<8x1024xf32>,\n+    %rhs : tensor<8x1024xf32>) -> tensor<8x1024xf32> {\n+  %out = tensor.empty() : tensor<8x1024xf32>\n+\n+  %intermediate = linalg.elementwise kind=#linalg.elementwise_kind<mul>\n+    ins(%lhs, %rhs : tensor<8x1024xf32>, tensor<8x1024xf32>)\n+    outs(%out : tensor<8x1024xf32>) -> tensor<8x1024xf32>\n+  %result = linalg.elementwise kind=#linalg.elementwise_kind<add>\n+    ins(%intermediate, %rhs : tensor<8x1024xf32>, tensor<8x1024xf32>)\n+    outs(%out : tensor<8x1024xf32>) -> tensor<8x1024xf32>\n+  return %result : tensor<8x1024xf32>\n+}\n+\n+// CHECK: linalg.generic\n+// CHECK:    (%[[LHS:.*]]: f32, %[[RHS:.*]]: f32, %[[OUT:.*]]: f32):\n+// CHECK:       %[[MUL:.*]] = arith.mulf %[[LHS]], %[[RHS]] : f32\n+// CHECK:       %[[RES:.*]] = arith.addf %[[MUL]], %[[RHS]] : f32\n+// CHECK:       linalg.yield %[[RES]] : f32\n+// CHECK:     }"
        },
        {
            "sha": "b0e3e4702c241bfe9ad1d3c119bea4ae7200f170",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/lower_xtile_entry.mlir",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flower_xtile_entry.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flower_xtile_entry.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flower_xtile_entry.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -8,6 +8,7 @@ xtile.entry_func @simple_wrap(%input: memref<1024xf32> {xla.some_attr = 1},\n \n // CHECK: func.func @simple_wrap(%[[CALL_FRAME:.*]]: !xla_cpu.call_frame) -> !xla_cpu.error {\n \n+// CHECK-DAG: %[[C0:.*]] = arith.constant 0 : index\n // CHECK-DAG: %[[STEP:.*]] = arith.constant 1 : index\n // CHECK-DAG: %[[TILES_PER_WORKGROUP:.*]] = arith.constant 64 : index\n // CHECK-DAG: %[[TILE_COUNT:.*]] = arith.constant 1012 : index\n@@ -16,9 +17,10 @@ xtile.entry_func @simple_wrap(%input: memref<1024xf32> {xla.some_attr = 1},\n // CHECK: %[[OUTPUT:.*]] = xla_cpu.load %[[CALL_FRAME]], 1 : memref<32xf64>\n // CHECK: %[[WORKGROUP_ID:.*]] = xla_cpu.extract_workgroup_id %[[CALL_FRAME]], x\n \n-// CHECK: %[[START_IDX:.*]] = arith.muli %[[WORKGROUP_ID]], %[[TILES_PER_WORKGROUP]] : index\n+// CHECK: %[[BOUNDED_WORKGROUP_ID:.*]] = arith.maxsi %[[WORKGROUP_ID]], %[[C0]] : index\n+// CHECK: %[[START_IDX:.*]] = arith.muli %[[BOUNDED_WORKGROUP_ID]], %[[TILES_PER_WORKGROUP]] overflow<nsw, nuw> : index\n // CHECK: %[[CLAMPED_START_IDX:.*]] = arith.minsi %[[START_IDX]], %[[TILE_COUNT]] : index\n-// CHECK: %[[END_IDX:.*]] = arith.addi %[[START_IDX]], %[[TILES_PER_WORKGROUP]] : index\n+// CHECK: %[[END_IDX:.*]] = arith.addi %[[START_IDX]], %[[TILES_PER_WORKGROUP]] overflow<nsw, nuw> : index\n // CHECK: %[[CLAMPED_END_IDX:.*]] = arith.minsi %[[END_IDX]], %[[TILE_COUNT]] : index\n // CHECK: scf.for %[[IDX:.*]] = %[[CLAMPED_START_IDX]] to %[[CLAMPED_END_IDX]] step %[[STEP]] {\n // CHECK:   func.call @[[IMPL_FUNC:.*]](%[[INPUT]], %[[OUTPUT]], %[[IDX]]) : (memref<1024xf32>, memref<32xf64>, index) -> ()"
        },
        {
            "sha": "8dd5709c7fead921510bb7342f63127603d016c4",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/shlo_to_vector.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -164,21 +164,11 @@ func.func @broadcast_2D_tensor_outer(%input : tensor<4xf32>) -> tensor<4x32xf32>\n \n // -----\n \n-func.func @reshape(%input : tensor<4xf32>) -> tensor<2x1x2xf32> {\n-  %result = stablehlo.reshape %input : (tensor<4xf32>) -> tensor<2x1x2xf32>\n-  return %result : tensor<2x1x2xf32>\n-}\n-\n-// CHECK-LABEL: @reshape\n-// CHECK:vector.shape_cast {{.*}} : vector<4xf32> to vector<2x1x2xf32>\n-\n-// -----\n-\n func.func @iota() -> tensor<4xi32> {\n   %result = stablehlo.iota dim = 0 : tensor<4xi32>\n   return %result : tensor<4xi32>\n }\n \n // CHECK-LABEL: @iota\n-// CHECK: arith.constant dense<[0, 1, 2, 3]> : vector<4xi32>\n+// CHECK: arith.constant dense<[0, 1, 2, 3]> : tensor<4xi32>\n "
        },
        {
            "sha": "ea6c7d81a9003bde352e337f7cd697c3b10c0113",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/tensor_ops_to_bufferizable.mlir",
            "status": "added",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ftensor_ops_to_bufferizable.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ftensor_ops_to_bufferizable.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ftensor_ops_to_bufferizable.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,9 @@\n+// RUN: fusion_compiler_opt %s -xtile-cpu-tensor-ops-to-bufferizable -split-input-file | FileCheck %s\n+\n+\n+func.func @bitcast(%arg0 : tensor<8xf32>) -> tensor<8xi32> {\n+  // CHECK: %[[RESULT:.*]] = arith.bitcast %arg0 : tensor<8xf32> to tensor<8xi32>\n+  %result = arith.bitcast %arg0 : tensor<8xf32> to tensor<8xi32>\n+  // CHECK: return %[[RESULT]] : tensor<8xi32>\n+  return %result : tensor<8xi32>\n+}"
        },
        {
            "sha": "c091c0256fb38bad880ed2dfe88e81f2f4cfd5cd",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/tensor_ops_to_vector.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ftensor_ops_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ftensor_ops_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Ftensor_ops_to_vector.mlir?ref=813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9",
            "patch": "@@ -1,15 +0,0 @@\n-// RUN: fusion_compiler_opt %s -xtile-cpu-tensor-ops-to-vector -split-input-file | FileCheck %s\n-\n-func.func @from_elements(%input : f32) -> tensor<f32> {\n-  // CHECK: vector.from_elements %{{.*}} : vector<f32>\n-  %result = tensor.from_elements %input : tensor<f32>\n-  return %result : tensor<f32>\n-}\n-\n-// -----\n-\n-func.func @extract(%input : tensor<f32>) -> f32 {\n-  // CHECK: vector.extract %{{.*}}[] : f32 from vector<f32>\n-  %result = tensor.extract %input[] : tensor<f32>\n-  return %result : f32\n-}"
        },
        {
            "sha": "2b4705ddc4abcc2b153a0bafa6e668ec80fd0a67",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/vector_to_scalar.mlir",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fvector_to_scalar.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fvector_to_scalar.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fvector_to_scalar.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,53 @@\n+// RUN: fusion_compiler_opt %s -xtile-cpu-vector-to-scalar -split-input-file | FileCheck %s\n+\n+func.func @vector_to_scalar_0d(%arg0 : vector<f32>, %arg1 : vector<f32>) -> vector<f32> {\n+  // CHECK-DAG: %[[SCALAR0:.*]] = vector.extract %arg0[]\n+  // CHECK-DAG: %[[SCALAR1:.*]] = vector.extract %arg1[]\n+  // CHECK: %[[SCALAR_ADD:.*]] = arith.addf %[[SCALAR0]], %[[SCALAR1]] : f32\n+  // CHECK: %[[VECTOR_ADD:.*]] = vector.from_elements %[[SCALAR_ADD]] : vector<f32>\n+  %add = arith.addf %arg0, %arg1 : vector<f32>\n+  // CHECK: return %[[VECTOR_ADD]] : vector<f32>\n+  return %add : vector<f32>\n+}\n+\n+//-----\n+\n+func.func @vector_to_scalar_1d(%arg0 : vector<1xf32>, %arg1 : vector<1xf32>) -> vector<1xf32> {\n+  // CHECK-DAG: %[[SCALAR0:.*]] = vector.extract %arg0[0]\n+  // CHECK-DAG: %[[SCALAR1:.*]] = vector.extract %arg1[0]\n+  // CHECK: %[[SCALAR_MUL:.*]] = arith.mulf %[[SCALAR0]], %[[SCALAR1]] : f32\n+  // CHECK: %[[VECTOR_MUL:.*]] = vector.from_elements %[[SCALAR_MUL]] : vector<1xf32>\n+  %mul = arith.mulf %arg0, %arg1 : vector<1xf32>\n+  // CHECK: return %[[VECTOR_MUL]] : vector<1xf32>\n+  return %mul : vector<1xf32>\n+}\n+\n+//-----\n+\n+func.func @vector_to_scalar_2d(%arg0 : vector<1x1xf32>) -> vector<1x1xf32> {\n+  // CHECK: %[[SCALAR0:.*]] = vector.extract %arg0[0, 0]\n+  // CHECK: %[[SCALAR_COS:.*]] = math.cos %[[SCALAR0]] : f32\n+  // CHECK: %[[VECTOR_COS:.*]] = vector.from_elements %[[SCALAR_COS]] : vector<1x1xf32>\n+  %cos = math.cos %arg0 : vector<1x1xf32>\n+  // CHECK: return %[[VECTOR_COS]] : vector<1x1xf32>\n+  return %cos : vector<1x1xf32>\n+}\n+\n+//-----\n+\n+func.func @vector_to_scalar_constant() -> vector<1x1xf32> {\n+  // CHECK: %[[SCALAR:.*]] = arith.constant 1.000000e+00 : f32\n+  // CHECK: %[[VECTOR:.*]] = vector.from_elements %[[SCALAR]] : vector<1x1xf32>\n+  %cos = arith.constant dense<1.0> : vector<1x1xf32>\n+  // CHECK: return %[[VECTOR]] : vector<1x1xf32>\n+  return %cos : vector<1x1xf32>\n+}\n+\n+//-----\n+\n+func.func @skips_multi_element(%arg0 : vector<2xf32>) -> vector<2xf32> {\n+  // CHECK: %[[RES:.*]] = math.sin %arg0 : vector<2xf32>\n+  %sin = math.sin %arg0 : vector<2xf32>\n+  // CHECK: return %[[RES]] : vector<2xf32>\n+  return %sin : vector<2xf32>\n+}"
        },
        {
            "sha": "92513bb55b6d4b3beb93c968a0cc752569c9de59",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/xtile_to_vector.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir?ref=813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9",
            "patch": "@@ -1,58 +0,0 @@\n-// RUN: fusion_compiler_opt %s --xtile-cpu-xtile-to-vector -cse -split-input-file | FileCheck %s\n-\n-// CHECK-LABEL: @simple_insert_extract\n-// CHECK-SAME: (%[[INPUT:.*]]: memref<1024xf32>, %[[OUTPUT:.*]]: memref<1024xf32>, %[[TILE_ID:.*]]: index)\n-xtile.entry_func @simple_insert_extract(%input: memref<1024xf32>, %output: memref<1024xf32>, %tile_id: index) {\n-  // CHECK-DAG: %[[POISON:.*]] = ub.poison : f32\n-  // CHECK-DAG: %[[C_0:.*]] = arith.constant 0 : index\n-  // CHECK: %[[IN_SUBVIEW:.*]] = memref.subview %[[INPUT]][%[[TILE_ID]]] [1] [1]\n-  // CHECK-SAME: memref<1024xf32> to memref<1xf32, strided<[1], offset: ?>>\n-  // CHECK: %[[MASK:.*]] = vector.create_mask\n-  // CHECK: %[[EXTRACT:.*]] = vector.transfer_read %[[IN_SUBVIEW]][%[[C_0]]], %[[POISON]], %[[MASK]]\n-  %tile = xtile.extract %input[%tile_id][1][1] : memref<1024xf32> -> tensor<1xf32>\n-  // CHECK: %[[OUT_SUBVIEW:.*]] = memref.subview %[[OUTPUT]][%[[TILE_ID]]] [1] [1]\n-  // CHECK-SAME: memref<1024xf32> to memref<1xf32, strided<[1], offset: ?>>\n-  // CHECK: vector.transfer_write %[[EXTRACT]], %[[OUT_SUBVIEW]][%[[C_0]]], %[[MASK]]\n-  xtile.insert %tile into %output[%tile_id][1][1] : tensor<1xf32> -> memref<1024xf32>\n-  xtile.return\n-}\n-\n-// -----\n-\n-// CHECK: @reduce_dimension(%[[INPUT:.*]]: memref<16x1024xf32>, %[[OUTPUT:.*]]: memref<16x1024xf32>, %[[TILE_ID:.*]]: index)\n-xtile.entry_func @reduce_dimension(%input: memref<16x1024xf32>, %output: memref<16x1024xf32>, %tile_id: index) {\n-  // CHECK: %[[C_0:.*]] = arith.constant 0 : index\n-  %offset = arith.constant 0 : index\n-  // CHECK: memref.subview %[[INPUT]][%[[C_0]], %[[TILE_ID]]] [10, 1] [1, 1]\n-  // CHECK-SAME: memref<16x1024xf32> to memref<10xf32, strided<[1024], offset: ?>>\n-  %tile = xtile.extract %input[%offset, %tile_id][10, 1][1, 1] : memref<16x1024xf32> -> tensor<10xf32>\n-  // CHECK: memref.subview %[[OUTPUT]][%[[C_0]], %[[TILE_ID]]] [10, 1] [1, 1]\n-  // CHECK-SAME: memref<16x1024xf32> to memref<10xf32, strided<[1024], offset: ?>>\n-  xtile.insert %tile into %output[%offset, %tile_id][10, 1][1, 1] : tensor<10xf32> -> memref<16x1024xf32>\n-  xtile.return\n-}\n-\n-// -----\n-\n-// CHECK: @extract_strided(%[[SOURCE:.*]]: memref<16xf32>, %[[TILE_ID:.*]]: index)\n-func.func @extract_strided(%source: memref<16xf32>, %tile_id: index) -> tensor<8xf32> {\n-  // CHECK: memref.subview %[[SOURCE]][%[[TILE_ID]]] [8] [2] :\n-  // CHECK-SAME: memref<16xf32> to memref<8xf32, strided<[2], offset: ?>>\n-  %tile = xtile.extract %source[%tile_id][8][2] : memref<16xf32> -> tensor<8xf32>\n-  return %tile : tensor<8xf32>\n-}\n-\n-// -----\n-\n-// CHECK: @insert_strided(\n-// CHECK-SAME: %[[SOURCE:.*]]: tensor<8xf32>,\n-// CHECK-SAME: %[[DESTINATION:.*]]: memref<16xf32>,\n-// CHECK-SAME: %[[TILE_ID:.*]]: index)\n-func.func @insert_strided(%source: tensor<8xf32>, %destination: memref<16xf32>, %tile_id: index) {\n-  // CHECK: memref.subview %[[DESTINATION]][%[[TILE_ID]]] [8] [2] :\n-  // CHECK-SAME: memref<16xf32> to memref<8xf32, strided<[2], offset: ?>>\n-  xtile.insert %source into %destination[%tile_id][8][2] : tensor<8xf32> -> memref<16xf32>\n-  return\n-}\n-\n-"
        },
        {
            "sha": "5a4dc5addcf83d77983a38008af7c4137effa109",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vector_to_scalar_pass.cc",
            "status": "added",
            "additions": 170,
            "deletions": 0,
            "changes": 170,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvector_to_scalar_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvector_to_scalar_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvector_to_scalar_pass.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,170 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DEF_VECTORTOSCALARPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+mlir::Type TypeConverter(mlir::Type type) {\n+  auto maybe_vector_type = mlir::dyn_cast<mlir::VectorType>(type);\n+  if (!maybe_vector_type) {\n+    return type;\n+  }\n+  if (maybe_vector_type.getNumElements() != 1) {\n+    return type;\n+  }\n+\n+  return maybe_vector_type.getElementType();\n+}\n+\n+mlir::Value SourceMaterialization(mlir::OpBuilder& builder,\n+                                  mlir::Type result_type,\n+                                  mlir::ValueRange inputs, mlir::Location loc) {\n+  if (inputs.size() != 1) {\n+    return nullptr;\n+  }\n+  return mlir::vector::FromElementsOp::create(builder, loc, result_type,\n+                                              inputs.front());\n+}\n+\n+mlir::Value TargetMaterialization(mlir::OpBuilder& builder,\n+                                  mlir::Type result_type,\n+                                  mlir::ValueRange inputs, mlir::Location loc) {\n+  if (inputs.size() != 1) {\n+    return nullptr;\n+  }\n+  auto input_vector_type =\n+      mlir::cast<mlir::VectorType>(inputs.front().getType());\n+  llvm::SmallVector<int64_t> indices(input_vector_type.getRank(), 0);\n+  return mlir::vector::ExtractOp::create(builder, loc, inputs.front(), indices);\n+}\n+\n+struct ElementwiseConverter\n+    : public mlir::OpTraitConversionPattern<mlir::OpTrait::Elementwise> {\n+ public:\n+  using OpTraitConversionPattern::OpTraitConversionPattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::Operation* op, mlir::ArrayRef<mlir::Value> operands,\n+      mlir::ConversionPatternRewriter& rewriter) const override {\n+    llvm::SmallVector<mlir::Type> new_result_types;\n+    if (mlir::failed(getTypeConverter()->convertTypes(op->getResultTypes(),\n+                                                      new_result_types))) {\n+      return rewriter.notifyMatchFailure(op, \"failed to convert type\");\n+    }\n+\n+    mlir::IRMapping mapping;\n+    mapping.map(op->getOperands(), operands);\n+    mlir::Operation* new_op = rewriter.clone(*op, mapping);\n+\n+    for (auto [results, new_type] :\n+         llvm::zip(new_op->getResults(), new_result_types)) {\n+      results.setType(new_type);\n+    }\n+\n+    rewriter.replaceOp(op, new_op);\n+    return mlir::success();\n+  }\n+};\n+\n+struct ConstantConversionPattern\n+    : public mlir::OpConversionPattern<mlir::arith::ConstantOp> {\n+  using OpConversionPattern<mlir::arith::ConstantOp>::OpConversionPattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::arith::ConstantOp op, OpAdaptor adaptor,\n+      mlir::ConversionPatternRewriter& rewriter) const override {\n+    auto dense_attr = mlir::cast<mlir::DenseElementsAttr>(op.getValueAttr());\n+    auto scalar_attr = dense_attr.getValues<mlir::TypedAttr>()[0];\n+    rewriter.replaceOpWithNewOp<mlir::arith::ConstantOp>(op, scalar_attr);\n+\n+    return mlir::success();\n+  }\n+};\n+\n+class VectorToScalarPass\n+    : public impl::VectorToScalarPassBase<VectorToScalarPass> {\n+ public:\n+  using VectorToScalarPassBase::VectorToScalarPassBase;\n+\n+  void runOnOperation() override {\n+    mlir::TypeConverter type_converter;\n+    type_converter.addConversion(&TypeConverter);\n+\n+    type_converter.addSourceMaterialization(&SourceMaterialization);\n+    type_converter.addTargetMaterialization(&TargetMaterialization);\n+\n+    mlir::ConversionTarget target(getContext());\n+\n+    target.markUnknownOpDynamicallyLegal(\n+        [&](mlir::Operation* op) -> std::optional<bool> {\n+          if (op->hasTrait<mlir::OpTrait::Elementwise>()) {\n+            return type_converter.isLegal(op);\n+          }\n+          return std::nullopt;\n+        });\n+\n+    target.addDynamicallyLegalOp<mlir::arith::ConstantOp>(\n+        [&](mlir::arith::ConstantOp op) {\n+          return type_converter.isLegal(op.getOperation());\n+        });\n+\n+    mlir::RewritePatternSet patterns(&getContext());\n+\n+    patterns.add<ElementwiseConverter, ConstantConversionPattern>(\n+        type_converter, &getContext());\n+\n+    if (mlir::failed(mlir::applyPartialConversion(getOperation(), target,\n+                                                  std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateVectorToScalarPass() {\n+  return std::make_unique<VectorToScalarPass>();\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "4c3c89e0471292a62c9f73409155a7b020ba0ea2",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/xtile_to_vector.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 224,
            "changes": 224,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc?ref=813bf1f77d9e047b2158cbdf19d5ee4dc97ba3b9",
            "patch": "@@ -1,224 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include <cassert>\n-#include <cstdint>\n-#include <memory>\n-#include <optional>\n-#include <utility>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"llvm/ADT/ArrayRef.h\"\n-#include \"llvm/ADT/DenseSet.h\"\n-#include \"llvm/ADT/STLExtras.h\"\n-#include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/ADT/SmallVectorExtras.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n-#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n-#include \"mlir/Dialect/UB/IR/UBOps.h\"\n-#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n-#include \"mlir/IR/AffineExpr.h\"\n-#include \"mlir/IR/Attributes.h\"\n-#include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinTypes.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"mlir/IR/OpDefinition.h\"\n-#include \"mlir/IR/PatternMatch.h\"\n-#include \"mlir/IR/Value.h\"\n-#include \"mlir/IR/ValueRange.h\"\n-#include \"mlir/IR/Visitors.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Support/LLVM.h\"\n-#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"xla/backends/cpu/codegen/tiled/transforms/lowering_utils.h\"\n-#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n-#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n-\n-namespace xla::cpu {\n-\n-#define GEN_PASS_DECL_XTILETOVECTORPASS\n-#define GEN_PASS_DEF_XTILETOVECTORPASS\n-#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n-\n-namespace {\n-\n-// Dims are dropped in the subview so we use the identity map.\n-mlir::AffineMapAttr GetIdentityMap(xtile::TiledBufferInterface op) {\n-  int64_t rank = op.getTile().getType().getRank();\n-  return mlir::AffineMapAttr::get(\n-      mlir::AffineMap::getMultiDimIdentityMap(rank, op.getContext()));\n-}\n-\n-mlir::TypedValue<mlir::MemRefType> GetSubView(\n-    mlir::ImplicitLocOpBuilder& builder, xtile::TiledBufferInterface op) {\n-  auto get_static_fold_result = [&](llvm::ArrayRef<int64_t> input) {\n-    return llvm::map_to_vector(input, [&builder](int64_t value) {\n-      return mlir::OpFoldResult(builder.getIndexAttr(value));\n-    });\n-  };\n-\n-  auto offsets = llvm::SmallVector<mlir::OpFoldResult>(op.getOffsets());\n-  auto full_tile_shape = get_static_fold_result(op.getFullTileShape());\n-  auto strides = get_static_fold_result(op.getStrides());\n-\n-  mlir::MemRefType subview_type =\n-      mlir::memref::SubViewOp::inferRankReducedResultType(\n-          op.getTile().getType().getShape(), op.getBuffer().getType(), offsets,\n-          full_tile_shape, get_static_fold_result(op.getStrides()));\n-\n-  return builder.create<mlir::memref::SubViewOp>(\n-      subview_type, op.getBuffer(), offsets, full_tile_shape, strides);\n-}\n-\n-llvm::SmallVector<mlir::Value> GetZeroIndexVector(\n-    mlir::ImplicitLocOpBuilder& builder, int64_t rank) {\n-  return llvm::SmallVector<mlir::Value>(\n-      rank, builder.create<mlir::arith::ConstantIndexOp>(0));\n-}\n-\n-mlir::ArrayAttr GetInBoundsAttr(mlir::ImplicitLocOpBuilder& builder,\n-                                int64_t rank) {\n-  // TODO(willfroom): Add proper support for inBounds attr.\n-  llvm::SmallVector<mlir::Attribute> in_bounds(rank,\n-                                               builder.getBoolAttr(false));\n-  return builder.getArrayAttr(in_bounds);\n-}\n-\n-// Get the mask for the given transfer_<read/write> op on a subview of the\n-// original memeref.\n-// The inequality we need to satisfy in 1D is:\n-//  1. offset + subview_idx * stride <= size - 1\n-//  2. subview_idx * stride <= size - 1 - offset\n-//  3. subview_idx <= (size - 1 - offset) / stride\n-//  4. subview_idx < ((size - 1 - offset) / stride) + 1\n-//  5. subview_idx < (size + stride - 1 - offset) / stride\n-mlir::Value GetMask(mlir::ImplicitLocOpBuilder& builder,\n-                    xtile::TiledBufferInterface op) {\n-  mlir::RankedTensorType tile_tensor_type = op.getTile().getType();\n-\n-  auto get_const_index_op = [&](int64_t value) {\n-    return builder.create<mlir::arith::ConstantIndexOp>(value);\n-  };\n-\n-  if (tile_tensor_type.getRank() == 0) {\n-    // Vector transfer read/write currently don't support 0D masks.\n-    auto mask_0D_type = mlir::VectorType::get({1}, builder.getI1Type());\n-    return builder.create<mlir::vector::CreateMaskOp>(\n-        mask_0D_type, mlir::OpFoldResult(builder.getIndexAttr(1)));\n-  }\n-\n-  llvm::SmallDenseSet<unsigned> reduced_dims = op.getReducedDimensions();\n-  llvm::SmallVector<mlir::Value> upper_bounds;\n-  int64_t idx = 0;\n-  for (auto [offset, size, stride] :\n-       llvm::zip(op.getOffsets(), op.getBuffer().getType().getShape(),\n-                 op.getStrides())) {\n-    if (reduced_dims.contains(idx++)) {\n-      continue;\n-    }\n-    upper_bounds.push_back(builder.create<mlir::arith::DivSIOp>(\n-        builder.create<mlir::arith::SubIOp>(\n-            get_const_index_op(size + stride - 1), offset),\n-        get_const_index_op(stride)));\n-  }\n-\n-  auto mask_type = mlir::VectorType::get(op.getTile().getType().getShape(),\n-                                         builder.getI1Type());\n-  return builder.create<mlir::vector::CreateMaskOp>(mask_type, upper_bounds);\n-}\n-\n-struct LowerExtractTile : mlir::OpRewritePattern<xtile::ExtractTileOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      xtile::ExtractTileOp op, mlir::PatternRewriter& rewriter) const override {\n-    mlir::ImplicitLocOpBuilder builder(op->getLoc(), rewriter);\n-    auto vector_type = GetVectorType(op.getResult().getType());\n-\n-    mlir::TypedValue<mlir::MemRefType> buffer_subview = GetSubView(builder, op);\n-\n-    int64_t reduced_rank = vector_type.getRank();\n-\n-    // The subview is already offset so the read has zero offsets.\n-    auto zero_index = GetZeroIndexVector(builder, reduced_rank);\n-    mlir::Value padding =\n-        builder.create<mlir::ub::PoisonOp>(vector_type.getElementType());\n-    mlir::Value mask = GetMask(builder, op);\n-    auto in_bounds = GetInBoundsAttr(builder, reduced_rank);\n-\n-    mlir::Value vector_value = rewriter.create<mlir::vector::TransferReadOp>(\n-        op->getLoc(), vector_type, buffer_subview, zero_index,\n-        GetIdentityMap(op), padding, mask, in_bounds);\n-\n-    rewriter.replaceOp(op, WriteVectorToTensor(builder, vector_value));\n-    return mlir::success();\n-  }\n-};\n-\n-struct LowerInsertTile : mlir::OpRewritePattern<xtile::InsertTileOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      xtile::InsertTileOp op, mlir::PatternRewriter& rewriter) const override {\n-    mlir::ImplicitLocOpBuilder builder(op->getLoc(), rewriter);\n-    mlir::TypedValue<mlir::VectorType> vector_tile =\n-        ReadTensorToVector(builder, op.getSource());\n-\n-    mlir::TypedValue<mlir::MemRefType> buffer_subview = GetSubView(builder, op);\n-\n-    int64_t reduced_rank = vector_tile.getType().getRank();\n-\n-    // The subview is already offset so the write has zero offsets.\n-    auto zero_index = GetZeroIndexVector(builder, reduced_rank);\n-    mlir::Value mask = GetMask(builder, op);\n-    auto in_bounds = GetInBoundsAttr(builder, reduced_rank);\n-\n-    mlir::vector::TransferWriteOp transfer_write =\n-        builder.create<mlir::vector::TransferWriteOp>(\n-            vector_tile, buffer_subview, zero_index, GetIdentityMap(op), mask,\n-            in_bounds);\n-\n-    rewriter.replaceOp(op, transfer_write);\n-    return mlir::success();\n-  }\n-};\n-\n-class XTileToVectorPass\n-    : public impl::XTileToVectorPassBase<XTileToVectorPass> {\n- public:\n-  using XTileToVectorPassBase::XTileToVectorPassBase;\n-\n-  void runOnOperation() override {\n-    mlir::MLIRContext* context = &getContext();\n-    mlir::RewritePatternSet patterns(context);\n-    patterns.add<LowerExtractTile, LowerInsertTile>(context);\n-    if (mlir::failed(\n-            mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n-      signalPassFailure();\n-      return;\n-    }\n-  }\n-};\n-\n-}  // namespace\n-\n-std::unique_ptr<mlir::Pass> CreateXTileToVectorPass() {\n-  return std::make_unique<XTileToVectorPass>();\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "c455737533765d79da73d3f3cab8426f1474e9ca",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tools/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,7 +20,10 @@ cc_library(\n     testonly = True,\n     srcs = [\"ir_compiler_opt_main.cc\"],\n     deps = [\n+        \"//xla:debug_options_flags\",\n+        \"//xla/backends/cpu:target_machine_options\",\n         \"//xla/backends/cpu/codegen:ir_compiler\",\n+        \"//xla/service/cpu:cpu_compiler_pure\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "03c71381010e3ac51a671f19e79660fc593c30cb",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tools/ir_compiler_opt_main.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Fir_compiler_opt_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Fir_compiler_opt_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Fir_compiler_opt_main.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -40,6 +40,9 @@ limitations under the License.\n #include \"llvm/Target/TargetMachine.h\"\n #include \"llvm/Target/TargetOptions.h\"\n #include \"xla/backends/cpu/codegen/ir_compiler.h\"\n+#include \"xla/backends/cpu/target_machine_options.h\"\n+#include \"xla/debug_options_flags.h\"\n+#include \"xla/service/cpu/cpu_compiler.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -114,11 +117,12 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> ParseLlvmIr(\n absl::StatusOr<std::string> RunIrCompilerPasses(const IrCompilerOptConfig& opts,\n                                                 int argc, char** argv) {\n   llvm::TargetOptions target_options;\n-  IrCompiler::Options ir_compiler_options;\n   CHECK(opts.opt_level >= 0 && opts.opt_level <= 3)\n       << \"Optimization level must be between 0 and 3\";\n-  ir_compiler_options.opt_level =\n-      static_cast<llvm::CodeGenOptLevel>(opts.opt_level);\n+  IrCompiler::Options ir_compiler_options{\n+      /*opt_level=*/static_cast<llvm::CodeGenOptLevel>(opts.opt_level),\n+      /*optimize_for_size=*/false,\n+      TargetMachineOptions(GetDebugOptionsFromFlags())};\n   auto ir_compiler = IrCompiler::Create(target_options, ir_compiler_options,\n                                         IrCompiler::CompilationHooks());\n \n@@ -133,7 +137,7 @@ absl::StatusOr<std::string> RunIrCompilerPasses(const IrCompilerOptConfig& opts,\n       std::unique_ptr<llvm::TargetMachine> target_machine,\n       ir_compiler->InferTargetMachine(\n           target_options, static_cast<llvm::CodeGenOptLevel>(opts.opt_level),\n-          std::nullopt));\n+          ir_compiler_options.target_machine_options));\n \n   llvm::Error error = ir_compiler->RunIrPasses(*module, target_machine.get());\n   if (error) {"
        },
        {
            "sha": "b637660cff9c90e45bd2fa9ae761735d3cfd0f47",
            "filename": "third_party/xla/xla/backends/cpu/nanort/ifrt_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -680,6 +680,10 @@ class ShardedNanoArray final : public NanoValue<ShardedNanoArray, ifrt::Array> {\n \n   absl::StatusOr<tsl::RCReference<NanoArray>> Assemble(\n       ifrt::ShardingRef sharding) {\n+    if (sharding->IsFullyReplicated()) {\n+      return shards_[0];\n+    }\n+\n     TF_ASSIGN_OR_RETURN(auto index_domains, sharding->IndexDomains(shape()));\n     if (ABSL_PREDICT_FALSE(index_domains.size() != shards_.size())) {\n       return absl::FailedPreconditionError("
        },
        {
            "sha": "bbff542362030171104dde06040180e30cdfefdc",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 50,
            "deletions": 4,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -24,6 +24,13 @@ filegroup(\n     srcs = [\n         \"convolution_lib_f16.cc\",\n         \"convolution_lib_f32.cc\",\n+        \"dot_lib_c128.cc\",\n+        \"dot_lib_c64.cc\",\n+        \"dot_lib_f16.cc\",\n+        \"dot_lib_f32.cc\",\n+        \"dot_lib_f64.cc\",\n+        \"dot_lib_s32.cc\",\n+        \"dot_lib_s8.cc\",\n         \"rng_state_lib.cc\",\n         \"sort_lib.cc\",\n     ],\n@@ -34,6 +41,7 @@ filegroup(\n     name = \"runtime_hdrs\",\n     srcs = [\n         \"convolution_lib.h\",\n+        \"dot_lib.h\",\n         \"kernel_c_api.h\",\n         \"rng_state_lib.h\",\n         \"sort_lib.h\",\n@@ -317,6 +325,7 @@ cc_library(\n     deps = [\n         \":thunk\",\n         \":thunk_executor\",\n+        \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n@@ -338,6 +347,7 @@ xla_cc_test(\n         \":conditional_thunk\",\n         \":thunk\",\n         \":thunk_testlib\",\n+        \"//xla:shape_util\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n@@ -743,6 +753,28 @@ cc_library(\n         \"dot_lib_s8.cc\",\n     ],\n     hdrs = [\"dot_lib.h\"],\n+    deps = [\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@eigen_archive//:eigen3\",\n+    ],\n+)\n+\n+# By including `eigen_contraction_kernel` into the list of dependencies, we enable the use of\n+# oneDNN Eigen contraction kernel for jit-compiling microkernels. This brings oneDNN to the list\n+# of transitive dependencies, and some clients (e.g. tfcompile) don't want this extra dependency.\n+cc_library(\n+    name = \"dot_lib_onednn\",\n+    srcs = [\n+        \"dot_lib_c128.cc\",\n+        \"dot_lib_c64.cc\",\n+        \"dot_lib_f16.cc\",\n+        \"dot_lib_f32.cc\",\n+        \"dot_lib_f64.cc\",\n+        \"dot_lib_s32.cc\",\n+        \"dot_lib_s8.cc\",\n+    ],\n+    hdrs = [\"dot_lib.h\"],\n     deps = [\n         \"//xla/tsl/framework/contraction:eigen_contraction_kernel\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -757,7 +789,7 @@ cc_library(\n     hdrs = [\"dot_thunk.h\"],\n     deps = [\n         \":dot_dims\",\n-        \":dot_lib\",\n+        \":dot_lib_onednn\",\n         \":thunk\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n@@ -834,9 +866,8 @@ xla_cc_test(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n \n@@ -1036,6 +1067,7 @@ cc_library(\n     hdrs = [\"sort_lib.h\"],\n     deps = [\n         \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/base:dynamic_annotations\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -1110,6 +1142,8 @@ cc_library(\n         \":buffer_allocations\",\n         \":thunk\",\n         \":thunk_executor\",\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:device_memory\",\n@@ -1135,6 +1169,7 @@ xla_cc_test(\n         \":thunk_testlib\",\n         \":while_thunk\",\n         \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n@@ -1171,18 +1206,29 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"topk_lib\",\n+    hdrs = [\"topk_lib.h\"],\n+    deps = [\n+        \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/base:dynamic_annotations\",\n+        \"@com_google_absl//absl/numeric:bits\",\n+    ],\n+)\n+\n cc_library(\n     name = \"topk_thunk\",\n     srcs = [\"topk_thunk.cc\"],\n     hdrs = [\"topk_thunk.h\"],\n     deps = [\n         \":thunk\",\n+        \":topk_lib\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/service/cpu:runtime_topk\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:dynamic_annotations\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status:statusor\",\n     ],"
        },
        {
            "sha": "908fad230246dd920dacad468529abd75d188e83",
            "filename": "third_party/xla/xla/backends/cpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -91,11 +91,13 @@ CollectiveThunk::CollectiveThunk(CollectiveKind collective_kind,\n Thunk::BufferUses CollectiveThunk::buffer_uses() const {\n   BufferUses uses;\n   uses.reserve(source_buffers().size() + destination_buffers().size());\n-  for (auto& source_buffer : source_buffers()) {\n-    uses.push_back(BufferUse::Read(source_buffer));\n+  for (int i = 0; i < source_buffers().size(); i++) {\n+    uses.push_back(BufferUse::Read(op_buffers_.source_buffers[i],\n+                                   op_buffers_.source_shapes[i]));\n   }\n-  for (auto& destination_buffer : destination_buffers()) {\n-    uses.push_back(BufferUse::Write(destination_buffer));\n+  for (int i = 0; i < destination_buffers().size(); i++) {\n+    uses.push_back(BufferUse::Write(op_buffers_.destination_buffers[i],\n+                                    op_buffers_.destination_shapes[i]));\n   }\n   return uses;\n }"
        },
        {
            "sha": "22b7d01cc229062bbfd1de023b984547525b63b7",
            "filename": "third_party/xla/xla/backends/cpu/runtime/conditional_thunk.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 11,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -31,12 +31,36 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk_executor.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n namespace xla::cpu {\n+namespace {\n+\n+absl::StatusOr<Shape> ShapeForBranchIndexBuffer(\n+    BufferAllocation::Slice& branch_index_buffer) {\n+  // See operation semantics documentation:\n+  // https://openxla.org/xla/operation_semantics#conditional\n+\n+  // Branch index is pred[].\n+  if (branch_index_buffer.size() == sizeof(bool)) {\n+    return ShapeUtil::MakeShape(PRED, {1});\n+  }\n+\n+  // Branch index is s32[].\n+  if (branch_index_buffer.size() == sizeof(int32_t)) {\n+    return ShapeUtil::MakeShape(S32, {1});\n+  }\n+\n+  return Internal(\"Unsupported branch index buffer size %d\",\n+                  branch_index_buffer.size());\n+}\n+\n+}  // namespace\n \n absl::StatusOr<std::unique_ptr<ConditionalThunk>> ConditionalThunk::Create(\n     Info info, BufferAllocation::Slice branch_index_buffer,\n@@ -48,16 +72,22 @@ absl::StatusOr<std::unique_ptr<ConditionalThunk>> ConditionalThunk::Create(\n                         ThunkExecutor::Create(std::move(branch_sequence)));\n     branch_executors.push_back(std::move(branch_executor));\n   }\n-  return absl::WrapUnique(new ConditionalThunk(std::move(info),\n-                                               std::move(branch_index_buffer),\n-                                               std::move(branch_executors)));\n+\n+  TF_ASSIGN_OR_RETURN(Shape shape,\n+                      ShapeForBranchIndexBuffer(branch_index_buffer));\n+\n+  return absl::WrapUnique(\n+      new ConditionalThunk(std::move(info), std::move(branch_index_buffer),\n+                           shape, std::move(branch_executors)));\n }\n \n ConditionalThunk::ConditionalThunk(Info info,\n                                    BufferAllocation::Slice branch_index_buffer,\n+                                   Shape branch_index_buffer_shape,\n                                    std::vector<ThunkExecutor> branch_executors)\n     : Thunk(Kind::kConditional, std::move(info)),\n       branch_index_buffer_(branch_index_buffer),\n+      branch_index_buffer_shape_(branch_index_buffer_shape),\n       branch_executors_(std::move(branch_executors)) {}\n \n tsl::AsyncValueRef<Thunk::ExecuteEvent> ConditionalThunk::Execute(\n@@ -79,18 +109,13 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> ConditionalThunk::Execute(\n                : branch_index;\n   };\n \n-  // See operation semantics documentation:\n-  // https://openxla.org/xla/operation_semantics#conditional\n-\n-  // Branch index is pred[].\n-  if (branch_index_buffer_.size() == sizeof(bool)) {\n+  if (branch_index_buffer_shape_.element_type() == PRED) {\n     bool* pred = reinterpret_cast<bool*>(branch_index_data.opaque());\n     VLOG(3) << \"  loaded pred[] branch index: \" << *pred;\n     return branch_executors_.at(*pred ? 0 : 1).Execute(params);\n   }\n \n-  // Branch index is s32[].\n-  if (branch_index_buffer_.size() == sizeof(int32_t)) {\n+  if (branch_index_buffer_shape_.element_type() == S32) {\n     int32_t* index = reinterpret_cast<int32_t*>(branch_index_data.opaque());\n     VLOG(3) << \"  loaded s32[] branch index: \" << *index;\n     return branch_executors_.at(clamp(*index)).Execute(params);\n@@ -101,7 +126,8 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> ConditionalThunk::Execute(\n }\n \n ConditionalThunk::BufferUses ConditionalThunk::buffer_uses() const {\n-  BufferUses buffer_uses = {BufferUse::Read(branch_index_buffer_)};\n+  BufferUses buffer_uses = {\n+      BufferUse::Read(branch_index_buffer_, branch_index_buffer_shape_)};\n   for (const auto& branch_executor : branch_executors_) {\n     BufferUses uses = branch_executor.buffer_uses();\n     buffer_uses.insert(buffer_uses.end(), uses.begin(), uses.end());"
        },
        {
            "sha": "4baca3d05da38b8dd644b27d829745cac1d223a6",
            "filename": "third_party/xla/xla/backends/cpu/runtime/conditional_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk_executor.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n \n namespace xla::cpu {\n@@ -53,9 +54,11 @@ class ConditionalThunk final : public Thunk {\n \n  private:\n   ConditionalThunk(Info info, BufferAllocation::Slice branch_index_buffer,\n+                   Shape branch_index_buffer_shape,\n                    std::vector<ThunkExecutor> branch_executors);\n \n   BufferAllocation::Slice branch_index_buffer_;\n+  Shape branch_index_buffer_shape_;\n   std::vector<ThunkExecutor> branch_executors_;\n };\n "
        },
        {
            "sha": "b2a918018fc137ad25f81feb7bd63d1fc265a1a1",
            "filename": "third_party/xla/xla/backends/cpu/runtime/conditional_thunk_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -25,6 +25,8 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n \n@@ -33,20 +35,24 @@ namespace {\n \n TEST(ConditionalThunkTest, BufferUses) {\n   BufferAllocation alloc(0, 1024, 0);\n+  Shape branch_index_slice_shape = ShapeUtil::MakeShape(S32, {1});\n   BufferAllocation::Slice branch_index_slice(&alloc, 0, sizeof(int32_t));\n-  BufferAllocation::Slice read_slice(&alloc, 10, 10);\n+  Shape read_slice_shape = ShapeUtil::MakeShape(F32, {4});\n+  BufferAllocation::Slice read_slice(&alloc, 10, 12);\n \n   std::vector<ThunkSequence> branch_sequences(1);\n-  branch_sequences[0].push_back(\n-      std::make_unique<BufferUseThunk>(BufferUse::Read(read_slice)));\n+  branch_sequences[0].push_back(std::make_unique<BufferUseThunk>(\n+      BufferUse::Read(read_slice, read_slice_shape)));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto thunk, ConditionalThunk::Create({\"conditional\"}, branch_index_slice,\n                                            std::move(branch_sequences)));\n \n   EXPECT_EQ(thunk->buffer_uses().size(), 2);\n-  EXPECT_EQ(thunk->buffer_uses()[0], BufferUse::Read(branch_index_slice));\n-  EXPECT_EQ(thunk->buffer_uses()[1], BufferUse::Read(read_slice));\n+  EXPECT_EQ(thunk->buffer_uses()[0],\n+            BufferUse::Read(branch_index_slice, branch_index_slice_shape));\n+  EXPECT_EQ(thunk->buffer_uses()[1],\n+            BufferUse::Read(read_slice, read_slice_shape));\n }\n \n TEST(ConditionalThunkTest, ResourceUses) {"
        },
        {
            "sha": "281451c65e2823a17eb8161eda4ef5659e03ea39",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_dims.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_dims.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_dims.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_dims.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -33,9 +33,9 @@ namespace xla::cpu {\n \n absl::InlinedVector<BufferUse, 4> ConvolutionBufferUses(\n     const ConvolutionSlices& slices) {\n-  return {BufferUse::Read(slices.input_buffer),\n-          BufferUse::Read(slices.kernel_buffer),\n-          BufferUse::Write(slices.output_buffer)};\n+  return {BufferUse::Read(slices.input_buffer, slices.input_shape),\n+          BufferUse::Read(slices.kernel_buffer, slices.kernel_shape),\n+          BufferUse::Write(slices.output_buffer, slices.output_shape)};\n }\n \n ConvolutionCanonicalDims::Dims::Dims(absl::Span<const int64_t> dims)"
        },
        {
            "sha": "006a4c10eb5117645b9d0083fb90f9c9b071567b",
            "filename": "third_party/xla/xla/backends/cpu/runtime/copy_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -57,7 +57,12 @@ absl::StatusOr<std::unique_ptr<CopyThunk>> CopyThunk::Create(\n         \"Source shape %s must be compatible with destination shape %s\",\n         src_shape.ToString(true), dst_shape.ToString(true));\n   }\n-\n+  if (src_shape != dst_shape) {\n+    if (!ShapeUtil::ByteStrides(src_shape).has_value()) {\n+      return InvalidArgument(\"Source shape %s must have valid byte strides\",\n+                             src_shape.ToString(true));\n+    }\n+  }\n   return absl::WrapUnique(new CopyThunk(std::move(info), src_buffer, src_shape,\n                                         dst_buffer, dst_shape));\n }\n@@ -78,6 +83,7 @@ CopyThunk::CopyThunk(Info info, BufferAllocation::Slice src_buffer,\n     options.dims = src_shape_.dimensions();\n \n     auto byte_strides = ShapeUtil::ByteStrides(src_shape_);\n+    CHECK(byte_strides.has_value());\n     options.input_layout = TransposePlan::Striding{*byte_strides};\n \n     absl::InlinedVector<int64_t, 4> permutation(options.dims.size());"
        },
        {
            "sha": "ce9c274da7b2a1023c3a3b846b02d98016a86761",
            "filename": "third_party/xla/xla/backends/cpu/runtime/copy_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -46,7 +46,8 @@ class CopyThunk final : public Thunk {\n   tsl::AsyncValueRef<ExecuteEvent> Execute(const ExecuteParams& params) final;\n \n   BufferUses buffer_uses() const final {\n-    return {BufferUse::Read(src_buffer_), BufferUse::Write(dst_buffer_)};\n+    return {BufferUse::Read(src_buffer_, src_shape_),\n+            BufferUse::Write(dst_buffer_, dst_shape_)};\n   }\n \n   const Shape& src_shape() const { return src_shape_; }"
        },
        {
            "sha": "a1bd9e28b80d7c765c3d84eeef076558f95cd80c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcustom_call_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -396,11 +396,13 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> CustomCallThunk::CallUntypedAPI(\n \n CustomCallThunk::BufferUses CustomCallThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n-  for (const auto& argument : op_buffers_.arguments_buffers) {\n-    buffer_uses.emplace_back(BufferUse::Read(argument));\n+  for (int i = 0; i < op_buffers_.arguments_buffers.size(); i++) {\n+    buffer_uses.emplace_back(BufferUse::Read(op_buffers_.arguments_buffers[i],\n+                                             op_buffers_.arguments_shapes[i]));\n   }\n-  for (const auto& result : op_buffers_.results_buffers) {\n-    buffer_uses.emplace_back(BufferUse::Write(result));\n+  for (int i = 0; i < op_buffers_.results_buffers.size(); i++) {\n+    buffer_uses.emplace_back(BufferUse::Write(op_buffers_.results_buffers[i],\n+                                              op_buffers_.results_shapes[i]));\n   }\n   return buffer_uses;\n }"
        },
        {
            "sha": "418c3a718835a18b9e7b0b665a9ef4b0536e7e30",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_dims.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_dims.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_dims.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_dims.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -40,9 +40,9 @@ limitations under the License.\n namespace xla::cpu {\n \n absl::InlinedVector<BufferUse, 4> DotBufferUses(const DotSlices& slices) {\n-  return {BufferUse::Read(slices.lhs_buffer),\n-          BufferUse::Read(slices.rhs_buffer),\n-          BufferUse::Write(slices.out_buffer)};\n+  return {BufferUse::Read(slices.lhs_buffer, slices.lhs_shape),\n+          BufferUse::Read(slices.rhs_buffer, slices.rhs_shape),\n+          BufferUse::Write(slices.out_buffer, slices.out_shape)};\n }\n \n std::string MakeVectorString(absl::Span<const int64_t> values) {"
        },
        {
            "sha": "2f6ac18d0694726a7e707cc9f415abe259cc45d0",
            "filename": "third_party/xla/xla/backends/cpu/runtime/fft_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ffft_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ffft_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ffft_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -197,7 +197,8 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> FftThunk::Execute(\n }\n \n Thunk::BufferUses FftThunk::buffer_uses() const {\n-  return {BufferUse::Read(input_buffer_), BufferUse::Write(output_buffer_)};\n+  return {BufferUse::Read(input_buffer_, input_shape_),\n+          BufferUse::Write(output_buffer_, output_shape_)};\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "95af21510e81069f37703226a35024537c083130",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -142,11 +142,13 @@ OneDnnOpThunk::~OneDnnOpThunk() = default;\n \n OneDnnOpThunk::BufferUses OneDnnOpThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n-  for (const auto& argument : op_buffers_.arguments_buffers) {\n-    buffer_uses.emplace_back(BufferUse::Read(argument));\n+  for (int i = 0; i < op_buffers_.arguments_buffers.size(); i++) {\n+    buffer_uses.emplace_back(BufferUse::Read(op_buffers_.arguments_buffers[i],\n+                                             op_buffers_.arguments_shapes[i]));\n   }\n-  for (const auto& result : op_buffers_.results_buffers) {\n-    buffer_uses.emplace_back(BufferUse::Write(result));\n+  for (int i = 0; i < op_buffers_.results_buffers.size(); i++) {\n+    buffer_uses.emplace_back(BufferUse::Write(op_buffers_.results_buffers[i],\n+                                              op_buffers_.results_shapes[i]));\n   }\n   return buffer_uses;\n }"
        },
        {
            "sha": "d7308550cb1171b5774aa195829c6b7c2dd33c7b",
            "filename": "third_party/xla/xla/backends/cpu/runtime/outfeed_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -99,7 +99,8 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> OutfeedThunk::Execute(\n OutfeedThunk::BufferUses OutfeedThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const OutfeedBuffer& outfeed_buffer : outfeed_buffers_) {\n-    buffer_uses.emplace_back(BufferUse::Read(outfeed_buffer.slice));\n+    buffer_uses.emplace_back(\n+        BufferUse::Read(outfeed_buffer.slice, outfeed_buffer.shape));\n   }\n   return buffer_uses;\n }"
        },
        {
            "sha": "28b5f6a58ca33e87d0a1f6a3a081bd758c65df54",
            "filename": "third_party/xla/xla/backends/cpu/runtime/outfeed_thunk_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -22,21 +22,22 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/statusor.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace xla::cpu {\n namespace {\n \n TEST(OutfeedThunkTest, BufferAndResourceUses) {\n   BufferAllocation alloc(0, 1024, 0);\n   BufferAllocation::Slice outfeed_slice(&alloc, 10, 40);\n+  Shape outfeed_shape = ShapeUtil::MakeShape(F32, {10});\n \n   OutfeedThunk::OutfeedBuffer outfeed_buffer = {\n       outfeed_slice,\n-      ShapeUtil::MakeShape(F32, {10}),\n+      outfeed_shape,\n   };\n \n   auto consume_token = Resource::Create(Resource::kToken);\n@@ -47,7 +48,8 @@ TEST(OutfeedThunkTest, BufferAndResourceUses) {\n                                                {consume_token, produce_token}));\n \n   EXPECT_EQ(thunk->buffer_uses().size(), 1);\n-  EXPECT_EQ(thunk->buffer_uses()[0], BufferUse::Read(outfeed_slice));\n+  EXPECT_EQ(thunk->buffer_uses()[0],\n+            BufferUse::Read(outfeed_slice, outfeed_shape));\n \n   EXPECT_EQ(thunk->resource_uses().size(), 2);\n   EXPECT_EQ(thunk->resource_uses()[0], ResourceUse::Read(consume_token));"
        },
        {
            "sha": "df58a2a5371817ec33fbe8d2b6aafb78bbfe4e55",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_lib.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/attributes.h\"\n+#include \"absl/base/dynamic_annotations.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/types/span.h\"\n@@ -564,6 +565,12 @@ void SortInplace(const SortDims& sort_dims, absl::Span<std::byte* const> data,\n   // Iterate over all the 1-dimensional slices of the buffers and sort them.\n   int64_t num_iterations = sort_dims.outer_dim_size * sort_dims.inner_dim_size;\n \n+  // Annotate memory that might have been initialized by jit-compiled code.\n+  for (int64_t i = 0; i < data.size(); ++i) {\n+    ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(\n+        data[i], primitive_sizes[i] * sort_dims.sort_dim_size * num_iterations);\n+  }\n+\n   for (int64_t i = 0; i < num_iterations; ++i) {\n     int64_t inner_idx = i % sort_dims.inner_dim_size;\n     int64_t offset = inner_idx + (i - inner_idx) * sort_dims.sort_dim_size;\n@@ -672,6 +679,10 @@ void SortInplace(const SortDims& sort_dims, T* data, bool is_stable,\n   // Iterate over all the 1-dimensional slices of the buffers and sort them.\n   int64_t num_iterations = sort_dims.outer_dim_size * sort_dims.inner_dim_size;\n \n+  // Annotate memory that might have been initialized by jit-compiled code.\n+  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(\n+      data, sizeof(T) * sort_dims.sort_dim_size * num_iterations);\n+\n   for (int64_t i = 0; i < num_iterations; ++i) {\n     int64_t inner_idx = i % sort_dims.inner_dim_size;\n     int64_t offset = inner_idx + (i - inner_idx) * sort_dims.sort_dim_size;"
        },
        {
            "sha": "3f753a6010ea0045930e9bfb9909e9c081579362",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -223,10 +223,6 @@ tsl::AsyncValueRef<SortThunk::ExecuteEvent> SortThunk::Execute(\n         params.buffer_allocations->GetDeviceAddress(input.slice));\n     shapes.push_back(input.shape);\n \n-    // Annotate memory that might have been initialized by jit-compiled code.\n-    ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(data.back().opaque(),\n-                                        data.back().size());\n-\n     VLOG(3) << absl::StreamFormat(\"  sort input #%d: %s in slice %s (%p)\", idx,\n                                   input.shape.ToString(/*print_layout=*/true),\n                                   input.slice.ToString(), data.back().opaque());"
        },
        {
            "sha": "c2c147c694eb96137228a6fb297235a9442a98d3",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_executor_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -242,10 +242,14 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> AddI32Thunk::Execute(\n AddI32Thunk::BufferUses AddI32Thunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const auto& src : srcs_) {\n-    buffer_uses.push_back(BufferUse::Read(src));\n+    buffer_uses.push_back(BufferUse::Read(\n+        src, ShapeUtil::MakeShape(\n+                 S32, {src.size() / ShapeUtil::ByteSizeOfPrimitiveType(S32)})));\n   }\n   for (const auto& dst : dsts_) {\n-    buffer_uses.push_back(BufferUse::Write(dst));\n+    buffer_uses.push_back(BufferUse::Write(\n+        dst, ShapeUtil::MakeShape(\n+                 S32, {dst.size() / ShapeUtil::ByteSizeOfPrimitiveType(S32)})));\n   }\n   return buffer_uses;\n }"
        },
        {
            "sha": "a8e425116af0388df21751eb5f23d228ce07825b",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_sequence_serdes_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -233,6 +233,13 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n \n     return absl::OkStatus();\n   }\n+  absl::Status AddPredBufferAllocation() {\n+    literals_.push_back(LiteralUtil::CreateFull<bool>({1}, false));\n+    TF_RETURN_IF_ERROR(buffer_allocations_.push_back(\n+        CreateBufferAllocation(buffer_allocations_.size(), literals_.back())));\n+\n+    return absl::OkStatus();\n+  }\n \n   // Thunk creation helper functions.\n   absl::StatusOr<std::unique_ptr<Thunk>> CreateAllGatherThunk(\n@@ -424,7 +431,7 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n       branch_sequences.push_back(std::move(called_sequence));\n     }\n \n-    TF_RETURN_IF_ERROR(AddBufferAllocations(1));\n+    TF_RETURN_IF_ERROR(AddPredBufferAllocation());\n \n     return ConditionalThunk::Create(\n         Thunk::Info(),\n@@ -435,7 +442,8 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n   }\n \n   absl::StatusOr<std::unique_ptr<Thunk>> CreateCustomCallThunk() {\n-    TF_RETURN_IF_ERROR(AddBufferAllocations(2));\n+    TF_RETURN_IF_ERROR(AddPredBufferAllocation());\n+    TF_RETURN_IF_ERROR(AddBufferAllocations(1));\n \n     return CustomCallThunk::Create(\n         Thunk::Info(), \"no_op\",\n@@ -575,7 +583,6 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n         /*batch_size=*/1,\n         /*input_size=*/1,\n         /*k=*/2\n-\n     );\n   }\n \n@@ -588,7 +595,7 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n     TF_ASSIGN_OR_RETURN(body_sequence.emplace_back(), CreateAllReduceThunk());\n     TF_ASSIGN_OR_RETURN(body_sequence.emplace_back(), CreateAllToAllThunk());\n \n-    TF_RETURN_IF_ERROR(AddBufferAllocations(1));\n+    TF_RETURN_IF_ERROR(AddPredBufferAllocation());\n     return WhileThunk::Create(\n         Thunk::Info(),\n         /*cond_buffer=*/"
        },
        {
            "sha": "f26043de1b5e87501c2254c98469418b43de4979",
            "filename": "third_party/xla/xla/backends/cpu/runtime/topk_lib.h",
            "status": "renamed",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_lib.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1,4 +1,4 @@\n-/* Copyright 2020 The OpenXLA Authors.\n+/* Copyright 2025 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n@@ -13,27 +13,29 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/service/cpu/runtime_topk.h\"\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_TOPK_LIB_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_TOPK_LIB_H_\n \n #include <algorithm>\n+#include <cstddef>\n #include <cstdint>\n-#include <cstring>\n #include <limits>\n #include <numeric>\n #include <vector>\n \n-#include \"absl/base/attributes.h\"\n #include \"absl/base/casts.h\"\n #include \"absl/base/dynamic_annotations.h\"\n \n+namespace xla::cpu::internal {\n+\n template <typename T>\n-static void TopK(int64_t batch_size, int64_t input_size, int64_t k,\n-                 const T* values, T* out_values, int32_t* out_indices) {\n-  // 'values' is managed by the JIT code, so msan can't tell they are\n-  // initialized.\n+void TopK(int64_t batch_size, int64_t input_size, int64_t k, const T* values,\n+          T* out_values, int32_t* out_indices) {\n+  // values is managed by the JIT code, so msan can't tell they are initialized.\n   ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(values,\n                                       input_size * batch_size * sizeof(T));\n-  static constexpr auto convert_to_int = [](T value) {\n+\n+  auto convert_to_int = [](T value) {\n     uint32_t x = absl::bit_cast<uint32_t>(value);\n     return static_cast<int32_t>(x) < 0 ? std::numeric_limits<int32_t>::max() - x\n                                        : x;\n@@ -47,7 +49,7 @@ static void TopK(int64_t batch_size, int64_t input_size, int64_t k,\n \n     auto kth_element = temp_indices.begin() + k;\n     std::partial_sort(temp_indices.begin(), kth_element, temp_indices.end(),\n-                      [values_batch](size_t i1, size_t i2) {\n+                      [&](size_t i1, size_t i2) {\n                         // Do the comparison in integers to enforce a total\n                         // order of -NaN < -Inf < -0 < +0 < +Inf < +NaN.\n                         int32_t v1 = convert_to_int(values_batch[i1]);\n@@ -67,8 +69,6 @@ static void TopK(int64_t batch_size, int64_t input_size, int64_t k,\n   }\n }\n \n-ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_TopKF32(\n-    int64_t batch_size, int64_t input_size, int64_t k, const float* values,\n-    float* out_values, int32_t* out_indices) {\n-  TopK(batch_size, input_size, k, values, out_values, out_indices);\n-}\n+}  // namespace xla::cpu::internal\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_TOPK_LIB_H_",
            "previous_filename": "third_party/xla/xla/service/cpu/runtime_topk.cc"
        },
        {
            "sha": "cc473b82459879e02fb92923193bb970836ddae4",
            "filename": "third_party/xla/xla/backends/cpu/runtime/topk_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/memory/memory.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/backends/cpu/runtime/topk_lib.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/cpu/runtime_topk.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -62,10 +62,11 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> TopKThunk::Execute(\n       se::DeviceMemoryBase indices,\n       params.buffer_allocations->GetDeviceAddress(indices_buffer_));\n \n-  __xla_cpu_runtime_TopKF32(batch_size_, input_size_, k_,\n-                            reinterpret_cast<const float*>(values.opaque()),\n-                            reinterpret_cast<float*>(output.opaque()),\n-                            reinterpret_cast<int32_t*>(indices.opaque()));\n+  internal::TopK<float>(batch_size_, input_size_, k_,\n+                        static_cast<const float*>(values.opaque()),\n+                        static_cast<float*>(output.opaque()),\n+                        static_cast<int32_t*>(indices.opaque()));\n+\n   return OkExecuteEvent();\n }\n "
        },
        {
            "sha": "2b1dc2f26039178516bd39f4952586d8a67c419a",
            "filename": "third_party/xla/xla/backends/cpu/runtime/while_thunk.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -36,9 +36,12 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk_executor.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n \n namespace xla::cpu {\n \n@@ -49,16 +52,23 @@ absl::StatusOr<std::unique_ptr<WhileThunk>> WhileThunk::Create(\n                       ThunkExecutor::Create(std::move(cond_sequence)));\n   TF_ASSIGN_OR_RETURN(ThunkExecutor body_executor,\n                       ThunkExecutor::Create(std::move(body_sequence)));\n-  return absl::WrapUnique(new WhileThunk(std::move(info), cond_buffer,\n-                                         std::move(cond_executor),\n-                                         std::move(body_executor), trip_count));\n+\n+  if (cond_buffer.size() != sizeof(bool)) {\n+    return Internal(\"Unsupported cond buffer size %d\", cond_buffer.size());\n+  }\n+\n+  return absl::WrapUnique(new WhileThunk(\n+      std::move(info), cond_buffer, ShapeUtil::MakeShape(PRED, {1}),\n+      std::move(cond_executor), std::move(body_executor), trip_count));\n }\n \n WhileThunk::WhileThunk(Info info, BufferAllocation::Slice cond_buffer,\n-                       ThunkExecutor cond_executor, ThunkExecutor body_executor,\n+                       Shape cond_buffer_shape, ThunkExecutor cond_executor,\n+                       ThunkExecutor body_executor,\n                        std::optional<int64_t> trip_count)\n     : Thunk(Kind::kWhile, std::move(info)),\n       cond_buffer_(cond_buffer),\n+      cond_buffer_shape_(cond_buffer_shape),\n       cond_executor_(std::move(cond_executor)),\n       body_executor_(std::move(body_executor)),\n       trip_count_(trip_count) {}\n@@ -273,7 +283,7 @@ tsl::AsyncValueRef<WhileThunk::ExecuteEvent> WhileThunk::ExecuteAsyncWhileLoop(\n }\n \n WhileThunk::BufferUses WhileThunk::buffer_uses() const {\n-  BufferUses buffer_uses = {BufferUse::Write(cond_buffer_)};\n+  BufferUses buffer_uses = {BufferUse::Write(cond_buffer_, cond_buffer_shape_)};\n \n   BufferUses cond_uses = cond_executor_.buffer_uses();\n   buffer_uses.insert(buffer_uses.end(), cond_uses.begin(), cond_uses.end());"
        },
        {
            "sha": "aff947cac7a3ea127a71afd87605ff1051e20821",
            "filename": "third_party/xla/xla/backends/cpu/runtime/while_thunk.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk_executor.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n \n namespace xla::cpu {\n@@ -61,8 +62,8 @@ class WhileThunk final : public Thunk {\n \n  private:\n   WhileThunk(Info info, BufferAllocation::Slice cond_buffer,\n-             ThunkExecutor cond_executor, ThunkExecutor body_executor,\n-             std::optional<int64_t> trip_count);\n+             Shape cond_buffer_shape, ThunkExecutor cond_executor,\n+             ThunkExecutor body_executor, std::optional<int64_t> trip_count);\n \n   tsl::AsyncValueRef<ExecuteEvent> ExecuteForLoop(const ExecuteParams& params,\n                                                   int64_t trip_count);\n@@ -84,6 +85,7 @@ class WhileThunk final : public Thunk {\n       bool* condition);\n \n   BufferAllocation::Slice cond_buffer_;\n+  Shape cond_buffer_shape_;\n   ThunkExecutor cond_executor_;\n   ThunkExecutor body_executor_;\n "
        },
        {
            "sha": "f8ae4f1ff8c5a05c83e4a8c2bed6f8cbe1cf1d86",
            "filename": "third_party/xla/xla/backends/cpu/runtime/while_thunk_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -29,6 +29,8 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/env.h\"\n@@ -46,27 +48,31 @@ namespace {\n \n TEST(WhileThunkTest, BufferUses) {\n   BufferAllocation alloc(0, 1024, 0);\n+  Shape pred_shape = ShapeUtil::MakeShape(PRED, {1});\n   BufferAllocation::Slice pred_slice(&alloc, 0, sizeof(char));\n-  BufferAllocation::Slice cond_read_slice(&alloc, 10, 10);\n-  BufferAllocation::Slice body_read_slice(&alloc, 20, 10);\n+  Shape read_slice_shape = ShapeUtil::MakeShape(F32, {4});\n+  BufferAllocation::Slice cond_read_slice(&alloc, 10, 12);\n+  BufferAllocation::Slice body_read_slice(&alloc, 22, 12);\n \n   ThunkSequence cond_sequence;\n-  cond_sequence.push_back(\n-      std::make_unique<BufferUseThunk>(BufferUse::Read(cond_read_slice)));\n+  cond_sequence.push_back(std::make_unique<BufferUseThunk>(\n+      BufferUse::Read(cond_read_slice, read_slice_shape)));\n \n   ThunkSequence body_sequence;\n-  body_sequence.push_back(\n-      std::make_unique<BufferUseThunk>(BufferUse::Read(body_read_slice)));\n+  body_sequence.push_back(std::make_unique<BufferUseThunk>(\n+      BufferUse::Read(body_read_slice, read_slice_shape)));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto thunk,\n       WhileThunk::Create({\"while\"}, pred_slice, std::move(cond_sequence),\n                          std::move(body_sequence)));\n \n   EXPECT_EQ(thunk->buffer_uses().size(), 3);\n-  EXPECT_EQ(thunk->buffer_uses()[0], BufferUse::Write(pred_slice));\n-  EXPECT_EQ(thunk->buffer_uses()[1], BufferUse::Read(cond_read_slice));\n-  EXPECT_EQ(thunk->buffer_uses()[2], BufferUse::Read(body_read_slice));\n+  EXPECT_EQ(thunk->buffer_uses()[0], BufferUse::Write(pred_slice, pred_shape));\n+  EXPECT_EQ(thunk->buffer_uses()[1],\n+            BufferUse::Read(cond_read_slice, read_slice_shape));\n+  EXPECT_EQ(thunk->buffer_uses()[2],\n+            BufferUse::Read(body_read_slice, read_slice_shape));\n }\n \n TEST(WhileThunkTest, ResourceUses) {\n@@ -123,7 +129,7 @@ class CondThunk : public Thunk {\n   }\n \n   BufferUses buffer_uses() const final {\n-    return {BufferUse::Write(pred_slice_)};\n+    return {BufferUse::Write(pred_slice_, ShapeUtil::MakeShape(PRED, {1}))};\n   }\n \n  private:"
        },
        {
            "sha": "3071e055abb3d1b5416e0214b02d6e0cec6895cc",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -272,10 +272,10 @@ XnnFusionThunk::~XnnFusionThunk() = default;\n XnnFusionThunk::BufferUses XnnFusionThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const Argument& argument : arguments_) {\n-    buffer_uses.push_back(BufferUse::Read(argument.slice));\n+    buffer_uses.push_back(BufferUse::Read(argument.slice, argument.shape));\n   }\n   for (const Result& result : results_) {\n-    buffer_uses.push_back(BufferUse::Write(result.slice));\n+    buffer_uses.push_back(BufferUse::Write(result.slice, result.shape));\n   }\n \n   return buffer_uses;"
        },
        {
            "sha": "a37949d4fe0239b4ced3c242871d71ba58c5450c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -54,6 +54,7 @@ cc_library(\n     deps = [\n         \"//xla:shape_util\",\n         \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n         \"//xla/tsl/platform:logging\",\n         \"@XNNPACK//ynnpack\",\n         \"@com_google_absl//absl/base:core_headers\","
        },
        {
            "sha": "d87974ed7053912b03a0608fd86db6eecffc60ed",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -287,10 +287,10 @@ YnnFusionThunk::~YnnFusionThunk() = default;\n YnnFusionThunk::BufferUses YnnFusionThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const Argument& argument : arguments_) {\n-    buffer_uses.push_back(BufferUse::Read(argument.slice));\n+    buffer_uses.push_back(BufferUse::Read(argument.slice, argument.shape));\n   }\n   for (const Result& result : results_) {\n-    buffer_uses.push_back(BufferUse::Write(result.slice));\n+    buffer_uses.push_back(BufferUse::Write(result.slice, result.shape));\n   }\n \n   return buffer_uses;"
        },
        {
            "sha": "67cc6fce39784592a3d410dd1f6ab360bc76d586",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_interop.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n "
        },
        {
            "sha": "372b833e054ac4d403e0d28cb645fa09a320c74a",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_interop.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n "
        },
        {
            "sha": "d52eb5ad92287da5f130deb23749548dbfb0a3a1",
            "filename": "third_party/xla/xla/backends/cpu/target_machine_options.cc",
            "status": "added",
            "additions": 180,
            "deletions": 0,
            "changes": 180,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,180 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/target_machine_options.h\"\n+\n+#include <algorithm>\n+#include <string>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/match.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/strings/str_split.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"llvm/ADT/StringRef.h\"  // IWYU pragma: keep\n+#include \"llvm/TargetParser/Host.h\"\n+#include \"xla/backends/cpu/codegen/cpu_features.h\"\n+#include \"xla/service/cpu/executable.pb.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace cpu {\n+\n+namespace {\n+\n+bool ValidateTargetMachineFeaturesString(absl::string_view features) {\n+  if (features.empty()) {\n+    return true;\n+  }\n+  for (const auto& feature : absl::StrSplit(features, ',')) {\n+    if ((!absl::StartsWith(feature, \"+\") && !absl::StartsWith(feature, \"-\")) ||\n+        feature.size() <= 1) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+void EnableFeaturesIfAVX512(std::vector<std::string>& features) {\n+  auto avx512_it = std::find_if(features.begin(), features.end(),\n+                                [](const std::string& feature) {\n+                                  return absl::StrContains(feature, \"avx512\");\n+                                });\n+  bool has_avx512 = avx512_it != features.end();\n+  if (!has_avx512) {\n+    return;\n+  }\n+\n+  auto prefer_no_scatter_it = std::find_if(\n+      features.begin(), features.end(), [](const std::string& feature) {\n+        return absl::StrContains(feature, \"prefer-no-scatter\");\n+      });\n+\n+  if (prefer_no_scatter_it == features.end()) {\n+    features.push_back(\"prefer-no-scatter\");\n+  }\n+\n+  auto prefer_no_gather_it = std::find_if(\n+      features.begin(), features.end(), [](const std::string& feature) {\n+        return absl::StrContains(feature, \"prefer-no-gather\");\n+      });\n+\n+  if (prefer_no_gather_it == features.end()) {\n+    features.push_back(\"prefer-no-gather\");\n+  }\n+}\n+\n+std::pair<std::vector<std::string>, std::vector<std::string>>\n+GetEnabledAndDisabledFeatures(const std::vector<std::string>& features) {\n+  std::vector<std::string> enabled_features;\n+  std::vector<std::string> disabled_features;\n+  for (const auto& feature : features) {\n+    if (absl::StartsWith(feature, \"+\")) {\n+      enabled_features.push_back(feature.substr(1));\n+    } else if (absl::StartsWith(feature, \"-\")) {\n+      disabled_features.push_back(feature.substr(1));\n+    }\n+  }\n+  return std::make_pair(enabled_features, disabled_features);\n+}\n+\n+}  // namespace\n+\n+TargetMachineOptions::TargetMachineOptions(const DebugOptions& debug_options) {\n+  triple_ = llvm::sys::getDefaultTargetTriple();\n+  auto xla_cpu_max_isa = CpuFeatureFromString(debug_options.xla_cpu_max_isa());\n+  auto detected_machine_attributes = DetectMachineAttributes(xla_cpu_max_isa);\n+\n+  std::tie(enabled_features_, disabled_features_) =\n+      GetEnabledAndDisabledFeatures(detected_machine_attributes.features);\n+\n+  // If `max_cpu_feature` is newer than the host CPU, we should keep the host\n+  // CPU name, e.g., we don't want to set the target CPU to Skylake when we\n+  // are on a Broadwell host.\n+  cpu_ = detected_machine_attributes.num_filtered_features\n+             ? CpuTargetFromMaxFeature(*xla_cpu_max_isa)\n+             : absl::string_view(llvm::sys::getHostCPUName());\n+\n+  EnableFeaturesIfAVX512(enabled_features_);\n+}\n+\n+TargetMachineOptions::TargetMachineOptions(absl::string_view triple,\n+                                           absl::string_view cpu,\n+                                           absl::string_view features)\n+    : triple_(triple), cpu_(cpu) {\n+  std::vector<std::string> features_vec = absl::StrSplit(features, ',');\n+  std::tie(enabled_features_, disabled_features_) =\n+      GetEnabledAndDisabledFeatures(features_vec);\n+  EnableFeaturesIfAVX512(enabled_features_);\n+}\n+\n+std::vector<std::string> TargetMachineOptions::GetTargetMachineFeaturesVector()\n+    const {\n+  std::vector<std::string> all_features;\n+  all_features.reserve(enabled_features_.size() + disabled_features_.size());\n+  for (const auto& feature : enabled_features_) {\n+    all_features.push_back(absl::StrCat(\"+\", feature));\n+  }\n+  for (const auto& feature : disabled_features_) {\n+    all_features.push_back(absl::StrCat(\"-\", feature));\n+  }\n+\n+  return all_features;\n+}\n+\n+std::string TargetMachineOptions::GetTargetMachineFeatures() const {\n+  return absl::StrJoin(GetTargetMachineFeaturesVector(), \",\");\n+}\n+\n+TargetMachineOptionsProto TargetMachineOptions::ToProto() const {\n+  TargetMachineOptionsProto proto;\n+  proto.set_triple(triple_);\n+  proto.set_cpu(cpu_);\n+  proto.set_features(GetTargetMachineFeatures());\n+  return proto;\n+}\n+\n+/*static*/\n+absl::StatusOr<TargetMachineOptions> TargetMachineOptions::FromProto(\n+    const TargetMachineOptionsProto& proto) {\n+  if (!ValidateTargetMachineFeaturesString(proto.features())) {\n+    return Internal(\"Invalid target machine features: %s\",\n+                    std::string(proto.features()));\n+  }\n+  return TargetMachineOptions(proto.triple(), proto.cpu(), proto.features());\n+}\n+\n+absl::Status TargetMachineOptions::SetFeatures(absl::string_view features) {\n+  if (!ValidateTargetMachineFeaturesString(features)) {\n+    return Internal(\"Trying to set invalid target machine features: %s\",\n+                    std::string(features));\n+  }\n+\n+  std::vector<std::string> features_vec = absl::StrSplit(features, ',');\n+  std::tie(enabled_features_, disabled_features_) =\n+      GetEnabledAndDisabledFeatures(features_vec);\n+  EnableFeaturesIfAVX512(enabled_features_);\n+\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace cpu\n+}  // namespace xla"
        },
        {
            "sha": "e503039bb2e90df9ceef1fbb553a09f1096685cf",
            "filename": "third_party/xla/xla/backends/cpu/target_machine_options.h",
            "status": "added",
            "additions": 78,
            "deletions": 0,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,78 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_TARGET_MACHINE_OPTIONS_H_\n+#define XLA_BACKENDS_CPU_TARGET_MACHINE_OPTIONS_H_\n+\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/service/cpu/executable.pb.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace cpu {\n+\n+// Helper class to manage the target machine options for CPU compilation.\n+class TargetMachineOptions {\n+ public:\n+  // Creates a TargetMachineOptions object from the given DebugOptions. This\n+  // will create a TargetMachineOptions object for the host machine.\n+  explicit TargetMachineOptions(const DebugOptions& debug_options);\n+\n+  // Creates a TargetMachineOptions object from the given triple, cpu, and\n+  // features.\n+  TargetMachineOptions(absl::string_view triple, absl::string_view cpu,\n+                       absl::string_view features);\n+\n+  TargetMachineOptionsProto ToProto() const;\n+  static absl::StatusOr<TargetMachineOptions> FromProto(\n+      const TargetMachineOptionsProto& proto);\n+\n+  const std::string& triple() const { return triple_; }\n+  const std::string& cpu() const { return cpu_; }\n+  const std::vector<std::string>& enabled_features() const {\n+    return enabled_features_;\n+  }\n+  const std::vector<std::string>& disabled_features() const {\n+    return disabled_features_;\n+  }\n+\n+  absl::Status SetFeatures(absl::string_view features);\n+\n+  // Returns the target machine features in the format that LLVM understands\n+  // (e.x. \"+avx2,-avx512\")).\n+  std::string GetTargetMachineFeatures() const;\n+\n+  // Returns the target machine features in the format that LLVM understands -\n+  // features prefixed with \"+\" or \"-\". E.x. {\"+avx2\", \"-avx512\"}.\n+  std::vector<std::string> GetTargetMachineFeaturesVector() const;\n+\n+ private:\n+  TargetMachineOptions() = default;\n+\n+  std::string triple_;\n+  std::string cpu_;\n+  std::vector<std::string> enabled_features_;\n+  std::vector<std::string> disabled_features_;\n+};\n+\n+}  // namespace cpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_CPU_TARGET_MACHINE_OPTIONS_H_"
        },
        {
            "sha": "fec5439e0bb31ded97fdc6422389a61d150ffbee",
            "filename": "third_party/xla/xla/backends/cpu/target_machine_options_test.cc",
            "status": "added",
            "additions": 146,
            "deletions": 0,
            "changes": 146,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftarget_machine_options_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,146 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/target_machine_options.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"xla/service/cpu/executable.pb.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace cpu {\n+namespace {\n+\n+TEST(TargetMachineOptionsTest, ToProto) {\n+  DebugOptions debug_options;\n+  TargetMachineOptions options(debug_options);\n+  TargetMachineOptionsProto proto = options.ToProto();\n+\n+  EXPECT_EQ(proto.triple(), options.triple());\n+  EXPECT_EQ(proto.cpu(), options.cpu());\n+  EXPECT_EQ(proto.features(), options.GetTargetMachineFeatures());\n+}\n+\n+TEST(TargetMachineOptionsTest, FromProto) {\n+  TargetMachineOptionsProto proto;\n+  proto.set_triple(\"test_triple\");\n+  proto.set_cpu(\"test_cpu\");\n+  proto.set_features(\"+enabled_feature,-disabled_feature\");\n+\n+  TF_ASSERT_OK_AND_ASSIGN(TargetMachineOptions options,\n+                          TargetMachineOptions::FromProto(proto));\n+\n+  EXPECT_EQ(options.triple(), \"test_triple\");\n+  EXPECT_EQ(options.cpu(), \"test_cpu\");\n+  EXPECT_THAT(options.enabled_features(),\n+              testing::ElementsAre(\"enabled_feature\"));\n+  EXPECT_THAT(options.disabled_features(),\n+              testing::ElementsAre(\"disabled_feature\"));\n+  EXPECT_EQ(options.GetTargetMachineFeatures(),\n+            \"+enabled_feature,-disabled_feature\");\n+}\n+\n+TEST(TargetMachineOptionsTest, ProtoRoundTrip) {\n+  DebugOptions debug_options;\n+  TargetMachineOptions options(debug_options);\n+  TargetMachineOptionsProto proto = options.ToProto();\n+  TF_ASSERT_OK_AND_ASSIGN(TargetMachineOptions new_options,\n+                          TargetMachineOptions::FromProto(proto));\n+\n+  EXPECT_EQ(new_options.triple(), options.triple());\n+  EXPECT_EQ(new_options.cpu(), options.cpu());\n+  EXPECT_EQ(new_options.GetTargetMachineFeatures(),\n+\n+            options.GetTargetMachineFeatures());\n+}\n+\n+TEST(TargetMachineOptionsTest, ConstructorWithFeatures) {\n+  TargetMachineOptions options(\"test_triple\", \"test_cpu\", \"+avx2,-avx512\");\n+\n+  EXPECT_EQ(options.triple(), \"test_triple\");\n+  EXPECT_EQ(options.cpu(), \"test_cpu\");\n+  EXPECT_THAT(options.enabled_features(), testing::ElementsAre(\"avx2\"));\n+  EXPECT_THAT(options.disabled_features(), testing::ElementsAre(\"avx512\"));\n+  EXPECT_EQ(options.GetTargetMachineFeatures(), \"+avx2,-avx512\");\n+}\n+\n+TEST(TargetMachineOptionsTest, GetTargetMachineFeaturesFormat) {\n+  TargetMachineOptions options1(\"t\", \"c\", \"+f1,-f2\");\n+  EXPECT_EQ(options1.GetTargetMachineFeatures(), \"+f1,-f2\");\n+\n+  TargetMachineOptions options2(\"t\", \"c\", \"-f2,+f1\");\n+  EXPECT_EQ(options2.GetTargetMachineFeatures(), \"+f1,-f2\");\n+\n+  TargetMachineOptions options3(\"t\", \"c\", \"+f1\");\n+  EXPECT_EQ(options3.GetTargetMachineFeatures(), \"+f1\");\n+\n+  TargetMachineOptions options4(\"t\", \"c\", \"-f2\");\n+  EXPECT_EQ(options4.GetTargetMachineFeatures(), \"-f2\");\n+\n+  TargetMachineOptions options5(\"t\", \"c\", \"\");\n+  EXPECT_EQ(options5.GetTargetMachineFeatures(), \"\");\n+\n+  TargetMachineOptions options6(\"t\", \"c\", \"+f1,-f2,+f3,-f4\");\n+  EXPECT_EQ(options6.GetTargetMachineFeatures(), \"+f1,+f3,-f2,-f4\");\n+}\n+\n+TEST(TargetMachineOptionsTest, FromProtoWithMalformedFeatures) {\n+  TargetMachineOptionsProto proto;\n+  proto.set_triple(\"test_triple\");\n+  proto.set_cpu(\"test_cpu\");\n+  proto.set_features(\"malformed\");\n+\n+  auto options = TargetMachineOptions::FromProto(proto);\n+\n+  EXPECT_EQ(options.status().code(), absl::StatusCode::kInternal);\n+}\n+\n+TEST(TargetMachineOptionsTest, FromProtoWithEmptyFeatureAfterPlus) {\n+  TargetMachineOptionsProto proto;\n+  proto.set_triple(\"test_triple\");\n+  proto.set_cpu(\"test_cpu\");\n+  proto.set_features(\"+\");\n+\n+  auto options = TargetMachineOptions::FromProto(proto);\n+\n+  EXPECT_EQ(options.status().code(), absl::StatusCode::kInternal);\n+}\n+\n+TEST(TargetMachineOptionsTest, SetFeatures) {\n+  TargetMachineOptions options(\"test_triple\", \"test_cpu\", \"\");\n+  TF_ASSERT_OK(options.SetFeatures(\"+avx2,-avx512\"));\n+\n+  EXPECT_EQ(options.GetTargetMachineFeatures(), \"+avx2,-avx512\");\n+}\n+\n+TEST(TargetMachineOptionsTest, AVX512ImpliesNoScatterAndNoGather) {\n+  TargetMachineOptions options(\"test_triple\", \"test_cpu\", \"+avx512\");\n+  EXPECT_EQ(options.GetTargetMachineFeatures(),\n+            \"+avx512,+prefer-no-scatter,+prefer-no-gather\");\n+}\n+\n+TEST(TargetMachineOptionsTest, GetTargetMachineFeaturesVector) {\n+  TargetMachineOptions options(\"test_triple\", \"test_cpu\", \"+avx2,-avx512\");\n+  EXPECT_THAT(options.GetTargetMachineFeaturesVector(),\n+              testing::ElementsAre(\"+avx2\", \"-avx512\"));\n+}\n+\n+}  // namespace\n+}  // namespace cpu\n+}  // namespace xla"
        },
        {
            "sha": "e4d57e372c0ef0ee63257c79f2f2c060a7caf9ac",
            "filename": "third_party/xla/xla/backends/cpu/testlib/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -24,6 +24,7 @@ cc_library(\n     hdrs = [\"kernel_runner.h\"],\n     deps = [\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/cpu:target_machine_options\",\n         \"//xla/backends/cpu/codegen:builtin_definition_generator\",\n         \"//xla/backends/cpu/codegen:cpu_features\",\n         \"//xla/backends/cpu/codegen:execution_engine\",\n@@ -41,6 +42,7 @@ cc_library(\n         \"//xla/codegen/testlib:kernel_runner\",\n         \"//xla/runtime:work_group\",\n         \"//xla/service:hlo_module_config\",\n+        \"//xla/service/cpu:cpu_compiler\",\n         \"//xla/service/cpu:cpu_options\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "e1597dc72dcde019de3d6f3f4b76d0d0847e9e58",
            "filename": "third_party/xla/xla/backends/cpu/testlib/kernel_runner.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -36,11 +36,13 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/function_library.h\"\n #include \"xla/backends/cpu/runtime/kernel.h\"\n #include \"xla/backends/cpu/runtime/kernel_c_api.h\"\n+#include \"xla/backends/cpu/target_machine_options.h\"\n #include \"xla/codegen/kernel_definition.h\"\n #include \"xla/codegen/kernel_spec.h\"\n #include \"xla/codegen/llvm_kernel_source.h\"\n #include \"xla/codegen/mlir_kernel_source.h\"\n #include \"xla/runtime/work_group.h\"\n+#include \"xla/service/cpu/cpu_compiler.h\"\n #include \"xla/service/cpu/cpu_options.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n@@ -103,7 +105,8 @@ absl::StatusOr<JitCompiler> KernelRunner::CreateJitCompiler(\n   IrCompiler::Options ir_compiler_options{\n       /*optimization_level=*/IrCompiler::GetCodeGenOptLevel(config),\n       /*optimize_for_size=*/options::OptimizeForSizeRequested(config),\n-      /*max_cpu_isa=*/CpuFeatureFromString(debug_options.xla_cpu_max_isa()),\n+      /*target_machine_options=*/\n+      TargetMachineOptions(debug_options),\n       /*fast_math_flags=*/llvm_ir::GetCpuFastMathFlags(config),\n       /*disable_expensive_passes=*/\n       debug_options.xla_llvm_disable_expensive_passes(),"
        },
        {
            "sha": "2fe19fec110a5ec99d66750b0b4b3eff865c2c7f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -113,7 +113,6 @@ xla_test(\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status:status_matchers\",\n@@ -197,9 +196,9 @@ xla_test(\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n@@ -263,9 +262,9 @@ xla_test(\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n@@ -342,11 +341,11 @@ xla_test(\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n@@ -405,10 +404,10 @@ xla_test(\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n@@ -514,6 +513,7 @@ cc_library(\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_float_support\",\n+        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:split_k_gemm_rewriter\",\n@@ -528,10 +528,12 @@ cc_library(\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n \n@@ -824,6 +826,7 @@ xla_test(\n     tags = [\"cuda-only\"],\n     deps = [\n         \":cublas\",\n+        \":custom_kernel\",\n         \":fission_backend\",\n         \":gpu_codegen_backend\",\n         \"//xla/backends/autotuner:codegen_backend\",\n@@ -835,6 +838,7 @@ xla_test(\n         \"//xla/service:executable\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n+        \"//xla/service/gpu/transforms:custom_kernel_fusion_rewriter\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n         \"//xla/stream_executor:device_description\",\n@@ -880,14 +884,14 @@ xla_cc_test(\n         \"//xla/backends/autotuner:autotuner_cache_interface\",\n         \"//xla/backends/autotuner:autotuner_cache_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@com_google_protobuf//:any_cc_proto\",\n         \"@local_tsl//tsl/platform:path\",\n@@ -969,10 +973,10 @@ xla_test(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/rocm:rocm_platform_id\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],"
        },
        {
            "sha": "b0d39affd588b8c1d8b00c2bd46ffa31ad9e90a6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -100,7 +100,7 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       xla::Compiler::GetForPlatform(platform));\n   se::StreamExecutor* stream_executor = platform->ExecutorForDevice(0).value();\n   DebugOptions debug_options = GetDebugOptionsFromFlags();\n-  Compiler::TargetConfig target_config(stream_executor);\n+  Compiler::GpuTargetConfig target_config(stream_executor);\n \n   auto& registry = stream_executor::PlatformObjectRegistry::GetGlobalRegistry();\n   TF_ASSIGN_OR_RETURN(const GetCodegenBackends::Type& get_codegen_backends,"
        },
        {
            "sha": "d3a3845de79efd15befc18f58e891e107d2cc965",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -46,7 +46,7 @@ class BlockLevelEmitterBackend : public GpuCodegenBackend {\n       const DebugOptions* absl_nonnull debug_options,\n       Compiler* absl_nonnull compiler,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-      const Compiler::TargetConfig* target_config,\n+      const Compiler::GpuTargetConfig* target_config,\n       bool use_default_config = false)\n       : GpuCodegenBackend(\"BlockLevelEmitter\", debug_options, compiler,\n                           target_config),"
        },
        {
            "sha": "5d4875ba19e360046584c77cbbe07c9ce6f1d73d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -36,7 +36,6 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"xla/xla.pb.h\"\n@@ -82,7 +81,7 @@ class TritonBlockLevelFusionEmitterBackendTest\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   BlockLevelEmitterBackend backend_;\n };\n "
        },
        {
            "sha": "a57eb16dedb22f6e9925eaa0a962764372ebc60d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -48,7 +48,7 @@ class CublasBackend : public GpuCodegenBackend {\n  public:\n   explicit CublasBackend(stream_executor::StreamExecutor* stream_executor,\n                          const DebugOptions* debug_options, Compiler* compiler,\n-                         const Compiler::TargetConfig* target_config)\n+                         const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"Cublas\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "93b25b8e08530c33fbab44403b8318b019ce723b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n@@ -34,7 +35,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"xla/xla.pb.h\"\n@@ -45,8 +45,6 @@ namespace gpu {\n using CublasBackendConfig = AutotuneResult::GemmKey;\n \n using ::tsl::proto_testing::EqualsProto;\n-using ::tsl::testing::IsOk;\n-using ::tsl::testing::IsOkAndHolds;\n \n const char kCublasCustomCallHlo[] = R\"(\n   HloModule module, entry_computation_layout={(f32[100,100]{1,0}, f32[100,100]{1,0})->f32[100,100]{1,0}}\n@@ -95,7 +93,7 @@ class CublasBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CublasBackend backend_;\n \n   CublasBackendTest()"
        },
        {
            "sha": "009b4fbf73e04029b8343b8841887a4764e85997",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -47,7 +47,7 @@ class CublasLtBackend : public GpuCodegenBackend {\n   explicit CublasLtBackend(stream_executor::StreamExecutor* stream_executor,\n                            const DebugOptions* debug_options,\n                            Compiler* compiler,\n-                           const Compiler::TargetConfig* target_config)\n+                           const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"CublasLt\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "9077a36b651a384a8856ceb36d2628c43088ec28",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n@@ -35,17 +36,13 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n \n namespace xla {\n namespace gpu {\n \n using CublasLtBackendConfig = AutotuneResult::GemmKey;\n-using ::tsl::testing::IsOk;\n-using ::tsl::testing::IsOkAndHolds;\n-using ::tsl::testing::StatusIs;\n \n const char kCublasLtCustomCallHlo[] = R\"(\n HloModule module\n@@ -107,7 +104,7 @@ class CublasLtBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CublasLtBackend backend_;\n \n   CublasLtBackendTest()"
        },
        {
            "sha": "a328cdefa2811b707f67e764d48c55c9c2153e71",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -58,7 +58,7 @@ class CudnnBackend : public GpuCodegenBackend {\n  public:\n   explicit CudnnBackend(stream_executor::StreamExecutor* stream_executor,\n                         const DebugOptions* debug_options, Compiler* compiler,\n-                        const Compiler::TargetConfig* target_config)\n+                        const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"Cudnn\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "63cefe11da420c1d7b39ccfe6abc40839cb64d43",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n@@ -34,7 +35,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/protobuf/dnn.pb.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n@@ -48,8 +48,6 @@ using CudnnBackendConfig = stream_executor::dnn::AlgorithmProto;\n using ::testing::Gt;\n using ::testing::SizeIs;\n using ::tsl::proto_testing::EqualsProto;\n-using ::tsl::testing::IsOkAndHolds;\n-using ::tsl::testing::StatusIs;\n \n const char kCudnnFusionHlo[] = R\"(\n   fusion1 {\n@@ -113,7 +111,7 @@ class CudnnBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CudnnBackend backend_;\n \n   CudnnBackendTest()"
        },
        {
            "sha": "aa166da4c5342fa9e9e87bacf5e64015fab13a6e",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -46,15 +46,14 @@ using CustomKernelBackendConfig = AutotuneResult::CustomKernelFusionKey;\n \n bool CustomKernelBackend::IsSupported(const HloInstruction& instr) {\n   if (instr.opcode() != HloOpcode::kFusion) {\n-    LOG(ERROR)\n-        << \"CustomKernelBackend doesn't support non-fusion instructions.\";\n+    VLOG(1) << \"CustomKernelBackend doesn't support non-fusion instructions.\";\n     return false;\n   }\n \n   if (instr.backend_config<GpuBackendConfig>()\n           ->fusion_backend_config()\n           .kind() != kCustomFusionKind) {\n-    LOG(ERROR) << \"CustomKernelBackend expected a custom fusion.\";\n+    VLOG(1) << \"CustomKernelBackend expected a custom fusion.\";\n     return false;\n   }\n "
        },
        {
            "sha": "c1aa956a61995b7e5a5e02628b019bd2574cebdc",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -36,7 +36,7 @@ class CustomKernelBackend : public GpuCodegenBackend {\n   explicit CustomKernelBackend(stream_executor::StreamExecutor* stream_executor,\n                                const DebugOptions* debug_options,\n                                Compiler* compiler,\n-                               const Compiler::TargetConfig* target_config)\n+                               const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"CustomKernel\", debug_options, compiler,\n                           target_config, stream_executor) {}\n "
        },
        {
            "sha": "ae4ad8b230ed7addd0be2dcc777388d09893500e",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n@@ -33,7 +34,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"xla/xla.pb.h\"\n@@ -44,9 +44,6 @@ namespace gpu {\n using CustomKernelBackendConfig = AutotuneResult::CustomKernelFusionKey;\n \n using ::tsl::proto_testing::EqualsProto;\n-using tsl::testing::IsOk;\n-using tsl::testing::IsOkAndHolds;\n-using tsl::testing::StatusIs;\n \n const char kCustomKernelFusionHlo[] = R\"(\n HloModule extracted\n@@ -103,7 +100,7 @@ class CustomKernelBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CustomKernelBackend backend_;\n \n   CustomKernelBackendTest()"
        },
        {
            "sha": "cf99586c21eff16aaf9d87258bce784bec23eaa4",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -32,14 +32,14 @@ namespace gpu {\n struct GetCodegenBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      const Compiler::TargetConfig*,\n+      const Compiler::GpuTargetConfig*,\n       SymbolicExprContext* symbolic_expr_context)>;\n };\n \n struct GetFissionBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      const Compiler::TargetConfig*,\n+      const Compiler::GpuTargetConfig*,\n       SymbolicExprContext* symbolic_expr_context)>;\n };\n "
        },
        {
            "sha": "28b5ba0106867e5d6ab082d575def09d0cc497cb",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -62,7 +62,7 @@ std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n@@ -79,7 +79,7 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<FissionBackend>("
        },
        {
            "sha": "2eed41c6dcbf7de6de73763ae35f42f2d0a49347",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_rocm.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -35,7 +35,7 @@ namespace gpu {\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n@@ -48,7 +48,7 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   return {};\n }"
        },
        {
            "sha": "94e30b996b1845bcf9a0c6ec0f30545273273eb1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -45,7 +45,7 @@ class FissionBackend : public GpuCodegenBackend {\n  public:\n   explicit FissionBackend(stream_executor::StreamExecutor* stream_executor,\n                           const DebugOptions* debug_options, Compiler* compiler,\n-                          const Compiler::TargetConfig* target_config,\n+                          const Compiler::GpuTargetConfig* target_config,\n                           SymbolicExprContext* symbolic_expr_context)\n       : GpuCodegenBackend(\"Fission\", debug_options, compiler, target_config),\n         cublas_backend_(stream_executor, debug_options, compiler,"
        },
        {
            "sha": "0c62f6bc8d160726a4a4c640300f624412a45f41",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -46,6 +47,11 @@ namespace {\n // computation.\n absl::Status InlineFissionedComputation(HloInstruction* fusion_instr,\n                                         HloComputation* fissioned_computation) {\n+  if (fusion_instr->opcode() != HloOpcode::kFusion) {\n+    return absl::InvalidArgumentError(\"Not a fusion instruction.\");\n+  }\n+  HloModule* original_module = fusion_instr->GetModule();\n+  HloCloneContext clone_context(original_module);\n   absl::flat_hash_map<const HloInstruction*, HloInstruction*>\n       cloned_instructions;\n   HloComputation* parent_computation = fusion_instr->parent();\n@@ -64,7 +70,7 @@ absl::Status InlineFissionedComputation(HloInstruction* fusion_instr,\n     }\n     HloInstruction* new_instruction = parent_computation->AddInstruction(\n         instruction_to_clone->CloneWithNewOperands(\n-            instruction_to_clone->shape(), new_operands));\n+            instruction_to_clone->shape(), new_operands, &clone_context));\n     cloned_instructions[instruction_to_clone] = new_instruction;\n   }\n   HloInstruction* new_root =\n@@ -81,9 +87,13 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   }\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> hlo_module,\n                       GetFissionedAndRewrittenModule(instr));\n-  TF_ASSIGN_OR_RETURN(HloInstruction * supported_instr,\n-                      FindFirstSupportedInstruction(hlo_module.get()));\n-  return codegen_backend_->GetSupportedConfigs(*supported_instr);\n+  absl::StatusOr<HloInstruction*> supported_instr =\n+      FindFirstSupportedInstruction(hlo_module.get());\n+  if (supported_instr.status().code() == absl::StatusCode::kNotFound) {\n+    return std::vector<std::unique_ptr<BackendConfig>>();\n+  }\n+  TF_RETURN_IF_ERROR(supported_instr.status());\n+  return codegen_backend_->GetSupportedConfigs(**supported_instr);\n \n   return std::vector<std::unique_ptr<BackendConfig>>();\n }\n@@ -98,8 +108,6 @@ absl::StatusOr<std::unique_ptr<BackendConfig>> FissionBackend::GetDefaultConfig(\n   TF_ASSIGN_OR_RETURN(HloInstruction * supported_instr,\n                       FindFirstSupportedInstruction(hlo_module.get()));\n   return codegen_backend_->GetDefaultConfig(*supported_instr);\n-\n-  return absl::InvalidArgumentError(\"No supported configs found.\");\n }\n \n absl::StatusOr<std::unique_ptr<HloModule>> FissionBackend::RunHloPasses(\n@@ -155,7 +163,7 @@ absl::StatusOr<HloInstruction*> FissionBackend::FindFirstSupportedInstruction(\n     }\n   }\n   if (supported_instructions.empty()) {\n-    return absl::InvalidArgumentError(\"No supported instructions found.\");\n+    return absl::NotFoundError(\"No supported instructions found.\");\n   }\n   if (supported_instructions.size() > 1) {\n     LOG(WARNING) << \"Backend \" << name()"
        },
        {
            "sha": "d5ca0850f63b12226129e204f63c6db2a1405b6d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -44,7 +44,7 @@ namespace xla::gpu {\n class FissionBackend : public GpuCodegenBackend {\n  public:\n   FissionBackend(const DebugOptions* debug_options, Compiler* compiler,\n-                 const Compiler::TargetConfig* target_config,\n+                 const Compiler::GpuTargetConfig* target_config,\n                  std::unique_ptr<GpuCodegenBackend> backend,\n                  std::unique_ptr<HloPassPipeline> rewriter_pipeline,\n                  SymbolicExprContext* symbolic_expr_context,"
        },
        {
            "sha": "248b76c99059488a2d94361ec44641946b6e6c11",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend_test.cc",
            "status": "modified",
            "additions": 133,
            "deletions": 31,
            "changes": 164,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/autotuner/fission_backend.h\"\n \n+#include <functional>\n #include <memory>\n #include <string>\n #include <utility>\n@@ -27,6 +28,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n+#include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -36,6 +38,7 @@ limitations under the License.\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n+#include \"xla/service/gpu/transforms/custom_kernel_fusion_rewriter.h\"\n #include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n #include \"xla/service/platform_util.h\"\n@@ -72,72 +75,144 @@ const char kTritonFusionHlo[] = R\"(\n       backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n   })\";\n \n-class CublasFissionTest : public HloHardwareIndependentTestBase {\n- protected:\n-  DebugOptions debug_options_;\n-  NVPTXCompiler compiler_;\n-  se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n-  se::DeviceDescription device_description_;\n-  std::unique_ptr<HloPassPipeline> rewriter_pipeline_;\n-  std::unique_ptr<GpuCodegenBackend> cublas_backend_;\n-  std::unique_ptr<FissionBackend> fission_backend_;\n-  mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+const char kUnsupportedFusionHlo[] = R\"(\n+  HloModule module\n+  computation {\n+    p0 = bf16[1024,1024]{1,0} parameter(0)\n+    convert0 = f32[1024,1024]{1,0} convert(p0)\n+    p1 = s8[1024,1024]{1,0} parameter(1)\n+    convert1 = f32[1024,1024]{1,0} convert(p1)\n+    ROOT add = f32[1024,1024]{1,0} add(convert0, convert1)\n+  }\n+\n+  ENTRY main {\n+    p0 = bf16[1024,1024]{1,0} parameter(0)\n+    p1 = s8[1024,1024]{1,0} parameter(1)\n+    ROOT fusion = f32[1024,1024]{1,0} fusion(p0, p1),\n+      kind=kCustom, calls=computation\n+  })\";\n \n-  std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline() {\n+struct FissionTestParams {\n+  std::string test_name;\n+  std::string hlo_string;\n+  // Factory function to create the rewriter pipeline.\n+  std::function<std::unique_ptr<HloPassPipeline>(\n+      const se::DeviceDescription& device_description)>\n+      pipeline_factory;\n+  // Factory function to create the underlying codegen backend.\n+  std::function<std::unique_ptr<GpuCodegenBackend>(\n+      se::StreamExecutor*, const DebugOptions*, Compiler*,\n+      const Compiler::GpuTargetConfig*)>\n+      backend_factory;\n+  // Substrings expected to be in the module after ApplyConfig.\n+  std::vector<std::string> expected_module_substrings;\n+  std::string expected_backend_name;\n+};\n+\n+class FissionTest : public HloHardwareIndependentTestBase,\n+                    public ::testing::WithParamInterface<FissionTestParams> {\n+ public:\n+  // Static helper to create the Cublas rewriter pipeline.\n+  static std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n+      const se::DeviceDescription& device_description) {\n     auto pipeline = std::make_unique<HloPassPipeline>(\"fission_pipeline\");\n     pipeline->AddPass(std::make_unique<DotAlgorithmRewriter>());\n     for (GemmRewriterOptions::DType dtype :\n          {GemmRewriterOptions::DType::kFp8Only,\n           GemmRewriterOptions::DType::kNonFp8Only}) {\n       auto gemm_rewriter = std::make_unique<GemmRewriter>(\n-          device_description_.gpu_compute_capability(),\n-          device_description_.runtime_version(), GemmRewriterOptions{dtype});\n+          device_description.gpu_compute_capability(),\n+          device_description.runtime_version(), GemmRewriterOptions{dtype});\n       pipeline->AddPass(std::move(gemm_rewriter));\n     }\n     return pipeline;\n   }\n \n-  CublasFissionTest()\n+  // Static helper to create the Custom Kernel rewriter pipeline.\n+  static std::unique_ptr<HloPassPipeline> GetCustomKernelRewriterPipeline(\n+      const se::DeviceDescription& device_description) {\n+    auto pipeline = std::make_unique<HloPassPipeline>(\"fission_pipeline\");\n+    pipeline->AddPass(\n+        std::make_unique<CustomKernelFusionRewriter>(&device_description));\n+    return pipeline;\n+  }\n+\n+  // Static helper to create a CublasBackend.\n+  static std::unique_ptr<GpuCodegenBackend> CreateCublasBackend(\n+      se::StreamExecutor* stream_executor, const DebugOptions* debug_options,\n+      Compiler* compiler, const Compiler::GpuTargetConfig* target_config) {\n+    return std::make_unique<CublasBackend>(stream_executor, debug_options,\n+                                           compiler, target_config);\n+  }\n+\n+  // Static helper to create a CustomKernelBackend.\n+  static std::unique_ptr<GpuCodegenBackend> CreateCustomKernelBackend(\n+      se::StreamExecutor* stream_executor, const DebugOptions* debug_options,\n+      Compiler* compiler, const Compiler::GpuTargetConfig* target_config) {\n+    return std::make_unique<CustomKernelBackend>(stream_executor, debug_options,\n+                                                 compiler, target_config);\n+  }\n+\n+ protected:\n+  DebugOptions debug_options_;\n+  NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::GpuTargetConfig target_config_;\n+  se::DeviceDescription device_description_;\n+  std::unique_ptr<HloPassPipeline> rewriter_pipeline_;\n+  std::unique_ptr<GpuCodegenBackend> base_codegen_backend_;\n+  std::unique_ptr<FissionBackend> fission_backend_;\n+  mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+\n+  FissionTest()\n       : stream_executor_(PlatformUtil::GetDefaultPlatform()\n                              .value()\n                              ->ExecutorForDevice(0)\n                              .value()),\n         target_config_(stream_executor_),\n         device_description_(stream_executor_->GetDeviceDescription()),\n-        rewriter_pipeline_(GetCublasRewriterPipeline()),\n-        cublas_backend_(std::make_unique<CublasBackend>(\n+        rewriter_pipeline_(GetParam().pipeline_factory(device_description_)),\n+        base_codegen_backend_(GetParam().backend_factory(\n             stream_executor_, &debug_options_, &compiler_, &target_config_)),\n         fission_backend_(std::make_unique<FissionBackend>(\n             &debug_options_, &compiler_, &target_config_,\n-            std::move(cublas_backend_), std::move(rewriter_pipeline_),\n+            std::move(base_codegen_backend_), std::move(rewriter_pipeline_),\n             &symbolic_expr_context_, stream_executor_)) {}\n };\n \n-TEST_F(CublasFissionTest, CanCreateFissionBackend) {\n-  EXPECT_EQ(fission_backend_->name(), \"Cublas_fission\");\n+TEST_P(FissionTest, CanCreateFissionBackend) {\n+  EXPECT_EQ(fission_backend_->name(), GetParam().expected_backend_name);\n }\n \n-TEST_F(CublasFissionTest, GetSupportedConfigs) {\n+TEST_P(FissionTest, GetSupportedConfigs) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+                          ParseAndReturnVerifiedModule(GetParam().hlo_string));\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n       fission_backend_->GetSupportedConfigs(\n           (*module->entry_computation()->root_instruction()));\n   EXPECT_THAT(configs, IsOkAndHolds(testing::SizeIs(1)));\n }\n \n-TEST_F(CublasFissionTest, GetDefaultConfig) {\n+TEST_P(FissionTest, GetSupportedConfigsUnsupportedFusion) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kUnsupportedFusionHlo));\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n+      fission_backend_->GetSupportedConfigs(\n+          (*module->entry_computation()->root_instruction()));\n+  EXPECT_THAT(configs, IsOkAndHolds(testing::IsEmpty()));\n+}\n+\n+TEST_P(FissionTest, GetDefaultConfig) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+                          ParseAndReturnVerifiedModule(GetParam().hlo_string));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n   EXPECT_THAT(fission_backend_->GetDefaultConfig(*fusion), IsOk());\n }\n \n-TEST_F(CublasFissionTest, Compile) {\n+TEST_P(FissionTest, Compile) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+                          ParseAndReturnVerifiedModule(GetParam().hlo_string));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n                           fission_backend_->GetDefaultConfig(*fusion));\n@@ -147,18 +222,45 @@ TEST_F(CublasFissionTest, Compile) {\n   EXPECT_NE(executable, nullptr);\n }\n \n-TEST_F(CublasFissionTest, ApplyConfig) {\n+TEST_P(FissionTest, ApplyConfig) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+                          ParseAndReturnVerifiedModule(GetParam().hlo_string));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n                           fission_backend_->GetDefaultConfig(*fusion));\n   EXPECT_THAT(fission_backend_->ApplyConfig(*fusion, *config), IsOk());\n   std::string module_str = module->ToString();\n-  EXPECT_THAT(module_str, HasSubstr(\"custom_call_target=\\\"__cublas$gemm\\\"\"));\n-  EXPECT_THAT(module_str, HasSubstr(\"\\\"selected_algorithm\\\":\\\"-1\\\"\"));\n+  for (const std::string& expected_substr :\n+       GetParam().expected_module_substrings) {\n+    EXPECT_THAT(module_str, HasSubstr(expected_substr));\n+  }\n }\n \n+INSTANTIATE_TEST_SUITE_P(\n+    FissionTests, FissionTest,\n+    ::testing::ValuesIn<FissionTestParams>({\n+        {\"TritonFusion_Cublas\",\n+         kTritonFusionHlo,\n+         &FissionTest::GetCublasRewriterPipeline,\n+         &FissionTest::CreateCublasBackend,\n+         /*expected_module_substrings=*/\n+         {\"custom_call_target=\\\"__cublas$gemm\\\"\",\n+          \"\\\"selected_algorithm\\\":\\\"-1\\\"\"},\n+         /*expected_backend_name=*/\"Cublas_fission\"},\n+        {\"TritonFusion_CustomKernel\",\n+         kTritonFusionHlo,\n+         &FissionTest::GetCustomKernelRewriterPipeline,\n+         &FissionTest::CreateCustomKernelBackend,\n+         /*expected_module_substrings=*/\n+         {\n+             \"\\\"kind\\\":\\\"__custom_fusion\\\"\",\n+         },\n+         /*expected_backend_name=*/\"CustomKernel_fission\"},\n+    }),\n+    [](const ::testing::TestParamInfo<FissionTest::ParamType>& info) {\n+      return info.param.test_name;\n+    });\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "36cd3ee128dabf7c0fa910dfe5eee8d5dfdd341c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -74,7 +74,7 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   FissionBackend backend_;\n   mlir::MLIRContext mlir_context_;\n   SymbolicExprContext symbolic_expr_context_{&mlir_context_};"
        },
        {
            "sha": "bdb247e4e3f577716c6d6067d33c0b0d4286de29",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -34,7 +34,7 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n \n namespace xla {\n-namespace  gpu {\n+namespace gpu {\n \n // Abstract base class for GPU backends, implementing the Backend interface.\n class GpuCodegenBackend : public CodegenBackend {\n@@ -43,7 +43,7 @@ class GpuCodegenBackend : public CodegenBackend {\n   // TODO(b/447096292): Remove stream_executor from GpuCodegenBackend.\n   GpuCodegenBackend(absl::string_view name, const DebugOptions* debug_options,\n                     Compiler* compiler,\n-                    const Compiler::TargetConfig* target_config,\n+                    const Compiler::GpuTargetConfig* target_config,\n                     stream_executor::StreamExecutor* stream_executor = nullptr)\n       : name_(name),\n         stream_executor_(stream_executor),\n@@ -53,7 +53,9 @@ class GpuCodegenBackend : public CodegenBackend {\n \n   absl::string_view name() const override { return name_; }\n \n-  const Compiler::TargetConfig& target_config() const { return target_config_; }\n+  const Compiler::GpuTargetConfig& target_config() const {\n+    return target_config_;\n+  }\n   const DebugOptions& debug_options() const { return debug_options_; }\n   stream_executor::StreamExecutor* stream_executor() {\n     return stream_executor_;\n@@ -75,7 +77,7 @@ class GpuCodegenBackend : public CodegenBackend {\n         allow_register_spills_);\n \n     Compiler::CompileOptions options;\n-    options.target_config = target_config_;\n+    options.gpu_target_config = target_config_;\n     options.is_autotuning_compilation = true;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));\n@@ -127,7 +129,7 @@ class GpuCodegenBackend : public CodegenBackend {\n \n   std::string name_;\n   stream_executor::StreamExecutor* stream_executor_;\n-  const Compiler::TargetConfig& target_config_;\n+  const Compiler::GpuTargetConfig& target_config_;\n   const DebugOptions& debug_options_;\n   // TODO(b/407494653): remove compiler when we don't need to run any HLO passes\n   // and the codegen backend can directly produce an executable without a"
        },
        {
            "sha": "536e39cc9339b2022febd0b77f7d28d639f16333",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -46,7 +47,7 @@ std::optional<LegacyCache::Config> LegacyCache::Lookup(\n   if (!result->has_value()) {\n     return std::nullopt;\n   }\n-  return GetConfig(result->value());\n+  return GetConfig(result->value(), instr->opcode() == HloOpcode::kFusion);\n }\n \n absl::Status LegacyCache::Insert(const HloInstruction* instr,\n@@ -99,20 +100,26 @@ AutotuneCacheKey LegacyCache::GetAutotuneCacheKey(const HloInstruction& instr) {\n }\n \n std::optional<LegacyCache::Config> LegacyCache::GetConfig(\n-    const AutotuneResult& result) {\n+    const AutotuneResult& result, bool is_fusion_instruction) {\n   Config config;\n   if (result.has_triton()) {\n     config.codegen_backend_name = \"Triton\";\n     config.backend_config.PackFrom(result.triton());\n   } else if (result.has_gemm()) {\n     config.codegen_backend_name = \"Cublas\";\n+    if (is_fusion_instruction) {\n+      config.codegen_backend_name = \"Cublas_fission\";\n+    }\n     config.backend_config.PackFrom(result.gemm());\n   } else if (result.has_algorithm()) {\n     config.codegen_backend_name = \"Cudnn\";\n     config.backend_config.PackFrom(result.algorithm());\n   } else if (result.has_other()) {\n     config.codegen_backend_name = result.other().name();\n     config.backend_config = result.other().config();\n+  } else if (result.has_custom_kernel_fusion()) {\n+    config.codegen_backend_name = \"CustomKernel\";\n+    config.backend_config.PackFrom(result.custom_kernel_fusion());\n   } else {\n     return std::nullopt;\n   }\n@@ -124,10 +131,13 @@ std::optional<AutotuneResult> LegacyCache::GetAutotuneResult(\n   AutotuneResult result;\n   if (config.codegen_backend_name == \"Triton\") {\n     config.backend_config.UnpackTo(result.mutable_triton());\n-  } else if (config.codegen_backend_name == \"Cublas\") {\n+  } else if (config.codegen_backend_name == \"Cublas\" ||\n+             config.codegen_backend_name == \"Cublas_fission\") {\n     config.backend_config.UnpackTo(result.mutable_gemm());\n   } else if (config.codegen_backend_name == \"Cudnn\") {\n     config.backend_config.UnpackTo(result.mutable_algorithm());\n+  } else if (config.codegen_backend_name == \"CustomKernel\") {\n+    config.backend_config.UnpackTo(result.mutable_custom_kernel_fusion());\n   } else {\n     result.mutable_other()->set_name(config.codegen_backend_name);\n     *result.mutable_other()->mutable_config() = config.backend_config;"
        },
        {
            "sha": "48c30b085235a4d111684f69f390f2b1b43daf3f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -59,7 +59,8 @@ class LegacyCache : public AutotunerCacheInterface {\n \n   // Translates between the AutotunerCacheInterface::Config and the\n   // AutotuneResult.\n-  std::optional<Config> GetConfig(const AutotuneResult& result);\n+  std::optional<Config> GetConfig(const AutotuneResult& result,\n+                                  bool is_fusion_instruction);\n   std::optional<AutotuneResult> GetAutotuneResult(const Config& config);\n \n   const std::string cache_dir_;"
        },
        {
            "sha": "86097bc05e7b63a4f749122655fcfb31a57bf546",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache_test.cc",
            "status": "modified",
            "additions": 49,
            "deletions": 1,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n #include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -100,13 +101,27 @@ class LegacyCacheTest : public ::testing::Test {\n     return config;\n   }\n \n+  Config CreateDummyCublasFissionConfig() {\n+    Config config;\n+    config.codegen_backend_name = \"Cublas_fission\";\n+    config.backend_config.PackFrom(AutotuneResult::GemmKey());\n+    return config;\n+  }\n+\n   Config CreateDummyCudnnConfig() {\n     Config config;\n     config.codegen_backend_name = \"Cudnn\";\n     config.backend_config.PackFrom(stream_executor::dnn::AlgorithmProto());\n     return config;\n   }\n \n+  Config CreateDummyCustomKernelConfig() {\n+    Config config;\n+    config.codegen_backend_name = \"CustomKernel\";\n+    config.backend_config.PackFrom(AutotuneResult::CustomKernelFusionKey());\n+    return config;\n+  }\n+\n   Config CreateDummyBackendConfig() {\n     using DummyOtherConfig = AutotuneResult::CustomKernelFusionKey;\n     Config config;\n@@ -166,6 +181,31 @@ TEST_F(LegacyCacheTest, InsertAndLookupCublas) {\n   EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n }\n \n+TEST_F(LegacyCacheTest, InsertAndLookupCublasFission) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  constexpr char kHLO[] = R\"(\n+HloModule test_module\n+\n+fused_computation {\n+  param.0 = f32[] parameter(0)\n+  param.1 = f32[] parameter(1)\n+  ROOT add.0 = f32[] add(param.0, param.1)\n+}\n+\n+ENTRY main {\n+  p0 = f32[] parameter(0)\n+  p1 = f32[] parameter(1)\n+  ROOT fusion.0 = f32[] fusion(p0, p1), kind=kLoop, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHLO));\n+  auto instr = module->entry_computation()->root_instruction();\n+  Config config = CreateDummyCublasFissionConfig();\n+\n+  TF_ASSERT_OK(cache.Insert(instr, config));\n+  EXPECT_THAT(cache.Lookup(instr), Optional(ConfigEq(config)));\n+}\n+\n TEST_F(LegacyCacheTest, InsertAndLookupCudnn) {\n   auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n   auto instr = CreateDummyInstr(\"hlo3\");\n@@ -175,9 +215,17 @@ TEST_F(LegacyCacheTest, InsertAndLookupCudnn) {\n   EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n }\n \n-TEST_F(LegacyCacheTest, InsertAndLookupOther) {\n+TEST_F(LegacyCacheTest, InsertAndLookupCustomKernel) {\n   auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n   auto instr = CreateDummyInstr(\"hlo4\");\n+  Config config = CreateDummyCustomKernelConfig();\n+  TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n+}\n+\n+TEST_F(LegacyCacheTest, InsertAndLookupOther) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  auto instr = CreateDummyInstr(\"hlo5\");\n   Config config = CreateDummyBackendConfig();\n \n   TF_ASSERT_OK(cache.Insert(instr.get(), config));"
        },
        {
            "sha": "73bc68705091a95f93a334d1834379417ccdc8ed",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/miopen.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -36,7 +36,7 @@ class MIOpenBackend : public GpuCodegenBackend {\n  public:\n   explicit MIOpenBackend(stream_executor::StreamExecutor* stream_executor,\n                          const DebugOptions* debug_options, Compiler* compiler,\n-                         const Compiler::TargetConfig* target_config)\n+                         const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"MIOpen\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "5a5538f51190affd75b4deacb1d55c60886d3be7",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/miopen_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n@@ -35,7 +36,6 @@ limitations under the License.\n #include \"xla/stream_executor/rocm/rocm_platform_id.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/protobuf/dnn.pb.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n@@ -46,9 +46,9 @@ namespace gpu {\n \n using MIOpenBackendConfig = stream_executor::dnn::AlgorithmProto;\n \n+using absl_testing::IsOkAndHolds;\n using ::testing::SizeIs;\n using ::tsl::proto_testing::EqualsProto;\n-using ::tsl::testing::IsOkAndHolds;\n \n const char kMIOpenCustomCallHlo[] = R\"(\n   HloModule module\n@@ -76,7 +76,7 @@ class MIOpenBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   AMDGPUCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   MIOpenBackend backend_;\n \n   MIOpenBackendTest()"
        },
        {
            "sha": "a7fc19d559bf1ab2c93961fb8fa1be444ad09d4a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -40,7 +40,7 @@ class NativeEmitterBackend : public GpuCodegenBackend {\n  public:\n   explicit NativeEmitterBackend(const DebugOptions* absl_nonnull debug_options,\n                                 Compiler* absl_nonnull compiler,\n-                                const Compiler::TargetConfig* target_config)\n+                                const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"NativeEmitter\", debug_options, compiler,\n                           target_config) {}\n "
        },
        {
            "sha": "bdf8dad85a9b8ba5cdbf0983f959644145db5ad5",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -101,7 +101,7 @@ class NativeEmitterBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   NativeEmitterBackend backend_;\n };\n "
        },
        {
            "sha": "efde8de4a5aea1f23609d600c5daad8f8b8e93cd",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -18,12 +18,13 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -38,6 +39,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/triton_configs.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_float_support.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/split_k_gemm_rewriter.h\"\n@@ -147,13 +149,13 @@ TritonBackend::GetSupportedConfigs(const HloInstruction& instr) {\n \n absl::StatusOr<std::unique_ptr<BackendConfig>> TritonBackend::GetDefaultConfig(\n     const HloInstruction& instr) {\n-  if (!IsSupported(instr)) {\n+  TF_ASSIGN_OR_RETURN(std::vector<std::unique_ptr<BackendConfig>> configs,\n+                      GetSupportedConfigs(instr));\n+  if (configs.empty()) {\n     return absl::InvalidArgumentError(\n         \"TritonBackend does not support this instruction.\");\n   }\n-  auto any = std::make_unique<google::protobuf::Any>();\n-  any->PackFrom(TritonGemmConfig(64, 64, 64, 1, 1, 2, 1, false).ToProto());\n-  return any;\n+  return std::move(configs[0]);\n }\n \n absl::Status TritonBackend::ApplyConfig(HloInstruction& instr,\n@@ -173,6 +175,7 @@ absl::Status TritonBackend::ApplyConfig(HloInstruction& instr,\n   FusionBackendConfig& backend_config =\n       *gpu_config.mutable_fusion_backend_config();\n \n+  backend_config.set_kind(kTritonGemmFusionKind);\n   *backend_config.mutable_triton_gemm_config() = triton_config_proto;\n   TF_RETURN_IF_ERROR(instr.set_backend_config(gpu_config));\n \n@@ -210,6 +213,18 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonBackend::RunHloPasses(\n \n   NestGemmFusion nest_gemm_fusion(gpu_device_info, symbolic_expr_context_);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(hlo_module.get()).status());\n+\n+  bool is_legacy_gemm_disabled = absl::c_contains(\n+      debug_options().xla_gpu_unsupported_generic_triton_emitter_features(),\n+      DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n+  bool is_triton_gemm_fusion =\n+      IsGpuFusionKind(*hlo_module->entry_computation()->root_instruction(),\n+                      kTritonGemmFusionKind);\n+  if (is_legacy_gemm_disabled && is_triton_gemm_fusion) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Unexpected \", kTritonGemmFusionKind,\n+                     \" fusion: \", hlo_module->ToString()));\n+  }\n   return hlo_module;\n }\n "
        },
        {
            "sha": "592f9c97f96f8d3b9def83b8ad3064414a9024b0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -37,7 +37,7 @@ namespace gpu {\n class TritonBackend : public GpuCodegenBackend {\n  public:\n   explicit TritonBackend(const DebugOptions* debug_options, Compiler* compiler,\n-                         const Compiler::TargetConfig* target_config,\n+                         const Compiler::GpuTargetConfig* target_config,\n                          SymbolicExprContext* symbolic_expr_context)\n       : GpuCodegenBackend(\"Triton\", debug_options, compiler, target_config),\n         symbolic_expr_context_(symbolic_expr_context) {}"
        },
        {
            "sha": "5c20f23cc5531652ca2e5db9f270093a207cb14e",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -89,7 +89,7 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   TritonBackend backend_;\n   mlir::MLIRContext mlir_context_;\n   SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n@@ -156,17 +156,11 @@ TEST_F(TritonBackendTest, GetSupportedConfigsForUnsupportedInstruction) {\n TEST_F(TritonBackendTest, GetDefaultConfig) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kHlo));\n-  TritonBackendConfig expected_config =\n-      TritonGemmConfig(64, 64, 64, 1, 1, 2, 1, false).ToProto();\n-\n   absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n       backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction()));\n \n   EXPECT_THAT(config, absl_testing::IsOk());\n-  TritonBackendConfig actual_config;\n-  ASSERT_TRUE(config.value()->UnpackTo(&actual_config));\n-  EXPECT_THAT(actual_config, EqualsProto(expected_config));\n }\n \n TEST_F(TritonBackendTest, GetDefaultConfigForUnsupportedInstruction) {"
        },
        {
            "sha": "f94f1716a5c1d04f35821000a84d3aaf32a51ffc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -127,11 +127,13 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/runtime:all_reduce_thunk\",\n         \"//xla/backends/gpu/runtime:collective_thunk\",\n         \"//xla/backends/gpu/runtime:copy_thunk\",\n         \"//xla/backends/gpu/runtime:custom_call_target\",\n         \"//xla/backends/gpu/runtime:custom_call_thunk\",\n+        \"//xla/backends/gpu/runtime:custom_kernel_thunk\",\n         \"//xla/backends/gpu/runtime:dynamic_slice_thunk\",\n         \"//xla/backends/gpu/runtime:gemm_thunk\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\","
        },
        {
            "sha": "f3713676400a0b570e407d77002e3e9f9dc3c1a5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -1250,15 +1250,18 @@ ENTRY main {\n       backend_config={\"fusion_backend_config\":{kind:\"__cudnn$fusion\"}}\n })\";\n   EXPECT_TRUE(*RunCuDnnFileCheck(kHloText, R\"(\n+CHECK: \"intermediate_data_type\": \"FLOAT\"\n CHECK: \"nodes\"\n CHECK: {\n CHECK: \"block_size\": [{{[[:space:]]*32[[:space:]]*}}]\n+CHECK: \"compute_data_type\": \"FLOAT\"\n CHECK: \"X\": \"lhs\"\n CHECK: \"scale\": \"lhs_scale\"\n CHECK: \"Y\": \"result_lhs_dq\"\n CHECK: \"tag\": \"BLOCK_SCALE_DEQUANTIZE\"\n CHECK: {\n CHECK: \"block_size\": [{{[[:space:]]*32[[:space:]]*}}]\n+CHECK: \"compute_data_type\": \"FLOAT\"\n CHECK: \"X\": \"rhs\"\n CHECK: \"scale\": \"rhs_scale\"\n CHECK: \"Y\": \"result_rhs_dq\""
        },
        {
            "sha": "8c59b16f4ac89a73d29cdf7186c736650a0015ca",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_target.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n@@ -84,6 +85,7 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n \n namespace xla {\n namespace gpu {\n@@ -100,9 +102,11 @@ absl::StatusOr<std::unique_ptr<Thunk>> BuildCustomKernelThunkForFusion(\n       emitters::KernelArguments::Create(ir_emitter_context.buffer_assignment(),\n                                         GetDefaultBufferAlignment(), &fusion));\n \n-  return std::make_unique<CustomKernelThunk>(\n-      &fusion, std::move(custom_kernel), std::move(kernel_arguments),\n-      ir_emitter_context.GetNextThunkId());\n+  Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      &fusion, ir_emitter_context.GetNextThunkId());\n+  return std::make_unique<CustomKernelThunk>(std::move(thunk_info),\n+                                             std::move(custom_kernel),\n+                                             std::move(kernel_arguments));\n }\n \n absl::StatusOr<BufferAllocation::Slice> GetOperandSlice("
        },
        {
            "sha": "370ce943bc07eb62c0b487696253c30ee5257d8b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -576,16 +576,18 @@ xla_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//llvm:ir_headers\",\n@@ -623,6 +625,7 @@ xla_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/stream_executor:device_description\",\n@@ -631,7 +634,6 @@ xla_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/status\",\n@@ -1166,13 +1168,14 @@ xla_test(\n         \"//xla/service/gpu:triton_fusion_analysis\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:status_matchers\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -1273,9 +1276,9 @@ xla_cc_test(\n         \":tma_utils\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "71b3ceaead1cb0197cd10c975808c0fe2e043f64",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -37,6 +37,7 @@ void CreateTritonXlaPipeline(\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerAtomicsPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerGetTidPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerXTilePass());\n+  pm->addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n \n   auto* cuda_cc = gpu_cc.cuda_compute_capability();\n   bool is_at_least_hopper = cuda_cc != nullptr && cuda_cc->IsAtLeastHopper();"
        },
        {
            "sha": "b52834f1b0d494e073aff374d2312cf31115c52a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 23,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -226,11 +226,6 @@ absl::StatusOr<TensorValue> EmitReduce(\n   const HloReduceInstruction& hlo_reduce =\n       *::xla::Cast<HloReduceInstruction>(tiled_hlo_reduce.hlo());\n   TensorValue input = values[tiled_hlo_reduce.operand(0)];\n-  llvm::ArrayRef<int64_t> input_shape = input.getType().getShape();\n-  absl::Span<const int64_t> source_tensor_shape =\n-      hlo_reduce.operand(0)->shape().dimensions();\n-\n-  int reduction_dimension = hlo_reduce.dimensions().front();\n \n   // Since every shape is padded to a power of 2 in Triton, the input tile may\n   // be padded with arbitrary values. These values could affect the result of\n@@ -240,29 +235,30 @@ absl::StatusOr<TensorValue> EmitReduce(\n   // hlo_reduce.operand(1) is thus always the right choice to ensure that the\n   // reduction is computed correctly, since it is the neutral value with\n   // regards to the reducer.\n-  int64_t source_tensor_reduction_dimension_size =\n-      source_tensor_shape[reduction_dimension];\n-  int64_t input_reduction_dimension_size = input_shape[reduction_dimension];\n-  if (input_reduction_dimension_size !=\n-      source_tensor_reduction_dimension_size) {\n-    TensorValue range = Iota(b, input_reduction_dimension_size);\n-    TensorValue bcast =\n-        BroadcastInDims(b, range, input_shape, {reduction_dimension});\n-    TensorValue constant = CreateConst(\n-        b, b.getI32Type(), source_tensor_reduction_dimension_size, input_shape);\n-    Value mask =\n-        b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast, constant);\n-\n-    TensorValue neutral = BroadcastInDims(\n-        b, values[tiled_hlo_reduce.operand(1)], input_shape, /*dims=*/{});\n-    input = mlir::cast<TensorValue>(\n-        b.create<arith::SelectOp>(mask, input, neutral).getResult());\n+\n+  absl::Span<const int64_t> unpadded_tile_sizes =\n+      tiled_hlo_reduce.operand(0)->tile_sizes();\n+  llvm::SmallVector<int64_t> mask_dim_bounds;\n+  mask_dim_bounds.reserve(unpadded_tile_sizes.size());\n+  for (auto [idx, dim_size] : llvm::enumerate(unpadded_tile_sizes)) {\n+    if (absl::c_contains(hlo_reduce.dimensions(), idx)) {\n+      // We only need to mask the reduction dimensions.\n+      mask_dim_bounds.push_back(dim_size);\n+    } else {\n+      mask_dim_bounds.push_back(input.getType().getDimSize(idx));\n+    }\n   }\n+  mlir::Value neutral_value =\n+      mlir::tensor::ExtractOp::create(b, values[tiled_hlo_reduce.operand(1)]);\n+  // Use createOrFold as the mask may be be reduntant, in which case it will be\n+  // folded away.\n+  input = mlir::cast<TensorValue>(\n+      b.createOrFold<xtile::MaskOp>(input, mask_dim_bounds, neutral_value));\n \n   Value init_value = values[tiled_hlo_reduce.operand(1)];\n \n   stablehlo::ReduceOp reduction =\n-      b.create<stablehlo::ReduceOp>(input, init_value, reduction_dimension);\n+      b.create<stablehlo::ReduceOp>(input, init_value, hlo_reduce.dimensions());\n   {\n     TF_ASSIGN_OR_RETURN(Type result_ty,\n                         TritonType(b, hlo_reduce.shape().element_type()));\n@@ -1627,7 +1623,14 @@ absl::Status IsTritonSupportedFusion(const HloFusionInstruction& fusion,\n             absl::StrCat(\"Pad is not supported: \", hlo->ToString()));\n       }\n     }\n+\n+    if (hlo->opcode() == HloOpcode::kReduce && hlo->dimensions().size() != 1) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Reduction with only a single dimension is supported: \",\n+                       hlo->ToString()));\n+    }\n   }\n+\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "1311ad24d2b77ed8950970d27404dece73caa3d4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #include <memory>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include <gmock/gmock.h>\n@@ -47,6 +46,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n #include \"xla/service/pattern_matcher.h\"\n@@ -55,7 +55,6 @@ limitations under the License.\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/xla.pb.h\""
        },
        {
            "sha": "5a609c34edf61a291ccef22632a6313343d4f5b3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,12 +17,12 @@ limitations under the License.\n #include <memory>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/log/log.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n@@ -43,13 +43,14 @@ limitations under the License.\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/service/pattern_matcher.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/xla.pb.h\"\n@@ -61,7 +62,6 @@ namespace gpu {\n namespace {\n \n namespace m = ::xla::match;\n-using tsl::testing::StatusIs;\n \n class TritonTest : public GpuCodegenTest {\n  public:\n@@ -1566,7 +1566,9 @@ ENTRY e {\n   EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{/*aabs=*/1e-6, /*arel=*/1e-6}));\n }\n \n-TEST_F(TritonGemmTest, DynamicSliceIsSupportedInLhsEndToEnd) {\n+// Dynamic slice is not supported by the generic Triton emitter yet and disabled\n+// in the triton gemm fusion pass.\n+TEST_F(TritonGemmTest, DISABLED_DynamicSliceIsSupportedInLhsEndToEnd) {\n   // The select is used to restrict the start index to values that make sense.\n   // If it was constant, then the dynamic-slice would be optimized to slice. It\n   // is not strictly needed, because we also support clamping the indices."
        },
        {
            "sha": "9537bcfc01d6aff4906dc66d714936062181b178",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -972,15 +972,13 @@ ENTRY main {\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_reduction_computation\",\n                                 R\"(\n \n-        CHECK:  stablehlo.iota\n-        CHECK:  stablehlo.broadcast_in_dim\n+        CHECK:  xtile.mask\n         CHECK:  stablehlo.reduce(%[[SELECT:.*]] init: %{{.*}}) across dimensions = [2] : (tensor<4x2x8x8x1xf32>, tensor<f32>) -> tensor<4x2x8x1xf32>\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n       this, xtile_module_and_hlo_module.first.get(), R\"(\n-CHECK:  tt.make_range\n-CHECK-COUNT-4:  tt.expand_dims\n+CHECK:  xtile.mask\n CHECK:  \"tt.reduce\"(%[[SELECT:.*]]) <{axis = 2 : i32}>\n   )\",\n       GetFusionInstruction(*xtile_module_and_hlo_module.second,\n@@ -1024,11 +1022,8 @@ ENTRY main {\n                                 R\"(\n ; Make sure input reduction tile is padded with a neutral value.\n CHECK:  %[[LOAD:.*]] = xtile.extract\n-CHECK:  %[[RANGE:.*]] = stablehlo.iota\n-CHECK:  %[[BROADCAST:.*]] = stablehlo.broadcast_in_dim %[[RANGE]]\n-CHECK:  %[[CMPI:.*]] = arith.cmpi slt, %[[BROADCAST]]\n-CHECK:  %[[SELECT:.*]] = arith.select %[[CMPI]], %[[LOAD]], %{{.*}}\n-CHECK: %[[REDUCE:.*]] = stablehlo.reduce(%[[SELECT]] init: %{{.*}}) across dimensions = [0] : (tensor<8x4xf32>, tensor<f32>) -> tensor<4xf32>\n+CHECK:  %[[MASKED:.*]] = xtile.mask %[[LOAD]]\n+CHECK:  %[[REDUCE:.*]] = stablehlo.reduce(%[[MASKED]] init: %{{.*}}) across dimensions = [0] : (tensor<8x4xf32>, tensor<f32>) -> tensor<4xf32>\n CHECK:   reducer(%[[ARG0:.*]]: tensor<f32>, %[[ARG1:.*]]: tensor<f32>)  {\n CHECK:   %[[MAX:.*]] = arith.maximumf %[[ARG0]], %[[ARG1]] : tensor<f32>\n CHECK:   stablehlo.return %[[MAX]] : tensor<f32>\n@@ -1039,12 +1034,8 @@ CHECK: }\n       this, xtile_module_and_hlo_module.first.get(), R\"(\n ; Make sure input reduction tile is padded with a neutral value.\n CHECK:  %[[LOAD:.*]] = xtile.extract\n-CHECK:  %[[RANGE:.*]] = tt.make_range\n-CHECK:  %[[EXPAND:.*]] = tt.expand_dims %[[RANGE]]\n-CHECK:  %[[BROADCAST:.*]] = tt.broadcast %[[EXPAND]]\n-CHECK:  %[[CMPI:.*]] = arith.cmpi slt, %[[BROADCAST]]\n-CHECK:  %[[SELECT:.*]] = arith.select %[[CMPI]], %[[LOAD]]\n-CHECK:  \"tt.reduce\"(%[[SELECT]]) <{axis = 0 : i32}>\n+CHECK:  %[[MASKED:.*]] = xtile.mask %[[LOAD]]\n+CHECK:  \"tt.reduce\"(%[[MASKED]]) <{axis = 0 : i32}>\n CHECK:  ^bb0(%[[ARG2:.*]]: f32, %[[ARG3:.*]]: f32):\n CHECK:    %[[MAXIMUM:.*]] = arith.maximumf %[[ARG2]], %[[ARG3]] : f32\n CHECK:    tt.reduce.return %[[MAXIMUM]] : f32"
        },
        {
            "sha": "27ecbd0cfea4326cb45e2a7b78573c3d3130ea20",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_int4_device_test.cc",
            "status": "modified",
            "additions": 506,
            "deletions": 805,
            "changes": 1311,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -19,9 +19,6 @@ limitations under the License.\n #include <vector>\n \n #include <gtest/gtest.h>\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_replace.h\"\n #include \"absl/strings/str_split.h\"\n@@ -32,11 +29,9 @@ limitations under the License.\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n-#include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n@@ -58,98 +53,9 @@ class TritonTest : public GpuCodegenTest {\n     debug_options.set_xla_gpu_gemm_rewrite_size_threshold(0);\n     debug_options\n         .set_xla_gpu_experimental_enable_subchannel_dequantisation_fusion(true);\n-    // TODO(b/393299275): remove this once flag is on by default and test is\n-    // updated.\n-    // Note that we clear\n-    // xla_gpu_unsupported_generic_triton_emitter_opts here to disable\n-    // nest gemm fusion pass as test will run the pass manually.\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n     return debug_options;\n   }\n \n-  ::testing::AssertionResult RunAndCompare(absl::string_view hlo_text,\n-                                           ErrorSpec error_spec) {\n-    auto module_or = GetOptimizedModule(hlo_text);\n-    if (!module_or.ok()) {\n-      return ::testing::AssertionFailure() << module_or.status().message();\n-    }\n-    return NestFusionsRunAndCompare(std::move(*module_or), error_spec);\n-  }\n-\n-  ::testing::AssertionResult RunAndCompare(std::unique_ptr<HloModule> module,\n-                                           ErrorSpec error_spec) {\n-    auto module_or = GetOptimizedModule(std::move(module));\n-    if (!module_or.ok()) {\n-      return ::testing::AssertionFailure() << module_or.status().message();\n-    }\n-    return NestFusionsRunAndCompare(std::move(*module_or), error_spec);\n-  }\n-\n-  ::testing::AssertionResult RunAndCompareNoHloPasses(\n-      absl::string_view hlo_text, ErrorSpec error_spec) {\n-    auto module_or = ParseAndReturnVerifiedModule(hlo_text);\n-    if (!module_or.ok()) {\n-      return ::testing::AssertionFailure() << module_or.status().message();\n-    }\n-    return NestFusionsRunAndCompare(std::move(*module_or), error_spec);\n-  }\n-\n-  ::testing::AssertionResult NestFusionsRunAndCompare(\n-      std::unique_ptr<HloModule> module, ErrorSpec error_spec) {\n-    if (absl::Status status = MaybeAddTritonGemmConfig(module.get());\n-        !status.ok()) {\n-      return ::testing::AssertionFailure() << status.message();\n-    }\n-    // NestGemmFusion pass is controlled by\n-    // xla_gpu_unsupported_generic_triton_emitter_opts flag, set it now.\n-    auto* emitter_opts =\n-        module->mutable_config()\n-            .mutable_debug_options()\n-            .mutable_xla_gpu_unsupported_generic_triton_emitter_features();\n-    emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n-    emitter_opts->Add(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    emitter_opts->Add(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n-    absl::StatusOr<bool> nested_or =\n-        NestGemmFusion(device_desc(), &symbolic_expr_context_)\n-            .Run(module.get());\n-    if (!nested_or.ok()) {\n-      return ::testing::AssertionFailure() << nested_or.status().message();\n-    }\n-    EXPECT_TRUE(nested_or.value());\n-    return GpuCodegenTest::RunAndCompareNoHloPasses(std::move(module),\n-                                                    error_spec);\n-  }\n-\n-  absl::Status MaybeAddTritonGemmConfig(HloModule* module) {\n-    auto instructions = module->entry_computation()->instructions();\n-    auto it = absl::c_find_if(instructions, [](auto instruction) {\n-      return instruction->opcode() == HloOpcode::kFusion;\n-    });\n-    if (it == instructions.end()) {\n-      return absl::InternalError(\"No fusion in entry computation.\");\n-    }\n-    HloInstruction* fusion = *it;\n-    TF_ASSIGN_OR_RETURN(auto gpu_config,\n-                        fusion->backend_config<GpuBackendConfig>());\n-    FusionBackendConfig* backend_config =\n-        gpu_config.mutable_fusion_backend_config();\n-    if (backend_config->has_triton_gemm_config()) {\n-      return absl::OkStatus();\n-    }\n-    auto* triton_gemm_key = backend_config->mutable_triton_gemm_config();\n-    triton_gemm_key->set_block_m(64);\n-    triton_gemm_key->set_block_k(64);\n-    triton_gemm_key->set_block_n(64);\n-    triton_gemm_key->set_split_k(1);\n-    triton_gemm_key->set_num_stages(1);\n-    triton_gemm_key->set_num_warps(2);\n-    triton_gemm_key->set_num_ctas(1);\n-    return fusion->set_backend_config(gpu_config);\n-  }\n-\n  protected:\n   const stream_executor::DeviceDescription& device_desc() {\n     return backend().default_stream_executor()->GetDeviceDescription();\n@@ -159,70 +65,63 @@ class TritonTest : public GpuCodegenTest {\n };\n \n // The following tests are for the channel and subchannel dequantization\n-// fusions. We run the fused version to avoid the hlo passes and prove that\n-// emitters work correctly and unfused version with the goal to fail if an hlo\n+// fusions. We run the fused version to avoid the HLO passes and prove that\n+// emitters work correctly and unfused version with the goal to fail if an HLO\n // rewrite broke the dequantization logic.\n // For the subchannel dequantization there are two cases:\n // 1. The case where we do:\n //   broadcast -> multiply -> bitcast -> dot.\n // 2. The case where we do:\n //   broadcast -> reshape -> multiply -> dot.\n-// On top of that there could be an additional bitcast between the parameter and\n-// the broadcast.\n TEST_F(TritonTest, FuseChannelDequantizationFused) {\n-  // This test is a Channel Dequantization fusion.\n-  // We run the fused version to avoid the hlo passes.\n-  // The case where we do:\n-  // param(1) -> bitcast -> broadcast -> multiply -> bitcast -> dot.\n+  // This test is a channel dequantization fusion of the form:\n+  //   param(1) -> bitcast -> broadcast -> multiply -> bitcast -> dot.\n+  // In a nested fusion, the parameter bitcast can be hoisted out of the fusion,\n+  // and is therefore not materialized in the HLO.\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule FuseChannelDequantizationFused\n-\n-    fusion {\n-      w.s4 = s4[32,128,256]{2,1,0:E(4)} parameter(0)\n-      w.s8 = s8[32,128,256] convert(w.s4)\n-      w.b16 = bf16[32,128,256] convert(w.s8)\n-\n-      s = bf16[32,1,256] parameter(1)\n-      s.bitcast = bf16[32,256] bitcast(s)\n-      s.broadcast = bf16[32,128,256] broadcast(s.bitcast), dimensions={0,2}\n-      w.scaled = bf16[32,128,256] multiply(w.b16, s.broadcast)\n-      w.scaled.bitcast = bf16[32,2,64,256] bitcast(w.scaled)\n-\n-      a = bf16[1,32,128,2,128] parameter(2)\n-      a.bitcast = bf16[32,128,256] bitcast(bf16[1,32,128,2,128] a)\n-      a.bitcast.2 = bf16[32,2,64,256] bitcast(a.bitcast)\n-      dot = f32[2,32,256,256] dot(w.scaled.bitcast, a.bitcast.2),\n-        lhs_batch_dims={1,0}, lhs_contracting_dims={2},\n-        rhs_batch_dims={1,0}, rhs_contracting_dims={2}\n-      ROOT bitcast = f32[2,32,256,2,1,128] bitcast(f32[2,32,256,256] dot)\n-    }\n+HloModule FuseChannelDequantizationFused\n+\n+lhs {\n+  parameter_0 = s4[32,2,64,256]{3,2,1,0:E(4)} parameter(0)\n+  w.s8 = s8[32,2,64,256]{3,2,1,0} convert(parameter_0)\n+  w.b16 = bf16[32,2,64,256]{3,2,1,0} convert(w.s8)\n+  parameter_1 = bf16[32,256]{1,0} parameter(1)\n+  s.broadcast = bf16[32,2,64,256]{3,2,1,0} broadcast(parameter_1), dimensions={0,3}\n+  ROOT w.scaled = bf16[32,2,64,256]{3,2,1,0} multiply(w.b16, s.broadcast)\n+}\n+\n+rhs {\n+  ROOT parameter_0 = bf16[32,2,64,256]{3,2,1,0} parameter(0)\n+}\n+\n+fusion {\n+  w.s4 = s4[32,2,64,256]{3,2,1,0:E(4)} parameter(0)\n+  s = bf16[32,256]{1,0} parameter(1)\n+  lhs = bf16[32,2,64,256]{3,2,1,0} fusion(w.s4, s), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\",\"1\",\"64\",\"128\"]}]}}}\n+  a = bf16[32,2,64,256]{3,2,1,0} parameter(2)\n+  rhs = bf16[32,2,64,256]{3,2,1,0} fusion(a), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\",\"1\",\"64\",\"128\"]}]}}}\n+  ROOT dot = f32[2,32,256,256]{3,2,1,0} dot(lhs, rhs),\n+    lhs_batch_dims={1,0}, lhs_contracting_dims={2},\n+    rhs_batch_dims={1,0}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY entry_computation {\n+  w.s4 = s4[32,2,64,256]{3,2,1,0:E(4)} parameter(0)\n+  s.bf16 = bf16[32,256]{1,0} parameter(1)\n+  a.bf16 = bf16[32,2,64,256]{3,2,1,0} parameter(2)                                                                                                                                                                                              bitcast = bf16[32,2,64,256]{3,2,1,0} bitcast(a.bf16)\n+  ROOT fusion = f32[2,32,256,256]{3,2,1,0} fusion(w.s4, s.bf16, a.bf16),\n+    kind=kCustom, calls=fusion, backend_config={\"fusion_backend_config\":\n+      {\"kind\":\"__triton_nested_gemm_fusion\",\n+       \"block_level_fusion_config\":{\n+        \"num_warps\":\"8\",\"output_tiles\":[{\"sizes\":[\"1\",\"1\",\"128\",\"128\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n \n-    ENTRY entry_computation {\n-      w.s4 = s4[32,128,256]{2,1,0:E(4)} parameter(0)\n-      s.bf16 = bf16[32,1,256] parameter(1)\n-      a.bf16 = bf16[1,32,128,2,128] parameter(2)\n-      ROOT fusion = f32[2,32,256,2,1,128] fusion(w.s4, s.bf16, a.bf16),\n-          kind=kCustom,\n-          calls=fusion,\n-          backend_config={\n-            \"operation_queue_id\":\"0\",\n-            \"wait_on_operation_queues\":[],\n-            \"fusion_backend_config\":{\n-              \"kind\":\"__triton_gemm\",\n-              \"triton_gemm_config\":{\n-                \"block_m\":\"128\",\n-                \"block_n\":\"128\",\n-                \"block_k\":\"64\",\n-                \"split_k\":\"2\",\n-                \"num_stages\":\"1\",\n-                \"num_warps\":\"8\",\n-                \"num_ctas\":\"1\"\n-              }\n-            },\n-            \"force_earliest_schedule\":false\n-          }\n-    }\n-  )\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n@@ -256,14 +155,14 @@ TEST_F(TritonTest, FuseSubchannelDequantizationWithTranspose) {\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n-    CHECK:    %[[bitcast:.*]] = bf16[2,8,64]{2,1,0} bitcast({{.*}})\n-    CHECK:    %[[transpose:.*]] = bf16[2,64,8]{2,1,0} transpose(%[[bitcast]]), dimensions={0,2,1}\n-    CHECK:    %[[broadcast:.*]] = bf16[2,64,8,256]{3,2,1,0} broadcast(%[[transpose]]), dimensions={0,1,2}\n-    CHECK:    %[[multiply:.*]] = bf16[2,64,8,256]{3,2,1,0} multiply({{.*}}, %[[broadcast]])\n+    CHECK:    %[[transpose:.*]] = bf16[2,64,8]{2,1,0} transpose(\n+    CHECK:    %[[broadcast:.*]] = {{.*}} broadcast(%[[transpose]])\n+    CHECK:    multiply({{.*}}, %[[broadcast]])\n+    CHECK:    ENTRY\n+    CHECK:    __triton_nested_gemm_fusion\n   )\"));\n-  EXPECT_TRUE(*RunFileCheck(module->ToString(), \"CHECK: __triton_gemm\"));\n \n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n       std::move(module), ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n }\n \n@@ -296,11 +195,18 @@ TEST_F(TritonTest, FuseSubchannelDequantization) {\n     }\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(*RunFileCheck(module->ToString(), \"CHECK: __triton_gemm\"));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n+  EXPECT_TRUE(\n+      *RunFileCheck(module->ToString(), \"CHECK: __triton_nested_gemm_fusion\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n       std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n+// Dump trick:\n+// TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n+// HloPrintOptions options = HloPrintOptions::ShortParsable();\n+// options.set_print_backend_config(true);\n+// std::cout << \"Dumping module: \" << module->ToString(options) << std::endl;\n+\n TEST_F(TritonTest, FuseChannelDequantization) {\n   // This test is a Channel Dequantization fusion.\n   // We run the non-fused version with the goal to fail if an hlo rewrite broke\n@@ -329,8 +235,9 @@ TEST_F(TritonTest, FuseChannelDequantization) {\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(*RunFileCheck(module->ToString(), \"CHECK: __triton_gemm\"));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n+  EXPECT_TRUE(\n+      *RunFileCheck(module->ToString(), \"CHECK: __triton_nested_gemm_fusion\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n       std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n@@ -340,166 +247,57 @@ TEST_F(TritonTest, FuseSubchannelDequantizationFused) {\n   // The case where we do:\n   // param -> bitcast -> broadcast -> multiply -> bitcast -> dot.\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule FuseSubchannelDequantizationFused\n-\n-    fusion {\n-      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n-      w.s8 = s8[2,2048,32] convert(w.s4)\n-      w.s8.bitcast = s8[2,8,256,32] bitcast(w.s8)\n-      w.bf16 = bf16[2,8,256,32] convert(w.s8.bitcast)\n-\n-      s.bf16 = bf16[2,8,1,32] parameter(1)\n-      s.bf16.bitcast = bf16[2,8,32] bitcast(s.bf16)\n-      s.bf16.broadcast = bf16[2,8,256,32] broadcast(s.bf16.bitcast), dimensions={0,1,3}\n-      w = bf16[2,8,256,32] multiply(w.bf16, s.bf16.broadcast)\n-      w.bitcast = bf16[2,2048,32] bitcast(w)\n-\n-      a = bf16[2,2,1,2048] parameter(2)\n-      a.bitcast = bf16[2,2,2048] bitcast(a)\n-      ROOT dot = f32[2,32,2] dot(w.bitcast, a.bitcast),\n-          lhs_batch_dims={0}, lhs_contracting_dims={1},\n-          rhs_batch_dims={1}, rhs_contracting_dims={2}\n-    } // fusion\n-\n-    ENTRY main {\n-      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n-      s.bf16 = bf16[2,8,1,32] parameter(1)\n-      a.bf16 = bf16[2,2,1,2048] parameter(2)\n-      ROOT fusion = f32[2,32,2] fusion(w.s4, s.bf16, a.bf16),\n-        kind=kCustom,\n-        calls=fusion,\n-        backend_config={\n-          \"operation_queue_id\":\"0\",\n-          \"wait_on_operation_queues\":[],\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\",\n-            \"triton_gemm_config\":{\n-              \"block_m\":16,\n-              \"block_n\":16,\n-              \"block_k\":256,\n-              \"split_k\":1,\n-              \"num_stages\":1,\n-              \"num_warps\":2,\n-              \"num_ctas\":1\n-            }\n-          },\n-          \"force_earliest_schedule\":false\n-        }\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n-}\n-\n-TEST_F(TritonTest, FuseSubchannelDequantizationFusedWithSmallBlockKSize) {\n-  // This test is a Subchannel Dequantization fusion.\n-  // We run the fused version to avoid the hlo passes.\n-  // The case where we do:\n-  // param -> bitcast -> broadcast -> multiply -> bitcast -> dot.\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule FuseSubchannelDequantizationFusedWithSmallBlockKSize\n-\n-    fusion {\n-      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n-      w.s8 = s8[2,2048,32] convert(w.s4)\n-      w.s8.bitcast = s8[2,8,256,32] bitcast(w.s8)\n-      w.bf16 = bf16[2,8,256,32] convert(w.s8.bitcast)\n-\n-      s.bf16 = bf16[2,8,1,32] parameter(1)\n-      s.bf16.bitcast = bf16[2,8,32] bitcast(s.bf16)\n-      s.bf16.broadcast = bf16[2,8,256,32] broadcast(s.bf16.bitcast), dimensions={0,1,3}\n-      w = bf16[2,8,256,32] multiply(w.bf16, s.bf16.broadcast)\n-      w.bitcast = bf16[2,2048,32] bitcast(w)\n-\n-      a = bf16[2,2,1,2048] parameter(2)\n-      a.bitcast = bf16[2,2,2048] bitcast(a)\n-      ROOT dot = f32[2,32,2] dot(w.bitcast, a.bitcast),\n-          lhs_batch_dims={0}, lhs_contracting_dims={1},\n-          rhs_batch_dims={1}, rhs_contracting_dims={2}\n-    } // fusion\n-\n-    ENTRY main {\n-      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n-      s.bf16 = bf16[2,8,1,32] parameter(1)\n-      a.bf16 = bf16[2,2,1,2048] parameter(2)\n-      ROOT fusion = f32[2,32,2] fusion(w.s4, s.bf16, a.bf16),\n-        kind=kCustom,\n-        calls=fusion,\n-        backend_config={\n-          \"operation_queue_id\":\"0\",\n-          \"wait_on_operation_queues\":[],\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\",\n-            \"triton_gemm_config\":{\n-              \"block_m\":16,\n-              \"block_n\":16,\n-              \"block_k\":128,\n-              \"split_k\":1,\n-              \"num_stages\":1,\n-              \"num_warps\":2,\n-              \"num_ctas\":1\n-            }\n-          },\n-          \"force_earliest_schedule\":false\n-        }\n-    }\n-  )\";\n+HloModule FuseSubchannelDequantizationFused\n+\n+lhs {\n+  w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n+  w.s8 = s8[2,2048,32] convert(w.s4)\n+  w.s8.bitcast = s8[2,8,256,32] bitcast(w.s8)\n+  w.bf16 = bf16[2,8,256,32] convert(w.s8.bitcast)\n+\n+  s.bf16 = bf16[2,8,1,32] parameter(1)\n+  s.bf16.bitcast = bf16[2,8,32] bitcast(s.bf16)\n+  s.bf16.broadcast = bf16[2,8,256,32] broadcast(s.bf16.bitcast), dimensions={0,1,3}\n+  w = bf16[2,8,256,32] multiply(w.bf16, s.bf16.broadcast)\n+  ROOT w.bitcast = bf16[2,2048,32] bitcast(w)\n+}\n+\n+rhs {\n+  a.bf16 = bf16[2,2,1,2048] parameter(0)\n+  ROOT a.bitcast = bf16[2,2,2048] bitcast(a.bf16)\n+}\n+\n+fusion {\n+  w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n+  s.bf16 = bf16[2,8,1,32] parameter(1)\n+  w.bitcast = bf16[2,2048,32] fusion(w.s4, s.bf16), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"128\", \"16\"]}]}}}\n+  a = bf16[2,2,1,2048] parameter(2)\n+  a.bitcast = bf16[2,2,2048] fusion(a), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\", \"1\", \"128\"]}]}}}\n+  ROOT dot = f32[2,32,2] dot(w.bitcast, a.bitcast),\n+      lhs_batch_dims={0}, lhs_contracting_dims={1},\n+      rhs_batch_dims={1}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY main {\n+  w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n+  s.bf16 = bf16[2,8,1,32] parameter(1)\n+  a.bf16 = bf16[2,2,1,2048] parameter(2)\n+  ROOT fusion = f32[2,32,2] fusion(w.s4, s.bf16, a.bf16), kind=kCustom,\n+    calls=fusion, backend_config={\n+      \"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"1\", \"16\", \"16\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n-TEST_F(TritonTest, FuseBroadcastInPrologue) {\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule FuseBroadcastInPrologue\n-\n-    ENTRY main {\n-      lhs = bf16[2,1024] parameter(0)\n-      lhs.broadcast = bf16[2,256,1024] broadcast(lhs), dimensions={0,2}\n-\n-      rhs = bf16[2,256,512] parameter(1)\n-\n-      ROOT dot = f32[2,1024,512] dot(lhs.broadcast, rhs),\n-        lhs_batch_dims={0}, lhs_contracting_dims={1},\n-        rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-  )\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n-    CHECK:    %[[broadcast:.*]] = bf16[2,256,1024]{2,1,0} broadcast\n-    CHECK:    %[[dot:.*]] = f32[2,1024,512]{2,1,0} dot\n-    CHECK:    ENTRY %main\n-  )\"));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n-      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n-}\n-\n-TEST_F(TritonTest, FuseBroadcastBitcastInPrologue) {\n-  // This test is a Subchannel Dequantization fusion.\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule FuseBroadcastBitcastInPrologue\n-\n-    ENTRY main {\n-      lhs = bf16[2,1024] parameter(0)\n-      lhs.broadcast = bf16[2,128,1024] broadcast(lhs), dimensions={0,2}\n-      lhs.bitcast = bf16[256,1024] reshape(lhs.broadcast)\n-\n-      rhs = bf16[256,512] parameter(1)\n-\n-      ROOT dot = f32[1024,512] dot(lhs.bitcast, rhs),\n-        lhs_contracting_dims={0}, rhs_contracting_dims={0}\n-    }\n-  )\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n-    CHECK:    %[[broadcast:.*]] = bf16[{{.*}}]{2,1,0} broadcast\n-    CHECK:    %[[bitcast:.*]] = bf16[{{.*}}]{1,0} bitcast\n-    CHECK:    ROOT %[[dot:.*]] = f32[{{.*}}]{1,0} dot\n-    CHECK:    ENTRY %main\n-  )\"));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n-      std::move(module), ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n-}\n-\n TEST_F(TritonTest, FuseBroadcastBitcastMultiplyInPrologue) {\n   // This test is a Subchannel Dequantization fusion.\n   constexpr absl::string_view kHloText = R\"(\n@@ -523,117 +321,62 @@ TEST_F(TritonTest, FuseBroadcastBitcastMultiplyInPrologue) {\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n-    CHECK:    %[[broadcast:.*]] = bf16[{{.*}}]{2,1,0} broadcast\n-    CHECK:    %[[bitcast:.*]] = bf16[{{.*}}]{1,0} bitcast\n-    CHECK:    %[[multiply:.*]] = [[type:.*]][{{.*}}]{1,0} multiply\n-    CHECK:    %[[dot:.*]] = f32[1024,512]{1,0} dot\n-    CHECK:    ENTRY %main\n+    // We don't need to check the bitcast, because it is hoisted.\n+    CHECK:    %[[broadcast:.*]] = {{.*}} broadcast\n+    CHECK:    %[[multiply:.*]] = {{.*}} multiply\n+    CHECK:    f32[1024,512]{1,0} dot\n+    CHECK:    ENTRY\n+    CHECK:    __triton_nested_gemm_fusion\n   )\"));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n-      std::move(module), ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n-}\n-\n-TEST_F(TritonTest, DotWithI4WeightsOnLhsWithBitcastTo3dTensor) {\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule DotWithI4WeightsOnLhsWithBitcastTo3dTensor\n-\n-    fusion {\n-      p0 = s4[256,16]{1,0:E(4)} parameter(0)\n-      p0.2 = bf16[256,16] convert(p0)\n-      p0.3 = bf16[4,64,16] bitcast(p0.2)\n-      p1 = bf16[4,32,64] parameter(1)\n-      ROOT dot = bf16[4,16,32] dot(p0.3, p1),\n-        lhs_batch_dims={0}, lhs_contracting_dims={1},\n-        rhs_batch_dims={0}, rhs_contracting_dims={2}\n-    }\n-\n-    ENTRY entry_computation {\n-      p0 = s4[256,16]{1,0:E(4)} parameter(0)\n-      p1 = bf16[4,32,64] parameter(1)\n-      ROOT dot = bf16[4,16,32] fusion(p0, p1),\n-        kind=kCustom,\n-        calls=fusion,\n-        backend_config={\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\"\n-          }\n-        }\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n-}\n-\n-TEST_F(TritonTest,\n-       DotWithI4WeightsOnLhsWithNonStandardLayoutAndMultplyInEpilogue) {\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule DotWithI4WeightsOnLhsWithNonStandardLayoutAndMultplyInEpilogue\n-\n-    fusion {\n-      p0 = s4[1,128,32]{1,2,0:E(4)} parameter(0)\n-      p0.1 = s4[1,32,128]{2,1,0:E(4)} bitcast(p0)\n-      p0.2 = bf16[1,32,128] convert(p0.1)\n-      p0.3 = bf16[1,128,32]{1,2,0} bitcast(p0.2)\n-      p1 = bf16[128,1,64] parameter(1)\n-      dot = bf16[1,32,64] dot(p0.3, p1),\n-        lhs_batch_dims={0}, lhs_contracting_dims={1},\n-        rhs_batch_dims={1}, rhs_contracting_dims={0}\n-      p2 = bf16[1,1,32]{2,0,1} parameter(2)\n-      p2.1 = bf16[1,32] bitcast(p2)\n-      p2.2 = bf16[1,32,64] broadcast(p2.1), dimensions={0,1}\n-      m = bf16[1,32,64] multiply(dot, p2.2)\n-      ROOT m.1 = bf16[1,1,32,64] bitcast(m)\n-    }\n-\n-    ENTRY entry_computation {\n-      p0 = s4[1,128,32]{1,2,0:E(4)} parameter(0)\n-      p1 = bf16[128,1,64] parameter(1)\n-      p2 = bf16[1,1,32]{2,0,1} parameter(2)\n-      ROOT gemm_fusion_dot.2 = bf16[1,1,32,64] fusion(p0, p1, p2),\n-        kind=kCustom,\n-        calls=fusion,\n-        backend_config={\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\"\n-          }\n-        }\n-    }\n-  )\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n+      std::move(module), ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n }\n \n TEST_F(TritonTest, DotWithInt4WeightsOnLhsFusedWithMultiplyByChannelScales) {\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales\n-\n-    DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales {\n-      w = s4[32,64,128] parameter(0)\n-      w.i8 = s8[32,64,128] convert(w)\n-      w.bf16 = bf16[32,64,128] convert(w.i8)\n-      scales = bf16[32,128] parameter(1)\n-      scales.broadcast = bf16[32,64,128] broadcast(scales), dimensions={0,2}\n-      weights.scaled = bf16[32,64,128] multiply(w.bf16, scales.broadcast)\n-      activations = bf16[32,64,256] parameter(2)\n-      ROOT dot = f32[32,128,256] dot(weights.scaled, activations),\n-        lhs_batch_dims={0}, lhs_contracting_dims={1},\n-        rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY main {\n-      w = s4[32,64,128] parameter(0)\n-      scales = bf16[32,128] parameter(1)\n-      p2 = bf16[32,64,256] parameter(2)\n-      ROOT dot = f32[32,128,256] fusion(w, scales, p2),\n-        kind=kCustom,\n-        calls=DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales,\n-        backend_config={\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\"\n-          }\n-        }\n-    }\n-  )\";\n+HloModule DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales\n+\n+lhs {\n+  parameter_0 = s4[32,64,128] parameter(0)\n+  parameter_1 = bf16[32,128] parameter(1)\n+  w.s8 = s8[32,64,128] convert(parameter_0)\n+  w.bf16 = bf16[32,64,128] convert(w.s8)\n+  scales.broadcast = bf16[32,64,128] broadcast(parameter_1), dimensions={0,2}\n+  ROOT weights.scaled = bf16[32,64,128] multiply(w.bf16, scales.broadcast)\n+}\n+\n+rhs {\n+  ROOT activations = bf16[32,64,256] parameter(0)\n+}\n+\n+DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales {\n+  w = s4[32,64,128] parameter(0)\n+  scales = bf16[32,128] parameter(1)\n+  lhs = bf16[32,64,128] fusion(w, scales), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  activations = bf16[32,64,256] parameter(2)\n+  rhs = bf16[32,64,256] fusion(activations), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  ROOT dot = f32[32,128,256] dot(lhs, rhs),\n+    lhs_batch_dims={0}, lhs_contracting_dims={1},\n+    rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY main {\n+  w = s4[32,64,128] parameter(0)\n+  scales = bf16[32,128] parameter(1)\n+  p2 = bf16[32,64,256] parameter(2)\n+  ROOT dot = f32[32,128,256] fusion(w, scales, p2),\n+    kind=kCustom,\n+    calls=DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n }\n@@ -662,6 +405,7 @@ TEST_F(TritonTest, FuseMultiplyInPrologue) {\n     CHECK:    %[[multiply:.*]] = [[type:.*]]{{.*}} multiply({{.*}}, {{.*}})\n     CHECK:    %[[dot:.*]] = f32[32,128,256]{2,1,0} dot\n     CHECK:    ENTRY %main\n+    CHECK:    __triton_nested_gemm_fusion\n   )\"));\n }\n \n@@ -687,6 +431,7 @@ TEST_F(TritonTest, DISABLED_FuseMultiplyInEpilogue) {\n       CHECK:  %[[dot:.*]] = bf16[4,64,32]{1,2,0} dot\n       CHECK:  %[[multiply:.*]] = [[type:.*]][4,32,64]{2,1,0} multiply\n       CHECK:  ENTRY %main\n+      CHECK:  __triton_nested_gemm_fusion\n     )\"));\n }\n \n@@ -703,8 +448,10 @@ TEST_F(TritonTest, NonstandardLayoutInt4) {\n   )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n-      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+  EXPECT_TRUE(\n+      *RunFileCheck(module->ToString(), \"CHECK: __triton_nested_gemm_fusion\"));\n+  EXPECT_TRUE(RunAndCompare(std::move(module),\n+                            ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n using ::testing::TestParamInfo;\n@@ -744,31 +491,46 @@ TEST_P(ParametrizedTritonTest, Int4WeightsOnTheLhs) {\n     GTEST_SKIP() << \"2d test ignores batch dim case.\";\n   }\n   constexpr absl::string_view kHloTextTemplate = R\"(\n-    HloModule lhs_${name}\n-\n-    lhs_${name} {\n-      w.s4 = s4[${lhs}] parameter(0)\n-      w.s8 = s8[${lhs}] convert(w.s4)\n-      w.bf16 = bf16[${lhs}] convert(w.s8)\n-      a = bf16[${rhs}] parameter(1)\n-      ROOT lhs_${name} = f32[${out}] dot(w.bf16, a),\n-        lhs_contracting_dims={${lhs_contracting_dim}},\n-        rhs_contracting_dims={${rhs_contracting_dim}}\n-    }\n+HloModule lhs_${name}\n+\n+lhs {\n+  parameter_0 = s4[${lhs}] parameter(0)\n+  w.s8 = s8[${lhs}] convert(parameter_0)\n+  ROOT w.b16 = bf16[${lhs}] convert(w.s8)\n+}\n+\n+rhs {\n+  ROOT parameter_0 = bf16[${rhs}] parameter(0)\n+}\n+\n+fusion {\n+  parameter_0 = s4[${lhs}] parameter(0)\n+\n+  lhs = bf16[${lhs}] fusion(parameter_0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  parameter_1 = bf16[${rhs}] parameter(1)\n+  rhs = bf16[${rhs}] fusion(parameter_1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  ROOT dot = f32[${out}] dot(lhs, rhs),\n+    lhs_contracting_dims={${lhs_contracting_dim}},\n+    rhs_contracting_dims={${rhs_contracting_dim}}\n+}\n+\n+ENTRY entry_computation {\n+  w = s4[${lhs}] parameter(0)\n+  a = bf16[${rhs}] parameter(1)\n+  ROOT fusion = f32[${out}] fusion(w, a),\n+    kind=kCustom, calls=fusion, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n \n-    ENTRY main {\n-      w = s4[${lhs}] parameter(0)\n-      a = bf16[${rhs}] parameter(1)\n-      ROOT gemm_fusion_dot.2 = f32[${out}] fusion(w, a),\n-        kind=kCustom,\n-        calls=lhs_${name},\n-        backend_config={\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\"\n-          }\n-        }\n-    }\n-  )\";\n   std::string hlo_text = GetParam().Format(kHloTextTemplate);\n   EXPECT_TRUE(RunAndCompareNoHloPasses(hlo_text,\n                                        ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}))\n@@ -780,31 +542,45 @@ TEST_P(ParametrizedTritonTest, Int4WeightsOnTheLhsWithBatchDim) {\n     GTEST_SKIP() << \"3d test ignores 2d case.\";\n   }\n   constexpr absl::string_view kHloTextTemplate = R\"(\n-    HloModule ${name}\n-\n-    fusion {\n-      w.s4 = s4[${lhs}] parameter(0)\n-      w.s8 = s8[${lhs}] convert(w.s4)\n-      w.bf16 = bf16[${lhs}] convert(w.s8)\n-      a = bf16[${rhs}] parameter(1)\n-      ROOT dot.0 = f32[${out}] dot(w.bf16, a),\n-        lhs_batch_dims={0}, lhs_contracting_dims={${lhs_contracting_dim}},\n-        rhs_batch_dims={0}, rhs_contracting_dims={${rhs_contracting_dim}}\n-    }\n-\n-    ENTRY gemm_fusion_dot_computation {\n-      w = s4[${lhs}] parameter(0)\n-      a = bf16[${rhs}] parameter(1)\n-      ROOT gemm_fusion_dot.2 = f32[${out}] fusion(w, a),\n-        kind=kCustom,\n-        calls=fusion,\n-        backend_config={\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\"\n-          }\n-        }\n-    }\n-  )\";\n+HloModule lhs_${name}\n+\n+lhs {\n+  parameter_0 = s4[${lhs}] parameter(0)\n+  w.s8 = s8[${lhs}] convert(parameter_0)\n+  ROOT w.b16 = bf16[${lhs}] convert(w.s8)\n+}\n+\n+rhs {\n+  ROOT parameter_0 = bf16[${rhs}] parameter(0)\n+}\n+\n+fusion {\n+  parameter_0 = s4[${lhs}] parameter(0)\n+\n+  lhs = bf16[${lhs}] fusion(parameter_0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  parameter_1 = bf16[${rhs}] parameter(1)\n+  rhs = bf16[${rhs}] fusion(parameter_1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  ROOT dot = f32[${out}] dot(lhs, rhs),\n+    lhs_batch_dims={0}, lhs_contracting_dims={${lhs_contracting_dim}},\n+    rhs_batch_dims={0}, rhs_contracting_dims={${rhs_contracting_dim}}\n+}\n+\n+ENTRY entry_computation {\n+  w = s4[${lhs}] parameter(0)\n+  a = bf16[${rhs}] parameter(1)\n+  ROOT fusion = f32[${out}] fusion(w, a),\n+    kind=kCustom, calls=fusion, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   std::string hlo_text = GetParam().Format(kHloTextTemplate);\n   EXPECT_TRUE(RunAndCompareNoHloPasses(hlo_text,\n                                        ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}))\n@@ -817,31 +593,46 @@ TEST_P(ParametrizedTritonTest, Int4WeightsOnTheRhs) {\n   }\n \n   constexpr absl::string_view kHloTextTemplate = R\"(\n-    HloModule rhs_${name}\n-\n-    rhs_${name} {\n-      a = bf16[${lhs}] parameter(0)\n-      w.s4 = s4[${rhs}] parameter(1)\n-      w.s8 = s8[${rhs}] convert(w.s4)\n-      w.bf16 = bf16[${rhs}] convert(w.s8)\n-      ROOT rhs_${name} = f32[${out}] dot(a, w.bf16),\n-        lhs_contracting_dims={${lhs_contracting_dim}},\n-        rhs_contracting_dims={${rhs_contracting_dim}}\n-    }\n+HloModule rhs_${name}\n+\n+lhs {\n+  ROOT parameter_0 = bf16[${lhs}] parameter(0)\n+}\n+\n+rhs {\n+  parameter_0 = s4[${rhs}] parameter(0)\n+  w.s8 = s8[${rhs}] convert(parameter_0)\n+  ROOT w.b16 = bf16[${rhs}] convert(w.s8)\n+}\n+\n+fusion {\n+  parameter_0 = bf16[${lhs}] parameter(0)\n+\n+  lhs = bf16[${lhs}] fusion(parameter_0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  parameter_1 = s4[${rhs}] parameter(1)\n+  rhs = bf16[${rhs}] fusion(parameter_1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  ROOT dot = f32[${out}] dot(lhs, rhs),\n+    lhs_contracting_dims={${lhs_contracting_dim}},\n+    rhs_contracting_dims={${rhs_contracting_dim}}\n+}\n+\n+ENTRY entry_computation {\n+  a = bf16[${lhs}] parameter(0)\n+  w = s4[${rhs}] parameter(1)\n+  ROOT fusion = f32[${out}] fusion(a, w),\n+    kind=kCustom, calls=fusion, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n \n-    ENTRY main {\n-      a = bf16[${lhs}] parameter(0)\n-      w = s4[${rhs}] parameter(1)\n-      ROOT rhs_${name} = f32[${out}] fusion(a, w),\n-        kind=kCustom,\n-        calls=rhs_${name},\n-        backend_config={\n-          \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\"\n-          }\n-        }\n-    }\n-  )\";\n   std::string hlo_text = GetParam().Format(kHloTextTemplate);\n   EXPECT_TRUE(RunAndCompareNoHloPasses(hlo_text,\n                                        ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}))\n@@ -887,7 +678,9 @@ TEST_F(TritonTest, NonstandardLayoutWithManyNonContractingDims) {\n   )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n+  EXPECT_TRUE(\n+      *RunFileCheck(module->ToString(), \"CHECK: __triton_nested_gemm_fusion\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n       std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-2}));\n }\n \n@@ -905,27 +698,12 @@ TEST_F(TritonTest, NonstandardLayoutWithManyNonContractingDimsReversedLayout) {\n   )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(NestFusionsRunAndCompare(\n+  EXPECT_TRUE(\n+      *RunFileCheck(module->ToString(), \"CHECK: __triton_nested_gemm_fusion\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n       std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n-TEST_F(TritonTest, NegatePlusConvertHLO) {\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule NegatePlusConvertHLO\n-\n-    ENTRY main {\n-      lhs = s4[2,32,64] parameter(0)\n-      lhs_negated = s4[2,32,64] negate(lhs)\n-      lhs_converted = bf16[2,32,64] convert(lhs_negated)\n-      rhs = bf16[2,64,16] parameter(1)\n-      ROOT dot = bf16[2,32,16] dot(lhs_converted, rhs),\n-          lhs_batch_dims={0}, lhs_contracting_dims={2},\n-          rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n-}\n-\n TEST_F(TritonTest, RejectTritonFusionForWithMinorBatchDim) {\n   constexpr absl::string_view kHloText = R\"(\n     HloModule RejectTritonFusionForWithMinorBatchDim\n@@ -940,63 +718,95 @@ TEST_F(TritonTest, RejectTritonFusionForWithMinorBatchDim) {\n     }\n   )\";\n \n-  const std::string pattern =\n-      R\"(CHECK-NOT: \"kind\":\"__triton_gemm\",\"triton_gemm_config\")\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n-  EXPECT_TRUE(*RunFileCheck(module->ToString(), pattern));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(),\n+                            \"CHECK-NOT: __triton_nested_gemm_fusion\"));\n }\n \n TEST_F(TritonTest, LHSWithMinorDimEqualTo1) {\n   // We prove that triton can handle int4 dot with non contracting dim size\n-  // equal to 1.\n+  // equal to 1 on the left-hand side.\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule LHSWithMinorDimEqualTo1\n-\n-    triton_computation {\n-      lhs = s4[2,1024,1] parameter(0)\n-      lhs_converted = bf16[2,1024,1] convert(lhs)\n-      rhs = bf16[2,64,1024] parameter(1)\n-      ROOT dot = bf16[2,1,64] dot(lhs_converted, rhs),\n-          lhs_batch_dims={0}, lhs_contracting_dims={1},\n-          rhs_batch_dims={0}, rhs_contracting_dims={2}\n-    }\n-\n-    ENTRY main {\n-      lhs = s4[2,1024,1] parameter(0)\n-      rhs = bf16[2,64,1024] parameter(1)\n-      ROOT dot = bf16[2,1,64] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n+HloModule LHSWithMinorDimEqualTo1\n+\n+lhs {\n+  lhs = s4[2,1024,1] parameter(0)\n+  ROOT lhs_converted = bf16[2,1024,1] convert(lhs)\n+}\n+\n+rhs {\n+  ROOT rhs = bf16[2,64,1024] parameter(0)\n+}\n+\n+triton_computation {\n+  p0 = s4[2,1024,1] parameter(0)\n+  p1 = bf16[2,64,1024] parameter(1)\n+  lhs = bf16[2,1024,1] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  rhs = bf16[2,64,1024] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  ROOT dot = bf16[2,1,64] dot(lhs, rhs),\n+      lhs_batch_dims={0}, lhs_contracting_dims={1},\n+      rhs_batch_dims={0}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY main {\n+  lhs = s4[2,1024,1] parameter(0)\n+  rhs = bf16[2,64,1024] parameter(1)\n+  ROOT dot = bf16[2,1,64] fusion(lhs, rhs), kind=kCustom,\n+    calls=triton_computation, backend_config={\"fusion_backend_config\":\n+      {\"kind\":\"__triton_nested_gemm_fusion\",\n+       \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"1\", \"64\",\"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n TEST_F(TritonTest, RHSWithMinorDimEqualTo1) {\n   // We prove that triton can handle int4 dot with non contracting dim size\n-  // equal to 1.\n+  // equal to 1 on the right-hand side.\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule RHSWithMinorDimEqualTo1\n-\n-    triton_computation {\n-      lhs = bf16[2,1024,64] parameter(0)\n-      rhs = s4[2,1024,1] parameter(1)\n-      rhs_converted = bf16[2,1024,1] convert(rhs)\n-      ROOT dot = bf16[2,64,1] dot(lhs, rhs_converted),\n-          lhs_batch_dims={0}, lhs_contracting_dims={1},\n-          rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY main {\n-      lhs = bf16[2,1024,64] parameter(0)\n-      rhs = s4[2,1024,1] parameter(1)\n-      ROOT dot = bf16[2,64,1] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n-\n+HloModule RHSWithMinorDimEqualTo1\n+\n+lhs {\n+  ROOT lhs = bf16[2,1024,64] parameter(0)\n+}\n+\n+rhs {\n+  rhs = s4[2,1024,1] parameter(0)\n+  ROOT rhs_converted = bf16[2,1024,1] convert(rhs)\n+}\n+\n+triton_computation {\n+  p0 = bf16[2,1024,64] parameter(0)\n+  p1 = s4[2,1024,1] parameter(1)\n+  lhs = bf16[2,1024,64] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  rhs = bf16[2,1024,1] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"1\", \"64\", \"64\"]}]}}}\n+  ROOT dot = bf16[2,64,1] dot(lhs, rhs),\n+      lhs_batch_dims={0}, lhs_contracting_dims={1},\n+      rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY main {\n+  lhs = bf16[2,1024,64] parameter(0)\n+  rhs = s4[2,1024,1] parameter(1)\n+  ROOT dot = bf16[2,64,1] fusion(lhs, rhs), kind=kCustom,\n+    calls=triton_computation, backend_config={\"fusion_backend_config\":\n+      {\"kind\":\"__triton_nested_gemm_fusion\",\n+       \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"1\", \"64\",\"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n@@ -1005,272 +815,163 @@ TEST_F(TritonTest, LHSNonMinorContractingDim) {\n   // We prove that triton can handle int4 dot with non minor\n   // lhs_contracting_dim.\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule LHSNonMinorContractingDim\n-\n-    triton_computation {\n-      lhs = s4[1024,8] parameter(0)\n-      lhs_converted = bf16[1024,8] convert(lhs)\n-      rhs = bf16[1024,4] parameter(1)\n-      ROOT dot = bf16[8,4] dot(lhs_converted, rhs),\n-          lhs_contracting_dims={0}, rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY main {\n-      lhs = s4[1024,8] parameter(0)\n-      rhs = bf16[1024,4] parameter(1)\n-      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n-\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n-}\n-\n-TEST_F(TritonTest, LHSNonMinorContractingDimWithBatchDim0) {\n-  // We prove that triton can handle int4 dot with non minor\n-  // lhs_contracting_dim.\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule LHSNonMinorContractingDimWithBatchDim0\n-\n-    triton_computation {\n-      lhs = s4[2,1024,8] parameter(0)\n-      lhs_converted = bf16[2,1024,8] convert(lhs)\n-      rhs = bf16[2,1024,4] parameter(1)\n-      ROOT dot = bf16[2,8,4] dot(lhs_converted, rhs),\n-        lhs_batch_dims={0}, lhs_contracting_dims={1},\n-        rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY main {\n-      lhs = s4[2,1024,8] parameter(0)\n-      rhs = bf16[2,1024,4] parameter(1)\n-      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n+HloModule LHSNonMinorContractingDim\n+\n+lhs {\n+  lhs = s4[1024,8] parameter(0)\n+  ROOT lhs_converted = bf16[1024,8] convert(lhs)\n+}\n+\n+rhs {\n+  ROOT rhs = bf16[1024,4] parameter(0)\n+}\n+\n+triton_computation {\n+  p0 = s4[1024,8] parameter(0)\n+  p1 = bf16[1024,4] parameter(1)\n+  lhs = bf16[1024,8] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  rhs = bf16[1024,4] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  ROOT dot = bf16[8,4] dot(lhs, rhs),\n+    lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY main {\n+  p0 = s4[1024,8] parameter(0)\n+  p1 = bf16[1024,4] parameter(1)\n+  ROOT dot = bf16[8,4] fusion(p0, p1), kind=kCustom,\n+    calls=triton_computation, backend_config={\"fusion_backend_config\":\n+      {\"kind\":\"__triton_nested_gemm_fusion\",\n+       \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"64\",\"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n TEST_F(TritonTest, LHSMinorContractingDim) {\n   // We prove that triton can handle int4 dot with minor lhs_contracting_dim.\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule LHSMinorContractingDim\n-\n-    triton_computation {\n-      lhs = s4[8,1024] parameter(0)\n-      lhs_converted = bf16[8,1024] convert(lhs)\n-      rhs = bf16[1024,4] parameter(1)\n-      ROOT dot = bf16[8,4] dot(lhs_converted, rhs),\n-        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY main {\n-      lhs = s4[8,1024] parameter(0)\n-      rhs = bf16[1024,4] parameter(1)\n-      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n-}\n-\n-TEST_F(TritonTest, ConvertPlusNegate) {\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule ConvertPlusNegate\n-\n-    triton_computation {\n-      lhs = s4[8,1024] parameter(0)\n-      lhs_converted = bf16[8,1024] convert(lhs)\n-      lhs_negated = bf16[8,1024] negate(lhs_converted)\n-      rhs = bf16[1024,4] parameter(1)\n-      ROOT dot = bf16[8,4] dot(lhs_negated, rhs),\n-        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY main {\n-      lhs = s4[8,1024] parameter(0)\n-      rhs = bf16[1024,4] parameter(1)\n-      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n-}\n-\n-TEST_F(TritonTest, LHSMinorContractingDimWithBatchDim0) {\n-  // We prove that triton can handle int4 dot with minor lhs_contracting_dim.\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule LHSMinorContractingDimWithBatchDim0\n-\n-    triton_computation {\n-      lhs = s4[2,8,1024] parameter(0)\n-      lhs_converted = bf16[2,8,1024] convert(lhs)\n-      rhs = bf16[2,1024,4] parameter(1)\n-      ROOT dot = bf16[2,8,4] dot(lhs_converted, rhs),\n-        lhs_batch_dims={0}, lhs_contracting_dims={2},\n-        rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY main {\n-      lhs = s4[2,8,1024] parameter(0)\n-      rhs = bf16[2,1024,4] parameter(1)\n-      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n+HloModule LHSMinorContractingDim\n+\n+lhs {\n+  lhs = s4[8,1024] parameter(0)\n+  ROOT lhs_converted = bf16[8,1024] convert(lhs)\n+}\n+\n+rhs {\n+  ROOT rhs = bf16[1024,4] parameter(0)\n+}\n+\n+triton_computation {\n+  p0 = s4[8,1024] parameter(0)\n+  p1 = bf16[1024,4] parameter(1)\n+  lhs = bf16[8,1024] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  rhs = bf16[1024,4] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  ROOT dot = bf16[8,4] dot(lhs, rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY main {\n+  p0 = s4[8,1024] parameter(0)\n+  p1 = bf16[1024,4] parameter(1)\n+  ROOT dot = bf16[8,4] fusion(p0, p1), kind=kCustom,\n+    calls=triton_computation, backend_config={\"fusion_backend_config\":\n+      {\"kind\":\"__triton_nested_gemm_fusion\",\n+       \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"64\",\"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n }\n \n TEST_F(TritonTest, RHSTestWithNotMinorContractingDim) {\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule RHSTestWithNotMinorContractingDim\n-\n-    triton_computation {\n-      lhs = bf16[8,1024] parameter(0)\n-      rhs = s4[1024,4] parameter(1)\n-      rhs_converted = bf16[1024,4] convert(rhs)\n-      ROOT dot = bf16[8,4] dot(lhs, rhs_converted),\n-          lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY main {\n-      lhs = bf16[8,1024] parameter(0)\n-      rhs = s4[1024,4] parameter(1)\n-      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n+HloModule RHSTestWithNotMinorContractingDim\n+\n+lhs {\n+  ROOT lhs = bf16[8,1024] parameter(0)\n+}\n+\n+rhs {\n+  rhs = s4[1024,4] parameter(0)\n+  ROOT rhs_converted = bf16[1024,4] convert(rhs)\n+}\n+\n+triton_computation {\n+  p0 = bf16[8,1024] parameter(0)\n+  p1 = s4[1024,4] parameter(1)\n+  lhs = bf16[8,1024] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  rhs = bf16[1024,4] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  ROOT dot = bf16[8,4] dot(lhs, rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY main {\n+  p0 = bf16[8,1024] parameter(0)\n+  p1 = s4[1024,4] parameter(1)\n+  ROOT dot = bf16[8,4] fusion(p0, p1), kind=kCustom,\n+    calls=triton_computation, backend_config={\"fusion_backend_config\":\n+      {\"kind\":\"__triton_nested_gemm_fusion\",\n+       \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"64\",\"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n }\n \n TEST_F(TritonTest, RHSTestWithMinorContractingDim) {\n   constexpr absl::string_view kHloText = R\"(\n-    HloModule RHSTestWithMinorContractingDim\n-\n-    triton_computation {\n-      lhs = bf16[8,1024] parameter(0)\n-      rhs = s4[4,1024] parameter(1)\n-      rhs_converted = bf16[4,1024] convert(rhs)\n-      ROOT dot = bf16[8,4] dot(lhs, rhs_converted),\n-          lhs_contracting_dims={1}, rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY main {\n-      lhs = bf16[8,1024] parameter(0)\n-      rhs = s4[4,1024] parameter(1)\n-      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n-}\n-\n-TEST_F(TritonTest, RHSTestWithMinorContractingDimWithBatchDim) {\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule RHSTestWithMinorContractingDimWithBatchDim\n-\n-    triton_computation {\n-      lhs = bf16[2,8,1024] parameter(0)\n-      rhs = s4[2,1024,4] parameter(1)\n-      rhs_converted = bf16[2,1024,4] convert(rhs)\n-      ROOT dot = bf16[2,8,4] dot(lhs, rhs_converted),\n-          lhs_batch_dims={0}, lhs_contracting_dims={2},\n-          rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY main {\n-      lhs = bf16[2,8,1024] parameter(0)\n-      rhs = s4[2,1024,4] parameter(1)\n-      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n-}\n-\n-TEST_F(TritonTest, RHSTestWithNotMinorContractingDimWithBatchDim0) {\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule RHSTestWithNotMinorContractingDimWithBatchDim0\n-\n-    triton_computation {\n-      lhs = bf16[2,8,1024] parameter(0)\n-      rhs = s4[2,4,1024] parameter(1)\n-      rhs_converted = bf16[2,4,1024] convert(rhs)\n-      ROOT dot = bf16[2,8,4] dot(lhs, rhs_converted),\n-          lhs_batch_dims={0}, lhs_contracting_dims={2},\n-          rhs_batch_dims={0}, rhs_contracting_dims={2}\n-    }\n-\n-    ENTRY main {\n-      lhs = bf16[2,8,1024] parameter(0)\n-      rhs = s4[2,4,1024] parameter(1)\n-      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n-        calls=triton_computation,\n-        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n-    }\n-  )\";\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n-}\n-\n-TEST_F(TritonTest, FusedBroadcastAddBroadcastMultiplyDotGeneratesValidTriton) {\n-  // Here we test that the Triton codegen can handle a fusion with the chain of\n-  // a broadcast, add, broadcast, multiply, and dot. First broadcast was causing\n-  // a problem in the past because it was not using a 1d tile shape. That was\n-  // necessary for the Triton kernel to be valid.\n-  constexpr absl::string_view kHloText = R\"(\n-    HloModule gemm_fusion_dot\n-\n-    %fusion  {\n-      p0 = bf16[1024,1,512]{2,1,0} parameter(0)\n-      p0_b = bf16[1,128,8,8,64]{4,3,2,1,0} bitcast(p0)\n-      p1 = bf16[1,128,8,8]{3,2,1,0} parameter(1)\n-      c0 = bf16[] constant(3.e-02)\n-      c0_b = bf16[1,128,8,8]{3,2,1,0} broadcast(c0), dimensions={}\n-      add_0 = bf16[1,128,8,8]{3,2,1,0} add(p1, c0_b)\n-      add_bitcast = bf16[128,8,8]{2,1,0} bitcast(add_0)\n-      add_broadcast = bf16[1,128,8,8,64]{4,3,2,1,0} broadcast(add_bitcast), dimensions={1,2,3}\n-      m_p0 = bf16[1,128,8,8,64]{4,3,2,1,0} multiply(p0_b, add_broadcast)\n-      p2 = bf16[8,64]{1,0} parameter(2)\n-      c1 = bf16[] constant(1)\n-      c1_broadcast = bf16[8,64]{1,0} broadcast(c1), dimensions={}\n-      add_p2 = bf16[8,64]{1,0} add(p2, c1_broadcast)\n-      add_p2_broadcast = bf16[1,128,8,8,64]{4,3,2,1,0} broadcast(add_p2),\n-          dimensions={3,4}\n-      m_m_p0 = bf16[1,128,8,8,64]{4,3,2,1,0} multiply(m_p0, add_p2_broadcast)\n-      m_m_p0_bitcast = bf16[1024,512]{1,0} bitcast(m_m_p0)\n-      p3 = bf16[64,512]{1,0} parameter(3)\n-      ROOT dot = bf16[1024,64]{1,0} dot(m_m_p0_bitcast, p3),\n-          lhs_contracting_dims={1},\n-          rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY entry_computation {\n-      p0 = bf16[1024,1,512]{2,1,0} parameter(0)\n-      p1 = bf16[1,128,8,8]{3,2,1,0} parameter(1)\n-      p2 = bf16[8,64]{1,0} parameter(2)\n-      p3 = bf16[64,512]{1,0} parameter(3)\n-      ROOT gemm_fusion_dot.1642 = bf16[1024,64]{1,0} fusion(p0, p1, p2, p3),\n-          kind=kCustom,\n-          calls=fusion\n-    }\n-  )\";\n+lhs {\n+  ROOT lhs = bf16[8,1024] parameter(0)\n+}\n+\n+rhs {\n+  rhs = s4[4,1024] parameter(0)\n+  ROOT rhs_converted = bf16[4,1024] convert(rhs)\n+}\n+\n+triton_computation {\n+  p0 = bf16[8,1024] parameter(0)\n+  p1 = s4[4,1024] parameter(1)\n+  lhs = bf16[8,1024] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  rhs = bf16[4,1024] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\", \"64\"]}]}}}\n+  ROOT dot = bf16[8,4] dot(lhs, rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY main {\n+  p0 = bf16[8,1024] parameter(0)\n+  p1 = s4[4,1024] parameter(1)\n+  ROOT dot = bf16[8,4] fusion(p0, p1), kind=kCustom,\n+    calls=triton_computation, backend_config={\"fusion_backend_config\":\n+      {\"kind\":\"__triton_nested_gemm_fusion\",\n+       \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\"output_tiles\":[{\"sizes\":[\"64\",\"64\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false,\n+        \"is_warp_specialization_allowed\":false}}}\n+})\";\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n }"
        },
        {
            "sha": "a218774db86235cc6a8889e1902095d06bd7abf3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -216,8 +216,8 @@ ENTRY e {\n       block_level_parameters,\n       R\"(\n CHECK: %[[INIT:.*]] = arith.constant dense<0.000000e+00> : tensor<f32>\n-CHECK: %[[REDUCE_INPUT:.*]] = arith.select {{.*}}\n-CHECK: %[[RES:.*]] = stablehlo.reduce(%[[REDUCE_INPUT]] init: %[[INIT]]) across dimensions = [0] : (tensor<256x16xf32>, tensor<f32>) -> tensor<16xf32>\n+CHECK: %[[MASKED_INPUT:.*]] = xtile.mask {{.*}}\n+CHECK: %[[RES:.*]] = stablehlo.reduce(%[[MASKED_INPUT]] init: %[[INIT]]) across dimensions = [0] : (tensor<256x16xf32>, tensor<f32>) -> tensor<16xf32>\n CHECK: reducer(%[[ARG_0:.*]]: tensor<f32>, %[[ARG_1:.*]]: tensor<f32>)  {\n CHECK:   %[[SUM:.*]] = arith.addf %[[ARG_0]], %[[ARG_1]] : tensor<f32>\n CHECK:   stablehlo.return %[[SUM]] : tensor<f32>"
        },
        {
            "sha": "c7940e37974f45806b04a5cdffa22e279acd42fd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n@@ -38,12 +39,12 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/triton_fusion_analysis.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/status_matchers.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {"
        },
        {
            "sha": "8af9a765b90034d0ece54a4e78536c7846be4a12",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/tma_utils_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,12 +20,12 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n@@ -37,7 +37,6 @@ using ::mlir::triton::xla::SwizzleMode;\n using ::stream_executor::gpu::TmaDescriptor;\n using ::testing::ElementsAre;\n using ::testing::HasSubstr;\n-using ::tsl::testing::StatusIs;\n \n TEST(CreateTmaDescriptorTest, Valid2DInputReturnCorrectDescriptor) {\n   mlir::MLIRContext mlir_context;"
        },
        {
            "sha": "fb75037f85f45d8048a539e2d25f5a2112834453",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -55,6 +55,7 @@ cc_library(\n         \":passes_inc_gen\",\n         \"//xla:permutation_util\",\n         \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton:dot_algorithms\",\n         \"//xla/backends/gpu/codegen/triton:emitter_helpers\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\","
        },
        {
            "sha": "a6b08d81aec6f5054a02636a8456090c9f364243",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/stablehlo_lower_to_triton.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -54,6 +54,7 @@ limitations under the License.\n #include \"xla/hlo/translate/mhlo_to_hlo/attribute_exporter.h\"\n #include \"xla/service/algorithm_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n+#include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/tensor_float_32_utils.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n "
        },
        {
            "sha": "6ccec39cc26835f982d4d54e5b4590aef78a5df7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_convert_unsupported_types.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -25,7 +25,8 @@ module {\n     // CHECK: %[[arg_3:.*]] = xtile.extract %arg3[%[[C_0]], %[[C_0]]] [1, 16] [1, 1] : memref<16x64xi8, #triton_xla.layout<[1, 0]>> -> tensor<1x16xi8>\n     %16 = arith.bitcast %extracted_tile_0 : tensor<16x1xf8E8M0FNU> to tensor<16x1xi8>\n     %17 = arith.bitcast %extracted_tile_2 : tensor<1x16xf8E8M0FNU> to tensor<1x16xi8>\n-    %18 = tt.dot_scaled %extracted_tile scale %16, %extracted_tile_1 scale %17, %cst lhs = e4m3 rhs = e4m3 {fastMath = true} : tensor<16x32xf8E4M3FN>, tensor<16x1xi8> * tensor<32x16xf8E4M3FN>, tensor<1x16xi8> -> tensor<16x16xf32>\n+    %18 = tt.trans %17 {order = array<i32: 1, 0>} : tensor<1x16xi8> -> tensor<16x1xi8>\n+    %19 = tt.dot_scaled %extracted_tile scale %16, %extracted_tile_1 scale %18, %cst lhs = e4m3 rhs = e4m3 {fastMath = true} : tensor<16x32xf8E4M3FN>, tensor<16x1xi8> * tensor<32x16xf8E4M3FN>, tensor<16x1xi8> -> tensor<16x16xf32>\n     xtile.return\n   }\n }"
        },
        {
            "sha": "89b8ea2c452d4ce7e1ce712165bd6912b722c547",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_lower_xtile.mlir",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -81,3 +81,24 @@ func.func @fold_transpose_into_ptr(\n   // CHECK: return %[[PTR]] : !tt.ptr<f64>\n   return %ptr : !tt.ptr<f64>\n }\n+\n+// -----\n+\n+// CHECK-LABEL: @mask_lowers_to_stable_hlo(%arg0: tensor<32xf64>, %arg1: f64) -> tensor<32xf64>\n+func.func @mask_lowers_to_stable_hlo(%arg0: tensor<32xf64>, %arg1: f64) -> tensor<32xf64> {\n+  // CHECK: %[[BOUND:.*]] = arith.constant dense<10> : tensor<32xi32>\n+  // CHECK: %[[IDX:.*]] = stablehlo.iota dim = 0 : tensor<32xi32>\n+  // CHECK: %[[IDX_BROADCASTED:.*]] = stablehlo.broadcast_in_dim %[[IDX]],\n+  // CHECK-SAME: dims = [0] : (tensor<32xi32>) -> tensor<32xi32>\n+  // CHECK: %[[MASK:.*]] = arith.cmpi slt, %[[IDX_BROADCASTED]], %[[BOUND]]\n+  // CHECK-SAME: : tensor<32xi32>\n+  // CHECK: %[[INIT:.*]] = tensor.from_elements %arg1 : tensor<f64>\n+  // CHECK: %[[INIT_TENSOR:.*]] = stablehlo.broadcast_in_dim %[[INIT]],\n+  // CHECK-SAME: dims = [] : (tensor<f64>) -> tensor<32xf64>\n+  // CHECK: %[[RESULT:.*]] = arith.select %[[MASK]], %arg0, %[[INIT_TENSOR]]\n+  // CHECK-SAME: : tensor<32xi1>, tensor<32xf64>\n+  %paded = xtile.mask %arg0 bounds [10], %arg1 : tensor<32xf64>\n+  // CHECK: return %[[RESULT]] : tensor<32xf64>\n+  return %paded : tensor<32xf64>\n+}\n+"
        },
        {
            "sha": "e4266d6b93abceb11fd54701e97e453805798d87",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_xtile_pass.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 1,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -45,6 +45,7 @@ limitations under the License.\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Inliner.h\"\n #include \"mlir/Transforms/InliningUtils.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n@@ -290,6 +291,50 @@ class XTileInsertToTriton\n   }\n };\n \n+class XTileMaskToTriton : public mlir::OpRewritePattern<::xla::xtile::MaskOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      ::xla::xtile::MaskOp op, mlir::PatternRewriter& rewriter) const override {\n+    llvm::SmallVector<int64_t> masked_dimensions = op.getMaskedDimensions();\n+    if (masked_dimensions.size() != 1) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"triton masking only supports masking over a single dimension\");\n+    }\n+\n+    int64_t mask_dimension = masked_dimensions.front();\n+    int64_t mask_bound = op.getBounds()[mask_dimension];\n+    int64_t masked_dim_size = op.getType().getDimSize(mask_dimension);\n+    auto iota_type =\n+        mlir::RankedTensorType::get(masked_dim_size, rewriter.getI32Type());\n+    auto range = stablehlo::IotaOp::create(rewriter, op.getLoc(), iota_type, 0);\n+    auto bcast_type = mlir::RankedTensorType::get(op.getType().getShape(),\n+                                                  iota_type.getElementType());\n+    auto bcast = stablehlo::BroadcastInDimOp::create(\n+        rewriter, op.getLoc(), bcast_type, range, {mask_dimension});\n+    auto constant = mlir::arith::ConstantOp::create(\n+        rewriter, op.getLoc(),\n+        mlir::DenseElementsAttr::get(bcast_type,\n+                                     rewriter.getI32IntegerAttr(mask_bound)));\n+    Value mask = arith::CmpIOp::create(\n+        rewriter, op.getLoc(), arith::CmpIPredicate::slt, bcast, constant);\n+\n+    auto mask_value_tensor = mlir::tensor::FromElementsOp::create(\n+        rewriter, op.getLoc(),\n+        mlir::RankedTensorType::get({}, op.getValue().getType()),\n+        op.getValue());\n+    auto neutral = stablehlo::BroadcastInDimOp::create(\n+        rewriter, op.getLoc(), op.getType(), mask_value_tensor,\n+        ArrayRef<int64_t>{});\n+\n+    rewriter.replaceOpWithNewOp<arith::SelectOp>(op, mask, op.getSource(),\n+                                                 neutral);\n+\n+    return mlir::success();\n+  }\n+};\n+\n class FoldIntoMemrefToPtr : public mlir::OpRewritePattern<MemrefToPtrOp> {\n  public:\n   using OpRewritePattern::OpRewritePattern;\n@@ -321,7 +366,7 @@ class TritonXLALowerXTilePass\n     mlir::RewritePatternSet patterns(context);\n \n     patterns.add<XTileEntryToTriton, XTileExtractToTriton, XTileInsertToTriton,\n-                 FoldIntoMemrefToPtr>(context);\n+                 XTileMaskToTriton, FoldIntoMemrefToPtr>(context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(module, std::move(patterns)))) {\n       signalPassFailure();"
        },
        {
            "sha": "76d1bd8e6817f11604a890a67c464afa85b088da",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_math_to_libdevice.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/service/gpu/target_util.h\"\n+#include \"xla/xla_data.pb.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n namespace mlir::triton::xla {"
        },
        {
            "sha": "a75900f8e41da6f3de0c3c12e7e4043e72d591b8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_squeeze_dims_pass.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/algorithm/container.h\"\n-#include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\""
        },
        {
            "sha": "536794722248c436dfedbe4437b0f11bd4dd8562",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 186,
            "deletions": 9,
            "changes": 195,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -67,11 +67,14 @@ cc_library(\n         \":all_to_all_thunk\",\n         \":annotation\",\n         \":collective_broadcast_thunk\",\n+        \":collective_permute_thunk\",\n         \":collective_thunk\",\n+        \":convolution_thunk\",\n         \":copy_thunk\",\n         \":custom_call_thunk\",\n         \":dynamic_slice_thunk\",\n         \":gpublas_lt_matmul_thunk\",\n+        \":p2p_thunk_common\",\n         \":shaped_slice\",\n         \":thunk\",\n         \":while_thunk\",\n@@ -110,6 +113,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n@@ -187,12 +191,14 @@ cc_library(\n         \":all_reduce_thunk\",\n         \":all_to_all_thunk\",\n         \":collective_broadcast_thunk\",\n+        \":collective_permute_thunk\",\n         \":collective_thunk\",\n         \":command_buffer_cmd\",\n         \":conditional_thunk\",\n         \":copy_thunk\",\n         \":cudnn_thunk\",\n         \":custom_call_thunk\",\n+        \":custom_kernel_thunk\",\n         \":dynamic_slice_thunk\",\n         \":gemm_thunk\",\n         \":gpublas_lt_matmul_thunk\",\n@@ -962,20 +968,19 @@ cc_library(\n     srcs = [\"kernel_thunk.cc\"],\n     hdrs = [\"kernel_thunk.h\"],\n     deps = [\n+        \":print_buffer_contents\",\n         \":thunk\",\n-        \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n-        \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/service/gpu/kernels:custom_kernel\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -989,7 +994,6 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n@@ -1228,6 +1232,7 @@ cc_library(\n     hdrs = [\"collective_kernel_thunk.h\"],\n     deps = [\n         \":all_reduce\",\n+        \":collective_metadata_thunk\",\n         \":collective_thunk\",\n         \":thunk\",\n         \"//xla:shape_util\",\n@@ -1237,7 +1242,6 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/service:rendezvous\",\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:stream_executor_util\",\n@@ -1248,10 +1252,8 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n-        \"//xla/stream_executor/gpu:gpu_executor_header\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n@@ -1524,6 +1526,41 @@ cc_library(\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@local_tsl//tsl/platform:casts\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"collective_permute_thunk_test\",\n+    srcs = [\"collective_permute_thunk_test.cc\"],\n+    backends = [\"gpu\"],\n+    deps = [\n+        \":collective_permute_thunk\",\n+        \":collective_thunk\",\n+        \":command_buffer_cmd\",\n+        \":command_buffer_cmd_emitter\",\n+        \":command_buffer_thunk\",\n+        \":sequential_thunk\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:verified_hlo_module\",\n+        \"//xla/service:backend\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service:executable\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/service/gpu:gpu_constants\",\n+        \"//xla/service/gpu:gpu_executable\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tests:hlo_test_base\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:casts\",\n     ],\n )\n@@ -1786,6 +1823,40 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"collective_metadata_thunk\",\n+    srcs = [\"collective_metadata_thunk.cc\"],\n+    hdrs = [\"collective_metadata_thunk.h\"],\n+    deps = [\n+        \":collective_thunk\",\n+        \":thunk\",\n+        \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/core/collectives:rank_id\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/runtime:device_id\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service:collective_ops_utils\",\n+        \"//xla/service:rendezvous\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n+        \"//xla/stream_executor/gpu:gpu_executor_header\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_protobuf//:protobuf_lite\",\n+    ],\n+)\n+\n cc_library(\n     name = \"sequential_thunk\",\n     srcs = [\"sequential_thunk.cc\"],\n@@ -2170,7 +2241,6 @@ xla_cc_test(\n         \":thunk\",\n         \":thunk_proto_cc\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status:status_matchers\",\n@@ -2289,7 +2359,6 @@ xla_test(\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -2502,6 +2571,7 @@ tf_proto_library(\n         \"//xla/service/gpu:gpu_conv_runner_proto\",\n         \"//xla/service/gpu:gpu_norm_runner_proto\",\n         \"//xla/service/gpu:launch_dimensions_proto\",\n+        \"//xla/service/gpu/kernels:custom_kernel_proto\",\n         \"//xla/stream_executor:launch_dim_proto\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt_proto\",\n         \"//xla/stream_executor/gpu:tma_metadata_proto\",\n@@ -2538,6 +2608,7 @@ cc_library(\n         \":cub_sort_thunk\",\n         \":cudnn_thunk\",\n         \":custom_call_thunk\",\n+        \":custom_kernel_thunk\",\n         \":dynamic_slice_thunk\",\n         \":fft_thunk\",\n         \":gemm_thunk\",\n@@ -2558,6 +2629,8 @@ cc_library(\n         \":while_thunk\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/stream_executor:kernel_spec\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/log:check\",\n@@ -2576,6 +2649,7 @@ xla_cc_test(\n     deps = [\n         \":conditional_thunk\",\n         \":copy_thunk\",\n+        \":custom_kernel_thunk\",\n         \":host_execute_thunk\",\n         \":host_send_recv_thunk\",\n         \":sequential_thunk\",\n@@ -2589,11 +2663,14 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:hlo_module_config\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util:safe_reinterpret_cast\",\n         \"//xla/tsl/util/proto:parse_text_proto\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n@@ -2925,6 +3002,7 @@ xla_test(\n         \":command_buffer_conversion_pass\",\n         \":command_buffer_thunk\",\n         \":conditional_thunk\",\n+        \":convolution_thunk\",\n         \":copy_thunk\",\n         \":cudnn_thunk\",\n         \":custom_call_thunk\",\n@@ -2962,19 +3040,22 @@ cc_library(\n         \"thunk_buffer_debug_filter.cc\",\n         \"thunk_buffer_debug_float_check.cc\",\n         \"thunk_buffer_debug_pass.cc\",\n+        \"thunk_buffer_debug_saver_inserter.cc\",\n     ],\n     hdrs = [\n         \"thunk_buffer_debug_checksum.h\",\n         \"thunk_buffer_debug_filter.h\",\n         \"thunk_buffer_debug_float_check.h\",\n         \"thunk_buffer_debug_pass.h\",\n+        \"thunk_buffer_debug_saver_inserter.h\",\n     ],\n     deps = [\n         \":buffer_debug_log_entry_metadata_store\",\n         \":buffer_debug_log_structs\",\n         \":buffers_checksum_thunk\",\n         \":buffers_float_check_thunk\",\n         \":custom_call_thunk\",\n+        \":runtime_intrinsics\",\n         \":sequential_thunk\",\n         \":shaped_slice\",\n         \":thunk\",\n@@ -3004,6 +3085,7 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_googlesource_code_re2//:re2\",\n     ],\n@@ -3017,19 +3099,23 @@ xla_cc_test(\n         \":buffers_float_check_thunk\",\n         \":conditional_thunk\",\n         \":custom_call_thunk\",\n+        \":runtime_intrinsics\",\n         \":sequential_thunk\",\n         \":thunk\",\n         \":thunk_buffer_debug_pass\",\n         \":thunk_id\",\n         \":thunk_pass_pipeline\",\n         \":while_thunk\",\n         \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n@@ -3272,6 +3358,97 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"print_buffer_contents\",\n+    srcs = [\"print_buffer_contents.cc\"],\n+    hdrs = [\"print_buffer_contents.h\"],\n+    deps = [\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel_args\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"print_buffer_contents_test\",\n+    srcs = [\"print_buffer_contents_test.cc\"],\n+    backends = [\"gpu\"],\n+    deps = [\n+        \":print_buffer_contents\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel_args\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"@com_google_absl//absl/log:scoped_mock_log\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"custom_kernel_thunk\",\n+    srcs = [\"custom_kernel_thunk.cc\"],\n+    hdrs = [\"custom_kernel_thunk.h\"],\n+    deps = [\n+        \":print_buffer_contents\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla/codegen/emitters:kernel_arguments\",\n+        \"//xla/runtime:buffer_use\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/kernels:custom_kernel\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"custom_kernel_thunk_test\",\n+    srcs = [\"custom_kernel_thunk_test.cc\"],\n+    deps = [\n+        \":custom_kernel_thunk\",\n+        \":thunk\",\n+        \"//xla:shape_util\",\n+        \"//xla/codegen/emitters:kernel_arguments\",\n+        \"//xla/runtime:buffer_use\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu/kernels:custom_kernel\",\n+        \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"buffer_debug_log_entry_metadata_store_test\",\n     srcs = [\"buffer_debug_log_entry_metadata_store_test.cc\"],"
        },
        {
            "sha": "27aceb205ef899b4c7e208adf026ee94b3e10f8a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -54,7 +54,6 @@ limitations under the License.\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tests/literal_test_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/types.h\"\n@@ -66,7 +65,6 @@ namespace {\n \n using ::stream_executor::gpu::AllReduceStrategy;\n using ::testing::HasSubstr;\n-using ::tsl::testing::StatusIs;\n \n se::StreamExecutor* GetGpuExecutor(int64_t device_ordinal) {\n   auto* platform =\n@@ -153,14 +151,15 @@ class AllReduceKernelTest : public ::testing::Test,\n     std::vector<se::DeviceMemoryBase> metadata_buffers;\n     // One for signal and one for input parameters.\n     constexpr int kNumPeerParameters = 2;\n-    size_t param_to_peers_size =\n-        sizeof(uint64_t) * kNumPeerParameters * num_ranks;\n-    std::vector<uint64_t> param_to_peers_ptrs;\n-    for (const auto& local_input_buffer : local_input_buffers) {\n-      param_to_peers_ptrs.push_back((uint64_t)local_input_buffer.opaque());\n+    size_t param_to_peers_size = sizeof(void*) * kNumPeerParameters * num_ranks;\n+    std::vector<void*> param_to_peers_ptrs;\n+    for (const stream_executor::DeviceMemoryBase& local_input_buffer :\n+         local_input_buffers) {\n+      param_to_peers_ptrs.push_back(local_input_buffer.opaque());\n     }\n-    for (const auto& signal_flags_buffer : signal_flags_buffers) {\n-      param_to_peers_ptrs.push_back((uint64_t)signal_flags_buffer.opaque());\n+    for (const stream_executor::DeviceMemoryBase& signal_flags_buffer :\n+         signal_flags_buffers) {\n+      param_to_peers_ptrs.push_back(signal_flags_buffer.opaque());\n     }\n \n     for (int i = 0; i < num_ranks; ++i) {\n@@ -174,9 +173,9 @@ class AllReduceKernelTest : public ::testing::Test,\n         TF_ASSIGN_OR_RETURN(\n             void* mapped_memory,\n             multicast_memory->MapMemory(allocated_buffers[i], gpu_executor));\n-        metadata.multicast_buffer_ptr = (uint64_t)mapped_memory;\n+        metadata.multicast_buffer_ptr = mapped_memory;\n       } else {\n-        metadata.multicast_buffer_ptr = 0;\n+        metadata.multicast_buffer_ptr = nullptr;\n       }\n \n       // First map from parameter to peer ptrs and then metadata.\n@@ -187,7 +186,7 @@ class AllReduceKernelTest : public ::testing::Test,\n           metadata_buffers[i].GetByteSlice(sizeof(CollectiveKernelMetadata),\n                                            param_to_peers_size);\n       metadata.param_to_peers =\n-          reinterpret_cast<uint64_t*>(param_to_peers_ptrs_buffer.opaque());\n+          reinterpret_cast<void**>(param_to_peers_ptrs_buffer.opaque());\n \n       TF_RETURN_IF_ERROR(streams[i]->Memcpy(&metadata_buffers[i], &metadata,\n                                             sizeof(CollectiveKernelMetadata)));"
        },
        {
            "sha": "22cfba07d0de9affb6049723840b7ae1ecc7494a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 25,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -16,16 +16,17 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n \n #include <cstdint>\n+#include <memory>\n #include <optional>\n #include <utility>\n #include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n+#include \"xla/backends/gpu/runtime/collective_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n@@ -64,14 +65,14 @@ absl::Status CheckImplementableInst(const HloInstruction* inst,\n }\n \n template <typename HloInstType>\n-CollectiveOpGroupMode GetGroupModeInst(HloInstType* inst) {\n+CollectiveOpGroupMode GetGroupModeInst(const HloInstType* inst) {\n   return GetAllReduceConfigInst(inst).config.group_mode;\n }\n \n }  // namespace\n \n-template <typename HloInstType>\n-AllReduceConfig GetAllReduceConfigInst(HloInstType* inst) {\n+AllReduceConfig GetAllReduceConfigInst(\n+    const HloAllReduceInstructionBase* inst) {\n   std::optional<ReductionKind> reduction_kind =\n       MatchReductionComputation(inst->called_computations().front());\n   CHECK(reduction_kind.has_value());\n@@ -118,25 +119,15 @@ AllReduceReduceScatterThunkBase::AllReduceReduceScatterThunkBase(\n   CHECK_EQ(config_.config.operand_count, buffers_.size());\n }\n \n-AllReduceStartThunk::AllReduceStartThunk(ThunkInfo thunk_info,\n-                                         const HloAllReduceInstruction* inst,\n-                                         std::vector<Buffer> buffers,\n-                                         bool p2p_memcpy_enabled)\n+AllReduceStartThunk::AllReduceStartThunk(\n+    ThunkInfo thunk_info, const HloAllReduceInstruction* inst,\n+    std::vector<Buffer> buffers,\n+    std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n+    bool p2p_memcpy_enabled)\n     : AllReduceReduceScatterThunkBase(\n           Thunk::kAllReduceStart, thunk_info, GetAllReduceConfigInst(inst),\n           std::move(buffers), IsGPUSyncCollective(*inst)),\n-      collective_kernel_thunk_{\n-          thunk_info,\n-          config_.config,\n-          config_.reduction_kind,\n-          IsAsync(),\n-          buffers_,\n-          /*is_collective_kernel_enabled=*/\n-          inst->GetModule()\n-              ->config()\n-              .debug_options()\n-              .xla_gpu_unsupported_use_all_reduce_one_shot_kernel(),\n-      } {}\n+      collective_kernel_thunk_(std::move(collective_kernel_thunk)) {}\n \n absl::Status AllReduceStartThunk::CheckImplementable(\n     const HloAllReduceInstruction* inst, int64_t replica_count,\n@@ -154,7 +145,7 @@ CollectiveOpGroupMode AllReduceStartThunk::GetGroupMode(\n absl::Status AllReduceStartThunk::Prepare(\n     const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n   TF_RETURN_IF_ERROR(CollectiveThunk::Prepare(params, resource_requests));\n-  return collective_kernel_thunk_.Prepare(params, resource_requests);\n+  return collective_kernel_thunk_->Prepare(params, resource_requests);\n }\n \n absl::Status AllReduceStartThunk::Initialize(const InitializeParams& params) {\n@@ -163,10 +154,10 @@ absl::Status AllReduceStartThunk::Initialize(const InitializeParams& params) {\n       GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, config()));\n   TF_ASSIGN_OR_RETURN(bool use_collective_kernel,\n-                      collective_kernel_thunk_.IsSupported(\n+                      collective_kernel_thunk_->IsSupported(\n                           clique_key, params.collective_cliques));\n   if (use_collective_kernel) {\n-    TF_RETURN_IF_ERROR(collective_kernel_thunk_.Initialize(params));\n+    TF_RETURN_IF_ERROR(collective_kernel_thunk_->Initialize(params));\n   }\n   return absl::OkStatus();\n }\n@@ -180,11 +171,11 @@ absl::StatusOr<bool> AllReduceStartThunk::RunCollective(\n                              config_.config.operand_element_type));\n \n   TF_ASSIGN_OR_RETURN(bool use_collective_kernel,\n-                      collective_kernel_thunk_.IsSupported(\n+                      collective_kernel_thunk_->IsSupported(\n                           comm_handle.clique_key, params.collective_cliques));\n \n   if (use_collective_kernel) {\n-    TF_RETURN_IF_ERROR(collective_kernel_thunk_.ExecuteOnStream(params));\n+    TF_RETURN_IF_ERROR(collective_kernel_thunk_->ExecuteOnStream(params));\n     return false;  // No need for \"first\" invocation to rendezvous when not\n                    // using nccl.\n   }"
        },
        {
            "sha": "46bf4af79ff7592b59b9c6d578bc0e45a322d7e3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.h",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_RUNTIME_ALL_REDUCE_THUNK_H_\n \n #include <cstdint>\n+#include <memory>\n #include <vector>\n \n #include \"absl/status/status.h\"\n@@ -37,8 +38,7 @@ struct AllReduceConfig {\n   ReductionKind reduction_kind;\n };\n \n-template <typename HloInstType>\n-AllReduceConfig GetAllReduceConfigInst(HloInstType* inst);\n+AllReduceConfig GetAllReduceConfigInst(const HloAllReduceInstructionBase* inst);\n \n // Thunk that performs a NCCL-based All-Reduce or Reduce-Scatter among CUDA\n // GPU-based replicas.\n@@ -64,9 +64,11 @@ class AllReduceReduceScatterThunkBase : public CollectiveThunk {\n \n class AllReduceStartThunk : public AllReduceReduceScatterThunkBase {\n  public:\n-  AllReduceStartThunk(ThunkInfo thunk_info, const HloAllReduceInstruction* inst,\n-                      std::vector<Buffer> buffers,\n-                      bool p2p_memcpy_enabled = false);\n+  AllReduceStartThunk(\n+      ThunkInfo thunk_info, const HloAllReduceInstruction* inst,\n+      std::vector<Buffer> buffers,\n+      std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n+      bool p2p_memcpy_enabled = false);\n \n   static const char* GetHloOpName() { return \"all-reduce-start\"; }\n \n@@ -87,7 +89,7 @@ class AllReduceStartThunk : public AllReduceReduceScatterThunkBase {\n                                      CommunicatorHandle comm) override;\n \n  private:\n-  CollectiveKernelThunk collective_kernel_thunk_;\n+  std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk_;\n };\n \n // -----------------------------------------------------------------------------"
        },
        {
            "sha": "98a44bfe28ce61b6b574d9ceff3742c77b41c7ad",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 127,
            "changes": 142,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.*/\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -33,20 +32,19 @@ limitations under the License.*/\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/all_reduce.h\"\n+#include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n-#include \"xla/service/rendezvous.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n-#include \"xla/stream_executor/gpu/gpu_executor.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -138,138 +136,26 @@ int64_t CollectiveKernelThunk::GetInputSizeBytes() const {\n              collective_config_.operand_element_type[0]);\n }\n \n-struct BaseRangePtrRendezvousValue {\n-  RankId rank;\n-  se::DeviceMemoryBase buffer_ptr;\n-  se::DeviceMemoryBase signal_ptr;\n-\n-  bool operator<(const BaseRangePtrRendezvousValue& other) const {\n-    return rank < other.rank;\n-  }\n-};\n-\n absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n     const GpuCliqueKey& clique_key, const InitializeParams& params,\n     StreamState& state) {\n-  BaseRangePtrRendezvousValue rendezvous_value;\n   const std::optional<RankId> rank =\n       clique_key.rank(params.collective_params->global_device_id);\n   TF_RET_CHECK(rank.has_value())\n       << \"Device \" << params.collective_params->global_device_id\n       << \"is not in the clique.\";\n-  rendezvous_value.rank = rank.value();\n-  rendezvous_value.buffer_ptr = state.local_buffers_handle.memory();\n-  rendezvous_value.signal_ptr = state.signal_buffers_handle.memory();\n-\n-  auto rendezvous_fn =\n-      [](absl::Span<const BaseRangePtrRendezvousValue* const> values) {\n-        std::vector<BaseRangePtrRendezvousValue> values_copy;\n-        for (const auto& value : values) {\n-          values_copy.push_back(*value);\n-        }\n-        // Sort to make sure that values are in the same order as the\n-        // devices are ordered in the communicator.\n-        absl::c_sort(values_copy);\n-        return values_copy;\n-      };\n-  const int64_t num_ranks = clique_key.num_devices();\n-  std::string start_rendezvous_key = absl::StrFormat(\n-      \"Initializing one-shot all-reduce for device %d, clique %s\",\n-      params.executor->device_ordinal(), clique_key.ToString());\n-  TF_ASSIGN_OR_RETURN(std::shared_ptr<std::vector<BaseRangePtrRendezvousValue>>\n-                          rendezvous_values,\n-                      Rendezvous<std::vector<BaseRangePtrRendezvousValue>>(\n-                          /*name=*/start_rendezvous_key, /*key=*/clique_key,\n-                          /*value=*/rendezvous_value, /*num_threads=*/num_ranks,\n-                          rendezvous_fn));\n-\n-  if (rendezvous_values->size() > num_ranks) {\n-    return absl::InvalidArgumentError(absl::StrFormat(\n-        \"Multi-device kernels require at most %d peers.\", num_ranks));\n-  }\n-  CollectiveKernelMetadata metadata;\n-  metadata.rank = rank.value().value();\n-  metadata.multicast_buffer_ptr =\n-      reinterpret_cast<uint64_t>(state.multicast_device_ptr);\n-\n-  std::vector<uint64_t> param_to_peers_ptrs;\n-  param_to_peers_ptrs.reserve(rendezvous_values->size() * 2);\n-  for (const auto& value : *rendezvous_values) {\n-    param_to_peers_ptrs.push_back(\n-        reinterpret_cast<uint64_t>(value.buffer_ptr.opaque()));\n-  }\n-  for (const auto& value : *rendezvous_values) {\n-    param_to_peers_ptrs.push_back(\n-        reinterpret_cast<uint64_t>(value.signal_ptr.opaque()));\n-  }\n-\n-  size_t param_to_peers_ptrs_size_bytes =\n-      param_to_peers_ptrs.size() * sizeof(uint64_t);\n-  se::DeviceMemoryBase metadata_ptr = params.executor->Allocate(\n-      sizeof(CollectiveKernelMetadata) + param_to_peers_ptrs_size_bytes, 0);\n-  se::DeviceMemoryBase param_to_peers_ptrs_buffer = metadata_ptr.GetByteSlice(\n-      sizeof(CollectiveKernelMetadata), param_to_peers_ptrs_size_bytes);\n-  VLOG(3) << \"[\" << params.executor->device_ordinal() << \"]\"\n-          << \" ExchangeStateMetadata: metadata_ptr = \" << metadata_ptr.opaque()\n-          << \", param_to_peers_ptrs_buffer = \"\n-          << param_to_peers_ptrs_buffer.opaque()\n-          << \", param_to_peers_ptrs_size = \" << param_to_peers_ptrs.size();\n-  metadata.param_to_peers =\n-      reinterpret_cast<uint64_t*>(param_to_peers_ptrs_buffer.opaque());\n-  TF_RETURN_IF_ERROR(params.stream->Memcpy(&metadata_ptr, (void*)&metadata,\n-                                           sizeof(CollectiveKernelMetadata)));\n-  TF_RETURN_IF_ERROR(params.stream->Memcpy(&param_to_peers_ptrs_buffer,\n-                                           param_to_peers_ptrs.data(),\n-                                           param_to_peers_ptrs_size_bytes));\n-  TF_RETURN_IF_ERROR(params.stream->BlockHostUntilDone());\n-\n-  state.metadata = metadata_ptr;\n-  return absl::OkStatus();\n-}\n-\n-absl::Status Barrier(int device_number, const GpuCliqueKey& clique_key) {\n-  std::string start_rendezvous_key = absl::StrFormat(\n-      \"Barrier for device %d, \"\n-      \"clique %s\",\n-      device_number, clique_key.ToString());\n-  return Rendezvous(\n-      /*name=*/\n-      start_rendezvous_key, /*key=*/clique_key,\n-      /*num_threads=*/clique_key.num_local_participants());\n-}\n-\n-absl::Status CollectiveKernelThunk::SetupMultimem(\n-    const GpuCliqueKey& clique_key, const se::StreamExecutor* stream_executor,\n-    StreamState& state) {\n-  const stream_executor::gpu::GpuExecutor* gpu_executor =\n-      dynamic_cast<const stream_executor::gpu::GpuExecutor*>(stream_executor);\n-  if (gpu_executor == nullptr) {\n-    return absl::UnimplementedError(\"Multicast is not supported on device.\");\n-  }\n-\n-  size_t data_size = buffers_[0].source_buffer.size();\n-  int device_number = gpu_executor->device_ordinal();\n-\n-  if (device_number == 0) {\n-    TF_ASSIGN_OR_RETURN(multicast_memory_,\n-                        gpu_executor->CreateMulticastMemory(\n-                            data_size, clique_key.num_local_participants()));\n-  }\n-\n-  // Wait for all devices to create the multicast object.\n-  TF_RETURN_IF_ERROR(Barrier(device_number, clique_key));\n-\n-  // Add current devices to the multicast object.\n-  TF_RETURN_IF_ERROR(multicast_memory_->SubscribeDevice(device_number));\n \n-  // Wait for all devices to register the multicast object.\n-  TF_RETURN_IF_ERROR(Barrier(device_number, clique_key));\n+  std::vector<se::DeviceMemoryBase> parameters;\n+  parameters.push_back(state.local_buffers_handle.memory());\n+  parameters.push_back(state.signal_buffers_handle.memory());\n \n-  TF_ASSIGN_OR_RETURN(state.multicast_device_ptr,\n-                      multicast_memory_->MapMemory(\n-                          state.local_buffers_handle.memory(), gpu_executor));\n-\n-  return absl::OkStatus();\n+  const size_t param_to_peers_ptrs_size_bytes =\n+      parameters.size() * clique_key.num_devices() * sizeof(uint64_t);\n+  state.metadata = params.executor->Allocate(\n+      sizeof(CollectiveKernelMetadata) + param_to_peers_ptrs_size_bytes, 0);\n+  return CollectiveMetadataThunk::ConstructCollectiveMetadata(\n+      std::move(parameters), params.stream, clique_key,\n+      state.multicast_device_ptr, rank.value().value(), state.metadata);\n }\n \n absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n@@ -356,8 +242,10 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n \n   if (state != nullptr) {\n     if (strategy == AllReduceStrategy::kMultimem) {\n-      se::StreamExecutor* stream_executor = params.executor;\n-      TF_RETURN_IF_ERROR(SetupMultimem(clique_key, stream_executor, *state));\n+      TF_ASSIGN_OR_RETURN(state->multicast_device_ptr,\n+                          address_space_provider_.SetupMultimemAddressSpace(\n+                              clique_key, params.executor,\n+                              state->local_buffers_handle.memory()));\n     }\n     TF_RETURN_IF_ERROR(ExchangeStateMetadata(clique_key, params, *state));\n   }"
        },
        {
            "sha": "e4849f354c009960fc57492b36453ad200f5837b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 6,
            "deletions": 13,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.*/\n #include <memory>\n #include <string>\n #include <utility>\n+#include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n@@ -29,14 +30,14 @@ limitations under the License.*/\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n-#include \"xla/stream_executor/gpu/gpu_executor.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/stream.h\"\n \n@@ -59,7 +60,7 @@ class CollectiveKernelThunk : public Thunk {\n \n   CollectiveKernelThunk(ThunkInfo info, CollectiveConfig collective_config,\n                         ReductionKind reduction_kind, bool is_async,\n-                        absl::Span<const CollectiveThunk::Buffer> buffers,\n+                        std::vector<CollectiveThunk::Buffer> buffers,\n                         bool is_collective_kernel_enabled,\n                         absl::string_view kernel_name = \"\",\n                         bool is_multimem_enabled = false)\n@@ -69,7 +70,7 @@ class CollectiveKernelThunk : public Thunk {\n         collective_config_(std::move(collective_config)),\n         reduction_kind_(reduction_kind),\n         kernel_name_(kernel_name),\n-        buffers_(buffers),\n+        buffers_(std::move(buffers)),\n         is_multimem_enabled_(is_multimem_enabled) {\n     per_stream_state_.reserve(kMaxNumExecutors);\n   }\n@@ -149,13 +150,6 @@ class CollectiveKernelThunk : public Thunk {\n                                      const InitializeParams& params,\n                                      StreamState& state);\n \n-  // Initializes and multimem memory. Each thunk participant should call this\n-  // method once. Multimem should be setup before usage when multimem strategy\n-  // is selected.\n-  absl::Status SetupMultimem(const GpuCliqueKey& clique_key,\n-                             const se::StreamExecutor* stream_executor,\n-                             StreamState& state);\n-\n   // Whether the one-shot kernel is enabled.\n   const bool collective_kernel_enabled_;\n   // Whether the collective is run on an async stream.\n@@ -168,10 +162,9 @@ class CollectiveKernelThunk : public Thunk {\n   // Must match the kernel name in the generated PTX kernel.\n   const std::string kernel_name_;\n   // Reference to the buffer related information required for the collective.\n-  absl::Span<const CollectiveThunk::Buffer> buffers_;\n+  std::vector<CollectiveThunk::Buffer> buffers_;\n \n-  std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>\n-      multicast_memory_;\n+  CollectiveMetadataThunk::MultimemAddressSpaceProvider address_space_provider_;\n   // Guard access to the stream state across different threads (which control\n   // different streams).\n   absl::Mutex mutex_;"
        },
        {
            "sha": "9177031c22e9258cb01df6a36815dc53068a2323",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "added",
            "additions": 273,
            "deletions": 0,
            "changes": 273,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,273 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/types/span.h\"\n+#include \"google/protobuf/repeated_ptr_field.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/layout.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/rendezvous.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n+#include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// TODO(460077850): Support global device ids and channel id.\n+CollectiveConfig CollectiveMetadataThunk::GetCollectiveConfig(\n+    const HloInstruction& hlo) {\n+  CollectiveConfig config;\n+  config.operand_count = hlo.operands().size();\n+  config.operand_element_type.reserve(config.operand_count);\n+  for (int i = 0; i < config.operand_count; i++) {\n+    config.operand_element_type.push_back(\n+        hlo.operand(i)->shape().element_type());\n+  }\n+\n+  config.collective_op_kind = RendezvousKey::kCrossReplica;\n+  config.op_id = static_cast<int64_t>(hlo.GetModule()->unique_id());\n+  if (hlo.has_backend_config()) {\n+    xla::gpu::GpuBackendConfig backend_config =\n+        hlo.backend_config<GpuBackendConfig>().value_or(GpuBackendConfig());\n+    if (backend_config.has_collective_metadata_backend_config()) {\n+      ::google::protobuf::RepeatedPtrField<ReplicaGroup> replica_groups =\n+          backend_config.collective_metadata_backend_config()\n+              .collective_devices()\n+              .replica_groups();\n+      config.replica_groups = std::vector<ReplicaGroup>(replica_groups.begin(),\n+                                                        replica_groups.end());\n+    }\n+  }\n+\n+  config.group_mode =\n+      CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_CROSS_REPLICA;\n+\n+  return config;\n+}\n+\n+struct CollectiveMetadataRendezvousValue {\n+  RankId rank;\n+  std::vector<se::DeviceMemoryBase> parameters;\n+\n+  bool operator<(const CollectiveMetadataRendezvousValue& other) const {\n+    return rank < other.rank;\n+  }\n+};\n+\n+absl::Status CollectiveMetadataThunk::ConstructCollectiveMetadata(\n+    std::vector<se::DeviceMemoryBase> parameters, se::Stream* stream,\n+    const GpuCliqueKey& clique_key, void* multimem_address_space,\n+    int device_ordinal, se::DeviceMemoryBase destination) {\n+  auto rendezvous_fn =\n+      [](absl::Span<const CollectiveMetadataRendezvousValue* const> values) {\n+        std::vector<CollectiveMetadataRendezvousValue> values_copy;\n+        for (const auto& value : values) {\n+          values_copy.push_back(*value);\n+        }\n+        // Sort to make sure that values are in the same order as the\n+        // devices are ordered in the communicator.\n+        absl::c_sort(values_copy);\n+        return values_copy;\n+      };\n+\n+  std::string start_rendezvous_key =\n+      absl::StrFormat(\"[%d] Initializing collective metadata for clique %s\",\n+                      device_ordinal, clique_key.ToString());\n+\n+  CollectiveMetadataRendezvousValue rendezvous_value;\n+  rendezvous_value.rank = device_ordinal;\n+  rendezvous_value.parameters = std::move(parameters);\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::shared_ptr<std::vector<CollectiveMetadataRendezvousValue>>\n+          rendezvous_values,\n+      Rendezvous<std::vector<CollectiveMetadataRendezvousValue>>(\n+          /*name=*/start_rendezvous_key, /*key=*/clique_key,\n+          /*value=*/rendezvous_value, /*num_threads=*/clique_key.num_devices(),\n+          rendezvous_fn));\n+\n+  CollectiveKernelMetadata metadata;\n+  metadata.rank = clique_key.rank(GlobalDeviceId(device_ordinal))\n+                      .value_or(RankId(-1))\n+                      .value();\n+  if (metadata.rank == -1) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Device %d not found in clique %s\", device_ordinal,\n+                        clique_key.ToString()));\n+  }\n+  metadata.multicast_buffer_ptr = multimem_address_space;\n+  TF_RET_CHECK(rendezvous_values->size() > 0)\n+      << \"Not enough devices in the clique.\";\n+  const size_t num_parameters = (*rendezvous_values)[0].parameters.size();\n+  for (const auto& value : *rendezvous_values) {\n+    TF_RET_CHECK(value.parameters.size() == num_parameters);\n+  }\n+\n+  std::vector<void*> param_to_peers_ptrs;\n+  param_to_peers_ptrs.reserve(rendezvous_values->size() * num_parameters);\n+  for (int param = 0; param < num_parameters; ++param) {\n+    for (int peer = 0; peer < clique_key.num_devices(); ++peer) {\n+      param_to_peers_ptrs.push_back(\n+          (*rendezvous_values)[peer].parameters[param].opaque());\n+    }\n+  }\n+\n+  const int param_to_peers_ptrs_size =\n+      param_to_peers_ptrs.size() * sizeof(void*);\n+  se::DeviceMemoryBase param_to_peers_ptrs_buffer = destination.GetByteSlice(\n+      sizeof(CollectiveKernelMetadata), param_to_peers_ptrs_size);\n+\n+  metadata.param_to_peers =\n+      reinterpret_cast<void**>(param_to_peers_ptrs_buffer.opaque());\n+\n+  TF_RETURN_IF_ERROR(stream->Memcpy(&destination, &metadata,\n+                                    sizeof(CollectiveKernelMetadata)));\n+  TF_RETURN_IF_ERROR(stream->Memcpy(&param_to_peers_ptrs_buffer,\n+                                    param_to_peers_ptrs.data(),\n+                                    param_to_peers_ptrs_size));\n+  return stream->BlockHostUntilDone();\n+}\n+\n+absl::Status CollectiveMetadataThunk::Initialize(\n+    const InitializeParams& params) {\n+  TF_ASSIGN_OR_RETURN(\n+      const GpuCliqueKey clique_key,\n+      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n+                                /*use_nccl=*/false));\n+  const int64_t num_ranks = clique_key.num_devices();\n+  TF_RET_CHECK(result_.size() ==\n+               sizeof(CollectiveKernelMetadata) +\n+                   num_ranks * parameters_.size() * sizeof(uint64_t));\n+\n+  std::vector<se::DeviceMemoryBase> parameters;\n+  parameters.reserve(parameters_.size());\n+  for (const CollectiveMetadataThunk::Buffer& parameter : parameters_) {\n+    parameters.push_back(\n+        params.buffer_allocations->GetDeviceAddress(parameter.slice));\n+  }\n+  se::DeviceMemoryBase result_ptr =\n+      params.buffer_allocations->GetDeviceAddress(result_);\n+\n+  TF_ASSIGN_OR_RETURN(void* multimem_address_space,\n+                      SetupMultimem(clique_key, params));\n+  return ConstructCollectiveMetadata(\n+      std::move(parameters), params.stream, clique_key, multimem_address_space,\n+      params.executor->device_ordinal(), result_ptr);\n+}\n+\n+absl::Status CollectiveMetadataThunk::ExecuteOnStream(\n+    const ExecuteParams& params) {\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<void*> CollectiveMetadataThunk::SetupMultimem(\n+    const GpuCliqueKey& clique_key, const InitializeParams& params) {\n+  se::DeviceMemoryBase memory_range;\n+  for (const CollectiveMetadataThunk::Buffer& parameter : parameters_) {\n+    if (parameter.memory_space == xla::Layout::kGenericFastMemorySpace) {\n+      TF_ASSIGN_OR_RETURN(\n+          memory_range,\n+          params.executor->GetMemoryRange(\n+              params.buffer_allocations->GetDeviceAddress(parameter.slice)));\n+      break;\n+    }\n+  }\n+\n+  // Since there is no parameter in the collective memory space, we don't need\n+  // to set up the multicast memory.\n+  if (memory_range.is_null()) {\n+    return nullptr;\n+  }\n+  return address_space_provider_.SetupMultimemAddressSpace(\n+      clique_key, params.executor, memory_range);\n+}\n+\n+absl::Status Barrier(int device_number, const GpuCliqueKey& clique_key) {\n+  std::string start_rendezvous_key = absl::StrFormat(\n+      \"Barrier for device %d, \"\n+      \"clique %s\",\n+      device_number, clique_key.ToString());\n+  return Rendezvous(\n+      /*name=*/\n+      start_rendezvous_key, /*key=*/clique_key,\n+      /*num_threads=*/clique_key.num_local_participants());\n+}\n+\n+absl::StatusOr<void*> CollectiveMetadataThunk::MultimemAddressSpaceProvider::\n+    SetupMultimemAddressSpace(const GpuCliqueKey& clique_key,\n+                              const se::StreamExecutor* stream_executor,\n+                              se::DeviceMemoryBase mapped_memory) {\n+  const auto* gpu_executor =\n+      dynamic_cast<const stream_executor::gpu::GpuExecutor*>(stream_executor);\n+  if (gpu_executor == nullptr) {\n+    return absl::UnimplementedError(\"Multicast is not supported on device.\");\n+  }\n+  int device_number = gpu_executor->device_ordinal();\n+  TF_RET_CHECK(clique_key.num_local_participants() > 0)\n+      << \"Number of local participants must be greater than 0.\";\n+  int64_t first_device = clique_key.devices()[0].value();\n+\n+  if (device_number == first_device) {\n+    TF_ASSIGN_OR_RETURN(\n+        std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>\n+            multicast_memory,\n+        gpu_executor->CreateMulticastMemory(\n+            mapped_memory.size(), clique_key.num_local_participants()));\n+    first_device_to_multicast_memory_.emplace(device_number,\n+                                              std::move(multicast_memory));\n+  }\n+\n+  // Wait for all devices to create the multicast object.\n+  TF_RETURN_IF_ERROR(Barrier(device_number, clique_key));\n+\n+  TF_RET_CHECK(first_device_to_multicast_memory_.contains(first_device))\n+      << \"Multicast memory is not created for device \" << first_device;\n+  // Add current devices to the multicast object.\n+  TF_RETURN_IF_ERROR(\n+      first_device_to_multicast_memory_[first_device]->SubscribeDevice(\n+          device_number));\n+\n+  // Wait for all devices to register the multicast object.\n+  TF_RETURN_IF_ERROR(Barrier(device_number, clique_key));\n+\n+  return first_device_to_multicast_memory_[first_device]->MapMemory(\n+      mapped_memory, gpu_executor);\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "e7496227f256b9cb38c6f853f8865c711f496e72",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.h",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,96 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_METADATA_THUNK_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_METADATA_THUNK_H_\n+\n+#include <cstdint>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+class CollectiveMetadataThunk : public Thunk {\n+ public:\n+  struct Buffer {\n+    BufferAllocation::Slice slice;\n+    int64_t memory_space;\n+  };\n+\n+  class MultimemAddressSpaceProvider {\n+   public:\n+    // Initializes and multimem memory. Each thunk participant should call this\n+    // method once. Multimem should be setup before usage when multimem strategy\n+    // is selected.\n+    absl::StatusOr<void*> SetupMultimemAddressSpace(\n+        const GpuCliqueKey& clique_key,\n+        const se::StreamExecutor* stream_executor,\n+        se::DeviceMemoryBase mapped_memory);\n+\n+   private:\n+    absl::flat_hash_map<\n+        int,\n+        std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>>\n+        first_device_to_multicast_memory_;\n+  };\n+\n+  explicit CollectiveMetadataThunk(ThunkInfo thunk_info,\n+                                   CollectiveConfig collective_config,\n+                                   std::vector<Buffer> parameters,\n+                                   BufferAllocation::Slice result)\n+      : Thunk(Thunk::Kind::kCollectiveMetadata, thunk_info),\n+        collective_config_(std::move(collective_config)),\n+        parameters_(std::move(parameters)),\n+        result_(result) {}\n+  absl::Status Initialize(const InitializeParams& params) override;\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+\n+  static CollectiveConfig GetCollectiveConfig(const HloInstruction& hlo);\n+\n+  // Constructs and places the collective metadata on the device.\n+  // All participants should call this method to construct their local\n+  // metadata.\n+  static absl::Status ConstructCollectiveMetadata(\n+      std::vector<se::DeviceMemoryBase> parameters, se::Stream* stream,\n+      const GpuCliqueKey& clique_key, void* multimem_address_space,\n+      int device_ordinal, se::DeviceMemoryBase destination);\n+\n+  absl::StatusOr<void*> SetupMultimem(const GpuCliqueKey& clique_key,\n+                                      const InitializeParams& params);\n+\n+ private:\n+  const CollectiveConfig collective_config_;\n+  std::vector<Buffer> parameters_;\n+  MultimemAddressSpaceProvider address_space_provider_;\n+  BufferAllocation::Slice result_;\n+};\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_METADATA_THUNK_H_"
        },
        {
            "sha": "7f9be0360e67e961d076d27d26265f0add1f6792",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -45,13 +45,11 @@ limitations under the License.\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/computation_placer.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -293,7 +291,7 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n \n   TF_RETURN_IF_ERROR(::xla::gpu::RunCollectivePermute(\n       source_target, device_buffers, stream, comm_handle.comm, device_string,\n-      current_id, use_memcpy, recv_ptr_map_,\n+      current_id, use_memcpy, &recv_ptr_map_,\n       config_.config.use_symmetric_buffer));\n \n   if (use_memcpy) {\n@@ -338,9 +336,10 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n \n absl::Status RunCollectivePermute(\n     P2PConfig::SourceTargetMapEntry source_target,\n-    std::vector<DeviceBufferPair>& buffers, se::Stream& stream,\n+    const std::vector<DeviceBufferPair>& buffers, se::Stream& stream,\n     Communicator* comm, absl::string_view device_string, int64_t current_id,\n-    bool use_memcpy, CollectivePermuteStartThunk::RecvPtrMap& recv_ptr_map,\n+    bool use_memcpy,\n+    const CollectivePermuteStartThunk::RecvPtrMap* recv_ptr_map,\n     bool use_symmetric_buffer) {\n   // Determine the source and target IDs for this instance. The source ID is the\n   // ID which will copy its data to this instance. The destination ID is the ID\n@@ -438,7 +437,9 @@ absl::Status RunCollectivePermute(\n   }\n \n   if (use_memcpy && target_id) {\n-    TF_ASSIGN_OR_RETURN(auto recv_ptrs, recv_ptr_map.GetRecvPtr(*target_id));\n+    CHECK(recv_ptr_map != nullptr);\n+    TF_ASSIGN_OR_RETURN(AsyncValueRef<std::vector<void*>> recv_ptrs,\n+                        recv_ptr_map->GetRecvPtr(*target_id));\n \n     VLOG(3) << \"Using memcpy, received target pointers, current_id: \"\n             << current_id << \" target_id: \" << *target_id;"
        },
        {
            "sha": "39476490ccfc80a8da0f8a35e8ad18c2f351e69f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.h",
            "status": "modified",
            "additions": 17,
            "deletions": 9,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -29,8 +29,10 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/p2p_thunk_common.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/stream_executor/event.h\"\n@@ -47,7 +49,7 @@ class CollectivePermuteStartThunk : public CollectiveThunk {\n  public:\n   class RecvPtrMap {\n    public:\n-    bool IsInitialized(int64_t current_id) {\n+    bool IsInitialized(int64_t current_id) const {\n       absl::MutexLock lock(mutex_);\n       return recv_ptrs_.find(current_id) != recv_ptrs_.end();\n     }\n@@ -74,17 +76,17 @@ class CollectivePermuteStartThunk : public CollectiveThunk {\n     }\n \n     absl::StatusOr<AsyncValueRef<std::vector<void*>>> GetRecvPtr(\n-        int64_t target_id) {\n+        int64_t target_id) const {\n       if (!IsInitialized(target_id)) {\n         return absl::InternalError(absl::StrCat(\"Target ID \", target_id,\n                                                 \" has not been initialized!\"));\n       }\n       absl::MutexLock lock(mutex_);\n-      return recv_ptrs_[target_id];\n+      return recv_ptrs_.at(target_id);\n     }\n \n    private:\n-    absl::Mutex mutex_;\n+    mutable absl::Mutex mutex_;\n     absl::node_hash_map<int64_t, AsyncValueRef<std::vector<void*>>> recv_ptrs_\n         ABSL_GUARDED_BY(mutex_);\n   };\n@@ -104,12 +106,18 @@ class CollectivePermuteStartThunk : public CollectiveThunk {\n                               const std::vector<Buffer>& buffers,\n                               bool p2p_memcpy_enabled,\n                               AsyncStreamKind stream_kind);\n+\n   absl::Status Initialize(const InitializeParams& params) override;\n \n   static const char* GetHloOpName() { return \"collective-permute-start\"; }\n \n- protected:\n   const CollectiveConfig& config() const override { return config_.config; }\n+\n+  absl::Span<const Buffer> buffers() const { return buffers_; }\n+\n+  const P2PConfig& p2p_config() const { return config_; }\n+\n+ protected:\n   absl::StatusOr<bool> RunCollective(const ExecuteParams& params,\n                                      se::Stream& stream,\n                                      CommunicatorHandle comm_handle) override;\n@@ -123,16 +131,16 @@ class CollectivePermuteStartThunk : public CollectiveThunk {\n       receiver_barrier_events_;\n   absl::flat_hash_map<int64_t, std::unique_ptr<se::Event>>\n       sender_barrier_events_;\n-\n   bool p2p_memcpy_enabled_ = false;\n-  int64_t device_count_;\n+  int64_t device_count_ = 0;\n };\n \n absl::Status RunCollectivePermute(\n     P2PConfig::SourceTargetMapEntry source_target,\n-    std::vector<DeviceBufferPair>& buffers, se::Stream& stream,\n+    const std::vector<DeviceBufferPair>& buffers, se::Stream& stream,\n     Communicator* comm, absl::string_view device_string, int64_t current_id,\n-    bool use_memcpy, CollectivePermuteStartThunk::RecvPtrMap& recv_ptr_map,\n+    bool use_memcpy = false,\n+    const CollectivePermuteStartThunk::RecvPtrMap* recv_ptr_map = nullptr,\n     bool use_symmetric_buffer = false);\n \n }  // namespace gpu"
        },
        {
            "sha": "5036ebd04d8b13e2f8cf0cd847f7d4146ebef49c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk_test.cc",
            "status": "added",
            "additions": 219,
            "deletions": 0,
            "changes": 219,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,219 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/command_buffer_cmd.h\"\n+#include \"xla/backends/gpu/runtime/command_buffer_cmd_emitter.h\"\n+#include \"xla/backends/gpu/runtime/command_buffer_thunk.h\"\n+#include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/testlib/verified_hlo_module.h\"\n+#include \"xla/service/backend.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/gpu_constants.h\"\n+#include \"xla/service/gpu/gpu_executable.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tests/hlo_test_base.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/casts.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::testing::ElementsAre;\n+using Kind = Thunk::Kind;\n+\n+class GpuCollectivePermuteTest : public HloTestBase {};\n+\n+// Test case to verify that a CollectivePermute HLO instruction is correctly\n+// converted into a sequence of command buffer commands (Start and Done).\n+TEST_F(GpuCollectivePermuteTest, TestConvertToCommands) {\n+  // Generate HLO text\n+  std::string hlo_text = R\"(\n+HloModule test, replica_count=2\n+ENTRY test_computation {\n+  p = u32[4] parameter(0)\n+  ROOT permute = u32[4] collective-permute(p), source_target_pairs={{0,1}, {1,0}}\n+}\n+)\";\n+\n+  // Configure module with debug options for command buffer.\n+  HloModuleConfig config;\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  debug_options.set_xla_gpu_graph_min_graph_size(1);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n+  config.set_debug_options(debug_options);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text, config));\n+\n+  // Get CollectivePermute Instruction\n+  const HloInstruction* root_instr =\n+      module->entry_computation()->root_instruction();\n+  ASSERT_EQ(root_instr->opcode(), HloOpcode::kCollectivePermute);\n+  const HloCollectivePermuteInstruction* cp_instr =\n+      tensorflow::down_cast<const HloCollectivePermuteInstruction*>(root_instr);\n+  ASSERT_NE(cp_instr, nullptr);\n+\n+  // Buffer and Allocation Setup\n+  using DataT = int32_t;\n+  constexpr int64_t kNumElements = 4;\n+  constexpr int64_t kAlignmentBytes = kXlaAllocatedBufferAlignBytes;\n+\n+  const int64_t kElementSize = sizeof(DataT);\n+  const int64_t kTotalDataBytes = kNumElements * kElementSize;\n+\n+  // Use RoundUpTo to calculate the actual size needed for one buffer.\n+  const int64_t kAlignedSliceBytes =\n+      xla::RoundUpTo<uint64_t>(kTotalDataBytes, kAlignmentBytes);\n+\n+  // The total buffer size must accommodate input and output slices.\n+  const int64_t kTotalBufferBytes = 2 * kAlignedSliceBytes;\n+\n+  BufferAllocation buffer_allocation(/*index=*/0, kTotalBufferBytes,\n+                                     /*color=*/0);\n+  BufferAllocation::Slice input_slice(&buffer_allocation, /*offset=*/0,\n+                                      kAlignedSliceBytes);\n+  BufferAllocation::Slice output_slice(&buffer_allocation, kAlignedSliceBytes,\n+                                       kAlignedSliceBytes);\n+\n+  // Use designated initializers if possible, or format for clarity.\n+  std::vector<CollectiveThunk::Buffer> buffers = {\n+      {/*element_count=*/kNumElements,\n+       /*source_buffer=*/input_slice,\n+       /*destination_buffer=*/output_slice,\n+       /*source_memory_space=*/0,\n+       /*destination_memory_space=*/0},\n+  };\n+\n+  // ThunkSequence Creation\n+  std::shared_ptr<CollectiveThunk::AsyncEvents> async_events =\n+      std::make_shared<CollectiveThunk::AsyncEvents>();\n+\n+  auto cp_start_thunk = std::make_unique<CollectivePermuteStartThunk>(\n+      Thunk::ThunkInfo{}, cp_instr, /*replica_count=*/2,\n+      /*partition_count=*/1, std::move(buffers),\n+      /*p2p_memcpy_enabled=*/false,\n+      AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE);\n+\n+  cp_start_thunk->set_async_events(async_events);\n+\n+  auto cp_done_thunk = std::make_unique<CollectiveDoneThunk>(\n+      Kind::kCollectivePermuteDone, Thunk::ThunkInfo{}, async_events,\n+      AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE);\n+\n+  ThunkSequence thunk_sequence;\n+  thunk_sequence.push_back(std::move(cp_start_thunk));\n+  thunk_sequence.push_back(std::move(cp_done_thunk));\n+\n+  // Convert to Commands and Verification\n+  ConvertToCommandsOptions conv_options;\n+  // Use LHS synchronization mode to append Done command\n+  conv_options.synchronization_mode =\n+      CommandBufferCmdExecutor::SynchronizationMode::kLHS;\n+  TF_ASSERT_OK_AND_ASSIGN(CommandBufferCmdExecutor cb_cmd_executor,\n+                          ConvertToCommands(thunk_sequence, conv_options));\n+\n+  // Check that we have two commands: start and done.\n+  EXPECT_EQ(cb_cmd_executor.size(), 2);\n+}\n+\n+TEST_F(GpuCollectivePermuteTest,\n+       TestCommandBufferThunkContainsCollectivePermute) {\n+  // Generate HLO text\n+  std::string hlo_text = R\"(\n+HloModule test, replica_count=2\n+ENTRY test_computation {\n+  replica = u32[] replica-id()\n+  p = u32[4] broadcast(replica), dimensions={}\n+  ROOT permute = u32[4] collective-permute(p), source_target_pairs={{0,1}, {1,0}}\n+}\n+)\";\n+\n+  // Configure module with debug options for command buffer.\n+  HloModuleConfig config;\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  debug_options.set_xla_gpu_graph_min_graph_size(1);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n+  config.set_debug_options(debug_options);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text, config));\n+\n+  se::StreamExecutor* executor = backend().default_stream_executor();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> compiled_module,\n+      backend().compiler()->RunHloPasses(module->Clone(), executor,\n+                                         /*device_allocator=*/nullptr));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Executable> executable,\n+      backend().compiler()->RunBackend(std::move(compiled_module), executor,\n+                                       {/*device_allocator=*/nullptr,\n+                                        /*thread_pool=*/nullptr,\n+                                        /*layout_canonicalization_callback=*/{},\n+                                        /*is_autotuning_compilation=*/false}));\n+  // Downcast to GPU executable\n+  xla::gpu::GpuExecutable* gpu_executable =\n+      tensorflow::down_cast<xla::gpu::GpuExecutable*>(executable.get());\n+  ASSERT_NE(gpu_executable, nullptr);\n+\n+  // Get the thunk sequence and check its size and type\n+  const SequentialThunk& seq_thunk = gpu_executable->GetThunk();\n+  ASSERT_EQ(seq_thunk.thunks().size(), 1);\n+\n+  const std::unique_ptr<Thunk>& thunk = seq_thunk.thunks().front();\n+  ASSERT_EQ(thunk->kind(), Thunk::kCommandBuffer);\n+\n+  // Downcast to the specific CommandBufferThunk type for inspection.\n+  CommandBufferThunk* cmd_buffer_thunk =\n+      tensorflow::down_cast<CommandBufferThunk*>(thunk.get());\n+  ASSERT_NE(cmd_buffer_thunk, nullptr);\n+\n+  // Inspect the Thunk kinds\n+  std::vector<Kind> kinds;\n+  const auto& inner_thunks = cmd_buffer_thunk->thunks()->thunks();\n+  kinds.reserve(inner_thunks.size());\n+  for (const auto& thunk : inner_thunks) {\n+    kinds.push_back(thunk->kind());\n+  }\n+  // Verify that the inner Thunks match the expected sequence from the HLO\n+  EXPECT_THAT(kinds, ElementsAre(Kind::kReplicaId, Kind::kKernel,\n+                                 Kind::kCollectivePermuteStart));\n+}\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "fc42aeb72fe3a01b8dd23e42171c5ea35f5804a8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -296,7 +296,12 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n   absl::flat_hash_set<IncarnationId> unique_incarnations;\n   if (params.incarnations) {\n     for (GlobalDeviceId id : participants) {\n-      unique_incarnations.insert(params.incarnations->at(id));\n+      auto it = params.incarnations->find(id);\n+      if (it == params.incarnations->end()) {\n+        return FailedPrecondition(\"Incarnation for device %d not found\",\n+                                  id.value());\n+      }\n+      unique_incarnations.insert(it->second);\n     }\n   }\n   std::vector<IncarnationId> incarnations(unique_incarnations.begin(),"
        },
        {
            "sha": "a33014cd1837b1e94779a30c3a2aabd4e9ace377",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 131,
            "deletions": 0,
            "changes": 131,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -52,10 +52,12 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n #include \"xla/backends/gpu/runtime/annotation.h\"\n #include \"xla/backends/gpu/runtime/collective_broadcast_thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\"\n+#include \"xla/backends/gpu/runtime/p2p_thunk_common.h\"\n #include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n@@ -87,6 +89,7 @@ limitations under the License.\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -1747,6 +1750,53 @@ CommandBufferCmd::BufferUseVector CublasLtCmd::buffers() const {\n   return buffer_usage;\n }\n \n+//===----------------------------------------------------------------------===//\n+// ConvolutionCmd\n+//===----------------------------------------------------------------------===//\n+\n+ConvolutionCmd::ConvolutionCmd(const ConvolutionThunk& thunk)\n+    : TracedCommandBufferCmd(CommandBufferCmdType::kConvolutionCmd),\n+      operand_buffers_(thunk.operand_buffers_),\n+      result_buffers_(thunk.result_buffers_),\n+      scratch_buffer_(thunk.scratch_buffer_),\n+      config_(thunk.config_) {}\n+\n+absl::Status ConvolutionCmd::Initialize(const Thunk::InitializeParams& params,\n+                                        StateManager& state) {\n+  // populate cache of ConvRunner\n+  cache_.GetOrCreate(config_, params.stream);\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<const se::CommandBuffer::Command*> ConvolutionCmd::Record(\n+    const Thunk::ExecuteParams& execute_params,\n+    const RecordParams& record_params, RecordAction record_action,\n+    se::CommandBuffer* command_buffer) {\n+  VLOG(5) << \"ConvolutionCmd\";\n+\n+  return RecordTracedCommand(\n+      execute_params, record_params, std::move(record_action), command_buffer,\n+      [&](se::Stream* stream) {\n+        return RunConvolutionOnStream(execute_params, operand_buffers_,\n+                                      result_buffers_, scratch_buffer_, config_,\n+                                      cache_, stream);\n+      });\n+}\n+\n+CommandBufferCmd::BufferUseVector ConvolutionCmd::buffers() const {\n+  BufferUseVector buffer_usage;\n+  buffer_usage.reserve(operand_buffers_.size() + result_buffers_.size() + 1);\n+\n+  for (BufferAllocation::Slice buffer : operand_buffers_) {\n+    buffer_usage.push_back({buffer, MemoryAccess::kRead});\n+  }\n+  for (BufferAllocation::Slice buffer : result_buffers_) {\n+    buffer_usage.push_back({buffer, MemoryAccess::kWrite});\n+  }\n+  buffer_usage.push_back({scratch_buffer_, MemoryAccess::kWrite});\n+  return buffer_usage;\n+}\n+\n //===----------------------------------------------------------------------===//\n // CuDnnCmd\n //===----------------------------------------------------------------------===//\n@@ -2227,6 +2277,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> AllToAllCmd::Record(\n               config().group_mode,\n               AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant\n \n+  // MemCpy case is not currently supported in CommandBuffer.\n   return RecordTracedCommand(\n       execute_params, record_params, std::move(record_action), command_buffer,\n       [&](se::Stream* stream) {\n@@ -2371,6 +2422,86 @@ CommandBufferCmd::BufferUseVector CollectiveBroadcastCmd::buffers() const {\n   return buffer_usage;\n }\n \n+//===----------------------------------------------------------------------===//\n+// CollectivePermuteCmd\n+//===----------------------------------------------------------------------===//\n+\n+CollectivePermuteCmd::CollectivePermuteCmd(\n+    CollectiveConfig config, P2PConfig p2p_config,\n+    absl::Span<const CollectiveThunk::Buffer> buffers,\n+    std::shared_ptr<CollectiveThunk::AsyncEvents> async_events)\n+    : CollectiveCmd(CommandBufferCmdType::kCollectivePermuteCmd,\n+                    std::move(config), std::move(async_events)),\n+      p2p_config_(std::move(p2p_config)),\n+      buffers_(buffers.begin(), buffers.end()) {}\n+\n+absl::StatusOr<const se::CommandBuffer::Command*> CollectivePermuteCmd::Record(\n+    const Thunk::ExecuteParams& execute_params,\n+    const RecordParams& record_params, RecordAction record_action,\n+    se::CommandBuffer* command_buffer) {\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<DeviceBufferPair> device_buffers,\n+      ConvertToDeviceBuffers(execute_params.buffer_allocations, buffers_,\n+                             config().operand_element_type));\n+\n+  int device_ordinal = execute_params.stream->parent()->device_ordinal();\n+  VLOG(5) << \"[\" << device_ordinal << \"] CollectivePermuteCmd:\";\n+\n+  for (size_t i = 0; i < device_buffers.size(); ++i) {\n+    VLOG(5) << \"[\" << device_ordinal << \"]  Src: \" << buffers_[i].source_buffer\n+            << \" (\" << device_buffers[i].source_buffer.opaque() << \")\";\n+    VLOG(5) << \"[\" << device_ordinal\n+            << \"]  Dst: \" << buffers_[i].destination_buffer << \" (\"\n+            << device_buffers[i].destination_buffer.opaque() << \")\";\n+  }\n+\n+  if (!execute_params.collective_params || !execute_params.collective_cliques) {\n+    return absl::InvalidArgumentError(\n+        \"CollectivePermuteCmd requires collective parameters and cliques\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n+                      Thunk::GetGpuCollectives(execute_params));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      CommunicatorHandle comm_handle,\n+      GetComm(collectives, *execute_params.collective_params,\n+              *execute_params.collective_cliques, config().replica_groups,\n+              config().group_mode,\n+              AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant\n+\n+  std::string device_string =\n+      CollectiveThunk::GetDeviceString(*execute_params.collective_params);\n+  bool use_symmetric_buffer = config().use_symmetric_buffer;\n+\n+  TF_ASSIGN_OR_RETURN(\n+      const int64_t current_id,\n+      GetCollectiveCurrentId(execute_params.collective_params, p2p_config_));\n+\n+  const P2PConfig::SourceTargetMapEntry source_target =\n+      P2PConfig::GetSourceTarget(p2p_config_.id_to_source_target, current_id);\n+\n+  // MemCpy case is not currently supported in CommandBuffer.\n+  return RecordTracedCommand(\n+      execute_params, record_params, std::move(record_action), command_buffer,\n+      [&](se::Stream* stream) {\n+        return RunCollectivePermute(source_target, device_buffers, *stream,\n+                                    comm_handle.comm, device_string, current_id,\n+                                    /*use_memcpy=*/false,\n+                                    /*recv_ptr_map=*/nullptr,\n+                                    use_symmetric_buffer);\n+      });\n+}\n+\n+CommandBufferCmd::BufferUseVector CollectivePermuteCmd::buffers() const {\n+  BufferUseVector buffer_usage;\n+  for (const CollectiveThunk::Buffer& buffer : buffers_) {\n+    buffer_usage.emplace_back(BufferUse::Read(buffer.source_buffer));\n+    buffer_usage.emplace_back(BufferUse::Write(buffer.destination_buffer));\n+  }\n+  return buffer_usage;\n+}\n+\n //===----------------------------------------------------------------------===//\n // DynamicSliceFusionCmd\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "428d32bc0aeb0c79dfe4df00229e0f6255546172",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -37,11 +37,14 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\"\n+#include \"xla/backends/gpu/runtime/p2p_thunk_common.h\"\n #include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/ffi/api/c_api.h\"\n@@ -81,6 +84,7 @@ namespace xla::gpu {\n   V(kLaunchCmd, \"LaunchCmd\")                                     \\\n   V(kCustomKernelLaunchCmd, \"CustomKernelLaunchCmd\")             \\\n   V(kCublasLtCmd, \"CublasLtCmd\")                                 \\\n+  V(kConvolutionCmd, \"ConvolutionCmd\")                           \\\n   V(kCuDnnCmd, \"CuDnnCmd\")                                       \\\n   V(kGemmCmd, \"GemmCmd\")                                         \\\n   V(kMemcpyDeviceToDeviceCmd, \"MemcpyDeviceToDeviceCmd\")         \\\n@@ -96,6 +100,7 @@ namespace xla::gpu {\n   V(kAllToAllCmd, \"AllToAllCmd\")                                 \\\n   V(kAllGatherCmd, \"AllGatherCmd\")                               \\\n   V(kCollectiveBroadcastCmd, \"CollectiveBroadcastCmd\")           \\\n+  V(kCollectivePermuteCmd, \"CollectivePermuteCmd\")               \\\n   V(kAsyncDone, \"AsyncDone\")                                     \\\n   V(kDynamicSliceFusionCmd, \"DynamicSliceFusionCmd\")             \\\n   V(kDynamicSliceCopyFusionCmd, \"DynamicSliceCopyFusionCmd\")     \\\n@@ -958,6 +963,34 @@ class CublasLtCmd : public TracedCommandBufferCmd, public CublasLtMatmulThunk {\n   bool IsNestedCommandBuffer() const final { return true; }\n };\n \n+//===----------------------------------------------------------------------===//\n+// ConvolutionCmd\n+//===----------------------------------------------------------------------===//\n+\n+class ConvolutionCmd : public TracedCommandBufferCmd {\n+ public:\n+  ConvolutionCmd(const ConvolutionThunk& conv_thunk);\n+\n+  absl::Status Initialize(const Thunk::InitializeParams& params,\n+                          StateManager& state) override;\n+\n+  absl::StatusOr<const se::CommandBuffer::Command*> Record(\n+      const Thunk::ExecuteParams& execute_params,\n+      const RecordParams& record_params, RecordAction record_action,\n+      se::CommandBuffer* command_buffer) override;\n+\n+  BufferUseVector buffers() const override;\n+\n+  bool IsNestedCommandBuffer() const final { return true; }\n+\n+ private:\n+  std::vector<BufferAllocation::Slice> operand_buffers_;\n+  std::vector<BufferAllocation::Slice> result_buffers_;\n+  BufferAllocation::Slice scratch_buffer_;\n+  GpuConvConfig config_;\n+  ConvRunnerCache cache_;\n+};\n+\n //===----------------------------------------------------------------------===//\n // CuDnnCmd\n //===----------------------------------------------------------------------===//\n@@ -1209,6 +1242,29 @@ class CollectiveBroadcastCmd : public CollectiveCmd {\n   std::vector<CollectiveThunk::Buffer> buffers_;\n };\n \n+//===----------------------------------------------------------------------===//\n+// CollectivePermuteCmd\n+//===----------------------------------------------------------------------===//\n+\n+class CollectivePermuteCmd : public CollectiveCmd {\n+ public:\n+  CollectivePermuteCmd(\n+      CollectiveConfig config, P2PConfig p2p_config,\n+      absl::Span<const CollectiveThunk::Buffer> buffers,\n+      std::shared_ptr<CollectiveThunk::AsyncEvents> async_events);\n+\n+  absl::StatusOr<const se::CommandBuffer::Command*> Record(\n+      const Thunk::ExecuteParams& execute_params,\n+      const RecordParams& record_params, RecordAction record_action,\n+      se::CommandBuffer* command_buffer) override;\n+\n+  BufferUseVector buffers() const override;\n+\n+ private:\n+  P2PConfig p2p_config_;\n+  std::vector<CollectiveThunk::Buffer> buffers_;\n+};\n+\n //===----------------------------------------------------------------------===//\n // DynamicSliceFusionCmd\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "193c705f9b46e5e9a90362a3269ab03fd1770a5c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -32,12 +32,14 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_broadcast_thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_cmd.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\"\n@@ -190,6 +192,13 @@ static absl::StatusOr<Command> Convert(\n       thunk.config(), thunk.buffers(), thunk.async_events());\n }\n \n+static absl::StatusOr<Command> Convert(\n+    const CollectivePermuteStartThunk& thunk) {\n+  return std::make_unique<CollectivePermuteCmd>(\n+      thunk.config(), thunk.p2p_config(), thunk.buffers(),\n+      thunk.async_events());\n+}\n+\n static absl::StatusOr<Command> Convert(\n     const DynamicSliceThunk& thunk, const ConvertToCommandsOptions& options) {\n   TF_ASSIGN_OR_RETURN(\n@@ -235,6 +244,10 @@ static absl::StatusOr<Command> Convert(const CuDnnThunk& thunk) {\n   return std::make_unique<CuDnnCmd>(thunk.arguments(), thunk.graph());\n }\n \n+static absl::StatusOr<Command> Convert(const ConvolutionThunk& thunk) {\n+  return std::make_unique<ConvolutionCmd>(thunk);\n+}\n+\n //===----------------------------------------------------------------------===//\n static absl::StatusOr<Command> CopyMetadata(absl::StatusOr<Command> cmd,\n                                             const Thunk& thunk) {\n@@ -296,6 +309,8 @@ static absl::Status AppendCommands(CommandBufferCmdSequence& cmd_sequence,\n       return append(Convert<AllToAllStartThunk>(thunk));\n     case Thunk::Kind::kCollectiveBroadcastStart:\n       return append(Convert<CollectiveBroadcastStartThunk>(thunk));\n+    case Thunk::Kind::kCollectivePermuteStart:\n+      return append(Convert<CollectivePermuteStartThunk>(thunk));\n     case Thunk::Kind::kPartitionId:\n       return append(Convert<PartitionIdThunk>(thunk));\n     case Thunk::Kind::kReplicaId:\n@@ -304,6 +319,8 @@ static absl::Status AppendCommands(CommandBufferCmdSequence& cmd_sequence,\n       return append(Convert<WhileThunk>(thunk, options));\n     case Thunk::Kind::kCuDnn:\n       return append(Convert<CuDnnThunk>(thunk));\n+    case Thunk::Kind::kConvolution:\n+      return append(Convert<ConvolutionThunk>(thunk));\n     case Thunk::Kind::kDynamicSlice:\n       return append(Convert<DynamicSliceThunk>(thunk, options));\n \n@@ -318,6 +335,7 @@ static absl::Status AppendCommands(CommandBufferCmdSequence& cmd_sequence,\n     case Thunk::Kind::kAllReduceDone:\n     case Thunk::Kind::kAllToAllDone:\n     case Thunk::Kind::kCollectiveBroadcastDone:\n+    case Thunk::Kind::kCollectivePermuteDone:\n     case Thunk::Kind::kReduceScatterDone:\n       if (options.synchronization_mode ==\n           CommandBufferCmdExecutor::SynchronizationMode::kLHS) {"
        },
        {
            "sha": "0c487834a49d76d2bf6e1070590a0cb6c084366d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -149,6 +149,7 @@ std::optional<DebugOptions::CommandBufferCmdType> GetCommandBufferCmdType(\n     case Thunk::kSend:\n       return DebugOptions::COLLECTIVES;\n     case Thunk::kCuDnn:\n+    case Thunk::kConvolution:\n       return DebugOptions::CUDNN;\n     case Thunk::kCustomCall:\n       return DebugOptions::CUSTOM_CALL;"
        },
        {
            "sha": "38530bf6354e52ee9a255c2c1578ef16e943497e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass_test.cc",
            "status": "modified",
            "additions": 95,
            "deletions": 0,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n+#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n@@ -152,6 +153,58 @@ std::unique_ptr<GemmThunk> CreateGemmThunk(const BufferAllocation& alloc1) {\n                                      slice1, slice1, slice1, true);\n }\n \n+std::unique_ptr<ConvolutionThunk> CreateConvolutionThunk(\n+    const BufferAllocation& alloc) {\n+  std::vector<BufferAllocation::Slice> operand_slices, result_slices;\n+  for (int i = 0, num = 3; i < num; i++) {\n+    operand_slices.emplace_back(&alloc, i * 16, 16);\n+    result_slices.emplace_back(&alloc, (i + num) * 16, 16);\n+  }\n+\n+  ConvolutionDimensionNumbers dnums;\n+  dnums.set_input_batch_dimension(0);\n+  dnums.set_input_feature_dimension(1);\n+  dnums.add_input_spatial_dimensions(2);\n+  dnums.add_input_spatial_dimensions(3);\n+  dnums.set_kernel_input_feature_dimension(0);\n+  dnums.set_kernel_output_feature_dimension(1);\n+  dnums.add_kernel_spatial_dimensions(2);\n+  dnums.add_kernel_spatial_dimensions(3);\n+  dnums.set_output_batch_dimension(0);\n+  dnums.set_output_feature_dimension(1);\n+  dnums.add_output_spatial_dimensions(2);\n+  dnums.add_output_spatial_dimensions(3);\n+\n+  Window window;\n+  const auto dim0 = window.add_dimensions();\n+  const auto dim1 = window.add_dimensions();\n+  dim0->set_size(4);\n+  dim1->set_size(4);\n+  dim0->set_base_dilation(1);\n+  dim1->set_base_dilation(1);\n+  dim0->set_stride(1);\n+  dim1->set_stride(1);\n+  dim0->set_window_dilation(3);\n+  dim1->set_window_dilation(2);\n+\n+  GpuConvDescriptor desc{\n+      .kind = CudnnConvKind::kForward,\n+      .backend_config = CudnnConvBackendConfig{},\n+      .operand0_shape = ShapeUtil::MakeShape(F32, {60, 38, 17, 13}),\n+      .operand1_shape = ShapeUtil::MakeShapeWithDenseLayout(F32, {38, 10, 4, 4},\n+                                                            {3, 2, 0, 1}),\n+      .result_shape = ShapeUtil::MakeShapeWithType<float>({64, 64, 64, 13}),\n+      .scratch_size = 128 * 1024,\n+      .window = window,\n+      .dnums = dnums,\n+      .feature_group_count = 1};\n+  auto thunk =\n+      ConvolutionThunk::Create(Thunk::ThunkInfo(), desc, operand_slices,\n+                               result_slices, result_slices.back());\n+  TF_CHECK_OK(thunk.status());\n+  return std::move(thunk).value();\n+}\n+\n std::unique_ptr<CollectiveDoneThunk> CreateAllGatherDoneThunk(\n     Thunk* start_thunk) {\n   auto async_events =\n@@ -315,6 +368,48 @@ TEST(CommandBufferConversionPassTest, PartiallyConvertsToCommandBufferThunk) {\n   EXPECT_THAT(thunks_in_command_buffer1, ThunkKindsAre(Thunk::kCopy));\n }\n \n+TEST(CommandBufferConversionPassTest, ConvertConvolutionAndGemmThunks) {\n+  CommandBufferConversionPass pass{\"test\"};\n+\n+  std::vector<std::unique_ptr<Thunk>> thunks;\n+\n+  // Create a {CopyThunk, GemmThunk, ConvolutionThunk}\n+  BufferAllocation alloc0(0, 1024, 0);\n+  BufferAllocation alloc1(1, 2048, 0);\n+  BufferAllocation alloc2(2, 2048, 0);\n+  thunks.push_back(CreateCopyThunk(alloc0));\n+  thunks.push_back(CreateGemmThunk(alloc1));\n+  thunks.push_back(CreateConvolutionThunk(alloc0));\n+\n+  auto root_thunk =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));\n+  DebugOptions debug_options;\n+\n+  // Enable only FUSION, which means GemmThunk should not be converted.\n+  debug_options.clear_xla_gpu_enable_command_buffer();\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUDNN);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUBLAS);\n+\n+  se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n+\n+  ASSERT_EQ(root_thunk->thunks().size(), 3);\n+\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, /*hlo_module=*/nullptr,\n+                       device_info, allocator),\n+              IsOkAndHolds(true));\n+\n+  ASSERT_EQ(root_thunk->thunks().size(), 1);\n+\n+  const auto* command_buffer_thunk =\n+      static_cast<const CommandBufferThunk*>(root_thunk->thunks()[0].get());\n+  const auto& thunks_in_command_buffer =\n+      command_buffer_thunk->thunks()->thunks();\n+  EXPECT_THAT(thunks_in_command_buffer,\n+              ThunkKindsAre(Thunk::kCopy, Thunk::kGemm, Thunk::kConvolution));\n+}\n+\n TEST(CommandBufferConversionPassTest, ConvertsAsyncPairToCommandBuffer) {\n   std::vector<std::unique_ptr<Thunk>> thunks;\n   // Create a start thunk"
        },
        {
            "sha": "2e19a45c2a3ddd954dda9b59ca2ea8acfa6aa392",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 33,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -69,76 +69,81 @@ ConvolutionThunk::ConvolutionThunk(\n       descriptor_(std::move(descriptor)),\n       config_(std::move(config)) {}\n \n-GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(\n-    const stream_executor::Stream* stream, bool* runner_created) {\n+std::pair<RunConvOptions, bool> ConvRunnerCache::GetOrCreate(\n+    const GpuConvConfig& config, const se::Stream* stream) {\n   absl::MutexLock lock(mu_);\n-  auto it = runner_cache_.find(stream);\n-  *runner_created = (it == runner_cache_.end());\n-  if (*runner_created) {\n-    it = runner_cache_\n-             .insert({stream, std::make_unique<GenericConvRunner>(config_)})\n-             .first;\n+  auto [it, inserted] =\n+      cache_.emplace(stream->parent(), std::unique_ptr<GenericConvRunner>{});\n+  if (inserted) {\n+    it->second = std::make_unique<GenericConvRunner>(config);\n   }\n-  return *it->second;\n+  return std::pair{RunConvOptions{nullptr, it->second.get()}, inserted};\n }\n \n-absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n+absl::Status RunConvolutionOnStream(\n+    const Thunk::ExecuteParams& params,\n+    const std::vector<BufferAllocation::Slice>& operand_buffers,\n+    const std::vector<BufferAllocation::Slice>& result_buffers,\n+    const BufferAllocation::Slice& scratch_buffer, const GpuConvConfig& config,\n+    ConvRunnerCache& cache, se::Stream* stream) {\n   const auto& buffer_allocations = *params.buffer_allocations;\n \n   std::vector<se::DeviceMemoryBase> operand_se_buffers, result_se_buffers;\n-  operand_se_buffers.reserve(operand_buffers_.size());\n-  for (BufferAllocation::Slice buffer : operand_buffers_) {\n+  operand_se_buffers.reserve(operand_buffers.size());\n+\n+  for (BufferAllocation::Slice buffer : operand_buffers) {\n     operand_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n+    VLOG(5) << \"operand buffer: \" << buffer.ToString()\n+            << \" addr: \" << operand_se_buffers.back().opaque();\n   }\n \n-  result_se_buffers.reserve(result_buffers_.size());\n-  for (BufferAllocation::Slice buffer : result_buffers_) {\n+  result_se_buffers.reserve(result_buffers.size());\n+  for (BufferAllocation::Slice buffer : result_buffers) {\n     result_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n+    VLOG(5) << \"result buffer: \" << buffer.ToString()\n+            << \" addr: \" << result_se_buffers.back().opaque();\n   }\n \n   se::DeviceMemoryBase scratch =\n-      buffer_allocations.GetDeviceAddress(scratch_buffer_);\n-\n-  bool runner_created = false;\n-  RunConvOptions opts;\n-  opts.runner_cache = &GetOrCreateRunner(params.stream, &runner_created);\n+      buffer_allocations.GetDeviceAddress(scratch_buffer);\n+  VLOG(5) << \"scratch buffer: \" << scratch_buffer\n+          << \" addr: \" << scratch.opaque();\n \n-  if (runner_created && params.stream->parent()\n+  auto [opts, runner_created] = cache.GetOrCreate(config, stream);\n+  if (runner_created && stream->parent()\n                             ->GetDeviceDescription()\n                             .gpu_compute_capability()\n                             .IsRocm()) {\n     TF_ASSIGN_OR_RETURN(\n         GpuConvParams conv_params,\n-        GetGpuConvParams(config_, operand_se_buffers, result_se_buffers));\n+        GetGpuConvParams(config, operand_se_buffers, result_se_buffers));\n \n     TF_ASSIGN_OR_RETURN(se::dnn::DataType input_type,\n-                        GetDNNDataTypeFromPrimitiveType(config_.input_type));\n+                        GetDNNDataTypeFromPrimitiveType(config.input_type));\n \n     TF_ASSIGN_OR_RETURN(se::dnn::DataType output_type,\n-                        GetDNNDataTypeFromPrimitiveType(config_.output_type));\n+                        GetDNNDataTypeFromPrimitiveType(config.output_type));\n \n-    TF_ASSIGN_OR_RETURN(auto dnn,\n-                        se::dnn::internal::GetDnnFromStream(params.stream));\n+    TF_ASSIGN_OR_RETURN(auto dnn, se::dnn::internal::GetDnnFromStream(stream));\n     se::OwningScratchAllocator<> scratch_allocator(\n         buffer_allocations.device_ordinal(),\n         buffer_allocations.memory_allocator());\n \n     std::vector<se::dnn::ProfileResult> profile_results;\n     dnn->GetMIOpenConvolveAlgorithms(\n-        CudnnConvKindToProto(config_.kind), input_type, output_type,\n-        params.stream, config_.input_descriptor, conv_params.input_buf,\n-        config_.filter_descriptor, conv_params.filter_buf,\n-        config_.output_descriptor, conv_params.output_buf, config_.conv_desc,\n+        CudnnConvKindToProto(config.kind), input_type, output_type, stream,\n+        config.input_descriptor, conv_params.input_buf,\n+        config.filter_descriptor, conv_params.filter_buf,\n+        config.output_descriptor, conv_params.output_buf, config.conv_desc,\n         &scratch_allocator, &profile_results);\n   }\n-\n-  TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),\n+  TF_RETURN_IF_ERROR(RunGpuConv(config, absl::MakeSpan(operand_se_buffers),\n                                 absl::MakeSpan(result_se_buffers), scratch,\n-                                params.stream, opts));\n+                                stream, opts));\n \n   // Note: Convolution has a tuple buffer as an output, but we don't need to\n   // populate it as no one should be reading from the tuple directly.\n-  if (!params.stream->ok()) {\n+  if (!stream->ok()) {\n     return Internal(\"ConvolutionThunk::ExecuteOnStream failed.\");\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "1ceb3e99d74478a3c2cbc4d7892f4a5dd99e48f0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.h",
            "status": "modified",
            "additions": 36,
            "deletions": 8,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -33,12 +33,40 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+struct ConvRunnerCache {\n+  ConvRunnerCache() = default;\n+  ConvRunnerCache(const ConvRunnerCache&) = delete;\n+  ConvRunnerCache& operator=(const ConvRunnerCache&) = delete;\n+\n+  std::pair<RunConvOptions, bool> GetOrCreate(const GpuConvConfig& config,\n+                                              const se::Stream* stream);\n+\n+ private:\n+  absl::Mutex mu_;\n+  absl::flat_hash_map<const se::StreamExecutor*,\n+                      std::unique_ptr<GenericConvRunner>>\n+      cache_ ABSL_GUARDED_BY(mu_);\n+};\n+\n+absl::Status RunConvolutionOnStream(\n+    const Thunk::ExecuteParams& params,\n+    const std::vector<BufferAllocation::Slice>& operand_buffers,\n+    const std::vector<BufferAllocation::Slice>& result_buffers,\n+    const BufferAllocation::Slice& scratch_buffer, const GpuConvConfig& config,\n+    ConvRunnerCache& cache, se::Stream* stream);\n+\n+// Forward declaration needed to initialize ConvolutionCmd with ConvolutionThunk\n+// members.\n+class ConvolutionCmd;\n+\n // This class stores everything that StreamExecutor needs to launch a DNN\n // convolution. It is generated by IrEmitter.\n //\n // This is thread-compatible.\n class ConvolutionThunk : public Thunk {\n  public:\n+  friend class ConvolutionCmd;\n+\n   // Constructs a thunk for launching a DNN convolution.\n   //\n   // operand_slices should be in the same order as cudnn_call->operands().\n@@ -51,7 +79,12 @@ class ConvolutionThunk : public Thunk {\n   ConvolutionThunk(const ConvolutionThunk&) = delete;\n   ConvolutionThunk& operator=(const ConvolutionThunk&) = delete;\n \n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override {\n+    VLOG(5) << \"ConvolutionThunk\";\n+    return RunConvolutionOnStream(params, operand_buffers_, result_buffers_,\n+                                  scratch_buffer_, config_, cache_,\n+                                  params.stream);\n+  }\n \n   static absl::StatusOr<std::unique_ptr<ConvolutionThunk>> FromProto(\n       ThunkInfo thunk_info, const ConvolutionThunkProto& proto,\n@@ -66,22 +99,17 @@ class ConvolutionThunk : public Thunk {\n                    std::vector<BufferAllocation::Slice> result_slices,\n                    BufferAllocation::Slice scratch_slice);\n \n+ protected:\n   std::vector<BufferAllocation::Slice> operand_buffers_;\n   std::vector<BufferAllocation::Slice> result_buffers_;\n   BufferAllocation::Slice scratch_buffer_;\n-  GenericConvRunner& GetOrCreateRunner(const stream_executor::Stream* stream,\n-                                       bool* runner_created);\n-\n   // Technically this is only needed during initialization to create the\n   // GpuConvConfig, but the actual GpuConvConfig is hard to serialize. So we\n   // keep the descriptor around for serialization purposes.\n   const GpuConvDescriptor descriptor_;\n   // Convolution config\n   const GpuConvConfig config_;\n-  absl::Mutex mu_;\n-  absl::flat_hash_map<const stream_executor::Stream*,\n-                      std::unique_ptr<GenericConvRunner>>\n-      runner_cache_ ABSL_GUARDED_BY(mu_);\n+  ConvRunnerCache cache_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "a7f37c0ef126a885e6b45bb2db1538d672f59326",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cudnn_thunk_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcudnn_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcudnn_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcudnn_thunk_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -25,14 +25,12 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n \n namespace xla::gpu {\n namespace {\n using tsl::proto_testing::EqualsProto;\n-using tsl::testing::IsOkAndHolds;\n \n TEST(CuDnnThunkTest, TestSerializationDeserialization) {\n   CudnnThunkProto cudnn_thunk_proto;"
        },
        {
            "sha": "87250c700c4fc196ed57f214db85a87c9746c8c1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk.cc",
            "status": "added",
            "additions": 176,
            "deletions": 0,
            "changes": 176,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,176 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n+\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/memory/memory.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/print_buffer_contents.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/runtime/buffer_use.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/kernels/custom_kernel.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+CustomKernelThunk::CustomKernelThunk(\n+    Thunk::ThunkInfo thunk_info, CustomKernel custom_kernel,\n+    const emitters::KernelArguments& kernel_arguments)\n+    : Thunk(Kind::kCustomKernel, std::move(thunk_info)),\n+      args_(kernel_arguments.GetArgumentBufferSlices()),\n+      args_shape_(kernel_arguments.GetArgumentBufferShapes()),\n+      written_(kernel_arguments.GetArgumentOutputFlags()),\n+      custom_kernel_(std::move(custom_kernel)) {}\n+\n+std::string CustomKernelThunk::ToString(int indent) const {\n+  return custom_kernel_.ToString();\n+}\n+\n+absl::Status CustomKernelThunk::Initialize(const InitializeParams& params) {\n+  absl::MutexLock lock(mutex_);\n+\n+  if (!kernel_cache_.contains(params.executor)) {\n+    TF_ASSIGN_OR_RETURN(\n+        std::unique_ptr<se::Kernel> kernel,\n+        params.executor->LoadKernel(custom_kernel_.kernel_spec()));\n+    kernel_cache_.emplace(params.executor, std::move(kernel));\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+absl::Status CustomKernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n+  se::StreamExecutor* executor = params.stream->parent();\n+\n+  se::Kernel* kernel = [&] {\n+    absl::MutexLock lock(mutex_);\n+    return kernel_cache_[executor].get();\n+  }();\n+\n+  int device_ordinal = executor->device_ordinal();\n+  VLOG(3) << \"[\" << device_ordinal << \"] Launching \"\n+          << custom_kernel_.ToString() << \" as device kernel \"\n+          << kernel->name();\n+\n+  absl::InlinedVector<se::DeviceMemoryBase, 4> buffer_args;\n+  for (const BufferAllocation::Slice& arg : args_) {\n+    se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n+    VLOG(3) << \"[\" << device_ordinal << \"]  Arg: alloc #\" << arg.index()\n+            << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n+            << buf.size() << \"B)\";\n+    buffer_args.push_back(buf);\n+  }\n+\n+  if (VLOG_IS_ON(100)) {\n+    absl::InlinedVector<se::KernelArgument, 4> kernel_args;\n+    for (const se::DeviceMemoryBase& arg : buffer_args) {\n+      kernel_args.push_back(arg);\n+    }\n+    PrintBufferContents(params.stream, kernel_args);\n+  }\n+\n+  se::KernelArgsDeviceMemoryArray args(buffer_args,\n+                                       custom_kernel_.shared_memory_bytes());\n+\n+  return kernel->Launch(custom_kernel_.thread_dims(),\n+                        custom_kernel_.block_dims(),\n+                        custom_kernel_.cluster_dims(), params.stream, args);\n+}\n+\n+Thunk::BufferUses CustomKernelThunk::buffer_uses() const {\n+  Thunk::BufferUses buffers;\n+  buffers.reserve(args_.size());\n+  for (int i = 0; i < args_.size(); ++i) {\n+    // We assume that any buffer is either an input or an output of the\n+    // kernel, and inout buffers are represented as 2 separate arguments.\n+    if (written_[i]) {\n+      buffers.push_back(BufferUse::Write(args_[i], args_shape_[i]));\n+    } else {\n+      buffers.push_back(BufferUse::Read(args_[i], args_shape_[i]));\n+    }\n+  }\n+  return buffers;\n+}\n+\n+CustomKernelThunk::CustomKernelThunk(Thunk::ThunkInfo thunk_info,\n+                                     CustomKernel custom_kernel,\n+                                     std::vector<BufferAllocation::Slice> args,\n+                                     std::vector<bool> written)\n+    : Thunk(Kind::kCustomKernel, std::move(thunk_info)),\n+      args_(std::move(args)),\n+      written_(std::move(written)),\n+      custom_kernel_(std::move(custom_kernel)) {}\n+\n+absl::StatusOr<ThunkProto> CustomKernelThunk::ToProto() const {\n+  ThunkProto thunk_proto;\n+  *thunk_proto.mutable_thunk_info() = thunk_info().ToProto();\n+\n+  CustomKernelThunkProto* custom_kernel_thunk_proto =\n+      thunk_proto.mutable_custom_kernel_thunk();\n+  for (const BufferAllocation::Slice& arg : args_) {\n+    TF_ASSIGN_OR_RETURN(*custom_kernel_thunk_proto->add_args(), arg.ToProto());\n+  }\n+  for (bool written : written_) {\n+    custom_kernel_thunk_proto->add_written(written);\n+  }\n+  TF_ASSIGN_OR_RETURN(*custom_kernel_thunk_proto->mutable_custom_kernel(),\n+                      custom_kernel_.ToProto());\n+  return thunk_proto;\n+}\n+\n+absl::StatusOr<std::unique_ptr<CustomKernelThunk>> CustomKernelThunk::FromProto(\n+    ThunkInfo thunk_info, const CustomKernelThunkProto& proto,\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    const std::optional<se::KernelLoaderSpec::SymbolResolver>&\n+        symbol_resolver) {\n+  TF_ASSIGN_OR_RETURN(\n+      CustomKernel custom_kernel,\n+      CustomKernel::FromProto(proto.custom_kernel(), symbol_resolver));\n+  std::vector<BufferAllocation::Slice> args;\n+  args.reserve(proto.args_size());\n+  for (const buffer_assignment::BufferAllocationSliceProto& arg_proto :\n+       proto.args()) {\n+    TF_ASSIGN_OR_RETURN(\n+        args.emplace_back(),\n+        BufferAllocation::Slice::FromProto(arg_proto, buffer_allocations));\n+  }\n+  std::vector<bool> written{proto.written().begin(), proto.written().end()};\n+  return absl::WrapUnique(new CustomKernelThunk(std::move(thunk_info),\n+                                                std::move(custom_kernel), args,\n+                                                std::move(written)));\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "8a74dc994a1da641c69a834c46909131efc919c6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk.h",
            "status": "added",
            "additions": 109,
            "deletions": 0,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,109 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_CUSTOM_KERNEL_THUNK_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_CUSTOM_KERNEL_THUNK_H_\n+\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/kernels/custom_kernel.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// CustomKernelThunk loads and executes kernels defined by a custom kernel\n+// (which in practice means hand written CUDA C++ kernel), instead of a kernel\n+// compiled by XLA and loaded from an executable source.\n+class CustomKernelThunk : public Thunk {\n+ public:\n+  CustomKernelThunk(Thunk::ThunkInfo thunk_info, CustomKernel custom_kernel,\n+                    const emitters::KernelArguments& kernel_arguments);\n+\n+  std::string ToString(int indent) const override;\n+\n+  absl::Status Initialize(const InitializeParams& params) override;\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+\n+  const CustomKernel& custom_kernel() const { return custom_kernel_; }\n+\n+  const std::vector<BufferAllocation::Slice>& arguments() const {\n+    return args_;\n+  }\n+\n+  absl::string_view custom_kernel_name() const { return custom_kernel_.name(); }\n+\n+  const std::vector<bool>& written() const { return written_; }\n+\n+  LaunchDimensions launch_dimensions() const {\n+    return LaunchDimensions(custom_kernel_.block_dims(),\n+                            custom_kernel_.thread_dims());\n+  }\n+\n+  int64_t shmem_bytes() const { return custom_kernel_.shared_memory_bytes(); }\n+\n+  BufferUses buffer_uses() const override;\n+\n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n+  static absl::StatusOr<std::unique_ptr<CustomKernelThunk>> FromProto(\n+      ThunkInfo thunk_info, const CustomKernelThunkProto& proto,\n+      absl::Span<const BufferAllocation> buffer_allocations,\n+      const std::optional<se::KernelLoaderSpec::SymbolResolver>&\n+          symbol_resolver = std::nullopt);\n+\n+ private:\n+  // Private constructor for deserialization.\n+  CustomKernelThunk(Thunk::ThunkInfo thunk_info, CustomKernel custom_kernel,\n+                    std::vector<BufferAllocation::Slice> args,\n+                    std::vector<bool> written);\n+\n+  // Buffer slices passed to the kernel as arguments.\n+  std::vector<BufferAllocation::Slice> args_;\n+  std::vector<Shape> args_shape_;\n+\n+  // args_[i] is written iff (written_[i] == true).\n+  std::vector<bool> written_;\n+\n+  CustomKernel custom_kernel_;\n+\n+  // Loaded kernels for each `StreamExecutor`.\n+  mutable absl::Mutex mutex_;\n+  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<se::Kernel>>\n+      kernel_cache_ ABSL_GUARDED_BY(mutex_);\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_CUSTOM_KERNEL_THUNK_H_"
        },
        {
            "sha": "9bcfd5db9d21a92ac6a10deb33b126536abeae28",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk_test.cc",
            "status": "added",
            "additions": 167,
            "deletions": 0,
            "changes": 167,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -0,0 +1,167 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/runtime/buffer_use.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/kernels/custom_kernel.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+using absl_testing::IsOkAndHolds;\n+using ::testing::Field;\n+using ::testing::Optional;\n+using tsl::proto_testing::EqualsProto;\n+using tsl::proto_testing::ParseTextProtoOrDie;\n+\n+TEST(CustomKernelThunkTest, BufferUsesReturnsCorrectBuffers) {\n+  Shape arg_shape = ShapeUtil::MakeShape(F32, {512});\n+  CustomKernel kernel(\n+      /*name=*/\"\",\n+      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n+          /*ptx=*/\"\", /*kernel_name=*/\"\", /*arity=*/0),\n+      se::BlockDim(), se::ThreadDim(), /*shared_memory_bytes=*/0);\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n+  BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n+  emitters::KernelArgument arg0(arg_shape, slice0);\n+  emitters::KernelArgument arg1(arg_shape, slice1);\n+  arg0.set_written(false);\n+  arg1.set_written(true);\n+  emitters::KernelArguments kernel_arguments({arg0, arg1});\n+  CustomKernelThunk thunk(Thunk::ThunkInfo{}, kernel, kernel_arguments);\n+\n+  Thunk::BufferUses buffers = thunk.buffer_uses();\n+\n+  ASSERT_THAT(buffers, testing::UnorderedElementsAre(\n+                           BufferUse::Read(slice0, arg_shape),\n+                           BufferUse::Write(slice1, arg_shape)));\n+}\n+\n+TEST(CustomKernelThunkTest, BufferUsesReturnsBuffersInConsistentOrder) {\n+  CustomKernel kernel(\n+      /*name=*/\"\",\n+      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n+          /*ptx=*/\"\", /*kernel_name=*/\"\", /*arity=*/0),\n+      se::BlockDim(), se::ThreadDim(), /*shared_memory_bytes=*/0);\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n+  BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n+  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n+  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n+  arg0.set_written(false);\n+  arg1.set_written(true);\n+  emitters::KernelArguments kernel_arguments({arg0, arg1});\n+  CustomKernelThunk thunk(Thunk::ThunkInfo{}, kernel, kernel_arguments);\n+\n+  Thunk::BufferUses buffers1 = thunk.buffer_uses();\n+  Thunk::BufferUses buffers2 = thunk.buffer_uses();\n+\n+  ASSERT_THAT(buffers1, testing::ContainerEq(buffers2));\n+}\n+\n+TEST(CustomKernelThunkTest, ToProto) {\n+  CustomKernel kernel(\"name\",\n+                      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n+                          \"PTX\", \"kernel_name\", /*arity=*/1),\n+                      se::BlockDim(3, 2, 1), se::ThreadDim(4, 5, 6),\n+                      /*shared_memory_bytes=*/42);\n+\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.profile_annotation = \"profile_annotation\";\n+  thunk_info.execution_stream_id = 7;\n+  thunk_info.thunk_id = 42;\n+\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n+  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n+  arg0.set_written(true);\n+  emitters::KernelArguments kernel_arguments({arg0});\n+  CustomKernelThunk thunk(thunk_info, kernel, kernel_arguments);\n+\n+  EXPECT_THAT(thunk.ToProto(), IsOkAndHolds(EqualsProto(R\"pb(\n+                thunk_info {\n+                  profile_annotation: \"profile_annotation\"\n+                  execution_stream_id: 7\n+                  thunk_id: 42\n+                }\n+                custom_kernel_thunk {\n+                  custom_kernel {\n+                    name: \"name\"\n+                    kernel_spec {\n+                      kernel_name: \"kernel_name\"\n+                      ptx { data: \"PTX\" }\n+                      arity: 1\n+                    }\n+                    block_dims { coordinates { x: 3, y: 2, z: 1 } }\n+                    thread_dims { coordinates { x: 4, y: 5, z: 6 } }\n+                    shared_memory_bytes: 42\n+                  }\n+                  args { buffer_allocation_index: 0, offset: 0, size: 512 }\n+                  written: true\n+                }\n+              )pb\")));\n+}\n+\n+TEST(CustomKernelThunkTest, FromProto) {\n+  CustomKernelThunkProto proto = ParseTextProtoOrDie<CustomKernelThunkProto>(\n+      R\"pb(\n+        custom_kernel {\n+          name: \"test_kernel\"\n+          kernel_spec {\n+            ptx { data: \"PTX\" }\n+            arity: 1\n+          }\n+          block_dims { coordinates { x: 1, y: 1, z: 1 } }\n+          thread_dims { coordinates { x: 1, y: 1, z: 1 } }\n+          shared_memory_bytes: 42\n+        }\n+        args { buffer_allocation_index: 0, offset: 0, size: 1024 }\n+        written: true\n+      )pb\");\n+\n+  std::vector<BufferAllocation> buffer_allocations;\n+  buffer_allocations.emplace_back(/*index=*/0, /*size=*/1024, /*color=*/0);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<CustomKernelThunk> thunk,\n+                          CustomKernelThunk::FromProto(\n+                              Thunk::ThunkInfo{}, proto, buffer_allocations));\n+\n+  EXPECT_THAT(thunk->custom_kernel().name(), \"test_kernel\");\n+  EXPECT_THAT(thunk->arguments(), testing::ElementsAre(BufferAllocation::Slice(\n+                                      &buffer_allocations[0], /*offset=*/0,\n+                                      /*size=*/1024)));\n+  EXPECT_THAT(thunk->written(), testing::ElementsAre(true));\n+  EXPECT_THAT(thunk->custom_kernel().kernel_spec().cuda_ptx_in_memory(),\n+              Optional(Field(&se::CudaPtxInMemory::ptx, \"PTX\")));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "7af58fec0956fa14ebc019107466c802d8982719",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 144,
            "changes": 168,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -15,13 +15,11 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n \n-#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <optional>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/container/inlined_vector.h\"\n@@ -32,20 +30,19 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"xla/backends/gpu/runtime/print_buffer_contents.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -60,27 +57,6 @@ using tsl::profiler::TraceMeLevel;\n namespace xla {\n namespace gpu {\n \n-Thunk::BufferUses BufferUseFromKernelArguments(\n-    absl::Span<const BufferAllocation::Slice> args,\n-    const std::vector<bool>& written) {\n-  Thunk::BufferUses buffers;\n-  buffers.reserve(args.size());\n-  for (int i = 0; i < args.size(); ++i) {\n-    // We assume that any buffer is either an input or an output of the\n-    // kernel, and inout buffers are represented as 2 separate arguments.\n-    if (written[i]) {\n-      buffers.push_back(BufferUse::Write(args[i]));\n-    } else {\n-      buffers.push_back(BufferUse::Read(args[i]));\n-    }\n-  }\n-  return buffers;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// KernelThunk\n-//===----------------------------------------------------------------------===//\n-\n KernelThunk::KernelThunk(Thunk::ThunkInfo thunk_info, std::string kernel_name,\n                          const emitters::KernelArguments& kernel_arguments,\n                          LaunchDimensions launch_dimensions,\n@@ -89,6 +65,7 @@ KernelThunk::KernelThunk(Thunk::ThunkInfo thunk_info, std::string kernel_name,\n                          stream_executor::gpu::TmaMetadata tma_metadata)\n     : Thunk(Kind::kKernel, std::move(thunk_info)),\n       args_(kernel_arguments.GetArgumentBufferSlices()),\n+      args_shape_(kernel_arguments.GetArgumentBufferShapes()),\n       written_(kernel_arguments.GetArgumentOutputFlags()),\n       kernel_name_(std::move(kernel_name)),\n       launch_dimensions_(std::move(launch_dimensions)),\n@@ -108,11 +85,10 @@ absl::StatusOr<ThunkProto> KernelThunk::ToProto() const {\n   *proto.mutable_thunk_info() = thunk_info().ToProto();\n \n   auto* kernel_proto = proto.mutable_kernel_thunk();\n-  for (const auto& arg : args_) {\n-    TF_ASSIGN_OR_RETURN(*kernel_proto->add_args(), arg.ToProto());\n-  }\n-  for (bool written : written_) {\n-    kernel_proto->add_written(written);\n+  for (int i = 0; i < args_.size(); i++) {\n+    TF_ASSIGN_OR_RETURN(*kernel_proto->add_args(), args_[i].ToProto());\n+    *kernel_proto->add_args_shape() = args_shape_[i].ToProto();\n+    kernel_proto->add_written(written_[i]);\n   }\n   kernel_proto->set_kernel_name(kernel_name_);\n   *kernel_proto->mutable_launch_dimensions() = launch_dimensions_.ToProto();\n@@ -136,9 +112,11 @@ absl::StatusOr<std::unique_ptr<KernelThunk>> KernelThunk::FromProto(\n         stream_executor::ClusterDim::FromProto(proto.cluster_dim()));\n   }\n \n-  if (proto.written().size() != proto.args().size()) {\n+  if (proto.written().size() != proto.args().size() ||\n+      proto.args().size() != proto.args_shape().size()) {\n     return absl::InvalidArgumentError(\n-        \"Proto fields `written` and `args` need to have the same cardinality.\");\n+        \"Proto fields `written`, `args` and `args_shape` need to have the same \"\n+        \"cardinality.\");\n   }\n \n   std::vector<emitters::KernelArgument> arguments;\n@@ -147,7 +125,9 @@ absl::StatusOr<std::unique_ptr<KernelThunk>> KernelThunk::FromProto(\n     TF_ASSIGN_OR_RETURN(BufferAllocation::Slice slice,\n                         BufferAllocation::Slice::FromProto(proto.args().at(i),\n                                                            buffer_allocations));\n-    emitters::KernelArgument argument{Shape{}, slice};\n+    TF_ASSIGN_OR_RETURN(Shape shape,\n+                        Shape::FromProto(proto.args_shape().at(i)));\n+    emitters::KernelArgument argument{shape, slice};\n     argument.set_written(proto.written().at(i));\n     arguments.push_back(std::move(argument));\n   }\n@@ -189,45 +169,6 @@ absl::Status KernelThunk::Initialize(const InitializeParams& params) {\n   return absl::OkStatus();\n }\n \n-void PrintBufferContents(se::Stream*, int input_idx, se::TensorMap tensor_map) {\n-  VLOG(100) << \"TENSOR_MAP(\" << input_idx << \") = \";\n-  for (std::byte element : tensor_map.storage) {\n-    VLOG(100) << absl::StrFormat(\"%x \", static_cast<unsigned>(element));\n-  }\n-}\n-\n-void PrintBufferContents(se::Stream* stream, int input_idx,\n-                         se::DeviceMemoryBase buf) {\n-  auto host_buffer = std::make_unique<char[]>(buf.size());\n-  CHECK_OK(stream->Memcpy(host_buffer.get(), buf, buf.size()));\n-  CHECK_OK(stream->BlockHostUntilDone());\n-\n-  std::string buffer_contents;\n-  for (int i = 0; i < buf.size(); ++i) {\n-    absl::StrAppendFormat(&buffer_contents, \"%x \",\n-                          static_cast<unsigned>(host_buffer[i]));\n-  }\n-  VLOG(100) << \"BUF(\" << input_idx << \") = \" << buffer_contents;\n-}\n-\n-void PrintBufferContents(se::Stream*, int input_idx, int64_t int_arg) {\n-  VLOG(100) << \"INT(\" << input_idx << \") = \";\n-  VLOG(100) << absl::StrFormat(\"%x \", int_arg);\n-}\n-\n-static void PrintBufferContents(\n-    se::Stream* stream, absl::Span<const se::KernelArgument> kernel_args) {\n-  for (const auto& [input_idx, arg] : llvm::enumerate(kernel_args)) {\n-    // pre-cpp-20-compat(P0588R1): Capturing structured bindings in lambdas is\n-    // ill-formed.\n-    std::visit(\n-        [&stream, &input_idx = input_idx](auto const& arg) {\n-          PrintBufferContents(stream, input_idx, arg);\n-        },\n-        arg);\n-  }\n-}\n-\n absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n   TraceMe trace(\n       [] { return TraceMeEncode(\"KernelThunk::ExecuteOnStream\", {}); },\n@@ -308,79 +249,18 @@ absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n }\n \n Thunk::BufferUses KernelThunk::buffer_uses() const {\n-  return BufferUseFromKernelArguments(absl::MakeConstSpan(args_), written_);\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// CustomKernelThunk\n-//===----------------------------------------------------------------------===//\n-\n-CustomKernelThunk::CustomKernelThunk(\n-    const HloInstruction* instr, CustomKernel custom_kernel,\n-    const emitters::KernelArguments& kernel_arguments, ThunkId thunk_id)\n-    : Thunk(Kind::kCustomKernel,\n-            Thunk::ThunkInfo::WithProfileAnnotation(instr, thunk_id)),\n-      args_(kernel_arguments.GetArgumentBufferSlices()),\n-      written_(kernel_arguments.GetArgumentOutputFlags()),\n-      custom_kernel_(std::move(custom_kernel)) {}\n-\n-std::string CustomKernelThunk::ToString(int indent) const {\n-  return custom_kernel_.ToString();\n-}\n-\n-absl::Status CustomKernelThunk::Initialize(const InitializeParams& params) {\n-  absl::MutexLock lock(mutex_);\n-\n-  if (!kernel_cache_.contains(params.executor)) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<se::Kernel> kernel,\n-        params.executor->LoadKernel(custom_kernel_.kernel_spec()));\n-    kernel_cache_.emplace(params.executor, std::move(kernel));\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n-absl::Status CustomKernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n-  se::StreamExecutor* executor = params.stream->parent();\n-\n-  se::Kernel* kernel = [&] {\n-    absl::MutexLock lock(mutex_);\n-    return kernel_cache_[executor].get();\n-  }();\n-\n-  int device_ordinal = executor->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal << \"] Launching \"\n-          << custom_kernel_.ToString() << \" as device kernel \"\n-          << kernel->name();\n-\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> buffer_args;\n-  for (const BufferAllocation::Slice& arg : args_) {\n-    se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n-    VLOG(3) << \"[\" << device_ordinal << \"]  Arg: alloc #\" << arg.index()\n-            << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n-            << buf.size() << \"B)\";\n-    buffer_args.push_back(buf);\n-  }\n-\n-  if (VLOG_IS_ON(100)) {\n-    absl::InlinedVector<se::KernelArgument, 4> kernel_args;\n-    for (const se::DeviceMemoryBase& arg : buffer_args) {\n-      kernel_args.push_back(arg);\n+  Thunk::BufferUses buffers;\n+  buffers.reserve(args_.size());\n+  for (int i = 0; i < args_.size(); ++i) {\n+    // We assume that any buffer is either an input or an output of the\n+    // kernel, and inout buffers are represented as 2 separate arguments.\n+    if (written_[i]) {\n+      buffers.push_back(BufferUse::Write(args_[i], args_shape_[i]));\n+    } else {\n+      buffers.push_back(BufferUse::Read(args_[i], args_shape_[i]));\n     }\n-    PrintBufferContents(params.stream, kernel_args);\n   }\n-\n-  se::KernelArgsDeviceMemoryArray args(buffer_args,\n-                                       custom_kernel_.shared_memory_bytes());\n-\n-  return kernel->Launch(custom_kernel_.thread_dims(),\n-                        custom_kernel_.block_dims(),\n-                        custom_kernel_.cluster_dims(), params.stream, args);\n-}\n-\n-Thunk::BufferUses CustomKernelThunk::buffer_uses() const {\n-  return BufferUseFromKernelArguments(absl::MakeConstSpan(args_), written_);\n+  return buffers;\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "9d0f87152ded45d324731700017db78ee0cca926",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 64,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de857f9a71f934ca661a9d325dafb225869fc6b0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h?ref=de857f9a71f934ca661a9d325dafb225869fc6b0",
            "patch": "@@ -26,17 +26,14 @@ limitations under the License.\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/shape.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -46,19 +43,13 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n-class GpuExecutable;\n-\n // TODO(ezhulenev): Unify KernelThunk and CustomKernelThunk as they are very\n // similar. XLA:GPU should use more of kernel loading APIs provided by\n // StreamExecutor out of the box and less custom kernel loading solutions.\n //\n // Today KernelThunk is required for lowering to XLA runtime, and\n // CustomKernelThunk is only supported for thunk execution.\n-\n-//===----------------------------------------------------------------------===//\n-// KernelThunk\n-//===----------------------------------------------------------------------===//\n-\n+//\n // This class stores everything that StreamExecutor needs for launching a\n // kernel. It implements the ExecuteOnStream interface for GpuExecutable to\n // invoke the corresponding kernel.\n@@ -116,7 +107,7 @@ class KernelThunk : public Thunk {\n  private:\n   // Buffer slices passed to the kernel as arguments.\n   std::vector<BufferAllocation::Slice> args_;\n-\n+  std::vector<Shape> args_shape_;\n   // args_[i] is written iff (written_[i] == true).\n   std::vector<bool> written_;\n \n@@ -141,58 +132,6 @@ class KernelThunk : public Thunk {\n       kernel_cache_ ABSL_GUARDED_BY(mutex_);\n };\n \n-//===----------------------------------------------------------------------===//\n-// CustomKernelThunk\n-//===----------------------------------------------------------------------===//\n-\n-// CustomKernelThunk loads and executes kernels defined by a custom kernel\n-// (which in practice means hand written CUDA C++ kernel), instead of a kernel\n-// compiled by XLA and loaded from an executable source.\n-class CustomKernelThunk : public Thunk {\n- public:\n-  CustomKernelThunk(const HloInstruction* inst, CustomKernel custom_kernel,\n-                    const emitters::KernelArguments& kernel_arguments,\n-                    ThunkId thunk_id);\n-\n-  std::string ToString(int indent) const override;\n-\n-  absl::Status Initialize(const InitializeParams& params) override;\n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n-\n-  const CustomKernel& custom_kernel() const { return custom_kernel_; }\n-\n-  const std::vector<BufferAllocation::Slice>& arguments() const {\n-    return args_;\n-  }\n-\n-  absl::string_view custom_kernel_name() const { return custom_kernel_.name(); }\n-\n-  const std::vector<bool>& written() const { return written_; }\n-\n-  LaunchDimensions launch_dimensions() const {\n-    return LaunchDimensions(custom_kernel_.block_dims(),\n-                            custom_kernel_.thread_dims());\n-  }\n-\n-  int64_t shmem_bytes() const { return custom_kernel_.shared_memory_bytes(); }\n-\n-  BufferUses buffer_uses() const override;\n-\n- private:\n-  // Buffer slices passed to the kernel as arguments.\n-  std::vector<BufferAllocation::Slice> args_;\n-\n-  // args_[i] is written iff (written_[i] == true).\n-  std::vector<bool> written_;\n-\n-  CustomKernel custom_kernel_;\n-\n-  // Loaded kernels for each `StreamExecutor`.\n-  mutable absl::Mutex mutex_;\n-  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<se::Kernel>>\n-      kernel_cache_ ABSL_GUARDED_BY(mutex_);\n-};\n-\n }  // namespace gpu\n }  // namespace xla\n "
        }
    ],
    "stats": {
        "total": 25454,
        "additions": 15437,
        "deletions": 10017
    }
}