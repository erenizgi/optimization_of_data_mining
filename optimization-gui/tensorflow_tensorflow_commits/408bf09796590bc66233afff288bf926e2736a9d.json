{
    "author": "thcmbs",
    "message": "[XLA:GPU]Disable TransposeDimensionGrouper pass and replace it with OTF normalization in emitters\n\n0) Fix a bug (?) in normalization util when normalized dim contains a single dimension\n1) Perform normalization OTF for Transpose emitter selection\n2) Use normalized shape for unrolling decision in kLoop emitter\n3) Use normalized shape to detect slow transposes in triton fusion rewriter\n\nPiperOrigin-RevId: 846191206",
    "sha": "408bf09796590bc66233afff288bf926e2736a9d",
    "files": [
        {
            "sha": "127b0d60a72832e468abc81e8d30a6ba038d2f33",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/triton_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 5,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -562,7 +562,10 @@ ENTRY e {\n                                ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n-TEST_F(TritonGemmTest, SplitLhsNoncontractingTransposeRhs) {\n+// TODO: b/422676780 - Enable the tests once the indexing maps-based tiling is\n+// deprecated. The test is disabled after we remove TransposeDimensionGrouper\n+// pass, because the infra currently requires grouping of adjacent dimensions.\n+TEST_F(TritonGemmTest, DISABLED_SplitLhsNoncontractingTransposeRhs) {\n   constexpr absl::string_view kHloText = R\"(\n HloModule t\n \n@@ -587,7 +590,10 @@ ENTRY e {\n   EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/0, /*arel=*/0}));\n }\n \n-TEST_F(TritonGemmTest, SplitLhsNoncontracting) {\n+// TODO: b/422676780 - Enable the tests once the indexing maps-based tiling is\n+// deprecated. The test is disabled after we remove TransposeDimensionGrouper\n+// pass, because the infra currently requires grouping of adjacent dimensions.\n+TEST_F(TritonGemmTest, DISABLED_SplitLhsNoncontracting) {\n   constexpr absl::string_view kHloText = R\"(\n ENTRY e {\n   p0 = f32[72,72] parameter(0)\n@@ -1776,12 +1782,17 @@ ENTRY e {\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           GetOptimizedModule(kHloText));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n-      module->entry_computation()->root_instruction(),\n-      GmockMatch(m::Bitcast(\n+      root,\n+      GmockMatch(\n           m::Fusion(m::Fusion(m::Parameter(), m::Parameter())\n                         .WithFusionKind(HloInstruction::FusionKind::kCustom))\n-              .WithFusionKind(HloInstruction::FusionKind::kInput))));\n+              .WithFusionKind(HloInstruction::FusionKind::kInput)));\n+\n+  const HloFusionInstruction* root_fusion = Cast<HloFusionInstruction>(root);\n+  EXPECT_EQ(root_fusion->fused_expression_root()->opcode(),\n+            HloOpcode::kTranspose);\n \n   EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }"
        },
        {
            "sha": "45185f41fab4d0e5fadfdb0332c40cffe19a2ef9",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -1751,7 +1751,6 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       // introduced the nested fusions. We also want to keep it close to the\n       // gemm rewriter to avoid the possibility of new passes to rewrite the\n       // transpose.\n-      pipeline.AddPass<TransposeDimensionGrouper>();\n       pipeline.AddPass<GemmFusion>(gpu_version);\n       pipeline.AddPass<GemmFusionSwapOperands>();\n     } else if (cuda_cc != nullptr &&\n@@ -1779,8 +1778,6 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n     // also have unsorted update_window_dims.\n     pipeline.AddPass<ScatterSimplifier>();\n     pipeline.AddPass<BroadcastCanonicalizer>();\n-    // BroadcastCanonicalizer can create transposes.\n-    pipeline.AddPass<TransposeDimensionGrouper>();\n     pipeline.AddPass<ReductionDegenerateDimRemover>();\n     pipeline.AddPass<ReductionLayoutNormalizer>();\n     // Run Softmax fusion after layout normalization. We expect a default layout"
        },
        {
            "sha": "33abfb08f9faec09861cf592f98d9c9013f051ca",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -1855,34 +1855,6 @@ TEST_F(PassOrderTest, NestGemmFusionRunsAfterHoistFusedBitcasts) {\n   VerifyPassOrder(\"hoist-fused-bitcasts\", \"nest_gemm_fusion\");\n }\n \n-TEST_F(PassOrderTest, TransposeDimensionGrouperRunsBeforeGemmRewriter) {\n-  if (!get_cuda_cc().IsAtLeastAmpere()) {\n-    GTEST_SKIP() << \"triton-gemm-rewriter requires at least Ampere to run.\";\n-  }\n-  if (!optimized_module_) {\n-    CompileModule(GetModuleConfigForTest());\n-  }\n-  // DebugOptions options = GetDebugOptionsForTest();\n-  // options.set_xla_gpu_enable_triton_gemm(true);\n-  // SetDebugOptions(options);\n-  // Verify that transpose-dimension-grouper runs immediately before\n-  // triton-gemm-rewriter. We want to keep them close together to avoid the\n-  // possibility of new passes to rewrite the transpose and make it\n-  // not compatible with the generic triton emitter.\n-  // Simple VerifyPassOrder does not work here as we want to check that passes\n-  // are run next to each other, also transpose-dimension-grouper runs one more\n-  // time after the gemm rewriter.\n-  CHECK(optimized_module_);\n-  std::string previous_pass_name;\n-  for (const HloPassMetadata& pass_metadata :\n-       optimized_module_->metadata().proto().pass_metadata()) {\n-    if (pass_metadata.pass_name() == \"triton-gemm-rewriter\") {\n-      EXPECT_EQ(previous_pass_name, \"transpose-dimension-grouper\");\n-    }\n-    previous_pass_name = pass_metadata.pass_name();\n-  }\n-}\n-\n TEST_F(PassOrderTest,\n        ReducePrecisionIsRemovedAfterAllCallsToSimplifyFPConversions) {\n   // Because of an issue with JAX remat and `SimplifyFPConversions` (see PR:"
        },
        {
            "sha": "e5b7f65cab6f699bebf1bc1aa69a9be0812e74da",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -63,7 +63,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[] constant({...})\\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n+  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[] constant({...})\\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\\n  tmp_5 = bf16[4,32,1024,1024]{3,2,1,0} transpose(bf16[4,32,1024,1024]{3,2,1,0} tmp_4), dimensions={0,1,3,2}\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_5)\\n  tmp_7 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_7)\\n  tmp_9 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_6, bf16[128,1024,1024]{2,1,0} tmp_8), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_10 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_9)\\n}\"\n   result {\n     gemm {\n       algorithm: -1\n@@ -183,7 +183,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"{\\n  tmp_0 = bf16[3,32,1024,4,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[3,32768,4,1024]{3,2,1,0} bitcast(bf16[3,32,1024,4,1024]{4,3,2,1,0} tmp_0)\\n  tmp_2 = bf16[3,4,32768,1024]{3,2,1,0} transpose(bf16[3,32768,4,1024]{3,2,1,0} tmp_1), dimensions={0,2,1,3}\\n  tmp_3 = bf16[3,4,32,1024,1024]{4,3,2,1,0} bitcast(bf16[3,4,32768,1024]{3,2,1,0} tmp_2)\\n  tmp_4 = bf16[1,3,32,1024]{3,2,1,0} parameter(1)\\n  tmp_5 = bf16[3,32,1024]{2,1,0} bitcast(bf16[1,3,32,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[3,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[3,32,1024]{2,1,0} tmp_5), dimensions={0,2,3}\\n  tmp_7 = bf16[3,4,32,1024,1024]{4,3,2,1,0} add(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_3, bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_6)\\n  tmp_8 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_7), slice={[1:2], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_7), slice={[0:1], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_10 = bf16[] constant({...})\\n  tmp_11 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_10), dimensions={}\\n  tmp_12 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_11)\\n  tmp_13 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_12)\\n  tmp_14 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_13), dimensions={0,2,1}\\n  ROOT tmp_15 = (bf16[1,4,32,1024,1024]{4,3,2,1,0}, bf16[128,1024,1024]{2,1,0}) tuple(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_14)\\n}\"\n+  hlo: \"{\\n  tmp_0 = bf16[3,32,1024,4,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[3,4,32,1024,1024]{4,3,2,1,0} transpose(bf16[3,32,1024,4,1024]{4,3,2,1,0} tmp_0), dimensions={0,3,1,2,4}\\n  tmp_2 = bf16[1,3,32,1024]{3,2,1,0} parameter(1)\\n  tmp_3 = bf16[3,32,1024]{2,1,0} bitcast(bf16[1,3,32,1024]{3,2,1,0} tmp_2)\\n  tmp_4 = bf16[3,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[3,32,1024]{2,1,0} tmp_3), dimensions={0,2,3}\\n  tmp_5 = bf16[3,4,32,1024,1024]{4,3,2,1,0} add(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_1, bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_4)\\n  tmp_6 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[1:2], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_7 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[0:1], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_8 = bf16[] constant({...})\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_8), dimensions={}\\n  tmp_10 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_7, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_10)\\n  tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} transpose(bf16[4,32,1024,1024]{3,2,1,0} tmp_11), dimensions={0,1,3,2}\\n  ROOT tmp_13 = (bf16[1,4,32,1024,1024]{4,3,2,1,0}, bf16[4,32,1024,1024]{3,2,1,0}) tuple(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_6, bf16[4,32,1024,1024]{3,2,1,0} tmp_12)\\n}\"\n   result {\n     other {\n       name: \"NativeEmitter\""
        },
        {
            "sha": "eab171344752442a361415b25fcd070afe77ba3b",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -64,9 +64,22 @@ bool ContainsTransposeWithSmallMostMinorDim(const HloFusionAdaptor& fusion,\n       return false;\n     }\n     const HloInstruction& transpose = instr.instruction();\n-    // We can assume that TransposeDimensionGrouper pass has run, so no need\n-    // to try to combine adjacent dimensions.\n-    return transpose.shape().dimensions().back() < unroll_factor;\n+    // The kLoop emitter operates on the original transpose, but it handles the\n+    // index calculation. The critical factor for performance (coalescing) is\n+    // the size of the contiguous memory block being accessed in the minor\n+    // dimension. Normalization reveals this true physical dimension size by\n+    // merging adjacent logical dimensions. If this normalized dimension is\n+    // large enough, the unrolled accesses will be coalesced, justifying the\n+    // unroll factor.\n+    absl::InlinedVector<int64_t, 3> permutation;\n+    auto normalized_dims_or = ShapeUtil::GetNormalizedLogicalTransposeShape(\n+        transpose.operand(0)->shape(), transpose.shape(),\n+        transpose.dimensions(), permutation);\n+    if (normalized_dims_or.ok()) {\n+      return normalized_dims_or.value().back() < unroll_factor;\n+    } else {\n+      return transpose.shape().dimensions().back() < unroll_factor;\n+    }\n   });\n }\n "
        },
        {
            "sha": "b19b554d0f4a3a3be4ef2e0537d3d006b7e5209b",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 12,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -603,9 +603,7 @@ TEST_F(GpuFusibleTest, FusionHeroesAreCompatible_TransposeFusionNotCompatible) {\n     fused_computation_1 {\n       p0.1 = f32[64,32]{1,0} parameter(0)\n       neg = f32[64,32]{1,0} negate(p0.1)\n-      bc = f32[1,64,32]{2,1,0} bitcast(neg)\n-      transpose = f32[1,32,64]{2,1,0} transpose(bc), dimensions={0,2,1}\n-      ROOT bc2 = f32[32,64]{1,0} bitcast(transpose)\n+      ROOT transpose = f32[32,64]{1,0} transpose(neg), dimensions={1,0}\n     }\n \n     fused_computation_2 {\n@@ -623,12 +621,10 @@ TEST_F(GpuFusibleTest, FusionHeroesAreCompatible_TransposeFusionNotCompatible) {\n   const HloInstruction* fusion_1 =\n       module->entry_computation()->root_instruction();\n   const HloInstruction* fusion_2 = fusion_1->operand(0);\n-  EXPECT_FALSE(\n-      FusionHeroesAreCompatible(fusion_1->fused_expression_root(),\n-                                fusion_2->fused_expression_root()->operand(0)));\n-  EXPECT_FALSE(\n-      FusionHeroesAreCompatible(fusion_2->fused_expression_root()->operand(0),\n-                                fusion_1->fused_expression_root()));\n+  EXPECT_FALSE(FusionHeroesAreCompatible(fusion_1->fused_expression_root(),\n+                                         fusion_2->fused_expression_root()));\n+  EXPECT_FALSE(FusionHeroesAreCompatible(fusion_2->fused_expression_root(),\n+                                         fusion_1->fused_expression_root()));\n }\n \n TEST_F(GpuFusibleTest, ShapesCompatibleForMultiOutputFusion_LoopFusions) {\n@@ -1310,9 +1306,9 @@ TEST_F(GpuFusibleTest, ChooseFusionKind) {\n HloModule module\n \n ENTRY computation {\n-    p = f32[1,5000,6000]{2,1,0} parameter(0)\n-    c = f32[1,6000,5000]{2,1,0} transpose(p), dimensions={0,2,1}\n-    ROOT r = f32[300,20,5000]{2,1,0} reshape(c)\n+    p = f32[5000,6000]{1,0} parameter(0)\n+    c = f32[6000,5000] transpose(p), dimensions={1,0}\n+    ROOT r = f32[300,20,5000] reshape(c)\n }\n )\")\n                     .value();\n@@ -1802,6 +1798,30 @@ ENTRY main {\n   EXPECT_EQ(config.unroll_factor, 8);\n }\n \n+TEST_F(GpuFusibleTest,\n+       ComputeLoopFusionConfigForLoopTransposeEffectiveLargerMinorDim) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY main {\n+  p0 = f16[256,2048,4,2]{3,2,1,0} parameter(0)\n+  ROOT res = f16[2048,256,4,2]{3,2,1,0} transpose(p0), dimensions={1,0,2,3}\n+}\n+)\"));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  se::DeviceDescription device_info_h100{\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n+  auto analysis = HloFusionAnalysis::Create(*root, device_info_h100);\n+  auto config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+\n+  se::DeviceDescription device_info_b200{\n+      TestGpuDeviceInfo::RTXB200SXMDeviceInfo()};\n+  analysis = HloFusionAnalysis::Create(*root, device_info_b200);\n+  config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 8);\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "6eca640b573d94b87d25cab51d00d3167a61f2a6",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 19,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -245,20 +245,23 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n     return std::nullopt;\n   }\n \n-  // We can assume that TransposeDimensionGrouper pass has run, so no need to\n-  // call GetNormalizedLogicalTransposeShape here.\n-  absl::InlinedVector<int64_t, 3> permutation(hero.dimensions().begin(),\n-                                              hero.dimensions().end());\n+  absl::InlinedVector<int64_t, 3> permutation;\n+  auto normalized_dims_or = ShapeUtil::GetNormalizedLogicalTransposeShape(\n+      hero.operand(0)->shape(), hero.shape(), hero.dimensions(), permutation);\n+  if (!normalized_dims_or.ok()) {\n+    return std::nullopt;\n+  }\n+  auto normalized_dims = normalized_dims_or.value();\n+  auto normalized_operand_dims =\n+      Permute(normalized_dims, InversePermutation(permutation));\n   // A real transpose needs at least 2 transpose dimensions.\n   if (permutation.size() < 2) {\n     return std::nullopt;\n   }\n   auto bit_width = GetBitwidth(hero.shape().element_type());\n-  absl::InlinedVector<int64_t, 3> dimensions(hero.shape().dimensions().begin(),\n-                                             hero.shape().dimensions().end());\n-  int64_t operand_most_minor_dim = hero.operand(0)->shape().dimensions().back();\n+  int64_t operand_most_minor_dim = normalized_operand_dims.back();\n \n-  TransposeDescription desc{&hero, dimensions, permutation,\n+  TransposeDescription desc{&hero, normalized_dims, permutation,\n                             /*shmem_usage=*/0};\n   if (CanEmitPackedTranspose(desc)) {\n     int64_t vector_size =\n@@ -267,27 +270,28 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n         kNumShmemBanks * (kBankBitwidth / 8) * kNumShmemBanks * vector_size;\n     return desc;\n   }\n-  if (permutation.back() == dimensions.size() - 1) {\n+  // Minor dimension is preserved.\n+  if (permutation.back() == normalized_dims.size() - 1) {\n     operand_most_minor_dim =\n-        hero.operand(0)->shape().dimensions(dimensions.size() - 2);\n-    if (bit_width * dimensions.back() <= kMaxBitsInMostMinorDimension &&\n-        bit_width * dimensions.back() *\n+        normalized_operand_dims[normalized_dims.size() - 2];\n+    if (bit_width * normalized_dims.back() <= kMaxBitsInMostMinorDimension &&\n+        bit_width * normalized_dims.back() *\n                 std::min(operand_most_minor_dim,\n-                         dimensions[dimensions.size() - 2]) >=\n+                         normalized_dims[normalized_dims.size() - 2]) >=\n             8 * kMinDimensionToTransposeTiled) {\n       // Tile size for transposition.\n       int64_t shmem_usage_bytes =\n           CeilOfRatio(kNumShmemBanks * (kNumShmemBanks + 1LL) * bit_width *\n-                          dimensions.back(),\n+                          normalized_dims.back(),\n                       8LL);\n-      return TransposeDescription{&hero, dimensions, permutation,\n+      return TransposeDescription{&hero, normalized_dims, permutation,\n                                   shmem_usage_bytes};\n     }\n   } else if ((operand_most_minor_dim >= kMinDimensionToTransposeTiled &&\n-              dimensions.back() >= kMinDimensionToTransposeTiled) ||\n+              normalized_dims.back() >= kMinDimensionToTransposeTiled) ||\n              (operand_most_minor_dim >= kMinDimensionToTransposeTiled2 &&\n-              dimensions.back() >= kMinDimensionToTransposeTiled2 &&\n-              operand_most_minor_dim * dimensions.back() >=\n+              normalized_dims.back() >= kMinDimensionToTransposeTiled2 &&\n+              operand_most_minor_dim * normalized_dims.back() >=\n                   kMinTotalDimensionsToTransposeTiled)) {\n     // TODO(b/415741994): TransposeEmitter is regressing for S4 when the last\n     // dimension is being transposed. The issue seems to be related to bank\n@@ -297,7 +301,7 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n     }\n     int64_t shmem_usage_bytes =\n         CeilOfRatio(kNumShmemBanks * (kNumShmemBanks + 1LL) * bit_width, 8LL);\n-    return TransposeDescription{&hero, dimensions, permutation,\n+    return TransposeDescription{&hero, normalized_dims, permutation,\n                                 shmem_usage_bytes};\n   }\n   return std::nullopt;"
        },
        {
            "sha": "dea30196de473e7243007f018449e54bfc06ea6d",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils_test.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 20,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -82,12 +82,13 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogicalTranspose) {\n HloModule module\n \n ENTRY entry {\n-  p = f32[1536,64]{1,0} parameter(0)\n-  ROOT t = f32[64,1536]{1,0} transpose(p), dimensions={1,0}\n+  p = f32[32,48,64]{2,1,0} parameter(0)\n+  ROOT t = f32[64,32,48]{2,1,0} transpose(p), dimensions={2,0,1}\n }\n )\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnVerifiedModule(hlo));\n+\n   HloInstruction* tr = module->entry_computation()->root_instruction();\n \n   auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n@@ -102,12 +103,12 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogical102Transpose) {\n HloModule module\n \n ENTRY entry {\n-  p = f32[32,48,2]{2,1,0} parameter(0)\n-  ROOT t = f32[48,32,2]{2,1,0} transpose(p), dimensions={1,0,2}\n+  p = f32[32,48,1,2]{3,2,1,0} parameter(0)\n+  ROOT t = f32[48,32,1,2]{3,2,1,0} transpose(p), dimensions={1,0,2,3}\n }\n )\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnVerifiedModule(hlo));\n   HloInstruction* tr = module->entry_computation()->root_instruction();\n \n   auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n@@ -412,10 +413,8 @@ fusion {\n   p = f32[32,48,64]{2,1,0} parameter(0)\n   p2 = f32[48,32,64]{2,1,0} parameter(1)\n   t = f32[64,48,32]{2,1,0} transpose(p), dimensions={2,1,0}\n-  bc = f32[1,1536,64]{2,1,0} bitcast(p2)\n-  t2 = f32[1,64,1536]{2,1,0} transpose(bc), dimensions={0,2,1}\n-  bc2 = f32[64,48,32]{2,1,0} bitcast(t2)\n-  ROOT add = f32[64,48,32]{2,1,0} add(t, bc2)\n+  t2 = f32[64,48,32]{2,1,0} transpose(p2), dimensions={2,0,1}\n+  ROOT add = f32[64,48,32]{2,1,0} add(t, t2)\n }\n \n ENTRY main {\n@@ -434,6 +433,26 @@ ENTRY main {\n   EXPECT_EQ(&FindNonTrivialHero(*r), r);\n }\n \n+TEST_F(IrEmissionUtilsTest, FindTiledLogicalTransposeWithGrouping) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  p = f32[32,32,64]{2,1,0} parameter(0)\n+  ROOT t = f32[64,32,32]{2,1,0} transpose(p), dimensions={2,0,1}\n+}\n+)\";\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnVerifiedModule(hlo));\n+  HloInstruction* tr = module->entry_computation()->root_instruction();\n+\n+  auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n+  EXPECT_TRUE(result.has_value());\n+  EXPECT_EQ(result->instr, tr);\n+  EXPECT_EQ(result->dimensions, InlinedVector({64, 1024}));\n+  EXPECT_EQ(result->permutation, InlinedVector({1, 0}));\n+}\n+\n TEST_F(IrEmissionUtilsTest, FindNonTrivialHeroOutsideFusion) {\n   const char* hlo = R\"(\n HloModule module\n@@ -533,13 +552,13 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogicalTransposeOneSwapDimIsSmall) {\n HloModule module\n \n fusion {\n-  p = f32[1100,12,8]{2,1,0} parameter(0)\n-  ROOT t = f32[8,12,1100]{2,1,0} transpose(p), dimensions={2,1,0}\n+  p = f32[100,11,12,8]{3,2,1,0} parameter(0)\n+  ROOT t = f32[8,12,100,11]{3,2,1,0} transpose(p), dimensions={3,2,0,1}\n }\n \n ENTRY main {\n-  param = f32[1100,12,8]{2,1,0} parameter(0)\n-  ROOT fusion = f32[8,12,1100]{2,1,0} fusion(param), kind=kInput, calls=fusion\n+  param = f32[100,11,12,8]{3,2,1,0} parameter(0)\n+  ROOT fusion = f32[8,12,100,11]{3,2,1,0} fusion(param), kind=kInput, calls=fusion\n }\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n@@ -559,13 +578,13 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogicalTransposeOtherSwapDimIsSmall) {\n HloModule module\n \n fusion {\n-  p = f32[8,12,1100]{2,1,0} parameter(0)\n-  ROOT t = f32[1100,12,8]{2,1,0} transpose(p), dimensions={2,1,0}\n+  p = f32[8,12,100,11]{3,2,1,0} parameter(0)\n+  ROOT t = f32[100,11,12,8]{3,2,1,0} transpose(p), dimensions={2,3,1,0}\n }\n \n ENTRY main {\n-  param = f32[8,12,1100]{2,1,0} parameter(0)\n-  ROOT fusion = f32[1100,12,8]{2,1,0} fusion(param), kind=kInput, calls=fusion\n+  param = f32[8,12,100,11]{3,2,1,0} parameter(0)\n+  ROOT fusion = f32[100,11,12,8]{3,2,1,0} fusion(param), kind=kInput, calls=fusion\n }\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n@@ -580,6 +599,28 @@ ENTRY main {\n   EXPECT_EQ(result->permutation, InlinedVector({2, 1, 0}));\n }\n \n+TEST_F(IrEmissionUtilsTest,\n+       FindTiledLogicalTransposeWithSize1DimensionInRawShape) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  p = f32[32,1,16,2]{3,2,1,0} parameter(0)\n+  ROOT t = f32[16,1,32,2]{3,2,1,0} transpose(p), dimensions={2,1,0,3}\n+}\n+)\";\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnVerifiedModule(hlo));\n+\n+  HloInstruction* tr = module->entry_computation()->root_instruction();\n+\n+  auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n+  EXPECT_TRUE(result.has_value());\n+  EXPECT_EQ(result->instr, tr);\n+  EXPECT_EQ(result->dimensions, InlinedVector({16, 32, 2}));\n+  EXPECT_EQ(result->permutation, InlinedVector({1, 0, 2}));\n+}\n+\n TEST_F(IrEmissionUtilsTest, IsContiguousSlice) {\n   const char* hlo = R\"(\n HloModule module"
        },
        {
            "sha": "098b8f94fd04c0849de4bb3179a09153d6f61b54",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -177,13 +177,13 @@ TEST_F(CoalescingTest, Transpose) {\n     HloModule module\n \n     fusion {\n-      %input = f32[1, 6400, 32] parameter(0)\n-      ROOT transpose = f32[1, 32, 6400] transpose(%input), dimensions={0, 2, 1}\n+      %input = f32[100, 64, 32] parameter(0)\n+      ROOT transpose = f32[32, 100, 64] transpose(%input), dimensions={2, 0, 1}\n     }\n \n     ENTRY entry {\n-      %input = f32[1, 6400, 32] parameter(0)\n-      ROOT %fusion = f32[1, 32, 6400] fusion(%input), kind=kLoop, calls=fusion\n+      %input = f32[100, 64, 32] parameter(0)\n+      ROOT %fusion = f32[32, 100, 64] fusion(%input), kind=kLoop, calls=fusion\n   })\";\n   // thread_x to linearized input mapping for thread_x in [0, 31]:\n   // Operand 1:  (thread_x)[s0] -> (thread_x + s0 * 128) for s0 in [0, 7]\n@@ -258,15 +258,15 @@ TEST_F(CoalescingTest, TransposeOfBroadcastHeuristic) {\n     HloModule module\n \n     fusion {\n-      input = f32[1, 32, 6400] parameter(0)\n-      ROOT slice = f32[1, 32, 100] slice(input), slice={[0:1:1], [0:32:1], [0:6400:64]}\n+      input = f32[32, 100, 64] parameter(0)\n+      ROOT slice = f32[32, 100, 1] slice(input), slice={[0:32:1], [0:100:1], [0:1:1]}\n     }\n \n     ENTRY entry {\n       p0 = f32[32] parameter(0)\n-      broadcast = f32[1, 6400, 32] broadcast(p0), dimensions={2}\n-      transpose = f32[1, 32, 6400] transpose(broadcast), dimensions={0, 2, 1}\n-      ROOT %fusion = f32[1, 32, 100] fusion(transpose), kind=kLoop, calls=fusion\n+      broadcast = f32[100, 64, 32] broadcast(p0), dimensions={2}\n+      transpose = f32[32, 100, 64] transpose(broadcast), dimensions={2, 0, 1}\n+      ROOT %fusion = f32[32, 100, 1] fusion(transpose), kind=kLoop, calls=fusion\n   })\";\n   EXPECT_TRUE(IsReadCoalescedHeuristic(ir));\n }"
        },
        {
            "sha": "08a13b2bb05b9b4849c546914879b510e28977e7",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -210,11 +210,11 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n     ],"
        },
        {
            "sha": "2073fc9f90b8582bcb40228162c91b7da0b9d3c7",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_norm_rewriter_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 17,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -287,7 +287,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[6], {{.*}}: f32[6]) -> f32[2,4,6,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[8,8,6]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,4,8,6]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[64,6,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[6]{0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,6,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -299,8 +299,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[64,6,1,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[8,6,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} bitcast([[FUSION]])\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -348,7 +347,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2Degenerate1) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,1,6,8], {{.*}}: f32[6], {{.*}}: f32[6]) -> f32[2,1,6,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,1,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,6]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[1,2,8,6]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,6,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[6]{0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,6,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -360,8 +359,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2Degenerate1) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[16,6,1,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[2,6,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,1,6,8]{3,2,1,0} bitcast([[FUSION]])\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,1,6,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -409,7 +407,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[4,6], {{.*}}: f32[4,6]) -> f32[2,4,6,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,24]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4,6]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,6]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,6,1]{3,2,1,0} bitcast([[P1]])\n@@ -421,8 +419,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[2,24,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} bitcast([[FUSION]])\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -470,7 +467,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12Degenerate2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,1,8], {{.*}}: f32[4,1], {{.*}}: f32[4,1]) -> f32[2,4,1,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,1,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[1,2,8,4]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,1]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -482,8 +479,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12Degenerate2) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[2,4,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,1,8]{3,2,1,0} bitcast([[FUSION]])\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,1,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -825,7 +821,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrain4D12) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[4,6], {{.*}}: f32[4,6]) -> (f32[2,4,6,8], f32[2,8], f32[2,8], f32[2,8]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,24]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4,6]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,6]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,6,1]{3,2,1,0} bitcast([[P1]])\n@@ -885,7 +881,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrain4D12Degenerate2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,1,8], {{.*}}: f32[4,1], {{.*}}: f32[4,1]) -> (f32[2,4,1,8], f32[2,8], f32[2,8], f32[2,8]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,1,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[1,2,8,4]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,1]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -1181,7 +1177,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrainBackward4D2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[6], {{.*}}: f32[6], {{.*}}: f32[2,4,6,8]) -> (f32[2,4,6,8], f32[2,4,6,8], f32[6], f32[6]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[8,8,6]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,4,8,6]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[64,6,1,1]{3,2,1,0} bitcast([[TRANSPOSE0]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[6]{0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,6,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -1274,7 +1270,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrainBackward4D12) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[4,6], {{.*}}: f32[4,6], {{.*}}: f32[2,4,6,8]) -> (f32[2,4,6,8], f32[2,4,6,8], f32[4,6], f32[4,6]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,8,24]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,8,4,6]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} bitcast([[TRANSPOSE0]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,6]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,6,1]{3,2,1,0} bitcast([[P1]])\n@@ -1367,7 +1363,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrainBackward4D12Degenerate2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,1,8], {{.*}}: f32[4,1], {{.*}}: f32[4,1], {{.*}}: f32[2,4,1,8]) -> (f32[2,4,1,8], f32[2,4,1,8], f32[4,1], f32[4,1]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,1,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,8,4]{2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[1,2,8,4]{3,2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} bitcast([[TRANSPOSE0]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,1]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,1,1]{3,2,1,0} bitcast([[P1]])"
        },
        {
            "sha": "06d92020417493e5953f76088a421657484c8f93",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -20,13 +20,13 @@ limitations under the License.\n #include <variant>\n \n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/pattern_matcher.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -78,20 +79,28 @@ bool ShouldRewriteLoopTransposeFusion(\n   // is neither the minormost nor the second minormost dimension in the output,\n   // and the output minormost dimension is swapped with the new minormost\n   // dimension.\n-  int64_t rank = root->shape().dimensions().size();\n \n-  // The transpose dimension grouper has run, so it should be enough to check\n-  // that the minormost dimension's index within the result is smaller than\n-  // rank - 2, and that the new minormost dimension is swapped with it.\n+  // We use the normalized logical transpose shape so it should be enough to\n+  // check that the minormost dimension's index within the result is smaller\n+  // than rank - 2, and that the new minormost dimension is swapped with it.\n+  absl::InlinedVector<int64_t, 3> permutation;\n+  auto normalized_dims_or = ShapeUtil::GetNormalizedLogicalTransposeShape(\n+      root->operand(0)->shape(), root->shape(), root->dimensions(),\n+      permutation);\n+  if (!normalized_dims_or.ok()) {\n+    return false;\n+  }\n+  auto normalized_dims = normalized_dims_or.value();\n+  int64_t rank = normalized_dims.size();\n+\n   // This only triggers for transposes with major-to-minor layout.\n   bool has_major_to_minor_layout =\n       LayoutUtil::IsMonotonicWithDim0Major(root->shape().layout());\n-  absl::Span<int64_t const> transpose_dimensions = root->dimensions();\n-  int64_t result_minormost_dim_in_operand = transpose_dimensions.back();\n+  int64_t result_minormost_dim_in_operand = permutation.back();\n \n   if (!(has_major_to_minor_layout &&\n-        transpose_dimensions[result_minormost_dim_in_operand] == rank - 1 &&\n-        transpose_dimensions[rank - 1] < rank - 2)) {\n+        permutation[result_minormost_dim_in_operand] == rank - 1 &&\n+        permutation[rank - 1] < rank - 2)) {\n     return false;\n   }\n "
        },
        {
            "sha": "8fc1960a1a628dd7a26599b4239f2f054e83bb54",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -211,6 +211,38 @@ ENTRY entry  {\n   EXPECT_TRUE(HasTritonBlockLevelFusionConfig(root));\n }\n \n+TEST_F(FusionBlockLevelRewriterTest,\n+       RewritesLoopTransposeFusionWithSplitDimensions) {\n+  // This test checks if the rewriter can handle a transpose where dimensions\n+  // are split in the HLO but logically contiguous.\n+  // Logical shape: [100, 200, 300] -> [300, 200, 100] (Swap dim 0 and 2).\n+  // Physical shape: [100, 200, 10, 30] -> [10, 30, 200, 100].\n+  // The normalized logical transpose shape should recover the logical swap.\n+  const absl::string_view hlo_text = R\"(\n+fusion_computation {\n+  p0 = f32[100,200,10,30] parameter(0)\n+  ROOT transpose = f32[10,30,200,100] transpose(p0), dimensions={2,3,1,0}\n+}\n+\n+ENTRY entry {\n+  p0 = f32[100,200,10,30] parameter(0)\n+  ROOT fusion = f32[10,30,200,100] fusion(p0), kind=kLoop,\n+    calls=fusion_computation\n+})\";\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnVerifiedModule(hlo_text));\n+\n+  EXPECT_THAT(\n+      FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n+                               &mlir_context_)\n+          .Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_EQ(root->opcode(), HloOpcode::kFusion);\n+  EXPECT_EQ(root->fusion_kind(), HloInstruction::FusionKind::kCustom);\n+  EXPECT_TRUE(HasTritonBlockLevelFusionConfig(root));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "3b5c92a19388d201f745d1433dd43608d3b052f8",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_a100.hlo",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -1,10 +1,9 @@\n // RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../backends/gpu/target_config/specs/a100_pcie_80.txtpb --split-input-file | FileCheck %s\n \n-// CHECK: fused_transpose\n+// CHECK: %wrapped_transpose_computation\n // CHECK-NEXT: bf16[3,3,16,32]{3,2,1,0} parameter(0)\n-// CHECK-NEXT: bf16[144,32]{1,0} bitcast\n-// CHECK-NEXT: bf16[32,144]{1,0} transpose\n-// CHECK-SAME: dimensions={1,0}\n+// CHECK-NEXT: bf16[32,3,3,16]{3,2,1,0} transpose\n+// CHECK-SAME: dimensions={3,0,1,2}\n // CHECK: (bf16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n // CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n "
        },
        {
            "sha": "1b82bb55c80b2b49b4bd79be3292078733ffcdbd",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_h100.hlo",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -1,10 +1,9 @@\n // RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../backends/gpu/target_config/specs/h100_sxm.txtpb --split-input-file | FileCheck %s\n \n-// CHECK: fused_transpose\n+// CHECK: %wrapped_transpose_computation\n // CHECK-NEXT: f8e4m3fn[3,3,16,32]{3,2,1,0} parameter(0)\n-// CHECK-NEXT: f8e4m3fn[144,32]{1,0} bitcast\n-// CHECK-NEXT: f8e4m3fn[32,144]{1,0} transpose\n-// CHECK-SAME: dimensions={1,0}\n+// CHECK-NEXT: f8e4m3fn[32,3,3,16]{3,2,1,0} transpose\n+// CHECK-SAME: dimensions={3,0,1,2}\n // CHECK: (f8e4m3fn[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n // CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n "
        },
        {
            "sha": "d5baeb8a42af7da1d0e2401aa711010d313ad4f5",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_v100.hlo",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -1,10 +1,9 @@\n // RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../backends/gpu/target_config/specs/v100.txtpb --split-input-file | FileCheck %s\n \n-// CHECK: fused_transpose\n+// CHECK: %wrapped_transpose_computation\n // CHECK-NEXT: f16[3,3,16,32]{3,2,1,0} parameter(0)\n-// CHECK-NEXT: f16[144,32]{1,0} bitcast\n-// CHECK-NEXT: f16[32,144]{1,0} transpose\n-// CHECK-SAME: dimensions={1,0}\n+// CHECK-NEXT: f16[32,3,3,16]{3,2,1,0} transpose\n+// CHECK-SAME: dimensions={3,0,1,2}\n // CHECK: (f16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n // CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n "
        },
        {
            "sha": "5d29d0c2b76fb37982b8ea03cef15a60d899eb15",
            "filename": "third_party/xla/xla/shape_util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fshape_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fshape_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fshape_util.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -2379,6 +2379,7 @@ absl::InlinedVector<int64_t, 3> GetNormalizedTransposeShapeHelper(\n       normalized_shape.dimensions().begin(),\n       normalized_shape.dimensions().end());\n   if (segments.size() == 1) {\n+    permutation.push_back(0);\n     return normalized_dims;\n   }\n   // Derive the permutation from the segments."
        },
        {
            "sha": "5af96d0805c84d293abb75a46f2022ac969bd612",
            "filename": "third_party/xla/xla/shape_util_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fshape_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/408bf09796590bc66233afff288bf926e2736a9d/third_party%2Fxla%2Fxla%2Fshape_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fshape_util_test.cc?ref=408bf09796590bc66233afff288bf926e2736a9d",
            "patch": "@@ -1815,7 +1815,20 @@ TEST(ShapeUtilTest, GetNormalizedLogicalTransposeShape_NoTranspose) {\n                            input_shape, output_shape, dimensions, permutation));\n \n   EXPECT_THAT(normalized_shape, ElementsAre(8192));\n-  EXPECT_THAT(permutation, IsEmpty());\n+  EXPECT_THAT(permutation, ElementsAre(0));\n+}\n+\n+TEST(ShapeUtilTest, GetNormalizedLogicalTransposeShape_IdentityWithMerges) {\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {10, 20});\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {20, 10});\n+  // Identity transpose that allows merging dimensions.\n+  absl::InlinedVector<int64_t, 3> dimensions = {0, 1};\n+  absl::InlinedVector<int64_t, 3> permutation;\n+  ASSERT_OK_AND_ASSIGN(auto normalized_shape,\n+                       ShapeUtil::GetNormalizedLogicalTransposeShape(\n+                           input_shape, output_shape, dimensions, permutation));\n+  EXPECT_THAT(normalized_shape, ElementsAre(200));\n+  EXPECT_THAT(permutation, ElementsAre(0));\n }\n \n TEST(ShapeUtilTest, GetNormalizedLogicalTransposeShape_Simple2D) {"
        }
    ],
    "stats": {
        "total": 388,
        "additions": 247,
        "deletions": 141
    }
}