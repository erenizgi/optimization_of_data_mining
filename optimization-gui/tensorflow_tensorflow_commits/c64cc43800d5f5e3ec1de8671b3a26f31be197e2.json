{
    "author": "tensorflower-gardener",
    "message": "New streamz which tracks work coming into a batch queue by criticality.\n\nPiperOrigin-RevId: 803945106",
    "sha": "c64cc43800d5f5e3ec1de8671b3a26f31be197e2",
    "files": [
        {
            "sha": "2b9117dc53f44ecf4d68d6eef60a91ad97c43de7",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 7,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c64cc43800d5f5e3ec1de8671b3a26f31be197e2/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c64cc43800d5f5e3ec1de8671b3a26f31be197e2/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc?ref=c64cc43800d5f5e3ec1de8671b3a26f31be197e2",
            "patch": "@@ -136,18 +136,28 @@ void RecordInputBatchSize(int32_t batch_size, const string& model_name,\n   cell->GetCell(model_name, op_name)->Add(static_cast<double>(batch_size));\n }\n \n-void RecordInputBatchSizeV2(int32_t batch_size, const string& model_name,\n-                            const string& op_name) {\n-  static auto* cell = tensorflow::monitoring::Sampler<2>::New(\n+void RecordInputStatsV2(int32_t batch_size, const string& model_name,\n+                        const string& op_name,\n+                        const tsl::criticality::Criticality& criticality) {\n+  static auto* cell = tensorflow::monitoring::Sampler<3>::New(\n       {\"/tensorflow/serving/batching/input_batch_size_v2\",\n        \"Tracks the batch size distribution on the inputs by model_name (if \"\n        \"available).\",\n-       \"model_name\", \"op_name\"},\n+       \"model_name\", \"op_name\", \"criticality\"},\n       // Buckets centered at powers of 2, and have bounds:\n       // [(2/3) * 2^i, (4/3) * 2^i] for i = 0, ..., 13.\n       // Largest bucket has range: [(2/3) *  2^14, DBL_MAX]\n       monitoring::Buckets::Exponential(2.0 / 3.0, 2, 15));\n-  cell->GetCell(model_name, op_name)->Add(static_cast<double>(batch_size));\n+  const std::string criticality_str = absl::StrCat(criticality);\n+  cell->GetCell(model_name, op_name, criticality_str)\n+      ->Add(static_cast<double>(batch_size));\n+\n+  static auto* num_tasks_counter = tensorflow::monitoring::Counter<3>::New(\n+      \"/tensorflow/serving/batching/input_num_tasks\",\n+      \"Tracks the number of batches submitted to the batching scheduler.\",\n+      \"model_name\", \"op_name\", \"criticality\");\n+  num_tasks_counter->GetCell(model_name, op_name, criticality_str)\n+      ->IncrementBy(1);\n }\n \n // Record the actual batch size without padding.\n@@ -415,8 +425,9 @@ absl::Status BatchResourceBase::RegisterInput(\n   }\n   RecordInputBatchSize(tensors[0].shape().dim_size(0), GetModelName(context),\n                        context->op_kernel().name());\n-  RecordInputBatchSizeV2(tensors[0].shape().dim_size(0), GetModelName(context),\n-                         context->op_kernel().name());\n+  RecordInputStatsV2(tensors[0].shape().dim_size(0), GetModelName(context),\n+                     context->op_kernel().name(),\n+                     batch_components->criticality());\n   if (batcher_) {\n     RecordBatchParamBatchTimeoutMicros(\n         batcher_queue_options_.batch_timeout_micros, GetModelName(context),"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 18,
        "deletions": 7
    }
}