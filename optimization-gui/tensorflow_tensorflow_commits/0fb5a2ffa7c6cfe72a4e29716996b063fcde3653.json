{
    "author": "nvgrw",
    "message": "Port batch_normalization_test to HloTestBase.\n\nPiperOrigin-RevId: 840389993",
    "sha": "0fb5a2ffa7c6cfe72a4e29716996b063fcde3653",
    "files": [
        {
            "sha": "abf6a48f70cf870517434e4286c07b0df85f0645",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fb5a2ffa7c6cfe72a4e29716996b063fcde3653/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fb5a2ffa7c6cfe72a4e29716996b063fcde3653/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=0fb5a2ffa7c6cfe72a4e29716996b063fcde3653",
            "patch": "@@ -1784,32 +1784,32 @@ xla_test(\n     ],\n     shard_count = 40,\n     deps = [\n-        \":client_library_test_base\",\n+        \":client_library_test_runner_mixin\",\n         \":hlo_test_base\",\n-        \":literal_test_util\",\n-        \":test_utils\",\n         \":xla_internal_test_main\",\n         \"//xla:array2d\",\n+        \"//xla:array3d\",\n         \"//xla:array4d\",\n+        \"//xla:error_spec\",\n         \"//xla:literal\",\n+        \"//xla:literal_util\",\n         \"//xla:reference_util\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/client:local_client\",\n         \"//xla/hlo/builder:xla_builder\",\n         \"//xla/hlo/builder:xla_computation\",\n         \"//xla/hlo/builder/lib:arithmetic\",\n         \"//xla/hlo/builder/lib:math\",\n-        \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:test\",\n         \"//xla/hlo/testlib:test_helpers\",\n+        \"//xla/service\",\n         \"//xla/tsl/lib/math:math_util\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n "
        },
        {
            "sha": "f76f477be93fbd9ec9390e2b59fc121886a1d8ea",
            "filename": "third_party/xla/xla/tests/batch_normalization_test.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 65,
            "changes": 99,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fb5a2ffa7c6cfe72a4e29716996b063fcde3653/third_party%2Fxla%2Fxla%2Ftests%2Fbatch_normalization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fb5a2ffa7c6cfe72a4e29716996b063fcde3653/third_party%2Fxla%2Fxla%2Ftests%2Fbatch_normalization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fbatch_normalization_test.cc?ref=0fb5a2ffa7c6cfe72a4e29716996b063fcde3653",
            "patch": "@@ -14,41 +14,44 @@ limitations under the License.\n ==============================================================================*/\n \n #include <cmath>\n+#include <cstdint>\n #include <memory>\n+#include <ostream>\n #include <vector>\n \n+#include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_join.h\"\n #include \"xla/array2d.h\"\n+#include \"xla/array3d.h\"\n #include \"xla/array4d.h\"\n-#include \"xla/client/local_client.h\"\n+#include \"xla/error_spec.h\"\n #include \"xla/hlo/builder/lib/arithmetic.h\"\n #include \"xla/hlo/builder/lib/math.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/test.h\"\n #include \"xla/hlo/testlib/test_helpers.h\"\n #include \"xla/literal.h\"\n+#include \"xla/literal_util.h\"\n #include \"xla/reference_util.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/tests/client_library_test_base.h\"\n+#include \"xla/tests/client_library_test_runner_mixin.h\"\n #include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tests/literal_test_util.h\"\n-#include \"xla/tests/test_utils.h\"\n #include \"xla/tsl/lib/math/math_util.h\"\n+#include \"xla/tsl/platform/test.h\"\n #include \"xla/types.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace xla {\n namespace {\n \n-class BatchNormalizationTest : public ClientLibraryTestBase {\n+constexpr ErrorSpec kErrorSpec{0.001, 0.001};\n+\n+class BatchNormalizationTest\n+    : public ClientLibraryTestRunnerMixin<HloTestBase> {\n  protected:\n   BatchNormalizationTest() : input_array_(kSamples, kZ, kY, kX) {\n     Array2D<float> pz({\n@@ -81,7 +84,6 @@ class BatchNormalizationTest : public ClientLibraryTestBase {\n \n   Array4D<float> input_array_;\n   Literal input_literal_;\n-  const ErrorSpec error_spec_{0.001, 0.001};\n };\n \n TEST_F(BatchNormalizationTest, SubtractInZ) {\n@@ -97,7 +99,7 @@ TEST_F(BatchNormalizationTest, SubtractInZ) {\n       {5.0f - 3.14f, 4.4f - 4.25f},   // p2\n   });\n   expected.FillWithPZ(pz);\n-  ComputeAndCompareR4<float>(&builder, expected, {}, error_spec_);\n+  ComputeAndCompareR4<float>(&builder, expected, {}, kErrorSpec);\n }\n \n TEST_F(BatchNormalizationTest, SquareTesseractElementwise) {\n@@ -114,7 +116,7 @@ TEST_F(BatchNormalizationTest, SquareTesseractElementwise) {\n       {MathUtil::IPow(5.0f, 2), MathUtil::IPow(4.4f, 2)},\n   });\n   expected.FillWithPZ(expected_pz);\n-  ComputeAndCompareR4<float>(&builder, expected, {}, error_spec_);\n+  ComputeAndCompareR4<float>(&builder, expected, {}, kErrorSpec);\n }\n \n TEST_F(BatchNormalizationTest, SumToZ) {\n@@ -125,7 +127,7 @@ TEST_F(BatchNormalizationTest, SumToZ) {\n   Reduce(input_activations, ConstantR0<float>(&builder, 0.0f), add, {0, 2, 3});\n \n   std::vector<float> expected = {6, 12.6};\n-  ComputeAndCompareR1<float>(&builder, expected, {}, error_spec_);\n+  ComputeAndCompareR1<float>(&builder, expected, {}, kErrorSpec);\n }\n \n TEST_F(BatchNormalizationTest, SquareAndReduce) {\n@@ -139,7 +141,7 @@ TEST_F(BatchNormalizationTest, SquareAndReduce) {\n   Reduce(dev_squares, ConstantR0<float>(&builder, 0.0f), add, {0, 2, 3});\n \n   std::vector<float> expected = {18, 0.06};\n-  ComputeAndCompareR1<float>(&builder, expected, {}, error_spec_);\n+  ComputeAndCompareR1<float>(&builder, expected, {}, kErrorSpec);\n }\n \n TEST_F(BatchNormalizationTest, VarianceToStddev) {\n@@ -148,7 +150,7 @@ TEST_F(BatchNormalizationTest, VarianceToStddev) {\n   Sqrt(variance);\n \n   std::vector<float> expected = {2.44948974f, 0.14142136f};\n-  ComputeAndCompareR1<float>(&builder, expected, {}, error_spec_);\n+  ComputeAndCompareR1<float>(&builder, expected, {}, kErrorSpec);\n }\n \n // Compare against a forward batch normalization example in the NN spec\n@@ -209,7 +211,7 @@ TEST_F(BatchNormalizationTest, SpecComparisonForward) {\n   });\n   expected.FillWithPZ(pz);\n \n-  ComputeAndCompareR4<float>(&builder, expected, {}, error_spec_);\n+  ComputeAndCompareR4<float>(&builder, expected, {}, kErrorSpec);\n }\n \n TEST_F(BatchNormalizationTest, BasicTraining) {\n@@ -345,8 +347,7 @@ TEST_F(BatchNormalizationTest, TrainingWithFeatureOnLowDimension) {\n        LiteralUtil::CreateR1<float>(std::vector<float>(260, 1.0f)),\n        LiteralUtil::CreateR1<float>(std::vector<float>(260, 0.0f))});\n \n-  ComputeAndCompareTuple(&builder, expected,\n-                         {operand.get(), scale.get(), offset.get()},\n+  ComputeAndCompareTuple(&builder, expected, {&operand, &scale, &offset},\n                          ErrorSpec(0.1));\n }\n \n@@ -378,8 +379,7 @@ TEST_F(BatchNormalizationTest, LargeEpsilonTest) {\n        LiteralUtil::CreateR1<float>(std::vector<float>(1, 15.0f)),\n        LiteralUtil::CreateR1<float>(std::vector<float>(1, 125.0f))});\n \n-  ComputeAndCompareTuple(&builder, expected,\n-                         {operand.get(), scale.get(), offset.get()},\n+  ComputeAndCompareTuple(&builder, expected, {&operand, &scale, &offset},\n                          ErrorSpec(0.1));\n }\n \n@@ -467,7 +467,7 @@ struct BatchNormTestParam {\n \n // Tests to test the fused operation of BatchNorm.\n class BatchNormTestManySizes\n-    : public ClientLibraryTestBase,\n+    : public ClientLibraryTestRunnerMixin<HloTestBase>,\n       public ::testing::WithParamInterface<BatchNormTestParam> {};\n \n std::vector<BatchNormTestParam> BuildBatchNormTestParams() {\n@@ -589,24 +589,16 @@ TEST_P(BatchNormTestManySizes, RandomizedTrainingTests) {\n       {expected_normalized, LiteralUtil::CreateR1<float>(mean),\n        LiteralUtil::CreateR1<float>(var)});\n \n-  std::unique_ptr<GlobalData> input_data =\n-      client_->TransferToServer(input_literal).value();\n-  std::unique_ptr<GlobalData> scale_data =\n-      client_->TransferToServer(scale_literal).value();\n-  std::unique_ptr<GlobalData> offset_data =\n-      client_->TransferToServer(offset_literal).value();\n-\n   BatchNormTraining(input_activations, scale_activations, offset_activations,\n                     epsilon, feature_index);\n \n   // Run all HLO passes during this test.  In particular, ClientLibraryTestBase\n   // disables constant folding, but we want it enabled for our zero-sized tensor\n   // testcase.\n-  execution_options_.mutable_debug_options()->clear_xla_disable_hlo_passes();\n-  ComputeAndCompareTuple(\n-      &builder, expected,\n-      {input_data.get(), scale_data.get(), offset_data.get()},\n-      ErrorSpec(0.01, 1));\n+  mutable_debug_options()->clear_xla_disable_hlo_passes();\n+  ComputeAndCompareTuple(&builder, expected,\n+                         {&input_literal, &scale_literal, &offset_literal},\n+                         ErrorSpec(0.01, 1));\n }\n \n TEST_P(BatchNormTestManySizes, RandomizedInferencingTests) {\n@@ -690,31 +682,19 @@ TEST_P(BatchNormTestManySizes, RandomizedInferencingTests) {\n \n   Array4D<float> expected = normalized;\n \n-  std::unique_ptr<GlobalData> input_data =\n-      client_->TransferToServer(input_literal).value();\n-  std::unique_ptr<GlobalData> scale_data =\n-      client_->TransferToServer(scale_literal).value();\n-  std::unique_ptr<GlobalData> offset_data =\n-      client_->TransferToServer(offset_literal).value();\n-  std::unique_ptr<GlobalData> mean_data =\n-      client_->TransferToServer(mean_literal).value();\n-  std::unique_ptr<GlobalData> variance_data =\n-      client_->TransferToServer(var_literal).value();\n-\n   BatchNormInference(input_activations, scale_activations, offset_activations,\n                      mean_activations, variance_activations, epsilon,\n                      feature_index);\n \n   // Run all HLO passes during this test.  In particular, ClientLibraryTestBase\n   // disables constant folding, but we want it enabled for our zero-sized tensor\n   // testcase.\n-  execution_options_.mutable_debug_options()->clear_xla_disable_hlo_passes();\n+  mutable_debug_options()->clear_xla_disable_hlo_passes();\n \n-  ComputeAndCompareR4<float>(\n-      &builder, expected,\n-      {input_data.get(), scale_data.get(), offset_data.get(), mean_data.get(),\n-       variance_data.get()},\n-      ErrorSpec(0.01, 1));\n+  ComputeAndCompareR4<float>(&builder, expected,\n+                             {&input_literal, &scale_literal, &offset_literal,\n+                              &mean_literal, &var_literal},\n+                             ErrorSpec(0.01, 1));\n }\n \n TEST_P(BatchNormTestManySizes, RandomizedGradTests) {\n@@ -875,17 +855,6 @@ TEST_P(BatchNormTestManySizes, RandomizedGradTests) {\n   auto grad_output_parameter =\n       Parameter(&builder, 4, grad_output_literal.shape(), \"grad_output\");\n \n-  std::unique_ptr<GlobalData> input_data =\n-      client_->TransferToServer(input_literal).value();\n-  std::unique_ptr<GlobalData> scale_data =\n-      client_->TransferToServer(scale_literal).value();\n-  std::unique_ptr<GlobalData> mean_data =\n-      client_->TransferToServer(mean_literal).value();\n-  std::unique_ptr<GlobalData> var_data =\n-      client_->TransferToServer(var_literal).value();\n-  std::unique_ptr<GlobalData> grad_output_data =\n-      client_->TransferToServer(grad_output_literal).value();\n-\n   BatchNormGrad(input_parameter, scale_parameter, mean_parameter, var_parameter,\n                 grad_output_parameter, epsilon, feature_index);\n \n@@ -896,11 +865,11 @@ TEST_P(BatchNormTestManySizes, RandomizedGradTests) {\n   // Run all HLO passes during this test.  In particular, ClientLibraryTestBase\n   // disables constant folding, but we want it enabled for our zero-sized tensor\n   // testcase.\n-  execution_options_.mutable_debug_options()->clear_xla_disable_hlo_passes();\n+  mutable_debug_options()->clear_xla_disable_hlo_passes();\n \n   ComputeAndCompareTuple(&builder, expected,\n-                         {input_data.get(), scale_data.get(), mean_data.get(),\n-                          var_data.get(), grad_output_data.get()},\n+                         {&input_literal, &scale_literal, &mean_literal,\n+                          &var_literal, &grad_output_literal},\n                          ErrorSpec(0.01, 1));\n }\n "
        }
    ],
    "stats": {
        "total": 113,
        "additions": 41,
        "deletions": 72
    }
}