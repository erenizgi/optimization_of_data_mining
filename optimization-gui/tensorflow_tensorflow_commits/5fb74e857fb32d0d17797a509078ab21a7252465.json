{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Pass collective metadata used to calculate symmetric memory on peers.\n\nPiperOrigin-RevId: 809981524",
    "sha": "5fb74e857fb32d0d17797a509078ab21a7252465",
    "files": [
        {
            "sha": "83749577775e0f97d28c3bd57b2dc3327b6d50dc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -1090,16 +1090,19 @@ cc_library(\n         \":thunk\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:rendezvous\",\n+        \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_handle\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n+        \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -1967,6 +1970,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n+        \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util:safe_reinterpret_cast\",\n@@ -2000,20 +2004,23 @@ xla_test(\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:types\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:hlo_runner\",\n         \"//xla/service:platform_util\",\n+        \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_handle\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n+        \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/stream_executor/gpu:gpu_init\",\n         \"//xla/stream_executor/host:host_platform\",\n         \"//xla/tests:literal_test_util\","
        },
        {
            "sha": "e3a3576f6977e498d426d91b6b7534642d0b1c89",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 42,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n+#include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -70,13 +71,15 @@ static constexpr int64_t kMaxThreadsPerBlock = 512;\n static constexpr int64_t kWarpSize = 32;\n \n template <typename TagType>\n-absl::Status LaunchTypedKernel(\n-    TagType, se::Stream* stream, const LaunchDimensions& launch_dimensions,\n-    absl::Span<const se::DeviceMemoryBase> remote_input_buffers,\n-    se::DeviceMemoryBase local_input_buffer, se::DeviceMemoryBase output_buffer,\n-    int64_t rank, int64_t num_ranks, int64_t num_elements,\n-    absl::Span<const se::DeviceMemoryBase> signal_flags_buffers,\n-    uint32_t signal_value) {\n+absl::Status LaunchTypedKernel(TagType, se::Stream* stream,\n+                               const LaunchDimensions& launch_dimensions,\n+                               se::DeviceMemoryBase symmetric_input_buffer,\n+                               se::DeviceMemoryBase local_input_buffer,\n+                               se::DeviceMemoryBase output_buffer, int64_t rank,\n+                               int64_t num_ranks, int64_t num_elements,\n+                               se::DeviceMemoryBase symmetric_signal_buffer,\n+                               uint32_t signal_value,\n+                               se::DeviceMemoryBase metadata) {\n   using ElementType = typename TagType::ElementType;\n   static constexpr bool kIsTwoShot =\n       TagType::kAllReduceStrategy == AllReduceStrategy::kTwoShot;\n@@ -90,15 +93,14 @@ absl::Status LaunchTypedKernel(\n                stream->parent())));\n \n   se::gpu::AllReduceKernelParams<ElementType> params{};\n-  absl::c_transform(\n-      remote_input_buffers, params.remote_input_buffers.begin(),\n-      [](se::DeviceMemoryBase buffer) {\n-        return tsl::safe_reinterpret_cast<ElementType*>(buffer.opaque());\n-      });\n   params.input_buffer =\n       tsl::safe_reinterpret_cast<ElementType*>(local_input_buffer.opaque());\n   params.output_buffer =\n       tsl::safe_reinterpret_cast<ElementType*>(output_buffer.opaque());\n+  params.symmetric_input_ptrs =\n+      tsl::safe_reinterpret_cast<ElementType*>(symmetric_input_buffer.opaque());\n+  params.symmetric_signal_ptrs =\n+      tsl::safe_reinterpret_cast<uint32_t*>(symmetric_signal_buffer.opaque());\n   params.rank = rank;\n   params.num_ranks = num_ranks;\n   params.num_elements = num_elements;\n@@ -110,17 +112,14 @@ absl::Status LaunchTypedKernel(\n       CeilOfRatio(params.num_elements_per_rank,\n                   absl::implicit_cast<int64_t>(launch_dimensions.num_blocks())),\n       se::gpu::kNumElementsPerThread);\n-  absl::c_transform(\n-      signal_flags_buffers, params.signal_flags_buffers.begin(),\n-      [](se::DeviceMemoryBase buffer) {\n-        return tsl::safe_reinterpret_cast<uint32_t*>(buffer.opaque());\n-      });\n   params.rank_offset =\n       kIsTwoShot ? params.rank * params.num_elements_per_rank : 0;\n   for (int i = 0; i < params.num_ranks; ++i) {\n     params.rotated_ranks[i] = (i + rank) % params.num_ranks;\n   }\n   params.signal_value = signal_value;\n+  params.metadata =\n+      tsl::safe_reinterpret_cast<CollectiveKernelMetadata*>(metadata.opaque());\n \n   VLOG(3) << \"Launching all-reduce kernel with params: \" << \"strategy: \"\n           << absl::StrFormat(\"%v\", TagType::kAllReduceStrategy)\n@@ -196,20 +195,20 @@ bool IsAllReduceKernelSupported(int64_t num_ranks, int64_t num_elements,\n }\n \n absl::Status RunAllReduceKernel(\n-    se::Stream* stream,                                           //\n-    const LaunchDimensions& launch_dimensions,                    //\n-    PrimitiveType element_type,                                   //\n-    ReductionKind reduction_kind,                                 //\n-    AllReduceStrategy all_reduce_strategy,                        //\n-    absl::Span<const se::DeviceMemoryBase> remote_input_buffers,  //\n-    se::DeviceMemoryBase local_input_buffer,                      //\n-    se::DeviceMemoryBase output_buffer,                           //\n-    RankId rank,                                                  //\n-    int64_t num_ranks,                                            //\n-    int64_t num_elements,                                         //\n-    absl::Span<const se::DeviceMemoryBase> signal_flags_buffers,  //\n-    uint32_t signal_value                                         //\n-) {\n+    se::Stream* stream,                            //\n+    const LaunchDimensions& launch_dimensions,     //\n+    PrimitiveType element_type,                    //\n+    ReductionKind reduction_kind,                  //\n+    AllReduceStrategy all_reduce_strategy,         //\n+    se::DeviceMemoryBase symmetric_input_buffer,   //\n+    se::DeviceMemoryBase local_input_buffer,       //\n+    se::DeviceMemoryBase output_buffer,            //\n+    RankId rank,                                   //\n+    int64_t num_ranks,                             //\n+    int64_t num_elements,                          //\n+    se::DeviceMemoryBase symmetric_signal_buffer,  //\n+    uint32_t signal_value,                         //\n+    se::DeviceMemoryBase metadata) {\n   if (!IsAllReduceKernelSupported(num_ranks, num_elements, element_type,\n                                   reduction_kind, all_reduce_strategy)) {\n     return absl::InvalidArgumentError(\n@@ -220,18 +219,11 @@ absl::Status RunAllReduceKernel(\n                      \", \", ReductionKindToString(reduction_kind)));\n   }\n \n-  if (remote_input_buffers.size() >\n-      stream_executor::gpu::kMaxNumAllReduceInputPtrs) {\n-    return absl::InvalidArgumentError(\n-        \"Number of input pointers exceeds the maximum supported number of \"\n-        \"input pointers.\");\n-  }\n-\n   const auto launch_kernel_impl = [&](auto tag) -> absl::Status {\n-    return LaunchTypedKernel(tag, stream, launch_dimensions,\n-                             remote_input_buffers, local_input_buffer,\n-                             output_buffer, rank.value(), num_ranks,\n-                             num_elements, signal_flags_buffers, signal_value);\n+    return LaunchTypedKernel(\n+        tag, stream, launch_dimensions, symmetric_input_buffer,\n+        local_input_buffer, output_buffer, rank.value(), num_ranks,\n+        num_elements, symmetric_signal_buffer, signal_value, metadata);\n   };\n   const auto launch_kernel = [&](auto tag_registry,\n                                  AllReduceStrategy strategy) -> absl::Status {"
        },
        {
            "sha": "78656499452388f54349a5bf1b7ae86f25c65162",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.h",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -74,19 +74,20 @@ bool IsAllReduceKernelSupported(int64_t num_ranks, int64_t num_elements,\n //    different for different invocations of the kernel with the same signal\n //    buffer.\n absl::Status RunAllReduceKernel(\n-    se::Stream* stream,                                           //\n-    const LaunchDimensions& launch_dimensions,                    //\n-    PrimitiveType element_type,                                   //\n-    ReductionKind reduction_kind,                                 //\n-    se::gpu::AllReduceStrategy all_reduce_strategy,               //\n-    absl::Span<const se::DeviceMemoryBase> remote_input_buffers,  //\n-    se::DeviceMemoryBase local_input_buffer,                      //\n-    se::DeviceMemoryBase output_buffer,                           //\n-    RankId rank,                                                  //\n-    int64_t num_ranks,                                            //\n-    int64_t num_elements,                                         //\n-    absl::Span<const se::DeviceMemoryBase> signal_flags_buffers,  //\n-    uint32_t signal_value                                         //\n+    se::Stream* stream,                              //\n+    const LaunchDimensions& launch_dimensions,       //\n+    PrimitiveType element_type,                      //\n+    ReductionKind reduction_kind,                    //\n+    se::gpu::AllReduceStrategy all_reduce_strategy,  //\n+    se::DeviceMemoryBase symmetric_input_buffer,     //\n+    se::DeviceMemoryBase local_input_buffer,         //\n+    se::DeviceMemoryBase output_buffer,              //\n+    RankId rank,                                     //\n+    int64_t num_ranks,                               //\n+    int64_t num_elements,                            //\n+    se::DeviceMemoryBase symmetric_signal_buffer,    //\n+    uint32_t signal_value,                           //\n+    se::DeviceMemoryBase metadata                    //\n );\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "1a5344730d245c2139de4cbabeb982b54fed851c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_test.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 29,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -36,13 +36,15 @@ limitations under the License.\n #include \"xla/literal_util.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/hlo_runner.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n+#include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -53,6 +55,7 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/types.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::gpu {\n@@ -101,39 +104,62 @@ class AllReduceKernelTest : public ::testing::Test,\n     TF_RETURN_IF_ERROR(executors[1]->EnablePeerAccessTo(executors[0]));\n \n     std::vector<std::unique_ptr<se::Stream>> streams;\n-    std::vector<se::DeviceMemoryHandle> local_input_buffers;\n-    std::vector<se::DeviceMemoryHandle> data_buffers;\n-    std::vector<se::DeviceMemoryHandle> signal_flags_buffers;\n-    std::vector<se::DeviceMemoryBase> remote_input_buffers_span;\n-    std::vector<se::DeviceMemoryBase> signal_flags_buffers_span;\n-\n+    std::vector<se::DeviceMemoryBase> allocated_buffers;\n+    std::vector<se::DeviceMemoryBase> local_input_buffers;\n+    std::vector<se::DeviceMemoryBase> data_buffers;\n+    std::vector<se::DeviceMemoryBase> signal_flags_buffers;\n+\n+    uint64_t input_size = num_elements * sizeof(T);\n+    uint64_t aligned_input_size =\n+        xla::RoundUpTo<uint64_t>(input_size, kXlaAllocatedBufferAlignBytes);\n+    uint64_t signal_size =\n+        num_ranks * launch_dimensions.num_blocks() * sizeof(int32_t);\n+    uint64_t aligned_signal_size =\n+        xla::RoundUpTo<uint64_t>(signal_size, kXlaAllocatedBufferAlignBytes);\n     for (int i = 0; i < num_ranks; ++i) {\n       auto* executor = executors[i];\n       streams.push_back(executor->CreateStream().value());\n \n+      uint64_t total_size =\n+          /*local_input_buffer_size=*/aligned_input_size +\n+          /*data_buffer_size=*/aligned_input_size +\n+          /*signal_buffer_size=*/aligned_signal_size;\n+      allocated_buffers.emplace_back(executor->AllocateArray<T>(total_size));\n       local_input_buffers.emplace_back(\n-          executor, executor->AllocateArray<T>(num_elements));\n-      TF_RET_CHECK(!local_input_buffers[i].memory().is_null());\n+          allocated_buffers[i].GetByteSlice(0, aligned_input_size));\n+      TF_RET_CHECK(!local_input_buffers[i].is_null());\n+\n+      data_buffers.emplace_back(allocated_buffers[i].GetByteSlice(\n+          aligned_input_size, aligned_input_size));\n+      TF_RET_CHECK(!data_buffers[i].is_null());\n+\n+      signal_flags_buffers.emplace_back(allocated_buffers[i].GetByteSlice(\n+          2 * aligned_input_size, aligned_signal_size));\n+      TF_RET_CHECK(!signal_flags_buffers[i].is_null());\n+      TF_RETURN_IF_ERROR(executor->SynchronousMemZero(&signal_flags_buffers[i],\n+                                                      aligned_signal_size));\n+      TF_RETURN_IF_ERROR(streams[i]->Memcpy(&local_input_buffers[i],\n+                                            input_data[i].data(), input_size));\n+    }\n \n-      data_buffers.emplace_back(executor,\n-                                executor->AllocateArray<T>(num_elements));\n-      TF_RET_CHECK(!data_buffers[i].memory().is_null());\n+    std::vector<se::DeviceMemoryBase> metadata_buffers;\n \n-      signal_flags_buffers.emplace_back(\n-          executor, executor->AllocateArray<uint32_t>(\n-                        num_ranks * launch_dimensions.num_blocks()));\n-      TF_RET_CHECK(!signal_flags_buffers[i].memory().is_null());\n+    for (int i = 0; i < num_ranks; ++i) {\n+      CollectiveKernelMetadata metadata;\n+      metadata.rank = i;\n \n-      TF_RETURN_IF_ERROR(executor->SynchronousMemZero(\n-          signal_flags_buffers[i].memory_ptr(),\n-          signal_flags_buffers[i].memory().size()));\n+      for (int j = 0; j < num_ranks; ++j) {\n+        // One-Shot all-reduce doesn't use an input buffer from the peers.\n+        metadata.buffer_root_ptrs[j] = 0;\n+        metadata.local_buffer_root_ptrs[j] =\n+            (uint64_t)allocated_buffers[j].opaque();\n+      }\n \n-      TF_RETURN_IF_ERROR(streams[i]->Memcpy(local_input_buffers[i].memory_ptr(),\n-                                            input_data[i].data(),\n-                                            num_elements * sizeof(T)));\n+      metadata_buffers.emplace_back(executors[i]->AllocateArray<uint64_t>(\n+          sizeof(CollectiveKernelMetadata)));\n \n-      remote_input_buffers_span.push_back(data_buffers[i].memory());\n-      signal_flags_buffers_span.push_back(signal_flags_buffers[i].memory());\n+      TF_RETURN_IF_ERROR(streams[i]->Memcpy(&metadata_buffers[i], &metadata,\n+                                            sizeof(CollectiveKernelMetadata)));\n     }\n \n     for (int i = 0; i < num_ranks; ++i) {\n@@ -147,15 +173,16 @@ class AllReduceKernelTest : public ::testing::Test,\n           primitive_util::NativeToPrimitiveType<T>(),\n           /*reduction_kind=*/reduction_kind,\n           /*all_reduce_strategy=*/params_.all_reduce_strategy,\n-          /*remote_input_buffers=*/remote_input_buffers_span,\n+          /*symmetric_input_buffer=*/data_buffers[i],\n           // Memory is aliased for both input and output (similar to what nccl\n           // would do).\n-          /*local_input_buffer=*/local_input_buffers[i].memory(),\n-          /*output_buffer=*/local_input_buffers[i].memory(),\n+          /*local_input_buffer=*/local_input_buffers[i],\n+          /*output_buffer=*/local_input_buffers[i],\n           /*rank=*/RankId(i), /*num_ranks=*/num_ranks,\n           /*num_elements=*/num_elements,\n-          /*signal_flags_buffers=*/signal_flags_buffers_span,\n-          /*signal_value=*/1));\n+          /*symmetric_signal_buffer=*/signal_flags_buffers[i],\n+          /*signal_value=*/1,\n+          /*metadata=*/metadata_buffers[i]));\n     }\n \n     for (int i = 0; i < num_ranks; ++i) {\n@@ -167,7 +194,7 @@ class AllReduceKernelTest : public ::testing::Test,\n     for (int i = 0; i < num_ranks; ++i) {\n       Array<T> output_results({num_elements});\n       TF_RETURN_IF_ERROR(streams[i]->Memcpy(output_results.data(),\n-                                            local_input_buffers[i].memory(),\n+                                            local_input_buffers[i],\n                                             num_elements * sizeof(T)));\n \n       results.push_back(std::move(output_results));"
        },
        {
            "sha": "01dd970b62ef14a0752a8b20ab6530ee9760bca9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 117,
            "deletions": 70,
            "changes": 187,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -35,16 +35,19 @@ limitations under the License.*/\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n+#include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::gpu {\n@@ -134,59 +137,77 @@ int64_t CollectiveKernelThunk::GetInputSizeBytes() const {\n              collective_config_.operand_element_type[0]);\n }\n \n-absl::Status CollectiveKernelThunk::RendezvousAfterInit(\n-    const GpuCliqueKey& clique_key, StreamState& state) {\n+struct BaseRangePtrRendezvousValue {\n+  RankId rank;\n+  se::DeviceMemoryBase locally_allocated_buffer_ptr;\n+  se::DeviceMemoryBase buffer_ptr;\n+\n+  bool operator<(const BaseRangePtrRendezvousValue& other) const {\n+    return rank < other.rank;\n+  }\n+};\n+\n+absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n+    const GpuCliqueKey& clique_key, StreamState& state,\n+    const InitializeParams& params) {\n+  BaseRangePtrRendezvousValue rendezvous_value;\n+  const std::optional<RankId> rank =\n+      clique_key.rank(params.collective_params->global_device_id);\n+  TF_RET_CHECK(rank.has_value())\n+      << \"Device \" << params.collective_params->global_device_id\n+      << \"is not in the clique.\";\n+  rendezvous_value.rank = rank.value();\n+  rendezvous_value.locally_allocated_buffer_ptr = state.local_buffer.memory();\n+  TF_ASSIGN_OR_RETURN(rendezvous_value.buffer_ptr,\n+                      params.executor->GetMemoryRange(\n+                          params.buffer_allocations->GetDeviceAddress(\n+                              buffers_[0].source_buffer)));\n+\n+  auto rendezvous_fn =\n+      [](absl::Span<const BaseRangePtrRendezvousValue* const> values) {\n+        std::vector<BaseRangePtrRendezvousValue> values_copy;\n+        for (const auto& value : values) {\n+          values_copy.push_back(*value);\n+        }\n+        // Sort to make sure that values are in the same order as the\n+        // devices are ordered in the communicator.\n+        absl::c_sort(values_copy);\n+        return values_copy;\n+      };\n   const int64_t num_ranks = clique_key.num_devices();\n   std::string start_rendezvous_key = absl::StrFormat(\n       \"Initializing one-shot all-reduce for device %d, clique %s\",\n-      state.device_ordinal, clique_key.ToString());\n-  // NB: This callback is called on one thread per participating group.\n-  // i.e.; If participating groups are {{0,1},{2,3}} then it would be called\n-  // twice. Once with StreamStates for 0,1 and once with StreamStates for 2,3.\n-  auto completion_fn = [](absl::Span<const StreamState*> states)\n-      -> std::vector<const StreamState*> {\n-    std::vector<const StreamState*> copy(states.begin(), states.end());\n-    // Sort by rank for stable order.\n-    absl::c_sort(copy,\n-                 [](const StreamState* const a, const StreamState* const b) {\n-                   return a->rank < b->rank;\n-                 });\n-    return copy;\n-  };\n-  TF_ASSIGN_OR_RETURN(\n-      std::shared_ptr<std::vector<const StreamState*>> rendezvous_values,\n-      Rendezvous<std::vector<const StreamState*>>(\n-          /*name=*/start_rendezvous_key, /*key=*/clique_key,\n-          /*value=*/state,\n-          /*num_threads=*/num_ranks, completion_fn));\n-\n-  // Sanity check to ensure that Rendezvous() was called only once.\n-  for (int i = 0; i < state.remote_buffer_ptrs.size(); ++i) {\n-    TF_RET_CHECK(state.remote_buffer_ptrs[i].empty())\n-        << \"Remote buffer ptrs was expected to be empty. Was: \"\n-        << state.remote_buffer_ptrs[i].size();\n-    TF_RET_CHECK(state.signal_buffer_ptrs[i].empty())\n-        << \"Signal buffer ptrs was expected to be empty. Was: \"\n-        << state.signal_buffer_ptrs[i].size();\n+      params.executor->device_ordinal(), clique_key.ToString());\n+  TF_ASSIGN_OR_RETURN(std::shared_ptr<std::vector<BaseRangePtrRendezvousValue>>\n+                          rendezvous_values,\n+                      Rendezvous<std::vector<BaseRangePtrRendezvousValue>>(\n+                          /*name=*/\n+                          start_rendezvous_key, /*key=*/clique_key,\n+                          /*value=*/rendezvous_value, /*num_threads=*/num_ranks,\n+                          rendezvous_fn));\n+\n+  if (rendezvous_values->size() > CollectiveKernelMetadata::kMaxNumDevices) {\n+    return absl::InvalidArgumentError(\n+        absl::StrFormat(\"Multi-device kernels require at most %d peers.\",\n+                        CollectiveKernelMetadata::kMaxNumDevices));\n   }\n-  for (auto* rendezvous_state : *rendezvous_values) {\n-    // NB: This is a double buffer allocation. So size of a single buffer is\n-    // half of the total allocation.\n-    const int64_t buffer_size =\n-        rendezvous_state->local_buffer.memory().size() / kNumBuffers;\n-    const int64_t signal_buffer_size =\n-        rendezvous_state->signal_buffer.memory().size() / kNumBuffers;\n-    for (int i = 0; i < state.remote_buffer_ptrs.size(); ++i) {\n-      state.remote_buffer_ptrs[i].emplace_back(\n-          rendezvous_state->local_buffer.memory().GetByteSlice(\n-              /*offset_bytes=*/i * buffer_size,\n-              /*size_bytes=*/buffer_size));\n-      state.signal_buffer_ptrs[i].emplace_back(\n-          rendezvous_state->signal_buffer.memory().GetByteSlice(\n-              /*offset_bytes=*/i * signal_buffer_size,\n-              /*size_bytes=*/signal_buffer_size));\n-    }\n+  CollectiveKernelMetadata metadata;\n+  metadata.rank = rank.value().value();\n+  for (int i = 0; i < rendezvous_values->size(); ++i) {\n+    metadata.local_buffer_root_ptrs[i] =\n+        (uint64_t)rendezvous_values->at(i)\n+            .locally_allocated_buffer_ptr.opaque();\n+    metadata.buffer_root_ptrs[i] =\n+        (uint64_t)rendezvous_values->at(i).buffer_ptr.opaque();\n   }\n+\n+  se::DeviceMemoryBase metadata_ptr =\n+      params.executor->Allocate(sizeof(CollectiveKernelMetadata), 0);\n+  TF_RETURN_IF_ERROR(params.stream->Memcpy(&metadata_ptr, (void*)&metadata,\n+                                           sizeof(CollectiveKernelMetadata)));\n+  TF_RETURN_IF_ERROR(params.stream->BlockHostUntilDone());\n+\n+  state.metadata = metadata_ptr;\n   return absl::OkStatus();\n }\n \n@@ -202,47 +223,62 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n   const LaunchDimensions launch_dimensions = AllReduceLaunchDimensions(\n       buffers_[0].element_count, clique_key.num_local_participants(),\n       GetAllReduceStrategy(GetInputSizeBytes()));\n+\n   StreamState* state = nullptr;\n   {\n     absl::MutexLock lock(mutex_);\n     if (!per_stream_state_.contains(params.executor)) {\n-      // Step1: Allocate local buffer\n-      TF_ASSIGN_OR_RETURN(\n-          se::DeviceMemoryHandle local_buffer_alloc,\n-          AllocateMemory(params.executor,\n-                         buffers_[0].source_buffer.size() * kNumBuffers,\n-                         \"LocalBuffer\"));\n-\n-      // Step2: Allocate signal buffer\n-      // We needs 1 atomic flag per block per device on each device.\n+      // Step1: Allocate signal and local buffers.\n       const int64_t kNumSignalFlags =\n           clique_key.num_local_participants() * launch_dimensions.num_blocks();\n+\n+      int64_t kSignalBufferSize = xla::RoundUpTo<uint64_t>(\n+          kNumSignalFlags * sizeof(int32_t), kXlaAllocatedBufferAlignBytes);\n+      const int64_t kLocalBufferSize = xla::RoundUpTo<uint64_t>(\n+          buffers_[0].source_buffer.size(), kXlaAllocatedBufferAlignBytes);\n       TF_ASSIGN_OR_RETURN(\n-          se::DeviceMemoryHandle signal_flags_alloc,\n+          se::DeviceMemoryHandle local_buffer_alloc,\n           AllocateMemory(params.executor,\n-                         kNumSignalFlags * sizeof(int32_t) * kNumBuffers,\n-                         \"SignalBuffer\"));\n+                         (kSignalBufferSize + kLocalBufferSize) * kNumBuffers,\n+                         \"Local and Signal buffers\"));\n+\n+      // Step2: We needs 1 atomic flag per block per device on each device.\n       // One-shot kernel expects that the signal flags buffer is zeroed out.\n       // Initial state of device memory is undefined, so we need to zero out\n       // the buffer. The kernel will take care of leaving the buffer in\n       // correct state after use, so we don't need to zero out after\n       // initialization.\n       TF_RETURN_IF_ERROR(params.executor->SynchronousMemZero(\n-          signal_flags_alloc.memory_ptr(), signal_flags_alloc.memory().size()));\n+          local_buffer_alloc.memory_ptr(), local_buffer_alloc.memory().size()));\n \n       // Step3: Emplace into the stream state.\n       per_stream_state_.emplace(\n-          params.executor,\n-          std::make_unique<StreamState>(\n-              params.executor->device_ordinal(), rank.value(),\n-              std::move(local_buffer_alloc), std::move(signal_flags_alloc)));\n+          params.executor, std::make_unique<StreamState>(\n+                               params.executor->device_ordinal(), rank.value(),\n+                               std::move(local_buffer_alloc)));\n       state = per_stream_state_.at(params.executor).get();\n+\n+      // NB: This is a double buffer allocation. So size of a single buffer is\n+      // half of the total allocation.\n+      for (int i = 0; i < kNumBuffers; ++i) {\n+        uint64_t offset = i * (kLocalBufferSize + kSignalBufferSize);\n+        state->remote_buffer_ptrs[i] =\n+            state->local_buffer.memory_ptr()->GetByteSlice(\n+                /*offset_bytes=*/offset,\n+                /*size_bytes=*/kLocalBufferSize);\n+\n+        state->signal_buffer_ptrs[i] =\n+            state->local_buffer.memory_ptr()->GetByteSlice(\n+                /*offset_bytes=*/offset + kLocalBufferSize,\n+                /*size_bytes=*/kSignalBufferSize);\n+      }\n     }\n   }\n-  // Only invoke rendezvous if a new state was initialized.\n+\n   if (state != nullptr) {\n-    return RendezvousAfterInit(clique_key, *state);\n+    TF_RETURN_IF_ERROR(ExchangeStateMetadata(clique_key, *state, params));\n   }\n+\n   return absl::OkStatus();\n }\n \n@@ -285,6 +321,7 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n         << \"Stream not found in per_stream_state_\";\n     state = it->second.get();\n   }\n+\n   const uint32_t buffer_index = state->invocation_count % kNumBuffers;\n   auto const strategy = GetAllReduceStrategy(GetInputSizeBytes());\n   const LaunchDimensions launch_dimensions =\n@@ -294,21 +331,31 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n   VLOG(3) << \"[\" << device_ordinal\n           << \"] Performing one-shot all-reduce for clique \"\n           << clique_key.ToString();\n+\n+  se::DeviceMemoryBase input_buffer_ptr =\n+      state->remote_buffer_ptrs[buffer_index];\n+  se::DeviceMemoryBase signal_buffer_ptr =\n+      state->signal_buffer_ptrs[buffer_index];\n+  VLOG(3) << \"[\" << device_ordinal\n+          << \"] input_buffer_ptr: \" << (uint64_t)input_buffer_ptr.opaque()\n+          << \" signal_buffer_ptr: \" << (uint64_t)signal_buffer_ptr.opaque();\n+\n   // TODO(b/407736956): Change this to emitted kernel.\n   return RunAllReduceKernel(\n       /*stream=*/stream,\n       /*launch_dimensions=*/launch_dimensions,\n       /*element_type=*/element_type,\n       /*reduction_kind=*/reduction_kind_,\n       /*all_reduce_strategy=*/strategy,\n-      /*remote_input_buffers=*/state->remote_buffer_ptrs[buffer_index],\n+      /*symmetric_input_buffer=*/input_buffer_ptr,\n       /*local_input_buffer=*/source_buffer,\n       /*output_buffer=*/destination_buffer,\n       /*rank=*/rank.value(),\n       /*num_ranks=*/kNumRanks,\n       /*num_elements=*/buffer.element_count,\n-      /*signal_flags_buffers=*/state->signal_buffer_ptrs[buffer_index],\n-      /*signal_value=*/state->invocation_count);\n+      /*symmetric_signal_buffer=*/signal_buffer_ptr,\n+      /*signal_value=*/state->invocation_count,\n+      /*metadata=*/state->metadata);\n }\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "b9f7c4612725746c887bf8dbd08d743722762acc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -99,36 +99,34 @@ class CollectiveKernelThunk : public Thunk {\n     //   before they can sync on the second invocation.\n     // - Alternate back to Buffer 0 on third invocation. And so on.\n     se::DeviceMemoryHandle local_buffer;\n-    se::DeviceMemoryHandle signal_buffer;\n+\n+    // Pointer to the collective kernel metadata on device.\n+    se::DeviceMemoryBase metadata;\n+\n     // These vectors are merely pointers into the buffer(s) above ordered\n     // by RankId. They are initialized once at the end of Initialize() and never\n     // changed.\n-    std::array<absl::InlinedVector<se::DeviceMemoryBase, kMaxNumExecutors>,\n-               kNumBuffers>\n-        remote_buffer_ptrs{};\n-    std::array<absl::InlinedVector<se::DeviceMemoryBase, kMaxNumExecutors>,\n-               kNumBuffers>\n-        signal_buffer_ptrs{};\n+    std::array<se::DeviceMemoryBase, kNumBuffers> remote_buffer_ptrs;\n+    std::array<se::DeviceMemoryBase, kNumBuffers> signal_buffer_ptrs;\n     uint32_t invocation_count = 0;\n \n     // Constructor to make OSS builds happy.\n     StreamState() = default;\n     StreamState(int device_ordinal_arg, RankId rank_arg,\n-                se::DeviceMemoryHandle local_buffer_arg,\n-                se::DeviceMemoryHandle signal_buffer_arg)\n+                se::DeviceMemoryHandle local_buffer_arg)\n         : device_ordinal(device_ordinal_arg),\n           rank(rank_arg),\n-          local_buffer(std::move(local_buffer_arg)),\n-          signal_buffer(std::move(signal_buffer_arg)) {}\n+          local_buffer(std::move(local_buffer_arg)) {}\n   };\n \n   // Returns the input size in bytes for the collective.\n   int64_t GetInputSizeBytes() const;\n \n   // Internal method to sync thread after Initialize.\n-  // Modifies the state to include pointers to all buffers in the clique.\n-  absl::Status RendezvousAfterInit(const GpuCliqueKey& clique_key,\n-                                   StreamState& state);\n+  // Returns the collective kernel metadata for the given clique key.\n+  absl::Status ExchangeStateMetadata(const GpuCliqueKey& clique_key,\n+                                     StreamState& state,\n+                                     const InitializeParams& params);\n \n   // Whether the one-shot kernel is enabled.\n   const bool collective_kernel_enabled_;"
        },
        {
            "sha": "2326a2639c7047565ee2e0c9d4fad216a58a381a",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -858,11 +858,17 @@ cc_library(\n     name = \"all_reduce_kernel\",\n     hdrs = [\"all_reduce_kernel.h\"],\n     deps = [\n+        \":collective_kernel_metadata\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n \n+cc_library(\n+    name = \"collective_kernel_metadata\",\n+    hdrs = [\"collective_kernel_metadata.h\"],\n+)\n+\n cc_library(\n     name = \"topk_kernel\",\n     hdrs = [\"topk_kernel.h\"],"
        },
        {
            "sha": "eaf75c7c81b07320a9b28ee34b0e681bd1de9ad3",
            "filename": "third_party/xla/xla/stream_executor/gpu/all_reduce_kernel.h",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel.h?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n \n #include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n@@ -52,8 +53,8 @@ using RestrictedPtr = U* __restrict__;\n \n template <typename T>\n struct AllReduceKernelParams {\n-  // Shared buffers of all devices ordered by rank.\n-  std::array<RestrictedPtr<T>, kMaxNumAllReduceInputPtrs> remote_input_buffers;\n+  // Pointer to the input buffer which is symmetric around peer ranks.\n+  RestrictedPtr<T> symmetric_input_ptrs = nullptr;\n   // Local buffer of the device.\n   RestrictedPtr<T> input_buffer;\n   // Output buffer of the device. Can be the same as the local input buffer in\n@@ -77,12 +78,17 @@ struct AllReduceKernelParams {\n   // Ranks rotated by `rank` % `num_ranks` to circumvent all GPUs reading from\n   // the same location simultaneously. Index 0 is the rank itself.\n   std::array<int64_t, kMaxNumAllReduceInputPtrs> rotated_ranks;\n-  // Signal flags buffers of all devices ordered by rank.\n-  std::array<RestrictedPtr<uint32_t>, kMaxNumAllReduceInputPtrs>\n-      signal_flags_buffers;\n+\n   // Value to be written to the signal flags. Should be different for different\n   // invocations of the kernel with the same signal buffer.\n   uint32_t signal_value;\n+\n+  // Pointer to the signal flags buffer which is symmetric around peer ranks.\n+  // TODO(446447767): Remove this once we have a single pointer to symmetric\n+  // memory.\n+  RestrictedPtr<uint32_t> symmetric_signal_ptrs = nullptr;\n+\n+  RestrictedPtr<CollectiveKernelMetadata> metadata;\n };\n \n // Defines a trait for the AllReduce kernel that can be used to register"
        },
        {
            "sha": "823b32c68c8b48685f81c4110046eb54d9c4df3b",
            "filename": "third_party/xla/xla/stream_executor/gpu/all_reduce_kernel_lib.cu.h",
            "status": "modified",
            "additions": 52,
            "deletions": 17,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -84,6 +84,16 @@ __device__ __forceinline__ void VecOp(Vec<T>& res, const Vec<T>& vec) {\n   res.data[3] = ApplyBinaryOp<T, ReductionKindT>(res.data[3], vec.data[3]);\n }\n \n+template <typename T>\n+__device__ __forceinline__ RestrictedPtr<T> GetPeerPtr(\n+    void* ptr, int64_t peer_rank, const CollectiveKernelMetadata& metadata) {\n+  uint64_t current_base = metadata.local_buffer_root_ptrs[metadata.rank];\n+  uint64_t offset = (uint64_t)ptr - current_base;\n+\n+  return (RestrictedPtr<T>)(metadata.local_buffer_root_ptrs[peer_rank] +\n+                            offset);\n+}\n+\n template <PlatformType T = PlatformType::NOGPU>\n __device__ __forceinline__ void PutSignalFlag(uint32_t* addr, uint32_t val) {}\n \n@@ -111,30 +121,42 @@ template <typename T, xla::ReductionKind ReductionKindT,\n           PlatformType PlatformT = PlatformType::NOGPU>\n __device__ __forceinline__ void OneShotAllReduceKernelImpl(\n     const AllReduceKernelParams<T>& args) {\n+  __shared__ std::array<RestrictedPtr<uint32_t>, kMaxNumAllReduceInputPtrs>\n+      signal_flags_buffers;\n+  __shared__ std::array<RestrictedPtr<T>, kMaxNumAllReduceInputPtrs>\n+      remote_input_buffers;\n+\n+  if (threadIdx.x < kMaxNumAllReduceInputPtrs) {\n+    signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n+        args.symmetric_signal_ptrs, threadIdx.x, *args.metadata);\n+    remote_input_buffers[threadIdx.x] =\n+        GetPeerPtr<T>(args.symmetric_input_ptrs, threadIdx.x, *args.metadata);\n+  }\n+\n+  __syncthreads();\n+\n   int64_t offset =\n       kNumElementsPerThread * (blockIdx.x * blockDim.x + threadIdx.x);\n   int64_t stride = kNumElementsPerThread * blockDim.x * gridDim.x;\n \n   // Copy data from local input buffer to remote input buffer.\n   for (int i = offset; i < args.num_elements; i += stride) {\n-    VecStore(args.remote_input_buffers[args.rank] + i,\n-             VecLoad(args.input_buffer + i));\n+    VecStore(args.symmetric_input_ptrs + i, VecLoad(args.input_buffer + i));\n   }\n \n-  SyncRemoteBlocks<PlatformT>(args.signal_flags_buffers, args.rank,\n-                              args.num_ranks, args.signal_value);\n+  SyncRemoteBlocks<PlatformT>(signal_flags_buffers, args.rank, args.num_ranks,\n+                              args.signal_value);\n   __syncthreads();\n \n   for (int i = offset; i < args.num_elements; i += stride) {\n-    Vec<T> acc = VecLoad(args.remote_input_buffers[0] + i);\n+    Vec<T> acc = VecLoad(remote_input_buffers[0] + i);\n \n     // Since `remote_input_buffers` are provided in rank order, we get stable\n     // reduction results on all devices.\n #pragma unroll\n     for (int j = 1; j < kMaxNumAllReduceInputPtrs; ++j) {\n       if (j < args.num_ranks) {\n-        VecOp<T, ReductionKindT>(acc,\n-                                 VecLoad(args.remote_input_buffers[j] + i));\n+        VecOp<T, ReductionKindT>(acc, VecLoad(remote_input_buffers[j] + i));\n       }\n     }\n \n@@ -146,6 +168,20 @@ template <typename T, xla::ReductionKind ReductionKindT,\n           PlatformType PlatformT = PlatformType::NOGPU>\n __device__ __forceinline__ void TwoShotAllReduceKernelImpl(\n     const AllReduceKernelParams<T>& args) {\n+  __shared__ std::array<RestrictedPtr<uint32_t>, kMaxNumAllReduceInputPtrs>\n+      signal_flags_buffers;\n+  __shared__ std::array<RestrictedPtr<T>, kMaxNumAllReduceInputPtrs>\n+      remote_input_buffers;\n+\n+  if (threadIdx.x < kMaxNumAllReduceInputPtrs) {\n+    signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n+        args.symmetric_signal_ptrs, threadIdx.x, *args.metadata);\n+    remote_input_buffers[threadIdx.x] =\n+        GetPeerPtr<T>(args.symmetric_input_ptrs, threadIdx.x, *args.metadata);\n+  }\n+\n+  __syncthreads();\n+\n   const int64_t offset = blockIdx.x * args.num_elements_per_block +\n                          threadIdx.x * kNumElementsPerThread;\n   const int64_t offset_end =\n@@ -167,15 +203,15 @@ __device__ __forceinline__ void TwoShotAllReduceKernelImpl(\n       if (offset_i >= args.num_elements) {\n         continue;\n       }\n-      VecStore(args.remote_input_buffers[args.rank] + offset_i,\n+      VecStore(remote_input_buffers[args.rank] + offset_i,\n                VecLoad(args.input_buffer + offset_i));\n     }\n   }\n \n   // Shot1: Wait for all participating devices to finish copying data to their\n   // shared buffer.\n-  SyncRemoteBlocks<PlatformT>(args.signal_flags_buffers, args.rank,\n-                              args.num_ranks, args.signal_value);\n+  SyncRemoteBlocks<PlatformT>(signal_flags_buffers, args.rank, args.num_ranks,\n+                              args.signal_value);\n   __syncthreads();\n \n   // Step2: Accumulate data for the responsible indices in the shared buffers.\n@@ -193,7 +229,7 @@ __device__ __forceinline__ void TwoShotAllReduceKernelImpl(\n         continue;\n       }\n       accs[args.rotated_ranks[r]] =\n-          VecLoad(args.remote_input_buffers[args.rotated_ranks[r]] + offset_i);\n+          VecLoad(remote_input_buffers[args.rotated_ranks[r]] + offset_i);\n     }\n \n     Vec<T> acc = accs[0];\n@@ -206,14 +242,14 @@ __device__ __forceinline__ void TwoShotAllReduceKernelImpl(\n       }\n       VecOp<T, ReductionKindT>(acc, accs[r]);\n     }\n-    VecStore(args.remote_input_buffers[args.rank] + offset_i, acc);\n+    VecStore(remote_input_buffers[args.rank] + offset_i, acc);\n   }\n \n   // Shot2: Wait for all participating devices to finish accumulating data in\n   // the shared buffer. Note that signal_value + 1 is used to ensure that the\n   // synchronization is different from the one used above.\n-  SyncRemoteBlocks<PlatformT>(args.signal_flags_buffers, args.rank,\n-                              args.num_ranks, args.signal_value + 1);\n+  SyncRemoteBlocks<PlatformT>(signal_flags_buffers, args.rank, args.num_ranks,\n+                              args.signal_value + 1);\n   __syncthreads();\n \n   // Step3: Copy data from the shared buffers to the output buffer.\n@@ -228,9 +264,8 @@ __device__ __forceinline__ void TwoShotAllReduceKernelImpl(\n       if (offset_i >= args.num_elements) {\n         continue;\n       }\n-      VecStore(\n-          args.output_buffer + offset_i,\n-          VecLoad(args.remote_input_buffers[args.rotated_ranks[r]] + offset_i));\n+      VecStore(args.output_buffer + offset_i,\n+               VecLoad(remote_input_buffers[args.rotated_ranks[r]] + offset_i));\n     }\n   }\n }"
        },
        {
            "sha": "e0b9906e29069c28f63175a347de3c7ada51854e",
            "filename": "third_party/xla/xla/stream_executor/gpu/collective_kernel_metadata.h",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb74e857fb32d0d17797a509078ab21a7252465/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h?ref=5fb74e857fb32d0d17797a509078ab21a7252465",
            "patch": "@@ -0,0 +1,40 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_GPU_COLLECTIVE_KERNEL_METADATA_H_\n+#define XLA_STREAM_EXECUTOR_GPU_COLLECTIVE_KERNEL_METADATA_H_\n+\n+#include <stdint.h>\n+\n+// Metadata parameter which is passed to the collective kernel.\n+// The metadata allows to compute the address of a peer's buffer in the\n+// collective kernel and get the current rank of a peer device.\n+// Right now two root pointers are getting passed. One is used for buffers\n+// allocated by the buffer assignment and allows kernel to address input and\n+// output buffers. The second one is used for buffers allocated within the\n+// collective kernel thunk.\n+// TODO(patrios): Unify two root pointers once symmetric memory allocator will\n+// be implemented.\n+struct CollectiveKernelMetadata {\n+  constexpr static int kMaxNumDevices = 8;\n+  int64_t rank;\n+  // Root pointer for buffers allocated by the buffer assignment.\n+  int64_t buffer_root_ptrs[kMaxNumDevices];\n+\n+  // Root pointer for buffers allocated by the collective kernel thunk.\n+  int64_t local_buffer_root_ptrs[kMaxNumDevices];\n+};\n+\n+#endif  // XLA_STREAM_EXECUTOR_GPU_COLLECTIVE_KERNEL_METADATA_H_"
        }
    ],
    "stats": {
        "total": 539,
        "additions": 349,
        "deletions": 190
    }
}