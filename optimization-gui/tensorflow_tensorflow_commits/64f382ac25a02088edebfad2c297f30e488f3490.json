{
    "author": "majiddadashi",
    "message": "Add support for kTfLiteInt2 (srq) in tfl.fully_connected.\n\nPiperOrigin-RevId: 822405584",
    "sha": "64f382ac25a02088edebfad2c297f30e488f3490",
    "files": [
        {
            "sha": "5b701e674dc66c377a3d00c89fbb249f2fffbea4",
            "filename": "tensorflow/compiler/mlir/lite/ir/tfl_ops.td",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -1100,7 +1100,7 @@ def TFL_FullyConnectedOp : TFL_Op<\"fully_connected\", [\n \n   let arguments = (ins\n     TFL_TensorOf<[F32, QI8, QUI8, QI16, QUI16]>:$input,\n-    TFL_TensorOf<[F32, QI4, QI8, QUI8, QI16]>:$filter,\n+    TFL_TensorOf<[F32, QI2, QI4, QI8, QUI8, QI16]>:$filter,\n     TFL_TensorOfOrNone<[F32, QI32, QUI32]>:$bias,\n \n     TFL_AFAttr:$fused_activation_function,"
        },
        {
            "sha": "6a238409ea8c14f9794db7794548c421cdb52a87",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/op_version.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -177,6 +177,10 @@ int GetBuiltinOperatorVersion(const OpSignature& op_sig) {\n           reinterpret_cast<TfLiteFullyConnectedParams*>(op_sig.builtin_data);\n       TFLITE_DCHECK(fully_connected_params != nullptr);\n \n+      if (op_sig.inputs.at(1).type == kTfLiteInt2) {\n+        return 14;\n+      }\n+\n       if (op_sig.inputs.at(0).type == kTfLiteInt16 &&\n           op_sig.inputs.at(1).type == kTfLiteInt4 &&\n           op_sig.outputs.at(0).type == kTfLiteInt16) {"
        },
        {
            "sha": "87313665d1811fde80329d82d41be253ebf01f4e",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/op_version_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -733,6 +733,15 @@ TEST(OpVersionTest, VersioningFullyConnectedTest) {\n   };\n   fake_op_sig.ext_options.fully_connected.is_per_channel_quantized = true;\n   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 12);\n+\n+  fake_op_sig = {\n+      .op = BuiltinOperator_FULLY_CONNECTED,\n+      .inputs = CreateOpSignatureTensorSpecs(\n+          std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt2}),\n+      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),\n+      .builtin_data = reinterpret_cast<void*>(&fully_connected_params),\n+  };\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 14);\n }\n \n TEST(OpVersionTest, VersioningDequantizeTest) {"
        },
        {
            "sha": "54702a97d7a57da83d3468ee9c13cf86ede17b67",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/runtime_version.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -139,6 +139,7 @@ std::string FindMinimumRuntimeVersionForOp(tflite::BuiltinOperator op_code,\n               {{BuiltinOperator_FULLY_CONNECTED, 11}, \"2.15.0\"},\n               {{BuiltinOperator_FULLY_CONNECTED, 12}, \"2.17.0\"},\n               {{BuiltinOperator_FULLY_CONNECTED, 13}, \"2.18.0\"},\n+              {{BuiltinOperator_FULLY_CONNECTED, 14}, \"2.21.0\"},\n               {{BuiltinOperator_GATHER, 1}, \"1.6.0\"},\n               {{BuiltinOperator_GATHER, 2}, \"1.14.0\"},\n               {{BuiltinOperator_GATHER, 3}, \"1.15.0\"},"
        },
        {
            "sha": "2c13edae231f105ba28f559ba03bfd7fc2c75bd9",
            "filename": "tensorflow/lite/core/kernels/register.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -83,7 +83,7 @@ BuiltinOpResolver::BuiltinOpResolver() {\n              Register_EMBEDDING_LOOKUP_SPARSE());\n   AddBuiltin(BuiltinOperator_FULLY_CONNECTED, Register_FULLY_CONNECTED(),\n              /* min_version = */ 1,\n-             /* max_version = */ 13);\n+             /* max_version = */ 14);\n   AddBuiltin(BuiltinOperator_LSH_PROJECTION, Register_LSH_PROJECTION());\n   AddBuiltin(BuiltinOperator_HASHTABLE_LOOKUP, Register_HASHTABLE_LOOKUP());\n   AddBuiltin(BuiltinOperator_SOFTMAX, Register_SOFTMAX(),"
        },
        {
            "sha": "2decbb76ffcfaadacac0a3ccad53471df6b9469f",
            "filename": "tensorflow/lite/kernels/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2FBUILD?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -2225,6 +2225,7 @@ cc_test(\n         \"//tensorflow/lite/core/api\",\n         \"//tensorflow/lite/kernels/internal:tensor_utils\",\n         \"//tensorflow/lite/schema:schema_fbs\",\n+        \"@com_google_absl//absl/log:absl_check\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_googletest//:gtest\",\n         \"@flatbuffers\","
        },
        {
            "sha": "dcce5022c49dbb97291e3fce19993aee7f32338e",
            "filename": "tensorflow/lite/kernels/fully_connected.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 12,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -186,7 +186,7 @@ inline TfLiteStatus CheckTypes(TfLiteContext* context,\n                                TfLiteFullyConnectedParams* params) {\n   const bool is_quantized =\n       ((filter->type == kTfLiteUInt8) || (filter->type == kTfLiteInt8) ||\n-       (filter->type == kTfLiteInt4));\n+       (filter->type == kTfLiteInt4) || (filter->type == kTfLiteInt2));\n   const bool is_hybrid = is_quantized && (input->type == kTfLiteFloat32);\n   const bool is_shuffled =\n       is_quantized && (params->weights_format ==\n@@ -448,7 +448,8 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node,\n       TF_LITE_ENSURE(context,\n                      input->type == kTfLiteInt8 || input->type == kTfLiteInt16);\n       TF_LITE_ENSURE(context, (filter->type == kTfLiteInt8 ||\n-                               filter->type == kTfLiteInt4));\n+                               filter->type == kTfLiteInt4 ||\n+                               filter->type == kTfLiteInt2));\n       TF_LITE_ENSURE_EQ(context, affine_quantization->scale->size,\n                         per_channel_quantization_size);\n       TF_LITE_ENSURE_EQ(\n@@ -654,7 +655,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   const bool is_quantized =\n       ((filter->type == kTfLiteUInt8) || (filter->type == kTfLiteInt8) ||\n-       (filter->type == kTfLiteInt4));\n+       (filter->type == kTfLiteInt4) || (filter->type == kTfLiteInt2));\n   const bool is_hybrid = is_quantized && (input->type == kTfLiteFloat32);\n   const bool is_pie = kernel_type == kLegacyPie;\n \n@@ -666,7 +667,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                                 params->activation == kTfLiteActReluN1To1 ||\n                                 params->activation == kTfLiteActRelu6);\n   }\n-  if (filter->type == kTfLiteInt4) {\n+  if (filter->type == kTfLiteInt4 || filter->type == kTfLiteInt2) {\n     TF_LITE_ENSURE_MSG(\n         context,\n         kTfLiteOk == VerifyQuantizationZeroPoint(filter, /*expected_value=*/0),\n@@ -1420,6 +1421,7 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n       case kTfLiteUInt8:\n         if (kernel_type == kReference) {\n           TF_LITE_ENSURE(context, filter->type != kTfLiteInt4);\n+          TF_LITE_ENSURE(context, filter->type != kTfLiteInt2);\n           reference_ops::FullyConnected(\n               op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n               GetTensorShape(filter), GetTensorData<uint8_t>(filter),\n@@ -1456,8 +1458,10 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                 \"Invalid quantized and sparse fully-connected format.\");\n             return kTfLiteError;\n           }\n-          // Int4 support for sparse filter tensor is currently not supported\n+          // Int4/Int2 support for sparse filter tensor is currently not\n+          // supported\n           TF_LITE_ENSURE(context, filter->type != kTfLiteInt4);\n+          TF_LITE_ENSURE(context, filter->type != kTfLiteInt2);\n           if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\n               sparsity.dim_metadata[2].dense_size == 16) {\n             // Block sparse with block size of 1x16.\n@@ -1485,6 +1489,14 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                 GetTensorShape(filter).FlatSize(), /*bit_width=*/4,\n                 unpacked_filter_data.get());\n             filter_data = unpacked_filter_data.get();\n+          } else if (filter->type == kTfLiteInt2) {\n+            const size_t bytes_unpacked = filter->bytes * 4;\n+            unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n+            tflite::tensor_utils::UnpackPackedIntToInt8(\n+                GetTensorData<int8_t>(filter),\n+                GetTensorShape(filter).FlatSize(), /*bit_width=*/2,\n+                unpacked_filter_data.get());\n+            filter_data = unpacked_filter_data.get();\n           } else {\n             filter_data = GetTensorData<int8_t>(filter);\n           }\n@@ -1514,6 +1526,14 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                 GetTensorShape(filter).FlatSize(), /*bit_width=*/4,\n                 unpacked_filter_data.get());\n             filter_data = unpacked_filter_data.get();\n+          } else if (filter->type == kTfLiteInt2) {\n+            const size_t bytes_unpacked = filter->bytes * 4;\n+            unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n+            tflite::tensor_utils::UnpackPackedIntToInt8(\n+                GetTensorData<int8_t>(filter),\n+                GetTensorShape(filter).FlatSize(), /*bit_width=*/2,\n+                unpacked_filter_data.get());\n+            filter_data = unpacked_filter_data.get();\n           } else {\n             filter_data = GetTensorData<int8_t>(filter);\n           }\n@@ -1762,14 +1782,8 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n         return kTfLiteError;\n       }\n     case kTfLiteInt8:\n-      if (params->weights_format == kTfLiteFullyConnectedWeightsFormatDefault) {\n-        return EvalQuantized<kernel_type>(context, node, params, data, input,\n-                                          filter, bias, output);\n-      } else {\n-        TF_LITE_KERNEL_LOG(context, \"Unhandled fully-connected weights format\");\n-        return kTfLiteError;\n-      }\n     case kTfLiteInt4:\n+    case kTfLiteInt2:\n       if (params->weights_format == kTfLiteFullyConnectedWeightsFormatDefault) {\n         return EvalQuantized<kernel_type>(context, node, params, data, input,\n                                           filter, bias, output);"
        },
        {
            "sha": "bf707d135e4eca9fb58cab37c8cbef64cd21d086",
            "filename": "tensorflow/lite/kernels/fully_connected_test.cc",
            "status": "modified",
            "additions": 127,
            "deletions": 10,
            "changes": 137,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Ffully_connected_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Ffully_connected_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ffully_connected_test.cc?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/log/absl_check.h\"\n #include \"tensorflow/lite/core/interpreter.h\"\n #include \"tensorflow/lite/kernels/test_util.h\"\n #include \"tensorflow/lite/schema/schema_generated.h\"\n@@ -159,22 +160,34 @@ class BaseFullyConnectedOpModel : public SingleOpModel {\n       std::vector<int64_t> per_channel_quantization_offsets(\n           per_channel_quantization_scales.size(), 0);\n       weights_ = AddInput({filter_type,\n-                           {units_, input_size_},\n-                           0,\n-                           0,\n-                           0,\n-                           0,\n-                           true,\n+                           /*shape=*/{units_, input_size_},\n+                           /*min=*/0,\n+                           /*max=*/0,\n+                           /*scale=*/0,\n+                           /*zero_point=*/0,\n+                           /*per_channel_quantization=*/true,\n                            per_channel_quantization_scales,\n                            per_channel_quantization_offsets,\n-                           0});\n+                           /*channel_index=*/0});\n     } else {\n       // per-tensor\n       float min = input.min;\n       float max = input.max;\n-      if (filter_type == TensorType_INT4 || filter_type == TensorType_INT8) {\n-        min = filter_type == TensorType_INT4 ? -7.f : -63.5f;\n-        max = filter_type == TensorType_INT4 ? 7.f : 64.f;\n+      switch (filter_type) {\n+        case TensorType_INT4:\n+          min = -7.f;\n+          max = 7.f;\n+          break;\n+        case TensorType_INT2:\n+          min = -2.f;\n+          max = 2.f;\n+          break;\n+        case TensorType_INT8:\n+          min = -63.5f;\n+          max = 64.f;\n+          break;\n+        default:\n+          break;\n       }\n       weights_ = AddInput({filter_type, {units_, input_size_}, min, max});\n     }\n@@ -292,6 +305,13 @@ class QuantizedFullyConnectedOpModel : public BaseFullyConnectedOpModel {\n     QuantizeAndPopulate4bit(weights_, data);\n   }\n \n+  void SetWeights2bit(const std::vector<float>& data) {\n+    TfLiteTensor* t = interpreter_->tensor(weights_);\n+    std::vector<int8_t> u =\n+        Quantize<int8_t>(data, t->params.scale, t->params.zero_point, t->type);\n+    PopulateTensor2bit(weights_, 0, u.data(), u.data() + u.size());\n+  }\n+\n   template <typename T>\n   void ShuffleAndSetWeights(const std::vector<float>& data, int input_depth,\n                             int output_depth) {\n@@ -372,6 +392,12 @@ class PerChannelQuantizedFullyConnectedOpModel\n     PerChannelSymmetricQuantizeAndPopulate(weights_, data);\n   }\n \n+  void SetWeights2bit(const std::vector<float>& data) {\n+    // 2 bit logic handled in PerChannelSymmetricQuantizeAndPopulate.\n+    ABSL_CHECK_EQ(interpreter_->tensor(weights_)->type, kTfLiteInt2);\n+    PerChannelSymmetricQuantizeAndPopulate(weights_, data);\n+  }\n+\n   template <typename T>\n   void SetInput(const std::vector<float>& data) {\n     QuantizeAndPopulate<T>(input_, data);\n@@ -734,6 +760,38 @@ TEST_P(QuantizedFullyConnectedOpTest, SimpleTestQuantizedInt4) {\n   EXPECT_THAT(m.GetOutput<int8_t>(), ElementsAre(103, 104, 105, 97, 98, 99));\n }\n \n+TEST_P(QuantizedFullyConnectedOpTest, SimpleTestQuantizedInt2) {\n+  QuantizedFullyConnectedOpModel m(\n+      GetRegistration(), /*units=*/3, /*batches*/ 2,\n+      /*input=*/{TensorType_INT8, {2, 10}, -63.5, 64},\n+      /*output=*/{TensorType_INT8, {}, -127, 128}, TensorType_INT32, false,\n+      false, ActivationFunctionType_RELU,\n+      FullyConnectedOptionsWeightsFormat_DEFAULT, -1, TensorType_INT2);\n+\n+  m.SetWeights2bit({\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 0\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 1\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 2\n+  });\n+  m.SetBias({1., 2., 3.});\n+\n+  m.SetInput<int8_t>({\n+      1, 2, 3, 4, 5, 6, 7, 8,  -9, -10,  // b = 0\n+      1, 2, 3, 4, 5, 6, 7, -8, 9,  -10,  // b = 1\n+  });\n+\n+  // The quantization parameters for the model.\n+  // input s, zp: 0.5, -1\n+  // filter s, zp: 0.5, 0\n+  // output s, zp: 1, -1\n+\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  EXPECT_THAT(m.GetDequantizedOutput<int8_t>(),\n+              testing::Pointwise(testing::FloatEq(),\n+                                 {26.0, 27.0, 28.0, 8.0, 9.0, 10.0}));\n+  EXPECT_THAT(m.GetOutput<int8_t>(), ElementsAre(25, 26, 27, 7, 8, 9));\n+}\n+\n TEST_P(QuantizedFullyConnectedOpTest, SimpleTestQuantizedInt8) {\n   QuantizedFullyConnectedOpModel m(\n       GetRegistration(), /*units=*/3, /*batches*/ 2,\n@@ -863,6 +921,34 @@ TEST_P(QuantizedFullyConnectedOpTest, SimpleTestPerChannelQuantizedInt4) {\n   EXPECT_THAT(m.GetOutput<int8_t>(), ElementsAre(103, 104, 105, 97, 98, 99));\n }\n \n+TEST_P(QuantizedFullyConnectedOpTest, SimpleTestPerChannelQuantizedInt2) {\n+  PerChannelQuantizedFullyConnectedOpModel m(\n+      GetRegistration(), /*units=*/3, /*batches*/ 2,\n+      /*input=*/{TensorType_INT8, {2, 10}, -63.5, 64},\n+      /*per_channel_quantization_scales=*/{1.0, 1.0, 1.0},\n+      /*output=*/{TensorType_INT8, {}, -127, 128},\n+      /*bias_type=*/TensorType_INT32, false, false, ActivationFunctionType_RELU,\n+      FullyConnectedOptionsWeightsFormat_DEFAULT, -1, TensorType_INT2);\n+\n+  m.SetWeights2bit({\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 0\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 1\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 2\n+  });\n+  m.SetBias({1, 2, 3});\n+\n+  m.SetInput<int8_t>({\n+      1, 2, 3, 4, 5, 6, 7, 8,  -9, -10,  // b = 0\n+      1, 2, 3, 4, 5, 6, 7, -8, 9,  -10,  // b = 1\n+  });\n+\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+\n+  EXPECT_THAT(m.GetDequantizedOutput<int8_t>(),\n+              ElementsAreArray(ArrayFloatNear({26, 27, 28, 8, 9, 10})));\n+  EXPECT_THAT(m.GetOutput<int8_t>(), ElementsAre(25, 26, 27, 7, 8, 9));\n+}\n+\n TEST_P(QuantizedFullyConnectedOpTest, SimpleTestQuantizedInt16NoBias) {\n   const float scale = 128.0 / 65536;\n   QuantizedFullyConnectedOpModel m(\n@@ -1018,6 +1104,37 @@ TEST_P(QuantizedFullyConnectedOpTest,\n               ElementsAre(1536, 2048, 2560, 11776, 12288, 12800));\n }\n \n+TEST_P(QuantizedFullyConnectedOpTest,\n+       SimpleTestPerChannelQuantizedInt16Bias32Weight2) {\n+  const float scale = 128.0 / 65536;\n+  PerChannelQuantizedFullyConnectedOpModel m(\n+      GetRegistration(), /*units=*/3, /*batches*/ 2,\n+      /*input=*/{TensorType_INT16, {2, 10}, 0, 0, scale, 0},\n+      /*per_channel_quantization_scales=*/{1.0, 1.0, 1.0},\n+      /*output=*/{TensorType_INT16, {}, 0, 0, scale, 0},\n+      /*bias_type=*/TensorType_INT32, false, false, ActivationFunctionType_RELU,\n+      FullyConnectedOptionsWeightsFormat_DEFAULT, -1, TensorType_INT2);\n+\n+  m.SetWeights2bit({\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 0\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 1\n+      1, 0, 1, 0, 1, 0, 1, 0, -1, 0,  // u = 2\n+  });\n+  m.SetBias({1, 2, 3});\n+\n+  m.SetInput<int16_t>({\n+      1, 2, 3, 4, 5, 6, 7, 8,  -9, -10,  // b = 0\n+      1, 2, 3, 4, 5, 6, 7, -8, 9,  -10,  // b = 1\n+  });\n+\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+\n+  EXPECT_THAT(m.GetDequantizedOutput<int16_t>(),\n+              ElementsAreArray(ArrayFloatNear({26, 27, 28, 8, 9, 10})));\n+  EXPECT_THAT(m.GetOutput<int16_t>(),\n+              ElementsAre(13312, 13824, 14336, 4096, 4608, 5120));\n+}\n+\n TEST_P(QuantizedFullyConnectedOpTest, SimpleTestQuantizedInt16Bias64) {\n   const float scale = 128.0 / 65536;\n   QuantizedFullyConnectedOpModel m("
        },
        {
            "sha": "f486e54da7b4c5d8083f43c242300475ae2a9e2c",
            "filename": "tensorflow/lite/kernels/register_ref.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -280,7 +280,7 @@ BuiltinRefOpResolver::BuiltinRefOpResolver() {\n              Register_EMBEDDING_LOOKUP_SPARSE());\n   AddBuiltin(BuiltinOperator_FULLY_CONNECTED, Register_FULLY_CONNECTED_REF(),\n              /* min_version */ 1,\n-             /* max_version */ 11);\n+             /* max_version */ 14);\n   AddBuiltin(BuiltinOperator_LSH_PROJECTION, Register_LSH_PROJECTION());\n   AddBuiltin(BuiltinOperator_HASHTABLE_LOOKUP, Register_HASHTABLE_LOOKUP());\n   AddBuiltin(BuiltinOperator_SOFTMAX, Register_SOFTMAX_REF(),"
        },
        {
            "sha": "cbdb74d29d04aa7b638d63c0824a44bc2f975bd5",
            "filename": "tensorflow/lite/kernels/test_util.h",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Ftest_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/64f382ac25a02088edebfad2c297f30e488f3490/tensorflow%2Flite%2Fkernels%2Ftest_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ftest_util.h?ref=64f382ac25a02088edebfad2c297f30e488f3490",
            "patch": "@@ -109,6 +109,9 @@ inline std::vector<T> Quantize(const std::vector<float>& data, float scale,\n   if (type == kTfLiteInt4) {\n     min = -7;\n     max = 7;\n+  } else if (type == kTfLiteInt2) {\n+    min = -2;\n+    max = 1;\n   }\n \n   q.reserve(data.size());\n@@ -570,6 +573,15 @@ class SingleOpModel {\n                        quantized_output.data() + quantized_output.size());\n   }\n \n+  void QuantizeAndPopulate2bit(int index, const std::vector<float>& data) {\n+    TfLiteTensor* t = interpreter_->tensor(index);\n+    t->type = kTfLiteInt2;\n+    std::vector<int8_t> quantized_output =\n+        Quantize<int8_t>(data, t->params.scale, t->params.zero_point, t->type);\n+    PopulateTensor2bit(index, /*offset=*/0, quantized_output.data(),\n+                       quantized_output.data() + quantized_output.size());\n+  }\n+\n   void SymmetricQuantizeAndPopulate(int index, const std::vector<float>& data) {\n     std::vector<int8_t> q = QuantizeTensor(index, data);\n     PopulateTensor(index, /*offset=*/0, reinterpret_cast<uint8_t*>(q.data()),\n@@ -583,6 +595,10 @@ class SingleOpModel {\n       std::vector<int8_t> q = Quantize<int8_t>(data, t->params.scale,\n                                                t->params.zero_point, t->type);\n       PopulateTensor4bit(index, /*offset=*/0, q.data(), q.data() + q.size());\n+    } else if (t->type == kTfLiteInt2) {\n+      std::vector<int8_t> q = Quantize<int8_t>(data, t->params.scale,\n+                                               t->params.zero_point, t->type);\n+      PopulateTensor2bit(index, /*offset=*/0, q.data(), q.data() + q.size());\n     } else {\n       std::vector<int8_t> q = QuantizeTensor(index, data);\n       PopulateTensor(index, /*offset=*/0, q.data(), q.data() + q.size());\n@@ -663,6 +679,9 @@ class SingleOpModel {\n       PopulateTensor4bit(index, /*offset=*/0, quantized_output.data(),\n                          quantized_output.data() + quantized_output.size());\n \n+    } else if (t->type == kTfLiteInt2) {\n+      PopulateTensor2bit(index, /*offset=*/0, quantized_output.data(),\n+                         quantized_output.data() + quantized_output.size());\n     } else {\n       PopulateTensor(index, /*offset=*/0, quantized_output.data(),\n                      quantized_output.data() + quantized_output.size());\n@@ -888,6 +907,9 @@ class SingleOpModel {\n         } else if (t.type == TensorType_INT4) {\n           std::tie(t.scale, t.zero_point) =\n               QuantizationParams<int8_t>(t.min, t.max, kTfLiteInt4);\n+        } else if (t.type == TensorType_INT2) {\n+          std::tie(t.scale, t.zero_point) =\n+              QuantizationParams<int8_t>(t.min, t.max, kTfLiteInt2);\n         } else {\n           ABSL_LOG(FATAL) << \"No support for the requested quantized type\";\n         }\n@@ -940,6 +962,9 @@ class SingleOpModel {\n     if (type == kTfLiteInt4) {\n       qmin = -7;\n       qmax = 7;\n+    } else if (type == kTfLiteInt2) {\n+      qmin = -2;\n+      qmax = 2;\n     } else {\n       qmin = std::numeric_limits<T>::min();\n       qmax = std::numeric_limits<T>::max();"
        }
    ],
    "stats": {
        "total": 221,
        "additions": 196,
        "deletions": 25
    }
}