{
    "author": "chsigg",
    "message": "Add more patterns to `triton_xla_fold_transpose_pass`.\n\nThis change adds patterns to push `tt.trans` through `tt.broadcast`, `tt.expand_dims` and into `scf.if`.\n\nThis way, more `tt.trans` ops get folded.\n\nPiperOrigin-RevId: 805792016",
    "sha": "70a78819b7565b6307f9ce5d6109cc3758ccfb9f",
    "files": [
        {
            "sha": "0cfee1a98fd5ce5b2d47114b88b04f156138c90c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_fold_transpose.mlir",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70a78819b7565b6307f9ce5d6109cc3758ccfb9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70a78819b7565b6307f9ce5d6109cc3758ccfb9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir?ref=70a78819b7565b6307f9ce5d6109cc3758ccfb9f",
            "patch": "@@ -13,6 +13,24 @@ func.func @fold_transpose_of_extract(%arg0: !tt.ptr<f32>, %arg1: i32) -> tensor<\n   return %1 : tensor<8x4xf32>\n }\n \n+// CHECK-LABEL: func @push_transpose_up_through_broadcast\n+func.func @push_transpose_up_through_broadcast(%arg0: tensor<4x1xi1>) -> tensor<8x4xi1> {\n+  // CHECK: %[[TRANS:.*]] = tt.trans %arg0 {order = array<i32: 1, 0>} : tensor<4x1xi1> -> tensor<1x4xi1>\n+  // CHECK: tt.broadcast %[[TRANS]] : tensor<1x4xi1> -> tensor<8x4xi1>\n+  %0 = tt.broadcast %arg0 : tensor<4x1xi1> -> tensor<4x8xi1>\n+  %1 = tt.trans %0 {order = array<i32: 1, 0>} : tensor<4x8xi1> -> tensor<8x4xi1>\n+  return %1 : tensor<8x4xi1>\n+}\n+\n+// CHECK-LABEL: func @push_transpose_up_through_expand_dims\n+func.func @push_transpose_up_through_expand_dims(%arg0: tensor<4x8xi1>) -> tensor<1x8x4xi1> {\n+  // CHECK: %[[TRANS:.*]] = tt.trans %arg0 {order = array<i32: 1, 0>} : tensor<4x8xi1> -> tensor<8x4xi1>\n+  // CHECK: tt.expand_dims %[[TRANS]] {axis = 0 : i32} : tensor<8x4xi1> -> tensor<1x8x4xi1>\n+  %0 = tt.expand_dims %arg0 {axis = 1 : i32} : tensor<4x8xi1> -> tensor<4x1x8xi1>\n+  %1 = tt.trans %0 {order = array<i32: 1, 2, 0>} : tensor<4x1x8xi1> -> tensor<1x8x4xi1>\n+  return %1 : tensor<1x8x4xi1>\n+}\n+\n // CHECK-LABEL: func @push_transpose_up_through_elementwise\n func.func @push_transpose_up_through_elementwise(%arg0: tensor<4x8xf32>) -> tensor<8x4xf32> {\n   // CHECK: arith.negf {{.*}} : tensor<8x4xf32>\n@@ -21,6 +39,20 @@ func.func @push_transpose_up_through_elementwise(%arg0: tensor<4x8xf32>) -> tens\n   return %1 : tensor<8x4xf32>\n }\n \n+// CHECK-LABEL: func @push_transpose_up_into_if\n+func.func @push_transpose_up_into_if(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>, %cond: i1) -> tensor<8x4xf32> {\n+  %0 = scf.if %cond -> tensor<4x8xf32> {\n+    // CHECK: tt.trans %arg0 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n+    scf.yield %arg0 : tensor<4x8xf32>\n+  } else {\n+    // CHECK: tt.trans %arg1 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n+    scf.yield %arg1 : tensor<4x8xf32>\n+  }\n+  // CHECK-NOT: tt.trans\n+  %1 = tt.trans %0 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n+  return %1 : tensor<8x4xf32>\n+}\n+\n // CHECK-LABEL: func @push_transpose_up_through_reshape\n func.func @push_transpose_up_through_reshape(%arg0: tensor<4x8x2xf32>) -> tensor<16x4xf32> {\n   // CHECK: tt.reshape {{.*}} : tensor<8x2x4xf32> -> tensor<16x4xf32>"
        },
        {
            "sha": "b9825ca54913f68b407ff625caf7ee20098885ac",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_fold_transpose_pass.cc",
            "status": "modified",
            "additions": 108,
            "deletions": 1,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70a78819b7565b6307f9ce5d6109cc3758ccfb9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70a78819b7565b6307f9ce5d6109cc3758ccfb9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc?ref=70a78819b7565b6307f9ce5d6109cc3758ccfb9f",
            "patch": "@@ -25,7 +25,9 @@ limitations under the License.\n #include \"llvm/ADT/DenseSet.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/Support/ErrorHandling.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/OperationSupport.h\"\n@@ -40,7 +42,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/util.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace mlir::triton::xla {\n \n@@ -49,6 +50,14 @@ namespace mlir::triton::xla {\n \n namespace {\n \n+// Sets the insertion point at the given op and returns the guard.\n+[[nodiscard]] OpBuilder::InsertionGuard SetInsertionPoint(OpBuilder& builder,\n+                                                          Operation* op) {\n+  OpBuilder::InsertionGuard guard(builder);\n+  builder.setInsertionPoint(op);\n+  return guard;\n+}\n+\n LogicalResult FoldTransposeOfExtract(TransOp op, PatternRewriter& rewriter) {\n   auto extract = op.getSrc().getDefiningOp<ExtractOp>();\n   if (!extract) {\n@@ -123,6 +132,49 @@ LogicalResult FoldTransposeOfExtract(TransOp op, PatternRewriter& rewriter) {\n   return success();\n }\n \n+LogicalResult PushTransposeUpThroughBroadcast(TransOp op,\n+                                              PatternRewriter& rewriter) {\n+  auto broadcast = op.getSrc().getDefiningOp<BroadcastOp>();\n+  if (!broadcast) {\n+    return rewriter.notifyMatchFailure(  //\n+        op, \"Transpose source is not a broadcast.\");\n+  }\n+  Value new_trans = rewriter.create<TransOp>(op.getLoc(), broadcast.getSrc(),\n+                                             op.getOrderAttr());\n+  rewriter.replaceOpWithNewOp<BroadcastOp>(op, op.getType(), new_trans);\n+  return success();\n+}\n+\n+LogicalResult PushTransposeUpThroughExpandDims(TransOp op,\n+                                               PatternRewriter& rewriter) {\n+  auto expand_dims = op.getSrc().getDefiningOp<ExpandDimsOp>();\n+  if (!expand_dims) {\n+    return rewriter.notifyMatchFailure(\n+        op, \"Transpose source is not an expand_dims.\");\n+  }\n+\n+  unsigned new_axis = [&] {\n+    for (auto [i, dim] : llvm::enumerate(op.getOrder())) {\n+      if (dim == expand_dims.getAxis()) {\n+        return i;\n+      }\n+    }\n+    llvm_unreachable(\"Transpose order does not contain expand_dims axis\");\n+  }();\n+\n+  auto new_order = llvm::to_vector(op.getOrder());\n+  new_order.erase(new_order.begin() + new_axis);\n+  for (auto& dim : new_order) {\n+    dim -= dim > expand_dims.getAxis();\n+  }\n+\n+  Value new_trans =\n+      rewriter.create<TransOp>(op.getLoc(), expand_dims.getSrc(), new_order);\n+  rewriter.replaceOpWithNewOp<ExpandDimsOp>(op, op.getType(), new_trans,\n+                                            new_axis);\n+  return success();\n+}\n+\n LogicalResult PushTransposeUpThroughElementwise(TransOp op,\n                                                 PatternRewriter& rewriter) {\n   Operation* elementwise = op.getSrc().getDefiningOp();\n@@ -149,6 +201,58 @@ LogicalResult PushTransposeUpThroughElementwise(TransOp op,\n   return success();\n }\n \n+// Pushes tt.trans up into scf.if.\n+//\n+// Example:\n+//   %0 = scf.if %cond -> type1 {\n+//     scf.yield %then : type1\n+//   } else {\n+//     scf.yield %else : type1\n+//   }\n+//   %1 = tt.trans %0 {order = [1, 0]}\n+// is rewritten to:\n+//   %0 = scf.if %cond -> type2 {\n+//     %1 = tt.trans %then {order = [1, 0]}\n+//     scf.yield %1 : type2\n+//   } else {\n+//     %2 = tt.trans %else {order = [1, 0]}\n+//     scf.yield %2 : type2\n+//   }\n+LogicalResult PushTransposeUpIntoIf(TransOp op, PatternRewriter& rewriter) {\n+  Value src = op.getSrc();\n+  auto if_op = src.getDefiningOp<scf::IfOp>();\n+  if (!if_op || !src.hasOneUse()) {\n+    return rewriter.notifyMatchFailure(op, \"Expected scf.if producer.\");\n+  }\n+\n+  // Compute the new types for the if op.\n+  unsigned result_number = cast<OpResult>(op.getSrc()).getResultNumber();\n+  auto new_types = llvm::to_vector(if_op.getResultTypes());\n+  new_types[result_number] = op.getType();\n+\n+  auto new_if_op = rewriter.create<scf::IfOp>(\n+      op.getLoc(), new_types, if_op.getCondition(), /*addThenBlock=*/false,\n+      /*addElseBlock=*/false);\n+\n+  // Update then and else regions.\n+  for (auto [old_region, new_region] :\n+       llvm::zip(if_op.getRegions(), new_if_op.getRegions())) {\n+    rewriter.inlineRegionBefore(*old_region, *new_region, new_region->end());\n+    if (new_region->empty()) {\n+      continue;\n+    }\n+    auto yield_op = new_region->front().getTerminator();\n+    OpBuilder::InsertionGuard guard = SetInsertionPoint(rewriter, yield_op);\n+    auto trans_op = rewriter.create<TransOp>(\n+        op.getLoc(), op.getType(), yield_op->getOperand(result_number),\n+        op.getOrderAttr());\n+    yield_op->setOperand(result_number, trans_op);\n+  }\n+  rewriter.replaceOp(op, new_if_op.getResult(result_number));\n+  rewriter.replaceOp(if_op, new_if_op);\n+  return success();\n+}\n+\n SmallVector<int32_t> GetInversePermutation(ArrayRef<int32_t> permutation) {\n   SmallVector<int32_t> result(permutation.size());\n   for (int32_t i = 0; i < permutation.size(); ++i) {\n@@ -216,7 +320,10 @@ class TritonXLAFoldTransposePass\n   void runOnOperation() override {\n     RewritePatternSet patterns(&getContext());\n     patterns.add(FoldTransposeOfExtract);\n+    patterns.add(PushTransposeUpIntoIf);\n+    patterns.add(PushTransposeUpThroughBroadcast);\n     patterns.add(PushTransposeUpThroughElementwise);\n+    patterns.add(PushTransposeUpThroughExpandDims);\n     patterns.add(PushTransposeUpThroughReshape);\n     if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       return signalPassFailure();"
        },
        {
            "sha": "72656015d45688c6383240e0eca6ee81502c696f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_squeeze_dims_pass.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70a78819b7565b6307f9ce5d6109cc3758ccfb9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70a78819b7565b6307f9ce5d6109cc3758ccfb9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc?ref=70a78819b7565b6307f9ce5d6109cc3758ccfb9f",
            "patch": "@@ -409,24 +409,25 @@ LogicalResult PushSqueezeDimsUpThroughExpandDims(SqueezeDimsOp op,\n   return success();\n }\n \n-// Pushes squeeze_dims up through tt.expand_dims, or folds them.\n+// Pushes squeeze_dims up into tt.expand_dims.\n+//\n // Example:\n-//   %1 = scf.if %cond -> type1 {\n+//   %0 = scf.if %cond -> type1 {\n //     scf.yield %then : type1\n //   } else {\n //     scf.yield %else : type1\n //   }\n-//   %2 = squeeze_dims %1, axis=0\n+//   %1 = squeeze_dims %0, axis=0\n // is rewritten to:\n-//   %1 = scf.if %cond -> type2 {\n-//     %then_ = squeeze_dims %then, axis=0\n-//     scf.yield %2 : type2\n+//   %0 = scf.if %cond -> type2 {\n+//     %1 = squeeze_dims %then, axis=0\n+//     scf.yield %1 : type2\n //   } else {\n-//     %else_ = squeeze_dims %else, axis=0\n-//     scf.yield %else_ : type2\n+//     %2 = squeeze_dims %else, axis=0\n+//     scf.yield %2 : type2\n //   }\n-LogicalResult PushSqueezeDimsUpThroughIf(SqueezeDimsOp op,\n-                                         PatternRewriter& rewriter) {\n+LogicalResult PushSqueezeDimsUpIntoIf(SqueezeDimsOp op,\n+                                      PatternRewriter& rewriter) {\n   Value src = op.getSrc();\n   auto if_op = src.getDefiningOp<scf::IfOp>();\n   if (!if_op || !src.hasOneUse()) {\n@@ -518,7 +519,7 @@ class TritonXLASqueezeDimsPass\n     patterns.add<PushSqueezeDimsUpThroughElementwise>(&getContext());\n     patterns.add(PushSqueezeDimsUpThroughBroadcast);\n     patterns.add(PushSqueezeDimsUpThroughExpandDims);\n-    patterns.add(PushSqueezeDimsUpThroughIf);\n+    patterns.add(PushSqueezeDimsUpIntoIf);\n     patterns.add(PushSqueezeDimsUpThroughJoin);\n     patterns.add(PushSqueezeDimsUpThroughReduce);\n     patterns.add(PushSqueezeDimsUpThroughTrans);"
        }
    ],
    "stats": {
        "total": 164,
        "additions": 152,
        "deletions": 12
    }
}