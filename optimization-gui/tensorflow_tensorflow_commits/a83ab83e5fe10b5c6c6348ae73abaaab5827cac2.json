{
    "author": "tensorflower-gardener",
    "message": "Squashing commits after sync tool breakage\n\nPiperOrigin-RevId: 843718817",
    "sha": "a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
    "files": [
        {
            "sha": "094070fa86a602daaac73eecf3c0fcc9c2541ecc",
            "filename": "tensorflow/compiler/mlir/tensorflow/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -905,7 +905,6 @@ cc_library(\n         \"//tensorflow/core/platform:status\",\n         \"//tensorflow/core/util:managed_stack_trace\",\n         \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Support\",\n         \"@local_xla//xla/mlir/utils:error_util\",\n@@ -949,10 +948,8 @@ tf_cc_test(\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core:test\",\n         \"//tensorflow/core:test_main\",\n-        \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n         \"@local_xla//xla/hlo/testlib:test\",\n-        \"@local_xla//xla/mlir/utils:error_util\",\n     ],\n )\n "
        },
        {
            "sha": "a7ea08924aea5e2041b83170fc1ac040ef9e617f",
            "filename": "tensorflow/compiler/mlir/tensorflow/utils/error_util_test.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 42,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Ferror_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Ferror_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Ferror_util_test.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -15,57 +15,71 @@ limitations under the License.\n \n #include \"tensorflow/compiler/mlir/tensorflow/utils/error_util.h\"\n \n-#include \"llvm/ADT/Twine.h\"\n #include \"mlir/IR/Builders.h\"  // from @llvm-project\n+#include \"mlir/IR/Diagnostics.h\"  // from @llvm-project\n+#include \"mlir/IR/Location.h\"  // from @llvm-project\n #include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n #include \"xla/hlo/testlib/test.h\"\n-#include \"tensorflow/core/lib/core/errors.h\"\n-#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n+#include \"tensorflow/core/platform/status.h\"\n \n namespace mlir {\n namespace {\n \n-using testing::HasSubstr;\n+using ::testing::HasSubstr;\n \n-TEST(ErrorUtilTest, StatusScopedDiagnosticHandler) {\n-  MLIRContext context;\n-  auto id = StringAttr::get(&context, \"//tensorflow/python/test.py\");\n-  auto loc = FileLineColLoc::get(&context, id, 0, 0);\n+class ErrorUtilTest : public ::testing::Test {\n+ protected:\n+  ErrorUtilTest()\n+      : id_(StringAttr::get(&context_, \"//tensorflow/python/test.py\")),\n+        loc_(FileLineColLoc::get(&context_, id_, 0, 0)) {}\n+\n+  MLIRContext context_;\n+  StringAttr id_;\n+  FileLineColLoc loc_;\n+};\n+\n+using StatusScopedDiagnosticHandlerTest = ErrorUtilTest;\n+\n+TEST_F(StatusScopedDiagnosticHandlerTest,\n+       OkWithoutDiagnosticGetsPassedThrough) {\n+  TF_ASSERT_OK(\n+      StatusScopedDiagnosticHandler(&context_).Combine(tensorflow::OkStatus()));\n+}\n+\n+TEST_F(StatusScopedDiagnosticHandlerTest,\n+       VerifyDiagnosticsAreCapturedAsUnknownStatus) {\n+  StatusScopedDiagnosticHandler handler(&context_);\n+  emitError(loc_) << \"Diagnostic message\";\n+  ASSERT_TRUE(tensorflow::errors::IsUnknown(handler.ConsumeStatus()));\n+}\n+\n+TEST_F(StatusScopedDiagnosticHandlerTest, VerifyPassedInErrorsArePropagated) {\n+  const Status err = tensorflow::errors::Internal(\"Passed in error\");\n+  ASSERT_TRUE(tensorflow::errors::IsInternal(\n+      StatusScopedDiagnosticHandler(&context_).Combine(err)));\n+}\n+\n+TEST_F(StatusScopedDiagnosticHandlerTest,\n+       VerifyThatReportedDiagnosticsAreAppendedToPassedInError) {\n+  StatusScopedDiagnosticHandler ssdh(&context_);\n+  emitError(loc_) << \"Diagnostic message reported\";\n+  emitError(loc_) << \"Second diagnostic message reported\";\n+  const Status s =\n+      ssdh.Combine(tensorflow::errors::Internal(\"Passed in error\"));\n+  ASSERT_TRUE(tensorflow::errors::IsInternal(s));\n+  EXPECT_THAT(s.message(), HasSubstr(\"Passed in error\"));\n+  EXPECT_THAT(s.message(), HasSubstr(\"Diagnostic message reported\"));\n+  EXPECT_THAT(s.message(), HasSubstr(\"Second diagnostic message reported\"));\n+}\n \n-  // Test OK without diagnostic gets passed through.\n-  {\n-    TF_ASSERT_OK(\n-        StatusScopedDiagnosticHandler(&context).Combine(absl::OkStatus()));\n-  }\n-\n-  // Verify diagnostics are captured as Unknown status.\n-  {\n-    StatusScopedDiagnosticHandler handler(&context);\n-    emitError(loc) << \"Diagnostic message\";\n-    ASSERT_TRUE(absl::IsUnknown(handler.ConsumeStatus()));\n-  }\n-\n-  // Verify passed in errors are propagated.\n-  {\n-    Status err = tensorflow::errors::Internal(\"Passed in error\");\n-    ASSERT_TRUE(\n-        absl::IsInternal(StatusScopedDiagnosticHandler(&context).Combine(err)));\n-  }\n-\n-  // Verify diagnostic reported are append to passed in error.\n-  {\n-    auto function = [&]() {\n-      emitError(loc) << \"Diagnostic message reported\";\n-      emitError(loc) << \"Second diagnostic message reported\";\n-      return tensorflow::errors::Internal(\"Passed in error\");\n-    };\n-    StatusScopedDiagnosticHandler ssdh(&context);\n-    Status s = ssdh.Combine(function());\n-    ASSERT_TRUE(absl::IsInternal(s));\n-    EXPECT_THAT(s.message(), HasSubstr(\"Passed in error\"));\n-    EXPECT_THAT(s.message(), HasSubstr(\"Diagnostic message reported\"));\n-    EXPECT_THAT(s.message(), HasSubstr(\"Second diagnostic message reported\"));\n-  }\n+TEST_F(StatusScopedDiagnosticHandlerTest, VerifyThatWarningsAreIgnored) {\n+  // Note: this logic is actually implemented in BaseScopedDiagnosticHandler's\n+  // handler() function, but only StatusScopedDiagnosticHandler uses it.\n+  StatusScopedDiagnosticHandler handler(&context_);\n+  emitWarning(loc_) << \"Warning message\";\n+  TF_EXPECT_OK(handler.ConsumeStatus());\n }\n \n TEST(ErrorUtilTest, StatusScopedDiagnosticHandlerWithFilter) {"
        },
        {
            "sha": "2c2e703735ef079542c65af5870091d127899224",
            "filename": "tensorflow/lite/core/c/common.h",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Flite%2Fcore%2Fc%2Fcommon.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Flite%2Fcore%2Fc%2Fcommon.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Fc%2Fcommon.h?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -1352,7 +1352,15 @@ typedef enum TfLiteDelegateFlags {\n   /// operator information using `Profiler::EventType::OPERATOR_INVOKE_EVENT`\n   /// and the results will appear in the operator-wise Profiling section and not\n   /// in the Delegate internal section.\n-  kTfLiteDelegateFlagsPerOperatorProfiling = 4\n+  kTfLiteDelegateFlagsPerOperatorProfiling = 4,\n+\n+  // This flag can be used by callers to hint that the delegate is likely to\n+  // delegate the entire graph to a single delegate so certain allocations can\n+  // be skipped.\n+  // This is an ADVANCED feature and should only be used if the caller has\n+  // prior knowledge that the delegate will fully delegate all subgraphs\n+  // to a single delegate.\n+  kTfLiteDelegateFlagsHintFullyDelegatedToSingleDelegate = 8,\n } TfLiteDelegateFlags;\n \n /// WARNING: This is an experimental interface that is subject to change."
        },
        {
            "sha": "5b0e15d4515ffad5808c1fc5619e5a3284cd454b",
            "filename": "tensorflow/lite/core/subgraph.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Flite%2Fcore%2Fsubgraph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/tensorflow%2Flite%2Fcore%2Fsubgraph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Fsubgraph.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -2489,9 +2489,11 @@ TfLiteStatus Subgraph::ModifyGraphWithDelegateImpl(TfLiteDelegate* delegate) {\n   // Restore delegation state if applicable.\n   TF_LITE_ENSURE_STATUS(RedoAllDelegates());\n \n+  int64_t delegate_flags = TfLiteDelegateGetFlagsInternal(delegate);\n   const bool delegate_supports_dynamic_shapes =\n-      TfLiteDelegateGetFlagsInternal(delegate) &\n-      kTfLiteDelegateFlagsAllowDynamicTensors;\n+      delegate_flags & kTfLiteDelegateFlagsAllowDynamicTensors;\n+  const bool hint_fully_delegated_to_single_delegate =\n+      delegate_flags & kTfLiteDelegateFlagsHintFullyDelegatedToSingleDelegate;\n   const auto pre_delegation_state = state_;\n \n   if (state_ == kStateInvokableAndImmutable) {\n@@ -2500,7 +2502,8 @@ TfLiteStatus Subgraph::ModifyGraphWithDelegateImpl(TfLiteDelegate* delegate) {\n     // tensors.\n     // Reset the state to force tensor/op reallocation.\n     state_ = kStateUninvokable;\n-  } else if (!delegate_supports_dynamic_shapes) {\n+  } else if (!delegate_supports_dynamic_shapes &&\n+             !hint_fully_delegated_to_single_delegate) {\n     // Check if graph has dynamic tensors by preparing ops.\n     int last_execution_plan_index_prepared;\n     TF_LITE_ENSURE_STATUS(PrepareOpsStartingAt(\n@@ -2539,9 +2542,12 @@ TfLiteStatus Subgraph::ModifyGraphWithDelegateImpl(TfLiteDelegate* delegate) {\n   if (!delegate_supports_dynamic_shapes) {\n     // CASE 1: Current delegate does not support dynamic shapes.\n     // Reset the state to force tensor/op reallocation.\n-    state_ = kStateUninvokable;\n-    TF_LITE_ENSURE_STATUS(\n-        reset_delegation_if_not_ok(EnsureMemoryAllocations()));\n+    if (!hint_fully_delegated_to_single_delegate) {\n+      state_ = kStateUninvokable;\n+      TF_LITE_ENSURE_STATUS(\n+          reset_delegation_if_not_ok(EnsureMemoryAllocations()));\n+    }\n+\n     // After using a delegate which doesn't support dynamic tensors, make the\n     // entire graph immutable.\n     state_ = kStateInvokableAndImmutable;"
        },
        {
            "sha": "82d755c32bbfbaad572daf52a0e6c18d6409f8a9",
            "filename": "third_party/py/python_init_toolchains.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fpy%2Fpython_init_toolchains.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fpy%2Fpython_init_toolchains.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fpy%2Fpython_init_toolchains.bzl?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -41,7 +41,6 @@ def python_init_toolchains(name = \"python\", python_version = None, **kwargs):\n         tool_version = MINOR_MAPPING.get(HERMETIC_PYTHON_VERSION)\n         if not tool_version:\n             tool_version = HERMETIC_PYTHON_VERSION + \".0\"\n-        url_components = HERMETIC_PYTHON_URL.split(\"://\", 1)\n \n         sha256s = {}\n         for platform in PLATFORMS.keys():\n@@ -51,12 +50,12 @@ def python_init_toolchains(name = \"python\", python_version = None, **kwargs):\n \n         python_register_toolchains(\n             name = get_toolchain_name_per_python_version(name),\n-            base_url = url_components[0] + \"://\",\n+            base_url = \"\",\n             ignore_root_user_error = True,\n             python_version = tool_version,\n             tool_versions = {\n                 tool_version: {\n-                    \"url\": url_components[1],\n+                    \"url\": HERMETIC_PYTHON_URL,\n                     \"sha256\": sha256s,\n                     \"strip_prefix\": HERMETIC_PYTHON_PREFIX,\n                 },"
        },
        {
            "sha": "82d755c32bbfbaad572daf52a0e6c18d6409f8a9",
            "filename": "third_party/xla/third_party/py/python_init_toolchains.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fthird_party%2Fpy%2Fpython_init_toolchains.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fthird_party%2Fpy%2Fpython_init_toolchains.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fpy%2Fpython_init_toolchains.bzl?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -41,7 +41,6 @@ def python_init_toolchains(name = \"python\", python_version = None, **kwargs):\n         tool_version = MINOR_MAPPING.get(HERMETIC_PYTHON_VERSION)\n         if not tool_version:\n             tool_version = HERMETIC_PYTHON_VERSION + \".0\"\n-        url_components = HERMETIC_PYTHON_URL.split(\"://\", 1)\n \n         sha256s = {}\n         for platform in PLATFORMS.keys():\n@@ -51,12 +50,12 @@ def python_init_toolchains(name = \"python\", python_version = None, **kwargs):\n \n         python_register_toolchains(\n             name = get_toolchain_name_per_python_version(name),\n-            base_url = url_components[0] + \"://\",\n+            base_url = \"\",\n             ignore_root_user_error = True,\n             python_version = tool_version,\n             tool_versions = {\n                 tool_version: {\n-                    \"url\": url_components[1],\n+                    \"url\": HERMETIC_PYTHON_URL,\n                     \"sha256\": sha256s,\n                     \"strip_prefix\": HERMETIC_PYTHON_PREFIX,\n                 },"
        },
        {
            "sha": "10c8aafaa98f992beb7fcf75e07293bba7125484",
            "filename": "third_party/xla/xla/backends/cpu/transforms/library_rewriter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -160,6 +160,10 @@ inline absl::Status InsertConvertIfNecessary(\n   return absl::OkStatus();\n }\n \n+inline bool IsElementwiseAndNotConstant(const HloInstruction* instr) {\n+  return instr->IsElementwise() && !instr->IsConstant();\n+}\n+\n }  // namespace\n \n absl::StatusOr<LibraryMatcher*> LibraryRewriter::ChooseLibrary(\n@@ -298,7 +302,7 @@ absl::StatusOr<bool> LibraryRewriter::ProcessComputation(\n       fusion_starters.push_back(*it);\n     } else if (fuse_reduce_ && (*it)->opcode() == HloOpcode::kReduce) {\n       fusion_starters.push_back(*it);\n-    } else if (fuse_eltwise_ && (*it)->IsElementwise()) {\n+    } else if (fuse_eltwise_ && IsElementwiseAndNotConstant(*it)) {\n       eltwise_ops.push_back(*it);\n     }\n   }"
        },
        {
            "sha": "dacd10d369da45187589db12656f1111519d1380",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -479,6 +479,7 @@ cc_library(\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor/cuda:nvshmem\",\n         \"//xla/stream_executor/gpu:gpu_stream\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n@@ -568,9 +569,11 @@ xla_test(\n     env = {\n         \"XLA_FLAGS\": \"--xla_gpu_experimental_enable_nvshmem=true\",\n     },\n+    tags = [\"cuda-only\"],\n     deps = [\n         \"//xla:debug_options_flags\",\n         \"//xla:status_macros\",\n+        \"//xla/core/collectives:communicator\",\n         \"//xla/pjrt/distributed\",\n         \"//xla/pjrt/distributed:client\",\n         \"//xla/pjrt/distributed:service\",\n@@ -585,7 +588,10 @@ xla_test(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_googletest//:gtest\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n-    ] + if_cuda_is_configured([\":nvshmem_collectives\"]),\n+    ] + if_cuda_is_configured([\n+        \":nvshmem_collectives\",\n+        \"//xla/stream_executor/cuda:nvshmem\",\n+    ]),\n )\n \n cc_library("
        },
        {
            "sha": "2398a7cc37bec59c3784d0ec752d9418ca37ac8a",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_collectives.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 87,
            "changes": 100,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -14,39 +14,38 @@ limitations under the License.\n ==============================================================================*/\n #include \"xla/backends/gpu/collectives/nvshmem_collectives.h\"\n \n-#include <cstddef>\n #include <cstdint>\n-#include <cstring>\n #include <memory>\n-#include <string>\n \n-#include \"absl/base/call_once.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/time/time.h\"\n #include \"third_party/nvshmem/nvshmem.h\"   // IWYU pragma: keep\n #include \"third_party/nvshmem/nvshmemx.h\"  // IWYU pragma: keep\n #include \"xla/backends/gpu/collectives/nvshmem_communicator.h\"\n #include \"xla/core/collectives/collectives.h\"\n #include \"xla/core/collectives/collectives_registry.h\"\n-#include \"xla/pjrt/distributed/key_value_store_interface.h\"\n+#include \"xla/core/collectives/communicator.h\"\n+#include \"xla/stream_executor/cuda/nvshmem.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/platform/casts.h\"\n #include \"tsl/platform/numbers.h\"\n \n namespace xla::gpu {\n \n NvshmemCollectives::~NvshmemCollectives() {\n-  if (initialized_) {\n-    Finalize();\n+  if (se::gpu::nvshmem::IsInitialized()) {\n+    se::gpu::nvshmem::Finalize();\n   }\n }\n \n+bool NvshmemCollectives::IsInitialized() const {\n+  return se::gpu::nvshmem::IsInitialized();\n+}\n+\n NvshmemCollectives* NvshmemCollectives::Default() {\n   absl::StatusOr<Collectives*> collectives =\n       CollectivesRegistry::Get(\"gpu\", \"nvshmem\");\n@@ -61,87 +60,14 @@ NvshmemCollectives* NvshmemCollectives::Default() {\n }\n \n absl::Status NvshmemCollectives::InitializeTopology(Topology topology) {\n-  SetEnvInfo(topology.node_id, topology.num_nodes,\n-             topology.device_count_per_process, topology.kv_store);\n+  se::gpu::nvshmem::SetEnvInfo(topology.node_id, topology.num_nodes,\n+                               topology.device_count_per_process,\n+                               topology.kv_store);\n   return absl::OkStatus();\n }\n \n-void NvshmemCollectives::SetEnvInfo(\n-    int process_id, size_t num_processes, size_t device_count_per_process,\n-    std::weak_ptr<KeyValueStoreInterface> kv_store) {\n-  process_id_ = process_id;\n-  num_processes_ = num_processes;\n-  device_count_per_process_ = device_count_per_process;\n-  kv_store_ = kv_store;\n-}\n-\n-absl::Status NvshmemCollectives::InitializeOnce() {\n-  auto init_fn = [this]() -> absl::Status {\n-    if (process_id_ == -1) {\n-      LOG(FATAL)\n-          << \"NvshmemCollectives::SetEnvInfo was not called before using \"\n-             \"NVSHMEM API\";\n-    }\n-    if (device_count_per_process_ != 1) {\n-      LOG(FATAL) << \"NVSHMEM API is only supported with one device per process\";\n-    }\n-    nvshmemx_init_attr_t nvshmem_init_attr = NVSHMEMX_INIT_ATTR_INITIALIZER;\n-    nvshmemx_uniqueid_t nvshmem_id = NVSHMEMX_UNIQUEID_INITIALIZER;\n-\n-    // Initialize NVSHMEM\n-    if (std::shared_ptr<KeyValueStoreInterface> kv_store = kv_store_.lock()) {\n-      if (process_id_ == 0) {\n-        if (nvshmemx_get_uniqueid(&nvshmem_id) != 0) {\n-          return absl::InternalError(\"nvshmemx_get_uniqueid failed.\");\n-        }\n-        char buf[sizeof(nvshmemx_uniqueid_t)];\n-        std::memcpy(buf, &nvshmem_id, sizeof(nvshmemx_uniqueid_t));\n-        absl::string_view nvshmem_id_str{buf, sizeof(buf)};\n-        TF_RETURN_IF_ERROR(kv_store->Set(kKvStoreKey, nvshmem_id_str));\n-      } else {\n-        TF_ASSIGN_OR_RETURN(std::string id_str,\n-                            kv_store->Get(kKvStoreKey, absl::Minutes(10)));\n-        CHECK(id_str.size() >= sizeof(nvshmemx_uniqueid_t));\n-        std::memcpy(&nvshmem_id, id_str.data(), sizeof(nvshmemx_uniqueid_t));\n-      }\n-    } else {\n-      return absl::InternalError(\n-          \"KV store is not available for nvshmem initialization.\");\n-    }\n-\n-    if (nvshmemx_set_attr_uniqueid_args(process_id_, num_processes_,\n-                                        &nvshmem_id, &nvshmem_init_attr) != 0) {\n-      return absl::InternalError(\"nvshmemx_set_attr_uniqueid_args failed.\");\n-    }\n-    if (nvshmemx_hostlib_init_attr(NVSHMEMX_INIT_WITH_UNIQUEID,\n-                                   &nvshmem_init_attr) != 0) {\n-      return absl::InternalError(\"nvshmemx_hostlib_init_attr failed.\");\n-    }\n-\n-    VLOG(3) << absl::StreamFormat(\n-        \"Initialized NVSHMEM on process %d; num_processes=%llu\", process_id_,\n-        num_processes_);\n-    return absl::OkStatus();\n-  };\n-\n-  static absl::once_flag once_flag;\n-  absl::Status status = absl::OkStatus();\n-  absl::call_once(once_flag, [&]() {\n-    status = init_fn();\n-    initialized_ = true;\n-  });\n-  return status;\n-}\n-\n-void NvshmemCollectives::Finalize() {\n-  VLOG(3) << absl::StreamFormat(\n-      \"Finilizing NVSHMEM on process %d; num_processes=%llu\", process_id_,\n-      num_processes_);\n-  nvshmemx_hostlib_finalize();\n-}\n-\n absl::StatusOr<void*> NvshmemCollectives::Allocate(uint64_t bytes) {\n-  TF_RETURN_IF_ERROR(InitializeOnce());\n+  TF_RETURN_IF_ERROR(se::gpu::nvshmem::InitializeOnce());\n   VLOG(3) << absl::StreamFormat(\n       \"Start allocation of %s (%llu bytes) for NVSHMEM\",\n       tsl::strings::HumanReadableNumBytes(bytes), bytes);\n@@ -155,7 +81,7 @@ absl::StatusOr<void*> NvshmemCollectives::Allocate(uint64_t bytes) {\n }\n \n absl::Status NvshmemCollectives::Deallocate(void* buffer) {\n-  TF_RETURN_IF_ERROR(InitializeOnce());\n+  TF_RETURN_IF_ERROR(se::gpu::nvshmem::InitializeOnce());\n   VLOG(3) << absl::StreamFormat(\"Start de-allocation for NVSHMEM buffer: %p\",\n                                 buffer);\n   nvshmem_free(buffer);"
        },
        {
            "sha": "82f717e8cb85d942b7962cab12d77a9240b7c9d2",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_collectives.h",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.h?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_COLLECTIVES_NVSHMEM_COLLECTIVES_H_\n #define XLA_BACKENDS_GPU_COLLECTIVES_NVSHMEM_COLLECTIVES_H_\n \n-#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <optional>\n@@ -31,7 +30,6 @@ limitations under the License.\n #include \"xla/core/collectives/collectives.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n-#include \"xla/pjrt/distributed/key_value_store_interface.h\"\n \n namespace xla::gpu {\n \n@@ -41,11 +39,8 @@ class NvshmemCollectives : public GpuCollectives {\n   ~NvshmemCollectives() override;\n \n   static NvshmemCollectives* Default();\n-  bool IsInitialized() { return initialized_; }\n \n-  void SetEnvInfo(int process_id, size_t num_processes,\n-                  size_t device_count_per_process,\n-                  std::weak_ptr<KeyValueStoreInterface> kv_store);\n+  bool IsInitialized() const;\n \n   absl::StatusOr<void*> Allocate(uint64_t bytes) final;\n \n@@ -82,19 +77,6 @@ class NvshmemCollectives : public GpuCollectives {\n   }\n \n   absl::Status InitializeTopology(Topology topology) final;\n-\n- private:\n-  absl::Status InitializeOnce();\n-\n-  void Finalize();\n-\n-  int process_id_ = -1;\n-  size_t num_processes_ = 0;\n-  size_t device_count_per_process_ = 0;\n-  std::weak_ptr<KeyValueStoreInterface> kv_store_;\n-  bool initialized_ = false;\n-\n-  static constexpr char kKvStoreKey[] = \"nvshmem_global_init\";\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "ef5ddcc9459ae77103fac97f79f7990b7e98bd48",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_collectives_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives_test.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -25,11 +25,13 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/time/time.h\"\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n+#include \"xla/core/collectives/communicator.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/pjrt/distributed/client.h\"\n #include \"xla/pjrt/distributed/distributed.h\"\n #include \"xla/pjrt/distributed/service.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/cuda/nvshmem.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -82,7 +84,7 @@ absl::Status InitializationTestBody(const int node_id, const int num_nodes) {\n   auto kv_store =\n       GetDistributedKeyValueStore(distributed_client, /*key_prefix=*/\"gpu:\");\n \n-  NvshmemCollectives::Default()->SetEnvInfo(node_id, num_nodes, 1, kv_store);\n+  se::gpu::nvshmem::SetEnvInfo(node_id, num_nodes, 1, kv_store);\n   cudaSetDevice(node_id);\n   TF_ASSIGN_OR_RETURN(void* ptr, NvshmemCollectives::Default()->Allocate(1024));\n   TF_RET_CHECK(ptr != nullptr);"
        },
        {
            "sha": "b686b7a78f6a478dc3129f097567cd48ba2e7f6b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -2030,23 +2030,18 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:device_id\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/service:rendezvous\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@com_google_protobuf//:protobuf_lite\",\n     ],\n )"
        },
        {
            "sha": "4a2f1f754c353620dbdd1860bd2bfd492c090744",
            "filename": "third_party/xla/xla/pjrt/distributed/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -144,7 +144,6 @@ xla_cc_test(\n         \":service\",\n         \":topology_util\",\n         \"//xla:status_macros\",\n-        \"//xla/pjrt/distributed/coordination:coordination_service_agent\",\n         \"//xla/runtime:device_id\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "3798e213c65051c02d822fa753716e5818368939",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -103,14 +103,11 @@ xla_cc_test(\n         \":coordination_service\",\n         \":coordination_service_error_util\",\n         \":test_device_proto_cc\",\n-        \"//xla/service:global_device_id\",\n         \"//xla/tsl/distributed_runtime:call_options\",\n-        \"//xla/tsl/distributed_runtime/coordination:coordination_service\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:test\",\n-        \"//xla/tsl/platform:types\",\n         \"//xla/tsl/protobuf:coordination_config_proto_cc\",\n         \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n         \"//xla/tsl/util/proto:proto_matchers\","
        },
        {
            "sha": "9c8ff6f6b206d23f204170d9072dbfb85414a0c9",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -615,6 +615,7 @@ xla_test(\n         \"//xla/tests:literal_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\","
        },
        {
            "sha": "173e10171627081a1ae014352abc4736402ffa70",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_compiler_aot_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler_aot_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler_aot_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler_aot_test.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include <gtest/gtest.h>\n #include \"absl/memory/memory.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -47,6 +48,9 @@ limitations under the License.\n namespace xla {\n namespace {\n \n+using ::absl_testing::IsOkAndHolds;\n+using ::testing::SizeIs;\n+\n constexpr absl::string_view kProgram = R\"(HloModule Computation\n \n ENTRY Computation() -> s32[] {\n@@ -100,6 +104,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessAotCompileMlirAndLoad) {\n   TF_ASSERT_OK_AND_ASSIGN(auto executable,\n                           compiler.Compile(opts, mlir_module.get(), *topology,\n                                            /*client=*/nullptr));\n+  EXPECT_THAT(executable->GetHloModules(), IsOkAndHolds(SizeIs(1)));\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto loaded_executable,\n       se_client->Load(std::move(executable), LoadOptions()));\n@@ -129,6 +134,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessAotCompileXlaAndLoad) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtExecutable> executable,\n       compiler.Compile(opts, computation, *topology, /*client=*/nullptr));\n+  EXPECT_THAT(executable->GetHloModules(), IsOkAndHolds(SizeIs(1)));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtLoadedExecutable> loaded_executable,\n       se_client->Load(std::move(executable), LoadOptions()));\n@@ -155,6 +161,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessLoadFromSerializedExecutable) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtExecutable> executable,\n       compiler.Compile(opts, computation, *topology, /*client=*/nullptr));\n+  EXPECT_THAT(executable->GetHloModules(), IsOkAndHolds(SizeIs(1)));\n \n   // Serialize the executable and load it.\n   TF_ASSERT_OK_AND_ASSIGN(std::string serialized_executable,\n@@ -192,6 +199,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessSerializeDeserialize) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtExecutable> executable,\n       compiler.Compile(opts, computation, *topology, /*client=*/nullptr));\n+  EXPECT_THAT(executable->GetHloModules(), IsOkAndHolds(SizeIs(1)));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtLoadedExecutable> loaded_executable,\n       se_client->Load(std::move(executable), LoadOptions()));\n@@ -242,6 +250,7 @@ TEST(StreamExecutorGpuCompilerTest, UnloadedExecutableMemoryStats) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtExecutable> executable,\n       compiler.Compile(options, computation, *topology, /*client=*/nullptr));\n+  EXPECT_THAT(executable->GetHloModules(), IsOkAndHolds(SizeIs(1)));\n \n   TF_ASSERT_OK_AND_ASSIGN(CompiledMemoryStats compiled_memory_stats,\n                           executable->GetCompiledMemoryStats());"
        },
        {
            "sha": "4245c4da9ba10063c4d092f09f83c8c5fe229119",
            "filename": "third_party/xla/xla/pjrt/stream_executor_executable.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/util.h\"\n \n namespace xla {\n+\n absl::StatusOr<std::string> StreamExecutorExecutable::SerializeExecutable()\n     const {\n   std::string serialized;\n@@ -81,6 +82,27 @@ absl::StatusOr<std::string> StreamExecutorExecutable::SerializeExecutable()\n   return proto.SerializeAsString();\n }\n \n+StreamExecutorExecutable::StreamExecutorExecutable(\n+    const CompileOptions& compile_options,\n+    std::vector<std::unique_ptr<xla::AotCompilationResult>> executables,\n+    int num_replicas, int num_partitions, absl::string_view name,\n+    absl::string_view fingerprint, absl::string_view default_memory_kind)\n+    : compile_options_(compile_options),\n+      executables_(std::move(executables)),\n+      num_replicas_(num_replicas),\n+      num_partitions_(num_partitions),\n+      name_(name),\n+      fingerprint_(fingerprint),\n+      default_memory_kind_(default_memory_kind) {\n+  std::vector<std::shared_ptr<HloModule>> hlo_modules;\n+  for (const auto& executable :\n+       std::get<std::vector<std::unique_ptr<xla::AotCompilationResult>>>(\n+           executables_)) {\n+    hlo_modules.push_back(executable->shared_optimized_module());\n+  }\n+  hlo_modules_ = std::move(hlo_modules);\n+}\n+\n StreamExecutorExecutable::StreamExecutorExecutable(\n     const CompileOptions& compile_options,\n     std::optional<HloModuleProto> unoptimized_hlo_module_proto,"
        },
        {
            "sha": "a0a97daa935646e2a693b2bc42332b1f1c741214",
            "filename": "third_party/xla/xla/pjrt/stream_executor_executable.h",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.h?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <utility>\n+#include <variant>\n #include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n@@ -37,20 +38,14 @@ limitations under the License.\n #include \"xla/service/hlo_proto_util.h\"\n \n namespace xla {\n+\n class StreamExecutorExecutable : public PjRtExecutable {\n  public:\n   StreamExecutorExecutable(\n       const CompileOptions& compile_options,\n       std::vector<std::unique_ptr<xla::AotCompilationResult>> executables,\n       int num_replicas, int num_partitions, absl::string_view name,\n-      absl::string_view fingerprint, absl::string_view default_memory_kind)\n-      : compile_options_(compile_options),\n-        executables_(std::move(executables)),\n-        num_replicas_(num_replicas),\n-        num_partitions_(num_partitions),\n-        name_(name),\n-        fingerprint_(fingerprint),\n-        default_memory_kind_(default_memory_kind) {}\n+      absl::string_view fingerprint, absl::string_view default_memory_kind);\n \n   StreamExecutorExecutable(\n       const CompileOptions& compile_options,"
        },
        {
            "sha": "0c80d94287c65cdbbbdae78a453bf485ab557212",
            "filename": "third_party/xla/xla/service/cpu/tests/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -432,12 +432,7 @@ xla_cc_test(\n     deps = [\n         \":cpu_codegen_test_main\",\n         \"//xla:literal\",\n-        \"//xla:literal_util\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/tests:literal_test_util\",\n-        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n     ],\n )"
        },
        {
            "sha": "698fc2f88afa2e6309b0a255834704677d0fad75",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -90,14 +90,12 @@ xla_cc_test(\n         \":sol_latency_estimator\",\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n-        \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:latency_hiding_scheduler\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\","
        },
        {
            "sha": "c0f7e612472574205a8e3ccdcc8a375eafe7d3e7",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/export_manual_reduction_collectives.cc",
            "status": "modified",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"llvm/ADT/DenseMap.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringMap.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n@@ -258,6 +259,95 @@ int64_t convertReduceScatter(sdy::ReduceScatterOp op, int64_t nextChannelId,\n   return nextChannelId;\n }\n \n+void convertShardedToUnreduced(sdy::ShardedToUnreducedOp op,\n+                               mlir::IRRewriter& rewriter) {\n+  TensorShardingAttr outSharding = op.getOutSharding();\n+  MeshAttr mesh = outSharding.getMesh(op);\n+  // If the mesh does not have iota device ids, we need an extra step to convert\n+  // partition id to logical device id. We do not support this case for now.\n+  CHECK(mesh.getDeviceIds().empty());\n+\n+  mlir::Location loc = op.getLoc();\n+  rewriter.setInsertionPoint(op);\n+\n+  ManualComputationOp manualComputation = createFullyManualComputation(\n+      loc, op.getTensor(), outSharding, mesh, rewriter,\n+      [&](mlir::BlockArgument arg, OpBuilder& blockBuilder) {\n+        RankedTensorType fullType =\n+            mlir::cast<RankedTensorType>(op.getResult().getType());\n+        RankedTensorType inputType =\n+            sdy::getSharding(op.getTensor())\n+                .getLocalTensorType(fullType, mesh,\n+                                    /*allowNonDivisible=*/false);\n+        CHECK(inputType) << kNonDivisibleShardingError;\n+        RankedTensorType outputType =\n+            outSharding.getLocalTensorType(fullType, mesh);\n+\n+        Value zero = stablehlo::ConstantOp::create(\n+            blockBuilder, loc,\n+            blockBuilder.getZeroAttr(outputType.getElementType()));\n+        Value broadcast = stablehlo::BroadcastOp::create(\n+            blockBuilder, loc, outputType, zero, outputType.getShape());\n+\n+        // Decompose partitionId into axis coordinates.\n+        Value partitionId = stablehlo::PartitionIdOp::create(blockBuilder, loc);\n+        Value currentRem = stablehlo::ConvertOp::create(\n+            blockBuilder, loc,\n+            RankedTensorType::get({}, blockBuilder.getIntegerType(32)),\n+            partitionId);\n+        llvm::StringMap<Value> axisSizes, axisCoordinates;\n+        for (sdy::MeshAxisAttr axis : llvm::reverse(mesh.getAxes())) {\n+          Value axisSize = stablehlo::ConstantOp::create(\n+              blockBuilder, loc,\n+              blockBuilder.getI32IntegerAttr(axis.getSize()));\n+          axisSizes[axis.getName()] = axisSize;\n+          axisCoordinates[axis.getName()] =\n+              stablehlo::RemOp::create(blockBuilder, loc, currentRem, axisSize);\n+          currentRem =\n+              stablehlo::DivOp::create(blockBuilder, loc, currentRem, axisSize);\n+        }\n+\n+        SmallVector<Value> offsets;\n+        offsets.reserve(outputType.getRank());\n+        Value zeroOffset = stablehlo::ConstantOp::create(\n+            blockBuilder, loc, blockBuilder.getI32IntegerAttr(0));\n+        for (int64_t dim = 0; dim < outputType.getRank(); ++dim) {\n+          if (op.getAxes()[dim].empty()) {\n+            offsets.push_back(zeroOffset);\n+            continue;\n+          }\n+\n+          Value offset, prevAxisSize;\n+          for (AxisRefAttr axis : op.getAxes()[dim].getValue()) {\n+            CHECK(!axis.getSubAxisInfo()) << \"Sub-axes not supported in \"\n+                                             \"ShardedToUnreducedOp.\";\n+            StringRef axisName = axis.getName();\n+            if (prevAxisSize == nullptr) {\n+              offset = axisCoordinates[axisName];\n+            } else {\n+              offset = stablehlo::MulOp::create(blockBuilder, loc, offset,\n+                                                prevAxisSize);\n+              offset = stablehlo::AddOp::create(blockBuilder, loc, offset,\n+                                                axisCoordinates[axisName]);\n+            }\n+\n+            prevAxisSize = axisSizes[axisName];\n+          }\n+\n+          Value localDimSize = stablehlo::ConstantOp::create(\n+              blockBuilder, loc,\n+              blockBuilder.getI32IntegerAttr(inputType.getDimSize(dim)));\n+          offset =\n+              stablehlo::MulOp::create(blockBuilder, loc, offset, localDimSize);\n+          offsets.push_back(offset);\n+        }\n+\n+        return stablehlo::DynamicUpdateSliceOp::create(\n+            blockBuilder, loc, outputType, broadcast, arg, offsets);\n+      });\n+  rewriter.replaceOp(op, manualComputation);\n+}\n+\n void syncInOutUnreducedAxes(mlir::Operation* op) {\n   Value input = op->getOperand(0);\n   TensorShardingAttr outSharding = sdy::getSharding(op->getResult(0));\n@@ -322,6 +412,9 @@ class StablehloExportManualReductionCollectivesPass\n           nextChannelId =\n               convertReduceScatter(reduceScatter, nextChannelId, rewriter);\n         }\n+      } else if (auto shardedToUnreduced =\n+                     mlir::dyn_cast<sdy::ShardedToUnreducedOp>(op)) {\n+        convertShardedToUnreduced(shardedToUnreduced, rewriter);\n       }\n     });\n   }"
        },
        {
            "sha": "288dff8d476c67bb41de48427eb54dcbaac4bba7",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/stablehlo_export_manual_reduction_collectives.mlir",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -84,7 +84,6 @@ func.func @all_reduce_single_axis(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sh\n   return %0 : tensor<8x8xf32>\n }\n \n-\n // CHECK-LABEL: func @all_reduce_single_axis_2\n func.func @all_reduce_single_axis_2(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_x_2_y_2, [{\"x\"}, {}], unreduced={\"y\"}>}) -> tensor<8x8xf32> {\n   // CHECK{LITERAL}: replica_groups = dense<[[0, 1], [2, 3]]>\n@@ -345,3 +344,30 @@ func.func @unreduced_sine_of_replicated_dot(%arg0: tensor<8x4xf32>, %arg1: tenso\n   %1 = stablehlo.sine %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}], unreduced={\"x\"}>]>} : tensor<8x2xf32>\n   return %1 : tensor<8x2xf32>\n }\n+\n+// CHECK-LABEL: func @sharded_to_unreduced\n+func.func @sharded_to_unreduced(%arg0: tensor<16x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<16x16xf32> {\n+  // CHECK-NEXT: %[[MANUAL_COMP:.*]] = sdy.manual_computation(%arg0)\n+  // CHECK-SAME:     in_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>]\n+  // CHECK-SAME:     out_shardings=[<@mesh, [{}, {\"y\"}], unreduced={\"x\"}>]\n+  // CHECK-SAME:     manual_axes={\"x\", \"y\"} (%arg1: tensor<4x8xf32>) {\n+  // CHECK-NEXT:   %[[CST:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n+  // CHECK-NEXT:   %[[BROADCAST:.*]] = stablehlo.broadcast %[[CST]], sizes = [16, 8] : (tensor<f32>) -> tensor<16x8xf32>\n+  // CHECK-NEXT:   %[[PID:.*]] = stablehlo.partition_id : tensor<ui32>\n+  // CHECK-NEXT:   %[[PID_I32:.*]] = stablehlo.convert %[[PID]] : (tensor<ui32>) -> tensor<i32>\n+  // CHECK-NEXT:   %[[C2:.*]] = stablehlo.constant dense<2> : tensor<i32>\n+  // CHECK-NEXT:   %[[REM2:.*]] = stablehlo.remainder %[[PID_I32]], %[[C2]] : tensor<i32>\n+  // CHECK-NEXT:   %[[DIV2:.*]] = stablehlo.divide %[[PID_I32]], %[[C2]] : tensor<i32>\n+  // CHECK-NEXT:   %[[C4:.*]] = stablehlo.constant dense<4> : tensor<i32>\n+  // CHECK-NEXT:   %[[REM4:.*]] = stablehlo.remainder %[[DIV2]], %[[C4]] : tensor<i32>\n+  // CHECK-NEXT:   %[[DIV4:.*]] = stablehlo.divide %[[DIV2]], %[[C4]] : tensor<i32>\n+  // CHECK-NEXT:   %[[C0:.*]] = stablehlo.constant dense<0> : tensor<i32>\n+  // CHECK-NEXT:   %[[C4_2:.*]] = stablehlo.constant dense<4> : tensor<i32>\n+  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %[[REM4]], %[[C4_2]] : tensor<i32>\n+  // CHECK-NEXT:   %[[DUS:.*]] = stablehlo.dynamic_update_slice %[[BROADCAST]], %arg1, %[[MULT]], %[[C0]] : (tensor<16x8xf32>, tensor<4x8xf32>, tensor<i32>, tensor<i32>) -> tensor<16x8xf32>\n+  // CHECK-NEXT:   sdy.return %[[DUS]] : tensor<16x8xf32>\n+  // CHECK-NEXT: } : (tensor<16x16xf32>) -> tensor<16x16xf32>\n+  // CHECK-NEXT: return %[[MANUAL_COMP]] : tensor<16x16xf32>\n+  %0 = sdy.sharded_to_unreduced [{\"x\"}, {}] %arg0 out_sharding=<@mesh, [{}, {\"y\"}], unreduced={\"x\"}> : tensor<16x16xf32>\n+  return %0 : tensor<16x16xf32>\n+}"
        },
        {
            "sha": "03eb4da25e21f6c467c894bcbc5a695370501793",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -954,6 +954,31 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"nvshmem\",\n+    srcs = [\"nvshmem.cc\"],\n+    hdrs = [\"nvshmem.h\"],\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \"//xla/pjrt/distributed:key_value_store_interface\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/time\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@nvshmem//:nvshmem_lib\",\n+    ],\n+)\n+\n cc_library(\n     name = \"nvjitlink_support\",\n     srcs = [\"nvjitlink_support.cc\"],"
        },
        {
            "sha": "a88b84332c6c282a063c823d96f5cb27c2aea7bf",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvshmem.cc",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem.cc?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -0,0 +1,134 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/stream_executor/cuda/nvshmem.h\"\n+\n+#include <cstring>\n+#include <memory>\n+#include <string>\n+\n+#include \"absl/base/call_once.h\"\n+#include \"absl/base/no_destructor.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/time/time.h\"\n+#include \"third_party/nvshmem/nvshmem.h\"   // IWYU pragma: keep\n+#include \"third_party/nvshmem/nvshmemx.h\"  // IWYU pragma: keep\n+#include \"xla/pjrt/distributed/key_value_store_interface.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace stream_executor::gpu::nvshmem {\n+\n+// NVSHMEM environment information is stored per process in a static variable.\n+namespace {\n+struct EnvInfo {\n+  int process_id = -1;\n+  size_t num_processes = 0;\n+  size_t device_count_per_process = 0;\n+  std::weak_ptr<xla::KeyValueStoreInterface> kv_store;\n+  bool initialized = false;\n+};\n+\n+static absl::NoDestructor<EnvInfo> env;\n+}  // namespace\n+\n+void SetEnvInfo(int process_id, size_t num_processes,\n+                size_t device_count_per_process,\n+                std::weak_ptr<xla::KeyValueStoreInterface> kv_store) {\n+  env->process_id = process_id;\n+  env->num_processes = num_processes;\n+  env->device_count_per_process = device_count_per_process;\n+  env->kv_store = kv_store;\n+}\n+\n+bool IsInitialized() { return env->initialized; }\n+\n+absl::Status InitializeOnce() {\n+  static constexpr absl::string_view kKvStoreKey = \"nvshmem_global_init\";\n+\n+  auto init_fn = []() -> absl::Status {\n+    VLOG(2) << \"Initializing NVSHMEM: process_id=\" << env->process_id\n+            << \", num_processes=\" << env->num_processes\n+            << \", device_count_per_process=\" << env->device_count_per_process;\n+\n+    if (env->process_id == -1) {\n+      LOG(FATAL)\n+          << \"NvshmemCollectives::SetEnvInfo was not called before using \"\n+             \"NVSHMEM API\";\n+    }\n+    if (env->device_count_per_process != 1) {\n+      LOG(FATAL) << \"NVSHMEM API is only supported with one device per process\";\n+    }\n+    nvshmemx_init_attr_t nvshmem_init_attr = NVSHMEMX_INIT_ATTR_INITIALIZER;\n+    nvshmemx_uniqueid_t nvshmem_id = NVSHMEMX_UNIQUEID_INITIALIZER;\n+\n+    // Initialize NVSHMEM\n+    if (std::shared_ptr<xla::KeyValueStoreInterface> kv_store =\n+            env->kv_store.lock()) {\n+      if (env->process_id == 0) {\n+        if (nvshmemx_get_uniqueid(&nvshmem_id) != 0) {\n+          return absl::InternalError(\"nvshmemx_get_uniqueid failed.\");\n+        }\n+        char buf[sizeof(nvshmemx_uniqueid_t)];\n+        std::memcpy(buf, &nvshmem_id, sizeof(nvshmemx_uniqueid_t));\n+        absl::string_view nvshmem_id_str{buf, sizeof(buf)};\n+        TF_RETURN_IF_ERROR(kv_store->Set(kKvStoreKey, nvshmem_id_str));\n+      } else {\n+        TF_ASSIGN_OR_RETURN(std::string id_str,\n+                            kv_store->Get(kKvStoreKey, absl::Minutes(10)));\n+        CHECK(id_str.size() >= sizeof(nvshmemx_uniqueid_t));\n+        std::memcpy(&nvshmem_id, id_str.data(), sizeof(nvshmemx_uniqueid_t));\n+      }\n+    } else {\n+      return absl::InternalError(\n+          \"KV store is not available for nvshmem initialization.\");\n+    }\n+\n+    if (nvshmemx_set_attr_uniqueid_args(env->process_id, env->num_processes,\n+                                        &nvshmem_id, &nvshmem_init_attr) != 0) {\n+      return absl::InternalError(\"nvshmemx_set_attr_uniqueid_args failed.\");\n+    }\n+    if (nvshmemx_hostlib_init_attr(NVSHMEMX_INIT_WITH_UNIQUEID,\n+                                   &nvshmem_init_attr) != 0) {\n+      return absl::InternalError(\"nvshmemx_hostlib_init_attr failed.\");\n+    }\n+\n+    VLOG(3) << absl::StreamFormat(\n+        \"Initialized NVSHMEM on process %d; num_processes=%llu\",\n+        env->process_id, env->num_processes);\n+    return absl::OkStatus();\n+  };\n+\n+  static absl::once_flag once_flag;\n+  absl::Status status = absl::OkStatus();\n+  absl::call_once(once_flag, [&]() {\n+    status = init_fn();\n+    env->initialized = true;\n+  });\n+  return status;\n+}\n+\n+void Finalize() {\n+  VLOG(3) << absl::StreamFormat(\n+      \"Finilizing NVSHMEM on process %d; num_processes=%llu\", env->process_id,\n+      env->num_processes);\n+  nvshmemx_hostlib_finalize();\n+}\n+\n+}  // namespace stream_executor::gpu::nvshmem"
        },
        {
            "sha": "4fa7313a9e57d2368fbe3caa7c6311ddee23342f",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvshmem.h",
            "status": "added",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a83ab83e5fe10b5c6c6348ae73abaaab5827cac2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem.h?ref=a83ab83e5fe10b5c6c6348ae73abaaab5827cac2",
            "patch": "@@ -0,0 +1,43 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_CUDA_NVSHMEM_H_\n+#define XLA_STREAM_EXECUTOR_CUDA_NVSHMEM_H_\n+\n+#include <cstddef>\n+#include <memory>\n+\n+#include \"absl/status/status.h\"\n+#include \"xla/pjrt/distributed/key_value_store_interface.h\"\n+\n+namespace stream_executor::gpu::nvshmem {\n+\n+// Set environment information for NVSHMEM library.\n+void SetEnvInfo(int process_id, size_t num_processes,\n+                size_t device_count_per_process,\n+                std::weak_ptr<xla::KeyValueStoreInterface> kv_store);\n+\n+// Returns true if NVSHMEM library is initialized.\n+bool IsInitialized();\n+\n+// Initializes NVSHMEM library once per process.\n+absl::Status InitializeOnce();\n+\n+// Finalizes NVSHMEM library\n+void Finalize();\n+\n+}  // namespace stream_executor::gpu::nvshmem\n+\n+#endif  // XLA_STREAM_EXECUTOR_CUDA_NVSHMEM_H_"
        }
    ],
    "stats": {
        "total": 659,
        "additions": 467,
        "deletions": 192
    }
}