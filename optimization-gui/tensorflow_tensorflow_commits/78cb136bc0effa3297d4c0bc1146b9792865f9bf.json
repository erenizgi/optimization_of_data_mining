{
    "author": "bchetioui",
    "message": "[XLA:GPU] Only mask operand `dot` tiles when necessary in the generic Triton emitter.\n\nPreviously, the masking logic was executed unconditionally, including for full\ntiles. This caused us to execute a lot of useless arithmetic operations at\nevery step.\n\nNow, masking should only occur whenever we encounter a padded tile---i.e. once\na tile has elements that exceed the boundary of the contracting dimension.\n\nPiperOrigin-RevId: 799707235",
    "sha": "78cb136bc0effa3297d4c0bc1146b9792865f9bf",
    "files": [
        {
            "sha": "8babe2f4506f779afc7eaa34b6e904fcba5bea47",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 30,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78cb136bc0effa3297d4c0bc1146b9792865f9bf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78cb136bc0effa3297d4c0bc1146b9792865f9bf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=78cb136bc0effa3297d4c0bc1146b9792865f9bf",
            "patch": "@@ -774,40 +774,63 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder& b,\n   int64_t tile_size = tile_shape[contraction_dimension_index];\n \n   if (contracting_dimension_size % tile_size != 0) {\n-    // When the contracting dimension is not divisible by the tile size, we\n-    // need to mask out the last tile. We do this with the following logic:\n-    //\n-    // indices =\n-    //   contracting_dimension_tile_index * tile_size + range(0, tile_size)\n-    // mask = indices < contracting_dimension_size\n-    // operand = select(broadcast(mask, operand.shape), operand, 0)\n-    Value range = Range(b, tile_size).UnwrapTensor();\n+    // Only mask out tiles that we know to go beyond boundaries of the\n+    // contracting dimension---i.e. tiles whose index exceeds the number of\n+    // full tiles (tiles without padding).\n+    Type result_type = dot_operand_value.getType();\n     Value tile_size_value =\n         CreateConst(b, b.getI32Type(), tile_size, {}).UnwrapScalar();\n-    Value tile_offset = b.create<arith::MulIOp>(\n-        contracting_dimension_tile_index, tile_size_value);\n-    Value broadcasted_tile_offset =\n-        Splat(b, ScalarOrTensor(tile_offset), {tile_size}).UnwrapTensor();\n-    Value indices = b.create<arith::AddIOp>(range, broadcasted_tile_offset);\n-\n-    Value boundary =\n-        CreateConst(b, b.getI32Type(), contracting_dimension_size, {tile_size})\n-            .UnwrapTensor();\n-\n-    Value mask =\n-        b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, indices, boundary);\n-\n-    mask = BroadcastInDims(b, ScalarOrTensor(mask), tile_shape,\n-                           {contraction_dimension_index})\n-               .UnwrapTensor();\n-    TF_ASSIGN_OR_RETURN(\n-        auto element_type,\n-        TritonType(b, dot_operand.hlo()->shape().element_type()));\n+    Value num_full_tiles = b.create<arith::DivSIOp>(\n+        CreateConst(b, b.getI32Type(), contracting_dimension_size, {})\n+            .UnwrapScalar(),\n+        tile_size_value);\n+    // if tile_index >= num_full_tiles...\n+    auto cond = b.create<arith::CmpIOp>(arith::CmpIPredicate::sge,\n+                                        contracting_dimension_tile_index,\n+                                        num_full_tiles);\n+    auto if_op = b.create<mlir::scf::IfOp>(mlir::TypeRange(result_type), cond,\n+                                           /*withElseRegion=*/true);\n+    // then ...\n+    {\n+      b.setInsertionPointToStart(if_op.thenBlock());\n+      // indices =\n+      //   contracting_dimension_tile_index * tile_size + range(0, tile_size)\n+      // mask = indices < contracting_dimension_size\n+      // operand = select(broadcast(mask, operand.shape), operand, 0)\n+      Value tile_offset = b.create<arith::MulIOp>(\n+          contracting_dimension_tile_index, tile_size_value);\n+      Value range = Range(b, tile_size).UnwrapTensor();\n+      Value broadcasted_tile_offset =\n+          Splat(b, ScalarOrTensor(tile_offset), {tile_size}).UnwrapTensor();\n+      Value indices = b.create<arith::AddIOp>(range, broadcasted_tile_offset);\n+\n+      Value boundary = CreateConst(b, b.getI32Type(),\n+                                   contracting_dimension_size, {tile_size})\n+                           .UnwrapTensor();\n+\n+      Value mask =\n+          b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, indices, boundary);\n+\n+      mask = BroadcastInDims(b, ScalarOrTensor(mask), tile_shape,\n+                             {contraction_dimension_index})\n+                 .UnwrapTensor();\n+      TF_ASSIGN_OR_RETURN(\n+          auto element_type,\n+          TritonType(b, dot_operand.hlo()->shape().element_type()));\n \n-    ScalarOrTensor zero = CreateConst(b, element_type, 0.0f, tile_shape);\n+      ScalarOrTensor zero = CreateConst(b, element_type, 0.0f, tile_shape);\n \n-    return b.create<arith::SelectOp>(mask, dot_operand_value,\n-                                     zero.UnwrapTensor());\n+      Value masked_dot_operand = b.create<arith::SelectOp>(\n+          mask, dot_operand_value, zero.UnwrapTensor());\n+      b.create<mlir::scf::YieldOp>(masked_dot_operand);\n+    }\n+    // else ...\n+    {\n+      b.setInsertionPointToStart(if_op.elseBlock());\n+      b.create<mlir::scf::YieldOp>(dot_operand_value);\n+    }\n+    b.setInsertionPointAfter(if_op);\n+    return if_op.getResult(0);\n   }\n \n   return dot_operand_value;"
        },
        {
            "sha": "c4dc3ba1215b6ec4d3d14bf740ae3a23e7fc0f59",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78cb136bc0effa3297d4c0bc1146b9792865f9bf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78cb136bc0effa3297d4c0bc1146b9792865f9bf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=78cb136bc0effa3297d4c0bc1146b9792865f9bf",
            "patch": "@@ -59,7 +59,6 @@ limitations under the License.\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/types.h\"\n@@ -2981,6 +2980,18 @@ ENTRY entry {\n           \"num_ctas\":\"1\",\n           \"num_stages\":\"1\"}}}\n })\";\n+\n+  TF_EXPECT_OK(CreateTritonIrAndFileCheck(this, kHloText, \"fdot\", R\"(\n+  // Ensure that masking is applied only conditionally to both operands.\n+  CHECK:      %[[MASKED_OPERAND0:.*]] = scf.if\n+  CHECK:        %[[SELECT0:.*]] = arith.select\n+  CHECK-NEXT:   scf.yield %[[SELECT0]]\n+  CHECK:      %[[MASKED_OPERAND1:.*]] = scf.if\n+  CHECK:        %[[SELECT1:.*]] = arith.select\n+  CHECK-NEXT:   scf.yield %[[SELECT1]]\n+  CHECK:      tt.dot %[[MASKED_OPERAND0]], %[[MASKED_OPERAND1]]\n+)\"));\n+\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       kHloText, ErrorSpec{/*aabs=*/1e-4, /*arel=*/1e-6}));\n }"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 65,
        "deletions": 31
    }
}