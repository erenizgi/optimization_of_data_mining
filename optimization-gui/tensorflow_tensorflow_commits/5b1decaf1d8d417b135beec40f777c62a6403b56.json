{
    "author": "pifon2a",
    "message": "[XLA:GPU] Collect kernel modules and link them outside of ThunkEmitter.\n\nPiperOrigin-RevId: 838474707",
    "sha": "5b1decaf1d8d417b135beec40f777c62a6403b56",
    "files": [
        {
            "sha": "91b52d4d011ee6862f157ba9c237a2cf27e62f93",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -284,16 +284,14 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n               TF_ASSIGN_OR_RETURN(\n                   module,\n                   CreateLLVMModule(\n-                      mlir_context,\n-                      ir_emitter_context.llvm_module()->getContext(),\n+                      mlir_context, *ir_emitter_context.llvm_context(),\n                       ir_emitter_context.gpu_device_info(), fusion, kernel_name,\n                       &ir_emitter_context.buffer_assignment()));\n               auto* kernel_func = module->getFunction(kernel_name);\n               AddRanges(kernel_func, launch_dims, module.get());\n \n-              auto* target = ir_emitter_context.llvm_module();\n-              module->setDataLayout(target->getDataLayout());\n-              module->setTargetTriple(target->getTargetTriple());\n+              module->setDataLayout(ir_emitter_context.data_layout());\n+              module->setTargetTriple(ir_emitter_context.target_triple());\n \n               llvm::IRBuilder<> builder(module->getContext());\n               AnnotateFunctionAsGpuKernel(module.get(), kernel_func, &builder);"
        },
        {
            "sha": "8ecab390d4d120836f793406a0ccc175e2aa2c3f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/llvm_emitter.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 52,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -124,9 +124,10 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n   // Constructs an IrEmitter with the given IrEmitter context.\n   // ir_emitter_context is owned by the caller and should outlive the IrEmitter\n   // object.\n-  explicit IrEmitter(IrEmitterContext* ir_emitter_context, bool is_nested)\n+  explicit IrEmitter(IrEmitterContext* ir_emitter_context,\n+                     llvm::Module* llvm_module, bool is_nested)\n       : ir_emitter_context_(ir_emitter_context),\n-        module_(ir_emitter_context->llvm_module()),\n+        module_(llvm_module),\n         b_(module_->getContext()),\n         bindings_(&b_, module_, is_nested) {}\n \n@@ -359,9 +360,10 @@ absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n                                    llvm::Value* output) {\n   TF_RET_CHECK(computation.num_parameters() > 0);\n \n-  TF_ASSIGN_OR_RETURN(llvm::Function * emitted_function,\n-                      IrEmitter(&ir_emitter_context, /*is_nested=*/true)\n-                          .CodegenNestedComputation(computation));\n+  TF_ASSIGN_OR_RETURN(\n+      llvm::Function * emitted_function,\n+      IrEmitter(&ir_emitter_context, llvm_module, /*is_nested=*/true)\n+          .CodegenNestedComputation(computation));\n \n   // Operands are in default address space for non-AMDGPU target.\n   // However for AMDGPU target, addrspacecast alloca variables from\n@@ -456,8 +458,8 @@ absl::StatusOr<llvm::Function*> IrEmitter::CodegenNestedComputation(\n     return function;\n   }\n \n-  TF_RETURN_IF_ERROR(EmitConstants(ir_emitter_context_->llvm_module(),\n-                                   ir_emitter_context_, nested_computation));\n+  TF_RETURN_IF_ERROR(\n+      EmitConstants(module_, ir_emitter_context_, nested_computation));\n   std::vector<const HloInstruction*> io_hlos;\n   std::vector<llvm::Type*> argument_types;\n   std::vector<int64_t> argument_dereferenceable_bytes;\n@@ -751,17 +753,7 @@ absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n     IrEmitterContext* ir_emitter_context) {\n   std::string op_name(sort->name());\n \n-  // Copy of the main context with the local module.\n-  IrEmitterContext local_ir_emitter_context(\n-      &ir_emitter_context->hlo_module(),\n-      &ir_emitter_context->buffer_assignment(),\n-      &ir_emitter_context->execution_stream_assignment(),\n-      std::string(ir_emitter_context->platform_name()),\n-      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n-      llvm_module, ir_emitter_context->llvm_module_constants(),\n-      ir_emitter_context->emit_kernels());\n-\n-  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+  IrEmitter ir_emitter(ir_emitter_context, llvm_module, /*nested=*/false);\n \n   int64_t dimension_to_sort = sort->sort_dimension();\n   const Shape& keys_shape = sort->operand(0)->shape();\n@@ -897,7 +889,7 @@ absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n                              : standard_num_iterations_in_sort_dim,\n         tile_size, kUnrollFactor,\n         [&](absl::Span<llvm::Value* const> operands, llvm::Value* output) {\n-          return CallNestedComputation(builder, local_ir_emitter_context,\n+          return CallNestedComputation(builder, *ir_emitter_context,\n                                        llvm_module, *comparator, operands,\n                                        output);\n         });\n@@ -935,17 +927,7 @@ absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n     IrEmitterContext* ir_emitter_context) {\n   std::string ir_name = std::string(hlo->name());\n \n-  // Copy of the main context with the local module.\n-  IrEmitterContext local_ir_emitter_context(\n-      &ir_emitter_context->hlo_module(),\n-      &ir_emitter_context->buffer_assignment(),\n-      &ir_emitter_context->execution_stream_assignment(),\n-      std::string(ir_emitter_context->platform_name()),\n-      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n-      llvm_module, ir_emitter_context->llvm_module_constants(),\n-      ir_emitter_context->emit_kernels());\n-\n-  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+  IrEmitter ir_emitter(ir_emitter_context, llvm_module, /*nested=*/false);\n \n   constexpr int kUnrollFactor = 1;\n   const Shape& input_shape = hlo->operand(0)->shape();\n@@ -1093,17 +1075,7 @@ absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n     IrEmitterContext* ir_emitter_context) {\n   std::string ir_name = std::string(hlo->name());\n \n-  // Copy of the main context with the local module.\n-  IrEmitterContext local_ir_emitter_context(\n-      &ir_emitter_context->hlo_module(),\n-      &ir_emitter_context->buffer_assignment(),\n-      &ir_emitter_context->execution_stream_assignment(),\n-      std::string(ir_emitter_context->platform_name()),\n-      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n-      llvm_module, ir_emitter_context->llvm_module_constants(),\n-      ir_emitter_context->emit_kernels());\n-\n-  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+  IrEmitter ir_emitter(ir_emitter_context, llvm_module, /*nested=*/false);\n   constexpr int kUnrollFactor = 1;\n   const Shape& input_shape = hlo->operand(0)->shape();\n \n@@ -1239,17 +1211,7 @@ absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateStateLLVMIR(\n     IrEmitterContext* ir_emitter_context) {\n   std::string ir_name = std::string(hlo->name());\n \n-  // Copy of the main context with the local module.\n-  IrEmitterContext local_ir_emitter_context(\n-      &ir_emitter_context->hlo_module(),\n-      &ir_emitter_context->buffer_assignment(),\n-      &ir_emitter_context->execution_stream_assignment(),\n-      std::string(ir_emitter_context->platform_name()),\n-      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n-      llvm_module, ir_emitter_context->llvm_module_constants(),\n-      ir_emitter_context->emit_kernels());\n-\n-  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+  IrEmitter ir_emitter(ir_emitter_context, llvm_module, /*nested=*/false);\n \n   auto& b = *ir_emitter.builder();\n   // Emit a kernel to increment the global state for Philox RNG"
        },
        {
            "sha": "e018e8291020fad889936ac8576ebad19675bd73",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -119,7 +119,7 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     absl::Span<const Shape> unmanaged_arguments) const {\n   std::string suggested_kernel_name = std::string(fusion.name());\n   auto local_module =\n-      ir_emitter_context.CreateLocalLLVMModule(suggested_kernel_name);\n+      ir_emitter_context.CreateLLVMModule(suggested_kernel_name);\n   llvm::IRBuilder builder(local_module->getContext());\n   VLOG(3) << fusion.ToString();\n   TF_ASSIGN_OR_RETURN(\n@@ -187,9 +187,7 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n                                        sanitized_kernel_name, launch_dimensions,\n                                        kernel_arguments));\n \n-    PopulateNvvmAnnotations(ir_emitter_context.llvm_module(), kernel,\n-                            triton_wrapper_result);\n-\n+    PopulateNvvmAnnotations(local_module.get(), kernel, triton_wrapper_result);\n \n     return {{kernel->getName().str(), launch_dimensions,\n              triton_wrapper_result.cluster_dim,"
        },
        {
            "sha": "0238d62219bdd66c39b8644c9339635f0102faf3",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -1483,6 +1483,7 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@llvm-project//llvm:AsmParser\",\n+        \"@llvm-project//llvm:Linker\",\n         \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:TransformUtils\",\n         \"@llvm-project//llvm:ir_headers\","
        },
        {
            "sha": "c4ed040810da995c9e8affd23484c614ce52a8af",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 62,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/log/check.h\"\n@@ -39,6 +38,7 @@ limitations under the License.\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n+#include \"llvm/Linker/Linker.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"llvm/Transforms/Utils/SplitModule.h\"\n #include \"mlir/IR/Diagnostics.h\"\n@@ -119,27 +119,9 @@ void RemoveUnusedAndUninitializedGlobals(\n   }\n }\n \n-CompileModuleResults InitializeResults(const HloModule* hlo_module,\n-                                       llvm::LLVMContext* llvm_context,\n-                                       const std::string& target_triple,\n-                                       const std::string& data_layout,\n-                                       const bool split_constants_module) {\n-  absl::string_view module_name = hlo_module->name();\n+CompileModuleResults InitializeResults(const HloModule* hlo_module) {\n   CompileModuleResults results;\n-  results.module_name = module_name;\n-  results.llvm_module =\n-      std::make_unique<llvm::Module>(module_name, *llvm_context);\n-  results.llvm_module->setTargetTriple(llvm::Triple(target_triple));\n-  results.llvm_module->setDataLayout(data_layout);\n-\n-  if (split_constants_module) {\n-    // Constants are emitted into a separate module to avoid caching them.\n-    results.llvm_module_constants = std::make_unique<llvm::Module>(\n-        absl::StrCat(module_name, \"_consts\"), *llvm_context);\n-    results.llvm_module_constants->setTargetTriple(llvm::Triple(target_triple));\n-    results.llvm_module_constants->setDataLayout(data_layout);\n-  }\n-\n+  results.module_name = hlo_module->name();\n   results.use_original_allocations = true;\n   results.execution_stream_assignment =\n       std::make_unique<ExecutionStreamAssignment>(hlo_module);\n@@ -179,37 +161,6 @@ bool UseCache(const DebugOptions& options, bool split_constants_module) {\n          !options.xla_gpu_kernel_cache_file().empty();\n }\n \n-absl::StatusOr<std::unique_ptr<SequentialThunk>> LowerHlo(\n-    const HloModule* hlo_module, IrEmitterContext& ir_emitter_context,\n-    llvm::Module* llvm_module_constants, se::Platform::Id platform_id,\n-    bool use_cache) {\n-  const DebugOptions& options = hlo_module->config().debug_options();\n-  ScopedAnnotation annotation(Phase(\"XlaEmitLlvmIr\", hlo_module));\n-  uint64_t start_usecs = tsl::Env::Default()->NowMicros();\n-\n-  if (use_cache) {\n-    TF_RETURN_IF_ERROR(\n-        LoadCache(ir_emitter_context, options.xla_gpu_kernel_cache_file()));\n-  }\n-  auto thunk_emitter = std::make_unique<ThunkEmitter>(&ir_emitter_context);\n-  XLA_SCOPED_LOGGING_TIMER(absl::StrCat(\n-      \"GpuCompiler::RunBackend - IR emission for \", hlo_module->name()));\n-\n-  TF_ASSIGN_OR_RETURN(auto thunks,\n-                      thunk_emitter->EmitHloEntryComputation(hlo_module));\n-\n-  RemoveUnusedAndUninitializedGlobals(\n-      platform_id, options, ir_emitter_context.llvm_module_constants(),\n-      ir_emitter_context.constants());\n-\n-  // This won't record values for calls that error out (because if they error\n-  // out we have no way of telling how far through the process we got).\n-  uint64_t end_usecs = tsl::Env::Default()->NowMicros();\n-  RecordHloToLlvmDuration(end_usecs - start_usecs);\n-  return std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n-                                           std::move(thunks));\n-}\n-\n }  // namespace\n \n absl::Status LoadCache(IrEmitterContext& ir_emitter_context,\n@@ -285,9 +236,7 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(\n   const bool use_cache =\n       UseCache(hlo_module->config().debug_options(), split_constants_module);\n \n-  CompileModuleResults results =\n-      InitializeResults(hlo_module, llvm_context, target_triple, data_layout,\n-                        split_constants_module);\n+  CompileModuleResults results = InitializeResults(hlo_module);\n \n   TF_ASSIGN_OR_RETURN(\n       results.buffer_assignment,\n@@ -309,14 +258,49 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(\n   IrEmitterContext ir_emitter_context(\n       hlo_module, results.buffer_assignment.get(),\n       results.execution_stream_assignment.get(), platform->Name(), device_desc,\n-      mlir_context.get(), results.llvm_module.get(),\n-      results.llvm_module_constants.get(),\n-      /*emit_kernels=*/true);\n+      mlir_context.get(), llvm_context, /*emit_kernels=*/true,\n+      llvm::Triple(target_triple), data_layout);\n+  ThunkEmitter thunk_emitter(&ir_emitter_context);\n \n-  TF_ASSIGN_OR_RETURN(\n-      results.executable,\n-      LowerHlo(hlo_module, ir_emitter_context,\n-               results.llvm_module_constants.get(), platform->id(), use_cache));\n+  const DebugOptions& options = hlo_module->config().debug_options();\n+  ScopedAnnotation annotation(Phase(\"XlaEmitLlvmIr\", hlo_module));\n+  uint64_t start_usecs = tsl::Env::Default()->NowMicros();\n+\n+  if (use_cache) {\n+    TF_RETURN_IF_ERROR(\n+        LoadCache(ir_emitter_context, options.xla_gpu_kernel_cache_file()));\n+  }\n+  XLA_SCOPED_LOGGING_TIMER(absl::StrCat(\n+      \"GpuCompiler::RunBackend - IR emission for \", hlo_module->name()));\n+\n+  TF_ASSIGN_OR_RETURN(auto sequential_thunk,\n+                      thunk_emitter.EmitHloEntryComputation(hlo_module));\n+  results.executable = std::move(sequential_thunk);\n+\n+  // Assemble the LLVM module with all kernels.\n+  results.llvm_module =\n+      split_constants_module\n+          ? ir_emitter_context.CreateLLVMModule(hlo_module->name())\n+          : thunk_emitter.ConsumeConstantsModule();\n+  for (auto& kernel_module : thunk_emitter.ConsumeKernelModules()) {\n+    CHECK(!llvm::Linker::linkModules(*results.llvm_module.get(),\n+                                     std::move(kernel_module),\n+                                     llvm::Linker::Flags::OverrideFromSrc));\n+  }\n+  if (split_constants_module) {\n+    results.llvm_module_constants = thunk_emitter.ConsumeConstantsModule();\n+  }\n+\n+  RemoveUnusedAndUninitializedGlobals(platform->id(), options,\n+                                      split_constants_module\n+                                          ? results.llvm_module_constants.get()\n+                                          : results.llvm_module.get(),\n+                                      ir_emitter_context.constants());\n+\n+  // This won't record values for calls that error out (because if they error\n+  // out we have no way of telling how far through the process we got).\n+  uint64_t end_usecs = tsl::Env::Default()->NowMicros();\n+  RecordHloToLlvmDuration(end_usecs - start_usecs);\n \n   results.constants = std::move(ir_emitter_context.constants());\n   if (use_cache) {"
        },
        {
            "sha": "c528f354fdfac950316189640dfc2aa120478df4",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -3006,9 +3006,6 @@ GpuCompiler::LoadExecutableFromAotResult(\n       stream_exec.GetDeviceDescription();\n   llvm::LLVMContext llvm_context;\n \n-  auto llvm_module = std::make_unique<llvm::Module>(\"\", llvm_context);\n-  llvm_module->setTargetTriple(llvm::Triple(target_triple()));\n-  llvm_module->setDataLayout(data_layout());\n \n   // Recreate BufferAssignment from proto.\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n@@ -3019,8 +3016,8 @@ GpuCompiler::LoadExecutableFromAotResult(\n \n   IrEmitterContext ir_emitter_context(\n       hlo_module.get(), buffer_assignment.get(), &execution_stream_assignment,\n-      platform_name, gpu_device_info, mlir_context(), llvm_module.get(),\n-      /*llvm_module_constants=*/nullptr, /*emit_kernels=*/false);\n+      platform_name, gpu_device_info, mlir_context(), &llvm_context,\n+      /*emit_kernels=*/false, llvm::Triple(target_triple()), data_layout());\n \n   absl::string_view cache_file_path =\n       hlo_module->config().debug_options().xla_gpu_kernel_cache_file();\n@@ -3031,9 +3028,9 @@ GpuCompiler::LoadExecutableFromAotResult(\n     TF_RETURN_IF_ERROR(LoadCache(ir_emitter_context, cache_file_path));\n   }\n \n-  auto thunk_emitter = std::make_unique<ThunkEmitter>(&ir_emitter_context);\n-  TF_ASSIGN_OR_RETURN(auto thunks,\n-                      thunk_emitter->EmitHloEntryComputation(hlo_module.get()));\n+  ThunkEmitter thunk_emitter(&ir_emitter_context);\n+  TF_ASSIGN_OR_RETURN(auto sequential_thunk,\n+                      thunk_emitter.EmitHloEntryComputation(hlo_module.get()));\n \n   // Get all other fields required by GpuExecutable.\n   std::vector<GpuExecutable::ConstantInfo> constants =\n@@ -3055,9 +3052,7 @@ GpuCompiler::LoadExecutableFromAotResult(\n         /*dnn_compiled_graphs=*/\n         BinaryMap(proto.dnn_compiled_graphs().cbegin(),\n                   proto.dnn_compiled_graphs().cend()),\n-        /*executable=*/\n-        std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n-                                          std::move(thunks)),\n+        /*executable=*/std::move(sequential_thunk),\n         /*constants=*/std::move(constants),\n         /*output_info=*/std::move(output_info),\n         /*module_name=*/std::move(hlo_module_name),"
        },
        {
            "sha": "8268f48c5809de04c4c6bbb7385ba8b41f101734",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 16,
            "deletions": 24,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -71,16 +71,18 @@ class IrEmitterContext {\n                    const ExecutionStreamAssignment* execution_stream_assignment,\n                    std::string platform_name,\n                    const se::DeviceDescription& gpu_device_info,\n-                   mlir::MLIRContext* mlir_context, llvm::Module* llvm_module,\n-                   llvm::Module* llvm_module_constants, bool emit_kernels)\n+                   mlir::MLIRContext* mlir_context,\n+                   llvm::LLVMContext* llvm_context, bool emit_kernels,\n+                   llvm::Triple target_triple, std::string data_layout)\n       : hlo_module_(hlo_module),\n         buffer_assignment_(buffer_assignment),\n         execution_stream_assignment_(execution_stream_assignment),\n         platform_name_(std::move(platform_name)),\n         gpu_device_info_(gpu_device_info),\n         mlir_context_(mlir_context),\n-        llvm_module_(llvm_module),\n-        llvm_module_constants_(llvm_module_constants),\n+        llvm_context_(llvm_context),\n+        data_layout_(std::move(data_layout)),\n+        target_triple_(std::move(target_triple)),\n         emit_kernels_(emit_kernels) {}\n   // Disallow copy and assign.\n   IrEmitterContext(const IrEmitterContext&) = delete;\n@@ -103,13 +105,11 @@ class IrEmitterContext {\n   }\n \n   mlir::MLIRContext* mlir_context() { return mlir_context_; }\n-  llvm::Module* llvm_module() { return llvm_module_; }\n+  llvm::LLVMContext* llvm_context() { return llvm_context_; }\n+\n+  const std::string& data_layout() { return data_layout_; }\n+  const llvm::Triple& target_triple() { return target_triple_; }\n \n-  // A separate module can optionally be used to emit constants.\n-  llvm::Module* llvm_module_constants() {\n-    return (llvm_module_constants_ == nullptr) ? llvm_module_\n-                                               : llvm_module_constants_;\n-  }\n   absl::StatusOr<InlinedModule*> get_inlined_module() {\n     if (inlined_module_ == nullptr) {\n       TF_ASSIGN_OR_RETURN(InlinedModule inlined_module,\n@@ -122,15 +122,6 @@ class IrEmitterContext {\n \n   std::vector<GpuExecutable::ConstantInfo>& constants() { return constants_; }\n \n-  std::unique_ptr<llvm::Module> CreateLocalLLVMModule(\n-      const std::string& module_name) {\n-    auto llvm_module =\n-        std::make_unique<llvm::Module>(module_name, llvm_module_->getContext());\n-    llvm_module->setTargetTriple(llvm::Triple(llvm_module_->getTargetTriple()));\n-    llvm_module->setDataLayout(llvm_module_->getDataLayout());\n-    return llvm_module;\n-  }\n-\n   const DebugOptions& debug_options() const {\n     return hlo_module_->config().debug_options();\n   }\n@@ -159,9 +150,9 @@ class IrEmitterContext {\n   std::unique_ptr<llvm::Module> CreateLLVMModule(\n       const std::string& module_name) {\n     auto llvm_module =\n-        std::make_unique<llvm::Module>(module_name, llvm_module_->getContext());\n-    llvm_module->setTargetTriple(llvm::Triple(llvm_module_->getTargetTriple()));\n-    llvm_module->setDataLayout(llvm_module_->getDataLayout());\n+        std::make_unique<llvm::Module>(module_name, *llvm_context_);\n+    llvm_module->setTargetTriple(target_triple_);\n+    llvm_module->setDataLayout(data_layout_);\n     return llvm_module;\n   }\n \n@@ -172,12 +163,13 @@ class IrEmitterContext {\n   std::string platform_name_;\n   const se::DeviceDescription& gpu_device_info_;\n   mlir::MLIRContext* mlir_context_;\n-  llvm::Module* llvm_module_;\n-  llvm::Module* llvm_module_constants_;\n+  llvm::LLVMContext* llvm_context_;\n   NameUniquer name_uniquer_;\n   std::vector<GpuExecutable::ConstantInfo> constants_;\n   KernelReuseCache kernel_cache_;\n   std::unique_ptr<InlinedModule> inlined_module_;\n+  const std::string data_layout_;\n+  llvm::Triple target_triple_;\n \n   CollectivesAsyncEvents collectives_async_events_;\n   InstructionToHostExecuteAsyncEvents instruction_to_host_execute_async_events_;"
        },
        {
            "sha": "10067e5fc1c90dbcfe9631839794e3d0f8338ab8",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 31,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -39,13 +39,8 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/APInt.h\"\n #include \"llvm/ADT/StringRef.h\"\n-#include \"llvm/IR/BasicBlock.h\"\n-#include \"llvm/IR/Function.h\"\n #include \"llvm/IR/Instructions.h\"\n #include \"llvm/IR/LLVMContext.h\"\n-#include \"llvm/IR/Module.h\"\n-#include \"llvm/Linker/Linker.h\"\n-#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/AsmParser/AsmParser.h\"\n #include \"mlir/Dialect/MemRef/Transforms/Passes.h\"\n #include \"mlir/IR/Attributes.h\"\n@@ -59,7 +54,6 @@ limitations under the License.\n #include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/NVVM/NVVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/ROCDL/ROCDLToLLVMIRTranslation.h\"\n-#include \"mlir/Target/LLVMIR/Export.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/backends/gpu/codegen/llvm/llvm_emitter.h\"\n@@ -341,7 +335,9 @@ ThunkEmitter::ThunkEmitter(IrEmitterContext* ir_emitter_context)\n       send_recv_events_(std::make_shared<HostSendRecvAsyncEvents>()),\n       copy_events_(std::make_shared<CopyThunk::AsyncEvents>()),\n       nvshmem_buffer_addresses_(std::make_shared<NvshmemBufferAddresses>()),\n-      call_graph_(CallGraph::Build(&ir_emitter_context->hlo_module())) {}\n+      call_graph_(CallGraph::Build(&ir_emitter_context->hlo_module())),\n+      constants_module_(ir_emitter_context_->CreateLLVMModule(\n+          absl::StrCat(ir_emitter_context_->hlo_module().name(), \"_consts\"))) {}\n \n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConstant(\n     const HloConstantInstruction* instr) {\n@@ -358,9 +354,9 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConstant(\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice slice,\n                       GetAllocationSliceForHlo(instr, {}));\n \n-  GpuExecutable::ConstantInfo info = AppendGlobalConstant(\n-      ir_emitter_context_->llvm_module_constants(), num_elements, element_bytes,\n-      global_name, slice.index(), std::move(content));\n+  GpuExecutable::ConstantInfo info =\n+      AppendGlobalConstant(constants_module_.get(), num_elements, element_bytes,\n+                           global_name, slice.index(), std::move(content));\n   ir_emitter_context_->constants().push_back(std::move(info));\n   return ThunkSequence{};\n }\n@@ -412,9 +408,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitPadToStatic(\n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitPadToStaticLLVMIR(instr, local_llvm_module.get(),\n                                             ir_emitter_context_));\n-  CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n-                                   std::move(local_llvm_module),\n-                                   llvm::Linker::Flags::OverrideFromSrc));\n+  kernel_modules_.push_back(std::move(local_llvm_module));\n   return thunk_sequence;\n }\n \n@@ -428,9 +422,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSliceToDynamic(\n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitSliceToDynamicLLVMIR(instr, local_llvm_module.get(),\n                                                ir_emitter_context_));\n-  CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n-                                   std::move(local_llvm_module),\n-                                   llvm::Linker::Flags::OverrideFromSrc));\n+  kernel_modules_.push_back(std::move(local_llvm_module));\n   return thunk_sequence;\n }\n \n@@ -1303,9 +1295,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n                              .status());\n     }\n \n-    TF_RET_CHECK(!llvm::Linker::linkModules(\n-        *ir_emitter_context_->llvm_module(), std::move(local_module),\n-        llvm::Linker::Flags::OverrideFromSrc));\n+    kernel_modules_.push_back(std::move(local_module));\n     return {{kernel_name, launch_dimensions, result.cluster_dim,\n              result.shmem_bytes}};\n   };\n@@ -1378,9 +1368,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitFusion(\n \n   // Use override flag because libdevice functions can be present in both.\n   if (result.module) {\n-    TF_RET_CHECK(!llvm::Linker::linkModules(\n-        *ir_emitter_context_->llvm_module(), std::move(result.module),\n-        llvm::Linker::Flags::OverrideFromSrc));\n+    kernel_modules_.push_back(std::move(result.module));\n   }\n \n   const ExecutionStreamAssignment& stream_assignment =\n@@ -1518,9 +1506,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitRngGetAndUpdateState(\n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitRngGetAndUpdateStateLLVMIR(\n                           instr, local_llvm_module.get(), ir_emitter_context_));\n-  CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n-                                   std::move(local_llvm_module),\n-                                   llvm::Linker::Flags::OverrideFromSrc));\n+  kernel_modules_.push_back(std::move(local_llvm_module));\n   return thunk_sequence;\n }\n \n@@ -1572,9 +1558,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSort(\n                       EmitBitonicSortLLVMIR(sort, local_llvm_module.get(),\n                                             ir_emitter_context_));\n   AppendThunkSequence(thunks, sort_thunks);\n-  llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n-                            std::move(local_llvm_module),\n-                            llvm::Linker::Flags::OverrideFromSrc);\n+  kernel_modules_.push_back(std::move(local_llvm_module));\n   return thunks;\n }\n \n@@ -2823,9 +2807,12 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloInstruction(\n   return Internal(\"Unhandled HLO instruction\");\n }\n \n-absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloEntryComputation(\n-    const HloModule* module) {\n-  return EmitHloComputation(module->entry_computation());\n+absl::StatusOr<std::unique_ptr<SequentialThunk>>\n+ThunkEmitter::EmitHloEntryComputation(const HloModule* module) {\n+  TF_ASSIGN_OR_RETURN(auto thunks,\n+                      EmitHloComputation(module->entry_computation()));\n+  return std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n+                                           std::move(thunks));\n }\n \n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloComputation("
        },
        {
            "sha": "a478a2aff0729af7149abf61d46eb881d85642ab",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.h",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b1decaf1d8d417b135beec40f777c62a6403b56/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h?ref=5b1decaf1d8d417b135beec40f777c62a6403b56",
            "patch": "@@ -20,11 +20,13 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n+#include <utility>\n #include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"llvm/IR/Module.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n@@ -53,9 +55,17 @@ class ThunkEmitter {\n   ThunkEmitter(const ThunkEmitter&) = delete;\n   ThunkEmitter& operator=(const ThunkEmitter&) = delete;\n \n-  absl::StatusOr<ThunkSequence> EmitHloEntryComputation(\n+  absl::StatusOr<std::unique_ptr<SequentialThunk>> EmitHloEntryComputation(\n       const HloModule* module);\n \n+  llvm::Module* constants_module() { return constants_module_.get(); }\n+  std::unique_ptr<llvm::Module> ConsumeConstantsModule() {\n+    return std::move(constants_module_);\n+  }\n+  std::vector<std::unique_ptr<llvm::Module>> ConsumeKernelModules() {\n+    return std::move(kernel_modules_);\n+  }\n+\n  private:\n   // Emits code for the given HLO computation.\n   //\n@@ -227,6 +237,12 @@ class ThunkEmitter {\n \n   // Cache to store the call_graph.\n   std::unique_ptr<CallGraph> call_graph_;\n+\n+  // Module with constants.\n+  std::unique_ptr<llvm::Module> constants_module_;\n+\n+  // Modules for each emitted kernel.\n+  std::vector<std::unique_ptr<llvm::Module>> kernel_modules_;\n };\n \n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 313,
        "additions": 123,
        "deletions": 190
    }
}