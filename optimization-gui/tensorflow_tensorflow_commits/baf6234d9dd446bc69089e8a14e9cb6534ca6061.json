{
    "author": "serach24",
    "message": "PR #31746: Change the padded index value to be invalid in prefix scan of scatter_determinism_expander\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31746\n\nüìù Summary of Changes\nFixed a critical bug in the ScatterDeterminismExpander where the prefix scan algorithm was using zero padding for indices, causing false matches with valid index 0. This led to incorrect accumulation in scatter operations, particularly affecting scatter_set operations.\nKey changes:\nModified CreateScanWithIndices to use operand tensor size as padding value instead of zero\nUpdated function signatures to pass operand_dims instead of full scatter instruction\nFixed HLO verification test expectations to match the corrected generated code\n\nüéØ Justification\nThis fix is critical for correctness of scatter operations, especially:\nScatter_set operations where zero is a valid update value\nTensorFlow scatter operations that were failing due to incorrect accumulation\nThe bug was causing incorrect results in scenarios like:\nScattering [1, 2, 0, 0] into positions [0, 1, 2, 3] would incorrectly produce [0, 2, 0, 0] instead of [1, 2, 0, 0]\nConsecutive duplicate indices like [0, 0] were not being properly accumulated\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüß™ Unit Tests\nUpdated existing tests:\nScatterAddHloVerificationTest: Updated HLO verification patterns to match corrected generated code\nAll existing scatter determinism expander tests now pass with the fix\nTest coverage:\nScalar scatter operations with duplicate indices\nMulti-dimensional scatter operations (InversePermutation test)\nScatter-set vs scatter-add operations\nOut-of-bounds index handling\n\nüß™ Execution Tests:\nExisting tests that now pass:\nTensorFlowScatterV1_UpdateTwice: Tests consecutive duplicate indices [0, 0]\nTest scenarios covered:\nScatter operations with index 0 (previously causing false matches)\nScatter operations with consecutive duplicate indices\nMulti-dimensional scatter operations with various index patterns\nBoth scatter_add and scatter_set operations\nThe fix ensures that all scatter operations now produce correct, deterministic results regardless of whether they use index 0 or have consecutive duplicate indices.\nCopybara import of the project:\n\n--\n093f6e5ea5b50fc82778b5683770e1b7658b8f2d by Chenhao Jiang <chenhaoj@nvidia.com>:\n\nChange the padded index value to be invalid in prefix scan of scatter_determinism_expander\n\nMerging this change closes #31746\n\nPiperOrigin-RevId: 810793550",
    "sha": "baf6234d9dd446bc69089e8a14e9cb6534ca6061",
    "files": [
        {
            "sha": "8476085fdf84d5460e5b3af43b8d54d7701bf3f8",
            "filename": "third_party/xla/xla/service/scatter_determinism_expander.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/baf6234d9dd446bc69089e8a14e9cb6534ca6061/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/baf6234d9dd446bc69089e8a14e9cb6534ca6061/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc?ref=baf6234d9dd446bc69089e8a14e9cb6534ca6061",
            "patch": "@@ -311,7 +311,7 @@ static std::vector<HloInstruction*> SortIndicesAndUpdates(\n //   input for the next iteration.\n static absl::StatusOr<HloInstruction*> CreateScanWithIndices(\n     HloComputation* parent, HloInstruction* updates, HloInstruction* indices,\n-    HloComputation* to_apply) {\n+    HloComputation* to_apply, absl::Span<const int64_t> operand_dims) {\n   const Shape& updates_shape = updates->shape();\n   const Shape& indices_shape = indices->shape();\n   // Get the length of the input array\n@@ -351,11 +351,19 @@ static absl::StatusOr<HloInstruction*> CreateScanWithIndices(\n \n     auto* shifted_indices = parent->AddInstruction(HloInstruction::CreateSlice(\n         shifted_indices_shape, indices, start_indices, end_indices, strides));\n+    // Use the total size of the operand tensor as out-of-bounds value\n+    // This matches how FlattenIndices works - it uses the total tensor size\n+    int64_t total_size = 1;\n+    for (int64_t dim : operand_dims) {\n+      total_size *= dim;\n+    }\n+    int64_t out_of_bounds_value = total_size;\n     auto* padding_indices =\n         parent->AddInstruction(HloInstruction::CreateBroadcast(\n             padding_indices_shape,\n-            parent->AddInstruction(HloInstruction::CreateConstant(\n-                LiteralUtil::CreateR0(indices_shape.element_type(), 0))),\n+            parent->AddInstruction(\n+                HloInstruction::CreateConstant(LiteralUtil::CreateR0(\n+                    indices_shape.element_type(), out_of_bounds_value))),\n             {}));\n \n     auto* concatenated_updates =\n@@ -368,15 +376,6 @@ static absl::StatusOr<HloInstruction*> CreateScanWithIndices(\n     auto* indices_mask = parent->AddInstruction(HloInstruction::CreateCompare(\n         ShapeUtil::MakeShape(PRED, {num_updates}), indices,\n         concatenated_indices, ComparisonDirection::kEq));\n-    auto* first_element_false = parent->AddInstruction(\n-        HloInstruction::CreateConstant(LiteralUtil::CreateR1<bool>({false})));\n-    auto* remaining_mask = parent->AddInstruction(HloInstruction::CreateSlice(\n-        ShapeUtil::MakeShape(PRED, {num_updates - 1}), indices_mask, {1},\n-        {num_updates}, {1}));\n-    indices_mask = parent->AddInstruction(HloInstruction::CreateConcatenate(\n-        ShapeUtil::MakeShape(PRED, {num_updates}),\n-        {first_element_false, remaining_mask}, 0));\n-\n     std::vector<HloInstruction*> map_operands = {current_updates,\n                                                  concatenated_updates};\n     TF_ASSIGN_OR_RETURN(HloInstruction * reduced_updates,\n@@ -391,16 +390,17 @@ static absl::StatusOr<HloInstruction*> CreateScanWithIndices(\n absl::StatusOr<std::vector<HloInstruction*>> ComputePrefixScan(\n     const std::vector<HloInstruction*>& sorted_updates,\n     HloInstruction* sorted_scalar_indices, HloScatterInstruction* scatter,\n-    HloComputation* parent) {\n+    HloComputation* parent, absl::Span<const int64_t> operand_dims) {\n   std::vector<HloInstruction*> prefix_scans(sorted_updates.size());\n   HloInstruction* prefix_scan_update = nullptr;\n   for (int i = 0; i < sorted_updates.size(); i++) {\n     TF_ASSIGN_OR_RETURN(\n         HloComputation * to_apply,\n         CallComputationAndGetIthOutputWithBinaryParams(scatter->to_apply(), i));\n-    TF_ASSIGN_OR_RETURN(prefix_scan_update,\n-                        CreateScanWithIndices(parent, sorted_updates[i],\n-                                              sorted_scalar_indices, to_apply));\n+    TF_ASSIGN_OR_RETURN(\n+        prefix_scan_update,\n+        CreateScanWithIndices(parent, sorted_updates[i], sorted_scalar_indices,\n+                              to_apply, operand_dims));\n     CHECK(prefix_scan_update != nullptr) << i << \"th update is nullptr\";\n     prefix_scans[i] = prefix_scan_update;\n   }\n@@ -842,9 +842,10 @@ absl::StatusOr<HloInstruction*> ScatterDeterminismExpander::ExpandInstruction(\n     sorted_indices = sorted_tensors[sorted_tensors.size() - 1];\n   }\n \n-  TF_ASSIGN_OR_RETURN(std::vector<HloInstruction*> prefix_scan_updates,\n-                      ComputePrefixScan(sorted_updates, sorted_scalar_indices,\n-                                        scatter, parent));\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<HloInstruction*> prefix_scan_updates,\n+      ComputePrefixScan(sorted_updates, sorted_scalar_indices, scatter, parent,\n+                        scatter_operands[0]->shape().dimensions()));\n   if (non_scalar_update) {\n     // As the indices are expanded, we need to recompute out-of-bound tensor\n     // with the same shape"
        },
        {
            "sha": "9a68bde4f0d5198fa4472d56257c81ea441b0c11",
            "filename": "third_party/xla/xla/service/scatter_determinism_expander_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/baf6234d9dd446bc69089e8a14e9cb6534ca6061/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/baf6234d9dd446bc69089e8a14e9cb6534ca6061/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander_test.cc?ref=baf6234d9dd446bc69089e8a14e9cb6534ca6061",
            "patch": "@@ -883,21 +883,18 @@ TEST_F(ScatterDeterminismExpanderTest, ScatterAddHloVerificationTest) {\n     CHECK-DAG:   %[[CONSTANT:.*]] = s32[1]{0} constant({2})\n     CHECK-DAG:   %[[BROADCAST0:.*]] = s32[2,1]{1,0} broadcast(%[[CONSTANT]]), dimensions={1}\n     CHECK-DAG:   %[[SELECT1:.*]] = s32[2,1]{1,0} select(%[[BROADCAST3]], %[[RESHAPE5]], %[[BROADCAST0]])\n-    CHECK-DAG:   %[[FALSE:.*]] = pred[1]{0} constant({0})\n-    CHECK-DAG:   %[[CONSTANT2:.*]] = s32[] constant(0)\n+    CHECK-DAG:   %[[CONSTANT2:.*]] = s32[] constant(2)\n     CHECK-DAG:   %[[BROADCAST1:.*]] = s32[1]{0} broadcast(%[[CONSTANT2]]), dimensions={}\n     CHECK-DAG:   %[[SLICE1:.*]] = s32[1]{0} slice(%[[GET_TUPLE_ELEMENT]]), slice={[0:1]}\n     CHECK-DAG:   %[[CONCATENATE1:.*]] = s32[2]{0} concatenate(%[[BROADCAST1]], %[[SLICE1]]), dimensions={0}\n     CHECK-DAG:   %[[COMPARE1:.*]] = pred[2]{0} compare(%[[GET_TUPLE_ELEMENT]], %[[CONCATENATE1]]), direction=EQ\n-    CHECK-DAG:   %[[SLICE2:.*]] = pred[1]{0} slice(%[[COMPARE1]]), slice={[1:2]}\n-    CHECK-DAG:   %[[CONCATENATE3:.*]] = pred[2]{0} concatenate(%[[FALSE]], %[[SLICE2]]), dimensions={0}\n     CHECK-DAG:   %[[GET_TUPLE_ELEMENT1:.*]] = f32[2]{0} get-tuple-element(%[[SORT]]), index=1\n     CHECK-DAG:   %[[CONSTANT1:.*]] = f32[] constant(0)\n     CHECK-DAG:   %[[BROADCAST:.*]] = f32[1]{0} broadcast(%[[CONSTANT1]]), dimensions={}\n     CHECK-DAG:   %[[SLICE:.*]] = f32[1]{0} slice(%[[GET_TUPLE_ELEMENT1]]), slice={[0:1]}\n     CHECK-DAG:   %[[CONCATENATE:.*]] = f32[2]{0} concatenate(%[[BROADCAST]], %[[SLICE]]), dimensions={0}\n     CHECK-DAG:   %[[MAP:.*]] = f32[2]{0} map(%[[GET_TUPLE_ELEMENT1]], %[[CONCATENATE]]), dimensions={0}, to_apply=%scatter_computation\n-    CHECK-DAG:   %[[SELECT:.*]] = f32[2]{0} select(%[[CONCATENATE3]], %[[MAP]], %[[GET_TUPLE_ELEMENT1]])\n+    CHECK-DAG:   %[[SELECT:.*]] = f32[2]{0} select(%[[COMPARE1]], %[[MAP]], %[[GET_TUPLE_ELEMENT1]])\n     CHECK-DAG:  ROOT %[[SCATTER:.*]] = f32[2]{0} scatter(%[[OPERAND]], %[[SELECT1]], %[[SELECT]]),\n     CHECK-SAME:   update_window_dims={},\n     CHECK-SAME:   inserted_window_dims={0},"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 22,
        "deletions": 24
    }
}