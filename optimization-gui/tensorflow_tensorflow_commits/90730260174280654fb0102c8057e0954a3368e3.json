{
    "author": "ermilovmaxim",
    "message": "unify unused triton argument removal\n\nPiperOrigin-RevId: 836364268",
    "sha": "90730260174280654fb0102c8057e0954a3368e3",
    "files": [
        {
            "sha": "2cafa6e1f930ed758c182efcb1d2067ea315b0d9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=90730260174280654fb0102c8057e0954a3368e3",
            "patch": "@@ -258,6 +258,7 @@ cc_library(\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "26f5a4413bdeff85962849877dbba15bd5288e09",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 10,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=90730260174280654fb0102c8057e0954a3368e3",
            "patch": "@@ -27,13 +27,15 @@ limitations under the License.\n #include \"llvm/IR/Argument.h\"\n #include \"llvm/IR/Attributes.h\"\n #include \"llvm/IR/BasicBlock.h\"\n+#include \"llvm/IR/Constants.h\"\n #include \"llvm/IR/DerivedTypes.h\"\n #include \"llvm/IR/Function.h\"\n #include \"llvm/IR/GlobalValue.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Instructions.h\"\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Type.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/IR/AffineExpr.h\"\n@@ -50,6 +52,7 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n@@ -113,16 +116,6 @@ std::string GetSanitizedUniqueName(IrEmitterContext& ir_emitter_context,\n       llvm_ir::SanitizeFunctionName(suggested_name));\n }\n \n-absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n-    llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n-    const std::string& impl_fn_name, const std::string& unique_kernel_name,\n-    const emitters::KernelArguments& arguments,\n-    const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder) {\n-  return BuildKernelPrototypeFromUniqueName(\n-      llvm_module, gpu_device_info, impl_fn_name, unique_kernel_name, arguments,\n-      launch_dimensions, builder);\n-}\n-\n absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n     llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n     const std::string& impl_fn_name, const std::string& unique_kernel_name,\n@@ -192,5 +185,63 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n   return kernel;\n }\n \n+absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n+    llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n+    const std::string& impl_fn_name, const std::string& unique_kernel_name,\n+    const emitters::KernelArguments& arguments,\n+    const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder) {\n+  return BuildKernelPrototypeFromUniqueName(\n+      llvm_module, gpu_device_info, impl_fn_name, unique_kernel_name, arguments,\n+      launch_dimensions, builder);\n+}\n+\n+// Triton's kernel ABI expects additional scratchpad global memory for\n+// TMA and profiling information.\n+// For now it is only used for on-device creation of TMA descriptors, which\n+// we do not use yet, so we are just replacing this argument with a null\n+// pointer.\n+// TODO: b/381242007 - Allocate a proper buffer if we want to use\n+// device-side TMA APIs.\n+absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n+    IrEmitterContext& ir_emitter_context,\n+    const std::string& sanitized_kernel_name,\n+    LaunchDimensions& launch_dimensions,\n+    const emitters::KernelArguments& kernel_arguments) {\n+  llvm::Function* impl_fn =\n+      ir_emitter_context.llvm_module()->getFunction(sanitized_kernel_name);\n+  TF_RET_CHECK(impl_fn);\n+  impl_fn->setName(ir_emitter_context.name_uniquer()->GetUniqueName(\n+      sanitized_kernel_name + \"_impl\"));\n+\n+  llvm::IRBuilder builder(ir_emitter_context.llvm_module()->getContext());\n+\n+  TF_ASSIGN_OR_RETURN(llvm::Function * kernel,\n+                      BuildKernelPrototypeFromUniqueName(\n+                          ir_emitter_context.llvm_module(),\n+                          ir_emitter_context.gpu_device_info(),\n+                          impl_fn->getName().str(), sanitized_kernel_name,\n+                          kernel_arguments, launch_dimensions, &builder));\n+\n+  // Move function body into kernel prototype.\n+  llvm::Function* prototype_func = builder.GetInsertBlock()->getParent();\n+  prototype_func->splice(prototype_func->begin(), impl_fn);\n+  for (const auto& [impl_fn_arg, kernel_arg] :\n+       llvm::zip(impl_fn->args(), kernel->args())) {\n+    impl_fn_arg.replaceAllUsesWith(&kernel_arg);\n+  }\n+  CHECK_EQ(impl_fn->arg_size(), kernel->arg_size() + 2);\n+\n+  auto tma_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 2);\n+  tma_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n+      llvm::cast<llvm::PointerType>(tma_scratchpad_arg->getType())));\n+  auto profiling_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n+  profiling_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n+      llvm::cast<llvm::PointerType>(profiling_scratchpad_arg->getType())));\n+\n+  impl_fn->eraseFromParent();\n+\n+  return kernel;\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "9f85ba856a2db936bcf0c3b37cf351124fc43ac6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=90730260174280654fb0102c8057e0954a3368e3",
            "patch": "@@ -105,11 +105,11 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n     const emitters::KernelArguments& arguments,\n     const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder);\n \n-absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n-    llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n-    const std::string& impl_fn_name, const std::string& unique_kernel_name,\n-    const emitters::KernelArguments& arguments,\n-    const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder);\n+absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n+    IrEmitterContext& ir_emitter_context,\n+    const std::string& sanitized_kernel_name,\n+    LaunchDimensions& launch_dimensions,\n+    const emitters::KernelArguments& arguments);\n \n // Compute the kernel name. The opcode string may contain \"-\" which cannot be\n // in a PTX function name, so sanitize the name before uniquifying it."
        },
        {
            "sha": "4286a96ae96026d6a06a3184d3f89a265f1403b1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 35,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=90730260174280654fb0102c8057e0954a3368e3",
            "patch": "@@ -159,12 +159,12 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n   auto generate = [&]() -> absl::StatusOr<KernelReuseCache::Entry> {\n     VLOG(3) << \"Generating: \" << suggested_kernel_name;\n \n-    const std::string impl_fn_name = GetSanitizedUniqueName(\n-        ir_emitter_context, absl::StrCat(suggested_kernel_name, \"_impl\"));\n+    const std::string sanitized_kernel_name =\n+        GetSanitizedUniqueName(ir_emitter_context, suggested_kernel_name);\n \n     TF_ASSIGN_OR_RETURN(\n         TritonWrapperResult triton_wrapper_result,\n-        GenerateTritonKernelAndWrapper(fusion, impl_fn_name,\n+        GenerateTritonKernelAndWrapper(fusion, sanitized_kernel_name,\n                                        ir_emitter_context.gpu_device_info(),\n                                        ir_emitter_context.llvm_module(),\n                                        ir_emitter_context.mlir_context()));\n@@ -206,42 +206,14 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     CHECK(launch_config.has_value());\n     launch_dimensions = std::move(launch_config->launch_dimensions);\n \n-    llvm::Function* impl_fn =\n-        ir_emitter_context.llvm_module()->getFunction(impl_fn_name);\n-    TF_RET_CHECK(impl_fn);\n-\n-    TF_ASSIGN_OR_RETURN(\n-        llvm::Function * kernel,\n-        BuildKernelPrototype(\n-            ir_emitter_context.llvm_module(),\n-            ir_emitter_context.gpu_device_info(), impl_fn_name,\n-            GetSanitizedUniqueName(ir_emitter_context, suggested_kernel_name),\n-            kernel_arguments, launch_dimensions, &builder));\n+    TF_ASSIGN_OR_RETURN(llvm::Function * kernel,\n+                        RemoveUnusedTritonAbiArguments(\n+                            ir_emitter_context, sanitized_kernel_name,\n+                            launch_dimensions, kernel_arguments));\n \n     PopulateNvvmAnnotations(ir_emitter_context.llvm_module(), kernel,\n                             triton_wrapper_result);\n \n-    // Move function body into kernel prototype.\n-    llvm::Function* prototype_func = builder.GetInsertBlock()->getParent();\n-    prototype_func->splice(prototype_func->begin(), impl_fn);\n-    for (const auto& [impl_fn_arg, kernel_arg] :\n-         llvm::zip(impl_fn->args(), kernel->args())) {\n-      impl_fn_arg.replaceAllUsesWith(&kernel_arg);\n-    }\n-    // Triton's kernel ABI expects additional scratchpad global memory for\n-    // TMA and profiling information.\n-    // For now it is only used for on-device creation of TMA descriptors, which\n-    // we do not use yet, so we are just replacing this argument with a null\n-    // pointer.\n-    // TODO: b/381242007 - Allocate a proper buffer if we want to use\n-    // device-side TMA APIs.\n-    CHECK_EQ(impl_fn->arg_size(), kernel->arg_size() + 2);\n-    auto tma_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 2);\n-    tma_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n-        llvm::cast<llvm::PointerType>(tma_scratchpad_arg->getType())));\n-    auto profiling_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n-    profiling_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n-        llvm::cast<llvm::PointerType>(profiling_scratchpad_arg->getType())));\n \n     return {{kernel->getName().str(), launch_dimensions,\n              triton_wrapper_result.cluster_dim,"
        },
        {
            "sha": "cf8ec599609b0ffeff3a52fdfd79b45a4c0d9dcb",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 50,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=90730260174280654fb0102c8057e0954a3368e3",
            "patch": "@@ -1500,59 +1500,14 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n             call.num_warps *\n             ir_emitter_context_->gpu_device_info().threads_per_warp()));\n \n-    std::string sanitized_kernel_name =\n-        GetSanitizedUniqueName(*ir_emitter_context_, kernel_name);\n-\n     if (emit_kernels) {\n-      llvm::Function* impl_fn =\n-          ir_emitter_context_->llvm_module()->getFunction(kernel_name);\n-      TF_RET_CHECK(impl_fn);\n-      impl_fn->setName(\n-          GetSanitizedUniqueName(*ir_emitter_context_, kernel_name + \"_impl\"));\n-\n-      llvm::IRBuilder builder(ir_emitter_context_->llvm_module()->getContext());\n-\n-      TF_ASSIGN_OR_RETURN(llvm::Function * kernel,\n-                          BuildKernelPrototypeFromUniqueName(\n-                              ir_emitter_context_->llvm_module(),\n-                              ir_emitter_context_->gpu_device_info(),\n-                              impl_fn->getName().str(), sanitized_kernel_name,\n-                              kernel_arguments, launch_dimensions, &builder));\n-\n-      // Move function body into kernel prototype.\n-      llvm::Function* prototype_func = builder.GetInsertBlock()->getParent();\n-      prototype_func->splice(prototype_func->begin(), impl_fn);\n-      for (const auto& [impl_fn_arg, kernel_arg] :\n-           llvm::zip(impl_fn->args(), kernel->args())) {\n-        impl_fn_arg.replaceAllUsesWith(&kernel_arg);\n-      }\n-      // Triton's kernel ABI expects additional scratchpad global memory for TMA\n-      // and profiling information. For now it is only used for on-device\n-      // creation of TMA descriptors, which we do not use yet, so we are just\n-      // replacing this argument with a null pointer.\n-      // TODO: b/381242007 - Allocate a proper buffer if we want to use\n-      // device-side TMA APIs.\n-      CHECK_EQ(impl_fn->arg_size(), kernel->arg_size() + 2);\n-      auto tma_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 2);\n-      tma_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n-          llvm::cast<llvm::PointerType>(tma_scratchpad_arg->getType())));\n-      auto profiling_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n-      profiling_scratchpad_arg->replaceAllUsesWith(\n-          llvm::ConstantPointerNull::get(llvm::cast<llvm::PointerType>(\n-              profiling_scratchpad_arg->getType())));\n-\n-      impl_fn->eraseFromParent();\n-\n-      for (auto& arg : prototype_func->args()) {\n-        // Remove the alignment and aliasing attributes to avoid\n-        // recompiling the kernel for each alignment/aliasing\n-        // combination.\n-        arg.removeAttr(llvm::Attribute::Alignment);\n-        arg.removeAttr(llvm::Attribute::NoAlias);\n-      }\n+      TF_RETURN_IF_ERROR(\n+          RemoveUnusedTritonAbiArguments(*ir_emitter_context_, kernel_name,\n+                                         launch_dimensions, kernel_arguments)\n+              .status());\n     }\n \n-    return {{sanitized_kernel_name, launch_dimensions, result.cluster_dim,\n+    return {{kernel_name, launch_dimensions, result.cluster_dim,\n              result.shmem_bytes}};\n   };\n "
        },
        {
            "sha": "9da5272780ac8fa48a5c4b36b81618e9b1c42c17",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_triton_custom_call_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_triton_custom_call_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90730260174280654fb0102c8057e0954a3368e3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_triton_custom_call_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_triton_custom_call_test.cc?ref=90730260174280654fb0102c8057e0954a3368e3",
            "patch": "@@ -117,8 +117,7 @@ class GpuIrEmitterUnnestedTest : public GpuCodegenTest {\n   }\n };\n \n-TEST_F(GpuIrEmitterUnnestedTest,\n-       EmitTritonCustomCallWithCorrectLoweringAndWithoutNoaliasOrAlignment) {\n+TEST_F(GpuIrEmitterUnnestedTest, EmitTritonCustomCallWithCorrectLowering) {\n   if (!GetCudaComputeCapability().IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"Triton support is only enabled for Ampere GPUs and up.\";\n   }\n@@ -150,13 +149,9 @@ TEST_F(GpuIrEmitterUnnestedTest,\n   CompileAndVerifyIr(std::move(module),\n                      R\"(\n ; CHECK: @add_one\n-; CHECK-NOT: noalias align\n ; CHECK-SAME: dereferenceable(4) %arg0\n-; CHECK-NOT: noalias align\n ; CHECK-SAME: dereferenceable(4) %arg1\n-; CHECK-NOT: noalias align\n ; CHECK-SAME: dereferenceable(4) %arg2\n-; CHECK-NOT: noalias align\n ; CHECK-SAME: dereferenceable(4) %arg3\n ; CHECK-DAG:  addrspacecast ptr %arg0 to ptr addrspace(1)\n ; CHECK-DAG:  addrspacecast ptr %arg1 to ptr addrspace(1)"
        }
    ],
    "stats": {
        "total": 186,
        "additions": 80,
        "deletions": 106
    }
}