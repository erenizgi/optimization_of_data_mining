{
    "author": "pifon2a",
    "message": "[XLA:GPU] Move linking logic for emitters to ir_emitter_unnested.\n\nPiperOrigin-RevId: 836385464",
    "sha": "3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328",
    "files": [
        {
            "sha": "0f8fbeed07b2cac5c8871fafc399c8f65c7b49d7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328",
            "patch": "@@ -270,19 +270,19 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n                                      GetDefaultBufferAlignment(), &fusion));\n   auto launch_dims = launch_dimensions();\n   mlir::MLIRContext& mlir_context = *ir_emitter_context.mlir_context();\n+  std::unique_ptr<llvm::Module> module;\n   auto [status_or_entry, cached] =\n       ir_emitter_context.kernel_cache().GetWithStatus(\n           fusion.fused_instructions_computation(), args.args(),\n           /*discriminator=*/\"\",\n           [&]() -> absl::StatusOr<KernelReuseCache::Entry> {\n-            std::string kernel_name =\n-                ir_emitter_context.name_uniquer()->GetUniqueName(\n-                    llvm_ir::SanitizeFunctionName(std::string(fusion.name())));\n+            std::string kernel_name = GetSanitizedUniqueName(\n+                ir_emitter_context, std::string{fusion.name()});\n             if (ir_emitter_context.emit_kernels()) {\n               mlir_context.appendDialectRegistry(GetDialectRegistry());\n               mlir_context.loadAllAvailableDialects();\n               TF_ASSIGN_OR_RETURN(\n-                  auto module,\n+                  module,\n                   CreateLLVMModule(\n                       mlir_context,\n                       ir_emitter_context.llvm_module()->getContext(),\n@@ -300,12 +300,6 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n               TF_RETURN_IF_ERROR(AnnotateKernelLaunchDimensions(\n                   ir_emitter_context.gpu_device_info(), launch_dims,\n                   kernel_func, module.get()));\n-\n-              // Use override flag because libdevice functions can be present in\n-              // both.\n-              CHECK(!llvm::Linker::linkModules(\n-                  *target, std::move(module),\n-                  llvm::Linker::Flags::OverrideFromSrc));\n             } else {\n               VLOG(3) << \"Skipped kernel compilation.\";\n             }\n@@ -321,6 +315,7 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n   }\n \n   FusionEmissionResult result;\n+  result.module = std::move(module);\n   result.thunks.emplace_back(std::make_unique<KernelThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           &fusion, ir_emitter_context.GetNextThunkId()),"
        },
        {
            "sha": "df0b3257b09a2d3af696e00493db796ef8f2656d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328",
            "patch": "@@ -45,6 +45,7 @@ namespace xla {\n namespace gpu {\n \n struct FusionEmissionResult {\n+  std::unique_ptr<llvm::Module> module;\n   std::vector<std::unique_ptr<Thunk>> thunks;\n };\n "
        },
        {
            "sha": "82be49dadbd31faee8951481471632763b4540ee",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=3ed1e7a0e28efd0e77fd9fabca604a66d3bfe328",
            "patch": "@@ -1577,6 +1577,13 @@ absl::Status IrEmitterUnnested::EmitFusion(const HloFusionInstruction* instr) {\n       ir_emitter_context_->mlir_context());\n   TF_ASSIGN_OR_RETURN(auto result, emitter->Emit(*ir_emitter_context_, *instr));\n \n+  // Use override flag because libdevice functions can be present in both.\n+  if (result.module) {\n+    TF_RET_CHECK(!llvm::Linker::linkModules(\n+        *ir_emitter_context_->llvm_module(), std::move(result.module),\n+        llvm::Linker::Flags::OverrideFromSrc));\n+  }\n+\n   const ExecutionStreamAssignment& stream_assignment =\n       ir_emitter_context_->execution_stream_assignment();\n   for (std::unique_ptr<Thunk>& thunk : result.thunks) {"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 13,
        "deletions": 10
    }
}