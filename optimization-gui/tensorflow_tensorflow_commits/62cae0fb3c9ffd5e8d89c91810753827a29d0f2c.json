{
    "author": "pifon2a",
    "message": "[XLA:GPU] Make name_uniquer private to IrEmitterContext.\n\nIt looks like we have at least 2 reimplementation of GetUniqueSanitizedName.\n\nPiperOrigin-RevId: 838138583",
    "sha": "62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
    "files": [
        {
            "sha": "8b81a4352e9c13259b981e6c625430fa6e9ea8c6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
            "patch": "@@ -276,8 +276,8 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n           fusion.fused_instructions_computation(), args.args(),\n           /*discriminator=*/\"\",\n           [&]() -> absl::StatusOr<KernelReuseCache::Entry> {\n-            std::string kernel_name = GetSanitizedUniqueName(\n-                ir_emitter_context, std::string{fusion.name()});\n+            std::string kernel_name = ir_emitter_context.GetSanitizedUniqueName(\n+                std::string(fusion.name()));\n             if (ir_emitter_context.emit_kernels()) {\n               mlir_context.appendDialectRegistry(GetDialectRegistry());\n               mlir_context.loadAllAvailableDialects();"
        },
        {
            "sha": "8077ba74d16124769e8c67e99e92b47ec285f290",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
            "patch": "@@ -109,12 +109,6 @@ IndexingMap KernelFusionInterface::GetDefaultThreadIdIndexingMap(\n                                                  mlir_context);\n }\n \n-std::string GetSanitizedUniqueName(IrEmitterContext& ir_emitter_context,\n-                                   const std::string& suggested_name) {\n-  return ir_emitter_context.name_uniquer()->GetUniqueName(\n-      llvm_ir::SanitizeFunctionName(suggested_name));\n-}\n-\n absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n     llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n     const std::string& impl_fn_name, const std::string& unique_kernel_name,\n@@ -227,8 +221,8 @@ absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n     const emitters::KernelArguments& kernel_arguments) {\n   llvm::Function* impl_fn = llvm_module->getFunction(sanitized_kernel_name);\n   TF_RET_CHECK(impl_fn);\n-  impl_fn->setName(ir_emitter_context.name_uniquer()->GetUniqueName(\n-      sanitized_kernel_name + \"_impl\"));\n+  impl_fn->setName(ir_emitter_context.GetSanitizedUniqueName(\n+      absl::StrCat(sanitized_kernel_name, \"_impl\")));\n \n   llvm::IRBuilder builder(llvm_module->getContext());\n "
        },
        {
            "sha": "60d38cb3372e446b1178c75bc0095a9c16915be5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
            "patch": "@@ -112,11 +112,6 @@ absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n     LaunchDimensions& launch_dimensions,\n     const emitters::KernelArguments& arguments);\n \n-// Compute the kernel name. The opcode string may contain \"-\" which cannot be\n-// in a PTX function name, so sanitize the name before uniquifying it.\n-std::string GetSanitizedUniqueName(IrEmitterContext& ir_emitter_context,\n-                                   const std::string& suggested_name);\n-\n absl::Status AnnotateKernelLaunchDimensions(\n     const se::DeviceDescription& device_info,\n     const LaunchDimensions& launch_dims, llvm::Function* kernel,"
        },
        {
            "sha": "c67d2133f286605f59ed66039bd92aad88e23472",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
            "patch": "@@ -137,7 +137,7 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     VLOG(3) << \"Generating: \" << suggested_kernel_name;\n \n     const std::string sanitized_kernel_name =\n-        GetSanitizedUniqueName(ir_emitter_context, suggested_kernel_name);\n+        ir_emitter_context.GetSanitizedUniqueName(suggested_kernel_name);\n \n     TF_ASSIGN_OR_RETURN(\n         TritonWrapperResult triton_wrapper_result,"
        },
        {
            "sha": "b3ba29f9e1cd45f5f80aa3064651a4d9aecd3440",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
            "patch": "@@ -233,8 +233,7 @@ absl::Status LoadCache(IrEmitterContext& ir_emitter_context,\n     // Register all cached kernel names with the name uniquer to avoid\n     // naming conflicts.\n     for (const auto& [name, _] : proto.entries()) {\n-      TF_RET_CHECK(ir_emitter_context.name_uniquer()->GetUniqueName(name) ==\n-                   name)\n+      TF_RET_CHECK(ir_emitter_context.GetSanitizedUniqueName(name) == name)\n           << \"Failed registering \" << name << \"in NameUniquer.\";\n     }\n     TF_RETURN_IF_ERROR(ir_emitter_context.kernel_cache().Load(proto));"
        },
        {
            "sha": "c72d3d19d57823f4fcfb336999784971d8549f08",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
            "patch": "@@ -110,8 +110,6 @@ class IrEmitterContext {\n     return (llvm_module_constants_ == nullptr) ? llvm_module_\n                                                : llvm_module_constants_;\n   }\n-  NameUniquer* name_uniquer() { return &name_uniquer_; }\n-\n   absl::StatusOr<InlinedModule*> get_inlined_module() {\n     if (inlined_module_ == nullptr) {\n       TF_ASSIGN_OR_RETURN(InlinedModule inlined_module,\n@@ -151,6 +149,8 @@ class IrEmitterContext {\n \n   ThunkId GetNextThunkId() { return thunk_id_generator_.GetNextThunkId(); }\n \n+  // Compute the kernel name. The opcode string may contain \"-\" which cannot be\n+  // in a PTX function name, so sanitize the name before uniquifying it.\n   std::string GetSanitizedUniqueName(const std::string& suggested_name) {\n     return name_uniquer_.GetUniqueName(\n         llvm_ir::SanitizeFunctionName(suggested_name));"
        },
        {
            "sha": "57d7c8075b1f0bf395e0c5e68da039d0f2c72255",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62cae0fb3c9ffd5e8d89c91810753827a29d0f2c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=62cae0fb3c9ffd5e8d89c91810753827a29d0f2c",
            "patch": "@@ -1124,7 +1124,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n     LoadMlirDialectsForTriton(mlir_context);\n     auto call =\n         TritonCall::Parse(instr->raw_backend_config_string(), &mlir_context);\n-    auto kernel_name = GetSanitizedUniqueName(*ir_emitter_context_, call.name);\n+    auto kernel_name = ir_emitter_context_->GetSanitizedUniqueName(call.name);\n     VLOG(3) << \"Generating: \" << kernel_name;\n     auto local_module = ir_emitter_context_->CreateLocalLLVMModule(kernel_name);\n "
        }
    ],
    "stats": {
        "total": 30,
        "additions": 9,
        "deletions": 21
    }
}