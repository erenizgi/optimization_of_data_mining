{
    "author": "mani-ananth",
    "message": "Adding HBM derate curve for H100 in dot fusion cost model\n\nPiperOrigin-RevId: 804625995",
    "sha": "ec291e86f3c47bc8b510cc897053bc235554d657",
    "files": [
        {
            "sha": "533bd25283a6b5eec737404182f68c2253058285",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_dot_fusion_cost_model.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 14,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec291e86f3c47bc8b510cc897053bc235554d657/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec291e86f3c47bc8b510cc897053bc235554d657/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model.cc?ref=ec291e86f3c47bc8b510cc897053bc235554d657",
            "patch": "@@ -16,7 +16,9 @@ limitations under the License.\n #include \"xla/service/gpu/model/gpu_dot_fusion_cost_model.h\"\n \n #include <algorithm>\n+#include <array>\n #include <cstdint>\n+#include <utility>\n #include <vector>\n \n #include \"absl/container/inlined_vector.h\"\n@@ -224,15 +226,6 @@ absl::StatusOr<absl::Duration> CalculateL2Time(\n   // TODO(maniananth): L2 bandwidth has been hardcoded for H100 based on\n   // microbenchmarking L2 bandwidth within a partition, but we should add this\n   // to the device info and extend for more GPUs.\n-  // TODO(maniananth): Enforcing this check will cause unit tests written for\n-  // RTX A6000 device descriptions to fail. We should enable this check once we\n-  // have the L2 bandwidth for RTX A6000 or move unit tests to use H100\n-  // device description.\n-  // if (device_info.cuda_compute_capability() !=\n-  //     se::CudaComputeCapability(9, 0)) {\n-  //   return absl::InvalidArgumentError(\n-  //       \"L2 time calculation is only supported for H100 GPUs.\");\n-  // }\n \n   GpuDotFusionCostModel::DotProblemDimensions dims(*dot);\n   int64_t tile_m = tile_shape[0], tile_n = tile_shape[1];\n@@ -244,13 +237,56 @@ absl::StatusOr<absl::Duration> CalculateL2Time(\n                        device_l2_bandwidth);\n }\n \n+// Returns the effective HBM bandwidth in bytes per second for a given dma_size.\n+// dma_size is the total amount of data transferred to/from HBM in bytes.\n+float GetEffectiveHbmBandwidth(const int64_t dma_size,\n+                               const se::DeviceDescription& device_info) {\n+  using HbmBandwidthLookupEntry =\n+      std::pair</*dma_size*/ int64_t, /*measured bandwidth*/ float>;\n+  std::array<HbmBandwidthLookupEntry, 18> hbm_bandwidth_GBps_lookup_h100 = {\n+      {{8192, 1.42f},\n+       {16384, 3.03f},\n+       {32768, 6.02f},\n+       {65536, 11.77f},\n+       {131072, 23.68f},\n+       {262144, 47.35f},\n+       {524288, 92.56f},\n+       {1048576, 179.06f},\n+       {2097152, 346.75f},\n+       {4194304, 639.38f},\n+       {8388608, 1069.98f},\n+       {16777216, 1583.95f},\n+       {33554432, 1974.72f},\n+       {67108864, 2343.19f},\n+       {134217728, 2632.96f},\n+       {268435456, 2766.69f},\n+       {536870912, 2968.89f},\n+       {1073741824, 3126.0f}}};\n+\n+  if (dma_size <= hbm_bandwidth_GBps_lookup_h100.front().first) {\n+    return hbm_bandwidth_GBps_lookup_h100.front().second * (1 << 30);\n+  }\n+  if (dma_size >= hbm_bandwidth_GBps_lookup_h100.back().first) {\n+    return hbm_bandwidth_GBps_lookup_h100.back().second * (1 << 30);\n+  }\n+\n+  auto it2 = std::lower_bound(hbm_bandwidth_GBps_lookup_h100.begin(),\n+                              hbm_bandwidth_GBps_lookup_h100.end(), dma_size,\n+                              [](const std::pair<int64_t, float>& a,\n+                                 const int64_t b) { return a.first < b; });\n+  auto it1 = it2 - 1;\n+\n+  // Linear interpolation between the two entries in the lookup table. std::lerp\n+  // is not used as it is only available since C++20.\n+  auto a = it1->second;\n+  auto b = it2->second;\n+  auto t =\n+      (dma_size - it1->first) / static_cast<float>(it2->first - it1->first);\n+  return (a + t * (b - a)) * (1 << 30);\n+}\n+\n absl::Duration CalculateHbmTime(const HloDotInstruction* dot,\n                                 const se::DeviceDescription& device_info) {\n-  // TODO(maniananth): Implement HBM derate lookup using profiled tables.\n-  float hbm_bandwidth_utilization_rate = 0.8;\n-  float dram_bandwidth =\n-      device_info.memory_bandwidth() * hbm_bandwidth_utilization_rate;\n-\n   GpuDotFusionCostModel::DotProblemDimensions dims(*dot);\n   PrimitiveType lhs_element_type = dot->operand(0)->shape().element_type();\n   PrimitiveType rhs_element_type = dot->operand(1)->shape().element_type();\n@@ -271,6 +307,11 @@ absl::Duration CalculateHbmTime(const HloDotInstruction* dot,\n   int64_t main_loop_bytes = lhs_tile_bytes + rhs_tile_bytes;\n   int64_t epilogue_bytes = output_tile_bytes;\n \n+  // Calculate the effective HBM bandwidth for the input and output bytes using\n+  // the derate lookup table.\n+  float dram_bandwidth =\n+      GetEffectiveHbmBandwidth(main_loop_bytes + epilogue_bytes, device_info);\n+\n   // Calculate the HBM time using the effective bandwidth for each transfer\n   // size. In the current implementation, we are assuming that the main loop and\n   // epilogue loop have the same effective DRAM bandwidth. This could change in"
        },
        {
            "sha": "e833504cd2c5a15e224b4a09fb9b98b83d9dc46e",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_dot_fusion_cost_model.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec291e86f3c47bc8b510cc897053bc235554d657/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec291e86f3c47bc8b510cc897053bc235554d657/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model.h?ref=ec291e86f3c47bc8b510cc897053bc235554d657",
            "patch": "@@ -59,6 +59,11 @@ absl::StatusOr<BlockLevelParameters> FindBestBlockLevelParameters(\n \n namespace detail {\n \n+// Returns the effective HBM bandwidth in bytes per second for a given dma_size.\n+// dma_size is the total amount of data transferred to/from HBM in bytes.\n+float GetEffectiveHbmBandwidth(int64_t dma_size,\n+                               const se::DeviceDescription& device_info);\n+\n // Calculates the HBM time for a GPU DOT operation. Current implementation\n // uses a flat derate on top of the spec bandwidth. A HBM bandwidth model based\n // derate lookup from profiled data will be added in the future."
        },
        {
            "sha": "34497b157be777838a1c7fa4d70a8001bc8badfd",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_dot_fusion_cost_model_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 13,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec291e86f3c47bc8b510cc897053bc235554d657/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec291e86f3c47bc8b510cc897053bc235554d657/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_dot_fusion_cost_model_test.cc?ref=ec291e86f3c47bc8b510cc897053bc235554d657",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/gpu/model/gpu_dot_fusion_cost_model.h\"\n \n+#include <cstdint>\n #include <memory>\n \n #include <gtest/gtest.h>\n@@ -35,7 +36,6 @@ namespace {\n \n class GpuDotFusionCostModelTest : public HloHardwareIndependentTestBase {\n  protected:\n-  se::DeviceDescription dda6000_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   se::DeviceDescription ddh100_{TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n };\n \n@@ -50,24 +50,13 @@ lhs_contracting_dims={1}, rhs_contracting_dims={0}, algorithm=dot_bf16_bf16_bf16\n })\"));\n \n   BlockLevelParameters block_params;\n-  block_params.output_tile_sizes = {{16, 16}};\n+  block_params.output_tile_sizes = {{64, 64}};\n   block_params.num_warps = 4;\n   block_params.num_ctas = 1;\n   block_params.num_stages = 1;\n   auto* dot =\n       Cast<HloDotInstruction>(module->entry_computation()->root_instruction());\n   ASSERT_IS_OK(GpuDotFusionCostModel::IsSupported(dot));\n-  absl::Duration runtime_a6000 =\n-      GpuDotFusionCostModel::EstimateRunTimeForDotOpWithBlockParameters(\n-          dot, block_params, dda6000_)\n-          .value();\n-  absl::Duration expected_runtime_compute_bound_a6000 =\n-      detail::CalculateComputeTimeWithTileAndWaveQuantization(\n-          dot, block_params.output_tile_sizes[0], dda6000_)\n-          .value();\n-  ASSERT_EQ(runtime_a6000, expected_runtime_compute_bound_a6000);\n-\n-  block_params.output_tile_sizes = {{64, 64}};\n   absl::Duration runtime_h100 =\n       GpuDotFusionCostModel::EstimateRunTimeForDotOpWithBlockParameters(\n           dot, block_params, ddh100_)\n@@ -79,6 +68,36 @@ lhs_contracting_dims={1}, rhs_contracting_dims={0}, algorithm=dot_bf16_bf16_bf16\n   ASSERT_EQ(runtime_h100, expected_runtime_compute_bound_h100);\n }\n \n+TEST_F(GpuDotFusionCostModelTest, GpuDotMemoryBoundBf16) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+ENTRY e {\n+p0 = bf16[4,4096] parameter(0)\n+p1 = bf16[4096,4096] parameter(1)\n+ROOT r = bf16[4,4096] dot(p0, p1),\n+lhs_contracting_dims={1}, rhs_contracting_dims={0}, algorithm=dot_bf16_bf16_bf16\n+})\"));\n+\n+  BlockLevelParameters block_params;\n+  block_params.output_tile_sizes = {{4, 32}};\n+  block_params.num_warps = 4;\n+  block_params.num_ctas = 1;\n+  block_params.num_stages = 1;\n+  auto* dot =\n+      Cast<HloDotInstruction>(module->entry_computation()->root_instruction());\n+  ASSERT_IS_OK(GpuDotFusionCostModel::IsSupported(dot));\n+  absl::Duration runtime_h100 =\n+      GpuDotFusionCostModel::EstimateRunTimeForDotOpWithBlockParameters(\n+          dot, block_params, ddh100_)\n+          .value();\n+  int64_t approx_total_bytes = 2 /*BF16*/ * (4096 + 4 * 2) * 4096;\n+  float approx_hbm_bandwidth =\n+      detail::GetEffectiveHbmBandwidth(approx_total_bytes, ddh100_);\n+  absl::Duration approx_hbm_time =\n+      absl::Seconds(1.0f * approx_total_bytes / approx_hbm_bandwidth);\n+  ASSERT_EQ(runtime_h100, approx_hbm_time);\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 119,
        "additions": 92,
        "deletions": 27
    }
}