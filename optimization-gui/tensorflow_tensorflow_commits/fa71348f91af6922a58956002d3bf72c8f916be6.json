{
    "author": "thomasjoerg",
    "message": "[XLA:GPU] Add a comment about numerics to the cuDNN GEMM FLAG.\n\ncuDNN GEMMs ignore HLO dot precision algorithms and always allow tf32 matmuls.\n\nPiperOrigin-RevId: 831353388",
    "sha": "fa71348f91af6922a58956002d3bf72c8f916be6",
    "files": [
        {
            "sha": "61dd29e84c2eb63d8000ba9761fe2341a3a0752a",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fa71348f91af6922a58956002d3bf72c8f916be6/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fa71348f91af6922a58956002d3bf72c8f916be6/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=fa71348f91af6922a58956002d3bf72c8f916be6",
            "patch": "@@ -462,6 +462,11 @@ message DebugOptions {\n   optional string xla_gpu_cuda_data_dir = 61;\n \n   // Let GEMM fusion autotuning probe cuDNN as a backend.\n+  //\n+  // CAUTION:\n+  // * HLO dot precision (e.g. `algorithm=dot_bf16_bf16_f32`) is ignored.\n+  // * `tf32` matmuls are enabled unconditionally.\n+  //\n   // Current levels:\n   // 0: Disabled.\n   // 1: Enabled on Blackwell+ GPUs."
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}