{
    "author": "ezhulenev",
    "message": "[xla:cpu] Copy service/cpu/runtime_fp16 to cpu/codegen/builtin_fp16\n\nPiperOrigin-RevId: 828137921",
    "sha": "4a9fc06abbd4b35b0f8511f3c2108d5c904815be",
    "files": [
        {
            "sha": "ecc7168166c90fa92d4505dd32252a987a58115e",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=4a9fc06abbd4b35b0f8511f3c2108d5c904815be",
            "patch": "@@ -23,6 +23,14 @@ package_group(\n     ],\n )\n \n+cc_library(\n+    name = \"builtin_fp16\",\n+    srcs = [\"builtin_fp16.cc\"],\n+    hdrs = [\"builtin_fp16.h\"],\n+    visibility = internal_visibility([\":friends\"]),\n+    deps = [\"@com_google_absl//absl/base:core_headers\"],\n+)\n+\n cc_library(\n     name = \"builtin_pow\",\n     srcs = [\"builtin_pow.cc\"],"
        },
        {
            "sha": "4208d838c0a4d434ce2bbcf2ab2719cf4666dcac",
            "filename": "third_party/xla/xla/backends/cpu/codegen/builtin_fp16.cc",
            "status": "added",
            "additions": 146,
            "deletions": 0,
            "changes": 146,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fbuiltin_fp16.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fbuiltin_fp16.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fbuiltin_fp16.cc?ref=4a9fc06abbd4b35b0f8511f3c2108d5c904815be",
            "patch": "@@ -0,0 +1,146 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/codegen/builtin_fp16.h\"\n+\n+#include <cstdint>\n+#include <cstring>\n+\n+#include \"absl/base/attributes.h\"\n+\n+namespace {\n+\n+// Helper class that lets us access the underlying bit representation\n+// of a float without breaking C++ strict aliasing.\n+class AliasedFloatInt {\n+ public:\n+  static_assert(sizeof(float) == sizeof(uint32_t), \"\");\n+\n+  static AliasedFloatInt FromFloat(float f) {\n+    AliasedFloatInt value;\n+    value.set_float(f);\n+    return value;\n+  }\n+\n+  static AliasedFloatInt FromUInt(uint32_t u) {\n+    AliasedFloatInt value;\n+    value.set_uint(u);\n+    return value;\n+  }\n+\n+  void set_float(float f) { memcpy(&value_, &f, sizeof(f)); }\n+  float as_float() const {\n+    float f;\n+    memcpy(&f, &value_, sizeof(f));\n+    return f;\n+  }\n+\n+  void set_uint(uint32_t u) { value_ = u; }\n+  uint32_t as_uint() const { return value_; }\n+\n+ private:\n+  uint32_t value_;\n+};\n+}  // namespace\n+\n+// __gnu_f2h_ieee and __gnu_h2f_ieee are marked as weak symbols so if XLA is\n+// built with compiler-rt (that also defines these symbols) we don't get a\n+// duplicate definition linker error.  Making these symbols weak also ensures\n+// that the compiler-rt definitions \"win\", but that isn't essential.\n+\n+// Algorithm copied from Eigen.\n+XlaF16ABIType ABSL_ATTRIBUTE_WEAK __gnu_f2h_ieee(float float_value) {\n+  AliasedFloatInt f = AliasedFloatInt::FromFloat(float_value);\n+\n+  const AliasedFloatInt f32infty = AliasedFloatInt::FromUInt(255 << 23);\n+  const AliasedFloatInt f16max = AliasedFloatInt::FromUInt((127 + 16) << 23);\n+  const AliasedFloatInt denorm_magic =\n+      AliasedFloatInt::FromUInt(((127 - 15) + (23 - 10) + 1) << 23);\n+  unsigned int sign_mask = 0x80000000u;\n+  uint32_t o = static_cast<uint16_t>(0x0u);\n+\n+  unsigned int sign = f.as_uint() & sign_mask;\n+  f.set_uint(f.as_uint() ^ sign);\n+\n+  // NOTE all the integer compares in this function can be safely\n+  // compiled into signed compares since all operands are below\n+  // 0x80000000. Important if you want fast straight SSE2 code\n+  // (since there's no unsigned PCMPGTD).\n+\n+  if (f.as_uint() >=\n+      f16max.as_uint()) {  // result is Inf or NaN (all exponent bits set)\n+    o = (f.as_uint() > f32infty.as_uint()) ? 0x7e00\n+                                           : 0x7c00;  // NaN->qNaN and Inf->Inf\n+  } else {                            // (De)normalized number or zero\n+    if (f.as_uint() < (113 << 23)) {  // resulting FP16 is subnormal or zero\n+      // use a magic value to align our 10 mantissa bits at the bottom of\n+      // the float. as long as FP addition is round-to-nearest-even this\n+      // just works.\n+      f.set_float(f.as_float() + denorm_magic.as_float());\n+\n+      // and one integer subtract of the bias later, we have our final float!\n+      o = static_cast<uint16_t>(f.as_uint() - denorm_magic.as_uint());\n+    } else {\n+      unsigned int mant_odd =\n+          (f.as_uint() >> 13) & 1;  // resulting mantissa is odd\n+\n+      // update exponent, rounding bias part 1\n+      f.set_uint(f.as_uint() + (static_cast<unsigned int>(15 - 127) << 23) +\n+                 0xfff);\n+      // rounding bias part 2\n+      f.set_uint(f.as_uint() + mant_odd);\n+      // take the bits!\n+      o = static_cast<uint16_t>(f.as_uint() >> 13);\n+    }\n+  }\n+\n+  o |= static_cast<uint16_t>(sign >> 16);\n+  // The output can be a float type, bitcast it from uint16_t.\n+  auto ho = static_cast<uint16_t>(o);\n+  XlaF16ABIType ret = 0;\n+  std::memcpy(&ret, &ho, sizeof(ho));\n+  return ret;\n+}\n+\n+// Algorithm copied from Eigen.\n+float ABSL_ATTRIBUTE_WEAK __gnu_h2f_ieee(XlaF16ABIType hf) {\n+  const AliasedFloatInt magic = AliasedFloatInt::FromUInt(113 << 23);\n+  const unsigned int shifted_exp = 0x7c00 << 13;  // exponent mask after shift\n+  AliasedFloatInt o;\n+\n+  // The input can be a float type, bitcast it to uint16_t.\n+  uint16_t h;\n+  std::memcpy(&h, &hf, sizeof(h));\n+  o.set_uint((h & 0x7fff) << 13);                // exponent/mantissa bits\n+  unsigned int exp = shifted_exp & o.as_uint();  // just the exponent\n+  o.set_uint(o.as_uint() + ((127 - 15) << 23));  // exponent adjust\n+\n+  // handle exponent special cases\n+  if (exp == shifted_exp) {                        // Inf/NaN?\n+    o.set_uint(o.as_uint() + ((128 - 16) << 23));  // extra exp adjust\n+  } else if (exp == 0) {                           // Zero/Denormal?\n+    o.set_uint(o.as_uint() + (1 << 23));           // extra exp adjust\n+    o.set_float(o.as_float() - magic.as_float());  // renormalize\n+  }\n+\n+  o.set_uint(o.as_uint() | (h & 0x8000) << 16);  // sign bit\n+  return o.as_float();\n+}\n+\n+XlaF16ABIType ABSL_ATTRIBUTE_WEAK __truncdfhf2(double d) {\n+  // This does a double rounding step, but it's precise enough for our use\n+  // cases.\n+  return __gnu_f2h_ieee(static_cast<float>(d));\n+}"
        },
        {
            "sha": "cbe75ec2e18684cdc040fb7db29bd4e6b515deba",
            "filename": "third_party/xla/xla/backends/cpu/codegen/builtin_fp16.h",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fbuiltin_fp16.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fbuiltin_fp16.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fbuiltin_fp16.h?ref=4a9fc06abbd4b35b0f8511f3c2108d5c904815be",
            "patch": "@@ -0,0 +1,44 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_CODEGEN_BUILTIN_FP16_H_\n+#define XLA_BACKENDS_CPU_CODEGEN_BUILTIN_FP16_H_\n+\n+// _Float16 always gets us the correct ABI type, so use that if available.\n+// AArch64 GCC defines __FLT16_MANT_DIG__ even when _Float16 is not available.\n+#if defined(__FLT16_MANT_DIG__) && \\\n+    (defined(__clang__) || !(defined(__GNUC__) && defined(__aarch64__)))\n+using XlaF16ABIType = _Float16;\n+#elif defined(__x86_64__)\n+// Older versions of Clang don't have _Float16. Since both float and _Float16\n+// are passed in the same register we can use the wider type and careful casting\n+// to conform to x86_64 psABI. This only works with the assumption that we're\n+// dealing with little-endian values passed in wider registers.\n+using XlaF16ABIType = float;\n+#else\n+// Default to uint16_t if we have nothing else.\n+using XlaF16ABIType = uint16_t;\n+#endif\n+\n+// Converts an F32 value to a F16.\n+extern \"C\" XlaF16ABIType __gnu_f2h_ieee(float);\n+\n+// Converts an F16 value to a F32.\n+extern \"C\" float __gnu_h2f_ieee(XlaF16ABIType);\n+\n+// Converts an F64 value to a F16.\n+extern \"C\" XlaF16ABIType __truncdfhf2(double);\n+\n+#endif  // XLA_BACKENDS_CPU_CODEGEN_BUILTIN_FP16_H_"
        },
        {
            "sha": "54ac77290052125475d2087e5e1b13d615c1a430",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=4a9fc06abbd4b35b0f8511f3c2108d5c904815be",
            "patch": "@@ -571,7 +571,7 @@ cc_library(\n     hdrs = [\"runtime_symbol_generator.h\"],\n     copts = tsl_copts(),\n     deps = [\n-        \":runtime_fp16\",\n+        \"//xla/backends/cpu/codegen:builtin_fp16\",\n         \"//xla/backends/cpu/codegen:builtin_pow\",\n         \"@com_google_absl//absl/base:no_destructor\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "dd171da688a70ee69d50028e49ed268d31953100",
            "filename": "third_party/xla/xla/service/cpu/runtime_symbol_generator.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a9fc06abbd4b35b0f8511f3c2108d5c904815be/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc?ref=4a9fc06abbd4b35b0f8511f3c2108d5c904815be",
            "patch": "@@ -37,8 +37,8 @@ limitations under the License.\n #include \"llvm/ExecutionEngine/Orc/Shared/ExecutorSymbolDef.h\"\n #include \"llvm/IR/DataLayout.h\"\n #include \"llvm/Support/Error.h\"\n+#include \"xla/backends/cpu/codegen/builtin_fp16.h\"\n #include \"xla/backends/cpu/codegen/builtin_pow.h\"\n-#include \"xla/service/cpu/runtime_fp16.h\"\n \n namespace xla::cpu {\n "
        }
    ],
    "stats": {
        "total": 202,
        "additions": 200,
        "deletions": 2
    }
}