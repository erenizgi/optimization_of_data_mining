{
    "author": "ZixuanJiang",
    "message": "Add an overload for `SpmdPartitioner::SetPartitionedHlo` to avoid unnecessary lambda functions.\n\nPiperOrigin-RevId: 825819367",
    "sha": "ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38",
    "files": [
        {
            "sha": "59ef91f222e5fed979668afeb6964f943fa70377",
            "filename": "third_party/xla/xla/service/spmd/custom_call_handler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc?ref=ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38",
            "patch": "@@ -341,7 +341,7 @@ absl::Status SpmdPartitioningVisitor::HandleCustomCallSPMDInternal_RotateRight(\n   };\n   HloInstruction* rotated0 = rotate_with_padding(amount);\n   if (right_padding == 0) {\n-    SetPartitionedHlo(hlo, [&] { return rotated0; });\n+    SetPartitionedHlo(hlo, rotated0);\n     return absl::OkStatus();\n   }\n \n@@ -374,10 +374,9 @@ absl::Status SpmdPartitioningVisitor::HandleCustomCallSPMDInternal_RotateRight(\n   HloInstruction* pred = b_.AddInstruction(HloInstruction::CreateCompare(\n       ShapeUtil::ChangeElementType(iota->shape(), PRED), iota,\n       selection_boundary, Comparison::Direction::kLt));\n-  SetPartitionedHlo(hlo, [&] {\n-    return b_.AddInstruction(HloInstruction::CreateTernary(\n-        rotated0->shape(), HloOpcode::kSelect, pred, rotated1, rotated0));\n-  });\n+  SetPartitionedHlo(hlo, b_.AddInstruction(HloInstruction::CreateTernary(\n+                             rotated0->shape(), HloOpcode::kSelect, pred,\n+                             rotated1, rotated0)));\n   return absl::OkStatus();\n }\n \n@@ -405,7 +404,7 @@ absl::Status SpmdPartitioningVisitor::HandleCustomCall(HloInstruction* hlo) {\n         input->shape(), MakePartitionedShape(hlo->shape(), hlo->sharding())));\n     auto copy = b_.AddInstruction(\n         HloInstruction::CreateUnary(input->shape(), HloOpcode::kCopy, input));\n-    SetPartitionedHlo(hlo, [&] { return copy; });\n+    SetPartitionedHlo(hlo, copy);\n     return absl::OkStatus();\n   }\n   if (hlo->custom_call_target() == \"SPMDShardToFullShape\") {\n@@ -416,7 +415,7 @@ absl::Status SpmdPartitioningVisitor::HandleCustomCall(HloInstruction* hlo) {\n         HloInstruction::CreateUnary(input->shape(), HloOpcode::kCopy, input));\n     CHECK(ShapeUtil::Compatible(\n         copy->shape(), MakePartitionedShape(hlo->shape(), hlo->sharding())));\n-    SetPartitionedHlo(hlo, [&] { return copy; });\n+    SetPartitionedHlo(hlo, copy);\n     return absl::OkStatus();\n   }\n "
        },
        {
            "sha": "c737330b49fea5d4e6f1a9d35e1872acf25669f5",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38",
            "patch": "@@ -4349,7 +4349,7 @@ absl::Status SpmdPartitioningVisitor::HandleDotHelper(\n                      num_partitions_, create_sharded_dot, conv_window, module_,\n                      hlo, options_, &b_, &windowed_dot_general_loops_, this));\n   }\n-  SetPartitionedHlo(hlo, [partitioned_dot] { return partitioned_dot; });\n+  SetPartitionedHlo(hlo, partitioned_dot);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "715a634193a606defb3845f9130340e65313c6ec",
            "filename": "third_party/xla/xla/service/spmd/fft_handler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc?ref=ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38",
            "patch": "@@ -426,10 +426,7 @@ absl::Status SpmdPartitioningVisitor::HandleFft(HloInstruction* hlo) {\n       partitioned_input.state().next_channel_id, module_,\n       partitioned_input.state().b);\n \n-  result->set_sharding(hlo->sharding());\n-  auto partitioned_fft =\n-      PartitionedHlo(result, hlo->shape(), partitioned_input.state());\n-  SetPartitionedHlo(hlo, std::move(partitioned_fft));\n+  SetPartitionedHlo(hlo, result);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "80279bb773d1de9ba19bfbf1fb8b7dd198ed4214",
            "filename": "third_party/xla/xla/service/spmd/gather_scatter_handler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fgather_scatter_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fgather_scatter_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fgather_scatter_handler.cc?ref=ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38",
            "patch": "@@ -1009,8 +1009,7 @@ absl::Status SpmdPartitioningVisitor::HandleGather(HloInstruction* hlo) {\n       PartitionGather(gather, operand, indices, gather->shape(),\n                       gather->sharding(), absl::MakeConstSpan(batch_dims),\n                       gather->gather_slice_sizes(), this));\n-  SetPartitionedHlo(gather, PartitionedHlo(pgather, gather->shape(),\n-                                           MakePartitioningState()));\n+  SetPartitionedHlo(gather, pgather);\n   return absl::OkStatus();\n }\n \n@@ -1904,8 +1903,7 @@ absl::Status SpmdPartitioningVisitor::HandleScatter(HloInstruction* hlo) {\n   if (!pscatter) {\n     return DefaultAction(hlo);\n   }\n-  SetPartitionedHlo(scatter, PartitionedHlo(pscatter, scatter->shape(),\n-                                            MakePartitioningState()));\n+  SetPartitionedHlo(scatter, pscatter);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "d8f83e994d554761569153220fc9f59330cbf5ef",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 70,
            "deletions": 93,
            "changes": 163,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38",
            "patch": "@@ -2782,10 +2782,10 @@ absl::Status SpmdPartitioningVisitor::HandleElementwise(HloInstruction* hlo) {\n     new_operands.push_back(\n         GetPartitionedHlo(operand).Reshard(hlo->sharding()).hlo());\n   }\n-  SetPartitionedHlo(hlo, [&] {\n-    return b_.AddInstruction(hlo->CloneWithNewOperands(\n-        MakePartitionedShape(hlo->shape(), hlo->sharding()), new_operands));\n-  });\n+  SetPartitionedHlo(\n+      hlo,\n+      b_.AddInstruction(hlo->CloneWithNewOperands(\n+          MakePartitionedShape(hlo->shape(), hlo->sharding()), new_operands)));\n   return absl::OkStatus();\n }\n \n@@ -2908,7 +2908,7 @@ absl::Status SpmdPartitioningVisitor::HandleSlice(HloInstruction* hlo) {\n     return DefaultAction(hlo);\n   }\n \n-  SetPartitionedHlo(hlo, [&] { return final_operand; });\n+  SetPartitionedHlo(hlo, final_operand);\n   return absl::OkStatus();\n }\n \n@@ -2933,9 +2933,7 @@ absl::Status SpmdPartitioningVisitor::HandleSort(HloInstruction* hlo) {\n     }\n     auto clone = b_.AddInstruction(\n         hlo->CloneWithNewOperands(hlo->shape(), new_operands));\n-    clone->set_sharding(sharding);\n-    SetPartitionedHlo(\n-        hlo, PartitionedHlo(clone, hlo->shape(), MakePartitioningState()));\n+    SetPartitionedHlo(hlo, clone);\n     return absl::OkStatus();\n   }\n   // Special handling for sort in TopK when first operand partitioined at\n@@ -3126,10 +3124,10 @@ absl::Status SpmdPartitioningVisitor::HandleSort(HloInstruction* hlo) {\n   for (HloInstruction* operand : hlo->operands()) {\n     new_operands.push_back(GetPartitionedHlo(operand).Reshard(sharding).hlo());\n   }\n-  SetPartitionedHlo(hlo, [&] {\n-    return b_.AddInstruction(hlo->CloneWithNewOperands(\n-        MakePartitionedShape(hlo->shape(), hlo->sharding()), new_operands));\n-  });\n+  SetPartitionedHlo(\n+      hlo,\n+      b_.AddInstruction(hlo->CloneWithNewOperands(\n+          MakePartitionedShape(hlo->shape(), hlo->sharding()), new_operands)));\n   return absl::OkStatus();\n }\n \n@@ -3149,10 +3147,10 @@ absl::Status SpmdPartitioningVisitor::HandleTranspose(HloInstruction* hlo) {\n   auto operand = GetPartitionedHlo(hlo->operand(0))\n                      .Reshard(desired_operand_sharding)\n                      .hlo();\n-  SetPartitionedHlo(hlo, [&] {\n-    return b_.AddInstruction(hlo->CloneWithNewOperands(\n-        MakePartitionedShape(hlo->shape(), hlo->sharding()), {operand}));\n-  });\n+  SetPartitionedHlo(\n+      hlo,\n+      b_.AddInstruction(hlo->CloneWithNewOperands(\n+          MakePartitionedShape(hlo->shape(), hlo->sharding()), {operand})));\n   return absl::OkStatus();\n }\n \n@@ -3199,7 +3197,7 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n     PartitionedHlo reshard_reshape =\n         PartitionedHlo(reshape, hlo->shape(), MakePartitioningState())\n             .Reshard(sharding);\n-    SetPartitionedHlo(hlo, [&] { return reshard_reshape.hlo(); });\n+    SetPartitionedHlo(hlo, reshard_reshape.hlo());\n \n     if (sharding_pairs.size() == 2 &&\n         sharding_pairs[1].first == operand.sharding() &&\n@@ -3456,7 +3454,7 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n   };\n   TF_ASSIGN_OR_RETURN(HloInstruction * partitioned,\n                       recursive_shard(operand, sharding, hlo->shape()));\n-  SetPartitionedHlo(hlo, [&] { return partitioned; });\n+  SetPartitionedHlo(hlo, partitioned);\n   return absl::OkStatus();\n }\n \n@@ -3545,11 +3543,9 @@ absl::Status SpmdPartitioningVisitor::HandleSingleDevice(\n     false_computation = module_->AddEmbeddedComputation(false_b.Build(root));\n   }\n \n-  SetPartitionedHlo(hlo, [&]() {\n-    return b_.AddInstruction(HloInstruction::CreateConditional(\n-        hlo->shape(), pred, operand, true_computation, operand,\n-        false_computation));\n-  });\n+  SetPartitionedHlo(hlo, b_.AddInstruction(HloInstruction::CreateConditional(\n+                             hlo->shape(), pred, operand, true_computation,\n+                             operand, false_computation)));\n   return absl::OkStatus();\n }\n \n@@ -3665,10 +3661,8 @@ absl::Status SpmdPartitioningVisitor::HandleBroadcast(HloInstruction* hlo) {\n       new_dims);\n   auto input = operand.Reshard(desired_input_sharding).hlo();\n   auto output_shard_shape = MakePartitionedShape(hlo->shape(), hlo->sharding());\n-  SetPartitionedHlo(hlo, [&] {\n-    return b_.AddInstruction(\n-        hlo->CloneWithNewOperands(output_shard_shape, {input}));\n-  });\n+  SetPartitionedHlo(hlo, b_.AddInstruction(hlo->CloneWithNewOperands(\n+                             output_shard_shape, {input})));\n   return absl::OkStatus();\n }\n \n@@ -3866,15 +3860,14 @@ absl::Status SpmdPartitioningVisitor::HandleDynamicUpdateSlice(\n   // Create dynamic update slice.\n   auto dus = add_hlo(HloInstruction::CreateDynamicUpdateSlice(\n       partitioned_shape, partitioned_input, replicate_update, new_indices));\n-  SetPartitionedHlo(hlo, [&]() {\n-    // Select if update is needed.\n-    return add_hlo(HloInstruction::CreateTernary(\n-        dus->shape(), HloOpcode::kSelect,\n-        add_hlo(HloInstruction::CreateBroadcast(\n-            ShapeUtil::ChangeElementType(dus->shape(), PRED),\n-            all_dims_within_partition, {})),\n-        dus, partitioned_input));\n-  });\n+  // Select if update is needed.\n+  HloInstruction* select = add_hlo(HloInstruction::CreateTernary(\n+      dus->shape(), HloOpcode::kSelect,\n+      add_hlo(HloInstruction::CreateBroadcast(\n+          ShapeUtil::ChangeElementType(dus->shape(), PRED),\n+          all_dims_within_partition, {})),\n+      dus, partitioned_input));\n+  SetPartitionedHlo(hlo, select);\n   return absl::OkStatus();\n }\n \n@@ -3893,8 +3886,7 @@ absl::Status SpmdPartitioningVisitor::HandleGetTupleElement(\n   PartitionedHlo source_partitioned_gte(\n       gte, tuple.base_shape().tuple_shapes(hlo->tuple_index()),\n       MakePartitioningState());\n-  source_partitioned_gte = source_partitioned_gte.Reshard(hlo->sharding());\n-  SetPartitionedHlo(hlo, std::move(source_partitioned_gte));\n+  SetPartitionedHlo(hlo, source_partitioned_gte.Reshard(hlo->sharding()));\n   return absl::OkStatus();\n }\n \n@@ -3907,19 +3899,15 @@ absl::Status SpmdPartitioningVisitor::HandleInfeed(HloInstruction* hlo) {\n     // elements for non-empty tuple. So if it has a nested empty tuple, we\n     // cannot invoke GetSubSharding() since it expects a sharding for the empty\n     // tuple. This is a workaround for that case.\n-    SetPartitionedHlo(hlo, [&]() {\n-      return b_.AddInstruction(\n-          HloInstruction::CreateInfeed(shape, token, hlo->infeed_config()));\n-    });\n+    SetPartitionedHlo(hlo, b_.AddInstruction(HloInstruction::CreateInfeed(\n+                               shape, token, hlo->infeed_config())));\n     return absl::OkStatus();\n   }\n   auto sharding = hlo->sharding().GetSubSharding(hlo->shape(), {0});\n   auto shard_shape = MakePartitionedShape(shape, sharding);\n   if (EvenlyPartitions(shape, sharding)) {\n-    SetPartitionedHlo(hlo, [&]() {\n-      return b_.AddInstruction(HloInstruction::CreateInfeed(\n-          shard_shape, token, hlo->infeed_config()));\n-    });\n+    SetPartitionedHlo(hlo, b_.AddInstruction(HloInstruction::CreateInfeed(\n+                               shard_shape, token, hlo->infeed_config())));\n     return absl::OkStatus();\n   }\n \n@@ -4023,11 +4011,11 @@ absl::Status SpmdPartitioningVisitor::HandleInfeed(HloInstruction* hlo) {\n     }\n     branches[i] = module_->AddEmbeddedComputation(branch_b.Build());\n   }\n-  SetPartitionedHlo(hlo, [&]() {\n-    return b_.AddInstruction(HloInstruction::CreateConditional(\n-        ShapeUtil::MakeTupleShape({shard_shape, token->shape()}), branch_index,\n-        branches, std::vector<HloInstruction*>(branches.size(), token)));\n-  });\n+  SetPartitionedHlo(\n+      hlo, b_.AddInstruction(HloInstruction::CreateConditional(\n+               ShapeUtil::MakeTupleShape({shard_shape, token->shape()}),\n+               branch_index, branches,\n+               std::vector<HloInstruction*>(branches.size(), token))));\n   return absl::OkStatus();\n }\n \n@@ -4223,10 +4211,9 @@ absl::Status SpmdPartitioningVisitor::HandleReverse(HloInstruction* hlo) {\n   if (!left_padded_operand) {\n     return DefaultAction(hlo);\n   }\n-  SetPartitionedHlo(hlo, [&] {\n-    return b_.AddInstruction(hlo->CloneWithNewOperands(\n-        left_padded_operand->shape(), {left_padded_operand}));\n-  });\n+  SetPartitionedHlo(hlo,\n+                    b_.AddInstruction(hlo->CloneWithNewOperands(\n+                        left_padded_operand->shape(), {left_padded_operand})));\n   return absl::OkStatus();\n }\n \n@@ -4237,7 +4224,7 @@ absl::Status SpmdPartitioningVisitor::HandleWhile(HloInstruction* hlo) {\n       hlo->while_body(),\n       GetPartitionedHlo(hlo->operand(0)).Reshard(sharding).hlo()));\n   hlo->SetupDerivedInstruction(whileOp);\n-  SetPartitionedHlo(hlo, [&] { return whileOp; });\n+  SetPartitionedHlo(hlo, whileOp);\n   return absl::OkStatus();\n }\n \n@@ -4282,21 +4269,15 @@ absl::Status SpmdPartitioningVisitor::HandleOutfeed(HloInstruction* hlo) {\n     return HandleSingleDevice(hlo);\n   }\n   if (hlo->sharding().IsManual()) {\n-    auto clone_from_original = [&](const HloSharding& shared_sharding) {\n-      std::vector<HloInstruction*> new_operands;\n-      new_operands.reserve(hlo->operand_count());\n-      for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n-        new_operands.push_back(\n-            GetPartitionedHlo(hlo->operand(i)).Reshard(shared_sharding).hlo());\n-      }\n-      auto clone = b_.AddInstruction(\n-          hlo->CloneWithNewOperands(hlo->shape(), new_operands));\n-      clone->set_sharding(shared_sharding);\n-      return clone;\n-    };\n-\n-    SetPartitionedHlo(hlo,\n-                      [&] { return clone_from_original(hlo->sharding()); });\n+    std::vector<HloInstruction*> new_operands;\n+    new_operands.reserve(hlo->operand_count());\n+    for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n+      new_operands.push_back(\n+          GetPartitionedHlo(hlo->operand(i)).Reshard(hlo->sharding()).hlo());\n+    }\n+    auto clone = b_.AddInstruction(\n+        hlo->CloneWithNewOperands(hlo->shape(), new_operands));\n+    SetPartitionedHlo(hlo, clone);\n     return absl::OkStatus();\n   }\n \n@@ -4331,10 +4312,9 @@ absl::Status SpmdPartitioningVisitor::HandleOutfeed(HloInstruction* hlo) {\n     Shape outfeed_shape = operand->shape();\n     TF_RETURN_IF_ERROR(LayoutUtil::CopyLayoutBetweenShapes(hlo->outfeed_shape(),\n                                                            &outfeed_shape));\n-    SetPartitionedHlo(hlo, [&]() {\n-      return b_.AddInstruction(HloInstruction::CreateOutfeed(\n-          outfeed_shape, operand, token, hlo->outfeed_config()));\n-    });\n+    SetPartitionedHlo(\n+        hlo, b_.AddInstruction(HloInstruction::CreateOutfeed(\n+                 outfeed_shape, operand, token, hlo->outfeed_config())));\n     return absl::OkStatus();\n   }\n \n@@ -4453,13 +4433,13 @@ absl::Status SpmdPartitioningVisitor::HandleOutfeed(HloInstruction* hlo) {\n         hlo->outfeed_config()));\n     branches[i] = module_->AddEmbeddedComputation(branch_b.Build());\n   }\n-  SetPartitionedHlo(hlo, [&]() {\n-    return b_.AddInstruction(HloInstruction::CreateConditional(\n-        token->shape(), branch_index, branches,\n-        std::vector<HloInstruction*>(\n-            branches.size(),\n-            b_.AddInstruction(HloInstruction::CreateTuple({operand, token})))));\n-  });\n+  SetPartitionedHlo(\n+      hlo,\n+      b_.AddInstruction(HloInstruction::CreateConditional(\n+          token->shape(), branch_index, branches,\n+          std::vector<HloInstruction*>(\n+              branches.size(), b_.AddInstruction(HloInstruction::CreateTuple(\n+                                   {operand, token}))))));\n   return absl::OkStatus();\n }\n \n@@ -4481,8 +4461,7 @@ absl::Status SpmdPartitioningVisitor::HandleRng(HloInstruction* hlo) {\n   };\n \n   if (hlo->sharding().IsManual()) {\n-    SetPartitionedHlo(hlo,\n-                      [&] { return clone_from_original(hlo->sharding()); });\n+    SetPartitionedHlo(hlo, clone_from_original(hlo->sharding()));\n     return absl::OkStatus();\n   }\n \n@@ -4507,11 +4486,10 @@ absl::Status SpmdPartitioningVisitor::HandleRng(HloInstruction* hlo) {\n   }\n \n   if (!hlo->sharding().ReplicateOnLastTileDim()) {\n-    SetPartitionedHlo(hlo, [&] {\n-      return b_.AddInstruction(HloInstruction::CreateRng(\n-          MakePartitionedShape(hlo->shape(), hlo->sharding()),\n-          hlo->random_distribution(), new_operands));\n-    });\n+    SetPartitionedHlo(hlo,\n+                      b_.AddInstruction(HloInstruction::CreateRng(\n+                          MakePartitionedShape(hlo->shape(), hlo->sharding()),\n+                          hlo->random_distribution(), new_operands)));\n   } else {\n     std::vector<int64_t> group_dims(\n         hlo->sharding().tile_assignment().num_dimensions() - 1);\n@@ -4830,9 +4808,8 @@ absl::Status SpmdPartitioningVisitor::HandleTuple(HloInstruction* hlo) {\n             .Reshard(hlo->sharding().GetSubSharding(hlo->shape(), {i}))\n             .hlo());\n   }\n-  SetPartitionedHlo(hlo, [&]() {\n-    return b_.AddInstruction(HloInstruction::CreateTuple(new_operands));\n-  });\n+  SetPartitionedHlo(\n+      hlo, b_.AddInstruction(HloInstruction::CreateTuple(new_operands)));\n   return absl::OkStatus();\n }\n \n@@ -4921,7 +4898,7 @@ absl::Status SpmdPartitioningVisitor::HandleRaggedDot(HloInstruction* hlo) {\n         MakeBinaryAdd(phlo->shape().element_type(), lhs.state().module));\n   }\n \n-  SetPartitionedHlo(hlo, [&]() { return phlo; });\n+  SetPartitionedHlo(hlo, phlo);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "af431d2ebe6add86bee35f03d9dc7b7ecb56743f",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=ba10feaa24cacdab7d5140d97e5dc3a32d8d9f38",
            "patch": "@@ -795,17 +795,21 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n   void SetPartitionedHlo(const HloInstruction* hlo,\n                          PartitionedHlo&& partitioned_hlo);\n \n-  // Convenient wrapper that creates PartitionedHlo from the result of the func\n-  // and maps it to the given original hlo.\n-  void SetPartitionedHlo(const HloInstruction* hlo,\n-                         absl::FunctionRef<HloInstruction*()> func) {\n-    HloInstruction* new_hlo = func();\n+  // Convenient wrapper that creates PartitionedHlo from `new_hlo`.\n+  void SetPartitionedHlo(const HloInstruction* hlo, HloInstruction* new_hlo) {\n     new_hlo->set_sharding(hlo->sharding());\n     SetPartitionedHlo(\n         hlo, PartitionedHlo(new_hlo, hlo->shape(), MakePartitioningState()));\n     changed_ = true;\n   }\n \n+  // Convenient wrapper that creates PartitionedHlo from the result of the func\n+  // and maps it to the given original hlo.\n+  void SetPartitionedHlo(const HloInstruction* hlo,\n+                         absl::FunctionRef<HloInstruction*()> func) {\n+    return SetPartitionedHlo(hlo, func());\n+  }\n+\n   int64_t NewChannel() { return (*next_channel_id_)++; }\n \n   PartitionedHlo::PartitioningState MakePartitioningState();"
        }
    ],
    "stats": {
        "total": 203,
        "additions": 89,
        "deletions": 114
    }
}