{
    "author": "mkuperst",
    "message": "[XLA] Support complex CFGs in the SPMD partitioner.\n\nThis adds SPMD partitioner support for graphs where a single computation may have multiple callsites.\n\nPiperOrigin-RevId: 799577820",
    "sha": "688e4e4d7677cb7e43c3bc737c7bb501105c875f",
    "files": [
        {
            "sha": "8973b00a2957d9fb1e557c6380f7aac73c299365",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 227,
            "deletions": 46,
            "changes": 273,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688e4e4d7677cb7e43c3bc737c7bb501105c875f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688e4e4d7677cb7e43c3bc737c7bb501105c875f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=688e4e4d7677cb7e43c3bc737c7bb501105c875f",
            "patch": "@@ -5452,6 +5452,8 @@ absl::StatusOr<bool> SpmdPartitioner::Run(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   set_execution_threads(execution_threads);\n   TF_RETURN_IF_ERROR(PreprocessSharding(module, execution_threads));\n+  TF_ASSIGN_OR_RETURN(bool changed,\n+                      PreprocessCallSites(module, execution_threads));\n   TF_RETURN_IF_ERROR(PreprocessHlos(module, execution_threads));\n \n   XLA_VLOG_LINES(1, SpmdLogger::ReportBeforePartition(\n@@ -5461,15 +5463,11 @@ absl::StatusOr<bool> SpmdPartitioner::Run(\n   // that unreduced sharding can be recovered at the end of this pass.\n   TF_RETURN_IF_ERROR(ConvertUnreducedSharding(module, execution_threads));\n \n-  FlattenCallGraph flatten;\n-  TF_ASSIGN_OR_RETURN(auto changed, flatten.Run(module));\n-\n   SpmdLogger logger(options_.report_instruction_count,\n                     /*disabled=*/!VLOG_IS_ON(1));\n   auto program_shape = module->entry_computation()->ComputeProgramShape();\n   int64_t next_channel_id = hlo_query::NextChannelId(*module);\n   std::unique_ptr<CallGraph> call_graph = CallGraph::Build(module);\n-  CHECK(call_graph->IsFlattened());\n   TF_RETURN_IF_ERROR(\n       call_graph->VisitNodes([&](const CallGraphNode& node) -> absl::Status {\n         HloComputation* computation = node.computation();\n@@ -5495,10 +5493,8 @@ absl::StatusOr<bool> SpmdPartitioner::Run(\n         if (node.caller_callsites().empty()) {\n           return absl::OkStatus();\n         }\n-        if (node.caller_callsites().size() > 1) {\n-          return absl::InternalError(\n-              \"Expected CFG to be flattened before SPMD partitioner.\");\n-        }\n+        // PreprocessCallSites made sure a computation is only used by a single\n+        // opcode and with a single sharding on the arguments.\n         HloInstruction* caller = node.caller_callsites()[0].instruction();\n         switch (caller->opcode()) {\n           case HloOpcode::kWhile: {\n@@ -5596,7 +5592,6 @@ absl::StatusOr<bool> SpmdPartitioner::Run(\n     pass.AddPass<TupleSimplifier>();\n     pass.AddPass<HloDCE>(/*remove_cross_partition_collective_ops=*/true);\n     pass.AddPass<HloCSE>(/*is_layout_sensitive=*/false);\n-    pass.AddPass<FlattenCallGraph>();\n     TF_RETURN_IF_ERROR(pass.Run(module, execution_threads).status());\n   }\n \n@@ -5647,42 +5642,6 @@ absl::Status SpmdPartitioner::PreprocessSharding(\n               HloSharding::Single(hlo->shape(), HloSharding::Replicate()));\n         }\n       }\n-\n-      // For control-flow constructs, we must make sure that the inputs and\n-      // outputs of the called computation have the same sharding as the\n-      // arguments being passed in.\n-      switch (hlo->opcode()) {\n-        case HloOpcode::kWhile: {\n-          hlo->while_condition()->parameter_instruction(0)->set_sharding(\n-              hlo->sharding());\n-          hlo->while_body()->parameter_instruction(0)->set_sharding(\n-              hlo->sharding());\n-          // The condition root must be replicated so that all partitions follow\n-          // the same control flow.\n-          HloInstruction* cond_root =\n-              hlo->while_condition()->root_instruction();\n-          const HloSharding cond_root_sharding =\n-              hlo_sharding_util::ReplicateAllDataDims(cond_root->sharding());\n-          cond_root->set_sharding(cond_root_sharding);\n-          break;\n-        }\n-        case HloOpcode::kConditional: {\n-          for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n-            hlo->branch_computation(i)->parameter_instruction(0)->set_sharding(\n-                hlo->operand(i + 1)->sharding());\n-          }\n-          break;\n-        }\n-        case HloOpcode::kCall: {\n-          for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n-            hlo->to_apply()->parameter_instruction(i)->set_sharding(\n-                hlo->operand(i)->sharding());\n-          }\n-          break;\n-        }\n-        default:\n-          break;\n-      }\n     }\n   }\n \n@@ -5714,7 +5673,6 @@ absl::Status SpmdPartitioner::PreprocessSharding(\n           << param->sharding().ToString();\n     }\n   }\n-\n   return absl::OkStatus();\n }\n \n@@ -5994,6 +5952,229 @@ absl::Status SpmdPartitioner::PreprocessHlos(\n   return absl::OkStatus();\n }\n \n+namespace {\n+struct CallSiteInfo {\n+  HloOpcode opcode;\n+  std::vector<std::shared_ptr<const HloSharding>> param_sharding;\n+\n+  bool operator==(const CallSiteInfo& other) const {\n+    if (opcode != other.opcode) {\n+      return false;\n+    }\n+    if (param_sharding.size() != other.param_sharding.size()) {\n+      return false;\n+    }\n+    for (int64_t i = 0; i < param_sharding.size(); ++i) {\n+      if (*param_sharding[i] != *other.param_sharding[i]) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  template <typename H>\n+  friend H AbslHashValue(H h, const CallSiteInfo& call_site_info) {\n+    h = H::combine(std::move(h), call_site_info.opcode);\n+    for (const auto& param_sharding : call_site_info.param_sharding) {\n+      h = H::combine(std::move(h), *param_sharding);\n+    }\n+    return h;\n+  }\n+};\n+\n+// We want to group the call-sites of every computation (that's used in a\n+// control-flow context) based on (a) the opcode of the caller, and (b) how the\n+// parameters are sharded.\n+// Given a computation and a calling instruction for that computation,\n+// `GetCallSiteInfos` extracts the aforementioned information.\n+absl::StatusOr<std::vector<CallSiteInfo>> GetCallSiteInfos(\n+    HloInstruction* caller, HloComputation* computation) {\n+  std::vector<CallSiteInfo> call_site_infos;\n+  // For `while` and `call`, a computation should only be referenced once, so we\n+  // only have a single callsite info. For `conditional`, we may have the same\n+  // computation called on multiple branches, and with different sharding, so we\n+  // may need multiple.\n+  switch (caller->opcode()) {\n+    case HloOpcode::kWhile: {\n+      CallSiteInfo call_site_info;\n+      call_site_info.opcode = caller->opcode();\n+      call_site_info.param_sharding.push_back(caller->sharding_ptr());\n+      call_site_infos.push_back(call_site_info);\n+      break;\n+    }\n+    case HloOpcode::kCall: {\n+      CallSiteInfo call_site_info;\n+      call_site_info.opcode = caller->opcode();\n+      call_site_info.param_sharding.reserve(caller->operand_count());\n+      for (HloInstruction* operand : caller->operands()) {\n+        call_site_info.param_sharding.push_back(operand->sharding_ptr());\n+      }\n+      call_site_infos.push_back(call_site_info);\n+      break;\n+    }\n+    case HloOpcode::kConditional: {\n+      for (int64_t i = 0; i < caller->branch_count(); ++i) {\n+        if (caller->branch_computation(i) == computation) {\n+          CallSiteInfo call_site_info;\n+          call_site_info.opcode = caller->opcode();\n+          call_site_info.param_sharding.push_back(\n+              caller->operand(i + 1)->sharding_ptr());\n+          call_site_infos.push_back(call_site_info);\n+        }\n+      }\n+      break;\n+    }\n+    default:\n+      return absl::InternalError(\"Unexpected opcode in call context.\");\n+  }\n+  return call_site_infos;\n+}\n+\n+// Do we already know which computation callsites that use a particular sharding\n+// should refer to? If not, create one, otherwise reuse the existing one.\n+HloComputation* GetCanonicalComputation(\n+    HloComputation* computation, const CallSiteInfo& call_site_info,\n+    absl::flat_hash_map<CallSiteInfo, HloComputation*>& info_to_computation,\n+    bool& changed) {\n+  HloComputation* canonical_computation = nullptr;\n+  if (auto it = info_to_computation.find(call_site_info);\n+      it == info_to_computation.end()) {\n+    if (info_to_computation.empty()) {\n+      // Don't actually make a copy for the first callsite info we\n+      // encounter, just reuse the original computation. This is\n+      // likely the common case - either there's only one callsite,\n+      // or all the callsites agree on how the arguments are\n+      // partitioned.\n+      canonical_computation = computation;\n+    } else {\n+      canonical_computation =\n+          computation->parent()->AddEmbeddedComputation(computation->Clone());\n+      changed = true;\n+    }\n+    info_to_computation[call_site_info] = canonical_computation;\n+  } else {\n+    canonical_computation = it->second;\n+  }\n+  return canonical_computation;\n+}\n+}  // namespace\n+\n+absl::StatusOr<bool> SpmdPartitioner::PreprocessCallSites(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  bool changed = false;\n+  std::unique_ptr<CallGraph> call_graph = CallGraph::Build(module);\n+\n+  // For each computation that is used in multiple contexts, make\n+  // duplicates that will avoid conflicts.\n+  absl::flat_hash_map<HloComputation*,\n+                      absl::flat_hash_map<CallSiteInfo, HloComputation*>>\n+      canonical_computations;\n+  TF_RETURN_IF_ERROR(call_graph->VisitNodes([&](const CallGraphNode& node)\n+                                                -> absl::Status {\n+    HloComputation* computation = node.computation();\n+    if (!execution_threads.empty() &&\n+        !execution_threads.contains(computation->execution_thread())) {\n+      return absl::OkStatus();\n+    }\n+    if (node.context() != CallContext::kControlFlow) {\n+      return absl::OkStatus();\n+    }\n+    for (const CallSite& call_site : node.caller_callsites()) {\n+      HloInstruction* caller = call_site.instruction();\n+      absl::flat_hash_map<CallSiteInfo, HloComputation*>& info_to_computation =\n+          canonical_computations[computation];\n+      TF_ASSIGN_OR_RETURN(std::vector<CallSiteInfo> call_site_infos,\n+                          GetCallSiteInfos(caller, computation));\n+      switch (caller->opcode()) {\n+        case HloOpcode::kWhile:\n+        case HloOpcode::kCall: {\n+          CHECK_EQ(call_site_infos.size(), 1)\n+              << \"Unexpected number of call site infos for \"\n+              << caller->ToString();\n+          const CallSiteInfo& call_site_info = call_site_infos[0];\n+          HloComputation* canonical_computation = GetCanonicalComputation(\n+              computation, call_site_info, info_to_computation, changed);\n+          call_site.instruction()->ReplaceCalledComputations(\n+              [&](HloComputation* called) {\n+                return (called == computation) ? canonical_computation : called;\n+              });\n+          break;\n+        }\n+        case HloOpcode::kConditional: {\n+          // Since call_site_infos only contains information for branches\n+          // that refer to `computation`, rather than all branches, we need\n+          // to keep track of how many branches of this kConditional\n+          // actually point to `computation`. (Note that in most cases, this\n+          // will be 1, but more general cases are supported.)\n+          int64_t matched_branches = 0;\n+          for (int i = 0; i < caller->branch_count(); ++i) {\n+            HloComputation* branch = caller->branch_computation(i);\n+            if (branch != computation) {\n+              continue;\n+            }\n+            const CallSiteInfo& call_site_info =\n+                call_site_infos[matched_branches++];\n+            HloComputation* canonical_computation = GetCanonicalComputation(\n+                computation, call_site_info, info_to_computation, changed);\n+            call_site.instruction()->set_branch_computation(\n+                i, canonical_computation);\n+          }\n+          break;\n+        }\n+        default:\n+          return absl::InternalError(\"Unexpected opcode in call context.\");\n+      }\n+    }\n+    return absl::OkStatus();\n+  }));\n+\n+  // We've ensured that there are no more conflicts between different callsites\n+  // of the same computation by specializing the required computations. So now\n+  // we can continue the fixup process under the assumption we can always\n+  // propagate from arguments to the corresponding parameters.\n+  for (HloComputation* computation :\n+       module->MakeComputationPostOrder(execution_threads)) {\n+    for (HloInstruction* hlo : computation->MakeInstructionPostOrder()) {\n+      // We must make sure that the inputs and outputs of the called computation\n+      // have the same sharding as the arguments being passed in.\n+      switch (hlo->opcode()) {\n+        case HloOpcode::kWhile: {\n+          hlo->while_condition()->parameter_instruction(0)->set_sharding(\n+              hlo->sharding());\n+          hlo->while_body()->parameter_instruction(0)->set_sharding(\n+              hlo->sharding());\n+          // The condition root must be replicated so that all partitions follow\n+          // the same control flow.\n+          HloInstruction* cond_root =\n+              hlo->while_condition()->root_instruction();\n+          const HloSharding cond_root_sharding =\n+              hlo_sharding_util::ReplicateAllDataDims(cond_root->sharding());\n+          cond_root->set_sharding(cond_root_sharding);\n+          break;\n+        }\n+        case HloOpcode::kConditional: {\n+          for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n+            hlo->branch_computation(i)->parameter_instruction(0)->set_sharding(\n+                hlo->operand(i + 1)->sharding());\n+          }\n+          break;\n+        }\n+        case HloOpcode::kCall: {\n+          for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n+            hlo->to_apply()->parameter_instruction(i)->set_sharding(\n+                hlo->operand(i)->sharding());\n+          }\n+          break;\n+        }\n+        default:\n+          break;\n+      }\n+    }\n+  }\n+  return changed;\n+}\n+\n void SpmdPartitioningVisitor::SetPartitionedHlo(\n     const HloInstruction* hlo, PartitionedHlo&& partitioned_hlo) {\n   CHECK_EQ(partitioned_instructions_.count(hlo), 0);"
        },
        {
            "sha": "22cb5bf1fd8db8a0a8fb341d32a2791ac38973e8",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688e4e4d7677cb7e43c3bc737c7bb501105c875f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688e4e4d7677cb7e43c3bc737c7bb501105c875f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=688e4e4d7677cb7e43c3bc737c7bb501105c875f",
            "patch": "@@ -426,6 +426,13 @@ class SpmdPartitioner : public HloModulePass {\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads);\n \n+  // Preprocesses the graph to make sure that computations called in\n+  // control-flow contexts (call, while, conditional) have matching sharding\n+  // annotations on callee parameters and the caller arguments.\n+  absl::StatusOr<bool> PreprocessCallSites(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads);\n+\n   void set_execution_threads(\n       const absl::flat_hash_set<absl::string_view>& execution_threads) {\n     execution_threads_ = execution_threads;"
        },
        {
            "sha": "2ca5feec7c8cc41b38ceeb779f59099fc8cef922",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 127,
            "deletions": 9,
            "changes": 136,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688e4e4d7677cb7e43c3bc737c7bb501105c875f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688e4e4d7677cb7e43c3bc737c7bb501105c875f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=688e4e4d7677cb7e43c3bc737c7bb501105c875f",
            "patch": "@@ -288,18 +288,18 @@ TEST_P(SpmdPartitioningTest, PartitionCall) {\n HloModule jit_f\n \n g {\n-  Arg_0.6 = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n-  constant.0 = s32[] constant(2), sharding={replicated}\n-  broadcast.0 = s32[8,2]{1,0} broadcast(constant.0), dimensions={}, sharding={devices=[2,2]<=[4]}\n-  ROOT multiply.9 = s32[8,2]{1,0} multiply(Arg_0.6, broadcast.0), sharding={devices=[2,2]<=[4]}\n+  param = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  constant.0 = s32[] constant(0), sharding={replicated}\n+  broadcast = s32[8,2]{1,0} broadcast(constant.0), dimensions={}, sharding={devices=[2,2]<=[4]}\n+  ROOT multiply = s32[8,2]{1,0} multiply(param, broadcast), sharding={devices=[2,2]<=[4]}\n }\n \n ENTRY main {\n-  Arg_0.1 = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n-  constant.1 = s32[] constant(3), sharding={replicated}\n-  broadcast.1 = s32[8,2]{1,0} broadcast(constant.1), dimensions={}, sharding={devices=[2,2]<=[4]}\n-  multiply.4 = s32[8,2]{1,0} multiply(Arg_0.1, broadcast.1), sharding={devices=[2,2]<=[4]}\n-  ROOT call = s32[8,2]{1,0} call(multiply.4), to_apply=g, sharding={devices=[2,2]<=[4]}, backend_config={\"flag_configs\":[],\"scoped_memory_configs\":[],\"compute_type\":\"COMPUTE_TYPE_DEFAULT\",\"device_type\":\"DEVICE_TYPE_HOST\",\"used_scoped_memory_configs\":[]}\n+  input = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  constant.3 = s32[] constant(3), sharding={replicated}\n+  broadcast = s32[8,2]{1,0} broadcast(constant.3), dimensions={}, sharding={devices=[2,2]<=[4]}\n+  multiply = s32[8,2]{1,0} multiply(input, broadcast), sharding={devices=[2,2]<=[4]}\n+  ROOT call = s32[8,2]{1,0} call(multiply), to_apply=g, sharding={devices=[2,2]<=[4]}, backend_config={\"flag_configs\":[],\"scoped_memory_configs\":[],\"compute_type\":\"COMPUTE_TYPE_DEFAULT\",\"device_type\":\"DEVICE_TYPE_HOST\",\"used_scoped_memory_configs\":[]}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/4));\n@@ -313,6 +313,124 @@ ENTRY main {\n                                     op::Shape(\"s32[4,1]\")));\n }\n \n+TEST_P(SpmdPartitioningTest, PartitionCallMultipleCallsites) {\n+  absl::string_view hlo_string = R\"(\n+HloModule jit_f\n+\n+g {\n+  param = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  constant.0 = s32[] constant(0), sharding={replicated}\n+  broadcast = s32[8,2]{1,0} broadcast(constant.0), dimensions={}, sharding={devices=[2,2]<=[4]}\n+  ROOT multiply = s32[8,2]{1,0} multiply(param, broadcast), sharding={devices=[2,2]<=[4]}\n+}\n+\n+ENTRY main {\n+  input = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  constant.3 = s32[] constant(3), sharding={replicated}\n+  broadcast = s32[8,2]{1,0} broadcast(constant.3), dimensions={}, sharding={devices=[2,2]<=[4]}\n+  multiply = s32[8,2]{1,0} multiply(input, broadcast), sharding={devices=[2,2]<=[4]}\n+  call.0 = s32[8,2]{1,0} call(multiply), to_apply=g, sharding={devices=[2,2]<=[4]}\n+  add = s32[8,2]{1,0} add(input, broadcast), sharding={devices=[2,2]<=[4]}\n+  call.1 = s32[8,2]{1,0} call(add), to_apply=g, sharding={devices=[2,2]<=[4]}\n+  ROOT root = s32[8,2]{1,0} add(call.0, call.1), sharding={devices=[2,2]<=[4]}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/4));\n+  VLOG(1) << module->ToString();\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              AllOf(op::Add(op::Call(), op::Call()), op::Shape(\"s32[4,1]\")));\n+  const HloInstruction* call0 = root->operand(0);\n+  const HloInstruction* call1 = root->operand(1);\n+  EXPECT_EQ(call0->to_apply(), call1->to_apply());\n+  const HloInstruction* call_comp_root = call0->to_apply()->root_instruction();\n+  EXPECT_THAT(call_comp_root, AllOf(op::Multiply(op::Parameter(0),\n+                                                 op::Broadcast(op::Constant())),\n+                                    op::Shape(\"s32[4,1]\")));\n+}\n+\n+TEST_P(SpmdPartitioningTest, PartitionCallMultipleMismatchedCallsites) {\n+  absl::string_view hlo_string = R\"(\n+HloModule jit_f\n+\n+g {\n+  param = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  constant.0 = s32[] constant(0), sharding={replicated}\n+  broadcast = s32[8,2]{1,0} broadcast(constant.0), dimensions={}, sharding={devices=[2,2]<=[4]}\n+  ROOT multiply = s32[8,2]{1,0} multiply(param, broadcast), sharding={devices=[2,2]<=[4]}\n+}\n+\n+ENTRY main {\n+  input0 = s32[8,2]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  input1 = s32[8,2]{1,0} parameter(1), sharding={devices=[4,1]<=[4]}\n+  multiply = s32[8,2]{1,0} multiply(input0, input0), sharding={devices=[2,2]<=[4]}\n+  call.0 = s32[8,2]{1,0} call(multiply), to_apply=g, sharding={devices=[2,2]<=[4]}\n+  add = s32[8,2]{1,0} add(input1, input1), sharding={devices=[4,1]<=[4]}\n+  call.1 = s32[8,2]{1,0} call(add), to_apply=g, sharding={devices=[2,2]<=[4]}\n+  ROOT root = s32[8,2]{1,0} add(call.0, call.1), sharding={devices=[2,2]<=[4]}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/4));\n+  VLOG(1) << module->ToString();\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              AllOf(op::Add(op::Call(), op::Call()), op::Shape(\"s32[4,1]\")));\n+  const HloInstruction* call0 = root->operand(0);\n+  const HloInstruction* call1 = root->operand(1);\n+  EXPECT_NE(call0->to_apply(), call1->to_apply());\n+  EXPECT_THAT(call0->operand(0),\n+              AllOf(op::Multiply(op::Parameter(0), op::Parameter(0)),\n+                    op::Shape(\"s32[4,1]\")));\n+  EXPECT_THAT(call1->operand(0),\n+              AllOf(op::Add(op::Parameter(1), op::Parameter(1)),\n+                    op::Shape(\"s32[2,2]\")));\n+  const HloInstruction* call0_comp_root = call0->to_apply()->root_instruction();\n+  EXPECT_THAT(\n+      call0_comp_root,\n+      AllOf(op::Multiply(op::Parameter(0), op::Broadcast(op::Constant())),\n+            op::Shape(\"s32[4,1]\")));\n+  const HloInstruction* call1_comp_root = call1->to_apply()->root_instruction();\n+  EXPECT_THAT(call1_comp_root,\n+              AllOf(op::Multiply(op::Reshape(), op::Broadcast(op::Constant())),\n+                    op::Shape(\"s32[4,1]\")));\n+}\n+\n+TEST_P(SpmdPartitioningTest, PartitionConditionalMismatchedBranches) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+branch_ceil {\n+  param = f32[8,2] parameter(0), sharding={devices=[2,2]<=[4]}\n+  ROOT tgte1 = f32[8,2] ceil(param), sharding={devices=[2,2]<=[4]}\n+}\n+\n+branch_floor {\n+  param = f32[8,2] parameter(0), sharding={devices=[2,2]<=[4]}\n+  ROOT tgte1 = f32[8,2] floor(param), sharding={devices=[2,2]<=[4]}\n+}\n+\n+ENTRY entry {\n+  p0 = f32[8,2] parameter(0), sharding={devices=[2,2]<=[4]}\n+  p1 = f32[8,2] parameter(1), sharding={devices=[4,1]<=[4]}\n+  b0 = s32[] parameter(2), sharding={replicated}\n+  ROOT conditional = f32[8,2] conditional(b0, p0, p1, p0, p0),\n+    branch_computations={branch_ceil, branch_floor, branch_ceil, branch_floor},\n+    sharding={devices=[2,2]<=[4]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/4));\n+  VLOG(1) << module->ToString();\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, AllOf(op::Conditional(op::Parameter(2), op::Parameter(0),\n+                                          op::Parameter(1), op::Parameter(0),\n+                                          op::Parameter(0)),\n+                          op::Shape(\"f32[4,1]\")));\n+\n+  EXPECT_EQ(root->branch_computation(0), root->branch_computation(2));\n+  EXPECT_NE(root->branch_computation(1), root->branch_computation(3));\n+}\n+\n TEST_P(SpmdPartitioningTest, TiledToReplicated) {\n   absl::string_view hlo_string = R\"(\n HloModule module"
        }
    ],
    "stats": {
        "total": 416,
        "additions": 361,
        "deletions": 55
    }
}