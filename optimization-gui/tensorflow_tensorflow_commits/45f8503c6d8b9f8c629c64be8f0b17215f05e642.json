{
    "author": "shawnwang18",
    "message": "PR #29884: [XLA:GPU] Add GetInlinedModule helper function in call_inliner\n\nImported from GitHub PR https://github.com/openxla/xla/pull/29884\n\nThis PR add a function `GetInlinedModule`, which is  Given a module, this function first clone the module, then inline the module, and return the inlined module, clone context and inlined map in InlinedModule struct.\n\nThis class is required because we found that while_loop_analysis pass can not work on the module that has been parsed by command buffer rewriter, so to do loop analysis, we have to first inline the command buffer call.\n\nThis PR is depended by :  https://github.com/openxla/xla/pull/28740\nCopybara import of the project:\n\n--\n14955adeb69face9feaddf82bc22251e34cf2af1 by Shawn Wang <shawnw@nvidia.com>:\n\nadd two helper functions in call_inliner\n\n--\n97266eefa54b216ddd2d14b3bcd5e6868126479c by Shawn Wang <shawnw@nvidia.com>:\n\nremove unused variable\n\n--\nec6e1beb19e8fc4a337cbe2364532d06960be2fb by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n95abdbbf34383c0ef121f364038e30044aeab29b by Shawn Wang <shawnw@nvidia.com>:\n\nfix typos\n\nMerging this change closes #29884\n\nPiperOrigin-RevId: 799052398",
    "sha": "45f8503c6d8b9f8c629c64be8f0b17215f05e642",
    "files": [
        {
            "sha": "1c1bee064e93edf84616ac3c4a75f227164aef0e",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=45f8503c6d8b9f8c629c64be8f0b17215f05e642",
            "patch": "@@ -989,6 +989,7 @@ cc_library(\n         \"//xla/service/spmd/shardy:constants\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "9bea40a87284eb2656c59fcfd4f7effc7e976796",
            "filename": "third_party/xla/xla/service/call_inliner.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 9,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.cc?ref=45f8503c6d8b9f8c629c64be8f0b17215f05e642",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -33,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_original_value.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n@@ -345,7 +347,8 @@ bool CallInliner::ShouldInline(const CallGraph& call_graph,\n \n absl::StatusOr<bool> CallInliner::InlineAndLegalize(\n     const CallGraph& call_graph, HloComputation* computation,\n-    absl::Span<HloInstruction* const> instruction_sequence) const {\n+    absl::Span<HloInstruction* const> instruction_sequence,\n+    std::optional<InlinedInstructionMap*> inline_map) {\n   HloModule* module = computation->parent();\n   bool did_node_mutate = false;\n   std::vector<HloInstruction*> inlined_instructions;\n@@ -358,24 +361,30 @@ absl::StatusOr<bool> CallInliner::InlineAndLegalize(\n       // The caller instruction will get removed after inlining. Record the\n       // callee computation beforehand, so we can find its schedule.\n       HloComputation* callee = instruction->to_apply();\n-      TF_ASSIGN_OR_RETURN(CallInliner::InlinedInstructionMap inline_map,\n-                          Inline(instruction));\n+      TF_ASSIGN_OR_RETURN(\n+          CallInliner::InlinedInstructionMap inline_map_cur_call,\n+          Inline(instruction));\n       if (module->has_schedule()) {\n         for (HloInstruction* inlined_instruction :\n              module->schedule().sequence(callee).instructions()) {\n           // Parameters were already added to sequence as operands to the\n           // call.\n           if (inlined_instruction->opcode() != HloOpcode::kParameter) {\n-            inlined_instructions.push_back(inline_map[inlined_instruction]);\n+            inlined_instructions.push_back(\n+                inline_map_cur_call[inlined_instruction]);\n           }\n         }\n       }\n       if (update_domain_) {\n         HloDomainIsolator isolator([]() { return ShardingDomainCreator{}; });\n-        for (const auto& [call_inst, inlined_inst] : inline_map) {\n+        for (const auto& [call_inst, inlined_inst] : inline_map_cur_call) {\n           TF_RETURN_IF_ERROR(isolator.UpdateDomains(inlined_inst).status());\n         }\n       }\n+      if (inline_map.has_value()) {\n+        inline_map.value()->insert(inline_map_cur_call.begin(),\n+                                   inline_map_cur_call.end());\n+      }\n       did_node_mutate = true;\n     } else if (module->has_schedule()) {\n       inlined_instructions.push_back(instruction);\n@@ -396,8 +405,8 @@ absl::StatusOr<bool> CallInliner::InlineAndLegalize(\n   return did_node_mutate;\n }\n \n-absl::StatusOr<bool> CallInliner::Run(\n-    HloModule* module,\n+absl::StatusOr<bool> CallInliner::RunWithInlineMap(\n+    HloModule* module, std::optional<InlinedInstructionMap*> inline_map,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   std::unique_ptr<CallGraph> call_graph = CallGraph::Build(module);\n   // Because call graph nodes are visited in post-order (callees before callers)\n@@ -415,12 +424,12 @@ absl::StatusOr<bool> CallInliner::Run(\n               HloInstructionSequence& sequence =\n                   module->schedule().GetOrCreateSequence(node.computation());\n               return InlineAndLegalize(*call_graph, node.computation(),\n-                                       sequence.instructions());\n+                                       sequence.instructions(), inline_map);\n             }\n \n             return InlineAndLegalize(\n                 *call_graph, node.computation(),\n-                node.computation()->MakeInstructionPostOrder());\n+                node.computation()->MakeInstructionPostOrder(), inline_map);\n           }));\n   if (did_mutate) {\n     // Run DCE to remove called computations which are now becoming unused.\n@@ -436,4 +445,48 @@ absl::StatusOr<bool> CallInliner::Run(\n   return did_mutate;\n }\n \n+absl::StatusOr<bool> CallInliner::Run(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  return RunWithInlineMap(module, std::nullopt, execution_threads);\n+}\n+\n+bool IsInlineableComputation(HloComputation* computation) {\n+  auto is_inlineable_call_op = [](HloInstruction* instruction) {\n+    bool prerequisite = instruction->opcode() == HloOpcode::kCall &&\n+                        !instruction->has_backend_config() &&\n+                        !instruction->parent()->IsAsyncComputation();\n+    if (!prerequisite || !InlineInstruction(instruction)) {\n+      return false;\n+    }\n+    return true;\n+  };\n+  return absl::c_any_of(computation->instructions(), is_inlineable_call_op);\n+}\n+\n+const HloInstruction* InlinedModule::get_inlined_inst(\n+    const HloInstruction* inst) {\n+  auto it = clone_context->cloned_instructions().find(inst);\n+  if (it != clone_context->cloned_instructions().end()) {\n+    auto it2 = clone_inlined_map.find(it->second);\n+    if (it2 != clone_inlined_map.end()) {\n+      return it2->second;\n+    }\n+    return it->second;\n+  }\n+  return nullptr;\n+}\n+\n+absl::StatusOr<InlinedModule> GetInlinedModule(HloModule* module) {\n+  auto [cloned_module, clone_context] =\n+      module->CloneWithContext(\"inline\", module->config());\n+  CallInliner::InlinedInstructionMap clone_inlined_map;\n+  CallInliner inliner;\n+  TF_RETURN_IF_ERROR(\n+      inliner.RunWithInlineMap(cloned_module.get(), &clone_inlined_map, {})\n+          .status());\n+  return InlinedModule{std::move(cloned_module), std::move(clone_context),\n+                       std::move(clone_inlined_map)};\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "710336ab716155909521350f2ee9f12e48c7aa4b",
            "filename": "third_party/xla/xla/service/call_inliner.h",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.h?ref=45f8503c6d8b9f8c629c64be8f0b17215f05e642",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n@@ -71,6 +72,10 @@ class CallInliner : public HloModulePass {\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n \n+  absl::StatusOr<bool> RunWithInlineMap(\n+      HloModule* module, std::optional<InlinedInstructionMap*> inline_map,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads);\n+\n   // Returns true if the instruction is a kCall operation and is eligible for\n   // inlining.\n   virtual bool IsInlineableCallOp(HloInstruction* instruction) const;\n@@ -81,7 +86,8 @@ class CallInliner : public HloModulePass {\n  private:\n   absl::StatusOr<bool> InlineAndLegalize(\n       const CallGraph& call_graph, HloComputation* computation,\n-      absl::Span<HloInstruction* const> instruction_sequence) const;\n+      absl::Span<HloInstruction* const> instruction_sequence,\n+      std::optional<InlinedInstructionMap*> inline_map);\n \n   bool ShouldInline(const CallGraph& call_graph,\n                     HloInstruction* instruction) const;\n@@ -95,6 +101,21 @@ class CallInliner : public HloModulePass {\n       should_inline_;\n };\n \n+// Returns true if the computation has instructions that are inlinable.\n+bool IsInlineableComputation(HloComputation* computation);\n+\n+struct InlinedModule {\n+  std::unique_ptr<HloModule> module;\n+  std::unique_ptr<HloCloneContext> clone_context;\n+  CallInliner::InlinedInstructionMap clone_inlined_map;\n+  const HloInstruction* get_inlined_inst(const HloInstruction* inst);\n+};\n+\n+// Given a module, this function first clones the module, then inlines the\n+// module, and returns the inlined module, clone context and inlined map in\n+// InlinedModule struct.\n+absl::StatusOr<InlinedModule> GetInlinedModule(HloModule* module);\n+\n }  // namespace xla\n \n #endif  // XLA_SERVICE_CALL_INLINER_H_"
        },
        {
            "sha": "97a042803e04f715cc7eafe532d30629acf28424",
            "filename": "third_party/xla/xla/service/call_inliner_test.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45f8503c6d8b9f8c629c64be8f0b17215f05e642/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner_test.cc?ref=45f8503c6d8b9f8c629c64be8f0b17215f05e642",
            "patch": "@@ -861,6 +861,36 @@ ENTRY main {\n   EXPECT_EQ(root->to_apply()->root_instruction()->metadata().op_name(), \"\");\n }\n \n+TEST_F(CallInlinerTest, GetInlinedModule) {\n+  const char* hlo = R\"(\n+\n+reducer {\n+  x = f32[] parameter(0)\n+  y = f32[] parameter(1)\n+  ROOT add = f32[] add(x, y)\n+}\n+\n+callee {\n+  input = f32[128,32] parameter(0)\n+  const = f32[] constant(0)\n+  ROOT reduce = f32[128] reduce(input, const), dimensions={1}, to_apply=reducer, metadata={op_name=\"reduce\"}\n+}\n+\n+ENTRY main {\n+  input = f32[128,32] parameter(0)\n+  ROOT result = f32[128] call(input), to_apply=callee, metadata={op_name=\"x\"}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> m,\n+                          ParseAndReturnVerifiedModule(hlo));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto inlined_module, GetInlinedModule(m.get()));\n+  auto root = inlined_module.module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, op::Reduce());\n+  EXPECT_EQ(root->metadata().op_name(), \"x/reduce\");\n+  EXPECT_EQ(root->to_apply()->root_instruction()->metadata().op_name(), \"\");\n+}\n+\n TEST_F(CallInlinerTest, InliningDoesNotDuplicateLongOpNames) {\n   const char* hlo = R\"(\n callee {"
        }
    ],
    "stats": {
        "total": 125,
        "additions": 115,
        "deletions": 10
    }
}