{
    "author": "tensorflower-gardener",
    "message": "Reverts 6d3c0f702ff1a90769541228ac10ba1fa5774aa8\n\nPiperOrigin-RevId: 846131788",
    "sha": "e560901dcd27f09dccdb8ec6c26ec4ad46d0ab79",
    "files": [
        {
            "sha": "f2126b3e9ad1bae51d49757a97e6a2ddb5106638",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 58,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e560901dcd27f09dccdb8ec6c26ec4ad46d0ab79/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e560901dcd27f09dccdb8ec6c26ec4ad46d0ab79/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=e560901dcd27f09dccdb8ec6c26ec4ad46d0ab79",
            "patch": "@@ -1029,64 +1029,10 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"nvshmem_memory_allocator_if_builtin_used\",\n-    tags = [\n-        \"cuda-only\",\n-        \"gpu\",\n-    ],\n-    deps = select({\n-        \"//xla/stream_executor/cuda:no_builtin_used\": [\n-            \":nvshmem_memory_allocator_stub\",\n-        ],\n-        \"//conditions:default\": [\":nvshmem_memory_allocator\"],\n-    }),\n-)\n-\n-cc_library(\n-    name = \"nvshmem_memory_allocator_if_supported\",\n-    hdrs = [\"nvshmem_memory_allocator.h\"],\n-    tags = [\n-        \"cuda-only\",\n-        \"gpu\",\n-    ],\n-    deps = select({\n-        \"//xla/stream_executor/cuda:nvshmem_supported\": [\n-            \":nvshmem_memory_allocator_if_builtin_used\",\n-        ],\n-        \"//conditions:default\": [\":nvshmem_memory_allocator_stub\"],\n-    }) + [\n-        \"//xla/stream_executor:memory_allocation\",\n-        \"//xla/stream_executor:memory_allocator\",\n-        \"@com_google_absl//absl/status:statusor\",\n-    ],\n-)\n-\n-# Used when NVSHMEM is not linked or can't be used.\n-cc_library(\n-    name = \"nvshmem_memory_allocator_stub\",\n-    srcs = [\n-        \"nvshmem_memory_allocator.h\",\n-        \"nvshmem_memory_allocator_stub.cc\",\n-    ],\n-    tags = [\n-        \"cuda-only\",\n-        \"gpu\",\n-    ],\n-    deps = [\n-        \"//xla/stream_executor:memory_allocation\",\n-        \"//xla/stream_executor:memory_allocator\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-    ],\n-)\n-\n cc_library(\n     name = \"nvshmem_memory_allocator\",\n-    srcs = [\n-        \"nvshmem_memory_allocator.cc\",\n-        \"nvshmem_memory_allocator.h\",\n-    ],\n+    srcs = [\"nvshmem_memory_allocator.cc\"],\n+    hdrs = [\"nvshmem_memory_allocator.h\"],\n     tags = [\n         \"cuda-only\",\n         \"gpu\",\n@@ -1321,10 +1267,11 @@ cc_library(\n         \":cuda_timer\",\n         \":cuda_version_parser\",\n         \":cudnn_api_wrappers\",\n-        \":nccl_memory_allocator\",\n-        \":nvshmem_memory_allocator_if_supported\",\n         \":tma_util\",\n         \"//xla:util\",\n+        \"//xla/backends/gpu/collectives:gpu_collectives\",\n+        \"//xla/core/collectives\",\n+        \"//xla/core/collectives:collectives_registry\",\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:command_buffer\","
        },
        {
            "sha": "32d6ef67a058ddf6b3dc37c5900144201b18810c",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 12,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e560901dcd27f09dccdb8ec6c26ec4ad46d0ab79/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e560901dcd27f09dccdb8ec6c26ec4ad46d0ab79/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=e560901dcd27f09dccdb8ec6c26ec4ad46d0ab79",
            "patch": "@@ -51,6 +51,9 @@ limitations under the License.\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n #include \"third_party/gpus/cuda/include/driver_types.h\"\n #include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n+#include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/core/collectives/collectives.h\"\n+#include \"xla/core/collectives/collectives_registry.h\"\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n@@ -66,8 +69,6 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_timer.h\"\n #include \"xla/stream_executor/cuda/cuda_version_parser.h\"\n #include \"xla/stream_executor/cuda/cudnn_api_wrappers.h\"\n-#include \"xla/stream_executor/cuda/nccl_memory_allocator.h\"\n-#include \"xla/stream_executor/cuda/nvshmem_memory_allocator.h\"\n #include \"xla/stream_executor/cuda/tma_util.h\"\n #include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -840,6 +841,14 @@ CudaExecutor::~CudaExecutor() {\n   CHECK(gpu_binary_to_module_.empty()) << \"CudaExecutor has loaded modules.\";\n }\n \n+absl::StatusOr<xla::gpu::GpuCollectives*> GetGpuCollectives(\n+    StreamExecutor* executor) {\n+  std::unique_ptr<ActivateContext> activation = executor->Activate();\n+  TF_ASSIGN_OR_RETURN(xla::Collectives * collectives,\n+                      xla::CollectivesRegistry::Default(\"gpu\"));\n+  return tsl::down_cast<xla::gpu::GpuCollectives*>(collectives);\n+}\n+\n CudaExecutor::VmmMemoryHandle::~VmmMemoryHandle() { CHECK_OK(Release()); }\n \n absl::Status CudaExecutor::VmmMemoryHandle::Release() {\n@@ -970,6 +979,27 @@ absl::StatusOr<bool> CudaExecutor::VmmDeallocateMemory(void* ptr) {\n   return true;\n }\n \n+absl::StatusOr<void*> CollectiveMemoryAllocate(StreamExecutor* executor,\n+                                               uint64_t bytes) {\n+  if (bytes == 0) {\n+    return nullptr;\n+  }\n+\n+  std::unique_ptr<ActivateContext> activation = executor->Activate();\n+  TF_ASSIGN_OR_RETURN(xla::gpu::GpuCollectives * gpu_collectives,\n+                      GetGpuCollectives(executor));\n+  return gpu_collectives->Allocate(bytes);\n+}\n+\n+absl::Status CollectiveMemoryDeallocate(StreamExecutor* executor,\n+                                        void* location) {\n+  std::unique_ptr<ActivateContext> activation = executor->Activate();\n+\n+  TF_ASSIGN_OR_RETURN(xla::gpu::GpuCollectives * gpu_collectives,\n+                      GetGpuCollectives(executor));\n+  return gpu_collectives->Deallocate(location);\n+}\n+\n absl::StatusOr<std::unique_ptr<MemoryAllocator>>\n CudaExecutor::CreateMemoryAllocator(MemorySpace type) {\n   if (type == MemorySpace::kUnified) {\n@@ -1005,16 +1035,28 @@ CudaExecutor::CreateMemoryAllocator(MemorySpace type) {\n   }\n \n   if (type == MemorySpace::kCollective) {\n-    switch (collective_allocator_type_) {\n-      case CollectiveAllocatorType::kNvshmem:\n-        return std::make_unique<NvshmemMemoryAllocator>();\n-      case CollectiveAllocatorType::kNccl:\n-        return std::make_unique<NcclMemoryAllocator>(this);\n-      default:\n-        return absl::UnimplementedError(\n-            absl::StrCat(\"Unsupported collective allocator type: \",\n-                         collective_allocator_type_));\n-    }\n+    // TODO(469289220): Use NCCL/NVSHMEM memory allocator here instead.\n+    return std::make_unique<GenericMemoryAllocator>(\n+        [this](uint64_t size)\n+            -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n+          TF_ASSIGN_OR_RETURN(void* ptr, CollectiveMemoryAllocate(this, size));\n+          XLA_VLOG_DEVICE(2, device_ordinal())\n+              << \"allocated \" << ptr << \" for context \" << cuda_context_\n+              << \" of \" << size << \" bytes of collective memory\";\n+          return std::make_unique<GenericMemoryAllocation>(\n+              ptr, size, [this](void* location, uint64_t size) {\n+                auto status = CollectiveMemoryDeallocate(this, location);\n+                if (!status.ok()) {\n+                  XLA_LOG_DEVICE(ERROR, device_ordinal())\n+                      << \"failed to free collective memory at \" << location\n+                      << \"; result: \" << status;\n+                } else {\n+                  XLA_VLOG_DEVICE(2, device_ordinal())\n+                      << \"deallocated collective memory at \" << location\n+                      << \" for context \" << cuda_context_;\n+                }\n+              });\n+        });\n   }\n \n   if (type == MemorySpace::kHost) {\n@@ -1363,6 +1405,17 @@ DeviceAddressBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n       << \"CudaExecutor::Allocate size: \" << size\n       << \" memory_space: \" << memory_space;\n \n+  if (memory_space == static_cast<int64_t>(MemorySpace::kCollective)) {\n+    auto result = CollectiveMemoryAllocate(this, size);\n+    if (!result.ok()) {\n+      XLA_LOG_DEVICE(ERROR, device_ordinal())\n+          << \"CudaExecutor::Allocate returns \" << result.value();\n+    }\n+    XLA_VLOG_DEVICE(1, device_ordinal())\n+        << \"CudaExecutor::Allocate returns \" << result.value();\n+    return DeviceAddressBase(result.value(), size);\n+  }\n+\n   if (memory_space == static_cast<int64_t>(MemorySpace::kHost)) {\n     auto result = HostAllocate(cuda_context_, numa_node_, size);\n     if (!result.ok()) {"
        },
        {
            "sha": "d4d124b89af8dbaa281754635420253f1ad2283a",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvshmem_memory_allocator_stub.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c32a65652c2312c37dcaede1c11c5f0bca70dbc/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem_memory_allocator_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c32a65652c2312c37dcaede1c11c5f0bca70dbc/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem_memory_allocator_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem_memory_allocator_stub.cc?ref=8c32a65652c2312c37dcaede1c11c5f0bca70dbc",
            "patch": "@@ -1,29 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include <cstdint>\n-#include <memory>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/cuda/nvshmem_memory_allocator.h\"\n-#include \"xla/stream_executor/memory_allocation.h\"\n-\n-namespace stream_executor::gpu {\n-absl::StatusOr<std::unique_ptr<MemoryAllocation>>\n-NvshmemMemoryAllocator::Allocate(uint64_t size) {\n-  return absl::UnimplementedError(\"NVSHMEM is not supported on this platform.\");\n-}\n-}  // namespace stream_executor::gpu"
        }
    ],
    "stats": {
        "total": 169,
        "additions": 70,
        "deletions": 99
    }
}