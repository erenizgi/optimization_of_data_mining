{
    "author": "tensorflower-gardener",
    "message": "Integrate Triton up to [de2ba394](https://github.com/openai/triton/commits/de2ba3946bc2a7a55ad77331ed60fd7c685156bf)\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-\n\nPiperOrigin-RevId: 819807700",
    "sha": "6c321062383cbebd9de2441f2e76f893e9d0a364",
    "files": [
        {
            "sha": "656b9c894904d8511d2c54523fd3883f09f6e876",
            "filename": "third_party/xla/third_party/triton/llvm_integration/series.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl?ref=6c321062383cbebd9de2441f2e76f893e9d0a364",
            "patch": "@@ -8,9 +8,5 @@ LLVM nor MLIR integrator, please do not add any patches to this list.\n \"\"\"\n \n llvm_patch_list = [\n-    \"//third_party/triton:llvm_integration/cl801607173.patch\",\n-    \"//third_party/triton:llvm_integration/cl808150672.patch\",\n-    \"//third_party/triton:llvm_integration/cl809972027.patch\",\n-    \"//third_party/triton:llvm_integration/cl812994567.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "084249e632d1a9b3232c3b9503f37d9187bc60b7",
            "filename": "third_party/xla/third_party/triton/temporary/cherrypick_dominance_fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fcherrypick_dominance_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fcherrypick_dominance_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fcherrypick_dominance_fix.patch?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -1,19 +0,0 @@\n-This is a cherry-pick of https://github.com/triton-lang/triton/pull/7719 which\n-should make it into the next integration so this patch can simply be deleted\n-next integrate.\n-\n-diff --git a/lib/Dialect/TritonGPU/Transforms/HoistTMEMAlloc.cpp b/lib/Dialect/TritonGPU/Transforms/HoistTMEMAlloc.cpp\n---- a/lib/Dialect/TritonGPU/Transforms/HoistTMEMAlloc.cpp\n-+++ b/lib/Dialect/TritonGPU/Transforms/HoistTMEMAlloc.cpp\n-@@ -181,6 +181,11 @@ public:\n-       return failure();\n-     if (alloc->getBlock() != store->getBlock())\n-       return failure();\n-+    if (auto srcDef = store.getSrc().getDefiningOp()) {\n-+      if (alloc->getBlock() == srcDef->getBlock() &&\n-+          alloc->isBeforeInBlock(srcDef))\n-+        return failure();\n-+    }\n-     alloc.getSrcMutable().assign(store.getSrc());\n-     rewriter.replaceOp(store, alloc.getToken());\n-     return success();"
        },
        {
            "sha": "f933f2a8946e9c413d6ac137a7433c01feb4f842",
            "filename": "third_party/xla/third_party/triton/temporary/construction_order.patch",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconstruction_order.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconstruction_order.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconstruction_order.patch?ref=6c321062383cbebd9de2441f2e76f893e9d0a364",
            "patch": "@@ -0,0 +1,23 @@\n+\n+--- a/third_party/nvidia/lib/Dialect/NVWS/Transforms/InsertTmemAref.cpp\t2025-09-25 06:36:50.000000000 -0700\n++++ b/third_party/nvidia/lib/Dialect/NVWS/Transforms/InsertTmemAref.cpp\t2025-09-29 04:55:20.000000000 -0700\n+@@ -54,8 +54,8 @@\n+     SmallVector<std::unique_ptr<Node>> subDags;\n+     Node(Operation *op, OpOperand *tokOperand,\n+          std::optional<PartitionId> partitionId, Node *parent)\n+-        : op(op), tokOperand(tokOperand), partitionId(partitionId),\n+-          parent(parent), parentDag(nullptr) {}\n++        : op(op), tokOperand(tokOperand), parent(parent), parentDag(nullptr),\n++          partitionId(partitionId) {}\n+ \n+     // ------------------------------------------------------------------------\n+ \n+@@ -364,7 +364,7 @@\n+   enum Kind { PUT, GET };\n+ \n+   TMEMAref(Value aref, Value origBuffer, Value replToken)\n+-      : aref(aref), origBuffer(origBuffer), replToken(replToken), kind(PUT) {}\n++      : origBuffer(origBuffer), aref(aref), replToken(replToken), kind(PUT) {}\n+ \n+   void acquire(OpBuilder &b, Location loc,\n+                std::pair<std::optional<PartitionId>, StageCluster>"
        },
        {
            "sha": "74e7e69d8d29487dfe53b7607fe61d6e8e14c03d",
            "filename": "third_party/xla/third_party/triton/temporary/convert_layout_op_to_llvm_small_width.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width.patch?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -1,71 +0,0 @@\n-This fix should be upstreamed.\n-\n---- a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-08-28 04:30:50.000000000 -0700\n-+++ b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-09-10 06:50:50.000000000 -0700\n-@@ -331,11 +331,11 @@\n-     // Pack registers if possible.\n-     int elemsPerVec = 1 << nPack;\n-     int bitsPerVecElem = 32 / elemsPerVec;\n-+    bool packTwo16Bit = bitwidth < bitsPerVecElem && elemsPerVec == 2;\n-     if (elemsPerVec > 1) {\n-       SmallVector<Value> packedVals;\n-       packedVals.reserve(regDim / elemsPerVec);\n--      if (bitwidth < bitsPerVecElem) {\n--        // Should have bitsPerVecElem == 16 here.\n-+      if (packTwo16Bit) {\n-         for (int i = 0; i < regDim; i += elemsPerVec) {\n-           Value x0 = b.zext(i32_ty, b.bitcast(inVals[i], int_ty(bitwidth)));\n-           Value x1 = b.zext(i32_ty, b.bitcast(inVals[i + 1], int_ty(bitwidth)));\n-@@ -343,6 +343,10 @@\n-           packedVals.emplace_back(b.or_(x0, x1));\n-         }\n-       } else {\n-+        // For small types, we need to extend the values to i8.\n-+        if (bitwidth < 8) {\n-+          llvm::for_each(inVals, [&](Value& v) { v = b.zext(i8_ty, v); });\n-+        }\n-         for (int i = 0; i < regDim; i += elemsPerVec) {\n-           auto slice = ArrayRef<Value>(inVals).slice(i, elemsPerVec);\n-           Value v = packLLVector(loc, slice, rewriter);\n-@@ -376,9 +380,21 @@\n-     if (elemsPerVec > 1) {\n-       SmallVector<Value> unpackedVals;\n-       unpackedVals.reserve(regDim);\n--      if (bitwidth >= bitsPerVecElem) {\n-+      if (packTwo16Bit) {\n-+        for (auto packedVal : outVals) {\n-+          Value x0 =\n-+              b.trunc(int_ty(bitwidth), b.and_(packedVal, b.i32_val(0xFF)));\n-+          Value x1 =\n-+              b.trunc(int_ty(bitwidth), b.lshr(packedVal, b.i32_val(16)));\n-+          unpackedVals.push_back(b.bitcast(x0, elemTy));\n-+          unpackedVals.push_back(b.bitcast(x1, elemTy));\n-+        }\n-+      } else {\n-         auto packedTy =\n-             bitwidth < bitsPerVecElem ? int_ty(bitsPerVecElem) : elemTy;\n-+        if (bitwidth < 8) {\n-+          packedTy = i8_ty;\n-+        }\n-         auto vecTy = vec_ty(packedTy, elemsPerVec);\n-         auto unpackVal = [&](Value v) {\n-           v = b.bitcast(v, vecTy);\n-@@ -388,14 +404,10 @@\n-           auto unpacked = unpackVal(v);\n-           unpackedVals.append(unpacked.begin(), unpacked.end());\n-         }\n--      } else {\n--        for (auto packedVal : outVals) {\n--          Value x0 =\n--              b.trunc(int_ty(bitwidth), b.and_(packedVal, b.i32_val(0xFF)));\n--          Value x1 =\n--              b.trunc(int_ty(bitwidth), b.lshr(packedVal, b.i32_val(16)));\n--          unpackedVals.push_back(b.bitcast(x0, elemTy));\n--          unpackedVals.push_back(b.bitcast(x1, elemTy));\n-+        if (bitwidth < 8) {\n-+          // Truncate the values to the original bitwidth from i8.\n-+          llvm::for_each(unpackedVals,\n-+                         [&](Value& v) { v = b.trunc(elemTy, v); });\n-         }\n-       }\n-       outVals = std::move(unpackedVals);"
        },
        {
            "sha": "9748aa50da77cdb1e98a8c1dd6791fe40233e0e1",
            "filename": "third_party/xla/third_party/triton/temporary/convert_layout_op_to_llvm_small_width_2.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 72,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width_2.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width_2.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width_2.patch?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -1,72 +0,0 @@\n-# patch to match https://github.com/triton-lang/triton/pull/8155\n-\n---- a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-09-11 02:55:30.000000000 -0700\n-+++ b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-09-11 23:33:17.000000000 -0700\n-@@ -331,11 +331,11 @@\n-     // Pack registers if possible.\n-     int elemsPerVec = 1 << nPack;\n-     int bitsPerVecElem = 32 / elemsPerVec;\n--    bool packTwo16Bit = bitwidth < bitsPerVecElem && elemsPerVec == 2;\n-     if (elemsPerVec > 1) {\n-       SmallVector<Value> packedVals;\n-       packedVals.reserve(regDim / elemsPerVec);\n--      if (packTwo16Bit) {\n-+      if (bitwidth == 8 && bitsPerVecElem == 16) {\n-+        // TODO: Can remove `if` part of `if-else` once ptxas bugfix lands.\n-         for (int i = 0; i < regDim; i += elemsPerVec) {\n-           Value x0 = b.zext(i32_ty, b.bitcast(inVals[i], int_ty(bitwidth)));\n-           Value x1 = b.zext(i32_ty, b.bitcast(inVals[i + 1], int_ty(bitwidth)));\n-@@ -343,9 +343,12 @@\n-           packedVals.emplace_back(b.or_(x0, x1));\n-         }\n-       } else {\n--        // For small types, we need to extend the values to i8.\n--        if (bitwidth < 8) {\n--          llvm::for_each(inVals, [&](Value& v) { v = b.zext(i8_ty, v); });\n-+        if (bitwidth < bitsPerVecElem) {\n-+          for (Value &v : inVals) {\n-+            if (elemTy != int_ty(bitwidth))\n-+              v = b.bitcast(v, int_ty(bitwidth));\n-+            v = b.zext(int_ty(bitsPerVecElem), v);\n-+          }\n-         }\n-         for (int i = 0; i < regDim; i += elemsPerVec) {\n-           auto slice = ArrayRef<Value>(inVals).slice(i, elemsPerVec);\n-@@ -380,21 +383,8 @@\n-     if (elemsPerVec > 1) {\n-       SmallVector<Value> unpackedVals;\n-       unpackedVals.reserve(regDim);\n--      if (packTwo16Bit) {\n--        for (auto packedVal : outVals) {\n--          Value x0 =\n--              b.trunc(int_ty(bitwidth), b.and_(packedVal, b.i32_val(0xFF)));\n--          Value x1 =\n--              b.trunc(int_ty(bitwidth), b.lshr(packedVal, b.i32_val(16)));\n--          unpackedVals.push_back(b.bitcast(x0, elemTy));\n--          unpackedVals.push_back(b.bitcast(x1, elemTy));\n--        }\n--      } else {\n-         auto packedTy =\n-             bitwidth < bitsPerVecElem ? int_ty(bitsPerVecElem) : elemTy;\n--        if (bitwidth < 8) {\n--          packedTy = i8_ty;\n--        }\n-         auto vecTy = vec_ty(packedTy, elemsPerVec);\n-         auto unpackVal = [&](Value v) {\n-           v = b.bitcast(v, vecTy);\n-@@ -404,10 +394,11 @@\n-           auto unpacked = unpackVal(v);\n-           unpackedVals.append(unpacked.begin(), unpacked.end());\n-         }\n--        if (bitwidth < 8) {\n--          // Truncate the values to the original bitwidth from i8.\n--          llvm::for_each(unpackedVals,\n--                         [&](Value& v) { v = b.trunc(elemTy, v); });\n-+      if (bitwidth < bitsPerVecElem) {\n-+        for (Value &v : unpackedVals) {\n-+          v = b.trunc(int_ty(bitwidth), v);\n-+          if (elemTy != int_ty(bitwidth))\n-+            v = b.bitcast(v, elemTy);\n-         }\n-       }\n-       outVals = std::move(unpackedVals);"
        },
        {
            "sha": "100913dd425573a7912ebc3c5d319e41a6af9dab",
            "filename": "third_party/xla/third_party/triton/temporary/launcher_overflow_fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_overflow_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_overflow_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_overflow_fix.patch?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -1,15 +0,0 @@\n-Merge with launcher.patch\n-\n---- a/third_party/nvidia/backend/cuda_utils.cc\n-+++ b/third_party/nvidia/backend/cuda_utils.cc\n-@@ -605,8 +605,8 @@\n-     return nullptr;\n-   }\n- \n--  // +1 for the global scratch pointer.\n--  std::size_t num_params = signature_metadata.size() + 1;\n-+  // +2 for the global scratch pointer and profile scratch pointer.\n-+  std::size_t num_params = signature_metadata.size() + 2;\n-   // Use alloca to set up kernel parameters on the stack and avoid dynamic\n-   // memory allocations.\n-   config.params = static_cast<void**>(alloca(num_params * sizeof(void*)));"
        },
        {
            "sha": "4fa55269e3323cf15808181679605105d6e2a224",
            "filename": "third_party/xla/third_party/triton/temporary/series.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl?ref=6c321062383cbebd9de2441f2e76f893e9d0a364",
            "patch": "@@ -14,11 +14,5 @@ those to this list.\n \"\"\"\n \n temporary_patch_list = [\n-    \"//third_party/triton:temporary/verify_nvmma_encoding.patch\",\n-    \"//third_party/triton:temporary/triton-tensor-layout-init-fiasco.patch\",\n-    \"//third_party/triton:temporary/launcher_overflow_fix.patch\",\n-    \"//third_party/triton:temporary/convert_layout_op_to_llvm_small_width.patch\",\n-    \"//third_party/triton:temporary/convert_layout_op_to_llvm_small_width_2.patch\",\n-    \"//third_party/triton:temporary/cherrypick_dominance_fix.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "53dcecea088c03989f4e4dead4a2808bdbc3ea16",
            "filename": "third_party/xla/third_party/triton/temporary/type-fix-in-test.patch",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftype-fix-in-test.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftype-fix-in-test.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftype-fix-in-test.patch?ref=6c321062383cbebd9de2441f2e76f893e9d0a364",
            "patch": "@@ -0,0 +1,28 @@\n+This test was broken on it's first integration already. This can be upstreamed.\n+--- a/test/LLVMIR/convert-to-llvmir-with-dbg-info.mlir\t2025-09-25 06:36:50.000000000 -0700\n++++ b/test/LLVMIR/convert-to-llvmir-with-dbg-info.mlir\t2025-10-02 02:23:37.000000000 -0700\n+@@ -29,6 +29,7 @@\n+                         %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: !llvm.ptr<1>) {\n+     %constant_i32 = llvm.mlir.constant(9 : i32) : i32\n+     %constant_i16 = llvm.mlir.constant(0 : i16) : i16\n++    %constant_i64 = llvm.mlir.constant(0 : i64) : i64\n+ \n+     // CHECK: !DILocalVariable(name: \"pid\", scope:\n+     %pid = rocdl.workgroup.id.x : i32 loc(#loc14)\n+@@ -49,14 +50,14 @@\n+ \n+     // CHECK: !DILocalVariable(name: \"x\", scope:\n+     %x_ptr = llvm.getelementptr %arg0[%block_start] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32\n+-    %x_buffer_ptr = rocdl.make.buffer.rsrc %x_ptr, %constant_i16, %constant_i32, %constant_i32 : <1> to <8> loc(#loc18)\n++    %x_buffer_ptr = rocdl.make.buffer.rsrc %x_ptr, %constant_i16, %constant_i64, %constant_i32 : <1> to <8> loc(#loc18)\n+     llvm.intr.dbg.value #di_local_variable4 = %x_buffer_ptr : !llvm.ptr<8> loc(#loc8)\n+     %x_val = rocdl.raw.ptr.buffer.load %x_buffer_ptr, %mask_i1, %constant_i32, %constant_i32 : vector<4xf32> loc(#loc18)\n+     %x_scalar = llvm.extractelement %x_val[%constant_i32 : i32] : vector<4xf32> loc(#loc18)\n+ \n+     // CHECK: !DILocalVariable(name: \"y\", scope:\n+     %y_ptr = llvm.getelementptr %arg1[%block_start] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32\n+-    %y_buffer_ptr = rocdl.make.buffer.rsrc %y_ptr, %constant_i16, %constant_i32, %constant_i32 : <1> to <8> loc(#loc19)\n++    %y_buffer_ptr = rocdl.make.buffer.rsrc %y_ptr, %constant_i16, %constant_i64, %constant_i32 : <1> to <8> loc(#loc19)\n+     llvm.intr.dbg.value #di_local_variable5 = %y_buffer_ptr : !llvm.ptr<8> loc(#loc10)\n+     %y_val = rocdl.raw.ptr.buffer.load %y_buffer_ptr, %mask_i1, %constant_i32, %constant_i32 : vector<4xf32> loc(#loc19)\n+     %y_scalar = llvm.extractelement %y_val[%constant_i32 : i32] : vector<4xf32> loc(#loc19)"
        },
        {
            "sha": "fe2cfd33f7662fd45f3ec9526aa00459a8ad6452",
            "filename": "third_party/xla/third_party/triton/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl?ref=6c321062383cbebd9de2441f2e76f893e9d0a364",
            "patch": "@@ -7,8 +7,8 @@ load(\"//third_party/triton:temporary/series.bzl\", \"temporary_patch_list\")\n def repo():\n     \"\"\"Imports Triton.\"\"\"\n \n-    TRITON_COMMIT = \"triton_integrate_branch-1.12\"\n-    TRITON_SHA256 = \"6754c1c474c58916c1ddd88ceb1adb2a553ec3609afbe5fec936902a0297a7ad\"\n+    TRITON_COMMIT = \"triton_integrate_branch-1.13\"\n+    TRITON_SHA256 = \"390ce756b3e0ce7be0a69633897f11bfd3227682ad90bd720fe4860bfedc4849\"\n     tf_http_archive(\n         name = \"triton\",\n         sha256 = TRITON_SHA256,"
        },
        {
            "sha": "4ecc62a998f6ab2f65b5eec2b0cd122af9fba79d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc?ref=6c321062383cbebd9de2441f2e76f893e9d0a364",
            "patch": "@@ -115,15 +115,15 @@ static void MakeTTGIR(mlir::OpPassManager* pm,\n       mt::gpu::createTritonGPUOptimizeDotOperands({cuda_cc.IsAtLeastAmpere()}));\n   pm->addPass(mt::gpu::createTritonGPUCoalesceAsyncCopy());\n   pm->addPass(ttng::createTritonNvidiaGPUOptimizeTMemLayoutsPass());\n+  if (cuda_cc.IsAtLeastHopper()) {\n+    pm->addPass(ttng::createTritonNvidiaGPUTMALoweringPass());\n+  }\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   pm->addPass(ttng::createTritonNvidiaGPUInterleaveTMemPass());\n   pm->addPass(mt::gpu::createTritonGPUReduceDataDuplication());\n   pm->addPass(mt::gpu::createTritonGPUReorderInstructions());\n   pm->addPass(mt::createTritonLoopAwareCSE());\n   pm->addPass(mlir::createSymbolDCEPass());\n-  if (cuda_cc.IsAtLeastHopper()) {\n-    pm->addPass(ttng::createTritonNvidiaGPUTMALoweringPass());\n-  }\n   pm->addPass(ttng::createTritonGPUFenceInsertion({cuda_cc_as_int}));\n   pm->addPass(ttng::createTritonNvidiaGPUMMALoweringPass());\n   pm->addPass(mlir::createSCCPPass());"
        },
        {
            "sha": "9305220c9bfb14b873ee835cd111e653507dc000",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c321062383cbebd9de2441f2e76f893e9d0a364/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h?ref=6c321062383cbebd9de2441f2e76f893e9d0a364",
            "patch": "@@ -32,7 +32,7 @@ class AutotuneCacheKey {\n   // Tie a version to the cache key in order to invalidate the cache when\n   // necessary. This should be incremented on triton upgrades or any other\n   // changes that may affect the autotuning results.\n-  static constexpr int kCurrentVersion = 13;\n+  static constexpr int kCurrentVersion = 14;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n                    const HloInstruction& instruction,"
        }
    ],
    "stats": {
        "total": 250,
        "additions": 57,
        "deletions": 193
    }
}