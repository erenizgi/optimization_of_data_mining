{
    "author": "ermilovmaxim",
    "message": "Port to new GpuComputeCapability API\n\nPiperOrigin-RevId: 821845460",
    "sha": "361f1c64eb138c197be30e0f91cfb13f042e6c7b",
    "files": [
        {
            "sha": "969286250aa1e9030c1f80d479551507656f8a34",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -66,12 +66,11 @@ FileBasedAutotunerCache::FileBasedAutotunerCache(\n std::string FileBasedAutotunerCache::DeviceDescriptionToString(\n     const se::DeviceDescription& device_desc) {\n   std::string compute_capability;\n-  if (auto* ccc = std::get_if<se::CudaComputeCapability>(\n-          &device_desc.gpu_compute_capability())) {\n+  if (auto* ccc =\n+          device_desc.gpu_compute_capability().cuda_compute_capability()) {\n     compute_capability = absl::StrCat(\"CUDA: \", ccc->major, \".\", ccc->minor);\n   } else {\n-    auto* rcc = std::get_if<se::RocmComputeCapability>(\n-        &device_desc.gpu_compute_capability());\n+    auto* rcc = device_desc.gpu_compute_capability().rocm_compute_capability();\n     compute_capability = absl::StrCat(\"ROCM: \", rcc->gfx_version());\n   }\n "
        },
        {
            "sha": "6d1d4b3a9e436562f3e0f3e743ccd3536c3f6f18",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -59,15 +59,15 @@ namespace gpu {\n namespace {\n std::vector<TritonGemmConfig> GetDefaultTritonConfigs(\n     se::GpuComputeCapability compute_capability, bool autotune_tma) {\n-  if (std::holds_alternative<se::CudaComputeCapability>(compute_capability)) {\n-    auto cuda_compute_capability =\n-        std::get<se::CudaComputeCapability>(compute_capability);\n+  if (compute_capability.IsCuda()) {\n+    auto* cuda_compute_capability =\n+        compute_capability.cuda_compute_capability();\n     std::vector<TritonGemmConfig> configs;\n \n-    if (cuda_compute_capability.IsAtLeastBlackwell()) {\n+    if (cuda_compute_capability->IsAtLeastBlackwell()) {\n       configs = *kBlackwellConfigs;\n-    } else if (cuda_compute_capability.IsHopper() ||\n-               cuda_compute_capability.IsAmpere()) {\n+    } else if (cuda_compute_capability->IsHopper() ||\n+               cuda_compute_capability->IsAmpere()) {\n       configs = *kHopperAmpereConfigs;\n     } else {\n       configs = *kDefaultCudaConfigs;\n@@ -88,7 +88,7 @@ std::vector<TritonGemmConfig> GetDefaultTritonConfigs(\n     }\n     return tma_parameterized_configs;\n   }\n-  if (std::holds_alternative<se::RocmComputeCapability>(compute_capability)) {\n+  if (compute_capability.IsRocm()) {\n     return *kDefaultRocmConfigs;\n   }\n   return {};"
        },
        {
            "sha": "5c01a71985d650409bc85f83e9f83ed1156f8bd6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -505,15 +505,14 @@ void AddLoweringPasses(mlir::OpPassManager& pm,\n   pm.addPass(mlir::createCSEPass());\n \n   // This pass has to run before `ExpandFloatOpsPass`.\n-  if (auto* cc = std::get_if<se::CudaComputeCapability>(\n-          &device.gpu_compute_capability())) {\n+  if (auto* cc = device.gpu_compute_capability().cuda_compute_capability()) {\n     se::SemanticVersion ptx_version =\n         nvptx::DetermineHighestSupportedPtxVersionFromCudaVersion(\n             device.runtime_version());\n     pm.addPass(CreateConvertFloatNvidiaPass(\n         cc->major, cc->minor, ptx_version.major(), ptx_version.minor()));\n-  } else if (auto* cc = std::get_if<se::RocmComputeCapability>(\n-                 &device.gpu_compute_capability())) {\n+  } else if (auto* cc =\n+                 device.gpu_compute_capability().rocm_compute_capability()) {\n     if (cc->has_fp8_support()) {\n       pm.addPass(CreateConvertFloatAMDPass(*cc));\n     }"
        },
        {
            "sha": "f8d2ddd5a6db6ebb46159344678d3ad66ca540f7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -871,6 +871,7 @@ xla_test(\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"//xla/tests:test_utils\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n         \"//xla/tsl/lib/core:status_test_util\","
        },
        {
            "sha": "68408307a644bf01935c63bce9dca5d1a6667d07",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -36,7 +36,7 @@ void CreateTritonXlaPipeline(\n   pm->addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n \n-  auto* cuda_cc = std::get_if<stream_executor::CudaComputeCapability>(&gpu_cc);\n+  auto* cuda_cc = gpu_cc.cuda_compute_capability();\n   bool is_at_least_hopper = cuda_cc != nullptr && cuda_cc->IsAtLeastHopper();\n \n   if (rewrite_int4) {\n@@ -75,15 +75,13 @@ void CreateTritonPipeline(\n     const stream_executor::GpuComputeCapability& gpu_cc, int num_warps,\n     int num_ctas, int num_stages,\n     mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n-  if (auto* cuda_cc =\n-          std::get_if<stream_executor::CudaComputeCapability>(&gpu_cc)) {\n+  if (auto* cuda_cc = gpu_cc.cuda_compute_capability()) {\n     return CreateTritonCudaPipeline(pm, *cuda_cc, num_warps, num_ctas,\n                                     num_stages, out_cluster_info);\n   }\n \n-  CreateTritonRocmPipeline(\n-      pm, std::get<stream_executor::RocmComputeCapability>(gpu_cc), num_warps,\n-      num_ctas, num_stages);\n+  CreateTritonRocmPipeline(pm, *gpu_cc.rocm_compute_capability(), num_warps,\n+                           num_ctas, num_stages);\n   // There is no clusters in ROCm for now.\n   out_cluster_info.clusterDimX = 1;\n   out_cluster_info.clusterDimY = 1;"
        },
        {
            "sha": "5dc9461d79f3b3a16729d53699ca1a7d0e29392d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms_legacy_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_legacy_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -537,7 +537,7 @@ CHECK-COUNT-6:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<64x32xbf16>\n }\n \n TEST_F(Triton6xBF16GemmTest, Emit6xBF16GemmEndToEnd) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"ALG_DOT_BF16_BF16_F32_X6 not supported on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -641,7 +641,7 @@ CHECK-COUNT-3:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<64x32xbf16>\n }\n \n TEST_F(Triton3xBF16GemmTest, Emit3xBF16GemmEndToEnd) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"ALG_DOT_BF16_BF16_F32_X3 not supported on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -667,7 +667,7 @@ CHECK-NOT: mma.sync.aligned.{{.*}}.row.col.f32.tf32.tf32.f32\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X3) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -690,7 +690,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X3) {\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X6) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -713,7 +713,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X6) {\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -738,7 +738,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32) {\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32_X3) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -764,7 +764,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32) {\n   if (!SupportsBF16(GpuComputeComp())) {\n     GTEST_SKIP() << \"BF16 not supported.\";\n   }\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -1834,7 +1834,7 @@ MATCHER_P(RelativeDifferenceIsWithin, max_rel_difference, \"\") {\n }\n \n TEST_P(PrecisionTests, PrecisionCheck) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Precision tests is unknown for ROCM.\";\n   }\n \n@@ -1916,7 +1916,7 @@ TEST_P(PrecisionTests, CheckPrecisionDegradationAlongKDimension) {\n         << \"To run the test, set --v=1 and rerun the test.\\n\"\n         << \"The test is quite slow and produces output for manual inspection.\";\n   }\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Precision tests is unknown for ROCM.\";\n   }\n   Backend backend = std::get<1>(GetParam());"
        },
        {
            "sha": "6764dbb4145aa45b3ffb90008b0790662c03729a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -562,7 +562,7 @@ CHECK-COUNT-6:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<64x32xbf16>\n }\n \n TEST_F(Triton6xBF16GemmTest, Emit6xBF16GemmEndToEnd) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"ALG_DOT_BF16_BF16_F32_X6 not supported on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -666,7 +666,7 @@ CHECK-COUNT-3:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<64x32xbf16>\n }\n \n TEST_F(Triton3xBF16GemmTest, Emit3xBF16GemmEndToEnd) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"ALG_DOT_BF16_BF16_F32_X3 not supported on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -692,7 +692,7 @@ CHECK-NOT: mma.sync.aligned.{{.*}}.row.col.f32.tf32.tf32.f32\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X3) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -715,7 +715,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X3) {\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X6) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -738,7 +738,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X6) {\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -763,7 +763,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32) {\n }\n \n TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32_X3) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -789,7 +789,7 @@ TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32) {\n   if (!SupportsBF16(GpuComputeComp())) {\n     GTEST_SKIP() << \"BF16 not supported.\";\n   }\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -1866,7 +1866,7 @@ MATCHER_P(RelativeDifferenceIsWithin, max_rel_difference, \"\") {\n }\n \n TEST_P(PrecisionTests, PrecisionCheck) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Precision tests is unknown for ROCM.\";\n   }\n \n@@ -1948,7 +1948,7 @@ TEST_P(PrecisionTests, CheckPrecisionDegradationAlongKDimension) {\n         << \"To run the test, set --v=1 and rerun the test.\\n\"\n         << \"The test is quite slow and produces output for manual inspection.\";\n   }\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Precision tests is unknown for ROCM.\";\n   }\n   Backend backend = std::get<1>(GetParam());"
        },
        {
            "sha": "2dae9657d7cce459fca9c3749be89ffc728538a1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -389,8 +389,7 @@ absl::StatusOr<Value> EmitElementwiseLibdeviceFunction(\n         absl::StrCat(\"Unsupported elementwise operation \", hlo.ToString()));\n   }\n   llvm::Triple triple(\"nvptx64-unknown-unknown\");\n-  if (std::holds_alternative<se::RocmComputeCapability>(\n-          device_info.gpu_compute_capability())) {\n+  if (device_info.gpu_compute_capability().IsRocm()) {\n     triple.setTriple(\"amdgcn-unknown-unknown\");\n   }\n   llvm::SmallVector<Value, 2> casted_inputs;"
        },
        {
            "sha": "7e0b400ea039138959c22efefa209c51a56e92bc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -1957,7 +1957,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n }\n \n absl::Status CheckAtLeastAmpere(const se::GpuComputeCapability& gpu_cc) {\n-  if (auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_cc);\n+  if (auto* cuda_cc = gpu_cc.cuda_compute_capability();\n       cuda_cc != nullptr && !cuda_cc->IsAtLeastAmpere()) {\n     return absl::FailedPreconditionError(\n         absl::StrCat(\"Triton support is only enabled for Ampere GPUs (compute \",\n@@ -2092,7 +2092,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n         shared_mem_bytes, device_info.shared_memory_per_block_optin()));\n   }\n \n-  if (auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_cc);\n+  if (auto* cuda_cc = gpu_cc.cuda_compute_capability();\n       cuda_cc != nullptr && cuda_cc->IsBlackwell()) {\n     // https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-memory\n     constexpr int kTensorMemoryColumns = 512;\n@@ -2185,8 +2185,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n \n std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n                              const se::DeviceDescription& device_info) {\n-  if (std::holds_alternative<se::CudaComputeCapability>(\n-          device_info.gpu_compute_capability())) {\n+  if (device_info.gpu_compute_capability().IsCuda()) {\n     return nvptx::LibDevicePath(\n         hlo_config.debug_options().xla_gpu_cuda_data_dir());\n   }"
        },
        {
            "sha": "f32b8c5674944a926c5c14e605d6157b1d66f518",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -113,12 +113,12 @@ class TritonTest : public GpuCodegenTest {\n   }\n \n   stream_executor::GpuComputeCapability CudaAmpereOrRocm() {\n-    if (std::holds_alternative<stream_executor::RocmComputeCapability>(\n-            GpuComputeCapability())) {\n+    if (GpuComputeCapability().IsRocm()) {\n       return stream_executor::GpuComputeCapability{\n           device_desc().rocm_compute_capability()};\n     }\n-    return se::CudaComputeCapability::Ampere();\n+    return stream_executor::GpuComputeCapability{\n+        se::CudaComputeCapability::Ampere()};\n   }\n \n   // Returns the module, its fusion computation and associated block level\n@@ -513,12 +513,13 @@ ENTRY entry {\n \n   const HloFusionInstruction* fusion1 = Cast<HloFusionInstruction>(\n       module1_and_metadata.computation->FusionInstruction());\n-  EXPECT_THAT(TritonWrapper(\"test_fn\", fusion1, cc, device_info,\n-                            module1_and_metadata.block_level_parameters,\n-                            &llvm_module, symbolic_expr_context_),\n-              absl_testing::StatusIs(\n-                  tsl::error::RESOURCE_EXHAUSTED,\n-                  ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n+  EXPECT_THAT(\n+      TritonWrapper(\"test_fn\", fusion1, se::GpuComputeCapability{cc},\n+                    device_info, module1_and_metadata.block_level_parameters,\n+                    &llvm_module, symbolic_expr_context_),\n+      absl_testing::StatusIs(\n+          tsl::error::RESOURCE_EXHAUSTED,\n+          ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n \n   TF_ASSERT_OK_AND_ASSIGN(ModuleAndNestedFusionMetadata module2_and_metadata,\n                           GetModuleAndNestedFusionMetadata(absl::Substitute(\n@@ -529,9 +530,9 @@ ENTRY entry {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const auto result,\n-      TritonWrapper(\"test_fn\", fusion2, cc, device_info,\n-                    module2_and_metadata.block_level_parameters, &llvm_module,\n-                    symbolic_expr_context_));\n+      TritonWrapper(\"test_fn\", fusion2, se::GpuComputeCapability{cc},\n+                    device_info, module2_and_metadata.block_level_parameters,\n+                    &llvm_module, symbolic_expr_context_));\n   // Use optin shared memory which is > shared_memory_per_block.\n   EXPECT_GT(result.shmem_bytes, device_info.shared_memory_per_block());\n }\n@@ -859,11 +860,12 @@ ENTRY entry {\n \n   const HloFusionInstruction* fusion1 = Cast<HloFusionInstruction>(\n       module1_and_metadata.computation->FusionInstruction());\n-  EXPECT_THAT(TritonWrapper(\"test_fn\", fusion1, cc, device_info,\n-                            module1_and_metadata.block_level_parameters,\n-                            &llvm_module, symbolic_expr_context_),\n-              absl_testing::StatusIs(tsl::error::RESOURCE_EXHAUSTED,\n-                                     \"Tiling complexity heuristic exceeded\"));\n+  EXPECT_THAT(\n+      TritonWrapper(\"test_fn\", fusion1, se::GpuComputeCapability{cc},\n+                    device_info, module1_and_metadata.block_level_parameters,\n+                    &llvm_module, symbolic_expr_context_),\n+      absl_testing::StatusIs(tsl::error::RESOURCE_EXHAUSTED,\n+                             \"Tiling complexity heuristic exceeded\"));\n \n   // Succeeds if the tiling is not too complex.\n   TF_ASSERT_OK_AND_ASSIGN(ModuleAndNestedFusionMetadata module2_and_metadata,\n@@ -873,7 +875,8 @@ ENTRY entry {\n   const HloFusionInstruction* fusion2 = Cast<HloFusionInstruction>(\n       module1_and_metadata.computation->FusionInstruction());\n \n-  TF_EXPECT_OK(TritonWrapper(\"test_fn\", fusion2, cc, device_info,\n+  TF_EXPECT_OK(TritonWrapper(\"test_fn\", fusion2, se::GpuComputeCapability{cc},\n+                             device_info,\n                              module2_and_metadata.block_level_parameters,\n                              &llvm_module, symbolic_expr_context_)\n                    .status());\n@@ -1208,8 +1211,7 @@ ENTRY e {\n // TODO(b/393299275): this should just be a fusion test and does not need to be\n // in the codegen directory.\n TEST_F(TritonGemmTest, DoNotFuseConcatenationOfSplitNonContractingDimension) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(\n-          GpuComputeCapability())) {\n+  if (GpuComputeCapability().IsRocm()) {\n     GTEST_SKIP() << \"Not using autotuner on ROCM yet.\";\n   }\n   if (!SupportsBF16(GpuComputeCapability())) {\n@@ -1811,8 +1813,7 @@ TEST_F(TritonGemmTest, DISABLED_SplitLHSInputOutputIsFused) {\n   if (!SupportsBF16(GpuComputeCapability())) {\n     GTEST_SKIP() << \"BF16 not supported.\";\n   }\n-  if (std::holds_alternative<se::RocmComputeCapability>(\n-          GpuComputeCapability())) {\n+  if (GpuComputeCapability().IsRocm()) {\n     GTEST_SKIP() << \"Skipped until corresponding issue on ROCm is fixed.\";\n   }\n \n@@ -1968,8 +1969,7 @@ ENTRY e {\n // probably be made deviceless and repurposed to test that opt-in shared memory\n // is used only.\n TEST_F(CompareTest, UsingOptinSharedMemoryProducesSameResult) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(\n-          GpuComputeCapability())) {\n+  if (GpuComputeCapability().IsRocm()) {\n     GTEST_SKIP() << \"No Optin Shared Memory on AMD.\";\n   }\n   const se::DeviceDescription dev_info ="
        },
        {
            "sha": "e70e35d02e0fdb39337552c52ad01d103a1d4843",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -76,8 +76,7 @@ class TritonTest : public GpuCodegenTest {\n     return device_desc().gpu_compute_capability();\n   }\n   stream_executor::GpuComputeCapability CudaAmpereOrRocm() {\n-    if (std::holds_alternative<stream_executor::RocmComputeCapability>(\n-            GpuComputeComp())) {\n+    if (GpuComputeComp().IsRocm()) {\n       return stream_executor::GpuComputeCapability{\n           device_desc().rocm_compute_capability()};\n     } else {\n@@ -668,7 +667,7 @@ CHECK: mma\n }\n \n TEST_F(TritonGemmTest, FailIfTooMuchShmem) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"GEMM padding requirements for ROCM not included yet.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -1186,7 +1185,7 @@ ENTRY e {\n }\n \n TEST_F(TritonGemmTestWithoutTritonGemmAny, SkipU8) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"GEMM padding requirements for ROCM not included yet.\";\n   }\n   const std::string hlo_text = R\"(\n@@ -1247,7 +1246,7 @@ CHECK:          %[[RES3:.*]] = arith.select %[[CMP3]], %[[ZERO]], %[[RES2]]\n }\n \n TEST_F(TritonGemmTestWithoutTritonGemmAny, SkipF32F32) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"GEMM padding requirements for ROCM not included yet.\";\n   }\n   const std::string hlo_text = R\"(\n@@ -1396,7 +1395,7 @@ ENTRY e {\n }\n \n TEST_F(TritonGemmTest, SingleElementTileIsHandled) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not using autotuner on ROCM yet.\";\n   }\n   MatchOptimizedHlo(R\"(\n@@ -1497,7 +1496,7 @@ ENTRY e {\n }\n \n TEST_F(TritonGemmTest, DoAddConstantToScalarAndBroadcastThat) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not using autotuner on ROCM yet.\";\n   }\n   const std::string hlo_text = R\"(\n@@ -1807,7 +1806,7 @@ ENTRY e {\n }\n \n TEST_F(TritonGemmTest, DoNotFuseConcatenationOfSplitNonContractingDimension) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not using autotuner on ROCM yet.\";\n   }\n   if (!SupportsBF16(GpuComputeComp())) {\n@@ -2698,7 +2697,7 @@ TEST_F(TritonGemmTest, SplitLHSInputOutputIsFused) {\n   if (!SupportsBF16(GpuComputeComp())) {\n     GTEST_SKIP() << \"BF16 not supported.\";\n   }\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Skipped until corresponding issue on ROCm is fixed.\";\n   }\n \n@@ -3029,7 +3028,7 @@ ENTRY e {\n }\n \n TEST_F(CompareTest, UsingOptinSharedMemoryOnAmpereProducesSameResult) {\n-  if (std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"No Optin Shared Memory on AMD.\";\n   }\n   const se::DeviceDescription dev_info =\n@@ -4245,7 +4244,7 @@ CHECK: wgmma.mma_async.sync.aligned.m64n16k16.f32.bf16.bf16\n // when gemm autotuner is not present in pipeline,\n // (which is currently the case on rocm).\n TEST_F(TritonGemmTest, TestNoAutotuner) {\n-  if (std::holds_alternative<se::CudaComputeCapability>(GpuComputeComp())) {\n+  if (GpuComputeComp().IsCuda()) {\n     GTEST_SKIP() << \"Autotuner is always in pipeline on Cuda.\";\n   }\n   constexpr absl::string_view kHloText = R\"("
        },
        {
            "sha": "f16eb3bf54270114a924538579254ed3f0c72d56",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/tests/test_utils.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n@@ -3022,7 +3023,7 @@ ENTRY entry_computation {\n TEST_F(TritonEmitterTest, ConvertF16ToF8E5M2Exhaustive) {\n   // TODO(b/396595945): enable post-Ampere once Triton respects RTNE semantics\n   // on H100.\n-  if (auto cc = std::get_if<se::CudaComputeCapability>(&GpuComputeCapability());\n+  if (auto cc = GpuComputeCapability().cuda_compute_capability();\n       cc && cc->IsAtLeastHopper()) {\n     GTEST_SKIP() << \"Skipping tests above Ampere, Triton's conversion isn't \"\n                     \"always correct\";\n@@ -4074,7 +4075,7 @@ INSTANTIATE_TEST_SUITE_P(\n     DotUnsetAlgorithmEmitterTest::ParamToString);\n \n TEST_F(TritonEmitterTest, ScaledDotIsSupportedByReferencePlatform) {\n-  if (!std::get_if<se::CudaComputeCapability>(&GpuComputeCapability())) {\n+  if (GpuComputeCapability().IsRocm()) {\n     GTEST_SKIP() << \"Ignore scaled dot test on ROCM.\";\n   }\n   constexpr absl::string_view kHloText = R\"(\n@@ -4095,7 +4096,7 @@ TEST_F(TritonEmitterTest, ScaledDotIsSupportedByReferencePlatform) {\n }\n \n TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n-  if (std::get_if<se::CudaComputeCapability>(&GpuComputeCapability())) {\n+  if (GpuComputeCapability().IsCuda()) {\n     GTEST_SKIP() << \"Warp size is always 32 on CUDA\";\n   }\n \n@@ -4126,7 +4127,8 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   const se::DeviceDescription dev_info =\n       TestGpuDeviceInfo::AMDMI210DeviceInfo();\n   TF_ASSERT_OK(TritonWrapper(\n-      \"test_fn\", triton_fusion, se::RocmComputeCapability(\"gfx942\"), dev_info,\n+      \"test_fn\", triton_fusion,\n+      se::GpuComputeCapability{se::RocmComputeCapability(\"gfx942\")}, dev_info,\n       BlockLevelParameters(), &llvm_module, symbolic_expr_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));\n@@ -4142,7 +4144,8 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   const se::DeviceDescription dev_info_n =\n       TestGpuDeviceInfo::AMDRX7900DeviceInfo();\n   TF_ASSERT_OK(TritonWrapper(\n-      \"test_fn\", triton_fusion, se::RocmComputeCapability(\"gfx1100\"),\n+      \"test_fn\", triton_fusion,\n+      se::GpuComputeCapability{se::RocmComputeCapability(\"gfx1100\")},\n       dev_info_n, BlockLevelParameters(), &llvm_module, symbolic_expr_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));"
        },
        {
            "sha": "4131a617880e06fc2d62e24e24b09b0b1d7b0837",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -158,10 +158,11 @@ ENTRY entry {\n   block_level_parameters.output_tile_sizes = {{1, 1}};\n   block_level_parameters.num_warps = 0;\n \n-  EXPECT_THAT(TritonWrapper(\"test_fn\", triton_fusion,\n-                            se::CudaComputeCapability::Hopper(), dev_info,\n-                            block_level_parameters, &llvm_module,\n-                            symbolic_expr_context),\n+  EXPECT_THAT(TritonWrapper(\n+                  \"test_fn\", triton_fusion,\n+                  se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+                  dev_info, block_level_parameters, &llvm_module,\n+                  symbolic_expr_context),\n               absl_testing::StatusIs(\n                   absl::StatusCode::kFailedPrecondition,\n                   ::testing::HasSubstr("
        },
        {
            "sha": "ff0ff8de464100047a3386908983cfa8afcdc8af",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_large_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_large_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_large_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_large_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -38,8 +38,7 @@ class TritonGemmTest : public GpuCodegenTest {\n   }\n \n   void SetUp() override {\n-    if (std::holds_alternative<se::RocmComputeCapability>(\n-            GetGpuComputeCapability())) {\n+    if (GetGpuComputeCapability().IsRocm()) {\n       GTEST_SKIP() << \"Not supported on ROCm until Triton is re-enabled.\";\n     }\n   }"
        },
        {
            "sha": "aaab8c1d84e1136f2a44be8a87d9ca7379f87d70",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_int4_device_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_int4_device_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -75,8 +75,7 @@ class TritonTest : public GpuCodegenTest {\n     return device_desc().gpu_compute_capability();\n   }\n   stream_executor::GpuComputeCapability CudaAmpereOrRocm() {\n-    if (std::holds_alternative<stream_executor::RocmComputeCapability>(\n-            GpuComputeComp())) {\n+    if (GpuComputeComp().IsRocm()) {\n       return stream_executor::GpuComputeCapability{\n           device_desc().rocm_compute_capability()};\n     }"
        },
        {
            "sha": "afce47b2e43b35234d9a55c4e08f1d04e62a19e5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_parametrized_legacy_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -62,8 +62,7 @@ class MixedTypeTest : public GpuCodegenTest,\n   }\n \n   void SetUp() override {\n-    if (std::holds_alternative<se::RocmComputeCapability>(\n-            GetGpuComputeCapability())) {\n+    if (GetGpuComputeCapability().IsRocm()) {\n       GTEST_SKIP()\n           << \"Related fusions are not performed on ROCm without Triton.\";\n     }"
        },
        {
            "sha": "0ceae13fce9ec4fa0b81913cc9c390fa9988789e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -64,12 +64,11 @@ bool IsTritonSupportedDataType(PrimitiveType type,\n       return true;\n     case F8E5M2:\n     case F8E4M3FN:\n-      return std::holds_alternative<se::CudaComputeCapability>(gpu_version);\n+      return gpu_version.IsCuda();\n     case BF16:\n-      return std::holds_alternative<se::CudaComputeCapability>(gpu_version) ||\n-             (std::holds_alternative<se::RocmComputeCapability>(gpu_version) &&\n-              std::get<se::RocmComputeCapability>(gpu_version)\n-                  .has_bf16_dtype_support());\n+      return gpu_version.IsCuda() ||\n+             (gpu_version.IsRocm() &&\n+              gpu_version.rocm_compute_capability()->has_bf16_dtype_support());\n     default:\n       return false;\n   }\n@@ -157,8 +156,8 @@ CodegenDecision IsTritonSupportedConversion(\n   };\n \n   if (input != output && any_is(PrimitiveType::F8E4M3FN) &&\n-      std::holds_alternative<se::CudaComputeCapability>(gpu_version) &&\n-      !std::get<se::CudaComputeCapability>(gpu_version).IsAtLeastHopper()) {\n+      gpu_version.IsCuda() &&\n+      !gpu_version.cuda_compute_capability()->IsAtLeastHopper()) {\n     return error_message();\n   }\n \n@@ -344,7 +343,7 @@ CodegenDecision AreTypesSupportedByAlgUnsetDot(\n   }\n \n   if (input_type == F8E4M3FN || result_type == F8E4M3FN) {\n-    if (auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version);\n+    if (auto* cuda_cc = gpu_version.cuda_compute_capability();\n         cuda_cc && !cuda_cc->IsAtLeastHopper()) {\n       return CodegenDecision::Forbid(\n           \"Dot operation for F8E4M3FN is not supported before Hopper.\");\n@@ -416,7 +415,7 @@ CodegenDecision AreDotAlgorithmInputAndOutputConversionsSupported(\n \n   if (algorithm == PrecisionConfig::ALG_DOT_F64_F64_F64 &&\n       primitive_util::BitWidth(lhs_type) < 32 &&\n-      !std::get<se::CudaComputeCapability>(gpu_version).IsAtLeastBlackwell()) {\n+      !gpu_version.cuda_compute_capability()->IsAtLeastBlackwell()) {\n     return forbid(\"Unsupported BF16 on GPUs before Blackwell\");\n   }\n \n@@ -724,9 +723,9 @@ bool IsTritonUnsupportedOpcode(HloOpcode opcode) {\n absl::Status EnsureTritonSupportsComputeCapability(\n     const se::GpuComputeCapability& gpu_compute_capability) {\n   auto cuda_compute_capability =\n-      std::get_if<se::CudaComputeCapability>(&gpu_compute_capability);\n+      gpu_compute_capability.cuda_compute_capability();\n   auto rocm_compute_capability =\n-      std::get_if<se::RocmComputeCapability>(&gpu_compute_capability);\n+      gpu_compute_capability.rocm_compute_capability();\n   if (!cuda_compute_capability && !rocm_compute_capability) {\n     return absl::FailedPreconditionError(\n         \"Triton support is only enabled for CUDA and ROCm GPUs.\");"
        },
        {
            "sha": "3ec9acc875e2ed882d9e2beff790e4eec93e7713",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -216,10 +216,8 @@ CodegenDecision CanTritonHandleElementwise(\n bool IsDotAlgorithmSupportedByTriton(\n     PrecisionConfig::Algorithm algorithm,\n     const se::GpuComputeCapability& gpu_version) {\n-  auto cuda_compute_capability =\n-      std::get_if<se::CudaComputeCapability>(&gpu_version);\n-  auto rocm_compute_capability =\n-      std::get_if<se::RocmComputeCapability>(&gpu_version);\n+  auto cuda_compute_capability = gpu_version.cuda_compute_capability();\n+  auto rocm_compute_capability = gpu_version.rocm_compute_capability();\n   switch (algorithm) {\n     case PrecisionConfig::ALG_DOT_TF32_TF32_F32:\n     case PrecisionConfig::ALG_DOT_TF32_TF32_F32_X3:\n@@ -276,10 +274,8 @@ CodegenDecision AreDotInputAndOutputTypesSupportedAndCompatible(\n // Filters GEMMs which can be handled using Triton.\n CodegenDecision CanTritonHandleGEMM(\n     const HloDotInstruction& dot, const se::GpuComputeCapability& gpu_version) {\n-  auto cuda_compute_capability =\n-      std::get_if<se::CudaComputeCapability>(&gpu_version);\n-  auto rocm_compute_capability =\n-      std::get_if<se::RocmComputeCapability>(&gpu_version);\n+  auto cuda_compute_capability = gpu_version.cuda_compute_capability();\n+  auto rocm_compute_capability = gpu_version.rocm_compute_capability();\n \n   CHECK(cuda_compute_capability || rocm_compute_capability);\n "
        },
        {
            "sha": "77c2f01a10e3ecc64f46c6142534be0907c61411",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -55,11 +55,10 @@ se::GpuComputeCapability GetComputeCapability() {\n bool CombinationCrashesTriton(PrimitiveType lhs_type, PrimitiveType rhs_type,\n                               PrimitiveType output_type,\n                               se::GpuComputeCapability gpu_compute_capability) {\n-  if (std::holds_alternative<se::CudaComputeCapability>(\n-          gpu_compute_capability)) {\n-    auto cuda_compute_capability =\n-        std::get<se::CudaComputeCapability>(gpu_compute_capability);\n-    if (!cuda_compute_capability.IsAtLeastHopper() &&\n+  if (gpu_compute_capability.IsCuda()) {\n+    auto* cuda_compute_capability =\n+        gpu_compute_capability.cuda_compute_capability();\n+    if (!cuda_compute_capability->IsAtLeastHopper() &&\n         (lhs_type == F8E4M3FN || rhs_type == F8E4M3FN ||\n          output_type == F8E4M3FN)) {\n       return true;"
        },
        {
            "sha": "21d8071b36b974aaf34aba27c11781238bb05748",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -265,9 +265,8 @@ class TritonSupportTest : public TritonSupportTestBase {\n     BlockLevelParameters block_level_parameters =\n         FromOutputTileSizes(std::move(output_tile_sizes));\n     const se::DeviceDescription dev_info =\n-        std::holds_alternative<se::CudaComputeCapability>(cc)\n-            ? TestGpuDeviceInfo::RTXA6000DeviceInfo(cc)\n-            : TestGpuDeviceInfo::AMDMI210DeviceInfo();\n+        cc.IsCuda() ? TestGpuDeviceInfo::RTXA6000DeviceInfo(cc)\n+                    : TestGpuDeviceInfo::AMDMI210DeviceInfo();\n     auto run_triton_codegen = [&]() {\n       return TritonWrapper(\"test_fn\", &ti.TritonFusion(), cc, dev_info,\n                            block_level_parameters, &llvm_module_,\n@@ -542,8 +541,7 @@ ENTRY triton_computation {\n \n   bool crashes_on_failure = false;\n   if (data_type_in != data_type_out && any_is(PrimitiveType::F8E4M3FN) &&\n-      std::holds_alternative<se::CudaComputeCapability>(cc) &&\n-      !std::get<se::CudaComputeCapability>(cc).IsAtLeastHopper()) {\n+      cc.IsCuda() && !cc.cuda_compute_capability()->IsAtLeastHopper()) {\n     crashes_on_failure |= any_is(F16) || any_is(BF16) || any_is(F32);\n \n     // Crashes due to unsupported/unspecified rounding mode.\n@@ -1869,7 +1867,7 @@ TEST_P(DotTypesTest, Dot) {\n \n   ExpectedFailMode fail_mode = ExpectedFailMode::kFail;\n   if (input_type == F8E4M3FN || result_type == F8E4M3FN) {\n-    if (auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&cc);\n+    if (auto* cuda_cc = cc.cuda_compute_capability();\n         cuda_cc && !cuda_cc->IsAtLeastHopper()) {\n       // Hits llvm::report_fatal_error during Triton compilation.\n       fail_mode = ExpectedFailMode::kFailOrCrash;"
        },
        {
            "sha": "8299cc88ac501c9886187fd83ce9eaad0b5029a6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -83,13 +83,11 @@ std::vector<xla::PrimitiveType> AllXlaDataTypes() {\n }\n \n bool SupportsBF16(const stream_executor::GpuComputeCapability& cc) {\n-  if (std::holds_alternative<stream_executor::CudaComputeCapability>(cc)) {\n-    return std::get<stream_executor::CudaComputeCapability>(cc).IsAtLeast(\n+  if (cc.IsCuda()) {\n+    return cc.cuda_compute_capability()->IsAtLeast(\n         se::CudaComputeCapability::kAmpere);\n-  } else if (std::holds_alternative<stream_executor::RocmComputeCapability>(\n-                 cc)) {\n-    return std::get<stream_executor::RocmComputeCapability>(cc)\n-        .has_bf16_dtype_support();\n+  } else if (cc.IsRocm()) {\n+    return cc.rocm_compute_capability()->has_bf16_dtype_support();\n   }\n   CHECK(false);\n }\n@@ -239,10 +237,10 @@ std::string PrimitiveTypeAndHloOpcodeToString(PrimitiveType data_type,\n \n std::string ComputeCapabilityToString(\n     const stream_executor::GpuComputeCapability& cc) {\n-  if (auto cuda_cc = std::get_if<se::CudaComputeCapability>(&cc)) {\n+  if (auto* cuda_cc = cc.cuda_compute_capability()) {\n     return absl::StrReplaceAll(cuda_cc->ToString(), {{\".\", \"\"}});\n   } else {\n-    CHECK(std::holds_alternative<se::RocmComputeCapability>(cc));\n+    CHECK(cc.IsRocm());\n     return \"rocm\";\n   }\n }"
        },
        {
            "sha": "8bf2d7a8efddc10e27101bc8ad317da8f01621a8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -63,8 +63,8 @@ static se::StreamExecutor* GpuExecutor() {\n // Some of the tests rely on CUDA 12.9+ features.\n bool IsAtLeastCuda12900(const se::StreamExecutor* stream_executor) {\n   const auto& device_description = stream_executor->GetDeviceDescription();\n-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n-      &device_description.gpu_compute_capability());\n+  const auto* cuda_cc =\n+      device_description.gpu_compute_capability().cuda_compute_capability();\n   if (cuda_cc != nullptr) {\n     // We need a recent driver to support the feature at runtime and we need a\n     // recent version of the toolkit at compile time, so that we have access to"
        },
        {
            "sha": "0f5df8e81b47c82ceca4c6209edee2a5c9a73d6e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -119,8 +119,8 @@ KernelArgsPacking CreateDefaultArgsPacking() {\n // Some of the tests rely on CUDA 12.3+ features.\n bool IsAtLeastCuda12300(const se::StreamExecutor* stream_executor) {\n   const auto& device_description = stream_executor->GetDeviceDescription();\n-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n-      &device_description.gpu_compute_capability());\n+  const auto* cuda_cc =\n+      device_description.gpu_compute_capability().cuda_compute_capability();\n   if (cuda_cc != nullptr) {\n     // We need a recent driver to support the feature at runtime and we need a\n     // recent version of the toolkit at compile time, so that we have access to\n@@ -137,8 +137,8 @@ bool IsAtLeastCuda12300(const se::StreamExecutor* stream_executor) {\n \n bool IsAtLeastCuda12900(const se::StreamExecutor* stream_executor) {\n   const auto& device_description = stream_executor->GetDeviceDescription();\n-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n-      &device_description.gpu_compute_capability());\n+  const auto* cuda_cc =\n+      device_description.gpu_compute_capability().cuda_compute_capability();\n   if (cuda_cc != nullptr) {\n     // We need a recent driver to support the feature at runtime and we need a\n     // recent version of the toolkit at compile time, so that we have access to"
        },
        {
            "sha": "3e2e171c5c12bb532eed3cacf6896f7223dc6d5e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -105,10 +105,10 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n   RunConvOptions opts;\n   opts.runner_cache = &GetOrCreateRunner(params.stream, &runner_created);\n \n-  if (runner_created && std::holds_alternative<se::RocmComputeCapability>(\n-                            params.stream->parent()\n-                                ->GetDeviceDescription()\n-                                .gpu_compute_capability())) {\n+  if (runner_created && params.stream->parent()\n+                            ->GetDeviceDescription()\n+                            .gpu_compute_capability()\n+                            .IsRocm()) {\n     TF_ASSIGN_OR_RETURN(\n         GpuConvParams conv_params,\n         GetGpuConvParams(config_, operand_se_buffers, result_se_buffers));"
        },
        {
            "sha": "2003e3effe18399c7267ed3dd8abc7f126a32845",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -93,7 +93,7 @@ class GpuBlasLtMatmulThunkTest : public HloTestBase {\n   }\n \n   void SetUp() override {\n-    if (auto* rocm = std::get_if<se::RocmComputeCapability>(&gpu_comp());\n+    if (auto* rocm = gpu_comp().rocm_compute_capability();\n         rocm != nullptr && !rocm->has_hipblaslt()) {\n       GTEST_SKIP() << \"No hipblas-lt support on this architecture!\";\n     }"
        },
        {
            "sha": "411fb658b5caec4522744f08cb7326f06978417e",
            "filename": "third_party/xla/xla/codegen/device_spec.h",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fcodegen%2Fdevice_spec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fcodegen%2Fdevice_spec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fdevice_spec.h?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -50,14 +50,10 @@ class DeviceSpec {\n     return std::holds_alternative<stream_executor::DeviceDescription>(type_);\n   }\n   bool IsAmdGpu() const {\n-    return IsGpu() &&\n-           std::holds_alternative<stream_executor::RocmComputeCapability>(\n-               gpu().gpu_compute_capability());\n+    return IsGpu() && gpu().gpu_compute_capability().IsRocm();\n   }\n   bool IsNvidiaGpu() const {\n-    return IsGpu() &&\n-           std::holds_alternative<stream_executor::CudaComputeCapability>(\n-               gpu().gpu_compute_capability());\n+    return IsGpu() && gpu().gpu_compute_capability().IsCuda();\n   }\n   bool IsIntelGpu() const {\n     // TODO(intel-gpu): Align with CUDA and ROCM approach of detecting Intel"
        },
        {
            "sha": "f0588c42e7e31ab49f023738a2a519da28bc8ee0",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -1553,15 +1553,15 @@ absl::StatusOr<DeviceTopologyPair> BuildDistributedDevices(\n \n std::string MakeComputeCapabilityString(const se::DeviceDescription* desc) {\n   se::GpuComputeCapability cc = desc->gpu_compute_capability();\n-  if (std::holds_alternative<se::CudaComputeCapability>(cc)) {\n-    auto nvcc = std::get<se::CudaComputeCapability>(cc);\n-    return absl::StrCat(nvcc.major, \".\", nvcc.minor);\n-  } else if (std::holds_alternative<se::RocmComputeCapability>(cc)) {\n-    auto rocmcc = std::get<se::RocmComputeCapability>(cc);\n-    return rocmcc.gfx_version();\n-  } else {\n-    return \"unknown\";\n+  if (cc.IsCuda()) {\n+    auto* nvcc = cc.cuda_compute_capability();\n+    return absl::StrCat(nvcc->major, \".\", nvcc->minor);\n+  }\n+  if (cc.IsRocm()) {\n+    auto* rocmcc = cc.rocm_compute_capability();\n+    return rocmcc->gfx_version();\n   }\n+  return \"unknown\";\n }\n \n StreamExecutorGpuDevice::StreamExecutorGpuDevice("
        },
        {
            "sha": "2f9e9d43c25b3095ad7df1b759e6a9a17528baa0",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -1258,8 +1258,9 @@ TEST(StreamExecutorGpuClientTest, GetDeviceFabricInfo) {\n                 ->local_device_state();\n         if (local_device_state != nullptr) {\n           se::StreamExecutor* executor = local_device_state->executor();\n-          if (auto* cc = std::get_if<se::CudaComputeCapability>(\n-                  &executor->GetDeviceDescription().gpu_compute_capability())) {\n+          if (auto* cc = executor->GetDeviceDescription()\n+                             .gpu_compute_capability()\n+                             .cuda_compute_capability()) {\n             if (cc->IsAtLeastHopper()) {\n               auto fabric_info =\n                   GetDeviceFabricInfo(executor->device_ordinal());"
        },
        {
            "sha": "5d5635fcc33f3c018e8a57761de2e8ca72a0dcde",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -1708,9 +1708,9 @@ TEST(TfrtGpuClientTest, DeviceAttributes) {\n     TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::se::DeviceDescription> desc,\n                             platform->DescriptionForDevice(0));\n     stream_executor::GpuComputeCapability cc = desc->gpu_compute_capability();\n-    auto nvcc = std::get<stream_executor::CudaComputeCapability>(cc);\n+    auto* nvcc = cc.cuda_compute_capability();\n     std::string expected_compute_capability =\n-        absl::StrCat(nvcc.major, \".\", nvcc.minor);\n+        absl::StrCat(nvcc->major, \".\", nvcc->minor);\n     EXPECT_EQ(compute_capability, expected_compute_capability);\n \n     // Attribute `coords`."
        },
        {
            "sha": "13b49c0d56447e8dc71eada581adc1b185f034b7",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -726,13 +726,13 @@ absl::StatusOr<DeviceTopologyPair> BuildDistributedDevices(\n   auto make_compute_capability_string =\n       [](const stream_executor::DeviceDescription* desc) -> std::string {\n     stream_executor::GpuComputeCapability cc = desc->gpu_compute_capability();\n-    if (std::holds_alternative<stream_executor::CudaComputeCapability>(cc)) {\n-      auto nvcc = std::get<stream_executor::CudaComputeCapability>(cc);\n-      return absl::StrCat(nvcc.major, \".\", nvcc.minor);\n+    if (cc.IsCuda()) {\n+      auto* nvcc = cc.cuda_compute_capability();\n+      return absl::StrCat(nvcc->major, \".\", nvcc->minor);\n     }\n-    if (std::holds_alternative<stream_executor::RocmComputeCapability>(cc)) {\n-      auto rocmcc = std::get<stream_executor::RocmComputeCapability>(cc);\n-      return rocmcc.gfx_version();\n+    if (cc.IsRocm()) {\n+      auto* rocmcc = cc.rocm_compute_capability();\n+      return rocmcc->gfx_version();\n     }\n     return \"unknown\";\n   };"
        },
        {
            "sha": "acfdb636abd9b2caff19568b3dce70ad32723be1",
            "filename": "third_party/xla/xla/pjrt/triton_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -233,8 +233,9 @@ absl::StatusOr<CompilationResult> Compile(absl::string_view module,\n   TF_ASSIGN_OR_RETURN(\n       auto cuda_cc,\n       stream_executor::CudaComputeCapability::FromString(arch_name));\n-  xla::gpu::CreateTritonPipeline(&pm, cuda_cc, num_warps, num_ctas, num_stages,\n-                                 cluster_info);\n+  xla::gpu::CreateTritonPipeline(&pm,\n+                                 stream_executor::GpuComputeCapability(cuda_cc),\n+                                 num_warps, num_ctas, num_stages, cluster_info);\n   if (failed(pm.run(*module_op))) {\n     return absl::InternalError(\"Failed to compile Triton IR to LLVM IR\");\n   }"
        },
        {
            "sha": "648bea311dee4cf4877059fa7492d83dcabd8109",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -151,9 +151,7 @@ class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n         IsCuda() ? \"f8e5m2\" : \"f8e5m2fnuz\";\n   }\n \n-  bool IsCuda() {\n-    return std::holds_alternative<se::CudaComputeCapability>(Capability());\n-  }\n+  bool IsCuda() { return Capability().IsCuda(); }\n \n   const se::GpuComputeCapability& Capability() {\n     return hlo_runner_->backend()\n@@ -164,10 +162,9 @@ class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n \n   bool HasFp8Support() {\n     if (IsCuda()) {\n-      return std::get<se::CudaComputeCapability>(Capability()).IsAtLeast(8, 9);\n+      return Capability().cuda_compute_capability()->IsAtLeast(8, 9);\n     }\n-    return std::get<se::RocmComputeCapability>(Capability())\n-               .has_fp8_support() &&\n+    return Capability().rocm_compute_capability()->has_fp8_support() &&\n            GetDebugOptionsForTest().xla_gpu_enable_cublaslt();\n   }\n "
        },
        {
            "sha": "f9293c75aa5958db5a0c384ce7f93ca30b71c69e",
            "filename": "third_party/xla/xla/tests/collective_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -2672,9 +2672,7 @@ class Fp8CollectiveOpsTest : public CollectiveOpsTest {\n   }\n \n  protected:\n-  bool IsCuda() {\n-    return std::holds_alternative<se::CudaComputeCapability>(Capability());\n-  }\n+  bool IsCuda() { return Capability().IsCuda(); }\n \n   const se::GpuComputeCapability& Capability() {\n     return backend()"
        },
        {
            "sha": "268482a3ec05eee30d058e8997caf56a88797edd",
            "filename": "third_party/xla/xla/tests/dot_operation_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fdot_operation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fdot_operation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fdot_operation_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -323,7 +323,7 @@ class ParametricDotTest : public DotOperationTest,\n                                .default_stream_executor()\n                                ->GetDeviceDescription()\n                                .gpu_compute_capability();\n-    if (std::holds_alternative<se::RocmComputeCapability>(gpu_comp)) {\n+    if (gpu_comp.IsRocm()) {\n       absl::string_view name(\n           ::testing::UnitTest::GetInstance()->current_test_info()->name());\n       if (name.find(\"TestF16/270x270x520_MajorToMinor\") != std::string::npos) {"
        },
        {
            "sha": "b9c151889df7f6648e90c74e1057f3dee658b8a2",
            "filename": "third_party/xla/xla/tests/matmul_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fmatmul_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fmatmul_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fmatmul_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -39,7 +39,7 @@ class MatmulTestWithCublas : public HloTestBase,\n                                .default_stream_executor()\n                                ->GetDeviceDescription()\n                                .gpu_compute_capability();\n-      if (auto* rocm = std::get_if<se::RocmComputeCapability>(&gpu_cc);\n+      if (auto* rocm = gpu_cc.rocm_compute_capability();\n           rocm != nullptr && !rocm->has_hipblaslt()) {\n         GTEST_SKIP() << \"No hipblas-lt support on this architecture!\";\n       }"
        },
        {
            "sha": "5f3e344e9902ec0ef6c95bb3519e4223ed63f8f0",
            "filename": "third_party/xla/xla/tests/matrix_ops_simple_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fmatrix_ops_simple_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftests%2Fmatrix_ops_simple_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fmatrix_ops_simple_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -181,10 +181,7 @@ class MatOpsDotAddTest\n     auto stream_executor = client_->platform()->ExecutorForDevice(0).value();\n     auto gpu_compute_capability =\n         stream_executor->GetDeviceDescription().gpu_compute_capability();\n-    if ((std::holds_alternative<stream_executor::CudaComputeCapability>(\n-            gpu_compute_capability)) ||\n-        std::holds_alternative<stream_executor::RocmComputeCapability>(\n-            gpu_compute_capability)) {\n+    if (gpu_compute_capability.IsCuda() || gpu_compute_capability.IsRocm()) {\n       return true;\n     }\n     return false;"
        },
        {
            "sha": "a27a0434a08030cbce337e73b2dccfb0dbfd6eba",
            "filename": "third_party/xla/xla/tools/collective_perf_table_gen_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftools%2Fcollective_perf_table_gen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftools%2Fcollective_perf_table_gen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fcollective_perf_table_gen_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -41,11 +41,11 @@ class CollectivePerfTableGenTest : public HloTestBase {\n \n  protected:\n   bool IsCuda() {\n-    return std::holds_alternative<stream_executor::CudaComputeCapability>(\n-        backend()\n-            .default_stream_executor()\n-            ->GetDeviceDescription()\n-            .gpu_compute_capability());\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability()\n+        .IsCuda();\n   }\n \n   CollectivePerfTableGen::Config cfg_;"
        },
        {
            "sha": "2004c7b1b0c82d2fb6ae09e061d47bc2740ea7d1",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_opt.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -136,8 +136,7 @@ class GpuOptProvider : public CompiledOptProvider {\n     se::GpuComputeCapability gpu_compute_capability;\n     if (device_description.ok()) {\n       gpu_compute_capability = device_description->gpu_compute_capability();\n-      if (std::holds_alternative<se::CudaComputeCapability>(\n-              gpu_compute_capability)) {\n+      if (gpu_compute_capability.IsCuda()) {\n         alias_info_ =\n             std::make_unique<gpu::NVPTXAliasInfo>(*device_description);\n       } else {"
        },
        {
            "sha": "070cde9d58fc0b826ed3f95cd8de64e31c0fc7c6",
            "filename": "third_party/xla/xla/tools/matmul_perf_table_gen_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftools%2Fmatmul_perf_table_gen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/361f1c64eb138c197be30e0f91cfb13f042e6c7b/third_party%2Fxla%2Fxla%2Ftools%2Fmatmul_perf_table_gen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmatmul_perf_table_gen_test.cc?ref=361f1c64eb138c197be30e0f91cfb13f042e6c7b",
            "patch": "@@ -42,11 +42,11 @@ class MatmulPerfTableGenTest : public HloTestBase {\n \n  protected:\n   bool IsCuda() {\n-    return std::holds_alternative<stream_executor::CudaComputeCapability>(\n-        backend()\n-            .default_stream_executor()\n-            ->GetDeviceDescription()\n-            .gpu_compute_capability());\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability()\n+        .IsCuda();\n   }\n };\n "
        }
    ],
    "stats": {
        "total": 368,
        "additions": 171,
        "deletions": 197
    }
}