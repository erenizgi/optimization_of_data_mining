{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Move out pieces of fusion_emitter required for collective codegen\n\nThis is a simple move of contents from fusion_emitter -> emitter_helpers.\nNo functional changes are included in this change.\n\nPiperOrigin-RevId: 831323642",
    "sha": "1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a",
    "files": [
        {
            "sha": "3d3ed331951fd92924778fcf3696c1fe5a89aade",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a",
            "patch": "@@ -108,28 +108,35 @@ cc_library(\n         \"//xla:comparison_util\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n+        \"//xla/codegen/emitters:elemental_hlo_to_mlir\",\n+        \"//xla/codegen/tiling:tiled_hlo_instruction\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n+        \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/mlir_hlo\",\n         \"//xla/mlir_hlo:map_mhlo_to_scalar_op\",\n         \"//xla/mlir_hlo:transformation_helpers\",\n         \"//xla/service/gpu:target_util\",\n+        \"//xla/service/gpu:triton_fusion_analysis\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n-        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\","
        },
        {
            "sha": "c0c5ce5ed479ad040a604ff692e365664e935ae9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 185,
            "deletions": 2,
            "changes": 187,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a",
            "patch": "@@ -16,15 +16,16 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n \n #include <cstdint>\n-#include <variant>\n #include <vector>\n \n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/IR/Metadata.h\"\n@@ -49,19 +50,29 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/emitters/elemental_hlo_to_mlir.h\"\n+#include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/comparison_util.h\"\n+#include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n+#include \"xla/layout_util.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n #include \"xla/mlir_hlo/mhlo/transforms/map_mhlo_to_scalar_op.h\"\n #include \"xla/mlir_hlo/mhlo/transforms/transformation_helpers.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/target_util.h\"\n+#include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n-#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n@@ -83,6 +94,84 @@ namespace mh = ::mlir::mhlo;\n namespace mm = ::mlir::math;\n namespace mt = ::mlir::triton;\n \n+namespace {\n+using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n+\n+// Emit a value as Index clamped to [lower, upper].\n+Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n+                       int64_t upper) {\n+  Value clamped_index =\n+      b.create<ma::MaxSIOp>(value, CreateConst(b, value.getType(), lower));\n+  clamped_index = b.create<ma::MinSIOp>(clamped_index,\n+                                        CreateConst(b, value.getType(), upper));\n+  return b.create<ma::IndexCastOp>(b.getIndexType(), clamped_index);\n+}\n+\n+absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n+    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n+    const TiledHloInstruction& tiled_hlo) {\n+  TF_ASSIGN_OR_RETURN(IndexingMap tile_offsets_indexing,\n+                      tiled_hlo.tile_offsets_indexing());\n+  const std::vector<IndexingMap::Variable>& rt_vars =\n+      tile_offsets_indexing.GetRTVars();\n+  CHECK_EQ(rt_vars.size(), runtime_values.size())\n+      << absl::StrCat(tiled_hlo.ToString(), \" has \", rt_vars.size(),\n+                      \" runtime variables in tile_offsets_indexing but only \",\n+                      runtime_values.size(), \" runtime values were provided\");\n+  CHECK_EQ(tile_offsets_indexing.GetRangeVars().size(), 0)\n+      << \"Range variables must be converted to dimensions. Instruction: \"\n+      << tiled_hlo.ToString();\n+  // emitters::ApplyIndexing does not support symbols at the moment. As a\n+  // workaround we convert them to dimensions.\n+  IndexingMap dim_only_tiling =\n+      tile_offsets_indexing.ConvertSymbolsToDimensions();\n+  SmallVector<Value> dims;\n+  dims.reserve(1 /* pid */ + runtime_values.size());\n+  dims.push_back(pid);\n+  for (const auto& [rt_var, value] : llvm::zip(rt_vars, runtime_values)) {\n+    Value clamped_index =\n+        EmitClampedIndex(b, value, rt_var.bounds.lower, rt_var.bounds.upper);\n+    dims.push_back(triton::Cast(b, clamped_index, pid.getType()));\n+  }\n+  return emitters::ApplyIndexing(dim_only_tiling, /*dims=*/dims,\n+                                 /*symbols=*/{}, b);\n+}\n+\n+// Emit code corresponding to a fusion instruction somehow nested within the\n+// initial Triton fusion. This can happen when we carry around auxiliary\n+// computations, e.g. with reduces. Since we are emitting a single Triton\n+// fusion, we simply flatten the fusion inside the computation.\n+//\n+// TODO(b/331413981): get rid of this special handling once this is solved.\n+absl::StatusOr<TensorValue> EmitNestedFusion(\n+    EmitterLocOpBuilder b, const HloFusionInstruction& fusion_instruction,\n+    absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n+  // TODO(b/331402498): revisit the order of scope once we completely\n+  // deprecate Triton fusion analysis.\n+  const HloComputation* fusion_computation =\n+      fusion_instruction.fused_instructions_computation();\n+\n+  absl::flat_hash_map<const HloInstruction*, TensorValue> region_values;\n+\n+  std::vector<const HloInstruction*> to_emit;\n+  for (const HloInstruction* instr :\n+       fusion_computation->MakeInstructionPostOrder()) {\n+    if (instr->opcode() == HloOpcode::kParameter) {\n+      int64_t parameter_number = instr->parameter_number();\n+      auto it = values.find(fusion_instruction.operand(parameter_number));\n+      TF_RET_CHECK(it != values.end());\n+      TF_RET_CHECK(region_values.insert({instr, it->second}).second);\n+    } else {\n+      to_emit.push_back(instr);\n+    }\n+  }\n+\n+  TF_RET_CHECK(to_emit.back() == fusion_computation->root_instruction());\n+\n+  return EmitScope(b, /*analysis=*/nullptr, to_emit, region_values);\n+}\n+}  // namespace\n+\n SmallVector<int64_t> GetPaddedTileSizes(ArrayRef<int64_t> tile_sizes) {\n   SmallVector<int64_t> result;\n   result.reserve(tile_sizes.size());\n@@ -655,4 +744,98 @@ mt::PointerType GetGlobalPointerType(mlir::Type element_type) {\n   return mlir::cast<mt::PointerType>(mt::getPointerTypeToElement(element_type));\n }\n \n+/*static */ absl::StatusOr<TileInfo> TileInfo::Construct(\n+    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n+    const TiledHloInstruction& tiled_hlo) {\n+  TF_ASSIGN_OR_RETURN(SmallVector<Value> offsets,\n+                      ComputeOffsetsForTile(b, pid, runtime_values, tiled_hlo));\n+\n+  // Triton requires that all block dimensions are a power of 2.\n+  auto padded_tile_sizes = GetPaddedTileSizes(tiled_hlo.tile_sizes());\n+  SmallVector<int64_t> original_shape;\n+  original_shape.assign(tiled_hlo.hlo()->shape().dimensions().begin(),\n+                        tiled_hlo.hlo()->shape().dimensions().end());\n+\n+  const Shape& shape = tiled_hlo.hlo()->shape();\n+  TF_ASSIGN_OR_RETURN(Type expected_element_type,\n+                      TritonType(b, shape.element_type()));\n+  auto storage_type = StorageType(expected_element_type);\n+\n+  auto tile_strides = tiled_hlo.tile_strides();\n+  auto minor_to_major_layout = llvm::to_vector(LayoutUtil::MinorToMajor(shape));\n+\n+  return TileInfo(offsets, tile_strides, original_shape, padded_tile_sizes,\n+                  minor_to_major_layout, storage_type);\n+}\n+\n+TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n+                                 const TileInfo& tile_info, Value arg) {\n+  auto tensor_type = mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n+                                                 tile_info.storage_type());\n+\n+  return b.create<xla::xtile::ExtractTileOp>(\n+      tensor_type, arg, tile_info.offsets(), tile_info.padded_tile_sizes(),\n+      tile_info.tile_strides());\n+}\n+\n+absl::StatusOr<TensorValue> EmitScope(\n+    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n+    absl::Span<const HloInstruction* const> instructions,\n+    absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n+  for (const HloInstruction* hlo : instructions) {\n+    TensorValue result;\n+    if (hlo->opcode() == HloOpcode::kConcatenate ||\n+        hlo->opcode() == HloOpcode::kDynamicSlice) {\n+      // Parameter loads and their concatenations are handled outside EmitScope.\n+      TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n+      continue;\n+    }\n+    if (hlo->opcode() == HloOpcode::kParameter) {\n+      if (hlo->users()[0]->opcode() == HloOpcode::kConcatenate ||\n+          hlo->users()[0]->opcode() == HloOpcode::kDynamicSlice) {\n+        continue;\n+      }\n+      TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n+      continue;\n+    }\n+    if (hlo->opcode() == HloOpcode::kBroadcast) {\n+      return absl::InvalidArgumentError(\n+          \"Broadcast is not yet supported in EmitScope().\");\n+    }\n+    if (hlo->opcode() == HloOpcode::kConstant) {\n+      TF_ASSIGN_OR_RETURN(result, EmitConstant(b, *hlo));\n+    } else if (HloInstruction::IsOpElementwise(hlo->opcode())) {\n+      std::vector<Value> operands;\n+      operands.reserve(hlo->operands().size());\n+      for (const HloInstruction* operand : hlo->operands()) {\n+        operands.push_back(values[operand]);\n+      }\n+      TF_ASSIGN_OR_RETURN(Value elementwise_result,\n+                          EmitElementwise(b, *hlo, operands));\n+      result = mlir::cast<TensorValue>(elementwise_result);\n+    } else if (hlo->opcode() == HloOpcode::kTuple) {\n+      TF_RET_CHECK(hlo->IsRoot()) << hlo->ToString();\n+    } else if (hlo->opcode() == HloOpcode::kBitcast ||\n+               hlo->opcode() == HloOpcode::kTranspose ||\n+               hlo->opcode() == HloOpcode::kSlice ||\n+               hlo->opcode() == HloOpcode::kReshape ||\n+               hlo->opcode() == HloOpcode::kPad) {\n+      // All these are currently supported only as operations on indices\n+      // which are pushed to loads and stores. No operations on tiles are\n+      // performed here.\n+      result = values[hlo->operand(0)];\n+    } else if (hlo->opcode() == HloOpcode::kFusion) {\n+      const auto* fusion_instruction = ::xla::Cast<HloFusionInstruction>(hlo);\n+      TF_ASSIGN_OR_RETURN(result,\n+                          EmitNestedFusion(b, *fusion_instruction, values));\n+    } else {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Unsupported operation \", hlo->ToString()));\n+    }\n+    TF_RET_CHECK(values.insert({hlo, result}).second) << hlo->ToString();\n+    VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n+  }\n+  return values[instructions.back()];\n+}\n+\n }  // namespace xla::gpu::triton"
        },
        {
            "sha": "8fff3ab3f09af24d064be2873c2e159d248cd656",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a",
            "patch": "@@ -18,12 +18,15 @@ limitations under the License.\n \n #include <cstdint>\n #include <string>\n+#include <utility>\n #include <vector>\n \n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Module.h\"\n@@ -39,9 +42,11 @@ limitations under the License.\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/literal.h\"\n+#include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -54,6 +59,8 @@ limitations under the License.\n \n namespace xla::gpu::triton {\n \n+using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n+\n // Returns a string representation of the given MLIR entity.\n template <typename T>\n std::string MlirToString(T&& value) {\n@@ -63,6 +70,62 @@ std::string MlirToString(T&& value) {\n   return result;\n }\n \n+// Constructs and holds information needed to construct a tile. This information\n+// is propagated to Extract/Insert ops to use them to load and store the correct\n+// tiles.\n+class TileInfo {\n+ public:\n+  static absl::StatusOr<TileInfo> Construct(\n+      EmitterLocOpBuilder b, mlir::Value pid, mlir::ValueRange runtime_values,\n+      const TiledHloInstruction& tiled_hlo);\n+\n+  // Tile offsets. Its size is equal to the rank of the output shape.\n+  inline mlir::ValueRange offsets() const { return offsets_; }\n+\n+  // Tile strides. Its size is equal to the rank of the output shape.\n+  inline mlir::ArrayRef<int64_t> tile_strides() const { return tile_strides_; }\n+\n+  // The original shape of the tensor.\n+  inline mlir::ArrayRef<int64_t> original_shape() const {\n+    return original_shape_;\n+  }\n+\n+  // Tile sizes after padding to a power of 2 (Triton requirement).\n+  inline mlir::ArrayRef<int64_t> padded_tile_sizes() const {\n+    return padded_tile_sizes_;\n+  }\n+\n+  // The layout of the tensor in minor-to-major order.\n+  inline const llvm::SmallVector<int64_t>& minor_to_major_layout() const {\n+    return minor_to_major_layout_;\n+  }\n+\n+  // The storage type of the tensor. This could be different from the element\n+  // type. e.g. predicates are stored as i8 instead of i1.\n+  mlir::Type storage_type() const { return storage_type_; }\n+\n+ private:\n+  llvm::SmallVector<mlir::Value> offsets_;\n+  llvm::SmallVector<int64_t> tile_strides_;\n+  llvm::SmallVector<int64_t> original_shape_;\n+  llvm::SmallVector<int64_t> padded_tile_sizes_;\n+  llvm::SmallVector<int64_t> minor_to_major_layout_;\n+  mlir::Type storage_type_;\n+\n+  inline TileInfo(llvm::SmallVector<mlir::Value> offsets,\n+                  llvm::SmallVector<int64_t> tile_strides,\n+                  llvm::SmallVector<int64_t> original_shape,\n+                  llvm::SmallVector<int64_t> padded_tile_sizes,\n+                  llvm::SmallVector<int64_t> minor_to_major_layout,\n+                  mlir::Type storage_type)\n+      : offsets_(std::move(offsets)),\n+        tile_strides_(std::move(tile_strides)),\n+        original_shape_(std::move(original_shape)),\n+        padded_tile_sizes_(std::move(padded_tile_sizes)),\n+        minor_to_major_layout_(std::move(minor_to_major_layout)),\n+        storage_type_(std::move(storage_type)) {}\n+};\n+\n // Triton requires that all block dimensions are a power of 2.\n // TODO(b/353484968): Delete this function once we have constraints to only\n // propagate tile sizes that are a power of 2.\n@@ -188,6 +251,25 @@ absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n // element type.\n ::mlir::triton::PointerType GetGlobalPointerType(mlir::Type element_type);\n \n+// Emits an xtile::ExtractTileOp for the given tile info and argument.\n+TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n+                                 const TileInfo& tile_info, mlir::Value arg);\n+\n+// Emits a sequence of HLO instructions within a specific scope.\n+//\n+// This function traverses the provided `hlo_instructions` in a\n+// defined-before-use order and emits the corresponding MLIR operations using\n+// the given `EmitterLocOpBuilder`. It uses `emitted_values` to look up already\n+// emitted results for instructions, typically parameters or results from\n+// outer scopes. New results are added to the `emitted_values` map.\n+//\n+// Example usage within [EmitReduce] includes using it to emit the body of the\n+// `HloInstruction::to_apply` computation.\n+absl::StatusOr<TensorValue> EmitScope(\n+    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n+    absl::Span<const HloInstruction* const> instructions,\n+    absl::flat_hash_map<const HloInstruction*, TensorValue>& values);\n+\n }  // namespace xla::gpu::triton\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_EMITTER_HELPERS_H_"
        },
        {
            "sha": "dd1702881003c743e2d1b5fec8453dec44d72f71",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 228,
            "changes": 231,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=1a5c0ce8704c2125cc0a45d7ee8258984ed4ed6a",
            "patch": "@@ -99,7 +99,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n-#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/elemental_hlo_to_mlir.h\"\n@@ -135,7 +134,6 @@ limitations under the License.\n #include \"xla/service/gpu/llvm_gpu_backend/nvptx_libdevice_path.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/model/triton_emitter_constraints.h\"\n-#include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n@@ -169,7 +167,6 @@ namespace xgt = ::xla::gpu::triton;\n using ::llvm::SmallVector;\n using ::mlir::AffineMap;\n using ::mlir::ArrayRef;\n-using ::mlir::ShapedType;\n using ::mlir::Type;\n using ::mlir::Value;\n using ::mlir::ValueRange;\n@@ -178,134 +175,19 @@ using ::xla::gpu::triton::Cast;\n using ::xla::gpu::triton::CreateConst;\n using ::xla::gpu::triton::EmitConstant;\n using ::xla::gpu::triton::EmitElementwise;\n+using ::xla::gpu::triton::EmitScope;\n using ::xla::gpu::triton::GetPaddedTileSizes;\n using ::xla::gpu::triton::StorageType;\n+using ::xla::gpu::triton::TensorValue;\n+using ::xla::gpu::triton::TileInfo;\n using ::xla::gpu::triton::TritonType;\n \n namespace {\n \n-using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n-\n Value MakeIndex(EmitterLocOpBuilder& b, int64_t value) {\n   return b.create<arith::ConstantIndexOp>(value);\n }\n \n-// Emit a value as Index clamped to [lower, upper].\n-Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n-                       int64_t upper) {\n-  Value clamped_index =\n-      b.create<arith::MaxSIOp>(value, CreateConst(b, value.getType(), lower));\n-  clamped_index = b.create<arith::MinSIOp>(\n-      clamped_index, CreateConst(b, value.getType(), upper));\n-  return b.create<arith::IndexCastOp>(b.getIndexType(), clamped_index);\n-}\n-\n-absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n-    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n-    const TiledHloInstruction& tiled_hlo) {\n-  TF_ASSIGN_OR_RETURN(IndexingMap tile_offsets_indexing,\n-                      tiled_hlo.tile_offsets_indexing());\n-  const std::vector<IndexingMap::Variable>& rt_vars =\n-      tile_offsets_indexing.GetRTVars();\n-  CHECK_EQ(rt_vars.size(), runtime_values.size())\n-      << absl::StrCat(tiled_hlo.ToString(), \" has \", rt_vars.size(),\n-                      \" runtime variables in tile_offsets_indexing but only \",\n-                      runtime_values.size(), \" runtime values were provided\");\n-  CHECK_EQ(tile_offsets_indexing.GetRangeVars().size(), 0)\n-      << \"Range variables must be converted to dimensions. Instruction: \"\n-      << tiled_hlo.ToString();\n-  // emitters::ApplyIndexing does not support symbols at the moment. As a\n-  // workaround we convert them to dimensions.\n-  IndexingMap dim_only_tiling =\n-      tile_offsets_indexing.ConvertSymbolsToDimensions();\n-  SmallVector<Value> dims;\n-  dims.reserve(1 /* pid */ + runtime_values.size());\n-  dims.push_back(pid);\n-  for (const auto& [rt_var, value] : llvm::zip(rt_vars, runtime_values)) {\n-    Value clamped_index =\n-        EmitClampedIndex(b, value, rt_var.bounds.lower, rt_var.bounds.upper);\n-    dims.push_back(triton::Cast(b, clamped_index, pid.getType()));\n-  }\n-  return emitters::ApplyIndexing(dim_only_tiling, /*dims=*/dims,\n-                                 /*symbols=*/{}, b);\n-}\n-\n-// Constructs and holds information needed to construct a tile. This information\n-// is propagated to Extract/Insert ops to use them to load and store the correct\n-// tiles.\n-class TileInfo {\n- public:\n-  static absl::StatusOr<TileInfo> Construct(\n-      EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n-      const TiledHloInstruction& tiled_hlo);\n-\n-  // Tile offsets. Its size is equal to the rank of the output shape.\n-  ValueRange offsets() const { return offsets_; }\n-\n-  // Tile strides. Its size is equal to the rank of the output shape.\n-  ArrayRef<int64_t> tile_strides() const { return tile_strides_; }\n-\n-  // The original shape of the tensor.\n-  ArrayRef<int64_t> original_shape() const { return original_shape_; }\n-\n-  // Tile sizes after padding to a power of 2 (Triton requirement).\n-  ArrayRef<int64_t> padded_tile_sizes() const { return padded_tile_sizes_; }\n-\n-  // The layout of the tensor in minor-to-major order.\n-  const SmallVector<int64_t>& minor_to_major_layout() const {\n-    return minor_to_major_layout_;\n-  }\n-\n-  // The storage type of the tensor. This could be different from the element\n-  // type. e.g. predicates are stored as i8 instead of i1.\n-  Type storage_type() const { return storage_type_; }\n-\n- private:\n-  SmallVector<Value> offsets_;\n-  SmallVector<int64_t> tile_strides_;\n-  SmallVector<int64_t> original_shape_;\n-  SmallVector<int64_t> padded_tile_sizes_;\n-  SmallVector<int64_t> minor_to_major_layout_;\n-  Type storage_type_;\n-\n-  explicit TileInfo(SmallVector<Value> offsets,\n-                    SmallVector<int64_t> tile_strides,\n-                    SmallVector<int64_t> original_shape,\n-                    SmallVector<int64_t> padded_tile_sizes,\n-                    SmallVector<int64_t> minor_to_major_layout,\n-                    Type storage_type)\n-      : offsets_(std::move(offsets)),\n-        tile_strides_(std::move(tile_strides)),\n-        original_shape_(std::move(original_shape)),\n-        padded_tile_sizes_(std::move(padded_tile_sizes)),\n-        minor_to_major_layout_(std::move(minor_to_major_layout)),\n-        storage_type_(std::move(storage_type)) {}\n-};\n-\n-absl::StatusOr<TileInfo> TileInfo::Construct(\n-    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n-    const TiledHloInstruction& tiled_hlo) {\n-  TF_ASSIGN_OR_RETURN(SmallVector<Value> offsets,\n-                      ComputeOffsetsForTile(b, pid, runtime_values, tiled_hlo));\n-\n-  // Triton requires that all block dimensions are a power of 2.\n-  auto padded_tile_sizes = GetPaddedTileSizes(tiled_hlo.tile_sizes());\n-  SmallVector<int64_t> original_shape;\n-  original_shape.assign(tiled_hlo.hlo()->shape().dimensions().begin(),\n-                        tiled_hlo.hlo()->shape().dimensions().end());\n-\n-  const Shape& shape = tiled_hlo.hlo()->shape();\n-  TF_ASSIGN_OR_RETURN(Type expected_element_type,\n-                      TritonType(b, shape.element_type()));\n-  auto storage_type = StorageType(expected_element_type);\n-\n-  auto tile_strides = tiled_hlo.tile_strides();\n-  auto minor_to_major_layout = llvm::to_vector(LayoutUtil::MinorToMajor(shape));\n-\n-  return TileInfo(offsets, tile_strides, original_shape, padded_tile_sizes,\n-                  minor_to_major_layout, storage_type);\n-}\n-\n // Same as HLO BroadcastInDims. The sorted indices in `dims` specify the mapping\n // of the input dimensions to the output dimensions.\n TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n@@ -334,21 +216,6 @@ TensorValue Iota(EmitterLocOpBuilder b, int32_t limit) {\n   return b.create<stablehlo::IotaOp>(type, /*iota_dimension=*/0);\n }\n \n-TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n-                                 const TileInfo& tile_info, Value arg) {\n-  auto tensor_type = mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n-                                                 tile_info.storage_type());\n-\n-  return b.create<xla::xtile::ExtractTileOp>(\n-      tensor_type, arg, tile_info.offsets(), tile_info.padded_tile_sizes(),\n-      tile_info.tile_strides());\n-}\n-\n-absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n-    absl::Span<const HloInstruction* const> instructions,\n-    absl::flat_hash_map<const HloInstruction*, TensorValue>& values);\n-\n absl::StatusOr<TensorValue> EmitReduce(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_hlo_reduce,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n@@ -437,40 +304,6 @@ absl::StatusOr<TensorValue> EmitReduce(\n   return mlir::cast<TensorValue>(reduction.getResult(0));\n }\n \n-// Emit code corresponding to a fusion instruction somehow nested within the\n-// initial Triton fusion. This can happen when we carry around auxiliary\n-// computations, e.g. with reduces. Since we are emitting a single Triton\n-// fusion, we simply flatten the fusion inside the computation.\n-//\n-// TODO(b/331413981): get rid of this special handling once this is solved.\n-absl::StatusOr<TensorValue> EmitNestedFusion(\n-    EmitterLocOpBuilder b, const HloFusionInstruction& fusion_instruction,\n-    absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n-  // TODO(b/331402498): revisit the order of scope once we completely\n-  // deprecate Triton fusion analysis.\n-  const HloComputation* fusion_computation =\n-      fusion_instruction.fused_instructions_computation();\n-\n-  absl::flat_hash_map<const HloInstruction*, TensorValue> region_values;\n-\n-  std::vector<const HloInstruction*> to_emit;\n-  for (const HloInstruction* instr :\n-       fusion_computation->MakeInstructionPostOrder()) {\n-    if (instr->opcode() == HloOpcode::kParameter) {\n-      int64_t parameter_number = instr->parameter_number();\n-      auto it = values.find(fusion_instruction.operand(parameter_number));\n-      TF_RET_CHECK(it != values.end());\n-      TF_RET_CHECK(region_values.insert({instr, it->second}).second);\n-    } else {\n-      to_emit.push_back(instr);\n-    }\n-  }\n-\n-  TF_RET_CHECK(to_emit.back() == fusion_computation->root_instruction());\n-\n-  return EmitScope(b, /*analysis=*/nullptr, to_emit, region_values);\n-}\n-\n template <typename T>\n ArrayRef<T> MakeArrayRef(const absl::Span<const T> span) {\n   return ArrayRef(span.data(), span.size());\n@@ -1487,64 +1320,6 @@ absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n   return std::move(results);\n }\n \n-// Emit sequence of instructions using compatible tiling ordered producers\n-// before consumers.\n-absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n-    absl::Span<const HloInstruction* const> instructions,\n-    absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n-  for (const HloInstruction* hlo : instructions) {\n-    TensorValue result;\n-    if (hlo->opcode() == HloOpcode::kConcatenate ||\n-        hlo->opcode() == HloOpcode::kDynamicSlice) {\n-      // Parameter loads and their concatenations are handled outside EmitScope.\n-      TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n-      continue;\n-    } else if (hlo->opcode() == HloOpcode::kParameter) {\n-      if (hlo->users()[0]->opcode() == HloOpcode::kConcatenate ||\n-          hlo->users()[0]->opcode() == HloOpcode::kDynamicSlice) {\n-        continue;\n-      }\n-      TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n-      continue;\n-    } else if (hlo->opcode() == HloOpcode::kConstant) {\n-      TF_ASSIGN_OR_RETURN(result, EmitConstant(b, *hlo));\n-    } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n-      return absl::InvalidArgumentError(\n-          \"Broadcast is not yet supported in EmitScope().\");\n-    } else if (HloInstruction::IsOpElementwise(hlo->opcode())) {\n-      std::vector<Value> operands;\n-      operands.reserve(hlo->operands().size());\n-      for (const HloInstruction* operand : hlo->operands()) {\n-        operands.push_back(values[operand]);\n-      }\n-      TF_ASSIGN_OR_RETURN(Value elementwise_result,\n-                          EmitElementwise(b, *hlo, operands));\n-      result = mlir::cast<TensorValue>(elementwise_result);\n-    } else if (hlo->opcode() == HloOpcode::kTuple) {\n-      TF_RET_CHECK(hlo->IsRoot()) << hlo->ToString();\n-    } else if (hlo->opcode() == HloOpcode::kBitcast ||\n-               hlo->opcode() == HloOpcode::kTranspose ||\n-               hlo->opcode() == HloOpcode::kSlice ||\n-               hlo->opcode() == HloOpcode::kReshape ||\n-               hlo->opcode() == HloOpcode::kPad) {\n-      // All these are currently supported only as operations on indices\n-      // which are pushed to loads and stores. No operations on tiles are\n-      // performed here.\n-      result = values[hlo->operand(0)];\n-    } else if (hlo->opcode() == HloOpcode::kFusion) {\n-      const auto* fusion_instruction = ::xla::Cast<HloFusionInstruction>(hlo);\n-      TF_ASSIGN_OR_RETURN(result,\n-                          EmitNestedFusion(b, *fusion_instruction, values));\n-    } else {\n-      return absl::InvalidArgumentError(\n-          absl::StrCat(\"Unsupported operation \", hlo->ToString()));\n-    }\n-    TF_RET_CHECK(values.insert({hlo, result}).second) << hlo->ToString();\n-    VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n-  }\n-  return values[instructions.back()];\n-}\n }  // namespace\n \n namespace ir_emitter_triton_internal {"
        }
    ],
    "stats": {
        "total": 509,
        "additions": 278,
        "deletions": 231
    }
}