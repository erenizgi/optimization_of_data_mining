{
    "author": "mooskagh",
    "message": "[XLA:GPU] Move default Triton configs to text proto format.\n\nThis is to make default configuration consistent to what new `--xla_gpu_gemm_autotuner_override_file` flag takes.\n\nPiperOrigin-RevId: 842176141",
    "sha": "10344d0c57913c6abffe86c6dbc5bac8322b19f2",
    "files": [
        {
            "sha": "6a6246ee386be9d40a05f372f16454e0c71d8efc",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=10344d0c57913c6abffe86c6dbc5bac8322b19f2",
            "patch": "@@ -60,20 +60,20 @@ namespace {\n std::vector<TritonGemmConfig> GetDefaultTritonConfigs(\n     se::GpuComputeCapability compute_capability, bool autotune_tma) {\n   if (compute_capability.IsRocm()) {\n-    return *kDefaultRocmConfigs;\n+    return GetTritonConfigsForPlatform(TritonConfigsPlatform::kDefaultRocm);\n   }\n \n   CHECK(compute_capability.IsCuda());\n   auto* cuda_compute_capability = compute_capability.cuda_compute_capability();\n   std::vector<TritonGemmConfig> configs;\n \n   if (cuda_compute_capability->IsAtLeastBlackwell()) {\n-    configs = *kBlackwellConfigs;\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kBlackwell);\n   } else if (cuda_compute_capability->IsHopper() ||\n              cuda_compute_capability->IsAmpere()) {\n-    configs = *kHopperAmpereConfigs;\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kHopperAmpere);\n   } else {\n-    configs = *kDefaultCudaConfigs;\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kDefaultCuda);\n   }\n \n   if (!autotune_tma) {"
        },
        {
            "sha": "5ca2354d355add214ef03021041a229621e76835",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=10344d0c57913c6abffe86c6dbc5bac8322b19f2",
            "patch": "@@ -758,6 +758,16 @@ cc_library(\n \n cc_library(\n     name = \"triton_configs\",\n+    srcs = [\"triton_configs.cc\"],\n     hdrs = [\"triton_configs.h\"],\n-    deps = [\"//xla/service/gpu:matmul_utils\"],\n+    deps = [\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla/service/gpu:matmul_utils\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_protobuf//:protobuf\",\n+    ],\n )"
        },
        {
            "sha": "336b668d4b31606240bf6cd94add526af57eb62e",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=10344d0c57913c6abffe86c6dbc5bac8322b19f2",
            "patch": "@@ -118,11 +118,11 @@ std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n   std::vector<TritonGemmConfig> configs;\n \n   if (compute_capability.IsAtLeastBlackwell()) {\n-    configs = *kBlackwellConfigs;\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kBlackwell);\n   } else if (compute_capability.IsHopper() || compute_capability.IsAmpere()) {\n-    configs = *kHopperAmpereConfigs;\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kHopperAmpere);\n   } else {\n-    configs = *kDefaultCudaConfigs;\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kDefaultCuda);\n   }\n \n   if (!debug_options_.xla_gpu_experimental_enable_triton_tma() ||"
        },
        {
            "sha": "e7d072f1f0d96e3c3b3a34e3a8369c129e20d4c2",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_rocm.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc?ref=10344d0c57913c6abffe86c6dbc5bac8322b19f2",
            "patch": "@@ -49,7 +49,7 @@ GemmFusionAutotuner::GetPlatformCodegenBackends(\n \n std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     const {\n-  return *kDefaultRocmConfigs;\n+  return GetTritonConfigsForPlatform(TritonConfigsPlatform::kDefaultRocm);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "e57bb34bf71e97ec5e2f6b37ef7e19f79c2d522c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/triton_configs.cc",
            "status": "added",
            "additions": 207,
            "deletions": 0,
            "changes": 207,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.cc?ref=10344d0c57913c6abffe86c6dbc5bac8322b19f2",
            "patch": "@@ -0,0 +1,207 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/autotuning/triton_configs.h\"\n+\n+#include <initializer_list>\n+#include <iterator>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/base/no_destructor.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"google/protobuf/text_format.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+// TODO(b/467265599): Replace string constants with cc_embed_data when\n+// https://github.com/bazelbuild/rules_cc/issues/41 is fixed.\n+\n+constexpr absl::string_view kBlackwellTritonConfigs = R\"(\n+config { block_m: 128 block_n: 128 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 64 split_k: 1 num_stages: 1 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 64 split_k: 8 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 16 split_k: 512 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 32 split_k: 16 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 1 num_stages: 5 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 64 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 64 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 64 split_k: 2 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 64 split_k: 4 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 64 block_k: 64 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 64 block_k: 64 split_k: 16 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 64 block_k: 64 split_k: 8 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 64 split_k: 8 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 32 block_k: 64 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 256 block_n: 128 block_k: 64 split_k: 1 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+config { block_m: 256 block_n: 16 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 256 block_n: 32 block_k: 32 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 16 block_k: 32 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 32 block_n: 16 block_k: 512 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 16 block_k: 64 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 32 block_n: 16 block_k: 64 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 16 split_k: 1 num_stages: 1 num_warps: 16 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 16 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 1 num_stages: 2 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 128 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 32 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 64 split_k: 64 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 128 split_k: 8 num_stages: 1 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 16 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+)\";\n+\n+constexpr absl::string_view kDefaultCudaTritonConfigs = R\"(\n+config { block_m: 32 block_n: 32 block_k: 256 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 32 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 64 block_k: 64 split_k: 4 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 64 split_k: 4 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 256 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 128 block_k: 32 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 64 block_k: 128 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 128 block_k: 32 split_k: 8 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 512 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 16 block_k: 512 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 64 split_k: 1 num_stages: 2 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 32 split_k: 1 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+config { block_m: 256 block_n: 128 block_k: 32 split_k: 1 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+config { block_m: 256 block_n: 64 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 256 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 64 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 256 block_n: 128 block_k: 128 split_k: 1 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+config { block_m: 256 block_n: 64 block_k: 128 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 256 block_k: 128 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 128 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 64 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 32 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 128 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 256 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 64 split_k: 2 num_stages: 1 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 64 split_k: 1 num_stages: 2 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 64 block_k: 256 split_k: 8 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 256 block_n: 256 block_k: 128 split_k: 1 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+)\";\n+\n+constexpr absl::string_view kDefaultRocmTritonConfigs = R\"(\n+config { block_m: 32 block_n: 32 block_k: 256 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 32 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 64 block_k: 64 split_k: 4 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 64 split_k: 4 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 256 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 128 block_k: 32 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+)\";\n+\n+constexpr absl::string_view kHopperAmpereTritonConfigs = R\"(\n+config { block_m: 16 block_n: 16 block_k: 64 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 128 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 16 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 256 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 32 block_n: 32 block_k: 128 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 256 block_k: 32 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 256 block_k: 32 split_k: 16 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 32 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 32 split_k: 16 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 4 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 16 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 128 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 128 split_k: 16 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 64 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 128 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 128 split_k: 128 num_stages: 2 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 64 split_k: 4 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 128 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 256 split_k: 16 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 16 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 64 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 128 split_k: 8 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 256 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 32 split_k: 8 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 32 block_k: 32 split_k: 8 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 32 split_k: 8 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 32 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 64 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 8 block_k: 128 split_k: 2 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+)\";\n+\n+absl::flat_hash_map<TritonConfigsPlatform, std::vector<TritonGemmConfig>>\n+LoadTritonConfigs() {\n+  absl::flat_hash_map<TritonConfigsPlatform, std::vector<TritonGemmConfig>>\n+      result;\n+\n+  auto parse_config =\n+      [](absl::string_view config_str) -> std::vector<TritonGemmConfig> {\n+    TritonGemmConfigsProto proto;\n+    CHECK(tsl::protobuf::TextFormat::ParseFromString(config_str, &proto))\n+        << config_str;\n+    std::vector<TritonGemmConfig> configs;\n+    absl::c_transform(proto.config(), std::back_inserter(configs),\n+                      [](const AutotuneResult::TritonGemmKey& config_proto) {\n+                        absl::StatusOr<TritonGemmConfig> config =\n+                            TritonGemmConfig::FromProto(config_proto);\n+                        CHECK_OK(config);\n+                        return *config;\n+                      });\n+    return configs;\n+  };\n+\n+  const std::initializer_list<\n+      std::pair<TritonConfigsPlatform, absl::string_view>>\n+      kConfigsMap = {\n+          {TritonConfigsPlatform::kBlackwell, kBlackwellTritonConfigs},\n+          {TritonConfigsPlatform::kDefaultCuda, kDefaultCudaTritonConfigs},\n+          {TritonConfigsPlatform::kDefaultRocm, kDefaultRocmTritonConfigs},\n+          {TritonConfigsPlatform::kHopperAmpere, kHopperAmpereTritonConfigs},\n+      };\n+  for (const auto& [platform, config_str] : kConfigsMap) {\n+    result[platform] = parse_config(config_str);\n+  }\n+\n+  return result;\n+}\n+\n+}  // namespace\n+\n+const std::vector<TritonGemmConfig>& GetTritonConfigsForPlatform(\n+    TritonConfigsPlatform platform) {\n+  static const absl::NoDestructor<\n+      absl::flat_hash_map<TritonConfigsPlatform, std::vector<TritonGemmConfig>>>\n+      kConfigs(LoadTritonConfigs());\n+  return kConfigs->at(platform);\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "252b4be2b1b692b86d7e81d6d5b8a4ba12244bab",
            "filename": "third_party/xla/xla/service/gpu/autotuning/triton_configs.h",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10344d0c57913c6abffe86c6dbc5bac8322b19f2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.h?ref=10344d0c57913c6abffe86c6dbc5bac8322b19f2",
            "patch": "@@ -23,74 +23,15 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n-using Config = TritonGemmConfig;\n-\n-static const std::vector<TritonGemmConfig>* const kBlackwellConfigs =\n-    new std::vector<TritonGemmConfig>(\n-        {Config(128, 128, 32, 1, 4, 4), Config(128, 128, 64, 1, 1, 8),\n-         Config(128, 128, 64, 8, 3, 4), Config(128, 16, 16, 512, 4, 2),\n-         Config(128, 16, 32, 16, 3, 2), Config(128, 16, 64, 1, 5, 4),\n-         Config(128, 16, 64, 16, 3, 4), Config(128, 16, 64, 64, 1, 2),\n-         Config(128, 256, 64, 1, 4, 8), Config(128, 256, 64, 2, 4, 8),\n-         Config(128, 256, 64, 4, 3, 8), Config(128, 64, 64, 1, 3, 4),\n-         Config(128, 64, 64, 16, 4, 8), Config(128, 64, 64, 8, 4, 4),\n-         Config(16, 16, 128, 1, 3, 2),  Config(16, 16, 16, 1, 1, 2),\n-         Config(16, 16, 64, 8, 3, 2),   Config(16, 32, 64, 1, 3, 2),\n-         Config(256, 128, 64, 1, 3, 8), Config(256, 16, 16, 1, 1, 2),\n-         Config(256, 32, 32, 16, 3, 4), Config(32, 16, 32, 1, 4, 2),\n-         Config(32, 16, 512, 1, 1, 4),  Config(32, 16, 64, 1, 1, 2),\n-         Config(32, 16, 64, 1, 4, 2),   Config(64, 128, 16, 1, 1, 16),\n-         Config(64, 128, 16, 1, 3, 2),  Config(64, 128, 64, 1, 4, 4),\n-         Config(64, 16, 64, 1, 2, 2),   Config(64, 32, 128, 1, 3, 2),\n-         Config(64, 32, 32, 1, 4, 2),   Config(64, 32, 64, 64, 3, 2),\n-         Config(64, 64, 128, 8, 1, 8),  Config(64, 64, 16, 1, 1, 2),\n-         Config(64, 64, 16, 1, 3, 2)});\n-\n-static const std::vector<TritonGemmConfig>* const kHopperAmpereConfigs =\n-    new std::vector<TritonGemmConfig>(\n-        {Config(16, 16, 64, 1, 4, 2),    Config(16, 16, 128, 1, 4, 4),\n-         Config(16, 16, 128, 128, 4, 2), Config(16, 16, 128, 16, 1, 2),\n-         Config(16, 256, 16, 1, 1, 2),   Config(32, 32, 128, 16, 1, 4),\n-         Config(32, 256, 32, 1, 3, 4),   Config(32, 256, 32, 16, 3, 8),\n-         Config(64, 16, 32, 1, 4, 2),    Config(64, 16, 32, 16, 4, 2),\n-         Config(64, 16, 64, 1, 1, 4),    Config(64, 16, 64, 4, 3, 2),\n-         Config(64, 16, 64, 16, 4, 4),   Config(64, 16, 128, 1, 4, 2),\n-         Config(64, 16, 128, 16, 4, 4),  Config(64, 32, 32, 1, 4, 4),\n-         Config(64, 32, 64, 16, 3, 4),   Config(64, 32, 128, 1, 3, 2),\n-         Config(64, 32, 128, 128, 2, 4), Config(64, 64, 32, 1, 4, 4),\n-         Config(64, 64, 64, 1, 4, 4),    Config(64, 64, 64, 4, 4, 4),\n-         Config(64, 64, 128, 16, 3, 4),  Config(64, 64, 256, 16, 4, 8),\n-         Config(64, 128, 16, 1, 4, 2),   Config(64, 128, 64, 1, 3, 4),\n-         Config(64, 128, 128, 8, 1, 4),  Config(64, 256, 32, 1, 4, 4),\n-         Config(128, 16, 32, 8, 4, 2),   Config(128, 16, 64, 16, 3, 2),\n-         Config(128, 16, 64, 16, 1, 4),  Config(128, 32, 32, 8, 4, 2),\n-         Config(128, 128, 32, 8, 4, 8),  Config(128, 256, 32, 1, 4, 8),\n-         Config(128, 256, 64, 1, 4, 8),  Config(64, 8, 128, 2, 3, 4, 1)});\n-\n-static const std::vector<TritonGemmConfig>* const kDefaultCudaConfigs =\n-    new std::vector<TritonGemmConfig>(\n-        {Config(32, 32, 256, 1, 1, 4),   Config(64, 32, 32, 16, 1, 4),\n-         Config(32, 64, 64, 4, 1, 4),    Config(128, 128, 64, 4, 1, 4),\n-         Config(16, 16, 256, 1, 1, 4),   Config(16, 128, 32, 16, 1, 4),\n-         Config(16, 64, 128, 1, 1, 4),   Config(16, 128, 32, 8, 1, 4),\n-         Config(16, 16, 512, 1, 1, 4),   Config(32, 16, 512, 1, 1, 4),\n-         Config(64, 32, 64, 1, 2, 8),    Config(128, 256, 32, 1, 3, 8),\n-         Config(256, 128, 32, 1, 3, 8),  Config(256, 64, 32, 1, 4, 4),\n-         Config(64, 256, 32, 1, 4, 4),   Config(128, 64, 32, 1, 4, 4),\n-         Config(64, 128, 32, 1, 4, 4),   Config(256, 128, 128, 1, 3, 8),\n-         Config(256, 64, 128, 1, 4, 4),  Config(64, 256, 128, 1, 4, 4),\n-         Config(128, 128, 128, 1, 4, 4), Config(128, 64, 64, 1, 4, 4),\n-         Config(64, 128, 64, 1, 4, 4),   Config(128, 32, 64, 1, 4, 4),\n-         Config(64, 32, 64, 1, 4, 4),    Config(32, 128, 32, 1, 4, 4),\n-         Config(128, 128, 32, 1, 4, 4),  Config(16, 16, 256, 1, 3, 4),\n-         Config(128, 128, 64, 2, 1, 8),  Config(64, 64, 64, 1, 2, 4),\n-         Config(16, 64, 256, 8, 1, 4),   Config(256, 256, 128, 1, 3, 8)});\n-\n-static const std::vector<TritonGemmConfig>* const kDefaultRocmConfigs =\n-    new std::vector<TritonGemmConfig>(\n-        {Config(32, 32, 256, 1, 1, 4), Config(64, 32, 32, 16, 1, 4),\n-         Config(32, 64, 64, 4, 1, 4), Config(128, 128, 64, 4, 1, 4),\n-         Config(16, 16, 256, 1, 1, 4), Config(16, 128, 32, 16, 1, 4)});\n+enum class TritonConfigsPlatform {\n+  kBlackwell,\n+  kDefaultCuda,\n+  kDefaultRocm,\n+  kHopperAmpere,\n+};\n+\n+const std::vector<TritonGemmConfig>& GetTritonConfigsForPlatform(\n+    TritonConfigsPlatform);\n \n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 312,
        "additions": 235,
        "deletions": 77
    }
}