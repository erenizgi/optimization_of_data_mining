{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Refactor RaggedAllToAllStartThunk to make helper functions member methods.\n\nMove `RunMemCpyRaggedAllToAll` and `RunOneShotRaggedAllToAll` into `RaggedAllToAllStartThunk` and update their signatures to use `StreamState` and `config_` members.\n\nPiperOrigin-RevId: 804854848",
    "sha": "c92b9109998b46e3b823b168e0cb786731f1e959",
    "files": [
        {
            "sha": "4efc9ece304cf15ade463d53081ec54b389b9749",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 50,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c92b9109998b46e3b823b168e0cb786731f1e959/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c92b9109998b46e3b823b168e0cb786731f1e959/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=c92b9109998b46e3b823b168e0cb786731f1e959",
            "patch": "@@ -314,15 +314,17 @@ absl::Status RendezvousAfterKernelFinish(\n   return absl::OkStatus();\n }\n \n-absl::Status RunMemCpyRaggedAllToAll(\n-    const GpuCliqueKey& clique_key, RankId rank,\n-    int64_t ragged_row_element_size, int64_t num_total_updates,\n-    absl::Span<DeviceBufferPair const> buffers, se::Stream& stream,\n-    Communicator* comm, absl::Span<int64_t* const> ragged_metadata_allocs,\n-    se::Event* start_event, se::Event* end_event) {\n+}  // namespace\n+\n+absl::Status RaggedAllToAllStartThunk::RunMemCpyRaggedAllToAll(\n+    const GpuCliqueKey& clique_key, se::Stream& stream,\n+    const StreamState& state, absl::Span<DeviceBufferPair const> buffers,\n+    absl::Span<int64_t* const> ragged_metadata_allocs) {\n   int device_ordinal = stream.parent()->device_ordinal();\n+  const RankId& rank = state.rank;\n+  const int64_t num_ranks = clique_key.num_local_participants();\n+\n   VLOG(3) << \"[\" << device_ordinal << \"] Performing mem-copy-ragged-all-to-all\";\n-  TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm->NumRanks());\n \n   PrimitiveType element_type = buffers[0].element_type;\n \n@@ -332,7 +334,7 @@ absl::Status RunMemCpyRaggedAllToAll(\n   TF_RETURN_IF_ERROR(\n       LoadRaggedTensorMetadata(stream, buffers, ragged_metadata_allocs));\n \n-  int64_t num_updates_per_replica = num_total_updates / num_ranks;\n+  const int64_t num_updates_per_replica = config_.num_total_updates / num_ranks;\n \n   const int64_t* input_offsets = ragged_metadata_allocs[0];\n   const int64_t* send_sizes = ragged_metadata_allocs[1];\n@@ -342,44 +344,43 @@ absl::Status RunMemCpyRaggedAllToAll(\n       std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n       RendezvousBeforeKernelStart(\n           /*name=*/\"memcpy\", clique_key, rank, num_ranks, output_buffer, stream,\n-          start_event, end_event));\n+          state.start_event.get(), state.end_event.get()));\n \n   // Transfer a slice of data to each peer's output buffer.\n   for (int64_t i = 0; i < num_updates_per_replica; ++i) {\n     for (int peer = 0; peer < num_ranks; ++peer) {\n       int64_t idx = peer * num_updates_per_replica + i;\n       se::DeviceMemoryBase send_slice =\n           GpuCollectives::Slice(input_buffer, element_type,\n-                                input_offsets[idx] * ragged_row_element_size,\n-                                send_sizes[idx] * ragged_row_element_size);\n+                                input_offsets[idx] * config_.num_row_elements,\n+                                send_sizes[idx] * config_.num_row_elements);\n       se::DeviceMemoryBase dst_slice = GpuCollectives::Slice(\n           (*rendezvous_values)[peer].output_buffer, element_type,\n-          output_offsets[idx] * ragged_row_element_size,\n-          send_sizes[idx] * ragged_row_element_size);\n+          output_offsets[idx] * config_.num_row_elements,\n+          send_sizes[idx] * config_.num_row_elements);\n       TF_RETURN_IF_ERROR(\n           stream.MemcpyD2D(&dst_slice, send_slice, send_slice.size()));\n     }\n   }\n \n   TF_RETURN_IF_ERROR(RendezvousAfterKernelFinish(\n-      /*name=*/\"memcpy\", clique_key, rank, num_ranks, stream, end_event,\n-      rendezvous_values));\n+      /*name=*/\"memcpy\", clique_key, rank, num_ranks, stream,\n+      state.end_event.get(), rendezvous_values));\n \n   return absl::OkStatus();\n }\n \n-absl::Status RunOneShotRaggedAllToAll(\n-    const GpuCliqueKey& clique_key, int64_t num_input_rows,\n-    int64_t num_row_elements, int64_t num_total_updates,\n-    const std::vector<DeviceBufferPair>& buffers, se::Stream& stream,\n-    RankId rank, Communicator* comm, se::Event* start_event,\n-    se::Event* end_event) {\n+absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n+    const GpuCliqueKey& clique_key, se::Stream& stream,\n+    const StreamState& state, absl::Span<DeviceBufferPair const> buffers) {\n   int device_ordinal = stream.parent()->device_ordinal();\n+  const RankId& rank = state.rank;\n+\n+  const int64_t num_ranks = clique_key.num_local_participants();\n+\n   VLOG(3) << \"[\" << device_ordinal\n           << \"] Performing one-shot ragged-all-to-all rank: \" << rank.value();\n \n-  TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm->NumRanks());\n-\n   PrimitiveType element_type = buffers[0].element_type;\n \n   se::DeviceMemoryBase input_buffer = buffers[0].source_buffer;\n@@ -389,9 +390,9 @@ absl::Status RunOneShotRaggedAllToAll(\n       std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n       RendezvousBeforeKernelStart(\n           /*name=*/\"one-shot\", clique_key, rank, num_ranks, output_buffer,\n-          stream, start_event, end_event));\n+          stream, state.start_event.get(), state.end_event.get()));\n \n-  int64_t num_updates_per_replica = num_total_updates / num_ranks;\n+  const int64_t num_updates_per_replica = config_.num_total_updates / num_ranks;\n \n   absl::InlinedVector<se::DeviceMemoryBase, 4> output_ptrs;\n   for (auto& value : *rendezvous_values) {\n@@ -402,15 +403,13 @@ absl::Status RunOneShotRaggedAllToAll(\n       &stream, element_type, input_buffer, output_ptrs,\n       buffers[2].source_buffer, buffers[3].source_buffer,\n       buffers[4].source_buffer, num_ranks, num_updates_per_replica,\n-      num_input_rows, num_row_elements));\n+      config_.num_input_rows, config_.num_row_elements));\n \n   return RendezvousAfterKernelFinish(\n-      /*name=*/\"one-shot\", clique_key, rank, num_ranks, stream, end_event,\n-      rendezvous_values);\n+      /*name=*/\"one-shot\", clique_key, rank, num_ranks, stream,\n+      state.end_event.get(), rendezvous_values);\n }\n \n-}  // namespace\n-\n RaggedAllToAllStartThunk::RaggedAllToAllStartThunk(\n     ThunkInfo thunk_info, const HloRaggedAllToAllInstruction* instr,\n     std::vector<CollectiveThunk::Buffer> buffers, bool p2p_memcpy_enabled)\n@@ -539,8 +538,6 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n       ConvertToDeviceBuffers(params, buffers_,\n                              config_.config.operand_element_type));\n \n-  std::optional<RankId> rank =\n-      comm_handle.clique_key.rank(params.collective_params->global_device_id);\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm_handle.comm->NumRanks());\n \n   TF_ASSIGN_OR_RETURN(\n@@ -553,34 +550,29 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n     state = per_stream_states_[stream.parent()].get();\n   }\n \n-  // Get buffer allocs to load sizes and offsets of ragged tensors from device\n-  // memory.\n-  absl::InlinedVector<int64_t*, 8> ragged_metadata_allocs;\n-  for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n-    ragged_metadata_allocs.push_back(\n-        reinterpret_cast<int64_t*>(state->host_buffer_allocs[i]->opaque()));\n-  }\n-\n   bool should_use_one_shot_kernel =\n       is_local() && one_shot_kernel_enabled_ && peer_access_enabled &&\n       IsRaggedAllToAllKernelSupported(num_ranks,\n                                       device_buffers[0].element_type);\n \n   if (should_use_one_shot_kernel) {\n-    TF_RETURN_IF_ERROR(RunOneShotRaggedAllToAll(\n-        comm_handle.clique_key, config_.num_input_rows,\n-        config_.num_row_elements, config_.num_total_updates, device_buffers,\n-        stream, *rank, comm_handle.comm, state->start_event.get(),\n-        state->end_event.get()));\n+    TF_RETURN_IF_ERROR(RunOneShotRaggedAllToAll(comm_handle.clique_key, stream,\n+                                                *state, device_buffers));\n     return false;\n   }\n \n+  // Get buffer allocs to load sizes and offsets of ragged tensors from device\n+  // memory.\n+  absl::InlinedVector<int64_t*, 8> ragged_metadata_allocs;\n+  for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n+    ragged_metadata_allocs.push_back(\n+        reinterpret_cast<int64_t*>(state->host_buffer_allocs[i]->opaque()));\n+  }\n+\n   if (should_use_memcpy()) {\n-    TF_RETURN_IF_ERROR(RunMemCpyRaggedAllToAll(\n-        comm_handle.clique_key, *rank, config_.num_row_elements,\n-        config_.num_total_updates, device_buffers, stream, comm_handle.comm,\n-        ragged_metadata_allocs, state->start_event.get(),\n-        state->end_event.get()));\n+    TF_RETURN_IF_ERROR(RunMemCpyRaggedAllToAll(comm_handle.clique_key, stream,\n+                                               *state, device_buffers,\n+                                               ragged_metadata_allocs));\n     return false;\n   }\n "
        },
        {
            "sha": "44982a95c27cb289ce0082436cfadf7c80a280ac",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c92b9109998b46e3b823b168e0cb786731f1e959/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c92b9109998b46e3b823b168e0cb786731f1e959/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h?ref=c92b9109998b46e3b823b168e0cb786731f1e959",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n@@ -100,6 +101,15 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n         : device_ordinal(device_ordinal), rank(rank) {}\n   };\n \n+  absl::Status RunMemCpyRaggedAllToAll(\n+      const GpuCliqueKey& clique_key, se::Stream& stream,\n+      const StreamState& state, absl::Span<DeviceBufferPair const> buffers,\n+      absl::Span<int64_t* const> ragged_metadata_allocs);\n+\n+  absl::Status RunOneShotRaggedAllToAll(\n+      const GpuCliqueKey& clique_key, se::Stream& stream,\n+      const StreamState& state, absl::Span<DeviceBufferPair const> buffers);\n+\n   bool is_local() const;\n   bool should_use_memcpy() const { return p2p_memcpy_enabled_ && is_local(); }\n "
        }
    ],
    "stats": {
        "total": 102,
        "additions": 52,
        "deletions": 50
    }
}