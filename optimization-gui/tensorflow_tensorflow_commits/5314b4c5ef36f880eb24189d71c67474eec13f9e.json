{
    "author": "tlongeri",
    "message": "Add xla::LayoutUtil::IsUntiledLayout to check if tiling is a no-op\n\nPiperOrigin-RevId: 828227443",
    "sha": "5314b4c5ef36f880eb24189d71c67474eec13f9e",
    "files": [
        {
            "sha": "46931593d4d96698b1958b306a8a1959357ab903",
            "filename": "third_party/xla/xla/layout_util.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5314b4c5ef36f880eb24189d71c67474eec13f9e/third_party%2Fxla%2Fxla%2Flayout_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5314b4c5ef36f880eb24189d71c67474eec13f9e/third_party%2Fxla%2Fxla%2Flayout_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Flayout_util.cc?ref=5314b4c5ef36f880eb24189d71c67474eec13f9e",
            "patch": "@@ -632,4 +632,39 @@ Layout LayoutUtil::MoveDimToMinor(const Layout& layout, const int64_t dim) {\n              : std::nullopt;\n }\n \n+/*static*/ bool LayoutUtil::IsUntiledLayout(absl::Span<const Tile> tiles,\n+                                            absl::Span<const int64_t> shape) {\n+  // Tiles are applied recursively to expand current_shape\n+  // Example: (t0, t1) tile applied to (..., n, m) expands it to\n+  // (..., ceildiv(n, t0), ceildiv(m, t1), t0, t1)\n+  std::vector<int64_t> current_shape(shape.begin(), shape.end());\n+  for (const Tile& tile : tiles) {\n+    const int64_t tile_ndims = tile.dimensions().size();\n+    CHECK_LE(tile_ndims, current_shape.size());\n+    const absl::Span<const int64_t> tiled_shape =\n+        absl::Span<const int64_t>(current_shape).last(tile_ndims);\n+    // new_tiled_shape will hold the tiled shape after the tile is applied.\n+    std::vector<int64_t> new_tiled_shape(2 * tile_ndims);\n+    bool allow_multiple_tiles = true;\n+    for (int64_t i = 0; i < tile_ndims; ++i) {\n+      if (tiled_shape[i] % tile.dimension(i) != 0) {\n+        return false;\n+      }\n+      CHECK_GT(tile.dimension(i), 0);\n+      new_tiled_shape[i] = tiled_shape[i] / tile.dimension(i);\n+      new_tiled_shape[tile_ndims + i] = tile.dimension(i);\n+      if (!allow_multiple_tiles && new_tiled_shape[i] != 1) {\n+        return false;\n+      }\n+      if (tile.dimension(i) != 1) {\n+        allow_multiple_tiles = false;\n+      }\n+    }\n+    current_shape.erase(current_shape.end() - tile_ndims, current_shape.end());\n+    current_shape.insert(current_shape.end(), new_tiled_shape.begin(),\n+                         new_tiled_shape.end());\n+  }\n+  return true;\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "2c9a28edd858640c7e4a2dbbcc201c48a5d30442",
            "filename": "third_party/xla/xla/layout_util.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5314b4c5ef36f880eb24189d71c67474eec13f9e/third_party%2Fxla%2Fxla%2Flayout_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5314b4c5ef36f880eb24189d71c67474eec13f9e/third_party%2Fxla%2Fxla%2Flayout_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Flayout_util.h?ref=5314b4c5ef36f880eb24189d71c67474eec13f9e",
            "patch": "@@ -273,6 +273,13 @@ class LayoutUtil {\n \n   // Returns a shape's split config if present.\n   static std::optional<SplitConfig> GetSplitConfig(const Shape& shape);\n+\n+  // Returns true if the layout tiling is equivalent to having no tiles at all.\n+  // This is not a complete check and may return false for some unusual tilings\n+  // even if they _are_ effectively untiled.\n+  // The tiling should be valid for the provided shape.\n+  static bool IsUntiledLayout(absl::Span<const Tile> tiles,\n+                              absl::Span<const int64_t> shape);\n };\n \n }  // namespace xla"
        },
        {
            "sha": "f98caadf32e71a6bf7d9c651bcf975e8959241b4",
            "filename": "third_party/xla/xla/layout_util_test.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5314b4c5ef36f880eb24189d71c67474eec13f9e/third_party%2Fxla%2Fxla%2Flayout_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5314b4c5ef36f880eb24189d71c67474eec13f9e/third_party%2Fxla%2Fxla%2Flayout_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Flayout_util_test.cc?ref=5314b4c5ef36f880eb24189d71c67474eec13f9e",
            "patch": "@@ -513,5 +513,25 @@ TEST_F(LayoutUtilTest, MaxElementsInPerSplit) {\n   EXPECT_EQ(LayoutUtil::MaxElementsInPerSplit(shape), 150 * 90 * 70);\n }\n \n+struct IsUntiledLayoutTestCase {\n+  std::vector<int64_t> shape;\n+  std::vector<Tile> tiles;\n+  bool expected_result;\n+};\n+\n+using IsUntiledLayoutTest = ::testing::TestWithParam<IsUntiledLayoutTestCase>;\n+\n+TEST_P(IsUntiledLayoutTest, IsUntiledLayout) {\n+  IsUntiledLayoutTestCase params = GetParam();\n+  EXPECT_EQ(LayoutUtil::IsUntiledLayout(params.tiles, params.shape),\n+            params.expected_result);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(IsUntiledLayoutTests, IsUntiledLayoutTest,\n+                         ::testing::ValuesIn<IsUntiledLayoutTestCase>(\n+                             {{{24, 128}, {Tile({8, 128})}, true},\n+                              {{4, 256}, {Tile({1, 128})}, true},\n+                              {{2, 3, 4}, {Tile({8, 128})}, false}}));\n+\n }  // namespace\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 62,
        "additions": 62,
        "deletions": 0
    }
}