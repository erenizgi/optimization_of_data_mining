{
    "author": "mrguenther",
    "message": "Integrate StableHLO at openxla/stablehlo@1ef9e390\n\nPiperOrigin-RevId: 842827681",
    "sha": "ea93d433c3a1a99c918292e9ea846727413b5e2f",
    "files": [
        {
            "sha": "ca34dee010d16dd25d6ca6910b2d899bac0fd1d4",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 12,
            "deletions": 902,
            "changes": 914,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea93d433c3a1a99c918292e9ea846727413b5e2f/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea93d433c3a1a99c918292e9ea846727413b5e2f/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=ea93d433c3a1a99c918292e9ea846727413b5e2f",
            "patch": "@@ -34,41 +34,6 @@ diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel\n  gentbl_cc_library(\n      name = \"func_builder_inc\",\n      tbl_outs = {\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n---- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n-+++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n-@@ -913,6 +913,15 @@\n- \n- // -----\n- \n-+// CHECK-LABEL: func @reshape_0D_0D\n-+func.func @reshape_0D_0D(%arg0: tensor<i32>) ->tensor<i32> {\n-+  %0 = \"stablehlo.reshape\"(%arg0) : (tensor<i32>) -> tensor<i32>\n-+  func.return %0 : tensor<i32>\n-+}\n-+// CHECK: return %arg0 : tensor<i32>\n-+\n-+// -----\n-+\n- // CHECK-LABEL: func @reshape_0D_1D_unsigned\n- // CHECK-SAME:    %[[ARG_UNSIGNED:[a-zA-Z0-9_]*]]\n- func.func @reshape_0D_1D_unsigned(%arg0: tensor<ui32>) -> tensor<1xui32> {\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n---- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-@@ -1103,6 +1103,12 @@\n- \n-     if (!resultType.hasStaticShape()) return failure();\n- \n-+    // If the reshape is a no-op simply fold it away.\n-+    if (resultType == operandType) {\n-+      rewriter.replaceOp(reshapeOp, operand);\n-+      return success();\n-+    }\n-+\n-     // If any of the output dimensions is 0, the tensor has no elements. In that\n-     // case, we can just replace the reshape with an empty op.\n-     if (llvm::is_contained(resultType.getShape(), 0)) {\n diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp\n --- stablehlo/stablehlo/dialect/Base.cpp\n +++ stablehlo/stablehlo/dialect/Base.cpp\n@@ -130,91 +95,6 @@ diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.cpp b/stablehlo/stablehlo/diale\n    return success();\n  }\n  \n-diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp\n---- stablehlo/stablehlo/dialect/StablehloOps.cpp\n-+++ stablehlo/stablehlo/dialect/StablehloOps.cpp\n-@@ -4024,6 +4024,61 @@\n-   ReturnOp::create(*builder, loc, compare);\n- }\n- \n-+void buildMaxAndArgmaxBody(Type elementType, Type indices_type, Region& body,\n-+                           OpBuilder& builder) {\n-+  OpBuilder::InsertionGuard guard(builder);\n-+  if (body.getBlocks().empty()) builder.createBlock(&body);\n-+  Block* block = &body.getBlocks().front();\n-+\n-+  Type value_type = RankedTensorType::get(/*shape=*/{}, elementType);\n-+  Type index_type = RankedTensorType::get(/*shape=*/{}, indices_type);\n-+  Location loc = body.getLoc();\n-+  block->addArguments({value_type, index_type}, {loc, loc});\n-+  block->addArguments({value_type, index_type}, {loc, loc});\n-+\n-+  auto lhs_value = block->getArgument(0);\n-+  auto lhs_index = block->getArgument(1);\n-+  auto rhs_value = block->getArgument(2);\n-+  auto rhs_index = block->getArgument(3);\n-+\n-+  auto gt_pred =\n-+      builder\n-+          .create<CompareOp>(loc, lhs_value, rhs_value, ComparisonDirection::GT)\n-+          .getResult();\n-+\n-+  // Tie-Breaker Condition: (lhs == rhs) AND (lhs_index < rhs_index)\n-+  auto eq_pred =\n-+      builder\n-+          .create<CompareOp>(loc, lhs_value, rhs_value, ComparisonDirection::EQ)\n-+          .getResult();\n-+  auto lt_index_pred =\n-+      builder\n-+          .create<CompareOp>(loc, lhs_index, rhs_index, ComparisonDirection::LT)\n-+          .getResult();\n-+  auto tie_breaker_condition =\n-+      builder.create<AndOp>(loc, eq_pred, lt_index_pred).getResult();\n-+\n-+  // Final lhs Selection Condition: (gt_pred) OR (tie_breaker_condition)\n-+  auto final_lhs_condition =\n-+      builder.create<OrOp>(loc, gt_pred, tie_breaker_condition).getResult();\n-+\n-+  // Select Final Results:\n-+  // if final_lhs_condition:\n-+  //     return (lhs_value, lhs_index)\n-+  // else:\n-+  //     return (rhs_value, rhs_index)\n-+  auto selected_value = builder\n-+                            .create<stablehlo::SelectOp>(\n-+                                loc, final_lhs_condition, lhs_value, rhs_value)\n-+                            .getResult();\n-+  auto selected_index = builder\n-+                            .create<stablehlo::SelectOp>(\n-+                                loc, final_lhs_condition, lhs_index, rhs_index)\n-+                            .getResult();\n-+  builder.create<stablehlo::ReturnOp>(\n-+      loc, mlir::ValueRange{selected_value, selected_index});\n-+}\n-+\n- SortOp createSortOp(PatternRewriter* rewriter, const Location& loc,\n-                     const llvm::ArrayRef<Value>& operands,\n-                     const llvm::ArrayRef<Type>& elementTypes, int64_t dimension,\n-diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h\n---- stablehlo/stablehlo/dialect/StablehloOps.h\n-+++ stablehlo/stablehlo/dialect/StablehloOps.h\n-@@ -204,6 +204,16 @@\n-   stablehlo::ReturnOp::create(builder, loc, reducer.getResult());\n- }\n- \n-+// Builds the region `body` for a max-and-argmax computation, suitable for\n-+// use in ReduceWindow operations with varidic value and index inputs.\n-+// It creates four block arguments (val1, idx1, val2, idx2) of `elementType` and\n-+// `indices_type`, and returns two results: result_val and result_idx.\n-+// result_val is the maximum of val1 and val2, and result_idx is the index\n-+// corresponding to result_val. If val1 >= val2, idx1 is returned, otherwise\n-+// idx2 is returned.\n-+void buildMaxAndArgmaxBody(Type elementType, Type indices_type, Region& body,\n-+                           OpBuilder& builder);\n-+\n- // PrecisionConfigAttr is a constraint attribute on ArrayAttrs.\n- // Create this class to allow for building this attr similar to other\n- // attributes.\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt b/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n --- stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n +++ stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n@@ -414,28 +294,6 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp b/\n +\n +}  // namespace chlo\n +}  // namespace mlir\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n-@@ -203,6 +203,9 @@\n-   // If the op does not support type inference, return a default output shape\n-   // parameter that must be injected.\n-   MethodParameter getDefaultOutputShape() {\n-+    if (hasSingleVariadicResult(getOp()) || getOp().getNumResults() > 1) {\n-+      return MethodParameter(\"TypeRange\", \"resultTypes\");\n-+    }\n-     return MethodParameter(\"Type\", \"resultType\");\n-   }\n- \n-@@ -276,7 +279,7 @@\n-     BuilderParams params = getOpBuilderParameters();\n-     SmallVector<MethodParameter> parameters;\n-     if (params.outputShape.has_value()) {\n--      parameters.push_back(getDefaultOutputShape());\n-+      parameters.push_back(params.outputShape.value());\n-     }\n-     for (auto& operand : params.operands) {\n-       parameters.push_back(\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n --- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n +++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n@@ -447,61 +305,6 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b\n    if (isa<ComplexType>(inputType.getElementType()) &&\n        !isa<ComplexType>(resultElementType)) {\n      operand = stablehlo::Real(operand);\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n-@@ -17,12 +17,12 @@\n- #include <cstdint>\n- #include <string>\n- \n--#include \"gtest/gtest.h\"\n- #include \"mlir/IR/BuiltinAttributes.h\"\n- #include \"mlir/IR/BuiltinOps.h\"\n- #include \"mlir/IR/DialectRegistry.h\"\n- #include \"mlir/IR/MLIRContext.h\"\n- #include \"mlir/IR/OwningOpRef.h\"\n-+#include \"mlir/IR/Types.h\"\n- #include \"mlir/IR/Verifier.h\"\n- #include \"mlir/Support/DebugStringHelper.h\"\n- #include \"mlir/Support/LLVM.h\"\n-@@ -32,6 +32,7 @@\n- #include \"stablehlo/integrations/cpp/builder/FuncBuilder.h\"\n- #include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n- #include \"stablehlo/integrations/cpp/builder/StablehloBuilder.h\"\n-+#include \"gtest/gtest.h\"\n- \n- namespace mlir {\n- namespace stablehlo {\n-@@ -1517,6 +1518,29 @@\n-   EXPECT_EQ(expected, debugString(*module));\n- }\n- \n-+TEST(MlirBuilderTest, VariadicResult) {\n-+  std::string expected = R\"mlir(module {\n-+  func.func @main() -> (tensor<f64>, tensor<f64>) {\n-+    %0:2 = stablehlo.custom_call @two_outs() : () -> (tensor<f64>, tensor<f64>)\n-+    return %0#0, %0#1 : tensor<f64>, tensor<f64>\n-+  }\n-+})mlir\";\n-+\n-+  StablehloModuleBuilder mb;\n-+  {\n-+    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n-+    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n-+    auto type = makeTensorType(fb.getContext(), {}, ElementType::F64);\n-+    SmallVector<Type> resultTypes = {type, type};\n-+    // Pass double data with i64 type.\n-+    auto cc = stablehlo::CustomCall(fb, resultTypes, {}, \"two_outs\");\n-+    func::Return(fb, {cc});\n-+  }\n-+\n-+  OwningOpRef<ModuleOp> module = mb->build();\n-+  EXPECT_EQ(expected, debugString(*module));\n-+}\n-+\n- ////////\n- // Custom Attribute Tests\n- ////////\n diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n --- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n +++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n@@ -888,100 +691,6 @@ diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast\n      -> tensor<4xf32> {\n    // CHECK-NOT: chlo.broadcast_zeta\n    // CHECK-NOT: chlo.zeta\n-diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stablehlo/tests/ops_broadcasting.mlir\n---- stablehlo/stablehlo/tests/ops_broadcasting.mlir\n-+++ stablehlo/stablehlo/tests/ops_broadcasting.mlir\n-@@ -92,6 +92,8 @@\n- // [<=10] x [1] => [<=10]\n- // [1] x [<=10] => [<=10]\n- // [1] x [1, <=10, 1] => [1, <=10, 1]\n-+// [5] x [10, 1] => [10, 5]\n-+// [5] x [<=10, 1] => [<=10, 5]\n- \n- \n- // [1] x [1] => [1]\n-@@ -232,6 +234,38 @@\n- \n- // -----\n- \n-+// [5] x [10, 1] => [10, 5]\n-+// CHECK-LABEL: func @tensor_broadcast_5_x_10_1\n-+func.func @tensor_broadcast_5_x_10_1(%arg0: tensor<5xf64>, %arg1: tensor<10x1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf64>) -> tensor<10x5xf64>\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<10x1xf64>) -> tensor<10x5xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<5xf64>, tensor<10x1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [<=10, 1] x [5] => [<=10, 5]\n-+// CHECK-LABEL: func @tensor_broadcast_b5_1_x_5\n-+func.func @tensor_broadcast_b5_1_x_5(\n-+  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+  %arg1: tensor<5xf64>\n-+) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x5xf64, #stablehlo.bounds<10, ?>>\n-+  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf64>) -> tensor<10x5xf64>\n-+  // CHECK: %[[ARG0_DIM0_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[ARG0_DIM0_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (\n-+    tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+    tensor<5xf64>\n-+  ) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n- //////\n- // N-ary broadcast tests.\n- \n-@@ -247,3 +281,42 @@\n-   return %0 : !stablehlo.token\n- }\n- \n-+// -----\n-+\n-+/////\n-+// Broadcast errors\n-+\n-+// [10] x [5] => error\n-+// expected-error @+1 {{incompatible shapes for broadcasting 10 and 5}}\n-+func.func @broadcast_error_10_x_5(%arg0: tensor<10xf64>, %arg1: tensor<5xf64>) -> !stablehlo.token {\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, tensor<5xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [10] x [<=10] => error\n-+// expected-error @+1 {{cannot mix bounded and static dimensions in broadcast}}\n-+func.func @broadcast_error_10_x_b10(%arg0: tensor<10xf64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [10] x not_tensor => error\n-+func.func @broadcast_error_not_tensor(%arg0: tensor<10xf64>, %arg1: !stablehlo.token) -> !stablehlo.token {\n-+  // expected-error @+1 {{expected ranked tensor type for broadcast inputs}}\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, !stablehlo.token) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [] => error\n-+func.func @broadcast_error_empty() -> !stablehlo.token {\n-+  // expected-error @+1 {{requires at least one operand to broadcast}}\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"() : () -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir b/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n --- stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n +++ stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n@@ -994,104 +703,6 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir b/stablehlo/\n    func.return %0 : tensor<16x16xf32>\n  }\n  \n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -47,8 +47,8 @@\n- ////////\n- // CaseOp\n- \n--// CHECK-LABEL: func.func @case_fold_constant_branch_index\n--func.func @case_fold_constant_branch_index(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {\n-+// CHECK-LABEL: func.func @case_fold_constant_branch_index_int_result\n-+func.func @case_fold_constant_branch_index_int_result(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {\n-   // CHECK-NEXT: {{(^ *|func\\.)}}return %arg1\n-   // CHECK-NOT:  stablehlo.case\n-   %branch_index = stablehlo.constant dense<1> : tensor<i32>\n-@@ -60,6 +60,47 @@\n-     stablehlo.return %arg2 : tensor<i32>\n-   }) : (tensor<i32>) -> tensor<i32>\n-   func.return %result: tensor<i32>\n-+}\n-+\n-+// -----\n-+\n-+// CHECK-LABEL: func.func @case_fold_constant_branch_index_complex_result\n-+func.func @case_fold_constant_branch_index_complex_result(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>, %arg2: tensor<complex<f32>>) -> tensor<complex<f32>> {\n-+  // CHECK-NEXT: {{(^ *|func\\.)}}return %arg1\n-+  // CHECK-NOT:  stablehlo.case\n-+  %branch_index = stablehlo.constant dense<1> : tensor<i32>\n-+  %result = \"stablehlo.case\"(%branch_index) ({\n-+    stablehlo.return %arg0 : tensor<complex<f32>>\n-+  }, {\n-+    stablehlo.return %arg1 : tensor<complex<f32>>\n-+  }, {\n-+    stablehlo.return %arg2 : tensor<complex<f32>>\n-+  }) : (tensor<i32>) -> tensor<complex<f32>>\n-+  func.return %result: tensor<complex<f32>>\n-+}\n-+\n-+// -----\n-+\n-+// CHECK-LABEL: func.func @case_fold_inline_call_tf_function\n-+func.func @case_fold_inline_call_tf_function(%arg0: !stablehlo.token {jax.token = true}, %arg1: tensor<16xi32>, %arg2: tensor<16xi64>) -> (!stablehlo.token {jax.token = true}, tensor<16xi32> {jax.result_info = \"result\"}) {\n-+  // CHECK: [[RESULT_TOKEN:%.+]] = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2)\n-+  // CHECK: [[UNUSED_TOKEN:%.+]] = {{\"?}}stablehlo.case{{\"?}}(\n-+  // CHECK: return [[RESULT_TOKEN]], %arg1\n-+  %c = stablehlo.constant dense<1> : tensor<i32>\n-+  %c_0 = stablehlo.constant dense<0> : tensor<i32>\n-+  %0 = \"stablehlo.case\"(%c_0) ({\n-+    stablehlo.return %c_0 : tensor<i32>\n-+  }, {\n-+    stablehlo.return %c : tensor<i32>\n-+  }) : (tensor<i32>) -> tensor<i32>\n-+  %1 = \"stablehlo.case\"(%0) ({\n-+    %2 = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_index = 0 : i64, has_token_input_output = true}} : (!stablehlo.token, tensor<16xi32>, tensor<16xi64>) -> !stablehlo.token\n-+    stablehlo.return %2 : !stablehlo.token\n-+  }, {\n-+    %2 = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_index = 1 : i64, has_token_input_output = true}} : (!stablehlo.token, tensor<16xi32>, tensor<16xi64>) -> !stablehlo.token\n-+    stablehlo.return %2 : !stablehlo.token\n-+  }) : (tensor<i32>) -> !stablehlo.token\n-+  return %1, %arg1 : !stablehlo.token, tensor<16xi32>\n- }\n- \n- // -----\n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n-@@ -128,6 +128,16 @@\n-   return %7 : tensor<3x2x3x3xi32>\n- }\n- \n-+// CHECK-LABEL: func.func @broadcast_in_dim_nested_bounded\n-+func.func @broadcast_in_dim_nested_bounded(%arg0: tensor<3x3xi32>, %arg1: tensor<i32>) -> tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>> {\n-+  // CHECK: [[SDS:%.+]] = stablehlo.set_dimension_size\n-+  // CHECK-NEXT: stablehlo.broadcast_in_dim [[SDS]], dims = [2, 0] : (tensor<?x3xi32, #stablehlo.bounds<3, ?>>) -> tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>>\n-+  %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<3x3xi32>, tensor<i32>) -> tensor<?x3xi32, #stablehlo.bounds<3, ?>>\n-+  %1 = stablehlo.broadcast_in_dim %0, dims = [1, 0] : (tensor<?x3xi32, #stablehlo.bounds<3, ?>>) -> tensor<3x?x2xi32, #stablehlo.bounds<?, 3, ?>>\n-+  %2 = stablehlo.broadcast_in_dim %1, dims = [0, 2, 1] : (tensor<3x?x2xi32, #stablehlo.bounds<?, 3, ?>>) -> tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>>\n-+  return %2 : tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>>\n-+}\n-+\n- // CHECK-LABEL: func.func @broadcast_in_dim_reshape\n- // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x6xi32>)\n- func.func @broadcast_in_dim_reshape(%arg0: tensor<3x6xi32>)\n-@@ -140,6 +150,15 @@\n- \n-   // CHECK-NEXT: return [[R0]], [[R5]]\n-   return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>\n-+}\n-+\n-+// CHECK-LABEL: func.func @broadcast_in_dim_bounded_no_reshape\n-+func.func @broadcast_in_dim_bounded_no_reshape(%arg0: tensor<20xf32>, %arg1: tensor<i32>) -> tensor<1x?xf32, #stablehlo.bounds<?, 20>> {\n-+  %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<20xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<20>>\n-+  // CHECK: stablehlo.set_dimension_size\n-+  // CHECK-NEXT: stablehlo.broadcast_in_dim\n-+  %1 = stablehlo.broadcast_in_dim %0, dims = [1] : (tensor<?xf32, #stablehlo.bounds<20>>) -> tensor<1x?xf32, #stablehlo.bounds<?, 20>>\n-+  return %1 : tensor<1x?xf32, #stablehlo.bounds<?, 20>>\n- }\n- \n- // CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape\n diff --ruN a/stablehlo/stablehlo/transforms/CMakeLists.txt b/stablehlo/stablehlo/transforms/CMakeLists.txt\n --- stablehlo/stablehlo/transforms/CMakeLists.txt\n +++ stablehlo/stablehlo/transforms/CMakeLists.txt\n@@ -1268,15 +879,16 @@ diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stable\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-@@ -59,29 +59,11 @@\n+@@ -59,26 +59,6 @@\n    };\n  }\n  \n -FailureOr<Dimensions> getDimensions(Value op) {\n -  // Get tensor type\n -  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n -  if (!tensor_type)\n--    return emitError(op.getLoc(), \"expected ranked tensor type\");\n+-    return emitError(op.getLoc(),\n+-                     \"expected ranked tensor type for broadcast inputs\");\n -\n -  auto encoding =\n -      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n@@ -1291,40 +903,12 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n -  return dimensions;\n -}\n -\n--FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(const Dimensions& a,\n-+FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(Value op,\n-+                                                       const Dimensions& a,\n+ FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(Value op,\n+                                                        const Dimensions& a,\n                                                         const Dimensions& b) {\n-   LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] inputs: \"\n--                          << toString(a) << \" * \" << toString(b));\n-+                          << toString(a) << \" * \" << toString(b) << \"\\n\");\n-   size_t max_rank = std::max(a.size(), b.size());\n-   Dimensions result(max_rank);\n- \n-@@ -110,14 +92,14 @@\n- \n-     // If both LHS and RHS are not 1, dim size must match.\n-     if (dim_a.size != dim_b.size) {\n--      return emitError(a[a_idx].boundOp.value().getLoc(),\n--                       \"incompatible shapes for broadcasting \")\n-+      // FIXME\n-+      return emitError(op.getLoc(), \"incompatible shapes for broadcasting \")\n-              << dim_a.size << \" and \" << dim_b.size;\n-     }\n- \n-     // If bounded both must be bounded\n-     if (dim_a.boundOp.has_value() != dim_b.boundOp.has_value()) {\n--      return emitError(a[a_idx].boundOp.value().getLoc(),\n-+      return emitError(op.getLoc(),\n-                        \"cannot mix bounded and static dimensions in broadcast\");\n-     }\n- \n-@@ -126,8 +108,30 @@\n-   }\n- \n+@@ -130,6 +110,28 @@\n    LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n--                          << toString(result));\n-+                          << toString(result) << \"\\n\");\n+                           << toString(result) << \"\\n\");\n    return result;\n +}\n +\n@@ -1351,170 +935,18 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n  }\n  \n  mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n-@@ -153,10 +157,12 @@\n+@@ -155,7 +157,6 @@\n    return mlir::RankedTensorType::get(shape, element_type, encoding);\n  }\n  \n -}  // namespace\n--\n--FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops) {\n--  if (ops.empty()) return failure();\n-+\n-+FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n-+                                             ArrayRef<Value> ops) {\n-+  if (ops.empty())\n-+    return emitError(builder.getInsertionPoint()->getLoc(),\n-+                     \"requires at least one operand to broadcast\");\n- \n-   Value first = ops[0];\n-   auto bcastShapeOrFail = getDimensions(first);\n-@@ -168,7 +174,7 @@\n-     auto dims = getDimensions(currOp);\n-     if (failed(dims)) return failure();\n-     auto currBcastShapeOrFail =\n--        getNumpyBroadcastShapeWithBounds(bcastShape, *dims);\n-+        getNumpyBroadcastShapeWithBounds(currOp, bcastShape, *dims);\n-     if (failed(currBcastShapeOrFail)) return failure();\n-     bcastShape = std::move(*currBcastShapeOrFail);\n-   }\n-@@ -192,7 +198,7 @@\n- FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n-                                                      ArrayRef<Value> operands) {\n-   // Figure out the broadcast shape\n--  auto bcastShapeOrFail = getNumpyBroadcastShape(operands);\n-+  auto bcastShapeOrFail = getNumpyBroadcastShape(builder, operands);\n-   if (failed(bcastShapeOrFail)) return failure();\n-   Dimensions bcastShape = std::move(*bcastShapeOrFail);\n- \n-@@ -208,35 +214,34 @@\n- \n- FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n-                                         const Dimensions& shape) {\n--  LLVM_DEBUG(llvm::dbgs() << \"[BroadcastIfNeeded] input: \" << input\n--                          << \" shape: \" << toString(shape));\n-+  LLVM_DEBUG(llvm::dbgs() << \"[numpyBroadcastIfNeeded] Broadcasting input \"\n-+                          << input.getType() << \" => \" << toString(shape)\n-+                          << \"\\n\");\n-   auto loc = input.getLoc();\n--  mlir::RankedTensorType input_type =\n-+  mlir::RankedTensorType inputType =\n-       dyn_cast<RankedTensorType>(input.getType());\n--  if (!input_type) return emitError(input.getLoc(), \"expected tensor type\");\n--  mlir::RankedTensorType output_type =\n--      getRankedTensorType(shape, input_type.getElementType());\n-+  if (!inputType)\n-+    return emitError(loc, \"expected ranked tensor type for broadcast inputs\");\n-+  mlir::RankedTensorType outputType =\n-+      getRankedTensorType(shape, inputType.getElementType());\n- \n-   // Short circuit if no broadcasting is needed.\n--  if (input_type == output_type) return input;\n--\n--  int64_t input_rank = input_type.getRank();\n--  int64_t output_rank = output_type.getRank();\n--  if (input_rank > output_rank)\n-+  if (inputType == outputType) return input;\n-+\n-+  int64_t inputRank = inputType.getRank();\n-+  int64_t outputRank = outputType.getRank();\n-+  if (inputRank > outputRank)\n-     return emitError(loc, \"input rank must be <= output rank, got \")\n--           << input_rank << \" vs \" << output_rank;\n--\n--  size_t rank_diff = output_rank - input_rank;\n--  SmallVector<int64_t> bcast_dims;\n--  bcast_dims.reserve(input_rank);\n--\n-+           << inputRank << \" vs \" << outputRank;\n-+\n-+  size_t rankDiff = outputRank - inputRank;\n-   auto inputShapeOrFail = getDimensions(input);\n-   if (failed(inputShapeOrFail)) return failure();\n-   Dimensions inputShape = std::move(*inputShapeOrFail);\n- \n-   // Construct broadcast dimensions.\n-   auto broadcastDimensions = llvm::to_vector(\n--      llvm::seq<int64_t>(output_rank - input_rank, output_rank));\n-+      llvm::seq<int64_t>(outputRank - inputRank, outputRank));\n- \n-   // Construct the result type of the broadcast\n-   //  - If input is static and target shape is static, use static shape.\n-@@ -244,33 +249,35 @@\n-   //  - If input is not bounded, but target shape is bounded, broadcast to\n-   //    the padded shape then call SetDimensionSize to make dynamic.\n-   auto bcastShape = shape;\n--  for (int64_t i = 0; i < input_rank; ++i) {\n--    int64_t input_dim_size = inputShape[i].size;\n--    int64_t result_idx = i + rank_diff;\n--    int64_t result_dim_size = shape[result_idx].size;\n--    if (input_dim_size != 1 && input_dim_size != result_dim_size)\n-+  for (int64_t i = 0; i < inputRank; ++i) {\n-+    int64_t inputDimSize = inputShape[i].size;\n-+    int64_t resultIdx = i + rankDiff;\n-+    int64_t resultDimSize = shape[resultIdx].size;\n-+    if (inputDimSize != 1 && inputDimSize != resultDimSize)\n-       return emitError(loc, \"Cannot broadcast input: \")\n--             << input_type << \" to target shape \" << toString(shape);\n-+             << inputType << \" to target shape \" << toString(shape);\n- \n-     if (!inputShape[i].boundOp.has_value() &&\n--        shape[result_idx].boundOp.has_value()) {\n-+        shape[resultIdx].boundOp.has_value()) {\n-       // Use padded shape in broadcast.\n--      bcastShape[result_idx] = DimensionInfo{shape[result_idx].size};\n--    }\n--    bcast_dims.push_back(result_idx);\n-+      bcastShape[resultIdx] = DimensionInfo{shape[resultIdx].size};\n-+    }\n-   }\n- \n-   // Broadcast to padded size for remaining dimensions.\n--  for (size_t i = input_rank; i < shape.size(); ++i) {\n-+  for (size_t i = 0; i < rankDiff; ++i) {\n-     bcastShape[i] = DimensionInfo{shape[i].size};\n-   }\n- \n-   // Insert broadcast ops\n--  mlir::RankedTensorType bcast_type =\n--      getRankedTensorType(bcastShape, input_type.getElementType());\n--  Value bcast_op = stablehlo::BroadcastInDimOp::create(\n--      builder, loc, bcast_type, input, broadcastDimensions);\n--  if (bcast_op.getType() == output_type) return bcast_op;\n-+  mlir::RankedTensorType bcastType =\n-+      getRankedTensorType(bcastShape, inputType.getElementType());\n-+  LLVM_DEBUG(\n-+      llvm::dbgs() << \"[numpyBroadcastIfNeeded] Broadcast to padded type \"\n-+                   << bcastType << \"\\n\");\n-+  Value bcastOp = stablehlo::BroadcastInDimOp::create(\n-+      builder, loc, bcastType, input, broadcastDimensions);\n-+  if (bcastOp.getType() == outputType) return bcastOp;\n- \n-   // Mark the padded broadcast as dynamic where the result is bounded.\n-   // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n-@@ -278,13 +285,13 @@\n-   for (size_t i = 0; i < shape.size(); ++i) {\n-     if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n-       Value boundOp = shape[i].boundOp.value();\n--      auto dim_size = stablehlo::GetDimensionSizeOp::create(\n-+      auto dimSize = stablehlo::GetDimensionSizeOp::create(\n-           builder, loc, boundOp, shape[i].boundOpDim);\n--      bcast_op = stablehlo::SetDimensionSizeOp::create(builder, loc, bcast_op,\n--                                                       dim_size, i);\n--    }\n--  }\n--  return bcast_op;\n-+      bcastOp = stablehlo::SetDimensionSizeOp::create(builder, loc, bcastOp,\n-+                                                       dimSize, i);\n-+    }\n-+  }\n-+  return bcastOp;\n- }\n  \n- }  // namespace stablehlo\n+ FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n+                                              ArrayRef<Value> ops) {\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n-@@ -47,9 +47,18 @@\n+@@ -47,6 +47,14 @@\n  using Dimensions = SmallVector<DimensionInfo>;\n  std::string toString(const Dimensions& dims);\n  \n@@ -1528,327 +960,5 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stabl\n +\n  // Returns the common shape these ops would broadcast to, or an error if the\n  // ops are not broadcastable.\n--FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops);\n-+FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n-+                                             ArrayRef<Value> ops);\n- \n- // Apply numpy broadcasting to the given operands, returning an error if any\n- // operands are not broadcastable.\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -14,6 +14,7 @@\n- \n- #include <cassert>\n- #include <cmath>\n-+#include <complex>\n- #include <cstddef>\n- #include <cstdint>\n- #include <functional>\n-@@ -38,6 +39,7 @@\n- #include \"mlir/Dialect/CommonFolders.h\"\n- #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n- #include \"mlir/Dialect/Utils/IndexingUtils.h\"\n-+#include \"mlir/IR/Builders.h\"\n- #include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n- #include \"mlir/IR/BuiltinAttributes.h\"\n- #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n-@@ -82,6 +84,71 @@\n-                 /*isUnsigned=*/!isSigned);\n- }\n- \n-+class LazyPlaceholderValue {\n-+ public:\n-+  static FailureOr<LazyPlaceholderValue> preparePlaceholderFor(\n-+      PatternRewriter& rewriter, Value likeValue) {\n-+    Type valueType = likeValue.getType();\n-+\n-+    // If `getZeroAttr(valueType)` returns a valid attribute, simply wrap the\n-+    // result in a `stablehlo.constant` op.\n-+    if (TypedAttr placeholderAttr = rewriter.getZeroAttr(valueType)) {\n-+      return LazyPlaceholderValue([&rewriter, placeholderAttr](Location loc) {\n-+        return ConstantOp::create(rewriter, loc, placeholderAttr);\n-+      });\n-+    }\n-+\n-+    // `getZeroAttr` doesn't support complex types, so we handle that case here.\n-+    if (auto shapedType = dyn_cast<ShapedType>(valueType)) {\n-+      if (auto complexElementType =\n-+              dyn_cast<ComplexType>(shapedType.getElementType())) {\n-+        if (!isa<FloatType>(complexElementType.getElementType()))\n-+          return rewriter.notifyMatchFailure(\n-+              likeValue.getLoc(),\n-+              \"unexpected real component type for complex element type\");\n-+        auto realImagComponentFloatType =\n-+            cast<FloatType>(complexElementType.getElementType());\n-+        APFloat apFloatZero(0.0);\n-+        bool losesInfo;\n-+        apFloatZero.convert(realImagComponentFloatType.getFloatSemantics(),\n-+                            llvm::RoundingMode::NearestTiesToEven, &losesInfo);\n-+        std::complex<APFloat> complexZeroScalar(apFloatZero, apFloatZero);\n-+        auto complexZeroSplat =\n-+            SplatElementsAttr::get(shapedType, complexZeroScalar);\n-+        return LazyPlaceholderValue(\n-+            [&rewriter, complexZeroSplat](Location loc) {\n-+              return ConstantOp::create(rewriter, loc, complexZeroSplat);\n-+            });\n-+      }\n-+    }\n-+\n-+    // If `valueType` is a token type, use `stablehlo.after_all` with no\n-+    // arguments to create a placeholder token.\n-+    if (isa<TokenType>(valueType)) {\n-+      return LazyPlaceholderValue([&rewriter](Location loc) {  //\n-+        return AfterAllOp::create(rewriter, loc, {});\n-+      });\n-+    }\n-+\n-+    // TODO: Support quantized and buffer types.\n-+\n-+    return rewriter.notifyMatchFailure(\n-+        likeValue.getLoc(), \"unable to create placeholder value for type\");\n-+  }\n-+\n-+  Value createAt(Location loc) const {\n-+    if (!lazyInitializer)\n-+      llvm::report_fatal_error(\"No lazy initializer for this value type.\");\n-+    return lazyInitializer(loc);\n-+  }\n-+\n-+ private:\n-+  LazyPlaceholderValue(std::function<Value(Location)> lazyInitializer)\n-+      : lazyInitializer(std::move(lazyInitializer)) {}\n-+\n-+  std::function<Value(Location)> lazyInitializer;\n-+};\n-+\n- LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,\n-                                         Operation* op, ShapedType resultType) {\n-   if (!resultType.hasStaticShape())\n-@@ -737,18 +804,14 @@\n-     Operation* terminator = blockToInline->getTerminator();\n-     ValueRange results = terminator->getOperands();\n- \n--    // TODO: Add support for complex, quantized, and token return types.\n--    // Currently, this pattern only supports int and float return types. We'll\n--    // need a more general equivalent of `getZeroAttr` to support other types.\n--    SmallVector<TypedAttr> placeholderAttrs;\n-+    SmallVector<LazyPlaceholderValue> lazyPlaceholderResults;\n-     for (auto result : op.getResults()) {\n--      TypedAttr placeholderAttr = rewriter.getZeroAttr(result.getType());\n--      if (!placeholderAttr)\n--        return rewriter.notifyMatchFailure(\n--            op,\n--            \"The case op's return type isn't currently supported by this \"\n--            \"optimization pattern.\");\n--      placeholderAttrs.push_back(placeholderAttr);\n-+      auto placeholder =\n-+          LazyPlaceholderValue::preparePlaceholderFor(rewriter, result);\n-+\n-+      if (failed(placeholder)) return failure();\n-+\n-+      lazyPlaceholderResults.push_back(std::move(placeholder.value()));\n-     }\n- \n-     // Inline the active branch of the `case` op.\n-@@ -763,9 +826,9 @@\n-     Block& noopBlock = region.emplaceBlock();\n-     SmallVector<Value> placeholderResults;\n-     rewriter.setInsertionPointToEnd(&noopBlock);\n--    for (auto placeholderAttr : placeholderAttrs) {\n-+    for (const auto& lazyPlaceholderResult : lazyPlaceholderResults) {\n-       placeholderResults.push_back(\n--          ConstantOp::create(rewriter, region.getLoc(), placeholderAttr));\n-+          lazyPlaceholderResult.createAt(region.getLoc()));\n-     }\n-     stablehlo::ReturnOp::create(rewriter, region.getLoc(), placeholderResults);\n- \n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n-@@ -44,7 +44,8 @@\n-     \"same number of elements\">;\n- \n- def BroadcastNotReducibleToReshape : Constraint<\n--    CPred<\"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && \"\n-+    CPred<\"!llvm::cast<ShapedType>($0.getType()).hasStaticShape() || \"\n-+          \"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && \"\n-           \"!(\"\n-             \"llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && \"\n-             \"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\"\n-@@ -134,8 +135,7 @@\n- \n- def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n- \n--def MergeDiscardableAttributes\n--    : NativeCodeCall<\"mergeDiscardableAttributes($0, $1)\">;\n-+def MergeDiscardableAttributes : NativeCodeCall<\"mergeDiscardableAttributes($0, $1)\">;\n- \n- def StableHLO_ConvertOpWithShape : NativeCodeCall<\n-     \"stablehlo::ConvertOp::create($_builder, $_loc, $0.getType(), $1)\">;\n-@@ -151,10 +151,10 @@\n- \n- // op(cst, X) -> op(X, cst)\n- class CanonicalizeConstantToRhs<Op StableHLO_OpType>\n--    : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n--          (StableHLO_OpType:$new_op $rhs, $lhs),\n--          [(NotConstantOp $rhs), (CommutativeOp $op)],\n--          [(MergeDiscardableAttributes $op, $new_op)]>;\n-+  : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n-+        (StableHLO_OpType:$new_op $rhs, $lhs),\n-+        [(NotConstantOp $rhs), (CommutativeOp $op)],\n-+        [(MergeDiscardableAttributes $op, $new_op)]>;\n- \n- ////////\n- // AddOp\n-@@ -165,9 +165,9 @@\n- \n- // Pattern: add(X, 0) -> X\n- def AddOp_RemoveNoop\n--    : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),\n--          (replaceWithValue $lhs), [],\n--          [(MergeDiscardableAttributes $op, $lhs)]>;\n-+  : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),\n-+        (replaceWithValue $lhs), [],\n-+        [(MergeDiscardableAttributes $op, $lhs)]>;\n- \n- ////////\n- // AndOp\n-@@ -177,25 +177,26 @@\n-   : CanonicalizeConstantToRhs<StableHLO_AndOp>;\n- \n- // Pattern: and(X, 0) -> 0\n--def AndOp_FoldToZero : Pat<(StableHLO_AndOp:$op $lhs,\n--                               (StableHLO_ConstantOp:$zero IntZero:$value)),\n--                           (replaceWithValue $zero), [],\n--                           [(MergeDiscardableAttributes $op, $zero)]>;\n-+def AndOp_FoldToZero\n-+  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n-+        (replaceWithValue $zero), [],\n-+        [(MergeDiscardableAttributes $op, $zero)]>;\n- \n- // Pattern: and(X, 1) -> X\n--def AndOp_RemoveNoop : Pat<(StableHLO_AndOp:$op $lhs,\n--                               (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n--                           (replaceWithValue $lhs), [],\n--                           [(MergeDiscardableAttributes $op, $lhs)]>;\n-+def AndOp_RemoveNoop\n-+  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n-+        (replaceWithValue $lhs), [],\n-+        [(MergeDiscardableAttributes $op, $lhs)]>;\n- \n- ////////\n- // BroadcastInDimOp\n- \n- // Pattern: broadcast_in_dim(X, [iota...]) -> X\n- def BroadcastInDimOp_RemoveNoop\n--    : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n--          (replaceWithValue $operand), [(TypesEqual $op, $operand)],\n--          [(MergeDiscardableAttributes $op, $operand)]>;\n-+  : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n-+        (replaceWithValue $operand),\n-+        [(TypesEqual $op, $operand)],\n-+        [(MergeDiscardableAttributes $op, $operand)]>;\n- \n- // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])\n- //       -> broadcast_in_dim(X, merge(dimsA, dimsB))\n-@@ -210,8 +211,10 @@\n- \n- // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])\n- //          [if same numel]\n-+// TODO: Figure out if static extents matching is valid (i.e. <=10 -> 1x[<=10])\n-+// for bounded dynamism, same for BroadcastInDimOp_ReplaceWithReshape\n- def BroadcastInDimOp_ReplaceWithReshape\n--  : Pat<(StableHLO_BroadcastInDimOp:$op $operand, SortedDims:$dims),\n-+  : Pat<(StableHLO_BroadcastInDimOp:$op AnyStaticShapeTensor:$operand, SortedDims:$dims),\n-         (StableHLO_ReshapeOpWithShape $op, $operand),\n-         [(NumberOfElementsEqual $op, $operand)],\n-         [],\n-@@ -220,7 +223,7 @@\n- // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])\n- //          [if same numel & rank]\n- def BroadcastInDimOp_ReplaceWithTranspose\n--  : Pat<(StableHLO_BroadcastInDimOp:$op $operand, $dims),\n-+  : Pat<(StableHLO_BroadcastInDimOp:$op AnyStaticShapeTensor:$operand, $dims),\n-         (StableHLO_TransposeOp $operand, (InvertBroadcastDims $dims)),\n-         [(NumberOfElementsEqual $op, $operand), (RankEqual $op, $operand)]>;\n- \n-@@ -259,9 +262,10 @@\n- \n- // Pattern: convert(X, [X.type]) -> X\n- def ConvertOp_RemoveNoop\n--    : Pat<(StableHLO_ConvertOp:$convert $operand),\n--          (replaceWithValue $operand), [(TypesEqual $convert, $operand)],\n--          [(MergeDiscardableAttributes $convert, $operand)]>;\n-+  : Pat<(StableHLO_ConvertOp:$convert $operand),\n-+        (replaceWithValue $operand),\n-+        [(TypesEqual $convert, $operand)],\n-+        [(MergeDiscardableAttributes $convert, $operand)]>;\n- \n- ////////\n- // DynamicBroadcastInDimOp\n-@@ -447,16 +451,16 @@\n- //\n- // Multiplication by 0. This fold is not trivial for floats in presence of NaNs,\n- // so we currently only enable it for ints.\n--def MulOp_FoldToZero : Pat<(StableHLO_MulOp:$mul_op $lhs,\n--                               (StableHLO_ConstantOp:$zero IntZero:$value)),\n--                           (replaceWithValue $zero), [],\n--                           [(MergeDiscardableAttributes $mul_op, $zero)]>;\n-+def MulOp_FoldToZero\n-+  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n-+        (replaceWithValue $zero), [],\n-+        [(MergeDiscardableAttributes $mul_op, $zero)]>;\n- \n- // Pattern: multiply(X, 1i) -> X\n- def MulOp_RemoveNoop\n--    : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n--          (replaceWithValue $lhs), [],\n--          [(MergeDiscardableAttributes $mul_op, $lhs)]>;\n-+  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n-+        (replaceWithValue $lhs), [],\n-+        [(MergeDiscardableAttributes $mul_op, $lhs)]>;\n- \n- ////////\n- // OrOp\n-@@ -465,16 +469,16 @@\n- def OrOp_CanonicalizeConstantToRhs : CanonicalizeConstantToRhs<StableHLO_OrOp>;\n- \n- // Pattern: or(X, 1) -> 1\n--def OrOp_FoldToOne : Pat<(StableHLO_OrOp:$op $lhs,\n--                             (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n--                         (replaceWithValue $one), [],\n--                         [(MergeDiscardableAttributes $op, $one)]>;\n-+def OrOp_FoldToOne\n-+  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n-+        (replaceWithValue $one), [],\n-+        [(MergeDiscardableAttributes $op, $one)]>;\n- \n- // Pattern: or(X, 0) -> X\n--def OrOp_RemoveNoop : Pat<(StableHLO_OrOp:$op $lhs,\n--                              (StableHLO_ConstantOp:$zero IntZero:$value)),\n--                          (replaceWithValue $lhs), [],\n--                          [(MergeDiscardableAttributes $op, $lhs)]>;\n-+def OrOp_RemoveNoop\n-+  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n-+        (replaceWithValue $lhs), [],\n-+        [(MergeDiscardableAttributes $op, $lhs)]>;\n- \n- ////////\n- // PadOp\n-@@ -574,10 +578,10 @@\n-         (StableHLO_ConstantLike<\"0\"> $operand)>;\n- \n- // Pattern: subtract(X, 0) -> X\n--def SubtractOp_RemoveNoop : Pat<(StableHLO_SubtractOp:$op $lhs,\n--                                    (StableHLO_ConstantOp AnyZero:$value)),\n--                                (replaceWithValue $lhs), [],\n--                                [(MergeDiscardableAttributes $op, $lhs)]>;\n-+def SubtractOp_RemoveNoop\n-+  : Pat<(StableHLO_SubtractOp:$op $lhs, (StableHLO_ConstantOp AnyZero:$value)),\n-+        (replaceWithValue $lhs), [],\n-+        [(MergeDiscardableAttributes $op, $lhs)]>;\n- \n- ////////\n- // SliceOp\n+ FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n "
        },
        {
            "sha": "48e631619a6888f93dee0c85ec435d17d00e05c0",
            "filename": "third_party/xla/third_party/stablehlo/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea93d433c3a1a99c918292e9ea846727413b5e2f/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea93d433c3a1a99c918292e9ea846727413b5e2f/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl?ref=ea93d433c3a1a99c918292e9ea846727413b5e2f",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n     # LINT.IfChange\n-    STABLEHLO_COMMIT = \"96acdcb7724f4a9eec6d2e5af2597b0750c13948\"\n-    STABLEHLO_SHA256 = \"68e068a78d71f0764d5dd385ef434df922050530de99001969493298a00d64a0\"\n+    STABLEHLO_COMMIT = \"1ef9e390b5295e676d2b864fe1924bc2f3f4cf0f\"\n+    STABLEHLO_SHA256 = \"818c951ad0ba0ac6c26d3ed01fed8f9a0e5ca93f5aed35005f75f0faf11bdfb0\"\n     # LINT.ThenChange(Google-internal path)\n \n     tf_http_archive("
        }
    ],
    "stats": {
        "total": 918,
        "additions": 14,
        "deletions": 904
    }
}