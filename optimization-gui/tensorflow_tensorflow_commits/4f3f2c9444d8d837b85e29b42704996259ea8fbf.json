{
    "author": "loislo",
    "message": "[XLA:GPU] add NanCount thunk to thunk_buffer_debug_pass\n\nWe call the pass for f32 and bf16 output buffers.\n\nPiperOrigin-RevId: 826808271",
    "sha": "4f3f2c9444d8d837b85e29b42704996259ea8fbf",
    "files": [
        {
            "sha": "3f81398cb24407aac403de156ced8a539690a0ce",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -2902,6 +2902,7 @@ cc_library(\n     hdrs = [\"thunk_buffer_debug_pass.h\"],\n     deps = [\n         \":buffers_checksum_thunk\",\n+        \":buffers_nan_count_thunk\",\n         \":custom_call_thunk\",\n         \":sequential_thunk\",\n         \":thunk\",\n@@ -2934,6 +2935,7 @@ xla_cc_test(\n     srcs = [\"thunk_buffer_debug_pass_test.cc\"],\n     deps = [\n         \":buffers_checksum_thunk\",\n+        \":buffers_nan_count_thunk\",\n         \":custom_call_thunk\",\n         \":sequential_thunk\",\n         \":thunk\",\n@@ -3096,6 +3098,7 @@ xla_test(\n         \":thunk\",\n         \":thunk_buffer_id\",\n         \":thunk_id\",\n+        \"//xla:types\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:executable\",\n         \"//xla/service/gpu:buffer_allocations\","
        },
        {
            "sha": "20feebfd07bd7e301f5a0121cd62883d01747c2b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -88,18 +87,21 @@ absl::Status BuffersDebugNanCountThunk::ExecuteOnStream(\n   se::gpu::BufferDebugLog buffer_debug_log =\n       se::gpu::BufferDebugLog::FromDeviceMemoryUnchecked(log_ptr);\n \n-  for (const auto& [entry_id, buffer_slice_pair] : buffers_) {\n-    BufferAllocation::Slice buffer = buffer_slice_pair.buffer;\n-    PrimitiveType buffer_type = buffer_slice_pair.element_type;\n+  for (const auto& [entry_id, buffer] : buffers_) {\n+    PrimitiveType buffer_type = buffer.element_type();\n     se::DeviceMemoryBase device_buffer =\n         params.buffer_allocations->GetDeviceAddress(buffer);\n     if (buffer_type == PrimitiveType::F32) {\n+      VLOG(1) << \"F32 buffer detected with id: \" << entry_id\n+              << \" and size: \" << device_buffer.size();\n       se::DeviceMemory<float> f32_buffer(device_buffer);\n       TF_RETURN_IF_ERROR(kernel_f32_->Launch(\n           thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n           f32_buffer, f32_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n           buffer_debug_log.GetDeviceEntries()));\n     } else if (buffer_type == PrimitiveType::BF16) {\n+      VLOG(1) << \"BF16 buffer detected with id: \" << entry_id\n+              << \" and size: \" << device_buffer.size();\n       se::DeviceMemory<Eigen::bfloat16> bf16_buffer(device_buffer);\n       TF_RETURN_IF_ERROR(kernel_bf16_->Launch(\n           thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n@@ -117,10 +119,9 @@ absl::Status BuffersDebugNanCountThunk::ExecuteOnStream(\n std::string BuffersDebugNanCountThunk::ToString(int indent) const {\n   std::string result;\n   absl::StrAppend(&result, \", buffers = \", buffers_.size());\n-  for (const auto& [buffer_id, buffer_slice_pair] : buffers_) {\n+  for (const auto& [buffer_id, buffer] : buffers_) {\n     absl::StrAppend(&result, \"\\n\", std::string(indent + 2, ' '),\n-                    \"buffer_id: \", buffer_id,\n-                    \", buffer: \", buffer_slice_pair.buffer.ToString());\n+                    \"buffer_id: \", buffer_id, \", buffer: \", buffer.ToString());\n   }\n   return result;\n }"
        },
        {
            "sha": "e4ee0f896a9babf189b45b0938ab97c2377f0bb2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -31,14 +31,9 @@ namespace xla::gpu {\n \n class BuffersDebugNanCountThunk : public Thunk {\n  public:\n-  struct BufferToCount {\n-    BufferAllocation::Slice buffer;\n-    PrimitiveType element_type;\n-  };\n-\n   explicit BuffersDebugNanCountThunk(\n       ThunkInfo info, BufferAllocation::Slice log_slice,\n-      absl::flat_hash_map<ThunkBufferId, BufferToCount> buffers)\n+      absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers)\n       : Thunk(Thunk::Kind::kBuffersDebugNanCount, std::move(info)),\n         log_slice_(log_slice),\n         buffers_(std::move(buffers)) {}\n@@ -53,14 +48,19 @@ class BuffersDebugNanCountThunk : public Thunk {\n     return {};\n   }\n \n+  const absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice>&\n+  buffer_slices() const {\n+    return buffers_;\n+  }\n+\n  private:\n   // Loaded in Initialize.\n   std::optional<stream_executor::gpu::BufferDebugNanCountF32Kernel::KernelType>\n       kernel_f32_;\n   std::optional<stream_executor::gpu::BufferDebugNanCountBf16Kernel::KernelType>\n       kernel_bf16_;\n   BufferAllocation::Slice log_slice_;\n-  absl::flat_hash_map<ThunkBufferId, BufferToCount> buffers_;\n+  absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "3aceee5b65233cc8629aad3dccb9dbd310cbeec3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk_test.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 15,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/types.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -85,12 +86,19 @@ TEST_F(BuffersDebugNanCountThunkTest, CalculatesNanCounts) {\n   BufferAllocation alloc(/*index=*/0,\n                          /*size=*/kTotalDeviceMemoryBytes,\n                          /*color=*/0);\n+  int64_t input_offset = kLogSize;\n   BufferAllocation::Slice log_slice(&alloc, /*offset=*/0, kLogSize);\n+  input_offset += kLogSize;\n+\n   BufferAllocation::Slice inputs[2];\n-  for (int i = 0; i < 2; ++i) {\n-    inputs[i] = BufferAllocation::Slice(\n-        &alloc, /*offset=*/kLogSize + i * kInputSizeInBytes, kInputSizeInBytes);\n-  }\n+  int64_t input_size_bf16 = kInputElems * sizeof(Eigen::bfloat16);\n+  inputs[0] = BufferAllocation::Slice(&alloc, input_offset, input_size_bf16,\n+                                      PrimitiveType::BF16);\n+  input_offset += input_size_bf16;\n+\n+  inputs[1] = BufferAllocation::Slice(\n+      &alloc, input_offset, kInputElems * sizeof(float), PrimitiveType::F32);\n+\n   BufferAllocations allocations(\n       {executor_->AllocateArray<uint8_t>(kTotalDeviceMemoryBytes)},\n       executor_->device_ordinal(), allocator_.get());\n@@ -102,13 +110,18 @@ TEST_F(BuffersDebugNanCountThunkTest, CalculatesNanCounts) {\n                           BufferDebugLog::CreateOnDevice(\n                               *stream_, se::DeviceMemory<uint8_t>(log_mem)));\n   // Fill inputs with some data\n-  std::vector<float> data(kInputElems, 0);\n-  data[123] = std::numeric_limits<float>::quiet_NaN();\n-  TF_ASSERT_OK(stream_->Memcpy(&inputs0_mem, data.data(), kInputSizeInBytes));\n-  data[123] = 0;\n-  data[456] = std::numeric_limits<float>::quiet_NaN();\n-  data[789] = std::numeric_limits<float>::quiet_NaN();\n-  TF_ASSERT_OK(stream_->Memcpy(&inputs1_mem, data.data(), kInputSizeInBytes));\n+  {\n+    std::vector<Eigen::bfloat16> data(kInputElems, Eigen::bfloat16(0));\n+    data[123] = std::numeric_limits<Eigen::bfloat16>::quiet_NaN();\n+    TF_ASSERT_OK(stream_->Memcpy(&inputs0_mem, data.data(), kInputSizeInBytes));\n+  }\n+  {\n+    std::vector<float> data(kInputElems, 0);\n+    data[456] = std::numeric_limits<float>::quiet_NaN();\n+    data[789] = std::numeric_limits<float>::quiet_NaN();\n+    TF_ASSERT_OK(stream_->Memcpy(&inputs1_mem, data.data(), kInputSizeInBytes));\n+  }\n+\n   // Setup parameters for Initialize/Prepare/ExecuteOnStream\n   Thunk::InitializeParams init_params;\n   init_params.executor = executor_;\n@@ -121,10 +134,8 @@ TEST_F(BuffersDebugNanCountThunkTest, CalculatesNanCounts) {\n \n   BuffersDebugNanCountThunk thunk(\n       Thunk::ThunkInfo(), log_slice,\n-      {{ThunkBufferId::Create(ThunkId(123), 4).value(),\n-        {inputs[0], PrimitiveType::F32}},\n-       {ThunkBufferId::Create(ThunkId(456), 8).value(),\n-        {inputs[1], PrimitiveType::F32}}});\n+      {{ThunkBufferId::Create(ThunkId(123), 4).value(), inputs[0]},\n+       {ThunkBufferId::Create(ThunkId(456), 8).value(), inputs[1]}});\n   TF_ASSERT_OK(thunk.Initialize(init_params));\n   TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}, resource_requests));\n   TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));"
        },
        {
            "sha": "48bddfd291c9a3628a2c8c0ac5f4524b8c2205b5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass.cc",
            "status": "modified",
            "additions": 90,
            "deletions": 5,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.cc?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/runtime/buffers_checksum_thunk.h\"\n+#include \"xla/backends/gpu/runtime/buffers_nan_count_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -65,7 +66,7 @@ namespace {\n // If the thunk got wrapped, the data dependencies between the thunks will be\n // configured to ensure `predecessor_thunk` executes before the wrapped thunk\n // and `successor_thunk` executes after.\n-absl::StatusOr<std::unique_ptr<Thunk>> WrapThunk(\n+absl::StatusOr<std::unique_ptr<Thunk>> WrapWithChecksumThunk(\n     std::unique_ptr<Thunk> thunk, BufferAllocation::Slice log_slice,\n     const Thunk& predecessor_thunk, Thunk& successor_thunk) {\n   const auto& thunk_buffers = thunk->buffer_uses();\n@@ -127,6 +128,79 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapThunk(\n   return wrapped_thunk;\n }\n \n+absl::StatusOr<std::unique_ptr<Thunk>> WrapWithNanCounterThunk(\n+    std::unique_ptr<Thunk> thunk, BufferAllocation::Slice log_slice,\n+    const Thunk& predecessor_thunk, Thunk& successor_thunk) {\n+  const auto& thunk_buffers = thunk->buffer_uses();\n+  if (thunk_buffers.empty()) {\n+    VLOG(1) << \"No buffers in thunk \" << thunk->thunk_info().thunk_id\n+            << \", skipping\";\n+    return thunk;\n+  }\n+\n+  absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers_to_check;\n+  for (size_t buffer_idx = 0; buffer_idx < thunk_buffers.size(); ++buffer_idx) {\n+    VLOG(1) << \"Buffer \" << buffer_idx << \" in thunk \"\n+            << thunk->thunk_info().thunk_id;\n+    const BufferUse& use = thunk_buffers[buffer_idx];\n+    const BufferAllocation::Slice& slice = use.slice();\n+    if (slice.allocation() == nullptr) {\n+      VLOG(1) << \"Buffer \" << buffer_idx << \" in thunk \"\n+              << thunk->thunk_info().thunk_id\n+              << \" has null allocation, skipping\";\n+      continue;\n+    }\n+    auto buffer_id =\n+        ThunkBufferId::Create(thunk->thunk_info().thunk_id, buffer_idx);\n+    if (!buffer_id.ok()) {\n+      LOG(WARNING) << \"ThunkBufferId::Create failed: Skipping buffer \"\n+                   << buffer_idx << \" in thunk \" << thunk->thunk_info().thunk_id\n+                   << \": \" << buffer_id.status();\n+      continue;\n+    }\n+    if (slice.element_type() != PrimitiveType::F32 &&\n+        slice.element_type() != PrimitiveType::BF16) {\n+      VLOG(1) << \"Buffer \" << buffer_idx << \" in thunk \"\n+              << thunk->thunk_info().thunk_id\n+              << \" has unsupported element type \"\n+              << PrimitiveType_Name(slice.element_type()) << \", skipping\";\n+      continue;\n+    }\n+    if (!use.HasDefinedContentsOnOutput()) {\n+      VLOG(1) << \"Buffer \" << buffer_idx << \" in thunk \"\n+              << thunk->thunk_info().thunk_id\n+              << \" has no defined contents on output, skipping\";\n+      continue;\n+    }\n+    buffers_to_check.emplace(buffer_id.value(), use.slice());\n+    VLOG(1) << \"Found buffer \" << buffer_idx << \" in thunk \"\n+            << thunk->thunk_info().thunk_id << \" with element type \"\n+            << PrimitiveType_Name(slice.element_type()) << \" and size \"\n+            << slice.size();\n+  }\n+\n+  if (buffers_to_check.empty()) {\n+    return thunk;\n+  }\n+\n+  VLOG(1) << \"Wrapping thunk \" << thunk->thunk_info().thunk_id\n+          << \" with nan counter thunk due to presence of buffers: \"\n+          << buffers_to_check.size();\n+  std::vector<std::unique_ptr<Thunk>> thunk_and_checks;\n+  Thunk* thunk_ptr = thunk.get();\n+  thunk_and_checks.push_back(std::move(thunk));\n+  auto buffer_debug_nan_counter_thunk =\n+      std::make_unique<BuffersDebugNanCountThunk>(Thunk::ThunkInfo(), log_slice,\n+                                                  std::move(buffers_to_check));\n+  buffer_debug_nan_counter_thunk->add_control_predecessor(thunk_ptr);\n+  thunk_and_checks.push_back(std::move(buffer_debug_nan_counter_thunk));\n+  auto wrapped_thunk = std::make_unique<SequentialThunk>(\n+      Thunk::ThunkInfo(), std::move(thunk_and_checks));\n+  wrapped_thunk->add_control_predecessor(&predecessor_thunk);\n+  successor_thunk.add_control_predecessor(wrapped_thunk.get());\n+  return wrapped_thunk;\n+}\n+\n XLA_FFI_DEFINE_HANDLER_SYMBOL(\n     kDebugLogInitHandler,\n     [](se::Stream* absl_nonnull stream, xla::ffi::Buffer<U8> log_buffer) {\n@@ -207,10 +281,21 @@ absl::StatusOr<bool> ThunkBufferDebugPass::Run(\n \n   ThunkSequence& thunks = root_thunk->thunks();\n   for (auto& thunk : thunks) {\n-    TF_ASSIGN_OR_RETURN(\n-        thunk, WrapThunk(std::move(thunk), log_slice,\n-                         /*predecessor_thunk=*/*buffer_debug_init_thunk.get(),\n-                         /*successor_thunk=*/*buffer_debug_dump_thunk.get()));\n+    if (mode_ == Mode::kChecksum) {\n+      VLOG(1) << \"Wrapping with checksum thunk\";\n+      TF_ASSIGN_OR_RETURN(\n+          thunk, WrapWithChecksumThunk(\n+                     std::move(thunk), log_slice,\n+                     /*predecessor_thunk=*/*buffer_debug_init_thunk.get(),\n+                     /*successor_thunk=*/*buffer_debug_dump_thunk.get()));\n+    } else if (mode_ == Mode::kNanCounter) {\n+      VLOG(1) << \"Wrapping with nan counter thunk\";\n+      TF_ASSIGN_OR_RETURN(\n+          thunk, WrapWithNanCounterThunk(\n+                     std::move(thunk), log_slice,\n+                     /*predecessor_thunk=*/*buffer_debug_init_thunk.get(),\n+                     /*successor_thunk=*/*buffer_debug_dump_thunk.get()));\n+    }\n   }\n \n   thunks.reserve(thunks.size() + 2);"
        },
        {
            "sha": "3b2219d89c5417fb565b234075a2e538e30b21ff",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass.h",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.h?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -30,7 +30,12 @@ namespace gpu {\n // Adds buffer debug tracing to thunks.\n class ThunkBufferDebugPass : public ThunkPassInterface {\n  public:\n-  ThunkBufferDebugPass() = default;\n+  enum class Mode {\n+    kChecksum,\n+    kNanCounter,\n+  };\n+\n+  explicit ThunkBufferDebugPass(Mode mode) : mode_(mode) {}\n \n   absl::string_view name() const override { return \"thunk-buffer-debug\"; }\n \n@@ -39,6 +44,9 @@ class ThunkBufferDebugPass : public ThunkPassInterface {\n                            const HloModule* absl_nullable hlo_module,\n                            const se::DeviceDescription& device_info,\n                            ThunkPassBufferAllocator& allocator) override;\n+\n+ private:\n+  Mode mode_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "6bf7266975602efe10f18abfa068e4487b739cb1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass_test.cc",
            "status": "modified",
            "additions": 88,
            "deletions": 2,
            "changes": 90,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/runtime/buffers_checksum_thunk.h\"\n+#include \"xla/backends/gpu/runtime/buffers_nan_count_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -101,7 +102,7 @@ TEST(ThunkBufferDebugPassTest, IsNoOpWhenHloModuleIsNull) {\n   auto root_thunk =\n       std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));\n \n-  ThunkBufferDebugPass pass;\n+  ThunkBufferDebugPass pass(ThunkBufferDebugPass::Mode::kChecksum);\n   TF_ASSERT_OK_AND_ASSIGN(\n       bool changed, pass.Run(root_thunk.get(), debug_options,\n                              /*hlo_module=*/nullptr, device_info, allocator));\n@@ -152,7 +153,7 @@ TEST(ThunkBufferDebugPassTest, InsertsBuffersDebugChecksumThunks) {\n   auto root_thunk =\n       std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));\n \n-  ThunkBufferDebugPass pass;\n+  ThunkBufferDebugPass pass(ThunkBufferDebugPass::Mode::kChecksum);\n   TF_ASSERT_OK_AND_ASSIGN(bool changed,\n                           pass.Run(root_thunk.get(), debug_options, &hlo_module,\n                                    device_info, allocator));\n@@ -205,6 +206,91 @@ TEST(ThunkBufferDebugPassTest, InsertsBuffersDebugChecksumThunks) {\n           Pair(ThunkBufferId::Create(kTestThunkId, 2).value(), slice_io)));\n }\n \n+TEST(ThunkBufferDebugPassTest, InsertsBuffersDebugNanCounterThunks) {\n+  static constexpr ThunkId kTestThunkId = ThunkId(123);\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_experimental_enable_nan_counter_on_thunks(true);\n+  se::DeviceDescription device_info;\n+  FakeThunkPassBufferAllocator allocator;\n+  // The callbacks created by ThunkBufferDebugPass require a HloModule with\n+  // a non-null entry computation.\n+  auto builder = HloComputation::Builder(\"entry\");\n+  HloInstruction* root = builder.AddInstruction(\n+      HloInstruction::CreateConstant(LiteralUtil::CreateR0(1.0f)));\n+  std::unique_ptr<HloComputation> entry_computation = builder.Build(root);\n+  HloModule hlo_module(\"test_module\", HloModuleConfig());\n+  hlo_module.AddEntryComputation(std::move(entry_computation));\n+  // Create a fake thunk with a few different buffer uses.\n+  BufferAllocation alloc(0, 1024, 0);\n+  BufferAllocation::Slice slice_i(&alloc, 0, 1, PrimitiveType::F32);\n+  BufferAllocation::Slice slice_o(&alloc, 1, 1, PrimitiveType::F32);\n+  BufferAllocation::Slice slice_io(&alloc, 2, 1, PrimitiveType::F32);\n+  BufferAllocation::Slice slice_scratch(&alloc, 3, 1, PrimitiveType::F32);\n+  Thunk::ThunkInfo fake_thunk_info;\n+  fake_thunk_info.thunk_id = ThunkId(kTestThunkId);\n+  auto fake_thunk = std::make_unique<FakeThunk>(\n+      fake_thunk_info,\n+      Thunk::BufferUses{\n+          // Consume means the thunk can reuse the buffer for scratch space, so\n+          // only check it on input.\n+          BufferUse::Consume(slice_i),\n+          // Write is undefined on input, but defined on output.\n+          BufferUse::Write(slice_o),\n+          // Unlike Consume, Read is supposed to preserve the contents of the\n+          // buffer, so we check it on input *and* output.\n+          BufferUse::Read(slice_io),\n+          // Scratch buffers are not checked at all.\n+          BufferUse::Scratch(slice_scratch),\n+      });\n+  Thunk* fake_thunk_ptr = fake_thunk.get();\n+  std::vector<std::unique_ptr<Thunk>> thunks;\n+  thunks.push_back(std::move(fake_thunk));\n+  auto root_thunk =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));\n+\n+  ThunkBufferDebugPass pass(ThunkBufferDebugPass::Mode::kNanCounter);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          pass.Run(root_thunk.get(), debug_options, &hlo_module,\n+                                   device_info, allocator));\n+  EXPECT_TRUE(changed);\n+\n+  // Expected thunk structure after the pass:\n+  // 1. CustomCallThunk (buffer debug log init)\n+  // 2. SequentialThunk\n+  //    1. FakeThunk\n+  //    2. BuffersDebugNanCountThunk (nan counter output buffers)\n+  // 3. CustomCallThunk (buffer debug log dump)\n+  const std::vector<std::unique_ptr<Thunk>>& new_thunks = root_thunk->thunks();\n+  EXPECT_THAT(new_thunks, SizeIs(3));\n+  EXPECT_EQ(new_thunks[0]->kind(), Thunk::Kind::kCustomCall);\n+  EXPECT_EQ(new_thunks[1]->kind(), Thunk::Kind::kSequential);\n+  EXPECT_EQ(new_thunks[2]->kind(), Thunk::Kind::kCustomCall);\n+\n+  const CustomCallThunk& buffer_debug_init_thunk =\n+      static_cast<const CustomCallThunk&>(*new_thunks[0]);\n+  EXPECT_EQ(buffer_debug_init_thunk.target_name(),\n+            \"xla_gpu_buffer_debug_log_init\");\n+\n+  const CustomCallThunk& buffer_debug_dump_thunk =\n+      static_cast<const CustomCallThunk&>(*new_thunks[2]);\n+  EXPECT_EQ(buffer_debug_dump_thunk.target_name(),\n+            \"xla_gpu_buffer_debug_log_dump\");\n+\n+  const std::vector<std::unique_ptr<Thunk>>& sub_thunks =\n+      static_cast<const SequentialThunk&>(*new_thunks[1]).thunks();\n+  EXPECT_THAT(sub_thunks, SizeIs(2));\n+  EXPECT_THAT(sub_thunks[0], Pointer(fake_thunk_ptr));\n+  EXPECT_EQ(sub_thunks[1]->kind(), Thunk::Kind::kBuffersDebugNanCount);\n+\n+  const BuffersDebugNanCountThunk& buffer_debug_after_fake_thunk =\n+      static_cast<const BuffersDebugNanCountThunk&>(*sub_thunks[1]);\n+  EXPECT_THAT(\n+      buffer_debug_after_fake_thunk.buffer_slices(),\n+      UnorderedElementsAre(\n+          Pair(ThunkBufferId::Create(kTestThunkId, 1).value(), slice_o),\n+          Pair(ThunkBufferId::Create(kTestThunkId, 2).value(), slice_io)));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "c4ab2c7252fbcf8d3b67e4c3f6337bb6fe7c9bf3",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -471,6 +471,8 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_cpu_collective_call_terminate_timeout_seconds(40);\n \n   opts.set_xla_keep_shardings_after_spmd(false);\n+  opts.set_xla_gpu_experimental_enable_checksum_tracing_on_thunks(false);\n+  opts.set_xla_gpu_experimental_enable_nan_counter_on_thunks(false);\n   return opts;\n }\n \n@@ -2654,6 +2656,13 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       debug_options->xla_gpu_experimental_enable_checksum_tracing_on_thunks(),\n       \"Enables an experimental feature to record checksums of selected thunk \"\n       \"inputs/outputs.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_experimental_enable_nan_counter_on_thunks\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_gpu_experimental_enable_nan_counter_on_thunks),\n+      debug_options->xla_gpu_experimental_enable_nan_counter_on_thunks(),\n+      \"Enables an experimental feature to record the number of nans in thunk \"\n+      \"outputs.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_experimental_thunk_buffer_debug_filter_by_thunk_id_ranges\",\n       setter_for_thunk_buffer_debug_filter_by_thunk_id, \"(none)\","
        },
        {
            "sha": "87232351aff695cdd566a729380f93684da8add3",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -5866,6 +5866,9 @@ tf_proto_library(\n     name = \"buffer_assignment_proto\",\n     srcs = [\"buffer_assignment.proto\"],\n     make_default_target_header_only = True,\n+    protodeps = [\n+        \"//xla:xla_data_proto\",\n+    ],\n )\n \n cc_library("
        },
        {
            "sha": "fef83711d7664a1e5dec9edfc3b433d8caa3f415",
            "filename": "third_party/xla/xla/service/buffer_assignment.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.cc?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -245,6 +245,7 @@ BufferAllocation::Slice::ToProto() const {\n   proto.set_offset(offset());\n   proto.set_size(size());\n   proto.set_buffer_allocation_index(allocation() == nullptr ? -1 : index());\n+  proto.set_element_type(element_type());\n   return proto;\n }\n \n@@ -259,13 +260,14 @@ absl::StatusOr<BufferAllocation::Slice> BufferAllocation::Slice::FromProto(\n   }\n   const BufferAllocation& allocation =\n       buffer_allocations[proto.buffer_allocation_index()];\n-  return BufferAllocation::Slice(&allocation, proto.offset(), proto.size());\n+  return BufferAllocation::Slice(&allocation, proto.offset(), proto.size(),\n+                                 proto.element_type());\n }\n \n BufferAllocation::Slice BufferAllocation::GetSlice(\n     const HloValue& buffer) const {\n   const OffsetSize os = FindOrDie(assigned_buffers_, &buffer);\n-  return Slice(this, os.offset, os.size);\n+  return Slice(this, os.offset, os.size, buffer.shape().element_type());\n }\n \n absl::Status BufferAllocation::AddAssignment(const HloValue& buffer,\n@@ -331,6 +333,8 @@ BufferAllocationProto BufferAllocation::ToProto() const {\n     proto_assigned->set_logical_buffer_id(buffer_offset_size.first->id());\n     proto_assigned->set_offset(buffer_offset_size.second.offset);\n     proto_assigned->set_size(buffer_offset_size.second.size);\n+    proto_assigned->set_element_type(\n+        buffer_offset_size.first->shape().element_type());\n   }\n   absl::c_sort(*proto.mutable_assigned(),\n                [](const BufferAllocationProto::Assigned& assign1,"
        },
        {
            "sha": "0ba61f183b4ca7c0a1e805add615e73278cef429",
            "filename": "third_party/xla/xla/service/buffer_assignment.h",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.h?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -190,15 +190,22 @@ class BufferAllocation {\n   class Slice {\n    public:\n     Slice() = default;\n-    Slice(const BufferAllocation* allocation, int64_t offset, int64_t size)\n-        : allocation_(allocation), offset_(offset), size_(size) {}\n+    Slice(const BufferAllocation* allocation, int64_t offset, int64_t size,\n+          PrimitiveType element_type = PrimitiveType::PRIMITIVE_TYPE_INVALID)\n+        : allocation_(allocation),\n+          offset_(offset),\n+          size_(size),\n+          element_type_(element_type) {}\n \n     const BufferAllocation* allocation() const { return allocation_; }\n     Index index() const { return allocation_->index(); }\n     int64_t offset() const { return offset_; }\n     int64_t size() const { return size_; }\n+    PrimitiveType element_type() const { return element_type_; }\n \n     bool operator==(const Slice& other) const {\n+      // We don't compare element_type_ because it's not always set, and it's\n+      // not relevant for the comparison here.\n       return index() == other.index() && offset_ == other.offset_ &&\n              size_ == other.size_;\n     }\n@@ -252,6 +259,7 @@ class BufferAllocation {\n     const BufferAllocation* allocation_ = nullptr;\n     int64_t offset_ = 0;\n     int64_t size_ = 0;\n+    PrimitiveType element_type_ = PrimitiveType::PRIMITIVE_TYPE_INVALID;\n   };\n \n   // GetSlice returns the Slice of contiguous memory that holds the value"
        },
        {
            "sha": "df6714b0faa069b243dbf27ddee44abacd3c92d6",
            "filename": "third_party/xla/xla/service/buffer_assignment.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.proto?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -15,6 +15,8 @@ limitations under the License.\n \n syntax = \"proto3\";\n \n+import \"xla/xla_data.proto\";\n+\n package xla.buffer_assignment;\n \n // This defines the buffer isolation configuration, which is a debugging tool to\n@@ -108,4 +110,5 @@ message BufferAllocationSliceProto {\n   int64 offset = 1;\n   int64 size = 2;\n   int64 buffer_allocation_index = 3;\n+  xla.PrimitiveType element_type = 4;\n }"
        },
        {
            "sha": "d747602057959a450b2388dc37af407e98af6ccf",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -179,7 +179,12 @@ static absl::Status RunThunkPasses(const DebugOptions& debug_options,\n                                    ThunkPassBufferAllocator& allocator) {\n   ThunkPassPipeline pipeline(\"thunk-passes\");\n   if (debug_options.xla_gpu_experimental_enable_checksum_tracing_on_thunks()) {\n-    pipeline.AddPass(std::make_unique<ThunkBufferDebugPass>());\n+    pipeline.AddPass(std::make_unique<ThunkBufferDebugPass>(\n+        ThunkBufferDebugPass::Mode::kChecksum));\n+  }\n+  if (debug_options.xla_gpu_experimental_enable_nan_counter_on_thunks()) {\n+    pipeline.AddPass(std::make_unique<ThunkBufferDebugPass>(\n+        ThunkBufferDebugPass::Mode::kNanCounter));\n   }\n   if (debug_options.xla_gpu_experimental_enable_command_buffer_on_thunks()) {\n     pipeline.AddPass(std::make_unique<CommandBufferConversionPass>("
        },
        {
            "sha": "6ea3da98fd7cba52fc7abbc6eb0848090d76f80a",
            "filename": "third_party/xla/xla/service/hlo.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fhlo.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fservice%2Fhlo.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo.proto?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -667,6 +667,7 @@ message BufferAllocationProto {\n     int64 logical_buffer_id = 1;\n     int64 offset = 2;\n     int64 size = 3;\n+    xla.PrimitiveType element_type = 4;\n   }\n \n   int64 index = 1;"
        },
        {
            "sha": "3d889f63a1bd4fde6cb03dfe1abe9e9d97e8990d",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f3f2c9444d8d837b85e29b42704996259ea8fbf/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=4f3f2c9444d8d837b85e29b42704996259ea8fbf",
            "patch": "@@ -348,7 +348,7 @@ message DebugOptions {\n \n   // Limits the thunk buffer debug instrumentation to specific thunks.\n   optional ThunkBufferDebugFilter\n-      xla_gpu_experimental_thunk_buffer_debug_filter = 423;\n+      xla_gpu_experimental_thunk_buffer_debug_filter = 424;\n \n   // If true, every time an HLO module is run, we will dump an\n   // HloUnoptimizedSnapshot (essentially, a serialized unoptimizedmodule plus\n@@ -657,6 +657,9 @@ message DebugOptions {\n   optional bool xla_gpu_experimental_enable_heuristic_collective_combining =\n       366;\n \n+  // If true, enable buffer nan counter on thunks.\n+  optional bool xla_gpu_experimental_enable_nan_counter_on_thunks = 423;\n+\n   // Enable NCCL symmetric buffers.\n   optional bool xla_gpu_experimental_enable_nccl_symmetric_buffers = 406;\n \n@@ -1388,7 +1391,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 424\n+  // Next id: 425\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 318,
        "additions": 274,
        "deletions": 44
    }
}