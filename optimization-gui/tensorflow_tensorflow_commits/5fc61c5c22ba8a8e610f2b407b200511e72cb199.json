{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Add xla.get_dynamic_dim_size op and its lowering.\n\nThe new op is needed to implement PadToStatic custom call.\n\nPiperOrigin-RevId: 837107619",
    "sha": "5fc61c5c22ba8a8e610f2b407b200511e72cb199",
    "files": [
        {
            "sha": "4c92446bb0897ab605cd20a3c0f02cf067754834",
            "filename": "third_party/xla/xla/codegen/emitters/ir/tests/ops.mlir",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Ftests%2Fops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Ftests%2Fops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Ftests%2Fops.mlir?ref=5fc61c5c22ba8a8e610f2b407b200511e72cb199",
            "patch": "@@ -167,3 +167,12 @@ func.func @workgroup_id_op() -> (index, index, index) {\n // CHECK: [[WORKGROUP_ID_X:.*]] = xla.workgroup_id x {xla.range = [0 : index, 1023 : index]}\n // CHECK: [[WORKGROUP_ID_Y:.*]] = xla.workgroup_id y\n // CHECK: [[WORKGROUP_ID_Z:.*]] = xla.workgroup_id z\n+\n+// -----\n+\n+func.func @get_dynamic_dim_size(%in: tensor<16x8x4xf32>) -> (i32) {\n+  %out = xla.get_dynamic_dim_size %in 1 : tensor<16x8x4xf32>\n+  func.return %out : i32\n+}\n+// CHECK-LABEL: @get_dynamic_dim_size\n+// CHECK: xla.get_dynamic_dim_size"
        },
        {
            "sha": "3e4738d494146e7451cf4e7954d8487089698bf1",
            "filename": "third_party/xla/xla/codegen/emitters/ir/xla_ops.td",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.td?ref=5fc61c5c22ba8a8e610f2b407b200511e72cb199",
            "patch": "@@ -298,6 +298,19 @@ def WorkGroupIdOp : XLA_Op<\"workgroup_id\", [\n   let results = (outs Index);\n }\n \n+def GetDynamicDimSizeOp : XLA_Op<\"get_dynamic_dim_size\", [\n+    Pure,\n+  ]> {\n+  let summary = \"Returns the dynamic size of a dimension. The dynamic sizes are \"\n+                \"stored in the same buffer, after the main values as an array \"\n+                \"of s32. The `dim` argument can be larger than `tensor`'s rank, \"\n+                \"because XLA has passes like flatten_tensors that only change \"\n+                \"the view of the memory.\";\n+  let arguments =(ins AnyStaticShapeTensor:$tensor, I64Attr:$dim);\n+  let results = (outs I32:$result);\n+\n+  let assemblyFormat = \"$tensor $dim attr-dict `:` type($tensor)\";\n+}\n \n #endif // XLA_CODEGEN_EMITTERS_IR_XLA_OPS\n "
        },
        {
            "sha": "f603a2d89a7b252af5a452cdc31dbbe69e34f6fd",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/flatten_tensors.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 4,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc?ref=5fc61c5c22ba8a8e610f2b407b200511e72cb199",
            "patch": "@@ -31,11 +31,11 @@ limitations under the License.\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n-#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/TypeRange.h\"\n@@ -46,11 +46,12 @@ limitations under the License.\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Support/WalkResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_ops.h\"\n #include \"xla/backends/gpu/codegen/emitters/ir/xla_gpu_ops.h\"\n+#include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -748,6 +749,28 @@ struct RewriteSyncThreads : OpRewritePattern<gpu::SyncThreadsOp> {\n   }\n };\n \n+struct RewriteGetDynamicDimSizeOp : OpRewritePattern<GetDynamicDimSizeOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(GetDynamicDimSizeOp op,\n+                                PatternRewriter& rewriter) const override {\n+    auto tensor = op.getTensor();\n+    auto tensor_type = tensor.getType();\n+    if (tensor_type.getRank() < 2) {\n+      return rewriter.notifyMatchFailure(op, \"the tensor is already flat\");\n+    }\n+\n+    auto tensor_1D = rewriter\n+                         .create<UnrealizedConversionCastOp>(\n+                             op.getLoc(), GetFlattenedType(tensor_type), tensor)\n+                         .getResult(0);\n+    rewriter.replaceOpWithNewOp<GetDynamicDimSizeOp>(op, tensor_1D,\n+                                                     op.getDim());\n+\n+    return mlir::success();\n+  }\n+};\n+\n class FlattenTensorsPass\n     : public impl::FlattenTensorsPassBase<FlattenTensorsPass> {\n  public:\n@@ -760,8 +783,10 @@ class FlattenTensorsPass\n         RewriteAllocateShared,\n         RewriteAtomicRMW,\n         RewriteConstant,\n+        RewriteCpuLoad,\n         RewriteFor,\n         RewriteFunctionSignatures,\n+        RewriteGetDynamicDimSizeOp,\n         RewriteIf,\n         RewriteIndexSwitch,\n         RewritePureCall,\n@@ -771,8 +796,7 @@ class FlattenTensorsPass\n         RewriteVectorExtract,\n         RewriteVectorFromElements,\n         RewriteVectorInsert,\n-        RewriteVectorTransferRead,\n-        RewriteCpuLoad\n+        RewriteVectorTransferRead\n     >(mlir_context);\n     // clang-format on\n     ApplyIndexingOp::getCanonicalizationPatterns(patterns, mlir_context);"
        },
        {
            "sha": "62a0f262506b0cfce148ef39aee12582f3e2ccdc",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/lower_tensors.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 10,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_tensors.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_tensors.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_tensors.cc?ref=5fc61c5c22ba8a8e610f2b407b200511e72cb199",
            "patch": "@@ -1325,6 +1325,55 @@ class RewriteAtomicRMW : public OpRewritePattern<AtomicRMWOp> {\n   const DeviceSpec& device_spec_;\n };\n \n+class RewriteGetDynamicDimSize : public OpRewritePattern<GetDynamicDimSizeOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(\n+      GetDynamicDimSizeOp op, mlir::PatternRewriter& rewriter) const override {\n+    mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n+\n+    auto tensor = op.getTensor();\n+    auto tensor_type = mlir::dyn_cast<mlir::RankedTensorType>(tensor.getType());\n+\n+    Type element_type = tensor_type.getElementType();\n+    int64_t num_elements = tensor_type.getNumElements();\n+    std::optional<int> sub_byte_width = GetSubByteBitWidth(element_type);\n+    if (sub_byte_width) {\n+      element_type = b.getI8Type();\n+      // Elements are packed.\n+      num_elements = CeilOfRatio<int64_t>(num_elements, 8 / *sub_byte_width);\n+    }\n+\n+    // The offset of the dim size from the start of the buffer. The dynamic dim\n+    // sizes are stored after the tensor data as a tail-allocated metadata of\n+    // s32 type.\n+    int64_t dynamic_size_offset_in_bytes =\n+        num_elements * element_type.getIntOrFloatBitWidth() / 8 +\n+        op.getDim() * b.getI32Type().getWidth() / 8;\n+\n+    int64_t alignment = dynamic_size_offset_in_bytes % 4;\n+    // TODO(b/463569416): Support unaligned loads.\n+    if (alignment != 0) {\n+      return op->emitOpError(\"dynamic size offset is not 4-byte aligned\");\n+    }\n+\n+    auto ptr_type = ml::LLVMPointerType::get(b.getContext());\n+    Value tensor_ptr =\n+        b.create<UnrealizedConversionCastOp>(ptr_type, tensor).getResult(0);\n+\n+    Value addr_offset =\n+        b.create<ml::ConstantOp>(b.getI64Type(), dynamic_size_offset_in_bytes);\n+\n+    Value addr_int = b.create<ml::PtrToIntOp>(b.getI64Type(), tensor_ptr);\n+    Value metadata_addr_int = b.create<ml::AddOp>(addr_int, addr_offset);\n+    Value metadata_addr = b.create<ml::IntToPtrOp>(ptr_type, metadata_addr_int);\n+\n+    rewriter.replaceOpWithNewOp<ml::LoadOp>(op, b.getI32Type(), metadata_addr);\n+\n+    return success();\n+  }\n+};\n+\n class LowerTensorsPass : public impl::LowerTensorsPassBase<LowerTensorsPass> {\n  public:\n   explicit LowerTensorsPass(const LowerTensorsPassOptions& options)\n@@ -1351,10 +1400,11 @@ class LowerTensorsPass : public impl::LowerTensorsPassBase<LowerTensorsPass> {\n     mlir::RewritePatternSet tensor_patterns(mlir_context);\n \n     tensor_patterns.add<RewriteAtomicRMW>(mlir_context, device_spec_);\n-    tensor_patterns\n-        .add<RewriteAllocateShared, RewriteNonScalarConstants,\n-             RewriteSyncThreads, RewriteTensorExtract, RewriteTransferRead,\n-             RewriteTensorInsert, RewriteTransferWrite>(mlir_context);\n+    tensor_patterns.add<RewriteAllocateShared, RewriteGetDynamicDimSize,\n+                        RewriteNonScalarConstants, RewriteSyncThreads,\n+                        RewriteTensorExtract, RewriteTensorInsert,\n+                        RewriteTransferRead, RewriteTransferWrite>(\n+        mlir_context);\n     if (mlir::failed(mlir::applyPatternsGreedily(getOperation(),\n                                                  std::move(tensor_patterns)))) {\n       signalPassFailure();\n@@ -1396,14 +1446,8 @@ class LowerTensorsPass : public impl::LowerTensorsPassBase<LowerTensorsPass> {\n           if (func.getArgAttr(base.getArgNumber(), \"xla.invariant\")) {\n             load.setInvariant(true);\n           }\n-          return;\n         }\n       }\n-      if (!device_spec_.IsCpu()) {\n-        load.emitOpError(\n-            \"load op address is not (a GEP of) a function argument\");\n-        signalPassFailure();\n-      }\n     });\n   }\n "
        },
        {
            "sha": "8ac45ff8fa22eb33efe55823a483d62d1ae3ed41",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/tests/flatten_tensors.mlir",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fflatten_tensors.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fflatten_tensors.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fflatten_tensors.mlir?ref=5fc61c5c22ba8a8e610f2b407b200511e72cb199",
            "patch": "@@ -398,3 +398,13 @@ func.func @constant_vector() -> vector<2x3xf32> {\n // CHECK-LABEL: func.func @constant_vector\n // CHECK-SAME: -> vector<6xf32>\n // CHECK-NOT:  builtin.unrealized_conversion_cast\n+\n+// -----\n+\n+func.func @get_dynamic_dim_size(%in: tensor<16x8x4xf32>) -> (i32) {\n+  %out = xla.get_dynamic_dim_size %in 1 : tensor<16x8x4xf32>\n+  func.return %out : i32\n+}\n+// CHECK-LABEL: func.func @get_dynamic_dim_size(\n+// CHECK-SAME:      %[[TENSOR:.*]]: tensor<512xf32>) -> i32 {\n+// CHECK:         xla.get_dynamic_dim_size %[[TENSOR]] 1 : tensor<512xf32>"
        },
        {
            "sha": "4469755829bc5402ce247b7ee097822273954ca6",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/tests/lower_tensors.mlir",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_tensors.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fc61c5c22ba8a8e610f2b407b200511e72cb199/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_tensors.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_tensors.mlir?ref=5fc61c5c22ba8a8e610f2b407b200511e72cb199",
            "patch": "@@ -1106,3 +1106,30 @@ func.func @transfer_write_f4(%arg0: tensor<43xf4E2M1FN> {xla.slice_index = 1},\n // CHECK-LABEL: @transfer_write_f4\n // CHECK: %[[PTR:.*]] = llvm.getelementptr inbounds %arg0[0, 5] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<22 x i8>\n // CHECK: %[[OUT:.*]] = builtin.unrealized_conversion_cast %{{.*}} : vector<2xf4E2M1FN> to vector<2xi4>\n+\n+// -----\n+\n+func.func @get_dynamic_dim_size(%arg0: tensor<512xf32>) -> i32 {\n+  %0 = xla.get_dynamic_dim_size %arg0 1 : tensor<512xf32>\n+  func.return %0 : i32\n+}\n+// CHECK-LABEL: @get_dynamic_dim_size\n+// CHECK: llvm.mlir.constant(2052 : i64) : i64\n+\n+// -----\n+\n+func.func @get_dynamic_dim_size_sub_byte_width(%arg0: tensor<512xi4>) -> i32 {\n+  %0 = xla.get_dynamic_dim_size %arg0 1 : tensor<512xi4>\n+  func.return %0 : i32\n+}\n+// CHECK-LABEL: @get_dynamic_dim_size_sub_byte_width\n+// CHECK: llvm.mlir.constant(260 : i64) : i64\n+\n+// // -----\n+\n+func.func @get_dynamic_dim_size_unaligned(%arg0: tensor<7xf16>) -> i32 {\n+// expected-error @+1 {{'xla.get_dynamic_dim_size' op dynamic size offset is not 4-byte aligned}}\n+  %0 = xla.get_dynamic_dim_size %arg0 1 : tensor<7xf16>\n+  func.return %0 : i32\n+}\n+"
        }
    ],
    "stats": {
        "total": 155,
        "additions": 141,
        "deletions": 14
    }
}