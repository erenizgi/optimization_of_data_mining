{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 845648979",
    "sha": "efddb34c27796bfefda22ddaaceb66e2c6091d2d",
    "files": [
        {
            "sha": "1bebcc31d45c76059af872a30cf9277f45687b88",
            "filename": "tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efddb34c27796bfefda22ddaaceb66e2c6091d2d/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_debug_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efddb34c27796bfefda22ddaaceb66e2c6091d2d/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_debug_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_debug_allocator.cc?ref=efddb34c27796bfefda22ddaaceb66e2c6091d2d",
            "patch": "@@ -44,7 +44,8 @@ int64_t* before_mask = NewMask(0xabababababababab);\n int64_t* after_mask = NewMask(0xcdcdcdcdcdcdcdcd);\n \n bool CheckMask(se::StreamExecutor* exec, void* ptr, int64_t* mask) {\n-  se::DeviceMemory<int64_t> gpu_ptr{se::DeviceMemoryBase{ptr, MASK_BYTES}};\n+  stream_executor::DeviceAddress<int64_t> gpu_ptr{\n+      stream_executor::DeviceAddressBase{ptr, MASK_BYTES}};\n   int64_t tmp[MASK_WORDS];\n \n   absl::Status result = exec->SynchronousMemcpyD2H(gpu_ptr, MASK_BYTES, tmp);\n@@ -66,7 +67,8 @@ bool CheckMask(se::StreamExecutor* exec, void* ptr, int64_t* mask) {\n }\n \n void InitMask(se::StreamExecutor* exec, void* ptr, int64_t* mask) {\n-  se::DeviceMemory<int64_t> gpu_ptr{se::DeviceMemoryBase{ptr, MASK_BYTES}};\n+  stream_executor::DeviceAddress<int64_t> gpu_ptr{\n+      stream_executor::DeviceAddressBase{ptr, MASK_BYTES}};\n   absl::Status result = exec->SynchronousMemcpyH2D(mask, MASK_BYTES, &gpu_ptr);\n   if (!result.ok()) {\n     LOG(FATAL) << \"Could not copy debug mask, \" << result;\n@@ -175,8 +177,9 @@ void* GPUNanResetAllocator::AllocateRaw(size_t alignment, size_t num_bytes) {\n   size_t req_size = base_allocator_->RequestedSize(allocated_ptr);\n   std::vector<float> nans((req_size + sizeof(float) - 1) / sizeof(float),\n                           std::nanf(\"\"));\n-  se::DeviceMemory<float> nan_ptr{\n-      se::DeviceMemoryBase{static_cast<float*>(allocated_ptr), req_size}};\n+  stream_executor::DeviceAddress<float> nan_ptr{\n+      stream_executor::DeviceAddressBase{static_cast<float*>(allocated_ptr),\n+                                         req_size}};\n \n   absl::Status result =\n       stream_exec_->SynchronousMemcpyH2D(&nans[0], req_size, &nan_ptr);\n@@ -192,8 +195,8 @@ void GPUNanResetAllocator::DeallocateRaw(void* ptr) {\n     size_t req_size = base_allocator_->RequestedSize(ptr);\n     std::vector<float> nans((req_size + sizeof(float) - 1) / sizeof(float),\n                             std::nanf(\"\"));\n-    se::DeviceMemory<float> nan_ptr{\n-        se::DeviceMemoryBase{static_cast<float*>(ptr), req_size}};\n+    stream_executor::DeviceAddress<float> nan_ptr{\n+        stream_executor::DeviceAddressBase{static_cast<float*>(ptr), req_size}};\n     absl::Status result =\n         stream_exec_->SynchronousMemcpyH2D(&nans[0], req_size, &nan_ptr);\n     if (!result.ok()) {"
        },
        {
            "sha": "6fb3a800d0ab602142356536f4235b5ce6931f5c",
            "filename": "tensorflow/core/common_runtime/gpu/gpu_util.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efddb34c27796bfefda22ddaaceb66e2c6091d2d/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efddb34c27796bfefda22ddaaceb66e2c6091d2d/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_util.cc?ref=efddb34c27796bfefda22ddaaceb66e2c6091d2d",
            "patch": "@@ -548,34 +548,35 @@ absl::Status GPUUtil::SyncAll(Device* gpu_device) {\n   return absl::OkStatus();\n }\n \n-string GPUUtil::MemoryDebugString(const Device* device, Tensor* tensor) {\n-  string ret;\n+std::string GPUUtil::MemoryDebugString(const Device* device, Tensor* tensor) {\n+  std::string ret;\n   CHECK(tensor);\n   const int64_t num_bytes = std::min<int64_t>(\n       FLAGS_brain_gpu_util_debug_string_maxlen, tensor->TotalBytes());\n   void* ptr = (num_bytes > 0) ? GetBase(tensor) : nullptr;\n-  strings::Appendf(&ret, \"%p:\", ptr);\n+  void* arg1 = ptr;\n+  absl::StrAppendFormat(&ret, \"%p:\", arg1);\n   if (num_bytes > 0) {\n     auto* dev_info = device->tensorflow_accelerator_device_info();\n     if (!dev_info) {\n-      strings::StrAppend(\n+      absl::StrAppend(\n           &ret, PrintMemory(reinterpret_cast<const char*>(ptr), num_bytes));\n     } else {\n-      string buf;\n+      std::string buf;\n       buf.resize(num_bytes);\n       DeviceMemoryBase gpu_ptr(ptr, num_bytes);\n       auto s = dev_info->stream->parent()->SynchronousMemcpyD2H(\n           gpu_ptr, num_bytes, &*buf.begin());\n-      strings::StrAppend(&ret, PrintMemory(&*buf.begin(), num_bytes));\n+      absl::StrAppend(&ret, PrintMemory(&*buf.begin(), num_bytes));\n     }\n   }\n   return ret;\n }\n \n // TODO(pbar) Checksum is called from places without a valid device context.\n-uint64 GPUUtil::Checksum(Device* gpu_device,\n-                         const DeviceContext* device_context,\n-                         const Tensor& tensor) {\n+uint64_t GPUUtil::Checksum(Device* gpu_device,\n+                           const DeviceContext* device_context,\n+                           const Tensor& tensor) {\n   Tensor copy(tensor.dtype(), tensor.shape());\n   absl::Status s;\n   absl::Notification n;\n@@ -589,7 +590,7 @@ uint64 GPUUtil::Checksum(Device* gpu_device,\n   return Checksum(copy);\n }\n \n-uint64 GPUUtil::Checksum(const Tensor& tensor) {\n+uint64_t GPUUtil::Checksum(const Tensor& tensor) {\n   const float* fptr = reinterpret_cast<const float*>(GetBase(&tensor));\n   size_t num_bytes = tensor.TotalBytes();\n   size_t num_floats = num_bytes / sizeof(float);"
        },
        {
            "sha": "6675aa3802c081c417bc4d87ddc20c04da0271ec",
            "filename": "tensorflow/core/common_runtime/gpu/gpu_util.h",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efddb34c27796bfefda22ddaaceb66e2c6091d2d/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efddb34c27796bfefda22ddaaceb66e2c6091d2d/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_util.h?ref=efddb34c27796bfefda22ddaaceb66e2c6091d2d",
            "patch": "@@ -62,7 +62,7 @@ class GPUUtil {\n   // For debugging purpose, given a \"device\" and a \"tensor\" allocated\n   // on the device, return a string printing each byte in the tensor\n   // (up to a limit).  \"device\" can be either a CPU or a GPU device.\n-  static string MemoryDebugString(const Device* device, Tensor* tensor);\n+  static std::string MemoryDebugString(const Device* device, Tensor* tensor);\n \n   // Map a Tensor as a DeviceMemory object wrapping the given typed\n   // buffer.\n@@ -72,18 +72,19 @@ class GPUUtil {\n   template <typename T>\n   static se::DeviceMemory<T> AsDeviceMemory(const Tensor& t) {\n     T* ptr = reinterpret_cast<T*>(const_cast<void*>(DMAHelper::base(&t)));\n-    return se::DeviceMemory<T>(se::DeviceMemoryBase(ptr, t.TotalBytes()));\n+    return se::DeviceMemory<T>(\n+        stream_executor::DeviceAddressBase(ptr, t.TotalBytes()));\n   }\n \n   // Computes a checksum over the contents of \"tensor\", which is allocated\n   // on \"gpu_device\".\n-  static uint64 Checksum(Device* gpu_device,\n-                         const DeviceContext* device_context,\n-                         const Tensor& tensor);\n+  static uint64_t Checksum(Device* gpu_device,\n+                           const DeviceContext* device_context,\n+                           const Tensor& tensor);\n \n   // Computes a checksum over the contents of \"tensor\", which is allocated\n   // in local CPU RAM.\n-  static uint64 Checksum(const Tensor& tensor);\n+  static uint64_t Checksum(const Tensor& tensor);\n \n   static void CopyCPUTensorToGPU(const Tensor* cpu_tensor,\n                                  const DeviceContext* device_context,"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 27,
        "deletions": 22
    }
}