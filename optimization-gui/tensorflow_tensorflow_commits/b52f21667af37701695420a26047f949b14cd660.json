{
    "author": "hsharsha",
    "message": "PR #34533: [ROCm] Make TmaPTX tests cuda specific\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34533\n\nüìù Summary of Changes\nPTX tests are cuda specific and skip them on ROCm platform\n\nüöÄ Kind of Contribution\nüêõ Bug Fix,\n\nüß™ Execution Tests:\n//xla/backends/gpu/runtime:kernel_thunk_test.cc\n\nCopybara import of the project:\n\n--\nb064e5f74be31b09dd25a2fcbf534d4aeae06975 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:\n\n[ROCm] Make TmaPTX tests cuda specific\n\nMerging this change closes #34533\n\nPiperOrigin-RevId: 838714652",
    "sha": "b52f21667af37701695420a26047f949b14cd660",
    "files": [
        {
            "sha": "ac6024667d0e7e8e54a693ce0fd2a3b2773db024",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b52f21667af37701695420a26047f949b14cd660/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b52f21667af37701695420a26047f949b14cd660/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=b52f21667af37701695420a26047f949b14cd660",
            "patch": "@@ -1021,6 +1021,7 @@ xla_test(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:executable\",\n+        \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu/kernels:custom_kernel\","
        },
        {
            "sha": "c89092360b2dd426113c1b4e669d5bfb79492ce1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b52f21667af37701695420a26047f949b14cd660/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b52f21667af37701695420a26047f949b14cd660/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc?ref=b52f21667af37701695420a26047f949b14cd660",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n@@ -427,8 +428,13 @@ class KernelThunkTmaPTXTest : public ::testing::TestWithParam<bool> {\n };\n \n TEST_P(KernelThunkTmaPTXTest, TmaPTX) {\n+  auto name = absl::AsciiStrToUpper(\n+      xla::PlatformUtil::CanonicalPlatformName(\"gpu\").value());\n+  if (name == \"ROCM\") {\n+    GTEST_SKIP() << \"TmaPTX cannot run on ROCm.\";\n+  }\n   TF_ASSERT_OK_AND_ASSIGN(se::Platform * platform,\n-                          se::PlatformManager::PlatformWithName(\"cuda\"));\n+                          se::PlatformManager::PlatformWithName(name));\n   TF_ASSERT_OK_AND_ASSIGN(se::StreamExecutor * executor,\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 8,
        "deletions": 1
    }
}