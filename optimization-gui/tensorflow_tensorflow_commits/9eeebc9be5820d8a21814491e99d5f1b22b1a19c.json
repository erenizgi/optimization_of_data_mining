{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Use a single intra-host ragged-all-to-all in the decomposition.\n\nInstead of 2 ra2a + concat, we can double the output buffer and adjust output offsets. This way we can save on latency by having only one multi-GPU synchronization.\n\nPiperOrigin-RevId: 826122665",
    "sha": "9eeebc9be5820d8a21814491e99d5f1b22b1a19c",
    "files": [
        {
            "sha": "6b898532ddb164b41bb8ce022ed5a98dcaaea380",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.cc",
            "status": "modified",
            "additions": 58,
            "deletions": 48,
            "changes": 106,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9eeebc9be5820d8a21814491e99d5f1b22b1a19c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9eeebc9be5820d8a21814491e99d5f1b22b1a19c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc?ref=9eeebc9be5820d8a21814491e99d5f1b22b1a19c",
            "patch": "@@ -254,61 +254,75 @@ absl::StatusOr<bool> DecomposeCombineRaggedAllToAll(\n       HloInstruction::CreateConstant(LiteralUtil::Zero(\n           ragged_all_to_all->operand(1)->shape().element_type())));\n \n+  Shape tmp_output_shape = ragged_all_to_all->shape();\n+  tmp_output_shape.set_dimensions(0,\n+                                  num_hosts * tmp_output_shape.dimensions(0));\n+\n   auto* zero_broadcast =\n       computation->AddInstruction(HloInstruction::CreateBroadcast(\n-          /*shape=*/ragged_all_to_all->operand(1)->shape(), zero,\n-          /*broadcast_dimensions=*/{}));\n+          /*shape=*/tmp_output_shape, zero, /*broadcast_dimensions=*/{}));\n \n-  int64_t num_updates_per_host =\n-      ragged_all_to_all->operand(2)->shape().dimensions(0) / num_hosts;\n+  int64_t num_devices_in_replica_per_host = num_devices_in_replica / num_hosts;\n \n-  auto slice_metadata_operand = [&](int64_t host_id,\n-                                    HloInstruction* metadata_operand) {\n-    Shape slice_shape = metadata_operand->shape();\n-    slice_shape.set_dimensions(0, num_updates_per_host);\n+  int64_t num_updates_per_replica =\n+      ragged_all_to_all->operand(2)->shape().dimensions(0) /\n+      num_devices_in_replica;\n \n-    return computation->AddInstruction(HloInstruction::CreateSlice(\n-        /*shape=*/slice_shape,\n-        /*operand=*/metadata_operand,\n-        /*start_indices=*/{num_updates_per_host * host_id},\n-        /*limit_indices=*/{num_updates_per_host * (host_id + 1)},\n-        /*strides=*/{1}));\n-  };\n+  auto get_intra_host_metadata = [&](HloInstruction* metadata_operand,\n+                                     bool correct_offsets) {\n+    metadata_operand =\n+        computation->AddInstruction(HloInstruction::CreateReshape(\n+            /*shape=*/ShapeUtil::MakeShape(\n+                metadata_operand->shape().element_type(),\n+                {num_hosts, num_devices_in_replica_per_host,\n+                 num_updates_per_replica}),\n+            /*operand=*/metadata_operand));\n+\n+    if (correct_offsets) {\n+      metadata_operand =\n+          CorrectOffsets(ragged_all_to_all->operand(1)->shape().dimensions(0),\n+                         metadata_operand, computation);\n+    }\n \n-  absl::InlinedVector<HloInstruction*, 4> intra_host_ragged_all_to_alls(\n-      num_hosts);\n-  for (int64_t host_id = 0; host_id < num_hosts; ++host_id) {\n-    absl::InlinedVector<HloInstruction*, 4> ragged_all_to_all_operands{\n-        ragged_all_to_all->mutable_operand(0),\n-        zero_broadcast,\n-        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(2)),\n-        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(3)),\n-        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(4)),\n-        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(5)),\n-    };\n-\n-    intra_host_ragged_all_to_alls[host_id] =\n-        computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n-            /*shape=*/ragged_all_to_all->shape(),\n-            /*operands=*/ragged_all_to_all_operands,\n-            /*replica_groups=*/intra_host_replica_groups,\n-            /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n-                ? std::make_optional(NextChannelId(*computation->parent()))\n-                : std::nullopt));\n-  }\n+    metadata_operand =\n+        computation->AddInstruction(HloInstruction::CreateTranspose(\n+            /*shape=*/ShapeUtil::MakeShape(\n+                metadata_operand->shape().element_type(),\n+                {num_devices_in_replica_per_host, num_hosts,\n+                 num_updates_per_replica}),\n+            /*operand=*/metadata_operand,\n+            /*dimensions=*/{1, 0, 2}));\n \n-  Shape concatenated_inputs_shape = ragged_all_to_all->shape();\n-  concatenated_inputs_shape.set_dimensions(\n-      0, num_hosts * ragged_all_to_all->shape().dimensions(0));\n+    return computation->AddInstruction(HloInstruction::CreateReshape(\n+        /*shape=*/ragged_all_to_all->operand(2)->shape(),\n+        /*operand=*/metadata_operand));\n+  };\n \n-  HloInstruction* concatenated_inputs =\n-      computation->AddInstruction(HloInstruction::CreateConcatenate(\n-          /*shape=*/concatenated_inputs_shape,\n-          /*operands=*/intra_host_ragged_all_to_alls, /*dimension=*/0));\n+  absl::InlinedVector<HloInstruction*, 4> intra_host_ragged_all_to_all_operands{\n+      ragged_all_to_all->mutable_operand(0),\n+      zero_broadcast,\n+      get_intra_host_metadata(ragged_all_to_all->mutable_operand(2),\n+                              /*correct_offsets=*/false),\n+      get_intra_host_metadata(ragged_all_to_all->mutable_operand(3),\n+                              /*correct_offsets=*/false),\n+      get_intra_host_metadata(ragged_all_to_all->mutable_operand(4),\n+                              /*correct_offsets=*/true),\n+      get_intra_host_metadata(ragged_all_to_all->mutable_operand(5),\n+                              /*correct_offsets=*/false),\n+  };\n+\n+  HloInstruction* intra_host_ragged_all_to_all =\n+      computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n+          /*shape=*/zero_broadcast->shape(),\n+          /*operands=*/intra_host_ragged_all_to_all_operands,\n+          /*device_list=*/CollectiveDeviceList(intra_host_replica_groups),\n+          /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n+              ? std::make_optional(NextChannelId(*computation->parent()))\n+              : std::nullopt));\n \n   HloInstruction* local_inputs =\n       computation->AddInstruction(HloInstruction::CreateAllToAll(\n-          concatenated_inputs->shape(), {concatenated_inputs},\n+          intra_host_ragged_all_to_all->shape(), {intra_host_ragged_all_to_all},\n           /*device_list=*/CollectiveDeviceList(inter_host_replica_groups),\n           /*constrain_layout=*/false,\n           /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n@@ -323,8 +337,6 @@ absl::StatusOr<bool> DecomposeCombineRaggedAllToAll(\n   }\n \n   HloInstruction* output_offsets = ragged_all_to_all->mutable_operand(4);\n-  int64_t num_updates_per_replica =\n-      output_offsets->shape().dimensions(0) / num_devices_in_replica;\n \n   output_offsets = computation->AddInstruction(HloInstruction::CreateReshape(\n       /*shape=*/ShapeUtil::MakeShape(\n@@ -344,8 +356,6 @@ absl::StatusOr<bool> DecomposeCombineRaggedAllToAll(\n \n   HloInstruction* corrected_output_offsets = output_offsets;\n \n-  int64_t num_devices_in_replica_per_host = num_devices_in_replica / num_hosts;\n-\n   corrected_output_offsets =\n       computation->AddInstruction(HloInstruction::CreateReshape(\n           /*shape=*/ShapeUtil::MakeShape("
        },
        {
            "sha": "83534c80f0853a21fba981994a69150b0691478a",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9eeebc9be5820d8a21814491e99d5f1b22b1a19c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9eeebc9be5820d8a21814491e99d5f1b22b1a19c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc?ref=9eeebc9be5820d8a21814491e99d5f1b22b1a19c",
            "patch": "@@ -151,7 +151,7 @@ ENTRY main {\n   TF_EXPECT_OK(HloCSE(true).Run(module.get()));\n \n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n-    // CHECK-COUNT-2: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}{{[}]}}\n+    // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}{{[}]}}\n     // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n     // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}{{[}]}}\n     // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},{11},{12},{13},{14},{15}{{[}]}}"
        }
    ],
    "stats": {
        "total": 108,
        "additions": 59,
        "deletions": 49
    }
}