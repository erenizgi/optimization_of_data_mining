{
    "author": "felixwqp",
    "message": "Support collective-permute to use the s-curve for cross-partition and perf-table for intra-partition.\n\nPiperOrigin-RevId: 845072088",
    "sha": "08b52d49714b67cda1f28efb32e6cc9df4ceb62f",
    "files": [
        {
            "sha": "5663de4e143406f8ee576a3558dc74f8867c35ce",
            "filename": "third_party/xla/xla/service/gpu/model/collective_interpolator.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/08b52d49714b67cda1f28efb32e6cc9df4ceb62f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/08b52d49714b67cda1f28efb32e6cc9df4ceb62f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc?ref=08b52d49714b67cda1f28efb32e6cc9df4ceb62f",
            "patch": "@@ -687,7 +687,8 @@ absl::StatusOr<absl::Duration> CollectiveInterpolator::EstimatedRuntime(\n   int64_t bytes_transferred =\n       GetBytesTransferred(instr, device_info_, analysis_);\n \n-  if (instr.opcode() == HloOpcode::kCollectivePermute) {\n+  if (instr.opcode() == HloOpcode::kCollectivePermute ||\n+      instr.opcode() == HloOpcode::kCollectivePermuteStart) {\n     auto* cp = Cast<HloCollectivePermuteInstruction>(&instr);\n     const CollectivePermuteCostModelType& permute_type =\n         GetCollectivePermuteCostModelType(\n@@ -700,7 +701,7 @@ absl::StatusOr<absl::Duration> CollectiveInterpolator::EstimatedRuntime(\n              << \" for instr: \" << instr.ToString() << \" num_partitions:\"\n              << cp->GetModule()->config().num_partitions();\n     ExactInterpolatorKey exact_key{\n-        /*opcode=*/instr.opcode(),\n+        /*opcode=*/HloOpcode::kCollectivePermute,\n         /*collective_params=*/permute_type,\n         /*data_type=*/std::nullopt,\n     };"
        },
        {
            "sha": "3b734ee1bb3f6e512f5c32e2134947b9a75f9280",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 20,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/08b52d49714b67cda1f28efb32e6cc9df4ceb62f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/08b52d49714b67cda1f28efb32e6cc9df4ceb62f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=08b52d49714b67cda1f28efb32e6cc9df4ceb62f",
            "patch": "@@ -61,7 +61,10 @@ using ::mlir::MLIRContext;\n bool IsSupportedCollectiveOp(const HloInstruction& instr) {\n   return HloPredicateIsOp<HloOpcode::kAllReduceStart, HloOpcode::kAllReduce,\n                           HloOpcode::kReduceScatter, HloOpcode::kAllGatherStart,\n-                          HloOpcode::kAllGather, HloOpcode::kAllToAll>(&instr);\n+                          HloOpcode::kAllToAll,\n+                          HloOpcode::kCollectivePermuteStart,\n+                          HloOpcode::kCollectivePermute, HloOpcode::kAllGather>(\n+      &instr);\n }\n \n bool IsHostOffloaded(const HloInstruction& instr) {\n@@ -183,7 +186,9 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n       break;\n     }\n     case HloOpcode::kRecv:\n-    case HloOpcode::kSend: {\n+    case HloOpcode::kSend:\n+    case HloOpcode::kCollectivePermute:\n+    case HloOpcode::kCollectivePermuteStart: {\n       TF_ASSIGN_OR_RETURN(\n           absl::Duration runtime,\n           sol_model.RingLatency(msg_size, num_participating_hosts,\n@@ -325,26 +330,53 @@ SolLatencyEstimator::ComputeCollectiveTime(\n     return absl::ZeroDuration();\n   }\n \n-  const HloCollectiveInstruction* collective_instr =\n-      DynCast<HloCollectiveInstruction>(\n-          instr.IsAsynchronous() ? instr.async_wrapped_instruction() : &instr);\n-\n-  if (collective_instr == nullptr) {\n-    return absl::InvalidArgumentError(\n-        absl::StrCat(\"Unsupported collective instruction: \", instr.ToString()));\n+  const HloInstruction* collective =\n+      instr.IsAsynchronous() ? instr.async_wrapped_instruction() : &instr;\n+  if (const auto* cp = DynCast<HloCollectivePermuteInstruction>(collective)) {\n+    // Handles the collective-permute ops.\n+    int64_t partition_size = GetPartitionSize(*cp, sol_flags);\n+    CollectivePermuteCostModelType cost_model_type =\n+        GetCollectivePermuteCostModelType(*cp, partition_size);\n+\n+    switch (cost_model_type) {\n+      case CollectivePermuteCostModelType::kIntraPartitionOneWay:\n+      case CollectivePermuteCostModelType::kIntraPartitionTwoWayAllMutual:\n+      case CollectivePermuteCostModelType::kIntraPartitionTwoWayHasNonMutual:\n+        return collective_interpolator->EstimatedRuntime(*cp);\n+      case CollectivePermuteCostModelType::kInterPartitionOneWay:\n+      case CollectivePermuteCostModelType::kInterPartitionTwoWayAllMutual:\n+      case CollectivePermuteCostModelType::kInterPartitionTwoWayHasNonMutual: {\n+        // TODO(wfelix): Distinguish different types of inter-partition\n+        // collectives.\n+        TF_ASSIGN_OR_RETURN(\n+            absl::Duration duration,\n+            DCNCollectiveDuration(/*num_participating_hosts=*/2,\n+                                  /*num_communicators=*/1, *cp, gpu_device_info,\n+                                  sol_flags, analysis, mlir_context));\n+        return duration;\n+      }\n+      case CollectivePermuteCostModelType::kUnknown:\n+        return absl::InvalidArgumentError(\n+            \"Unknown collective permute cost model type.\");\n+    }\n+  } else if (const auto* collective_instr =\n+                 DynCast<HloCollectiveInstruction>(collective)) {\n+    // Handles the collective ops.\n+    int64_t partition_size = GetPartitionSize(*collective_instr, sol_flags);\n+    TF_ASSIGN_OR_RETURN(\n+        GPUCommunicationType communication_type,\n+        CommunicationType(partition_size, *collective_instr,\n+                          gpu_device_info.gpu_compute_capability()));\n+    TF_ASSIGN_OR_RETURN(\n+        absl::Duration result,\n+        DispatchEstimation(communication_type, *collective_instr,\n+                           gpu_device_info, sol_flags, analysis,\n+                           collective_interpolator, mlir_context));\n+    return result;\n   }\n \n-  int64_t partition_size = GetPartitionSize(*collective_instr, sol_flags);\n-  TF_ASSIGN_OR_RETURN(\n-      GPUCommunicationType communication_type,\n-      CommunicationType(partition_size, *collective_instr,\n-                        gpu_device_info.gpu_compute_capability()));\n-  TF_ASSIGN_OR_RETURN(\n-      absl::Duration result,\n-      DispatchEstimation(communication_type, *collective_instr, gpu_device_info,\n-                         sol_flags, analysis, collective_interpolator,\n-                         mlir_context));\n-  return result;\n+  return absl::InvalidArgumentError(\n+      absl::StrCat(\"Unsupported collective instruction: \", instr.ToString()));\n }\n \n /*static*/ absl::StatusOr<std::unique_ptr<SolLatencyEstimator>>"
        },
        {
            "sha": "49756f5d8bb53e25f160860f515c7f425b583aef",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 137,
            "deletions": 9,
            "changes": 146,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/08b52d49714b67cda1f28efb32e6cc9df4ceb62f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/08b52d49714b67cda1f28efb32e6cc9df4ceb62f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=08b52d49714b67cda1f28efb32e6cc9df4ceb62f",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/replica_group.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/literal_util.h\"\n@@ -349,7 +350,7 @@ HloModule m\n ENTRY e {\n   p0 = bf16[1024,1024] parameter(0)\n   p1 = bf16[1024,1024] parameter(1)\n-  ROOT _ =  (bf16[1024,1024], s8[2097152]{0}) custom-call(p0,p1),\n+  ROOT _ =  (bf16[1024,1024], s8[2097152]) custom-call(p0,p1),\n     custom_call_target=\"__cublas$gemm\",\n     backend_config={\n       \"gemm_backend_config\":{\n@@ -377,7 +378,7 @@ HloModule m\n ENTRY e {\n   p0 = f8e5m2[1024,1024] parameter(0)\n   p1 = f8e4m3fn[1024,1024] parameter(1)\n-  ROOT _ =  (bf16[1024,1024], s8[2097152]{0}) custom-call(p0,p1),\n+  ROOT _ =  (bf16[1024,1024], s8[2097152]) custom-call(p0,p1),\n     custom_call_target=\"__cublas$lt$matmul$f8\",\n     backend_config={\n       \"gemm_backend_config\":{\n@@ -405,7 +406,7 @@ HloModule m\n ENTRY e {\n   p0 = f8e5m2[1024,1024] parameter(0)\n   p1 = f8e4m3fn[1024,1024] parameter(1)\n-  ROOT _ =  (bf16[1024,1024], s8[2097152]{0}) custom-call(p0,p1),\n+  ROOT _ =  (bf16[1024,1024], s8[2097152]) custom-call(p0,p1),\n     custom_call_target=\"__cublas$lt$matmul$f8\",\n     backend_config={\n       \"gemm_backend_config\":{\n@@ -433,7 +434,7 @@ HloModule m\n ENTRY e {\n   p0 = f8e4m3fn[1024,1024] parameter(0)\n   p1 = f8e5m2[1024,1024] parameter(1)\n-  ROOT _ =  (bf16[1024,1024], s8[2097152]{0}) custom-call(p0,p1),\n+  ROOT _ =  (bf16[1024,1024], s8[2097152]) custom-call(p0,p1),\n     custom_call_target=\"__cublas$lt$matmul$f8\",\n     backend_config={\n       \"gemm_backend_config\":{\n@@ -461,7 +462,7 @@ HloModule m\n ENTRY e {\n   p0 = f8e4m3fn[1024,1024] parameter(0)\n   p1 = f8e4m3fn[1024,1024] parameter(1)\n-  ROOT _ =  (bf16[1024,1024], s8[2097152]{0}) custom-call(p0,p1),\n+  ROOT _ =  (bf16[1024,1024], s8[2097152]) custom-call(p0,p1),\n     custom_call_target=\"__cublas$lt$matmul$f8\",\n     backend_config={\n       \"gemm_backend_config\":{\n@@ -530,8 +531,109 @@ ENTRY e {\n       /*cost_type=*/CostType::kNodeCost,\n       /*expected_latency=*/absl::ZeroDuration(),\n   };\n+  // Test for CollectivePermuteCostModelType::kIntraPartitionTwoWayHasNonMutual\n+  EstimatorTestCase collective_permute_intra_host_ring_shift = {\n+      /*test_name=*/\"collective_permute_intra_host_ring_shift\",\n+      /*module_string=*/R\"(\n+HloModule m, num_partitions=4\n+\n+ENTRY main {\n+  %param.2 = f32[262144,1024] parameter(0), sharding={devices=[4,1]<=[4]}\n+  %collective-permute-start = (f32[262144,1024], f32[262144,1024]) collective-permute-start(%param.2), channel_id=1, source_target_pairs={{0,3},{1,0},{2,1},{3,2}}\n+  ROOT %collective-permute-done = f32[262144,1024] collective-permute-done(%collective-permute-start)\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kCollectivePermuteStart,\n+      /*cost_type=*/CostType::kEdgeCost,\n+      /*expected_latency=*/absl::Microseconds(3706),\n+  };\n+\n+  // Test for CollectivePermuteCostModelType::kIntraPartitionTwoWayAllMutual\n+  EstimatorTestCase collective_permute_intra_host_bidirectional = {\n+      /*test_name=*/\"collective_permute_intra_host_bidirectional\",\n+      /*module_string=*/R\"(\n+HloModule m, num_partitions=4\n+\n+ENTRY main {\n+  %param.2 = f32[262144,1024] parameter(0), sharding={devices=[4,1]<=[4]}\n+  %collective-permute-start = (f32[262144,1024], f32[262144,1024]) collective-permute-start(%param.2), channel_id=1, source_target_pairs={{0,1},{1,0},{2,3},{3,2}}\n+  ROOT %collective-permute-done = f32[262144,1024] collective-permute-done(%collective-permute-start)\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kCollectivePermuteStart,\n+      /*cost_type=*/CostType::kEdgeCost,\n+      /*expected_latency=*/absl::Microseconds(3696),\n+  };\n+\n+  // Test for CollectivePermuteCostModelType::kIntraPartitionOneWay\n+  EstimatorTestCase collective_permute_intra_host_one_way = {\n+      /*test_name=*/\"collective_permute_intra_host_one_way\",\n+      /*module_string=*/R\"(\n+HloModule m, num_partitions=4\n+\n+ENTRY main {\n+  %param.2 = f32[262144,1024] parameter(0), sharding={devices=[4,1]<=[4]}\n+  %collective-permute-start = (f32[262144,1024], f32[262144,1024]) collective-permute-start(%param.2), channel_id=1, source_target_pairs={{0,1},{2,3}}\n+  ROOT %collective-permute-done = f32[262144,1024] collective-permute-done(%collective-permute-start)\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kCollectivePermuteStart,\n+      /*cost_type=*/CostType::kEdgeCost,\n+      /*expected_latency=*/absl::Microseconds(3961),\n+  };\n+\n+  EstimatorTestCase collective_permute_inter_host_global = {\n+      /*test_name=*/\"collective_permute_inter_host_global\",\n+      /*module_string=*/R\"(\n+HloModule m, num_partitions=16\n+\n+ENTRY main {\n+  %param.2 = f32[262144,1024] parameter(0)\n+  %collective-permute-start = (f32[262144,1024], f32[262144,1024]) collective-permute-start(%param.2), channel_id=1,\n+      source_target_pairs={{0,15},{1,0},{2,1},{3,2},{4,3},{5,4},{6,5},{7,6},{8,7},{9,8},{10,9},{11,10},{12,11},{13,12},{14,13},{15,14}}\n+  ROOT %collective-permute-done = f32[262144,1024] collective-permute-done(%collective-permute-start)\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kCollectivePermuteStart,\n+      /*cost_type=*/CostType::kEdgeCost,\n+      /*expected_latency=*/absl::Microseconds(27816),\n+  };\n \n-  return {all_gather_intra_host,\n+  EstimatorTestCase collective_permute_inter_host_rail_aligned_bidirection = {\n+      /*test_name=*/\"collective_permute_inter_host_rail_aligned_bidirection\",\n+      /*module_string=*/R\"(\n+HloModule m, num_partitions=16\n+\n+ENTRY main {\n+  %param.2 = f32[262144,1024] parameter(0)\n+  %collective-permute-start = (f32[262144,1024], f32[262144,1024]) collective-permute-start(%param.2), channel_id=1,\n+      source_target_pairs={{0,8},{8,0},{1,9},{9,1},{2,10},{10,2},{3,11},{11,3},{4,12},{12,4},{5,13},{13,5},{6,14},{14,6},{7,15},{15,7}}\n+  ROOT %collective-permute-done = f32[262144,1024] collective-permute-done(%collective-permute-start)\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kCollectivePermuteStart,\n+      /*cost_type=*/CostType::kEdgeCost,\n+      /*expected_latency=*/absl::Microseconds(27816),\n+  };\n+\n+  EstimatorTestCase collective_permute_inter_host_rail_aligned_unidirection = {\n+      /*test_name=*/\"collective_permute_inter_host_rail_aligned_unidirection\",\n+      /*module_string=*/R\"(\n+HloModule m, num_partitions=16\n+\n+ENTRY main {\n+  %param.2 = f32[262144,1024] parameter(0)\n+  %collective-permute-start = (f32[262144,1024], f32[262144,1024]) collective-permute-start(%param.2), channel_id=1,\n+      source_target_pairs={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}\n+  ROOT %collective-permute-done = f32[262144,1024] collective-permute-done(%collective-permute-start)\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kCollectivePermuteStart,\n+      /*cost_type=*/CostType::kEdgeCost,\n+      /*expected_latency=*/absl::Microseconds(27816),\n+  };\n+\n+  return {collective_permute_intra_host_ring_shift,\n+          collective_permute_intra_host_bidirectional,\n+          collective_permute_intra_host_one_way,\n+          collective_permute_inter_host_global,\n+          collective_permute_inter_host_rail_aligned_bidirection,\n+          collective_permute_inter_host_rail_aligned_unidirection,\n+          all_gather_intra_host,\n           all_gather_inter_host_pairwise,\n           all_gather_all_ranks,\n           reduce_scatter_all_ranks,\n@@ -639,6 +741,30 @@ class IsSolLatencyEstimatorEnabledTest : public HloTestBase {\n         /*channel_id=*/std::nullopt, /*use_global_device_ids=*/false));\n   }\n \n+  // Helper to add a AllToAll instruction.\n+  void AddAlltoAll(HloModule* module) {\n+    HloComputation* entry = module->entry_computation();\n+    Shape shape = ShapeUtil::MakeShape(F32, {2, 2});\n+    auto dummy_operand = entry->AddInstruction(HloInstruction::CreateConstant(\n+        LiteralUtil::CreateR2<float>({{1, 2}, {3, 4}})));\n+    entry->AddInstruction(HloInstruction::CreateAllToAll(\n+        shape, {dummy_operand},\n+        /*device_list=*/CollectiveDeviceList(),\n+        /*constrain_layout=*/false, /*channel_id=*/false,\n+        /*split_dimension=*/std::nullopt));\n+  }\n+\n+  void AddCollectiveBcast(HloModule* module) {\n+    HloComputation* entry = module->entry_computation();\n+    Shape shape = ShapeUtil::MakeShape(F32, {2, 2});\n+    auto dummy_operand = entry->AddInstruction(HloInstruction::CreateConstant(\n+        LiteralUtil::CreateR2<float>({{1, 2}, {3, 4}})));\n+    entry->AddInstruction(HloInstruction::CreateCollectiveBroadcast(\n+        shape, {dummy_operand},\n+        /*device_list=*/CollectiveDeviceList(),\n+        /*constrain_layout=*/false, /*channel_id=*/std::nullopt));\n+  }\n+\n   // Helper to add a CollectivePermute instruction.\n   void AddCollectivePermute(HloModule* module) {\n     HloComputation* entry = module->entry_computation();\n@@ -702,7 +828,7 @@ TEST_F(IsSolLatencyEstimatorEnabledTest,\n       stream_executor::CudaComputeCapability::Hopper());\n \n   auto module = CreateTestModule(config);\n-  AddCollectivePermute(module.get());  // Unsupported collective\n+  AddCollectiveBcast(module.get());  // Unsupported collective\n \n   EXPECT_FALSE(\n       SolLatencyEstimator::IsSupportedForModule(*module, gpu_device_info_));\n@@ -718,8 +844,10 @@ TEST_F(IsSolLatencyEstimatorEnabledTest,\n       stream_executor::CudaComputeCapability::Hopper());\n \n   auto module = CreateTestModule(config);\n-  AddAllReduce(module.get());          // Supported\n-  AddCollectivePermute(module.get());  // Unsupported\n+  AddAllReduce(module.get());          // Supported collective\n+  AddCollectivePermute(module.get());  // Supported collective\n+  AddAlltoAll(module.get());           // Supported collective\n+  AddCollectiveBcast(module.get());    // Unsupported collective\n \n   EXPECT_FALSE(\n       SolLatencyEstimator::IsSupportedForModule(*module, gpu_device_info_));"
        }
    ],
    "stats": {
        "total": 223,
        "additions": 192,
        "deletions": 31
    }
}