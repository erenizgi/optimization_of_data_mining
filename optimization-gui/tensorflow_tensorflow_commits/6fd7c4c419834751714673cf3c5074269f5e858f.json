{
    "author": "akuegel",
    "message": "[XLA:GPU] Fetch all hero operand indexing maps at once.\n\nAll call sites of ComputeThreadIdToInputIndexing actually want more than one\nhero operand indexing map. Currently, ComputeThreadIdToInputIndexing often\ncomputes maps for all operands but then restricts to the one that is being\nasked for. This duplicates the work and we can avoid it.\n\nPiperOrigin-RevId: 811322641",
    "sha": "6fd7c4c419834751714673cf3c5074269f5e858f",
    "files": [
        {
            "sha": "d98b16a05e2275b488954167582c766ac5661b7e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <utility>\n+#include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -62,12 +63,17 @@ std::optional<IndexingMap> ConcatenateFusion::ComputeThreadIdToOutputIndexing(\n   return std::nullopt;\n }\n \n-std::optional<IndexingMap> ConcatenateFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index,\n-    mlir::MLIRContext* ctx) const {\n-  // TODO(b/331356433): Add constraints depending on the `hero_operand_index`.\n-  return KernelEmitter::ComputeWorkItemIdToOutputIndexing(GetWorkDimensions(),\n-                                                          largest_shape_, ctx);\n+std::optional<std::vector<IndexingMap>>\n+ConcatenateFusion::ComputeThreadIdToInputIndexing(\n+    int64_t root_index, mlir::MLIRContext* ctx) const {\n+  IndexingMap map_for_largest_shape =\n+      KernelEmitter::ComputeWorkItemIdToOutputIndexing(GetWorkDimensions(),\n+                                                       largest_shape_, ctx);\n+  // TODO(b/331356433): Handle other hero operands correctly.\n+  std::vector<IndexingMap> result(\n+      analysis_.fusion_hero(root_index).GetOperands().size(),\n+      map_for_largest_shape);\n+  return result;\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>>"
        },
        {
            "sha": "6f1ca56478c442f25e6db91c25979e81ab226984",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -49,9 +49,8 @@ class ConcatenateFusion final : public EmitterBase {\n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n       int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* ctx) const override;\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule("
        },
        {
            "sha": "de6fb8b61a87c1c5a248b2c25b8d3014aebe805c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #include <cstdint>\n #include <optional>\n #include <string>\n+#include <vector>\n \n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n@@ -59,8 +60,8 @@ class DummyCopyEmitter : public EmitterBase {\n     return std::nullopt;\n   }\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t, int64_t, mlir::MLIRContext*) const final {\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t, mlir::MLIRContext*) const final {\n     return std::nullopt;\n   }\n "
        },
        {
            "sha": "ebe39d4b233c49bb2a115e2e01d73715d1e5e07d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -57,20 +57,20 @@ LaunchDimensions InPlaceDynamicUpdateSliceFusion::launch_dimensions() const {\n                                    config_);\n }\n \n-std::optional<IndexingMap>\n+std::optional<std::vector<IndexingMap>>\n InPlaceDynamicUpdateSliceFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index,\n-    mlir::MLIRContext* indexing_context) const {\n+    int64_t root_index, mlir::MLIRContext* indexing_context) const {\n   // TODO(b/331355203): Implement thread ID -> operand indexing.\n-  if (hero_operand_index != kDUSUpdateIndex) {\n-    return std::nullopt;\n-  }\n+  std::vector<IndexingMap> result(\n+      analysis_.fusion_hero(root_index).GetOperands().size(),\n+      IndexingMap::GetUndefined());\n \n   using KernelEmitter = emitters::DynamicUpdateSliceKernelEmitter;\n-  return KernelEmitter::ComputeWorkItemIdToOutputIndexing(\n+  result[kDUSUpdateIndex] = KernelEmitter::ComputeWorkItemIdToOutputIndexing(\n       GetWorkDimensions(),\n       KernelEmitter::GetIndexingShape(analysis_.fusion_spec()),\n       indexing_context);\n+  return result;\n }\n \n std::vector<emitters::EpilogueSpecification>"
        },
        {
            "sha": "90aca8f4901f5a5cefbb82f46b562782b42283c5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -61,9 +61,8 @@ class InPlaceDynamicUpdateSliceFusion : public EmitterBase {\n     return std::nullopt;\n   }\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* indexing_context) const override;\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t root_index, mlir::MLIRContext* indexing_context) const override;\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule("
        },
        {
            "sha": "93f86eb81a6bc812535c8c345acd6509fc67dc22",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 12,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <utility>\n+#include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -62,9 +63,9 @@ std::optional<IndexingMap> LoopFusion::ComputeThreadIdToOutputIndexing(\n       GetIndexShape(analysis_.fusion_root(root_index).shape()), ctx);\n }\n \n-std::optional<IndexingMap> LoopFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index,\n-    mlir::MLIRContext* ctx) const {\n+std::optional<std::vector<IndexingMap>>\n+LoopFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n+                                           mlir::MLIRContext* ctx) const {\n   std::optional<IndexingMap> thread_id_to_output_indexing =\n       ComputeThreadIdToOutputIndexing(root_index, ctx);\n   if (!thread_id_to_output_indexing.has_value()) {\n@@ -74,15 +75,22 @@ std::optional<IndexingMap> LoopFusion::ComputeThreadIdToInputIndexing(\n       &analysis_.fusion_root(root_index).instruction();\n   auto output_to_input_indexing =\n       ComputeOutputToInputIndexing(fusion_root, /*output_id=*/0, ctx);\n-  IndexingMapSet output_to_input_indexing_set = ToIndexingMapSet(\n-      output_to_input_indexing.indexing_maps[hero_operand_index]);\n-  // Since we are computing the indexing for a non-fusion op, there is only one\n-  // indexing map per operand.\n-  CHECK_EQ(output_to_input_indexing_set.size(), 1);\n-  IndexingMap thread_id_to_input_indexing_map = ComposeIndexingMaps(\n-      *thread_id_to_output_indexing, *output_to_input_indexing_set.begin());\n-  thread_id_to_input_indexing_map.Simplify();\n-  return thread_id_to_input_indexing_map;\n+  std::vector<IndexingMap> result;\n+  result.reserve(fusion_root->operand_count());\n+  for (int64_t operand_index = 0; operand_index < fusion_root->operand_count();\n+       ++operand_index) {\n+    auto output_to_input_indexing_maps =\n+        output_to_input_indexing.indexing_maps[operand_index];\n+    // Since we are computing the indexing for a non-fusion op, there is only\n+    // one indexing map per operand.\n+    CHECK_EQ(output_to_input_indexing_maps.size(), 1);\n+    IndexingMap thread_id_to_input_indexing_map =\n+        ComposeIndexingMaps(*thread_id_to_output_indexing,\n+                            output_to_input_indexing_maps.begin()->map());\n+    thread_id_to_input_indexing_map.Simplify();\n+    result.push_back(thread_id_to_input_indexing_map);\n+  }\n+  return result;\n }\n \n LaunchDimensions LoopFusion::launch_dimensions() const {"
        },
        {
            "sha": "303358a488c59700e91d12ea815e1002e48a4580",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <optional>\n+#include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -43,9 +44,8 @@ class LoopFusion final : public EmitterBase {\n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n       int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* ctx) const override;\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n  private:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule("
        },
        {
            "sha": "ac700150ed46cd8461010ca2a3e859b4c6cb1224",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 21,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -507,30 +507,42 @@ HloValueMap ReductionFusion::GetInits(int group_id, EmitterState& state) const {\n   return result;\n }\n \n-std::optional<IndexingMap> ReductionFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index, MLIRContext* ctx) const {\n+std::optional<std::vector<IndexingMap>>\n+ReductionFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n+                                                MLIRContext* ctx) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n-  if (groups_.is_reduction_root[root_index] &&\n-      hero_operand_index >= hero.operand_count() / 2) {\n-    // We don't have indexing for the init values.\n-    return std::nullopt;\n-  }\n+  std::vector<IndexingMap> result(hero.operand_count(),\n+                                  IndexingMap::GetUndefined());\n   if (!groups_.is_reduction_root[root_index]) {\n-    return ComposeIndexingMaps(\n-        *ComputeThreadIdToOutputIndexing(root_index, ctx),\n-        ComputeOutputToInputIndexing(\n-            &analysis_.fusion_root(root_index).instruction(), 0, ctx)\n-            .indexing_maps[hero_operand_index]\n-            .begin()\n-            ->map());\n+    auto thread_id_to_output_indexing =\n+        ComputeThreadIdToOutputIndexing(root_index, ctx);\n+    if (!thread_id_to_output_indexing.has_value()) {\n+      return std::nullopt;\n+    }\n+    for (int64_t operand_index = 0; operand_index < hero.operand_count();\n+         ++operand_index) {\n+      result[operand_index] = ComposeIndexingMaps(\n+          *thread_id_to_output_indexing,\n+          ComputeOutputToInputIndexing(\n+              &analysis_.fusion_root(root_index).instruction(), 0, ctx)\n+              .indexing_maps[operand_index]\n+              .begin()\n+              ->map());\n+      result[operand_index].Simplify();\n+    }\n+    return result;\n   }\n-  auto projected_map = ComputeReductionInputIndexing(ctx);\n-  AddGroupIdConstraint(projected_map, root_index, groups_);\n-  auto map = projected_map *\n-             GetBitcastMap(input_shape_,\n-                           hero.operand(hero_operand_index)->shape(), ctx);\n-  map.Simplify();\n-  return map;\n+  // We don't have indexing for the init values.\n+  for (int64_t operand_index = 0; operand_index < hero.operand_count() / 2;\n+       ++operand_index) {\n+    auto projected_map = ComputeReductionInputIndexing(ctx);\n+    AddGroupIdConstraint(projected_map, root_index, groups_);\n+    result[operand_index] =\n+        projected_map *\n+        GetBitcastMap(input_shape_, hero.operand(operand_index)->shape(), ctx);\n+    result[operand_index].Simplify();\n+  }\n+  return result;\n }\n \n std::optional<IndexingMap> ReductionFusion::ComputeThreadIdToOutputIndexing("
        },
        {
            "sha": "e75aa724fd36fe420f33b95cf78ed5bdc488e161",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -58,9 +58,8 @@ class ReductionFusion : public EmitterBase {\n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n       int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* ctx) const override;\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n   LaunchDimensions launch_dimensions() const override;\n "
        },
        {
            "sha": "d335562d6d7dd3bed71d0acebe76051c8a31027d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -366,31 +366,30 @@ ScatterFusion::ScatterFusion(const HloFusionAnalysis& analysis,\n       warp_size_(WarpSize(analysis_.device_info())),\n       vector_size_(vector_size) {}\n \n-std::optional<IndexingMap> ScatterFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index, MLIRContext* ctx) const {\n+std::optional<std::vector<IndexingMap>>\n+ScatterFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n+                                              MLIRContext* ctx) const {\n   CHECK(ScatterSimplifier::IsSimplifiedScatter(description_.scatter))\n       << \"Non-simplified HLO Scatter is not supported.\";\n \n   int64_t scatter_operand_count = description_.scatter->scatter_operand_count();\n-  // Scatter operands a packed in the following way:\n+  // Scatter operands are packed in the following way:\n   // Operand IDs [0, scatter_operand_count - 1] for `scatter operands`.\n   // Operand ID  scatter_operand_count for `scatter indices`.\n   // Operand IDs [scatter_operand_count + 1, 2 * scatter_operand_count] for\n   // `scatter updates`.\n \n+  std::vector<IndexingMap> results(description_.scatter->operand_count(),\n+                                   IndexingMap::GetUndefined());\n+  // Compute the indexing for the scatter indices operand.\n+  ComputeIndexing(ctx, /*updates_map=*/nullptr,\n+                  &results[scatter_operand_count]);\n   // For scatter operands we do not know the thread ID indexing.\n-  if (hero_operand_index < scatter_operand_count) {\n-    return std::nullopt;\n+  for (int64_t operand_index = scatter_operand_count + 1;\n+       operand_index < results.size(); ++operand_index) {\n+    ComputeIndexing(ctx, &results[operand_index], /*indices_map=*/nullptr);\n   }\n-\n-  bool is_indices_operand = hero_operand_index == scatter_operand_count;\n-  auto map = IndexingMap::GetUndefined();\n-  if (is_indices_operand) {\n-    ComputeIndexing(ctx, /*updates_map=*/nullptr, &map);\n-    return map;\n-  }\n-  ComputeIndexing(ctx, &map, /*indices_map=*/nullptr);\n-  return map;\n+  return results;\n }\n \n std::vector<emitters::EpilogueSpecification> ScatterFusion::GetEpilogues("
        },
        {
            "sha": "f89c820542f067603fca64a3e02e2b638823b4a1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -83,9 +83,8 @@ class ScatterFusion : public EmitterBase {\n     return std::nullopt;\n   }\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* ctx) const override;\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n  protected:\n   virtual absl::Status EmitEntryFunctionImpl("
        },
        {
            "sha": "99eaae2428720569e359dbdf1c64655065263ecc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.cc",
            "status": "modified",
            "additions": 47,
            "deletions": 13,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -270,23 +270,34 @@ std::optional<IndexingMap> TransposeFusion::ComputeThreadIdToOutputIndexing(\n   return GetIndexing(/*input=*/false, hero.shape(), mlir_context);\n }\n \n-std::optional<IndexingMap> TransposeFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index,\n-    MLIRContext* mlir_context) const {\n+std::optional<std::vector<IndexingMap>>\n+TransposeFusion::ComputeThreadIdToInputIndexing(\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n-  if (!GetDescriptionForTiledTransposeEmitter(hero)) {\n+  if (GetDescriptionForTiledTransposeEmitter(hero)) {\n+    return std::vector<IndexingMap>{GetIndexing(\n+        /*input=*/true, hero.operand(0)->shape(), mlir_context)};\n+  }\n+  std::vector<IndexingMap> result;\n+  result.reserve(hero.operand_count());\n+  auto thread_id_to_output_indexing =\n+      ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n+  if (!thread_id_to_output_indexing.has_value()) {\n+    return std::nullopt;\n+  }\n+  for (int64_t operand_index = 0; operand_index < hero.operand_count();\n+       ++operand_index) {\n     auto map = ComposeIndexingMaps(\n-        *ComputeThreadIdToOutputIndexing(root_index, mlir_context),\n+        *thread_id_to_output_indexing,\n         ComputeOutputToInputIndexing(\n             &analysis_.fusion_root(root_index).instruction(), 0, mlir_context)\n-            .indexing_maps[hero_operand_index]\n+            .indexing_maps[operand_index]\n             .begin()\n             ->map());\n     map.Simplify();\n-    return map;\n+    result.push_back(map);\n   }\n-  return GetIndexing(/*input=*/true, hero.operand(hero_operand_index)->shape(),\n-                     mlir_context);\n+  return result;\n }\n \n LaunchDimensions TransposeFusion::launch_dimensions() const {\n@@ -572,10 +583,33 @@ std::optional<IndexingMap> PackedTranspose::ComputeThreadIdToOutputIndexing(\n   return GetOutputIndexing(mlir_context);\n }\n \n-std::optional<IndexingMap> PackedTranspose::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index,\n-    MLIRContext* mlir_context) const {\n-  return GetInputIndexing(mlir_context);\n+std::optional<std::vector<IndexingMap>>\n+PackedTranspose::ComputeThreadIdToInputIndexing(\n+    int64_t root_index, MLIRContext* mlir_context) const {\n+  const auto& hero = analysis_.fusion_hero(root_index).instruction();\n+  if (GetDescriptionForTiledTransposeEmitter(hero)) {\n+    return std::vector<IndexingMap>{GetInputIndexing(mlir_context)};\n+  }\n+  std::vector<IndexingMap> result;\n+  result.reserve(hero.operand_count());\n+  auto thread_id_to_output_indexing =\n+      ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n+  if (!thread_id_to_output_indexing.has_value()) {\n+    return std::nullopt;\n+  }\n+  for (int64_t operand_index = 0; operand_index < hero.operand_count();\n+       ++operand_index) {\n+    auto map = ComposeIndexingMaps(\n+        *thread_id_to_output_indexing,\n+        ComputeOutputToInputIndexing(\n+            &analysis_.fusion_root(root_index).instruction(), 0, mlir_context)\n+            .indexing_maps[operand_index]\n+            .begin()\n+            ->map());\n+    map.Simplify();\n+    result.push_back(map);\n+  }\n+  return result;\n }\n \n LaunchDimensions PackedTranspose::launch_dimensions() const {"
        },
        {
            "sha": "2501d1b5ba1d6e417c6b536c5ddd8807f911ce46",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.h",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -121,9 +121,8 @@ class TransposeFusion : public TransposeFusionBase {\n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n       int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* mlir_context) const override;\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n  protected:\n   WriteResult EmitWriteToShMemMlir(\n@@ -238,9 +237,8 @@ class PackedTranspose : public TransposeFusionBase {\n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n       int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n-  std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* mlir_context) const override;\n+  std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n  protected:\n   WriteResult EmitWriteToShMemMlir("
        },
        {
            "sha": "a6ebb3a5b9d86d2034a934202a7e27564c230c64",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -80,12 +80,12 @@ class KernelFusionInterface : public FusionInterface {\n   virtual std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n       int64_t root_index, mlir::MLIRContext* ctx) const = 0;\n \n-  // Computes an indexing map from thread to input element(s) of the root's\n+  // Computes indexing maps from thread id to input elements of the root's\n   // **hero**. Note that in many cases this is not computable from the output\n   // indexing. The indexing may only be known for some operands of the hero.\n-  virtual std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* ctx) const = 0;\n+  virtual std::optional<std::vector<IndexingMap>>\n+  ComputeThreadIdToInputIndexing(int64_t root_index,\n+                                 mlir::MLIRContext* ctx) const = 0;\n \n   static constexpr std::array<int, 3> kIndexingMapThreadIdxDims = {0, 1, 2};\n   static constexpr std::array<int, 3> kIndexingMapBlockIdxDims = {3, 4, 5};"
        },
        {
            "sha": "03d042f836206fb536e9216dae5e14916c1bf5ff",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/gpu_test_correctness.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -113,11 +113,11 @@ TEST_F(CorrectnessTest, InputIndexingIsBijection) {\n   for (const auto& [hero_name, ids] : flags.bijection_inputs) {\n     TF_ASSERT_OK_AND_ASSIGN(int64_t hero_index,\n                             GetHeroIndex(hero_name, *emitter_data->analysis));\n+    auto indexing = emitter_data->emitter->ComputeThreadIdToInputIndexing(\n+        hero_index, &context);\n+    ASSERT_TRUE(indexing.has_value());\n     for (int64_t id : ids) {\n-      auto indexing = emitter_data->emitter->ComputeThreadIdToInputIndexing(\n-          hero_index, id, &context);\n-      ASSERT_TRUE(indexing.has_value());\n-      TF_ASSERT_OK(TestBijection(*indexing,\n+      TF_ASSERT_OK(TestBijection(indexing.value()[id],\n                                  emitter_data->analysis->fusion_hero(hero_index)\n                                      .GetOperand(id)\n                                      .shape()"
        },
        {
            "sha": "0bdeb98ecaa52b82e12bb447c83d8f0d0bf3739f",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fd7c4c419834751714673cf3c5074269f5e858f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc?ref=6fd7c4c419834751714673cf3c5074269f5e858f",
            "patch": "@@ -548,24 +548,15 @@ std::optional<CoalescingMap> ComputeCoalescingForAllOperands(\n   GroupedByOpIndexingMap thread_id_to_input_memory_layouts;\n   for (const auto& [root_index, hero] :\n        llvm::enumerate(fusion_analysis.fusion_heroes())) {\n-    std::vector<IndexingMap> hero_indexing_maps;\n-    hero_indexing_maps.reserve(hero.GetOperands().size());\n     // Compute thread ID -> hero operand indexing maps.\n-    for (int64_t hero_operand_index = 0;\n-         hero_operand_index < hero.GetOperands().size(); ++hero_operand_index) {\n-      // TODO(b/447057917): This can be improved to just a single call to\n-      // ComputeThreadIdToInputIndexing (without the outer for loop). The maps\n-      // for all operands are computed, anyway.\n-      std::optional<IndexingMap> thread_id_to_hero_operand_map =\n-          fusion_interface->ComputeThreadIdToInputIndexing(\n-              root_index, hero_operand_index, mlir_context);\n-      if (!thread_id_to_hero_operand_map.has_value()) {\n-        return std::nullopt;\n-      }\n-      hero_indexing_maps.push_back(thread_id_to_hero_operand_map.value());\n+    std::optional<std::vector<IndexingMap>> hero_indexing_maps =\n+        fusion_interface->ComputeThreadIdToInputIndexing(root_index,\n+                                                         mlir_context);\n+    if (!hero_indexing_maps.has_value()) {\n+      return std::nullopt;\n     }\n     GetThreadIdToInputMemoryLayoutsMaps(\n-        fusion_analysis.fusion(), hero_indexing_maps,\n+        fusion_analysis.fusion(), *hero_indexing_maps,\n         fusion_analysis.fusion_hero(root_index), operands,\n         operand_logical_to_linearized_physical_maps, mlir_context,\n         thread_id_to_input_memory_layouts);"
        }
    ],
    "stats": {
        "total": 283,
        "additions": 164,
        "deletions": 119
    }
}