{
    "author": "basioli-k",
    "message": "[XLA] Rename TargetConfig to GpuTargetConfig and add CpuTargetConfig to CompilerOptions\n\nWe need a way to pass target information to the cpu compiler, and TargetConfig seems to fit that purpose.\n\nPiperOrigin-RevId: 833355711",
    "sha": "2ec59ee45a2f72dafa6080b9e43c328864d723e4",
    "files": [
        {
            "sha": "ef8e371457400ba8a234dc5c27c598e2132d72e9",
            "filename": "tensorflow/core/tfrt/saved_model/saved_model_aot_compile.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model_aot_compile.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model_aot_compile.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model_aot_compile.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -308,15 +308,15 @@ AotCompileToGpuPjRtExecutable(\n       may_alias_resource_update, &options, compilation_result));\n \n   TF_ASSIGN_OR_RETURN(\n-      xla::Compiler::TargetConfig gpu_config,\n-      xla::Compiler::TargetConfig::FromProto(gpu_target_config));\n+      xla::Compiler::GpuTargetConfig gpu_config,\n+      xla::Compiler::GpuTargetConfig::FromProto(gpu_target_config));\n   xla::StreamExecutorGpuCompiler pjrt_gpu_compiler;\n   // Create a trivial topology, which won't be used.\n   xla::StreamExecutorGpuTopologyDescription topology(xla::CudaId(),\n                                                      xla::CudaName(), nullptr);\n   xla::CompileOptions pjrt_options =\n       GetPjRtCompileOptions(options, **compilation_result);\n-  pjrt_options.target_config = gpu_config;\n+  pjrt_options.gpu_target_config = gpu_config;\n   return pjrt_gpu_compiler.Compile(\n       pjrt_options, *((*compilation_result)->computation), topology, nullptr);\n }"
        },
        {
            "sha": "b0d39affd588b8c1d8b00c2bd46ffa31ad9e90a6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -100,7 +100,7 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       xla::Compiler::GetForPlatform(platform));\n   se::StreamExecutor* stream_executor = platform->ExecutorForDevice(0).value();\n   DebugOptions debug_options = GetDebugOptionsFromFlags();\n-  Compiler::TargetConfig target_config(stream_executor);\n+  Compiler::GpuTargetConfig target_config(stream_executor);\n \n   auto& registry = stream_executor::PlatformObjectRegistry::GetGlobalRegistry();\n   TF_ASSIGN_OR_RETURN(const GetCodegenBackends::Type& get_codegen_backends,"
        },
        {
            "sha": "d3a3845de79efd15befc18f58e891e107d2cc965",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -46,7 +46,7 @@ class BlockLevelEmitterBackend : public GpuCodegenBackend {\n       const DebugOptions* absl_nonnull debug_options,\n       Compiler* absl_nonnull compiler,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-      const Compiler::TargetConfig* target_config,\n+      const Compiler::GpuTargetConfig* target_config,\n       bool use_default_config = false)\n       : GpuCodegenBackend(\"BlockLevelEmitter\", debug_options, compiler,\n                           target_config),"
        },
        {
            "sha": "d4622c1615966f1d29a5515c4ccfa75fe8a0e5c2",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -82,7 +82,7 @@ class TritonBlockLevelFusionEmitterBackendTest\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   BlockLevelEmitterBackend backend_;\n };\n "
        },
        {
            "sha": "a57eb16dedb22f6e9925eaa0a962764372ebc60d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -48,7 +48,7 @@ class CublasBackend : public GpuCodegenBackend {\n  public:\n   explicit CublasBackend(stream_executor::StreamExecutor* stream_executor,\n                          const DebugOptions* debug_options, Compiler* compiler,\n-                         const Compiler::TargetConfig* target_config)\n+                         const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"Cublas\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "5132a6a9298d2e278ed78638f4fe24d74743b8d1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -95,7 +95,7 @@ class CublasBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CublasBackend backend_;\n \n   CublasBackendTest()"
        },
        {
            "sha": "009b4fbf73e04029b8343b8841887a4764e85997",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -47,7 +47,7 @@ class CublasLtBackend : public GpuCodegenBackend {\n   explicit CublasLtBackend(stream_executor::StreamExecutor* stream_executor,\n                            const DebugOptions* debug_options,\n                            Compiler* compiler,\n-                           const Compiler::TargetConfig* target_config)\n+                           const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"CublasLt\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "43f877b2b406e6a87c14eb31d41580bb9ed79a1f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -107,7 +107,7 @@ class CublasLtBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CublasLtBackend backend_;\n \n   CublasLtBackendTest()"
        },
        {
            "sha": "a328cdefa2811b707f67e764d48c55c9c2153e71",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -58,7 +58,7 @@ class CudnnBackend : public GpuCodegenBackend {\n  public:\n   explicit CudnnBackend(stream_executor::StreamExecutor* stream_executor,\n                         const DebugOptions* debug_options, Compiler* compiler,\n-                        const Compiler::TargetConfig* target_config)\n+                        const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"Cudnn\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "ed6a3caaa7f3117128f44673570dc2da4935c945",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -113,7 +113,7 @@ class CudnnBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CudnnBackend backend_;\n \n   CudnnBackendTest()"
        },
        {
            "sha": "c1aa956a61995b7e5a5e02628b019bd2574cebdc",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -36,7 +36,7 @@ class CustomKernelBackend : public GpuCodegenBackend {\n   explicit CustomKernelBackend(stream_executor::StreamExecutor* stream_executor,\n                                const DebugOptions* debug_options,\n                                Compiler* compiler,\n-                               const Compiler::TargetConfig* target_config)\n+                               const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"CustomKernel\", debug_options, compiler,\n                           target_config, stream_executor) {}\n "
        },
        {
            "sha": "04c5d3047fcecd35bc3c44f1d400a5402d4e2e36",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -103,7 +103,7 @@ class CustomKernelBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   CustomKernelBackend backend_;\n \n   CustomKernelBackendTest()"
        },
        {
            "sha": "cf99586c21eff16aaf9d87258bce784bec23eaa4",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -32,14 +32,14 @@ namespace gpu {\n struct GetCodegenBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      const Compiler::TargetConfig*,\n+      const Compiler::GpuTargetConfig*,\n       SymbolicExprContext* symbolic_expr_context)>;\n };\n \n struct GetFissionBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      const Compiler::TargetConfig*,\n+      const Compiler::GpuTargetConfig*,\n       SymbolicExprContext* symbolic_expr_context)>;\n };\n "
        },
        {
            "sha": "28b5ba0106867e5d6ab082d575def09d0cc497cb",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -62,7 +62,7 @@ std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n@@ -79,7 +79,7 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<FissionBackend>("
        },
        {
            "sha": "2eed41c6dcbf7de6de73763ae35f42f2d0a49347",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_rocm.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -35,7 +35,7 @@ namespace gpu {\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n@@ -48,7 +48,7 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   return {};\n }"
        },
        {
            "sha": "94e30b996b1845bcf9a0c6ec0f30545273273eb1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -45,7 +45,7 @@ class FissionBackend : public GpuCodegenBackend {\n  public:\n   explicit FissionBackend(stream_executor::StreamExecutor* stream_executor,\n                           const DebugOptions* debug_options, Compiler* compiler,\n-                          const Compiler::TargetConfig* target_config,\n+                          const Compiler::GpuTargetConfig* target_config,\n                           SymbolicExprContext* symbolic_expr_context)\n       : GpuCodegenBackend(\"Fission\", debug_options, compiler, target_config),\n         cublas_backend_(stream_executor, debug_options, compiler,"
        },
        {
            "sha": "d5ca0850f63b12226129e204f63c6db2a1405b6d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -44,7 +44,7 @@ namespace xla::gpu {\n class FissionBackend : public GpuCodegenBackend {\n  public:\n   FissionBackend(const DebugOptions* debug_options, Compiler* compiler,\n-                 const Compiler::TargetConfig* target_config,\n+                 const Compiler::GpuTargetConfig* target_config,\n                  std::unique_ptr<GpuCodegenBackend> backend,\n                  std::unique_ptr<HloPassPipeline> rewriter_pipeline,\n                  SymbolicExprContext* symbolic_expr_context,"
        },
        {
            "sha": "a1409cfd50d32b74c88179c6501b03f3ce51b1ef",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -77,7 +77,7 @@ class CublasFissionTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   se::DeviceDescription device_description_;\n   std::unique_ptr<HloPassPipeline> rewriter_pipeline_;\n   std::unique_ptr<GpuCodegenBackend> cublas_backend_;"
        },
        {
            "sha": "36cd3ee128dabf7c0fa910dfe5eee8d5dfdd341c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -74,7 +74,7 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   FissionBackend backend_;\n   mlir::MLIRContext mlir_context_;\n   SymbolicExprContext symbolic_expr_context_{&mlir_context_};"
        },
        {
            "sha": "bdb247e4e3f577716c6d6067d33c0b0d4286de29",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -34,7 +34,7 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n \n namespace xla {\n-namespace  gpu {\n+namespace gpu {\n \n // Abstract base class for GPU backends, implementing the Backend interface.\n class GpuCodegenBackend : public CodegenBackend {\n@@ -43,7 +43,7 @@ class GpuCodegenBackend : public CodegenBackend {\n   // TODO(b/447096292): Remove stream_executor from GpuCodegenBackend.\n   GpuCodegenBackend(absl::string_view name, const DebugOptions* debug_options,\n                     Compiler* compiler,\n-                    const Compiler::TargetConfig* target_config,\n+                    const Compiler::GpuTargetConfig* target_config,\n                     stream_executor::StreamExecutor* stream_executor = nullptr)\n       : name_(name),\n         stream_executor_(stream_executor),\n@@ -53,7 +53,9 @@ class GpuCodegenBackend : public CodegenBackend {\n \n   absl::string_view name() const override { return name_; }\n \n-  const Compiler::TargetConfig& target_config() const { return target_config_; }\n+  const Compiler::GpuTargetConfig& target_config() const {\n+    return target_config_;\n+  }\n   const DebugOptions& debug_options() const { return debug_options_; }\n   stream_executor::StreamExecutor* stream_executor() {\n     return stream_executor_;\n@@ -75,7 +77,7 @@ class GpuCodegenBackend : public CodegenBackend {\n         allow_register_spills_);\n \n     Compiler::CompileOptions options;\n-    options.target_config = target_config_;\n+    options.gpu_target_config = target_config_;\n     options.is_autotuning_compilation = true;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));\n@@ -127,7 +129,7 @@ class GpuCodegenBackend : public CodegenBackend {\n \n   std::string name_;\n   stream_executor::StreamExecutor* stream_executor_;\n-  const Compiler::TargetConfig& target_config_;\n+  const Compiler::GpuTargetConfig& target_config_;\n   const DebugOptions& debug_options_;\n   // TODO(b/407494653): remove compiler when we don't need to run any HLO passes\n   // and the codegen backend can directly produce an executable without a"
        },
        {
            "sha": "73bc68705091a95f93a334d1834379417ccdc8ed",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/miopen.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -36,7 +36,7 @@ class MIOpenBackend : public GpuCodegenBackend {\n  public:\n   explicit MIOpenBackend(stream_executor::StreamExecutor* stream_executor,\n                          const DebugOptions* debug_options, Compiler* compiler,\n-                         const Compiler::TargetConfig* target_config)\n+                         const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"MIOpen\", debug_options, compiler, target_config,\n                           stream_executor) {}\n "
        },
        {
            "sha": "cda72dfbac291010beb8a50be50a2bd3376b1739",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/miopen_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -76,7 +76,7 @@ class MIOpenBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   AMDGPUCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   MIOpenBackend backend_;\n \n   MIOpenBackendTest()"
        },
        {
            "sha": "a7fc19d559bf1ab2c93961fb8fa1be444ad09d4a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -40,7 +40,7 @@ class NativeEmitterBackend : public GpuCodegenBackend {\n  public:\n   explicit NativeEmitterBackend(const DebugOptions* absl_nonnull debug_options,\n                                 Compiler* absl_nonnull compiler,\n-                                const Compiler::TargetConfig* target_config)\n+                                const Compiler::GpuTargetConfig* target_config)\n       : GpuCodegenBackend(\"NativeEmitter\", debug_options, compiler,\n                           target_config) {}\n "
        },
        {
            "sha": "bdf8dad85a9b8ba5cdbf0983f959644145db5ad5",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -101,7 +101,7 @@ class NativeEmitterBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   NativeEmitterBackend backend_;\n };\n "
        },
        {
            "sha": "592f9c97f96f8d3b9def83b8ad3064414a9024b0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -37,7 +37,7 @@ namespace gpu {\n class TritonBackend : public GpuCodegenBackend {\n  public:\n   explicit TritonBackend(const DebugOptions* debug_options, Compiler* compiler,\n-                         const Compiler::TargetConfig* target_config,\n+                         const Compiler::GpuTargetConfig* target_config,\n                          SymbolicExprContext* symbolic_expr_context)\n       : GpuCodegenBackend(\"Triton\", debug_options, compiler, target_config),\n         symbolic_expr_context_(symbolic_expr_context) {}"
        },
        {
            "sha": "79e00fbebc3f3c95ba08a84da416178e7553fb49",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -89,7 +89,7 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   se::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  Compiler::GpuTargetConfig target_config_;\n   TritonBackend backend_;\n   mlir::MLIRContext mlir_context_;\n   SymbolicExprContext symbolic_expr_context_{&mlir_context_};"
        },
        {
            "sha": "6f9d0d70e56df0a481de09e0dc6177d120bb03c8",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_gpu_internal.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_internal.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_internal.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_internal.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -248,7 +248,7 @@ absl::StatusOr<TargetConfigAndDevices> GetTargetConfigFromOptions(\n        xla_client->backend().stream_executors()) {\n     device_ids.push_back(executor->device_ordinal());\n   }\n-  auto gpu_target_config = xla::Compiler::TargetConfig(executor);\n+  auto gpu_target_config = xla::Compiler::GpuTargetConfig(executor);\n   return {{gpu_target_config.ToProto(), device_ids}};\n }\n "
        },
        {
            "sha": "2c31ae23b2cfdd34f00dce1bb04403b2d3200da0",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -180,7 +180,7 @@ GetTargetConfigForDevices(absl::Span<PjRtDevice* const> devices) {\n         tensorflow::down_cast<const PjRtStreamExecutorDevice*>(device)\n             ->local_device_state();\n     if (local_device_state != nullptr) {\n-      return xla::Compiler::TargetConfig(local_device_state->executor())\n+      return xla::Compiler::GpuTargetConfig(local_device_state->executor())\n           .ToProto();\n     }\n   }"
        },
        {
            "sha": "5d0632a7f7aa4d6f175d8a5bc36a0dd00204a586",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -124,7 +124,7 @@ StreamExecutorGpuCompiler::Compile(CompileOptions options,\n                       GetCompilerForPlatform(requested_platform_id_));\n \n   CompileOptions input_options = options;\n-  if (!options.target_config) {\n+  if (!options.gpu_target_config) {\n     if (client != nullptr) {\n       TF_RET_CHECK(IsGpuClient(*client))\n           << \"GPU compilation requires a GPU PjRt client.\";\n@@ -138,9 +138,9 @@ StreamExecutorGpuCompiler::Compile(CompileOptions options,\n             topology);\n     if (gpu_topology.target_config().has_value()) {\n       TF_ASSIGN_OR_RETURN(\n-          Compiler::TargetConfig target_config,\n-          Compiler::TargetConfig::FromProto(*gpu_topology.target_config()));\n-      options.target_config.emplace(std::move(target_config));\n+          Compiler::GpuTargetConfig target_config,\n+          Compiler::GpuTargetConfig::FromProto(*gpu_topology.target_config()));\n+      options.gpu_target_config.emplace(std::move(target_config));\n     } else {\n       return absl::UnimplementedError(\n           \"Compilation without client and without target_config specified is \"\n@@ -168,10 +168,10 @@ StreamExecutorGpuCompiler::Compile(CompileOptions options,\n                                   gpu_compiler.get(), std::placeholders::_1));\n   DumpHloModuleIfEnabled(*hlo_module, kBeforeOptimizationsDumpName);\n   Compiler::CompileOptions opts;\n-  opts.target_config = options.target_config;\n+  opts.gpu_target_config = options.gpu_target_config;\n \n   AotCompilationOptions aot_options(gpu_compiler->PlatformId());\n-  aot_options.set_target_config(*options.target_config);\n+  aot_options.set_gpu_target_config(*options.gpu_target_config);\n   aot_options.set_run_backend_only(\n       options.executable_build_options.run_backend_only());\n \n@@ -193,7 +193,7 @@ StreamExecutorGpuCompiler::Compile(CompileOptions options,\n                                    mlir::ModuleOp module,\n                                    const PjRtTopologyDescription& topology,\n                                    PjRtClient* client) {\n-  if (!options.target_config && client != nullptr) {\n+  if (!options.gpu_target_config && client != nullptr) {\n     TF_RET_CHECK(IsGpuClient(*client))\n         << \"GPU compilation requires a GPU PjRt client.\";\n     TF_RETURN_IF_ERROR(IsValidTopologyAndClientForCompile(topology, client));"
        },
        {
            "sha": "1c9da024d93e4b6b0ece34108257c094f0a30de0",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_compiler_aot_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler_aot_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler_aot_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler_aot_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -85,7 +85,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessAotCompileMlirAndLoad) {\n                           GetStreamExecutorGpuClient(GpuClientOptions()));\n   auto se_client = absl::WrapUnique(\n       tensorflow::down_cast<StreamExecutorGpuClient*>(client.release()));\n-  Compiler::TargetConfig gpu_target_config = xla::Compiler::TargetConfig(\n+  Compiler::GpuTargetConfig gpu_target_config = xla::Compiler::GpuTargetConfig(\n       se_client->client()->backend().default_stream_executor());\n   StreamExecutorGpuCompiler compiler(se_client->client()->platform()->id());\n \n@@ -95,7 +95,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessAotCompileMlirAndLoad) {\n       mlir::parseSourceString<mlir::ModuleOp>(mlir_str, &context);\n   TF_ASSERT_OK_AND_ASSIGN(auto topology, se_client->GetTopologyDescription());\n   xla::CompileOptions opts;\n-  opts.target_config = gpu_target_config;\n+  opts.gpu_target_config = gpu_target_config;\n \n   TF_ASSERT_OK_AND_ASSIGN(auto executable,\n                           compiler.Compile(opts, mlir_module.get(), *topology,\n@@ -115,7 +115,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessAotCompileXlaAndLoad) {\n                           GetStreamExecutorGpuClient(GpuClientOptions()));\n   auto se_client = absl::WrapUnique(\n       tensorflow::down_cast<StreamExecutorGpuClient*>(client.release()));\n-  Compiler::TargetConfig gpu_target_config{\n+  Compiler::GpuTargetConfig gpu_target_config{\n       se_client->client()->backend().default_stream_executor()};\n   StreamExecutorGpuCompiler compiler(se_client->client()->platform()->id());\n \n@@ -124,7 +124,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessAotCompileXlaAndLoad) {\n   TF_ASSERT_OK_AND_ASSIGN(const PjRtTopologyDescription* topology,\n                           se_client->GetTopologyDescription());\n   xla::CompileOptions opts;\n-  opts.target_config = gpu_target_config;\n+  opts.gpu_target_config = gpu_target_config;\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtExecutable> executable,\n@@ -145,7 +145,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessLoadFromSerializedExecutable) {\n       tensorflow::down_cast<StreamExecutorGpuClient*>(client.release()));\n   StreamExecutorGpuCompiler compiler(se_client->client()->platform()->id());\n   xla::CompileOptions opts;\n-  opts.target_config = Compiler::TargetConfig(\n+  opts.gpu_target_config = Compiler::GpuTargetConfig(\n       se_client->client()->backend().default_stream_executor());\n \n   TF_ASSERT_OK_AND_ASSIGN(XlaComputation computation,\n@@ -182,7 +182,7 @@ TEST(StreamExecutorGpuCompilerTest, SuccessSerializeDeserialize) {\n       tensorflow::down_cast<StreamExecutorGpuClient*>(client.release()));\n   StreamExecutorGpuCompiler compiler(se_client->client()->platform()->id());\n   xla::CompileOptions opts;\n-  opts.target_config = Compiler::TargetConfig(\n+  opts.gpu_target_config = Compiler::GpuTargetConfig(\n       se_client->client()->backend().default_stream_executor());\n \n   TF_ASSERT_OK_AND_ASSIGN(XlaComputation computation,\n@@ -225,7 +225,7 @@ TEST(StreamExecutorGpuCompilerTest, UnloadedExecutableMemoryStats) {\n       tensorflow::down_cast<StreamExecutorGpuClient*>(client.release()));\n   StreamExecutorGpuCompiler compiler(se_client->client()->platform()->id());\n   xla::CompileOptions options;\n-  options.target_config = Compiler::TargetConfig(\n+  options.gpu_target_config = Compiler::GpuTargetConfig(\n       se_client->client()->backend().default_stream_executor());\n \n   // Build the output shape with the correct memory space set."
        },
        {
            "sha": "26f9311b77569cfd3f74f698b96149dcc0b66318",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -285,7 +285,7 @@ std::optional<stream_executor::GpuTargetConfigProto> GetTargetConfigForDevices(\n     se::StreamExecutor* executor =\n         tensorflow::down_cast<const TfrtGpuDevice*>(device)->executor();\n     if (executor != nullptr) {\n-      return xla::Compiler::TargetConfig(executor).ToProto();\n+      return xla::Compiler::GpuTargetConfig(executor).ToProto();\n     }\n   }\n   return std::nullopt;"
        },
        {
            "sha": "779f3cd516e57b8fde0140e1cad73e6c75542fbe",
            "filename": "third_party/xla/xla/pjrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -100,8 +100,8 @@ absl::StatusOr<CompileOptionsProto> CompileOptions::ToProto() const {\n                env_option_override.second);\n   }\n \n-  if (target_config.has_value()) {\n-    *output.mutable_target_config() = target_config->ToProto();\n+  if (gpu_target_config.has_value()) {\n+    *output.mutable_target_config() = gpu_target_config->ToProto();\n   }\n   return output;\n }\n@@ -137,8 +137,9 @@ absl::StatusOr<CompileOptions> CompileOptions::FromProto(\n                       LoadEnvOptionOverrides(proto.env_option_overrides()));\n \n   if (proto.has_target_config()) {\n-    TF_ASSIGN_OR_RETURN(output.target_config, Compiler::TargetConfig::FromProto(\n-                                                  proto.target_config()));\n+    TF_ASSIGN_OR_RETURN(\n+        output.gpu_target_config,\n+        Compiler::GpuTargetConfig::FromProto(proto.target_config()));\n   }\n   return output;\n }"
        },
        {
            "sha": "14959cefc9a30fad9b62c285b450c53e2b020ec3",
            "filename": "third_party/xla/xla/pjrt/pjrt_executable.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -112,7 +112,7 @@ struct CompileOptions {\n       std::vector<std::pair<std::string, OptionOverride>>;\n   EnvironmentOptionOverrides env_option_overrides;\n \n-  std::optional<xla::Compiler::TargetConfig> target_config;\n+  std::optional<xla::Compiler::GpuTargetConfig> gpu_target_config;\n \n   // Allow to modify the input MLIR / XLA program.\n   // This is used to run passes on the MLIR parameter without having to clone it"
        },
        {
            "sha": "675a26b149743206bf153572fc40a956a53fde8b",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -1580,6 +1580,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n+        \"//xla/service/cpu:executable_proto_cc\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\","
        },
        {
            "sha": "bcd1e5c5a29c1d2e6335727b50e163f4a8cff517",
            "filename": "third_party/xla/xla/service/compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -41,7 +41,7 @@ namespace xla {\n \n /* static */ absl::Mutex Compiler::platform_compiler_mutex_(absl::kConstInit);\n \n-Compiler::TargetConfig::TargetConfig(se::StreamExecutor* s)\n+Compiler::GpuTargetConfig::GpuTargetConfig(se::StreamExecutor* s)\n     : device_description(s->GetDeviceDescription()),\n       platform_name(s->GetPlatform()->Name()),\n       device_description_str(s->GetDeviceDescription().name()) {\n@@ -54,9 +54,9 @@ Compiler::TargetConfig::TargetConfig(se::StreamExecutor* s)\n   }\n }\n \n-absl::StatusOr<Compiler::TargetConfig> Compiler::TargetConfig::FromProto(\n+absl::StatusOr<Compiler::GpuTargetConfig> Compiler::GpuTargetConfig::FromProto(\n     const se::GpuTargetConfigProto& proto) {\n-  TargetConfig target_config;\n+  GpuTargetConfig target_config;\n   TF_ASSIGN_OR_RETURN(\n       target_config.device_description,\n       stream_executor::DeviceDescription::FromProto(proto.gpu_device_info()));\n@@ -76,7 +76,7 @@ absl::StatusOr<Compiler::TargetConfig> Compiler::TargetConfig::FromProto(\n   return target_config;\n }\n \n-se::GpuTargetConfigProto Compiler::TargetConfig::ToProto() const {\n+se::GpuTargetConfigProto Compiler::GpuTargetConfig::ToProto() const {\n   stream_executor::GpuTargetConfigProto proto;\n   *proto.mutable_gpu_device_info() = device_description.ToGpuProto();\n   proto.set_platform_name(platform_name);"
        },
        {
            "sha": "fea7e886d548141789dfe592edbae1f0e390873c",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 26,
            "deletions": 11,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_value.h\"\n #include \"xla/service/computation_placer.h\"\n+#include \"xla/service/cpu/executable.pb.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -131,15 +132,15 @@ class AotCompilationMetadata {\n class Compiler {\n  public:\n   // Description of a target device for compilation.\n-  struct TargetConfig {\n-    explicit TargetConfig(se::StreamExecutor* s);\n+  struct GpuTargetConfig {\n+    explicit GpuTargetConfig(se::StreamExecutor* s);\n \n-    static absl::StatusOr<TargetConfig> FromProto(\n+    static absl::StatusOr<GpuTargetConfig> FromProto(\n         const se::GpuTargetConfigProto& proto);\n \n     se::GpuTargetConfigProto ToProto() const;\n \n-    bool operator==(const TargetConfig& other) const {\n+    bool operator==(const GpuTargetConfig& other) const {\n       // TODO(cheshire): More efficient comparator, this is currently just for\n       // tests.\n       return ToProto().SerializeAsString() ==\n@@ -154,7 +155,17 @@ class Compiler {\n     std::string device_description_str;\n \n    private:\n-    TargetConfig() = default;\n+    GpuTargetConfig() = default;\n+  };\n+\n+  // Description of a target CPU for compilation.\n+  struct CpuTargetConfig {\n+    explicit CpuTargetConfig(\n+        cpu::TargetMachineOptionsProto& target_machine_options_proto)\n+        : cpu_target_machine_options_proto(target_machine_options_proto) {};\n+\n+    // If not set, we default to the options inferred from the host machine.\n+    cpu::TargetMachineOptionsProto cpu_target_machine_options_proto;\n   };\n \n   struct CompileOptions {\n@@ -176,7 +187,10 @@ class Compiler {\n \n     // AOT device description. If provided, used instead of querying the device\n     // on which compilation is performed.\n-    std::optional<TargetConfig> target_config;\n+    std::optional<GpuTargetConfig> gpu_target_config;\n+\n+    // CPU specific target information.\n+    std::optional<CpuTargetConfig> cpu_target_config;\n \n     MultiProcessKeyValueStore key_value_store;\n \n@@ -475,11 +489,12 @@ class AotCompilationOptions {\n     sanitize_abilists_dataflow_ = abilists;\n   }\n \n-  const std::optional<Compiler::TargetConfig>& target_config() const {\n-    return target_config_;\n+  const std::optional<Compiler::GpuTargetConfig>& gpu_target_config() const {\n+    return gpu_target_config_;\n   }\n-  void set_target_config(const Compiler::TargetConfig& target_config) {\n-    target_config_ = target_config;\n+  void set_gpu_target_config(\n+      const Compiler::GpuTargetConfig& gpu_target_config) {\n+    gpu_target_config_ = gpu_target_config;\n   }\n \n  protected:\n@@ -500,7 +515,7 @@ class AotCompilationOptions {\n   bool sanitize_dataflow_ = false;\n   std::vector<std::string> sanitize_abilists_dataflow_;\n   // Contains target-specific information required by AOT compilation.\n-  std::optional<Compiler::TargetConfig> target_config_;\n+  std::optional<Compiler::GpuTargetConfig> gpu_target_config_;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "b5df2881583cf6c8c9a8041d6d5b93597df8b2f4",
            "filename": "third_party/xla/xla/service/compiler_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -35,7 +35,7 @@ TEST(TargetConfigTest, ExecutorConstructorFillsAllFields) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       stream_executor::StreamExecutor * executor,\n       stream_executor::GPUMachineManager()->ExecutorForDevice(0));\n-  Compiler::TargetConfig config(executor);\n+  Compiler::GpuTargetConfig config(executor);\n   stream_executor::GpuTargetConfigProto target = config.ToProto();\n \n   // We don't attempt to validate values because doing so would require talking\n@@ -63,7 +63,7 @@ TEST(TargetConfigTest, ProtoConstructorFillsAllFields) {\n   config_proto.set_device_description_str(\"foo\");\n \n   TF_ASSERT_OK_AND_ASSIGN(auto config,\n-                          Compiler::TargetConfig::FromProto(config_proto));\n+                          Compiler::GpuTargetConfig::FromProto(config_proto));\n   stream_executor::GpuTargetConfigProto target = config.ToProto();\n \n   EXPECT_EQ(target.dnn_version_info().major(),"
        },
        {
            "sha": "c9017c3b85bf537bc62973a0a1853a9e681e3ab6",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -181,7 +181,7 @@ absl::Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization(\n \n absl::Status AMDGPUCompiler::OptimizeHloPostLayoutAssignment(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n-    const CompileOptions& options, const TargetConfig& gpu_target_config,\n+    const CompileOptions& options, const GpuTargetConfig& gpu_target_config,\n     const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool) {\n   HloPassPipeline pre_pipeline(\"AMDGPU post-layout_assignment part 1\");\n \n@@ -243,7 +243,7 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n     const CompileOptions& options, HloModule* hlo_module,\n     AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n     se::StreamExecutor* stream_exec,\n-    const Compiler::TargetConfig* target_config) {\n+    const Compiler::GpuTargetConfig* target_config) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n   if (hlo_module->config()\n           .debug_options()"
        },
        {
            "sha": "37f1f691bcbc95120b2be3bfbf5234d0c880da5b",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -53,7 +53,7 @@ class AMDGPUCompiler : public GpuCompiler {\n \n   absl::Status OptimizeHloPostLayoutAssignment(\n       HloModule* hlo_module, se::StreamExecutor* stream_exec,\n-      const CompileOptions& options, const TargetConfig& gpu_target_config,\n+      const CompileOptions& options, const GpuTargetConfig& gpu_target_config,\n       const GpuAliasInfo* alias_info,\n       tsl::thread::ThreadPool* thread_pool) override;\n \n@@ -66,7 +66,7 @@ class AMDGPUCompiler : public GpuCompiler {\n       const CompileOptions& options, HloModule* hlo_module,\n       AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n       se::StreamExecutor* stream_exec,\n-      const Compiler::TargetConfig* target_config) override;\n+      const Compiler::GpuTargetConfig* target_config) override;\n \n   absl::StatusOr<BackendCompileResult> CompileTargetBinary(\n       const HloModuleConfig& module_config, llvm::Module* llvm_module,"
        },
        {
            "sha": "a48e0a653ef87c114c798068a512a9e813143e61",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -93,7 +93,7 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     const DebugOptions& debug_options,\n     stream_executor::StreamExecutor* stream_executor,\n     tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     se::DeviceMemoryAllocator* allocator, bool optimize_scratch_bytes,\n     MultiProcessKeyValueStore key_value_store) {\n   std::unique_ptr<Profiler> profiler = nullptr;"
        },
        {
            "sha": "7aab960df615f9fb46e6cf1f91cf57b02571c037",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -45,7 +45,7 @@ class AutotunerPass : public HloModulePass {\n       std::vector<std::unique_ptr<CodegenBackend>> backends,\n       const DebugOptions& debug_options, se::StreamExecutor* stream_executor,\n       tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n-      const Compiler::TargetConfig* target_config,\n+      const Compiler::GpuTargetConfig* target_config,\n       se::DeviceMemoryAllocator* allocator = nullptr,\n       bool optimize_scratch_bytes = true,\n       MultiProcessKeyValueStore key_value_store = MultiProcessKeyValueStore());"
        },
        {
            "sha": "da4b987f7f9b7463ed7ddb2b1de6911ff97ff8aa",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -106,7 +106,7 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n-  GpuCompiler::TargetConfig target_config(stream_executor_);\n+  GpuCompiler::GpuTargetConfig target_config(stream_executor_);\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_executor_, &module->config().debug_options(), &compiler_,\n       &target_config));\n@@ -134,7 +134,7 @@ TEST_F(AutotunerPassTest, CublasGemmIsNotAutotunedWhenFilterReturnsFalse) {\n \n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n-  GpuCompiler::TargetConfig target_config(stream_executor_);\n+  GpuCompiler::GpuTargetConfig target_config(stream_executor_);\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_executor_, &module->config().debug_options(), &compiler_,\n@@ -172,7 +172,7 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n \n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n-  GpuCompiler::TargetConfig target_config(stream_executor_);\n+  GpuCompiler::GpuTargetConfig target_config(stream_executor_);\n \n   // Run the pass for the first time, this should populate the cache.\n   {\n@@ -255,7 +255,7 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedWithCacheOnly) {\n \n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n-  GpuCompiler::TargetConfig target_config(stream_executor_);\n+  GpuCompiler::GpuTargetConfig target_config(stream_executor_);\n \n   // Run the pass for the first time, this should populate the cache.\n   {\n@@ -319,7 +319,7 @@ TEST_F(AutotunerPassTest, DevicelessUsesDefaultConfigIfNoCache) {\n \n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n-  GpuCompiler::TargetConfig target_config(stream_executor_);\n+  GpuCompiler::GpuTargetConfig target_config(stream_executor_);\n \n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<CublasBackend>(\n@@ -376,7 +376,7 @@ ENTRY %main (arg0: f32[100,100], arg1: f32[100,100]) -> f32[100,100] {\n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n-  GpuCompiler::TargetConfig target_config(stream_executor_);\n+  GpuCompiler::GpuTargetConfig target_config(stream_executor_);\n \n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_executor_, &module->config().debug_options(), &compiler_,"
        },
        {
            "sha": "e9d2d5595adabbc14bf257ca07b6dfdf7e6bdfde",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -1702,8 +1702,8 @@ absl::StatusOr<bool> GemmFusionAutotuner::RunViaNewInfra(\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<Compiler> compiler,\n                       Compiler::GetForPlatform(stream_exec->GetPlatform()));\n   se::DeviceMemoryAllocator* device_allocator = config_.GetAllocator();\n-  std::unique_ptr<Compiler::TargetConfig> target_config;\n-  target_config = std::make_unique<Compiler::TargetConfig>(stream_exec);\n+  std::unique_ptr<Compiler::GpuTargetConfig> target_config;\n+  target_config = std::make_unique<Compiler::GpuTargetConfig>(stream_exec);\n   backends.push_back(std::make_unique<TritonBackend>(\n       &debug_options, compiler.get(), target_config.get(),\n       symbolic_expr_context_));"
        },
        {
            "sha": "a91a7250ba5cfe936e7cf9d40395a94920293f75",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -105,9 +105,9 @@ ENTRY main {\n                           platform->ExecutorForDevice(0));\n \n   // Stream executor is not passed as an option.\n-  Compiler::TargetConfig gpu_target_config(stream_exec);\n+  Compiler::GpuTargetConfig gpu_target_config(stream_exec);\n   AotCompilationOptions aot_options(compiler->PlatformId());\n-  aot_options.set_target_config(gpu_target_config);\n+  aot_options.set_gpu_target_config(gpu_target_config);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::vector<std::unique_ptr<AotCompilationResult>> aot_results,"
        },
        {
            "sha": "5f5679e768c4979deaec5445693b01f9870566f3",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -354,7 +354,7 @@ MaybeOwningThreadPool CreateMaybeOwningThreadPool(\n \n DeviceOrDevicelessConfig GetDeviceConfig(\n     se::StreamExecutor* stream_exec, const GpuCompiler::CompileOptions& options,\n-    const Compiler::TargetConfig& gpu_target_config) {\n+    const Compiler::GpuTargetConfig& gpu_target_config) {\n   if (stream_exec) {\n     return DeviceOrDevicelessConfig{\n         DeviceConfig{stream_exec, options.device_allocator}};\n@@ -459,7 +459,7 @@ absl::Status RunPreSPMDPartitionerPasses(HloModule* hlo_module) {\n }\n \n absl::Status RunSPMDPasses(\n-    HloModule* hlo_module, const Compiler::TargetConfig& gpu_target_config,\n+    HloModule* hlo_module, const Compiler::GpuTargetConfig& gpu_target_config,\n     const AliasInfo* alias_info,\n     const AlgebraicSimplifierOptions& layout_insensitive_algsimp_opts,\n     int64_t max_windowed_einsum_iteration) {\n@@ -545,7 +545,7 @@ bool BackendConfigDeviceTypeIsHost(HloInstruction* instr) {\n \n absl::Status RunOptimizationPasses(\n     HloModule* hlo_module, stream_executor::StreamExecutor* stream_exec,\n-    const Compiler::TargetConfig& gpu_target_config,\n+    const Compiler::GpuTargetConfig& gpu_target_config,\n     const AlgebraicSimplifierOptions& layout_insensitive_algsimp_opts,\n     absl::string_view platform_name) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n@@ -1003,7 +1003,7 @@ absl::Status RunLayoutAssignmentPasses(\n }\n \n absl::Status RunFusionPasses(HloModule* hlo_module,\n-                             const Compiler::TargetConfig& gpu_target_config,\n+                             const Compiler::GpuTargetConfig& gpu_target_config,\n                              tsl::thread::ThreadPool* thread_pool,\n                              HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n                              SymbolicExprContext* symbolic_expr_context) {\n@@ -1133,7 +1133,7 @@ absl::Status RunPostFusionPasses(\n absl::Status RunPostFusionSimplificationPasses(\n     HloModule* hlo_module, const AlgebraicSimplifierOptions& algsimp_options,\n     se::GpuComputeCapability gpu_version,\n-    const Compiler::TargetConfig& gpu_target_config) {\n+    const Compiler::GpuTargetConfig& gpu_target_config) {\n   HloPassPipeline pipeline(\"post-fusion-simplification-pipeline optimization\");\n   pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n \n@@ -1163,7 +1163,7 @@ absl::Status RunPostFusionSimplificationPasses(\n absl::Status RunPostFusionVerificationPasses(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n     const GpuCompiler::CompileOptions& options,\n-    const Compiler::TargetConfig& gpu_target_config,\n+    const Compiler::GpuTargetConfig& gpu_target_config,\n     SymbolicExprContext* symbolic_expr_context) {\n   HloPassPipeline pipeline(\"post-fusion-verification-pipeline optimization\");\n \n@@ -1332,7 +1332,7 @@ absl::Status GpuCompiler::RunCollectiveScheduleLinearizerPasses(\n // Runs optimization passes on the given HLO module.\n absl::Status GpuCompiler::OptimizeHloModule(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n-    const CompileOptions& options, const TargetConfig& gpu_target_config,\n+    const CompileOptions& options, const GpuTargetConfig& gpu_target_config,\n     const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"OptimizeHloModule\");\n   const se::DeviceDescription& device_description =\n@@ -1505,7 +1505,7 @@ void AddGemmRewriterPasses(HloPassPipeline& pipeline,\n \n absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n-    const CompileOptions& options, const TargetConfig& gpu_target_config,\n+    const CompileOptions& options, const GpuTargetConfig& gpu_target_config,\n     const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool) {\n   // Constants:\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n@@ -1787,11 +1787,12 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n \n // Returns the TargetConfig, either from the module debug options, or from the\n // CompilationOptions, or if both of those are absent, from the attached GPU.\n-/*static*/ absl::StatusOr<Compiler::TargetConfig> GpuCompiler::GetTargetConfig(\n-    const Compiler::CompileOptions& options, const DebugOptions& debug_opts,\n-    se::StreamExecutor* executor) {\n-  if (options.target_config.has_value()) {\n-    return *options.target_config;\n+/*static*/ absl::StatusOr<Compiler::GpuTargetConfig>\n+GpuCompiler::GetTargetConfig(const Compiler::CompileOptions& options,\n+                             const DebugOptions& debug_opts,\n+                             se::StreamExecutor* executor) {\n+  if (options.gpu_target_config.has_value()) {\n+    return *options.gpu_target_config;\n   }\n   if (!debug_opts.xla_gpu_target_config_filename().empty()) {\n     std::string gpu_target_config_string;\n@@ -1805,10 +1806,11 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n           \"Failed to parse GpuTargetConfigProto\");\n     }\n \n-    return Compiler::TargetConfig::FromProto(gpu_target_config_proto);\n+    return Compiler::GpuTargetConfig::FromProto(gpu_target_config_proto);\n   }\n   if (executor) {\n-    Compiler::TargetConfig target_config = Compiler::TargetConfig{executor};\n+    Compiler::GpuTargetConfig target_config =\n+        Compiler::GpuTargetConfig{executor};\n     int64_t device_memory_size =\n         target_config.device_description.device_memory_size();\n     // Checking for device_memory_size == -1 is how we detect that we are\n@@ -1838,10 +1840,10 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n \n   const DebugOptions debug_opts = module->config().debug_options();\n   TF_RETURN_IF_ERROR(LoadAutotuneResultsFromFile(debug_opts));\n-  bool is_deviceless = options.target_config.has_value() ||\n+  bool is_deviceless = options.gpu_target_config.has_value() ||\n                        !debug_opts.xla_gpu_target_config_filename().empty();\n \n-  TF_ASSIGN_OR_RETURN(TargetConfig gpu_target_config,\n+  TF_ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n                       GetTargetConfig(options, debug_opts, stream_exec));\n   const std::optional<std::string> unoptimized_fingerprint =\n       MaybeUploadUnoptimizedGpuSymbols(module.get(),\n@@ -2522,7 +2524,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   }\n \n   const DebugOptions& debug_opts = module->config().debug_options();\n-  TF_ASSIGN_OR_RETURN(TargetConfig gpu_target_config,\n+  TF_ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n                       GetTargetConfig(options, debug_opts, stream_exec));\n \n   if (DumpingEnabledForHloModule(*module)) {\n@@ -2665,7 +2667,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n     }};\n     CompileOptions compile_options;\n     compile_options.device_allocator = options.device_allocator();\n-    compile_options.target_config = options.target_config();\n+    compile_options.gpu_target_config = options.gpu_target_config();\n     TF_ASSIGN_OR_RETURN(optimized_module,\n                         RunHloPasses(std::move(hlo_module), options.executor(),\n                                      compile_options));\n@@ -2675,8 +2677,8 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n \n   std::vector<std::unique_ptr<AotCompilationResult>> results;\n \n-  const std::optional<Compiler::TargetConfig>& target_config =\n-      options.target_config();\n+  const std::optional<Compiler::GpuTargetConfig>& target_config =\n+      options.gpu_target_config();\n   CHECK(target_config.has_value() || options.executor() != nullptr);\n   const se::DeviceDescription& gpu_device_info =\n       target_config.has_value() ? target_config->device_description"
        },
        {
            "sha": "0ebb138f823918467295cee8d36977fee9d31e51",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -105,7 +105,7 @@ class GpuCompiler : public LLVMCompiler {\n \n   int64_t GetPointerSize() const { return pointer_size_; }\n \n-  static absl::StatusOr<Compiler::TargetConfig> GetTargetConfig(\n+  static absl::StatusOr<Compiler::GpuTargetConfig> GetTargetConfig(\n       const Compiler::CompileOptions& options, const DebugOptions& debug_opts,\n       se::StreamExecutor* executor);\n \n@@ -155,7 +155,7 @@ class GpuCompiler : public LLVMCompiler {\n   // thread_pool is used to speed up compilation during autotuning.\n   virtual absl::Status OptimizeHloPostLayoutAssignment(\n       HloModule* hlo_module, se::StreamExecutor* stream_exec,\n-      const CompileOptions& options, const TargetConfig& gpu_target_config,\n+      const CompileOptions& options, const GpuTargetConfig& gpu_target_config,\n       const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool);\n \n   // CollectivesScheduleLinearizer enforces a total ordering between collectives\n@@ -176,7 +176,7 @@ class GpuCompiler : public LLVMCompiler {\n       const CompileOptions& options, HloModule* hlo_module,\n       AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n       se::StreamExecutor* stream_exec,\n-      const Compiler::TargetConfig* target_config) {\n+      const Compiler::GpuTargetConfig* target_config) {\n     return absl::OkStatus();\n   }\n \n@@ -195,7 +195,7 @@ class GpuCompiler : public LLVMCompiler {\n       HloPassPipeline* pipeline, HloModule* hlo_module,\n       const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n       stream_executor::StreamExecutor* stream_executor,\n-      const Compiler::TargetConfig* target_config,\n+      const Compiler::GpuTargetConfig* target_config,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n     return absl::OkStatus();\n   }\n@@ -248,7 +248,7 @@ class GpuCompiler : public LLVMCompiler {\n   absl::Status OptimizeHloModule(HloModule* hlo_module,\n                                  se::StreamExecutor* stream_exec,\n                                  const CompileOptions& options,\n-                                 const TargetConfig& gpu_target_config,\n+                                 const GpuTargetConfig& gpu_target_config,\n                                  const GpuAliasInfo* alias_info);\n \n   virtual absl::Status OptimizeHloConvolutionCanonicalization("
        },
        {
            "sha": "033d3bf845c3ef8c81928a3b6928266a6f5cf6e2",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -2007,7 +2007,8 @@ ENTRY %main {\n   ASSERT_TRUE(tsl::Env::Default()->LocalTempFilename(&target_file));\n   TF_ASSERT_OK(tsl::WriteTextProto(\n       tsl::Env::Default(), target_file,\n-      Compiler::TargetConfig(backend().default_stream_executor()).ToProto()));\n+      Compiler::GpuTargetConfig(backend().default_stream_executor())\n+          .ToProto()));\n   debug_options.set_xla_gpu_target_config_filename(target_file);\n   config.set_debug_options(debug_options);\n \n@@ -2044,7 +2045,8 @@ TEST_F(GpuCompilerTest, CompilingAndCollectingMetadata) {\n   ASSERT_TRUE(tsl::Env::Default()->LocalTempFilename(&target_file));\n   TF_ASSERT_OK(tsl::WriteTextProto(\n       tsl::Env::Default(), target_file,\n-      Compiler::TargetConfig(backend().default_stream_executor()).ToProto()));\n+      Compiler::GpuTargetConfig(backend().default_stream_executor())\n+          .ToProto()));\n   debug_options.set_xla_gpu_target_config_filename(target_file);\n   config.set_debug_options(debug_options);\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,"
        },
        {
            "sha": "6945d21cef6bf8997f220d6d02e2784caf532843",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -264,7 +264,7 @@ absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n \n absl::Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n-    const CompileOptions& options, const TargetConfig& gpu_target_config,\n+    const CompileOptions& options, const GpuTargetConfig& gpu_target_config,\n     const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool) {\n   // This needs to run before GemmRewriter, which is part of\n   // OptimizeHloPostLayoutAssignment().\n@@ -346,7 +346,7 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n     const CompileOptions& options, HloModule* hlo_module,\n     AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n     se::StreamExecutor* stream_exec,\n-    const Compiler::TargetConfig* target_config) {\n+    const Compiler::GpuTargetConfig* target_config) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n   if (hlo_module->config()\n           .debug_options()\n@@ -422,7 +422,7 @@ absl::Status NVPTXCompiler::AddFusionAutotuningPass(\n     HloPassPipeline* pipeline, HloModule* hlo_module,\n     const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n     stream_executor::StreamExecutor* stream_executor,\n-    const Compiler::TargetConfig* target_config,\n+    const Compiler::GpuTargetConfig* target_config,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n   if (stream_executor == nullptr) {\n     return absl::OkStatus();"
        },
        {
            "sha": "36d21cc3f9dd7b3122cb771bf46a06a385388de4",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -62,7 +62,7 @@ class NVPTXCompiler : public GpuCompiler {\n \n   absl::Status OptimizeHloPostLayoutAssignment(\n       HloModule* hlo_module, se::StreamExecutor* stream_exec,\n-      const CompileOptions& options, const TargetConfig& gpu_target_config,\n+      const CompileOptions& options, const GpuTargetConfig& gpu_target_config,\n       const GpuAliasInfo* alias_info,\n       tsl::thread::ThreadPool* thread_pool) override;\n \n@@ -75,7 +75,7 @@ class NVPTXCompiler : public GpuCompiler {\n       const CompileOptions& options, HloModule* hlo_module,\n       AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n       se::StreamExecutor* stream_exec,\n-      const Compiler::TargetConfig* target_config) override;\n+      const Compiler::GpuTargetConfig* target_config) override;\n \n   absl::Status AddGemmFusionAutotuningPasses(\n       HloPassPipeline* pipeline, HloModule* hlo_module,\n@@ -89,7 +89,7 @@ class NVPTXCompiler : public GpuCompiler {\n       HloPassPipeline* pipeline, HloModule* hlo_module,\n       const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n       stream_executor::StreamExecutor* stream_executor,\n-      const Compiler::TargetConfig* target_config,\n+      const Compiler::GpuTargetConfig* target_config,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn) override;\n \n   absl::Status RunCudnnCompilerPasses(HloModule* module,"
        },
        {
            "sha": "49da257b79ff3dd9fd30e2caad4aa975570417b7",
            "filename": "third_party/xla/xla/service/local_service.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -96,7 +96,8 @@ LocalService::CompileExecutables(\n       build_options.compile_thread_pool(),\n       build_options.layout_canonicalization_callback(),\n       false,\n-      {},\n+      /*gpu_target_config=*/{},\n+      /*cpu_target_config=*/{},\n       {build_options.key_value_store(), build_options.process_index(),\n        build_options.process_count()},\n       build_options.slice_size()};"
        },
        {
            "sha": "1f202a8dc6c38dc2b5424545c0fc1b67ddebca80",
            "filename": "third_party/xla/xla/service/symbol_repository.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fsymbol_repository.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Fservice%2Fsymbol_repository.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fsymbol_repository.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -57,7 +57,7 @@ struct HloModuleAndMetadata {\n   virtual ~HloModuleAndMetadata() = default;\n \n   std::unique_ptr<HloModule> hlo_module;\n-  std::unique_ptr<Compiler::TargetConfig> target_config;\n+  std::unique_ptr<Compiler::GpuTargetConfig> target_config;\n   // Use static_cast to cast this to a concrete type.\n   std::unique_ptr<BackendSpecificData> backend_specific_data;\n };"
        },
        {
            "sha": "3cf91cb8555f1d344d55b9fefe37afea2e1ab1bf",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_opt.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -192,7 +192,7 @@ class GpuOptProvider : public CompiledOptProvider {\n       const HloModule* module) {\n     Compiler::CompileOptions opts;\n     TF_ASSIGN_OR_RETURN(\n-        Compiler::TargetConfig target_config,\n+        Compiler::GpuTargetConfig target_config,\n         gpu::GpuCompiler::GetTargetConfig(\n             opts, module->config().debug_options(), /*executor=*/nullptr));\n     return target_config.device_description;"
        },
        {
            "sha": "8d15dbb7e52690d6d5e732bda2df449d77efecd0",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2FREADME.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2FREADME.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2FREADME.md?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -1,5 +1,5 @@\n The specs in this folder are obtained by calling\n-`Compiler::TargetConfig::ToString()`, which turns the config into a\n+`Compiler::GpuTargetConfig::ToString()`, which turns the config into a\n `GpuTargetConfigProto`, and then to a `std::string`. Most of the spec is the\n device description as a proto `GpuDeviceInfoProto`.\n "
        },
        {
            "sha": "488553c2b023c5bc7e955db387f5574bf18d1ebb",
            "filename": "third_party/xla/xla/tools/xla_compile_lib.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -87,7 +87,7 @@ static absl::StatusOr<std::string> AotCompileCpuExecutable(\n \n static absl::StatusOr<std::string> CompileGpuExecutable(\n     std::unique_ptr<HloModule> hlo_module,\n-    std::optional<Compiler::TargetConfig> target_config,\n+    std::optional<Compiler::GpuTargetConfig> target_config,\n     CompilationResult& result) {\n   TF_ASSIGN_OR_RETURN(std::string platform_name,\n                       xla::PlatformUtil::CanonicalPlatformName(\"gpu\"));\n@@ -101,7 +101,7 @@ static absl::StatusOr<std::string> CompileGpuExecutable(\n \n   if (aot) {\n     AotCompilationOptions aot_options(platform->id());\n-    aot_options.set_target_config(*target_config);\n+    aot_options.set_gpu_target_config(*target_config);\n     // We need the optimized module, so we call RunHloPasses ourselves above.\n     aot_options.set_run_backend_only(true);\n \n@@ -133,7 +133,7 @@ static absl::StatusOr<std::string> CompileGpuExecutable(\n \n absl::StatusOr<std::string> CompileExecutable(\n     std::unique_ptr<HloModule> hlo_module, BackendType backend,\n-    std::optional<Compiler::TargetConfig> target_config,\n+    std::optional<Compiler::GpuTargetConfig> target_config,\n     CompilationResult& result) {\n   if (backend == BackendType::kCpu) {\n     return AotCompileCpuExecutable(std::move(hlo_module));\n@@ -218,7 +218,7 @@ ReadModuleFromSymbolRepo(absl::string_view symbol_repo,\n   return mod;\n }\n \n-static std::unique_ptr<Compiler::TargetConfig> ReadTargetConfigFromModule(\n+static std::unique_ptr<Compiler::GpuTargetConfig> ReadTargetConfigFromModule(\n     HloModuleAndMetadata* mod, BackendType backend) {\n   if (backend == BackendType::kGpu) {\n     if (auto* data = static_cast<gpu::GpuBackendSpecificData*>(\n@@ -253,7 +253,7 @@ absl::StatusOr<bool> LoadAutotuneDataFromModule(HloModuleAndMetadata* mod,\n \n absl::Status XlaCompileMain(const XlaCompileOptions& options) {\n   std::unique_ptr<HloModule> hlo_module;\n-  std::unique_ptr<Compiler::TargetConfig> target_config;\n+  std::unique_ptr<Compiler::GpuTargetConfig> target_config;\n   if (options.platform != \"cpu\" && options.platform != \"gpu\") {\n     return absl::UnimplementedError(\n         absl::StrCat(\"platform\", options.platform, \" is not supported\"));\n@@ -306,7 +306,7 @@ absl::Status XlaCompileMain(const XlaCompileOptions& options) {\n     }\n   });\n   // Run AOT compilation.\n-  std::optional<Compiler::TargetConfig> cfg = std::nullopt;\n+  std::optional<Compiler::GpuTargetConfig> cfg = std::nullopt;\n   if (backend == BackendType::kGpu) {\n     if (absl::string_view gpu_target_config_path =\n             options.gpu_options.gpu_target_config_path;\n@@ -324,9 +324,9 @@ absl::Status XlaCompileMain(const XlaCompileOptions& options) {\n       }\n \n       TF_ASSIGN_OR_RETURN(\n-          Compiler::TargetConfig parsed_target_config,\n-          Compiler::TargetConfig::FromProto(gpu_target_config_proto));\n-      target_config = std::make_unique<Compiler::TargetConfig>(\n+          Compiler::GpuTargetConfig parsed_target_config,\n+          Compiler::GpuTargetConfig::FromProto(gpu_target_config_proto));\n+      target_config = std::make_unique<Compiler::GpuTargetConfig>(\n           std::move(parsed_target_config));\n \n       if (absl::string_view autotune_results_path ="
        },
        {
            "sha": "689aa7778c8170aa92116a14bc047308a6b34fb8",
            "filename": "third_party/xla/xla/tools/xla_compile_lib.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2ec59ee45a2f72dafa6080b9e43c328864d723e4/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.h?ref=2ec59ee45a2f72dafa6080b9e43c328864d723e4",
            "patch": "@@ -40,7 +40,7 @@ namespace xla {\n // This is the expected entry point to the compilation functionality.\n absl::StatusOr<std::string> CompileExecutable(\n     std::unique_ptr<HloModule> hlo_module, BackendType backend,\n-    std::optional<Compiler::TargetConfig> target_config,\n+    std::optional<Compiler::GpuTargetConfig> target_config,\n     CompilationResult& result);\n \n // Merges the measured duration into compilation_result and writes"
        }
    ],
    "stats": {
        "total": 306,
        "additions": 165,
        "deletions": 141
    }
}