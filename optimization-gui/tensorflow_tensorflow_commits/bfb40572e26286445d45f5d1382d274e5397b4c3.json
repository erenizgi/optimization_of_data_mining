{
    "author": "loislo",
    "message": "[XLA:GPU] Enhance float check debug logging with thunk profile annotations.\n\nThe main improvement is that we now dump to the log not only the instruction name but instruction itself and if it is a fusion call we also dump to the log the corresponding computation.\n\nThis change adds a `profile_annotation` field to `BufferDebugLogEntryMetadata` and populates it with the thunk's profile annotation. The error messages for NaN/Inf detections are updated to include this additional metadata text. The `ThunkInfo` for float check thunks is also improved to include a suffix indicating it's a float check.\n\nPiperOrigin-RevId: 831357143",
    "sha": "bfb40572e26286445d45f5d1382d274e5397b4c3",
    "files": [
        {
            "sha": "903625b3b6271f81f301a4a5b4cb9047310700fe",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=bfb40572e26286445d45f5d1382d274e5397b4c3",
            "patch": "@@ -3180,6 +3180,7 @@ cc_library(\n         \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "06ea762e3c7246091df632cbe98f35ebcfaf7da7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store.h?ref=bfb40572e26286445d45f5d1382d274e5397b4c3",
            "patch": "@@ -58,6 +58,12 @@ class BufferDebugLogEntryMetadataStore {\n     // The type of check that produced this entry.\n     BufferDebugLogEntryProto::CheckType check_type;\n \n+    // Profile annotation of the HLO instruction that produced this entry.\n+    // This is used to identify the HLO instruction in HloModule that was under\n+    // the check. We need that to be able to log the HLO instruction when\n+    // a non-zero number of infs or nans were found.\n+    std::string profile_annotation;\n+\n     std::string ToString() const {\n       return absl::StrCat(\n           \"thunk_id: \", thunk_id.value(), \", buffer_idx: \", buffer_idx,"
        },
        {
            "sha": "155b72015d6938d59d5069424ac1134f232c844b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc?ref=bfb40572e26286445d45f5d1382d274e5397b4c3",
            "patch": "@@ -55,8 +55,8 @@ absl::Status BuffersDebugFloatCheckThunk::Initialize(\n            .IsAtLeastPascal()) {\n     VLOG(1)\n         << \"Buffer float checking not supported on CUDA architectures older \"\n-           \"than \"\n-           \"Pascal due to missing atomic fetch_add with system scope, skipping\";\n+           \"than Pascal due to missing atomic fetch_add with system scope, \"\n+           \"skipping\";\n     return absl::OkStatus();\n   }\n \n@@ -114,11 +114,12 @@ absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n \n   for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {\n     BufferDebugLogEntryMetadataStore::Metadata metadata{\n-        checked_thunk_id_,\n+        checked_thunk_info_.thunk_id,\n         buffer_idx,\n         execution_id,\n-        /*is_input=*/runs_before_checked_thunk_,\n+        /*is_input=*/false,\n         BufferDebugLogEntryProto::CHECK_TYPE_FLOAT_CHECKS,\n+        checked_thunk_info_.profile_annotation,\n     };\n     const BufferDebugLogEntryId entry_id = metadata_store_->AssignId(metadata);\n "
        },
        {
            "sha": "5d2f78e80edb99236c9c15d0ef3a3c9983cff9c8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.h",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h?ref=bfb40572e26286445d45f5d1382d274e5397b4c3",
            "patch": "@@ -19,34 +19,33 @@ limitations under the License.\n #include <atomic>\n #include <cstddef>\n #include <memory>\n-#include <optional>\n #include <string>\n #include <utility>\n \n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_float_check_kernel.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla::gpu {\n \n class BuffersDebugFloatCheckThunk : public Thunk {\n  public:\n   explicit BuffersDebugFloatCheckThunk(\n-      ThunkInfo info, BufferAllocation::Slice log_slice,\n-      ThunkId checked_thunk_id,\n+      ThunkInfo info, const ThunkInfo& checked_thunk_info,\n+      BufferAllocation::Slice log_slice,\n       absl::flat_hash_map<size_t, BufferAllocation::Slice>\n           checked_thunk_buffers,\n-      bool runs_before_checked_thunk,\n       std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store)\n       : Thunk(Thunk::Kind::kBuffersDebugFloatCheck, std::move(info)),\n         log_slice_(log_slice),\n-        checked_thunk_id_(checked_thunk_id),\n+        checked_thunk_info_(checked_thunk_info),\n         checked_thunk_buffers_(std::move(checked_thunk_buffers)),\n-        runs_before_checked_thunk_(runs_before_checked_thunk),\n         metadata_store_(std::move(metadata_store)) {}\n \n   absl::Status Initialize(const InitializeParams& params) override;\n@@ -80,9 +79,8 @@ class BuffersDebugFloatCheckThunk : public Thunk {\n       kernels_ ABSL_GUARDED_BY(kernels_mutex_);\n \n   BufferAllocation::Slice log_slice_;\n-  ThunkId checked_thunk_id_;\n+  ThunkInfo checked_thunk_info_;\n   absl::flat_hash_map<size_t, BufferAllocation::Slice> checked_thunk_buffers_;\n-  bool runs_before_checked_thunk_;\n   std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store_;\n   std::atomic<size_t> execution_count_ = 0;\n };"
        },
        {
            "sha": "d6b566e6006c0f6ade6217d111c9da1bc3b00951",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc?ref=bfb40572e26286445d45f5d1382d274e5397b4c3",
            "patch": "@@ -158,11 +158,12 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n       /*collective_params=*/nullptr, /*collective_cliques=*/nullptr);\n   auto metadata_store = std::make_shared<BufferDebugLogEntryMetadataStore>();\n \n+  Thunk::ThunkInfo checked_thunk_info;\n+  checked_thunk_info.thunk_id = ThunkId(123);\n   BuffersDebugFloatCheckThunk thunk(\n-      Thunk::ThunkInfo(), log_slice,\n-      /*checked_thunk_id=*/ThunkId(123),\n+      Thunk::ThunkInfo(), checked_thunk_info, log_slice,\n       {{/*buffer_idx=*/0, inputs[0]}, {/*buffer_idx=*/1, inputs[1]}},\n-      /*runs_before_checked_thunk=*/true, metadata_store);\n+      metadata_store);\n   TF_ASSERT_OK(thunk.Initialize(init_params));\n   TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}, resource_requests));\n   TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));\n@@ -179,7 +180,7 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n                           /*thunk_id=*/ThunkId(123),\n                           /*buffer_idx=*/0,\n                           /*execution_id=*/0,\n-                          /*is_input=*/true,\n+                          /*is_input=*/false,\n                           BufferDebugLogEntryProto::CHECK_TYPE_FLOAT_CHECKS,\n                       }),\n                   IsEntryWithMetadata(\n@@ -188,7 +189,7 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n                           /*thunk_id=*/ThunkId(123),\n                           /*buffer_idx=*/1,\n                           /*execution_id=*/0,\n-                          /*is_input=*/true,\n+                          /*is_input=*/false,\n                           BufferDebugLogEntryProto::CHECK_TYPE_FLOAT_CHECKS,\n                       })));\n }\n@@ -234,11 +235,11 @@ TEST_F(BuffersDebugFloatCheckThunkTest,\n                                     PrimitiveType::F32);\n   BufferAllocation::Slice bf16_slice(&allocation, kLogSizeBytes,\n                                      kInputSizeBytes, PrimitiveType::BF16);\n+  Thunk::ThunkInfo checked_thunk_info;\n+  checked_thunk_info.thunk_id = ThunkId(123);\n   BuffersDebugFloatCheckThunk thunk(\n-      Thunk::ThunkInfo(), log_slice,\n-      /*checked_thunk_id=*/ThunkId(123),\n+      Thunk::ThunkInfo(), checked_thunk_info, log_slice,\n       {{/*buffer_idx=*/0, f32_slice}, {/*buffer_idx=*/1, bf16_slice}},\n-      /*runs_before_checked_thunk=*/true,\n       std::make_shared<BufferDebugLogEntryMetadataStore>());\n \n   // Initialize the Thunk on both devices and run the kernel. An attempt to run"
        },
        {
            "sha": "32d51fd9b21d284fa66b59ffb7c8d95b2aec8689",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_float_check.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 32,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bfb40572e26286445d45f5d1382d274e5397b4c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc?ref=bfb40572e26286445d45f5d1382d274e5397b4c3",
            "patch": "@@ -43,8 +43,12 @@ limitations under the License.\n #include \"xla/ffi/api/c_api.h\"\n #include \"xla/ffi/attribute_map.h\"\n #include \"xla/ffi/ffi.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n@@ -57,7 +61,7 @@ namespace xla::gpu {\n \n namespace se = stream_executor;\n \n-// With BufferDebugFloatCheckEntry size of 8 bytes, this is enough to hold ~8K\n+// With BufferDebugFloatCheckEntry size of 16 bytes, this is enough to hold ~4K\n // entries.\n constexpr size_t kLogSizeBytes = 64 * 1024;\n \n@@ -119,9 +123,8 @@ std::unique_ptr<Thunk> WrapWithFloatCheckThunk(\n   thunk_and_checks.push_back(std::move(thunk));\n   auto buffer_debug_float_check_thunk =\n       std::make_unique<BuffersDebugFloatCheckThunk>(\n-          Thunk::ThunkInfo(), log_slice, thunk_ptr->thunk_info().thunk_id,\n-          std::move(buffers_to_check),\n-          /*runs_before_checked_thunk=*/false, std::move(metadata_store));\n+          Thunk::ThunkInfo(), thunk_ptr->thunk_info(), log_slice,\n+          std::move(buffers_to_check), std::move(metadata_store));\n   buffer_debug_float_check_thunk->add_control_predecessor(thunk_ptr);\n   thunk_and_checks.push_back(std::move(buffer_debug_float_check_thunk));\n   auto wrapped_thunk = std::make_unique<SequentialThunk>(\n@@ -131,11 +134,26 @@ std::unique_ptr<Thunk> WrapWithFloatCheckThunk(\n   return wrapped_thunk;\n }\n \n-// Saves the contents of the BufferDebugLog stored in `log_buffer` to a file..\n-//\n-// `metadata_store` is used to retrieve the metadata for the log entries.\n-// The filename is derived from the HLO module name and the log dump path\n-// configured in `debug_options`.\n+void LogHloInstructionWithId(const HloModule* hlo_module,\n+                             const std::string& id) {\n+  for (const HloComputation* computation : hlo_module->computations()) {\n+    for (const HloInstruction* instruction : computation->instructions()) {\n+      if (instruction->name() == id) {\n+        LOG(ERROR) << \"HLO instruction with id \" << id << \":\\n\\n\"\n+                   << instruction->ToString() << \"\\n\\n\";\n+        if (instruction->opcode() == HloOpcode::kFusion) {\n+          auto fusion = xla::Cast<HloFusionInstruction>(instruction);\n+          LOG(ERROR) << \"HLO fusion instruction computation:\\n\\n\"\n+                     << fusion->fused_instructions_computation()->ToString()\n+                     << \"\\n\\n\";\n+        }\n+        return;\n+      }\n+    }\n+  }\n+  LOG(ERROR) << \"HLO instruction with id \" << id << \" was not found\";\n+}\n+\n absl::Status BufferDebugFloatCheck(\n     std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store,\n     se::Stream* stream, const HloComputation* absl_nonnull hlo_computation,\n@@ -165,7 +183,7 @@ absl::Status BufferDebugFloatCheck(\n \n   VLOG(1) << \"read \" << entries.size() << \" entries\";\n   auto entries_metadata = metadata_store->GetEntryMetadataBatch(entry_ids);\n-  int non_zero_float_check_modules_count = 0;\n+  int non_zero_nan_check_modules_count = 0;\n   int non_zero_inf_check_modules_count = 0;\n   CHECK_EQ(entries.size(), entries_metadata.size());\n \n@@ -177,37 +195,41 @@ absl::Status BufferDebugFloatCheck(\n                    << \" for float check not found in metadata\";\n       continue;\n     }\n-    if (metadata->check_type ==\n+    if (metadata->check_type !=\n         BufferDebugLogEntryProto::CHECK_TYPE_FLOAT_CHECKS) {\n-      if (nan_check_enabled && entry.nan_count > 0) {\n-        LOG(ERROR) << \"Found entry with non zero float check count \"\n-                   << entry.nan_count << \" for thunk \" << entry.entry_id\n-                   << \" and execution \" << metadata->execution_id\n-                   << \" for module: \\n\"\n-                   << hlo_module->ToString();\n-        non_zero_float_check_modules_count++;\n-      }\n-      if (inf_check_enabled && entry.inf_count > 0) {\n-        LOG(ERROR) << \"Found entry with non zero inf check count \"\n-                   << entry.inf_count << \" for thunk \" << entry.entry_id\n-                   << \" and execution \" << metadata->execution_id\n-                   << \" for module: \\n\"\n-                   << hlo_module->ToString();\n-        non_zero_inf_check_modules_count++;\n-      }\n+      continue;\n+    }\n+    if (nan_check_enabled && entry.nan_count > 0) {\n+      LOG(ERROR) << \"Found entry with non zero nan count \" << entry.nan_count\n+                 << \" for thunk \" << entry.entry_id << \" and execution \"\n+                 << \"with metadata: \" << metadata->profile_annotation;\n+      non_zero_nan_check_modules_count++;\n+      LogHloInstructionWithId(hlo_module, metadata->profile_annotation);\n+    }\n+    if (inf_check_enabled && entry.inf_count > 0) {\n+      LOG(ERROR) << \"Found entry with non zero inf count \" << entry.inf_count\n+                 << \" for thunk \" << entry.entry_id << \" and execution \"\n+                 << metadata->execution_id\n+                 << \"with metadata: \" << metadata->profile_annotation;\n+      non_zero_inf_check_modules_count++;\n+      LogHloInstructionWithId(hlo_module, metadata->profile_annotation);\n     }\n   }\n-  if (non_zero_float_check_modules_count > 0 &&\n+  if (non_zero_nan_check_modules_count > 0 &&\n       hlo_module->config().debug_options().xla_gpu_detect_nan() ==\n           DebugOptions::DETECTION_MODE_FAIL) {\n-    LOG(FATAL) << \"Found \" << non_zero_float_check_modules_count\n-               << \" modules with non zero float check count\";\n+    LOG(FATAL) << \"Crash execution as requested by the xla_gpu_detect_nan flag \"\n+                  \"because \"\n+               << non_zero_nan_check_modules_count\n+               << \" NaN values were found in buffers.\";\n   }\n   if (non_zero_inf_check_modules_count > 0 &&\n       hlo_module->config().debug_options().xla_gpu_detect_inf() ==\n           DebugOptions::DETECTION_MODE_FAIL) {\n-    LOG(FATAL) << \"Found \" << non_zero_float_check_modules_count\n-               << \" modules with non zero inf check count\";\n+    LOG(FATAL) << \"Crash execution as requested by the xla_gpu_detect_inf flag \"\n+                  \"because \"\n+               << non_zero_inf_check_modules_count\n+               << \" infinite values were found in buffers.\";\n   }\n   return absl::OkStatus();\n }"
        }
    ],
    "stats": {
        "total": 135,
        "additions": 82,
        "deletions": 53
    }
}