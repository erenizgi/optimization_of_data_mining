{
    "author": "pschuh",
    "message": "Delete unused PjRtStreamExecutorBuffer::ScopedHold related code.\n\nPiperOrigin-RevId: 822779025",
    "sha": "cf3e49ba23c56bfaf76880f357c76e4f0a5f364a",
    "files": [
        {
            "sha": "cd399accb6c814eb5eb0449d48cbe71ebf3b6b71",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 536,
            "changes": 536,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=cf3e49ba23c56bfaf76880f357c76e4f0a5f364a",
            "patch": "@@ -162,378 +162,6 @@ absl::Status RunCallbackOnStream(se::Stream* stream,\n       });\n }\n \n-class GpuAsyncHostToDeviceTransferManager\n-    : public xla::PjRtClient::AsyncHostToDeviceTransferManager {\n- public:\n-  static absl::StatusOr<std::unique_ptr<AsyncHostToDeviceTransferManager>>\n-  Create(absl::Span<const PjRtClient::ShapeSpec> shape_specs,\n-         std::optional<absl::Span<const std::optional<Layout>>> device_layouts,\n-         PjRtStreamExecutorDevice* device, PjRtStreamExecutorClient* client,\n-         PjRtMemorySpace* memory_space) {\n-    if (device_layouts.has_value() &&\n-        device_layouts->size() != shape_specs.size()) {\n-      return InvalidArgument(\n-          \"Number of layouts %d does not match the number of shapes %d\",\n-          device_layouts->size(), shape_specs.size());\n-    }\n-    absl::InlinedVector<std::unique_ptr<PjRtBuffer>, 4> buffers;\n-    absl::InlinedVector<tsl::RCReference<RawSEDeviceMemory>, 4> buffer_ptrs;\n-    absl::InlinedVector<BufferSequencingEventRef, 4> definition_events;\n-    absl::InlinedVector<Shape, 4> device_shapes;\n-    buffers.reserve(shape_specs.size());\n-    buffer_ptrs.reserve(shape_specs.size());\n-    definition_events.reserve(shape_specs.size());\n-    device_shapes.reserve(shape_specs.size());\n-    for (int i = 0; i < shape_specs.size(); ++i) {\n-      const PjRtClient::ShapeSpec& shape_spec = shape_specs[i];\n-      if (shape_spec.element_type == TUPLE) {\n-        return Unimplemented(\n-            \"Async buffer transfer of tuples not implemented.\");\n-      }\n-      // Initialize a definition event for each async buffer. The definition\n-      // event will block the buffer usage until the transfer is done.\n-      definition_events.push_back(\n-          BufferSequencingEvent::Create(client->thread_pool()));\n-      Shape& device_shape = device_shapes.emplace_back(\n-          ShapeUtil::MakeShape(shape_spec.element_type, shape_spec.dims));\n-      if (device_layouts.has_value() && (*device_layouts)[i].has_value()) {\n-        *device_shape.mutable_layout() = *(*device_layouts)[i];\n-      } else {\n-        TF_ASSIGN_OR_RETURN(device_shape,\n-                            client->client()\n-                                ->backend()\n-                                .transfer_manager()\n-                                ->ChooseCompactLayoutForShape(device_shape));\n-      }\n-      LocalDeviceState* local_device = device->local_device_state();\n-      se::Stream* h2d_stream = local_device->host_to_device_stream();\n-      TF_ASSIGN_OR_RETURN(auto buffer,\n-                          AllocateDestinationBuffer(\n-                              device_shape, device, local_device, h2d_stream,\n-                              /*is_uninitialized_create=*/true, client,\n-                              definition_events.back(), memory_space));\n-      // Get a temporary hold just so we can fish out a shared_ptr to the\n-      // TrackedDeviceBuffer. It's ok to drop the hold before return the\n-      // buffers, because the invariants of this class ensure that the buffer\n-      // definition event will not fire until after all of this class' uses of\n-      // the TrackedDeviceBuffer have completed.\n-      auto* se_buffer =\n-          tensorflow::down_cast<PjRtStreamExecutorBuffer*>(buffer.get());\n-      DCHECK(se_buffer);\n-      auto hold = se_buffer->GetBufferWithUsageHold();\n-      buffer_ptrs.push_back(hold->device_memory());\n-      buffers.push_back(std::move(buffer));\n-    }\n-\n-    return std::make_unique<GpuAsyncHostToDeviceTransferManager>(\n-        std::move(buffers), std::move(buffer_ptrs),\n-        std::move(definition_events), std::move(device_shapes), device);\n-  }\n-\n-  GpuAsyncHostToDeviceTransferManager(\n-      absl::InlinedVector<std::unique_ptr<PjRtBuffer>, 4> buffers,\n-      absl::InlinedVector<tsl::RCReference<RawSEDeviceMemory>, 4> buffer_ptrs,\n-      absl::InlinedVector<BufferSequencingEventRef, 4> definition_events,\n-      absl::InlinedVector<Shape, 4> device_shapes,\n-      PjRtStreamExecutorDevice* device)\n-      : buffers_(std::move(buffers)),\n-        buffer_ptrs_(std::move(buffer_ptrs)),\n-        definition_events_(std::move(definition_events)),\n-        device_shapes_(std::move(device_shapes)),\n-        remaining_buffer_count_(buffer_ptrs_.size()),\n-        transfers_in_flight_(0),\n-        device_(device) {\n-    buffer_sizes_.reserve(buffer_ptrs_.size());\n-    for (const auto& ptr : buffer_ptrs_) {\n-      DCHECK(ptr);\n-      buffer_sizes_.push_back(ptr->mem().size());\n-    }\n-    last_transfer_started_.resize(buffer_ptrs_.size(), false);\n-  }\n-\n-  ~GpuAsyncHostToDeviceTransferManager() override {\n-    auto transfers_finished = [this]() {\n-      mu_.AssertHeld();\n-      return transfers_in_flight_ == 0;\n-    };\n-    {\n-      absl::MutexLock l(mu_);\n-      // Make sure we don't leave dangling pointers in cleanup routines even\n-      // if the client lets the object go out of scope.\n-      mu_.Await(absl::Condition(&transfers_finished));\n-    }\n-  }\n-\n-  size_t buffer_count() const override { return buffers_.size(); };\n-\n-  size_t buffer_size(int buffer_index) const override {\n-    DCHECK_LT(buffer_index, buffer_sizes_.size());\n-    return buffer_sizes_[buffer_index];\n-  }\n-\n-  PjRtDevice* device() const override { return device_; }\n-\n-  std::unique_ptr<PjRtBuffer> RetrieveBuffer(int buffer_index) override {\n-    DCHECK_LT(buffer_index, buffers_.size());\n-    return std::move(buffers_[buffer_index]);\n-  };\n-\n-  absl::Status TransferLiteralToBuffer(\n-      int buffer_index, const LiteralSlice& literal,\n-      absl::AnyInvocable<void() &&> on_done) override {\n-    tsl::profiler::TraceMe traceme(\n-        \"GpuAsyncHostToDeviceTransferManager::TransferLiteralToBuffer\");\n-    auto* stream = device_->local_device_state()->host_to_device_stream();\n-    auto* se_client =\n-        tensorflow::down_cast<PjRtStreamExecutorClient*>(device_->client());\n-    DCHECK(se_client);\n-\n-    TransferManager* transfer_manager =\n-        se_client->client()->backend().transfer_manager();\n-\n-    tsl::RCReference<RawSEDeviceMemory> buffer;\n-    {\n-      absl::MutexLock l(mu_);\n-\n-      DCHECK_LT(buffer_index, buffer_ptrs_.size());\n-      if (last_transfer_started_[buffer_index]) {\n-        return InvalidArgument(\n-            \"TransferLiteralToBuffer requested for buffer index %d which has \"\n-            \"already been fully transferred\",\n-            buffer_index);\n-      }\n-      last_transfer_started_[buffer_index] = true;\n-      buffer = buffer_ptrs_[buffer_index];\n-      DCHECK(buffer);\n-      ++transfers_in_flight_;\n-    }\n-\n-    // The host to device transfer is performed on a thread pool, mostly because\n-    // it includes linearization that may be slow.\n-    // TODO(misard) assess if it would be preferable to introduce a heuristic to\n-    // put the transfer into the calling thread for small literals.\n-    auto transfer_h2d = [this, se_client, buffer_index, stream,\n-                         transfer_manager, literal, device = device_,\n-                         device_buffer = buffer,\n-                         local_device =\n-                             std::move(device_->local_device_state()),\n-                         on_done = std::move(on_done)]() mutable {\n-      tsl::profiler::TraceMe traceme(\n-          \"GpuAsyncHostToDeviceTransferManager::TransferLiteralToBuffer::\"\n-          \"transfer_\"\n-          \"h2d\");\n-\n-      auto event = local_device->event_pool().AllocateEvent(stream->parent());\n-\n-      // Initiate linearization and transfer of the buffer on the stream.\n-      ShapedBuffer buffer =\n-          device_buffer->AsShapedBuffer(device, device_shapes_[buffer_index]);\n-      TF_CHECK_OK(transfer_manager->TransferLiteralToDeviceAsync(\n-          stream, literal, buffer));\n-      local_device->event_pool().ThenRecordEvent(stream, event.value());\n-\n-      // Call cleanup once the transfer has finished on the stream.\n-      auto cleanup = [this, buffer_index, stream, on_done = std::move(on_done),\n-                      event = std::move(event).value()]() mutable {\n-        CleanUp(buffer_index, std::move(event), stream,\n-                /*is_last_transfer=*/true, std::move(on_done));\n-      };\n-      auto status = RunCallbackOnStream(stream, se_client->thread_pool(),\n-                                        std::move(cleanup));\n-      if (!status.ok()) {\n-        LOG(ERROR) << \"DoHostCallback failed: \" << status;\n-      }\n-    };\n-    se_client->thread_pool()->Schedule(\n-        ([ptr = new absl::AnyInvocable<void()>(std::move(transfer_h2d))]() {\n-          (*ptr)();\n-          delete ptr;\n-        }));\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status TransferRawDataToBuffer(\n-      int buffer_index, absl::string_view data,\n-      absl::AnyInvocable<void() &&> on_done) override {\n-    return TransferRawDataToSubBuffer(buffer_index, data.data(),\n-                                      /*offset=*/0, data.size(),\n-                                      /*is_last_transfer=*/true,\n-                                      std::move(on_done));\n-  }\n-\n-  absl::Status TransferRawDataToSubBuffer(\n-      int buffer_index, const void* data, int64_t offset, int64_t transfer_size,\n-      bool is_last_transfer, absl::AnyInvocable<void() &&> on_done) override {\n-    auto* stream = device_->local_device_state()->host_to_device_stream();\n-\n-    auto* client =\n-        tensorflow::down_cast<PjRtStreamExecutorClient*>(device_->client());\n-    bool should_stage_host_to_device_transfers =\n-        client->should_stage_host_to_device_transfers() &&\n-        (!client->IsDmaMapped(data, transfer_size));\n-\n-    std::shared_ptr<void> staging_buffer;\n-    if (should_stage_host_to_device_transfers) {\n-      auto* host_memory_allocator = client->host_memory_allocator();\n-      if (host_memory_allocator == nullptr) {\n-        return InvalidArgument(\n-            \"host_memory_allocator should be initialized for staging buffer \"\n-            \"transfer.\");\n-      }\n-\n-      void* ptr = host_memory_allocator->AllocateRaw(\n-          tsl::Allocator::kAllocatorAlignment, transfer_size);\n-      staging_buffer = std::shared_ptr<void>(\n-          ptr, [host_memory_allocator = host_memory_allocator](void* ptr) {\n-            host_memory_allocator->DeallocateRaw(ptr);\n-          });\n-    }\n-\n-    absl::ReleasableMutexLock l(mu_);\n-    DCHECK_LT(buffer_index, buffer_ptrs_.size());\n-    if (last_transfer_started_[buffer_index]) {\n-      return InvalidArgument(\n-          \"TransferRawData requested for buffer index %d which has \"\n-          \"already been fully transferred\",\n-          buffer_index);\n-    }\n-    if (is_last_transfer) {\n-      last_transfer_started_[buffer_index] = true;\n-    }\n-    DCHECK(buffer_ptrs_[buffer_index]);\n-    auto& buffer_memory = buffer_ptrs_[buffer_index]->mem();\n-    se::DeviceMemoryBase sub_buffer;\n-    CHECK_LE(offset, buffer_memory.size());\n-    CHECK_LE(transfer_size, buffer_memory.size() - offset);\n-    if (transfer_size < buffer_memory.size()) {\n-      sub_buffer = buffer_memory.GetByteSlice(offset, transfer_size);\n-    } else {\n-      sub_buffer = buffer_memory;\n-    }\n-\n-    ++transfers_in_flight_;\n-    // Release the lock before transfer in case transfer or cleanup could be\n-    // called on this thread, to avoid deadlock.\n-    l.Release();\n-\n-    auto event = device_->local_device_state()->event_pool().AllocateEvent(\n-        stream->parent());\n-\n-    if (transfer_size != 0) {\n-      if (staging_buffer != nullptr) {\n-        auto copy_to_staging_buffer = [data, transfer_size,\n-                                       staging_buffer]() mutable {\n-          std::memcpy(staging_buffer.get(), data, transfer_size);\n-        };\n-        if (auto status =\n-                stream->DoHostCallback(std::move(copy_to_staging_buffer));\n-            !status.ok()) {\n-          return status;\n-        }\n-        if (auto status = stream->Memcpy(&sub_buffer, staging_buffer.get(),\n-                                         transfer_size);\n-            !status.ok()) {\n-          return status;\n-        }\n-      } else if (auto status = stream->Memcpy(&sub_buffer, data, transfer_size);\n-                 !status.ok()) {\n-        return status;\n-      }\n-    }\n-    device_->local_device_state()->event_pool().ThenRecordEvent(stream,\n-                                                                event.value());\n-\n-    auto cleanup = [this, buffer_index, event = std::move(event).value(),\n-                    stream, is_last_transfer, on_done = std::move(on_done),\n-                    staging_buffer = std::move(staging_buffer)]() mutable {\n-      CleanUp(buffer_index, std::move(event), stream, is_last_transfer,\n-              std::move(on_done));\n-    };\n-    return RunCallbackOnStream(stream, client->thread_pool(),\n-                               std::move(cleanup));\n-  }\n-\n-  void SetBufferError(int buffer_index, absl::Status error) override {\n-    BufferSequencingEventRef event;\n-    {\n-      absl::MutexLock l(mu_);\n-      // For a given buffer_index, SetBufferError can't be called twice, or\n-      // called after the last transfer has been enqueued.\n-      event = std::move(definition_events_[buffer_index]);\n-      CHECK(event);\n-      CHECK(!event->IsDefined());\n-    }\n-    VLOG(1) << \"SetBufferError sets the \" << buffer_index\n-            << \"th buffer error: \" << error;\n-    tensorflow::down_cast<PjRtStreamExecutorClient*>(device_->client())\n-        ->SetEventAsError(event, error);\n-  }\n-\n-  void AddTransferMetadata(const TransferMetadata& meta) override {}\n-\n- private:\n-  absl::Mutex mu_;\n-  // The newly created buffers, which will be returned to the caller via\n-  // Retrieve.\n-  absl::InlinedVector<std::unique_ptr<PjRtBuffer>, 4> buffers_;\n-  // Cached versions of the sizes of all the buffers, so we can return them\n-  // without acquiring mu_.\n-  absl::InlinedVector<size_t, 4> buffer_sizes_;\n-  // References to the underlying storage for all the buffers, which ensures\n-  // that the buffers can't be freed before all transfers complete.\n-  absl::InlinedVector<tsl::RCReference<RawSEDeviceMemory>, 4> buffer_ptrs_\n-      ABSL_GUARDED_BY(mu_);\n-  // True if the last transfer for a buffer has been initiated. Used to prevent\n-  // a client initiating another transfer after the last transfer has already\n-  // been initiated.\n-  absl::InlinedVector<bool, 4> last_transfer_started_ ABSL_GUARDED_BY(mu_);\n-  // The buffer definition events on all the buffers, unblocked once the\n-  // corresponding buffer transfer has completed.\n-  absl::InlinedVector<BufferSequencingEventRef, 4> definition_events_\n-      ABSL_GUARDED_BY(mu_);\n-  // Device shapes for all buffers with either compact or custom layout.\n-  const absl::InlinedVector<Shape, 4> device_shapes_;\n-  // Count of buffers that have not yet been fully transferred.\n-  size_t remaining_buffer_count_ ABSL_GUARDED_BY(mu_);\n-  // Count of transfers that have been started but have not yet called cleanup.\n-  // Used to block in the destructor to avoid dangling pointers in cleanup.\n-  int transfers_in_flight_ ABSL_GUARDED_BY(mu_);\n-\n-  PjRtStreamExecutorDevice* device_;  // not owned.\n-\n-  void CleanUp(int buffer_index, EventPool::Handle device_event,\n-               se::Stream* stream, bool is_last_transfer,\n-               absl::AnyInvocable<void() &&> on_done) {\n-    BufferSequencingEventRef event;\n-    {\n-      absl::MutexLock l(mu_);\n-\n-      CHECK_GT(transfers_in_flight_, 0);\n-      --transfers_in_flight_;\n-      if (is_last_transfer) {\n-        // Drop our reference to the TrackedDeviceBuffer for this buffer.\n-        CHECK(buffer_ptrs_[buffer_index]);\n-        buffer_ptrs_[buffer_index] = tsl::RCReference<xla::RawSEDeviceMemory>();\n-        CHECK_GT(remaining_buffer_count_, 0);\n-        --remaining_buffer_count_;\n-        definition_events_[buffer_index]->SetSequencingEvent(\n-            std::move(device_event), stream);\n-        event = std::move(definition_events_[buffer_index]);\n-        if (remaining_buffer_count_ == 0) {\n-          VLOG(1) << \"TransferLiteralToBuffer for all buffers is done.\";\n-        }\n-      }\n-    }\n-\n-    // Call on_done after finishing all housekeeping and releasing the lock.\n-    std::move(on_done)();\n-    // CleanUp happens after the events have already been waited on.\n-    if (event) {\n-      event.SetStateConcrete();\n-    }\n-  }\n-};\n-\n static std::optional<stream_executor::GpuTargetConfigProto>\n GetTargetConfigForDevices(absl::Span<PjRtDevice* const> devices) {\n   // Temporary ability to disable TargetConfig via env var until\n@@ -714,170 +342,6 @@ StreamExecutorGpuClient::GetDefaultDeviceAssignment(int num_replicas,\n                                                               num_partitions);\n }\n \n-Future<> StreamExecutorGpuClient::CopyRawSubBufferToHost(\n-    PjRtBuffer* pjrt_buffer, Future<void*> dst, int64_t offset,\n-    int64_t transfer_size) {\n-  auto* buffer = tensorflow::down_cast<PjRtStreamExecutorBuffer*>(pjrt_buffer);\n-  DCHECK(buffer);\n-  auto* device =\n-      tensorflow::down_cast<PjRtStreamExecutorDevice*>(buffer->device());\n-  LocalDeviceState* local_device = device->local_device_state();\n-  se::Stream* stream = local_device->GetDeviceToHostStream();\n-\n-  // Acquire the usage hold inline so that the buffer is kept alive even if\n-  // `dst` is not immediately available.\n-  PjRtStreamExecutorBuffer::ScopedHold hold(buffer->GetBufferWithUsageHold());\n-  if (!hold.ok()) {\n-    return Future<>(hold.status());\n-  }\n-\n-  auto device_memory = hold->device_memory();\n-  if (!device_memory) {\n-    return Future<>(\n-        InvalidArgument(\"Copy raw buffer called on an invalid buffer\"));\n-  }\n-\n-  auto [promise, future] = Future<>::MakePromise();\n-  auto usage_event = BufferSequencingEvent::Create(this->thread_pool());\n-\n-  auto definition_events = hold->definition_events();\n-  auto first_definition_event = definition_events[0];\n-\n-  // When using the ComputeSynchronized allocation model, retain a reference to\n-  // the device_buffer until the copy completes, to ensure that the buffer isn't\n-  // deleted or donated while it is still in use. The choice of retaining a\n-  // reference at the host is a heuristic; the alternative is to ensure, before\n-  // freeing the buffer, that the compute stream is synchronized past the\n-  // transfer, but it seems better to hold onto the buffer too long than to\n-  // stall the compute stream.\n-  hold.ConvertUsageHold(stream, usage_event, /*reference_held=*/true);\n-\n-  auto async_copy = [this, promise = std::move(promise).ToShared(), offset,\n-                     transfer_size, stream, local_device,\n-                     owning_device_memory = std::move(device_memory),\n-                     definition_events = std::move(definition_events),\n-                     usage_event = std::move(usage_event)](\n-                        absl::StatusOr<void*> dst) mutable {\n-    absl::StatusOr<EventPool::Handle> event =\n-        local_device->event_pool().AllocateEvent(stream->parent());\n-    if (!event.ok()) {\n-      promise->Set(event.status());\n-      return;\n-    }\n-\n-    absl::Status defined_status = definition_events[0]->GetDefinedStatus();\n-    if (!defined_status.ok()) {\n-      promise->Set(defined_status);\n-      return;\n-    }\n-\n-    auto& device_memory = owning_device_memory->mem();\n-    if (offset < 0 || offset > device_memory.size() ||\n-        device_memory.size() - offset < transfer_size) {\n-      promise->Set(\n-          InvalidArgument(\"Copy raw buffer called on buffer size %lld with \"\n-                          \"invalid offset %lld, transfer size %lld\",\n-                          device_memory.size(), offset, transfer_size));\n-      return;\n-    }\n-\n-    std::unique_ptr<se::DeviceMemoryBase> sub_buffer;\n-    if (transfer_size < device_memory.size()) {\n-      sub_buffer = std::make_unique<se::DeviceMemoryBase>(\n-          device_memory.GetByteSlice(offset, transfer_size));\n-    } else {\n-      sub_buffer = std::make_unique<se::DeviceMemoryBase>(device_memory);\n-    }\n-\n-    WaitForBufferDefinitionEventsOnStream(absl::MakeSpan(definition_events),\n-                                          stream);\n-\n-    if (transfer_size != 0) {\n-      if (should_stage_host_to_device_transfers() &&\n-          !IsDmaMapped(dst.value(), transfer_size)) {\n-        if (host_memory_allocator() == nullptr) {\n-          promise->Set(\n-              InvalidArgument(\"host_memory_allocator should be initialized for \"\n-                              \"staging buffer transfer.\"));\n-          return;\n-        }\n-        void* ptr = host_memory_allocator()->AllocateRaw(\n-            tsl::Allocator::kAllocatorAlignment, transfer_size);\n-\n-        std::shared_ptr<void> staging_buffer = std::shared_ptr<void>(\n-            ptr, [host_memory_allocator = host_memory_allocator()](void* ptr) {\n-              host_memory_allocator->DeallocateRaw(ptr);\n-            });\n-        if (auto status = stream->Memcpy(staging_buffer.get(), *sub_buffer,\n-                                         transfer_size);\n-            !status.ok()) {\n-          promise->Set(std::move(status));\n-          return;\n-        }\n-        auto copy_to_staging_buffer = [dst, transfer_size,\n-                                       staging_buffer]() mutable {\n-          std::memcpy(*dst, staging_buffer.get(), transfer_size);\n-        };\n-        if (auto status = stream->DoHostCallback(copy_to_staging_buffer);\n-            !status.ok()) {\n-          promise->Set(std::move(status));\n-          return;\n-        }\n-      } else {\n-        // D2H request holds a non-owned pointer into sub_buffer base address\n-        // that needs to outlive the transfer until the stream callback is\n-        // invoked.\n-        auto status = stream->Memcpy(*dst, *sub_buffer, transfer_size);\n-        if (!status.ok()) {\n-          promise->Set(std::move(status));\n-          return;\n-        }\n-      }\n-    }\n-\n-    ThenRecordEvent(usage_event, local_device, std::move(event).value(),\n-                    stream);\n-\n-    auto callback_status = local_device->ThenExecuteCallback(\n-        stream, [promise, owning_device_memory =\n-                              std::move(owning_device_memory)]() mutable {\n-          promise->Set();\n-        });\n-    if (!callback_status.ok()) {\n-      promise->Set(std::move(callback_status));\n-      return;\n-    }\n-  };\n-\n-  first_definition_event->ExecuteOrAddToFutureTasks(\n-      absl::StrFormat(\"async_copy_raw_sub_buffer_to_host_%p\", &async_copy),\n-      [this, dst, async_copy = std::move(async_copy)]() mutable {\n-        dst.OnReady([this, async_copy = std::move(async_copy)](\n-                        absl::StatusOr<void*> dst) {\n-          // Trampoline through a thread pool since GPUs do not allow calling\n-          // D2H inside the callback's context.\n-          thread_pool()->Schedule(absl::bind_front(async_copy, std::move(dst)));\n-        });\n-      });\n-\n-  return FutureHelpers::WithProfiling(\n-      std::move(future),\n-      /*on_block_start=*/\n-      []() {\n-        tsl::profiler::TraceMeProducer traceme(\n-            \"StreamExecutorGpuClient::CopyRawSubBufferToHost\");\n-        VLOG(1) << \"StreamExecutorGpuClient::CopyRawSubBufferToHost\";\n-        return FutureHelpers::ProfilingKeys(\n-            {/*traceme_context_id =*/traceme.GetContextId()});\n-      },\n-      /*on_block_end=*/\n-      [](FutureHelpers::ProfilingKeys keys) {\n-        tsl::profiler::TraceMeConsumer traceme(\n-            \"StreamExecutorGpuClient::CopyRawSubBufferToHost\",\n-            keys.traceme_context_id);\n-      });\n-}\n-\n absl::Status StreamExecutorGpuClient::UpdateCompileOptionsInternal(\n     CompileOptions* options, ExecutableExtras* returned_extras,\n     bool lookup_addressable_devices) {"
        },
        {
            "sha": "c66fed7584b23e6ee78a42f09aeb1dea2500c078",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.h",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h?ref=cf3e49ba23c56bfaf76880f357c76e4f0a5f364a",
            "patch": "@@ -140,10 +140,6 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n       std::optional<absl::Span<const std::optional<Layout>>> device_layouts,\n       PjRtMemorySpace* memory_space) override;\n \n-  Future<> CopyRawSubBufferToHost(PjRtBuffer* buffer, Future<void*> dst,\n-                                  int64_t offset,\n-                                  int64_t transfer_size) override;\n-\n   void ScheduleRemoteSend(\n       PjRtMemorySpace* memory_space,\n       tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,"
        },
        {
            "sha": "f51db0d8a6155c8ec8d4b029cba558ad02ab194f",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 68,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=cf3e49ba23c56bfaf76880f357c76e4f0a5f364a",
            "patch": "@@ -396,44 +396,6 @@ void StallStreamOnError(LocalDeviceState* local_device, se::Stream* stream) {\n   }\n }\n \n-// Does all necessary bookkeeping, after a buffer is successfully enqueued onto\n-// a stream, to ensure that the buffer will be kept alive until its use on that\n-// stream is complete.\n-//\n-//   device_buffer:              the buffer that was enqueued.\n-//   buffer_local_device:        the device the buffer was allocated on.\n-//   stream_local_device:        the device that manages usage_stream.\n-//   event:                      an event that was recorded on usage_stream\n-//                               after the usage of device_buffer was enqueued.\n-//   usage_stream:               the stream the operation using device_buffer\n-//                               was enqueued on.\n-void RecordUsage(PjRtStreamExecutorBuffer::ScopedHold device_buffer,\n-                 LocalDeviceState* buffer_local_device,\n-                 LocalDeviceState* stream_local_device,\n-                 BufferSequencingEventRef event, se::Stream* usage_stream,\n-                 std::vector<tsl::RCReference<RawSEDeviceMemory>>*\n-                     buffers_to_release = nullptr) {\n-  tsl::profiler::TraceMe traceme(\"RecordUsage\");\n-  bool retain_buffer_until_completion =\n-      // If the buffer wasn't allocated on the same device as the stream, always\n-      // retain a reference.\n-      (stream_local_device != buffer_local_device) ||\n-      // In the synchronous allocation model, always retain a reference.\n-      (stream_local_device->allocation_model() ==\n-       LocalDeviceState::kSynchronous);\n-  if (retain_buffer_until_completion) {\n-    if (buffers_to_release) {\n-      buffers_to_release->push_back(device_buffer->device_memory());\n-    } else {\n-      buffer_local_device\n-          ->ThenRelease(usage_stream, device_buffer->device_memory())\n-          .IgnoreError();\n-    }\n-  }\n-  device_buffer.ConvertUsageHold(usage_stream, event,\n-                                 retain_buffer_until_completion);\n-}\n-\n // Adds necessary synchronization after a copy has been enqueued to a buffer.\n // definition_event was added when the buffer was allocated, but has not yet\n // had an event recorded.\n@@ -708,16 +670,6 @@ AllocateDestinationBuffer(const Shape& on_host_shape, PjRtDevice* device,\n   return py_buffer;\n }\n \n-void PjRtStreamExecutorBuffer::ScopedHold::ConvertUsageHold(\n-    se::Stream* usage_stream, BufferSequencingEventRef event,\n-    bool reference_held) {\n-  CHECK(ok());\n-  CHECK_EQ(type(), kUsage);\n-  parent()->ConvertUsageHold(buffer(), usage_stream, std::move(event),\n-                             reference_held);\n-  SetState(kConverted);\n-}\n-\n bool PjRtStreamExecutorClient::IsOnCpu(PjRtMemorySpace* memory_space) {\n   return memory_space->kind() == PinnedHostMemorySpace::kKind;\n }\n@@ -1390,26 +1342,6 @@ void PjRtStreamExecutorBuffer::Delete() {\n   TF_CHECK_OK(Release(/*wait_for_operations_to_complete=*/false).status());\n }\n \n-void PjRtStreamExecutorBuffer::ConvertUsageHold(TrackedDeviceBuffer* buffer,\n-                                                se::Stream* usage_stream,\n-                                                BufferSequencingEventRef event,\n-                                                bool reference_held) {\n-  absl::MutexLock lock(&mu_);\n-  CHECK(device_buffer() == buffer || device_buffer() == nullptr);\n-  buffer->AddUsageEvent(std::move(event), reference_held);\n-  DecrementUsage();\n-}\n-\n-PjRtStreamExecutorBuffer::ScopedHold\n-PjRtStreamExecutorBuffer::GetBufferWithHold(ScopedHold::Type type) {\n-  absl::MutexLock lock(&mu_);\n-  // Ensure that at most one donation hold can be in progress at a time.\n-  WaitForOutstandingDonationHold();\n-  ScopedHold hold(this, type);\n-  AcquireHoldLocked(&hold);\n-  return hold;\n-}\n-\n Future<> PjRtStreamExecutorBuffer::GetReadyFuture() {\n   absl::InlinedVector<BufferSequencingEventRef, 2> definition_events;\n   Promise<> definition_promise;"
        },
        {
            "sha": "4ea2d660a01de5d7a4a775fcb57bf4e6867e2c03",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.h",
            "status": "modified",
            "additions": 0,
            "deletions": 85,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cf3e49ba23c56bfaf76880f357c76e4f0a5f364a/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h?ref=cf3e49ba23c56bfaf76880f357c76e4f0a5f364a",
            "patch": "@@ -424,35 +424,6 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n   friend class PjRtStreamExecutorBuffer;\n   friend class PjRtStreamExecutorRawBuffer;\n \n-  virtual void CopyToRemoteDevice(PjRtBuffer* buffer,\n-                                  absl::string_view serialized_descriptor,\n-                                  PjRtBuffer::RemoteSendCallback on_done) {\n-    on_done(Unimplemented(\"Cross host sends not implemented.\"),\n-            /*sends_were_enqueued=*/false);\n-  }\n-\n-  virtual Future<> CopyRawSubBufferToHost(PjRtBuffer* buffer, Future<void*> dst,\n-                                          int64_t offset,\n-                                          int64_t transfer_size) {\n-    return Future<>(Unimplemented(\"Raw copies to host not implemented.\"));\n-  }\n-\n-  virtual tsl::RCReference<PjRtDeviceEvent> CopyRawHostToDevice(\n-      LocalDeviceState* local_device,\n-      tsl::RCReference<RawSEDeviceMemory> device_buffer, const void* src,\n-      int64_t offset, int64_t transfer_size) {\n-    return CreateErrorDeviceEvent(\n-        Unimplemented(\"Raw copies h2d not implemented.\"));\n-  }\n-\n-  virtual tsl::RCReference<PjRtDeviceEvent> CopyRawDeviceToHost(\n-      LocalDeviceState* local_device,\n-      tsl::RCReference<RawSEDeviceMemory> device_buffer, void* dst,\n-      int64_t offset, int64_t transfer_size) {\n-    return CreateErrorDeviceEvent(\n-        Unimplemented(\"Raw copies d2h not implemented.\"));\n-  }\n-\n   // Helper function for creating PjRtStreamExecutorExecutables. Modifies\n   // `options` in-place.\n   struct ExecutableExtras {\n@@ -557,38 +528,6 @@ absl::StatusOr<DeviceAssignment> DevicesToDeviceAssignment(\n \n class PjRtStreamExecutorBuffer : public CommonPjRtBufferImpl {\n  public:\n-  class ScopedHold : public CommonPjRtBuffer::ScopedHold {\n-   public:\n-    // Converts the hold into a usage event. Only valid for holds of type\n-    // kUsage.\n-    //\n-    //   usage_stream:   the stream that the buffer was used on.\n-    //   event:          an event that has been recorded on usage_stream after\n-    //                   the buffer was used.\n-    //   reference_held: true if and only if the caller has caused a\n-    //                   reference to this->buffer() to stay live until after\n-    //                   the host is sure that the usage (transfer or execution)\n-    //                   has completed.\n-    void ConvertUsageHold(se::Stream* usage_stream,\n-                          BufferSequencingEventRef event, bool reference_held);\n-\n-    TrackedDeviceBuffer* buffer() const {\n-      return static_cast<TrackedDeviceBuffer*>(\n-          CommonPjRtBuffer::ScopedHold::buffer());\n-    }\n-    TrackedDeviceBuffer* operator->() const { return buffer(); }\n-    const TrackedDeviceBuffer& operator*() const { return *buffer(); }\n-\n-    PjRtStreamExecutorBuffer* parent() const {\n-      return static_cast<PjRtStreamExecutorBuffer*>(\n-          CommonPjRtBuffer::ScopedHold::parent());\n-    }\n-\n-   private:\n-    using CommonPjRtBuffer::ScopedHold::ScopedHold;\n-    friend class PjRtStreamExecutorBuffer;\n-    friend class PjRtStreamExecutorClient;\n-  };\n   PjRtStreamExecutorBuffer(Shape on_device_shape,\n                            std::unique_ptr<TrackedDeviceBuffer> device_buffer,\n                            PjRtClient* client, PjRtDevice* device,\n@@ -611,16 +550,6 @@ class PjRtStreamExecutorBuffer : public CommonPjRtBufferImpl {\n   // external framework drops the reference.\n   void Delete() override;\n \n-  // Returns a hold on the TrackedDeviceBuffer holding the device\n-  // buffers. See comment on ScopedHold.\n-  ScopedHold GetBufferWithHold(ScopedHold::Type type);\n-  ScopedHold GetBufferWithUsageHold() {\n-    return GetBufferWithHold(ScopedHold::kUsage);\n-  }\n-  ScopedHold GetBufferWithExternalReference() {\n-    return GetBufferWithHold(ScopedHold::kExternalReference);\n-  }\n-\n   Future<> GetReadyFuture() override;\n \n   // Similar to Delete, drops the buffer's reference to its associated device\n@@ -643,20 +572,6 @@ class PjRtStreamExecutorBuffer : public CommonPjRtBufferImpl {\n \n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> DonateWithControlDependency(\n       Future<> dependency) override;\n-\n- private:\n-  friend class PjRtClient;\n-\n-  TrackedDeviceBuffer* device_buffer() const\n-      ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n-    return static_cast<TrackedDeviceBuffer*>(CommonPjRtBuffer::device_buffer());\n-  }\n-\n-  // Drops a usage hold and calls device_buffer_->AddUsageEvent. Does a sanity\n-  // check that buffer==device_buffer_ or device_buffer_==nullptr. Called after\n-  // device_buffer_ was successfully enqueued on a stream.\n-  void ConvertUsageHold(TrackedDeviceBuffer* buffer, se::Stream* usage_stream,\n-                        BufferSequencingEventRef event, bool reference_held);\n };\n \n // Allocates the device buffers for a buffer that will be used as the"
        }
    ],
    "stats": {
        "total": 693,
        "additions": 0,
        "deletions": 693
    }
}