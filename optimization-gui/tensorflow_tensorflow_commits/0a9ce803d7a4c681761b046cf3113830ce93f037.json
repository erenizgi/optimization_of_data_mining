{
    "author": "apaszke",
    "message": "[XLA:GPU] Make sure to insert parameter copies if they feed Mosaic GPU collectives\n\nOtherwise the kernels don't actually get the operands in symmetric memory and can crash.\n\nPiperOrigin-RevId: 818560766",
    "sha": "0a9ce803d7a4c681761b046cf3113830ce93f037",
    "files": [
        {
            "sha": "e408d3f8cf4c9c3f6afaaec8af49a7194aab0bff",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=0a9ce803d7a4c681761b046cf3113830ce93f037",
            "patch": "@@ -108,6 +108,7 @@ cc_library(\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n         \":backend_configs_cc\",\n+        \":ir_emission_utils\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/analysis:hlo_alias_analysis\",\n         \"//xla/hlo/analysis:hlo_ordering\","
        },
        {
            "sha": "3c0a63334b95d3aad1201153cd932a4e30d69755",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=0a9ce803d7a4c681761b046cf3113830ce93f037",
            "patch": "@@ -2107,7 +2107,8 @@ bool ShouldAddCopyForCollectiveMemorySpace(const HloValue* value) {\n           module->entry_computation()->parameter_instructions(), inst) ||\n       (inst->opcode() == HloOpcode::kConstant)) {\n     for (auto& use : value->GetUses()) {\n-      if (IsCollective(use.instruction)) {\n+      if (IsCollective(use.instruction) ||\n+          IsCollectiveMosaicGpuInstruction(*use.instruction)) {\n         return true;\n       }\n     }"
        },
        {
            "sha": "c7afb1f4dee036ea3fc063c01a1d38aae8263d99",
            "filename": "third_party/xla/xla/service/gpu/gpu_memory_space_assignment.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc?ref=0a9ce803d7a4c681761b046cf3113830ce93f037",
            "patch": "@@ -18,13 +18,13 @@ limitations under the License.\n #include \"absl/base/no_destructor.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/status.h\"\n-#include \"absl/strings/match.h\"\n #include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_value.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/hlo_value.h\"\n \n namespace xla::gpu {\n@@ -61,13 +61,6 @@ bool IsNvshmemInstruction(const HloInstruction* inst) {\n   return is_nvshmem_collective;\n }\n \n-bool IsCollectiveMosaicGpuInstruction(const HloInstruction* inst) {\n-  return inst->opcode() == HloOpcode::kCustomCall &&\n-         (inst->custom_call_target() == \"mosaic_gpu\" ||\n-          inst->custom_call_target() == \"mosaic_gpu_v2\") &&\n-         absl::StrContains(inst->raw_backend_config_string(), \"nvshmem\");\n-}\n-\n bool IsCollectiveMemoryInstruction(const HloInstruction* inst) {\n   return kSupportedCollectiveOpcodes->contains(inst->opcode()) ||\n          // opcode or async wrapped opcode is in kSupportedCollectiveOpcodes.\n@@ -92,11 +85,11 @@ bool HasCollectiveMemoryInstruction(const HloValue* input_alias,\n \n bool HasCollectiveMosaicInstruction(const HloValue* input_alias) {\n   for (auto& use : input_alias->GetUses()) {\n-    if (IsCollectiveMosaicGpuInstruction(use.instruction)) {\n+    if (IsCollectiveMosaicGpuInstruction(*use.instruction)) {\n       return true;\n     }\n   }\n-  return IsCollectiveMosaicGpuInstruction(input_alias->instruction());\n+  return IsCollectiveMosaicGpuInstruction(*input_alias->instruction());\n }\n \n // Set memory space to MemorySpaceColor::kCollective for all allocations used by"
        },
        {
            "sha": "eea080fb97dcacd1d869ab87c75103649e37614d",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=0a9ce803d7a4c681761b046cf3113830ce93f037",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/escaping.h\"\n+#include \"absl/strings/match.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n@@ -151,6 +152,13 @@ bool IsCustomCallToPtxKernel(const HloInstruction& hlo) {\n          hlo.custom_call_target() == \"__gpu$xla.gpu.ptx\";\n }\n \n+bool IsCollectiveMosaicGpuInstruction(const HloInstruction& hlo) {\n+  return hlo.opcode() == HloOpcode::kCustomCall &&\n+         (hlo.custom_call_target() == \"mosaic_gpu\" ||\n+          hlo.custom_call_target() == \"mosaic_gpu_v2\") &&\n+         absl::StrContains(hlo.raw_backend_config_string(), \"nvshmem\");\n+}\n+\n static bool IsContiguousSlice(\n     const Shape& orig, const Shape& sliced,\n     std::optional<absl::Span<const int64_t>> slice_strides) {"
        },
        {
            "sha": "4a7caf23d9377c2f8235b9b68d9064019d619b80",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a9ce803d7a4c681761b046cf3113830ce93f037/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h?ref=0a9ce803d7a4c681761b046cf3113830ce93f037",
            "patch": "@@ -159,6 +159,9 @@ bool IsCustomCallToTopK(const HloInstruction& hlo);\n // implementation.\n bool IsCustomCallToPtxKernel(const HloInstruction& hlo);\n \n+// Returns true if instruction is a Mosaic GPU collective instruction.\n+bool IsCollectiveMosaicGpuInstruction(const HloInstruction& hlo);\n+\n // Cholesky decomposition. Takes a (batched) matrix as input, and returns a\n // tuple of (result, workspace, info), where result is the result of the\n // Cholesky decomposition, workspace is scratch space for cuSolver, and info"
        }
    ],
    "stats": {
        "total": 28,
        "additions": 17,
        "deletions": 11
    }
}