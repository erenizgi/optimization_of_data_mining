{
    "author": "vksnk",
    "message": "Support bf16 x bf16 -> f32 and i8 x i8 -> i32 data types  in convolution op with YNNPACK enabled.\n\nPiperOrigin-RevId: 843432738",
    "sha": "f54f40d8f2b6bd14965d531eaf7ec25dabb8a3bf",
    "files": [
        {
            "sha": "4c8825d192aa8b75019f44f5e5318cffa0179eae",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f54f40d8f2b6bd14965d531eaf7ec25dabb8a3bf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f54f40d8f2b6bd14965d531eaf7ec25dabb8a3bf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc?ref=f54f40d8f2b6bd14965d531eaf7ec25dabb8a3bf",
            "patch": "@@ -280,9 +280,7 @@ bool IsConvolutionOpSupportedByYnn(const HloInstruction* instr) {\n   // Stores tuple of allowed (input, output) dtypes.\n   static const absl::NoDestructor<absl::flat_hash_set<\n       std::tuple<PrimitiveType, PrimitiveType, PrimitiveType>>>\n-      kAllowedTypes({\n-          {F32, F32, F32},\n-      });\n+      kAllowedTypes({{F32, F32, F32}, {BF16, BF16, F32}, {S8, S8, S32}});\n \n   PrimitiveType lhs_dtype = conv->operand(0)->shape().element_type();\n   PrimitiveType rhs_dtype = conv->operand(1)->shape().element_type();"
        },
        {
            "sha": "300eaaa319baa96b8297514a960885a42483a12a",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 13,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f54f40d8f2b6bd14965d531eaf7ec25dabb8a3bf/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f54f40d8f2b6bd14965d531eaf7ec25dabb8a3bf/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=f54f40d8f2b6bd14965d531eaf7ec25dabb8a3bf",
            "patch": "@@ -542,6 +542,19 @@ std::unique_ptr<HloPassFix<HloPassPipeline>> CreateSimplificationPipeline(\n   return pipeline;\n }\n \n+auto LibrarySupportsConvolution(\n+    HloModule* module, TargetMachineFeatures* target_machine_features) {\n+  const bool ynnpack_convolution_enabled = absl::c_linear_search(\n+      module->config().debug_options().xla_cpu_experimental_ynn_fusion_type(),\n+      DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_CONVOLUTION);\n+  return [=](const HloInstruction& instr) {\n+#ifdef XLA_YNNPACK\n+    return ynnpack_convolution_enabled && IsConvolutionOpSupportedByYnn(&instr);\n+#endif  // XLA_YNNPACK\n+    return false;\n+  };\n+}\n+\n auto LibrarySupportsDot(HloModule* module,\n                         TargetMachineFeatures* target_machine_features) {\n   // TODO(b/406806134): Stop calling XNNPACK from regular Dot thunks. All XNN\n@@ -670,31 +683,41 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   auto library_supports_dot =\n       LibrarySupportsDot(module, target_machine_features);\n \n-  auto call_library_for_dot = [&](const HloInstruction& instr) {\n-    if (instr.opcode() != HloOpcode::kDot) {\n+  auto library_supports_convolution =\n+      LibrarySupportsConvolution(module, target_machine_features);\n+\n+  auto call_library_for_instruction = [&](const HloInstruction& instr) {\n+    if (instr.opcode() != HloOpcode::kDot &&\n+        instr.opcode() != HloOpcode::kConvolution) {\n       return false;\n     }\n \n-    auto dot_strategy = GetDotImplementationStrategy(\n-        module->config(), instr, *target_machine_features,\n-        /*allow_runtime_calls=*/true);\n-    if (dot_strategy != DotImplementationStrategy::kEigen) {\n-      // We aren't going to call a library for this dot.\n-      return false;\n+    if (instr.opcode() == HloOpcode::kDot) {\n+      auto dot_strategy = GetDotImplementationStrategy(\n+          module->config(), instr, *target_machine_features,\n+          /*allow_runtime_calls=*/true);\n+      if (dot_strategy != DotImplementationStrategy::kEigen) {\n+        // We aren't going to call a library for this dot.\n+        return false;\n+      }\n+      return library_supports_dot(instr);\n+    }\n+    if (instr.opcode() == HloOpcode::kConvolution) {\n+      return library_supports_convolution(instr);\n     }\n \n-    return library_supports_dot(instr);\n+    return false;\n   };\n \n   // If YNNPACK is enabled, we only need to upcast dots that YnnDotThunk does\n   // not support. `upcaster_filter` returns false if the instruction shouldn't\n   // be processed.\n   HloPredicate upcaster_filter = [&](const HloInstruction* instr) {\n-    return !call_library_for_dot(*instr);\n+    return !call_library_for_instruction(*instr);\n   };\n \n-  // xla::cpu::GetDotImplementationStrategy (used by call_library_for_dot)\n-  // relies on the canonical form of dots.\n+  // xla::cpu::GetDotImplementationStrategy (used by\n+  // call_library_for_instruction) relies on the canonical form of dots.\n   pipeline.AddPass<DotDecomposer>();\n   pipeline.AddPass<OperandUpcaster>(upcaster_filter);\n \n@@ -754,7 +777,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   // Convert BF16 and F8 operations to F32 and F16 respectively so that the CPU\n   // backend can support BF16/F8 operations without directly implementing a\n   // BF16/F8 lowering for most ops.\n-  CpuFloatSupport bf16_support(BF16, call_library_for_dot);\n+  CpuFloatSupport bf16_support(BF16, call_library_for_instruction);\n #ifdef XLA_ONEDNN\n   bool use_onednn_graph =\n       module->config().debug_options().xla_cpu_use_onednn() &&"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 37,
        "deletions": 16
    }
}