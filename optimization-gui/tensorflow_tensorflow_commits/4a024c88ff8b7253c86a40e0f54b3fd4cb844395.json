{
    "author": "Moerafaat",
    "message": "[XLA:GPU/TMA] Move the restriction on GEMMs with broadcasts and pipelining from the autotuner to the emitter.\n\nThis is a more precise filter, allowing previously disabled configurations to run and only disable TMA optionality on parameters that go through broadcasts.\n\nPiperOrigin-RevId: 829356852",
    "sha": "4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
    "files": [
        {
            "sha": "6b7f4814c802a6d9d63e4196303449338cbe2da0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -32,7 +32,7 @@ namespace xla::gpu {\n void CreateTritonXlaPipeline(\n     mlir::OpPassManager* pm,\n     const stream_executor::GpuComputeCapability& gpu_cc, bool rewrite_int4,\n-    bool allow_tma) {\n+    bool allow_tma, int num_stages) {\n   pm->addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerXTilePass());\n@@ -46,7 +46,7 @@ void CreateTritonXlaPipeline(\n   }\n \n   pm->addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n-      /*allow_tma=*/allow_tma && is_at_least_hopper));\n+      /*allow_tma=*/allow_tma && is_at_least_hopper, num_stages));\n \n   // Lower affine expressions into arithmetic ops.\n   pm->addPass(mlir::createLowerAffinePass());"
        },
        {
            "sha": "8ae26d3a691cc5ef7e87b810328204ffba13d562",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -26,7 +26,7 @@ namespace xla::gpu {\n void CreateTritonXlaPipeline(\n     mlir::OpPassManager* pm,\n     const stream_executor::GpuComputeCapability& gpu_cc, bool rewrite_int4,\n-    bool allow_tma);\n+    bool allow_tma, int num_stages);\n \n // Creates a Triton compilation pipeline.\n //"
        },
        {
            "sha": "2c9272ce02b4f76af2ea5c54252f26d829ebb9ff",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -39,7 +39,8 @@ TEST(CompilationPipelineTest, UnswitchLoopsAfterLICM) {\n   mlir::PassManager pm(&ctx);\n \n   CreateTritonXlaPipeline(&pm, stream_executor::CudaComputeCapability(),\n-                          /*rewrite_int4=*/false, /*allow_tma=*/true);\n+                          /*rewrite_int4=*/false, /*allow_tma=*/true,\n+                          /*num_stages=*/1);\n \n   std::vector<std::string> pass_names;\n   for (const mlir::Pass& pass : pm.getPasses()) {"
        },
        {
            "sha": "2bc796f0d3c543a506687213e873002f3f900e85",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -2002,7 +2002,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   }\n \n   CreateTritonXlaPipeline(&pm, gpu_cc, /*rewrite_int4=*/is_xla_fusion,\n-                          block_level_parameters.is_tma_allowed);\n+                          block_level_parameters.is_tma_allowed,\n+                          block_level_parameters.num_stages);\n \n   int num_warps = block_level_parameters.num_warps;\n   int num_ctas = block_level_parameters.num_ctas;"
        },
        {
            "sha": "94fe858bd0fea858dd9486f0ffed8cfd4f3442af",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -32,7 +32,7 @@ namespace mlir::triton::xla {\n \n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass(\n-    bool allow_tma);\n+    bool allow_tma, int num_stages);\n std::unique_ptr<mlir::Pass> CreateTritonXLASqueezeDimsPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAFoldTransposePass();\n std::unique_ptr<mlir::Pass> CreateGeneralizeKernelSignaturePass();"
        },
        {
            "sha": "c16b096881b83fe63b87222b7534a656b1ceadfc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -28,6 +28,8 @@ def TritonXLAExtractInsertToTritonPass : Pass<\"triton-xla-extract-insert-to-trit\n   let options = [\n     Option<\"allow_tma_\", \"allow_tma\", \"bool\", \"false\",\n            \"Whether to permit lowering to TMA.\">,\n+    Option<\"num_stages_\", \"num_stages\", \"int\", \"1\",\n+           \"Number of stages for pipelining.\">,\n   ];\n   let dependentDialects = [\n     \"triton::TritonDialect\","
        },
        {
            "sha": "a5b50bb5533f7be8d734d75b98b31bf8a06895c8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_extract_insert_to_triton.mlir",
            "status": "modified",
            "additions": 26,
            "deletions": 1,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -3,7 +3,7 @@\n // RUN: | FileCheck %s\n \n // RUN: xla-opt %s -split-input-file \\\n-// RUN: -triton-xla-extract-insert-to-triton=allow_tma=1 \\\n+// RUN: -triton-xla-extract-insert-to-triton=\"allow_tma=1 num_stages=3\" \\\n // RUN: | FileCheck %s --check-prefix=CHECK-TMA\n \n func.func @lower_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n@@ -250,3 +250,28 @@ module {\n // CHECK-TMA-LABEL: tt.func @incompatible_tma_dynamic_offset_not_divisible_by_16_bytes\n // CHECK-TMA:         tt.load\n // CHECK-TMA:         tt.descriptor_store\n+\n+// -----\n+\n+func.func @parameter_into_broadcast_with_3_or_more_stages_does_not_use_tma(\n+          %arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32>\n+  %extracted_tile = triton_xla.extract from %arg0 as\n+      memref<64xf32, #triton_xla.layout<[0]>> [0] [64] [1] : tensor<64xf32>\n+  %0 = tt.expand_dims %extracted_tile {axis = 1 : i32}\n+      : tensor<64xf32> -> tensor<64x1xf32>\n+  %1 = tt.broadcast %0 : tensor<64x1xf32> -> tensor<64x64xf32>\n+  %extracted_tile_0 = triton_xla.extract from %arg1 as\n+      memref<64x64xf32, #triton_xla.layout<[1, 0]>> [0, 0] [64, 64] [1, 1]\n+      : tensor<64x64xf32>\n+  %2 = tt.dot %1, %extracted_tile_0, %cst, inputPrecision = tf32\n+      : tensor<64x64xf32> * tensor<64x64xf32> -> tensor<64x64xf32>\n+  triton_xla.insert %2 into %arg2 as\n+      memref<64x64xf32, #triton_xla.layout<[1, 0]>> [0, 0] [64, 64] [1, 1]\n+      : tensor<64x64xf32>\n+  return\n+}\n+\n+// CHECK-TMA-LABEL: tt.func @parameter_into_broadcast_with_3_or_more_stages_does_not_use_tma\n+// CHECK-TMA-NOT:         tt.descriptor_load %arg0\n+// CHECK-TMA:             tt.descriptor_load %arg1"
        },
        {
            "sha": "0c4a0b7ccd53a60e37c6628caf01011a991e4d9f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 13,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -71,6 +71,17 @@ namespace xgt = xg::triton;\n \n namespace {\n \n+bool HasBroadcastConsumer(Operation* op) {\n+  llvm::SetVector<Operation*> slice;\n+  mlir::getForwardSlice(op, &slice);\n+  for (Operation* sliced_op : slice) {\n+    if (llvm::isa<triton::BroadcastOp>(sliced_op)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n PointerType GetTensorPtrType(Type type) {\n   return PointerType::get(\n       xgt::StorageType(type),\n@@ -150,7 +161,8 @@ bool IsOffsetDivisibilityGuaranteed(mlir::Value offset_val,\n //      minor tile dimension (in bytes) must be divisible by 16, it is\n //      sufficient to check that the offset in the minor dimension (in bytes) is\n //      divisible by 16.\n-bool CanUseTma(bool allow_tma, const ArrayRef<int64_t>& original_shape,\n+bool CanUseTma(Operation* op, bool allow_tma, int num_stages,\n+               const ArrayRef<int64_t>& original_shape,\n                const ArrayRef<int64_t>& tile_shape,\n                const ArrayRef<int64_t>& tile_strides, ValueRange offsets,\n                const TypedValue<PointerType>& pointer,\n@@ -170,6 +182,15 @@ bool CanUseTma(bool allow_tma, const ArrayRef<int64_t>& original_shape,\n     return false;\n   }\n \n+  // TODO(b/421858850): CUDA_ERROR_MISALIGNED_ADDRESS errors are\n+  // happening for some cases when pipelining stages are > 2. The pattern\n+  // observed is that these happen in the presence of a broadcast.\n+  // This is a temporary solution. We should remove this once we have a fix for\n+  // the error.\n+  if (num_stages > 2 && HasBroadcastConsumer(op)) {\n+    return false;\n+  }\n+\n   // Some TMA constraints can't be validated if tile strides are dynamic.\n   if (mlir::ShapedType::isDynamicShape(tile_strides)) {\n     return false;\n@@ -493,8 +514,10 @@ static std::pair<Value, Value> CreateTensorOfPointersAndMask(\n \n class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n  public:\n-  RewriteExtract(mlir::MLIRContext* context, bool allow_tma)\n-      : OpRewritePattern(context), allow_tma_(allow_tma) {}\n+  RewriteExtract(mlir::MLIRContext* context, bool allow_tma, int num_stages)\n+      : OpRewritePattern(context),\n+        allow_tma_(allow_tma),\n+        num_stages_(num_stages) {}\n   using OpRewritePattern::OpRewritePattern;\n \n  private:\n@@ -521,8 +544,8 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n     auto sizes = op.getStaticSizes();\n     auto strides = to_vector(op.getStaticStrides());\n \n-    if (CanUseTma(allow_tma_, src_shape, sizes, strides, offsets, op.getSrc(),\n-                  src_layout)) {\n+    if (CanUseTma(op, allow_tma_, num_stages_, src_shape, sizes, strides,\n+                  offsets, op.getSrc(), src_layout)) {\n       if (auto result = CanonicalizeTileStrides(strides, sizes, src_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n@@ -584,12 +607,15 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n   }\n \n   const bool allow_tma_;\n+  const int num_stages_;\n };\n \n class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n  public:\n-  RewriteInsert(mlir::MLIRContext* context, bool allow_tma)\n-      : OpRewritePattern(context), allow_tma_(allow_tma) {}\n+  RewriteInsert(mlir::MLIRContext* context, bool allow_tma, int num_stages)\n+      : OpRewritePattern(context),\n+        allow_tma_(allow_tma),\n+        num_stages_(num_stages) {}\n   using OpRewritePattern::OpRewritePattern;\n \n  private:\n@@ -624,8 +650,8 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n     SmallVector<unsigned> reduced_dims = to_vector(*reduction_mask);\n     absl::c_sort(reduced_dims);\n \n-    if (CanUseTma(allow_tma_, dst_shape, sizes, strides, offsets, op.getDst(),\n-                  dst_layout)) {\n+    if (CanUseTma(op, allow_tma_, num_stages_, dst_shape, sizes, strides,\n+                  offsets, op.getDst(), dst_layout)) {\n       if (auto result = CanonicalizeTileStrides(strides, sizes, dst_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n@@ -670,6 +696,7 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n   }\n \n   const bool allow_tma_;\n+  const int num_stages_;\n };\n \n // Rewriting tensor::InsertOp as tt.store.\n@@ -730,8 +757,8 @@ class TritonXLAExtractInsertToTritonPass\n   void runOnOperation() override {\n     mlir::MLIRContext* mlir_context = &getContext();\n     mlir::RewritePatternSet patterns(mlir_context);\n-    patterns.add<RewriteExtract, RewriteInsert>(mlir_context,\n-                                                allow_tma_.getValue());\n+    patterns.add<RewriteExtract, RewriteInsert>(\n+        mlir_context, allow_tma_.getValue(), num_stages_.getValue());\n     patterns.add<RewriteScalarExtract, RewriteScalarInsert>(mlir_context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n@@ -754,9 +781,9 @@ std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass() {\n }\n \n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass(\n-    bool allow_tma) {\n+    bool allow_tma, int num_stages) {\n   return std::make_unique<TritonXLAExtractInsertToTritonPass>(\n-      TritonXLAExtractInsertToTritonPassOptions{allow_tma});\n+      TritonXLAExtractInsertToTritonPassOptions{allow_tma, num_stages});\n }\n \n }  // namespace mlir::triton::xla"
        },
        {
            "sha": "9fdd07a2acc6ba9e09f26897fdacb6692e2feed1",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 35,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -676,26 +676,6 @@ std::string GetSelectedGemmBackendAsString(const HloModule* module) {\n   return \"\";\n }\n \n-bool HasBroadcastProducer(const HloInstruction& instr) {\n-  return HloBfsFindIf({&instr},\n-                      [](const HloInstruction* node) {\n-                        return node->opcode() == HloOpcode::kBroadcast;\n-                      })\n-      .has_value();\n-}\n-\n-// CUDA_ERROR_MISALIGNED_ADDRESS errors are happening for some cases when\n-// pipelining stages are > 2. The pattern observed is that these happen in the\n-// presence of a broadcast.\n-void RestrictTmaConfigs(std::vector<TritonGemmConfig>& configs) {\n-  configs.erase(std::remove_if(configs.begin(), configs.end(),\n-                               [&](const TritonGemmConfig& config) {\n-                                 return config.is_tma_allowed &&\n-                                        config.num_stages > 2;\n-                               }),\n-                configs.end());\n-}\n-\n }  // anonymous namespace\n \n absl::Status GemmFusionAutotunerRewriterVisitor::HandleFusion(\n@@ -970,12 +950,7 @@ absl::StatusOr<std::vector<TritonGemmConfig>>\n GemmFusionAutotunerImpl::GenerateTritonConfigs(\n     const HloScaledDotInstruction& dot) {\n   tsl::profiler::TraceMe traceme(\"GenerateTritonConfigs\");\n-  // TODO(b/421858850): Restricting configs for dots from broadcasts is a\n-  // temporary solution. We should remove this once we have a fix for the error.\n   auto configs = GetDefaultTritonConfigs();\n-  if (HasBroadcastProducer(dot)) {\n-    RestrictTmaConfigs(configs);\n-  }\n \n   if (!IsAutotuningEnabled()) {\n     // Keep the first config, which likely does not spill registers.\n@@ -1023,18 +998,10 @@ GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n       /*autotune_tma=*/autotune_tma,\n       /*autotune_warp_specialization=*/autotune_warp_specialization);\n \n-  // TODO(b/421858850): Restricting configs for dots from broadcasts is a\n-  // temporary solution. We should remove this once we have a fix for the error.\n-  auto default_configs = GetDefaultTritonConfigs();\n-  if (HasBroadcastProducer(dot)) {\n-    RestrictTmaConfigs(configs);\n-    RestrictTmaConfigs(default_configs);\n-  }\n-\n   if (!debug_options_.xla_gpu_exhaustive_tiling_search()) {\n     VLOG(1) << \"Restricting configs to the default set.\";\n-    configs =\n-        search_space.OptimizeConfigSet(configs, /*hints=*/default_configs);\n+    configs = search_space.OptimizeConfigSet(\n+        configs, /*hints=*/GetDefaultTritonConfigs());\n   }\n   if (!IsAutotuningEnabled()) {\n     // Keep the first config, which likely does not spill registers."
        },
        {
            "sha": "fcdfbd134974746b6f7d82b432e967604a2fe2ee",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -1876,8 +1876,7 @@ TEST_F(GemmFusionAutotunerEnableTma,\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n }\n \n-TEST_F(GemmFusionAutotunerEnableTma,\n-       TmaConfigsGeneratedAndRunCorrectlyForDotsOfBroadcasts) {\n+TEST_F(GemmFusionAutotunerEnableTma, TmaRunCorrectlyForDotsOfBroadcasts) {\n   if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n@@ -1901,15 +1900,6 @@ TEST_F(GemmFusionAutotunerEnableTma,\n           GetToolkitVersion(), GetDebugOptionsForTest(),\n           &symbolic_expr_context_));\n \n-  auto is_disallowed_tma_config = [](const TritonGemmConfig& c) {\n-    return c.num_stages > 2 && c.is_tma_allowed;\n-  };\n-  auto is_allowed_tma_config = [](const TritonGemmConfig& c) {\n-    return c.num_stages <= 2 && c.is_tma_allowed;\n-  };\n-  EXPECT_FALSE(absl::c_any_of(hopper_configs, is_disallowed_tma_config));\n-  EXPECT_TRUE(absl::c_any_of(hopper_configs, is_allowed_tma_config));\n-\n   EXPECT_TRUE(RunAndCompare(std::move(module),\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n }"
        },
        {
            "sha": "850a788aaf73ef5fcd8a9082c882e01058385301",
            "filename": "third_party/xla/xla/service/gpu/tests/xla-opt.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a024c88ff8b7253c86a40e0f54b3fd4cb844395/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc?ref=4a024c88ff8b7253c86a40e0f54b3fd4cb844395",
            "patch": "@@ -72,7 +72,8 @@ mlir::PassPipelineRegistration<TritonPipelineOptions>\n             gpu_cc = rocm_cc;\n           }\n           xla::gpu::CreateTritonXlaPipeline(&pm, gpu_cc, options.rewrite_int4,\n-                                            options.allow_tma);\n+                                            options.allow_tma,\n+                                            options.num_stages);\n           xla::gpu::CreateTritonPipeline(&pm, gpu_cc, options.num_warps,\n                                          options.num_ctas, options.num_stages,\n                                          cluster_info);"
        }
    ],
    "stats": {
        "total": 148,
        "additions": 81,
        "deletions": 67
    }
}