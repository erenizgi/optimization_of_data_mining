{
    "author": "pifon2a",
    "message": "[XLA:GPU] Remove cusolver dependency from XLA:GPU.\n\nCholesky HLO was the only user for cusolver. After cr/817718825 switched JAX to using FFI for cusolver instead of the dedicated op, we can remove the lowering to the cusolver custom-call and CholeskyThunk. TF users if any, can rely on the CholeskyExpander added to the GPU pipeline.\n\nPiperOrigin-RevId: 830927167",
    "sha": "630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
    "files": [
        {
            "sha": "1d0047c83179aa33e39764ac806a4331a81dc635",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -310,57 +310,6 @@ xla_test(\n     ],\n )\n \n-cc_library(\n-    name = \"cholesky_thunk\",\n-    srcs = [\"cholesky_thunk.cc\"],\n-    hdrs = [\"cholesky_thunk.h\"],\n-    deps = [\n-        \":make_batch_pointers\",\n-        \":thunk\",\n-        \":thunk_proto_cc\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/service:buffer_assignment\",\n-        \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:gpu_solver_context\",\n-        \"//xla/stream_executor:platform\",\n-        \"//xla/stream_executor:stream\",\n-        \"//xla/stream_executor/platform:platform_object_registry\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/functional:any_invocable\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n-xla_test(\n-    name = \"cholesky_thunk_test\",\n-    srcs = [\"cholesky_thunk_test.cc\"],\n-    backends = [\"gpu\"],\n-    deps = [\n-        \":cholesky_thunk\",\n-        \":thunk\",\n-        \":thunk_proto_cc\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:gpu_solver_context\",\n-        \"//xla/stream_executor/platform:platform_object_registry\",\n-        \"//xla/tests:hlo_test_base\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"//xla/tsl/util/proto:proto_matchers\",\n-        \"@com_google_absl//absl/status:status_matchers\",\n-        \"@com_google_googletest//:gtest_main\",\n-    ],\n-)\n-\n cc_library(\n     name = \"command_buffer_thunk\",\n     srcs = [\"command_buffer_thunk.cc\"],\n@@ -2577,7 +2526,6 @@ cc_library(\n     srcs = [\"thunk_proto_deserialization.cc\"],\n     hdrs = [\"thunk_proto_deserialization.h\"],\n     deps = [\n-        \":cholesky_thunk\",\n         \":conditional_thunk\",\n         \":convolution_reorder_thunk\",\n         \":convolution_thunk\","
        },
        {
            "sha": "0785a5a30969188a63303ba88e47b0d6cbe7afee",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cholesky_thunk.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 232,
            "changes": 232,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk.cc?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,232 +0,0 @@\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/gpu/runtime/cholesky_thunk.h\"\n-\n-#include <complex>\n-#include <cstdint>\n-#include <functional>\n-#include <memory>\n-#include <utility>\n-\n-#include \"absl/functional/any_invocable.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/strings/str_format.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/gpu/runtime/make_batch_pointers.h\"\n-#include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-#include \"xla/stream_executor/platform.h\"\n-#include \"xla/stream_executor/platform/platform_object_registry.h\"\n-#include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-namespace {\n-\n-template <typename T>\n-absl::Status DoPotrfBatched(CholeskyParams* params, se::Stream* stream,\n-                            stream_executor::GpuSolverContext& context) {\n-  T* a_base = static_cast<T*>(params->a_buffer.opaque());\n-  se::DeviceMemory<int> infos(params->info_buffer);\n-#if TENSORFLOW_USE_ROCSOLVER\n-  // hipsolver is not supported so allocate a GPU buffer\n-  se::ScopedDeviceMemory<T*> ptrs(\n-      stream->parent(), stream->parent()->AllocateArray<T*>(batch_size_));\n-  auto as = *ptrs;\n-#else\n-  se::DeviceMemory<T*> as(params->workspace_buffer);\n-#endif\n-\n-  CHECK_GE(as.size(), params->batch_size);\n-  CHECK_GE(infos.size(), params->batch_size);\n-\n-  // Run a kernel that sets as[i] = &a_base[i * stride].\n-  const int64_t stride_bytes = params->n * params->n * sizeof(T);\n-  TF_RETURN_IF_ERROR(MakeBatchPointers(\n-      stream, se::DeviceMemoryBase(a_base), stride_bytes,\n-      static_cast<int>(params->batch_size), se::DeviceMemoryBase(as)));\n-\n-  // Now that we've set up the `as` array, we can call cusolver.\n-  return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,\n-                              params->batch_size);\n-}\n-\n-template <typename T>\n-absl::Status DoPotrfUnbatched(CholeskyParams* params, se::Stream* stream,\n-                              stream_executor::GpuSolverContext& context) {\n-  T* a_base = static_cast<T*>(params->a_buffer.opaque());\n-  int* info_base = static_cast<int*>(params->info_buffer.opaque());\n-\n-  int64_t stride = params->n * params->n;\n-  for (int64_t i = 0; i < params->batch_size; ++i) {\n-    se::DeviceMemory<T> a_data(\n-        se::DeviceMemoryBase(&a_base[i * stride], sizeof(T) * stride));\n-    se::DeviceMemory<int> info_data(\n-        se::DeviceMemoryBase(&info_base[i], sizeof(int)));\n-    se::DeviceMemory<T> workspace_data(params->workspace_buffer);\n-    TF_RETURN_IF_ERROR(context.Potrf(params->uplo, params->n, a_data, params->n,\n-                                     info_data, workspace_data));\n-  }\n-  return absl::OkStatus();\n-}\n-\n-absl::Status RunCholesky(PrimitiveType type, CholeskyParams* cholesky_params,\n-                         se::Stream* stream,\n-                         stream_executor::GpuSolverContext* local_context) {\n-  TF_RETURN_IF_ERROR(local_context->SetStream(stream));\n-  if (cholesky_params->batch_size > 1) {\n-    switch (type) {\n-      case F32:\n-        return DoPotrfBatched<float>(cholesky_params, stream, *local_context);\n-      case F64:\n-        return DoPotrfBatched<double>(cholesky_params, stream, *local_context);\n-      case C64:\n-        return DoPotrfBatched<std::complex<float>>(cholesky_params, stream,\n-                                                   *local_context);\n-      case C128:\n-        return DoPotrfBatched<std::complex<double>>(cholesky_params, stream,\n-                                                    *local_context);\n-      default:\n-        return InvalidArgument(\"Invalid type for cholesky %s\",\n-                               PrimitiveType_Name(type));\n-    }\n-  } else {\n-    switch (type) {\n-      case F32:\n-        return DoPotrfUnbatched<float>(cholesky_params, stream, *local_context);\n-      case F64:\n-        return DoPotrfUnbatched<double>(cholesky_params, stream,\n-                                        *local_context);\n-      case C64:\n-        return DoPotrfUnbatched<std::complex<float>>(cholesky_params, stream,\n-                                                     *local_context);\n-      case C128:\n-        return DoPotrfUnbatched<std::complex<double>>(cholesky_params, stream,\n-                                                      *local_context);\n-      default:\n-        return InvalidArgument(\"Invalid type for cholesky %s\",\n-                               PrimitiveType_Name(type));\n-    }\n-  }\n-}\n-\n-}  // namespace\n-\n-CholeskyThunk::CholeskyThunk(\n-    ThunkInfo thunk_info, const CholeskyOptions& options,\n-    BufferAllocation::Slice a_buffer, BufferAllocation::Slice workspace_buffer,\n-    BufferAllocation::Slice info_buffer, PrimitiveType type, int64_t batch_size,\n-    int64_t n,\n-    absl::AnyInvocable<\n-        absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-        solver_context_creator)\n-    : Thunk(Kind::kCholesky, thunk_info),\n-      uplo_(options.lower() ? se::blas::UpperLower::kLower\n-                            : se::blas::UpperLower::kUpper),\n-      a_buffer_(a_buffer),\n-      workspace_buffer_(workspace_buffer),\n-      info_buffer_(info_buffer),\n-      type_(type),\n-      batch_size_(batch_size),\n-      n_(n),\n-      solver_context_creator_(std::move(solver_context_creator)) {}\n-\n-absl::Status CholeskyThunk::ExecuteOnStream(const ExecuteParams& params) {\n-  VLOG(3) << \"type=\" << PrimitiveType_Name(type_)\n-          << \" uplo=\" << se::blas::UpperLowerString(uplo_)\n-          << \" batch_size=\" << batch_size_ << \" n=\" << n_\n-          << \" a=\" << a_buffer_.ToString()\n-          << \" workspace=\" << workspace_buffer_.ToString()\n-          << \" info=\" << info_buffer_.ToString();\n-\n-  se::DeviceMemoryBase a_buffer =\n-      params.buffer_allocations->GetDeviceAddress(a_buffer_);\n-  se::DeviceMemoryBase info_buffer =\n-      params.buffer_allocations->GetDeviceAddress(info_buffer_);\n-  se::DeviceMemoryBase workspace_buffer =\n-      params.buffer_allocations->GetDeviceAddress(workspace_buffer_);\n-  CholeskyParams cholesky_params{n_,       batch_size_,      uplo_,\n-                                 a_buffer, workspace_buffer, info_buffer};\n-  thread_local absl::StatusOr<\n-      std::unique_ptr<stream_executor::GpuSolverContext>>\n-      context = solver_context_creator_();\n-  TF_RETURN_IF_ERROR(context.status());\n-  auto local_context = context.value().get();\n-  return RunCholesky(type_, &cholesky_params, params.stream, local_context);\n-}\n-\n-absl::StatusOr<ThunkProto> CholeskyThunk::ToProto() const {\n-  ThunkProto proto;\n-  *proto.mutable_thunk_info() = thunk_info().ToProto();\n-\n-  CholeskyThunkProto* cholesky_thunk_proto = proto.mutable_cholesky_thunk();\n-\n-  auto options = cholesky_thunk_proto->mutable_options();\n-  options->set_lower(uplo_ == se::blas::UpperLower::kLower);\n-\n-  TF_ASSIGN_OR_RETURN(*cholesky_thunk_proto->mutable_a_buffer(),\n-                      a_buffer_.ToProto());\n-  TF_ASSIGN_OR_RETURN(*cholesky_thunk_proto->mutable_workspace_buffer(),\n-                      workspace_buffer_.ToProto());\n-  TF_ASSIGN_OR_RETURN(*cholesky_thunk_proto->mutable_info_buffer(),\n-                      info_buffer_.ToProto());\n-  cholesky_thunk_proto->set_type(type_);\n-  cholesky_thunk_proto->set_batch_size(batch_size_);\n-  cholesky_thunk_proto->set_n(n_);\n-  return proto;\n-}\n-\n-absl::StatusOr<std::unique_ptr<CholeskyThunk>> CholeskyThunk::FromProto(\n-    ThunkInfo thunk_info, const CholeskyThunkProto& proto,\n-    absl::Span<const BufferAllocation> allocations,\n-    absl::string_view platform_name) {\n-  TF_ASSIGN_OR_RETURN(se::Platform * platform,\n-                      PlatformUtil::GetPlatform(platform_name));\n-  TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice a_buffer,\n-      BufferAllocation::Slice::FromProto(proto.a_buffer(), allocations));\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice workspace_buffer,\n-                      BufferAllocation::Slice::FromProto(\n-                          proto.workspace_buffer(), allocations));\n-  TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice info_buffer,\n-      BufferAllocation::Slice::FromProto(proto.info_buffer(), allocations));\n-  TF_ASSIGN_OR_RETURN(\n-      std::function<\n-          absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-          solver_creator,\n-      stream_executor::PlatformObjectRegistry::GetGlobalRegistry()\n-          .FindObject<stream_executor::GpuSolverContextFactory>(\n-              platform->id()));\n-  return std::make_unique<CholeskyThunk>(\n-      thunk_info, proto.options(), a_buffer, workspace_buffer, info_buffer,\n-      proto.type(), proto.batch_size(), proto.n(), solver_creator);\n-}\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "68a5426bfc9e39aa113303c564f474e6f895d3ff",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cholesky_thunk.h",
            "status": "removed",
            "additions": 0,
            "deletions": 95,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk.h?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,95 +0,0 @@\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_GPU_RUNTIME_CHOLESKY_THUNK_H_\n-#define XLA_BACKENDS_GPU_RUNTIME_CHOLESKY_THUNK_H_\n-\n-#include <cstdint>\n-#include <memory>\n-\n-#include \"absl/functional/any_invocable.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// This class stores everything that StreamExecutor needs to launch a Cholesky\n-// decomposition (LAPACK potrf). It is generated by IrEmitter.\n-//\n-// As an implementation detail, we may run potrf (potentially in a loop, if\n-// batch_size >1), or potrfBatched.\n-//\n-// Thread-compatible.\n-class CholeskyThunk : public Thunk {\n- public:\n-  static absl::StatusOr<std::unique_ptr<CholeskyThunk>> FromProto(\n-      ThunkInfo thunk_info, const CholeskyThunkProto& proto,\n-      absl::Span<const BufferAllocation> allocations,\n-      absl::string_view platform_name);\n-\n-  CholeskyThunk(\n-      ThunkInfo thunk_info, const CholeskyOptions& options,\n-      BufferAllocation::Slice a_buffer,\n-      BufferAllocation::Slice workspace_buffer,\n-      BufferAllocation::Slice info_buffer, PrimitiveType type,\n-      int64_t batch_size, int64_t n,\n-      absl::AnyInvocable<\n-          absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-          solver_context_creator);\n-\n-  CholeskyThunk(const CholeskyThunk&) = delete;\n-  CholeskyThunk& operator=(const CholeskyThunk&) = delete;\n-\n-  absl::StatusOr<ThunkProto> ToProto() const override;\n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n-\n- private:\n-  se::blas::UpperLower uplo_;\n-\n-  const BufferAllocation::Slice a_buffer_;\n-  const BufferAllocation::Slice workspace_buffer_;\n-  const BufferAllocation::Slice info_buffer_;\n-\n-  const PrimitiveType type_;\n-  const int64_t batch_size_;\n-  const int64_t n_;\n-  absl::AnyInvocable<\n-      absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-      solver_context_creator_;\n-};\n-\n-struct CholeskyParams {\n-  int64_t n;\n-  int64_t batch_size;\n-  se::blas::UpperLower uplo;\n-  se::DeviceMemoryBase a_buffer;\n-  se::DeviceMemoryBase workspace_buffer;\n-  se::DeviceMemoryBase info_buffer;\n-};\n-\n-}  // namespace gpu\n-}  // namespace xla\n-\n-#endif  // XLA_BACKENDS_GPU_RUNTIME_CHOLESKY_THUNK_H_"
        },
        {
            "sha": "283621cdfd883bc477825a133bda39429d9c3758",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cholesky_thunk_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 79,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcholesky_thunk_test.cc?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,79 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/gpu/runtime/cholesky_thunk.h\"\n-\n-#include <cstdint>\n-#include <memory>\n-#include <vector>\n-\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-#include \"absl/status/status_matchers.h\"\n-#include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-#include \"xla/stream_executor/platform/platform_object_registry.h\"\n-#include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/util/proto/proto_matchers.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::gpu {\n-namespace {\n-\n-using ::absl_testing::IsOkAndHolds;\n-using ::tsl::proto_testing::EqualsProto;\n-\n-class CholeskyThunkTest : public HloTestBase {};\n-\n-TEST_F(CholeskyThunkTest, ProtoRoundTrip) {\n-  Thunk::ThunkInfo thunk_info;\n-  thunk_info.profile_annotation = \"cholesky\";\n-  CholeskyOptions options;\n-  options.set_lower(true);\n-  std::vector<BufferAllocation> buffer_allocations = {\n-      BufferAllocation(0, 256, 0), BufferAllocation(1, 128, 0),\n-      BufferAllocation(2, 4, 0)};\n-  BufferAllocation::Slice a_buffer(&buffer_allocations[0], 0, 256);\n-  BufferAllocation::Slice workspace_buffer(&buffer_allocations[1], 0, 128);\n-  BufferAllocation::Slice info_buffer(&buffer_allocations[2], 0, 4);\n-  PrimitiveType type = F32;\n-  int64_t batch_size = 1;\n-  int64_t n = 16;\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto solver_creator,\n-      stream_executor::PlatformObjectRegistry::GetGlobalRegistry()\n-          .FindObject<stream_executor::GpuSolverContextFactory>(\n-              backend().platform()->id()));\n-\n-  CholeskyThunk thunk(thunk_info, options, a_buffer, workspace_buffer,\n-                      info_buffer, type, batch_size, n, solver_creator);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, thunk.ToProto());\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<CholeskyThunk> round_trip_thunk,\n-      CholeskyThunk::FromProto(thunk.thunk_info(), proto.cholesky_thunk(),\n-                               buffer_allocations,\n-                               backend().platform()->Name()));\n-\n-  EXPECT_THAT(round_trip_thunk->ToProto(), IsOkAndHolds(EqualsProto(proto)));\n-}\n-\n-}  // namespace\n-}  // namespace xla::gpu"
        },
        {
            "sha": "e15f2b83263bb0404f24d54e5b5a73c8bcb9f0bd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -259,7 +259,6 @@ Thunk::ExecuteParams::ExecuteParams(\n     CASE(kAllToAllStart);\n     CASE(kBuffersDebugChecksum);\n     CASE(kBuffersDebugFloatCheck);\n-    CASE(kCholesky);\n     CASE(kCollectiveBroadcast);\n     CASE(kCollectiveBroadcastDone);\n     CASE(kCollectiveBroadcastStart);"
        },
        {
            "sha": "4faba47e1c8dd8a0c652d9a39c5caae8fb3b885a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -139,7 +139,6 @@ class Thunk {\n     kAllToAllStart,\n     kBuffersDebugChecksum,\n     kBuffersDebugFloatCheck,\n-    kCholesky,\n     kCollectiveBroadcast,\n     kCollectiveBroadcastDone,\n     kCollectiveBroadcastStart,"
        },
        {
            "sha": "667076f1a69f71868865f1f40857de6af2cce439",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -120,16 +120,6 @@ message TriangularSolveThunkProto {\n   int64 b_batch_stride = 10;\n }\n \n-message CholeskyThunkProto {\n-  xla.CholeskyOptions options = 1;\n-  xla.buffer_assignment.BufferAllocationSliceProto a_buffer = 2;\n-  xla.buffer_assignment.BufferAllocationSliceProto workspace_buffer = 3;\n-  xla.buffer_assignment.BufferAllocationSliceProto info_buffer = 4;\n-  xla.PrimitiveType type = 5;\n-  int64 batch_size = 6;\n-  int64 n = 7;\n-}\n-\n message ReplicaIdThunkProto {\n   xla.buffer_assignment.BufferAllocationSliceProto dest_buffer = 1;\n }\n@@ -301,8 +291,7 @@ message ThunkProto {\n     ConvolutionThunkProto convolution_thunk = 25;\n     ConvolutionReorderThunkProto convolution_reorder_thunk = 26;\n     FftThunkProto fft_thunk = 27;\n-    CholeskyThunkProto cholesky_thunk = 28;\n-    Memset32BitValueThunkProto memset32bit_value_thunk = 29;\n+    Memset32BitValueThunkProto memset32bit_value_thunk = 28;\n     CustomCallThunkProto custom_call_thunk = 30;\n     CubSortThunkProto cub_sort_thunk = 31;\n   }"
        },
        {
            "sha": "4aa4638258640d23459ec1331248b900aff3efab",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"google/protobuf/descriptor.h\"\n #include \"google/protobuf/message.h\"\n-#include \"xla/backends/gpu/runtime/cholesky_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n #include \"xla/backends/gpu/runtime/convolution_reorder_thunk.h\"\n #include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n@@ -211,10 +210,6 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProtoImpl(\n       return OutfeedThunk::FromProto(std::move(thunk_info),\n                                      thunk_proto.outfeed_thunk(),\n                                      buffer_allocations);\n-    case ThunkProto::kCholeskyThunk:\n-      return CholeskyThunk::FromProto(std::move(thunk_info),\n-                                      thunk_proto.cholesky_thunk(),\n-                                      buffer_allocations, platform_name);\n \n     default:\n       std::optional<absl::string_view> unsupported_thunk_type ="
        },
        {
            "sha": "52e2c0bc1521b82da2e198517c30699c0fc7baa1",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -461,7 +461,6 @@ cc_library(\n         \"//xla/backends/gpu/runtime:all_gather_thunk\",\n         \"//xla/backends/gpu/runtime:all_reduce_thunk\",\n         \"//xla/backends/gpu/runtime:all_to_all_thunk\",\n-        \"//xla/backends/gpu/runtime:cholesky_thunk\",\n         \"//xla/backends/gpu/runtime:collective_broadcast_thunk\",\n         \"//xla/backends/gpu/runtime:collective_group_thunk\",\n         \"//xla/backends/gpu/runtime:collective_permute_thunk\",\n@@ -1685,6 +1684,7 @@ cc_library(\n         \"//xla/hlo/transforms/collectives:collectives_schedule_linearizer\",\n         \"//xla/hlo/transforms/collectives:convert_async_collectives_to_sync\",\n         \"//xla/hlo/transforms/expanders:bitcast_dtypes_expander\",\n+        \"//xla/hlo/transforms/expanders:cholesky_expander\",\n         \"//xla/hlo/transforms/expanders:comparison_expander\",\n         \"//xla/hlo/transforms/expanders:convolution_4d_expander\",\n         \"//xla/hlo/transforms/expanders:convolution_pred_expander\",\n@@ -2208,7 +2208,6 @@ cc_library(\n         \"//xla/service/gpu/transforms:cudnn_norm_rewriter\",\n         \"//xla/service/gpu/transforms:cudnn_pad_for_convolutions\",\n         \"//xla/service/gpu/transforms:cudnn_simplify_padding\",\n-        \"//xla/service/gpu/transforms:gpusolver_rewriter\",\n         \"//xla/service/gpu/transforms:triangular_solve_rewriter\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n@@ -2222,7 +2221,6 @@ cc_library(\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/cuda:cuda_diagnostics\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n-        \"//xla/stream_executor/cuda:cuda_solver_context\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2450,7 +2448,6 @@ cc_library(\n         \"//xla/service/gpu/transforms:conv_rewriter\",\n         \"//xla/service/gpu/transforms:cublas_pad_for_gemms\",\n         \"//xla/service/gpu/transforms:cudnn_fused_conv_rewriter\",\n-        \"//xla/service/gpu/transforms:gpusolver_rewriter\",\n         \"//xla/service/gpu/transforms:triangular_solve_rewriter\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:dnn\","
        },
        {
            "sha": "5bc76acbad72e0a728cccb29a00bbed1c4ca5517",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -60,7 +60,6 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/conv_rewriter.h\"\n #include \"xla/service/gpu/transforms/cublas_pad_for_gemms.h\"\n #include \"xla/service/gpu/transforms/cudnn_fused_conv_rewriter.h\"\n-#include \"xla/service/gpu/transforms/gpusolver_rewriter.h\"\n #include \"xla/service/gpu/transforms/triangular_solve_rewriter.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_verifier.h\"\n@@ -124,8 +123,6 @@ absl::Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization(\n   ConvBfloat16Support conv_bf16_support(*gpu_version.rocm_compute_capability());\n   pipeline.AddPass<FloatNormalization>(&conv_bf16_support);\n \n-  pipeline.AddPass<GpusolverRewriter>(\n-      stream_executor::RocmSolverContext::Create);\n   pipeline.AddPass<ConvRewriter>(gpu_version);\n   pipeline.AddPass<ConvPaddingLegalization>();\n   auto rcc = gpu_version.rocm_compute_capability();"
        },
        {
            "sha": "fd51f22a7beec34fad26341df99a2e81fa3e9f06",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -83,6 +83,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/collectives/collectives_schedule_linearizer.h\"\n #include \"xla/hlo/transforms/convert_memory_placement_to_internal_annotations.h\"\n #include \"xla/hlo/transforms/expanders/bitcast_dtypes_expander.h\"\n+#include \"xla/hlo/transforms/expanders/cholesky_expander.h\"\n #include \"xla/hlo/transforms/expanders/comparison_expander.h\"\n #include \"xla/hlo/transforms/expanders/convolution_4d_expander.h\"\n #include \"xla/hlo/transforms/expanders/convolution_pred_expander.h\"\n@@ -622,6 +623,9 @@ absl::Status RunOptimizationPasses(\n   // Rewrite select-and-scatter as a scatter and a reduce-window.\n   pipeline.AddPass<SelectAndScatterExpander>();\n \n+  // Rewrite Cholesky.\n+  pipeline.AddPass<CholeskyExpander>();\n+\n   if (RequireDeterminism(hlo_module->config())) {\n     // Scatter can be indeterministic if indices are not unique or a non\n     // associative combiner function is used. Eliminate these Scatter ops."
        },
        {
            "sha": "d44c1b73d5cfb24f2f0d0a85829a9185fd38ed87",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -133,14 +133,6 @@ absl::StatusOr<bool> IsCublasSupportedMatMul(\n       return false;\n   }\n }\n-const char* const kCusolverCholeskyCallTarget = \"__cusolver$cholesky\";\n-\n-bool IsCustomCallToCusolver(const HloInstruction& hlo) {\n-  if (hlo.opcode() != HloOpcode::kCustomCall) {\n-    return false;\n-  }\n-  return hlo.custom_call_target() == kCusolverCholeskyCallTarget;\n-}\n \n bool IsCustomCallToTopK(const HloInstruction& hlo) {\n   return hlo.opcode() == HloOpcode::kCustomCall &&"
        },
        {
            "sha": "fc2eb8f20c0411dbea7db4b29586e2d376cf9e27",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.h",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -140,13 +140,6 @@ std::optional<std::string> GetCustomFusionConfigName(\n // fusion. This is determined by checking the name of custom fusion config.\n bool IsDynamicSliceFusion(const HloInstruction* instr);\n \n-// Returns true if `hlo` will be implemented as a call to a cuSolver routine.\n-//\n-// This returns true if `hlo` is a CustomCall HLO with a call target equal to\n-// one of the kCusolver... constants, but returns *false* for HLOs with\n-// say, a kCholesky opcode.\n-bool IsCustomCallToCusolver(const HloInstruction& hlo);\n-\n // Returns true if `hlo` will be implemented as a call to a TopK routine.\n bool IsCustomCallToTopK(const HloInstruction& hlo);\n \n@@ -157,12 +150,6 @@ bool IsCustomCallToPtxKernel(const HloInstruction& hlo);\n // Returns true if instruction is a Mosaic GPU collective instruction.\n bool IsCollectiveMosaicGpuInstruction(const HloInstruction& hlo);\n \n-// Cholesky decomposition. Takes a (batched) matrix as input, and returns a\n-// tuple of (result, workspace, info), where result is the result of the\n-// Cholesky decomposition, workspace is scratch space for cuSolver, and info\n-// is a success/failure code per batch element.\n-extern const char* const kCusolverCholeskyCallTarget;\n-\n // Returns true if `instr` is a slice (or dynamic slice) instruction and\n // operates on a contiguous slice of the input buffer.\n bool IsContiguousSlice(const HloInstruction& instr);"
        },
        {
            "sha": "fee453c05a5a8099bcf3a28246d6cab7c602598c",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 68,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -75,7 +75,6 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n-#include \"xla/backends/gpu/runtime/cholesky_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_broadcast_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_group_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n@@ -1087,70 +1086,6 @@ absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(\n   return absl::OkStatus();\n }\n \n-absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {\n-  TF_ASSIGN_OR_RETURN(CholeskyOptions options,\n-                      instr->backend_config<CholeskyOptions>());\n-  const Shape& shape = instr->operand(0)->shape();\n-  int ndim = shape.dimensions().size();\n-  CHECK_GE(ndim, 2);\n-  int64_t n = shape.dimensions(ndim - 1);\n-\n-  const absl::Span<const int64_t>& dims = shape.dimensions();\n-  int64_t batch_size =\n-      std::accumulate(dims.begin(), dims.end() - 2, int64_t{1},\n-                      [](int64_t a, int64_t b) { return a * b; });\n-\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice operand_buffer,\n-                      GetAllocationSliceForHlo(instr->operand(0), {}));\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice a_buffer,\n-                      GetAllocationSliceForHlo(instr, {0}));\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice workspace_buffer,\n-                      GetAllocationSliceForHlo(instr, {1}));\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice info_buffer,\n-                      GetAllocationSliceForHlo(instr, {2}));\n-\n-  ThunkSequence thunks;\n-\n-  if (operand_buffer != a_buffer) {\n-    thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(\n-            instr, ir_emitter_context_->GetNextThunkId()),\n-        /*source_buffer=*/operand_buffer,\n-        /*destination_buffer=*/a_buffer,\n-        /*mem_size=*/ShapeUtil::ByteSizeOf(shape)));\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(\n-      se::Platform * platform,\n-      PlatformUtil::GetPlatform(ir_emitter_context_->platform_name()));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::function<\n-          absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-          solver_creator,\n-      stream_executor::PlatformObjectRegistry::GetGlobalRegistry()\n-          .FindObject<stream_executor::GpuSolverContextFactory>(\n-              platform->id()));\n-\n-  thunks.push_back(std::make_unique<CholeskyThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(\n-          instr, ir_emitter_context_->GetNextThunkId()),\n-      options, a_buffer, workspace_buffer, info_buffer, shape.element_type(),\n-      batch_size, n, std::move(solver_creator)));\n-\n-  // Elide the sequential thunk if there's no copy.\n-  if (thunks.size() == 1) {\n-    AddThunkToThunkSequence(std::move(thunks[0]));\n-  } else {\n-    AddThunkToThunkSequence(std::make_unique<SequentialThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(\n-            instr, ir_emitter_context_->GetNextThunkId()),\n-        std::move(thunks)));\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n     const HloCustomCallInstruction* instr) {\n   const std::string& call_target_name = instr->custom_call_target();\n@@ -3290,9 +3225,6 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n       if (IsCustomCallToDnnConvolution(*instr)) {\n         return EmitConvolutionThunk(custom_call);\n       }\n-      if (IsCustomCallToCusolver(*instr)) {\n-        return EmitCholeskyThunk(instr);\n-      }\n       if (IsTriangularSolve(*instr)) {\n         return EmitTriangularSolveCustomCall(instr);\n       }"
        },
        {
            "sha": "4fe71b79f5dca92093c0d4d6459bb09601fb5a02",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -126,7 +126,6 @@ class IrEmitterUnnested : public IrEmitter {\n   absl::Status EmitCuDnnThunk(const HloCustomCallInstruction* instr);\n   absl::Status EmitPtxCustomCall(const HloCustomCallInstruction* instr);\n   absl::Status EmitCubDeviceRadixSort(const HloCustomCallInstruction* instr);\n-  absl::Status EmitCholeskyThunk(const HloInstruction* instr);\n   absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);\n   absl::Status EmitFftThunk(const HloFftInstruction* instr);\n   absl::Status EmitAsyncComputation(const HloInstruction* instr);"
        },
        {
            "sha": "fc2d786865262a385ef8593978628cb132d29ff0",
            "filename": "third_party/xla/xla/service/gpu/nvptx_alias_info.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_alias_info.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_alias_info.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_alias_info.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -55,10 +55,6 @@ std::optional<bool> NVPTXAliasInfo::MayAlias(\n         return (config.beta() != 0.) && operand == user->operand(2) &&\n                absl::c_count(user->operands(), operand) == 1;\n       }\n-      // The operand of cholesky can be shared with the first output.\n-      if (user->custom_call_target() == kCusolverCholeskyCallTarget) {\n-        return user_index.size() == 1 && user_index[0] == 0;\n-      }\n       return false;\n     default:\n       return GpuAliasInfo::MayAlias(operand, operand_index, user, user_index);"
        },
        {
            "sha": "88dd72790edf2016b961849d2bf95854645f054d",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -91,7 +91,6 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/cudnn_norm_rewriter.h\"\n #include \"xla/service/gpu/transforms/cudnn_pad_for_convolutions.h\"\n #include \"xla/service/gpu/transforms/cudnn_simplify_padding.h\"\n-#include \"xla/service/gpu/transforms/gpusolver_rewriter.h\"\n #include \"xla/service/gpu/transforms/triangular_solve_rewriter.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -104,7 +103,6 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_diagnostics.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/cuda/cuda_solver_context.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n@@ -198,8 +196,6 @@ absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n   MatmulBfloat16Support matmul_bf16_support(*cuda_compute_capability);\n   pipeline.AddPass<FloatNormalization>(&matmul_bf16_support);\n \n-  pipeline.AddPass<GpusolverRewriter>(\n-      stream_executor::CudaSolverContext::Create);\n   if (!hlo_module->config()\n            .debug_options()\n            .xla_gpu_experimental_disable_binary_libraries()) {"
        },
        {
            "sha": "bb34f06c9065c09b6459864e8e811dfcebfef869",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 51,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -1734,57 +1734,6 @@ xla_cc_test(\n     ],\n )\n \n-cc_library(\n-    name = \"gpusolver_rewriter\",\n-    srcs = [\"gpusolver_rewriter.cc\"],\n-    hdrs = [\"gpusolver_rewriter.h\"],\n-    tags = [\"gpu\"],\n-    deps = [\n-        \"//xla:comparison_util\",\n-        \"//xla:literal\",\n-        \"//xla:literal_util\",\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/pass:hlo_pass\",\n-        \"//xla/service/gpu:ir_emission_utils\",\n-        \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:gpu_solver_context\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/functional:any_invocable\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"gpusolver_rewriter_test\",\n-    srcs = [\"gpusolver_rewriter_test.cc\"],\n-    tags = [\"gpu\"],\n-    deps = [\n-        \":gpusolver_rewriter\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n-        \"//xla/service:pattern_matcher\",\n-        \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:gpu_solver_context\",\n-        \"//xla/stream_executor:stream\",\n-        \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_googletest//:gtest_main\",\n-    ],\n-)\n-\n cc_library(\n     name = \"layout_assignment\",\n     srcs = [\"layout_assignment.cc\"],"
        },
        {
            "sha": "58eb5daa8e9eebe24790d590aa9c37bb45640651",
            "filename": "third_party/xla/xla/service/gpu/transforms/gpusolver_rewriter.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 209,
            "changes": 209,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter.cc?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,209 +0,0 @@\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/transforms/gpusolver_rewriter.h\"\n-\n-#include <cstdint>\n-#include <functional>\n-#include <memory>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/functional/any_invocable.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/comparison_util.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/layout_util.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/literal_util.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-namespace {\n-\n-void SetFortranLayout(Shape* shape) {\n-  LayoutUtil::SetToDefaultLayout(shape);\n-  int n = shape->mutable_layout()->minor_to_major().size();\n-  CHECK_GE(n, 2);\n-  std::swap(shape->mutable_layout()->mutable_minor_to_major()->at(0),\n-            shape->mutable_layout()->mutable_minor_to_major()->at(1));\n-}\n-\n-absl::StatusOr<HloInstruction*> CreateCholesky(\n-    stream_executor::GpuSolverContext* context, HloInstruction* operand,\n-    const CholeskyOptions& options, const OpMetadata& metadata) {\n-  HloComputation* computation = operand->parent();\n-\n-  Shape a_shape = operand->shape();\n-  int ndim = a_shape.dimensions().size();\n-  CHECK_GE(ndim, 2);\n-  int64_t n = a_shape.dimensions(ndim - 1);\n-\n-  std::vector<int64_t> batch_dims(a_shape.dimensions().begin(),\n-                                  a_shape.dimensions().end() - 2);\n-  std::vector<int64_t> batch_dim_ids(batch_dims.size());\n-  absl::c_iota(batch_dim_ids, 0);\n-  int64_t batch_size = absl::c_accumulate(batch_dims, 1, std::multiplies<>{});\n-\n-  // Find the workspace size.\n-  se::blas::UpperLower uplo = options.lower() ? se::blas::UpperLower::kLower\n-                                              : se::blas::UpperLower::kUpper;\n-  int64_t workspace_size;  // Number of elements of size a_shape.element_type()\n-  TF_ASSIGN_OR_RETURN(\n-      workspace_size,\n-      context->PotrfBufferSize(a_shape.element_type(), uplo, n, n, batch_size));\n-\n-  // TODO(phawkins): Ideally we would relax this constraint. What we actually\n-  // want is that:\n-  // a) the batch dimensions are major, in no particular order.\n-  // b) the two minor dimensions are in fortran (column-major) order,\n-\n-  SetFortranLayout(&a_shape);\n-\n-  // This call returns a tuple of (cholesky_result, workspace, info) where:\n-  // * cholesky_result is the result of the Cholesky decomposition,\n-  // * workspace is temporary scratch memory used by cuSolver.\n-  // * info contains the Potrf success/failure status.\n-  // Currently we have no meaningful way to report an error, so we simply\n-  // discard the success/failure information. Obviously this is suboptimal.\n-  Shape info_shape = ShapeUtil::MakeShape(S32, batch_dims);\n-  Shape call_shape = ShapeUtil::MakeTupleShape(\n-      {a_shape,\n-       ShapeUtil::MakeShape(operand->shape().element_type(), {workspace_size}),\n-       info_shape});\n-\n-  HloInstruction* custom_call =\n-      computation->AddInstruction(HloInstruction::CreateCustomCall(\n-          call_shape, {operand}, kCusolverCholeskyCallTarget, {a_shape}));\n-  custom_call->set_metadata(metadata);\n-  TF_RETURN_IF_ERROR(custom_call->set_backend_config(options));\n-  HloInstruction* out = computation->AddInstruction(\n-      HloInstruction::CreateGetTupleElement(a_shape, custom_call, 0));\n-  HloInstruction* info = computation->AddInstruction(\n-      HloInstruction::CreateGetTupleElement(info_shape, custom_call, 2));\n-\n-  // If info was non-zero, indicating that the Cholesky decomposition failed,\n-  // returns an array full of NaNs for the corresponding batch element.\n-  HloInstruction* zero = computation->AddInstruction(\n-      HloInstruction::CreateConstant(LiteralUtil::Zero(S32)));\n-  HloInstruction* zeros =\n-      computation->AddInstruction(HloInstruction::CreateBroadcast(\n-          info_shape, zero, /*broadcast_dimensions=*/{}));\n-  HloInstruction* ok = computation->AddInstruction(\n-      HloInstruction::CreateCompare(ShapeUtil::MakeShape(PRED, batch_dims),\n-                                    info, zeros, ComparisonDirection::kEq));\n-  ok = computation->AddInstruction(HloInstruction::CreateBroadcast(\n-      ShapeUtil::MakeShape(PRED, a_shape.dimensions()), ok,\n-      /*broadcast_dimensions=*/batch_dim_ids));\n-\n-  TF_ASSIGN_OR_RETURN(Literal nan_literal,\n-                      LiteralUtil::NanValue(a_shape.element_type()));\n-  HloInstruction* nan = computation->AddInstruction(\n-      HloInstruction::CreateConstant(std::move(nan_literal)));\n-  HloInstruction* nans =\n-      computation->AddInstruction(HloInstruction::CreateBroadcast(\n-          a_shape, nan, /*broadcast_dimensions=*/{}));\n-\n-  HloInstruction* select =\n-      computation->AddInstruction(HloInstruction::CreateTernary(\n-          a_shape, HloOpcode::kSelect, ok, out, nans));\n-  return select;\n-}\n-\n-// Tries to rewrite a single convolution into a call to cudnn.\n-absl::StatusOr<bool> RunOnInstruction(\n-    stream_executor::GpuSolverContext* context, HloInstruction* instruction) {\n-  if (HloPredicateIsNotOp<HloOpcode::kCholesky>(instruction)) {\n-    return false;\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(\n-      HloInstruction * custom_call,\n-      CreateCholesky(context, instruction->mutable_operand(0),\n-                     instruction->cholesky_options(), instruction->metadata()));\n-\n-  VLOG(1) << \"Replacing \" << instruction->ToString() << \" with \"\n-          << custom_call->ToString();\n-\n-  TF_RETURN_IF_ERROR(\n-      instruction->parent()->ReplaceInstruction(instruction, custom_call));\n-  return true;\n-}\n-\n-}  // namespace\n-\n-// Rewrites the convolutions in the given computation into calls to cudnn.\n-// Returns true if it made any changes.\n-absl::StatusOr<bool> GpusolverRewriter::RunOnComputation(\n-    HloComputation* computation) {\n-  std::vector<HloInstruction*> cusolver_calls;\n-  for (auto* hlo : computation->instructions()) {\n-    if (HloPredicateIsOp<HloOpcode::kCholesky>(hlo)) {\n-      cusolver_calls.push_back(hlo);\n-    }\n-  }\n-\n-  if (cusolver_calls.empty()) {\n-    return false;\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(auto context, solver_context_creator_());\n-\n-  bool changed = false;\n-  for (HloInstruction* instruction : cusolver_calls) {\n-    TF_ASSIGN_OR_RETURN(bool result,\n-                        RunOnInstruction(context.get(), instruction));\n-    changed |= result;\n-  }\n-  return changed;\n-}\n-\n-GpusolverRewriter::GpusolverRewriter(\n-    absl::AnyInvocable<\n-        absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-        solver_context_creator)\n-    : solver_context_creator_(std::move(solver_context_creator)) {}\n-\n-absl::StatusOr<bool> GpusolverRewriter::RunImpl(\n-    HloModule* module,\n-    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  bool changed = false;\n-  for (HloComputation* computation :\n-       module->MakeNonfusionComputations(execution_threads)) {\n-    TF_ASSIGN_OR_RETURN(bool result, RunOnComputation(computation));\n-    changed |= result;\n-  }\n-  return changed;\n-}\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "9a49dc452463cdf3b194f55000c998a0416cd905",
            "filename": "third_party/xla/xla/service/gpu/transforms/gpusolver_rewriter.h",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter.h?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,57 +0,0 @@\n-#include \"absl/functional/any_invocable.h\"\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_SERVICE_GPU_TRANSFORMS_GPUSOLVER_REWRITER_H_\n-#define XLA_SERVICE_GPU_TRANSFORMS_GPUSOLVER_REWRITER_H_\n-\n-#include <memory>\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/pass/hlo_pass_interface.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// Rewrites Cholesky calls into CustomCall HLOs that call into cuSolver.\n-class GpusolverRewriter : public HloModulePass {\n- public:\n-  explicit GpusolverRewriter(\n-      absl::AnyInvocable<\n-          absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-          solver_context_creator);\n-  absl::string_view name() const override { return \"gpusolver-rewriter\"; }\n-\n- protected:\n-  absl::StatusOr<bool> RunImpl(\n-      HloModule* module,\n-      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n-\n- private:\n-  absl::StatusOr<bool> RunOnComputation(HloComputation* computation);\n-  absl::AnyInvocable<\n-      absl::StatusOr<std::unique_ptr<stream_executor::GpuSolverContext>>()>\n-      solver_context_creator_;\n-};\n-\n-}  // namespace gpu\n-}  // namespace xla\n-\n-#endif  // XLA_SERVICE_GPU_TRANSFORMS_GPUSOLVER_REWRITER_H_"
        },
        {
            "sha": "840435acf554113dc2fdc3e626dbdd304595dffd",
            "filename": "third_party/xla/xla/service/gpu/transforms/gpusolver_rewriter_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 148,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgpusolver_rewriter_test.cc?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,148 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/transforms/gpusolver_rewriter.h\"\n-\n-#include <complex>\n-#include <cstdint>\n-#include <memory>\n-\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-#include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n-#include \"xla/service/pattern_matcher.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-#include \"xla/stream_executor/stream.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-namespace {\n-\n-namespace m = ::xla::match;\n-\n-class GpuSolverContextStub : stream_executor::GpuSolverContext {\n- public:\n-  GpuSolverContextStub() = default;\n-  static absl::StatusOr<std::unique_ptr<GpuSolverContext>> Create() {\n-    return absl::WrapUnique(\n-        static_cast<GpuSolverContext*>(new GpuSolverContextStub));\n-  }\n-\n-  absl::Status SetStream(stream_executor::Stream* stream) override {\n-    return UnimplementedError();\n-  }\n-\n-  absl::Status PotrfBatched(stream_executor::blas::UpperLower uplo, int n,\n-                            stream_executor::DeviceMemory<float*> as, int lda,\n-                            stream_executor::DeviceMemory<int> lapack_info,\n-                            int batch_size) override {\n-    return UnimplementedError();\n-  }\n-  absl::Status PotrfBatched(stream_executor::blas::UpperLower uplo, int n,\n-                            stream_executor::DeviceMemory<double*> as, int lda,\n-                            stream_executor::DeviceMemory<int> lapack_info,\n-                            int batch_size) override {\n-    return UnimplementedError();\n-  }\n-  absl::Status PotrfBatched(\n-      stream_executor::blas::UpperLower uplo, int n,\n-      stream_executor::DeviceMemory<std::complex<float>*> as, int lda,\n-      stream_executor::DeviceMemory<int> lapack_info, int batch_size) override {\n-    return UnimplementedError();\n-  }\n-  absl::Status PotrfBatched(\n-      stream_executor::blas::UpperLower uplo, int n,\n-      stream_executor::DeviceMemory<std::complex<double>*> as, int lda,\n-      stream_executor::DeviceMemory<int> lapack_info, int batch_size) override {\n-    return UnimplementedError();\n-  }\n-\n-  absl::Status Potrf(stream_executor::blas::UpperLower uplo, int n,\n-                     stream_executor::DeviceMemory<float> a, int lda,\n-                     stream_executor::DeviceMemory<int> lapack_info,\n-                     stream_executor::DeviceMemory<float> workspace) override {\n-    return UnimplementedError();\n-  }\n-  absl::Status Potrf(stream_executor::blas::UpperLower uplo, int n,\n-                     stream_executor::DeviceMemory<double> a, int lda,\n-                     stream_executor::DeviceMemory<int> lapack_info,\n-                     stream_executor::DeviceMemory<double> workspace) override {\n-    return UnimplementedError();\n-  }\n-  absl::Status Potrf(\n-      stream_executor::blas::UpperLower uplo, int n,\n-      stream_executor::DeviceMemory<std::complex<float>> a, int lda,\n-      stream_executor::DeviceMemory<int> lapack_info,\n-      stream_executor::DeviceMemory<std::complex<float>> workspace) override {\n-    return UnimplementedError();\n-  }\n-  absl::Status Potrf(\n-      stream_executor::blas::UpperLower uplo, int n,\n-      stream_executor::DeviceMemory<std::complex<double>> a, int lda,\n-      stream_executor::DeviceMemory<int> lapack_info,\n-      stream_executor::DeviceMemory<std::complex<double>> workspace) override {\n-    return UnimplementedError();\n-  }\n-\n-  absl::StatusOr<int64_t> PotrfBufferSize(\n-      xla::PrimitiveType type, stream_executor::blas::UpperLower uplo, int n,\n-      int lda, int batch_size) override {\n-    return 0;\n-  }\n-\n- private:\n-  static absl::Status UnimplementedError() {\n-    return absl::UnimplementedError(\"Not needed for the unit test\");\n-  }\n-};\n-\n-class GpusolverRewriterTest : public HloHardwareIndependentTestBase {\n- public:\n-  GpusolverRewriter gpusolver_rewriter_{GpuSolverContextStub::Create};\n-};\n-\n-TEST_F(GpusolverRewriterTest, CholeskyTest) {\n-  auto module = ParseAndReturnVerifiedModule(R\"(\n-  HloModule CholeskyTest\n-\n-  ENTRY entry_computation {\n-    input = f32[1,256,256] parameter(0)\n-    ROOT decomp = f32[1,256,256] cholesky(input)\n-  }\n-)\")\n-                    .value();\n-\n-  EXPECT_TRUE(gpusolver_rewriter_.Run(module.get()).value());\n-\n-  const HloInstruction* entry_root =\n-      module->entry_computation()->root_instruction();\n-  ASSERT_THAT(\n-      entry_root,\n-      GmockMatch(m::Select(\n-          m::Broadcast(\n-              m::Compare(m::GetTupleElement(), m::Broadcast(m::Constant()))),\n-          m::GetTupleElement(m::CustomCall()), m::Broadcast(m::Constant()))));\n-}\n-}  // namespace\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "c7b7494069b6c052d50dd68d92892acbde56869b",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/630956bed0a5f8d6a3d0bbc29ee6ba26289088cc/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=630956bed0a5f8d6a3d0bbc29ee6ba26289088cc",
            "patch": "@@ -320,34 +320,6 @@ cc_library(\n     alwayslink = True,\n )\n \n-cc_library(\n-    name = \"cuda_solver_context\",\n-    srcs = [\"cuda_solver_context.cc\"],\n-    hdrs = [\"cuda_solver_context.h\"],\n-    tags = [\n-        \"cuda-only\",\n-        \"gpu\",\n-    ],\n-    deps = [\n-        \":cuda_platform_id\",\n-        \"//xla:comparison_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:gpu_solver_context\",\n-        \"//xla/stream_executor:stream\",\n-        \"//xla/stream_executor/platform:platform_object_registry\",\n-        \"//xla/tsl/cuda:cusolver\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@local_config_cuda//cuda:cuda_headers\",\n-    ],\n-    alwayslink = 1,\n-)\n-\n cc_library(\n     name = \"cuda_blas_utils\",\n     srcs = [\"cuda_blas_utils.cc\"],\n@@ -1311,15 +1283,13 @@ cc_library(\n         \":buffer_debug_xor_checksum_kernel_cuda\",\n         \":cublas_plugin\",\n         \":cuda_platform\",\n-        \":cuda_solver_context\",\n         \":cudnn_plugin\",\n         \":cufft_plugin\",\n         \":make_batch_pointers_kernel_cuda\",\n         \":ragged_all_to_all_kernel_cuda\",\n         \":redzone_allocator_kernel_cuda\",\n         \":repeat_buffer_kernel_cuda\",\n         \":topk_kernel_cuda\",\n-        \"//xla/tsl/cuda:cusolver\",\n         \"//xla/tsl/cuda:cusparse\",\n         \"//xla/tsl/cuda:tensorrt_rpath\",\n     ] + [\":cub_sort_kernel_cuda_\" + suffix for suffix in get_cub_sort_kernel_types()],"
        },
        {
            "sha": "f5ee54b01c8e65ca122f3a51589161ad3df7e659",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_solver_context.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 272,
            "changes": 272,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_solver_context.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_solver_context.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_solver_context.cc?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,272 +0,0 @@\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/stream_executor/cuda/cuda_solver_context.h\"\n-\n-#include <algorithm>\n-#include <complex>\n-#include <cstddef>\n-#include <cstdint>\n-#include <memory>\n-\n-#include \"absl/log/log.h\"\n-#include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"third_party/gpus/cuda/include/cuComplex.h\"\n-#include \"third_party/gpus/cuda/include/cusolverDn.h\"\n-#include \"third_party/gpus/cuda/include/cusolver_common.h\"\n-#include \"third_party/gpus/cuda/include/library_types.h\"\n-#include \"xla/primitive_util.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-#include \"xla/stream_executor/platform/platform_object_registry.h\"\n-#include \"xla/stream_executor/stream.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace stream_executor {\n-\n-namespace {\n-\n-// Type traits to get CUDA complex types from std::complex<T>.\n-template <typename T>\n-struct GpuComplexT {\n-  typedef T type;\n-};\n-\n-template <>\n-struct GpuComplexT<std::complex<float>> {\n-  typedef cuComplex type;\n-};\n-template <>\n-struct GpuComplexT<std::complex<double>> {\n-  typedef cuDoubleComplex type;\n-};\n-\n-template <>\n-struct GpuComplexT<std::complex<float>*> {\n-  typedef cuComplex* type;\n-};\n-template <>\n-struct GpuComplexT<std::complex<double>*> {\n-  typedef cuDoubleComplex* type;\n-};\n-\n-template <typename T>\n-inline typename GpuComplexT<T>::type* ToDevicePointer(DeviceMemory<T> p) {\n-  return static_cast<typename GpuComplexT<T>::type*>(p.opaque());\n-}\n-\n-cublasFillMode_t GpuBlasUpperLower(blas::UpperLower uplo) {\n-  switch (uplo) {\n-    case blas::UpperLower::kUpper:\n-      return CUBLAS_FILL_MODE_UPPER;\n-    case blas::UpperLower::kLower:\n-      return CUBLAS_FILL_MODE_LOWER;\n-    default:\n-      LOG(FATAL) << \"Invalid value of blas::UpperLower.\";\n-  }\n-}\n-\n-// Converts a cuSolver absl::Status to a absl::Status.\n-absl::Status ConvertStatus(cusolverStatus_t status) {\n-  switch (status) {\n-    case CUSOLVER_STATUS_SUCCESS:\n-      return absl::OkStatus();\n-    case CUSOLVER_STATUS_NOT_INITIALIZED:\n-      return xla::FailedPrecondition(\"cuSolver has not been initialized\");\n-    case CUSOLVER_STATUS_ALLOC_FAILED:\n-      return xla::ResourceExhausted(\"cuSolver allocation failed\");\n-    case CUSOLVER_STATUS_INVALID_VALUE:\n-      return xla::InvalidArgument(\"cuSolver invalid value error\");\n-    case CUSOLVER_STATUS_ARCH_MISMATCH:\n-      return xla::FailedPrecondition(\"cuSolver architecture mismatch error\");\n-    case CUSOLVER_STATUS_MAPPING_ERROR:\n-      return xla::Unknown(\"cuSolver mapping error\");\n-    case CUSOLVER_STATUS_EXECUTION_FAILED:\n-      return xla::Unknown(\"cuSolver execution failed\");\n-    case CUSOLVER_STATUS_INTERNAL_ERROR:\n-      return xla::Internal(\"cuSolver internal error\");\n-    case CUSOLVER_STATUS_MATRIX_TYPE_NOT_SUPPORTED:\n-      return xla::Unimplemented(\"cuSolver matrix type not supported error\");\n-    case CUSOLVER_STATUS_NOT_SUPPORTED:\n-      return xla::Unimplemented(\"cuSolver not supported error\");\n-    case CUSOLVER_STATUS_ZERO_PIVOT:\n-      return xla::InvalidArgument(\"cuSolver zero pivot error\");\n-    case CUSOLVER_STATUS_INVALID_LICENSE:\n-      return xla::FailedPrecondition(\"cuSolver invalid license error\");\n-    default:\n-      return xla::Unknown(\"Unknown cuSolver error\");\n-  }\n-}\n-\n-}  // namespace\n-\n-absl::StatusOr<std::unique_ptr<GpuSolverContext>> CudaSolverContext::Create() {\n-  cusolverDnHandle_t handle;\n-  TF_RETURN_IF_ERROR(ConvertStatus(cusolverDnCreate(&handle)));\n-  return absl::WrapUnique(new CudaSolverContext(handle));\n-}\n-\n-absl::Status CudaSolverContext::SetStream(Stream* stream) {\n-  return ConvertStatus(cusolverDnSetStream(\n-      handle_,\n-      static_cast<cudaStream_t>(stream->platform_specific_handle().stream)));\n-}\n-\n-CudaSolverContext::CudaSolverContext(cusolverDnHandle_t handle)\n-    : handle_(handle) {}\n-\n-CudaSolverContext::~CudaSolverContext() {\n-  absl::Status status = ConvertStatus(cusolverDnDestroy(handle_));\n-  if (!status.ok()) {\n-    LOG(ERROR) << \"GpuSolverDestroy failed: \" << status;\n-  }\n-}\n-\n-// Note: NVidia have promised that it is safe to pass 'nullptr' as the argument\n-// buffers to cuSolver buffer size methods and this will be a documented\n-// behavior in a future cuSolver release.\n-absl::StatusOr<int64_t> CudaSolverContext::PotrfBufferSize(\n-    xla::PrimitiveType type, blas::UpperLower uplo, int n, int lda,\n-    int batch_size) {\n-  int size = -1;\n-  auto gpu_uplo = GpuBlasUpperLower(uplo);\n-  size_t d_lwork = 0; /* size of workspace */\n-  size_t h_lwork = 0; /* size of workspace */\n-\n-  cudaDataType_t cuda_data_type;\n-  switch (type) {\n-    case xla::F32: {\n-      cuda_data_type = CUDA_R_32F;\n-      break;\n-    }\n-    case xla::F64: {\n-      cuda_data_type = CUDA_R_64F;\n-      break;\n-    }\n-    case xla::C64: {\n-      cuda_data_type = CUDA_C_32F;\n-      break;\n-    }\n-    case xla::C128: {\n-      cuda_data_type = CUDA_C_64F;\n-      break;\n-    }\n-    default:\n-      return xla::InvalidArgument(\"Invalid type for cholesky decomposition: %s\",\n-                                  PrimitiveType_Name(type));\n-  }\n-  TF_RETURN_IF_ERROR(ConvertStatus(cusolverDnXpotrf_bufferSize(\n-      handle_, nullptr, gpu_uplo, n, cuda_data_type, nullptr, lda,\n-      cuda_data_type, &d_lwork, &h_lwork)));\n-  size = static_cast<int>(d_lwork);\n-\n-  // CUDA's potrfBatched needs space for the `as` array, which contains\n-  // batch_size pointers.  Divide by sizeof(type) because this function returns\n-  // not bytes but a number of elements of `type`.\n-  int64_t potrf_batched_scratch = xla::CeilOfRatio<int64_t>(\n-      batch_size * sizeof(void*), xla::primitive_util::ByteWidth(type));\n-\n-  return std::max<int64_t>(size, potrf_batched_scratch);\n-}\n-\n-absl::Status CudaSolverContext::PotrfBatched(blas::UpperLower uplo, int n,\n-                                             DeviceMemory<float*> as, int lda,\n-                                             DeviceMemory<int> lapack_info,\n-                                             int batch_size) {\n-  return ConvertStatus(cusolverDnSpotrfBatched(\n-      handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n-      ToDevicePointer(lapack_info), batch_size));\n-}\n-\n-absl::Status CudaSolverContext::PotrfBatched(blas::UpperLower uplo, int n,\n-                                             DeviceMemory<double*> as, int lda,\n-                                             DeviceMemory<int> lapack_info,\n-                                             int batch_size) {\n-  return ConvertStatus(cusolverDnDpotrfBatched(\n-      handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n-      ToDevicePointer(lapack_info), batch_size));\n-}\n-\n-absl::Status CudaSolverContext::PotrfBatched(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<float>*> as,\n-    int lda, DeviceMemory<int> lapack_info, int batch_size) {\n-  return ConvertStatus(cusolverDnCpotrfBatched(\n-      handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n-      ToDevicePointer(lapack_info), batch_size));\n-}\n-\n-absl::Status CudaSolverContext::PotrfBatched(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<double>*> as,\n-    int lda, DeviceMemory<int> lapack_info, int batch_size) {\n-  return ConvertStatus(cusolverDnZpotrfBatched(\n-      handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n-      ToDevicePointer(lapack_info), batch_size));\n-}\n-\n-absl::Status CudaSolverContext::Potrf(blas::UpperLower uplo, int n,\n-                                      DeviceMemory<double> a, int lda,\n-                                      DeviceMemory<int> lapack_info,\n-                                      DeviceMemory<double> workspace) {\n-  absl::Status status = ConvertStatus(cusolverDnXpotrf(\n-      handle_, nullptr, GpuBlasUpperLower(uplo), n, CUDA_R_64F,\n-      ToDevicePointer(a), lda, CUDA_R_64F, ToDevicePointer(workspace),\n-      workspace.ElementCount(), nullptr, 0, ToDevicePointer(lapack_info)));\n-  return status;\n-}\n-\n-absl::Status CudaSolverContext::Potrf(blas::UpperLower uplo, int n,\n-                                      DeviceMemory<float> a, int lda,\n-                                      DeviceMemory<int> lapack_info,\n-                                      DeviceMemory<float> workspace) {\n-  absl::Status status = ConvertStatus(cusolverDnXpotrf(\n-      handle_, nullptr, GpuBlasUpperLower(uplo), n, CUDA_R_32F,\n-      ToDevicePointer(a), lda, CUDA_R_32F, ToDevicePointer(workspace),\n-      workspace.ElementCount(), nullptr, 0, ToDevicePointer(lapack_info)));\n-  return status;\n-}\n-\n-absl::Status CudaSolverContext::Potrf(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<float>> a, int lda,\n-    DeviceMemory<int> lapack_info,\n-    DeviceMemory<std::complex<float>> workspace) {\n-  absl::Status status = ConvertStatus(cusolverDnXpotrf(\n-      handle_, nullptr, GpuBlasUpperLower(uplo), n, CUDA_C_32F,\n-      ToDevicePointer(a), lda, CUDA_C_32F, ToDevicePointer(workspace),\n-      workspace.ElementCount(), nullptr, 0, ToDevicePointer(lapack_info)));\n-  return status;\n-}\n-\n-absl::Status CudaSolverContext::Potrf(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<double>> a, int lda,\n-    DeviceMemory<int> lapack_info,\n-    DeviceMemory<std::complex<double>> workspace) {\n-  absl::Status status = ConvertStatus(cusolverDnXpotrf(\n-      handle_, nullptr, GpuBlasUpperLower(uplo), n, CUDA_C_64F,\n-      ToDevicePointer(a), lda, CUDA_C_64F, ToDevicePointer(workspace),\n-      workspace.ElementCount(), nullptr, 0, ToDevicePointer(lapack_info)));\n-  return status;\n-}\n-\n-STREAM_EXECUTOR_REGISTER_OBJECT_STATICALLY(CudaSolverContextFactory,\n-                                           GpuSolverContextFactory,\n-                                           cuda::kCudaPlatformId,\n-                                           CudaSolverContext::Create);\n-\n-}  // namespace stream_executor"
        },
        {
            "sha": "15c3a32660160256391d98045861ab27b5bd867c",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_solver_context.h",
            "status": "removed",
            "additions": 0,
            "deletions": 83,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_solver_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_solver_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_solver_context.h?ref=ac2bbd5b1cfca70a4fb5598f0d6e0e1553cb1fa5",
            "patch": "@@ -1,83 +0,0 @@\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_STREAM_EXECUTOR_CUDA_CUDA_SOLVER_CONTEXT_H_\n-#define XLA_STREAM_EXECUTOR_CUDA_CUDA_SOLVER_CONTEXT_H_\n-\n-#include <complex>\n-#include <cstdint>\n-#include <memory>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"third_party/gpus/cuda/include/cusolverDn.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/gpu_solver_context.h\"\n-#include \"xla/stream_executor/stream.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace stream_executor {\n-\n-class CudaSolverContext : public GpuSolverContext {\n- public:\n-  static absl::StatusOr<std::unique_ptr<GpuSolverContext>> Create();\n-\n-  ~CudaSolverContext() override;\n-\n-  absl::Status SetStream(Stream* stream) override;\n-  absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<float*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n-                            int batch_size) override;\n-  absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<double*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n-                            int batch_size) override;\n-  absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<std::complex<float>*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n-                            int batch_size) override;\n-  absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<std::complex<double>*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n-                            int batch_size) override;\n-  absl::Status Potrf(blas::UpperLower uplo, int n, DeviceMemory<float> a,\n-                     int lda, DeviceMemory<int> lapack_info,\n-                     DeviceMemory<float> workspace) override;\n-  absl::Status Potrf(blas::UpperLower uplo, int n, DeviceMemory<double> a,\n-                     int lda, DeviceMemory<int> lapack_info,\n-                     DeviceMemory<double> workspace) override;\n-  absl::Status Potrf(blas::UpperLower uplo, int n,\n-                     DeviceMemory<std::complex<float>> a, int lda,\n-                     DeviceMemory<int> lapack_info,\n-                     DeviceMemory<std::complex<float>> workspace) override;\n-  absl::Status Potrf(blas::UpperLower uplo, int n,\n-                     DeviceMemory<std::complex<double>> a, int lda,\n-                     DeviceMemory<int> lapack_info,\n-                     DeviceMemory<std::complex<double>> workspace) override;\n-  absl::StatusOr<int64_t> PotrfBufferSize(xla::PrimitiveType type,\n-                                          blas::UpperLower uplo, int n, int lda,\n-                                          int batch_size) override;\n-\n- private:\n-  explicit CudaSolverContext(cusolverDnHandle_t handle);\n-\n-  cusolverDnHandle_t handle_;\n-};\n-\n-}  // namespace stream_executor\n-\n-#endif  // XLA_STREAM_EXECUTOR_CUDA_CUDA_SOLVER_CONTEXT_H_"
        }
    ],
    "stats": {
        "total": 1438,
        "additions": 6,
        "deletions": 1432
    }
}