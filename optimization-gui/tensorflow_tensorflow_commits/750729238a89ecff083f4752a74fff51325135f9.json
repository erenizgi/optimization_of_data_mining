{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 850609638",
    "sha": "750729238a89ecff083f4752a74fff51325135f9",
    "files": [
        {
            "sha": "a60ce5ae8051509e7c9058b23da7f2b6026777b8",
            "filename": "tensorflow/core/kernels/linalg/cholesky_op_gpu.cu.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fcholesky_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fcholesky_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fcholesky_op_gpu.cu.cc?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -155,7 +155,7 @@ class CholeskyOpGpu : public AsyncOpKernel {\n     if (use_batched_solver) {\n       // For small matrices or large batch sizes, we use the batched interface\n       // from cuSolver.\n-      auto output_reshaped_ptrs = solver->GetScratchSpace<uint8>(\n+      auto output_reshaped_ptrs = solver->GetScratchSpace<uint8_t>(\n           sizeof(Scalar*) * batch_size, \"input_copt_ptrs\",\n           /* on_host */ true);\n       const Scalar** output_reshaped_ptrs_base =\n@@ -193,7 +193,7 @@ class CholeskyOpGpu : public AsyncOpKernel {\n \n     // Register callback to check info after kernels finish.\n     auto info_checker = [context, done, n](\n-                            const Status& status,\n+                            const absl::Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n       if (!status.ok() && absl::IsInvalidArgument(status) &&\n           !host_infos.empty()) {"
        },
        {
            "sha": "29b84e56c07f883abf86d826ef149e0444b1d084",
            "filename": "tensorflow/core/kernels/linalg/determinant_op_gpu.cu.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fdeterminant_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fdeterminant_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fdeterminant_op_gpu.cu.cc?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -85,8 +85,8 @@ struct DeterminantFromPivotedLUFunctor<GPUDevice, Scalar> {\n                   typename TTypes<Scalar, 3>::ConstTensor lu_factor,\n                   const int* pivots, typename TTypes<Scalar, 1>::Tensor output,\n                   int* info) {\n-    const int64 num_matrices = output.size();\n-    const int64 n = lu_factor.dimension(2);\n+    const int64_t num_matrices = output.size();\n+    const int64_t n = lu_factor.dimension(2);\n     GpuLaunchConfig config = GetGpuLaunchConfig(num_matrices, device);\n \n     TF_CHECK_OK(GpuLaunchKernel(\n@@ -108,8 +108,8 @@ struct LogDeterminantFromPivotedLUFunctor<GPUDevice, Scalar> {\n                   typename TTypes<Scalar, 3>::ConstTensor lu_factor,\n                   const int* pivots, typename TTypes<Scalar, 1>::Tensor sign,\n                   typename TTypes<Scalar, 1>::Tensor log_abs_det) {\n-    const int64 num_matrices = sign.size();\n-    const int64 n = lu_factor.dimension(2);\n+    const int64_t num_matrices = sign.size();\n+    const int64_t n = lu_factor.dimension(2);\n     GpuLaunchConfig config = GetGpuLaunchConfig(num_matrices, device);\n     TF_CHECK_OK(GpuLaunchKernel(\n         DeterminantFromPivotedLUKernel<Scalar, /*compute_log_abs_det=*/true>,"
        },
        {
            "sha": "7571f4bbdef2ac7eff07d6f9d8a0d653ea482e8c",
            "filename": "tensorflow/core/kernels/linalg/lu_op_gpu.cu.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Flu_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Flu_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Flu_op_gpu.cu.cc?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -38,7 +38,7 @@ typedef Eigen::GpuDevice GPUDevice;\n namespace {\n template <typename Scalar>\n __device__ void ComputePermutationFromTranspositions(\n-    int64 num_rows, const int* __restrict__ pivots,\n+    int64_t num_rows, const int* __restrict__ pivots,\n     Scalar* __restrict__ permutation_indices) {\n   // Fill in the output array with the identity permutation.\n   for (int i = 0; i < num_rows; ++i) {\n@@ -63,7 +63,7 @@ __device__ void ComputePermutationFromTranspositions(\n // transpositions.\n template <typename Scalar>\n __global__ void ComputePermutationFromTranspositionsKernel(\n-    GpuLaunchConfig config, const int64 num_rows,\n+    GpuLaunchConfig config, const int64_t num_rows,\n     const int* __restrict__ all_pivots,\n     Scalar* __restrict__ all_permutation_indices) {\n   // We only parallelize over batches here. Performance is not critical,\n@@ -92,8 +92,8 @@ class LuOpGpu : public AsyncOpKernel {\n         errors::InvalidArgument(\"Input must have rank >= 2, got \", input_rank),\n         done);\n \n-    const int64 num_rows = input.dim_size(input_rank - 2);\n-    const int64 num_cols = input.dim_size(input_rank - 1);\n+    const int64_t num_rows = input.dim_size(input_rank - 2);\n+    const int64_t num_cols = input.dim_size(input_rank - 1);\n \n     OP_REQUIRES_ASYNC(\n         context, num_rows == num_cols,\n@@ -156,17 +156,17 @@ class LuOpGpu : public AsyncOpKernel {\n     auto packed_triangular_factors_transpose_reshaped =\n         packed_triangular_factors_transpose\n             .template flat_inner_dims<Scalar, 3>();\n-    const int64 batch_size =\n+    const int64_t batch_size =\n         packed_triangular_factors_transpose_reshaped.dimension(0);\n \n     // Allocate pivots on the device.\n     Tensor pivots;\n     OP_REQUIRES_OK_ASYNC(context,\n                          solver->allocate_scoped_tensor(\n-                             DataTypeToEnum<int32>::value,\n+                             DataTypeToEnum<int32_t>::value,\n                              TensorShape{batch_size, num_rows}, &pivots),\n                          done);\n-    auto pivots_mat = pivots.template matrix<int32>();\n+    auto pivots_mat = pivots.template matrix<int32_t>();\n \n     // Transpose the input. This is necessary because cuBLAS assumes\n     // column-major storage while TensorFlow uses row-major.\n@@ -180,7 +180,7 @@ class LuOpGpu : public AsyncOpKernel {\n     if (num_rows == num_cols && num_rows / batch_size <= 128) {\n       // For small matrices or large batch sizes, we use the batched\n       // interface from cuBlas.\n-      auto packed_triangular_factors_ptrs = solver->GetScratchSpace<uint8>(\n+      auto packed_triangular_factors_ptrs = solver->GetScratchSpace<uint8_t>(\n           sizeof(Scalar*) * batch_size, \"packed_triangular_factors_ptrs\",\n           /* on_host */ true);\n \n@@ -237,7 +237,7 @@ class LuOpGpu : public AsyncOpKernel {\n     // kernels run.\n     // TODO(rmlarsen): Use move capture once C++14 becomes available.\n     auto info_checker = [context, done, dev_info](\n-                            const Status& status,\n+                            const absl::Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n       if (!status.ok() && absl::IsInvalidArgument(status) &&\n           !host_infos.empty()) {\n@@ -265,15 +265,15 @@ class LuOpGpu : public AsyncOpKernel {\n                               .TypeConstraint<idx_type>(\"output_idx_type\"), \\\n                           LuOpGpu<type, idx_type>);\n \n-REGISTER_LU_GPU(float, int32);\n-REGISTER_LU_GPU(double, int32);\n-REGISTER_LU_GPU(complex64, int32);\n-REGISTER_LU_GPU(complex128, int32);\n+REGISTER_LU_GPU(float, int32_t);\n+REGISTER_LU_GPU(double, int32_t);\n+REGISTER_LU_GPU(complex64, int32_t);\n+REGISTER_LU_GPU(complex128, int32_t);\n \n-REGISTER_LU_GPU(float, int64);\n-REGISTER_LU_GPU(double, int64);\n-REGISTER_LU_GPU(complex64, int64);\n-REGISTER_LU_GPU(complex128, int64);\n+REGISTER_LU_GPU(float, int64_t);\n+REGISTER_LU_GPU(double, int64_t);\n+REGISTER_LU_GPU(complex64, int64_t);\n+REGISTER_LU_GPU(complex128, int64_t);\n }  // namespace tensorflow\n \n #endif  // GOOGLE_CUDA"
        },
        {
            "sha": "2d82d09bc572fcf3af739f79156423973f0fe33b",
            "filename": "tensorflow/core/kernels/linalg/matrix_inverse_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_inverse_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_inverse_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_inverse_op.cc?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -199,10 +199,10 @@ class MatrixInverseOpGpu : public AsyncOpKernel {\n                                        TensorShape{batch_size, n}, &pivots),\n         done);\n     auto pivots_mat = pivots.template matrix<int>();\n-    auto input_copy_ptr_array = solver->GetScratchSpace<uint8>(\n+    auto input_copy_ptr_array = solver->GetScratchSpace<uint8_t>(\n         sizeof(Scalar*) * batch_size, \"input_copy_ptr_array\",\n         /* on_host */ true);\n-    auto output_ptr_array = solver->GetScratchSpace<uint8>(\n+    auto output_ptr_array = solver->GetScratchSpace<uint8_t>(\n         sizeof(Scalar*) * batch_size, \"output_copy_ptr_array\",\n         /* on_host */ true);\n     auto output_reshaped = output->template flat_inner_dims<Scalar, 3>();\n@@ -290,7 +290,7 @@ class MatrixInverseOpGpu : public AsyncOpKernel {\n     }\n     // Callback for checking info after kernels finish.\n     auto info_checker = [context, done](\n-                            const Status& status,\n+                            const absl::Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n       if (!status.ok() && absl::IsInvalidArgument(status)) {\n         for (const auto& host_info : host_infos) {"
        },
        {
            "sha": "b42e330d4dd5ba700543302956a8df0fc4133fa0",
            "filename": "tensorflow/core/kernels/linalg/matrix_triangular_solve_op_impl.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_triangular_solve_op_impl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_triangular_solve_op_impl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_triangular_solve_op_impl.h?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -46,7 +46,7 @@ typedef Eigen::GpuDevice GPUDevice;\n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n template <typename Scalar>\n se::DeviceMemory<Scalar> AsDeviceMemory(const Scalar* gpu_memory) {\n-  se::DeviceMemoryBase wrapped(const_cast<Scalar*>(gpu_memory));\n+  stream_executor::DeviceAddressBase wrapped(const_cast<Scalar*>(gpu_memory));\n   se::DeviceMemory<Scalar> typed(wrapped);\n   return typed;\n }\n@@ -260,8 +260,8 @@ struct LaunchBatchMatrixTriangularSolve<GPUDevice, Scalar> {\n                      const MatMulBCast& bcast, Tensor* out) {\n     auto* stream = context->op_device_context()->stream();\n \n-    const uint64 m = in_x.dim_size(1);\n-    const uint64 n = out->dim_size(2);\n+    const uint64_t m = in_x.dim_size(1);\n+    const uint64_t n = out->dim_size(2);\n \n     //  Do a memcpy when we don't need to broadcast.\n     if (!bcast.IsBroadcastingRequired() || out->shape() == in_y.shape()) {\n@@ -329,10 +329,10 @@ struct LaunchBatchMatrixTriangularSolve<GPUDevice, Scalar> {\n #endif\n \n     auto solver = absl::make_unique<GpuSolver>(context);\n-    const uint64 leading_dim_matrix = m;\n-    const uint64 leading_dim_output = n;\n-    const uint64 colmajor_rows = n;\n-    const uint64 colmajor_cols = m;\n+    const uint64_t leading_dim_matrix = m;\n+    const uint64_t leading_dim_output = n;\n+    const uint64_t colmajor_rows = n;\n+    const uint64_t colmajor_cols = m;\n \n     const int64_t batch_size = bcast.output_batch_size();\n     std::vector<const Scalar*> a_ptrs;"
        },
        {
            "sha": "ab25adcf8595d18fb90bc90a339246ee1e04ad51",
            "filename": "tensorflow/core/kernels/linalg/svd_op_gpu.cu.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fsvd_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fsvd_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fsvd_op_gpu.cu.cc?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -61,8 +61,8 @@ namespace {\n // The result is stored in V[batch] and has the same sign as the\n // real value of V (which should be computed)\n template <class Scalar>\n-__global__ void ComputeValueOfVKernel(Gpu2DLaunchConfig config, int64 m,\n-                                      int64 ldu, const Scalar* __restrict__ M,\n+__global__ void ComputeValueOfVKernel(Gpu2DLaunchConfig config, int64_t m,\n+                                      int64_t ldu, const Scalar* __restrict__ M,\n                                       const Scalar* __restrict__ U,\n                                       const Scalar* __restrict__ S,\n                                       Scalar* __restrict__ V) {\n@@ -96,8 +96,8 @@ class SvdOpGpu : public AsyncOpKernel {\n     OP_REQUIRES_OK(context, context->GetAttr(\"full_matrices\", &full_matrices_));\n   }\n \n-  void RunSVD(OpKernelContext* context, DoneCallback done, int64 m, int64 n,\n-              int64 p, Tensor& M_copy, Tensor* S, Tensor* U, Tensor* V,\n+  void RunSVD(OpKernelContext* context, DoneCallback done, int64_t m, int64_t n,\n+              int64_t p, Tensor& M_copy, Tensor* S, Tensor* U, Tensor* V,\n               std::unique_ptr<GpuSolver> solver) {\n     // Compute U S V* = M.\n     // 1. cuSolver works in column-major rather than row-major.\n@@ -111,7 +111,7 @@ class SvdOpGpu : public AsyncOpKernel {\n     RealScalar* outputS_ptr;\n     auto input_reshaped = M_copy.template flat_inner_dims<Scalar, 3>();\n     input_ptr = input_reshaped.data();\n-    const int64 batch_size =\n+    const int64_t batch_size =\n         M_copy.dims() > 2 ? input_reshaped.dimension(0) : 1;\n     // Gesvdjbatched handles matrices up to 32x32.\n     // TODO(jamessspencer): if not full_matrices, compute full U and V matrices\n@@ -201,7 +201,7 @@ class SvdOpGpu : public AsyncOpKernel {\n       eigen_assert(false && \"not supported\");\n #endif\n     } else {\n-      for (int64 batch = 0; batch < batch_size; ++batch) {\n+      for (int64_t batch = 0; batch < batch_size; ++batch) {\n         Scalar* input = input_ptr + batch * m * n;\n         RealScalar* outputS = outputS_ptr + batch * p;\n         Scalar* outputU = NULL;\n@@ -277,9 +277,9 @@ class SvdOpGpu : public AsyncOpKernel {\n                    const std::vector<DeviceLapackInfo>& dev_info,\n                    std::unique_ptr<GpuSolver> solver) {\n     auto info_checker = [context, done](\n-                            const Status& status,\n+                            const absl::Status& status,\n                             const std::vector<HostLapackInfo>& /* unused */) {\n-      Status full_status = status;\n+      absl::Status full_status = status;\n       if (!full_status.ok()) {\n         full_status.Update(errors::InvalidArgument(kErrMsg));\n       }\n@@ -294,9 +294,9 @@ class SvdOpGpu : public AsyncOpKernel {\n   // The SVD if m >= n\n   // TODO: can the two cases (MgeqN and MlessN) be simplified,\n   //   common boilerplate be reduced, or even combined in one method?\n-  void PerformSVD_MgeqN(OpKernelContext* context, DoneCallback done, int64 m,\n-                        int64 n, int64 p, const Tensor& M, Tensor* S, Tensor* U,\n-                        Tensor* V) {\n+  void PerformSVD_MgeqN(OpKernelContext* context, DoneCallback done, int64_t m,\n+                        int64_t n, int64_t p, const Tensor& M, Tensor* S,\n+                        Tensor* U, Tensor* V) {\n     // Transpose M, because cuSolver expects it to be column-major\n     TensorShape shapeRaw = M.shape();\n     shapeRaw.RemoveLastDims(2);\n@@ -319,8 +319,8 @@ class SvdOpGpu : public AsyncOpKernel {\n   }\n \n   // The SVD if m < n\n-  void PerformSVD_MlessN(OpKernelContext* context, DoneCallback done, int64 m,\n-                         int64 n, int64 p, const Tensor& M, Tensor* S,\n+  void PerformSVD_MlessN(OpKernelContext* context, DoneCallback done, int64_t m,\n+                         int64_t n, int64_t p, const Tensor& M, Tensor* S,\n                          Tensor* U, Tensor* V) {\n     // Perform the SVD on M'. cuSolver works column major so don't need to\n     // transpose M.\n@@ -358,9 +358,9 @@ class SvdOpGpu : public AsyncOpKernel {\n         errors::InvalidArgument(\"Input must have rank >= 2, got \", ndims),\n         done);\n \n-    const int64 m = input.dim_size(ndims - 2);\n-    const int64 n = input.dim_size(ndims - 1);\n-    const int64 p = std::min(m, n);\n+    const int64_t m = input.dim_size(ndims - 2);\n+    const int64_t n = input.dim_size(ndims - 1);\n+    const int64_t p = std::min(m, n);\n \n     if (n == 1) {\n       OP_REQUIRES_ASYNC("
        },
        {
            "sha": "7645341cb0fc4d8587d7ca1b2f7eb6b72b161b5e",
            "filename": "tensorflow/core/kernels/linalg/tridiagonal_matmul_op_gpu.cu.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Ftridiagonal_matmul_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Ftridiagonal_matmul_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Ftridiagonal_matmul_op_gpu.cu.cc?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -70,7 +70,7 @@ class TridiagonalMatMulOpGpu : public OpKernel {\n     OP_REQUIRES_OK(context, ValidateInputTensor(superdiag, \"superdiag\", rhs));\n     OP_REQUIRES_OK(context, ValidateInputTensor(maindiag, \"maindiag\", rhs));\n     OP_REQUIRES_OK(context, ValidateInputTensor(subdiag, \"subdiag\", rhs));\n-    int64 batch_size = 1;\n+    int64_t batch_size = 1;\n     for (int i = 0; i < ndims - 2; i++) {\n       batch_size *= rhs.dim_size(i);\n     }\n@@ -91,9 +91,9 @@ class TridiagonalMatMulOpGpu : public OpKernel {\n   }\n \n  private:\n-  Status ValidateInputTensor(const Tensor& tensor,\n-                             const std::string& tensor_name,\n-                             const Tensor& rhs) {\n+  absl::Status ValidateInputTensor(const Tensor& tensor,\n+                                   const std::string& tensor_name,\n+                                   const Tensor& rhs) {\n     const int ndims = rhs.dims();\n     if (tensor.dims() != ndims) {\n       return errors::InvalidArgument(tensor_name,\n@@ -120,7 +120,7 @@ class TridiagonalMatMulOpGpu : public OpKernel {\n                                      tensor.dim_size(ndims - 1), \" and \",\n                                      rhs.dim_size(ndims - 2));\n     }\n-    return OkStatus();\n+    return absl::OkStatus();\n   }\n };\n "
        },
        {
            "sha": "4758dc5e5d37886064006bbf2d07065a3a82ea1d",
            "filename": "tensorflow/core/kernels/linalg/tridiagonal_solve_op_gpu.cu.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Ftridiagonal_solve_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/750729238a89ecff083f4752a74fff51325135f9/tensorflow%2Fcore%2Fkernels%2Flinalg%2Ftridiagonal_solve_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Ftridiagonal_solve_op_gpu.cu.cc?ref=750729238a89ecff083f4752a74fff51325135f9",
            "patch": "@@ -64,7 +64,7 @@ __global__ void SolveForSizeOneOrTwoKernel(const int m,\n \n template <typename Scalar>\n se::DeviceMemory<Scalar> AsDeviceMemory(const Scalar* cuda_memory) {\n-  se::DeviceMemoryBase wrapped(const_cast<Scalar*>(cuda_memory));\n+  stream_executor::DeviceAddressBase wrapped(const_cast<Scalar*>(cuda_memory));\n   se::DeviceMemory<Scalar> typed(wrapped);\n   return typed;\n }\n@@ -253,9 +253,9 @@ class TridiagonalSolveOpGpu : public OpKernel {\n     const Tensor& lhs = context->input(0);\n     const Tensor& rhs = context->input(1);\n     const int ndims = lhs.dims();\n-    const int64 num_rhs = rhs.dim_size(rhs.dims() - 1);\n-    const int64 matrix_size = lhs.dim_size(ndims - 1);\n-    int64 batch_size = 1;\n+    const int64_t num_rhs = rhs.dim_size(rhs.dims() - 1);\n+    const int64_t matrix_size = lhs.dim_size(ndims - 1);\n+    int64_t batch_size = 1;\n     for (int i = 0; i < ndims - 2; i++) {\n       batch_size *= lhs.dim_size(i);\n     }\n@@ -343,7 +343,7 @@ class TridiagonalSolveOpGpu : public OpKernel {\n       dims.push_back(lhs.dim_size(index));\n     }\n     TensorShape lhs_transposed_shape(\n-        gtl::ArraySlice<int64_t>(dims.data(), ndims));\n+        absl::Span<const int64_t>(dims.data(), ndims));\n \n     std::unique_ptr<GpuSolver> cublas_solver(new GpuSolver(context));\n     OP_REQUIRES_OK(context, cublas_solver->allocate_scoped_tensor(\n@@ -352,7 +352,7 @@ class TridiagonalSolveOpGpu : public OpKernel {\n     auto device = context->eigen_device<Eigen::GpuDevice>();\n     OP_REQUIRES_OK(\n         context,\n-        DoTranspose(device, lhs, gtl::ArraySlice<int>(perm.data(), ndims),\n+        DoTranspose(device, lhs, absl::Span<const int>(perm.data(), ndims),\n                     &lhs_transposed));\n   }\n "
        }
    ],
    "stats": {
        "total": 120,
        "additions": 60,
        "deletions": 60
    }
}