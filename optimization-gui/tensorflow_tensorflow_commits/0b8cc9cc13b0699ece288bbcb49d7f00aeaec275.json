{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Implement Legacy Autotuner cache for backward compatibility.\n\nPiperOrigin-RevId: 804382084",
    "sha": "0b8cc9cc13b0699ece288bbcb49d7f00aeaec275",
    "files": [
        {
            "sha": "9aee3c0d670d123c82d20b0ba19096f91d123a59",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 40,
            "deletions": 1,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=0b8cc9cc13b0699ece288bbcb49d7f00aeaec275",
            "patch": "@@ -1,6 +1,6 @@\n load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm_is_configured\")\n-load(\"//xla:xla.default.bzl\", \"xla_cc_binary\")\n+load(\"//xla:xla.default.bzl\", \"xla_cc_binary\", \"xla_cc_test\")\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\n     \"//xla/tsl/platform/default:cuda_build_defs.bzl\",\n@@ -729,6 +729,45 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"legacy_cache\",\n+    srcs = [\"legacy_cache.cc\"],\n+    hdrs = [\"legacy_cache.h\"],\n+    deps = [\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:autotuner_cache_interface\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/autotuning:autotune_cache_key\",\n+        \"//xla/service/gpu/autotuning:autotuner_util\",\n+        \"//xla/stream_executor:device_description\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_protobuf//:duration_cc_proto\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"legacy_cache_test\",\n+    srcs = [\"legacy_cache_test.cc\"],\n+    deps = [\n+        \":legacy_cache\",\n+        \"//xla:literal_util\",\n+        \"//xla/backends/autotuner:autotuner_cache_interface\",\n+        \"//xla/backends/autotuner:autotuner_cache_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_protobuf//:any_cc_proto\",\n+        \"@local_tsl//tsl/platform:path\",\n+    ],\n+)\n+\n xla_cc_binary(\n     name = \"autotuner_main\",\n     srcs = [\"autotuner_main.cc\"],"
        },
        {
            "sha": "8ed1d0795f190b83d1dc114ea8b68206b8797eaa",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.cc",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc?ref=0b8cc9cc13b0699ece288bbcb49d7f00aeaec275",
            "patch": "@@ -0,0 +1,105 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/legacy_cache.h\"\n+\n+#include <optional>\n+\n+#include \"google/protobuf/duration.pb.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n+#include \"xla/service/gpu/autotuning/autotuner_util.h\"\n+\n+namespace xla {\n+\n+namespace gpu {\n+\n+std::optional<LegacyCache::Config> LegacyCache::Lookup(\n+    const HloInstruction* instr) {\n+  AutotuneCacheKey key = GetAutotuneCacheKey(*instr);\n+  absl::StatusOr<std::optional<AutotuneResult>> result =\n+      AutotunerUtil::TryFindInCache(key, cache_dir_);\n+  if (!result.ok()) {\n+    LOG(ERROR) << \"Failed to lookup autotune cache: \" << result.status();\n+    return std::nullopt;\n+  }\n+  if (!result->has_value()) {\n+    return std::nullopt;\n+  }\n+  return GetConfig(result->value());\n+}\n+\n+absl::Status LegacyCache::Insert(const HloInstruction* instr,\n+                                 Config& best_config) {\n+  AutotuneCacheKey key = GetAutotuneCacheKey(*instr);\n+  std::optional<AutotuneResult> opt_result = GetAutotuneResult(best_config);\n+  if (!opt_result.has_value()) {\n+    return absl::OkStatus();\n+  }\n+  absl::StatusOr<AutotunerUtil::ResultAndInserted> result_and_inserted =\n+      AutotunerUtil::AddResultToCaches(key, opt_result.value(), cache_dir_,\n+                                       cache_mode_);\n+  if (!result_and_inserted.ok()) {\n+    LOG(ERROR) << \"Failed to insert autotune cache: \"\n+               << result_and_inserted.status();\n+    return result_and_inserted.status();\n+  }\n+  return absl::OkStatus();\n+}\n+\n+AutotuneCacheKey LegacyCache::GetAutotuneCacheKey(const HloInstruction& instr) {\n+  AutotuneCacheKey key(device_desc_, instr);\n+  return key;\n+}\n+\n+std::optional<LegacyCache::Config> LegacyCache::GetConfig(\n+    const AutotuneResult& result) {\n+  Config config;\n+  if (result.has_triton()) {\n+    config.codegen_backend_name = \"Triton\";\n+    config.backend_config.PackFrom(result.triton());\n+  } else if (result.has_gemm()) {\n+    config.codegen_backend_name = \"Cublas\";\n+    config.backend_config.PackFrom(result.gemm());\n+  } else if (result.has_conv()) {\n+    config.codegen_backend_name = \"Cudnn\";\n+    config.backend_config.PackFrom(result.conv());\n+  } else {\n+    return std::nullopt;\n+  }\n+  return config;\n+}\n+\n+std::optional<AutotuneResult> LegacyCache::GetAutotuneResult(\n+    const LegacyCache::Config& config) {\n+  AutotuneResult result;\n+  if (config.codegen_backend_name == \"Triton\") {\n+    config.backend_config.UnpackTo(result.mutable_triton());\n+  } else if (config.codegen_backend_name == \"Cublas\") {\n+    config.backend_config.UnpackTo(result.mutable_gemm());\n+  } else if (config.codegen_backend_name == \"Cudnn\") {\n+    config.backend_config.UnpackTo(result.mutable_conv());\n+  } else {\n+    return std::nullopt;\n+  }\n+  return result;\n+}\n+\n+}  // namespace gpu\n+\n+}  // namespace xla"
        },
        {
            "sha": "5fe9925e129269976defb0c6afbb31f609bff2ec",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.h",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h?ref=0b8cc9cc13b0699ece288bbcb49d7f00aeaec275",
            "patch": "@@ -0,0 +1,67 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_AUTOTUNER_LEGACY_CACHE_H_\n+#define XLA_BACKENDS_GPU_AUTOTUNER_LEGACY_CACHE_H_\n+\n+#include <optional>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/status/status.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+\n+namespace gpu {\n+\n+// Wrapper around the legacy autotune cache from the AutotunerUtil which uses\n+// AutotuneResult proto. The insert for backends which are not supported in\n+// AutotuneResult proto will be a no-op. The lookup will return nullopt if the\n+// backend is not supported in AutotuneResult proto.\n+class LegacyCache : public AutotunerCacheInterface {\n+ public:\n+  LegacyCache(std::string cache_dir, DebugOptions::AutotuneCacheMode cache_mode,\n+              se::DeviceDescription device_desc)\n+      : cache_dir_(std::move(cache_dir)),\n+        cache_mode_(cache_mode),\n+        device_desc_(std::move(device_desc)) {}\n+  std::optional<Config> Lookup(const HloInstruction* instr) override;\n+  absl::Status Insert(const HloInstruction* instr,\n+                      Config& best_config) override;\n+\n+ private:\n+  AutotuneCacheKey GetAutotuneCacheKey(const HloInstruction& instr);\n+\n+  // Translates between the AutotunerCacheInterface::Config and the\n+  // AutotuneResult.\n+  std::optional<Config> GetConfig(const AutotuneResult& result);\n+  std::optional<AutotuneResult> GetAutotuneResult(const Config& config);\n+\n+  const std::string cache_dir_;\n+  const DebugOptions::AutotuneCacheMode cache_mode_;\n+  const se::DeviceDescription device_desc_;\n+};\n+\n+}  // namespace gpu\n+\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_AUTOTUNER_LEGACY_CACHE_H_"
        },
        {
            "sha": "df6d5fc408a8615270b9a7e465078a51ce122df0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache_test.cc",
            "status": "added",
            "additions": 228,
            "deletions": 0,
            "changes": 228,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc?ref=0b8cc9cc13b0699ece288bbcb49d7f00aeaec275",
            "patch": "@@ -0,0 +1,228 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/legacy_cache.h\"\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+\n+#include \"google/protobuf/any.pb.h\"\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n+#include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/env.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+using Config = ::xla::AutotunerCacheInterface::Config;\n+using ::testing::Eq;\n+using ::testing::Optional;\n+\n+// Helper to create a dummy DeviceDescription.\n+se::DeviceDescription CreateDummyDeviceDescription(\n+    std::string name = \"test_device\") {\n+  se::DeviceDescription desc;\n+  desc.set_name(name);\n+  desc.set_device_vendor(\"NVIDIA\");\n+  desc.set_platform_version(\"CUDA 12.0\");\n+  desc.set_gpu_compute_capability(se::CudaComputeCapability(8, 0));\n+  desc.set_core_count(108);\n+  desc.set_clock_rate_ghz(name == \"test_device\" ? 1.98 : 2.00);\n+  desc.set_memory_bandwidth(1000e9);\n+  desc.set_l2_cache_size(50 * 1024 * 1024);\n+  return desc;\n+}\n+\n+class LegacyCacheTest : public ::testing::Test {\n+ protected:\n+  void SetUp() override {\n+    ASSERT_TRUE(tsl::Env::Default()->LocalTempFilename(&test_dir_));\n+    TF_ASSERT_OK(tsl::Env::Default()->CreateDir(test_dir_));\n+  }\n+\n+  void TearDown() override {\n+    int64_t undeleted_files, undeleted_dirs;\n+    tsl::Env::Default()\n+        ->DeleteRecursively(test_dir_, &undeleted_files, &undeleted_dirs)\n+        .IgnoreError();\n+  }\n+\n+  std::string test_dir_;\n+  DebugOptions::AutotuneCacheMode mode_ =\n+      DebugOptions::AUTOTUNE_CACHE_MODE_UPDATE;\n+  std::string device_name_ = \"test_device\";\n+  se::DeviceDescription device_desc_ =\n+      CreateDummyDeviceDescription(device_name_);\n+\n+  std::unique_ptr<HloInstruction> CreateDummyInstr(const std::string& name) {\n+    return HloInstruction::CreateConstant(\n+        LiteralUtil::CreateR0<int32_t>(std::hash<std::string>()(name)));\n+  }\n+\n+  Config CreateDummyTritonConfig() {\n+    Config config;\n+    config.codegen_backend_name = \"Triton\";\n+    config.backend_config.PackFrom(AutotuneResult::TritonGemmKey());\n+    return config;\n+  }\n+\n+  Config CreateDummyCublasConfig() {\n+    Config config;\n+    config.codegen_backend_name = \"Cublas\";\n+    config.backend_config.PackFrom(AutotuneResult::GemmKey());\n+    return config;\n+  }\n+\n+  Config CreateDummyCudnnConfig() {\n+    Config config;\n+    config.codegen_backend_name = \"Cudnn\";\n+    config.backend_config.PackFrom(AutotuneResult::ConvKey());\n+    return config;\n+  }\n+};\n+\n+// Matcher for Config.\n+MATCHER_P(ConfigEq, expected_config, \"\") {\n+  const Config& actual_config = arg;\n+  if (actual_config.codegen_backend_name !=\n+      expected_config.codegen_backend_name) {\n+    *result_listener << \"codegen_backend mismatch: expected \"\n+                     << expected_config.codegen_backend_name << \", got \"\n+                     << actual_config.codegen_backend_name;\n+    return false;\n+  }\n+  // Compare backend_config (google::protobuf::Any)\n+  if (actual_config.backend_config.type_url() !=\n+      expected_config.backend_config.type_url()) {\n+    *result_listener << \"backend_config type_url mismatch: expected \"\n+                     << expected_config.backend_config.type_url() << \", got \"\n+                     << actual_config.backend_config.type_url();\n+    return false;\n+  }\n+  if (actual_config.backend_config.value() !=\n+      expected_config.backend_config.value()) {\n+    *result_listener << \"backend_config value mismatch\";\n+    return false;\n+  }\n+  return true;\n+}\n+\n+TEST_F(LegacyCacheTest, CreateEmpty) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  auto instr = CreateDummyInstr(\"hlo\");\n+  EXPECT_THAT(cache.Lookup(instr.get()), Eq(std::nullopt));\n+}\n+\n+TEST_F(LegacyCacheTest, InsertAndLookupTriton) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  auto instr = CreateDummyInstr(\"hlo1\");\n+  Config config = CreateDummyTritonConfig();\n+\n+  TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n+}\n+\n+TEST_F(LegacyCacheTest, InsertAndLookupCublas) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  auto instr = CreateDummyInstr(\"hlo2\");\n+  Config config = CreateDummyCublasConfig();\n+\n+  TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n+}\n+\n+TEST_F(LegacyCacheTest, InsertAndLookupCudnn) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  auto instr = CreateDummyInstr(\"hlo3\");\n+  Config config = CreateDummyCudnnConfig();\n+\n+  TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n+}\n+\n+TEST_F(LegacyCacheTest, InsertAndLookupForUnsupportedBackend) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  auto instr = CreateDummyInstr(\"hlo4\");\n+  Config config;\n+  config.codegen_backend_name = \"UnsupportedBackend\";\n+\n+  TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  EXPECT_THAT(cache.Lookup(instr.get()), Eq(std::nullopt));\n+}\n+\n+TEST_F(LegacyCacheTest, PersistAcrossInstances) {\n+  auto instr = CreateDummyInstr(\"hlo5\");\n+  Config config = CreateDummyTritonConfig();\n+\n+  // Create cache, insert, and let it save.\n+  {\n+    auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+    TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  }\n+\n+  // Create a new cache, which should load from disk.\n+  {\n+    auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+    EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n+  }\n+}\n+\n+TEST_F(LegacyCacheTest, LoadWithDifferentDevice) {\n+  auto instr = CreateDummyInstr(\"hlo6\");\n+  Config config = CreateDummyTritonConfig();\n+\n+  // Create cache, insert, and let it save.\n+  {\n+    auto cache = LegacyCache(test_dir_, mode_,\n+                             CreateDummyDeviceDescription(\"test_device\"));\n+    TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  }\n+\n+  // Create a new cache with different device, should not load the entry.\n+  {\n+    auto cache = LegacyCache(test_dir_, mode_,\n+                             CreateDummyDeviceDescription(\"other_device\"));\n+    EXPECT_THAT(cache.Lookup(instr.get()), Eq(std::nullopt));\n+  }\n+}\n+\n+TEST_F(LegacyCacheTest, OnlyInsertOncePerHlo) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  auto instr = CreateDummyInstr(\"hlo7\");\n+\n+  Config config = CreateDummyTritonConfig();\n+  TF_ASSERT_OK(cache.Insert(instr.get(), config));\n+  EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n+\n+  Config another_config = CreateDummyCublasConfig();\n+  TF_ASSERT_OK(cache.Insert(instr.get(), another_config));\n+  EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "bca81fbd34c021cbbcb6f4bedbc9a17b6df4b71b",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 53,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc?ref=0b8cc9cc13b0699ece288bbcb49d7f00aeaec275",
            "patch": "@@ -66,6 +66,7 @@ constexpr int kVersion = 3;\n }  // namespace\n \n using AutotuneCacheMap = absl::flat_hash_map<AutotuneCacheKey, AutotuneResult>;\n+using ResultAndInserted = AutotunerUtil::ResultAndInserted;\n \n static absl::Mutex autotune_cache_mu(absl::kConstInit);\n static auto& autotune_cache ABSL_GUARDED_BY(autotune_cache_mu) =\n@@ -97,16 +98,6 @@ absl::StatusOr<std::string> GetCacheFilePath(absl::string_view cache_dir,\n   return tsl::io::JoinPath(cache_dir, absl::StrCat(key_hash, \".textproto\"));\n }\n \n-struct ResultAndInserted {\n-  // The result that ended up in the cache. This is the existing result if\n-  // inserted is false, and the new result if inserted is true.\n-  //\n-  // We return a value, not a pointer, for thread safety reasons.\n-  AutotuneResult result;\n-  // Did we insert the given result into the cache?\n-  bool inserted;\n-};\n-\n ResultAndInserted AddResultToInMemoryCache(const AutotuneCacheKey& key,\n                                            AutotuneResult result)\n     ABSL_LOCKS_EXCLUDED(autotune_cache_mu) {\n@@ -158,19 +149,6 @@ absl::Status AddResultToFileBasedCacheIfEnabled(\n   return default_env->RenameFile(temp_file_path, file_path);\n }\n \n-absl::StatusOr<ResultAndInserted> AddResultToCaches(\n-    const AutotuneCacheKey& key, AutotuneResult result,\n-    absl::string_view cache_dir,\n-    DebugOptions::AutotuneCacheMode autotune_cache_mode)\n-    ABSL_LOCKS_EXCLUDED(autotune_cache_mu) {\n-  ResultAndInserted result_and_inserted = AddResultToInMemoryCache(key, result);\n-  if (result_and_inserted.inserted) {\n-    TF_RETURN_IF_ERROR(AddResultToFileBasedCacheIfEnabled(\n-        key, result_and_inserted.result, cache_dir, autotune_cache_mode));\n-  }\n-  return result_and_inserted;\n-}\n-\n std::optional<AutotuneResult> TryToFindInInMemoryCache(\n     const AutotuneCacheKey& key) ABSL_LOCKS_EXCLUDED(autotune_cache_mu) {\n   absl::MutexLock lock(&autotune_cache_mu);\n@@ -313,36 +291,6 @@ TryFindInAllCacheTypes(const AutotuneCacheKey& key, absl::string_view cache_dir)\n \n   return std::make_pair(CacheType::kNone, std::nullopt);\n }\n-\n-absl::StatusOr<std::optional<AutotuneResult>> TryFindInCache(\n-    const AutotuneCacheKey& key, absl::string_view cache_dir)\n-    ABSL_LOCKS_EXCLUDED(autotune_cache_mu) {\n-  TF_ASSIGN_OR_RETURN(auto cached, TryFindInAllCacheTypes(key, cache_dir));\n-\n-  if (VLOG_IS_ON(1)) {\n-    std::string logged_key =\n-        (VLOG_IS_ON(2)) ? absl::StrCat(\": key = \", key.ToString()) : \"\";\n-    switch (cached.first) {\n-      case CacheType::kNone:\n-        LOG(INFO) << \"Autotune cache miss\" << logged_key;\n-        break;\n-      case CacheType::kInMemory:\n-        LOG(INFO) << \"In-memory autotune cache hit\" << logged_key;\n-        break;\n-      case CacheType::kOnDisk:\n-        LOG(INFO) << \"File-based autotune cache hit\" << logged_key;\n-        break;\n-    }\n-  }\n-\n-  {\n-    auto cache_hit = cached.second.has_value();\n-    absl::MutexLock lock(&autotune_cache_mu);\n-    autotune_cache_stats.cache_hits += cache_hit ? 1 : 0;\n-    autotune_cache_stats.cache_misses += cache_hit ? 0 : 1;\n-  }\n-  return std::move(cached.second);\n-}\n }  // namespace\n \n AutotuneConfig AutotuneConfig::FromDebugOptions(\n@@ -420,6 +368,49 @@ AutotuneConfig AutotuneConfig::FromDebugOptions(\n   return result_and_inserted.result;\n }\n \n+absl::StatusOr<std::optional<AutotuneResult>> AutotunerUtil::TryFindInCache(\n+    const AutotuneCacheKey& key, absl::string_view cache_dir)\n+    ABSL_LOCKS_EXCLUDED(autotune_cache_mu) {\n+  TF_ASSIGN_OR_RETURN(auto cached, TryFindInAllCacheTypes(key, cache_dir));\n+\n+  if (VLOG_IS_ON(1)) {\n+    std::string logged_key =\n+        (VLOG_IS_ON(2)) ? absl::StrCat(\": key = \", key.ToString()) : \"\";\n+    switch (cached.first) {\n+      case CacheType::kNone:\n+        LOG(INFO) << \"Autotune cache miss\" << logged_key;\n+        break;\n+      case CacheType::kInMemory:\n+        LOG(INFO) << \"In-memory autotune cache hit\" << logged_key;\n+        break;\n+      case CacheType::kOnDisk:\n+        LOG(INFO) << \"File-based autotune cache hit\" << logged_key;\n+        break;\n+    }\n+  }\n+\n+  {\n+    auto cache_hit = cached.second.has_value();\n+    absl::MutexLock lock(&autotune_cache_mu);\n+    autotune_cache_stats.cache_hits += cache_hit ? 1 : 0;\n+    autotune_cache_stats.cache_misses += cache_hit ? 0 : 1;\n+  }\n+  return std::move(cached.second);\n+}\n+\n+absl::StatusOr<ResultAndInserted> AutotunerUtil::AddResultToCaches(\n+    const AutotuneCacheKey& key, AutotuneResult result,\n+    absl::string_view cache_dir,\n+    DebugOptions::AutotuneCacheMode autotune_cache_mode)\n+    ABSL_LOCKS_EXCLUDED(autotune_cache_mu) {\n+  ResultAndInserted result_and_inserted = AddResultToInMemoryCache(key, result);\n+  if (result_and_inserted.inserted) {\n+    TF_RETURN_IF_ERROR(AddResultToFileBasedCacheIfEnabled(\n+        key, result_and_inserted.result, cache_dir, autotune_cache_mode));\n+  }\n+  return result_and_inserted;\n+}\n+\n namespace {\n \n bool IsTextProtoPath(absl::string_view file_path) {"
        },
        {
            "sha": "d055218cbcd6682c4dbcde17314c089dcb04a968",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.h",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b8cc9cc13b0699ece288bbcb49d7f00aeaec275/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h?ref=0b8cc9cc13b0699ece288bbcb49d7f00aeaec275",
            "patch": "@@ -206,6 +206,27 @@ class AutotunerUtil {\n                                         AutotuneResult result,\n                                         const AutotuneConfig& config);\n \n+  // Used in the new autotuner to provide current cache compatibility.\n+  static absl::StatusOr<std::optional<AutotuneResult>> TryFindInCache(\n+      const AutotuneCacheKey& key, absl::string_view cache_dir);\n+\n+  // Used in the new autotuner to provide current cache compatibility.\n+  struct ResultAndInserted {\n+    // The result that ended up in the cache. This is the existing result if\n+    // inserted is false, and the new result if inserted is true.\n+    //\n+    // We return a value, not a pointer, for thread safety reasons.\n+    AutotuneResult result;\n+    // Did we insert the given result into the cache?\n+    bool inserted;\n+  };\n+\n+  // Used in the new autotuner to provide current cache compatibility.\n+  static absl::StatusOr<ResultAndInserted> AddResultToCaches(\n+      const AutotuneCacheKey& key, AutotuneResult result,\n+      absl::string_view cache_dir,\n+      DebugOptions::AutotuneCacheMode autotune_cache_mode);\n+\n   // Functions to save/load XLA's autotuning results.\n   //\n   // This is used for ahead-of-time autotuning.  Specifically:"
        }
    ],
    "stats": {
        "total": 559,
        "additions": 505,
        "deletions": 54
    }
}