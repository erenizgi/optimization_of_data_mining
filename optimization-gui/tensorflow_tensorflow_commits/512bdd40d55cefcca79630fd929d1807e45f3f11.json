{
    "author": "sgarciagoogle",
    "message": "Remove unused dependencies and fields from `converter_flags.proto`.\n\nPiperOrigin-RevId: 827561598",
    "sha": "512bdd40d55cefcca79630fd929d1807e45f3f11",
    "files": [
        {
            "sha": "ab6c5abeca86f0a3636a0e71aa2079fa12d7c93e",
            "filename": "tensorflow/compiler/mlir/lite/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD?ref=512bdd40d55cefcca79630fd929d1807e45f3f11",
            "patch": "@@ -1990,7 +1990,6 @@ cc_library(\n         \":tf_tfl_passes\",\n         \"//tensorflow/cc/saved_model:loader\",\n         \"//tensorflow/compiler/mlir:op_or_arg_name_mapper\",\n-        \"//tensorflow/compiler/mlir/lite/core:macros\",\n         \"//tensorflow/compiler/mlir/lite/debug\",\n         \"//tensorflow/compiler/mlir/lite/experimental/remat:metadata_util\",\n         \"//tensorflow/compiler/mlir/lite/metrics:converter_error_data_proto_cc\",\n@@ -2212,10 +2211,8 @@ tf_proto_library(\n     srcs = [\"converter_flags.proto\"],\n     make_default_target_header_only = True,\n     protodeps = [\n-        \"//tensorflow/compiler/mlir/quantization/stablehlo:quantization_options_proto\",\n-        \"//tensorflow/compiler/mlir/quantization/stablehlo:quantization_config_proto\",\n-        \"//tensorflow/compiler/mlir/lite/debug:debug_options_proto\",\n         \":types_proto\",\n+        \"//tensorflow/compiler/mlir/lite/debug:debug_options_proto\",\n     ],\n     visibility = [\"//visibility:public\"],\n )"
        },
        {
            "sha": "49795ad8337d9ab434b4e093dd0f07d6a5a3b136",
            "filename": "tensorflow/compiler/mlir/lite/converter_flags.proto",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fconverter_flags.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fconverter_flags.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fconverter_flags.proto?ref=512bdd40d55cefcca79630fd929d1807e45f3f11",
            "patch": "@@ -17,8 +17,6 @@ package tflite;\n \n import \"tensorflow/compiler/mlir/lite/debug/debug_options.proto\";\n import \"tensorflow/compiler/mlir/lite/types.proto\";\n-import \"tensorflow/compiler/mlir/quantization/stablehlo/quantization_config.proto\";\n-import \"tensorflow/compiler/mlir/quantization/stablehlo/quantization_options.proto\";\n \n // Supported I/O file formats. Some formats may be input-only or output-only.\n enum FileFormat {\n@@ -43,6 +41,8 @@ enum FileFormat {\n //\n // Next ID to use: 69.\n message ConverterFlags {\n+  reserved 54, 61;\n+\n   // Input file format\n   optional FileFormat input_format = 1;\n \n@@ -312,12 +312,6 @@ message ConverterFlags {\n   // If true, disable folding mul->fc as in layer norm during optimize pass.\n   optional bool disable_fuse_mul_and_fc = 53 [default = false];\n \n-  // Indicates the quantization specs. Quantization spec can be set to either\n-  // a preset method or a custom method.\n-  // Note: This is deprecated; use `quantization_config` instead.\n-  optional stablehlo.quantization.QuantizationOptions quantization_options = 54\n-      [deprecated = true];\n-\n   // Flag to enable hlo to tf conversion.\n   // This is useful to exercise StableHLO -> HLO -> TF -> TFLite path.\n   optional bool enable_hlo_to_tf_conversion = 55\n@@ -346,11 +340,6 @@ message ConverterFlags {\n   // WARNING: Experimental interface, subject to change.\n   optional string qdq_conversion_mode = 60 [default = \"NONE\"];\n \n-  // Configures quantization behavior. This config is fed to the StableHLO\n-  // Quantizer integrated in the converter.\n-  // WARNING: Experimental interface, subject to change.\n-  optional stablehlo.quantization.QuantizationConfig quantization_config = 61;\n-\n   // Disables per channel weights quantization for Dense layers and enables\n   // legacy per tensor quantization. The legacy quantization for Dense layers is\n   // inconsistent with Conv 1x1 which always performs per channel quantization."
        },
        {
            "sha": "8b97bc0f605cdd9547bb96a1f1b6a42a0fc7e198",
            "filename": "tensorflow/compiler/mlir/lite/python/saved_model_to_tfl_flatbuffer.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fpython%2Fsaved_model_to_tfl_flatbuffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fpython%2Fsaved_model_to_tfl_flatbuffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fpython%2Fsaved_model_to_tfl_flatbuffer.cc?ref=512bdd40d55cefcca79630fd929d1807e45f3f11",
            "patch": "@@ -210,8 +210,6 @@ absl::Status ConvertSavedModelToTFLiteFlatBuffer(\n       converter_flags.convert_to_stablehlo();\n   pass_config.legalize_custom_tensor_list_ops =\n       converter_flags.legalize_custom_tensor_list_ops();\n-  pass_config.enable_stablehlo_quantizer =\n-      converter_flags.has_quantization_config();\n   pass_config.enable_composite_direct_lowering =\n       converter_flags.enable_composite_direct_lowering();\n   pass_config.model_origin_framework = converter_flags.model_origin_framework();"
        },
        {
            "sha": "2ce933112a0a436dcae7c2280bab30d1d3d55508",
            "filename": "tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 19,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftf_to_tfl_flatbuffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/512bdd40d55cefcca79630fd929d1807e45f3f11/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftf_to_tfl_flatbuffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftf_to_tfl_flatbuffer.cc?ref=512bdd40d55cefcca79630fd929d1807e45f3f11",
            "patch": "@@ -323,21 +323,19 @@ absl::Status ConvertTFExecutorToStablehloFlatbuffer(\n \n     // TODO: b/264218457 - Refactor the component below once StableHLO Quantizer\n     // can run DRQ. Temporarily using TF Quantization for StableHLO DRQ.\n-    if (!converter_flags.has_quantization_options()) {\n-      // The default minimum number of elements a weights array must have to be\n-      // quantized by this transformation.\n-      const int kWeightsMinNumElementsDefault = 1024;\n-\n-      quantization::QuantizationOptions quantization_options;\n-\n-      quantization_options.mutable_quantization_method()->set_preset_method(\n-          quantization::QuantizationMethod::METHOD_DYNAMIC_RANGE_INT8);\n-      quantization_options.set_op_set(quantization::UNIFORM_QUANTIZED);\n-      quantization_options.set_min_num_elements_for_weights(\n-          kWeightsMinNumElementsDefault);\n-      quantization::AddQuantizePtqDynamicRangePasses(pass_manager,\n-                                                     quantization_options);\n-    }\n+    // The default minimum number of elements a weights array must have to be\n+    // quantized by this transformation.\n+    const int kWeightsMinNumElementsDefault = 1024;\n+\n+    quantization::QuantizationOptions quantization_options;\n+\n+    quantization_options.mutable_quantization_method()->set_preset_method(\n+        quantization::QuantizationMethod::METHOD_DYNAMIC_RANGE_INT8);\n+    quantization_options.set_op_set(quantization::UNIFORM_QUANTIZED);\n+    quantization_options.set_min_num_elements_for_weights(\n+        kWeightsMinNumElementsDefault);\n+    quantization::AddQuantizePtqDynamicRangePasses(pass_manager,\n+                                                   quantization_options);\n     if (failed(pass_manager.run(module))) {\n       return status_handler.ConsumeStatus();\n     }\n@@ -350,10 +348,6 @@ absl::Status ConvertTFExecutorToStablehloFlatbuffer(\n   pass_manager.addPass(mlir::odml::createPrintOpStatsPass(\n       mlir::odml::GetAcceptedStableHLODialects()));\n   mlir::odml::AddStablehloOptimizationPasses(pass_manager);\n-  if (converter_flags.has_quantization_options()) {\n-    stablehlo::quantization::AddQuantizationPasses(\n-        pass_manager, converter_flags.quantization_options());\n-  }\n   if (failed(pass_manager.run(module))) {\n     return status_handler.ConsumeStatus();\n   }"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 16,
        "deletions": 38
    }
}