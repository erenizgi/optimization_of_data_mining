{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 846051105",
    "sha": "066852ef9177198bd9459a225ae8c7d2cbdb4d51",
    "files": [
        {
            "sha": "101e944b84d813ce366a7cfb4cf36a2942124f64",
            "filename": "tensorflow/dtensor/mlir/collectives.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fcollectives.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fcollectives.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fcollectives.h?ref=066852ef9177198bd9459a225ae8c7d2cbdb4d51",
            "patch": "@@ -84,7 +84,7 @@ StatusOr<mlir::Operation*> EmitAllReduce(\n StatusOr<mlir::Operation*> EmitBarrierWithConstValue(mlir::OpBuilder& builder,\n                                                      mlir::Location loc,\n                                                      const Mesh& mesh,\n-                                                     int32 value);\n+                                                     int32_t value);\n \n // Given input `tensor` that is sharded across spatial dimensions, conduct\n // halo exchange such that each spatially sharded input blocks exchange"
        },
        {
            "sha": "37bdd53366af82f3b4e6b243fb037746647cc0b0",
            "filename": "tensorflow/dtensor/mlir/collectives_common.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fcollectives_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fcollectives_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fcollectives_common.cc?ref=066852ef9177198bd9459a225ae8c7d2cbdb4d51",
            "patch": "@@ -38,7 +38,7 @@ namespace dtensor {\n // a multi-host cluster will generate the same grouping, and therefore the same\n // XLA program fingerprint, independently. std::map guarantees the same\n // iteration order.\n-using AllReducePartitions = std::map<DeviceLocation, std::vector<int32>>;\n+using AllReducePartitions = std::map<DeviceLocation, std::vector<int32_t>>;\n \n // Computes AllReduce partitions using reduced mesh dimension names.\n //\n@@ -60,11 +60,11 @@ StatusOr<AllReducePartitions> GetAllReducePartitionsFromReducedDims(\n     const dtensor::Layout& output_layout,\n     const absl::flat_hash_set<std::string>& reduced_dims) {\n   AllReducePartitions partitions;\n-  for (int64 device = 0; device < output_layout.num_devices(); ++device) {\n+  for (int64_t device = 0; device < output_layout.num_devices(); ++device) {\n     TF_ASSIGN_OR_RETURN(const DeviceLocation device_loc,\n                         output_layout.mesh().device_location(device));\n     DeviceLocation kept_dims;\n-    for (int64 dim_idx = 0; dim_idx < device_loc.size(); ++dim_idx) {\n+    for (int64_t dim_idx = 0; dim_idx < device_loc.size(); ++dim_idx) {\n       if (!reduced_dims.contains(output_layout.mesh().dim_name(dim_idx))) {\n         kept_dims.push_back(device_loc[dim_idx]);\n       }"
        },
        {
            "sha": "fe8688ebc673af9e48a1c5d474a4f867fec49aab",
            "filename": "tensorflow/dtensor/mlir/collectives_common.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fcollectives_common.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fcollectives_common.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fcollectives_common.h?ref=066852ef9177198bd9459a225ae8c7d2cbdb4d51",
            "patch": "@@ -29,7 +29,7 @@ namespace tensorflow {\n namespace dtensor {\n \n // Computes AllReduce partitions using reduced mesh dimension names.\n-StatusOr<std::map<DeviceLocation, std::vector<int32>>>\n+StatusOr<std::map<DeviceLocation, std::vector<int32_t>>>\n GetAllReducePartitionsFromReducedDims(\n     const dtensor::Layout& output_layout,\n     const absl::flat_hash_set<std::string>& reduced_dims);"
        },
        {
            "sha": "e4cea2348f3d0978effcc983442f050c2c0b4ae7",
            "filename": "tensorflow/dtensor/mlir/dtensor_allreduce_combine_optimization.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_combine_optimization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_combine_optimization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_combine_optimization.cc?ref=066852ef9177198bd9459a225ae8c7d2cbdb4d51",
            "patch": "@@ -72,7 +72,7 @@ namespace ops_util = ::mlir::TF::collection_ops_util;\n \n // Pad the merged tensor shape to multiples of 1024B, so delinearization\n // skipping optimization in XLA can get activated.\n-constexpr int32 kAllReducePadding = 1024;\n+constexpr int32_t kAllReducePadding = 1024;\n \n // Returns true if `successor` depends on `predecessor`.\n // TODO(jiawenhao): Repeatedly computing dependency sets for a large cluster can"
        },
        {
            "sha": "b16eeb8230f860eef9fd1573b1d98dbab4352ccb",
            "filename": "tensorflow/dtensor/mlir/dtensor_allreduce_scatter_optimization.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_scatter_optimization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_scatter_optimization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_scatter_optimization.cc?ref=066852ef9177198bd9459a225ae8c7d2cbdb4d51",
            "patch": "@@ -64,16 +64,16 @@ mlir::DenseIntElementsAttr GetScatterGroupAssignment(\n   auto partitions =\n       GetAllReducePartitionsFromReducedDims(original_layout, scattered_dims)\n           .value();\n-  const int32 num_partitions = partitions.size();\n+  const int32_t num_partitions = partitions.size();\n \n   // Construct a flattened list of scatter partitions.\n-  std::vector<int32> partitions_flat;\n+  std::vector<int32_t> partitions_flat;\n   for (auto& p : partitions) {\n     partitions_flat.insert(partitions_flat.end(), p.second.begin(),\n                            p.second.end());\n   }\n \n-  int32 partition_size = partitions.begin()->second.size();\n+  int32_t partition_size = partitions.begin()->second.size();\n   mlir::OpBuilder builder(all_scatter);\n   auto group_shaped_type = mlir::RankedTensorType::get(\n       {num_partitions, partition_size},"
        },
        {
            "sha": "b722e1bba45e0da330582c687c7db2230c655b5d",
            "filename": "tensorflow/dtensor/mlir/dtensor_mixed_precision_reduce.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_mixed_precision_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066852ef9177198bd9459a225ae8c7d2cbdb4d51/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_mixed_precision_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_mixed_precision_reduce.cc?ref=066852ef9177198bd9459a225ae8c7d2cbdb4d51",
            "patch": "@@ -44,7 +44,7 @@ namespace {\n // the list of devices that are a part of the same reduction group.\n template <class ReduceOpType>\n mlir::LogicalResult GetAllReduceGroupSize(ReduceOpType reduce_op,\n-                                          int32* group_size) {\n+                                          int32_t* group_size) {\n   mlir::DenseIntElementsAttr group_assignment_attr;\n   if (!matchPattern(reduce_op.getGroupAssignment(),\n                     m_Constant(&group_assignment_attr)))\n@@ -80,7 +80,7 @@ mlir::LogicalResult MaybeUpcastForReduction(ReduceOpType reduce_op,\n   mlir::OpBuilder builder(reduce_op);\n   const mlir::Location loc = reduce_op.getLoc();\n \n-  int32 group_size;\n+  int32_t group_size;\n   if (mlir::failed(GetAllReduceGroupSize(reduce_op, &group_size)))\n     return mlir::failure();\n   if (group_size <= ReduceInBfloat16MaxGroupSize())"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 11,
        "deletions": 11
    }
}