{
    "author": "hawkinsp",
    "message": "[PJRT] Change transpose code to chunk nodes on the Loop nest representation, not the Node representation.\n\nIt is simpler to partition the Loops, which directly represent an iteration space, rather than the Nodes, which have to deal with a bunch of annoying details about tiling and partial tiles.\n\nRefactoring, no behavior changes intended.\n\nPiperOrigin-RevId: 846433128",
    "sha": "6383e3632c91bdc8eccd458f699b317f03968b84",
    "files": [
        {
            "sha": "f266bac15df8e07dfaa581479c1b4c00a79f0f66",
            "filename": "third_party/xla/xla/pjrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD?ref=6383e3632c91bdc8eccd458f699b317f03968b84",
            "patch": "@@ -972,6 +972,7 @@ cc_library(\n xla_cc_test(\n     name = \"transpose_test\",\n     srcs = [\"transpose_test.cc\"],\n+    shard_count = 10,\n     deps = [\n         \":transpose\",\n         \"//xla:array\","
        },
        {
            "sha": "5deddb6c0dee6041f7e602b7bbea2433fb1f44c9",
            "filename": "third_party/xla/xla/pjrt/transpose.cc",
            "status": "modified",
            "additions": 118,
            "deletions": 84,
            "changes": 202,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.cc?ref=6383e3632c91bdc8eccd458f699b317f03968b84",
            "patch": "@@ -600,47 +600,37 @@ bool TransposePlan::Loop::operator==(const Loop& other) const {\n          lda == other.lda && ldb == other.ldb &&\n          is_inner_dim_in_a == other.is_inner_dim_in_a &&\n          is_inner_dim_in_b == other.is_inner_dim_in_b &&\n-         parallelism == other.parallelism;\n+         parallelism == other.parallelism && start == other.start &&\n+         end == other.end;\n }\n \n // Helper function that builds a plan.\n-void TransposePlan::BuildPlanNodes(int thread_id,\n+void TransposePlan::BuildPlanNodes(int chunk_id,\n                                    std::vector<TransposePlan::Node>& nodes) {\n   VLOG(8) << \"Before plan build: \" << ToString();\n   const int ndim = a_dims_.size();\n   DCHECK_GT(ndim, 0);\n \n+  // Use the pre-computed chunk loops which have start/end bounds already set.\n+  absl::Span<const Loop> chunk_loops = chunk_loops_[chunk_id];\n+\n   // We build plans in a depth-first order, visiting loops from outermost to\n   // innermost. We use a stack (depth-first) order to handle trailing partial\n   // tiles, which we \"come back to\" after handling the non-trailing case.\n   struct Agendum {\n-    // The ID of the loop to visit in loop_order_.\n+    // The ID of the loop to visit in chunk_loops.\n     int loop_id;\n     // The parent node ID whose trailing tile should be made to point to this\n     // node.\n     int parent_node_id;\n \n-    // The number of parallel tasks available to run this loop and its\n-    // successors.\n-    int num_tasks_at_loop;\n-\n-    // The ID number of the current thread in the tasks at this loop.\n-    int task_id_at_loop;\n-\n     // For which dimensions of `a` are we to visit the partial trailing tile\n     // a loop that visits that tile's interior?\n     absl::InlinedVector<bool, 4> partial_tiles;\n   };\n   std::stack<Agendum> agenda;\n \n-  int total_tasks = 1;\n-  for (const Loop& loop : loop_order_) {\n-    total_tasks *= loop.parallelism;\n-  }\n-\n   agenda.push(Agendum{/*loop_id=*/0, /*parent_node_id=*/-1,\n-                      /*num_tasks_at_loop=*/total_tasks,\n-                      /*task_id_at_loop=*/thread_id,\n                       absl::InlinedVector<bool, 4>(ndim, false)});\n \n   auto loop_has_trivial_iteration_space = [](const Node& node) {\n@@ -659,9 +649,8 @@ void TransposePlan::BuildPlanNodes(int thread_id,\n           node_id - agendum.parent_node_id;\n     }\n \n-    if (agendum.loop_id == loop_order_.size()) {\n+    if (agendum.loop_id == chunk_loops.size()) {\n       // We've reached the end of the loop nest.\n-      DCHECK_EQ(agendum.num_tasks_at_loop, 1);\n       // Transpose loops have a sentinel node, indicated by a negative `inc`\n       // value, that describes the striding of the inner transpose kernel.\n       if (!inner_kernel_is_memcpy_) {\n@@ -675,14 +664,9 @@ void TransposePlan::BuildPlanNodes(int thread_id,\n       continue;\n     }\n \n-    const Loop& loop = loop_order_[agendum.loop_id];\n+    const Loop& loop = chunk_loops[agendum.loop_id];\n     int a_dim = loop.dim_in_a;\n \n-    // Compute the number of tasks for the next loop iteration.\n-    int task_id_at_loop = agendum.task_id_at_loop;\n-    int num_tasks_at_loop = agendum.num_tasks_at_loop / loop.parallelism;\n-    int task_id_at_next_loop = task_id_at_loop % num_tasks_at_loop;\n-\n     Node node;\n     node.lda = loop.lda;\n     node.ldb = loop.ldb;\n@@ -695,19 +679,18 @@ void TransposePlan::BuildPlanNodes(int thread_id,\n       node.inc = inner_block_elems_ * outer_block_elems_b_;\n     }\n \n-    int task_id = task_id_at_loop / num_tasks_at_loop;\n-\n     if (loop.tile_interior) {\n       // We are visiting the tile interior of a tiled dimension.\n       bool partial = agendum.partial_tiles[a_dim];\n \n       int64_t size = partial ? loop.dim_size % loop.tile_size : loop.tile_size;\n-      int64_t num_iterations = CeilOfRatio(size, node.inc);\n-      int64_t num_iterations_per_task =\n-          CeilOfRatio<int64_t>(num_iterations, loop.parallelism);\n-      node.start = std::min(size, task_id * num_iterations_per_task * node.inc);\n-      node.end =\n-          std::min(size, (task_id + 1) * num_iterations_per_task * node.inc);\n+      // loop.start and loop.end are in element units.\n+      // Verify alignment to block boundaries.\n+      CHECK(loop.start % node.inc == 0)\n+          << \"loop.start=\" << loop.start\n+          << \" must be aligned to node.inc=\" << node.inc;\n+      node.start = loop.start;\n+      node.end = std::min<int64_t>(size, loop.end);\n \n       if (node.is_inner_dim_in_a && inner_kernel_is_memcpy_) {\n         node.end = (node.end - node.start) * elem_size_in_bytes_;\n@@ -720,8 +703,6 @@ void TransposePlan::BuildPlanNodes(int thread_id,\n       Agendum new_agendum;\n       new_agendum.loop_id = agendum.loop_id + 1;\n       new_agendum.parent_node_id = -1;\n-      new_agendum.task_id_at_loop = task_id_at_next_loop;\n-      new_agendum.num_tasks_at_loop = num_tasks_at_loop;\n       new_agendum.partial_tiles = agendum.partial_tiles;\n       agenda.push(std::move(new_agendum));\n     } else {\n@@ -732,14 +713,16 @@ void TransposePlan::BuildPlanNodes(int thread_id,\n \n       // If there is a trailing partial tile as well as complete tiles, handle\n       // it as a trailer on the loop over complete tiles.\n+      // A chunk is responsible for the trailing tile if its loop.end covers\n+      // the full dimension.\n+      int64_t full_size = CeilOfRatio(loop.dim_size, loop.tile_size);\n+      bool handles_trailing =\n+          loop.end >= full_size && loop.start <= num_complete_tiles;\n       bool has_trailing_plan_node = false;\n-      if (num_complete_tiles > 0 && has_partial_tile &&\n-          task_id == loop.parallelism - 1) {\n+      if (num_complete_tiles > 0 && has_partial_tile && handles_trailing) {\n         Agendum new_agendum;\n         new_agendum.loop_id = agendum.loop_id + 1;\n         new_agendum.parent_node_id = node_id;\n-        new_agendum.task_id_at_loop = task_id_at_next_loop;\n-        new_agendum.num_tasks_at_loop = num_tasks_at_loop;\n         new_agendum.partial_tiles = agendum.partial_tiles;\n         new_agendum.partial_tiles[a_dim] = true;\n         agenda.push(std::move(new_agendum));\n@@ -751,15 +734,10 @@ void TransposePlan::BuildPlanNodes(int thread_id,\n       // path to handle the trailing tile.\n       bool partial = num_complete_tiles == 0 && has_partial_tile;\n \n-      // Evenly divide the loop iterations amongst the threads.\n+      // loop.start and loop.end are in tile units.\n       int64_t num_tiles = partial ? 1 : num_complete_tiles;\n-      int64_t num_iterations = CeilOfRatio(num_tiles, node.inc);\n-      int64_t num_iterations_per_task =\n-          CeilOfRatio<int64_t>(num_iterations, loop.parallelism);\n-      node.start =\n-          std::min(num_tiles, task_id * num_iterations_per_task * node.inc);\n-      node.end = std::min(num_tiles,\n-                          (task_id + 1) * num_iterations_per_task * node.inc);\n+      node.start = loop.start;\n+      node.end = std::min<int64_t>(num_tiles, loop.end);\n \n       if (node.is_inner_dim_in_a && inner_kernel_is_memcpy_) {\n         node.end = (node.end - node.start) * elem_size_in_bytes_;\n@@ -774,8 +752,6 @@ void TransposePlan::BuildPlanNodes(int thread_id,\n       Agendum new_agendum;\n       new_agendum.loop_id = agendum.loop_id + 1;\n       new_agendum.parent_node_id = -1;\n-      new_agendum.task_id_at_loop = task_id_at_next_loop;\n-      new_agendum.num_tasks_at_loop = num_tasks_at_loop;\n       new_agendum.partial_tiles = agendum.partial_tiles;\n       new_agendum.partial_tiles[a_dim] = partial;\n       agenda.push(std::move(new_agendum));\n@@ -999,7 +975,10 @@ void TransposePlan::Initialize() {\n                         : ldb_[pos_stride1a_in_b];\n   }\n \n-  loop_order_.reserve(ndim);\n+  // Order to traverse dimensions, from slowest-varying to fastest-varying.\n+  std::vector<Loop> loop_order;\n+\n+  loop_order.reserve(ndim);\n   for (int i = 0; i < ndim; ++i) {\n     Loop loop;\n     loop.dim_in_a = i;\n@@ -1017,7 +996,7 @@ void TransposePlan::Initialize() {\n     }\n     loop.is_inner_dim_in_a = (loop.tile_size == 1) && (i == pos_stride1a);\n     loop.is_inner_dim_in_b = (loop.tile_size == 1) && (i == pos_stride1b_in_a);\n-    loop_order_.push_back(loop);\n+    loop_order.push_back(loop);\n \n     if (loop.tile_size > 1) {\n       loop.tile_interior = true;\n@@ -1026,12 +1005,12 @@ void TransposePlan::Initialize() {\n                              : ldb_[inverse_permutation[i]];\n       loop.is_inner_dim_in_a = (i == pos_stride1a);\n       loop.is_inner_dim_in_b = (i == pos_stride1b_in_a);\n-      loop_order_.push_back(loop);\n+      loop_order.push_back(loop);\n     }\n   }\n \n-  RemoveTrivialLoops(loop_order_);\n-  CoalesceLoops(loop_order_);\n+  RemoveTrivialLoops(loop_order);\n+  CoalesceLoops(loop_order);\n \n   // Bound the block sizes so they are smaller than the stride-1 dimension\n   // size.\n@@ -1118,7 +1097,7 @@ void TransposePlan::Initialize() {\n                            inner_kernel_is_memcpy_ && l.tile_interior,\n                            -std::min<double>(a_stride * penalty, b_stride));\n   };\n-  absl::c_stable_sort(loop_order_, [&](const Loop& a, const Loop& b) {\n+  absl::c_stable_sort(loop_order, [&](const Loop& a, const Loop& b) {\n     return cost(a) < cost(b);\n   });\n   // It is a required invariant of the loop order that tile interiors always\n@@ -1127,13 +1106,14 @@ void TransposePlan::Initialize() {\n   // both input and output.\n \n   // The stride-1 loop must be innermost for a memcpy loop.\n-  DCHECK(!inner_kernel_is_memcpy_ || loop_order_.back().is_inner_dim_in_a)\n+  DCHECK(!inner_kernel_is_memcpy_ || loop_order.back().is_inner_dim_in_a)\n       << ToString();\n \n-  int num_threads = ChooseParallelizationStrategy();\n-  nodes_.resize(num_threads);\n-  for (int thread_id = 0; thread_id < num_threads; ++thread_id) {\n-    BuildPlanNodes(thread_id, nodes_[thread_id]);\n+  int num_chunks = ChooseParallelizationStrategy(loop_order);\n+  chunk_loops_ = PartitionLoops(num_chunks, loop_order);\n+  nodes_.resize(num_chunks);\n+  for (int chunk_id = 0; chunk_id < num_chunks; ++chunk_id) {\n+    BuildPlanNodes(chunk_id, nodes_[chunk_id]);\n   }\n \n   switch (transformation_) {\n@@ -1148,7 +1128,8 @@ void TransposePlan::Initialize() {\n   }\n }\n \n-int TransposePlan::ChooseParallelizationStrategy() {\n+int TransposePlan::ChooseParallelizationStrategy(\n+    std::vector<Loop>& loop_order) {\n   int available_parallelism = num_threads_requested_;\n \n   // Compute the number of iterations in `loop`.\n@@ -1170,14 +1151,14 @@ int TransposePlan::ChooseParallelizationStrategy() {\n   };\n \n   // Estimate the number of bytes each iteration of each loop processes.\n-  absl::InlinedVector<int64_t, 4> work_in_bytes(loop_order_.size());\n+  absl::InlinedVector<int64_t, 4> work_in_bytes(loop_order.size());\n   int64_t acc = elem_size_in_bytes_;\n   if (!inner_kernel_is_memcpy_) {\n     acc *= inner_block_elems_ * inner_block_elems_ * outer_block_elems_a_ *\n            outer_block_elems_b_;\n   }\n   auto work_it = work_in_bytes.rbegin();\n-  for (auto it = loop_order_.rbegin(); it != loop_order_.rend(); ++it) {\n+  for (auto it = loop_order.rbegin(); it != loop_order.rend(); ++it) {\n     *work_it++ = acc;\n     acc *= loop_iterations(*it);\n   }\n@@ -1186,28 +1167,76 @@ int TransposePlan::ChooseParallelizationStrategy() {\n \n   // Heuristic that attempts to parallelize the outermost loops, down to a\n   // minimum per-thread number of bytes processed.\n-  int num_threads = 1;\n-  for (size_t i = 0; i < loop_order_.size(); ++i) {\n-    Loop& loop = loop_order_[i];\n+  int num_chunks = 1;\n+  for (size_t i = 0; i < loop_order.size(); ++i) {\n+    Loop& loop = loop_order[i];\n     CHECK_GE(available_parallelism, 1);\n     int64_t iterations = loop_iterations(loop);\n+\n+    // Initialize loop iteration bounds to full range in element units.\n+    loop.start = 0;\n+    loop.end = loop.tile_interior ? loop.tile_size\n+                                  : CeilOfRatio(loop.dim_size, loop.tile_size);\n+\n     int kMinBytesPerThread = inner_kernel_is_memcpy_ ? (1 << 20) : (1 << 26);\n     int64_t min_iterations_per_thread =\n         CeilOfRatio<int64_t>(kMinBytesPerThread, work_in_bytes[i]);\n     int64_t parallel_work = CeilOfRatio(iterations, min_iterations_per_thread);\n \n     VLOG(8) << \"iterations=\" << iterations << \" parallel_work=\" << parallel_work\n             << \" available_parallelism=\" << available_parallelism;\n-    if (parallel_work >= available_parallelism) {\n-      loop.parallelism = available_parallelism;\n-      available_parallelism = 1;\n-    } else {\n-      loop.parallelism = parallel_work;\n-      available_parallelism /= parallel_work;\n+    int parallelism = std::min<int64_t>(available_parallelism, parallel_work);\n+    if (parallelism > 1) {\n+      // If we use CeilOfRatio(iterations, parallelism) as the chunk size, we\n+      // might end up with fewer chunks than parallelism if the chunk size is\n+      // large. For example, if iterations=17 and parallelism=16,\n+      // chunk_size=2. Then useful_tasks=9. We should reduce parallelism to 9.\n+      int64_t chunk_size =\n+          CeilOfRatio(iterations, static_cast<int64_t>(parallelism));\n+      int64_t useful_tasks = CeilOfRatio(iterations, chunk_size);\n+      parallelism = useful_tasks;\n+    }\n+    loop.parallelism = parallelism;\n+    available_parallelism /= parallelism;\n+    num_chunks *= loop.parallelism;\n+  }\n+  return num_chunks;\n+}\n+\n+std::vector<std::vector<TransposePlan::Loop>> TransposePlan::PartitionLoops(\n+    int num_chunks, const std::vector<Loop>& loop_order) {\n+  std::vector<std::vector<Loop>> result(num_chunks);\n+  for (int chunk_id = 0; chunk_id < num_chunks; ++chunk_id) {\n+    // Copy the base loop order for this chunk.\n+    result[chunk_id] = loop_order;\n+\n+    // For each loop, narrow the start/end bounds to this chunk's portion.\n+    int task_id_remaining = chunk_id;\n+    int num_tasks_remaining = num_chunks;\n+\n+    for (size_t i = 0; i < loop_order.size(); ++i) {\n+      Loop& chunk_loop = result[chunk_id][i];\n+      const Loop& base_loop = loop_order[i];\n+\n+      num_tasks_remaining /= base_loop.parallelism;\n+      int task_id = task_id_remaining / num_tasks_remaining;\n+      task_id_remaining = task_id_remaining % num_tasks_remaining;\n+\n+      // Divide this loop's iterations (in element units) among parallelism\n+      // tasks.\n+      int64_t iterations = base_loop.end - base_loop.start;\n+      int64_t iterations_per_task =\n+          CeilOfRatio<int64_t>(iterations, base_loop.parallelism);\n+\n+      chunk_loop.start =\n+          base_loop.start + std::min(iterations, task_id * iterations_per_task);\n+      chunk_loop.end =\n+          base_loop.start +\n+          std::min(iterations, (task_id + 1) * iterations_per_task);\n     }\n-    num_threads *= loop.parallelism;\n   }\n-  return num_threads;\n+\n+  return result;\n }\n \n std::string TransposePlan::ToString() const {\n@@ -1228,11 +1257,16 @@ std::string TransposePlan::ToString() const {\n                       node.is_inner_dim_in_b ? \"y\" : \"n\");\n                 }));\n       });\n-  auto format_loop_order = [](std::string* out, const Loop& loop) {\n-    return absl::StrAppend(out, loop.dim_in_a,\n-                           loop.tile_interior ? \"[tile]\" : \"\", \"(\",\n-                           loop.parallelism, \")\");\n+  auto format_loop = [](std::string* out, const Loop& loop) {\n+    absl::StrAppendFormat(out, \"%d%s[%d,%d](%d)\", loop.dim_in_a,\n+                          loop.tile_interior ? \"[tile]\" : \"\", loop.start,\n+                          loop.end, loop.parallelism);\n   };\n+  std::string chunk_loops_str = absl::StrJoin(\n+      chunk_loops_, \"\\n\",\n+      [&](std::string* out, const std::vector<Loop>& loops) {\n+        absl::StrAppend(out, \"    \", absl::StrJoin(loops, \", \", format_loop));\n+      });\n   std::string transformation_str;\n   switch (transformation_) {\n     case Transformation::kNone:\n@@ -1244,19 +1278,19 @@ std::string TransposePlan::ToString() const {\n   }\n   return absl::StrFormat(\n       \"elem_size=%d a_dims=%s b_dims=%s permutation=%s a_tiling=%s b_tiling=%s \"\n-      \"lda=%s lda_tile=%s ldb=%s ldb_tile=%s loop_order=%s \"\n+      \"lda=%s lda_tile=%s ldb=%s ldb_tile=%s \"\n       \"outer_bs=[%d,%d] inner_bs=%d \"\n       \"transformation=%s scratch_size=%d\\n\"\n+      \"chunk_loops:\\n%s\\n\"\n       \"nodes:\\n%s\",\n       elem_size_in_bytes_, absl::StrJoin(a_dims_, \",\"),\n       absl::StrJoin(Permute(a_dims_, permutation_), \",\"),\n       absl::StrJoin(permutation_, \",\"), absl::StrJoin(a_tiling_, \",\"),\n       absl::StrJoin(b_tiling_, \",\"), absl::StrJoin(lda_, \",\"),\n       absl::StrJoin(lda_tile_, \",\"), absl::StrJoin(ldb_, \",\"),\n-      absl::StrJoin(ldb_tile_, \",\"),\n-      absl::StrJoin(loop_order_, \",\", format_loop_order), outer_block_elems_a_,\n-      outer_block_elems_b_, inner_block_elems_, transformation_str,\n-      scratch_size_, nodes_str);\n+      absl::StrJoin(ldb_tile_, \",\"), outer_block_elems_a_, outer_block_elems_b_,\n+      inner_block_elems_, transformation_str, scratch_size_, chunk_loops_str,\n+      nodes_str);\n }\n \n bool TransposePlanCacheKey::operator==(\n@@ -1340,7 +1374,7 @@ absl::StatusOr<std::shared_ptr<TransposePlan>> TransposePlanCache::GetOrCreate(\n   }\n \n   // Coalesce from slow-varying to fast-varying (outer to inner).\n-  // loop_order_[0] is slowest.\n+  // loops[0] is slowest.\n   int write_pos = 0;\n   for (int read_pos = 1; read_pos < loops.size(); ++read_pos) {\n     Loop& outer = loops[write_pos];"
        },
        {
            "sha": "b0eccc5d37132b16e19a73dc5b27526aaf5fe4fd",
            "filename": "third_party/xla/xla/pjrt/transpose.h",
            "status": "modified",
            "additions": 18,
            "deletions": 7,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.h?ref=6383e3632c91bdc8eccd458f699b317f03968b84",
            "patch": "@@ -175,6 +175,11 @@ class TransposePlan {\n     // Number of parallel threads to use for this loop.\n     int64_t parallelism;\n \n+    // Iteration bounds for this chunk. Initially [0, full_iterations).\n+    // After chunk splitting, each chunk's loops have narrowed bounds.\n+    int64_t start = 0;  // Inclusive start of iteration range\n+    int64_t end = 0;    // Exclusive end of iteration range\n+\n     bool operator==(const Loop& other) const;\n   };\n \n@@ -186,11 +191,17 @@ class TransposePlan {\n   // Performs plan initialization that cannot fail.\n   void Initialize();\n \n-  void BuildPlanNodes(int thread_id, std::vector<Node>& output_nodes);\n+  void BuildPlanNodes(int chunk_id, std::vector<Node>& nodes);\n+\n+  // Chooses a parallelism for each loop. Returns the number of separate chunks\n+  // in the plan, and populates the `parallelism` field of each loop.\n+  int ChooseParallelizationStrategy(std::vector<Loop>& loop_order);\n \n-  // Chooses a parallelism for each loop. Returns the total number of parallel\n-  // work units.\n-  int ChooseParallelizationStrategy();\n+  // Creates per-chunk loop vectors by splitting loop_order_ into per-chunk\n+  // loops. Returns a vector of loop vectors, one per chunk. Each chunk's\n+  // loops have their start/end bounds narrowed to represent that chunk's work.\n+  std::vector<std::vector<Loop>> PartitionLoops(\n+      int num_chunks, const std::vector<Loop>& loop_order);\n \n   // The signature of ExecuteTyped uses char* pointers because we perform\n   // address calculations with strides in bytes; the strides need not be\n@@ -237,9 +248,9 @@ class TransposePlan {\n   bool a_is_tiled_;\n   bool b_is_tiled_;\n \n-  // Order to traverse dimensions, from slowest-varying to fastest-varying.\n-\n-  std::vector<Loop> loop_order_;\n+  // Per-chunk loop nests. Each loop nest has its own start/end bounds\n+  // representing one chunk of the work.\n+  std::vector<std::vector<Loop>> chunk_loops_;\n \n   // Root nodes of the plan, i.e., pointing to the outermost loops in the loop\n   // nest. The outer vector is indexed on the thread ID."
        },
        {
            "sha": "716f5d3bdff220d73f4b2d47a74cad2f52dc5db9",
            "filename": "third_party/xla/xla/pjrt/transpose_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6383e3632c91bdc8eccd458f699b317f03968b84/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose_test.cc?ref=6383e3632c91bdc8eccd458f699b317f03968b84",
            "patch": "@@ -462,7 +462,10 @@ std::vector<TransposeTestCase> GetTransposeTestCases() {\n                         /*permutation=*/{3, 1, 2, 0},\n                         /*input_tiling=*/{},\n                         /*output_tiling=*/{8, 128}),\n-  };\n+      TransposeTestCase{/*dims=*/{129, 1234567},\n+                        /*permutation=*/{0, 1},\n+                        /*input_tiling=*/{},\n+                        /*output_tiling=*/{8, 128}}};\n   return cases;\n }\n "
        }
    ],
    "stats": {
        "total": 233,
        "additions": 141,
        "deletions": 92
    }
}