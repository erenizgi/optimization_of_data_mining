{
    "author": "shawnwang18",
    "message": "PR #35132: [XLA:GPU] Update HLO cublas workspace size after autotuner select the algorithm\n\nImported from GitHub PR https://github.com/openxla/xla/pull/35132\n\nüìù Summary of Changes\n\nThis PR introduces a pass that updates the workspace size for cuBLAS/cuBLASLt GEMM operations after autotuning has selected a specific algorithm. The GemmRewriter pass conservatively allocates workspace before autotuning. After autotuning,we know the exact algorithm selected and can query its actual workspace requirement, potentially reducing memory usage.\n\nüéØ Justification\nPotentially reducing memory usage.\n\nüöÄ Kind of Contribution\nPlease remove what does not apply: ‚ö°Ô∏è Performance Improvement,\n\nüß™ Unit Tests:\nExisting gemm tests should cover the workspace size config.\n\nCopybara import of the project:\n\n--\na6ed2653e758a2a57e9bf2ce994549c5bc3e72d3 by Shawn Wang <shawnw@nvidia.com>:\n\nUpdate cublas workspace size with the exact size extracted from algorithm\n\n--\nd67a48ae705069e68c854f403abc7f1c1a07ef47 by Shawn Wang <shawnw@nvidia.com>:\n\nfix comments\n\n--\n613e0909ff390c5c1962345fc5f36f174e45393f by Shawn Wang <shawnw@nvidia.com>:\n\nadd unittest\n\nMerging this change closes #35132\n\nPiperOrigin-RevId: 845601031",
    "sha": "b73ff25f8ce5de512391e07858bd982e5a68d9b6",
    "files": [
        {
            "sha": "ceac0079baaaa31142233c3086c2ac5dd49a93db",
            "filename": "third_party/xla/xla/autotuning.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fautotuning.proto?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -64,6 +64,7 @@ message AutotuneResult {\n \n   message GemmKey {\n     int64 algorithm = 1;\n+    int64 autotune_workspace_size = 2;\n   }\n \n   // Legacy and unused in new data; superseded by AlgorithmProto."
        },
        {
            "sha": "4f80eff3317db61e3e59f8bdca4c432e632045c5",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -158,6 +158,8 @@ absl::Status CublasBackend::ApplyConfig(HloInstruction& instr,\n                       instr.backend_config<GpuBackendConfig>());\n   GemmBackendConfig& backend_config = *gpu_config.mutable_gemm_backend_config();\n   backend_config.set_selected_algorithm(gemm_key.algorithm());\n+  backend_config.set_autotune_workspace_size(\n+      gemm_key.autotune_workspace_size());\n   TF_RETURN_IF_ERROR(instr.set_backend_config(std::move(gpu_config)));\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "5235c0646073f5dc7f124bf2dbfe1b917157bc17",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -124,6 +124,7 @@ CublasLtBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   for (int i = 0; i < num_algorithms; ++i) {\n     CublasLtBackendConfig gemm_key;\n     gemm_key.set_algorithm(i);\n+    gemm_key.set_autotune_workspace_size(workspace_size);\n     auto any = std::make_unique<google::protobuf::Any>();\n     any->PackFrom(gemm_key);\n     configs.push_back(std::move(any));\n@@ -157,6 +158,8 @@ absl::Status CublasLtBackend::ApplyConfig(HloInstruction& instr,\n                       instr.backend_config<GpuBackendConfig>());\n   GemmBackendConfig& backend_config = *gpu_config.mutable_gemm_backend_config();\n   backend_config.set_selected_algorithm(gemm_key.algorithm());\n+  backend_config.set_autotune_workspace_size(\n+      gemm_key.autotune_workspace_size());\n   TF_RETURN_IF_ERROR(instr.set_backend_config(std::move(gpu_config)));\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "35e1ab249d4489f02ac977b02518c459626a5194",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -1132,8 +1132,9 @@ TEST(CommandBufferThunkTest, CublasLtCmd) {\n   CommandBufferCmdSequence commands;\n   commands.Emplace<CublasLtCmd>(CublasLtMatmulThunk(\n       Thunk::ThunkInfo(), /*canonical_hlo=*/\"\", config.value(),\n-      se::gpu::BlasLt::Epilogue::kDefault, 0, slice_a, slice_b, slice_c,\n-      slice_d, BufferAllocation::Slice(), BufferAllocation::Slice(),\n+      se::gpu::BlasLt::Epilogue::kDefault, /*algorithm_idx=*/0,\n+      /*autotune_workspace_size=*/0, slice_a, slice_b, slice_c, slice_d,\n+      BufferAllocation::Slice(), BufferAllocation::Slice(),\n       BufferAllocation::Slice(), BufferAllocation::Slice(),\n       BufferAllocation::Slice(), BufferAllocation::Slice(),\n       BufferAllocation::Slice(), slice_workspace));"
        },
        {
            "sha": "fd0176489df95715e701fabcaacd70507d3ee36b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 9,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -44,6 +44,7 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(const CublasLtMatmulThunk& rhs)\n       gemm_config_(rhs.gemm_config_),\n       epilogue_(rhs.epilogue_),\n       algorithm_idx_(rhs.algorithm_idx_),\n+      autotune_workspace_size_(rhs.autotune_workspace_size_),\n       canonical_hlo_(rhs.canonical_hlo_),\n       a_(rhs.a_),\n       b_(rhs.b_),\n@@ -61,7 +62,8 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(const CublasLtMatmulThunk& rhs)\n CublasLtMatmulThunk::CublasLtMatmulThunk(\n     Thunk::ThunkInfo thunk_info, std::string canonical_hlo,\n     GemmConfig gemm_config, se::gpu::BlasLt::Epilogue epilogue,\n-    int64_t algorithm_idx, BufferAllocation::Slice a, BufferAllocation::Slice b,\n+    int64_t algorithm_idx, int64_t autotune_workspace_size,\n+    BufferAllocation::Slice a, BufferAllocation::Slice b,\n     BufferAllocation::Slice c, BufferAllocation::Slice d,\n     BufferAllocation::Slice bias, BufferAllocation::Slice aux,\n     BufferAllocation::Slice a_scale, BufferAllocation::Slice b_scale,\n@@ -72,6 +74,7 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(\n       gemm_config_(std::move(gemm_config)),\n       epilogue_(epilogue),\n       algorithm_idx_(algorithm_idx),\n+      autotune_workspace_size_(autotune_workspace_size),\n       canonical_hlo_(std::move(canonical_hlo)),\n       a_(a),\n       b_(b),\n@@ -135,10 +138,11 @@ CublasLtMatmulThunk::GetCachedMatmulPlan(const ExecuteParams& params) {\n \n     TF_ASSIGN_OR_RETURN(auto plan,\n                         blas_lt->GetMatmulPlan(gemm_config_, epilogue_));\n-    // if workspace buffer is not provided, consider only the algorithms which\n-    // do not require a scratch space\n-    int64_t max_workspace =\n-        workspace_.has_value() ? workspace_.value().size() : 0;\n+\n+    // Set the workspace size to the size that was used for autotuning, so\n+    // algorithm index will be the same as returned by GetAlgorithms called\n+    // during autotuning.\n+    int64_t max_workspace = autotune_workspace_size_;\n \n     // If autotuning is disabled, there is no point on retrieving all\n     // algorithms, it's enough to get the default one only.\n@@ -182,6 +186,7 @@ absl::StatusOr<ThunkProto> CublasLtMatmulThunk::ToProto() const {\n   cublas_lt_matmul_thunk->set_epilogue(\n       stream_executor::gpu::BlasLt::EpilogueToProto(epilogue_));\n   cublas_lt_matmul_thunk->set_algorithm_idx(algorithm_idx_);\n+  cublas_lt_matmul_thunk->set_autotune_workspace_size(autotune_workspace_size_);\n   cublas_lt_matmul_thunk->set_canonical_hlo(canonical_hlo_);\n   TF_ASSIGN_OR_RETURN(*cublas_lt_matmul_thunk->mutable_a(), a_.ToProto());\n   TF_ASSIGN_OR_RETURN(*cublas_lt_matmul_thunk->mutable_b(), b_.ToProto());\n@@ -286,10 +291,10 @@ absl::StatusOr<std::unique_ptr<Thunk>> CublasLtMatmulThunk::FromProto(\n   return std::make_unique<CublasLtMatmulThunk>(\n       std::move(thunk_info), std::move(proto.canonical_hlo()),\n       xla::gpu::GemmConfig(std::move(gemm_config)), std::move(epilogue),\n-      proto.algorithm_idx(), std::move(a), std::move(b), std::move(c),\n-      std::move(d), std::move(bias), std::move(aux), std::move(a_scale),\n-      std::move(b_scale), std::move(c_scale), std::move(d_scale),\n-      std::move(d_amax), std::move(workspace));\n+      proto.algorithm_idx(), proto.autotune_workspace_size(), std::move(a),\n+      std::move(b), std::move(c), std::move(d), std::move(bias), std::move(aux),\n+      std::move(a_scale), std::move(b_scale), std::move(c_scale),\n+      std::move(d_scale), std::move(d_amax), std::move(workspace));\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "efd1276d2c9b1d37d9232cdbdc163b40d621ca73",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.h?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -38,6 +38,7 @@ class CublasLtMatmulThunk : public Thunk {\n   CublasLtMatmulThunk(Thunk::ThunkInfo thunk_info, std::string canonical_hlo,\n                       GemmConfig gemm_config,\n                       se::gpu::BlasLt::Epilogue epilogue, int64_t algorithm_idx,\n+                      int64_t autotune_workspace_size,\n                       BufferAllocation::Slice a, BufferAllocation::Slice b,\n                       BufferAllocation::Slice c, BufferAllocation::Slice d,\n                       BufferAllocation::Slice bias /* may be null */,\n@@ -75,6 +76,7 @@ class CublasLtMatmulThunk : public Thunk {\n   GemmConfig gemm_config_;\n   se::gpu::BlasLt::Epilogue epilogue_;\n   int64_t algorithm_idx_;\n+  int64_t autotune_workspace_size_;\n   std::string canonical_hlo_;\n   BufferAllocation::Slice a_;\n   BufferAllocation::Slice b_;"
        },
        {
            "sha": "8fa69ad0f10b36ce127c165cdcd54d7a280830c5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -159,9 +159,9 @@ class GpuBlasLtThunkBuilder {\n     return std::make_unique<CublasLtMatmulThunk>(\n         std::move(thunk_info), std::move(canonical_hlo), std::move(gemm_config),\n         epilogue,\n-        /*algorithm_idx*/ 0, slices[0], slices[1],\n-        has_matrix_bias ? slices[2] : slices.back(), slices.back(), bias,\n-        BufferAllocation::Slice{} /* aux */,\n+        /*algorithm_idx*/ 0, backend_config.autotune_workspace_size(),\n+        slices[0], slices[1], has_matrix_bias ? slices[2] : slices.back(),\n+        slices.back(), bias, BufferAllocation::Slice{} /* aux */,\n         BufferAllocation::Slice{} /* a_scale */,\n         BufferAllocation::Slice{} /* b_scale */,\n         BufferAllocation::Slice{} /* c_scale */,"
        },
        {
            "sha": "6d464b253e4409901caeecceab35d42c95b4a4d3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -309,6 +309,7 @@ message CublasLtMatmulThunkProto {\n   optional xla.buffer_assignment.BufferAllocationSliceProto d_scale = 14;\n   optional xla.buffer_assignment.BufferAllocationSliceProto d_amax = 15;\n   optional xla.buffer_assignment.BufferAllocationSliceProto workspace = 16;\n+  int64 autotune_workspace_size = 17;\n }\n \n message CubSortThunkProto {"
        },
        {
            "sha": "79e13fb1726a28758b72d4a1be5138ccd65010a4",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -1780,6 +1780,7 @@ cc_library(\n         \"//xla/service/gpu/transforms:gemm_fusion\",\n         \"//xla/service/gpu/transforms:gemm_fusion_swap_operands\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n+        \"//xla/service/gpu/transforms:gemm_workspace_rewriter\",\n         \"//xla/service/gpu/transforms:gemv_rewriter\",\n         \"//xla/service/gpu/transforms:hoist_fused_bitcasts\",\n         \"//xla/service/gpu/transforms:layout_assignment\",\n@@ -2184,6 +2185,7 @@ cc_library(\n         \"//xla/service/gpu/transforms:cudnn_norm_rewriter\",\n         \"//xla/service/gpu/transforms:cudnn_pad_for_convolutions\",\n         \"//xla/service/gpu/transforms:cudnn_simplify_padding\",\n+        \"//xla/service/gpu/transforms:gemm_workspace_rewriter\",\n         \"//xla/service/gpu/transforms:triangular_solve_rewriter\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\","
        },
        {
            "sha": "ff1799169002b9e00eaa3084c8ba475685c6edd7",
            "filename": "third_party/xla/xla/service/gpu/backend_configs.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -109,6 +109,9 @@ message GemmBackendConfig {\n   bool damax_output = 18;\n \n   reserved 19;\n+\n+  // The workspace size used during autotuning when the algorithm was selected.\n+  int64 autotune_workspace_size = 20;\n }\n \n // Backend config for bitcast operation generated from MLIR MHLO dialect."
        },
        {
            "sha": "56c680f4de81d78dca2c7d372cbcbcb44d4e4de2",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -91,6 +91,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/cudnn_norm_rewriter.h\"\n #include \"xla/service/gpu/transforms/cudnn_pad_for_convolutions.h\"\n #include \"xla/service/gpu/transforms/cudnn_simplify_padding.h\"\n+#include \"xla/service/gpu/transforms/gemm_workspace_rewriter.h\"\n #include \"xla/service/gpu/transforms/triangular_solve_rewriter.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -375,6 +376,11 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n                             thread_pool, should_autotune, target_config,\n                             options.device_allocator));\n   pipeline->AddPass(std::move(autotuner_pass));\n+\n+  // After autotuning, update GEMM workspace sizes to match the exact\n+  // requirements of the selected algorithms, potentially reducing memory usage.\n+  pipeline->AddPass<GemmWorkspaceRewriter>(gpu_version, stream_exec);\n+\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "d058a7276b1b96d1b90faff63635e31de9550b7e",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -672,8 +672,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCublasLtMatmulThunk(\n       HloPrintOptions::Fingerprint().set_print_backend_config(true));\n   auto thunk = std::make_unique<CublasLtMatmulThunk>(\n       std::move(thunk_info), std::move(canonical_hlo), std::move(gemm_config),\n-      blas_lt_epilogue, algorithm, a, b, c, d, bias, aux, a_scale, b_scale,\n-      c_scale, d_scale, d_amax, workspace_buffer);\n+      blas_lt_epilogue, algorithm, config.autotune_workspace_size(), a, b, c, d,\n+      bias, aux, a_scale, b_scale, c_scale, d_scale, d_amax, workspace_buffer);\n   return GetThunkSequence(std::move(thunk));\n }\n \n@@ -767,8 +767,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCublasLtMatmulThunkF8(\n       HloPrintOptions::Fingerprint().set_print_backend_config(true));\n   auto thunk = std::make_unique<CublasLtMatmulThunk>(\n       std::move(thunk_info), std::move(canonical_hlo), std::move(gemm_config),\n-      blas_lt_epilogue, algorithm, a, b, c, d, bias, aux, a_scale, b_scale,\n-      c_scale, d_scale, d_amax, workspace_buffer);\n+      blas_lt_epilogue, algorithm, config.autotune_workspace_size(), a, b, c, d,\n+      bias, aux, a_scale, b_scale, c_scale, d_scale, d_amax, workspace_buffer);\n   return GetThunkSequence(std::move(thunk));\n }\n "
        },
        {
            "sha": "9802b9648076d53f563e48db1fd37c0545c88047",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -1376,6 +1376,44 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"gemm_workspace_rewriter\",\n+    srcs = [\"gemm_workspace_rewriter.cc\"],\n+    hdrs = [\"gemm_workspace_rewriter.h\"],\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:cublas_cudnn\",\n+        \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/gpu:gpu_blas_lt\",\n+        \"//xla/tsl/platform:status_macros\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"gemm_workspace_rewriter_test\",\n+    srcs = [\"gemm_workspace_rewriter_test.cc\"],\n+    backends = [\"gpu\"],\n+    deps = [\n+        \":gemm_workspace_rewriter\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"gemm_fusion\",\n     srcs = [\"gemm_fusion.cc\"],"
        },
        {
            "sha": "31a17257a3ccd79c246b53f2d32df5e1ba31a6ec",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_workspace_rewriter.cc",
            "status": "added",
            "additions": 240,
            "deletions": 0,
            "changes": 240,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -0,0 +1,240 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/gemm_workspace_rewriter.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+namespace se = ::stream_executor;\n+using se::gpu::BlasLt;\n+\n+namespace {\n+\n+absl::StatusOr<BlasLt::Epilogue> AsBlasLtEpilogue(\n+    GemmBackendConfig_Epilogue epilogue) {\n+  switch (epilogue) {\n+    case GemmBackendConfig::DEFAULT:\n+      return BlasLt::Epilogue::kDefault;\n+    case GemmBackendConfig::RELU:\n+      return BlasLt::Epilogue::kReLU;\n+    case GemmBackendConfig::GELU:\n+      return BlasLt::Epilogue::kGELU;\n+    case GemmBackendConfig::GELU_AUX:\n+      return BlasLt::Epilogue::kGELUWithAux;\n+    case GemmBackendConfig::BIAS:\n+      return BlasLt::Epilogue::kBias;\n+    case GemmBackendConfig::BIAS_RELU:\n+      return BlasLt::Epilogue::kBiasThenReLU;\n+    case GemmBackendConfig::BIAS_GELU:\n+      return BlasLt::Epilogue::kBiasThenGELU;\n+    case GemmBackendConfig::BIAS_GELU_AUX:\n+      return BlasLt::Epilogue::kBiasThenGELUWithAux;\n+    default:\n+      return absl::InternalError(\"Unsupported Epilogue.\");\n+  }\n+}\n+\n+// Visitor that updates workspace sizes for cuBLASLt GEMM operations\n+// based on the selected algorithm's actual workspace requirement.\n+class GemmWorkspaceRewriteVisitor : public DfsHloRewriteVisitor {\n+ public:\n+  explicit GemmWorkspaceRewriteVisitor(\n+      const se::GpuComputeCapability& gpu_version,\n+      se::StreamExecutor* stream_exec)\n+      : gpu_version_(gpu_version), stream_exec_(stream_exec) {}\n+\n+  absl::Status HandleCustomCall(HloInstruction* instr) override {\n+    // Only handle cuBLASLt matmul calls\n+    if (instr->custom_call_target() != kCublasLtMatmulCallTarget &&\n+        instr->custom_call_target() != kCublasLtMatmulF8CallTarget) {\n+      return absl::OkStatus();\n+    }\n+\n+    // Skip if stream executor is not available\n+    if (stream_exec_ == nullptr) {\n+      return absl::OkStatus();\n+    }\n+\n+    // Get the backend config\n+    ASSIGN_OR_RETURN(auto gpu_config,\n+                     instr->backend_config<GpuBackendConfig>());\n+    const GemmBackendConfig& config = gpu_config.gemm_backend_config();\n+\n+    // Skip if no algorithm has been selected (not autotuned yet)\n+    if (config.algorithm_case() != GemmBackendConfig::kSelectedAlgorithm) {\n+      return absl::OkStatus();\n+    }\n+\n+    int64_t selected_algorithm = config.selected_algorithm();\n+\n+    // Get the current output shape - must be a tuple with workspace as last\n+    // element\n+    if (!instr->shape().IsTuple() || instr->shape().tuple_shapes().empty()) {\n+      return absl::OkStatus();\n+    }\n+\n+    // Get the current workspace size\n+    const Shape& current_workspace_shape = instr->shape().tuple_shapes().back();\n+    if (current_workspace_shape.element_type() != S8) {\n+      return absl::OkStatus();\n+    }\n+    int64_t current_workspace_size =\n+        ShapeUtil::ByteSizeOf(current_workspace_shape);\n+\n+    // Create GemmConfig to get the matmul plan\n+    ASSIGN_OR_RETURN(GemmConfig gemm_config,\n+                     GemmConfig::For(instr, gpu_version_));\n+\n+    // Get the epilogue\n+    ASSIGN_OR_RETURN(BlasLt::Epilogue epilogue,\n+                     AsBlasLtEpilogue(config.epilogue()));\n+\n+    // Create a stream to query algorithms\n+    ASSIGN_OR_RETURN(std::unique_ptr<se::Stream> stream,\n+                     stream_exec_->CreateStream());\n+\n+    // Get the matmul plan\n+    ASSIGN_OR_RETURN(\n+        std::unique_ptr<BlasLt::MatmulPlan> plan,\n+        se::gpu::BlasLt::GetMatmulPlan(stream.get(), gemm_config, epilogue));\n+\n+    // Query algorithms with the current workspace size limit\n+    ASSIGN_OR_RETURN(\n+        std::vector<BlasLt::MatmulAlgorithm> algorithms,\n+        plan->GetAlgorithms(stream.get(), GemmConfig::kNumAlgorithms,\n+                            current_workspace_size));\n+\n+    // Verify that the selected algorithm index is valid\n+    if (selected_algorithm < 0 ||\n+        selected_algorithm >= static_cast<int64_t>(algorithms.size())) {\n+      VLOG(3) << \"Selected algorithm index \" << selected_algorithm\n+              << \" is out of range for \" << instr->name()\n+              << \", skipping workspace update.\";\n+      return absl::OkStatus();\n+    }\n+\n+    // Get the actual workspace size for the selected algorithm\n+    int64_t actual_workspace_size =\n+        static_cast<int64_t>(algorithms[selected_algorithm].workspace_size);\n+\n+    // If the workspace size is already optimal, nothing to do\n+    if (actual_workspace_size == current_workspace_size) {\n+      return absl::OkStatus();\n+    }\n+\n+    // Ensure we're not increasing the workspace size\n+    if (actual_workspace_size > current_workspace_size) {\n+      VLOG(3) << \"Algorithm workspace size (\" << actual_workspace_size\n+              << \") exceeds current allocation (\" << current_workspace_size\n+              << \") for \" << instr->name() << \", skipping update.\";\n+      return absl::OkStatus();\n+    }\n+\n+    VLOG(2) << \"Updating workspace size for \" << instr->name() << \" from \"\n+            << current_workspace_size << \" to \" << actual_workspace_size;\n+\n+    // Build the new output shape with updated workspace size\n+    Shape new_output_shape = instr->shape();\n+    *new_output_shape.mutable_tuple_shapes(\n+        new_output_shape.tuple_shapes().size() - 1) =\n+        ShapeUtil::MakeShape(S8, {actual_workspace_size});\n+\n+    // Clone the instruction with the new shape\n+    HloInstruction* new_call = instr->AddInstruction(\n+        instr->CloneWithNewOperands(new_output_shape, instr->operands()));\n+\n+    // Update operand aliasing if present\n+    auto* custom_call = Cast<HloCustomCallInstruction>(new_call);\n+    if (!custom_call->output_to_operand_aliasing().empty()) {\n+      custom_call->set_output_to_operand_aliasing(\n+          Cast<HloCustomCallInstruction>(instr)->output_to_operand_aliasing());\n+    }\n+\n+    // Collect users first to avoid modifying during iteration\n+    std::vector<HloInstruction*> users(instr->users().begin(),\n+                                       instr->users().end());\n+\n+    // Replace all users of the old instruction\n+    for (HloInstruction* user : users) {\n+      HloGetTupleElementInstruction* user_get_tuple =\n+          DynCast<HloGetTupleElementInstruction>(user);\n+      if (user_get_tuple == nullptr) {\n+        continue;\n+      }\n+      HloInstruction* get_output =\n+          instr->AddInstruction(HloInstruction::CreateGetTupleElement(\n+              new_call, user_get_tuple->tuple_index()));\n+      RETURN_IF_ERROR(ReplaceInstruction(user_get_tuple, get_output));\n+    }\n+\n+    MarkAsChanged();\n+    return absl::OkStatus();\n+  }\n+\n+ private:\n+  se::GpuComputeCapability gpu_version_;\n+  se::StreamExecutor* stream_exec_;\n+};\n+\n+}  // namespace\n+\n+absl::StatusOr<bool> GemmWorkspaceRewriter::RunImpl(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  // Skip if stream executor is not available\n+  if (stream_exec_ == nullptr) {\n+    VLOG(2) << \"Stream executor not available, skipping workspace rewrite.\";\n+    return false;\n+  }\n+\n+  bool changed = false;\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations(execution_threads)) {\n+    GemmWorkspaceRewriteVisitor visitor(gpu_version_, stream_exec_);\n+    RETURN_IF_ERROR(computation->Accept(&visitor));\n+    changed |= visitor.changed();\n+  }\n+  return changed;\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "b7c5e3e5f47feb6e7d4d7039ab6cabba4d81f76f",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_workspace_rewriter.h",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter.h?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -0,0 +1,55 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef XLA_SERVICE_GPU_TRANSFORMS_GEMM_WORKSPACE_REWRITER_H_\n+#define XLA_SERVICE_GPU_TRANSFORMS_GEMM_WORKSPACE_REWRITER_H_\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// This pass updates the workspace size for cuBLAS/cuBLASLt GEMM operations\n+// after autotuning has selected a specific algorithm. The GemmRewriter pass\n+// conservatively allocates workspace before autotuning. After autotuning,\n+// we know the exact algorithm selected and can query its actual workspace\n+// requirement, potentially reducing memory usage.\n+class GemmWorkspaceRewriter : public HloModulePass {\n+ public:\n+  explicit GemmWorkspaceRewriter(const se::GpuComputeCapability& gpu_version,\n+                                 stream_executor::StreamExecutor* stream_exec)\n+      : gpu_version_(gpu_version), stream_exec_(stream_exec) {}\n+\n+  absl::string_view name() const override { return \"gemm-workspace-rewriter\"; }\n+\n+ protected:\n+  absl::StatusOr<bool> RunImpl(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+\n+ private:\n+  se::GpuComputeCapability gpu_version_;\n+  stream_executor::StreamExecutor* stream_exec_;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_SERVICE_GPU_TRANSFORMS_GEMM_WORKSPACE_REWRITER_H_"
        },
        {
            "sha": "e8028932f2ad5c083ccbc314cb0c407d1306213f",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_workspace_rewriter_test.cc",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b73ff25f8ce5de512391e07858bd982e5a68d9b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_workspace_rewriter_test.cc?ref=b73ff25f8ce5de512391e07858bd982e5a68d9b6",
            "patch": "@@ -0,0 +1,69 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/gemm_workspace_rewriter.h\"\n+\n+#include <memory>\n+\n+#include <gtest/gtest.h>\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+namespace se = ::stream_executor;\n+\n+class GemmWorkspaceRewriterTest : public GpuCodegenTest {};\n+\n+// Tests that cuBLASLt calls with a selected algorithm and large workspace\n+// are rewritten to use a smaller workspace.\n+TEST_F(GemmWorkspaceRewriterTest,\n+       CublasLtCallWithSelectedAlgorithmIsRewritten) {\n+  // This HLO simulates a cuBLASLt matmul after autotuning - it has\n+  // selected_algorithm set and a conservatively large workspace.\n+  const char* hlo_text = R\"(\n+HloModule TestModule\n+\n+ENTRY main {\n+  lhs = f32[32,64] parameter(0)\n+  rhs = f32[64,128] parameter(1)\n+  custom_call = (f32[32,128], s8[4194304]) custom-call(lhs, rhs),\n+    custom_call_target=\"__cublas$lt$matmul\",\n+    backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"gemm_backend_config\":{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\",\"selected_algorithm\":\"0\"}}\n+  ROOT result = f32[32,128] get-tuple-element(custom_call), index=0\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+\n+  se::StreamExecutor* stream_exec = backend().default_stream_executor();\n+  GemmWorkspaceRewriter pass(\n+      stream_exec->GetDeviceDescription().gpu_compute_capability(),\n+      stream_exec);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, pass.Run(module.get()));\n+\n+  // The pass should reduce the workspace size from 4MB to the algorithm's\n+  // actual requirement (typically much smaller).\n+  EXPECT_TRUE(changed);\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 464,
        "additions": 446,
        "deletions": 18
    }
}