{
    "author": "ermilovmaxim",
    "message": "First step to introduce GpuComputeCapability custom class instead of std::variant\n\nPiperOrigin-RevId: 820940828",
    "sha": "4a42fca8689cc30fb1f10fdb45cd604e13065cae",
    "files": [
        {
            "sha": "079500b8cd1ccfe8fe41ac6e80826825c461a83e",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fgpu_kernel_to_blob_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fgpu_kernel_to_blob_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fgpu_kernel_to_blob_pass.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -163,9 +163,11 @@ class GpuKernelToBlobPass\n         target->Options.AllowFPOpFusion =\n             llvm::FPOpFusion::FPOpFusionMode::Fast;\n       };\n-      TF_ASSIGN_OR_RETURN(std::string ptx, xla::gpu::nvptx::CompileToPtx(\n-                                               llvm_module_copy.get(), cc,\n-                                               options, enable_fusion));\n+      TF_ASSIGN_OR_RETURN(\n+          std::string ptx,\n+          xla::gpu::nvptx::CompileToPtx(\n+              llvm_module_copy.get(), stream_executor::GpuComputeCapability(cc),\n+              options, enable_fusion));\n       if (print_ptx_) {\n         llvm::dbgs() << \"Generated PTX code for module '\"\n                      << gpu_module.getName() << \"' on architecture sm_\" << arch"
        },
        {
            "sha": "3aa8fa1003fbb767dd1e402e763f06243834544f",
            "filename": "tensorflow/core/common_runtime/gpu/gpu_device_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device_test.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -68,12 +68,12 @@ se::CudaComputeCapability GetComputeCapability() {\n }\n \n bool IsRocm() {\n-  return std::holds_alternative<se::RocmComputeCapability>(\n-      se::GPUMachineManager()\n-          ->ExecutorForDevice(0)\n-          .value()\n-          ->GetDeviceDescription()\n-          .gpu_compute_capability());\n+  return se::GPUMachineManager()\n+      ->ExecutorForDevice(0)\n+      .value()\n+      ->GetDeviceDescription()\n+      .gpu_compute_capability()\n+      .IsRocm();\n }\n \n void ExpectErrorMessageSubstr(const Status& s, StringPiece substr) {"
        },
        {
            "sha": "4e6a8d5266608d8ed8c53bf23eb608c47f9efeeb",
            "filename": "tensorflow/core/kernels/matmul_op_fused.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_fused.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_fused.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_fused.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -516,7 +516,7 @@ struct LaunchFusedMatMulOp<GPUDevice, T> {\n \n     const auto& cc =\n         stream->parent()->GetDeviceDescription().gpu_compute_capability();\n-    if (auto* procm = std::get_if<se::RocmComputeCapability>(&cc)) {\n+    if (auto* procm = cc.rocm_compute_capability()) {\n       use_cudnn = !procm->gfx9_mi200_or_later();\n     }\n     BlasScratchAllocator scratch_allocator(context);"
        },
        {
            "sha": "e81cb4e2b8cf7a72b1c350e44f8d99a22a2205ae",
            "filename": "tensorflow/core/kernels/matmul_op_impl.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -604,7 +604,7 @@ struct LaunchBatchMatMul<GPUDevice, Scalar> {\n \n     const auto& cc =\n         stream->parent()->GetDeviceDescription().gpu_compute_capability();\n-    if (auto* procm = std::get_if<se::RocmComputeCapability>(&cc)) {\n+    if (auto* procm = cc.rocm_compute_capability()) {\n       bCublasLtSupport = procm->gfx9_mi200_or_later();\n     }\n "
        },
        {
            "sha": "3513014133f1c73d5c9f1df3ca2530014b64a8c5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -2000,8 +2000,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     mlir::MLIRContext& mlir_context, bool is_xla_fusion, bool emit_kernel) {\n   const auto& gpu_cc = device_info.gpu_compute_capability();\n   TF_RETURN_IF_ERROR(CheckAtLeastAmpere(gpu_cc));\n-  std::string arch_name =\n-      std::visit([](auto& cc) { return cc.ToString(); }, gpu_cc);\n+  std::string arch_name = gpu_cc.ToString();\n \n   const HloModuleConfig& hlo_config = hlo_module.config();\n "
        },
        {
            "sha": "726530cf9171cc181172c2f10a8e617754fe5e98",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 29,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -68,38 +68,25 @@ bool IsTritonSupportedDotOutputType(\n     case F32:\n       return true;\n     case F8E5M2:\n-      return std::visit(\n-          absl::Overload(\n-              [](const se::CudaComputeCapability& cc) {\n-                return cc.IsAtLeastAmpere();\n-              },\n-              [](const se::RocmComputeCapability& cc) { return false; }),\n-          gpu_version);\n-\n+      if (auto ptr = gpu_version.cuda_compute_capability()) {\n+        return ptr->IsAtLeastAmpere();\n+      }\n+      return false;\n     case F8E4M3FN:\n-      return std::visit(\n-          absl::Overload(\n-              [](const se::CudaComputeCapability& cc) {\n-                return cc.IsAtLeastHopper();\n-              },\n-              [](const se::RocmComputeCapability& cc) { return false; }),\n-          gpu_version);\n+      if (auto ptr = gpu_version.cuda_compute_capability()) {\n+        return ptr->IsAtLeastHopper();\n+      }\n+      return false;\n     case BF16:\n-      return std::visit(\n-          absl::Overload(\n-              [](const se::CudaComputeCapability& cc) { return true; },\n-              [](const se::RocmComputeCapability& cc) {\n-                return cc.has_bf16_dtype_support();\n-              }),\n-          gpu_version);\n+      if (auto ptr = gpu_version.rocm_compute_capability()) {\n+        return ptr->has_bf16_dtype_support();\n+      }\n+      return true;\n     case S32:\n-      return std::visit(\n-          absl::Overload(\n-              [](const se::CudaComputeCapability& cc) {\n-                return cc.IsAtLeastAmpere();\n-              },\n-              [](const se::RocmComputeCapability& cc) { return false; }),\n-          gpu_version);\n+      if (auto ptr = gpu_version.cuda_compute_capability()) {\n+        return ptr->IsAtLeastAmpere();\n+      }\n+      return false;\n     default:\n       return false;\n   }"
        },
        {
            "sha": "f3b558de34e2516a3ac4dad06c0000118de1ff7f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -100,19 +100,16 @@ CommandBufferConfig GetCommandBufferConfig(\n   };\n \n   // Check if CUDA/ROCM driver supports required features.\n-  auto erase_cuda = [&](const se::CudaComputeCapability& cuda_comp) {\n+  if (device_info.gpu_compute_capability().IsCuda()) {\n     if (std::min(device_info.runtime_version(), device_info.driver_version()) <\n         se::SemanticVersion{12, 3, 0}) {\n       erase(kRequireTracing);       // cuStreamBeginCaptureToGraph\n       erase(kRequireConditionals);  // on-device control flow\n     }\n-  };\n-  auto erase_rocm = [&](const se::RocmComputeCapability& rocm_comp) {\n+  }\n+  if (device_info.gpu_compute_capability().IsRocm()) {\n     erase(kRequireConditionals);  // on-device control flow\n-  };\n-\n-  std::visit(absl::Overload(erase_cuda, erase_rocm),\n-             device_info.gpu_compute_capability());\n+  }\n \n   return config;\n }"
        },
        {
            "sha": "305b9bcb723b3165093e9053af8df1ce485f1c8b",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -1388,6 +1388,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"@com_google_absl//absl/functional:overload\",\n     ],\n )"
        },
        {
            "sha": "fcab22e65ccc6c8c29f7750ad77fda63425b9318",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -146,16 +146,14 @@ CompileModuleResults InitializeResults(const HloModule* hlo_module,\n }\n \n std::string GetDumpName(const se::DeviceDescription& device_desc) {\n-  struct GetCcStr {\n-    std::string operator()(const se::CudaComputeCapability& cc) const {\n-      return absl::StrCat(\"sm_\", cc.ToString());\n-    }\n-    std::string operator()(const se::RocmComputeCapability& cc) const {\n-      return cc.gfx_version();\n-    }\n-  };\n-  std::string prefix =\n-      std::visit(GetCcStr(), device_desc.gpu_compute_capability());\n+  std::string prefix;\n+  if (auto* cc =\n+          device_desc.gpu_compute_capability().cuda_compute_capability()) {\n+    prefix = absl::StrCat(\"sm_\", cc->ToString());\n+  } else if (auto* cc = device_desc.gpu_compute_capability()\n+                            .rocm_compute_capability()) {\n+    prefix = cc->gfx_version();\n+  }\n   return absl::StrCat(prefix, \"_gpu_\", kAfterOptimizationsDumpName);\n }\n "
        },
        {
            "sha": "7f538a63124797f0db454439cfadbdd2c8bf8c40",
            "filename": "third_party/xla/xla/service/gpu/cublas_padding_requirements.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n@@ -33,26 +34,24 @@ namespace {\n \n bool DimensionRequiresPadding(const int64_t size, const PrimitiveType data_type,\n                               const se::GpuComputeCapability& gpu_cc) {\n-  return std::visit(\n-      absl::Overload(\n-          [&](const se::CudaComputeCapability& cc) {\n-            for (const auto& req : CublasPaddingRequirements) {\n-              if (cc.SupportsAllFeaturesOf(req.min_compute_capability) &&\n-                  data_type == req.data_type && size % req.multiple_of != 0) {\n-                return true;\n-              }\n-            }\n-            return false;\n-          },\n-          [&](const se::RocmComputeCapability& cc) {\n-            for (const auto& req : HipblasPaddingRequirements) {\n-              if (data_type == req.data_type && size % req.multiple_of != 0) {\n-                return true;\n-              }\n-            }\n-            return false;\n-          }),\n-      gpu_cc);\n+  if (const se::CudaComputeCapability* cc = gpu_cc.cuda_compute_capability()) {\n+    for (const auto& req : CublasPaddingRequirements) {\n+      if (cc->SupportsAllFeaturesOf(req.min_compute_capability) &&\n+          data_type == req.data_type && size % req.multiple_of != 0) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+  if (const se::RocmComputeCapability* cc = gpu_cc.rocm_compute_capability()) {\n+    for (const auto& req : HipblasPaddingRequirements) {\n+      if (data_type == req.data_type && size % req.multiple_of != 0) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+  return false;\n }\n \n bool ShapeRequiresPadding(const Shape& shape, int batch_dimensions_size,"
        },
        {
            "sha": "4fc8c1c87d22047b953303658c4ebfbae8c406e9",
            "filename": "third_party/xla/xla/service/gpu/float_support_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffloat_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffloat_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffloat_support_test.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -75,15 +75,9 @@ ENTRY e {\n }\n \n TEST_F(FloatSupportTestWithTriton, MixedTypeDotWithBF16IsNotUpcasted) {\n-  bool skip_test =\n-      std::visit(absl::Overload(\n-                     [](const se::CudaComputeCapability& cc) {\n-                       return !cc.IsAtLeast(se::CudaComputeCapability::kAmpere);\n-                     },\n-                     [](const se::RocmComputeCapability&) { return true; }),\n-                 GetGpuComputeCapability());\n-\n-  if (skip_test) {\n+  if (GetGpuComputeCapability().IsRocm() ||\n+      !GetGpuComputeCapability().cuda_compute_capability()->IsAtLeast(\n+          se::CudaComputeCapability::kAmpere)) {\n     GTEST_SKIP() << \"Not supported on this GPU architecture\";\n   }\n "
        },
        {
            "sha": "defcce91630be80b7703ba71f5e280bf2882a989",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 17,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -116,24 +116,21 @@ class AnalyticalLatencyHidingSchedulerTest : public GpuCodegenTest {\n \n TEST_F(AnalyticalLatencyHidingSchedulerTest, TestAnalyticalLatencyEstimator) {\n   auto gpu_compute_capability = GetGpuComputeCapability();\n-  auto visitor = [](const auto& c) {\n-    using cc = std::remove_const_t<std::remove_reference_t<decltype(c)>>;\n-    if constexpr (std::is_same_v<stream_executor::CudaComputeCapability, cc>) {\n-      if (!c.IsAtLeast(se::CudaComputeCapability::kPascal)) {\n-        GTEST_SKIP() << \"This test is for Pascal+ GPUs.\";\n-      }\n-      if (c.major == 12 && c.minor == 1) {\n-        // Skip this test for Spark. Because of the AllReduce, the test uses\n-        // gpu_collective_performance_model, which only makes sense in a\n-        // datacenter network setting.\n-        GTEST_SKIP() << \"This test is for datacenter GPUs.\";\n-      }\n-    } else if (!std::is_same_v<stream_executor::RocmComputeCapability, cc>) {\n-      GTEST_SKIP() << \"This test is for Pascal+ GPUs.\";\n-    }\n-  };\n+  if (gpu_compute_capability.IsRocm()) {\n+    GTEST_SKIP() << \"This test is for Pascal+ GPUs.\";\n+  }\n+\n+  auto* c = gpu_compute_capability.cuda_compute_capability();\n+  if (!c->IsAtLeast(se::CudaComputeCapability::kPascal)) {\n+    GTEST_SKIP() << \"This test is for Pascal+ GPUs.\";\n+  }\n+  if (c->major == 12 && c->minor == 1) {\n+    // Skip this test for Spark. Because of the AllReduce, the test uses\n+    // gpu_collective_performance_model, which only makes sense in a\n+    // datacenter network setting.\n+    GTEST_SKIP() << \"This test is for datacenter GPUs.\";\n+  }\n \n-  std::visit(visitor, gpu_compute_capability);\n   const se::DeviceDescription dev_info =\n       backend().default_stream_executor()->GetDeviceDescription();\n "
        },
        {
            "sha": "5c5bdeeb3f1191a69cf87993696e487072c1d371",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -440,11 +440,15 @@ GpuPerformanceWithCollectiveModel::ComputeAllreduceTime(\n     const se::DeviceDescription& gpu_device_info) {\n   // We use nccl group call to launch multiple allreduces so launch overhead\n   // only occurs once.\n-  const auto visitor = [&](const auto& cc) {\n+  if (auto ptr =\n+          gpu_device_info.gpu_compute_capability().cuda_compute_capability()) {\n     return ComputeAllreduceTimeImpl(instr, cost_analysis, gpu_device_info,\n-                                    CreateSettings(cc));\n-  };\n-  return std::visit(visitor, gpu_device_info.gpu_compute_capability());\n+                                    CreateSettings(*ptr));\n+  }\n+  return ComputeAllreduceTimeImpl(\n+      instr, cost_analysis, gpu_device_info,\n+      CreateSettings(\n+          *gpu_device_info.gpu_compute_capability().rocm_compute_capability()));\n }\n \n /*static*/ absl::Duration"
        },
        {
            "sha": "31d5409410a60f1f9c109572769786c93aa3f02b",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -378,6 +378,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\","
        },
        {
            "sha": "96fa801f5b33a8cc19e19d2cbdcaa105adacb8f0",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -843,20 +844,19 @@ absl::StatusOr<bool> CommandBufferScheduling::Run(\n   };\n \n   // Check if CUDA/ROCM driver supports required features.\n-  auto erase_cuda = [&](const se::CudaComputeCapability& cuda_comp) {\n+  if (auto* cuda_comp = device_description_.gpu_compute_capability()\n+                            .cuda_compute_capability()) {\n     if (std::min(device_description_.runtime_version(),\n                  device_description_.driver_version()) <\n         se::SemanticVersion{12, 3, 0}) {\n       erase(kRequireTracing);       // cuStreamBeginCaptureToGraph\n       erase(kRequireConditionals);  // on-device control flow\n     }\n-  };\n-  auto erase_rocm = [&](const se::RocmComputeCapability& rocm_comp) {\n+  } else if (const se::RocmComputeCapability* rocm_comp =\n+                 device_description_.gpu_compute_capability()\n+                     .rocm_compute_capability()) {\n     erase(kRequireConditionals);  // on-device control flow\n-  };\n-\n-  std::visit(absl::Overload(erase_cuda, erase_rocm),\n-             device_description_.gpu_compute_capability());\n+  }\n \n   auto order = module->MakeComputationPostOrder();\n   std::reverse(order.begin(), order.end());"
        },
        {
            "sha": "391cad3f7ebebce0fe430f0a91959b05d8639eae",
            "filename": "third_party/xla/xla/stream_executor/device_description.h",
            "status": "modified",
            "additions": 62,
            "deletions": 50,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a42fca8689cc30fb1f10fdb45cd604e13065cae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h?ref=4a42fca8689cc30fb1f10fdb45cd604e13065cae",
            "patch": "@@ -36,8 +36,35 @@ limitations under the License.\n \n namespace stream_executor {\n \n-using GpuComputeCapability =\n-    std::variant<CudaComputeCapability, RocmComputeCapability>;\n+class GpuComputeCapability\n+    : public std::variant<CudaComputeCapability, RocmComputeCapability> {\n+ public:\n+  using std::variant<CudaComputeCapability, RocmComputeCapability>::variant;\n+  using std::variant<CudaComputeCapability, RocmComputeCapability>::operator=;\n+\n+  bool IsCuda() const {\n+    return std::holds_alternative<CudaComputeCapability>(*this);\n+  }\n+\n+  bool IsRocm() const {\n+    return std::holds_alternative<RocmComputeCapability>(*this);\n+  }\n+\n+  const CudaComputeCapability* cuda_compute_capability() const {\n+    return std::get_if<CudaComputeCapability>(this);\n+  }\n+\n+  const RocmComputeCapability* rocm_compute_capability() const {\n+    return std::get_if<RocmComputeCapability>(this);\n+  }\n+\n+  std::string ToString() const {\n+    if (auto ptr = cuda_compute_capability()) {\n+      return ptr->ToString();\n+    }\n+    return rocm_compute_capability()->ToString();\n+  }\n+};\n \n // Data that describes the execution target of the StreamExecutor, in terms of\n // important logical parameters. These include dimensionality limits and\n@@ -193,60 +220,45 @@ class DeviceDescription {\n   // also we do not count what occupies cache, but rather claim that what is\n   // much smaller than the cache size will likely stay in it.\n   constexpr int64_t l1_cache_size_per_SM() const {\n-    return std::visit(\n-        [](const auto& capability) -> int64_t {\n-          if constexpr (std::is_same_v<std::decay_t<decltype(capability)>,\n-                                       RocmComputeCapability>) {\n-            // MI100 and MI200 has 16KB L1 cache per CU.\n-            if (capability.gfx9_mi100() || capability.gfx9_mi200()) {\n-              return 16 * 1024;\n-            }\n-            // MI300 has 32KB L1 cache per CU.\n-            if (capability.gfx9_mi300_series()) {\n-              return 32 * 1024;\n-            }\n-          }\n-          // Default return for other GPUs (e.g., RTX A6000).\n-          return 2 * 1024;\n-        },\n-        gpu_compute_capability_);\n+    if (auto* capability = gpu_compute_capability_.rocm_compute_capability()) {\n+      // MI100 and MI200 has 16KB L1 cache per CU.\n+      if (capability->gfx9_mi100() || capability->gfx9_mi200()) {\n+        return 16 * 1024;\n+      }\n+      // MI300 has 32KB L1 cache per CU.\n+      if (capability->gfx9_mi300_series()) {\n+        return 32 * 1024;\n+      }\n+    }\n+    // Default return for other GPUs (e.g., RTX A6000).\n+    return 2 * 1024;\n   }\n \n   constexpr int64_t dram_to_l2_transaction_size_bytes() const {\n-    return std::visit(\n-        [](const auto& capability) -> int {\n-          if constexpr (std::is_same_v<std::decay_t<decltype(capability)>,\n-                                       RocmComputeCapability>) {\n-            // DRAM->L2 bus is 128 Byte width for MI300.\n-            if (capability.gfx9_mi300_series()) {\n-              return 128;\n-            }\n-          }\n-          // Cache line is 128B that is split into 4 sectors of 32B. Default\n-          // transaction size from DRAM -> L2 = 64 Bytes = 2 sectors, since\n-          // V100, but it can be also configured.\n-          // https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21819-optimizing-applications-for-nvidia-ampere-gpu-architecture.pdf\n-          // (page 10).\n-          // return 64 Bytes by default.\n-          return 64;\n-        },\n-        gpu_compute_capability_);\n+    if (auto* capability = gpu_compute_capability_.rocm_compute_capability()) {\n+      // DRAM->L2 bus is 128 Byte width for MI300.\n+      if (capability->gfx9_mi300_series()) {\n+        return 128;\n+      }\n+    }\n+    // Cache line is 128B that is split into 4 sectors of 32B. Default\n+    // transaction size from DRAM -> L2 = 64 Bytes = 2 sectors, since\n+    // V100, but it can be also configured.\n+    // https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21819-optimizing-applications-for-nvidia-ampere-gpu-architecture.pdf\n+    // (page 10).\n+    // return 64 Bytes by default.\n+    return 64;\n   }\n \n   constexpr int64_t memory_transactions_per_clock() const {\n-    return std::visit(\n-        [](const auto& capability) -> int {\n-          if constexpr (std::is_same_v<std::decay_t<decltype(capability)>,\n-                                       RocmComputeCapability>) {\n-            // 16 works well on MI300.\n-            if (capability.gfx9_mi300_series()) {\n-              return 16;\n-            }\n-          }\n-          // Default return for other GPUs.\n-          return 32;\n-        },\n-        gpu_compute_capability_);\n+    if (auto* capability = gpu_compute_capability_.rocm_compute_capability()) {\n+      // 16 works well on MI300.\n+      if (capability->gfx9_mi300_series()) {\n+        return 16;\n+      }\n+    }\n+    // Default return for other GPUs.\n+    return 32;\n   }\n \n   GpuDeviceInfoProto ToGpuProto() const;"
        }
    ],
    "stats": {
        "total": 323,
        "additions": 157,
        "deletions": 166
    }
}