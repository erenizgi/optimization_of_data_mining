{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Add support to emit 0D tensor constants in the tiled emitter.\n\nOne of a chain of commits to remove the special casing for 0D tensors so that the triton specific requirement is moved to the lowering stage, e.g. the upcoming XTile::CPU backend doesn't have such a requirement, and consistently using tensors makes it simpler.\n\nPiperOrigin-RevId: 828890807",
    "sha": "dac01e1d48eb446966f8e6b7c20781f90f9139ff",
    "files": [
        {
            "sha": "97c767c817066e0123ceb4f53d94dd80dbcd8c70",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=dac01e1d48eb446966f8e6b7c20781f90f9139ff",
            "patch": "@@ -88,11 +88,9 @@ using AlgorithmEmitter = absl::StatusOr<Value> (*)(EmitterLocOpBuilder,\n // must override any accumulated result if the last partial product is\n // non-finite. See b/115844437.\n Value ZeroNaNs(EmitterLocOpBuilder b, Value input) {\n-  Value positive_inf =\n-      CreateConst<float>(b, b.getF32Type(),\n-                         std::numeric_limits<float>::infinity(),\n-                         mlir::cast<ShapedType>(input.getType()).getShape())\n-          .UnwrapTensor();\n+  Value positive_inf = CreateConst<float>(\n+      b, b.getF32Type(), std::numeric_limits<float>::infinity(),\n+      mlir::cast<ShapedType>(input.getType()).getShape());\n   Value abs_input = b.create<math::AbsFOp>(input);\n   Value is_finite = b.create<arith::CmpFOp>(arith::CmpFPredicate::OGT,\n                                             positive_inf, abs_input);"
        },
        {
            "sha": "58b640267cd8db5ab1653c6202d2e7ae234d1956",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=dac01e1d48eb446966f8e6b7c20781f90f9139ff",
            "patch": "@@ -260,20 +260,18 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n     }\n     // The current logic handles signed integer types only. Additional handling\n     // is needed for unsigned integer types.\n-    auto cst_int = [&](int64_t x) {\n+    auto cst_int = [&](int64_t x) -> Value {\n       if (auto src_shaped_ty = mlir::dyn_cast<ShapedType>(src_ty)) {\n-        return CreateConst(b, dst_element_ty, x, src_shaped_ty.getShape())\n-            .UnwrapUnsafe();\n+        return CreateConst(b, dst_element_ty, x, src_shaped_ty.getShape());\n       } else {\n-        return CreateConst(b, dst_element_ty, x).UnwrapUnsafe();\n+        return CreateConst(b, dst_element_ty, x);\n       }\n     };\n-    auto cst_float = [&](int64_t x) {\n+    auto cst_float = [&](int64_t x) -> Value {\n       if (auto src_shaped_ty = mlir::dyn_cast<ShapedType>(src_ty)) {\n-        return CreateConst(b, src_fp_element_ty, x, src_shaped_ty.getShape())\n-            .UnwrapUnsafe();\n+        return CreateConst(b, src_fp_element_ty, x, src_shaped_ty.getShape());\n       } else {\n-        return CreateConst(b, src_fp_element_ty, x).UnwrapUnsafe();\n+        return CreateConst(b, src_fp_element_ty, x);\n       }\n     };\n     auto fptosi = b.create<ma::FPToSIOp>(dst_ty, value);\n@@ -551,8 +549,8 @@ absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n   }\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitConstant(EmitterLocOpBuilder& b,\n-                                            const HloInstruction& constant) {\n+absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n+    EmitterLocOpBuilder& b, const HloInstruction& constant) {\n   TF_ASSIGN_OR_RETURN(Type ty, TritonType(b, constant.shape().element_type()));\n   llvm::SmallVector<int64_t> shape{constant.shape().dimensions().begin(),\n                                    constant.shape().dimensions().end()};"
        },
        {
            "sha": "c2ecc322389d5192172d82e1b13d54adadc01983",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 15,
            "deletions": 22,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=dac01e1d48eb446966f8e6b7c20781f90f9139ff",
            "patch": "@@ -128,47 +128,41 @@ T ScalarConstantValue(const HloInstruction& instr, PrimitiveType dst_type) {\n \n // Create a scalar constant.\n template <typename T>\n-ScalarOrTensor CreateConst(EmitterLocOpBuilder& b, mlir::Type type, T value) {\n+mlir::Value CreateConst(EmitterLocOpBuilder& b, mlir::Type type, T value) {\n   if (mlir::isa<mlir::IntegerType>(type)) {\n-    auto result =\n-        b.create<mlir::arith::ConstantOp>(b.getIntegerAttr(type, value));\n-    return ScalarOrTensor(result);\n+    return b.create<mlir::arith::ConstantOp>(b.getIntegerAttr(type, value));\n   }\n \n   if (mlir::isa<mlir::IndexType>(type)) {\n-    auto result = b.create<mlir::arith::ConstantOp>(b.getIndexAttr(value));\n-    return ScalarOrTensor(result);\n+    return b.create<mlir::arith::ConstantOp>(b.getIndexAttr(value));\n   }\n \n   if (mlir::isa<mlir::FloatType>(type)) {\n-    auto result = b.create<mlir::arith::ConstantOp>(\n+    return b.create<mlir::arith::ConstantOp>(\n         b.getFloatAttr(type, static_cast<double>(value)));\n-    return ScalarOrTensor(result);\n   }\n   LOG(FATAL) << \"Constant type not supported: \" << llvm_ir::DumpToString(type);\n }\n \n // Create a tensor constant.\n template <typename T>\n-ScalarOrTensor CreateConst(EmitterLocOpBuilder& b, mlir::Type type, T value,\n-                           llvm::ArrayRef<int64_t> shape) {\n-  if (shape.empty()) {\n-    return CreateConst<T>(b, type, value);\n-  }\n+mlir::TypedValue<mlir::RankedTensorType> CreateConst(\n+    EmitterLocOpBuilder& b, mlir::Type type, T value,\n+    llvm::ArrayRef<int64_t> shape) {\n   auto tensor_type = mlir::RankedTensorType::get(shape, type);\n   if (auto int_type = mlir::dyn_cast<mlir::IntegerType>(type)) {\n-    auto result =\n+    mlir::Value result =\n         b.create<mlir::arith::ConstantOp>(mlir::DenseElementsAttr::get(\n             tensor_type,\n             mlir::APInt(int_type.getIntOrFloatBitWidth(), value,\n                         /*isSigned=*/false, /*implicitTrunc=*/true)));\n-    return ScalarOrTensor(result);\n+    return mlir::cast<mlir::TypedValue<mlir::RankedTensorType>>(result);\n   }\n   if (auto float_type = mlir::dyn_cast<mlir::FloatType>(type)) {\n-    auto result =\n+    mlir::Value result =\n         b.create<mlir::arith::ConstantOp>(mlir::DenseElementsAttr::get(\n             tensor_type, b.getFloatAttr(type, static_cast<double>(value))));\n-    return ScalarOrTensor(result);\n+    return mlir::cast<mlir::TypedValue<mlir::RankedTensorType>>(result);\n   }\n   LOG(FATAL) << \"Constant type not supported: \" << llvm_ir::DumpToString(type);\n }\n@@ -178,10 +172,9 @@ template <typename T>\n mlir::Value ConstLike(EmitterLocOpBuilder& b, mlir::Value like, T new_value) {\n   if (auto src_shaped_ty = mlir::dyn_cast<mlir::ShapedType>(like.getType())) {\n     mlir::Type src_ty = src_shaped_ty.getElementType();\n-    return CreateConst(b, src_ty, new_value, src_shaped_ty.getShape())\n-        .UnwrapUnsafe();\n+    return CreateConst(b, src_ty, new_value, src_shaped_ty.getShape());\n   }\n-  return CreateConst(b, like.getType(), new_value).UnwrapUnsafe();\n+  return CreateConst(b, like.getType(), new_value);\n }\n \n inline mlir::Value ZerosLike(EmitterLocOpBuilder& b, mlir::Value x) {\n@@ -199,8 +192,8 @@ mlir::Value Cast(EmitterLocOpBuilder& b, mlir::Value value,\n                  mlir::Type dst_element_ty);\n \n // Emits a scalar constant.\n-absl::StatusOr<ScalarOrTensor> EmitConstant(EmitterLocOpBuilder& b,\n-                                            const HloInstruction& constant);\n+absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n+    EmitterLocOpBuilder& b, const HloInstruction& constant);\n \n bool IsSupportedElementwiseLibdeviceFunction(const HloInstruction& hlo);\n "
        },
        {
            "sha": "7889f0483a1a96e1443c5faa3b5e4b161834f8f9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 27,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=dac01e1d48eb446966f8e6b7c20781f90f9139ff",
            "patch": "@@ -216,10 +216,10 @@ Value MakeIndex(EmitterLocOpBuilder& b, int64_t value) {\n // Emit a value as Index clamped to [lower, upper].\n Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n                        int64_t upper) {\n-  Value clamped_index = b.create<arith::MaxSIOp>(\n-      value, CreateConst(b, value.getType(), lower).UnwrapUnsafe());\n+  Value clamped_index =\n+      b.create<arith::MaxSIOp>(value, CreateConst(b, value.getType(), lower));\n   clamped_index = b.create<arith::MinSIOp>(\n-      clamped_index, CreateConst(b, value.getType(), upper).UnwrapUnsafe());\n+      clamped_index, CreateConst(b, value.getType(), upper));\n   return b.create<arith::IndexCastOp>(b.getIndexType(), clamped_index);\n }\n \n@@ -401,10 +401,10 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n     TensorValue range = Iota(b, input_reduction_dimension_size);\n     TensorValue bcast =\n         BroadcastInDims(b, range, input_shape, {reduction_dimension});\n-    ScalarOrTensor constant = CreateConst(\n+    TensorValue constant = CreateConst(\n         b, b.getI32Type(), source_tensor_reduction_dimension_size, input_shape);\n-    Value mask = b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast,\n-                                         constant.UnwrapUnsafe());\n+    Value mask =\n+        b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast, constant);\n \n     TensorValue neutral = BroadcastInDims(\n         b, values[tiled_hlo_reduce.operand(1)], input_shape, /*dims=*/{});\n@@ -551,8 +551,7 @@ absl::StatusOr<TensorValue> EmitTiledIota(\n   Value range = b.create<arith::MulIOp>(\n       Iota(b, padded_tile_sizes[iota_dim]),\n       Splat(b,\n-            CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim])\n-                .UnwrapUnsafe(),\n+            CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim]),\n             padded_tile_sizes[iota_dim]));\n \n   // Then, add the base offset to the iota components.\n@@ -755,11 +754,9 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n     // contracting dimension---i.e. tiles whose index exceeds the number of\n     // full tiles (tiles without padding).\n     Type result_type = dot_operand_value.getType();\n-    Value tile_size_value =\n-        CreateConst(b, b.getI32Type(), tile_size, {}).UnwrapScalar();\n+    Value tile_size_value = CreateConst(b, b.getI32Type(), tile_size);\n     Value num_full_tiles = b.create<arith::DivSIOp>(\n-        CreateConst(b, b.getI32Type(), contracting_dimension_size, {})\n-            .UnwrapScalar(),\n+        CreateConst(b, b.getI32Type(), contracting_dimension_size),\n         tile_size_value);\n     // if tile_index >= num_full_tiles...\n     auto cond = b.create<arith::CmpIOp>(arith::CmpIPredicate::sge,\n@@ -781,8 +778,7 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n       Value indices = b.create<arith::AddIOp>(range, broadcasted_tile_offset);\n \n       Value boundary = CreateConst(b, b.getI32Type(),\n-                                   contracting_dimension_size, {tile_size})\n-                           .UnwrapTensor();\n+                                   contracting_dimension_size, {tile_size});\n \n       Value mask =\n           b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, indices, boundary);\n@@ -793,10 +789,10 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n           auto element_type,\n           TritonType(b, dot_operand.hlo()->shape().element_type()));\n \n-      ScalarOrTensor zero = CreateConst(b, element_type, 0.0f, tile_shape);\n+      TensorValue zero = CreateConst(b, element_type, 0.0f, tile_shape);\n \n-      Value masked_dot_operand = b.create<arith::SelectOp>(\n-          mask, dot_operand_value, zero.UnwrapTensor());\n+      Value masked_dot_operand =\n+          b.create<arith::SelectOp>(mask, dot_operand_value, zero);\n       b.create<mlir::scf::YieldOp>(masked_dot_operand);\n     }\n     // else ...\n@@ -946,8 +942,7 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n   TF_ASSIGN_OR_RETURN(Type accumulator_type,\n                       triton::GetDotAccumulatorType(b, dot));\n   Value accumulator =\n-      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims)\n-          .UnwrapTensor();\n+      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims);\n \n   TF_ASSIGN_OR_RETURN(int64_t loop_iteration_count,\n                       GetDotLoopIterationCount(tiled_hlo_dot));\n@@ -1082,8 +1077,7 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n \n   Type accumulator_type = b.getF32Type();\n   Value accumulator =\n-      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims)\n-          .UnwrapTensor();\n+      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims);\n \n   TF_ASSIGN_OR_RETURN(int64_t loop_iteration_count,\n                       GetDotLoopIterationCount(tiled_hlo_dot));\n@@ -1272,8 +1266,7 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n     // directly populates the `else` block of the previous `if_op`.\n     if (if_ops.size() < tiled_concatenate.operands().size() - 1) {\n       limit += operand->hlo()->shape().dimensions()[concatenate_dimension];\n-      Value offset_limit =\n-          CreateConst(b, b.getIndexType(), limit, {}).UnwrapScalar();\n+      Value offset_limit = CreateConst(b, b.getIndexType(), limit);\n \n       auto cond =\n           b.create<arith::CmpIOp>(arith::CmpIPredicate::slt,\n@@ -1354,8 +1347,7 @@ absl::StatusOr<ScalarOrTensor> EmitPad(\n     // RHS for the compare is splat(pad_input_dim_size - tile_offset).\n     Value tile_offset_i32 = Cast(b, tile_offset, i32_type);\n     Value threshold = b.create<arith::SubIOp>(\n-        CreateConst(b, i32_type, pad_input_dim_size).UnwrapScalar(),\n-        tile_offset_i32);\n+        CreateConst(b, i32_type, pad_input_dim_size), tile_offset_i32);\n     TensorValue threshold_splat = Splat(b, threshold, padded_tile_sizes);\n     Value cmp = b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast,\n                                         threshold_splat);\n@@ -1445,7 +1437,8 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n \n   if (hlo->opcode() == HloOpcode::kConstant) {\n     if (ShapeUtil::IsEffectiveScalar(hlo->shape())) {\n-      return EmitConstant(b, *hlo);\n+      TF_ASSIGN_OR_RETURN(TensorValue constant, EmitConstant(b, *hlo));\n+      return MakeScalarOrTensor(b, constant);\n     }\n     return absl::UnimplementedError(\n         absl::StrCat(\"Unsupported non-scalar constant \", hlo->ToString()));\n@@ -1588,7 +1581,8 @@ absl::StatusOr<ScalarOrTensor> EmitScope(\n       TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n       continue;\n     } else if (hlo->opcode() == HloOpcode::kConstant) {\n-      TF_ASSIGN_OR_RETURN(result, EmitConstant(b, *hlo));\n+      TF_ASSIGN_OR_RETURN(TensorValue constant, EmitConstant(b, *hlo));\n+      result = MakeScalarOrTensor(b, constant);\n     } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n       return absl::InvalidArgumentError(\n           \"Broadcast is not yet supported in EmitScope().\");"
        },
        {
            "sha": "329e82eff9e3cbbad83f4368c0c0e254caea3d3a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=dac01e1d48eb446966f8e6b7c20781f90f9139ff",
            "patch": "@@ -2032,8 +2032,7 @@ ENTRY main {\n // CHECK-SAME: %[[OUT:.*]]: memref<49xf32>\n \n // CHECK: %[[EXTRACT:.*]] = xtile.extract %[[IN]]{{.*}}\n-// CHECK: %[[PAD_VALUE:.*]] = arith.constant 1.000000e+00 : f32\n-// CHECK: %[[TO_TENSOR_PAD_VALUE:.*]] = xtile.to_tensor %[[PAD_VALUE]]\n+// CHECK: %[[PAD_VALUE:.*]] = arith.constant dense<1.000000e+00> : tensor<f32>\n // CHECK: %[[TILE_OFFSET:.*]] = xla.apply_indexing\n // CHECK: %[[IOTA_VAL:.*]] = stablehlo.iota dim = 0 : tensor<32xi32>\n // CHECK: %[[IOTA:.*]] = stablehlo.broadcast_in_dim %[[IOTA_VAL]], dims = [0] : (tensor<32xi32>) -> tensor<32xi32>\n@@ -2043,7 +2042,7 @@ ENTRY main {\n // CHECK: %[[TO_TENSOR_THRESHOLD:.*]] = xtile.to_tensor %[[THRESHOLD]]\n // CHECK: %[[THRESHOLD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[TO_TENSOR_THRESHOLD]], dims = []\n // CHECK: %[[MASK:.*]] = arith.cmpi slt, %[[IOTA]], %[[THRESHOLD_SPLAT]]\n-// CHECK: %[[PAD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[TO_TENSOR_PAD_VALUE]], dims = []\n+// CHECK: %[[PAD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[PAD_VALUE]], dims = []\n // CHECK: %[[SELECT:.*]] = arith.select %[[MASK]], %[[EXTRACT]], %[[PAD_SPLAT]]\n \n // CHECK:   xtile.insert %[[SELECT]] into %[[OUT]]"
        },
        {
            "sha": "60454cc8d41adf57b8121ae26eb507ff7134713a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dac01e1d48eb446966f8e6b7c20781f90f9139ff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=dac01e1d48eb446966f8e6b7c20781f90f9139ff",
            "patch": "@@ -215,9 +215,9 @@ ENTRY e {\n       this, *module->GetComputationWithName(\"reduce_fusion\"),\n       block_level_parameters,\n       R\"(\n-CHECK: %[[INIT_VALUE_TO_TENSOR:.*]] = xtile.to_tensor %{{.*}} : f32\n+CHECK: %[[INIT:.*]] = arith.constant dense<0.000000e+00> : tensor<f32>\n CHECK: %[[REDUCE_INPUT:.*]] = arith.select {{.*}}\n-CHECK: %[[RES:.*]] = stablehlo.reduce(%[[REDUCE_INPUT]] init: %[[INIT_VALUE_TO_TENSOR]]) across dimensions = [0] : (tensor<256x16xf32>, tensor<f32>) -> tensor<16xf32>\n+CHECK: %[[RES:.*]] = stablehlo.reduce(%[[REDUCE_INPUT]] init: %[[INIT]]) across dimensions = [0] : (tensor<256x16xf32>, tensor<f32>) -> tensor<16xf32>\n CHECK: reducer(%[[ARG_0:.*]]: tensor<f32>, %[[ARG_1:.*]]: tensor<f32>)  {\n CHECK:   %[[SUM:.*]] = arith.addf %[[ARG_0]], %[[ARG_1]] : tensor<f32>\n CHECK:   stablehlo.return %[[SUM]] : tensor<f32>"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 51,
        "deletions": 69
    }
}