{
    "author": "Varcho",
    "message": "[ReplicaGroupV3][Refactor][3/n] Use CollectiveDeviceListBase for polymorphic device list support.\n\nPiperOrigin-RevId: 846825118",
    "sha": "07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
    "files": [
        {
            "sha": "eb0c9fd9b969f2c59f787610e1d50155a4e73ae3",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 30,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -1692,7 +1692,7 @@ HloInstruction::CreateReducePrecision(const Shape& shape,\n \n /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateAllGather(\n     const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    int64_t all_gather_dimension, const CollectiveDeviceList& device_list,\n+    int64_t all_gather_dimension, const CollectiveDeviceListBase& device_list,\n     bool constrain_layout, const std::optional<int64_t>& channel_id,\n     bool use_global_device_ids) {\n   return std::make_unique<HloAllGatherInstruction>(\n@@ -1711,13 +1711,11 @@ HloInstruction::CreateReducePrecision(const Shape& shape,\n }\n \n /* static */ std::unique_ptr<HloInstruction>\n-HloInstruction::CreateAllGatherStart(const Shape& shape,\n-                                     absl::Span<HloInstruction* const> operands,\n-                                     int64_t all_gather_dimension,\n-                                     const CollectiveDeviceList& device_list,\n-                                     bool constrain_layout,\n-                                     const std::optional<int64_t>& channel_id,\n-                                     bool use_global_device_ids) {\n+HloInstruction::CreateAllGatherStart(\n+    const Shape& shape, absl::Span<HloInstruction* const> operands,\n+    int64_t all_gather_dimension, const CollectiveDeviceListBase& device_list,\n+    bool constrain_layout, const std::optional<int64_t>& channel_id,\n+    bool use_global_device_ids) {\n   return std::make_unique<HloAllGatherInstruction>(\n       HloOpcode::kAllGatherStart, shape, operands, all_gather_dimension,\n       device_list, constrain_layout, channel_id, use_global_device_ids);\n@@ -1737,9 +1735,9 @@ HloInstruction::CreateAllGatherStart(\n \n /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateAllReduce(\n     const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    HloComputation* reduce_computation, const CollectiveDeviceList& device_list,\n-    bool constrain_layout, const std::optional<int64_t>& channel_id,\n-    bool use_global_device_ids) {\n+    HloComputation* reduce_computation,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n+    const std::optional<int64_t>& channel_id, bool use_global_device_ids) {\n   return std::make_unique<HloAllReduceInstruction>(\n       HloOpcode::kAllReduce, shape, operands, reduce_computation, device_list,\n       constrain_layout, channel_id, use_global_device_ids);\n@@ -1756,11 +1754,14 @@ HloInstruction::CreateAllGatherStart(\n }\n \n /* static */ std::unique_ptr<HloInstruction>\n-HloInstruction::CreateReduceScatter(\n-    const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    HloComputation* reduce_computation, const CollectiveDeviceList& device_list,\n-    bool constrain_layout, const std::optional<int64_t>& channel_id,\n-    bool use_global_device_ids, int64_t scatter_dimension) {\n+HloInstruction::CreateReduceScatter(const Shape& shape,\n+                                    absl::Span<HloInstruction* const> operands,\n+                                    HloComputation* reduce_computation,\n+                                    const CollectiveDeviceListBase& device_list,\n+                                    bool constrain_layout,\n+                                    const std::optional<int64_t>& channel_id,\n+                                    bool use_global_device_ids,\n+                                    int64_t scatter_dimension) {\n   return std::make_unique<HloReduceScatterInstruction>(\n       shape, operands, reduce_computation, device_list, constrain_layout,\n       channel_id, use_global_device_ids, scatter_dimension);\n@@ -1779,13 +1780,11 @@ HloInstruction::CreateReduceScatter(\n }\n \n /* static */ std::unique_ptr<HloInstruction>\n-HloInstruction::CreateAllReduceStart(const Shape& shape,\n-                                     absl::Span<HloInstruction* const> operands,\n-                                     HloComputation* reduce_computation,\n-                                     const CollectiveDeviceList& device_list,\n-                                     bool constrain_layout,\n-                                     const std::optional<int64_t>& channel_id,\n-                                     bool use_global_device_ids) {\n+HloInstruction::CreateAllReduceStart(\n+    const Shape& shape, absl::Span<HloInstruction* const> operands,\n+    HloComputation* reduce_computation,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n+    const std::optional<int64_t>& channel_id, bool use_global_device_ids) {\n   return std::make_unique<HloAllReduceInstruction>(\n       HloOpcode::kAllReduceStart, shape, operands, reduce_computation,\n       device_list, constrain_layout, channel_id, use_global_device_ids);\n@@ -1804,7 +1803,7 @@ HloInstruction::CreateAllReduceStart(\n \n /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateAllToAll(\n     const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    const CollectiveDeviceList& device_list, bool constrain_layout,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n     const std::optional<int64_t>& channel_id,\n     const std::optional<int64_t>& split_dimension) {\n   return std::make_unique<HloAllToAllInstruction>(shape, operands, device_list,\n@@ -1822,10 +1821,10 @@ HloInstruction::CreateAllReduceStart(\n }\n \n /* static */ std::unique_ptr<HloInstruction>\n-HloInstruction::CreateRaggedAllToAll(const Shape& shape,\n-                                     absl::Span<HloInstruction* const> operands,\n-                                     const CollectiveDeviceList& device_list,\n-                                     const std::optional<int64_t>& channel_id) {\n+HloInstruction::CreateRaggedAllToAll(\n+    const Shape& shape, absl::Span<HloInstruction* const> operands,\n+    const CollectiveDeviceListBase& device_list,\n+    const std::optional<int64_t>& channel_id) {\n   return std::make_unique<HloRaggedAllToAllInstruction>(\n       shape, operands, device_list, channel_id);\n }\n@@ -1842,7 +1841,7 @@ HloInstruction::CreateRaggedAllToAll(\n /* static */ std::unique_ptr<HloInstruction>\n HloInstruction::CreateCollectiveBroadcast(\n     const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    const CollectiveDeviceList& device_list, bool constrain_layout,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n     const std::optional<int64_t>& channel_id) {\n   return std::make_unique<HloCollectiveBroadcastInstruction>(\n       HloOpcode::kCollectiveBroadcast, shape, operands, device_list,\n@@ -5814,7 +5813,7 @@ const std::vector<ReplicaGroup>& HloInstruction::replica_groups() const {\n   return Cast<HloCollectiveInstruction>(this)->replica_groups();\n }\n \n-const CollectiveDeviceList& HloInstruction::device_list() const {\n+const CollectiveDeviceListBase& HloInstruction::device_list() const {\n   return Cast<HloCollectiveInstruction>(this)->device_list();\n }\n "
        },
        {
            "sha": "40a178fa26958e8eca787b7a774cf5ff7c27a31e",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -496,7 +496,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   // order of inputs from different participants.\n   static std::unique_ptr<HloInstruction> CreateAllGather(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n-      int64_t all_gather_dimension, const CollectiveDeviceList& device_list,\n+      int64_t all_gather_dimension, const CollectiveDeviceListBase& device_list,\n       bool constrain_layout, const std::optional<int64_t>& channel_id,\n       bool use_global_device_ids);\n \n@@ -516,7 +516,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   // conjunction of a AllGatherDone op that synchronizes and returns the result.\n   static std::unique_ptr<HloInstruction> CreateAllGatherStart(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n-      int64_t all_gather_dimension, const CollectiveDeviceList& device_list,\n+      int64_t all_gather_dimension, const CollectiveDeviceListBase& device_list,\n       bool constrain_layout, const std::optional<int64_t>& channel_id,\n       bool use_global_device_ids);\n \n@@ -543,7 +543,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   static std::unique_ptr<HloInstruction> CreateAllReduce(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n       HloComputation* reduce_computation,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id, bool use_global_device_ids);\n \n   ABSL_DEPRECATED(\"Use CollectiveDeviceList instead of list of ReplicaGroup.\")\n@@ -559,7 +559,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   static std::unique_ptr<HloInstruction> CreateReduceScatter(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n       HloComputation* reduce_computation,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id, bool use_global_device_ids,\n       int64_t scatter_dimension);\n \n@@ -587,7 +587,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   static std::unique_ptr<HloInstruction> CreateAllReduceStart(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n       HloComputation* reduce_computation,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id, bool use_global_device_ids);\n \n   ABSL_DEPRECATED(\"Use CollectiveDeviceList instead of list of ReplicaGroup.\")\n@@ -625,7 +625,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   // performs AllToAll and then concatenates the results into a single array.\n   static std::unique_ptr<HloInstruction> CreateAllToAll(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id,\n       const std::optional<int64_t>& split_dimension = std::nullopt);\n \n@@ -733,7 +733,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   //\n   static std::unique_ptr<HloInstruction> CreateRaggedAllToAll(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n-      const CollectiveDeviceList& device_list,\n+      const CollectiveDeviceListBase& device_list,\n       const std::optional<int64_t>& channel_id);\n \n   ABSL_DEPRECATED(\"Use CollectiveDeviceList instead of list of ReplicaGroup.\")\n@@ -748,7 +748,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   // on that replica is a tensor consists of 0(s) in `shape`.\n   static std::unique_ptr<HloInstruction> CreateCollectiveBroadcast(\n       const Shape& shape, absl::Span<HloInstruction* const> operand,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id);\n \n   ABSL_DEPRECATED(\"Use CollectiveDeviceList instead of list of ReplicaGroup.\")\n@@ -2308,7 +2308,7 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   const std::vector<ReplicaGroup>& replica_groups() const;\n \n   // Delegates to HloCollectiveInstruction::device_list.\n-  const CollectiveDeviceList& device_list() const;\n+  const CollectiveDeviceListBase& device_list() const;\n \n   // Delegates to HloCollectivePermuteInstruction::source_target_pairs.\n   const std::vector<std::pair<int64_t, int64_t>>& source_target_pairs() const;"
        },
        {
            "sha": "cc8459830044e9b7a8763b674cfc8c20bf3968a0",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -918,10 +918,10 @@ HloRecvDoneInstruction::CloneWithNewOperandsImpl(\n HloCollectiveInstruction::HloCollectiveInstruction(\n     HloOpcode opcode, const Shape& shape,\n     absl::Span<HloInstruction* const> operands,\n-    const CollectiveDeviceList& device_list, bool constrain_layout,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n     const std::optional<int64_t>& channel_id)\n     : HloChannelInstruction(opcode, shape, channel_id),\n-      device_list_(std::make_shared<CollectiveDeviceList>(device_list)),\n+      device_list_(device_list.Clone()),\n       constrain_layout_(constrain_layout) {\n   for (auto operand : operands) {\n     AppendOperand(operand);\n@@ -985,7 +985,7 @@ bool HloCollectiveInstruction::IdenticalSlowPathIgnoringChannelIdValues(\n HloAllGatherInstruction::HloAllGatherInstruction(\n     HloOpcode opcode, const Shape& shape,\n     absl::Span<HloInstruction* const> operands, int64_t all_gather_dimension,\n-    const CollectiveDeviceList& device_list, bool constrain_layout,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n     const std::optional<int64_t>& channel_id, bool use_global_device_ids)\n     : HloCollectiveInstruction(opcode, shape, operands, device_list,\n                                constrain_layout, channel_id),\n@@ -1045,9 +1045,9 @@ bool HloAllGatherInstruction::IdenticalSlowPathIgnoringChannelIdValues(\n HloAllReduceInstructionBase::HloAllReduceInstructionBase(\n     HloOpcode opcode, const Shape& shape,\n     absl::Span<HloInstruction* const> operands,\n-    HloComputation* reduce_computation, const CollectiveDeviceList& device_list,\n-    bool constrain_layout, const std::optional<int64_t>& channel_id,\n-    bool use_global_device_ids)\n+    HloComputation* reduce_computation,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n+    const std::optional<int64_t>& channel_id, bool use_global_device_ids)\n     : HloCollectiveInstruction(opcode, shape, operands, device_list,\n                                constrain_layout, channel_id),\n       use_global_device_ids_(use_global_device_ids) {\n@@ -1106,9 +1106,10 @@ HloAllReduceInstruction::CloneWithNewOperandsImpl(\n \n HloReduceScatterInstruction::HloReduceScatterInstruction(\n     const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    HloComputation* reduce_computation, const CollectiveDeviceList& device_list,\n-    bool constrain_layout, const std::optional<int64_t>& channel_id,\n-    bool use_global_device_ids, int64_t scatter_dimension)\n+    HloComputation* reduce_computation,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n+    const std::optional<int64_t>& channel_id, bool use_global_device_ids,\n+    int64_t scatter_dimension)\n     : HloAllReduceInstructionBase(\n           HloOpcode::kReduceScatter, shape, operands, reduce_computation,\n           device_list, constrain_layout, channel_id, use_global_device_ids),\n@@ -1161,7 +1162,7 @@ HloReduceScatterInstruction::CloneWithNewOperandsImpl(\n \n HloAllToAllInstruction::HloAllToAllInstruction(\n     const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    const CollectiveDeviceList& device_list, bool constrain_layout,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n     const std::optional<int64_t>& channel_id,\n     const std::optional<int64_t>& split_dimension)\n     : HloCollectiveInstruction(HloOpcode::kAllToAll, shape, operands,\n@@ -1216,7 +1217,7 @@ bool HloAllToAllInstruction::IdenticalSlowPathIgnoringChannelIdValues(\n \n HloRaggedAllToAllInstruction::HloRaggedAllToAllInstruction(\n     const Shape& shape, absl::Span<HloInstruction* const> operands,\n-    const CollectiveDeviceList& device_list,\n+    const CollectiveDeviceListBase& device_list,\n     const std::optional<int64_t>& channel_id)\n     : HloCollectiveInstruction(HloOpcode::kRaggedAllToAll, shape, operands,\n                                device_list,\n@@ -1251,7 +1252,7 @@ void HloRaggedAllToAllInstruction::PrintExtraAttributesImpl(\n HloCollectiveBroadcastInstruction::HloCollectiveBroadcastInstruction(\n     HloOpcode opcode, const Shape& shape,\n     absl::Span<HloInstruction* const> operands,\n-    const CollectiveDeviceList& device_list, bool constrain_layout,\n+    const CollectiveDeviceListBase& device_list, bool constrain_layout,\n     const std::optional<int64_t>& channel_id)\n     : HloCollectiveInstruction(opcode, shape, operands, device_list,\n                                constrain_layout, channel_id) {}"
        },
        {
            "sha": "88f902c9b093f8c6da5c01494a7c130b3175271f",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -651,7 +651,7 @@ class HloCollectiveInstruction : public HloChannelInstruction {\n     return device_list_->replica_groups();\n   }\n \n-  const CollectiveDeviceList& device_list() const {\n+  const CollectiveDeviceListBase& device_list() const {\n     const CollectiveDeviceList* device_list_v1 =\n         dynamic_cast<const CollectiveDeviceList*>(device_list_.get());\n     // TODO(b/468442352): After XLA codebase is genericized to utilize\n@@ -683,8 +683,8 @@ class HloCollectiveInstruction : public HloChannelInstruction {\n   explicit HloCollectiveInstruction(\n       HloOpcode opcode, const Shape& shape,\n       absl::Span<HloInstruction* const> operands,\n-      const CollectiveDeviceList& collective_device_list, bool constrain_layout,\n-      const std::optional<int64_t>& channel_id);\n+      const CollectiveDeviceListBase& collective_device_list,\n+      bool constrain_layout, const std::optional<int64_t>& channel_id);\n \n   HloInstructionProto ToProto() const override;\n \n@@ -704,7 +704,7 @@ class HloAllGatherInstruction : public HloCollectiveInstruction {\n   explicit HloAllGatherInstruction(HloOpcode opcode, const Shape& shape,\n                                    absl::Span<HloInstruction* const> operands,\n                                    int64_t all_gather_dimension,\n-                                   const CollectiveDeviceList& device_list,\n+                                   const CollectiveDeviceListBase& device_list,\n                                    bool constrain_layout,\n                                    const std::optional<int64_t>& channel_id,\n                                    bool use_global_device_ids);\n@@ -760,7 +760,7 @@ class HloAllReduceInstructionBase : public HloCollectiveInstruction {\n       HloOpcode opcode, const Shape& shape,\n       absl::Span<HloInstruction* const> operands,\n       HloComputation* reduce_computation,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id, bool use_global_device_ids);\n \n   // Returns true if the ids in the ReplicaGroup config represent a global id of\n@@ -817,7 +817,7 @@ class HloReduceScatterInstruction : public HloAllReduceInstructionBase {\n   explicit HloReduceScatterInstruction(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n       HloComputation* reduce_computation,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id, bool use_global_device_ids,\n       int64_t scatter_dimension);\n \n@@ -862,7 +862,7 @@ class HloAllToAllInstruction : public HloCollectiveInstruction {\n  public:\n   explicit HloAllToAllInstruction(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id,\n       const std::optional<int64_t>& split_dimension);\n \n@@ -910,7 +910,7 @@ class HloRaggedAllToAllInstruction : public HloCollectiveInstruction {\n  public:\n   explicit HloRaggedAllToAllInstruction(\n       const Shape& shape, absl::Span<HloInstruction* const> operands,\n-      const CollectiveDeviceList& device_list,\n+      const CollectiveDeviceListBase& device_list,\n       const std::optional<int64_t>& channel_id);\n \n   ABSL_DEPRECATED(\"Use CollectiveDeviceList instead of list of ReplicaGroup.\")\n@@ -941,7 +941,7 @@ class HloCollectiveBroadcastInstruction : public HloCollectiveInstruction {\n   explicit HloCollectiveBroadcastInstruction(\n       HloOpcode opcode, const Shape& shape,\n       absl::Span<HloInstruction* const> operands,\n-      const CollectiveDeviceList& device_list, bool constrain_layout,\n+      const CollectiveDeviceListBase& device_list, bool constrain_layout,\n       const std::optional<int64_t>& channel_id);\n \n   ABSL_DEPRECATED(\"Use CollectiveDeviceList instead of list of ReplicaGroup.\")"
        },
        {
            "sha": "d0c67c7afd6387b6970f4b48d061136def7c5819",
            "filename": "third_party/xla/xla/hlo/ir/replica_group.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Freplica_group.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Freplica_group.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Freplica_group.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -489,4 +489,31 @@ CollectiveDeviceList CollectiveDeviceList::FromProto(\n   return FromProto(proto.collective_device_list());\n }\n \n+CollectiveDeviceList ConvertToV1CollectiveDeviceList(\n+    const CollectiveDeviceListBase& device_list) {\n+  switch (device_list.version()) {\n+    case CollectiveDeviceListVersion::kListOfLists: {\n+      return dynamic_cast<const CollectiveDeviceList&>(device_list);\n+    }\n+    case CollectiveDeviceListVersion::kIota: {\n+      if (const auto* v2 =\n+              dynamic_cast<const IotaReplicaGroupList*>(&device_list)) {\n+        return CollectiveDeviceList(*v2);\n+      }\n+      const auto* v1 = dynamic_cast<const CollectiveDeviceList*>(&device_list);\n+      CHECK(v1 != nullptr) << \"Failed to convert kIota to V1 list.\";\n+      return *v1;\n+    }\n+    case CollectiveDeviceListVersion::kMeshAxes: {\n+      const auto* v3 =\n+          dynamic_cast<const MeshAxesReplicaGroupList*>(&device_list);\n+      CHECK(v3 != nullptr) << \"Failed to convert kMeshAxes to V1 list.\";\n+      return v3->ToCollectiveDeviceList();\n+    }\n+    default:\n+      LOG(FATAL) << \"Unknown CollectiveDeviceListVersion: \"\n+                 << static_cast<int>(device_list.version());\n+  }\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "c55e5802a2d4bfbe4766ed02d69a127fa7ab7fbc",
            "filename": "third_party/xla/xla/hlo/ir/replica_group.h",
            "status": "modified",
            "additions": 38,
            "deletions": 1,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Freplica_group.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Freplica_group.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Freplica_group.h?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -53,6 +53,19 @@ class CollectiveDeviceListBase {\n   CollectiveDeviceListBase(CollectiveDeviceListBase&&) = default;\n   CollectiveDeviceListBase& operator=(CollectiveDeviceListBase&&) = default;\n \n+  // This is strict equality, which means that two different types\n+  // can't be compared for functional equality (i.e. even though an\n+  // IotaReplicaGroup and a CollectiveDeviceList may correspond to the same\n+  // underlying set of device groups, they will compare as unequal).\n+  friend bool operator==(const CollectiveDeviceListBase& lhs,\n+                         const CollectiveDeviceListBase& rhs) {\n+    if (typeid(lhs) != typeid(rhs)) {\n+      return false;\n+    }\n+    // If types are the same, delegate to the derived implementation\n+    return lhs.isEqual(rhs);\n+  }\n+\n   virtual int64_t num_replica_groups() const = 0;\n   virtual int64_t num_devices_per_group() const = 0;\n   int64_t num_total_devices() const {\n@@ -89,6 +102,9 @@ class CollectiveDeviceListBase {\n \n   // shared_ptr for fast copy.\n   mutable std::shared_ptr<std::vector<ReplicaGroup>> replica_groups_ = nullptr;\n+\n+ protected:\n+  virtual bool isEqual(const CollectiveDeviceListBase& other) const = 0;\n };\n \n class MeshAxesReplicaGroupList : public CollectiveDeviceListBase {\n@@ -129,6 +145,13 @@ class MeshAxesReplicaGroupList : public CollectiveDeviceListBase {\n   IotaReplicaGroupList ToIotaReplicaGroupList() const;\n   CollectiveDeviceList ToCollectiveDeviceList() const;\n \n+ protected:\n+  bool isEqual(const CollectiveDeviceListBase& other) const override {\n+    const MeshAxesReplicaGroupList& rhs =\n+        static_cast<const MeshAxesReplicaGroupList&>(other);\n+    return *this == rhs;\n+  }\n+\n  private:\n   absl::flat_hash_map<int64_t, ReshapeAndAggregateAxes>\n   GetDimToReshapeAndAggregateAxes() const;\n@@ -203,6 +226,13 @@ class IotaReplicaGroupList : public CollectiveDeviceListBase {\n \n   static IotaReplicaGroupList FromProto(const IotaReplicaGroupListProto& proto);\n \n+ protected:\n+  bool isEqual(const CollectiveDeviceListBase& other) const override {\n+    const IotaReplicaGroupList& rhs =\n+        static_cast<const IotaReplicaGroupList&>(other);\n+    return *this == rhs;\n+  }\n+\n  private:\n   IotaTileAssignment iota_tile_assignment_;\n   int64_t num_replica_groups_ = -1;\n@@ -306,6 +336,13 @@ class CollectiveDeviceList : public CollectiveDeviceListBase {\n     return std::make_unique<CollectiveDeviceList>(*this);\n   };\n \n+ protected:\n+  bool isEqual(const CollectiveDeviceListBase& other) const override {\n+    const CollectiveDeviceList& rhs =\n+        static_cast<const CollectiveDeviceList&>(other);\n+    return *this == rhs;\n+  }\n+\n  private:\n   // Construct collective device list from protobuf replica group start and end\n   // iterators.\n@@ -333,7 +370,7 @@ class CollectiveDeviceList : public CollectiveDeviceListBase {\n   std::optional<IotaReplicaGroupList> iota_replica_group_list_;\n };\n \n-std::optional<CollectiveDeviceList> ConvertToV1CollectiveDeviceList(\n+CollectiveDeviceList ConvertToV1CollectiveDeviceList(\n     const CollectiveDeviceListBase& device_list);\n \n }  // namespace xla"
        },
        {
            "sha": "b8ff16b756eec9ae96c5646cd5f45ac9e2a2fbf0",
            "filename": "third_party/xla/xla/service/collective_ops_utils.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 18,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -238,7 +238,8 @@ absl::StatusOr<CollectiveOpGroupMode> GetCollectiveOpGroupMode(\n   return Internal(\"Unexpected instruction type.\");\n }\n \n-const CollectiveDeviceList& GetCollectiveDeviceList(const HloInstruction* hlo) {\n+const CollectiveDeviceListBase& GetCollectiveDeviceList(\n+    const HloInstruction* hlo) {\n   return Cast<HloCollectiveInstruction>(hlo)->device_list();\n }\n \n@@ -375,21 +376,23 @@ GetParticipatingDevicesGroups(const HloInstruction* collective) {\n       device_assignment, GetCollectiveReplicaGroups(collective), mode);\n }\n \n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(\n     const DeviceAssignment& device_assignment,\n-    const CollectiveDeviceList& collective_device_list,\n+    const CollectiveDeviceListBase& collective_device_list,\n     CollectiveOpGroupMode group_mode) {\n   return GetParticipatingFlattenedIdGroups(\n       collective_device_list, group_mode, device_assignment.replica_count(),\n       device_assignment.computation_count());\n }\n \n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n-    const CollectiveDeviceList& collective_device_list,\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(\n+    const CollectiveDeviceListBase& collective_device_list,\n     CollectiveOpGroupMode group_mode, int replica_count, int partition_count) {\n   if (group_mode ==\n       CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_FLATTENED_ID) {\n-    return collective_device_list;\n+    return collective_device_list.Clone();\n   }\n   std::vector<ReplicaGroup> filled_empty_replica_group;\n   absl::Span<const ReplicaGroup> original_replica_groups =\n@@ -456,27 +459,29 @@ absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n       }\n     }\n   }\n-  return CollectiveDeviceList(flattened_replica_groups);\n+  return std::make_unique<CollectiveDeviceList>(flattened_replica_groups);\n }\n \n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n-    const HloInstruction* hlo, const DeviceAssignment& device_assignment) {\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(const HloInstruction* hlo,\n+                                  const DeviceAssignment& device_assignment) {\n   TF_ASSIGN_OR_RETURN(CollectiveOpGroupMode mode,\n                       GetCollectiveOpGroupMode(hlo));\n   TF_ASSIGN_OR_RETURN(\n-      CollectiveDeviceList collective_device_list,\n+      std::unique_ptr<CollectiveDeviceListBase> collective_device_list,\n       GetParticipatingFlattenedIdGroups(device_assignment,\n                                         GetCollectiveDeviceList(hlo), mode));\n   return collective_device_list;\n }\n \n // Same as above, used for cases where static_device_assignment is not present.\n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n-    const HloInstruction* hlo, int replica_count, int partition_count) {\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(const HloInstruction* hlo, int replica_count,\n+                                  int partition_count) {\n   TF_ASSIGN_OR_RETURN(CollectiveOpGroupMode mode,\n                       GetCollectiveOpGroupMode(hlo));\n   TF_ASSIGN_OR_RETURN(\n-      CollectiveDeviceList collective_device_list,\n+      std::unique_ptr<CollectiveDeviceListBase> collective_device_list,\n       GetParticipatingFlattenedIdGroups(GetCollectiveDeviceList(hlo), mode,\n                                         replica_count, partition_count));\n   return collective_device_list;\n@@ -664,13 +669,12 @@ absl::StatusOr<std::vector<int64_t>> GetPariticipantCountsForReplicaGroups(\n \n absl::StatusOr<std::optional<std::pair<int64_t, int64_t>>>\n GetReplicaGroupCountAndSize(const HloInstruction* hlo) {\n-  const CollectiveDeviceList& device_list = GetCollectiveDeviceList(hlo);\n+  const CollectiveDeviceListBase& device_list = GetCollectiveDeviceList(hlo);\n   auto config = hlo->GetModule()->config();\n \n-  if (device_list.iota_replica_group_list().has_value()) {\n-    return std::make_pair(\n-        device_list.iota_replica_group_list()->num_replica_groups(),\n-        device_list.iota_replica_group_list()->num_devices_per_group());\n+  if (device_list.version() == CollectiveDeviceListVersion::kIota) {\n+    return std::make_pair(device_list.num_replica_groups(),\n+                          device_list.num_devices_per_group());\n   }\n   TF_ASSIGN_OR_RETURN(CollectiveOpGroupMode group_mode,\n                       GetCollectiveOpGroupMode(hlo));"
        },
        {
            "sha": "bdb81ed62cae5bf3090a648747695ee5ecfabf41",
            "filename": "third_party/xla/xla/service/collective_ops_utils.h",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_SERVICE_COLLECTIVE_OPS_UTILS_H_\n \n #include <cstdint>\n+#include <memory>\n #include <optional>\n #include <string>\n #include <utility>\n@@ -80,7 +81,8 @@ absl::StatusOr<std::vector<int>> GetParticipatingIDs(\n absl::StatusOr<std::vector<std::vector<int64_t>>> GetAsyncReplicaGroups(\n     const HloInstruction* instruction);\n \n-const CollectiveDeviceList& GetCollectiveDeviceList(const HloInstruction* hlo);\n+const CollectiveDeviceListBase& GetCollectiveDeviceList(\n+    const HloInstruction* hlo);\n \n const std::vector<ReplicaGroup>& GetCollectiveReplicaGroups(\n     const HloInstruction* hlo);\n@@ -129,24 +131,28 @@ GetParticipatingDevicesGroups(const HloInstruction* collective);\n \n // Same as above, except that it returns the flattened id in the replica groups\n // instead of device id.\n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(\n     const DeviceAssignment& device_assignment,\n-    const CollectiveDeviceList& collective_device_list,\n+    const CollectiveDeviceListBase& collective_device_list,\n     CollectiveOpGroupMode group_mode);\n \n // Same as above, but take replica/partition count instead of device assignment.\n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n-    const CollectiveDeviceList& collective_device_list,\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(\n+    const CollectiveDeviceListBase& collective_device_list,\n     CollectiveOpGroupMode group_mode, int replica_count, int partition_count);\n \n // Same as above, with collective group mode determined by the collective\n // instruction.\n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n-    const HloInstruction* hlo, const DeviceAssignment& device_assignment);\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(const HloInstruction* hlo,\n+                                  const DeviceAssignment& device_assignment);\n \n // Same as above, used for cases where static_device_assignment is not present.\n-absl::StatusOr<CollectiveDeviceList> GetParticipatingFlattenedIdGroups(\n-    const HloInstruction* hlo, int replica_count, int partition_count);\n+absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+GetParticipatingFlattenedIdGroups(const HloInstruction* hlo, int replica_count,\n+                                  int partition_count);\n \n // Figures out which devices are participating in the collective subgroup.\n absl::StatusOr<std::vector<GlobalDeviceId>> GetParticipatingDevices("
        },
        {
            "sha": "6d762d9b13b7dfb9536c1f5128da81e073d101ca",
            "filename": "third_party/xla/xla/service/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 41,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils_test.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -57,11 +57,11 @@ using CycleType = collective_permute_cycle::CycleType;\n \n // Creates a container of ReplicaGroups.\n std::vector<ReplicaGroup> CreateReplicaGroups(\n-    const std::vector<std::vector<int64_t>> &replica_groups) {\n+    const std::vector<std::vector<int64_t>>& replica_groups) {\n   std::vector<ReplicaGroup> result;\n   result.reserve(replica_groups.size());\n-  for (const auto &replica_group : replica_groups) {\n-    ReplicaGroup &group = result.emplace_back();\n+  for (const auto& replica_group : replica_groups) {\n+    ReplicaGroup& group = result.emplace_back();\n     for (auto id : replica_group) {\n       group.add_replica_ids(id);\n     }\n@@ -116,7 +116,7 @@ TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnUnverifiedModule(hlo_string));\n \n-  HloInstruction *all_gather =\n+  HloInstruction* all_gather =\n       module->entry_computation()->GetInstructionWithName(\"all-gather\");\n \n   EXPECT_EQ(IsOrHasCollectiveWithChannelId(all_gather), all_gather);\n@@ -138,10 +138,10 @@ TEST(CollectiveOpsUtilsTest, IsNonFusionCollectiveSendRecv) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnUnverifiedModule(hlo_string));\n \n-  HloInstruction *recv_ctx =\n+  HloInstruction* recv_ctx =\n       module->entry_computation()->GetInstructionWithName(\"recv_ctx\");\n   ASSERT_NE(recv_ctx, nullptr);\n-  HloInstruction *send_ctx =\n+  HloInstruction* send_ctx =\n       module->entry_computation()->GetInstructionWithName(\"send_ctx\");\n   ASSERT_NE(send_ctx, nullptr);\n \n@@ -160,7 +160,7 @@ TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId2) {\n       HloInstruction * param_0,\n       builder.AddParameter(HloInstruction::CreateParameter(\n           0, ShapeUtil::MakeShape(BF16, {1, 512, 4096}), \"p0\")));\n-  HloInstruction *instr =\n+  HloInstruction* instr =\n       builder.AddInstruction(HloInstruction::CreateAllGather(\n           ShapeUtil::MakeShape(BF16, {1, 4096, 4096}), {param_0}, 1,\n           CollectiveDeviceList(std::vector<ReplicaGroup>({group})), true, 231,\n@@ -178,7 +178,7 @@ TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId2) {\n       HloInstruction * param_1,\n       builder2.AddParameter(HloInstruction::CreateParameter(\n           0, ShapeUtil::MakeShape(BF16, {1, 512, 4096}), \"p1\")));\n-  HloInstruction *instr_without_channel_id =\n+  HloInstruction* instr_without_channel_id =\n       builder2.AddInstruction(HloInstruction::CreateAllGather(\n           ShapeUtil::MakeShape(BF16, {1, 4096, 4096}), {param_1}, 1, {group},\n           true, std::nullopt, true));\n@@ -191,7 +191,6 @@ TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId2) {\n   EXPECT_EQ(IsOrHasCollectiveWithChannelId(fusion2.get()), nullptr);\n }\n \n-\n TEST(IsExclusivelyCrossModuleTest, CrossReplicaNoChannelSet) {\n   int64_t num_replicas = 4;\n   int64_t num_partitions = 2;\n@@ -280,14 +279,14 @@ TEST(CollectiveOpsUtilsTest, GetReplicaGroups) {\n   // Set up a collective permute start instruction\n   auto builder = HloComputation::Builder(\"GetReplicaGroupsTest\");\n   auto param_shape = ShapeUtil::MakeShape(F32, {4, 4});\n-  HloInstruction *param_0 = builder.AddInstruction(\n+  HloInstruction* param_0 = builder.AddInstruction(\n       HloInstruction::CreateParameter(0, param_shape, \"p0\"));\n \n   // Test for CollectivePermuteStart\n   std::vector<std::pair<int64_t, int64_t>> source_target_pairs = {\n       {0, 1}, {1, 2}, {2, 3}, {3, 0}};\n \n-  HloInstruction *permute_start =\n+  HloInstruction* permute_start =\n       builder.AddInstruction(HloInstruction::CreateCollectivePermuteStart(\n           param_shape, param_0, source_target_pairs, /*channel_id=*/1));\n \n@@ -303,7 +302,7 @@ TEST(CollectiveOpsUtilsTest, GetReplicaGroups) {\n   // Test for AllGatherStart\n   std::vector<ReplicaGroup> replica_groups =\n       CreateReplicaGroups({{0, 1}, {2, 3}});\n-  HloInstruction *all_gather_start =\n+  HloInstruction* all_gather_start =\n       builder.AddInstruction(HloInstruction::CreateAllGatherStart(\n           ShapeUtil::MakeTupleShape({param_shape, param_shape}), {param_0},\n           /*all_gather_dimension=*/0, replica_groups,\n@@ -326,10 +325,10 @@ TEST(CollectiveOpsUtilsTest, GetReplicaGroups) {\n   reducer_builder.AddInstruction(HloInstruction::CreateBinary(\n       ShapeUtil::MakeScalarShape(F32), HloOpcode::kAdd, reducer_x, reducer_y));\n \n-  HloComputation *add_computation =\n+  HloComputation* add_computation =\n       module.AddEmbeddedComputation(reducer_builder.Build());\n \n-  HloInstruction *all_reduce_start =\n+  HloInstruction* all_reduce_start =\n       builder.AddInstruction(HloInstruction::CreateAllReduceStart(\n           ShapeUtil::MakeTupleShape({param_shape, param_shape}), {param_0},\n           add_computation, replica_groups, /*constrain_layout=*/false,\n@@ -347,22 +346,22 @@ TEST(CollectiveOpsUtilsTest, IsAsyncCollective) {\n   HloModule module(\"test_module\", HloModuleConfig());\n   auto builder = HloComputation::Builder(\"IsAsyncCollectiveTest\");\n   auto param_shape = ShapeUtil::MakeShape(F32, {4, 4});\n-  HloInstruction *param_0 = builder.AddInstruction(\n+  HloInstruction* param_0 = builder.AddInstruction(\n       HloInstruction::CreateParameter(0, param_shape, \"p0\"));\n \n   // Test for CollectivePermuteStart and CollectivePermuteDone\n   std::vector<std::pair<int64_t, int64_t>> source_target_pairs = {\n       {0, 1}, {1, 2}, {2, 3}, {3, 0}};\n \n-  HloInstruction *permute_start =\n+  HloInstruction* permute_start =\n       builder.AddInstruction(HloInstruction::CreateCollectivePermuteStart(\n           param_shape, param_0, source_target_pairs, /*channel_id=*/1));\n \n   auto is_async_status = IsAsyncCollective(permute_start);\n   EXPECT_TRUE(is_async_status.ok());\n   EXPECT_TRUE(is_async_status.value());\n \n-  HloInstruction *permute_done =\n+  HloInstruction* permute_done =\n       builder.AddInstruction(HloInstruction::CreateUnary(\n           param_shape, HloOpcode::kCollectivePermuteDone, permute_start));\n \n@@ -374,7 +373,7 @@ TEST(CollectiveOpsUtilsTest, IsAsyncCollective) {\n   std::vector<ReplicaGroup> replica_groups =\n       CreateReplicaGroups({{0, 1}, {2, 3}});\n \n-  HloInstruction *all_gather_start =\n+  HloInstruction* all_gather_start =\n       builder.AddInstruction(HloInstruction::CreateAllGatherStart(\n           ShapeUtil::MakeTupleShape(\n               {ShapeUtil::MakeShape(F32, {8, 4}), param_shape}),\n@@ -386,7 +385,7 @@ TEST(CollectiveOpsUtilsTest, IsAsyncCollective) {\n   EXPECT_TRUE(is_async_status.ok());\n   EXPECT_TRUE(is_async_status.value());\n \n-  HloInstruction *all_gather_done = builder.AddInstruction(\n+  HloInstruction* all_gather_done = builder.AddInstruction(\n       HloInstruction::CreateUnary(ShapeUtil::MakeShape(F32, {8, 4}),\n                                   HloOpcode::kAllGatherDone, all_gather_start));\n \n@@ -397,17 +396,17 @@ TEST(CollectiveOpsUtilsTest, IsAsyncCollective) {\n   // Test for AllReduceStart and AllReduceDone\n   // First create a reduction computation\n   HloComputation::Builder reducer_builder(\"add\");\n-  HloInstruction *reducer_x = reducer_builder.AddInstruction(\n+  HloInstruction* reducer_x = reducer_builder.AddInstruction(\n       HloInstruction::CreateParameter(0, ShapeUtil::MakeScalarShape(F32), \"x\"));\n-  HloInstruction *reducer_y = reducer_builder.AddInstruction(\n+  HloInstruction* reducer_y = reducer_builder.AddInstruction(\n       HloInstruction::CreateParameter(1, ShapeUtil::MakeScalarShape(F32), \"y\"));\n   reducer_builder.AddInstruction(HloInstruction::CreateBinary(\n       ShapeUtil::MakeScalarShape(F32), HloOpcode::kAdd, reducer_x, reducer_y));\n \n-  HloComputation *add_computation =\n+  HloComputation* add_computation =\n       module.AddEmbeddedComputation(reducer_builder.Build());\n \n-  HloInstruction *all_reduce_start =\n+  HloInstruction* all_reduce_start =\n       builder.AddInstruction(HloInstruction::CreateAllReduceStart(\n           ShapeUtil::MakeTupleShape({param_shape, param_shape}), {param_0},\n           add_computation, replica_groups, /*constrain_layout=*/false,\n@@ -417,7 +416,7 @@ TEST(CollectiveOpsUtilsTest, IsAsyncCollective) {\n   EXPECT_TRUE(is_async_status.ok());\n   EXPECT_TRUE(is_async_status.value());\n \n-  HloInstruction *all_reduce_done =\n+  HloInstruction* all_reduce_done =\n       builder.AddInstruction(HloInstruction::CreateUnary(\n           param_shape, HloOpcode::kAllReduceDone, all_reduce_start));\n \n@@ -426,7 +425,7 @@ TEST(CollectiveOpsUtilsTest, IsAsyncCollective) {\n   EXPECT_TRUE(is_async_status.value());\n \n   // Test for regular CollectivePermute (non-async)\n-  HloInstruction *permute =\n+  HloInstruction* permute =\n       builder.AddInstruction(HloInstruction::CreateCollectivePermute(\n           param_shape, param_0, source_target_pairs, /*channel_id=*/1));\n \n@@ -612,7 +611,7 @@ std::vector<TestCase> GetTestCases() {\n class GetCollectOpGroupModeTest : public testing::TestWithParam<TestCase> {};\n \n TEST_P(GetCollectOpGroupModeTest, Test) {\n-  const TestCase &tc = GetParam();\n+  const TestCase& tc = GetParam();\n   absl::StatusOr<CollectiveOpGroupMode> actual =\n       GetCollectiveOpGroupMode(tc.has_channel_id, tc.use_global_device_ids);\n   if (tc.expected) {\n@@ -681,13 +680,13 @@ absl::StatusOr<std::unique_ptr<HloComputation>> CreateMaxComputation() {\n   TF_ASSIGN_OR_RETURN(HloInstruction * b,\n                       builder_max.AddParameter(\n                           HloInstruction::CreateParameter(1, scalar, \"b\")));\n-  HloInstruction *max = builder_max.AddInstruction(\n+  HloInstruction* max = builder_max.AddInstruction(\n       HloInstruction::CreateBinary(scalar, HloOpcode::kMaximum, a, b), \"max\");\n   return builder_max.Build(max);\n }\n \n TEST_P(GetCollectOpGroupModeTestForInstruction, Test) {\n-  const TestCaseForInstruction &test_case = GetParam();\n+  const TestCaseForInstruction& test_case = GetParam();\n   ReplicaGroup group;\n   for (int k = 0; k < 4; ++k) {\n     group.add_replica_ids(k);\n@@ -712,7 +711,7 @@ TEST_P(GetCollectOpGroupModeTestForInstruction, Test) {\n                           builder.AddParameter(HloInstruction::CreateParameter(\n                               0, two_elements, \"parameter\")));\n \n-  HloInstruction *collective;\n+  HloInstruction* collective;\n   switch (test_case.op_code) {\n     case HloOpcode::kAllGather:\n       collective = builder.AddInstruction(HloInstruction::CreateAllGather(\n@@ -823,7 +822,7 @@ std::string TestCase::ToString() const {\n   return s.str();\n }\n \n-std::ostream &operator<<(std::ostream &os, const TestCase &tc) {\n+std::ostream& operator<<(std::ostream& os, const TestCase& tc) {\n   os << tc.ToString();\n   return os;\n }\n@@ -1077,7 +1076,7 @@ std::vector<TestCase> GetTestCases() {\n class GetParticipatingTest : public testing::TestWithParam<TestCase> {};\n \n TEST_P(GetParticipatingTest, Test) {\n-  const TestCase &tc = GetParam();\n+  const TestCase& tc = GetParam();\n \n   int64_t num_replicas = tc.device_assignment.n1();\n   int64_t num_partitions = tc.device_assignment.n2();\n@@ -1103,7 +1102,7 @@ TEST_P(GetParticipatingTest, Test) {\n   }\n \n   // Test GetParticipatingDevices.\n-  for (const TestCase::CurrentIdAndOutput &subtest : tc.subtests) {\n+  for (const TestCase::CurrentIdAndOutput& subtest : tc.subtests) {\n     absl::StatusOr<std::vector<GlobalDeviceId>> actual =\n         GetParticipatingDevices(GlobalDeviceId(subtest.current_id),\n                                 device_assignment, replica_groups, *group_mode);\n@@ -1144,15 +1143,15 @@ TEST_P(GetParticipatingTest, Test) {\n               testing::UnorderedElementsAreArray(expect_device_groups));\n \n   // Test GetParticipatingFlattenedIdGroups.\n-  absl::StatusOr<CollectiveDeviceList> collective_device_list =\n-      GetParticipatingFlattenedIdGroups(\n+  absl::StatusOr<std::unique_ptr<CollectiveDeviceListBase>>\n+      collective_device_list = GetParticipatingFlattenedIdGroups(\n           device_assignment, CollectiveDeviceList(replica_groups), *group_mode);\n   if (!collective_device_list.ok()) {\n     EXPECT_TRUE(tc.expected_failure);\n     return;\n   }\n-  const std::vector<ReplicaGroup> &actual_flattened_id_groups =\n-      collective_device_list.value().replica_groups();\n+  const std::vector<ReplicaGroup>& actual_flattened_id_groups =\n+      collective_device_list.value()->replica_groups();\n \n   std::vector<std::vector<int64_t>> actual_flattened_id_groups_int;\n   actual_flattened_id_groups_int.reserve(actual_flattened_id_groups.size());\n@@ -1192,16 +1191,16 @@ TEST_P(GetParticipatingTest, Test) {\n       /*parameter_number=*/1, ShapeUtil::MakeShape(F32, {}), \"y\"));\n   sum_builder.AddInstruction(HloInstruction::CreateBinary(\n       ShapeUtil::MakeShape(F32, {}), HloOpcode::kAdd, x, y));\n-  HloComputation *reduction =\n+  HloComputation* reduction =\n       hlo_module.AddEmbeddedComputation(sum_builder.Build());\n   HloComputation::Builder entry_builder(\"test_entry\");\n-  HloInstruction *operand = entry_builder.AddInstruction(\n+  HloInstruction* operand = entry_builder.AddInstruction(\n       HloInstruction::CreateConstant(LiteralUtil::CreateR0<float>(1.0f)));\n   std::optional<int64_t> channel_id = std::nullopt;\n   if (tc.has_channel_id) {\n     channel_id = 0;\n   }\n-  HloInstruction *ar =\n+  HloInstruction* ar =\n       entry_builder.AddInstruction(HloInstruction::CreateAllReduce(\n           operand->shape(), {operand}, reduction, replica_groups,\n           /*constrain_layout=*/false,\n@@ -1241,7 +1240,7 @@ class GetPariticipantCountsForReplicaGroupsTest\n     : public testing::TestWithParam<TestCase> {};\n \n TEST_P(GetPariticipantCountsForReplicaGroupsTest, Test) {\n-  const TestCase &tc = GetParam();\n+  const TestCase& tc = GetParam();\n \n   std::vector<ReplicaGroup> replica_groups =\n       CreateReplicaGroups(tc.replica_groups);\n@@ -1294,7 +1293,7 @@ INSTANTIATE_TEST_SUITE_P(\n     GetPariticipantCountsForReplicaGroupsTest,\n     testing::ValuesIn(GetTestCases()),\n     [](const testing::TestParamInfo<\n-        GetPariticipantCountsForReplicaGroupsTest::ParamType> &info) {\n+        GetPariticipantCountsForReplicaGroupsTest::ParamType>& info) {\n       return info.param.test_name;\n     });\n "
        },
        {
            "sha": "c2408e4e709ac17baf02ad4047d05e29be5521bc",
            "filename": "third_party/xla/xla/service/gpu/model/collective_interpolator.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 11,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -101,13 +101,12 @@ struct InterpolationSpecification {\n // Returns number of participating devices in an input `device_list`. Supports\n // only `iota_replica_group_list`.\n absl::StatusOr<int> GetNumParticipatingDevices(\n-    const CollectiveDeviceList& device_list) {\n-  auto iota = device_list.iota_replica_group_list();\n-  if (!iota.has_value()) {\n+    const CollectiveDeviceListBase& device_list) {\n+  if (device_list.version() != CollectiveDeviceListVersion::kIota) {\n     return absl::FailedPreconditionError(\n         \"Only iota device assignment is supported.\");\n   }\n-  return iota->num_devices_per_group();\n+  return device_list.num_devices_per_group();\n }\n \n absl::StatusOr<InterpolationSpecification> Spec(\n@@ -147,13 +146,16 @@ absl::StatusOr<InterpolationSpecification> Spec(\n   TF_ASSIGN_OR_RETURN(int num_devices,\n                       GetNumParticipatingDevices(collective->device_list()));\n \n+  CollectiveDeviceList list_of_devices =\n+      ConvertToV1CollectiveDeviceList(collective->device_list());\n+\n   return InterpolationSpecification{\n       /*opcode=*/collective->opcode(),\n       /*num_devices=*/num_devices,\n       /*transfer_size=*/bytes_transferred,\n       /*data_type=*/collective->shape().element_type(),\n       /*collective_params=*/\n-      CollectiveOpSpecInfo{collective->device_list(), comm}};\n+      CollectiveOpSpecInfo{list_of_devices, comm}};\n }\n \n std::unique_ptr<HloModule> AllReduceModule(\n@@ -348,11 +350,13 @@ std::unique_ptr<HloModule> CollectivePermuteModule(\n   return module;\n }\n \n-std::optional<CollectiveDeviceList> CanonicalDeviceList(\n+std::optional<std::unique_ptr<CollectiveDeviceListBase>> CanonicalDeviceList(\n     const HloCollectiveInstruction& instr) {\n-  if (instr.device_list().iota_replica_group_list().has_value()) {\n-    return instr.device_list();\n+  const CollectiveDeviceListBase& device_list = instr.device_list();\n+  if (device_list.version() == CollectiveDeviceListVersion::kIota) {\n+    return device_list.Clone();\n   }\n+\n   auto num_groups_and_devices = GetReplicaGroupCountAndSize(&instr);\n   if (!num_groups_and_devices.ok() || !num_groups_and_devices->has_value()) {\n     VLOG(1) << \"Failed to determine a number of devices participating in \"\n@@ -363,7 +367,7 @@ std::optional<CollectiveDeviceList> CanonicalDeviceList(\n \n   IotaReplicaGroupList iota((*num_groups_and_devices)->first,\n                             (*num_groups_and_devices)->second);\n-  return CollectiveDeviceList(iota);\n+  return std::make_unique<CollectiveDeviceList>(iota);\n }\n \n HloOpcode AsyncToSyncOpcode(const HloCollectiveInstruction& instr) {\n@@ -720,12 +724,15 @@ absl::StatusOr<absl::Duration> CollectiveInterpolator::EstimatedRuntime(\n         absl::StrCat(\"Cannot find key for instr: \", instr.ToString()));\n   }\n   auto* collective = Cast<HloCollectiveInstruction>(&instr);\n-  std::optional<CollectiveDeviceList> devices =\n+  std::optional<std::unique_ptr<CollectiveDeviceListBase>> devices =\n       CanonicalDeviceList(*collective);\n   if (devices.has_value()) {\n+    CollectiveDeviceList list_of_devices =\n+        ConvertToV1CollectiveDeviceList(*devices.value());\n+\n     ExactInterpolatorKey exact_key{\n         /*opcode=*/instr.opcode(),\n-        /*collective_params=*/*devices,\n+        /*collective_params=*/list_of_devices,\n         /*data_type=*/\n         RequiresAccumulation(instr.opcode())\n             ? std::make_optional(instr.shape().element_type())"
        },
        {
            "sha": "daec75b8f1c563c10c86c92640f653ec20675b3f",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -739,7 +739,7 @@ class IsSolLatencyEstimatorEnabledTest : public HloTestBase {\n         module->AddEmbeddedComputation(wrapped_computation.Build());\n     entry->AddInstruction(HloInstruction::CreateAllReduce(\n         shape, {dummy_operand}, subcomp,\n-        /*replica_groups=*/{}, /*constrain_layout=*/false,\n+        /*device_list=*/CollectiveDeviceList(), /*constrain_layout=*/false,\n         /*channel_id=*/std::nullopt, /*use_global_device_ids=*/false));\n   }\n "
        },
        {
            "sha": "8aae8502e73d9baef414dbc8684564a6e757867f",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -824,12 +824,12 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n   }\n \n   virtual double GetCommunicationTimeInMilliSec(\n-      int64_t bytes, const CollectiveDeviceList& collective_device_list) {\n+      int64_t bytes, const CollectiveDeviceListBase& collective_device_list) {\n     return 0.0;\n   }\n \n   virtual int GetCommunicationMultiplier(\n-      const CollectiveDeviceList& collective_device_list) {\n+      const CollectiveDeviceListBase& collective_device_list) {\n     return 1;\n   }\n "
        },
        {
            "sha": "e824fafc006dacc6aa58cf69f5ab33b2e54cb86e",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 23,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=07acbd560e5d2ee59f8df8d4d0a7797d9cf03f9a",
            "patch": "@@ -532,13 +532,12 @@ ENTRY entry {\n   EXPECT_NE(all_gather, nullptr);\n \n   // Verify all-gather instruction contains ReplicaGroupV2.\n-  EXPECT_TRUE(all_gather->device_list().iota_replica_group_list().has_value());\n-  IotaReplicaGroupList list =\n-      all_gather->device_list().iota_replica_group_list().value();\n-  EXPECT_EQ(list.num_replica_groups(), 1);\n-  EXPECT_EQ(list.num_devices_per_group(), 4);\n-  EXPECT_THAT(list.reshape_dims(), ::testing::ElementsAre(4));\n-  EXPECT_THAT(list.transpose_perm(), ::testing::ElementsAre(0));\n+  EXPECT_TRUE(all_gather->device_list().version() ==\n+              CollectiveDeviceListVersion::kIota);\n+  EXPECT_EQ(all_gather->device_list(),\n+            CollectiveDeviceList(IotaReplicaGroupList(\n+                /*num_replica_groups=*/1, /*num_devices_per_group=*/4,\n+                /*reshape_dims=*/{4}, /*transpose_perm=*/{0})));\n }\n \n TEST_P(SpmdPartitioningTest, TiledToSingleDevice) {\n@@ -598,8 +597,10 @@ ENTRY entry {\n   EXPECT_EQ(all_to_all->replica_groups().size(), 1);\n   EXPECT_EQ(all_to_all->replica_groups()[0].replica_ids_size(), 8);\n   if (GetParam() == ShardingFormatPicker::ShardingType::kBestEffortV2) {\n-    EXPECT_EQ(all_to_all->device_list().iota_replica_group_list(),\n-              IotaReplicaGroupList(1, 8, {4, 2}, {1, 0}));\n+    EXPECT_EQ(all_to_all->device_list(),\n+              CollectiveDeviceList(IotaReplicaGroupList(\n+                  /*num_replica_groups=*/1, /*num_devices_per_group=*/8,\n+                  /*reshape_dims=*/{4, 2}, /*transpose_perm=*/{1, 0})));\n   } else {\n     std::vector<std::vector<int64_t>> expected_replica_groups = {\n         {0, 2, 4, 6, 1, 3, 5, 7}};\n@@ -2007,18 +2008,12 @@ ENTRY entry {\n             module->entry_computation()->instructions().end());\n \n   // Verify all-reduce instruction contains ReplicaGroupV2.\n-  EXPECT_TRUE((*all_reduce_instruction)\n-                  ->device_list()\n-                  .iota_replica_group_list()\n-                  .has_value());\n-  IotaReplicaGroupList list = (*all_reduce_instruction)\n-                                  ->device_list()\n-                                  .iota_replica_group_list()\n-                                  .value();\n-  EXPECT_EQ(list.num_replica_groups(), 1);\n-  EXPECT_EQ(list.num_devices_per_group(), 8);\n-  EXPECT_THAT(list.reshape_dims(), ::testing::ElementsAre(8));\n-  EXPECT_THAT(list.transpose_perm(), ::testing::ElementsAre(0));\n+  EXPECT_EQ((*all_reduce_instruction)->device_list().version(),\n+            CollectiveDeviceListVersion::kIota);\n+  EXPECT_EQ((*all_reduce_instruction)->device_list(),\n+            CollectiveDeviceList(IotaReplicaGroupList(\n+                /*num_replica_groups=*/1, /*num_devices_per_group=*/8,\n+                /*reshape_dims=*/{8}, /*transpose_perm=*/{0})));\n }\n \n TEST_P(SpmdPartitioningTest, ConvolutionLhsTiledRhsTiledWindowReversal) {\n@@ -12070,8 +12065,14 @@ ENTRY %module {\n   EXPECT_TRUE(all_to_all != nullptr);\n   if (GetParam() ==\n       test_only::ShardingFormatPicker::ShardingType::kBestEffortV2) {\n-    EXPECT_EQ(all_to_all->device_list().iota_replica_group_list().value(),\n-              IotaReplicaGroupList(4, 2, {2, 2, 2}, {0, 2, 1}));\n+    EXPECT_EQ(all_to_all->device_list().version(),\n+              CollectiveDeviceListVersion::kIota);\n+    EXPECT_EQ(all_to_all->device_list(),\n+              CollectiveDeviceList(IotaReplicaGroupList(\n+                  /*num_replica_groups=*/4, /*num_devices_per_group=*/2,\n+                  /*reshape_dims=*/{2, 2, 2},\n+                  /*transpose_perm=*/{0, 2, 1})));\n+\n   } else {\n     std::vector<std::vector<int64_t>> expected_replica_groups = {\n         {0, 2}, {1, 3}, {4, 6}, {5, 7}};"
        }
    ],
    "stats": {
        "total": 413,
        "additions": 247,
        "deletions": 166
    }
}