{
    "author": "hyeontaek",
    "message": "[IFRT] Migrate `Array::layout()` calls to use `Array::pjrt_layout()`.\n\nExisting users of `Array::layout()` are migrated to\n`Array::pjrt_layout()`. `Array::layout()` will be re-introduced with a\nnew version that uses IFRT `CustomLayoutRef` (which will eventually be\nupdated to return `LayoutRef` that is nullable).\n\nPiperOrigin-RevId: 811858034",
    "sha": "0f76662155d24a7ac992e671c8bfde7da691f053",
    "files": [
        {
            "sha": "36286ee3a59199d21ac1af8387faf4a6c366a0a4",
            "filename": "third_party/xla/xla/python/ifrt/array_impl_test_lib.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc?ref=0f76662155d24a7ac992e671c8bfde7da691f053",
            "patch": "@@ -315,7 +315,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferDefaultLayout) {\n             /*on_done_with_host_buffer=*/nullptr));\n     TF_ASSERT_OK(array->GetReadyFuture().Await());\n \n-    TF_ASSERT_OK_AND_ASSIGN(auto layout, array->layout());\n+    TF_ASSERT_OK_AND_ASSIGN(auto layout, array->pjrt_layout());\n     EXPECT_EQ(*layout, *default_layout);\n   }\n }\n@@ -710,7 +710,7 @@ TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithLayout) {\n   }\n \n   TF_ASSERT_OK(array->GetReadyFuture().Await());\n-  TF_ASSERT_OK_AND_ASSIGN(auto result_layout, array->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto result_layout, array->pjrt_layout());\n   EXPECT_EQ(*result_layout, *layout);\n }\n \n@@ -1448,7 +1448,7 @@ TEST(ArrayImplTest, CopyPreservesDefaultLayouts) {\n               /*on_done_with_host_buffer=*/nullptr));\n       TF_ASSERT_OK(array->GetReadyFuture().Await());\n \n-      TF_ASSERT_OK_AND_ASSIGN(auto src_layout, array->layout());\n+      TF_ASSERT_OK_AND_ASSIGN(auto src_layout, array->pjrt_layout());\n       TF_ASSERT_OK_AND_ASSIGN(\n           auto src_default_layout,\n           client->GetDefaultLayout(dtype, shape.dims(), device,\n@@ -1460,7 +1460,7 @@ TEST(ArrayImplTest, CopyPreservesDefaultLayouts) {\n                                               std::nullopt, dst_memory->Kind(),\n                                               ArrayCopySemantics::kAlwaysCopy));\n       ASSERT_THAT(new_arrays, SizeIs(1));\n-      TF_ASSERT_OK_AND_ASSIGN(auto dst_layout, new_arrays[0]->layout());\n+      TF_ASSERT_OK_AND_ASSIGN(auto dst_layout, new_arrays[0]->pjrt_layout());\n       TF_ASSERT_OK_AND_ASSIGN(\n           auto dst_default_layout,\n           client->GetDefaultLayout(dtype, shape.dims(), device,"
        },
        {
            "sha": "6c71efe1b3cd7fab35d44bd618eabf1d2064e21f",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/array_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray_test.cc?ref=0f76662155d24a7ac992e671c8bfde7da691f053",
            "patch": "@@ -165,15 +165,15 @@ TEST_F(ArrayTest, GetDefaultPjRtLayoutSuccess) {\n   auto array = tsl::MakeRef<Array>(\n       mock_client_.get(), rpc_helper_, DType(DType::Kind::kBF16), Shape({}),\n       sharding_, ArrayHandle{1234}, /*layout=*/nullptr);\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, array->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, array->pjrt_layout());\n   EXPECT_EQ(*layout_1, *kLayout1);\n }\n \n TEST_F(ArrayTest, GetCustomLayoutSuccess) {\n   auto array = tsl::MakeRef<Array>(mock_client_.get(), rpc_helper_,\n                                    DType(DType::Kind::kBF16), Shape({}),\n                                    sharding_, ArrayHandle{1234}, kLayout1);\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, array->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, array->pjrt_layout());\n   EXPECT_EQ(*layout_1, *kLayout1);\n }\n \n@@ -216,9 +216,9 @@ TEST_F(ArrayTest, MakeArraysFromHostBufferShardsSuccess) {\n       mock_client_.get(), rpc_helper_, absl::MakeSpan(specs),\n       xla::ifrt::Client::HostBufferSemantics::kImmutableOnlyDuringCall);\n   TF_ASSERT_OK(result.status());\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, result.value().at(0)->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, result.value().at(0)->pjrt_layout());\n   EXPECT_EQ(*layout_1, *kLayout1);\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout_2, result.value().at(1)->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout_2, result.value().at(1)->pjrt_layout());\n   EXPECT_EQ(*layout_2, *kLayout2);\n }\n \n@@ -239,7 +239,7 @@ TEST_F(ArrayTest, MakeErrorArraysSuccess) {\n                                        absl::InternalError(\"test error\"),\n                                        absl::MakeSpan(specs));\n   TF_ASSERT_OK(result.status());\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout, result.value().at(0)->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout, result.value().at(0)->pjrt_layout());\n   EXPECT_EQ(*layout, *kLayout1);\n }\n \n@@ -268,7 +268,7 @@ TEST_F(ArrayTest, AssembleArrayFromSingleDeviceArraysSuccess) {\n       sharding_, absl::MakeSpan(arrays), ArrayCopySemantics::kAlwaysCopy,\n       SingleDeviceShardSemantics::kAllShards);\n   TF_ASSERT_OK(result.status());\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout, result.value()->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout, result.value()->pjrt_layout());\n   EXPECT_EQ(*layout, *kLayout1);\n }\n \n@@ -298,7 +298,7 @@ TEST_F(ArrayTest, AssembleArrayFromSingleDeviceArraysDefaultPjRtLayoutSuccess) {\n       sharding_, absl::MakeSpan(arrays), ArrayCopySemantics::kAlwaysCopy,\n       SingleDeviceShardSemantics::kAllShards);\n   TF_ASSERT_OK(result.status());\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout, result.value()->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout, result.value()->pjrt_layout());\n   EXPECT_EQ(*layout, *kLayout1);\n }\n \n@@ -343,9 +343,9 @@ TEST_F(ArrayTest, RemapArraysSuccess) {\n                          ArrayCopySemantics::kAlwaysCopy);\n \n   TF_ASSERT_OK(result.status());\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, result.value().at(0)->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout_1, result.value().at(0)->pjrt_layout());\n   EXPECT_EQ(*layout_1, *kLayout2);\n-  TF_ASSERT_OK_AND_ASSIGN(auto layout_2, result.value().at(1)->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(auto layout_2, result.value().at(1)->pjrt_layout());\n   EXPECT_EQ(*layout_2, *kLayout1);\n }\n "
        },
        {
            "sha": "40b04602798961f7ba968840cc126a7a3f47d6a4",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc?ref=0f76662155d24a7ac992e671c8bfde7da691f053",
            "patch": "@@ -1386,7 +1386,7 @@ PjRtClient::CopyArraysForCrossHost(absl::Span<ArrayRef> arrays,\n     TF_ASSIGN_OR_RETURN(ShardingRef new_sharding,\n                         arrays[i]->shared_ptr_sharding()->WithDeviceAssignment(\n                             dst_devices, memory_kind));\n-    TF_ASSIGN_OR_RETURN(auto new_layout, arrays[i]->layout());\n+    TF_ASSIGN_OR_RETURN(auto new_layout, arrays[i]->pjrt_layout());\n     TF_ASSIGN_OR_RETURN(\n         new_arrays.emplace_back(),\n         PjRtArray::Create(this, arrays[i]->dtype(), arrays[i]->shape(),"
        },
        {
            "sha": "6b13e8e9f9b739e5848ef53402d149d5f5504b5c",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/reshard_impl_test_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc?ref=0f76662155d24a7ac992e671c8bfde7da691f053",
            "patch": "@@ -401,7 +401,7 @@ TEST_F(ReshardTest, DifferentDestinationLayout) {\n \n   // Make sure that the destination layout is actually different from the source\n   // layout in order to ensure the test coverage.\n-  TF_ASSERT_OK_AND_ASSIGN(const auto src_layout, src_array->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(const auto src_layout, src_array->pjrt_layout());\n   ASSERT_NE(src_layout->xla_layout(), dst_array_spec.layout->xla_layout());\n \n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -414,7 +414,7 @@ TEST_F(ReshardTest, DifferentDestinationLayout) {\n   EXPECT_EQ(dst_array->sharding(), *dst_array_spec.sharding);\n \n   // Verify that the destination array is created with the user-provided layout.\n-  TF_ASSERT_OK_AND_ASSIGN(const auto dst_layout, dst_array->layout());\n+  TF_ASSERT_OK_AND_ASSIGN(const auto dst_layout, dst_array->pjrt_layout());\n   EXPECT_EQ(dst_layout->xla_layout(), dst_array_spec.layout->xla_layout());\n \n   EXPECT_THAT(CopyArrayToLiteral(dst_array),"
        },
        {
            "sha": "4f783982a458dacefdfaea2002bbd908ba8c2045",
            "filename": "third_party/xla/xla/python/transfer/pjrt_transfer_server.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f76662155d24a7ac992e671c8bfde7da691f053/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc?ref=0f76662155d24a7ac992e671c8bfde7da691f053",
            "patch": "@@ -248,7 +248,7 @@ absl::Status PjRtTransferServer::CrossHostPull(\n         xla::DimensionVector(shape.dims().begin(), shape.dims().end())};\n     shape_specs.push_back(shape_spec);\n \n-    auto pjrt_layout = arrays[i]->layout();\n+    auto pjrt_layout = arrays[i]->pjrt_layout();\n     std::optional<xla::Layout> layout;\n     if (pjrt_layout.ok()) {\n       layout = (*pjrt_layout)->xla_layout();\n@@ -344,7 +344,7 @@ PjRtTransferServer::CopyArraysForCrossHost(\n     TF_ASSIGN_OR_RETURN(auto new_sharding,\n                         arrays[i]->shared_ptr_sharding()->WithDeviceAssignment(\n                             dst_devices, memory_kind));\n-    TF_ASSIGN_OR_RETURN(auto new_layout, arrays[i]->layout());\n+    TF_ASSIGN_OR_RETURN(auto new_layout, arrays[i]->pjrt_layout());\n     PjRtArray::PjRtBuffers array_buffers;\n     array_buffers.reserve(buffers_by_device.size());\n     for (auto& [_, bufs] : buffers_by_device) {"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 18,
        "deletions": 18
    }
}