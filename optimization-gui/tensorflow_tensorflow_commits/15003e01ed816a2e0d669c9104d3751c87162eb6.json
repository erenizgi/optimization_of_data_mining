{
    "author": "ezhulenev",
    "message": "[xla:cpu] Migrate XLA /service folder to se::DeviceAddress\n\nPiperOrigin-RevId: 841192203",
    "sha": "15003e01ed816a2e0d669c9104d3751c87162eb6",
    "files": [
        {
            "sha": "252abf3c4d8f98a832dd42b3143424ed52718c8b",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 19,
            "deletions": 17,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -1098,7 +1098,7 @@ cc_library(\n         \":stream_pool\",\n         \":transfer_manager\",\n         \"//xla:util\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n@@ -1155,7 +1155,7 @@ cc_library(\n         \"//xla/hlo/builder:xla_computation\",\n         \"//xla/hlo/evaluator:hlo_evaluator\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n@@ -1194,7 +1194,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/client:executable_build_options\",\n         \"//xla/hlo/builder:xla_computation\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1482,8 +1482,8 @@ cc_library(\n         \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log:check\",\n@@ -1504,8 +1504,8 @@ xla_cc_test(\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/testlib:test\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_absl//absl/log\",\n@@ -1540,8 +1540,8 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/client:executable_build_options\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n@@ -1601,8 +1601,8 @@ cc_library(\n         \"//xla/backends/cpu:target_machine_options\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:semantic_version\",\n@@ -1675,7 +1675,7 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -1707,8 +1707,8 @@ cc_library(\n         \"//xla:types\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n@@ -3227,7 +3227,7 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:platform\",\n@@ -3256,7 +3256,7 @@ xla_cc_test(\n         \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n@@ -4085,6 +4085,8 @@ cc_library(\n     srcs = [\"maybe_owning_device_memory.cc\"],\n     hdrs = [\"maybe_owning_device_memory.h\"],\n     deps = [\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n     ],\n@@ -4461,9 +4463,9 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "820a9b840493647a3cecbba48224a314e027a4e9",
            "filename": "third_party/xla/xla/service/allocation_tracker.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fallocation_tracker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fallocation_tracker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fallocation_tracker.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -28,8 +28,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/errors.h\"\n@@ -212,21 +212,21 @@ AllocationTracker::ResolveInternal(const GlobalDataHandle& data) const {\n }\n \n void AllocationTracker::AddAllocationOrIncrementRefCount(\n-    se::DeviceMemoryBase device_memory, int device_ordinal) {\n+    se::DeviceAddressBase device_memory, int device_ordinal) {\n   AllocationMap& allocation_map = opaque_to_allocation_map_[device_ordinal];\n   auto it = allocation_map.find(device_memory.opaque());\n   if (it == allocation_map.end()) {\n     allocation_map[device_memory.opaque()] = {\n-        se::OwningDeviceMemory(device_memory, device_ordinal,\n-                               backend_->memory_allocator()),\n+        se::ScopedDeviceAddress<uint8_t>(device_memory, device_ordinal,\n+                                         backend_->memory_allocator()),\n         /*ref_count=*/1};\n   } else {\n     it->second.ref_count++;\n   }\n }\n \n absl::Status AllocationTracker::DecrementRefCount(\n-    se::DeviceMemoryBase device_memory, int device_ordinal) {\n+    se::DeviceAddressBase device_memory, int device_ordinal) {\n   AllocationMap& allocation_map = opaque_to_allocation_map_[device_ordinal];\n   auto it = allocation_map.find(device_memory.opaque());\n   TF_RET_CHECK(it != allocation_map.end());"
        },
        {
            "sha": "f65a066fd40d8ea75f998e80257fb8eb6d57c295",
            "filename": "third_party/xla/xla/service/allocation_tracker.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fallocation_tracker.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fallocation_tracker.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fallocation_tracker.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -29,8 +29,8 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/service/backend.h\"\n #include \"xla/service/shaped_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/types.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -80,7 +80,7 @@ class AllocationTracker {\n   // Data structure encapsulating single memory allocation on the device.\n   struct Allocation {\n     // The pointer to this allocation.\n-    se::OwningDeviceMemory device_memory;\n+    se::ScopedDeviceAddress<uint8_t> device_memory;\n \n     // This is the number of times this memory allocation is referred to by\n     // registered data handles.\n@@ -103,13 +103,13 @@ class AllocationTracker {\n \n   // Adds the given device address to the allocation tracker, or if it already\n   // exists, then increment its reference count.\n-  void AddAllocationOrIncrementRefCount(se::DeviceMemoryBase device_memory,\n+  void AddAllocationOrIncrementRefCount(se::DeviceAddressBase device_memory,\n                                         int device_ordinal)\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n \n   // Decrements the reference count of the given device memory. Then, if it is\n   // zero, deallocate the memory.\n-  absl::Status DecrementRefCount(se::DeviceMemoryBase device_memory,\n+  absl::Status DecrementRefCount(se::DeviceAddressBase device_memory,\n                                  int device_ordinal)\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n "
        },
        {
            "sha": "613287f998cacf43e02599974b8a042026a3c194",
            "filename": "third_party/xla/xla/service/backend.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fbackend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fbackend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbackend.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -35,7 +35,7 @@ limitations under the License.\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/stream_pool.h\"\n #include \"xla/service/transfer_manager.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n@@ -93,10 +93,10 @@ class Backend {\n   // Accessors for the various objects.\n   se::Platform* platform() const { return platform_; }\n   Compiler* compiler() const { return compiler_.get(); }\n-  se::DeviceMemoryAllocator* memory_allocator() const {\n+  se::DeviceAddressAllocator* memory_allocator() const {\n     return memory_allocator_.get();\n   }\n-  std::shared_ptr<se::DeviceMemoryAllocator> shared_memory_allocator() const {\n+  std::shared_ptr<se::DeviceAddressAllocator> shared_memory_allocator() const {\n     return memory_allocator_;\n   }\n   TransferManager* transfer_manager() const { return transfer_manager_; }"
        },
        {
            "sha": "4d83f602f2cb5308b19170d28157beabe93c22db",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -49,8 +49,8 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/metrics_hook_interface.h\"\n #include \"xla/shape.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -176,7 +176,7 @@ class Compiler {\n     // compiler may allocate buffers on the device and then run variants of a\n     // given algorithm over those buffers, to see which variant is fastest.  Any\n     // space allocated will be deallocated before the compilation returns.\n-    se::DeviceMemoryAllocator* device_allocator = nullptr;\n+    se::DeviceAddressAllocator* device_allocator = nullptr;\n \n     // An optional thread pool for parallel compilation.\n     tsl::thread::ThreadPool* thread_pool = nullptr;\n@@ -213,7 +213,7 @@ class Compiler {\n       const CompileOptions& options) = 0;\n   absl::StatusOr<std::unique_ptr<HloModule>> RunHloPasses(\n       std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n-      se::DeviceMemoryAllocator* device_allocator) {\n+      se::DeviceAddressAllocator* device_allocator) {\n     return RunHloPasses(std::move(module), executor,\n                         CompileOptions{device_allocator});\n   }\n@@ -231,7 +231,7 @@ class Compiler {\n       const CompileOptions& options) = 0;\n   absl::StatusOr<std::unique_ptr<Executable>> RunBackend(\n       std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n-      se::DeviceMemoryAllocator* device_allocator) {\n+      se::DeviceAddressAllocator* device_allocator) {\n     return RunBackend(std::move(module), executor,\n                       CompileOptions{device_allocator});\n   }\n@@ -255,7 +255,7 @@ class Compiler {\n       std::unique_ptr<HloModule> module,\n       const BufferAssignmentProto* buffer_assignment_proto,\n       se::StreamExecutor* executor,\n-      se::DeviceMemoryAllocator* device_allocator) {\n+      se::DeviceAddressAllocator* device_allocator) {\n     return RunBackendWithBufferAssignment(std::move(module),\n                                           buffer_assignment_proto, executor,\n                                           CompileOptions{device_allocator});\n@@ -281,7 +281,7 @@ class Compiler {\n   absl::StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(\n       std::unique_ptr<HloModule> hlo_module,\n       std::vector<se::StreamExecutor*> stream_exec,\n-      se::DeviceMemoryAllocator* device_allocator) {\n+      se::DeviceAddressAllocator* device_allocator) {\n     return Compile(std::move(hlo_module), stream_exec,\n                    CompileOptions{device_allocator});\n   }\n@@ -423,10 +423,10 @@ class AotCompilationOptions {\n \n   // Optional allocator that may be used for allocating temp space on the device\n   // during compilation.\n-  se::DeviceMemoryAllocator* device_allocator() const {\n+  se::DeviceAddressAllocator* device_allocator() const {\n     return device_allocator_;\n   }\n-  void set_device_allocator(se::DeviceMemoryAllocator* device_allocator) {\n+  void set_device_allocator(se::DeviceAddressAllocator* device_allocator) {\n     device_allocator_ = device_allocator;\n   }\n \n@@ -505,7 +505,7 @@ class AotCompilationOptions {\n \n  private:\n   se::Platform::Id platform_id_;\n-  se::DeviceMemoryAllocator* device_allocator_ = nullptr;\n+  se::DeviceAddressAllocator* device_allocator_ = nullptr;\n   DebugOptions debug_options_;\n   std::optional<DeviceAssignment> static_device_assignment_;\n   std::vector<std::vector<bool>> fusion_config_;"
        },
        {
            "sha": "986fe761476f80281b5688ff91e4b528c1172deb",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -106,7 +106,7 @@ cc_library(\n         \"//xla/service:compiler\",\n         \"//xla/service:generic_transfer_manager\",\n         \"//xla/service:transfer_manager\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/host:host_platform_id\",\n@@ -553,8 +553,8 @@ cc_library(\n         \"//xla/service:maybe_owning_device_memory\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:xla_debug_info_manager\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n@@ -875,7 +875,7 @@ cc_library(\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/llvm_ir:llvm_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n@@ -982,7 +982,7 @@ cc_library(\n         \"//xla/runtime:device_id\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:computation_placer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "6e0cf855e34f97e0053c32793e3101a78e34ee54",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -65,8 +65,8 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -167,12 +167,12 @@ static absl::StatusOr<MaybeOwningDeviceMemory> MemoryForAllocation(\n     const BufferAllocation& allocation,\n     absl::Span<const ExecutionInput> arguments,\n     absl::Span<const ConstantAllocation> constants,\n-    se::DeviceMemoryAllocator* memory_allocator, int device_ordinal) {\n+    se::DeviceAddressAllocator* memory_allocator, int device_ordinal) {\n   VLOG(3) << allocation.ToString();\n   if (allocation.is_entry_computation_parameter()) {\n-    se::DeviceMemoryBase out = arguments[allocation.parameter_number()]\n-                                   .Buffer(allocation.param_shape_index())\n-                                   .AsDeviceMemoryBase();\n+    se::DeviceAddressBase out = arguments[allocation.parameter_number()]\n+                                    .Buffer(allocation.param_shape_index())\n+                                    .AsDeviceMemoryBase();\n     CHECK_LE(allocation.size(), out.size())\n         << \"Size mismatch on param \" << allocation.parameter_number()\n         << \" at shape index \" << allocation.param_shape_index().ToString();\n@@ -184,14 +184,14 @@ static absl::StatusOr<MaybeOwningDeviceMemory> MemoryForAllocation(\n       return MaybeOwningDeviceMemory(\n           constants[allocation.index()].AsDeviceMemoryBase());\n     }\n-    return MaybeOwningDeviceMemory{se::DeviceMemoryBase{}};\n+    return MaybeOwningDeviceMemory{se::DeviceAddressBase{}};\n   } else if (allocation.is_thread_local()) {\n     VLOG(3) << \"buffer is thread-local\";\n-    return MaybeOwningDeviceMemory{se::DeviceMemoryBase{}};\n+    return MaybeOwningDeviceMemory{se::DeviceAddressBase{}};\n   }\n \n   int64_t buffer_size = allocation.size();\n-  TF_ASSIGN_OR_RETURN(se::OwningDeviceMemory out,\n+  TF_ASSIGN_OR_RETURN(se::ScopedDeviceAddress<uint8_t> out,\n                       memory_allocator->Allocate(device_ordinal, buffer_size));\n   VLOG(3) << \"buffer allocated \" << buffer_size << \" bytes [\" << out->opaque()\n           << \"]\";\n@@ -205,7 +205,7 @@ static absl::StatusOr<MaybeOwningDeviceMemory> MemoryForAllocation(\n }\n \n absl::StatusOr<std::vector<MaybeOwningDeviceMemory>>\n-CpuExecutable::CreateBufferTable(se::DeviceMemoryAllocator* memory_allocator,\n+CpuExecutable::CreateBufferTable(se::DeviceAddressAllocator* memory_allocator,\n                                  int device_ordinal,\n                                  absl::Span<ExecutionInput const> arguments) {\n   std::vector<MaybeOwningDeviceMemory> buffers(\n@@ -326,12 +326,12 @@ absl::StatusOr<ExecutionOutput> CpuExecutable::CreateResultShapedBuffer(\n   HloInstruction* root = module().entry_computation()->root_instruction();\n   const Shape& root_shape = root->shape();\n \n-  // Move se::OwningDeviceMemory values which contain the array(s) of the result\n-  // into the respective location in ScopedShapedBuffer which is returned to the\n-  // caller.\n+  // Move se::ScopedDeviceAddress<uint8_t> values which contain the array(s) of\n+  // the result into the respective location in ScopedShapedBuffer which is\n+  // returned to the caller.\n   for (auto& p : result.MutableResult()->buffers()) {\n     const ShapeIndex& index = p.first;\n-    se::DeviceMemoryBase& result_buffer = p.second;\n+    se::DeviceAddressBase& result_buffer = p.second;\n     const HloValueSet& sources = this->GetRootValueSet().element(index);\n     // The points to set is unambiguous so the set should be a\n     // singleton.\n@@ -360,13 +360,13 @@ absl::StatusOr<ExecutionOutput> CpuExecutable::CreateResultShapedBuffer(\n             \"compile time but not donated at runtime: %s\",\n             alias->ToString());\n       }\n-      if (std::optional<se::OwningDeviceMemory> owning =\n+      if (std::optional<se::ScopedDeviceAddress<uint8_t>> owning =\n               maybe_owning_memory->Release()) {\n         // If the caller passes the ownership of the device memory, reuse it\n         // as the output buffer. It is up to the caller whether or not to\n         // donate a buffer; the aliasing information describes which buffers\n         // may alias, not buffers that must alias.\n-        se::DeviceMemoryBase argument_buffer = owning->Release();\n+        se::DeviceAddressBase argument_buffer = owning->Release();\n         *maybe_owning_memory = argument_buffer;\n         result_buffer = argument_buffer;\n         // The caller is giving us the\n@@ -384,7 +384,7 @@ absl::StatusOr<ExecutionOutput> CpuExecutable::CreateResultShapedBuffer(\n         int64_t allocation_size =\n             ShapeUtil::ByteSizeOf(ShapeUtil::GetSubshape(root_shape, index));\n         TF_ASSIGN_OR_RETURN(\n-            se::OwningDeviceMemory allocated_buffer,\n+            se::ScopedDeviceAddress<uint8_t> allocated_buffer,\n             run_options->allocator()->Allocate(\n                 stream->parent()->device_ordinal(), allocation_size));\n         result_buffer = allocated_buffer.Release();\n@@ -400,7 +400,7 @@ absl::StatusOr<ExecutionOutput> CpuExecutable::CreateResultShapedBuffer(\n \n     if (result_buffer.is_null()) {\n       MaybeOwningDeviceMemory& buffer = buffers[buffer_index];\n-      if (std::optional<se::OwningDeviceMemory> owned_buffer =\n+      if (std::optional<se::ScopedDeviceAddress<uint8_t>> owned_buffer =\n               buffer.Release()) {\n         result_buffer = owned_buffer->Release();\n         buffer = result_buffer;\n@@ -442,7 +442,7 @@ absl::StatusOr<ExecutionOutput> CpuExecutable::ExecuteAsyncOnStream(\n   }\n \n   se::Stream* stream = run_options->stream();\n-  se::DeviceMemoryAllocator* memory_allocator = run_options->allocator();\n+  se::DeviceAddressAllocator* memory_allocator = run_options->allocator();\n   TF_ASSIGN_OR_RETURN(\n       std::vector<MaybeOwningDeviceMemory> buffers,\n       CreateBufferTable(memory_allocator, stream->parent()->device_ordinal(),"
        },
        {
            "sha": "ee590e472dbf8362574fcc1db29e287f61fc8ce4",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -45,7 +45,7 @@ limitations under the License.\n #include \"xla/service/hlo_value.h\"\n #include \"xla/service/maybe_owning_device_memory.h\"\n #include \"xla/service/service_executable_run_options.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n \n namespace xla {\n namespace cpu {\n@@ -175,7 +175,7 @@ class CpuExecutable : public Executable {\n   //  - buffers_to_free: buffers whose ownership was donated by the caller that\n   //    are to be freed by the caller.\n   absl::StatusOr<std::vector<MaybeOwningDeviceMemory>> CreateBufferTable(\n-      se::DeviceMemoryAllocator* memory_allocator, int device_ordinal,\n+      se::DeviceAddressAllocator* memory_allocator, int device_ordinal,\n       absl::Span<ExecutionInput const> arguments);\n \n   // Creates an Execution output holding ScopedShapedBuffer for holding the"
        },
        {
            "sha": "693d89f3452a6a1fc790a59b448abe5d211dbee6",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -56,7 +56,7 @@ limitations under the License.\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/cpu/cpu_executable_run_options.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\""
        },
        {
            "sha": "4b6cf6b733278bc3e67cc95902ef3300adf08dcf",
            "filename": "third_party/xla/xla/service/cpu/cpu_transfer_manager.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_transfer_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_transfer_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_transfer_manager.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"xla/literal.h\"\n #include \"xla/service/generic_transfer_manager.h\"\n #include \"xla/service/transfer_manager.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -51,7 +51,7 @@ class CpuTransferManager : public GenericTransferManager {\n \n   bool CanBufferBeAccessedNow(\n       se::StreamExecutor* executor,\n-      const se::DeviceMemoryBase& device_buffer) const override {\n+      const se::DeviceAddressBase& device_buffer) const override {\n     return true;\n   }\n "
        },
        {
            "sha": "a1de25052a814ebde93bfb1bbd6a2d5e14c669ff",
            "filename": "third_party/xla/xla/service/cpu/cpu_xfeed.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_xfeed.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_xfeed.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_xfeed.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -281,7 +281,7 @@ absl::Status ReadDynamicShapesOnCpu(\n   Shape original_device_shape = *device_shape;\n   TF_RETURN_IF_ERROR(device_buffer->buffers().ForEachElementWithStatus(\n       [&](const ShapeIndex& index,\n-          const se::DeviceMemoryBase& buffer) -> absl::Status {\n+          const se::DeviceAddressBase& buffer) -> absl::Status {\n         const Shape& buffer_shape =\n             ShapeUtil::GetSubshape(*device_shape, index);\n         if (buffer_shape.IsTuple()) {"
        },
        {
            "sha": "3f9932fc78bc43745ea6d6e691e5442ea823b0b1",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -106,7 +106,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -1547,7 +1547,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitYnnFusionThunk(\n   }\n \n   absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers)>\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers)>\n       builder;\n   absl::Span<const int64_t> captured_arguments_ids;\n   if (instruction->opcode() == HloOpcode::kDot) {"
        },
        {
            "sha": "b52166c243dea164a8cca932c87bc6f9877dd464",
            "filename": "third_party/xla/xla/service/executable.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -33,8 +33,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -90,7 +90,7 @@ static ExecutionInput MakeMaybeOwningDeviceMemoryTree(\n     const ShapedBuffer& shaped_buffer) {\n   ExecutionInput result(shaped_buffer.on_device_shape());\n   shaped_buffer.buffers().ForEachElement(\n-      [&](const ShapeIndex& index, const se::DeviceMemoryBase& mem) {\n+      [&](const ShapeIndex& index, const se::DeviceAddressBase& mem) {\n         result.SetBuffer(index, MaybeOwningDeviceMemory(mem));\n       });\n   return result;\n@@ -253,7 +253,7 @@ void Executable::MarkToBeReleasedArguments(absl::Span<ExecutionInput> arguments,\n                                            ExecutionOutput& result) {\n   for (ExecutionInput& argument : arguments) {\n     for (auto& index_buffer : *argument.MutableBuffers()) {\n-      if (std::optional<se::OwningDeviceMemory> maybe_owning_buffer =\n+      if (std::optional<se::ScopedDeviceAddress<uint8_t>> maybe_owning_buffer =\n               index_buffer.second.Release()) {\n         result.AddToBeReleased(std::move(*maybe_owning_buffer));\n       }"
        },
        {
            "sha": "db444230abe34235b2e5df5afe17d0366ee15d39",
            "filename": "third_party/xla/xla/service/executable.h",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -45,7 +45,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -62,10 +62,12 @@ namespace xla {\n // 2) Donated by the caller but returned on error.\n // 3) Donated by the caller and freed on error.\n //\n-// Case (1) buffers are stored as MaybeOwningDeviceMemory(DeviceMemoryBase).\n-// Case (2) buffers are stored as MaybeOwningDeviceMemory(OwningDeviceMemory),\n+// Case (1) buffers are stored as\n+// MaybeOwningDeviceMemory(DeviceAddressBase). Case (2) buffers are\n+// stored as MaybeOwningDeviceMemory(ScopedDeviceAddress<uint8_t>),\n //   with their indices present in unowned_indices_.\n-// Case (3) buffers are stored as MaybeOwningDeviceMemory(OwningDeviceMemory),\n+// Case (3) buffers are stored as\n+// MaybeOwningDeviceMemory(ScopedDeviceAddress<uint8_t>),\n //   with their indices absent from unowned_indices_.\n class ExecutionInput {\n  public:\n@@ -174,16 +176,16 @@ class ExecutionOutput {\n   explicit ExecutionOutput(ScopedShapedBuffer result)\n       : result_(std::move(result)) {}\n   ExecutionOutput(ScopedShapedBuffer result,\n-                  std::vector<se::OwningDeviceMemory> to_be_released)\n+                  std::vector<se::ScopedDeviceAddress<uint8_t>> to_be_released)\n       : result_(std::move(result)),\n         to_be_released_(std::move(to_be_released)) {}\n   // TODO(b/170310047): remove this overload.\n   ExecutionOutput(Shape on_host_shape, Shape on_device_shape,\n-                  se::DeviceMemoryAllocator* allocator, int device_ordinal,\n+                  se::DeviceAddressAllocator* allocator, int device_ordinal,\n                   int physical_device_ordinal = -1)\n       : result_(std::move(on_device_shape), allocator, device_ordinal,\n                 physical_device_ordinal) {}\n-  ExecutionOutput(Shape on_device_shape, se::DeviceMemoryAllocator* allocator,\n+  ExecutionOutput(Shape on_device_shape, se::DeviceAddressAllocator* allocator,\n                   int device_ordinal, int physical_device_ordinal = -1)\n       : result_(std::move(on_device_shape), allocator, device_ordinal,\n                 physical_device_ordinal) {}\n@@ -195,15 +197,15 @@ class ExecutionOutput {\n     // indices, clear them off the ScopedShapedBuffer to prevent them to be\n     // released.\n     for (auto& index : aliased_indices_) {\n-      result_.set_buffer(se::OwningDeviceMemory(), index);\n+      result_.set_buffer(se::ScopedDeviceAddress<uint8_t>(), index);\n     }\n   }\n \n   void AddAliasedIndex(ShapeIndex index) {\n     aliased_indices_.push_back(std::move(index));\n   }\n \n-  void AddToBeReleased(se::OwningDeviceMemory mem) {\n+  void AddToBeReleased(se::ScopedDeviceAddress<uint8_t> mem) {\n     to_be_released_.push_back(std::move(mem));\n   }\n \n@@ -223,11 +225,11 @@ class ExecutionOutput {\n     return std::move(result_);\n   }\n \n-  const std::vector<se::OwningDeviceMemory>& ToBeReleased() const {\n+  const std::vector<se::ScopedDeviceAddress<uint8_t>>& ToBeReleased() const {\n     return to_be_released_;\n   }\n \n-  std::vector<se::OwningDeviceMemory> ConsumeToBeReleased() {\n+  std::vector<se::ScopedDeviceAddress<uint8_t>> ConsumeToBeReleased() {\n     return std::move(to_be_released_);\n   }\n \n@@ -242,7 +244,7 @@ class ExecutionOutput {\n \n   // Leftover buffers for the caller to release. Elements in this list are\n   // donated input memory buffers that are not reused by XLA as outputs.\n-  std::vector<se::OwningDeviceMemory> to_be_released_;\n+  std::vector<se::ScopedDeviceAddress<uint8_t>> to_be_released_;\n \n   // These are the indices in result_ which have been aliased from the caller.\n   // If the execution operation fails, the caller should maintain ownership of\n@@ -252,7 +254,7 @@ class ExecutionOutput {\n \n   // A shape table is a continuous region in memory that is used to hold the\n   // runtime dimension sizes of dynamic output shapes.\n-  se::OwningDeviceMemory output_shape_table_;\n+  se::ScopedDeviceAddress<uint8_t> output_shape_table_;\n };\n \n // A given platform's compiler will produce an Executable -- this is a uniform\n@@ -373,7 +375,7 @@ class Executable {\n   }\n \n   // The shape (including layout) that results from this execution. This is the\n-  // shape of the DeviceMemoryBase result value in ExecuteOnStream above.\n+  // shape of the DeviceAddressBase result value in ExecuteOnStream above.\n   virtual Shape result_shape() const {\n     CHECK(hlo_module_ != nullptr);\n     return hlo_module_->config().entry_computation_layout().result_shape();"
        },
        {
            "sha": "59c0ce8c4da4abf8addf05a7a2822cbfce7823f1",
            "filename": "third_party/xla/xla/service/generic_transfer_manager.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -34,7 +34,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -54,13 +54,13 @@ se::Platform::Id GenericTransferManager::PlatformId() const {\n }\n \n absl::Status GenericTransferManager::WriteSingleTupleIndexTable(\n-    se::Stream* stream, absl::Span<const se::DeviceMemoryBase> elements,\n-    const Shape& shape, se::DeviceMemoryBase* region) {\n+    se::Stream* stream, absl::Span<const se::DeviceAddressBase> elements,\n+    const Shape& shape, se::DeviceAddressBase* region) {\n   TF_RET_CHECK(elements.size() == ShapeUtil::TupleElementCount(shape));\n \n   auto element_pointers = std::make_shared<std::vector<const void*>>();\n   element_pointers->reserve(elements.size());\n-  for (const se::DeviceMemoryBase& element : elements) {\n+  for (const se::DeviceAddressBase& element : elements) {\n     element_pointers->push_back(element.opaque());\n   }\n   TF_RETURN_IF_ERROR(TransferBufferToDevice(\n@@ -165,7 +165,7 @@ absl::Status GenericTransferManager::TransferLiteralToDeviceAsync(\n           const ShapeIndex& index) -> absl::Status {\n         if (device_subshape.IsArray()) {\n           int64_t size = GetByteSizeRequirement(device_subshape);\n-          se::DeviceMemoryBase device_memory = device_buffer.buffer(index);\n+          se::DeviceAddressBase device_memory = device_buffer.buffer(index);\n           TF_RET_CHECK(size == device_memory.size());\n \n           auto TransferBuffer = [&](const void* source) {\n@@ -224,7 +224,7 @@ absl::Status GenericTransferManager::ResetDevices(\n }\n \n absl::Status GenericTransferManager::TransferBufferFromDevice(\n-    se::Stream* stream, const se::DeviceMemoryBase& source, int64_t size,\n+    se::Stream* stream, const se::DeviceAddressBase& source, int64_t size,\n     void* destination) {\n   if (source.size() < size) {\n     return absl::FailedPreconditionError(absl::StrFormat(\n@@ -237,7 +237,7 @@ absl::Status GenericTransferManager::TransferBufferFromDevice(\n \n absl::Status GenericTransferManager::TransferBufferToDevice(\n     se::Stream* stream, int64_t size, const void* source,\n-    se::DeviceMemoryBase* destination) {\n+    se::DeviceAddressBase* destination) {\n   if (destination->size() < size) {\n     return absl::FailedPreconditionError(absl::StrFormat(\n         \"Destination allocation on device not large enough for data transfer: \"\n@@ -248,7 +248,7 @@ absl::Status GenericTransferManager::TransferBufferToDevice(\n }\n \n absl::Status GenericTransferManager::TransferIntNArrayFromDevice(\n-    se::Stream* stream, const se::DeviceMemoryBase& source,\n+    se::Stream* stream, const se::DeviceAddressBase& source,\n     PrimitiveType element_type, int64_t num_elements, void* destination) {\n   int bit_width = primitive_util::BitWidth(element_type);\n   int64_t elements_per_byte = 8 / bit_width;\n@@ -268,7 +268,7 @@ absl::Status GenericTransferManager::TransferIntNArrayFromDevice(\n \n absl::Status GenericTransferManager::TransferIntNArrayToDevice(\n     se::Stream* stream, PrimitiveType element_type, int64_t num_elements,\n-    const void* source, se::DeviceMemoryBase* destination) {\n+    const void* source, se::DeviceAddressBase* destination) {\n   int bit_width = primitive_util::BitWidth(element_type);\n   int64_t elements_per_byte = 8 / bit_width;\n   auto packed_src_data = std::make_unique<std::vector<char>>("
        },
        {
            "sha": "db88562ee680e67d6205aa06aa3292de076b8a97",
            "filename": "third_party/xla/xla/service/generic_transfer_manager.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/service/transfer_manager.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -78,8 +78,8 @@ class GenericTransferManager : public TransferManager {\n   int64_t GetByteSizeRequirement(const Shape& shape) const override;\n \n   absl::Status WriteSingleTupleIndexTable(\n-      se::Stream* stream, absl::Span<const se::DeviceMemoryBase> elements,\n-      const Shape& shape, se::DeviceMemoryBase* region) override;\n+      se::Stream* stream, absl::Span<const se::DeviceAddressBase> elements,\n+      const Shape& shape, se::DeviceAddressBase* region) override;\n \n   Shape HostShapeToDeviceShape(const Shape& host_shape) const override;\n \n@@ -92,7 +92,7 @@ class GenericTransferManager : public TransferManager {\n   //\n   // size is the size to transfer to destination in bytes.\n   virtual absl::Status TransferBufferFromDevice(\n-      se::Stream* stream, const se::DeviceMemoryBase& source, int64_t size,\n+      se::Stream* stream, const se::DeviceAddressBase& source, int64_t size,\n       void* destination);\n \n   // Transfer a memory block of the given size from 'source' buffer to the given\n@@ -101,15 +101,15 @@ class GenericTransferManager : public TransferManager {\n   // size is the size to transfer from source in bytes.\n   virtual absl::Status TransferBufferToDevice(\n       se::Stream* stream, int64_t size, const void* source,\n-      se::DeviceMemoryBase* destination);\n+      se::DeviceAddressBase* destination);\n \n   // Transfers a buffer of packed int4 values from the device to the host, then\n   // unpacks them on the host. 'source' is a buffer with (num_elements+1)/2\n   // bytes where each byte stores two int4 values. 'destination' is a buffer\n   // with num_elements bytes, where a single int4 value will be written to each\n   // byte in the lower 4 bits.\n   virtual absl::Status TransferIntNArrayFromDevice(\n-      se::Stream* stream, const se::DeviceMemoryBase& source,\n+      se::Stream* stream, const se::DeviceAddressBase& source,\n       PrimitiveType element_type, int64_t num_elements, void* destination);\n \n   // Packs an array of int4 values then transfers the packed buffer from the\n@@ -119,7 +119,7 @@ class GenericTransferManager : public TransferManager {\n   // each byte.\n   virtual absl::Status TransferIntNArrayToDevice(\n       se::Stream* stream, PrimitiveType element_type, int64_t num_elements,\n-      const void* source, se::DeviceMemoryBase* destination);\n+      const void* source, se::DeviceAddressBase* destination);\n \n   // The platform this transfer manager targets.\n   const se::Platform::Id platform_id_;"
        },
        {
            "sha": "4347dded08428f03b5fd7620983a93b5f432a9fb",
            "filename": "third_party/xla/xla/service/generic_transfer_manager_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgeneric_transfer_manager_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/host/host_platform_id.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -78,7 +78,7 @@ class GenericTransferManagerTest : public ::testing::Test {\n   PackingTransferManager transfer_manager_;\n   se::StreamExecutor* stream_executor_;\n   std::unique_ptr<se::Stream> stream_;\n-  std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n+  std::unique_ptr<se::DeviceAddressAllocator> allocator_;\n };\n \n TEST_F(GenericTransferManagerTest, TransferLiteralToDevice) {\n@@ -87,7 +87,7 @@ TEST_F(GenericTransferManagerTest, TransferLiteralToDevice) {\n   TF_ASSERT_OK(transfer_manager_.TransferLiteralToDevice(stream_.get(), literal,\n                                                          buffer));\n \n-  se::DeviceMemoryBase device_mem = buffer.buffers().element({});\n+  se::DeviceAddressBase device_mem = buffer.buffers().element({});\n   uint16_t* device_ptr = static_cast<uint16_t*>(device_mem.opaque());\n   std::vector<uint16_t> expected = {1, 2, 3, 4};\n   EXPECT_EQ(absl::Span<uint16_t>(device_ptr, expected.size()), expected);\n@@ -120,7 +120,7 @@ TEST_F(GenericTransferManagerTest, TransferLiteralToDeviceInt4) {\n         AllocateBuffer(ShapeUtil::MakeShape(S4, {2, 2}));\n     TF_ASSERT_OK(transfer_manager_.TransferLiteralToDevice(stream_.get(),\n                                                            literal, buffer));\n-    se::DeviceMemoryBase device_mem = buffer.buffers().element({});\n+    se::DeviceAddressBase device_mem = buffer.buffers().element({});\n     ASSERT_EQ(device_mem.size(), pack ? 2 : 4);\n     int8_t* device_ptr = static_cast<int8_t*>(device_mem.opaque());\n     std::vector<int8_t> expected =\n@@ -136,7 +136,7 @@ TEST_F(GenericTransferManagerTest, TransferLiteralToDeviceInt4) {\n TEST_F(GenericTransferManagerTest, TransferLiteralFromDevice) {\n   ScopedShapedBuffer buffer = AllocateBuffer(ShapeUtil::MakeShape(U16, {2, 2}));\n \n-  se::DeviceMemoryBase device_mem = buffer.buffers().element({});\n+  se::DeviceAddressBase device_mem = buffer.buffers().element({});\n   uint16_t* device_ptr = static_cast<uint16_t*>(device_mem.opaque());\n   for (int i = 0; i < 4; i++) {\n     device_ptr[i] = i + 1;\n@@ -157,7 +157,7 @@ TEST_F(GenericTransferManagerTest, TransferLiteralFromDeviceInt4) {\n     ScopedShapedBuffer buffer =\n         AllocateBuffer(ShapeUtil::MakeShape(S4, {2, 2}));\n \n-    se::DeviceMemoryBase device_mem = buffer.buffers().element({});\n+    se::DeviceAddressBase device_mem = buffer.buffers().element({});\n     uint8_t* device_ptr = static_cast<uint8_t*>(device_mem.opaque());\n     if (pack) {\n       ASSERT_EQ(device_mem.size(), 2);"
        },
        {
            "sha": "090c9fde5a9b41e37158df13aac60f41fee06614",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -215,8 +215,8 @@ xla_test(\n         \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:executable\",\n         \"//xla/service:hlo_module_config\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:scratch_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:gpu_types_header\",\n@@ -654,8 +654,8 @@ cc_library(\n     hdrs = [\"buffer_allocations.h\"],\n     deps = [\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n@@ -717,9 +717,9 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:stream_pool\",\n         \"//xla/service:xla_debug_info_manager\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:event_based_timer\",\n         \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:module_spec\",\n@@ -1115,8 +1115,8 @@ cc_library(\n         \"//xla/service:matmul_indexing_utils\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/stream_executor:blas\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -1164,7 +1164,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:lazy_op_runner\",\n         \"//xla/stream_executor:stream\",\n@@ -1216,7 +1216,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:lazy_op_runner\",\n         \"//xla/stream_executor:stream\",\n@@ -1390,7 +1390,7 @@ cc_library(\n         \"//xla/service:generic_transfer_manager\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:transfer_manager\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:platform\",\n@@ -1829,9 +1829,9 @@ cc_library(\n         \"//xla/service/spmd:collective_permute_motion\",\n         \"//xla/service/spmd:schedule_aware_collective_ops_cse\",\n         \"//xla/service/spmd/shardy:shardy_xla_pass\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:platform\",\n@@ -2468,7 +2468,7 @@ cc_library(\n         \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -2673,7 +2673,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/stream_executor:data_type\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\","
        },
        {
            "sha": "f619be71cd9d86115407df51164002bf3a0f1413",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -190,9 +190,9 @@ cc_library(\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/service/gpu/transforms:priority_fusion\",\n         \"//xla/service/gpu/transforms:scaled_dot_rewriter\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -369,7 +369,7 @@ xla_test(\n         \"//xla/service/gpu:cublas_cudnn\",\n         \"//xla/service/gpu:gpu_compiler\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n@@ -451,8 +451,8 @@ cc_library(\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:dump\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tsl/platform:env\",\n@@ -500,8 +500,8 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n         \"//xla/service/gpu:ir_emission_utils\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:redzone_allocator\",\n         \"//xla/tsl/platform:errors\",\n@@ -529,8 +529,8 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:redzone_allocator\",\n         \"//xla/tsl/platform:errors\",\n@@ -591,9 +591,9 @@ cc_library(\n         \"//xla/service/gpu:gpu_conv_runner\",\n         \"//xla/service/gpu:hlo_algorithm_denylist\",\n         \"//xla/service/gpu:stream_executor_util\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:lazy_op_runner\",\n@@ -741,8 +741,8 @@ cc_library(\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:compiler\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "14ab52352ad0476671f69e0bc2bc48b3caceb64a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_compile_util.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -40,7 +40,7 @@ limitations under the License.\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -54,7 +54,7 @@ namespace gpu {\n namespace {\n \n std::vector<ExecutionInput> ExecutionInputsFromBuffers(\n-    absl::Span<se::DeviceMemoryBase const> buffers,\n+    absl::Span<se::DeviceAddressBase const> buffers,\n     absl::Span<Shape const> shapes) {\n   CHECK_EQ(buffers.size(), shapes.size());\n   std::vector<ExecutionInput> inputs;\n@@ -70,11 +70,10 @@ std::vector<ExecutionInput> ExecutionInputsFromBuffers(\n \n }  // namespace\n \n-AutotunerCompileUtil::AutotunerCompileUtil(std::unique_ptr<Compiler> compiler,\n-                                           se::StreamExecutor& stream_executor,\n-                                           se::Stream& stream,\n-                                           se::DeviceMemoryAllocator& allocator,\n-                                           const DebugOptions& opts)\n+AutotunerCompileUtil::AutotunerCompileUtil(\n+    std::unique_ptr<Compiler> compiler, se::StreamExecutor& stream_executor,\n+    se::Stream& stream, se::DeviceAddressAllocator& allocator,\n+    const DebugOptions& opts)\n     : compiler_(std::move(compiler)),\n       stream_executor_(stream_executor),\n       stream_(stream),\n@@ -88,7 +87,7 @@ AutotunerCompileUtil::AutotunerCompileUtil(std::unique_ptr<Compiler> compiler,\n absl::StatusOr<AutotunerCompileUtil::ProfilingOutput>\n AutotunerCompileUtil::ProfileExecutable(\n     Executable* executable, se::Stream* stream,\n-    absl::Span<se::DeviceMemoryBase const> input_buffers,\n+    absl::Span<se::DeviceAddressBase const> input_buffers,\n     absl::Span<Shape const> input_shapes) {\n   tsl::profiler::TraceMe traceme(\"ProfileExecutable\");\n   {\n@@ -155,7 +154,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> AutotunerCompileUtil::ExtractModule(\n         \"Deviceless autotuning is not supported.\");\n   }\n   se::StreamExecutor* stream_exec = config.GetExecutor();\n-  se::DeviceMemoryAllocator* allocator = config.GetAllocator();\n+  se::DeviceAddressAllocator* allocator = config.GetAllocator();\n   TF_ASSIGN_OR_RETURN(se::Stream* const stream, config.GetStream());\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<Compiler> compiler,\n                       Compiler::GetForPlatform(stream_exec->GetPlatform()));"
        },
        {
            "sha": "c14c1984ef02ec6adef056ce1d50cc6efa5812b7",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_compile_util.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -35,8 +35,8 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/util.h\"\n@@ -78,7 +78,7 @@ class AutotunerCompileUtil {\n   // `(cache_key, config)`.\n   absl::StatusOr<ProfilingOutput> ProfileExecutable(\n       Executable* executable, se::Stream* stream,\n-      absl::Span<se::DeviceMemoryBase const> input_buffers,\n+      absl::Span<se::DeviceAddressBase const> input_buffers,\n       absl::Span<Shape const> input_shapes);\n \n   // Generic method to compile a generated module from `extractor` in isolation.\n@@ -100,7 +100,7 @@ class AutotunerCompileUtil {\n  private:\n   AutotunerCompileUtil(std::unique_ptr<Compiler> compiler,\n                        se::StreamExecutor& stream_executor, se::Stream& stream,\n-                       se::DeviceMemoryAllocator& allocator,\n+                       se::DeviceAddressAllocator& allocator,\n                        const DebugOptions& opts);\n \n   absl::StatusOr<ExecutionOutput> Execute(Executable& executable,\n@@ -110,7 +110,7 @@ class AutotunerCompileUtil {\n   std::unique_ptr<Compiler> compiler_;\n   se::StreamExecutor& stream_executor_;\n   se::Stream& stream_;\n-  se::DeviceMemoryAllocator& allocator_;\n+  se::DeviceAddressAllocator& allocator_;\n   DebugOptions opts_;\n };\n "
        },
        {
            "sha": "0835c4fae539fe32fcda45af0fadd7efb1624b52",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -36,8 +36,8 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -94,7 +94,7 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     stream_executor::StreamExecutor* stream_executor,\n     tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n     const Compiler::GpuTargetConfig* target_config,\n-    se::DeviceMemoryAllocator* allocator, bool optimize_scratch_bytes,\n+    se::DeviceAddressAllocator* allocator, bool optimize_scratch_bytes,\n     MultiProcessKeyValueStore key_value_store) {\n   std::unique_ptr<Profiler> profiler = nullptr;\n   bool is_deviceless = stream_executor == nullptr;"
        },
        {
            "sha": "a2a6299a48d117131294a98aa7462fc7f00d13ba",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/compiler.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/xla.pb.h\"\n@@ -46,7 +46,7 @@ class AutotunerPass : public HloModulePass {\n       const DebugOptions& debug_options, se::StreamExecutor* stream_executor,\n       tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n       const Compiler::GpuTargetConfig* target_config,\n-      se::DeviceMemoryAllocator* allocator = nullptr,\n+      se::DeviceAddressAllocator* allocator = nullptr,\n       bool optimize_scratch_bytes = true,\n       MultiProcessKeyValueStore key_value_store = MultiProcessKeyValueStore());\n "
        },
        {
            "sha": "2c2926ca29dcb315d64ad2256d3b2ad1bd9ae489",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_compiler.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n@@ -73,7 +73,7 @@ class AutotunerPassTest : public HloHardwareIndependentTestBase {\n             stream_executor_)) {}\n \n   se::StreamExecutor* stream_executor_;\n-  std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n+  std::unique_ptr<se::DeviceAddressAllocator> allocator_;\n   NVPTXCompiler compiler_;\n };\n "
        },
        {
            "sha": "3a62f1ecf8709d28c5a9412c3e9689577004490a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -31,8 +31,8 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/xla.pb.h\"\n@@ -46,7 +46,7 @@ struct DeviceConfig {\n   // If the `allocator` parameter is not null, we will use it to allocate temp\n   // memory while timing the various convolution algorithms.  If it's null,\n   // we'll use the default allocator on the StreamExecutor.\n-  se::DeviceMemoryAllocator* allocator = nullptr;  // may be null\n+  se::DeviceAddressAllocator* allocator = nullptr;  // may be null\n };\n \n struct DevicelessConfig {\n@@ -67,7 +67,7 @@ class DeviceOrDevicelessConfig {\n     return std::get<DeviceConfig>(config_).stream_exec;\n   }\n \n-  se::DeviceMemoryAllocator* GetAllocator() const {\n+  se::DeviceAddressAllocator* GetAllocator() const {\n     CHECK(std::holds_alternative<DeviceConfig>(config_));\n     auto& cf = std::get<DeviceConfig>(config_);\n     if (cf.allocator != nullptr) {\n@@ -102,7 +102,7 @@ class DeviceOrDevicelessConfig {\n \n  private:\n   std::variant<DeviceConfig, DevicelessConfig> config_;\n-  mutable std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n+  mutable std::unique_ptr<se::DeviceAddressAllocator> allocator_;\n };\n \n class AutotuneConfig {\n@@ -158,7 +158,7 @@ class AutotuneConfig {\n \n   se::StreamExecutor* GetExecutor() const { return config_.GetExecutor(); }\n \n-  se::DeviceMemoryAllocator* GetAllocator() const {\n+  se::DeviceAddressAllocator* GetAllocator() const {\n     return config_.GetAllocator();\n   }\n "
        },
        {
            "sha": "bce97c0ed93c32f5fc3d8bfb99839ebe2448fb6d",
            "filename": "third_party/xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -59,9 +59,9 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n@@ -86,7 +86,7 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-using se::DeviceMemoryBase;\n+using se::DeviceAddressBase;\n using se::dnn::AlgorithmDesc;\n using std::optional;\n \n@@ -102,7 +102,7 @@ Shape MaybeTupleElementShape(Shape shape, int64_t tuple_idx) {\n class ScratchAllocator : public se::ScratchAllocator {\n  public:\n   ScratchAllocator(int device_ordinal,\n-                   se::DeviceMemoryAllocator* memory_allocator)\n+                   se::DeviceAddressAllocator* memory_allocator)\n       : device_ordinal_(device_ordinal), memory_allocator_(memory_allocator) {}\n \n   int64_t GetMemoryLimitInBytes() override {\n@@ -117,24 +117,24 @@ class ScratchAllocator : public se::ScratchAllocator {\n     return value * (1LL << 20);\n   }\n \n-  absl::StatusOr<se::DeviceMemory<uint8_t>> AllocateBytes(\n+  absl::StatusOr<se::DeviceAddress<uint8_t>> AllocateBytes(\n       int64_t byte_size) override;\n \n   template <typename T>\n-  absl::StatusOr<se::DeviceMemory<T>> Allocate(int64_t num_elements) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemory<uint8_t> bytes,\n+  absl::StatusOr<se::DeviceAddress<T>> Allocate(int64_t num_elements) {\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddress<uint8_t> bytes,\n                         AllocateBytes(num_elements * sizeof(T)));\n-    return se::DeviceMemory<T>(bytes);\n+    return se::DeviceAddress<T>(bytes);\n   }\n \n  private:\n   const int device_ordinal_;\n-  se::DeviceMemoryAllocator* memory_allocator_;\n-  std::vector<se::OwningDeviceMemory> allocated_buffers_;\n+  se::DeviceAddressAllocator* memory_allocator_;\n+  std::vector<se::ScopedDeviceAddress<uint8_t>> allocated_buffers_;\n   int64_t total_allocated_bytes_ = 0;\n };\n \n-absl::StatusOr<se::DeviceMemory<uint8_t>> ScratchAllocator::AllocateBytes(\n+absl::StatusOr<se::DeviceAddress<uint8_t>> ScratchAllocator::AllocateBytes(\n     int64_t byte_size) {\n   CHECK_GE(byte_size, 0) << \"byte_size must be positive.\";\n   if (byte_size > GetMemoryLimitInBytes()) {\n@@ -143,14 +143,14 @@ absl::StatusOr<se::DeviceMemory<uint8_t>> ScratchAllocator::AllocateBytes(\n         GetMemoryLimitInBytes()));\n   }\n \n-  TF_ASSIGN_OR_RETURN(se::OwningDeviceMemory allocated_buffer,\n+  TF_ASSIGN_OR_RETURN(se::ScopedDeviceAddress<uint8_t> allocated_buffer,\n                       memory_allocator_->Allocate(device_ordinal_, byte_size,\n                                                   /*retry_on_failure=*/false));\n   total_allocated_bytes_ += byte_size;\n \n-  se::DeviceMemoryBase buffer_addr = *allocated_buffer;\n+  se::DeviceAddressBase buffer_addr = *allocated_buffer;\n   allocated_buffers_.push_back(std::move(allocated_buffer));\n-  return se::DeviceMemory<uint8_t>(buffer_addr);\n+  return se::DeviceAddress<uint8_t>(buffer_addr);\n }\n \n absl::StatusOr<std::vector<GenericConvRunner>> GetAlgorithms(\n@@ -202,7 +202,7 @@ absl::StatusOr<std::vector<GenericConvRunner>> GetAlgorithms(\n \n     case se::dnn::ConvolutionKind::FORWARD_GRAPH: {\n       std::vector<std::unique_ptr<const se::dnn::GraphConvRunner>> runners;\n-      // This path is cuDNN-only, where the DeviceMemoryBase arguments and the\n+      // This path is cuDNN-only, where the DeviceAddressBase arguments and the\n       // allocator are unused; so, they're all provided as nullptr.\n       TF_RETURN_IF_ERROR(dnn->GetGraphConvolveRunners(\n           kind, input_type, output_type, stream, config.input_descriptor,\n@@ -222,15 +222,15 @@ absl::StatusOr<std::vector<GenericConvRunner>> GetAlgorithms(\n     case se::dnn::ConvolutionKind::BACKWARD_DATA:\n     case se::dnn::ConvolutionKind::BACKWARD_FILTER: {\n       std::vector<std::unique_ptr<const se::dnn::ConvRunner>> runners;\n-      // This path is cuDNN-only, where the DeviceMemoryBase arguments and the\n+      // This path is cuDNN-only, where the DeviceAddressBase arguments and the\n       // allocator are unused; so, they're all provided as nullptr.\n       TF_RETURN_IF_ERROR(dnn->GetConvolveRunners(\n           kind, input_type, output_type, stream, config.input_descriptor,\n-          /* input_data = */ DeviceMemoryBase(nullptr),\n+          /* input_data = */ DeviceAddressBase(nullptr),\n           config.filter_descriptor,\n-          /* filter_data = */ DeviceMemoryBase(nullptr),\n+          /* filter_data = */ DeviceAddressBase(nullptr),\n           config.output_descriptor,\n-          /* output_data = */ DeviceMemoryBase(nullptr), config.conv_desc,\n+          /* output_data = */ DeviceAddressBase(nullptr), config.conv_desc,\n           use_fallback, nullptr, engine_options, &runners));\n \n       for (auto& runner : runners) {\n@@ -249,8 +249,8 @@ absl::StatusOr<std::vector<GenericConvRunner>> GetAlgorithms(\n \n absl::StatusOr<std::vector<std::unique_ptr<const se::dnn::ConvRunner>>>\n GetMIOpenAlgorithms(const HloCustomCallInstruction* instr,\n-                    absl::Span<se::DeviceMemoryBase> operand_buffers,\n-                    absl::Span<se::DeviceMemoryBase> result_buffers,\n+                    absl::Span<se::DeviceAddressBase> operand_buffers,\n+                    absl::Span<se::DeviceAddressBase> result_buffers,\n                     se::StreamExecutor* stream_exec,\n                     ScratchAllocator* scratch_allocator, se::Stream* stream,\n                     const se::EngineOptions& engine_options) {\n@@ -597,7 +597,7 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n                         absl::StrCat(\"Scratch allocation failed: \",\n                                      scratch_or.status().ToString()));\n   }\n-  se::DeviceMemoryBase scratch_memory = scratch_or.value();\n+  se::DeviceAddressBase scratch_memory = scratch_or.value();\n \n   // Use assignment instead of brace-list to make GCC 4.9 happy.\n   RunConvOptions options;\n@@ -607,9 +607,9 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n   float max_time = 0;\n   float min_time = std::numeric_limits<float>::max();\n   absl::Status launch_status;\n-  std::vector<se::DeviceMemoryBase> operand_buffers =\n+  std::vector<se::DeviceAddressBase> operand_buffers =\n       runtime_arguments.rz_buffers.input_buffers();\n-  std::vector<se::DeviceMemoryBase> result_buffers =\n+  std::vector<se::DeviceAddressBase> result_buffers =\n       runtime_arguments.rz_buffers.output_buffers();\n \n   // Dry-run to warmup the plan.\n@@ -671,7 +671,7 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n   if (!ShouldCheckConv(runtime_arguments.hlo_module_config)) {\n     if (!reference_result->has_value()) {\n       (*reference_result) = {\n-          alg, std::vector<DeviceMemoryBase>(result_buffers.size())};\n+          alg, std::vector<DeviceAddressBase>(result_buffers.size())};\n     }\n     return result;\n   }\n@@ -762,7 +762,7 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n     }\n   } else {\n     XLA_SCOPED_LOGGING_TIMER_LEVEL(\"Memcpy Reference Result\", 2);\n-    std::vector<DeviceMemoryBase> reference_result_buffers(\n+    std::vector<DeviceAddressBase> reference_result_buffers(\n         result_buffers.size());\n     for (int i = 0; i < result_buffers.size(); ++i) {\n       TF_ASSIGN_OR_RETURN(\n@@ -872,7 +872,7 @@ GpuConvAlgorithmPicker::PickBestAlgorithmNoCacheCuda(\n         instr_log.add_operand_addresses(reinterpret_cast<uint64_t>(\n             runtime_arguments.rz_buffers.input_buffers()[i].opaque()));\n       }\n-      for (se::DeviceMemoryBase result_buffer :\n+      for (se::DeviceAddressBase result_buffer :\n            runtime_arguments.rz_buffers.output_buffers()) {\n         instr_log.add_result_addresses(\n             reinterpret_cast<uint64_t>(result_buffer.opaque()));\n@@ -921,14 +921,14 @@ GpuConvAlgorithmPicker::PickBestAlgorithmNoCacheRocm(\n \n   se::StreamExecutor* stream_exec = config_.GetExecutor();\n   const auto device_ordinal = stream_exec->device_ordinal();\n-  std::vector<se::DeviceMemoryBase> operand_buffers;\n+  std::vector<se::DeviceAddressBase> operand_buffers;\n \n   // allocator either points to this->allocator_ or, if that's null, to a\n   // se::StreamExecutorMemoryAllocator for stream_exec.\n-  se::DeviceMemoryAllocator* allocator = config_.GetAllocator();\n+  se::DeviceAddressAllocator* allocator = config_.GetAllocator();\n   ScratchAllocator input_output_allocator(device_ordinal, allocator);\n   TF_ASSIGN_OR_RETURN(se::Stream* const stream, config_.GetStream());\n-  const auto initialize_buffer = [stream](DeviceMemoryBase buffer) {\n+  const auto initialize_buffer = [stream](DeviceAddressBase buffer) {\n     // Although we don't have evidence this matters, zero out the buffers\n     // before autotuning.  It's conceivable that using uninitialized memory as\n     // the inputs might affect performance if e.g. the inputs contain\n@@ -947,7 +947,7 @@ GpuConvAlgorithmPicker::PickBestAlgorithmNoCacheRocm(\n     operand_buffers.push_back(buffer);\n   }\n \n-  std::vector<se::DeviceMemoryBase> result_buffers(\n+  std::vector<se::DeviceAddressBase> result_buffers(\n       instr->shape().tuple_shapes().size());\n   if (instr->shape().IsTuple()) {\n     for (int i = 0; i < instr->shape().tuple_shapes().size(); ++i) {\n@@ -1003,7 +1003,7 @@ GpuConvAlgorithmPicker::PickBestAlgorithmNoCacheRocm(\n               << instr->ToString();\n \n       TF_ASSIGN_OR_RETURN(\n-          DeviceMemoryBase scratch_memory,\n+          DeviceAddressBase scratch_memory,\n           scratch_allocator.AllocateBytes(runner->GetWorkspaceSize()));\n \n       TF_ASSIGN_OR_RETURN(auto lazy_runner,"
        },
        {
            "sha": "f1148fc953a5ea600ec442ffaeadf1afb294c318",
            "filename": "third_party/xla/xla/service/gpu/autotuning/conv_algorithm_picker.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/gpu_conv_runner.h\"\n #include \"xla/service/hlo_module_config.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/xla.pb.h\"\n \n@@ -111,7 +111,7 @@ class GpuConvAlgorithmPicker : public HloModulePass {\n   // autotuned algorithms.\n   struct ReferenceResult {\n     stream_executor::dnn::AlgorithmDesc algorithm;\n-    std::vector<stream_executor::DeviceMemoryBase> buffers;\n+    std::vector<stream_executor::DeviceAddressBase> buffers;\n   };\n \n   // Execution environment for autotuning. Runtime autotuning requires runtime"
        },
        {
            "sha": "6dfb5b9a5c7966cae7a95dc03690b722f2483916",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -103,8 +103,8 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler_helpers.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/integrations/tf_allocator_adapter.h\"\n@@ -1701,7 +1701,7 @@ absl::StatusOr<bool> GemmFusionAutotuner::RunViaNewInfra(\n   se::StreamExecutor* stream_exec = config_.GetExecutor();\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<Compiler> compiler,\n                       Compiler::GetForPlatform(stream_exec->GetPlatform()));\n-  se::DeviceMemoryAllocator* device_allocator = config_.GetAllocator();\n+  se::DeviceAddressAllocator* device_allocator = config_.GetAllocator();\n   std::unique_ptr<Compiler::GpuTargetConfig> target_config;\n   target_config = std::make_unique<Compiler::GpuTargetConfig>(stream_exec);\n   backends.push_back(std::make_unique<FissionBackend>("
        },
        {
            "sha": "8c1666480d0739e714f7dd5ce18b80b6fc9fcbbb",
            "filename": "third_party/xla/xla/service/gpu/autotuning/redzone_buffers.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fredzone_buffers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fredzone_buffers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fredzone_buffers.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -46,7 +46,7 @@ namespace gpu {\n namespace se = ::stream_executor;\n \n absl::StatusOr<RedzoneBuffers> RedzoneBuffers::FromInstruction(\n-    const HloInstruction& instruction, se::DeviceMemoryAllocator* allocator,\n+    const HloInstruction& instruction, se::DeviceAddressAllocator* allocator,\n     se::Stream* stream, BuffersToCreate buffers_to_create,\n     bool should_init_buffers, bool should_check_correctness,\n     int redzone_padding_bytes) {\n@@ -61,7 +61,7 @@ absl::StatusOr<RedzoneBuffers> RedzoneBuffers::FromInstruction(\n }\n \n absl::StatusOr<RedzoneBuffers> RedzoneBuffers::FromComputation(\n-    const HloComputation& computation, se::DeviceMemoryAllocator* allocator,\n+    const HloComputation& computation, se::DeviceAddressAllocator* allocator,\n     se::Stream* stream, BuffersToCreate buffers_to_create,\n     bool should_init_buffers, bool should_check_correctness,\n     int redzone_padding_bytes) {\n@@ -73,7 +73,7 @@ absl::StatusOr<RedzoneBuffers> RedzoneBuffers::FromComputation(\n absl::StatusOr<RedzoneBuffers> RedzoneBuffers::FromProgramShape(\n     const ProgramShape& program_shape, BuffersToCreate buffers_to_create,\n     bool should_init_buffers, bool should_check_correctness,\n-    int redzone_padding_bytes, se::DeviceMemoryAllocator* allocator,\n+    int redzone_padding_bytes, se::DeviceAddressAllocator* allocator,\n     se::Stream* stream) {\n   tsl::profiler::TraceMe traceme(\"create redzone buffers\");\n   RedzoneBuffers buffers;\n@@ -101,7 +101,7 @@ absl::Status RedzoneBuffers::CreateInputs(absl::Span<const Shape> input_shapes,\n                                           int64_t& rng_state) {\n   tsl::profiler::TraceMe traceme(\"create inputs\");\n   for (const auto& input_shape : input_shapes) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase buf,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase buf,\n                         redzone_allocator_->CreateBuffer(\n                             input_shape, should_init_buffers, rng_state));\n     input_buffers_.push_back(buf);\n@@ -116,7 +116,7 @@ absl::Status RedzoneBuffers::CreateOutputs(const Shape& output_shape,\n                                            int64_t& rng_state) {\n   tsl::profiler::TraceMe traceme(\"create outputs\");\n   if (!output_shape.IsTuple()) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase buf,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase buf,\n                         redzone_allocator_->CreateBuffer(\n                             output_shape, should_init_buffers, rng_state));\n     output_buffers_.push_back(buf);\n@@ -139,7 +139,7 @@ absl::Status RedzoneBuffers::CreateOutputs(const Shape& output_shape,\n     if (current_shape_it->IsTuple()) {\n       return Unimplemented(\"Nested tuples are unsupported by RedzoneBuffers.\");\n     }\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase buf,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase buf,\n                         redzone_allocator_->CreateBuffer(\n                             *current_shape_it, should_init_buffers, rng_state));\n     output_buffers_.push_back(buf);"
        },
        {
            "sha": "3a132dda73743204ff2f2049e2cd326fe61b9468",
            "filename": "third_party/xla/xla/service/gpu/autotuning/redzone_buffers.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fredzone_buffers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fredzone_buffers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fredzone_buffers.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -26,8 +26,8 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/util.h\"\n@@ -59,30 +59,30 @@ class RedzoneBuffers {\n   };\n \n   static absl::StatusOr<RedzoneBuffers> FromInstruction(\n-      const HloInstruction& instruction, se::DeviceMemoryAllocator* allocator,\n+      const HloInstruction& instruction, se::DeviceAddressAllocator* allocator,\n       se::Stream* stream, BuffersToCreate buffers_to_create,\n       bool should_init_buffers, bool should_check_correctness,\n       int redzone_padding_bytes);\n \n   static absl::StatusOr<RedzoneBuffers> FromComputation(\n-      const HloComputation& computation, se::DeviceMemoryAllocator* allocator,\n+      const HloComputation& computation, se::DeviceAddressAllocator* allocator,\n       se::Stream* stream, BuffersToCreate buffers_to_create,\n       bool should_init_buffers, bool should_check_correctness,\n       int redzone_padding_bytes);\n \n   static absl::StatusOr<RedzoneBuffers> FromProgramShape(\n       const ProgramShape& program_shape, BuffersToCreate buffers_to_create,\n       bool should_init_buffers, bool should_check_correctness,\n-      int redzone_padding_bytes, se::DeviceMemoryAllocator* allocator,\n+      int redzone_padding_bytes, se::DeviceAddressAllocator* allocator,\n       se::Stream* stream);\n \n-  const std::vector<se::DeviceMemoryBase>& input_buffers() const {\n+  const std::vector<se::DeviceAddressBase>& input_buffers() const {\n     return input_buffers_;\n   }\n \n   const std::vector<Shape>& input_shapes() const { return input_shapes_; }\n \n-  const std::vector<se::DeviceMemoryBase>& output_buffers() const {\n+  const std::vector<se::DeviceAddressBase>& output_buffers() const {\n     return output_buffers_;\n   }\n \n@@ -98,9 +98,9 @@ class RedzoneBuffers {\n                              bool should_init_buffers, int64_t& rng_state);\n \n   std::unique_ptr<se::RedzoneAllocator> redzone_allocator_;\n-  std::vector<se::DeviceMemoryBase> input_buffers_;\n+  std::vector<se::DeviceAddressBase> input_buffers_;\n   std::vector<Shape> input_shapes_;\n-  std::vector<se::DeviceMemoryBase> output_buffers_;\n+  std::vector<se::DeviceAddressBase> output_buffers_;\n   Shape output_shape_;\n };\n "
        },
        {
            "sha": "9213a7ce3ca4084f42d93448b9b3eeb0d1c59ec4",
            "filename": "third_party/xla/xla/service/gpu/buffer_allocations.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -21,21 +21,21 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace xla {\n namespace gpu {\n \n absl::Status BufferAllocations::TearDown(\n-    const std::set<se::DeviceMemoryBase>& live_addresses,\n+    const std::set<se::DeviceAddressBase>& live_addresses,\n     absl::Span<const BufferAllocation* const> allocations) {\n   // Deallocate temporary buffers, taking care to try to deallocate all of them\n   // even if one of the deallocations fails.\n   absl::Status status;\n   const int64_t num_buffers = allocations.size();\n   for (BufferAllocation::Index i = 0; i < num_buffers; ++i) {\n     const BufferAllocation& allocation = *allocations[i];\n-    se::DeviceMemoryBase buffer_address = GetDeviceAddress(allocation.index());\n+    se::DeviceAddressBase buffer_address = GetDeviceAddress(allocation.index());\n     // Deallocate buffers marked \"maybe_live_out\" but aren't actually live out,\n     // and temp buffers.\n     if ((allocation.maybe_live_out() &&\n@@ -51,24 +51,24 @@ absl::Status BufferAllocations::TearDown(\n   return status;\n }\n \n-se::DeviceMemoryBase BufferAllocations::GetDeviceAddress(\n+se::DeviceAddressBase BufferAllocations::GetDeviceAddress(\n     BufferAllocation::Index buffer_index) const {\n   CHECK_GE(buffer_index, 0);\n   CHECK_LT(buffer_index, buffers_.size());\n   return buffers_[buffer_index];\n }\n \n-se::DeviceMemoryBase& BufferAllocations::GetMutableDeviceAddress(\n+se::DeviceAddressBase& BufferAllocations::GetMutableDeviceAddress(\n     BufferAllocation::Index buffer_index) {\n   CHECK_GE(buffer_index, 0);\n   CHECK_LT(buffer_index, buffers_.size());\n   return buffers_[buffer_index];\n }\n \n-se::DeviceMemoryBase BufferAllocations::GetDeviceAddress(\n+se::DeviceAddressBase BufferAllocations::GetDeviceAddress(\n     const BufferAllocation::Slice& buffer_slice) const {\n   int64_t index = buffer_slice.index();\n-  se::DeviceMemoryBase base = GetDeviceAddress(index);\n+  se::DeviceAddressBase base = GetDeviceAddress(index);\n \n   int64_t offset = buffer_slice.offset();\n   CHECK_LE(buffer_slice.offset(), base.size())"
        },
        {
            "sha": "112ed6d493e8cdb387799bc5c70b724a76222020",
            "filename": "third_party/xla/xla/service/gpu/buffer_allocations.h",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -25,8 +25,8 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n \n namespace xla {\n namespace gpu {\n@@ -35,9 +35,9 @@ namespace gpu {\n // allocated device buffers.\n class BufferAllocations {\n  public:\n-  BufferAllocations(absl::Span<se::DeviceMemoryBase const> buffers,\n+  BufferAllocations(absl::Span<se::DeviceAddressBase const> buffers,\n                     int device_ordinal,\n-                    se::DeviceMemoryAllocator* memory_allocator)\n+                    se::DeviceAddressAllocator* memory_allocator)\n       : buffers_(buffers.begin(), buffers.end()),\n         device_ordinal_(device_ordinal),\n         memory_allocator_(memory_allocator) {}\n@@ -47,29 +47,29 @@ class BufferAllocations {\n   BufferAllocations(const BufferAllocations&) = delete;\n   BufferAllocations& operator=(const BufferAllocations&) = delete;\n \n-  se::DeviceMemoryAllocator* memory_allocator() const {\n+  se::DeviceAddressAllocator* memory_allocator() const {\n     return memory_allocator_;\n   }\n   int device_ordinal() const { return device_ordinal_; }\n \n   // Returns the device address of buffer `buffer_index`. `buffer_index` must be\n   // a valid index, i.e., in [0, buffer_count). This function returns null if\n   // `buffer_index` is not assigned to a buffer address.\n-  se::DeviceMemoryBase GetDeviceAddress(\n+  se::DeviceAddressBase GetDeviceAddress(\n       BufferAllocation::Index buffer_index) const;\n \n   // Returns a mutable value for the allocation at a given `buffer_index`.\n-  se::DeviceMemoryBase& GetMutableDeviceAddress(\n+  se::DeviceAddressBase& GetMutableDeviceAddress(\n       BufferAllocation::Index buffer_index);\n \n   // Same as above, but also adjusts the returned address for the offset and\n   // size contained in the given slice.\n-  se::DeviceMemoryBase GetDeviceAddress(\n+  se::DeviceAddressBase GetDeviceAddress(\n       const BufferAllocation::Slice& buffer_slice) const;\n \n   // Tears down all buffers allocated by this object that are not in\n   // `live_addresses`.\n-  absl::Status TearDown(const std::set<se::DeviceMemoryBase>& live_addresses,\n+  absl::Status TearDown(const std::set<se::DeviceAddressBase>& live_addresses,\n                         absl::Span<const BufferAllocation* const> allocations);\n \n   std::string ToString() const {\n@@ -88,9 +88,9 @@ class BufferAllocations {\n   // An array of device pointers that stores the address of each buffer\n   // indexed by Index. Each element can point to a temporary buffer, an\n   // input buffer, or nullptr if no buffer is needed for that Index.\n-  std::vector<se::DeviceMemoryBase> buffers_;\n+  std::vector<se::DeviceAddressBase> buffers_;\n   int device_ordinal_;\n-  se::DeviceMemoryAllocator* memory_allocator_;\n+  se::DeviceAddressAllocator* memory_allocator_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "28ad1dd81231a89d8a38e024866f2f3eb3301081",
            "filename": "third_party/xla/xla/service/gpu/custom_call_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -58,7 +58,7 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_types.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -280,8 +280,8 @@ TEST_F(CustomCallTest, PassAttributesByBackendConfig) {\n \n static absl::Status Memcpy(se::Stream* stream, ffi::AnyBuffer src,\n                            ffi::Result<ffi::AnyBuffer> dst) {\n-  se::DeviceMemoryBase dst_mem = dst->device_memory();\n-  se::DeviceMemoryBase src_mem = src.device_memory();\n+  se::DeviceAddressBase dst_mem = dst->device_memory();\n+  se::DeviceAddressBase src_mem = src.device_memory();\n   return stream->MemcpyD2D(&dst_mem, src_mem, src_mem.size());\n }\n "
        },
        {
            "sha": "969f0d032732ab65f4de5c3e0341822df63ab36b",
            "filename": "third_party/xla/xla/service/gpu/gpu_conv_runner.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -48,8 +48,8 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-using se::DeviceMemory;\n-using se::DeviceMemoryBase;\n+using se::DeviceAddress;\n+using se::DeviceAddressBase;\n using se::dnn::BatchDescriptor;\n using se::dnn::ConvolutionDescriptor;\n using se::dnn::DataLayout;\n@@ -60,10 +60,10 @@ using se::dnn::FilterLayout;\n template <typename ElementType, typename OutputType>\n absl::Status RunGpuConvUnfused(const GpuConvParams& params, se::Stream* stream,\n                                RunConvOptions options,\n-                               DeviceMemory<ElementType> input_buf,\n-                               DeviceMemory<ElementType> filter_buf,\n-                               DeviceMemory<OutputType> output_buf,\n-                               DeviceMemoryBase scratch_memory) {\n+                               DeviceAddress<ElementType> input_buf,\n+                               DeviceAddress<ElementType> filter_buf,\n+                               DeviceAddress<OutputType> output_buf,\n+                               DeviceAddressBase scratch_memory) {\n   if (params.config->conv_result_scale != 1) {\n     return Internal(\"StreamExecutor doesn't support scaled convolution: %lf.\",\n                     params.config->conv_result_scale);\n@@ -102,10 +102,10 @@ absl::Status RunGpuConvUnfused(const GpuConvParams& params, se::Stream* stream,\n template <typename ElementType, typename OutputType>\n absl::Status RunGpuConvGraph(const GpuConvParams& params, se::Stream* stream,\n                              RunConvOptions options,\n-                             DeviceMemory<ElementType> input_buf,\n-                             DeviceMemory<ElementType> filter_buf,\n-                             DeviceMemory<OutputType> output_buf,\n-                             DeviceMemoryBase scratch_memory) {\n+                             DeviceAddress<ElementType> input_buf,\n+                             DeviceAddress<ElementType> filter_buf,\n+                             DeviceAddress<OutputType> output_buf,\n+                             DeviceAddressBase scratch_memory) {\n   if (params.config->conv_result_scale != 1) {\n     return Internal(\"StreamExecutor doesn't support scaled convolution: %lf.\",\n                     params.config->conv_result_scale);\n@@ -138,7 +138,7 @@ absl::Status RunGpuConvGraph(const GpuConvParams& params, se::Stream* stream,\n   TF_ASSIGN_OR_RETURN(auto* runner,\n                       lazy_runner->GetOrCreateRunner(config, stream));\n \n-  std::vector<DeviceMemoryBase> operands = {input_buf, filter_buf, output_buf};\n+  std::vector<DeviceAddressBase> operands = {input_buf, filter_buf, output_buf};\n   // Insert the optional operands ahead of the output.\n   operands.insert(operands.end() - 1, params.operand_bufs.begin(),\n                   params.operand_bufs.end());\n@@ -153,9 +153,9 @@ absl::Status RunGpuConvGraph(const GpuConvParams& params, se::Stream* stream,\n template <typename ElementType, typename BiasType, typename OutputType>\n absl::Status RunGpuConvForwardActivation(\n     const GpuConvParams& params, se::Stream* stream, RunConvOptions options,\n-    DeviceMemory<ElementType> input_buf, DeviceMemory<ElementType> filter_buf,\n-    DeviceMemory<OutputType> output_buf, DeviceMemoryBase scratch_memory) {\n-  se::DeviceMemory<OutputType> side_input(params.fusion->side_input_buf);\n+    DeviceAddress<ElementType> input_buf, DeviceAddress<ElementType> filter_buf,\n+    DeviceAddress<OutputType> output_buf, DeviceAddressBase scratch_memory) {\n+  se::DeviceAddress<OutputType> side_input(params.fusion->side_input_buf);\n   // If there is no side input, use output as the side input.\n   if (side_input.is_null()) {\n     if (params.config->fusion->side_input_scale != 0) {\n@@ -221,10 +221,10 @@ template <typename ElementType, typename BiasType, typename OutputType,\n               !std::is_integral<ElementType>::value>::type* = nullptr>\n absl::Status RunGpuConvInternalImpl(const GpuConvParams& params,\n                                     se::Stream* stream, RunConvOptions options,\n-                                    DeviceMemory<ElementType> input_buf,\n-                                    DeviceMemory<ElementType> filter_buf,\n-                                    DeviceMemory<OutputType> output_buf,\n-                                    DeviceMemoryBase scratch_memory) {\n+                                    DeviceAddress<ElementType> input_buf,\n+                                    DeviceAddress<ElementType> filter_buf,\n+                                    DeviceAddress<OutputType> output_buf,\n+                                    DeviceAddressBase scratch_memory) {\n   switch (params.config->kind) {\n     case CudnnConvKind::kForward:\n     case CudnnConvKind::kBackwardInput:\n@@ -249,10 +249,10 @@ template <typename ElementType, typename BiasType, typename OutputType,\n               nullptr>\n absl::Status RunGpuConvInternalImpl(const GpuConvParams& params,\n                                     se::Stream* stream, RunConvOptions options,\n-                                    DeviceMemory<ElementType> input_buf,\n-                                    DeviceMemory<ElementType> filter_buf,\n-                                    DeviceMemory<OutputType> output_buf,\n-                                    DeviceMemoryBase scratch_memory) {\n+                                    DeviceAddress<ElementType> input_buf,\n+                                    DeviceAddress<ElementType> filter_buf,\n+                                    DeviceAddress<OutputType> output_buf,\n+                                    DeviceAddressBase scratch_memory) {\n   switch (params.config->kind) {\n     case CudnnConvKind::kForward:\n       return RunGpuConvUnfused(params, stream, options, input_buf, filter_buf,\n@@ -271,11 +271,11 @@ absl::Status RunGpuConvInternalImpl(const GpuConvParams& params,\n \n template <typename ElementType, typename BiasType, typename OutputType>\n absl::Status RunGpuConvImpl(const GpuConvParams& params, se::Stream* stream,\n-                            se::DeviceMemoryBase scratch_memory,\n+                            se::DeviceAddressBase scratch_memory,\n                             RunConvOptions options) {\n-  auto input_buf = se::DeviceMemory<ElementType>(params.input_buf);\n-  auto filter_buf = se::DeviceMemory<ElementType>(params.filter_buf);\n-  auto output_buf = se::DeviceMemory<OutputType>(params.output_buf);\n+  auto input_buf = se::DeviceAddress<ElementType>(params.input_buf);\n+  auto filter_buf = se::DeviceAddress<ElementType>(params.filter_buf);\n+  auto output_buf = se::DeviceAddress<OutputType>(params.output_buf);\n \n   absl::Status run_status =\n       RunGpuConvInternalImpl<ElementType, BiasType, OutputType>(\n@@ -556,8 +556,8 @@ absl::StatusOr<GpuConvConfig> GetGpuConvConfig(\n \n absl::StatusOr<GpuConvParams> GetGpuConvParams(\n     const GpuConvConfig& config,\n-    absl::Span<const se::DeviceMemoryBase> operand_buffers,\n-    absl::Span<const se::DeviceMemoryBase> result_buffers) {\n+    absl::Span<const se::DeviceAddressBase> operand_buffers,\n+    absl::Span<const se::DeviceAddressBase> result_buffers) {\n   GpuConvParams params;\n   params.config = &config;\n \n@@ -599,10 +599,10 @@ absl::StatusOr<GpuConvParams> GetGpuConvParams(\n }\n \n absl::Status RunGpuConv(const gpu::GpuConvConfig& config,\n-                        absl::Span<const se::DeviceMemoryBase> operand_buffers,\n-                        absl::Span<const se::DeviceMemoryBase> result_buffers,\n-                        se::DeviceMemoryBase scratch_memory, se::Stream* stream,\n-                        RunConvOptions options) {\n+                        absl::Span<const se::DeviceAddressBase> operand_buffers,\n+                        absl::Span<const se::DeviceAddressBase> result_buffers,\n+                        se::DeviceAddressBase scratch_memory,\n+                        se::Stream* stream, RunConvOptions options) {\n   TF_ASSIGN_OR_RETURN(\n       GpuConvParams params,\n       GetGpuConvParams(config, operand_buffers, result_buffers));"
        },
        {
            "sha": "a8c7c7ad6c5a85e87a688beb228a4b0bae9720dc",
            "filename": "third_party/xla/xla/service/gpu/gpu_conv_runner.h",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -35,7 +35,7 @@ limitations under the License.\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/gpu_conv_runner.pb.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -91,21 +91,21 @@ struct GpuConvConfig {\n struct GpuConvParams {\n   const GpuConvConfig* config;  // Not owned\n   struct FusionParams {\n-    se::DeviceMemoryBase bias_buf;\n-    se::DeviceMemoryBase side_input_buf;  // nullable\n+    se::DeviceAddressBase bias_buf;\n+    se::DeviceAddressBase side_input_buf;  // nullable\n   };\n \n-  se::DeviceMemoryBase input_buf;\n-  se::DeviceMemoryBase filter_buf;\n-  se::DeviceMemoryBase output_buf;\n+  se::DeviceAddressBase input_buf;\n+  se::DeviceAddressBase filter_buf;\n+  se::DeviceAddressBase output_buf;\n \n   // Buffers for operands of ops to be fused into the cuDNN\n   // convolution Custom Call.\n-  std::vector<se::DeviceMemoryBase> operand_bufs;\n+  std::vector<se::DeviceAddressBase> operand_bufs;\n \n   // Buffers for additional outputs of ops to be fused into the cuDNN\n   // convolution Custom Call.\n-  std::vector<se::DeviceMemoryBase> aux_bufs;\n+  std::vector<se::DeviceAddressBase> aux_bufs;\n \n   std::optional<FusionParams> fusion;\n };\n@@ -222,10 +222,10 @@ struct RunConvOptions {\n // the same conv, you can provide an explicitly preallocated scratch buffer of\n // that size, if you like.\n absl::Status RunGpuConv(const GpuConvConfig& conv_config,\n-                        absl::Span<const se::DeviceMemoryBase> operand_buffers,\n-                        absl::Span<const se::DeviceMemoryBase> result_buffers,\n-                        se::DeviceMemoryBase scratch_memory, se::Stream* stream,\n-                        RunConvOptions = {});\n+                        absl::Span<const se::DeviceAddressBase> operand_buffers,\n+                        absl::Span<const se::DeviceAddressBase> result_buffers,\n+                        se::DeviceAddressBase scratch_memory,\n+                        se::Stream* stream, RunConvOptions = {});\n \n // Struct to describe properties of a convolution without being tied to specific\n // IR. Will be used to help build Convolution thunks from either XLA HLO or\n@@ -260,8 +260,8 @@ absl::StatusOr<GpuConvConfig> GetGpuConvConfig(\n // Implementation details exposed for debugging and log analysis.\n absl::StatusOr<GpuConvParams> GetGpuConvParams(\n     const GpuConvConfig& conv_config,\n-    absl::Span<const se::DeviceMemoryBase> operand_buffers,\n-    absl::Span<const se::DeviceMemoryBase> result_buffers);\n+    absl::Span<const se::DeviceAddressBase> operand_buffers,\n+    absl::Span<const se::DeviceAddressBase> result_buffers);\n \n inline se::dnn::DataType BiasTypeForInputType(se::dnn::DataType input_type) {\n   switch (input_type) {"
        },
        {
            "sha": "96d1b4ca13c2d9110267fa79a3f9ff5e6ab82038",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -84,9 +84,9 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/module_spec.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -685,12 +685,12 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {\n   int submitted_mem_copies = 0;\n \n   for (const ConstantInfo& info : constants_) {\n-    absl::StatusOr<stream_executor::DeviceMemoryBase> global_status;\n+    absl::StatusOr<stream_executor::DeviceAddressBase> global_status;\n     if (static_cast<bool>(module_handle)) {\n       global_status = executor->GetSymbol(info.symbol_name, module_handle);\n     }\n \n-    se::DeviceMemoryBase global;\n+    se::DeviceAddressBase global;\n     if (static_cast<bool>(module_handle) && global_status.ok()) {\n       // The constant was defined in the PTX and has been allocated by the CUDA\n       // driver.\n@@ -739,17 +739,17 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {\n       .first->second.get();\n }\n \n-absl::StatusOr<se::DeviceMemoryBase> GpuExecutable::BufferForAllocation(\n+absl::StatusOr<se::DeviceAddressBase> GpuExecutable::BufferForAllocation(\n     VariantArguments arguments,\n     const GpuExecutable::BufferAllocToDeviceMemoryMap* globals,\n     const BufferAllocation& allocation,\n-    se::DeviceMemoryAllocator* const memory_allocator, int device_ordinal,\n+    se::DeviceAddressAllocator* const memory_allocator, int device_ordinal,\n     int64_t arg_idx) {\n   if (allocation.is_thread_local()) {\n-    return se::DeviceMemoryBase{};\n+    return se::DeviceAddressBase{};\n   } else if (allocation.is_entry_computation_parameter()) {\n     int64_t param_no = allocation.parameter_number();\n-    se::DeviceMemoryBase registered_buffer = [&] {\n+    se::DeviceAddressBase registered_buffer = [&] {\n       if (auto unowned_shapedbuffers =\n               std::get_if<absl::Span<const ShapedBuffer* const>>(&arguments)) {\n         return (*unowned_shapedbuffers)[param_no]->buffers().element(\n@@ -772,17 +772,17 @@ absl::StatusOr<se::DeviceMemoryBase> GpuExecutable::BufferForAllocation(\n   } else if (allocation.is_constant()) {\n     auto it = globals->find(arg_idx);\n     if (it == globals->end()) {\n-      return se::DeviceMemoryBase();\n+      return se::DeviceAddressBase();\n     }\n     return it->second;\n   } else {\n     // Allocate each allocation that might escape, or is the temp buffer.\n     CHECK(allocation.maybe_live_out() || allocation.IsPreallocatedTempBuffer());\n     const int64_t buffer_size = allocation.size();\n-    se::DeviceMemoryBase buffer_address;\n+    se::DeviceAddressBase buffer_address;\n     if (buffer_size > 0) {\n       TF_ASSIGN_OR_RETURN(\n-          se::OwningDeviceMemory buffer,\n+          se::ScopedDeviceAddress<uint8_t> buffer,\n           memory_allocator->Allocate(device_ordinal, buffer_size,\n                                      /*retry_on_failure=*/true,\n                                      /*memory_space=*/allocation.color()));\n@@ -793,7 +793,7 @@ absl::StatusOr<se::DeviceMemoryBase> GpuExecutable::BufferForAllocation(\n }\n \n static absl::Status CheckAlignment(const BufferAllocation& allocation,\n-                                   se::DeviceMemoryBase buffer, int arg_idx) {\n+                                   se::DeviceAddressBase buffer, int arg_idx) {\n   const int64_t expected_alignment = [&] {\n     if (allocation.is_entry_computation_parameter()) {\n       return kEntryParameterAlignBytes;\n@@ -816,14 +816,14 @@ static absl::Status CheckAlignment(const BufferAllocation& allocation,\n absl::StatusOr<BufferAllocations> GpuExecutable::GenerateBufferAllocations(\n     VariantArguments arguments,\n     const GpuExecutable::BufferAllocToDeviceMemoryMap* globals,\n-    se::DeviceMemoryAllocator* const memory_allocator, int device_ordinal) {\n+    se::DeviceAddressAllocator* const memory_allocator, int device_ordinal) {\n   tsl::profiler::TraceMe hlo_module_activity(\n       [&] { return std::string(\"Build buffer allocations\"); },\n       tsl::profiler::TraceMeLevel::kInfo);\n \n   absl::Span<const BufferAllocation* const> allocations = GetAllocations();\n   const int64_t num_buffers = allocations.size();\n-  std::vector<se::DeviceMemoryBase> buffers;\n+  std::vector<se::DeviceAddressBase> buffers;\n   buffers.reserve(num_buffers);\n   for (int64_t i = 0; i < num_buffers; ++i) {\n     const BufferAllocation& allocation = *allocations[i];\n@@ -855,7 +855,7 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n     VariantArguments arguments) {\n   XLA_SCOPED_LOGGING_TIMER(absl::StrCat(\n       \"GpuExecutable::ExecuteAsyncOnStreamImpl(\", module_name_, \")\"));\n-  se::DeviceMemoryAllocator* const memory_allocator = run_options->allocator();\n+  se::DeviceAddressAllocator* const memory_allocator = run_options->allocator();\n   se::StreamExecutor* executor = run_options->stream()->parent();\n \n   // GpuExecutable always bound to a single GpuContext during its execution, so\n@@ -901,7 +901,7 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n   VLOG(3) << buffer_allocations.ToString();\n   absl::Span<const BufferAllocation* const> allocations = GetAllocations();\n \n-  std::set<se::DeviceMemoryBase> buffers_in_result;\n+  std::set<se::DeviceAddressBase> buffers_in_result;\n \n   const bool is_entire_tuple_contents_aliased = [&] {\n     for (auto& p : result.MutableResult()->buffers().leaves()) {\n@@ -924,7 +924,7 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n     const OutputInfo& output_info = output_info_.at(index);\n     const BufferAllocation* allocation =\n         allocations[output_info.allocation_index];\n-    se::DeviceMemoryBase& result_buffer = p.second;\n+    se::DeviceAddressBase& result_buffer = p.second;\n \n     VLOG(4) << \"Looking at: allocation \" << output_info.allocation_index\n             << \" @ index: \" << index.ToString();\n@@ -952,13 +952,13 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n             output_info.allocation_index);\n       }\n       if (maybe_owning_memory && maybe_owning_memory->HasOwnership()) {\n-        std::optional<tensorflow::se::OwningDeviceMemory> owning =\n+        std::optional<tensorflow::se::ScopedDeviceAddress<uint8_t>> owning =\n             maybe_owning_memory->Release();\n         // If the caller passes the ownership of the device memory, reuse it\n         // as the output buffer. It is up to the caller whether or not to\n         // donate a buffer; the aliasing information describes which buffers\n         // may alias, not buffers that must alias.\n-        se::DeviceMemoryBase argument_buffer = owning->Release();\n+        se::DeviceAddressBase argument_buffer = owning->Release();\n         *maybe_owning_memory = argument_buffer;\n         result_buffer = argument_buffer;\n         // The caller is giving us the\n@@ -980,15 +980,15 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n                    \"buffer is not donated; allocating a fresh buffer\";\n         int64_t allocation_size = ShapeUtil::ByteSizeOf(\n             ShapeUtil::GetSubshape(program_shape_.result(), index));\n-        absl::StatusOr<se::OwningDeviceMemory> allocated_buffer =\n+        absl::StatusOr<se::ScopedDeviceAddress<uint8_t>> allocated_buffer =\n             memory_allocator->Allocate(device_ordinal, allocation_size,\n                                        /*retry_on_failure=*/true,\n                                        /*memory_space=*/allocation->color());\n         if (!allocated_buffer.ok()) {\n           return VerboseAllocationError(allocated_buffer.status());\n         }\n         result_buffer = allocated_buffer->Release();\n-        se::DeviceMemoryBase& aliased_buffer =\n+        se::DeviceAddressBase& aliased_buffer =\n             buffer_allocations.GetMutableDeviceAddress(\n                 output_info.allocation_index);\n         CHECK_EQ(aliased_buffer.size(), result_buffer.size());\n@@ -1048,7 +1048,7 @@ absl::Status GpuExecutable::ExecuteThunks(\n     // (no command buffer update cost).\n     absl::MutexLock lock(module_handle_mutex_);\n     if (module_allocations_.find(executor) == module_allocations_.end()) {\n-      std::vector<se::DeviceMemoryBase> allocs_addr;\n+      std::vector<se::DeviceAddressBase> allocs_addr;\n       allocs_addr.reserve(buffer_allocations.size());\n       for (int i = 0; i < buffer_allocations.size(); i++) {\n         allocs_addr.push_back(buffer_allocations.GetDeviceAddress(i));\n@@ -1067,7 +1067,7 @@ absl::Status GpuExecutable::ExecuteThunks(\n     }\n   }\n \n-  se::DeviceMemoryAllocator* const memory_allocator = run_options->allocator();\n+  se::DeviceAddressAllocator* const memory_allocator = run_options->allocator();\n   // Force synchronous execution if the allocator requires it.\n   const bool block_host_until_done =\n       !memory_allocator->AllowsAsynchronousDeallocation();"
        },
        {
            "sha": "ce1a5eff0bb591794c9b145da00e524f530fda61",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -51,9 +51,9 @@ limitations under the License.\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/scoped_module_handle.h\"\n@@ -203,7 +203,7 @@ class GpuExecutable : public Executable {\n                              const ServiceExecutableRunOptions* run_options);\n \n   using BufferAllocToDeviceMemoryMap =\n-      absl::flat_hash_map<BufferAllocation::Index, se::DeviceMemoryBase>;\n+      absl::flat_hash_map<BufferAllocation::Index, se::DeviceAddressBase>;\n \n   // Loads the PTX or CUBIN for this executable and initializes all\n   // constants that haven't already been initialized by the CUDA driver. Loaded\n@@ -257,13 +257,13 @@ class GpuExecutable : public Executable {\n   absl::StatusOr<BufferAllocations> GenerateBufferAllocations(\n       VariantArguments arguments,\n       const GpuExecutable::BufferAllocToDeviceMemoryMap* globals,\n-      se::DeviceMemoryAllocator* memory_allocator, int device_ordinal);\n+      se::DeviceAddressAllocator* memory_allocator, int device_ordinal);\n \n-  absl::StatusOr<se::DeviceMemoryBase> BufferForAllocation(\n+  absl::StatusOr<se::DeviceAddressBase> BufferForAllocation(\n       VariantArguments arguments,\n       const GpuExecutable::BufferAllocToDeviceMemoryMap* globals,\n       const BufferAllocation& allocation,\n-      se::DeviceMemoryAllocator* memory_allocator, int device_ordinal,\n+      se::DeviceAddressAllocator* memory_allocator, int device_ordinal,\n       int64_t arg_idx);\n \n   // The LLVM IR, in string format, of the unoptimized module generated for\n@@ -357,14 +357,14 @@ class GpuExecutable : public Executable {\n   // Cache previous memory allocations for current module, this is used to help\n   // identify if user's model have unstable pointers by turning on VLOG(5).\n   absl::flat_hash_map<stream_executor::StreamExecutor*,\n-                      std::vector<se::DeviceMemoryBase>>\n+                      std::vector<se::DeviceAddressBase>>\n       module_allocations_ ABSL_GUARDED_BY(module_handle_mutex_);\n \n   std::vector<ConstantInfo> constants_;\n   const absl::flat_hash_map<ShapeIndex, OutputInfo> output_info_;\n   // Retains shared ownership of on-device constants that are managed by XLA and\n   // potentially shared with other executables.\n-  std::vector<std::shared_ptr<se::DeviceMemoryBase>> shared_constants_;\n+  std::vector<std::shared_ptr<se::DeviceAddressBase>> shared_constants_;\n   bool enable_debug_info_manager_;\n \n   GpuExecutable(const GpuExecutable&) = delete;"
        },
        {
            "sha": "ff4253205e49ced0ab62033066cf8b62d6273a58",
            "filename": "third_party/xla/xla/service/gpu/gpu_norm_runner.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/gpu_norm_runner.pb.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -34,16 +34,16 @@ namespace xla {\n namespace gpu {\n \n absl::Status RunGpuNorm(const gpu::GpuNormConfig& config,\n-                        const se::DeviceMemoryBase& x_buffer,\n-                        const se::DeviceMemoryBase& scale_buffer,\n-                        const se::DeviceMemoryBase& y_or_dx_buffer,\n-                        std::optional<se::DeviceMemoryBase> bias_buffer,\n-                        std::optional<se::DeviceMemoryBase> dy_buffer,\n-                        std::optional<se::DeviceMemoryBase> expectation_buffer,\n-                        std::optional<se::DeviceMemoryBase> norm_factor_buffer,\n-                        std::optional<se::DeviceMemoryBase> dscale_buffer,\n-                        std::optional<se::DeviceMemoryBase> dbias_buffer,\n-                        const se::DeviceMemoryBase& scratch_memory,\n+                        const se::DeviceAddressBase& x_buffer,\n+                        const se::DeviceAddressBase& scale_buffer,\n+                        const se::DeviceAddressBase& y_or_dx_buffer,\n+                        std::optional<se::DeviceAddressBase> bias_buffer,\n+                        std::optional<se::DeviceAddressBase> dy_buffer,\n+                        std::optional<se::DeviceAddressBase> expectation_buffer,\n+                        std::optional<se::DeviceAddressBase> norm_factor_buffer,\n+                        std::optional<se::DeviceAddressBase> dscale_buffer,\n+                        std::optional<se::DeviceAddressBase> dbias_buffer,\n+                        const se::DeviceAddressBase& scratch_memory,\n                         se::Stream* stream, RunNormOptions options) {\n   se::dnn::LazyOpRunner<se::dnn::NormOp>* lazy_runner =\n       options.norm_runner->AsNormRunner();\n@@ -52,7 +52,7 @@ absl::Status RunGpuNorm(const gpu::GpuNormConfig& config,\n   TF_ASSIGN_OR_RETURN(auto* runner,\n                       lazy_runner->GetOrCreateRunner(ln_config, stream));\n \n-  std::vector<se::DeviceMemoryBase> operands;\n+  std::vector<se::DeviceAddressBase> operands;\n   operands.push_back(x_buffer);\n   operands.push_back(scale_buffer);\n   operands.push_back(y_or_dx_buffer);"
        },
        {
            "sha": "afe101d617d3cf625f1c5ed0bfde79d4adb7d3c1",
            "filename": "third_party/xla/xla/service/gpu/gpu_norm_runner.h",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_norm_runner.pb.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -186,16 +186,16 @@ struct RunNormOptions {\n };\n \n absl::Status RunGpuNorm(const GpuNormConfig& conv_config,\n-                        const se::DeviceMemoryBase& x_buffer,\n-                        const se::DeviceMemoryBase& scale_buffer,\n-                        const se::DeviceMemoryBase& y_or_dx_buffer,\n-                        std::optional<se::DeviceMemoryBase> bias_buffer,\n-                        std::optional<se::DeviceMemoryBase> dy_buffer,\n-                        std::optional<se::DeviceMemoryBase> expectation_buffer,\n-                        std::optional<se::DeviceMemoryBase> norm_factor_buffer,\n-                        std::optional<se::DeviceMemoryBase> dscale_buffer,\n-                        std::optional<se::DeviceMemoryBase> dbias_buffer,\n-                        const se::DeviceMemoryBase& scratch_memory,\n+                        const se::DeviceAddressBase& x_buffer,\n+                        const se::DeviceAddressBase& scale_buffer,\n+                        const se::DeviceAddressBase& y_or_dx_buffer,\n+                        std::optional<se::DeviceAddressBase> bias_buffer,\n+                        std::optional<se::DeviceAddressBase> dy_buffer,\n+                        std::optional<se::DeviceAddressBase> expectation_buffer,\n+                        std::optional<se::DeviceAddressBase> norm_factor_buffer,\n+                        std::optional<se::DeviceAddressBase> dscale_buffer,\n+                        std::optional<se::DeviceAddressBase> dbias_buffer,\n+                        const se::DeviceAddressBase& scratch_memory,\n                         se::Stream* stream, RunNormOptions options = {});\n \n }  // namespace gpu"
        },
        {
            "sha": "d92fa6df34e5126c496016ae82a52015191832b2",
            "filename": "third_party/xla/xla/service/gpu/gpu_offloading_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_offloading_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_offloading_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_offloading_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -270,7 +270,7 @@ TEST_F(GpuOffloadingTest, CopyIRCreationTest) {\n TEST_F(GpuOffloadingTest, XLAHostMemoryAllocationDeallocationTest) {\n   stream_executor::StreamExecutor* executor =\n       backend().default_stream_executor();\n-  stream_executor::DeviceMemoryBase host_ptr =\n+  stream_executor::DeviceAddressBase host_ptr =\n       executor->Allocate(64, (int64_t)(stream_executor::MemoryType::kHost));\n   TF_ASSERT_OK_AND_ASSIGN(auto memory_space,\n                           executor->GetPointerMemorySpace(host_ptr.opaque()));"
        },
        {
            "sha": "2a55ff428129fe3228d6270c8dd8dbfd880c332f",
            "filename": "third_party/xla/xla/service/gpu/gpu_transfer_manager.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -136,12 +136,12 @@ absl::Status GpuTransferManager::ReadDynamicShapes(\n \n   // First, figure out which parts of `device_shape` are dynamic and where the\n   // dynamic shapes live in GPU memory.  We'll copy the bytes at the\n-  // DeviceMemoryBase into the Shape*'s dimensions.\n-  std::vector<std::pair<se::DeviceMemoryBase, Shape*>> copies;\n+  // DeviceAddressBase into the Shape*'s dimensions.\n+  std::vector<std::pair<se::DeviceAddressBase, Shape*>> copies;\n \n   TF_RETURN_IF_ERROR(device_buffer->buffers().ForEachElementWithStatus(\n       [&](const ShapeIndex& index,\n-          const se::DeviceMemoryBase& buffer) -> absl::Status {\n+          const se::DeviceAddressBase& buffer) -> absl::Status {\n         const Shape& buffer_shape =\n             ShapeUtil::GetSubshape(*device_shape, index);\n         if (buffer_shape.IsTuple()) {\n@@ -162,7 +162,7 @@ absl::Status GpuTransferManager::ReadDynamicShapes(\n           return InvalidArgument(\"Dynamic shape metadata size should not be 0\");\n         }\n \n-        auto buffer_8 = se::DeviceMemory<uint8_t>(buffer);\n+        auto buffer_8 = se::DeviceAddress<uint8_t>(buffer);\n         auto metadata_buffer = buffer_8.GetSlice(offset, metadata_size);\n         copies.push_back(std::make_pair(metadata_buffer, &device_sub_shape));\n \n@@ -188,7 +188,7 @@ absl::Status GpuTransferManager::ReadDynamicShapes(\n     TF_RETURN_IF_ERROR(EnsurePinnedBuffersAllocated(stream->parent()));\n \n     for (const auto& src_dst : copies) {\n-      se::DeviceMemoryBase src = src_dst.first;\n+      se::DeviceAddressBase src = src_dst.first;\n       if (!pinned_buffers_.empty() && src.size() <= kPinnedBufferBytes) {\n         void* buf = pinned_buffers_.back();\n         pinned_buffers_.pop_back();\n@@ -208,7 +208,7 @@ absl::Status GpuTransferManager::ReadDynamicShapes(\n \n   // Copy into the h2d_memcpy_dsts.\n   for (int i = 0; i < copies.size(); i++) {\n-    se::DeviceMemoryBase src = copies[i].first;\n+    se::DeviceAddressBase src = copies[i].first;\n     void* dst = h2d_memcpy_dsts[i];\n     TF_RETURN_IF_ERROR(stream->Memcpy(dst, src, src.size()));\n   }\n@@ -246,7 +246,7 @@ static absl::Status ForEachChunk(\n }\n \n absl::Status GpuTransferManager::TransferBufferFromDevice(\n-    se::Stream* stream, const se::DeviceMemoryBase& source, int64_t size,\n+    se::Stream* stream, const se::DeviceAddressBase& source, int64_t size,\n     void* destination) {\n   if (source.size() < size) {\n     return absl::FailedPreconditionError(absl::StrFormat(\n@@ -270,7 +270,7 @@ absl::Status GpuTransferManager::TransferBufferFromDevice(\n     VLOG(5) << \"Transfer buffer chunk from device: offset=\" << chunk_offset\n             << \" size=\" << tsl::strings::HumanReadableNumBytes(chunk_size);\n \n-    se::DeviceMemoryBase chunk = source.GetByteSlice(chunk_offset, chunk_size);\n+    se::DeviceAddressBase chunk = source.GetByteSlice(chunk_offset, chunk_size);\n     TF_RETURN_IF_ERROR(stream->Memcpy(staging, chunk, chunk_size));\n \n     void* dst = reinterpret_cast<char*>(destination) + chunk_offset;\n@@ -288,7 +288,7 @@ absl::Status GpuTransferManager::TransferBufferFromDevice(\n \n absl::Status GpuTransferManager::TransferBufferToDevice(\n     se::Stream* stream, int64_t size, const void* source,\n-    se::DeviceMemoryBase* destination) {\n+    se::DeviceAddressBase* destination) {\n   if (destination->size() < size) {\n     return absl::FailedPreconditionError(absl::StrFormat(\n         \"Destination allocation on device not large enough for data transfer: \""
        },
        {
            "sha": "2f6d210ed253db6ce1312bbe0d697d0ef23db782",
            "filename": "third_party/xla/xla/service/gpu/gpu_transfer_manager.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/service/gpu/outfeed_manager.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -89,13 +89,13 @@ class GpuTransferManager : public GenericTransferManager {\n       se::StreamExecutor* executor);\n \n   absl::Status TransferBufferFromDevice(se::Stream* stream,\n-                                        const se::DeviceMemoryBase& source,\n+                                        const se::DeviceAddressBase& source,\n                                         int64_t size,\n                                         void* destination) override;\n \n   absl::Status TransferBufferToDevice(\n       se::Stream* stream, int64_t size, const void* source,\n-      se::DeviceMemoryBase* destination) override;\n+      se::DeviceAddressBase* destination) override;\n \n   // TODO(ezhulenev): Unify this with staged buffers for transfering literals.\n "
        },
        {
            "sha": "e86f99df1bc1bcd6d4d8044d7a180fbdf9d62b71",
            "filename": "third_party/xla/xla/service/gpu/infeed_manager.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Finfeed_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Finfeed_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Finfeed_manager.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/errors.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -45,7 +45,7 @@ InfeedManager::InfeedManager(se::StreamExecutor* executor)\n   stream_->SetName(\"Infeed manager\");\n }\n \n-static absl::StatusOr<se::DeviceMemoryHandle> CopyBufferToDevice(\n+static absl::StatusOr<se::DeviceAddressHandle> CopyBufferToDevice(\n     se::Stream* stream, int64_t size, const void* source) {\n   if (size > std::numeric_limits<int32_t>::max()) {\n     return InvalidArgument(\"GPU infeed of %d bytes exceeds maximum of %d bytes\",\n@@ -57,8 +57,8 @@ static absl::StatusOr<se::DeviceMemoryHandle> CopyBufferToDevice(\n   }\n \n   se::StreamExecutor* executor = stream->parent();\n-  se::DeviceMemoryHandle buffer(executor,\n-                                executor->AllocateArray<uint8_t>(size));\n+  se::DeviceAddressHandle buffer(executor,\n+                                 executor->AllocateArray<uint8_t>(size));\n   TF_RETURN_IF_ERROR(stream->Memcpy(buffer.address_ptr(), source, size));\n \n   return std::move(buffer);\n@@ -74,7 +74,7 @@ absl::Status InfeedManager::TransferLiteralToInfeed(\n \n   // For a tuple, we transfer each of its elements to the device and enqueue the\n   // resulting destination device addresses with the infeed manager.\n-  ShapeTree<se::DeviceMemoryHandle> buffer_tree(literal_shape);\n+  ShapeTree<se::DeviceAddressHandle> buffer_tree(literal_shape);\n   for (auto& leaf : buffer_tree.leaves()) {\n     const Shape& sub_shape = ShapeUtil::GetSubshape(literal_shape, leaf.first);\n     CHECK(sub_shape.IsArray()) << ShapeUtil::HumanStringWithLayout(sub_shape);"
        },
        {
            "sha": "3452b1f15f5393f754787cfe58bca57241b33d9c",
            "filename": "third_party/xla/xla/service/gpu/infeed_manager.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Finfeed_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Finfeed_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Finfeed_manager.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"xla/literal.h\"\n #include \"xla/service/gpu/xfeed_queue.h\"\n #include \"xla/shape_tree.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla {\n@@ -46,7 +46,7 @@ namespace gpu {\n \n // Client-side class used to enqueue infeed buffers.\n class InfeedManager\n-    : public BlockingXfeedQueue<ShapeTree<se::DeviceMemoryHandle>> {\n+    : public BlockingXfeedQueue<ShapeTree<se::DeviceAddressHandle>> {\n  public:\n   explicit InfeedManager(se::StreamExecutor* executor);\n "
        },
        {
            "sha": "a9d540a6465cf694cacb35cd81353a9bd2ea296b",
            "filename": "third_party/xla/xla/service/gpu/kernels/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -211,7 +211,7 @@ xla_test(\n     deps = [\n         \":cutlass_gemm_custom_kernel\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -239,8 +239,8 @@ xla_cc_binary(\n         \":cutlass_gemm_custom_kernel\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:gpu_plugin\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:platform\",\n@@ -413,7 +413,7 @@ cc_library(\n     visibility = [\":friends\"],\n     deps = [\n         \":custom_kernel\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:kernel_spec\",\n@@ -431,7 +431,7 @@ xla_test(\n     deps = [\n         \":custom_kernel\",\n         \":ptx_custom_kernel\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:launch_dim\","
        },
        {
            "sha": "7c45a2664ee8f08ce8327a328ec2d701e243816c",
            "filename": "third_party/xla/xla/service/gpu/kernels/cutlass_gemm_custom_kernel_benchmarks.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_benchmarks.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_benchmarks.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_benchmarks.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -19,8 +19,8 @@ limitations under the License.\n \n #include \"absl/log/check.h\"\n #include \"xla/service/gpu/kernels/cutlass_gemm_custom_kernel.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -64,16 +64,16 @@ static void BM_RowMajorGemm(benchmark::State& state) {\n                           executor->LoadKernel(custom_kernel.kernel_spec()));\n \n   // Prepare arguments: a=1.1, b=1.2, c=0.0\n-  se::DeviceMemory<float> a = executor->AllocateArray<float>(m * k, 0);\n-  se::DeviceMemory<float> b = executor->AllocateArray<float>(k * n, 0);\n-  se::DeviceMemory<float> c = executor->AllocateArray<float>(m * n, 0);\n+  se::DeviceAddress<float> a = executor->AllocateArray<float>(m * k, 0);\n+  se::DeviceAddress<float> b = executor->AllocateArray<float>(k * n, 0);\n+  se::DeviceAddress<float> c = executor->AllocateArray<float>(m * n, 0);\n \n   CHECK_OK(stream->Memset32(&a, BitPattern(1.1f), a.size()));\n   CHECK_OK(stream->Memset32(&b, BitPattern(1.2f), b.size()));\n   CHECK_OK(stream->MemZero(&c, c.size()));\n \n   se::KernelArgsDeviceMemoryArray args(\n-      std::vector<se::DeviceMemoryBase>({a, b, c}),\n+      std::vector<se::DeviceAddressBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n \n   for (auto s : state) {"
        },
        {
            "sha": "592b8fd7731b6ce32ac1e075efb1c908ca20324d",
            "filename": "third_party/xla/xla/service/gpu/kernels/cutlass_gemm_custom_kernel_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -21,7 +21,7 @@ limitations under the License.\n #include <vector>\n \n #include <gtest/gtest.h>\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -58,9 +58,9 @@ TEST(CutlassGemmKernelTest, SimpleGemm) {\n   int64_t byte_length = sizeof(float) * length;\n \n   // Prepare arguments: a=2, b=2, c=0\n-  se::DeviceMemory<float> a = executor->AllocateArray<float>(length, 0);\n-  se::DeviceMemory<float> b = executor->AllocateArray<float>(length, 0);\n-  se::DeviceMemory<float> c = executor->AllocateArray<float>(length, 0);\n+  se::DeviceAddress<float> a = executor->AllocateArray<float>(length, 0);\n+  se::DeviceAddress<float> b = executor->AllocateArray<float>(length, 0);\n+  se::DeviceAddress<float> c = executor->AllocateArray<float>(length, 0);\n \n   float value = 2.0;\n   uint32_t pattern;\n@@ -72,7 +72,7 @@ TEST(CutlassGemmKernelTest, SimpleGemm) {\n \n   // Launch gemm kernel with device memory arguments.\n   se::KernelArgsDeviceMemoryArray arr(\n-      std::vector<se::DeviceMemoryBase>({a, b, c}),\n+      std::vector<se::DeviceAddressBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n   TF_ASSERT_OK(gemm->Launch(custom_kernel.thread_dims(),\n                             custom_kernel.block_dims(), stream.get(), arr));"
        },
        {
            "sha": "3267f5dc02b35e4382f1dbbead6535d596c2f579",
            "filename": "third_party/xla/xla/service/gpu/kernels/ptx_custom_kernel.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/service/gpu/kernels/custom_kernel.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n@@ -37,7 +37,7 @@ absl::StatusOr<std::unique_ptr<se::KernelArgsPackedArrayBase>>\n KernelArgsPacking(const se::Kernel& kernel, const se::KernelArgs& args) {\n   auto* mem_args = se::Cast<se::KernelArgsDeviceAddressArray>(&args);\n \n-  return se::PackKernelArgs<se::DeviceMemoryBase>(\n+  return se::PackKernelArgs<se::DeviceAddressBase>(\n       mem_args->device_memory_args(), mem_args->number_of_shared_bytes());\n }\n "
        },
        {
            "sha": "c37e0ff701464ddb231ec9c329ee747ef9020d2d",
            "filename": "third_party/xla/xla/service/gpu/kernels/ptx_custom_kernel_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/stream_executor/cuda/cuda_platform.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -96,15 +96,15 @@ TEST(PtxCustomKernelTest, GetPtxCustomKernel) {\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n                           executor->CreateStream());\n-  se::DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n   CHECK_OK(stream->Memset32(&a, 1, byte_length));\n   CHECK_OK(stream->Memset32(&b, 2, byte_length));\n   CHECK_OK(stream->MemZero(&c, byte_length));\n \n   se::KernelArgsDeviceMemoryArray args(\n-      std::vector<se::DeviceMemoryBase>({a, b, c}),\n+      std::vector<se::DeviceAddressBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n   CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n                           custom_kernel.block_dims(), stream.get(), args));\n@@ -136,15 +136,15 @@ TEST(PtxCustomKernelTest, GetPtxCustomKernelWithClusterDim) {\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n                           executor->CreateStream());\n-  se::DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n   CHECK_OK(stream->Memset32(&a, 1, byte_length));\n   CHECK_OK(stream->Memset32(&b, 2, byte_length));\n   CHECK_OK(stream->MemZero(&c, byte_length));\n \n   se::KernelArgsDeviceMemoryArray args(\n-      std::vector<se::DeviceMemoryBase>({a, b, c}),\n+      std::vector<se::DeviceAddressBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n   CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n                           custom_kernel.block_dims(), stream.get(), args));\n@@ -215,15 +215,15 @@ TEST(PtxCustomKernelTest, GetOwnedPtxCustomKernel) {\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n                           executor->CreateStream());\n-  se::DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n   CHECK_OK(stream->Memset32(&a, 1, byte_length));\n   CHECK_OK(stream->Memset32(&b, 2, byte_length));\n   CHECK_OK(stream->MemZero(&c, byte_length));\n \n   se::KernelArgsDeviceMemoryArray args(\n-      std::vector<se::DeviceMemoryBase>({a, b, c}),\n+      std::vector<se::DeviceAddressBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n   CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n                           custom_kernel.block_dims(), stream.get(), args));"
        },
        {
            "sha": "6c9dea647fcf37df9c0f246b02ef61d2559ea7c8",
            "filename": "third_party/xla/xla/service/gpu/matmul_utils.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -44,8 +44,8 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/blas.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -478,10 +478,10 @@ bool IsTf32Allowed(PrecisionConfig::Algorithm algorithm,\n }\n \n absl::StatusOr<GemmConfig::DescriptorsTuple> GemmConfig::GetMatrixDescriptors(\n-    se::DeviceMemoryBase lhs_buf, se::DeviceMemoryBase rhs_buf,\n-    se::DeviceMemoryBase out_buf) const {\n+    se::DeviceAddressBase lhs_buf, se::DeviceAddressBase rhs_buf,\n+    se::DeviceAddressBase out_buf) const {\n   auto create_matrix_desc = [](const se::gpu::MatrixLayout& layout,\n-                               se::DeviceMemoryBase data)\n+                               se::DeviceAddressBase data)\n       -> absl::StatusOr<se::gpu::MatrixDescriptor> {\n     TF_ASSIGN_OR_RETURN(se::blas::DataType type,\n                         se::gpu::AsBlasDataType(layout.dtype));\n@@ -528,7 +528,7 @@ template <typename Scale, typename Input, typename Output>\n absl::Status DoGemmWithAlgorithm(const se::gpu::MatrixDescriptor& lhs,\n                                  const se::gpu::MatrixDescriptor& rhs,\n                                  const se::gpu::OutputMatrixDescriptor& output,\n-                                 se::DeviceMemoryBase workspace, Scale alpha,\n+                                 se::DeviceAddressBase workspace, Scale alpha,\n                                  Scale beta, se::Stream* stream,\n                                  PrecisionConfig::Algorithm precision_algorithm,\n                                  se::blas::AlgorithmType algorithm,\n@@ -543,7 +543,7 @@ absl::Status DoGemmWithAlgorithm(const se::gpu::MatrixDescriptor& lhs,\n       se::blas::ComputationType computation_type,\n       se::gpu::GetBlasComputationType(precision_algorithm, lhs_type,\n                                       output_type, compute_precision));\n-  se::DeviceMemory<Output> output_data(output.data);\n+  se::DeviceAddress<Output> output_data(output.data);\n \n   // Set a workspace for all Blas operations launched below.\n   auto* blas = stream->parent()->AsBlas();\n@@ -573,7 +573,7 @@ template <typename Scale, typename Input, typename Output>\n absl::Status DoGemm(const se::gpu::MatrixDescriptor& lhs,\n                     const se::gpu::MatrixDescriptor& rhs,\n                     const se::gpu::OutputMatrixDescriptor& output,\n-                    se::DeviceMemoryBase workspace, Scale alpha, Scale beta,\n+                    se::DeviceAddressBase workspace, Scale alpha, Scale beta,\n                     se::Stream* stream,\n                     PrecisionConfig::Algorithm precision_algorithm,\n                     std::optional<se::blas::AlgorithmType> algorithm,\n@@ -582,7 +582,7 @@ absl::Status DoGemm(const se::gpu::MatrixDescriptor& lhs,\n                     se::blas::ProfileResult* profile_result,\n                     se::blas::CallContext context) {\n   CHECK(output.transpose == se::blas::Transpose::kNoTranspose);\n-  se::DeviceMemory<Output> output_data(output.data);\n+  se::DeviceAddress<Output> output_data(output.data);\n   auto* blas = stream->parent()->AsBlas();\n   if (blas == nullptr) {\n     return absl::InternalError(\"No Blas support for stream\");\n@@ -615,10 +615,10 @@ absl::Status DoGemm(const se::gpu::MatrixDescriptor& lhs,\n \n }  // namespace\n \n-absl::Status RunGemm(const GemmConfig& config, se::DeviceMemoryBase lhs_buffer,\n-                     se::DeviceMemoryBase rhs_buffer,\n-                     se::DeviceMemoryBase output_buffer,\n-                     se::DeviceMemoryBase workspace_buffer,\n+absl::Status RunGemm(const GemmConfig& config, se::DeviceAddressBase lhs_buffer,\n+                     se::DeviceAddressBase rhs_buffer,\n+                     se::DeviceAddressBase output_buffer,\n+                     se::DeviceAddressBase workspace_buffer,\n                      bool deterministic_ops, se::Stream* stream,\n                      std::optional<se::blas::AlgorithmType> algorithm,\n                      se::blas::ProfileResult* profile_result) {"
        },
        {
            "sha": "8204e4e68c4f6733e0e9abe62408338bc605f074",
            "filename": "third_party/xla/xla/service/gpu/matmul_utils.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -32,8 +32,8 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/blas.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -145,18 +145,18 @@ struct GemmConfig : public se::gpu::GemmConfig {\n     bool operands_swapped;\n   };\n   absl::StatusOr<DescriptorsTuple> GetMatrixDescriptors(\n-      se::DeviceMemoryBase lhs_buf, se::DeviceMemoryBase rhs_buf,\n-      se::DeviceMemoryBase out_buf) const;\n+      se::DeviceAddressBase lhs_buf, se::DeviceAddressBase rhs_buf,\n+      se::DeviceAddressBase out_buf) const;\n };\n \n // Run the given GEMM instruction `gemm` subject to the configuration\n // in `gemm_config` and the passed buffers.\n //\n // If `algorithm` is provided, it overrides the one specified in `config`.\n absl::Status RunGemm(\n-    const GemmConfig& config, se::DeviceMemoryBase lhs_buffer,\n-    se::DeviceMemoryBase rhs_buffer, se::DeviceMemoryBase output_buffer,\n-    se::DeviceMemoryBase workspace_buffer, bool deterministic_ops,\n+    const GemmConfig& config, se::DeviceAddressBase lhs_buffer,\n+    se::DeviceAddressBase rhs_buffer, se::DeviceAddressBase output_buffer,\n+    se::DeviceAddressBase workspace_buffer, bool deterministic_ops,\n     se::Stream* stream,\n     std::optional<se::blas::AlgorithmType> algorithm = std::nullopt,\n     se::blas::ProfileResult* profile_result = nullptr);"
        },
        {
            "sha": "9d7d99728c81b50f8082805178f196f68c47a39a",
            "filename": "third_party/xla/xla/service/gpu/stream_executor_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -51,7 +51,7 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/data_type.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/gpu/repeat_buffer_kernel.h\"\n@@ -447,7 +447,7 @@ typename std::enable_if<std::is_floating_point<T>::value,\n \n template <typename T>\n static void InitializeTypedBuffer(se::Stream* stream,\n-                                  se::DeviceMemoryBase buffer,\n+                                  se::DeviceAddressBase buffer,\n                                   int64_t* rng_state) {\n   // Accesses to static variables are not locked, since the caller is already\n   // in a critical section.\n@@ -501,7 +501,7 @@ static void InitializeTypedBuffer(se::Stream* stream,\n   // Issue a second host->device copy to transfer the rest of host_buffer\n   int64_t second_size = std::min<int64_t>(host_index, elements_to_fill);\n   CHECK_LE(first_size + second_size, host_buffer_size);\n-  se::DeviceMemoryBase mem =\n+  se::DeviceAddressBase mem =\n       buffer.GetByteSlice(first_size * sizeof(T), second_size * sizeof(T));\n   CHECK_OK(stream->Memcpy(&mem, host_buffer->data(), mem.size()));\n   elements_to_fill -= second_size;\n@@ -532,7 +532,7 @@ static void InitializeTypedBuffer(se::Stream* stream,\n }\n \n void InitializeBuffer(se::Stream* stream, PrimitiveType buffer_type,\n-                      int64_t* rng_state, se::DeviceMemoryBase buffer) {\n+                      int64_t* rng_state, se::DeviceAddressBase buffer) {\n   return primitive_util::PrimitiveTypeSwitch<void>(\n       [&](auto primitive_type_constant) -> void {\n         if constexpr (primitive_util::IsFloatingPointType("
        },
        {
            "sha": "94836d863f37ddc27294a067a1abafe236ca1c03",
            "filename": "third_party/xla/xla/service/gpu/stream_executor_util.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/hlo_module_config.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -124,7 +124,7 @@ absl::Status ExecuteKernelOnStream(\n // Precondition: `buffer_type` is a floating point type, `rng_state` needs to be\n // initialized to zero on the first use.\n void InitializeBuffer(se::Stream* stream, PrimitiveType buffer_type,\n-                      int64_t* rng_state, se::DeviceMemoryBase buffer);\n+                      int64_t* rng_state, se::DeviceAddressBase buffer);\n \n // Converts the C++ enum `CudnnConvKind`, to the proto enum version\n // `ConvolutionKind`."
        },
        {
            "sha": "55c8c2316833fdfa3335657e7c810ebf443837b3",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -94,7 +94,7 @@ xla_test(\n             \"@local_tsl//tsl/platform:test\",\n         ],\n     ) + [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tests:hlo_pjrt_test_base\",\n         \"//xla/tests:xla_internal_test_main\",\n@@ -168,7 +168,7 @@ xla_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_runner_interface\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream\",\n@@ -801,7 +801,7 @@ xla_test(\n         \"@local_tsl//tsl/platform:test\",\n     ] + if_cuda_is_configured([\n         \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n     ]),\n )\n "
        },
        {
            "sha": "c0c2560f0486d1b80d41d0ede4151b9df5968a38",
            "filename": "third_party/xla/xla/service/gpu/tests/command_buffer_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -35,7 +35,7 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_runner_interface.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -252,8 +252,8 @@ static absl::Status Memcpy(se::Stream* stream, MemcpyState* state,\n                            ffi::AnyBuffer src,\n                            ffi::Result<ffi::AnyBuffer> dst) {\n   EXPECT_NE(state, nullptr);\n-  se::DeviceMemoryBase dst_mem = dst->device_memory();\n-  se::DeviceMemoryBase src_mem = src.device_memory();\n+  se::DeviceAddressBase dst_mem = dst->device_memory();\n+  se::DeviceAddressBase src_mem = src.device_memory();\n   return stream->MemcpyD2D(&dst_mem, src_mem, src_mem.size());\n }\n "
        },
        {
            "sha": "a847676119b235ebdd354a55cf5f039aa08e1242",
            "filename": "third_party/xla/xla/service/gpu/tests/dynamic_shared_memory_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_shared_memory_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_shared_memory_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_shared_memory_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -24,8 +24,8 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -158,7 +158,7 @@ TEST(SharedMemoryUseTest, ArrayReversalWorks) {\n                    /*shared_mem_bytes=*/buffer_size_bytes)\n           .value();\n \n-  se::DeviceMemory<data_type> device_buffer =\n+  se::DeviceAddress<data_type> device_buffer =\n       executor->AllocateArray<data_type>(n_elements);\n   std::vector<data_type> host_buffer(n_elements);\n   for (int row = 0; row < n_rows; ++row) {\n@@ -171,9 +171,9 @@ TEST(SharedMemoryUseTest, ArrayReversalWorks) {\n \n   CHECK_OK(\n       stream->Memcpy(&device_buffer, host_buffer.data(), buffer_size_bytes));\n-  se::DeviceMemory<uint32_t> dev_n_cols = executor->AllocateScalar<uint32_t>();\n+  se::DeviceAddress<uint32_t> dev_n_cols = executor->AllocateScalar<uint32_t>();\n   CHECK_OK(stream->Memcpy(&dev_n_cols, &n_cols, sizeof(uint32_t)));\n-  se::DeviceMemory<uint32_t> dev_n_rows = executor->AllocateScalar<uint32_t>();\n+  se::DeviceAddress<uint32_t> dev_n_rows = executor->AllocateScalar<uint32_t>();\n   CHECK_OK(stream->Memcpy(&dev_n_rows, &n_rows, sizeof(uint32_t)));\n   CHECK_OK(stream->BlockHostUntilDone());\n "
        },
        {
            "sha": "daaa94e827b466f73c55a7430d46402b6b077df2",
            "filename": "third_party/xla/xla/service/gpu/tests/dynamic_slice_fusion_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_slice_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_slice_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_slice_fusion_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n #include \"xla/error_spec.h\"\n #include \"xla/ffi/ffi.h\"\n #include \"xla/ffi/ffi_api.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tests/hlo_pjrt_test_base.h\"\n #include \"xla/tsl/platform/test.h\"\n@@ -145,8 +145,8 @@ TEST_F(DynamicSliceFusionTest, GemmSlice) {\n \n static absl::Status Memcpy(se::Stream* stream, ffi::AnyBuffer src,\n                            ffi::Result<ffi::AnyBuffer> dst) {\n-  se::DeviceMemoryBase dst_mem = dst->device_memory();\n-  se::DeviceMemoryBase src_mem = src.device_memory();\n+  se::DeviceAddressBase dst_mem = dst->device_memory();\n+  se::DeviceAddressBase src_mem = src.device_memory();\n   return stream->MemcpyD2D(&dst_mem, src_mem, src_mem.size());\n }\n "
        },
        {
            "sha": "e76b5f9ee12da0f63d78a1fdd579d297cf13a4d3",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -1226,7 +1226,7 @@ xla_cc_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:gpu_init_impl\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1551,8 +1551,8 @@ xla_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:gpu_executable\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\","
        },
        {
            "sha": "be6bc7fe201cc711571c9f4391018ba57ccb8d7c",
            "filename": "third_party/xla/xla/service/gpu/transforms/dynamic_slice_fusion_rewriter_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -34,7 +34,7 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n@@ -915,8 +915,8 @@ TEST_F(DynamicSliceFusionRewriterTest, SimpleGemmOperandsFromSameSlice) {\n \n static absl::Status Memcpy(se::Stream* stream, ffi::AnyBuffer src,\n                            ffi::AnyBuffer dst) {\n-  se::DeviceMemoryBase dst_mem = dst.device_memory();\n-  se::DeviceMemoryBase src_mem = src.device_memory();\n+  se::DeviceAddressBase dst_mem = dst.device_memory();\n+  se::DeviceAddressBase src_mem = src.device_memory();\n   return stream->MemcpyD2D(&dst_mem, src_mem, src_mem.size());\n }\n "
        },
        {
            "sha": "4ab0fb261d837cacbddcf043c642ab596b005f1d",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -43,8 +43,8 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n@@ -1498,7 +1498,7 @@ class GemmRewriteAllocationTest : public GpuCodegenTest {\n   }\n \n  private:\n-  std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n+  std::unique_ptr<se::DeviceAddressAllocator> allocator_;\n };\n \n TEST_F(GemmRewriteAllocationTest, SharedBufferAssignment) {"
        },
        {
            "sha": "d1de84a2567124874b018eed4ebd30f76b37a690",
            "filename": "third_party/xla/xla/service/hlo_module_config.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -107,7 +107,7 @@ std::string HloModuleConfig::compilation_cache_key() const {\n     StrAppend(&key, \"::fdo_profile=\", absl::BytesToHexString(fdo_profile()));\n   }\n   if (device_memory_size() != 0) {\n-    StrAppend(&key, \"::device_memory_size=\", device_memory_size());\n+    StrAppend(&key, \"::device_address_size=\", device_memory_size());\n   }\n   StrAppend(&key, \"::use_shardy_partitioner=\", use_shardy_partitioner());\n   if (partition_size() != 0) {"
        },
        {
            "sha": "d0e58a65b97d98f66c5407bc08843a1d8def2120",
            "filename": "third_party/xla/xla/service/hlo_runner.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -49,9 +49,9 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/env.h\"\n@@ -90,7 +90,7 @@ class HloRunnerExecutable : public OpaqueExecutable {\n }  // namespace\n \n HloRunner::HloRunner(se::Platform* platform, int intra_op_parallelism_threads,\n-                     std::unique_ptr<se::DeviceMemoryAllocator> allocator) {\n+                     std::unique_ptr<se::DeviceAddressAllocator> allocator) {\n   BackendOptions backend_options;\n   backend_options.set_platform(platform);\n   backend_options.set_intra_op_parallelism_threads(\n@@ -105,7 +105,7 @@ HloRunner::HloRunner(se::Platform* platform, int intra_op_parallelism_threads,\n \n HloRunner::~HloRunner() {}\n \n-se::DeviceMemoryAllocator* HloRunner::GetAllocator() {\n+se::DeviceAddressAllocator* HloRunner::GetAllocator() {\n   if (allocator_ == nullptr) {\n     return backend_->memory_allocator();\n   }\n@@ -195,7 +195,7 @@ absl::StatusOr<Literal> HloRunner::TransferLiteralFromDevice(\n   ShapedBuffer shaped_buffer(device_shape, buffer.device_ordinal());\n   // Populate buffer element by element since the shapes differ now.\n   shaped_buffer.buffers().ForEachMutableElement(\n-      [&](const xla::ShapeIndex& index, se::DeviceMemoryBase* base_buffer) {\n+      [&](const xla::ShapeIndex& index, se::DeviceAddressBase* base_buffer) {\n         *base_buffer = buffer.buffer(index);\n       });\n   return backend().transfer_manager()->TransferLiteralFromDevice(stream.get(),\n@@ -289,7 +289,7 @@ absl::StatusOr<Literal> HloRunner::ExecuteWithExecutableAndProfile(\n static std::vector<ExecutionInput> ExecutionInputsFromScopedShapedBuffers(\n     absl::Span<ScopedShapedBuffer const> inputs,\n     HloInputOutputAliasConfig alias_config, int device_ordinal,\n-    se::DeviceMemoryAllocator* allocator) {\n+    se::DeviceAddressAllocator* allocator) {\n   std::vector<ExecutionInput> execution_inputs;\n \n   for (int param_num = 0; param_num < inputs.size(); param_num++) {\n@@ -299,11 +299,12 @@ static std::vector<ExecutionInput> ExecutionInputsFromScopedShapedBuffers(\n \n     input_buffer.buffers().ForEachElement(\n         [&](const ShapeIndex& index,\n-            const se::DeviceMemoryBase& execution_input_buffer) {\n+            const se::DeviceAddressBase& execution_input_buffer) {\n           if (alias_config.ParameterHasAlias(param_num, index)) {\n             // Store owned.\n-            *buffer_tree.mutable_element(index) = se::OwningDeviceMemory{\n-                execution_input_buffer, device_ordinal, allocator};\n+            *buffer_tree.mutable_element(index) =\n+                se::ScopedDeviceAddress<uint8_t>{execution_input_buffer,\n+                                                 device_ordinal, allocator};\n           } else {\n             // Store unowned.\n             *buffer_tree.mutable_element(index) = execution_input_buffer;\n@@ -318,10 +319,10 @@ static std::vector<ExecutionInput> ExecutionInputsFromScopedShapedBuffers(\n // ExecutionInputs, and an owning vector of `OwningDeviceMemory`'s.\n static void ExecutionInputsFromMovedScopedShapedBuffers(\n     std::vector<ExecutionInput>* out_execution_inputs,\n-    std::vector<se::OwningDeviceMemory>* out_owned_args,\n+    std::vector<se::ScopedDeviceAddress<uint8_t>>* out_owned_args,\n     std::vector<ScopedShapedBuffer> inputs,\n     HloInputOutputAliasConfig alias_config, int device_ordinal,\n-    se::DeviceMemoryAllocator* allocator) {\n+    se::DeviceAddressAllocator* allocator) {\n   CHECK(out_execution_inputs->empty());\n   CHECK(out_owned_args->empty());\n \n@@ -333,7 +334,7 @@ static void ExecutionInputsFromMovedScopedShapedBuffers(\n \n     input_buffer.buffers().ForEachElement(\n         [&](const ShapeIndex& index,\n-            const se::DeviceMemoryBase& execution_input_buffer) {\n+            const se::DeviceAddressBase& execution_input_buffer) {\n           if (alias_config.ParameterHasAlias(param_num, index)) {\n             VLOG(1) << \"Input \" << param_num << \" index \" << index.ToString()\n                     << \" buffer \" << execution_input_buffer.opaque()\n@@ -342,8 +343,9 @@ static void ExecutionInputsFromMovedScopedShapedBuffers(\n             // Owned by out_execution_inputs.\n             // This allows the Executable to transfer the ownership to the\n             // ExecutionOutput.\n-            *buffer_tree.mutable_element(index) = se::OwningDeviceMemory{\n-                execution_input_buffer, device_ordinal, allocator};\n+            *buffer_tree.mutable_element(index) =\n+                se::ScopedDeviceAddress<uint8_t>{execution_input_buffer,\n+                                                 device_ordinal, allocator};\n           } else {\n             VLOG(1) << \"Input \" << param_num << \" index \" << index.ToString()\n                     << \" buffer \" << execution_input_buffer.opaque()\n@@ -419,7 +421,7 @@ absl::StatusOr<ExecutionOutput> HloRunner::ExecuteWithMovedDeviceBuffers(\n   std::vector<ExecutionInput> execution_arguments;\n   // We need this to keep the arguments not owned by execution_arguments\n   // alive.\n-  std::vector<se::OwningDeviceMemory> owned_arguments;\n+  std::vector<se::ScopedDeviceAddress<uint8_t>> owned_arguments;\n \n   ExecutionInputsFromMovedScopedShapedBuffers(\n       &execution_arguments, &owned_arguments, std::move(arguments),"
        },
        {
            "sha": "b2b5feab32b49131389ef693ed4b3720e17018c1",
            "filename": "third_party/xla/xla/service/hlo_runner.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/service/transfer_manager.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -65,7 +65,7 @@ class HloRunner : public HloRunnerInterface {\n   // the backend allocator.\n   explicit HloRunner(\n       se::Platform* platform, int intra_op_parallelism_threads = -1,\n-      std::unique_ptr<se::DeviceMemoryAllocator> allocator = nullptr);\n+      std::unique_ptr<se::DeviceAddressAllocator> allocator = nullptr);\n \n   ~HloRunner() override;\n \n@@ -274,7 +274,7 @@ class HloRunner : public HloRunnerInterface {\n       DeviceAssignment* device_assignment);\n \n   // Gets or creates the `DeviceMemoryAllocator`.\n-  se::DeviceMemoryAllocator* GetAllocator();\n+  se::DeviceAddressAllocator* GetAllocator();\n \n   // Calls `UpdateEntryComputationLayout` if HloRunner has not called it on the\n   // module before. This method is called before the module is executed. The\n@@ -287,7 +287,7 @@ class HloRunner : public HloRunnerInterface {\n   std::unique_ptr<Backend> backend_;\n   TransferManager::DeviceShapeRepresentationFn device_shape_representation_fn_;\n \n-  std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n+  std::unique_ptr<se::DeviceAddressAllocator> allocator_;\n \n   absl::Mutex mu_;\n   // Set of module unique_ids that we already called"
        },
        {
            "sha": "fb7baa616bc9dd4fce779682615c5fb1984d8e90",
            "filename": "third_party/xla/xla/service/llvm_compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -70,11 +70,11 @@ class LLVMCompiler : public Compiler {\n   //   absl::StatusOr<std::unique_ptr<Executable>> RunBackend(\n   //       std::unique_ptr<HloModule> module,\n   //       se::StreamExecutor* stream_exec,\n-  //       se::DeviceMemoryAllocator* device_allocator)\n+  //       se::DeviceAddressAllocator* device_allocator)\n   //   absl::StatusOr<std::unique_ptr<HloModule>> RunHloPasses(\n   //       std::unique_ptr<HloModule> module,\n   //       se::StreamExecutor* stream_exec,\n-  //       se::DeviceMemoryAllocator* device_allocator)\n+  //       se::DeviceAddressAllocator* device_allocator)\n   using Compiler::Compile;\n   using Compiler::RunBackend;\n   using Compiler::RunHloPasses;"
        },
        {
            "sha": "f49a27abca18acc4ab123ccb037e81cb892d497a",
            "filename": "third_party/xla/xla/service/local_service.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/service/service.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla_data.pb.h\"\n "
        },
        {
            "sha": "a7b3aa5e4b641c7d2edb27f6d70995be0c579fdb",
            "filename": "third_party/xla/xla/service/maybe_owning_device_memory.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fmaybe_owning_device_memory.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fmaybe_owning_device_memory.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmaybe_owning_device_memory.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -15,39 +15,43 @@ limitations under the License.\n \n #include \"xla/service/maybe_owning_device_memory.h\"\n \n+#include <cstdint>\n #include <optional>\n #include <utility>\n #include <variant>\n \n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n \n namespace xla {\n \n-tensorflow::se::DeviceMemoryBase MaybeOwningDeviceMemory::AsDeviceMemoryBase()\n+stream_executor::DeviceAddressBase MaybeOwningDeviceMemory::AsDeviceMemoryBase()\n     const {\n   if (HasOwnership()) {\n-    return *std::get<tensorflow::se::OwningDeviceMemory>(mem_);\n+    return *std::get<stream_executor::ScopedDeviceAddress<uint8_t>>(mem_);\n   }\n-  return std::get<tensorflow::se::DeviceMemoryBase>(mem_);\n+  return std::get<stream_executor::DeviceAddressBase>(mem_);\n }\n \n bool MaybeOwningDeviceMemory::HasOwnership() const {\n-  return std::holds_alternative<tensorflow::se::OwningDeviceMemory>(mem_);\n+  return std::holds_alternative<stream_executor::ScopedDeviceAddress<uint8_t>>(\n+      mem_);\n }\n \n-std::optional<tensorflow::se::OwningDeviceMemory>\n+std::optional<stream_executor::ScopedDeviceAddress<uint8_t>>\n MaybeOwningDeviceMemory::Release() {\n   if (!HasOwnership()) {\n     return {};\n   }\n-  return std::move(std::get<tensorflow::se::OwningDeviceMemory>(mem_));\n+  return std::move(\n+      std::get<stream_executor::ScopedDeviceAddress<uint8_t>>(mem_));\n }\n \n-const tensorflow::se::OwningDeviceMemory*\n+const stream_executor::ScopedDeviceAddress<uint8_t>*\n MaybeOwningDeviceMemory::AsOwningDeviceMemory() const {\n-  return HasOwnership() ? &std::get<tensorflow::se::OwningDeviceMemory>(mem_)\n-                        : nullptr;\n+  return HasOwnership()\n+             ? &std::get<stream_executor::ScopedDeviceAddress<uint8_t>>(mem_)\n+             : nullptr;\n }\n \n }  // namespace xla"
        },
        {
            "sha": "8f7b33b4d2d66e4ca51c78982c115ffe4bf56447",
            "filename": "third_party/xla/xla/service/maybe_owning_device_memory.h",
            "status": "modified",
            "additions": 26,
            "deletions": 18,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fmaybe_owning_device_memory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fmaybe_owning_device_memory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmaybe_owning_device_memory.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -16,65 +16,73 @@ limitations under the License.\n #ifndef XLA_SERVICE_MAYBE_OWNING_DEVICE_MEMORY_H_\n #define XLA_SERVICE_MAYBE_OWNING_DEVICE_MEMORY_H_\n \n+#include <cstdint>\n #include <optional>\n #include <utility>\n #include <variant>\n \n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n+#include \"xla/stream_executor/device_memory.h\"  // IWYU pragma: keep\n+#include \"xla/stream_executor/device_memory_allocator.h\"  // IWYU pragma: keep\n \n namespace xla {\n \n-// MaybeOwningDeviceMemory represents either an owned or unowned device memory.\n-// Like std::variant<se::OwningDeviceMemory, DeviceMemory>. When the object goes\n-// output of scope, it will free the underlying memory if it owns it.\n+// MaybeOwningDeviceMemory represents either an owned or unowned\n+// device memory. Like std::variant<se::ScopedDeviceAddress<uint8_t>,\n+// DeviceMemory>. When the object goes output of scope, it will free the\n+// underlying memory if it owns it.\n class MaybeOwningDeviceMemory {\n  public:\n   MaybeOwningDeviceMemory() = default;\n   ~MaybeOwningDeviceMemory() = default;\n \n-  explicit MaybeOwningDeviceMemory(stream_executor::OwningDeviceMemory owned)\n+  explicit MaybeOwningDeviceMemory(\n+      stream_executor::ScopedDeviceAddress<uint8_t> owned)\n       : mem_(std::move(owned)) {}\n \n-  explicit MaybeOwningDeviceMemory(stream_executor::DeviceMemoryBase unowned)\n+  explicit MaybeOwningDeviceMemory(stream_executor::DeviceAddressBase unowned)\n       : mem_(unowned) {}\n \n   MaybeOwningDeviceMemory(MaybeOwningDeviceMemory&&) = default;\n \n   MaybeOwningDeviceMemory& operator=(\n-      stream_executor::DeviceMemoryBase unowned) {\n+      stream_executor::DeviceAddressBase unowned) {\n     mem_ = unowned;\n     return *this;\n   }\n \n   MaybeOwningDeviceMemory& operator=(\n-      stream_executor::OwningDeviceMemory owned) {\n+      stream_executor::ScopedDeviceAddress<uint8_t> owned) {\n     mem_ = std::move(owned);\n     return *this;\n   }\n \n   MaybeOwningDeviceMemory& operator=(MaybeOwningDeviceMemory&&) = default;\n \n-  // Fetches the underlying DeviceMemoryBase from a MaybeOwningDeviceMemory. The\n-  // caller of this function is *not* responsible for freeing the memory.\n-  stream_executor::DeviceMemoryBase AsDeviceMemoryBase() const;\n+  // Fetches the underlying DeviceAddressBase from a\n+  // MaybeOwningDeviceMemory. The caller of this function is *not*\n+  // responsible for freeing the memory.\n+  stream_executor::DeviceAddressBase AsDeviceMemoryBase() const;\n \n-  // Release the stream_executor::OwningDeviceMemory without freeing it, and\n-  // moves the ownership of the memory buffer from the object to the caller.\n+  // Release the stream_executor::ScopedDeviceAddress<uint8_t> without freeing\n+  // it, and moves the ownership of the memory buffer from the object to the\n+  // caller.\n   //\n   // A nullopt is returned if the HasOwnership() == false;\n-  std::optional<stream_executor::OwningDeviceMemory> Release();\n+  std::optional<stream_executor::ScopedDeviceAddress<uint8_t>> Release();\n \n   // If the device memory is owned, returns a pointer to the internal\n   // OwningDeviceMemory, otherwise nullptr is returned.\n-  const stream_executor::OwningDeviceMemory* AsOwningDeviceMemory() const;\n+  const stream_executor::ScopedDeviceAddress<uint8_t>* AsOwningDeviceMemory()\n+      const;\n \n   // Returns true if the device_memory has ownership over underlying memory.\n   bool HasOwnership() const;\n \n  private:\n-  std::variant<stream_executor::DeviceMemoryBase,\n-               stream_executor::OwningDeviceMemory>\n+  std::variant<stream_executor::DeviceAddressBase,\n+               stream_executor::ScopedDeviceAddress<uint8_t>>\n       mem_;\n };\n "
        },
        {
            "sha": "98a08aaaf3c7cc73bdddcf762f956559dfe400ec",
            "filename": "third_party/xla/xla/service/service.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -61,7 +61,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\""
        },
        {
            "sha": "4d91b5d0032aaf3d328bc66cd2de6349f3b44feb",
            "filename": "third_party/xla/xla/service/service.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fservice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fservice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fservice.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/types.h\"\n #include \"xla/xla.pb.h\""
        },
        {
            "sha": "b50fae7886c1d7c09873f9bb9a0c9447e1d1bd83",
            "filename": "third_party/xla/xla/service/shaped_buffer.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -27,8 +27,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -82,16 +82,16 @@ absl::StatusOr<ShapedBuffer> ShapedBuffer::SubShapedBuffer(\n                       ShapeUtil::TryGetSubshape(on_device_shape(), index));\n   ShapedBuffer sub_shaped_buffer(*device_sub_shape, device_ordinal_,\n                                  physical_device_ordinal_);\n-  TF_ASSIGN_OR_RETURN(ShapeTree<se::DeviceMemoryBase> sub_buffers,\n+  TF_ASSIGN_OR_RETURN(ShapeTree<se::DeviceAddressBase> sub_buffers,\n                       buffers_.SubShapeTree(index));\n   sub_shaped_buffer.set_buffers(std::move(sub_buffers));\n   return std::move(sub_shaped_buffer);\n }\n \n void ShapedBuffer::clear() {\n   for (auto& pair : buffers_) {\n-    // A default constructed DeviceMemoryBase is a null pointer.\n-    pair.second = se::DeviceMemoryBase();\n+    // A default constructed DeviceAddressBase is a null pointer.\n+    pair.second = se::DeviceAddressBase();\n   }\n }\n \n@@ -110,7 +110,7 @@ std::string ShapedBuffer::ToString() const {\n         } else {\n           shape_str = ShapeUtil::HumanStringWithLayout(subshape);\n         }\n-        const se::DeviceMemoryBase& memory = buffer(index);\n+        const se::DeviceAddressBase& memory = buffer(index);\n         absl::StrAppendFormat(&s, \"  %s%p (%d bytes) : %s\\n\",\n                               std::string(index.size() * 2, ' '),\n                               memory.opaque(), memory.size(), shape_str);\n@@ -124,7 +124,7 @@ std::ostream& operator<<(std::ostream& out, const ShapedBuffer& buffer) {\n }\n \n ScopedShapedBuffer::ScopedShapedBuffer(Shape on_device_shape,\n-                                       se::DeviceMemoryAllocator* allocator,\n+                                       se::DeviceAddressAllocator* allocator,\n                                        int device_ordinal,\n                                        int physical_device_ordinal)\n     : ShapedBuffer(std::move(on_device_shape), device_ordinal,\n@@ -133,14 +133,14 @@ ScopedShapedBuffer::ScopedShapedBuffer(Shape on_device_shape,\n \n ScopedShapedBuffer::ScopedShapedBuffer(Shape on_host_shape,\n                                        Shape on_device_shape,\n-                                       se::DeviceMemoryAllocator* allocator,\n+                                       se::DeviceAddressAllocator* allocator,\n                                        int device_ordinal,\n                                        int physical_device_ordinal)\n     : ScopedShapedBuffer(std::move(on_device_shape), allocator, device_ordinal,\n                          physical_device_ordinal) {}\n \n ScopedShapedBuffer::ScopedShapedBuffer(ShapedBuffer shaped_buffer,\n-                                       se::DeviceMemoryAllocator* allocator)\n+                                       se::DeviceAddressAllocator* allocator)\n     : ShapedBuffer(std::move(shaped_buffer)), allocator_(allocator) {}\n \n ScopedShapedBuffer::ScopedShapedBuffer(ScopedShapedBuffer&& s) noexcept\n@@ -164,7 +164,7 @@ ScopedShapedBuffer::~ScopedShapedBuffer() { Deallocate(); }\n \n ShapedBuffer ScopedShapedBuffer::release() {\n   ShapedBuffer shaped_buffer(static_cast<ShapedBuffer&&>(*this));\n-  buffers_ = ShapeTree<se::DeviceMemoryBase>();\n+  buffers_ = ShapeTree<se::DeviceAddressBase>();\n   return shaped_buffer;\n }\n \n@@ -178,7 +178,7 @@ void ScopedShapedBuffer::Deallocate() {\n   // has been deallocated.\n   absl::flat_hash_set<void*> deallocated_ptrs;\n   for (auto& pair : buffers_) {\n-    se::DeviceMemoryBase& memory_base = pair.second;\n+    se::DeviceAddressBase& memory_base = pair.second;\n     if (!memory_base.is_null() &&\n         deallocated_ptrs.insert(memory_base.opaque()).second) {\n       CHECK_OK(allocator_->Deallocate(device_ordinal(), memory_base));\n@@ -196,7 +196,7 @@ ScopedShapedBuffer ScopedShapedBuffer::TakeSubTree(ShapeIndexView index) {\n   auto dst_it = output.buffers().begin();\n   while (dst_it != output.buffers().end()) {\n     dst_it->second = src_it->second;\n-    src_it->second = tensorflow::se::DeviceMemoryBase(nullptr, 0);\n+    src_it->second = tensorflow::se::DeviceAddressBase(nullptr, 0);\n     ++src_it;\n     ++dst_it;\n   }"
        },
        {
            "sha": "6a3ceff3aacab8690e7d851048985231eba040c6",
            "filename": "third_party/xla/xla/service/shaped_buffer.h",
            "status": "modified",
            "additions": 23,
            "deletions": 20,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_SERVICE_SHAPED_BUFFER_H_\n #define XLA_SERVICE_SHAPED_BUFFER_H_\n \n+#include <cstdint>\n #include <optional>\n #include <ostream>\n #include <string>\n@@ -26,8 +27,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -38,11 +39,11 @@ class ScopedShapedBuffer;\n // particular XLA shape.\n class ShapedBuffer {\n  public:\n-  // Construct a ShapedBuffer with null DeviceMemoryBases at each index. The\n+  // Construct a ShapedBuffer with null DeviceAddressBases at each index. The\n   // shape of the data on the host and the device may differ because the device\n   // may have a different representation for different data types. Therefore,\n   // both the on-host and on-device shape are required. The on-device shape\n-  // determines the number of device allocations (DeviceMemoryBase) held by the\n+  // determines the number of device allocations (DeviceAddressBase) held by the\n   // ShapedBuffer.\n   // Specify `physical_device_ordinal` if multiple devices share the same\n   // physical device, e.g., virtual GPUs.\n@@ -79,25 +80,26 @@ class ShapedBuffer {\n   int physical_device_ordinal() const { return physical_device_ordinal_; }\n \n   // Return the root buffer of the shape (shape index {}).\n-  const se::DeviceMemoryBase& root_buffer() const {\n+  const se::DeviceAddressBase& root_buffer() const {\n     return buffer(/*index=*/{});\n   }\n \n   // Returns the buffer at the given shape index where index is defined as in\n   // ShapeUtil::GetSubshape.\n-  const se::DeviceMemoryBase& buffer(const ShapeIndex& index) const {\n+  const se::DeviceAddressBase& buffer(const ShapeIndex& index) const {\n     return buffers_.element(index);\n   }\n \n   // Sets the device memory buffer at the given index.\n-  void set_buffer(const se::DeviceMemoryBase& buffer, const ShapeIndex& index) {\n+  void set_buffer(const se::DeviceAddressBase& buffer,\n+                  const ShapeIndex& index) {\n     *buffers_.mutable_element(index) = buffer;\n   }\n \n   // Sets all buffers.\n   //\n   // Precondition: buffers.shape == on_device_shape_\n-  void set_buffers(ShapeTree<se::DeviceMemoryBase> buffers) {\n+  void set_buffers(ShapeTree<se::DeviceAddressBase> buffers) {\n     CHECK(ShapeUtil::Equal(buffers.shape(), on_device_shape_));\n     buffers_ = std::move(buffers);\n     buffers_.replace_shape_ptr(on_device_shape_);\n@@ -125,8 +127,8 @@ class ShapedBuffer {\n \n   // Returns the underlying ShapeTree containing all the device addresses in the\n   // ShapedBuffer.\n-  const ShapeTree<se::DeviceMemoryBase>& buffers() const { return buffers_; }\n-  ShapeTree<se::DeviceMemoryBase>& buffers() { return buffers_; }\n+  const ShapeTree<se::DeviceAddressBase>& buffers() const { return buffers_; }\n+  ShapeTree<se::DeviceAddressBase>& buffers() { return buffers_; }\n \n   absl::StatusOr<ShapedBuffer> SubShapedBuffer(const ShapeIndex& index) const;\n \n@@ -148,7 +150,7 @@ class ShapedBuffer {\n   int physical_device_ordinal_;\n \n   // The tree of device buffers. Its shape is on_device_shape().\n-  ShapeTree<se::DeviceMemoryBase> buffers_;\n+  ShapeTree<se::DeviceAddressBase> buffers_;\n };\n \n std::ostream& operator<<(std::ostream& out, const ShapedBuffer& buffer);\n@@ -159,25 +161,25 @@ std::ostream& operator<<(std::ostream& out, const ShapedBuffer& buffer);\n // TODO(timshen): Remove inheritance between ScopedShapedBuffer and\n // ShapedBuffer.  There should never be a need to consider a ScopedShapedBuffer\n // as a ShapedBuffer, because in that case we should just be able to pass around\n-// our ShapeTree<DeviceMemoryBase>.  Inheritance only adds complexity.  See\n+// our ShapeTree<DeviceAddressBase>.  Inheritance only adds complexity.  See\n // discussion in cl/192849370.\n class ScopedShapedBuffer : public ShapedBuffer {\n  public:\n-  // Creates a ScopedShapedBuffer with null DeviceMemoryBases at each index.\n+  // Creates a ScopedShapedBuffer with null DeviceAddressBases at each index.\n   explicit ScopedShapedBuffer(Shape on_device_shape,\n-                              se::DeviceMemoryAllocator* allocator,\n+                              se::DeviceAddressAllocator* allocator,\n                               int device_ordinal,\n                               int physical_device_ordinal = -1);\n   // TODO(b/170310047): remove this overload.\n   explicit ScopedShapedBuffer(Shape on_host_shape, Shape on_device_shape,\n-                              se::DeviceMemoryAllocator* allocator,\n+                              se::DeviceAddressAllocator* allocator,\n                               int device_ordinal,\n                               int physical_device_ordinal = -1);\n \n   // Create a ScopedShapedBuffer by taking over the memory from the incoming\n   // ShapedBuffer.\n   explicit ScopedShapedBuffer(ShapedBuffer shaped_buffer,\n-                              se::DeviceMemoryAllocator* allocator);\n+                              se::DeviceAddressAllocator* allocator);\n \n   // Movable, but not copyable.\n   ScopedShapedBuffer(ScopedShapedBuffer&& s) noexcept;\n@@ -190,19 +192,20 @@ class ScopedShapedBuffer : public ShapedBuffer {\n \n   // Return the allocator used to allocate the device memory held in this\n   // ScopedShapedBuffer.\n-  se::DeviceMemoryAllocator* memory_allocator() const { return allocator_; }\n+  se::DeviceAddressAllocator* memory_allocator() const { return allocator_; }\n \n   // Sets the device memory buffer at the given index.\n   //\n   // If the given buffer's device memory is non-null, its device_ordinal and\n   // allocator must match those in `this`.\n-  void set_buffer(se::OwningDeviceMemory buffer, const ShapeIndex& index) {\n+  void set_buffer(se::ScopedDeviceAddress<uint8_t> buffer,\n+                  const ShapeIndex& index) {\n     if (!buffer.is_null()) {\n       CHECK_EQ(buffer.device_ordinal(), device_ordinal());\n       CHECK_EQ(buffer.allocator(), allocator_);\n       *buffers_.mutable_element(index) = buffer.Release();\n     } else {\n-      *buffers_.mutable_element(index) = se::DeviceMemoryBase();\n+      *buffers_.mutable_element(index) = se::DeviceAddressBase();\n     }\n   }\n \n@@ -220,7 +223,7 @@ class ScopedShapedBuffer : public ShapedBuffer {\n  protected:\n   void Deallocate();\n \n-  se::DeviceMemoryAllocator* allocator_;\n+  se::DeviceAddressAllocator* allocator_;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "4d3828a55eb56e560bd0e68ae6e299e237f2442c",
            "filename": "third_party/xla/xla/service/shaped_buffer_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer_test.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -27,8 +27,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -51,10 +51,11 @@ TEST(ShapedBufferTest, ScopedShapeBufferAsShapedBufferB71629047) {\n   buffer = nullptr;\n }\n \n-class TestAllocator : public se::DeviceMemoryAllocator {\n+class TestAllocator : public se::DeviceAddressAllocator {\n  public:\n   TestAllocator()\n-      : se::DeviceMemoryAllocator(PlatformUtil::GetDefaultPlatform().value()) {}\n+      : se::DeviceAddressAllocator(PlatformUtil::GetDefaultPlatform().value()) {\n+  }\n \n   ~TestAllocator() override {\n     if (!allocations_.empty()) {\n@@ -63,23 +64,23 @@ class TestAllocator : public se::DeviceMemoryAllocator {\n   }\n \n   // Pull in two-arg overload of Allocate.\n-  using se::DeviceMemoryAllocator::Allocate;\n+  using se::DeviceAddressAllocator::Allocate;\n \n-  absl::StatusOr<se::OwningDeviceMemory> Allocate(\n+  absl::StatusOr<se::ScopedDeviceAddress<uint8_t>> Allocate(\n       int device_ordinal, uint64_t size, bool /*retry_on_failure*/,\n       int64_t /*memory_space*/) override {\n     // By contract, we must return null if size == 0.\n     if (size == 0) {\n-      return se::OwningDeviceMemory();\n+      return se::ScopedDeviceAddress<uint8_t>();\n     }\n     void* buf = malloc(size);\n     allocations_.insert({device_ordinal, buf});\n-    return se::OwningDeviceMemory(se::DeviceMemoryBase(buf, size),\n-                                  device_ordinal, this);\n+    return se::ScopedDeviceAddress<uint8_t>(se::DeviceAddressBase(buf, size),\n+                                            device_ordinal, this);\n   }\n \n   absl::Status Deallocate(int device_ordinal,\n-                          se::DeviceMemoryBase mem) override {\n+                          se::DeviceAddressBase mem) override {\n     if (mem.is_null()) {\n       return absl::OkStatus();\n     }\n@@ -129,28 +130,28 @@ TEST(ScopedShapedBufferTest, TestTakeSubTree) {\n \n   ScopedShapedBuffer sb(s, &allocator, /*device_ordinal=*/0);\n   sb.buffers().ForEachMutableElement(\n-      [&](const xla::ShapeIndex& index, se::DeviceMemoryBase* buffer) {\n+      [&](const xla::ShapeIndex& index, se::DeviceAddressBase* buffer) {\n         TF_ASSERT_OK_AND_ASSIGN(\n-            se::OwningDeviceMemory m,\n+            se::ScopedDeviceAddress<uint8_t> m,\n             allocator.Allocate(/*device_ordinal=*/0, /*size=*/77));\n         *buffer = m.Release();\n       });\n-  ShapeTree<se::DeviceMemoryBase> buffers = sb.buffers();\n+  ShapeTree<se::DeviceAddressBase> buffers = sb.buffers();\n \n   // Takes a subtree out of 'sb', and verifies the buffers are as expected.\n   xla::ShapeIndex subtree_index = {1};\n   ScopedShapedBuffer output = sb.TakeSubTree(subtree_index);\n \n   output.buffers().ForEachElement([&](const xla::ShapeIndex& sub_index,\n-                                      const se::DeviceMemoryBase& buffer) {\n+                                      const se::DeviceAddressBase& buffer) {\n     xla::ShapeIndex orig_index = subtree_index;\n     for (int i : sub_index) {\n       orig_index.push_back(i);\n     }\n     EXPECT_TRUE(buffers.find(orig_index)->second.IsSameAs(buffer));\n   });\n   sb.buffers().ForEachElement([&](const xla::ShapeIndex& index,\n-                                  const se::DeviceMemoryBase& buffer) {\n+                                  const se::DeviceAddressBase& buffer) {\n     if ((index.size() >= subtree_index.size()) &&\n         ShapeIndexView(index).first(subtree_index.size()) == subtree_index) {\n       EXPECT_TRUE(buffer.is_null());\n@@ -167,9 +168,9 @@ TEST(ScopedShapedBufferTest, TestSubShapeTree) {\n   TestAllocator allocator;\n   ScopedShapedBuffer sb(tuple_shape, &allocator, /*device_ordinal=*/0);\n   sb.buffers().ForEachMutableElement(\n-      [&](const xla::ShapeIndex& index, se::DeviceMemoryBase* buffer) {\n+      [&](const xla::ShapeIndex& index, se::DeviceAddressBase* buffer) {\n         TF_ASSERT_OK_AND_ASSIGN(\n-            se::OwningDeviceMemory m,\n+            se::ScopedDeviceAddress<uint8_t> m,\n             allocator.Allocate(/*device_ordinal=*/0, /*size=*/32));\n         *buffer = m.Release();\n       });"
        },
        {
            "sha": "da4264b9cb302e82688af3da15839bbd569f9a1a",
            "filename": "third_party/xla/xla/service/transfer_manager.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Ftransfer_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Ftransfer_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Ftransfer_manager.cc?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -104,7 +104,7 @@ absl::Status TransferManager::TransferLiteralToDevice(\n }\n \n absl::StatusOr<Literal> TransferManager::TransferArrayFromDevice(\n-    se::Stream* stream, const Shape& shape, const se::DeviceMemoryBase& source,\n+    se::Stream* stream, const Shape& shape, const se::DeviceAddressBase& source,\n     const TransferMetadata* transfer_metadata) {\n   TF_RET_CHECK(shape.IsArray());\n   TF_RET_CHECK(Shape::Equal().MinorToMajorOnlyInLayout()(\n@@ -119,7 +119,7 @@ absl::StatusOr<Literal> TransferManager::TransferArrayFromDevice(\n \n absl::Status TransferManager::TransferArrayToDevice(\n     se::Stream* stream, const LiteralSlice& literal,\n-    const se::DeviceMemoryBase& dest,\n+    const se::DeviceAddressBase& dest,\n     const TransferMetadata* transfer_metadata) {\n   // Implement the synchronous version by waiting on the asynchronous version.\n   // Use a substream so that if we are called from a HostCallback we don't\n@@ -134,7 +134,7 @@ absl::Status TransferManager::TransferArrayToDevice(\n \n absl::Status TransferManager::TransferArrayToDeviceAsync(\n     se::Stream* stream, const LiteralSlice& literal,\n-    const se::DeviceMemoryBase& dest,\n+    const se::DeviceAddressBase& dest,\n     const TransferMetadata* transfer_metadata) {\n   TF_RET_CHECK(literal.shape().IsArray());\n   ShapedBuffer shaped_buffer(HostShapeToDeviceShape(literal.shape()),\n@@ -155,7 +155,7 @@ absl::Status TransferManager::ReadDynamicShapes(\n       auto compiler, Compiler::GetForPlatform(stream->parent()->GetPlatform()));\n   TF_RETURN_IF_ERROR(device_buffer->buffers().ForEachElementWithStatus(\n       [&](const ShapeIndex& index,\n-          const se::DeviceMemoryBase& buffer) -> absl::Status {\n+          const se::DeviceAddressBase& buffer) -> absl::Status {\n         const Shape& buffer_shape =\n             ShapeUtil::GetSubshape(*device_shape, index);\n         if (buffer_shape.IsTuple()) {\n@@ -176,7 +176,7 @@ absl::Status TransferManager::ReadDynamicShapes(\n         if (metadata_size == 0) {\n           return InvalidArgument(\"Dynamic shape metadata size should not be 0\");\n         }\n-        auto buffer_8 = se::DeviceMemory<uint8_t>(buffer);\n+        auto buffer_8 = se::DeviceAddress<uint8_t>(buffer);\n         auto metadata_buffer = buffer_8.GetSlice(offset, metadata_size);\n         TF_ASSIGN_OR_RETURN(\n             auto metadata,\n@@ -246,11 +246,11 @@ absl::Status TransferManager::WriteTupleIndexTablesAsync(\n           const ShapeIndex& index) -> absl::Status {\n         if (device_subshape.IsTuple() &&\n             ShapeUtil::TupleElementCount(device_subshape) > 0) {\n-          se::DeviceMemoryBase device_memory = device_buffer.buffer(index);\n+          se::DeviceAddressBase device_memory = device_buffer.buffer(index);\n           TF_RET_CHECK(GetByteSizeRequirement(device_subshape) ==\n                        device_memory.size());\n \n-          std::vector<se::DeviceMemoryBase> elements;\n+          std::vector<se::DeviceAddressBase> elements;\n           ShapeIndex element_index = index;\n           for (int64_t i = 0; i < ShapeUtil::TupleElementCount(device_subshape);\n                ++i) {\n@@ -272,11 +272,11 @@ absl::Status TransferManager::WriteRootTupleIndexTable(\n   if (ShapeUtil::TupleElementCount(device_buffer.on_device_shape()) == 0) {\n     return absl::OkStatus();\n   }\n-  se::DeviceMemoryBase device_memory = device_buffer.buffer({});\n+  se::DeviceAddressBase device_memory = device_buffer.buffer({});\n   TF_RET_CHECK(GetByteSizeRequirement(device_buffer.on_device_shape()) ==\n                device_memory.size());\n \n-  std::vector<se::DeviceMemoryBase> elements;\n+  std::vector<se::DeviceAddressBase> elements;\n   elements.reserve(\n       ShapeUtil::TupleElementCount(device_buffer.on_device_shape()));\n   for (int64_t i = 0;\n@@ -293,12 +293,12 @@ absl::Status TransferManager::WriteRootTupleIndexTable(\n   if (ShapeUtil::TupleElementCount(buffer_tree.shape()) == 0) {\n     return absl::OkStatus();\n   }\n-  se::DeviceMemoryBase device_memory =\n+  se::DeviceAddressBase device_memory =\n       buffer_tree.element({}).AsDeviceMemoryBase();\n   TF_RET_CHECK(GetByteSizeRequirement(buffer_tree.shape()) ==\n                device_memory.size());\n \n-  std::vector<se::DeviceMemoryBase> elements;\n+  std::vector<se::DeviceAddressBase> elements;\n   elements.reserve(ShapeUtil::TupleElementCount(buffer_tree.shape()));\n   for (int64_t i = 0; i < ShapeUtil::TupleElementCount(buffer_tree.shape());\n        ++i) {\n@@ -309,7 +309,7 @@ absl::Status TransferManager::WriteRootTupleIndexTable(\n }\n \n absl::StatusOr<ScopedShapedBuffer> TransferManager::AllocateScopedShapedBuffer(\n-    const Shape& on_host_shape, se::DeviceMemoryAllocator* allocator,\n+    const Shape& on_host_shape, se::DeviceAddressAllocator* allocator,\n     int device_ordinal, int physical_device_ordinal,\n     DeviceShapeRepresentationFn shape_representation_fn) {\n   if (!LayoutUtil::HasLayout(on_host_shape)) {\n@@ -329,7 +329,7 @@ absl::StatusOr<ScopedShapedBuffer> TransferManager::AllocateScopedShapedBuffer(\n   // including the tuple pointer arrays.\n   for (auto& pair : shaped_buffer.buffers()) {\n     const ShapeIndex& index = pair.first;\n-    se::DeviceMemoryBase& memory_base = pair.second;\n+    se::DeviceAddressBase& memory_base = pair.second;\n     const Shape& subshape =\n         ShapeUtil::GetSubshape(shaped_buffer.on_device_shape(), index);\n     TF_ASSIGN_OR_RETURN(auto memory,"
        },
        {
            "sha": "978bcfc523bfc1371ca12237dc150e257754eeb2",
            "filename": "third_party/xla/xla/service/transfer_manager.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Ftransfer_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15003e01ed816a2e0d669c9104d3751c87162eb6/third_party%2Fxla%2Fxla%2Fservice%2Ftransfer_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Ftransfer_manager.h?ref=15003e01ed816a2e0d669c9104d3751c87162eb6",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -163,17 +163,17 @@ class TransferManager {\n   // tells the actual implementation to do something special.\n   absl::Status TransferArrayToDevice(\n       se::Stream* stream, const LiteralSlice& literal,\n-      const se::DeviceMemoryBase& dest,\n+      const se::DeviceAddressBase& dest,\n       const TransferMetadata* transfer_metadata = nullptr);\n \n   absl::Status TransferArrayToDeviceAsync(\n       se::Stream* stream, const LiteralSlice& literal,\n-      const se::DeviceMemoryBase& dest,\n+      const se::DeviceAddressBase& dest,\n       const TransferMetadata* transfer_metadata = nullptr);\n \n   absl::StatusOr<Literal> TransferArrayFromDevice(\n       se::Stream* stream, const Shape& shape,\n-      const se::DeviceMemoryBase& source,\n+      const se::DeviceAddressBase& source,\n       const TransferMetadata* transfer_metadata = nullptr);\n \n   // Read from a device buffer and update the dynamic dimension sizes of\n@@ -242,12 +242,12 @@ class TransferManager {\n   // shape. The on-device shape may be different as indicated by\n   // HostShapeToDeviceShape.\n   absl::StatusOr<ScopedShapedBuffer> AllocateScopedShapedBuffer(\n-      const Shape& on_host_shape, se::DeviceMemoryAllocator* allocator,\n+      const Shape& on_host_shape, se::DeviceAddressAllocator* allocator,\n       int device_ordinal, int physical_device_ordinal,\n       DeviceShapeRepresentationFn shape_representation_fn = nullptr);\n \n   absl::StatusOr<ScopedShapedBuffer> AllocateScopedShapedBuffer(\n-      const Shape& on_host_shape, se::DeviceMemoryAllocator* allocator,\n+      const Shape& on_host_shape, se::DeviceAddressAllocator* allocator,\n       int device_ordinal,\n       DeviceShapeRepresentationFn shape_representation_fn = nullptr) {\n     return AllocateScopedShapedBuffer(on_host_shape, allocator, device_ordinal,\n@@ -277,7 +277,7 @@ class TransferManager {\n   // Equivalent to CanShapedBufferBeAccessedNow but for a single device buffer.\n   virtual bool CanBufferBeAccessedNow(\n       se::StreamExecutor* executor,\n-      const se::DeviceMemoryBase& device_buffer) const {\n+      const se::DeviceAddressBase& device_buffer) const {\n     return false;\n   }\n \n@@ -303,8 +303,8 @@ class TransferManager {\n   // to construct a tuple index table in the platform-specific tuple\n   // representation.\n   virtual absl::Status WriteSingleTupleIndexTable(\n-      se::Stream* stream, absl::Span<const se::DeviceMemoryBase> elements,\n-      const Shape& shape, se::DeviceMemoryBase* region) = 0;\n+      se::Stream* stream, absl::Span<const se::DeviceAddressBase> elements,\n+      const Shape& shape, se::DeviceAddressBase* region) = 0;\n \n   // Returns whether subbyte types (types less than 1 byte, e.g. U4) should\n   // have multiple values packed into a single byte on the device. Subbyte"
        }
    ],
    "stats": {
        "total": 1129,
        "additions": 575,
        "deletions": 554
    }
}