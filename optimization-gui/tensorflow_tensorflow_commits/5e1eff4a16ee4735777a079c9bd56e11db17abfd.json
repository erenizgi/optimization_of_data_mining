{
    "author": "metaflow",
    "message": "[XLA:GPU] doc updates to experimental tiling\n\nPiperOrigin-RevId: 817064107",
    "sha": "5e1eff4a16ee4735777a079c9bd56e11db17abfd",
    "files": [
        {
            "sha": "cfc42792a1b906c1dae626e4070e54c7b8e490b3",
            "filename": "third_party/xla/xla/service/gpu/model/experimental/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2FBUILD?ref=5e1eff4a16ee4735777a079c9bd56e11db17abfd",
            "patch": "@@ -31,6 +31,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/codegen/tiling:constraint_expression\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n+        \"//xla/hlo/analysis:interval\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "51043507dba87614473c0338438667dde6998334",
            "filename": "third_party/xla/xla/service/gpu/model/experimental/symbolic_tile.h",
            "status": "modified",
            "additions": 47,
            "deletions": 28,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Fsymbolic_tile.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Fsymbolic_tile.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Fsymbolic_tile.h?ref=5e1eff4a16ee4735777a079c9bd56e11db17abfd",
            "patch": "@@ -30,51 +30,69 @@ namespace xla::gpu::experimental {\n \n class TilingSpace;\n \n-// A map from tile IDs, sizes and runtime variables to tile's offsets, sizes,\n-// strides and upper bounds. Offsets-sizes-strides define what slice to extract,\n-// upper bounds define masking, i.e. if the tile attempts to extract elements\n-// with the indices outside of the bounds, the tile will be masked.\n+// Tiling of a single dimension.\n //\n-// (tile IDs) [tile sizes] {runtime variables} ->\n-//     offsets [offsets_]  sizes [sizes_] strides [strides_]\n-//     upper bounds [upper_bounds_]\n+// Offsets, sizes and strides define the slice of the tensor dimension. Upper\n+// bounds set the range [0, upper_bound), values outside of this range are\n+// masked.\n //\n-// tile IDs correspond to the dimension variables of the affine expressions;\n-// tile sizes and RT vars correspond to the symbol variables.\n+// Expressions for offset, size, stride and upper bound are AffineExpr\n+// functions. The TilingSpace keeps track of all dimensions and symbols we use\n+// in the expressions and allows to create a consistent mapping from dimensions\n+// and runtime variables to affine expression dimensions and symbols.\n //\n-// The masking condition of the upper bound can be written as:\n-// dimension_index < upper_bounds[i](tile IDs)\n+// N.B.! not all of the symbols that the TilingSpace defines are used in\n+// every expression. That depends on the position of the instruction in\n+// the graph and the traversal path that we took.\n+// Number of dimensions equals to the number of dimensions of the instruction\n+// output, parallel dimensions the corresponding root instruction are followed\n+// by sequential dimensions.\n //\n-// In most of the cases, the upper bounds will coincide with the shape of the\n-// tensor from which the tile is extracted.\n-//\n-// One example when upper bound does not match the shape is a reshape:\n-// output = s32[2, 17] reshape (s32[34] input)\n-//\n-// If we propagate the `output` tile with the ts0 == 1,\n-//\n-// (tid0, tid1)[ts1] -> offsets [tid0, tid1 * ts1] sizes [1, ts1] strides [1, 1]\n-//              upper bounds [2, 17]\n-//\n-// to the `input` we will get a stricter upper bound\n-//\n-// (tid0, tid1)[ts1] -> offsets [17 * tid0 + tid1 * ts1] sizes [ts1] strides [1]\n-//              upper bounds [17 * tid0]\n+// Symbols are:\n+//  - tile sizes of all dimensions, followed by\n+//  - runtime variables.\n struct DimTile {\n   bool operator==(const DimTile& other) const;\n \n   mlir::AffineExpr offset;\n   mlir::AffineExpr size;\n   mlir::AffineExpr stride;\n+  // The masking condition of the upper bound can be written as:\n+  // dimension_index < upper_bounds(tile IDs)[tile sizes]{runtime variables}\n+  //\n+  // In most of the cases, the upper bounds will coincide with the shape of the\n+  // tensor from which the tile is extracted. One example when upper bound does\n+  // not match the shape is a reshape:\n+  //\n+  // output = s32[2, 17] reshape (s32[34] input)\n+  //\n+  // If we propagate the `output` SymbolicTile with the tile size of first\n+  // dimension equal to 1\n+  //\n+  // (tid0, tid1)[ts1] -> offsets [tid0, tid1 * ts1]\n+  //                      sizes [1, ts1]\n+  //                      strides [1, 1]\n+  //                      upper bounds [2, 17]\n+  //\n+  // then for to the `input` instruction we will get a stricter upper bound\n+  //\n+  // (tid0, tid1)[ts1] -> offsets [17 * tid0 + tid1 * ts1]\n+  //                      sizes [ts1]\n+  //                      strides [1]\n+  //                      upper bounds [17 * tid0]\n   mlir::AffineExpr upper_bound;\n };\n+\n template <typename H>\n H AbslHashValue(H h, const DimTile& dim_tile) {\n   llvm::hash_code dim_tile_hash = llvm::hash_combine(\n       dim_tile.offset, dim_tile.size, dim_tile.stride, dim_tile.upper_bound);\n   return H::combine(std::move(h), static_cast<size_t>(dim_tile_hash));\n }\n \n+// SymbolicTile is a collection of tilings for every dimension of output tensor\n+// of an HLO instruction. SymbolicTiledHloInstruction associates a SymbolicTile\n+// with an HLO instruction.\n class SymbolicTile {\n  public:\n   SymbolicTile(const TilingSpace& tiling_space,\n@@ -120,8 +138,9 @@ H AbslHashValue(H h, const SymbolicTile& symbolic_tile) {\n   return h;\n }\n \n-// Returns a DimTile that covers the entire dimension, i.e.\n-// offset 0, size = next_power_of_2(dim_size), stride 1, upper_bound = dim_size.\n+// Returns a DimTile that covers the entire dimension with a single power of 2\n+// sized tile, i.e. offset 0, size = next_power_of_2(dim_size), stride 1,\n+// upper_bound = dim_size.\n DimTile GetFullDimTile(int64_t dim_size, mlir::MLIRContext* ctx);\n \n // Returns a DimTile that covers the entire dimension, i.e."
        },
        {
            "sha": "05aa7dd0ae1b20c03de4f8e3656808048842c9c9",
            "filename": "third_party/xla/xla/service/gpu/model/experimental/tiling_space.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 17,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Ftiling_space.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Ftiling_space.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Ftiling_space.cc?ref=5e1eff4a16ee4735777a079c9bd56e11db17abfd",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/IR/MLIRContext.h\"\n-#include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/analysis/interval.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -65,6 +65,23 @@ void TilingSpace::AppendRTVar(const HloInstruction* hlo, int64_t operand_id,\n   hlo_to_rt_var_[std::make_pair(hlo, operand_id)] = &rt_vars_.back();\n }\n \n+void TilingSpace::ProcessInstruction(const HloInstruction& hlo) {\n+  switch (hlo.opcode()) {\n+    case HloOpcode::kDot:\n+      ProcessDot(hlo);\n+      break;\n+    case HloOpcode::kReduce:\n+      ProcessReduce(hlo);\n+      break;\n+    case HloOpcode::kDynamicSlice:\n+      ProcessDynamicSlice(hlo);\n+      break;\n+    default:\n+      // TODO(goncharov): should have a explicit list of supported instructions?\n+      break;\n+  }\n+}\n+\n // Add dot contraction dimensions in the order of contracting dimensions.\n void TilingSpace::ProcessDot(const HloInstruction& hlo) {\n   auto dot = Cast<HloDotInstruction>(&hlo);\n@@ -143,15 +160,17 @@ const TilingSpace::DimensionInfo& TilingSpace::GetDimensionInfo(\n     const HloInstruction& hlo, int64_t dim_position) const {\n   auto it = hlo_to_dimension_.find(std::make_pair(&hlo, dim_position));\n   CHECK(it != hlo_to_dimension_.end())\n-      << \"Dimension not found: \" << hlo.ToString() << \" \" << dim_position;\n+      << \"Dimension not found for \" << hlo.ToString() << \" dimension \"\n+      << dim_position;\n   return *it->second;\n }\n \n const TilingSpace::RTVarInfo& TilingSpace::GetRTVarInfo(\n     const HloInstruction& hlo, int64_t operand_id) const {\n   auto it = hlo_to_rt_var_.find(std::make_pair(&hlo, operand_id));\n   CHECK(it != hlo_to_rt_var_.end())\n-      << \"Runtime variable not found: \" << hlo.ToString();\n+      << \"Runtime variable not found for \" << hlo.ToString() << \" operand \"\n+      << operand_id;\n   return *it->second;\n }\n \n@@ -163,8 +182,10 @@ std::unique_ptr<TilingSpace> TilingSpace::Create(const HloFusionAdaptor& fusion,\n   for (const HloInstructionAdaptor& root : roots) {\n     const Shape& root_shape = root.shape();\n     if (!root.shape().IsArray() && root.opcode() != HloOpcode::kReduce) {\n-      LOG(FATAL) << \"Unsupported root shape: \" << root_shape.ToString();\n+      LOG(FATAL) << \"Unsupported root shape \" << root_shape.ToString()\n+                 << \" for root \" << root.instruction().ToString();\n     }\n+    // TODO(goncharov): why do we only care about the first shape of a tuple?\n     absl::Span<const int64_t> dims =\n         GetFirstShape(&root.instruction()).dimensions();\n     llvm::SmallVector<DimTile> dim_tiles;\n@@ -186,19 +207,7 @@ std::unique_ptr<TilingSpace> TilingSpace::Create(const HloFusionAdaptor& fusion,\n   // Iterator in reversed post-order (use-before-def).\n   auto post_order = fusion.MakeInstructionPostOrder();\n   for (auto it = post_order.rbegin(); it != post_order.rend(); ++it) {\n-    switch (it->instruction().opcode()) {\n-      case HloOpcode::kDot:\n-        tiling_space->ProcessDot(it->instruction());\n-        break;\n-      case HloOpcode::kReduce:\n-        tiling_space->ProcessReduce(it->instruction());\n-        break;\n-      case HloOpcode::kDynamicSlice:\n-        tiling_space->ProcessDynamicSlice(it->instruction());\n-        break;\n-      default:\n-        break;\n-    }\n+    tiling_space->ProcessInstruction(it->instruction());\n   }\n   return tiling_space;\n }"
        },
        {
            "sha": "2b9d0b8e8edaa5eae0e79a83a1947116d3f42ae5",
            "filename": "third_party/xla/xla/service/gpu/model/experimental/tiling_space.h",
            "status": "modified",
            "additions": 50,
            "deletions": 20,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Ftiling_space.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e1eff4a16ee4735777a079c9bd56e11db17abfd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Ftiling_space.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Ftiling_space.h?ref=5e1eff4a16ee4735777a079c9bd56e11db17abfd",
            "patch": "@@ -26,22 +26,26 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/codegen/tiling/constraint_expression.h\"\n-#include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/analysis/interval.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_tile.h\"\n #include \"xla/shape.h\"\n \n namespace xla::gpu::experimental {\n \n-// TilingSpace contains information about all parallel and sequential dimensions\n-// and runtime variables in a fusion.\n-// The parallel dimensions correspond to the dimensions of the outputs of the\n-// fusion.\n-// The sequential dimensions correspond to the contraction/reduction dimensions\n-// of the dots/reduces in the fusion.\n-// The runtime variables correspond to the offsets of the dynamic slices in the\n-// fusion.\n+// TilingSpace holds information about all tiling parameters of a fusion.\n+//\n+// It defines symbolic tiles for the fusion roots as symbolic expressions and\n+// constraints of possible tile \"variables\":\n+// * parallel dimensions - output dimensions of the fusion;\n+// * sequential dimensions - contraction/reduction dimensions of operations in\n+//   the fusion;\n+// * runtime variables - for example, offsets of the dynamic slices.\n+//\n+// This information allows us later to explore the space of all possible tilings\n+// and assign concrete tilings for every instruction of the fusion with\n+// SymbolicTilePropagation.\n class TilingSpace {\n  public:\n   TilingSpace() : constraints_(ConstraintExpression::GetAlwaysSatisfied()) {}\n@@ -51,29 +55,46 @@ class TilingSpace {\n \n   enum class DimensionSemantics { kParallel, kSequential };\n   struct DimensionInfo {\n-    // Unique ID for the dimension.\n+    // Unique ID for the dimension within the tiling space.\n     ID id;\n     // Size of the dimension.\n     int64_t dimension_size;\n     // Type of the dimension.\n     DimensionSemantics type;\n-    // HLO instruction that defines the dimension.\n+    // HLO instruction that defines (introduces) the dimension. For example\n+    // fusion root instruction defines the parallel dimensions. Dot/reduce\n+    // defines the sequential (contraction) dimensions.\n     const HloInstruction* hlo;\n-    // Index into the ordered list of dimensions of the HLO instruction.\n-    // All dimensions in the HLO instruction are described as\n+    // Index into the ordered list of dimensions of the HLO instruction `hlo`\n+    // that defines the dimension.\n+    // All dimensions in the HLO instruction are ordered as\n     // [all parallel dims of the output, all reduction/contraction dims].\n     //\n-    // Example:\n-    // [output_dims] dot(lhs, rhs, lhs_contracting_dims, rhs_contracting_dims)\n-    // The dimensions are ordered as [output_dims, LHS[lhs_contracting_dims]].\n+    // Example, for `[a,b,c] = dot(lhs, rhs, lhs_contracting_dims={d,e}, ...)`.\n+    // The ordered list of dimensions is [a,b,c,d,e].\n     int64_t dim_position;\n   };\n \n+  // Information about a runtime variable.\n+  // For example:\n+  //\n+  // off = s32[] parameter(0)\n+  // ds = dynamic-slice(tensor, off), ...\n+  //\n+  // `off = s32[] parameter(0)` instruction (`hlo`) defines the runtime\n+  // variable.\n+  // User's (dynamic-slice) semantics sets the `bounds` of possible values.\n+  //\n+  // If the same hlo is used as runtime variable multiple times, there will be\n+  // multiple entries in the `rt_vars_` with different IDs.\n+  //\n+  // RTVarInfo are accessed by (user_hlo, operand_id), in this case it is\n+  // (dynamic-slice, 1).\n   struct RTVarInfo {\n-    // Unique ID for the runtime variable.\n+    // Unique ID for the runtime variable within the tiling space.\n     ID id;\n-    // Feasible bounds of the runtime variable. The values outside of the bounds\n-    // will be clamped.\n+    // Feasible bounds of the runtime variable.\n+    // The values outside of the bounds will be clamped.\n     Interval bounds;\n     // HLO instruction that defines the runtime variable.\n     const HloInstruction* hlo;\n@@ -90,9 +111,15 @@ class TilingSpace {\n     sink.Append(space.ToString());\n   }\n \n+  // Returns the dimension info for the given `hlo` and `dim_position`.\n+  // `dim_position` is the index into the ordered list of dimensions of the HLO\n+  // instruction `hlo` that defines the dimension. The dimension info must\n+  // exist.\n   const DimensionInfo& GetDimensionInfo(const HloInstruction& hlo,\n                                         int64_t dim_position) const;\n \n+  // Returns the runtime variable info for `hlo` that uses it and its\n+  // `operand_id`. This runtime variable info must exist.\n   const RTVarInfo& GetRTVarInfo(const HloInstruction& hlo,\n                                 int64_t operand_id) const;\n \n@@ -115,6 +142,7 @@ class TilingSpace {\n   void ProcessDot(const HloInstruction& hlo);\n   void ProcessReduce(const HloInstruction& hlo);\n   void ProcessDynamicSlice(const HloInstruction& hlo);\n+  void ProcessInstruction(const HloInstruction& hlo);\n \n   // Maps from (hlo, dim_position) to the dimension info.\n   absl::flat_hash_map<std::pair<const HloInstruction*, int64_t>,\n@@ -130,7 +158,9 @@ class TilingSpace {\n   // The deque is used to guarantee the pointer stability.\n   std::deque<RTVarInfo> rt_vars_;\n \n-  // Root instruction of the fusion.\n+  // Symbolic tiles for the fusion roots.\n+  // For tuple roots, there will be one tile per tuple element. Otherwise,\n+  // there will be only one symbolic tile.\n   llvm::SmallVector<SymbolicTile, 2> tiled_roots_;\n \n   // Constraint expression for the tiling space."
        }
    ],
    "stats": {
        "total": 189,
        "additions": 124,
        "deletions": 65
    }
}