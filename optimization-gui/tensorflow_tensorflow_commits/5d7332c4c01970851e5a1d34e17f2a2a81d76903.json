{
    "author": "chsigg",
    "message": "Remove EmitterLocOpBuilder and xla_gpu_unsupported_annotate_with_emitter_loc.\n\nThis change removes the custom EmitterLocOpBuilder and the associated debug option `xla_gpu_unsupported_annotate_with_emitter_loc`. The functionality provided by EmitterLocOpBuilder is no longer functional. All instances of EmitterLocOpBuilder are replaced with the standard mlir::ImplicitLocOpBuilder. The debug option is also removed from the proto and flag definitions.\n\nPiperOrigin-RevId: 837767460",
    "sha": "5d7332c4c01970851e5a1d34e17f2a2a81d76903",
    "files": [
        {
            "sha": "e0f4ebb023c00c452548bde3775547083cc2fe43",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -111,7 +111,6 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/emitters:elemental_hlo_to_mlir\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n         \"//xla/codegen/xtile/ir:xtile\",\n@@ -258,7 +257,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/emitters:elemental_hlo_to_mlir\",\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n@@ -325,7 +323,6 @@ cc_library(\n     deps = [\n         \"//xla:autotuning_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n         \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/codegen/tiling:tiled_hlo_fusion_instruction\",\n@@ -441,7 +438,6 @@ cc_library(\n     deps = [\n         \":emitter_helpers\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/translate/hlo_to_mhlo:attribute_importer\",\n@@ -480,7 +476,6 @@ cc_library(\n         \":support\",\n         \"//xla:autotuning_proto_cc\",\n         \"//xla:util\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n         \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n@@ -517,7 +512,6 @@ xla_cc_test(\n         \":xtile_compiler_stub_for_testing\",\n         \"//xla:literal\",\n         \"//xla:literal_util\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n@@ -536,7 +530,6 @@ xla_cc_test(\n     deps = [\n         \":xtile_compiler\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n@@ -1037,7 +1030,6 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/backends/gpu/runtime:all_reduce\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/hlo/ir:hlo\","
        },
        {
            "sha": "b3b794b6908ed06b68ee567da49db3161eed0bda",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"llvm/Support/MathExtras.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n@@ -40,7 +41,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/runtime/all_reduce.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -185,7 +185,7 @@ GetBlockLevelFusionConfigForAllReduce(\n }\n \n absl::StatusOr<TensorValue> EmitAllReduce(\n-    EmitterLocOpBuilder b, const HloComputation* computation,\n+    mlir::ImplicitLocOpBuilder& b, const HloComputation* computation,\n     const HloAllReduceInstruction& all_reduce,\n     const TiledHloInstruction& tiled_hlo_reduce,\n     const BlockLevelParameters& block_level_parameters,\n@@ -372,7 +372,7 @@ absl::StatusOr<std::vector<Shape>> GetCollectiveUnmanagedKernelArguments(\n }\n \n absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n-    llvm::SmallVector<mlir::Type>& fn_arg_types, EmitterLocOpBuilder& b,\n+    llvm::SmallVector<mlir::Type>& fn_arg_types, mlir::ImplicitLocOpBuilder& b,\n     const HloComputation* hlo_computation) {\n   // rank: i32\n   fn_arg_types.push_back(b.getI32Type());\n@@ -403,7 +403,7 @@ absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n }\n \n absl::StatusOr<TensorValue> EmitCollective(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_reduce,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, mlir::Value pid,"
        },
        {
            "sha": "0677d3ebc6ab662f913294b1e7b04dead3dfb04d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -23,11 +23,11 @@ limitations under the License.\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/statusor.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Interfaces/FunctionInterfaces.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -63,7 +63,7 @@ absl::StatusOr<bool> TrySetGpuBackendConfigForCollective(\n // The fn_arg_types is updated in place to add these.\n // Returns the number of metadata arguments added or error.\n absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n-    llvm::SmallVector<mlir::Type>& fn_arg_types, EmitterLocOpBuilder& b,\n+    llvm::SmallVector<mlir::Type>& fn_arg_types, mlir::ImplicitLocOpBuilder& b,\n     const HloComputation* hlo_computation);\n \n // Version of [AddCollectiveMetadataArguments] that does the same for\n@@ -75,7 +75,7 @@ absl::StatusOr<std::vector<Shape>> GetCollectiveUnmanagedKernelArguments(\n // See [EmitTiledHloInstruction] for an overview of how this fits into the\n // emitter.\n absl::StatusOr<triton::TensorValue> EmitCollective(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_reduce,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, mlir::Value pid,"
        },
        {
            "sha": "3a10731c68516614d9733bafa1ec73414ffdc8cf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -37,7 +37,6 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -104,7 +103,7 @@ absl::StatusOr<ttir::ScaleDotElemType> GetScaleDotElemType(Type value) {\n \n namespace {\n \n-absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n+absl::StatusOr<Value> ScaledDot(mlir::ImplicitLocOpBuilder& b,\n                                 ScaledDotOperands& operands) {\n   TF_ASSIGN_OR_RETURN(auto lhs_dot_elem_type,\n                       internal::GetScaleDotElemType(operands.lhs.getType()));\n@@ -136,8 +135,9 @@ absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n \n namespace {\n \n-Value EmitStableHloDotAndAdd(EmitterLocOpBuilder b, Value lhs, Value rhs,\n-                             Value acc, PrecisionSpec precision_spec) {\n+Value EmitStableHloDotAndAdd(mlir::ImplicitLocOpBuilder& b, Value lhs,\n+                             Value rhs, Value acc,\n+                             PrecisionSpec precision_spec) {\n   auto lhs_type = mlir::cast<ShapedType>(lhs.getType());\n   auto rhs_type = mlir::cast<ShapedType>(rhs.getType());\n \n@@ -172,7 +172,7 @@ Value EmitStableHloDotAndAdd(EmitterLocOpBuilder b, Value lhs, Value rhs,\n \n }  // namespace\n \n-absl::StatusOr<Type> GetAlgUnsetAccumulatorType(EmitterLocOpBuilder b,\n+absl::StatusOr<Type> GetAlgUnsetAccumulatorType(mlir::ImplicitLocOpBuilder& b,\n                                                 const HloDotInstruction& dot) {\n   TF_ASSIGN_OR_RETURN(Type lhs_type,\n                       TritonType(b, dot.operand(0)->shape().element_type()));\n@@ -204,7 +204,7 @@ absl::StatusOr<Type> GetAlgUnsetAccumulatorType(EmitterLocOpBuilder b,\n // the operands do not already conform to any of them. Returns `std::nullopt` if\n // no casting is a priori needed.\n absl::StatusOr<std::optional<Type>> GetForceOperandsType(\n-    EmitterLocOpBuilder b, const HloDotInstruction& dot,\n+    mlir::ImplicitLocOpBuilder& b, const HloDotInstruction& dot,\n     const DotOperands& dot_operands) {\n   PrecisionConfig::Algorithm algorithm = dot.precision_config().algorithm();\n   if (algorithm == PrecisionConfig::ALG_UNSET) {\n@@ -253,7 +253,7 @@ absl::StatusOr<std::optional<Type>> GetForceOperandsType(\n \n }  // namespace\n \n-absl::StatusOr<Type> GetDotAccumulatorType(EmitterLocOpBuilder b,\n+absl::StatusOr<Type> GetDotAccumulatorType(mlir::ImplicitLocOpBuilder& b,\n                                            const HloDotInstruction& dot) {\n   const PrecisionConfig::Algorithm algorithm =\n       dot.precision_config().algorithm();\n@@ -267,7 +267,7 @@ absl::StatusOr<Type> GetDotAccumulatorType(EmitterLocOpBuilder b,\n   return TritonType(b, accumulator_type);\n }\n \n-absl::StatusOr<Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n+absl::StatusOr<Value> EmitSingleTileDot(mlir::ImplicitLocOpBuilder& b,\n                                         const HloDotInstruction& dot,\n                                         DotOperands dot_operands) {\n   PrecisionConfig::Algorithm algorithm = dot.precision_config().algorithm();\n@@ -314,7 +314,7 @@ absl::StatusOr<Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n }\n \n absl::StatusOr<Value> EmitSingleTileScaledDot(\n-    EmitterLocOpBuilder b, const HloScaledDotInstruction& scaled_dot,\n+    mlir::ImplicitLocOpBuilder& b, const HloScaledDotInstruction& scaled_dot,\n     ScaledDotOperands dot_operands) {\n   return ScaledDot(b, dot_operands);\n }"
        },
        {
            "sha": "95fe08b2d12edd1ba26e546efb94c99c15c9bacb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -17,10 +17,10 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_CODEGEN_TRITON_DOT_ALGORITHMS_H_\n \n #include \"absl/status/statusor.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n@@ -58,20 +58,20 @@ struct ScaledDotOperands {\n // Returns the type to use for accumulation for the given `dot` instruction.\n // This also handles the case where the algorithm is `ALG_UNSET`.\n absl::StatusOr<::mlir::Type> GetDotAccumulatorType(\n-    EmitterLocOpBuilder b, const HloDotInstruction& dot);\n+    mlir::ImplicitLocOpBuilder& b, const HloDotInstruction& dot);\n \n // Emits a single-tile dot, considering the given `dot` instruction's algorithm\n // and operand precisions. Raises an `UnimplementedError` if the algorithm is\n // not supported.\n-absl::StatusOr<::mlir::Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n+absl::StatusOr<::mlir::Value> EmitSingleTileDot(mlir::ImplicitLocOpBuilder& b,\n                                                 const HloDotInstruction& dot,\n                                                 DotOperands dot_operands);\n \n // Emits a single-tile scaled-dot, considering the given `scaled-dot`\n // instruction's operand precisions. Raises an `InvalidArgumentError` if the\n // operand types are not supported.\n absl::StatusOr<::mlir::Value> EmitSingleTileScaledDot(\n-    EmitterLocOpBuilder b, const HloScaledDotInstruction& scaled_dot,\n+    mlir::ImplicitLocOpBuilder& b, const HloScaledDotInstruction& scaled_dot,\n     ScaledDotOperands dot_operands);\n \n namespace internal {"
        },
        {
            "sha": "30dd0f0dcc157d217bdfca493deb8f17a62dec48",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -43,7 +43,6 @@ limitations under the License.\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/elemental_hlo_to_mlir.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n@@ -84,8 +83,8 @@ namespace {\n using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n \n // Emit a value as Index clamped to [lower, upper].\n-Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n-                       int64_t upper) {\n+Value EmitClampedIndex(mlir::ImplicitLocOpBuilder& b, Value value,\n+                       int64_t lower, int64_t upper) {\n   Value clamped_index =\n       ma::MaxSIOp::create(b, value, CreateConst(b, value.getType(), lower));\n   clamped_index = ma::MinSIOp::create(b, clamped_index,\n@@ -94,7 +93,7 @@ Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n }\n \n absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n-    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n+    mlir::ImplicitLocOpBuilder& b, Value pid, ValueRange runtime_values,\n     const TiledHloInstruction& tiled_hlo) {\n   TF_ASSIGN_OR_RETURN(IndexingMap tile_offsets_indexing,\n                       tiled_hlo.tile_offsets_indexing());\n@@ -130,7 +129,8 @@ absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n //\n // TODO(b/331413981): get rid of this special handling once this is solved.\n absl::StatusOr<TensorValue> EmitNestedFusion(\n-    EmitterLocOpBuilder b, const HloFusionInstruction& fusion_instruction,\n+    mlir::ImplicitLocOpBuilder& b,\n+    const HloFusionInstruction& fusion_instruction,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n   // TODO(b/331402498): revisit the order of scope once we completely\n   // deprecate Triton fusion analysis.\n@@ -167,7 +167,8 @@ SmallVector<int64_t> GetPaddedTileSizes(ArrayRef<int64_t> tile_sizes) {\n   return result;\n }\n \n-absl::StatusOr<Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t) {\n+absl::StatusOr<Type> TritonType(mlir::ImplicitLocOpBuilder& b,\n+                                PrimitiveType t) {\n   switch (t) {\n     case F64:\n       return b.getF64Type();\n@@ -236,7 +237,7 @@ bool IsFp8Type(Type t) {\n                    mlir::Float8E4M3B11FNUZType>(t);\n }\n \n-Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n+Value Cast(mlir::ImplicitLocOpBuilder& b, Value value, Type dst_element_ty) {\n   Type src_ty = value.getType();\n   Type src_element_ty = src_ty;\n   Type fp16_ty = b.getF16Type();\n@@ -354,15 +355,15 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n              << llvm_ir::DumpToString(dst_element_ty);\n }\n \n-Value Subtract(EmitterLocOpBuilder& b, ValueRange values) {\n+Value Subtract(mlir::ImplicitLocOpBuilder& b, ValueRange values) {\n   if (mlir::isa<mlir::IntegerType>(mlir::getElementTypeOrSelf(values[0]))) {\n     return ma::SubIOp::create(b, values[0], values[1]);\n   } else {\n     return ma::SubFOp::create(b, values[0], values[1]);\n   }\n }\n \n-Value Compare(EmitterLocOpBuilder& b, ValueRange values,\n+Value Compare(mlir::ImplicitLocOpBuilder& b, ValueRange values,\n               mh::ComparisonDirection direction) {\n   const Type type = mlir::getElementTypeOrSelf(values[0]);\n   if (mlir::isa<mlir::IntegerType>(type)) {\n@@ -381,7 +382,7 @@ Value Compare(EmitterLocOpBuilder& b, ValueRange values,\n       values[0], values[1]);\n }\n \n-Value Maximum(EmitterLocOpBuilder& b, ValueRange values) {\n+Value Maximum(mlir::ImplicitLocOpBuilder& b, ValueRange values) {\n   auto type = mlir::getElementTypeOrSelf(values[0]);\n   if (mlir::isa<mlir::FloatType>(type)) {\n     return ma::MaximumFOp::create(b, values);\n@@ -394,7 +395,7 @@ Value Maximum(EmitterLocOpBuilder& b, ValueRange values) {\n   return ma::MaxSIOp::create(b, values);\n }\n \n-Value Minimum(EmitterLocOpBuilder& b, ValueRange values) {\n+Value Minimum(mlir::ImplicitLocOpBuilder& b, ValueRange values) {\n   auto type = mlir::getElementTypeOrSelf(values[0]);\n   if (mlir::isa<mlir::FloatType>(type)) {\n     return ma::MinimumFOp::create(b, values);\n@@ -407,7 +408,7 @@ Value Minimum(EmitterLocOpBuilder& b, ValueRange values) {\n   return ma::MinSIOp::create(b, values);\n }\n \n-absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> EmitElementwise(mlir::ImplicitLocOpBuilder& b,\n                                       const HloInstruction& hlo,\n                                       ValueRange inputs) {\n   const bool is_integer =\n@@ -550,7 +551,7 @@ absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n }\n \n absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n-    EmitterLocOpBuilder& b, const HloInstruction& constant) {\n+    mlir::ImplicitLocOpBuilder& b, const HloInstruction& constant) {\n   TF_ASSIGN_OR_RETURN(Type ty, TritonType(b, constant.shape().element_type()));\n   llvm::SmallVector<int64_t> shape{constant.shape().dimensions().begin(),\n                                    constant.shape().dimensions().end()};\n@@ -567,14 +568,14 @@ absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n   return CreateConst(b, ty, ScalarConstantValue<double>(constant, F64), shape);\n }\n \n-Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n+Value Bitcast(mlir::ImplicitLocOpBuilder& b, Value value, Type type) {\n   auto value_type = value.getType();\n   value_type = mlir::dyn_cast<ShapedType>(value_type).clone(type);\n   return mlir::arith::BitcastOp::create(b, value_type, value);\n }\n \n /*static */ absl::StatusOr<TileInfo> TileInfo::Construct(\n-    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n+    mlir::ImplicitLocOpBuilder& b, Value pid, ValueRange runtime_values,\n     const TiledHloInstruction& tiled_hlo) {\n   TF_ASSIGN_OR_RETURN(SmallVector<Value> offsets,\n                       ComputeOffsetsForTile(b, pid, runtime_values, tiled_hlo));\n@@ -597,7 +598,7 @@ Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n                   minor_to_major_layout, storage_type);\n }\n \n-TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n+TensorValue EmitParameterExtract(mlir::ImplicitLocOpBuilder& b,\n                                  const TileInfo& tile_info, Value arg) {\n   auto tensor_type = mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n                                                  tile_info.storage_type());\n@@ -608,7 +609,8 @@ TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n }\n \n absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, absl::Span<const HloInstruction* const> instructions,\n+    mlir::ImplicitLocOpBuilder& b,\n+    absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n   for (const HloInstruction* hlo : instructions) {\n     TensorValue result;\n@@ -666,7 +668,7 @@ absl::StatusOr<TensorValue> EmitScope(\n   return values[instructions.back()];\n }\n \n-TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n+TensorValue BroadcastInDims(mlir::ImplicitLocOpBuilder& b, TensorValue value,\n                             ArrayRef<int64_t> output_shape,\n                             ArrayRef<int64_t> dims) {\n   CHECK(llvm::is_sorted(dims)) << \"broadcast dims must be sorted\";\n@@ -677,7 +679,7 @@ TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n   return mlir::stablehlo::BroadcastInDimOp::create(b, result_type, value, dims);\n }\n \n-TensorValue Splat(EmitterLocOpBuilder b, Value value,\n+TensorValue Splat(mlir::ImplicitLocOpBuilder& b, Value value,\n                   ArrayRef<int64_t> output_shape) {\n   auto tensor_value = mlir::dyn_cast<TensorValue>(value);\n   if (!tensor_value) {"
        },
        {
            "sha": "85c0d07dc98011c0e5741750eb10ee4b2003d63b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 24,
            "deletions": 20,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -35,7 +35,6 @@ limitations under the License.\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -64,8 +63,8 @@ std::string MlirToString(T&& value) {\n class TileInfo {\n  public:\n   static absl::StatusOr<TileInfo> Construct(\n-      EmitterLocOpBuilder b, mlir::Value pid, mlir::ValueRange runtime_values,\n-      const TiledHloInstruction& tiled_hlo);\n+      mlir::ImplicitLocOpBuilder& b, mlir::Value pid,\n+      mlir::ValueRange runtime_values, const TiledHloInstruction& tiled_hlo);\n \n   // Tile offsets. Its size is equal to the rank of the output shape.\n   inline mlir::ValueRange offsets() const { return offsets_; }\n@@ -121,7 +120,8 @@ llvm::SmallVector<int64_t> GetPaddedTileSizes(\n     llvm::ArrayRef<int64_t> tile_sizes);\n \n // XLA -> Triton type conversions.\n-absl::StatusOr<mlir::Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t);\n+absl::StatusOr<mlir::Type> TritonType(mlir::ImplicitLocOpBuilder& b,\n+                                      PrimitiveType t);\n \n // Triton type -> XLA type conversions.\n absl::StatusOr<PrimitiveType> GetPrimitiveType(mlir::Type t);\n@@ -140,7 +140,8 @@ T ScalarConstantValue(const HloInstruction& instr, PrimitiveType dst_type) {\n \n // Create a scalar constant.\n template <typename T>\n-mlir::Value CreateConst(EmitterLocOpBuilder& b, mlir::Type type, T value) {\n+mlir::Value CreateConst(mlir::ImplicitLocOpBuilder& b, mlir::Type type,\n+                        T value) {\n   if (mlir::isa<mlir::IntegerType>(type)) {\n     return b.create<mlir::arith::ConstantOp>(b.getIntegerAttr(type, value));\n   }\n@@ -159,7 +160,7 @@ mlir::Value CreateConst(EmitterLocOpBuilder& b, mlir::Type type, T value) {\n // Create a tensor constant.\n template <typename T>\n mlir::TypedValue<mlir::RankedTensorType> CreateConst(\n-    EmitterLocOpBuilder& b, mlir::Type type, T value,\n+    mlir::ImplicitLocOpBuilder& b, mlir::Type type, T value,\n     llvm::ArrayRef<int64_t> shape) {\n   auto tensor_type = mlir::RankedTensorType::get(shape, type);\n   if (auto int_type = mlir::dyn_cast<mlir::IntegerType>(type)) {\n@@ -181,63 +182,66 @@ mlir::TypedValue<mlir::RankedTensorType> CreateConst(\n \n // Create a constant of the same shape as `like` but with a new type and value.\n template <typename T>\n-mlir::Value ConstLike(EmitterLocOpBuilder& b, mlir::Value like, T new_value) {\n+mlir::Value ConstLike(mlir::ImplicitLocOpBuilder& b, mlir::Value like,\n+                      T new_value) {\n   if (auto src_shaped_ty = mlir::dyn_cast<mlir::ShapedType>(like.getType())) {\n     mlir::Type src_ty = src_shaped_ty.getElementType();\n     return CreateConst(b, src_ty, new_value, src_shaped_ty.getShape());\n   }\n   return CreateConst(b, like.getType(), new_value);\n }\n \n-inline mlir::Value ZerosLike(EmitterLocOpBuilder& b, mlir::Value x) {\n+inline mlir::Value ZerosLike(mlir::ImplicitLocOpBuilder& b, mlir::Value x) {\n   return ConstLike(b, x, 0);\n }\n \n-inline mlir::Value OnesLike(EmitterLocOpBuilder& b, mlir::Value x) {\n+inline mlir::Value OnesLike(mlir::ImplicitLocOpBuilder& b, mlir::Value x) {\n   return ConstLike(b, x, 1);\n }\n \n bool IsFp8Type(mlir::Type t);\n \n // Triton type conversions.\n-mlir::Value Cast(EmitterLocOpBuilder& b, mlir::Value value,\n+mlir::Value Cast(mlir::ImplicitLocOpBuilder& b, mlir::Value value,\n                  mlir::Type dst_element_ty);\n \n // Emits a scalar constant.\n absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n-    EmitterLocOpBuilder& b, const HloInstruction& constant);\n+    mlir::ImplicitLocOpBuilder& b, const HloInstruction& constant);\n \n-absl::StatusOr<mlir::Value> EmitElementwise(EmitterLocOpBuilder& b,\n+absl::StatusOr<mlir::Value> EmitElementwise(mlir::ImplicitLocOpBuilder& b,\n                                             const HloInstruction& hlo,\n                                             mlir::ValueRange inputs);\n \n-mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n+mlir::Value Bitcast(mlir::ImplicitLocOpBuilder& b, mlir::Value value,\n+                    mlir::Type type);\n \n // Emits an xtile::ExtractTileOp for the given tile info and argument.\n-TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n+TensorValue EmitParameterExtract(mlir::ImplicitLocOpBuilder& b,\n                                  const TileInfo& tile_info, mlir::Value arg);\n \n // Emits a sequence of HLO instructions within a specific scope.\n //\n // This function traverses the provided `hlo_instructions` in a\n // defined-before-use order and emits the corresponding MLIR operations using\n-// the given `EmitterLocOpBuilder`. It uses `emitted_values` to look up already\n-// emitted results for instructions, typically parameters or results from\n-// outer scopes. New results are added to the `emitted_values` map.\n+// the given `mlir::ImplicitLocOpBuilder`. It uses `emitted_values` to look up\n+// already emitted results for instructions, typically parameters or results\n+// from outer scopes. New results are added to the `emitted_values` map.\n //\n // Example usage within [EmitReduce] includes using it to emit the body of the\n // `HloInstruction::to_apply` computation.\n absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, absl::Span<const HloInstruction* const> instructions,\n+    mlir::ImplicitLocOpBuilder& b,\n+    absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values);\n \n // Same as HLO BroadcastInDims. The sorted indices in `dims` specify the\n // mapping of the input dimensions to the output dimensions.\n-TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n+TensorValue BroadcastInDims(mlir::ImplicitLocOpBuilder& b, TensorValue value,\n                             ::mlir::ArrayRef<int64_t> output_shape,\n                             ::mlir::ArrayRef<int64_t> dims);\n \n-TensorValue Splat(EmitterLocOpBuilder b, ::mlir::Value value,\n+TensorValue Splat(mlir::ImplicitLocOpBuilder& b, ::mlir::Value value,\n                   ::mlir::ArrayRef<int64_t> output_shape);\n \n }  // namespace xla::gpu::triton"
        },
        {
            "sha": "5d88939f86d48a9a579d90104bba2b64278ba426",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 27,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -72,7 +72,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/elemental_hlo_to_mlir.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n@@ -140,17 +139,17 @@ using ::xla::gpu::triton::TritonType;\n \n namespace {\n \n-Value MakeIndex(EmitterLocOpBuilder& b, int64_t value) {\n+Value MakeIndex(mlir::ImplicitLocOpBuilder& b, int64_t value) {\n   return arith::ConstantIndexOp::create(b, value);\n }\n \n-TensorValue Iota(EmitterLocOpBuilder b, int32_t limit) {\n+TensorValue Iota(mlir::ImplicitLocOpBuilder& b, int32_t limit) {\n   auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n   return stablehlo::IotaOp::create(b, type, /*iota_dimension=*/0);\n }\n \n absl::StatusOr<TensorValue> EmitReduce(\n-    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_hlo_reduce,\n+    mlir::ImplicitLocOpBuilder& b, const TiledHloInstruction& tiled_hlo_reduce,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   // At the moment, we should only emit a full reduction over a single\n   // dimension using a scalar as a neutral element.\n@@ -239,7 +238,7 @@ ArrayRef<T> MakeArrayRef(const absl::Span<const T> span) {\n }\n \n TensorValue EmitTiledBroadcast(\n-    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_broadcast,\n+    mlir::ImplicitLocOpBuilder& b, const TiledHloInstruction& tiled_broadcast,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   const SmallVector<int64_t>& input_tile_shape =\n       tiled_broadcast.operand(0)->tile_sizes();\n@@ -260,7 +259,8 @@ TensorValue EmitTiledBroadcast(\n }\n \n absl::StatusOr<TensorValue> EmitTiledIota(\n-    EmitterLocOpBuilder b, Value pid, const TiledHloInstruction& tiled_iota) {\n+    mlir::ImplicitLocOpBuilder& b, Value pid,\n+    const TiledHloInstruction& tiled_iota) {\n   const HloIotaInstruction* hlo_iota =\n       ::xla::Cast<HloIotaInstruction>(tiled_iota.hlo());\n   int64_t iota_dim = hlo_iota->iota_dimension();\n@@ -323,7 +323,7 @@ SmallVector<Value> GetRuntimeValues(\n   return runtime_values;\n }\n \n-absl::StatusOr<TensorValue> EmitTiledReshape(EmitterLocOpBuilder b,\n+absl::StatusOr<TensorValue> EmitTiledReshape(mlir::ImplicitLocOpBuilder& b,\n                                              ArrayRef<int64_t> tile_sizes,\n                                              TensorValue input) {\n   mlir::RankedTensorType input_type = input.getType();\n@@ -343,7 +343,7 @@ absl::StatusOr<TensorValue> EmitTiledReshape(EmitterLocOpBuilder b,\n   return stablehlo::ReshapeOp::create(b, output_tensor_type, input);\n }\n \n-TensorValue EmitTiledTranspose(EmitterLocOpBuilder b,\n+TensorValue EmitTiledTranspose(mlir::ImplicitLocOpBuilder& b,\n                                ArrayRef<int64_t> tile_sizes,\n                                SmallVector<int64_t> dimensions,\n                                TensorValue input) {\n@@ -359,7 +359,7 @@ TensorValue EmitTiledTranspose(EmitterLocOpBuilder b,\n }\n \n absl::StatusOr<TensorValue> EmitTiledBitcast(\n-    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_bitcast,\n+    mlir::ImplicitLocOpBuilder& b, const TiledHloInstruction& tiled_bitcast,\n     TensorValue input) {\n   Shape input_shape = tiled_bitcast.hlo()->operand(0)->shape();\n   const Shape& output_shape = tiled_bitcast.hlo()->shape();\n@@ -433,7 +433,7 @@ absl::StatusOr<TensorValue> EmitTiledBitcast(\n }\n \n absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -467,7 +467,7 @@ absl::StatusOr<int64_t> GetDotLoopIterationCount(\n // Note: we currently assume that contracting_dimension_tile_index is an i32\n // scalar.\n absl::StatusOr<TensorValue> MaskDotOperand(\n-    EmitterLocOpBuilder b, const TiledHloInstruction& dot_operand,\n+    mlir::ImplicitLocOpBuilder& b, const TiledHloInstruction& dot_operand,\n     TensorValue dot_operand_value, Value contracting_dimension_tile_index,\n     int contraction_dimension_index) {\n   if (contracting_dimension_tile_index.getType() != b.getI32Type()) {\n@@ -573,8 +573,9 @@ enum class DotOperandSide { kLhs, kRhs };\n //\n // Returns an error if canonicalization is not possible.\n absl::StatusOr<TensorValue> CanonicalizeDotOperand(\n-    EmitterLocOpBuilder b, TensorValue operand, int64_t contracting_dim_idx,\n-    DotOperandSide side, TensorValue counterpart_operand = nullptr) {\n+    mlir::ImplicitLocOpBuilder& b, TensorValue operand,\n+    int64_t contracting_dim_idx, DotOperandSide side,\n+    TensorValue counterpart_operand = nullptr) {\n   llvm::ArrayRef<int64_t> shape = operand.getType().getShape();\n   llvm::ArrayRef<int64_t> counterpart_shape =\n       counterpart_operand == nullptr ? shape\n@@ -609,7 +610,7 @@ absl::StatusOr<TensorValue> CanonicalizeDotOperand(\n }\n \n absl::StatusOr<TensorValue> EmitDot(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -776,7 +777,7 @@ absl::StatusOr<TensorValue> EmitDot(\n }\n \n absl::StatusOr<TensorValue> EmitScaledDot(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -929,7 +930,7 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n }\n \n absl::StatusOr<TensorValue> EmitConcatenate(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_concatenate,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1038,7 +1039,7 @@ absl::StatusOr<TensorValue> EmitConcatenate(\n }\n \n absl::StatusOr<TensorValue> EmitPad(\n-    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_pad,\n+    mlir::ImplicitLocOpBuilder& b, const TiledHloInstruction& tiled_pad,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n     Value pid) {\n   // TODO(b/393299275): get rid of calls to `GetPaddedTileSizes` once tiling\n@@ -1095,7 +1096,7 @@ absl::StatusOr<TensorValue> EmitPad(\n }\n \n absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1234,7 +1235,7 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n }\n \n absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n-    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1374,11 +1375,7 @@ absl::Status EmitGeneric(\n   const HloInstruction* root =\n       symbolic_tile_analysis.GetSymbolicTiledHloComputation().back()->hlo();\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(root->name()));\n-  EmitterLocOpBuilder b(loc, builder,\n-                        root->GetModule()\n-                            ->config()\n-                            .debug_options()\n-                            .xla_gpu_unsupported_annotate_with_emitter_loc());\n+  mlir::ImplicitLocOpBuilder b(loc, builder);\n   absl::Span<const HloInstruction* const> roots =\n       symbolic_tile_analysis.GetRoots();\n   int64_t root_index = FindIndex(roots, root);\n@@ -1493,9 +1490,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n \n   auto loc = mlir::NameLoc::get(\n       mlir::StringAttr::get(&mlir_context, hlo_computation->name()));\n-  EmitterLocOpBuilder b(\n-      loc, &mlir_context,\n-      debug_options.xla_gpu_unsupported_annotate_with_emitter_loc());\n+  mlir::ImplicitLocOpBuilder b(loc, &mlir_context);\n \n   mlir::OwningOpRef<mlir::ModuleOp> triton_module =\n       llvm_ir::CreateMlirModuleOp(loc);"
        },
        {
            "sha": "6f1aaf7ac500b576cc8b664459902505656e1863",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 87,
            "changes": 87,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n #include \"llvm/IR/LLVMContext.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n@@ -40,20 +39,8 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n-using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n-\n using TritonEmitterDevicelessTest = HloHardwareIndependentTestBase;\n \n-class AnnotationsTest : public HloHardwareIndependentTestBase {\n- public:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options =\n-        HloHardwareIndependentTestBase::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_unsupported_annotate_with_emitter_loc(true);\n-    return debug_options;\n-  }\n-};\n-\n class WarpSpecializationTritonEmitterTest : public TritonEmitterDevicelessTest {\n  public:\n   DebugOptions GetDebugOptionsForTest() const override {\n@@ -66,80 +53,6 @@ class WarpSpecializationTritonEmitterTest : public TritonEmitterDevicelessTest {\n   }\n };\n \n-TEST_F(AnnotationsTest, Annotations) {\n-  static constexpr absl::string_view kHloText = R\"(\n-HloModule Annotations\n-\n-triton_dot_lhs {\n-  p0 = f32[8,8] parameter(0)\n-  ROOT copy = f32[8,8] copy(p0)\n-}\n-triton_dot_rhs {\n-  p1 = f32[8,8] parameter(0)\n-  ROOT copy = f32[8,8] copy(p1)\n-}\n-\n-triton_dot {\n-  p0 = f32[8,8] parameter(0)\n-  p1 = f32[8,8] parameter(1)\n-  a = f32[8,8] fusion(p0), kind=kCustom, calls=triton_dot_lhs,\n-    backend_config={\"fusion_backend_config\": {kind: \"__triton_nested_gemm_fusion\",\n-    block_level_fusion_config: {output_tiles:[{sizes:[\"8\",\"8\"]}]}}}\n-  b = f32[8,8] fusion(p1), kind=kCustom, calls=triton_dot_rhs,\n-    backend_config={\"fusion_backend_config\": {kind: \"__triton_nested_gemm_fusion\",\n-    block_level_fusion_config: {output_tiles:[{sizes:[\"8\",\"8\"]}]}}}\n-  ROOT dot = f32[8,8] dot(a, b),\n-    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n-    algorithm=dot_bf16_bf16_f32_x3\n-}\n-\n-ENTRY e {\n-  p0 = f32[8,8]{1, 0} parameter(0)\n-  p1 = f32[8,8]{1, 0} parameter(1)\n-  ROOT _ = f32[8,8] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n-    backend_config={\"fusion_backend_config\": {kind: \"__triton_nested_gemm_fusion\",\n-      block_level_fusion_config:\n-      {\n-        \"output_tiles\":[{\"sizes\":[\"8\",\"8\"]}],\n-        \"num_stages\":1,\n-        \"num_warps\":1,\n-        \"num_ctas\":1\n-      }\n-    }\n-  }\n-})\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n-  auto* fusion = Cast<HloFusionInstruction>(\n-      module->entry_computation()->root_instruction());\n-\n-  mlir::MLIRContext mlir_context;\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto triton_module,\n-      CreateTritonModule(\"triton_fn\", fusion,\n-                         TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         BlockLevelParameters::FromBlockLevelFusionConfig(\n-                             fusion->backend_config<GpuBackendConfig>()\n-                                 ->fusion_backend_config()\n-                                 .block_level_fusion_config()),\n-                         mlir_context));\n-\n-  std::string annotated_ir = DumpTritonIR(triton_module.get(), true);\n-\n-  if constexpr (EmitterLocOpBuilder::kSourceLocationSupported) {\n-    EXPECT_THAT(RunFileCheck(annotated_ir, R\"(\n-      CHECK:  [[SOMETHING:.*]] \"triton_dot -> [[FILE_LINE:fusion_emitter.*:.*]]\"\n-    )\"),\n-                absl_testing::IsOkAndHolds(true))\n-        << annotated_ir;\n-  } else {\n-    EXPECT_THAT(RunFileCheck(annotated_ir, R\"(\n-      CHECK:  [[SOMETHING:.*]] \"triton_dot\"\n-    )\"),\n-                absl_testing::IsOkAndHolds(true));\n-  }\n-}\n-\n TEST_F(TritonEmitterDevicelessTest, FailsGracefullyIfNumWarpsIsMissing) {\n   constexpr absl::string_view kHloText = R\"(\n triton_computation {"
        },
        {
            "sha": "5cc3e4f9c12505b6262474594730a16c25910c8f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -60,7 +60,6 @@ cc_library(\n         \"//xla/backends/gpu/codegen/triton:dot_algorithms\",\n         \"//xla/backends/gpu/codegen/triton:emitter_helpers\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n-        \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/hlo/translate/mhlo_to_hlo:attribute_exporter\","
        },
        {
            "sha": "2e63cce525e328b547823126e1517567b45a1466",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/stablehlo_lower_to_triton.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n@@ -49,7 +50,6 @@ limitations under the License.\n #include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/translate/mhlo_to_hlo/attribute_exporter.h\"\n #include \"xla/service/algorithm_util.h\"\n@@ -126,7 +126,7 @@ class LowerBroadcastInDim\n   mlir::LogicalResult matchAndRewrite(\n       stablehlo::BroadcastInDimOp op,\n       mlir::PatternRewriter& rewriter) const override {\n-    ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n+    mlir::ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n     auto input_tensor = op.getOperand();\n     auto input_shape = input_tensor.getType().getShape();\n     auto output_shape = op.getResult().getType().getShape();\n@@ -400,11 +400,11 @@ struct TritonPrecisionSpec {\n mlir::Type ElementType(mlir::Value v) { return mlir::getElementTypeOrSelf(v); }\n \n using AlgorithmEmitter = absl::StatusOr<Value> (*)(\n-    ::xla::EmitterLocOpBuilder, const ::xla::gpu::triton::DotOperands&,\n+    mlir::ImplicitLocOpBuilder&, const ::xla::gpu::triton::DotOperands&,\n     const TritonPrecisionSpec&);\n \n absl::StatusOr<Value> EmitDotAlgUnset(\n-    ::xla::EmitterLocOpBuilder b,\n+    mlir::ImplicitLocOpBuilder& b,\n     const ::xla::gpu::triton::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   // Execute matrix multiplication of input tiles and pass the accumulator.\n@@ -431,7 +431,7 @@ absl::StatusOr<Value> EmitDotAlgUnset(\n }\n \n absl::StatusOr<Value> EmitRegularDot(\n-    ::xla::EmitterLocOpBuilder b,\n+    mlir::ImplicitLocOpBuilder& b,\n     const ::xla::gpu::triton::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   Value lhs = dot_operands.lhs;\n@@ -471,7 +471,7 @@ absl::StatusOr<Value> EmitRegularDot(\n // We would get the wrong result if we sum these partial products. Instead, we\n // must override any accumulated result if the last partial product is\n // non-finite. See b/115844437.\n-Value ZeroNaNs(::xla::EmitterLocOpBuilder b, Value input) {\n+Value ZeroNaNs(mlir::ImplicitLocOpBuilder& b, Value input) {\n   Value positive_inf = ::xla::gpu::triton::CreateConst<float>(\n       b, b.getF32Type(), std::numeric_limits<float>::infinity(),\n       mlir::cast<ShapedType>(input.getType()).getShape());\n@@ -497,7 +497,7 @@ absl::Status ExpectType(Value v, Type expected_type) {\n   return absl::OkStatus();\n }\n \n-std::vector<Value> SplitF32(::xla::EmitterLocOpBuilder b, Value input,\n+std::vector<Value> SplitF32(mlir::ImplicitLocOpBuilder& b, Value input,\n                             int split_count) {\n   std::vector<Value> split_inputs;\n   split_inputs.reserve(split_count);\n@@ -513,7 +513,7 @@ std::vector<Value> SplitF32(::xla::EmitterLocOpBuilder b, Value input,\n   return split_inputs;\n }\n \n-Value IEEEDot(::xla::EmitterLocOpBuilder b, Value lhs, Value rhs, Value acc) {\n+Value IEEEDot(mlir::ImplicitLocOpBuilder& b, Value lhs, Value rhs, Value acc) {\n   return ttir::DotOp::create(b, lhs, rhs, acc,\n                              /*inputPrecision=*/ttir::InputPrecision::IEEE,\n                              /*maxNumImpreciseAcc=*/0);\n@@ -522,7 +522,7 @@ Value IEEEDot(::xla::EmitterLocOpBuilder b, Value lhs, Value rhs, Value acc) {\n // Leverages BF16 datatype for F32 matmul computation. It follows the guidance\n // from https://arxiv.org/pdf/1904.06376.pdf.\n absl::StatusOr<Value> EmitBF16x9Matmul(\n-    ::xla::EmitterLocOpBuilder b,\n+    mlir::ImplicitLocOpBuilder& b,\n     const ::xla::gpu::triton::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 3;\n@@ -561,7 +561,7 @@ absl::StatusOr<Value> EmitBF16x9Matmul(\n // Leverages BF16 datatype for F32 matmul computation. It follows the guidance\n // from https://arxiv.org/pdf/1904.06376.pdf.\n absl::StatusOr<Value> EmitBF16x6Matmul(\n-    ::xla::EmitterLocOpBuilder b,\n+    mlir::ImplicitLocOpBuilder& b,\n     const ::xla::gpu::triton::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 3;\n@@ -596,7 +596,7 @@ absl::StatusOr<Value> EmitBF16x6Matmul(\n // Compute F32 matmul with 3 BF16 dots. It is less accurate than\n // EmitBF16x6Matmul.\n absl::StatusOr<Value> EmitBF16x3Matmul(\n-    ::xla::EmitterLocOpBuilder b,\n+    mlir::ImplicitLocOpBuilder& b,\n     const ::xla::gpu::triton::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 2;\n@@ -712,7 +712,7 @@ LogicalResult RewriteDotGeneralToTritonDot(mlir::PatternRewriter& rewriter,\n \n   auto algorithm_emitter = algorithm_emitter_or_status.value();\n \n-  ::xla::EmitterLocOpBuilder builder(op->getLoc(), rewriter);\n+  mlir::ImplicitLocOpBuilder builder(op->getLoc(), rewriter);\n \n   ::xla::gpu::triton::DotOperands dot_operands{op.getLhs(), op.getRhs(),\n                                                accumulator};"
        },
        {
            "sha": "9a6d7b6ef8500a328c326f487677ed17b506259d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n@@ -57,7 +58,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/permutation_util.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n@@ -90,7 +90,7 @@ PointerType GetTensorPtrType(Type type) {\n       static_cast<unsigned>(mlir::NVVM::NVVMMemorySpace::Global));\n }\n \n-SmallVector<Value> IndexCast(::xla::EmitterLocOpBuilder& builder, Type type,\n+SmallVector<Value> IndexCast(ImplicitLocOpBuilder& builder, Type type,\n                              ValueRange values) {\n   SmallVector<Value> result;\n   result.reserve(values.size());\n@@ -233,7 +233,7 @@ bool CanUseTma(Operation* op, bool allow_tma, int num_stages,\n }\n \n // Add TMA attributes to the corresponding argument in the function.\n-void AddTmaAttributes(::xla::EmitterLocOpBuilder& builder,\n+void AddTmaAttributes(ImplicitLocOpBuilder& builder,\n                       const TypedValue<PointerType>& pointer,\n                       const ArrayRef<int64_t>& original_shape,\n                       const ArrayRef<int64_t>& layout,\n@@ -303,7 +303,7 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n  private:\n   mlir::LogicalResult matchAndRewrite(\n       func::FuncOp op, mlir::PatternRewriter& rewriter) const override {\n-    ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n+    ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n \n     auto input_types = op.getFunctionType().getInputs();\n \n@@ -410,7 +410,7 @@ SmallVector<unsigned> GetRetainedDims(ArrayRef<unsigned> reduced_dims,\n \n // Expands the value in all dimensions except `dim` and broadcasts the result\n // to the provided tile shape.\n-Value ExpandAndBroadcastValue(::xla::EmitterLocOpBuilder& builder, Value value,\n+Value ExpandAndBroadcastValue(ImplicitLocOpBuilder& builder, Value value,\n                               int dim, RankedTensorType tile_type) {\n   for (int i = 0; i < tile_type.getRank(); ++i) {\n     if (i != dim) {\n@@ -424,7 +424,7 @@ Value ExpandAndBroadcastValue(::xla::EmitterLocOpBuilder& builder, Value value,\n // - The first tensor is a tensor of pointers to load/store.\n // - The second tensor is a tensor of in-bounds predicates.\n static std::pair<Value, Value> CreateTensorOfPointersAndMask(\n-    ::xla::EmitterLocOpBuilder& builder, Value base_ptr,\n+    ImplicitLocOpBuilder& builder, Value base_ptr,\n     ArrayRef<int64_t> original_shape, ArrayRef<int64_t> layout,\n     ValueRange offsets, ArrayRef<int64_t> sizes, ArrayRef<int64_t> strides,\n     ArrayRef<unsigned> reduced_dims, ArrayRef<int64_t> tile_shape) {\n@@ -535,7 +535,7 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n   // tt.descriptor_load + tt.transpose.\n   mlir::LogicalResult matchAndRewrite(\n       ExtractOp op, mlir::PatternRewriter& rewriter) const override {\n-    ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n+    ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n     RankedTensorType tile_type = op.getType();\n     ArrayRef<int64_t> tile_shape = tile_type.getShape();\n     ArrayRef<int64_t> src_shape = op.getSrcShape();\n@@ -634,7 +634,7 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n   // tt.descriptor_store.\n   mlir::LogicalResult matchAndRewrite(\n       InsertOp op, mlir::PatternRewriter& rewriter) const override {\n-    ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n+    ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n     RankedTensorType tile_type = op.getSrc().getType();\n     ArrayRef<int64_t> tile_shape = tile_type.getShape();\n     ArrayRef<int64_t> dst_shape = op.getDstShape();\n@@ -712,7 +712,7 @@ class RewriteScalarInsert : public mlir::OpRewritePattern<tensor::InsertOp> {\n     if (op.getDest().getType().getRank() != 0) {\n       return rewriter.notifyMatchFailure(op, \"Expected dest to be scalar.\");\n     }\n-    ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n+    ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n     auto ptr_type = GetTensorPtrType(op.getScalar().getType());\n     auto cast_dst_to_tensor_ptr_type = mlir::UnrealizedConversionCastOp::create(\n                                            builder, ptr_type, op.getDest())\n@@ -736,7 +736,7 @@ class RewriteScalarExtract : public mlir::OpRewritePattern<tensor::ExtractOp> {\n     if (op.getTensor().getType().getRank() != 0) {\n       return rewriter.notifyMatchFailure(op, \"Expected src to be scalar.\");\n     }\n-    ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n+    ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n     auto ptr_type = GetTensorPtrType(op.getType());\n     auto cast_src_to_tensor_ptr_type = mlir::UnrealizedConversionCastOp::create(\n                                            builder, ptr_type, op.getTensor())"
        },
        {
            "sha": "131f05cc8a2642ea41f26d9669e18fee1e60fb81",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_math_to_libdevice.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -34,7 +34,6 @@ limitations under the License.\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/service/gpu/target_util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n@@ -191,7 +190,7 @@ class ConvertToLibdevice : public mlir::OpRewritePattern<OpTy> {\n       return rewriter.notifyMatchFailure(op, \"could not get primitive type\");\n     }\n \n-    ::xla::EmitterLocOpBuilder builder(op->getLoc(), rewriter);\n+    mlir::ImplicitLocOpBuilder builder(op->getLoc(), rewriter);\n \n     llvm::SmallVector<Value, 2> casted_inputs;\n     if (output_type_is_16bit_float) {"
        },
        {
            "sha": "8773d7045cbde4b96fd4ba7c3adc74295b12d55a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 20,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -129,7 +129,7 @@ namespace xgt = ::xla::gpu::triton;\n using ::llvm::SmallVector;\n using ::mlir::MLIRContext;\n \n-using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n+using ::xla::gpu::ir_emitter_triton_internal::GetModuleIrString;\n \n void LoadMlirDialectsForTriton(mlir::MLIRContext& mlir_context) {\n   mlir_context.loadDialect<\n@@ -250,13 +250,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n   if (DumpingEnabledForHloModule(*hlo_computation->parent()) &&\n       DumpingEnabledForEmitter(\"triton-fusion\", debug_options)) {\n     auto suffix = absl::StrCat(fusion->name(), \".before_validation.ttir.txt\");\n-    DumpToFileInDirOrStdout(\n-        *hlo_computation->parent(), \"\", suffix,\n-        DumpTritonIR(triton_module.get(),\n-                     fusion->GetModule()\n-                         ->config()\n-                         .debug_options()\n-                         .xla_gpu_unsupported_annotate_with_emitter_loc()));\n+    DumpToFileInDirOrStdout(*hlo_computation->parent(), \"\", suffix,\n+                            GetModuleIrString(triton_module.get()));\n     std::string fusion_suffix = absl::StrCat(fusion->name(), \".hlo\");\n     DumpToFileInDirOrStdout(\n         *hlo_computation->parent(), \"\", fusion_suffix,\n@@ -266,21 +261,12 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n   TF_RETURN_IF_ERROR(ir_emitter_triton_internal::LowerXTileToTriton(\n       triton_module.get(), mlir_context, *fusion, device_info));\n \n-  VLOG(6) << DumpTritonIR(triton_module.get(),\n-                          fusion->GetModule()\n-                              ->config()\n-                              .debug_options()\n-                              .xla_gpu_unsupported_annotate_with_emitter_loc());\n+  VLOG(6) << GetModuleIrString(triton_module.get());\n   if (DumpingEnabledForHloModule(*hlo_computation->parent()) &&\n       DumpingEnabledForEmitter(\"triton-fusion\", debug_options)) {\n     std::string suffix = absl::StrCat(fusion->name(), \".ttir.txt\");\n-    DumpToFileInDirOrStdout(\n-        *hlo_computation->parent(), \"\", suffix,\n-        DumpTritonIR(triton_module.get(),\n-                     fusion->GetModule()\n-                         ->config()\n-                         .debug_options()\n-                         .xla_gpu_unsupported_annotate_with_emitter_loc()));\n+    DumpToFileInDirOrStdout(*hlo_computation->parent(), \"\", suffix,\n+                            GetModuleIrString(triton_module.get()));\n   }\n \n   return std::move(triton_module);"
        },
        {
            "sha": "c0a6d7c393891e93eb73ebe5c8c8379c06fc2241",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler.h",
            "status": "modified",
            "additions": 4,
            "deletions": 17,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -33,7 +33,6 @@ limitations under the License.\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/autotuning.pb.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n@@ -109,24 +108,12 @@ std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n // Exposed for testing and experimental purposes only. Do not use.\n namespace ir_emitter_triton_internal {\n \n-// Computes the transformation from a 1-d program_id to a tile multi-index.\n-llvm::SmallVector<mlir::Value, 3> ComputeDelinearizedTileIndex(\n-    EmitterLocOpBuilder b, absl::Span<const int64_t> num_output_tiles_per_dim);\n-\n-// Dumps the Triton IR to a string.\n-//\n-// If `dump_annotations` is true, then the function also dumps the loc\n-// attributes of the instructions. Otherwise, it dumps the IR without\n-// annotations.\n-inline std::string DumpTritonIR(mlir::ModuleOp triton_module,\n-                                bool dump_annotations) {\n+// Returns the MLIR module as a string.\n+inline std::string GetModuleIrString(mlir::ModuleOp triton_module,\n+                                     mlir::OpPrintingFlags flags = {}) {\n   std::string triton_ir;\n   llvm::raw_string_ostream os(triton_ir);\n-  triton_module.print(os, mlir::OpPrintingFlags().enableDebugInfo(\n-                              dump_annotations, dump_annotations));\n-  if (dump_annotations) {\n-    return EmitterLocOpBuilder::FormatTritonIrWithAnnotations(triton_ir);\n-  }\n+  triton_module.print(os, flags);\n   return triton_ir;\n }\n "
        },
        {
            "sha": "f58bf888d460bbed50f4e68d9ccbc2fc73f78e94",
            "filename": "third_party/xla/xla/codegen/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -1,5 +1,4 @@\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n-load(\"//xla/tsl:tsl.bzl\", \"if_google\")\n load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n \n package(\n@@ -24,38 +23,6 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"emitter_loc_op_builder\",\n-    srcs = [\"emitter_loc_op_builder.cc\"],\n-    hdrs = [\"emitter_loc_op_builder.h\"],\n-    visibility = [\"//xla/backends:__subpackages__\"],\n-    deps = [\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//mlir:IR\",\n-        \"@llvm-project//mlir:Support\",\n-        \"@local_tsl//tsl/platform\",\n-    ] + if_google([\"@com_google_absl//absl/types:source_location\"]),\n-)\n-\n-xla_cc_test(\n-    name = \"emitter_loc_op_builder_test\",\n-    srcs = [\"emitter_loc_op_builder_test.cc\"],\n-    tags = [\"gpu\"],\n-    deps = [\n-        \":emitter_loc_op_builder\",\n-        \"//xla/backends/gpu/codegen/triton:xtile_compiler\",\n-        \"//xla/hlo/testlib:filecheck\",\n-        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/service/llvm_ir:llvm_util\",\n-        \"@com_google_absl//absl/status:status_matchers\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@llvm-project//mlir:ArithDialect\",\n-        \"@llvm-project//mlir:IR\",\n-    ],\n-)\n-\n cc_library(\n     name = \"kernel_emitter\",\n     hdrs = [\"kernel_emitter.h\"],"
        },
        {
            "sha": "32db4e476bbb0ccbb08827e3802f2cd71e5be9e9",
            "filename": "third_party/xla/xla/codegen/emitter_loc_op_builder.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 79,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder.cc?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -1,79 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n-\n-#include <algorithm>\n-#include <cstddef>\n-#include <string>\n-#include <vector>\n-\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/str_join.h\"\n-#include \"absl/strings/str_split.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/Location.h\"\n-#include \"mlir/Support/LLVM.h\"\n-\n-namespace xla {\n-\n-// Aligns the annotations to the Nth character of the lines.\n-constexpr size_t kAnnotationPadding = 100ul;\n-\n-/* static */ std::string EmitterLocOpBuilder::FormatTritonIrWithAnnotations(\n-    absl::string_view mlir_ir) {\n-  auto triton_with_annotations = absl::StrSplit(mlir_ir, '\\n');\n-  std::vector<std::string> formatted_lines;\n-  for (auto& line : triton_with_annotations) {\n-    std::vector<std::string> line_and_annotation = absl::StrSplit(line, '\"');\n-    constexpr int kInstructionLineFragments = 3;\n-    if (line_and_annotation.size() != kInstructionLineFragments) {\n-      // The line does not matches with the pattern:\n-      // x = instruction(y, z) \"annotation\"\n-      // So we just add it to the output as is.\n-      formatted_lines.emplace_back(line);\n-      continue;\n-    }\n-    auto text_size =\n-        std::min(line_and_annotation[0].size(), kAnnotationPadding);\n-    auto new_line =\n-        absl::StrCat(line_and_annotation[0],\n-                     std::string(kAnnotationPadding - text_size, ' '), \"\\\"\",\n-                     line_and_annotation[1], \"\\\"\", line_and_annotation[2]);\n-    formatted_lines.emplace_back(new_line);\n-  }\n-  return absl::StrJoin(formatted_lines, \"\\n\");\n-}\n-\n-mlir::Location EmitterLocOpBuilder::Loc(\n-    EmitterLocOpBuilder::SourceLocation location) const {\n-  if (!annotate_loc_ || location.line() == 0) {\n-    return current_loc_;\n-  }\n-  std::vector<std::string> file_name =\n-      absl::StrSplit(location.file_name(), '/');\n-  std::string previous_loc;\n-  if (mlir::isa<mlir::NameLoc>(current_loc_)) {\n-    auto name_loc = mlir::cast<mlir::NameLoc>(current_loc_);\n-    previous_loc = name_loc.getName().str();\n-  }\n-\n-  const std::string text = absl::StrCat(previous_loc, \" -> \", file_name.back(),\n-                                        \":\", location.line());\n-  return mlir::NameLoc::get(mlir::StringAttr::get(getContext(), text));\n-}\n-\n-}  // namespace xla"
        },
        {
            "sha": "cf1d9cfe6a36b5df31ca1722f5ea7dfb8085c0bf",
            "filename": "third_party/xla/xla/codegen/emitter_loc_op_builder.h",
            "status": "removed",
            "additions": 0,
            "deletions": 231,
            "changes": 231,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder.h?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -1,231 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_CODEGEN_EMITTER_LOC_OP_BUILDER_H_\n-#define XLA_CODEGEN_EMITTER_LOC_OP_BUILDER_H_\n-\n-#include <string>\n-\n-#include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n-#include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"tsl/platform/platform.h\"\n-\n-#if defined(PLATFORM_GOOGLE)\n-// The source_location.h is not available in open source.\n-#include \"absl/types/source_location.h\"\n-#endif\n-\n-namespace xla {\n-\n-// The builder that could add the NameLoc attribute to the newly created\n-// operations and fills this attribute with the SourceLocation(file:line) of the\n-// create<OpTy>(...) calls. The location info will be added to the current_loc_\n-// location that the builder got through the constructor. The copy constructor\n-// also remembers the source location where the copy was created.\n-//\n-// Why: it is useful for tracking up the emitter file and line from the\n-// generated MLIR.\n-//\n-// How:\n-// 1. create<OpTy>(...) functions have absl::SourceLocation as the last\n-// argument with the default value of SourceLocation::current(). Every time they\n-// construct a new NameLoc attribute that contains the string from the\n-// current_loc_ and file:line from the source location parameter.\n-//\n-// 2. The copy constructor also gets the source location as the argument and\n-// remembers it in the current_loc_ as a join of the original current_loc_ and\n-// the place where the copy was created.\n-class EmitterLocOpBuilder : public mlir::ImplicitLocOpBuilder {\n- public:\n-  // TODO(b/382419919): Remove ifdefs once we have absl::SourceLocation in absl\n-  // OSS builds.\n-#if defined(PLATFORM_GOOGLE)\n-  using SourceLocation = absl::SourceLocation;\n-  constexpr static bool kSourceLocationSupported = true;\n-#else\n-  // Mimicking absl::SourceLocation and doing nothing.\n-  class FakeSourceLocation {\n-   public:\n-    static FakeSourceLocation current() { return FakeSourceLocation(); }\n-    absl::string_view file_name() const { return \"\"; }\n-    int line() const { return 0; }\n-  };\n-  using SourceLocation = FakeSourceLocation;\n-  constexpr static bool kSourceLocationSupported = false;\n-#endif\n-\n-  // Constructor that takes the op builder and a flag indicating whether to\n-  // annotate the location of the operations.\n-  EmitterLocOpBuilder(mlir::ImplicitLocOpBuilder& op_builder, bool annotate_loc)\n-      : mlir::ImplicitLocOpBuilder(op_builder),\n-        annotate_loc_(annotate_loc),\n-        current_loc_(op_builder.getLoc()) {}\n-\n-  // A few constructors below that could be used when we replace the\n-  // mlir::ImplicitLocOpBuilder and mlir::OpBuilder one by one.\n-  // The intent is to use EmitterLocOpBuilder everywhere in the emitters.\n-\n-  // The constructor that should be used instead of mlir::ImplicitLocOpBuilder.\n-  EmitterLocOpBuilder(mlir::Location loc, mlir::OpBuilder& op_builder,\n-                      bool annotate_loc = false)\n-      : mlir::ImplicitLocOpBuilder(loc, op_builder),\n-\n-        annotate_loc_(annotate_loc),\n-        current_loc_(loc) {}\n-\n-  // The constructor that should be used instead of mlir::ImplicitLocOpBuilder.\n-  EmitterLocOpBuilder(mlir::Location loc, mlir::MLIRContext* mlir_context,\n-                      bool annotate_loc = false)\n-      : mlir::ImplicitLocOpBuilder(loc, mlir_context),\n-        annotate_loc_(annotate_loc),\n-        current_loc_(loc) {}\n-\n-  EmitterLocOpBuilder& operator=(const EmitterLocOpBuilder&) = delete;\n-\n-  // Copy constructor that also remembers the source location where the copy\n-  // was created. If the helper functions that gets the builder as the argument\n-  // receives the argument by value then the current location points to the\n-  // place where the copy was created.\n-  EmitterLocOpBuilder(const EmitterLocOpBuilder& builder,\n-                      SourceLocation location = SourceLocation::current())\n-      : mlir::ImplicitLocOpBuilder(builder),\n-        annotate_loc_(builder.annotate_loc_),\n-        current_loc_(builder.Loc(location)) {}\n-\n-  // Formats the MLIR IR with annotations to make it easier to read.\n-  static std::string FormatTritonIrWithAnnotations(absl::string_view mlir_ir);\n-\n-  // Below is the set of create() methods that are used to create operations.\n-  // These are all templated to allow for the creation of operations with\n-  // different numbers of arguments.\n-  //\n-  // For some reason the version of create that accepts the variadic arguments\n-  // and a source location with the default value does not work.\n-\n-  template <typename OpTy>\n-  OpTy create(SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(Loc(location));\n-  }\n-\n-  // Creates an operation with the given type and one argument.\n-  template <typename OpTy, typename Arg0>\n-  OpTy create(Arg0&& arg, SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(Loc(location), std::forward<Arg0>(arg));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(Loc(location), std::forward<Arg0>(arg0),\n-                                   std::forward<Arg1>(arg1));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1, typename Arg2>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1, Arg2&& arg2,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(Loc(location), std::forward<Arg0>(arg0),\n-                                   std::forward<Arg1>(arg1),\n-                                   std::forward<Arg2>(arg2));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1, typename Arg2,\n-            typename Arg3>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1, Arg2&& arg2, Arg3&& arg3,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(\n-        Loc(location), std::forward<Arg0>(arg0), std::forward<Arg1>(arg1),\n-        std::forward<Arg2>(arg2), std::forward<Arg3>(arg3));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1, typename Arg2,\n-            typename Arg3, typename Arg4>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1, Arg2&& arg2, Arg3&& arg3, Arg4&& arg4,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(\n-        Loc(location), std::forward<Arg0>(arg0), std::forward<Arg1>(arg1),\n-        std::forward<Arg2>(arg2), std::forward<Arg3>(arg3),\n-        std::forward<Arg4>(arg4));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1, typename Arg2,\n-            typename Arg3, typename Arg4, typename Arg5>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1, Arg2&& arg2, Arg3&& arg3, Arg4&& arg4,\n-              Arg5&& arg5,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(\n-        Loc(location), std::forward<Arg0>(arg0), std::forward<Arg1>(arg1),\n-        std::forward<Arg2>(arg2), std::forward<Arg3>(arg3),\n-        std::forward<Arg4>(arg4), std::forward<Arg5>(arg5));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1, typename Arg2,\n-            typename Arg3, typename Arg4, typename Arg5, typename Arg6>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1, Arg2&& arg2, Arg3&& arg3, Arg4&& arg4,\n-              Arg5&& arg5, Arg6&& arg6,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(\n-        Loc(location), std::forward<Arg0>(arg0), std::forward<Arg1>(arg1),\n-        std::forward<Arg2>(arg2), std::forward<Arg3>(arg3),\n-        std::forward<Arg4>(arg4), std::forward<Arg5>(arg5),\n-        std::forward<Arg6>(arg6));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1, typename Arg2,\n-            typename Arg3, typename Arg4, typename Arg5, typename Arg6,\n-            typename Arg7>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1, Arg2&& arg2, Arg3&& arg3, Arg4&& arg4,\n-              Arg5&& arg5, Arg6&& arg6, Arg7&& arg7,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(\n-        Loc(location), std::forward<Arg0>(arg0), std::forward<Arg1>(arg1),\n-        std::forward<Arg2>(arg2), std::forward<Arg3>(arg3),\n-        std::forward<Arg4>(arg4), std::forward<Arg5>(arg5),\n-        std::forward<Arg6>(arg6), std::forward<Arg7>(arg7));\n-  }\n-\n-  template <typename OpTy, typename Arg0, typename Arg1, typename Arg2,\n-            typename Arg3, typename Arg4, typename Arg5, typename Arg6,\n-            typename Arg7, typename Arg8>\n-  OpTy create(Arg0&& arg0, Arg1&& arg1, Arg2&& arg2, Arg3&& arg3, Arg4&& arg4,\n-              Arg5&& arg5, Arg6&& arg6, Arg7&& arg7, Arg8&& arg8,\n-              SourceLocation location = SourceLocation::current()) {\n-    return OpBuilder::create<OpTy>(\n-        Loc(location), std::forward<Arg0>(arg0), std::forward<Arg1>(arg1),\n-        std::forward<Arg2>(arg2), std::forward<Arg3>(arg3),\n-        std::forward<Arg4>(arg4), std::forward<Arg5>(arg5),\n-        std::forward<Arg6>(arg6), std::forward<Arg7>(arg7),\n-        std::forward<Arg8>(arg8));\n-  }\n-\n-  mlir::Location current_loc() const { return current_loc_; }\n-\n-  bool annotate_loc() const { return annotate_loc_; }\n-\n- private:\n-  // Helper function to create a location from a source location.\n-  mlir::Location Loc(SourceLocation location) const;\n-\n-  // Keep the current location of the builder and use it for annotating the\n-  // newly created operations.\n-  const bool annotate_loc_;\n-  const mlir::Location current_loc_;\n-};\n-\n-}  // namespace xla\n-\n-#endif  // XLA_CODEGEN_EMITTER_LOC_OP_BUILDER_H_"
        },
        {
            "sha": "1b440f5c009c9d3884f8defcc7d2403402546565",
            "filename": "third_party/xla/xla/codegen/emitter_loc_op_builder_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -1,96 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n-\n-#include <string>\n-\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-#include \"absl/status/status_matchers.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"mlir/IR/OwningOpRef.h\"\n-#include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n-#include \"xla/hlo/testlib/filecheck.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/service/llvm_ir/llvm_util.h\"\n-\n-namespace xla {\n-namespace {\n-\n-using mlir::NameLoc;\n-using mlir::StringAttr;\n-\n-using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n-\n-class EmitterLocOpBuilderTest : public HloHardwareIndependentTestBase {\n- protected:\n-  void SetUp() override { gpu::LoadMlirDialectsForTriton(mlir_context_); }\n-\n-  mlir::MLIRContext mlir_context_;\n-};\n-\n-NameLoc NameLoc(mlir::MLIRContext& context, absl::string_view name) {\n-  return NameLoc::get(StringAttr::get(&context, name));\n-}\n-\n-mlir::OwningOpRef<mlir::ModuleOp> MakeModuleWithOneOp(\n-    mlir::MLIRContext& context, EmitterLocOpBuilder& b) {\n-  auto loc = NameLoc(context, \"module\");\n-  auto triton_module = llvm_ir::CreateMlirModuleOp(loc);\n-  b.setInsertionPointToEnd(triton_module->getBody());\n-  auto i32_type = b.getI32Type();\n-  auto attr = b.getIntegerAttr(i32_type, 42);\n-  b.create<mlir::arith::ConstantOp>(attr);\n-  return triton_module;\n-}\n-\n-TEST_F(EmitterLocOpBuilderTest, IRWithAnnotations) {\n-  auto loc = NameLoc(mlir_context_, \"IRWithAnnotations\");\n-  EmitterLocOpBuilder b(loc, &mlir_context_, /*annotate_loc=*/true);\n-  auto triton_module = MakeModuleWithOneOp(mlir_context_, b);\n-  std::string ir = DumpTritonIR(triton_module.get(), /*dump_annotations=*/true);\n-  if constexpr (EmitterLocOpBuilder::kSourceLocationSupported) {\n-    EXPECT_THAT(RunFileCheck(ir, R\"(\n-      CHECK: \"IRWithAnnotations -> [[FILE:.*_test.cc]]:[[LINE:[0-9]+]]\"\n-    )\"),\n-                absl_testing::IsOkAndHolds(true));\n-  } else {\n-    EXPECT_THAT(RunFileCheck(ir, R\"(\n-      CHECK: \"IRWithAnnotations\"\n-    )\"),\n-                absl_testing::IsOkAndHolds(true));\n-  }\n-}\n-\n-TEST_F(EmitterLocOpBuilderTest, IRWithoutAnnotations) {\n-  auto loc = NameLoc(mlir_context_, \"IRWithoutAnnotations\");\n-  EmitterLocOpBuilder b(loc, &mlir_context_, /*annotate_loc=*/false);\n-  auto triton_module = MakeModuleWithOneOp(mlir_context_, b);\n-  std::string ir =\n-      DumpTritonIR(triton_module.get(), /*dump_annotations=*/false);\n-  EXPECT_THAT(RunFileCheck(ir, R\"(\n-    CHECK-NOT: IRWithoutAnnotations\n-  )\"),\n-              absl_testing::IsOkAndHolds(true));\n-}\n-\n-}  // namespace\n-}  // namespace xla"
        },
        {
            "sha": "08be4412c87166483395a5636f4630014bbf1d7a",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -203,7 +203,6 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_dump_large_constants(false);\n   opts.set_xla_dump_enable_mlir_pretty_form(true);\n   opts.set_xla_dump_full_hlo_config(true);\n-  opts.set_xla_gpu_unsupported_annotate_with_emitter_loc(false);\n   opts.set_xla_debug_buffer_assignment_show_max(15);\n   opts.set_xla_cpu_use_onednn(false);\n   opts.set_xla_cpu_experimental_onednn_custom_call(false);\n@@ -1311,15 +1310,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       \"xla_flags_reset\", bool_setter_for(&DebugOptions::set_xla_flags_reset),\n       debug_options->xla_flags_reset(),\n       \"Whether to reset XLA_FLAGS next time to parse.\"));\n-  flag_list->push_back(tsl::Flag(\n-      \"xla_gpu_unsupported_annotate_with_emitter_loc\",\n-      bool_setter_for(\n-          &DebugOptions::set_xla_gpu_unsupported_annotate_with_emitter_loc),\n-      debug_options->xla_gpu_unsupported_annotate_with_emitter_loc(),\n-      \"Forces emitters that use MLIR to annotate all the created MLIR \"\n-      \"instructions with the emitter's C++ source file and line number. The \"\n-      \"annotations should appear in the MLIR dumps. The emitters should use \"\n-      \"EmitterLocOpBuilder for that.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_dump_hlo_as_text\",\n       bool_setter_for(&DebugOptions::set_xla_dump_hlo_as_text),"
        },
        {
            "sha": "284aaec0ed9f8e3be5f54a737063fbdfb074ebe3",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d7332c4c01970851e5a1d34e17f2a2a81d76903/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=5d7332c4c01970851e5a1d34e17f2a2a81d76903",
            "patch": "@@ -919,11 +919,6 @@ message DebugOptions {\n   // memory, or have bugs.\n   optional bool xla_gpu_unsafe_fallback_to_driver_on_ptxas_not_found = 138;\n \n-  // If true, XLA will annotate instructions in the dumps with emitter code\n-  // location (source:line) annotations. This helps to identify the source of\n-  // the code that emits a particular instruction.\n-  optional bool xla_gpu_unsupported_annotate_with_emitter_loc = 358;\n-\n   // Internal testing flag to switch AllReduceDecomposer on or off.\n   optional bool xla_gpu_unsupported_enable_all_reduce_decomposer = 384;\n \n@@ -1427,14 +1422,15 @@ message DebugOptions {\n   reserved \"xla_hlo_dump_as_graphdef\";\n   reserved \"xla_hlo_tfgraph_device_scopes\";\n   reserved \"xla_use_shardy\";\n+  reserved \"xla_gpu_unsupported_annotate_with_emitter_loc\";\n \n   reserved 5, 63, 80, 93, 94, 98, 117, 130, 133, 134, 139, 141, 143, 152, 158,\n       160, 161, 162, 167, 168, 169, 171, 172, 173, 176, 177, 178, 179, 180, 183,\n       184, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206,\n       207, 211, 214, 218, 220, 221, 226, 229, 230, 233, 234, 238, 242, 249, 263,\n       264, 266, 270, 271, 275, 276, 278, 279, 281, 282, 286, 298, 299, 302, 303,\n-      309, 313, 314, 319, 320, 325, 326, 332, 346, 352, 361, 367, 369, 371, 385,\n-      398, 402, 423;\n+      309, 313, 314, 319, 320, 325, 326, 332, 346, 352, 358, 361, 367, 369, 371,\n+      385, 398, 402, 423;\n }\n \n // Contains flags which affects the GPU compilation result."
        }
    ],
    "stats": {
        "total": 822,
        "additions": 123,
        "deletions": 699
    }
}