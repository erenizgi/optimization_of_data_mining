{
    "author": "majiddadashi",
    "message": "Add a FuseQDQPass to the TFLite quantization pipeline.\n\nThis pass is designed to fuse dequantize and quantize operations into their surrounding ops, effectively creating quantized versions of those ops.\n\nThe pass includes several rewrite patterns to handle different scenarios:\n\n- FuseQDQ: The primary pattern for fusing QDQ chains.\n- PushForwardDrqFQ: Pushes dynamic-range fake-quant ops through padding ops to increase fusion opportunities.\n- RemoveUnusedFQ: Cleans up unused fake-quant ops.\n- QuantizeConstPattern: Folds quantize ops on constants into QConstOps.\n- Patterns for requantization (FuseDqQToRequant, FuseQQToRequant).\n\nThe pass can distinguish between static-range, dynamic-range, and weight-only quantization to apply the correct fusion logic.\n\nNew tests have been added to verify the pass's behavior in these different scenarios.\n\nPiperOrigin-RevId: 806457443",
    "sha": "5a18ee6f7b7ab0fa642f5761a450daef2df9776c",
    "files": [
        {
            "sha": "bf2596f83e032c57c0f22d121fe7885833e04ff1",
            "filename": "tensorflow/compiler/mlir/lite/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD?ref=5a18ee6f7b7ab0fa642f5761a450daef2df9776c",
            "patch": "@@ -1367,6 +1367,7 @@ cc_library(\n         \"transforms/prepare_quantize_dynamic_range.cc\",\n         \"transforms/prepare_quantize_helper.cc\",\n         \"transforms/quantization/bias_quantizer_pass.cc\",\n+        \"transforms/quantization/fuse_qdq_pass.cc\",\n         \"transforms/quantization/propagate_qsv_pass.cc\",\n         \"transforms/quantization/quant_utils.cc\",\n         \"transforms/quantize.cc\",\n@@ -1407,8 +1408,10 @@ cc_library(\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core:protos_all_cc\",\n         \"//tensorflow/core/platform:logging\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "c00829e63fd11c5374399a7b3ae59793495699f1",
            "filename": "tensorflow/compiler/mlir/lite/tests/fuse-qdq.mlir",
            "status": "added",
            "additions": 579,
            "deletions": 0,
            "changes": 579,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Ffuse-qdq.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Ffuse-qdq.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Ffuse-qdq.mlir?ref=5a18ee6f7b7ab0fa642f5761a450daef2df9776c",
            "patch": "@@ -0,0 +1,579 @@\n+// RUN: litert-opt %s -tfl-fuse-qdq | FileCheck %s\n+// CHECK-LABEL: QuantizeConvDRQ\n+func.func private @XlaCallModule_quant.fake_quant.impl_0(%arg0: tensor<1x4x4x3xf32>) -> tensor<1x4x4x3xf32>\n+func.func @QuantizeConvDRQ(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x4x4x1xf32>) {\n+  // CHECK:    %cst = arith.constant dense<0.000000e+00> : tensor<1xf32>\n+  %cst = arith.constant dense<0.000000e+00> : tensor<1xf32>\n+  %cst_0 = arith.constant dense<[[[[1.76285899, -0.257785767, 0.20429258], [1.16310906, 0.23124367, 0.529797196]], [[0.348971426, -0.319283515, -0.772461354], [0.316666812, 1.88180697, -1.78054631]]]]> : tensor<1x2x2x3xf32>\n+  %0 = stablehlo.composite \"quant.fake_quant\" %arg0 {composite_attributes = {dtype = \"i8\", narrow_range = false, quantization_dimension = 0 : i32, scale = dense<> : tensor<0xf64>, zero_point = dense<> : tensor<0xi64>}, decomposition = @XlaCallModule_quant.fake_quant.impl_0} : (tensor<1x4x4x3xf32>) -> tensor<1x4x4x3xf32>\n+  // CHECK{LITERAL}:    %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, value = dense<[[[[119, -17, 14], [78, 16, 36]], [[24, -22, -52], [21, 127, -120]]]]> : tensor<1x2x2x3xi8>}> : () -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %1 = \"tfl.quantize\"(%cst_0) <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>}> : (tensor<1x2x2x3xf32>) -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %2 = \"tfl.dequantize\"(%1) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  // CHECK:    %1 = \"tfl.conv_2d\"(%arg0, %0, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  %3 = \"tfl.conv_2d\"(%0, %2, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  // CHECK:    return %1 : tensor<1x4x4x1xf32>\n+  return %3 : tensor<1x4x4x1xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConvDrqWithPad\n+func.func private @XlaCallModule_quant.fake_quant.impl_1(%arg0: tensor<1x4x4x3xf32>) -> tensor<1x4x4x3xf32>\n+func.func @QuantizeConvDrqWithPad(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x6x6x1xf32>) {\n+  // CHECK:    %cst_0 = arith.constant dense<0.000000e+00> : tensor<1xf32>\n+  %cst = arith.constant dense<0.000000e+00> : tensor<1xf32>\n+  %cst_0 = arith.constant dense<[[[[1.76285899, -0.257785767, 0.20429258], [1.16310906, 0.23124367, 0.529797196]], [[0.348971426, -0.319283515, -0.772461354], [0.316666812, 1.88180697, -1.78054631]]]]> : tensor<1x2x2x3xf32>\n+  %0 = stablehlo.composite \"quant.fake_quant\" %arg0 {composite_attributes = {dtype = \"i8\", narrow_range = false, quantization_dimension = 0 : i32, scale = dense<> : tensor<0xf64>, zero_point = dense<> : tensor<0xi64>}, decomposition = @XlaCallModule_quant.fake_quant.impl_1} : (tensor<1x4x4x3xf32>) -> tensor<1x4x4x3xf32>\n+  // CHECK-LITERAL:    %cst = arith.constant dense<[[0, 0], [1, 1], [1, 1], [0, 0]]> : tensor<4x2xi32>\n+  %paddings = arith.constant dense<[[0, 0], [1, 1], [1, 1], [0, 0]]> : tensor<4x2xi32>\n+  // CHECK:    %0 = \"tfl.pad\"(%arg0, %cst) : (tensor<1x4x4x3xf32>, tensor<4x2xi32>) -> tensor<1x6x6x3xf32>\n+  %1 = \"tfl.pad\"(%0, %paddings) : (tensor<1x4x4x3xf32>, tensor<4x2xi32>) -> tensor<1x6x6x3xf32>\n+  // CHECK{LITERAL}:    %1 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, value = dense<[[[[119, -17, 14], [78, 16, 36]], [[24, -22, -52], [21, 127, -120]]]]> : tensor<1x2x2x3xi8>}> : () -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %2 = \"tfl.quantize\"(%cst_0) <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>}> : (tensor<1x2x2x3xf32>) -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %3 = \"tfl.dequantize\"(%2) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  // CHECK:    %2 = \"tfl.conv_2d\"(%0, %1, %cst_0) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x6x6x3xf32>, tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, tensor<1xf32>) -> tensor<1x6x6x1xf32>\n+  %4 = \"tfl.conv_2d\"(%1, %3, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x6x6x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x6x6x1xf32>\n+  // CHECK:    return %2 : tensor<1x6x6x1xf32>\n+  return %4 : tensor<1x6x6x1xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConvWithBiasDRQ\n+func.func @QuantizeConvWithBiasDRQ(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x4x4x1xf32>) {\n+  // CHECK:    %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+  %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+  %cst_0 = arith.constant dense<[[[[1.76285899, -0.257785767, 0.20429258], [1.16310906, 0.23124367, 0.529797196]], [[0.348971426, -0.319283515, -0.772461354], [0.316666812, 1.88180697, -1.78054631]]]]> : tensor<1x2x2x3xf32>\n+  %0 = stablehlo.composite \"quant.fake_quant\" %arg0 {composite_attributes = {dtype = \"i8\", narrow_range = false, quantization_dimension = 0 : i32, scale = dense<> : tensor<0xf64>, zero_point = dense<> : tensor<0xi64>}, decomposition = @XlaCallModule_quant.fake_quant.impl_0} : (tensor<1x4x4x3xf32>) -> tensor<1x4x4x3xf32>\n+  // CHECK{LITERAL}:    %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, value = dense<[[[[119, -17, 14], [78, 16, 36]], [[24, -22, -52], [21, 127, -120]]]]> : tensor<1x2x2x3xi8>}> : () -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %1 = \"tfl.quantize\"(%cst_0) <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>}> : (tensor<1x2x2x3xf32>) -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %2 = \"tfl.dequantize\"(%1) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  // CHECK:    %1 = \"tfl.conv_2d\"(%arg0, %0, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  %3 = \"tfl.conv_2d\"(%0, %2, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  // CHECK:    return %1 : tensor<1x4x4x1xf32>\n+  return %3 : tensor<1x4x4x1xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConvWithBiasAndReluDRQ\n+func.func @QuantizeConvWithBiasAndReluDRQ(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x4x4x1xf32>) {\n+  // CHECK: %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+  %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+  %cst_0 = arith.constant dense<[[[[1.76285899, -0.257785767, 0.20429258], [1.16310906, 0.23124367, 0.529797196]], [[0.348971426, -0.319283515, -0.772461354], [0.316666812, 1.88180697, -1.78054631]]]]> : tensor<1x2x2x3xf32>\n+  %0 = stablehlo.composite \"quant.fake_quant\" %arg0 {composite_attributes = {dtype = \"i8\", narrow_range = false, quantization_dimension = 0 : i32, scale = dense<> : tensor<0xf64>, zero_point = dense<> : tensor<0xi64>}, decomposition = @XlaCallModule_quant.fake_quant.impl_0} : (tensor<1x4x4x3xf32>) -> tensor<1x4x4x3xf32>\n+  // CHECK{LITERAL}: %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, value = dense<[[[[119, -17, 14], [78, 16, 36]], [[24, -22, -52], [21, 127, -120]]]]> : tensor<1x2x2x3xi8>}> : () -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %1 = \"tfl.quantize\"(%cst_0) <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>}> : (tensor<1x2x2x3xf32>) -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %2 = \"tfl.dequantize\"(%1) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  // CHECK: %1 = \"tfl.conv_2d\"(%arg0, %0, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  %3 = \"tfl.conv_2d\"(%0, %2, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  // CHECK: return %1 : tensor<1x4x4x1xf32>\n+  return %3 : tensor<1x4x4x1xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConvWithBiasAndReluWeightOnly\n+func.func @QuantizeConvWithBiasAndReluWeightOnly(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x4x4x1xf32>) {\n+  // CHECK:  %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+  %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+  %cst_0 = arith.constant dense<[[[[1.76285899, -0.257785767, 0.20429258], [1.16310906, 0.23124367, 0.529797196]], [[0.348971426, -0.319283515, -0.772461354], [0.316666812, 1.88180697, -1.78054631]]]]> : tensor<1x2x2x3xf32>\n+  // CHECK{LITERAL}:  %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, value = dense<[[[[119, -17, 14], [78, 16, 36]], [[24, -22, -52], [21, 127, -120]]]]> : tensor<1x2x2x3xi8>}> : () -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %0 = \"tfl.quantize\"(%cst_0) <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>}> : (tensor<1x2x2x3xf32>) -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  // CHECK:  %1 = \"tfl.dequantize\"(%0) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  // CHECK:  %2 = \"tfl.conv_2d\"(%arg0, %1, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  %2 = \"tfl.conv_2d\"(%arg0, %1, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  // CHECK:  return %2 : tensor<1x4x4x1xf32>\n+  return %2 : tensor<1x4x4x1xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConvWithBiasAndReluSRQ\n+func.func @QuantizeConvWithBiasAndReluSRQ(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x4x4x1xf32>) {\n+  %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+  // CHECK: %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>, value = dense<20578> : tensor<1xi32>}> : () -> tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>}> : (tensor<1xf32>) -> tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>) -> tensor<1xf32>\n+  %cst_0 = arith.constant dense<[[[[1.76285899, -0.257785767, 0.20429258], [1.16310906, 0.23124367, 0.529797196]], [[0.348971426, -0.319283515, -0.772461354], [0.316666812, 1.88180697, -1.78054631]]]]> : tensor<1x2x2x3xf32>\n+  // CHECK: %1 = \"tfl.quantize\"(%arg0) <{qtype = tensor<1x4x4x3x!quant.uniform<i8:f32, 0.0037634586915373802:-128>>}> : (tensor<1x4x4x3xf32>) -> tensor<1x4x4x3x!quant.uniform<i8:f32, 0.0037634586915373802:-128>>\n+  %2 = \"tfl.quantize\"(%arg0) <{qtype = tensor<1x4x4x3x!quant.uniform<i8:f32, 0.0037634586915373802:-128>>}> : (tensor<1x4x4x3xf32>) -> tensor<1x4x4x3x!quant.uniform<i8:f32, 0.0037634586915373802:-128>>\n+  %3 = \"tfl.dequantize\"(%2) : (tensor<1x4x4x3x!quant.uniform<i8:f32, 0.0037634586915373802:-128>>) -> tensor<1x4x4x3xf32>\n+  // CHECK{LITERAL}: %2 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, value = dense<[[[[119, -17, 14], [78, 16, 36]], [[24, -22, -52], [21, 127, -120]]]]> : tensor<1x2x2x3xi8>}> : () -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %4 = \"tfl.quantize\"(%cst_0) <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>}> : (tensor<1x2x2x3xf32>) -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %5 = \"tfl.dequantize\"(%4) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  // CHECK: %3 = \"tfl.conv_2d\"(%1, %2, %0) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3x!quant.uniform<i8:f32, 0.0037634586915373802:-128>>, tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>) -> tensor<1x4x4x1x!quant.uniform<i8:f32, 0.013401651754975319:-128>>\n+  %6 = \"tfl.conv_2d\"(%3, %5, %1) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x4x4x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x4x4x1xf32>\n+  %7 = \"tfl.quantize\"(%6) <{qtype = tensor<1x4x4x1x!quant.uniform<i8:f32, 0.013401651754975319:-128>>}> : (tensor<1x4x4x1xf32>) -> tensor<1x4x4x1x!quant.uniform<i8:f32, 0.013401651754975319:-128>>\n+  // CHECK: %4 = \"tfl.dequantize\"(%3) : (tensor<1x4x4x1x!quant.uniform<i8:f32, 0.013401651754975319:-128>>) -> tensor<1x4x4x1xf32>\n+  %8 = \"tfl.dequantize\"(%7) : (tensor<1x4x4x1x!quant.uniform<i8:f32, 0.013401651754975319:-128>>) -> tensor<1x4x4x1xf32>\n+  // CHECK: return %4 : tensor<1x4x4x1xf32>\n+  return %8 : tensor<1x4x4x1xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeEmbeddingLookupDrq\n+func.func @QuantizeEmbeddingLookupDrq(%arg0: tensor<2xi32>) -> (tensor<2x4xf32>){\n+  %cst = arith.constant dense<[[1.0545162, -0.969288647, -0.594602108, -0.0318857245], [2.41093326, -1.87844908, -0.784769594, -0.313708425], [0.333708912, 1.76770353, -1.02776456, 1.41117179], [-0.508497119, -0.526377499, 0.503150403, 1.05497932], [-0.0874073281, 0.795816719, 2.65656161, -0.58229059]]> : tensor<5x4xf32>\n+  // CHECK{LITERAL}: %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<5x4x!quant.uniform<i8:f32:0, {0.0082384077832102776,0.018835416063666344,0.013810183852910995,0.0082420259714126587,0.020754387602210045}>>, value = dense<[[127, -118, -72, -4], [127, -100, -42, -17], [24, 127, -74, 102], [-62, -64, 61, 127], [-4, 38, 127, -28]]> : tensor<5x4xi8>}> : () -> tensor<5x4x!quant.uniform<i8:f32:0, {0.0082384077832102776,0.018835416063666344,0.013810183852910995,0.0082420259714126587,0.020754387602210045}>>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<5x4x!quant.uniform<i8:f32:0, {0.0082384077832102776,0.018835416063666344,0.013810183852910995,0.0082420259714126587,0.020754387602210045}>>}> : (tensor<5x4xf32>) -> tensor<5x4x!quant.uniform<i8:f32:0, {0.0082384077832102776,0.018835416063666344,0.013810183852910995,0.0082420259714126587,0.020754387602210045}>>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<5x4x!quant.uniform<i8:f32:0, {0.0082384077832102776,0.018835416063666344,0.013810183852910995,0.0082420259714126587,0.020754387602210045}>>) -> tensor<5x4xf32>\n+  // CHECK: %1 = \"tfl.embedding_lookup\"(%arg0, %0) : (tensor<2xi32>, tensor<5x4x!quant.uniform<i8:f32:0, {0.0082384077832102776,0.018835416063666344,0.013810183852910995,0.0082420259714126587,0.020754387602210045}>>) -> tensor<2x4xf32>\n+  %2 = \"tfl.embedding_lookup\"(%arg0, %1) : (tensor<2xi32>, tensor<5x4xf32>) -> tensor<2x4xf32>\n+  // CHECK: return %1 : tensor<2x4xf32>\n+  return %2 : tensor<2x4xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: DQQToRequantize\n+func.func @DQQToRequantize(%arg0: tensor<1x128x128x320x!quant.uniform<i8:f32, 0.17072822153568268:6>>) -> (tensor<1x128x128x320x!quant.uniform<i8:f32, 0.1043805405497551:-6>>) {\n+    %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x128x128x320x!quant.uniform<i8:f32, 0.17072822153568268:6>>) -> tensor<1x128x128x320xf32>\n+// CHECK:    %0 = \"tfl.quantize\"(%arg0) <{qtype = tensor<1x128x128x320x!quant.uniform<i8:f32, 0.1043805405497551:-6>>}> : (tensor<1x128x128x320x!quant.uniform<i8:f32, 0.17072822153568268:6>>) -> tensor<1x128x128x320x!quant.uniform<i8:f32, 0.1043805405497551:-6>>\n+    %1 = \"tfl.quantize\"(%0) <{qtype = tensor<1x128x128x320x!quant.uniform<i8:f32, 0.1043805405497551:-6>>}> : (tensor<1x128x128x320xf32>) -> tensor<1x128x128x320x!quant.uniform<i8:f32, 0.1043805405497551:-6>>\n+// CHECK:    return %0 : tensor<1x128x128x320x!quant.uniform<i8:f32, 0.1043805405497551:-6>>\n+    return %1 : tensor<1x128x128x320x!quant.uniform<i8:f32, 0.1043805405497551:-6>>\n+}\n+\n+// -----\n+\n+func.func @VolatileQuantizeConst() -> (tensor<1xf32>) {\n+  %cst = arith.constant dense<1.14751196> : tensor<1xf32>\n+// CHECK: %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>, value = dense<20578> : tensor<1xi32>}> {volatile} : () -> tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>}> {volatile} : (tensor<1xf32>) -> tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>\n+// CHECK: %1 = \"tfl.dequantize\"(%0) : (tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>) -> tensor<1xf32>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<1x!quant.uniform<i32:f32, 5.576458833533339E-5>>) -> tensor<1xf32>\n+// CHECK: return %1 : tensor<1xf32>\n+  return %1 : tensor<1xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeFloatConst\n+func.func @QuantizeFloatConst() -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>> {\n+  %0 = arith.constant dense<-0.1> : tensor<2x2xf32>\n+// CHECK:  %[[cst:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>, value = dense<0> : tensor<2x2xi8>}>\n+  %1 = \"tfl.quantize\"(%0) {qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>} : (tensor<2x2xf32>) -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+// CHECK:  return %[[cst]]\n+  func.return %1 : tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeFloatConst4Bits\n+func.func @QuantizeFloatConst4Bits() -> tensor<2x4x!quant.uniform<i4:f32, 2.500000e-01:-1>> {\n+  %0 = arith.constant dense<[[-0.75, -0.5, -0.25, 0.0], [0.25, 0.5, 0.75, 1.0]]> : tensor<2x4xf32>\n+// CHECK:  %[[cst:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<2x4x!quant.uniform<i4:f32, 2.500000e-01:-1>>, value = dense<{{\\[\\[}}-4, -3, -2, -1{{\\]}}, [0, 1, 2, 3{{\\]\\]}}> : tensor<2x4xi4>}>\n+  %1 = \"tfl.quantize\"(%0) {qtype = tensor<2x4x!quant.uniform<i4:f32, 2.500000e-01:-1>>} : (tensor<2x4xf32>) -> tensor<2x4x!quant.uniform<i4:f32, 2.500000e-01:-1>>\n+// CHECK:  return %[[cst]]\n+  func.return %1 : tensor<2x4x!quant.uniform<i4:f32, 2.500000e-01:-1>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeDenseFloatConst\n+func.func @QuantizeDenseFloatConst() -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>> {\n+  %0 = arith.constant dense<[[-0.1, 1.0], [1.0, 3.0]]> : tensor<2x2xf32>\n+// CHECK:  %[[cst:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>, value = dense<{{\\[\\[}}0, -1], {{\\[}}-1, -1]]> : tensor<2x2xi8>}>\n+  %1 = \"tfl.quantize\"(%0) {qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>} : (tensor<2x2xf32>) -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+// CHECK:  return %[[cst]]\n+  func.return %1 : tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeSplatFloatConst\n+func.func @QuantizeSplatFloatConst() -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>> {\n+  %0 = arith.constant dense<3.0> : tensor<2x2xf32>\n+// CHECK:  %[[cst:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>, value = dense<-1> : tensor<2x2xi8>}>\n+  %1 = \"tfl.quantize\"(%0) {qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>} : (tensor<2x2xf32>) -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+// CHECK:  return %[[cst]]\n+  func.return %1 : tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: DequantizeAndQuantize\n+func.func @DequantizeAndQuantize() -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>> {\n+// CHECK:  %[[cst:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>, value = dense<-1> : tensor<2x2xi8>}>\n+  %cst = \"tfl.pseudo_qconst\"() {qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>, value = dense<-1> : tensor<2x2xi8>} : () -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+  %0 = \"tfl.dequantize\"(%cst) : (tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>) -> tensor<2x2xf32>\n+  %1 = \"tfl.quantize\"(%0) {qtype = tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>} : (tensor<2x2xf32>) -> tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+// CHECK:  return %[[cst]] : tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+  func.return %1 : tensor<2x2x!quant.uniform<u8:f32, 7.8431372549019615E-4:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConv2D\n+func.func @QuantizeConv2D(tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>> {\n+^bb0(%arg0: tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %cst = arith.constant dense<-1.23697901> : tensor<32xf32>\n+// CHECK: %[[cst0:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>, value = dense<-1583> : tensor<32xi32>}>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>}> : (tensor<32xf32>) -> tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>) -> tensor<32xf32>\n+  %2 = \"tfl.dequantize\"(%arg0) : (tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x224x224x3xf32>\n+  %w = arith.constant dense<-1.0> : tensor<32x3x3x3xf32>\n+// CHECK: %[[cst1:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 1.000000e-01>>, value = dense<1> : tensor<32x3x3x3xi8>}>\n+  %3 = \"tfl.quantize\"(%w) {qtype = tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 0.1>>} : (tensor<32x3x3x3xf32>) -> tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 0.1>>\n+  %4 = \"tfl.dequantize\"(%3) : (tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 0.1>>) -> tensor<32x3x3x3xf32>\n+// CHECK: %[[conv:.*]] = \"tfl.conv_2d\"(%arg0, %[[cst1]], %[[cst0]])\n+  %5 = \"tfl.conv_2d\"(%2, %4, %1) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x224x224x3xf32>, tensor<32x3x3x3xf32>, tensor<32xf32>) -> tensor<1x112x112x32xf32>\n+  %6 = \"tfl.quantize\"(%5) {qtype = tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>} : (tensor<1x112x112x32xf32>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+// CHECK: return %[[conv]] : tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+  func.return %6 : tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConv2D4Bit\n+func.func @QuantizeConv2D4Bit(tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>> {\n+^bb0(%arg0: tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %cst = arith.constant dense<-1.23697901> : tensor<32xf32>\n+// CHECK: %[[cst0:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>, value = dense<-1583> : tensor<32xi32>}>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>}> : (tensor<32xf32>) -> tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<32x!quant.uniform<i32:f32, 7.812500e-04>>) -> tensor<32xf32>\n+  %2 = \"tfl.dequantize\"(%arg0) : (tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x224x224x3xf32>\n+  %w = arith.constant dense<-1.0> : tensor<32x3x3x3xf32>\n+// CHECK: %[[cst1:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x3x3x3x!quant.uniform<u4<1:15>:f32, 1.000000e-01>>, value = dense<1> : tensor<32x3x3x3xi4>}>\n+  %3 = \"tfl.quantize\"(%w) {qtype = tensor<32x3x3x3x!quant.uniform<u4<1:15>:f32, 0.1>>} : (tensor<32x3x3x3xf32>) -> tensor<32x3x3x3x!quant.uniform<u4<1:15>:f32, 0.1>>\n+  %4 = \"tfl.dequantize\"(%3) : (tensor<32x3x3x3x!quant.uniform<u4<1:15>:f32, 0.1>>) -> tensor<32x3x3x3xf32>\n+// CHECK: %[[conv:.*]] = \"tfl.conv_2d\"(%arg0, %[[cst1]], %[[cst0]])\n+  %5 = \"tfl.conv_2d\"(%2, %4, %1) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x224x224x3xf32>, tensor<32x3x3x3xf32>, tensor<32xf32>) -> tensor<1x112x112x32xf32>\n+  %6 = \"tfl.quantize\"(%5) {qtype = tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>} : (tensor<1x112x112x32xf32>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+// CHECK: return %[[conv]] : tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+  func.return %6 : tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeDepthwiseConv2D\n+func.func @QuantizeDepthwiseConv2D(tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>> {\n+^bb0(%arg0: tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %cst = arith.constant dense<-1.23697901> : tensor<32xf32>\n+// CHECK: %[[cst0:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>, value = dense<-7254> : tensor<32xi32>}>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>}> : (tensor<32xf32>) -> tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>) -> tensor<32xf32>\n+  %2 = \"tfl.dequantize\"(%arg0) : (tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x224x224x3xf32>\n+// CHECK: %[[cst1:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>, value = dense<-76> : tensor<32x3x3x3xi8>}>\n+  %3 = \"tfl.pseudo_qconst\"() {qtype = tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>, value = dense<-76> : tensor<32x3x3x3xi8>} : () -> tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>\n+  %4 = \"tfl.dequantize\"(%3) : (tensor<32x3x3x3x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>) -> tensor<32x3x3x3xf32>\n+// CHECK: %[[conv:.*]] = \"tfl.depthwise_conv_2d\"(%arg0, %[[cst1]], %[[cst0]]) <{depth_multiplier = 4 : i32, dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 4 : i32, stride_w = 5 : i32}>\n+  %5 = \"tfl.depthwise_conv_2d\"(%2, %4, %1) {depth_multiplier = 4 : i32, dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 4 : i32, stride_w = 5 : i32} : (tensor<1x224x224x3xf32>, tensor<32x3x3x3xf32>, tensor<32xf32>) -> tensor<1x112x112x32xf32>\n+  %6 = \"tfl.quantize\"(%5) {qtype = tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>} : (tensor<1x112x112x32xf32>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+// CHECK: return %[[conv]]\n+  func.return %6 : tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeFullyConnected\n+func.func @QuantizeFullyConnected(tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>> {\n+^bb0(%arg0: tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %cst = arith.constant dense<-1.23697901> : tensor<32xf32>\n+// CHECK: %[[cst_0:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>, value = dense<-7254> : tensor<32xi32>}>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>}> : (tensor<32xf32>) -> tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<32x!quant.uniform<i32:f32, 1.7052092479439231E-4>>) -> tensor<32xf32>\n+  %2 = \"tfl.dequantize\"(%arg0) : (tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x224x224x3xf32>\n+// CHECK: %[[cst_1:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x12x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>, value = dense<-76> : tensor<32x12xi8>}>\n+  %3 = \"tfl.pseudo_qconst\"() {qtype = tensor<32x12x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>, value = dense<-76> : tensor<32x12xi8>} : () -> tensor<32x12x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>\n+  %4 = \"tfl.dequantize\"(%3) : (tensor<32x12x!quant.uniform<u8<1:255>:f32, 0.021826678373682216:151>>) -> tensor<32x12xf32>\n+// CHECK: %[[fc:.*]] = \"tfl.fully_connected\"(%arg0, %[[cst_1]], %[[cst_0]]) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}>\n+  %5 = \"tfl.fully_connected\"(%2, %4, %1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<1x224x224x3xf32>, tensor<32x12xf32>, tensor<32xf32>) -> tensor<1x112x112x32xf32>\n+  %6 = \"tfl.quantize\"(%5) {qtype = tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>} : (tensor<1x112x112x32xf32>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+// CHECK: return %[[fc]]\n+  func.return %6 : tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeFullyConnected4Bit\n+func.func @QuantizeFullyConnected4Bit(tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>> {\n+^bb0(%arg0: tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %cst = arith.constant dense<-1.23697901> : tensor<32xf32>\n+// CHECK: %[[cst_0:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x!quant.uniform<i32:f32, 0.0030937367812500002>>, value = dense<-400> : tensor<32xi32>}>\n+  %0 = \"tfl.quantize\"(%cst) <{qtype = tensor<32x!quant.uniform<i32:f32, 0.0030937367812500002>>}> : (tensor<32xf32>) -> tensor<32x!quant.uniform<i32:f32, 0.0030937367812500002>>\n+  %1 = \"tfl.dequantize\"(%0) : (tensor<32x!quant.uniform<i32:f32, 0.0030937367812500002>>) -> tensor<32xf32>\n+  %2 = \"tfl.dequantize\"(%arg0) : (tensor<1x224x224x3x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x224x224x3xf32>\n+// CHECK: %[[cst_1:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<32x12x!quant.uniform<u4<1:15>:f32, 0.39599830800000002:8>>, value = dense<-7> : tensor<32x12xi4>}>\n+  %3 = \"tfl.pseudo_qconst\"() {qtype = tensor<32x12x!quant.uniform<u4<1:15>:f32, 0.395998308:8>>, value = dense<-7> : tensor<32x12xi4>} : () -> tensor<32x12x!quant.uniform<u4<1:15>:f32, 0.395998308:8>>\n+  %4 = \"tfl.dequantize\"(%3) : (tensor<32x12x!quant.uniform<u4<1:15>:f32, 0.395998308:8>>) -> tensor<32x12xf32>\n+// CHECK: %[[fc:.*]] = \"tfl.fully_connected\"(%arg0, %[[cst_1]], %[[cst_0]]) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}>\n+  %5 = \"tfl.fully_connected\"(%2, %4, %1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<1x224x224x3xf32>, tensor<32x12xf32>, tensor<32xf32>) -> tensor<1x112x112x32xf32>\n+  %6 = \"tfl.quantize\"(%5) {qtype = tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>} : (tensor<1x112x112x32xf32>) -> tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+// CHECK: return %[[fc]]\n+  func.return %6 : tensor<1x112x112x32x!quant.uniform<u8:f32, 0.023528476789885875>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeNoBiasFullyConnected\n+func.func @QuantizeNoBiasFullyConnected(%arg0: tensor<3x!quant.uniform<u8:f32, 1.0>>, %arg1: tensor<3x3x!quant.uniform<u8<1:255>:f32, 1.0>>, %arg2: none) -> tensor<3x!quant.uniform<u8:f32, 1.0>> {\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<3x!quant.uniform<u8:f32, 1.0>>) -> tensor<3xf32>\n+  %1 = \"tfl.dequantize\"(%arg1) : (tensor<3x3x!quant.uniform<u8<1:255>:f32, 1.0>>) -> tensor<3x3xf32>\n+// CHECK: %[[fc:.*]] = \"tfl.fully_connected\"(%arg0, %arg1, %arg2)\n+  %2 = \"tfl.fully_connected\"(%0, %1, %arg2) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<3xf32>, tensor<3x3xf32>, none) -> tensor<3xf32>\n+  %3 = \"tfl.quantize\"(%2) {qtype = tensor<3x!quant.uniform<u8:f32, 1.0>>} : (tensor<3xf32>) -> tensor<3x!quant.uniform<u8:f32, 1.0>>\n+// CHECK: return %[[fc]]\n+  func.return %3 : tensor<3x!quant.uniform<u8:f32, 1.0>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeAveragePool2D\n+func.func @QuantizeAveragePool2D(tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>> {\n+^bb0(%arg0: tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16xf32>\n+// CHECK: %[[avgp:.*]] = \"tfl.average_pool_2d\"(%arg0)\n+  %1 = \"tfl.average_pool_2d\"(%0) {name = \"avgpool\", filter_height = 3 : i32, filter_width = 6 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 3 : i32, stride_w = 1 : i32} : (tensor<1x6x6x16xf32>) -> tensor<1x1x1x16xf32>\n+  %2 = \"tfl.quantize\"(%1) {qtype = tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>} : (tensor<1x1x1x16xf32>) -> tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+// CHECK: return %[[avgp]] : tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+  func.return %2 : tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+}\n+\n+// -----\n+\n+// This behavior is intentioally different than the legacy quantized pass.\n+// [quantized value] -> [DQ] -> [Float] pattern is no longer quantized.\n+// CHECK-LABEL: NoQuantizeAveragePool2D\n+func.func @NoQuantizeAveragePool2D(tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x1x1x16xf32> {\n+^bb0(%arg0: tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+// CHECK: %[[DQ:.*]] = \"tfl.dequantize\"(%arg0)\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16xf32>\n+// CHECK: %[[AVGP:.*]] = \"tfl.average_pool_2d\"(%[[DQ]])\n+  %1 = \"tfl.average_pool_2d\"(%0) {name = \"avgpool\", filter_height = 3 : i32, filter_width = 6 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 3 : i32, stride_w = 1 : i32} : (tensor<1x6x6x16xf32>) -> tensor<1x1x1x16xf32>\n+// CHECK: return %[[AVGP]] : tensor<1x1x1x16xf32>\n+  func.return %1 : tensor<1x1x1x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeReshape2D\n+func.func @QuantizeReshape2D(tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x36x16x!quant.uniform<u8:f32, 7.812500e-03:128>> {\n+^bb0(%arg0: tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %cst = arith.constant dense<[1, 36, 16]> : tensor<3xi32>\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16xf32>\n+// CHECK: %[[rs:.*]] = \"tfl.reshape\"(%arg0, %{{.*}})\n+  %1 = \"tfl.reshape\"(%0, %cst) : (tensor<1x6x6x16xf32>, tensor<3xi32>) -> tensor<1x36x16xf32>\n+  %2 = \"tfl.quantize\"(%1) {qtype = tensor<1x36x16x!quant.uniform<u8:f32, 7.812500e-03:128>>} : (tensor<1x36x16xf32>) -> tensor<1x36x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+// CHECK: return %[[rs]] : tensor<1x36x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+  func.return %2 : tensor<1x36x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+}\n+\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeSoftmax\n+func.func @QuantizeSoftmax(tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>> {\n+^bb0(%arg0: tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16xf32>\n+// CHECK: %[[sm:.*]] = \"tfl.softmax\"(%arg0)\n+  %1 = \"tfl.softmax\"(%0) {beta = 1.000000e+00 : f32} : (tensor<1x6x6x16xf32>) -> tensor<1x6x6x16xf32>\n+  %2 = \"tfl.quantize\"(%1) {qtype = tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>} : (tensor<1x6x6x16xf32>) -> tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>\n+// CHECK: return %[[sm]] : tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>\n+  func.return %2 : tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeLogistic\n+func.func @QuantizeLogistic(tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>> {\n+^bb0(%arg0: tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16xf32>\n+// CHECK: %[[lg:.*]] = \"tfl.logistic\"(%arg0) : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>)\n+  %1 = \"tfl.logistic\"(%0) : (tensor<1x6x6x16xf32>) -> tensor<1x6x6x16xf32>\n+  %2 = \"tfl.quantize\"(%1) {qtype = tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>} : (tensor<1x6x6x16xf32>) -> tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>\n+// CHECK: return %[[lg]] : tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>\n+  func.return %2 : tensor<1x6x6x16x!quant.uniform<u8:f32, 3.906250e-03>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeAdd\n+func.func @QuantizeAdd(tensor<1x56x56x24x!quant.uniform<u8:f32, 0.27583434161017922:119>>, tensor<1x56x56x24x!quant.uniform<u8:f32, 0.40149296779258581:136>>) -> tensor<1x56x56x24x!quant.uniform<u8:f32, 0.4321689530914905:133>> {\n+^bb0(%arg0: tensor<1x56x56x24x!quant.uniform<u8:f32, 0.27583434161017922:119>>, %arg1: tensor<1x56x56x24x!quant.uniform<u8:f32, 0.40149296779258581:136>>):\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x56x56x24x!quant.uniform<u8:f32, 0.27583434161017922:119>>) -> tensor<1x56x56x24xf32>\n+  %1 = \"tfl.dequantize\"(%arg1) : (tensor<1x56x56x24x!quant.uniform<u8:f32, 0.40149296779258581:136>>) -> tensor<1x56x56x24xf32>\n+// CHECK: %[[add:.*]] = tfl.add(%arg0, %arg1) <{fused_activation_function = \"NONE\"}> : (tensor<1x56x56x24x!quant.uniform<u8:f32, 0.27583434161017922:119>>, tensor<1x56x56x24x!quant.uniform<u8:f32, 0.40149296779258581:136>>)\n+  %2 = tfl.add %0, %1 {fused_activation_function = \"NONE\"} : tensor<1x56x56x24xf32> loc(\"Block\")\n+  %3 = \"tfl.quantize\"(%2) {qtype = tensor<1x56x56x24x!quant.uniform<u8:f32, 0.4321689530914905:133>>} : (tensor<1x56x56x24xf32>) -> tensor<1x56x56x24x!quant.uniform<u8:f32, 0.4321689530914905:133>>\n+// CHECK: return %[[add]] : tensor<1x56x56x24x!quant.uniform<u8:f32, 0.4321689530914905:133>>\n+  func.return %3 : tensor<1x56x56x24x!quant.uniform<u8:f32, 0.4321689530914905:133>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeConcat\n+func.func @QuantizeConcat(tensor<1x2x!quant.uniform<u8:f32, 1.000000e-01:128>>, tensor<1x2x!quant.uniform<u8:f32, 1.000000e-01:128>>) -> tensor<2x2x!quant.uniform<u8:f32, 1.000000e-01:128>> {\n+^bb0(%arg0: tensor<1x2x!quant.uniform<u8:f32, 1.000000e-01:128>>, %arg1: tensor<1x2x!quant.uniform<u8:f32, 1.000000e-01:128>>):\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x2x!quant.uniform<u8:f32, 1.000000e-01:128>>) -> tensor<1x2xf32>\n+  %1 = \"tfl.dequantize\"(%arg1) : (tensor<1x2x!quant.uniform<u8:f32, 1.000000e-01:128>>) -> tensor<1x2xf32>\n+// CHECK: %[[cc:.*]] = \"tfl.concatenation\"(%arg0, %arg1) <{axis = 0 : i32, fused_activation_function = \"NONE\"}>\n+  %2 = \"tfl.concatenation\"(%0, %1) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n+  %3 = \"tfl.quantize\"(%2) {qtype = tensor<2x2x!quant.uniform<u8:f32, 1.000000e-01:128>>} : (tensor<2x2xf32>) -> tensor<2x2x!quant.uniform<u8:f32, 1.000000e-01:128>>\n+// CHECK: return %[[cc]] : tensor<2x2x!quant.uniform<u8:f32, 1.000000e-01:128>>\n+  func.return %3 : tensor<2x2x!quant.uniform<u8:f32, 1.000000e-01:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeMaxPool2D\n+func.func @QuantizeMaxPool2D(tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>> {\n+^bb0(%arg0: tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>):\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x6x6x16xf32>\n+// CHECK: %[[mp:.*]] = \"tfl.max_pool_2d\"(%arg0) <{filter_height = 1 : i32, filter_width = 1 : i32, fused_activation_function = \"RELU6\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x6x6x16x!quant.uniform<u8:f32, 7.812500e-03:128>>) -> tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+  %1 = \"tfl.max_pool_2d\"(%0) {filter_height = 1 : i32, filter_width = 1 : i32, fused_activation_function = \"RELU6\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x6x6x16xf32>) -> tensor<1x1x1x16xf32>\n+  %2 = \"tfl.quantize\"(%1) {qtype = tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>} : (tensor<1x1x1x16xf32>) -> tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+// CHECK: return %[[mp]] : tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+  func.return %2 : tensor<1x1x1x16x!quant.uniform<u8:f32, 7.812500e-03:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeSplit\n+func.func @QuantizeSplit(%arg: tensor<4x!quant.uniform<u8:f32, 1.0>>, %cst: tensor<i32>) -> (tensor<2x!quant.uniform<u8:f32, 1.0>>,tensor<2x!quant.uniform<u8:f32, 1.0>>) {\n+  %0 = \"tfl.dequantize\"(%arg) : (tensor<4x!quant.uniform<u8:f32, 1.0>>) -> tensor<4xf32>\n+// CHECK: %[[sp:.*]]:2 = \"tfl.split\"(%arg1, %arg0) <{num_splits = 2 : i32}> : (tensor<i32>, tensor<4x!quant.uniform<u8:f32, 1.000000e+00>>)\n+  %1:2 = \"tfl.split\"(%cst, %0) {num_splits = 2 : i32} : (tensor<i32>, tensor<4xf32>) -> (tensor<2xf32>, tensor<2xf32>)\n+  %2 = \"tfl.quantize\"(%1#0) {qtype = tensor<2x!quant.uniform<u8:f32, 1.0>>} : (tensor<2xf32>) -> tensor<2x!quant.uniform<u8:f32, 1.0>>\n+  %3 = \"tfl.quantize\"(%1#1) {qtype = tensor<2x!quant.uniform<u8:f32, 1.0>>} : (tensor<2xf32>) -> tensor<2x!quant.uniform<u8:f32, 1.0>>\n+// CHECK: return %[[sp]]#0, %[[sp]]#1\n+  func.return %2, %3 : tensor<2x!quant.uniform<u8:f32, 1.0>>, tensor<2x!quant.uniform<u8:f32, 1.0>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeCustomTfOp\n+func.func @QuantizeCustomTfOp(%arg0: tensor<128x128x!quant.uniform<u8:f32, 0.1:127>>,\n+    %arg1: tensor<1x!quant.uniform<u8:f32, 0.2:127>>, %arg2: tensor<1x!quant.uniform<u8:f32, 0.4:127>>,\n+    %arg3: tensor<1xi32>) -> (tensor<128x128x!quant.uniform<u8:f32, 0.2:125>>) {\n+  %0 = \"tfl.dequantize\"(%arg0) : (tensor<128x128x!quant.uniform<u8:f32, 0.1:127>>) -> tensor<128x128xf32>\n+  %1 = \"tfl.dequantize\"(%arg1) : (tensor<1x!quant.uniform<u8:f32, 0.2:127>>) -> tensor<1xf32>\n+  %2 = \"tfl.dequantize\"(%arg2) : (tensor<1x!quant.uniform<u8:f32, 0.4:127>>) -> tensor<1xf32>\n+// CHECK: %4 = \"tfl.custom_tf\"(%arg0, %arg1, %arg2, %arg3) ({\n+// CHECK-NEXT: ^bb0(%arg4: tensor<128x128xf32>, %arg5: tensor<1xf32>, %arg6: tensor<1xf32>, %arg7: tensor<1xi32>):\n+// CHECK-NEXT:   \"tf.LayerNorm\"(%arg4, %arg5, %arg6, %arg7) {_tfl_quant_trait = \"fully_quantizable\", device = \"\"} : (tensor<128x128xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xi32>) -> tensor<128x128xf32>\n+// CHECK-NEXT:   \"tfl.yield\"\n+// CHECK-NEXT: }) {_tfl_quant_trait = \"fully_quantizable\", device = \"\"} :\n+// CHECK-SAME: (tensor<128x128x!quant.uniform<u8:f32, 1.000000e-01:127>>, tensor<1x!quant.uniform<u8:f32, 2.000000e-01:127>>, tensor<1x!quant.uniform<u8:f32, 4.000000e-01:127>>, tensor<1xi32>)\n+// CHECK-SAME: -> tensor<128x128x!quant.uniform<u8:f32, 2.000000e-01:125>>\n+  %3 = \"tfl.custom_tf\"(%0, %1, %2, %arg3) ({\n+  ^bb0(%a1: tensor<128x128xf32>, %a2: tensor<1xf32>, %a3: tensor<1xf32>, %a4: tensor<1xi32>):\n+    %4 = \"tf.LayerNorm\"(%a1, %a2, %a3, %a4) {_tfl_quant_trait = \"fully_quantizable\", device = \"\"} : (tensor<128x128xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xi32>) -> tensor<128x128xf32>\n+   \"tfl.yield\"(%4) : (tensor<128x128xf32>) -> ()\n+  }) {_tfl_quant_trait = \"fully_quantizable\", device = \"\"} : (tensor<128x128xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xi32>) -> tensor<128x128xf32>\n+  %4 = \"tfl.quantize\"(%3) {qtype = tensor<128x128x!quant.uniform<u8:f32, 0.2:125>>} : (tensor<128x128xf32>) -> tensor<128x128x!quant.uniform<u8:f32, 0.2:125>>\n+  func.return %4 : tensor<128x128x!quant.uniform<u8:f32, 0.2:125>>\n+}\n+\n+// -----\n+\n+// Checks that legacy path correctly handles asymmetric quantized values.\n+// CHECK-LABEL: CheckLegacyQuantizeAdd\n+func.func @CheckLegacyQuantizeAdd() -> tensor<1x2x!quant.uniform<i8:f32, 0.0078431372549019607:-128>> {\n+  %cst = arith.constant dense<[[1.000000e+00, 2.000000e+00]]> : tensor<1x2xf32>\n+// CHECK:  \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x!quant.uniform<i8:f32, 0.0078431372549019607:-128>>, value = dense<{{\\[\\[}}-1, 127]]> : tensor<1x2xi8>}>\n+  %0 = \"tfl.quantize\"(%cst) {qtype = tensor<1x2x!quant.uniform<i8:f32, 0.0078431372549019607:-128>>, volatile} : (tensor<1x2xf32>) -> tensor<1x2x!quant.uniform<i8:f32, 0.0078431372549019607:-128>>\n+  func.return %0 : tensor<1x2x!quant.uniform<i8:f32, 0.0078431372549019607:-128>>\n+}\n+\n+// -----\n+\n+func.func private @testIfThen(tensor<*xf32>) -> tensor<*xf32>\n+func.func private @testIfElse(tensor<*xf32>) -> tensor<*xf32>\n+\n+// CHECK-LABEL: NotQuantizeIf\n+func.func @NotQuantizeIf(%arg0: tensor<i1>,\n+                    %arg1: tensor<4x!quant.uniform<u8:f32, 1.0>>) -> (tensor<4x!quant.uniform<u8:f32, 1.0>>) {\n+  // CHECK: %[[dq:.*]] = \"tfl.dequantize\"(%arg1)\n+  %0 = \"tfl.dequantize\"(%arg1) : (tensor<4x!quant.uniform<u8:f32, 1.0>>) -> tensor<4xf32>\n+  // CHECK-NEXT: %[[if:.*]] = \"tf.If\"(%arg0, %[[dq]]\n+  %1 = \"tf.If\"(%arg0, %0) {then_branch = @testIfThen, else_branch = @testIfElse, is_stateless = false} : (tensor<i1>, tensor<4xf32>) -> tensor<4xf32>\n+  // CHECK-NEXT: %[[q:.*]] = \"tfl.quantize\"(%[[if]])\n+  %2 = \"tfl.quantize\"(%1) {qtype = tensor<4x!quant.uniform<u8:f32, 1.0>>} : (tensor<4xf32>) -> tensor<4x!quant.uniform<u8:f32, 1.0>>\n+\n+  // CHECK-NEXT: return %[[q]]\n+  func.return %2 : tensor<4x!quant.uniform<u8:f32, 1.0>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: NotQuantizeReadVariable\n+func.func @NotQuantizeReadVariable() -> tensor<1x2x3x!quant.uniform<u8<1:255>:f32, 0.047244094488188976:128>> {\n+  // CHECK: %[[handle:.*]] = \"tfl.var_handle\"() <{container = \"\", shared_name = \"states\"}> : () -> tensor<!tf_type.resource<tensor<1x2x3xf32>>>\n+  %0 = \"tfl.var_handle\"() {container = \"\", shared_name = \"states\"} : () -> tensor<!tf_type.resource<tensor<1x2x3xf32>>>\n+  // CHECK-NEXT: %[[read:.*]] = \"tfl.read_variable\"(%[[handle]]) : (tensor<!tf_type.resource<tensor<1x2x3xf32>>>) -> tensor<1x2x3xf32>\n+  %1 = \"tfl.read_variable\"(%0) : (tensor<!tf_type.resource<tensor<1x2x3xf32>>>) -> tensor<1x2x3xf32>\n+  // CHECK-NEXT: %[[quantize:.*]] = \"tfl.quantize\"(%[[read]]) <{qtype = tensor<1x2x3x!quant.uniform<u8<1:255>:f32, 0.047244094488188976:128>>}> : (tensor<1x2x3xf32>) -> tensor<1x2x3x!quant.uniform<u8<1:255>:f32, 0.047244094488188976:128>>\n+  %2 = \"tfl.quantize\"(%1) {qtype = tensor<1x2x3x!quant.uniform<u8<1:255>:f32, 0.047244094488188976:128>>} : (tensor<1x2x3xf32>) -> tensor<1x2x3x!quant.uniform<u8<1:255>:f32, 0.047244094488188976:128>>\n+  // CHECK-NEXT: return %[[quantize]]\n+  func.return %2 : tensor<1x2x3x!quant.uniform<u8<1:255>:f32, 0.047244094488188976:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: QuantizeTposeConv\n+func.func @QuantizeTposeConv(%arg0: tensor<2x2x3x2048xf32>) -> tensor<2x3x2x2048x!quant.uniform<u8:f32, 0.1:128>> {\n+  %output_shape = arith.constant dense<[2, 3, 2, 2048]> : tensor<4xi32>\n+  // CHECK: %[[QARG0:.*]] = \"tfl.quantize\"(%arg0)\n+  %q_arg0 = \"tfl.quantize\"(%arg0) {qtype = tensor<2x2x3x2048x!quant.uniform<u8:f32, 0.1:128>>} : (tensor<2x2x3x2048xf32>) -> tensor<2x2x3x2048x!quant.uniform<u8:f32, 0.1:128>>\n+  %dq_arg0 = \"tfl.dequantize\"(%q_arg0) : (tensor<2x2x3x2048x!quant.uniform<u8:f32, 0.1:128>>) -> tensor<2x2x3x2048xf32>\n+  // CHECK: %[[W:.*]] = \"tfl.pseudo_qconst\"()\n+  %q_weighs = \"tfl.pseudo_qconst\"() {qtype = tensor<4x2x2x2048x!quant.uniform<u8<1:255>:f32, 0.15:151>>, value = dense<-76> : tensor<4x2x2x2048xi8>} : () -> tensor<4x2x2x2048x!quant.uniform<u8<1:255>:f32, 0.15:151>>\n+  %dq_weights = \"tfl.dequantize\"(%q_weighs) : (tensor<4x2x2x2048x!quant.uniform<u8<1:255>:f32, 0.15:151>>) -> tensor<4x2x2x2048xf32>\n+  %bias = \"tfl.no_value\"() {value} : () -> none\n+  // CHECK: %[[CONV:.*]] = \"tfl.transpose_conv\"(%cst, %[[W]], %[[QARG0]], %0) <{fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<4xi32>, tensor<4x2x2x2048x!quant.uniform<u8<1:255>:f32, 1.500000e-01:151>>, tensor<2x2x3x2048x!quant.uniform<u8:f32, 1.000000e-01:128>>, none) -> tensor<2x3x2x2048x!quant.uniform<u8:f32, 1.000000e-01:128>>\n+  %out = \"tfl.transpose_conv\"(%output_shape, %dq_weights, %dq_arg0, %bias) {fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<4xi32>, tensor<4x2x2x2048xf32>, tensor<2x2x3x2048xf32>, none) -> tensor<2x3x2x2048xf32>\n+  %q_out = \"tfl.quantize\"(%out) {qtype = tensor<2x3x2x2048x!quant.uniform<u8:f32, 0.1:128>>} : (tensor<2x3x2x2048xf32>) -> tensor<2x3x2x2048x!quant.uniform<u8:f32, 0.1:128>>\n+  // CHECK: return %[[CONV]]\n+  func.return %q_out : tensor<2x3x2x2048x!quant.uniform<u8:f32, 0.1:128>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: foldQuantWeightsIntoEmbeddingLookup\n+func.func @foldQuantWeightsIntoEmbeddingLookup(%arg0: tensor<3xi32>) -> tensor<3x512xf32> {\n+  %q_weighs = \"tfl.pseudo_qconst\"() {qtype = tensor<3074x512x!quant.uniform<u8<1:255>:f32, 0.15:151>>, value = dense<-76> : tensor<3074x512xi8>} : () -> tensor<3074x512x!quant.uniform<u8<1:255>:f32, 0.15:151>>\n+  // CHECK-NOT: \"tfl.dequantize\"\n+  %dq_weights = \"tfl.dequantize\"(%q_weighs) : (tensor<3074x512x!quant.uniform<u8<1:255>:f32, 0.15:151>>) -> tensor<3074x512xf32>\n+  // CHECK: \"tfl.embedding_lookup\"(%arg0, %0) : (tensor<3xi32>, tensor<3074x512x!quant.uniform<u8<1:255>:f32\n+  %out = \"tfl.embedding_lookup\"(%arg0, %dq_weights) : (tensor<3xi32>, tensor<3074x512xf32>) -> tensor<3x512xf32>\n+  func.return %out : tensor<3x512xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: RequantizationSquash\n+func.func @RequantizationSquash(%arg0: tensor<64x1x128xf32>) -> tensor<64x128xf32>   {\n+  %cst_132 = arith.constant dense<[64, 128]> : tensor<2xi32>\n+  %683 = \"tfl.quantize\"(%arg0) <{qtype = tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>}> {volatile} : (tensor<64x1x128xf32>) -> tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>\n+  // CHECK-NOT: \"tfl.dequantize\"(%{{.*}}) : (tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>) -> tensor<64x1x128xf32>\n+  %688 = \"tfl.dequantize\"(%683) : (tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>) -> tensor<64x1x128xf32>\n+  %698 = \"tfl.quantize\"(%688) <{qtype = tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>}> : (tensor<64x1x128xf32>) -> tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>\n+  %699 = \"tfl.dequantize\"(%698) : (tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>) -> tensor<64x1x128xf32>\n+  // CHECK: \"tfl.reshape\"\n+  // CHECK-SAME: (tensor<64x1x128x!quant.uniform<i8:f32, 8.000000e-01>>, tensor<2xi32>) -> tensor<64x128x!quant.uniform<i8:f32, 8.000000e-01>>\n+  %700 = \"tfl.reshape\"(%699, %cst_132) : (tensor<64x1x128xf32>, tensor<2xi32>) -> tensor<64x128xf32>\n+  %701 = \"tfl.quantize\"(%700) <{qtype = tensor<64x128x!quant.uniform<i8:f32, 0.8>>}> : (tensor<64x128xf32>) -> tensor<64x128x!quant.uniform<i8:f32, 0.8>>\n+  %702 = \"tfl.dequantize\"(%701) : (tensor<64x128x!quant.uniform<i8:f32, 0.8>>) -> tensor<64x128xf32>\n+  func.return %702 : tensor<64x128xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: RequantizationDifferentScalesNoSquash\n+func.func @RequantizationDifferentScalesNoSquash(%arg0: tensor<64x1x128xf32>) -> tensor<64x128xf32>   {\n+  %cst_132 = arith.constant dense<[64, 128]> : tensor<2xi32>\n+  %683 = \"tfl.quantize\"(%arg0) <{qtype = tensor<64x1x128x!quant.uniform<i8:f32, 0.2>>}> {volatile} : (tensor<64x1x128xf32>) -> tensor<64x1x128x!quant.uniform<i8:f32, 0.2>>\n+  %688 = \"tfl.dequantize\"(%683) : (tensor<64x1x128x!quant.uniform<i8:f32, 0.2>>) -> tensor<64x1x128xf32>\n+  // CHECK: %[[REQUANT:.*]] = \"tfl.quantize\"(%{{.*}}) <{qtype = tensor<64x1x128x!quant.uniform<i8:f32, 8.000000e-01>>}> : (tensor<64x1x128xf32>) -> tensor<64x1x128x!quant.uniform<i8:f32, 8.000000e-01>>\n+  %698 = \"tfl.quantize\"(%688) <{qtype = tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>}> : (tensor<64x1x128xf32>) -> tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>\n+  %699 = \"tfl.dequantize\"(%698) : (tensor<64x1x128x!quant.uniform<i8:f32, 0.8>>) -> tensor<64x1x128xf32>\n+  // CHECK: \"tfl.reshape\"(%[[REQUANT]], %{{.*}}) : (tensor<64x1x128x!quant.uniform<i8:f32, 8.000000e-01>>, tensor<2xi32>) -> tensor<64x128x!quant.uniform<i8:f32, 8.000000e-01>>\n+  %700 = \"tfl.reshape\"(%699, %cst_132) : (tensor<64x1x128xf32>, tensor<2xi32>) -> tensor<64x128xf32>\n+  %701 = \"tfl.quantize\"(%700) <{qtype = tensor<64x128x!quant.uniform<i8:f32, 0.8>>}> : (tensor<64x128xf32>) -> tensor<64x128x!quant.uniform<i8:f32, 0.8>>\n+  %702 = \"tfl.dequantize\"(%701) : (tensor<64x128x!quant.uniform<i8:f32, 0.8>>) -> tensor<64x128xf32>\n+  func.return %702 : tensor<64x128xf32>\n+}"
        },
        {
            "sha": "fc65a80e550432911d5d573347c1848a7f7cf025",
            "filename": "tensorflow/compiler/mlir/lite/transforms/passes.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpasses.h?ref=5a18ee6f7b7ab0fa642f5761a450daef2df9776c",
            "patch": "@@ -126,6 +126,8 @@ std::unique_ptr<OperationPass<ModuleOp>> CreatePropagateQsvPass();\n \n std::unique_ptr<OperationPass<mlir::ModuleOp>> CreateBiasQuantizerPass();\n \n+std::unique_ptr<OperationPass<mlir::ModuleOp>> CreateFuseQDQPass();\n+\n // Overloading of CreateQuantizePass which takes only necessary flags to reduce\n // the binary size.\n std::unique_ptr<OperationPass<func::FuncOp>> CreateQuantizePass("
        },
        {
            "sha": "b07d6e8d516aac41f33a19e528d2e8bb79de8740",
            "filename": "tensorflow/compiler/mlir/lite/transforms/passes.td",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpasses.td?ref=5a18ee6f7b7ab0fa642f5761a450daef2df9776c",
            "patch": "@@ -379,6 +379,17 @@ def BiasQuantizerPass : Pass<\"tfl-bias-quantizer\", \"mlir::ModuleOp\"> {\n   ];\n }\n \n+def FuseQDQPass : Pass<\"tfl-fuse-qdq\", \"mlir::ModuleOp\"> {\n+  let summary = \"Fuses dq and q ops before and after ops that can be quantized.\";\n+  let description = [{\n+      Fuses dq and q ops before and after ops that can be quantized..\n+  }];\n+  let constructor = \"CreateFuseQDQPass()\";\n+  let dependentDialects = [\n+    \"TFL::TensorFlowLiteDialect\",\n+  ];\n+}\n+\n def QuantizeVariablesPass : Pass<\"tfl-quantize-variables\", \"mlir::ModuleOp\"> {\n   let summary = \"Quantize variables\";\n   let constructor = \"CreatePrepareQuantizeVariablesPass()\";"
        },
        {
            "sha": "82248665511040e5b3a80abdbb65ef942791111d",
            "filename": "tensorflow/compiler/mlir/lite/transforms/quantization/fuse_qdq_pass.cc",
            "status": "added",
            "additions": 550,
            "deletions": 0,
            "changes": 550,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantization%2Ffuse_qdq_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a18ee6f7b7ab0fa642f5761a450daef2df9776c/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantization%2Ffuse_qdq_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantization%2Ffuse_qdq_pass.cc?ref=5a18ee6f7b7ab0fa642f5761a450daef2df9776c",
            "patch": "@@ -0,0 +1,550 @@\n+/* Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This pass fuses Q and DQ ops and creates quantized kernels.\n+\n+#include <iterator>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/base/no_destructor.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"llvm/ADT/DenseMap.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n+#include \"mlir/IR/Builders.h\"  // from @llvm-project\n+#include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n+#include \"mlir/IR/BuiltinOps.h\"  // from @llvm-project\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"  // from @llvm-project\n+#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n+#include \"mlir/IR/Diagnostics.h\"  // from @llvm-project\n+#include \"mlir/IR/Matchers.h\"  // from @llvm-project\n+#include \"mlir/IR/OpDefinition.h\"  // from @llvm-project\n+#include \"mlir/IR/Operation.h\"  // from @llvm-project\n+#include \"mlir/IR/OperationSupport.h\"  // from @llvm-project\n+#include \"mlir/IR/PatternMatch.h\"  // from @llvm-project\n+#include \"mlir/IR/TypeUtilities.h\"  // from @llvm-project\n+#include \"mlir/IR/Types.h\"  // from @llvm-project\n+#include \"mlir/IR/Value.h\"  // from @llvm-project\n+#include \"mlir/Pass/Pass.h\"  // from @llvm-project\n+#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n+#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n+#include \"mlir/Support/TypeID.h\"  // from @llvm-project\n+#include \"mlir/Transforms/DialectConversion.h\"  // from @llvm-project\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"  // from @llvm-project\n+#include \"stablehlo/dialect/StablehloOps.h\"  // from @stablehlo\n+#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n+#include \"tensorflow/compiler/mlir/lite/quantization/common/quantization_lib/quantization_interface.h.inc\"\n+#include \"tensorflow/compiler/mlir/lite/quantization/common/quantization_lib/quantization_traits.h\"\n+#include \"tensorflow/compiler/mlir/lite/quantization/common/quantization_lib/quantization_utils.h\"\n+#include \"tensorflow/compiler/mlir/lite/transforms/lower_quant_annotations_helper.h\"\n+#include \"tensorflow/compiler/mlir/lite/transforms/passes.h\"  // IWYU pragma: keep\n+\n+namespace mlir {\n+namespace TFL {\n+namespace {\n+\n+#define GEN_PASS_DEF_FUSEQDQPASS\n+#include \"tensorflow/compiler/mlir/lite/transforms/passes.h.inc\"\n+\n+//===----------------------------------------------------------------------===//\n+// Helper Functions\n+//===----------------------------------------------------------------------===//\n+\n+enum QuantizationTrait { kFullQuantization, kDynamicRangeQuantization };\n+\n+enum class OpQuantizationType { kSRQ, kDRQ, kWeightOnly, kUnsupported };\n+\n+LogicalResult IsDrqTensor(mlir::Value value, mlir::Value& fq_input) {\n+  if (auto composite_op = llvm::dyn_cast_or_null<stablehlo::CompositeOp>(\n+          value.getDefiningOp())) {\n+    if (IsDrqFakeQuant(composite_op)) {\n+      fq_input = composite_op.getOperand(0);\n+      return success();\n+    }\n+  }\n+  return failure();\n+}\n+\n+LogicalResult HasDQParent(mlir::Value value, mlir::Value& dq_input) {\n+  if (auto dq_op =\n+          llvm::dyn_cast_or_null<DequantizeOp>(value.getDefiningOp())) {\n+    dq_input = dq_op.getOperand();\n+    return success();\n+  }\n+  return failure();\n+}\n+\n+bool IsQuantizableOp(mlir::Operation* op) {\n+  if (op->hasTrait<OpTrait::TFL::QuantizableResult>()) {\n+    return true;\n+  }\n+\n+  auto custom_op = llvm::dyn_cast_or_null<TFL::CustomTfOp>(op);\n+  if (!custom_op) {\n+    return false;\n+  }\n+\n+  auto quant_trait = custom_op->getAttrOfType<StringAttr>(\"_tfl_quant_trait\");\n+  return quant_trait && quant_trait.getValue() == \"fully_quantizable\";\n+}\n+\n+OpQuantizationType GetOpQuantizationType(mlir::Operation* op) {\n+  // The assumption here is that the op has at least one DQ operand since the\n+  // pattern's root is that.\n+\n+  static const absl::NoDestructor<absl::flat_hash_set<std::string>>\n+      kDrqOpsWithNoDrqInput({\"tfl.embedding_lookup\"});\n+\n+  // \"return\" is not going to be quantized\n+  if (op->hasTrait<OpTrait::IsTerminator>()) {\n+    return OpQuantizationType::kUnsupported;\n+  }\n+\n+  // Indicates if an input which is not an FQ is seen.\n+  bool non_fq_float_input_seen = false;\n+  mlir::Value fq_input, dq_input;\n+  for (auto operand : op->getOperands()) {\n+    if (IsDrqTensor(operand, fq_input).succeeded()) {\n+      // As soon as a DRQ tensor is encountered, the op is DRQ.\n+      return OpQuantizationType::kDRQ;\n+    }\n+\n+    if (HasDQParent(operand, dq_input).succeeded()) {\n+      // Operands with QDQ can not specify the quantization type.\n+      continue;\n+    }\n+\n+    if (kDrqOpsWithNoDrqInput->contains(op->getName().getStringRef().str())) {\n+      return OpQuantizationType::kDRQ;\n+    }\n+\n+    auto element_type = getElementTypeOrSelf(operand.getType());\n+\n+    // Ignore non-f32 tensors when determining the quantization type.\n+    // Examples:\n+    //  - i32 operands are generally index tensors (e.g. in transpose\n+    // permutation)\n+    //  - bool operands can be the `condition` operand in a select_v2 op.\n+    if (element_type.isF32()) {\n+      non_fq_float_input_seen = true;\n+    }\n+  }\n+  if (non_fq_float_input_seen) {\n+    return OpQuantizationType::kWeightOnly;\n+  }\n+\n+  for (auto result : op->getResults()) {\n+    // This check is required othwerwise ops would rematch even after their\n+    // uses are changed to the quantized version and before they're erased due\n+    // to being trivially dead.\n+    // If a result is not used, it doesn't affect the quantization type.\n+    if (result.use_empty()) {\n+      return OpQuantizationType::kUnsupported;\n+    }\n+    for (auto user : result.getUsers()) {\n+      if (!llvm::dyn_cast_or_null<QuantizeOp>(user)) {\n+        return OpQuantizationType::kUnsupported;\n+      }\n+    }\n+  }\n+\n+  // SRQ kernels need to have quantizable results.\n+  // Note that this check is required since this can not be fully controlled by\n+  // user annotations due to the fact that propagation can get annotations\n+  // around an op that is not quantizable and this check ensures the op is not\n+  // incorrectly SRQ quantized.\n+  if (!IsQuantizableOp(op)) {\n+    return OpQuantizationType::kUnsupported;\n+  }\n+\n+  return OpQuantizationType::kSRQ;\n+}\n+\n+// Returns a list of operations that consume the result of the given\n+// DequantizeOp. Returns an empty list if the DequantizeOp has more than one\n+// result.\n+SmallVector<mlir::Operation*, 4> GetQuantizingOps(mlir::TFL::DequantizeOp op) {\n+  llvm::SmallVector<mlir::Operation*, 4> quantizing_ops;\n+  if (op->getNumResults() != 1) {\n+    return quantizing_ops;\n+  }\n+  auto users = op->getResult(0).getUsers();\n+  quantizing_ops.append(users.begin(), users.end());\n+  return quantizing_ops;\n+}\n+\n+// Populates the inputs vector with the quantized inputs for the given\n+// quantizing_op. The inputs are determined based on the op_quant_type. Returns\n+// failure if an unsupported operand is encountered.\n+//\n+// Parameters:\n+//  quantizing_op: The operation whose inputs are being processed.\n+//  op_quant_type: The quantization type of the operation.\n+//  rewriter: The pattern rewriter.\n+//  inputs: The vector to populate with the quantized inputs.\n+//  updated: A boolean flag that is set to true if any input is updated.\n+LogicalResult GetQuantizedInputs(mlir::Operation* quantizing_op,\n+                                 OpQuantizationType op_quant_type,\n+                                 PatternRewriter& rewriter,\n+                                 SmallVector<mlir::Value, 4>& inputs,\n+                                 bool& updated) {\n+  inputs.reserve(quantizing_op->getNumOperands());\n+  for (auto operand : quantizing_op->getOperands()) {\n+    Type operand_type = operand.getType();\n+\n+    if (mlir::Value dq_input; HasDQParent(operand, dq_input).succeeded()) {\n+      if (op_quant_type == OpQuantizationType::kWeightOnly) {\n+        inputs.push_back(operand);\n+      } else {\n+        updated = true;\n+        inputs.push_back(dq_input);\n+      }\n+    } else if (mlir::Value fq_input;\n+               IsDrqTensor(operand, fq_input).succeeded()) {\n+      updated = true;\n+      inputs.push_back(fq_input);\n+    } else if (auto ele_type = getElementTypeOrSelf(operand_type);\n+               ele_type.isF32() || ele_type.isInteger(32) ||\n+               ele_type.isInteger(64) || ele_type.isInteger(1) ||\n+               mlir::isa<NoneType>(ele_type)) {\n+      // If it's F32 (non-weight-only and non-drq) or I32 or bool, just\n+      // directly add the input.\n+      inputs.push_back(operand);\n+    } else {\n+      return rewriter.notifyMatchFailure(\n+          quantizing_op,\n+          \"has unsupported operand received during quantization\");\n+    }\n+  }\n+  return success();\n+}\n+\n+// Populates the output_types vector with the quantized output types for the\n+// given quantizing_op. The output types are determined based on the\n+// op_quant_type. Also populates the outputs_replaced map with the values to be\n+// replaced. Returns failure if an unsupported output type is encountered.\n+//\n+// Parameters:\n+//  quantizing_op: The operation whose output types are being determined.\n+//  op_quant_type: The quantization type of the operation.\n+//  rewriter: The pattern rewriter.\n+//  outputs_replaced: A map to populate with values to be replaced. The key is\n+//    the value to be replaced, and the value is a pair containing the result\n+//    index and the target QuantizeOp (if any).\n+//  output_types: The vector to populate with the quantized output types.\n+//  updated: A boolean flag that is set to true if any output type is updated.\n+LogicalResult GetQuantizedOutputTypes(\n+    mlir::Operation* quantizing_op, OpQuantizationType op_quant_type,\n+    PatternRewriter& rewriter,\n+    llvm::SmallDenseMap<mlir::Value, std::pair<int, mlir::Operation*>>&\n+        outputs_replaced,\n+    SmallVector<Type, 4>& output_types, bool& updated) {\n+  output_types.reserve(quantizing_op->getNumResults());\n+  for (const auto& enumerated_result :\n+       llvm::enumerate(quantizing_op->getResults())) {\n+    mlir::Value result = enumerated_result.value();\n+    Type result_type = result.getType();\n+    Type result_ele_type = getElementTypeOrSelf(result_type);\n+\n+    if (op_quant_type == OpQuantizationType::kSRQ) {\n+      int num_observed_qs = 0;\n+      for (auto user : result.getUsers()) {\n+        auto user_q_op = llvm::dyn_cast_or_null<QuantizeOp>(user);\n+        if (!user_q_op) {\n+          quantizing_op->emitError(\n+              \"SRQ quantized op result should only be used by QuantizeOps.\");\n+          return failure();\n+        }\n+        updated = true;\n+        if (user_q_op->hasAttr(\"propagated\")) {\n+          outputs_replaced.insert(\n+              {user_q_op.getInput(), {enumerated_result.index(), user_q_op}});\n+        } else {\n+          CHECK(num_observed_qs == 0)\n+              << \"There must be only one observed scale for a tensor.\";\n+          outputs_replaced.insert(\n+              {user_q_op.getOutput(), {enumerated_result.index(), nullptr}});\n+          output_types.push_back(user_q_op.getType());\n+          num_observed_qs++;\n+        }\n+      }\n+      if (num_observed_qs == 0) {\n+        CHECK(std::distance(result.use_begin(), result.use_end()) == 1)\n+            << \"if there are no observed scales, there must be only one \"\n+               \"output annotation that is propagated from the input.\";\n+        auto q_user =\n+            dyn_cast_or_null<QuantizeOp>(*result.user_begin()).getOutput();\n+        output_types.push_back(q_user.getType());\n+      }\n+    } else if (result_ele_type.isF32() || mlir::isa<NoneType>(result_type)) {\n+      outputs_replaced.insert({result, {enumerated_result.index(), nullptr}});\n+      output_types.push_back(result.getType());\n+    } else {\n+      return rewriter.notifyMatchFailure(\n+          quantizing_op,\n+          \"is a fake quantized op with an output that is not float32.\");\n+    }\n+  }\n+  return success();\n+}\n+\n+// Creates a new quantized operation based on the given quantizing_op.\n+// The new operation will have the given inputs and output_types.\n+mlir::Operation* CreateQuantizedOp(mlir::Operation* quantizing_op,\n+                                   const SmallVector<mlir::Value, 4>& inputs,\n+                                   const SmallVector<Type, 4>& output_types,\n+                                   PatternRewriter& rewriter) {\n+  rewriter.setInsertionPointAfter(quantizing_op);\n+  OperationState new_state(quantizing_op->getLoc(),\n+                           quantizing_op->getName().getStringRef(), inputs,\n+                           output_types, quantizing_op->getAttrs());\n+  for (int i = 0; i < quantizing_op->getNumRegions(); ++i) {\n+    new_state.addRegion();\n+  }\n+  mlir::Operation* quantized_op = rewriter.create(new_state);\n+  if (quantizing_op->getNumRegions() != 0) {\n+    for (const auto& indexed_regions :\n+         llvm::enumerate(quantizing_op->getRegions())) {\n+      Region& target_region = quantized_op->getRegion(indexed_regions.index());\n+      IRMapping mapping;\n+      indexed_regions.value().cloneInto(&target_region, mapping);\n+    }\n+  }\n+  return quantized_op;\n+}\n+\n+// Replaces the uses of the old values in outputs_replaced with the\n+// corresponding results of the new quantized_op.\n+void ReplaceUses(\n+    const llvm::SmallDenseMap<mlir::Value, std::pair<int, mlir::Operation*>>&\n+        outputs_replaced,\n+    mlir::Operation* quantized_op, PatternRewriter& rewriter) {\n+  for (auto output : outputs_replaced) {\n+    mlir::Value replaced_value = output.getFirst();\n+    mlir::Operation* target_q_op = output.getSecond().second;\n+    int result_index = output.getSecond().first;\n+    if (target_q_op) {\n+      rewriter.replaceUsesWithIf(\n+          replaced_value, quantized_op->getResult(result_index),\n+          [&](OpOperand& use) { return use.getOwner() == target_q_op; });\n+    } else {\n+      rewriter.replaceAllUsesWith(replaced_value,\n+                                  quantized_op->getResult(result_index));\n+    }\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Rewrite Patterns\n+//===----------------------------------------------------------------------===//\n+\n+class RemoveUnusedFQ : public OpRewritePattern<stablehlo::CompositeOp> {\n+  using OpRewritePattern<stablehlo::CompositeOp>::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n+                                PatternRewriter& rewriter) const final {\n+    if (IsDrqFakeQuant(op) && op->getUses().empty()) {\n+      rewriter.eraseOp(op);\n+      return success();\n+    }\n+    return rewriter.notifyMatchFailure(\n+        op, \"is not a drq fake quant op with no uses.\");\n+  }\n+};\n+\n+// Pushes a drq fake quant op forward through a pad op.\n+// This is to allow DRQ FQ to be fused into the DRQ op.\n+// drq_fake_quant(input) -> pad -> output\n+// becomes\n+// input -> pad -> drq_fake_quant -> output\n+class PushForwardDrqFQ : public OpRewritePattern<stablehlo::CompositeOp> {\n+ public:\n+  using OpRewritePattern<stablehlo::CompositeOp>::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(stablehlo::CompositeOp drq_fq_op,\n+                                PatternRewriter& rewriter) const final {\n+    if (!IsDrqFakeQuant(drq_fq_op)) {\n+      return rewriter.notifyMatchFailure(drq_fq_op,\n+                                         \"is not a drq fake quant op.\");\n+    }\n+\n+    if (!drq_fq_op.getResult(0).hasOneUse()) {\n+      return rewriter.notifyMatchFailure(\n+          drq_fq_op, \"drq fake quant op does not have one use.\");\n+    }\n+    auto pad_op =\n+        llvm::dyn_cast<TFL::PadOp>(*drq_fq_op.getResult(0).user_begin());\n+    if (!pad_op) {\n+      return rewriter.notifyMatchFailure(drq_fq_op,\n+                                         \"user is not a tfl.pad op.\");\n+    }\n+\n+    // The input to the new pad op is the float input to the drq fake quant op.\n+    mlir::Value float_input =\n+        drq_fq_op.getOperand(drq_fq_op.getNumOperands() - 1);\n+\n+    // Create a new pad op.\n+    auto new_pad_op = rewriter.create<TFL::PadOp>(\n+        pad_op.getLoc(), pad_op.getType(), float_input, pad_op.getPadding());\n+\n+    // Create a new drq fake quant op.\n+    // Operands are the same, except for the last one.\n+    SmallVector<mlir::Value> new_drq_operands;\n+    for (mlir::Value operand : drq_fq_op.getOperands().drop_back()) {\n+      new_drq_operands.push_back(operand);\n+    }\n+    new_drq_operands.push_back(new_pad_op.getResult());\n+\n+    auto new_drq_fq_op = rewriter.create<stablehlo::CompositeOp>(\n+        drq_fq_op.getLoc(), pad_op.getType(), new_drq_operands,\n+        drq_fq_op->getAttrs());\n+\n+    rewriter.replaceOp(pad_op, new_drq_fq_op.getResult(0));\n+    return success();\n+  }\n+};\n+\n+class FuseQDQ : public OpRewritePattern<mlir::TFL::DequantizeOp> {\n+  using OpRewritePattern<mlir::TFL::DequantizeOp>::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(mlir::TFL::DequantizeOp op,\n+                                PatternRewriter& rewriter) const override {\n+    SmallVector<mlir::Operation*, 4> quantizing_ops = GetQuantizingOps(op);\n+    if (quantizing_ops.empty() && op->getNumResults() == 1) {\n+      return failure();\n+    }\n+\n+    bool updated = false;\n+\n+    // Rewrite the floating-point ops to the quantized version, by fusing\n+    // preceding dequantize ops and succeding quantize ops.\n+    for (mlir::Operation* quantizing_op : quantizing_ops) {\n+      auto op_quant_type = GetOpQuantizationType(quantizing_op);\n+\n+      if (op_quant_type == OpQuantizationType::kUnsupported) {\n+        return rewriter.notifyMatchFailure(quantizing_op,\n+                                           \"has unsupported quantization type\");\n+      }\n+\n+      SmallVector<mlir::Value, 4> inputs;\n+      if (failed(GetQuantizedInputs(quantizing_op, op_quant_type, rewriter,\n+                                    inputs, updated))) {\n+        return failure();\n+      }\n+\n+      llvm::SmallDenseMap<mlir::Value, std::pair<int, mlir::Operation*>>\n+          outputs_replaced;\n+      SmallVector<Type, 4> output_types;\n+      if (failed(GetQuantizedOutputTypes(quantizing_op, op_quant_type, rewriter,\n+                                         outputs_replaced, output_types,\n+                                         updated))) {\n+        return failure();\n+      }\n+\n+      if (!updated) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"has no further opportunities to fuse Q's and DQ's.\");\n+      }\n+\n+      mlir::Operation* quantized_op =\n+          CreateQuantizedOp(quantizing_op, inputs, output_types, rewriter);\n+\n+      ReplaceUses(outputs_replaced, quantized_op, rewriter);\n+    }\n+    return success();\n+  }\n+};\n+\n+// We need control over when this happens and so this cannot happen as a folder.\n+// Some optimizations like fusing a mul following a conv/FC into rhs, change the\n+// scale of rhs. So, if we fold Q into rhs early, we'll need a requant later\n+// which is losing information twice.\n+class QuantizeConstPattern : public OpRewritePattern<QuantizeOp> {\n+ public:\n+  // Does not take ownership of context, which must not be null and must outlive\n+  // this pattern.\n+  explicit QuantizeConstPattern(MLIRContext* context)\n+      : OpRewritePattern<QuantizeOp>(context) {}\n+  LogicalResult matchAndRewrite(QuantizeOp op,\n+                                PatternRewriter& rewriter) const override {\n+    DenseFPElementsAttr attr;\n+    if (matchPattern(op.getInput(), m_Constant(&attr))) {\n+      auto qtype = op.getQtypeAttr();\n+      Attribute quantized_attr = mlir::TFL::Quantize(attr, qtype.getValue());\n+      if (quantized_attr) {\n+        auto qconst_op =\n+            rewriter.create<QConstOp>(op.getLoc(), qtype, quantized_attr);\n+        if (auto volatile_attr = op->getAttr(mlir::TFL::kVolatileOpAttrName)) {\n+          qconst_op->setAttr(mlir::TFL::kVolatileOpAttrName, volatile_attr);\n+        }\n+        op.replaceAllUsesWith(qconst_op.getOutput());\n+        rewriter.eraseOp(op);\n+        return success();\n+      }\n+    }\n+    return failure();\n+  }\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// Pass Definition\n+//===----------------------------------------------------------------------===//\n+\n+struct FuseQDQPass : public impl::FuseQDQPassBase<FuseQDQPass> {\n+ public:\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(FuseQDQPass)\n+\n+  void runOnOperation() override;\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// Pass Implementation\n+//===----------------------------------------------------------------------===//\n+#include \"tensorflow/compiler/mlir/lite/transforms/quantization/generated_strict_quantize.inc\"\n+\n+void FuseQDQPass::runOnOperation() {\n+  MLIRContext* ctx = &getContext();\n+  mlir::ModuleOp module = getOperation();\n+\n+  RewritePatternSet patterns(ctx);\n+  patterns.add<FuseQDQ, PushForwardDrqFQ, RemoveUnusedFQ, QuantizeConstPattern,\n+               FuseDqQToRequant, FuseQQToRequant, RemoveNoOpQ>(ctx);\n+\n+  // Configure the greedy pattern rewrite driver.\n+  GreedyRewriteConfig greedy_config;\n+\n+  if (failed(\n+          applyPatternsGreedily(module, std::move(patterns), greedy_config))) {\n+    module.emitError(\"Failed to apply FuseQDQPass patterns.\");\n+    signalPassFailure();\n+  }\n+}\n+\n+}  // namespace\n+\n+//===----------------------------------------------------------------------===//\n+// Pass Creation Function\n+//===----------------------------------------------------------------------===//\n+\n+std::unique_ptr<OperationPass<mlir::ModuleOp>> CreateFuseQDQPass() {\n+  return std::make_unique<FuseQDQPass>();\n+}\n+\n+}  // namespace TFL\n+}  // namespace mlir"
        }
    ],
    "stats": {
        "total": 1145,
        "additions": 1145,
        "deletions": 0
    }
}