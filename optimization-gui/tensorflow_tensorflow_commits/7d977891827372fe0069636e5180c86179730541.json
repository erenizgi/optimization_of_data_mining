{
    "author": "tensorflower-gardener",
    "message": "[Autotuner]Support ScaledDot in Triton backend.\n\nPiperOrigin-RevId: 841297198",
    "sha": "7d977891827372fe0069636e5180c86179730541",
    "files": [
        {
            "sha": "d9eb72d2b71296ab8f4cd7c3c7793cedbd18c54a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=7d977891827372fe0069636e5180c86179730541",
            "patch": "@@ -436,7 +436,6 @@ cc_library(\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_float_support\",\n-        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:split_k_gemm_rewriter\",\n@@ -451,12 +450,10 @@ cc_library(\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n         \"@llvm-project//mlir:IR\",\n     ],\n )\n@@ -482,13 +479,11 @@ xla_test(\n         \"//xla:autotuning_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:platform_util\",\n-        \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\","
        },
        {
            "sha": "a6cc696a80ef916441abc15a04c7e5163146cf0d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 7,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=7d977891827372fe0069636e5180c86179730541",
            "patch": "@@ -20,11 +20,9 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n-#include \"absl/strings/str_cat.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n@@ -40,7 +38,6 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/triton_configs.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_float_support.h\"\n-#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/split_k_gemm_rewriter.h\"\n@@ -104,12 +101,25 @@ TritonBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   if (!IsSupported(instr)) {\n     return std::vector<std::unique_ptr<BackendConfig>>();\n   }\n-  const HloDotInstruction* dot =\n-      Cast<HloDotInstruction>(hlo_query::GetFirstInstructionWithOpcode(\n-          *instr.fused_instructions_computation(), HloOpcode::kDot));\n+  const HloInstruction* dot_instr = hlo_query::GetFirstInstructionWithOpcode(\n+      *instr.fused_instructions_computation(), HloOpcode::kDot);\n+  if (dot_instr != nullptr) {\n+    return GetSupportedConfigsForDot(dot_instr);\n+  }\n+  const HloInstruction* scaled_dot_instr =\n+      hlo_query::GetFirstInstructionWithOpcode(\n+          *instr.fused_instructions_computation(), HloOpcode::kScaledDot);\n+  if (scaled_dot_instr != nullptr) {\n+    return GetSupportedConfigsForScaledDot(scaled_dot_instr);\n+  }\n+  return std::vector<std::unique_ptr<BackendConfig>>();\n+}\n+\n+absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+TritonBackend::GetSupportedConfigsForDot(const HloInstruction* instr) {\n+  const HloDotInstruction* dot = Cast<HloDotInstruction>(instr);\n   TritonDotFusionSearchSpace search_space(target_config().device_description,\n                                           dot);\n-\n   bool supports_contracting_split =\n       HloBfsFindAll({dot}, [&](const HloInstruction* node) {\n         return node->opcode() == HloOpcode::kSlice;\n@@ -150,6 +160,25 @@ TritonBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   return configs;\n }\n \n+absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+TritonBackend::GetSupportedConfigsForScaledDot(const HloInstruction* instr) {\n+  std::vector<std::unique_ptr<BackendConfig>> configs;\n+  for (int block_m = 16; block_m <= 256; block_m *= 2) {\n+    for (int block_n = 16; block_n <= 256; block_n *= 2) {\n+      auto any = std::make_unique<google::protobuf::Any>();\n+      any->PackFrom(TritonGemmConfig(block_m, block_n,\n+                                     /*block_k=*/128, /*split_k=*/1,\n+                                     /*num_stages=*/1,\n+                                     /*num_warps=*/4,\n+                                     /*num_ctas=*/1,\n+                                     /*is_tma_allowed=*/false)\n+                        .ToProto());\n+      configs.push_back(std::move(any));\n+    }\n+  }\n+  return configs;\n+}\n+\n absl::StatusOr<std::unique_ptr<BackendConfig>> TritonBackend::GetDefaultConfig(\n     const HloInstruction& instr) {\n   TF_ASSIGN_OR_RETURN(std::vector<std::unique_ptr<BackendConfig>> configs,"
        },
        {
            "sha": "a36a06f70536583a9073121438c97a22908ae499",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=7d977891827372fe0069636e5180c86179730541",
            "patch": "@@ -55,6 +55,11 @@ class TritonBackend : public GpuCodegenBackend {\n  private:\n   bool IsSupported(const HloInstruction& instr) override;\n \n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+  GetSupportedConfigsForDot(const HloInstruction* instr);\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+  GetSupportedConfigsForScaledDot(const HloInstruction* instr);\n+\n   absl::StatusOr<std::unique_ptr<HloModule>> RunHloPasses(\n       std::unique_ptr<HloModule> hlo_module,\n       const Compiler::CompileOptions& options) override;"
        },
        {
            "sha": "595599700c6f3b617aad400ccc5a24b60309b427",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 2,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d977891827372fe0069636e5180c86179730541/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=7d977891827372fe0069636e5180c86179730541",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n-#include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n@@ -47,7 +46,6 @@ namespace {\n \n using absl_testing::IsOk;\n using absl_testing::StatusIs;\n-using ::tsl::proto_testing::EqualsProto;\n using TritonBackendConfig = AutotuneResult::TritonGemmKey;\n \n const char kHlo[] = R\"(\n@@ -70,6 +68,25 @@ const char kHlo[] = R\"(\n       backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n   })\";\n \n+const char kScaledDotHlo[] = R\"(\n+HloModule ScaledDotIsFused, entry_computation_layout={(bf16[4,4]{1,0}, bf16[4,4]{1,0}, bf16[1,1]{1,0}, bf16[1,1]{1,0})->bf16[4,4]{1,0}}\n+\n+%fusion_dot (parameter_0: bf16[4,4], parameter_1: bf16[4,4], parameter_2: bf16[1,1], parameter_3: bf16[1,1]) -> bf16[4,4] {\n+  %parameter_0 = bf16[4,4]{1,0} parameter(0)\n+  %parameter_1 = bf16[4,4]{1,0} parameter(1)\n+  %parameter_2 = bf16[1,1]{1,0} parameter(2)\n+  %parameter_3 = bf16[1,1]{1,0} parameter(3)\n+  ROOT %dot.1 = bf16[4,4]{1,0} scaled-dot(%parameter_0, %parameter_1, %parameter_2, %parameter_3), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name=\"foo\"}\n+}\n+\n+ENTRY %entry (lhs: bf16[4,4], rhs: bf16[4,4], lhs_scale: bf16[1,1], rhs_scale: bf16[1,1]) -> bf16[4,4] {\n+  %lhs = bf16[4,4]{1,0} parameter(0)\n+  %rhs = bf16[4,4]{1,0} parameter(1)\n+  %lhs_scale = bf16[1,1]{1,0} parameter(2)\n+  %rhs_scale = bf16[1,1]{1,0} parameter(3)\n+  ROOT %fusion = bf16[4,4]{1,0} fusion(%lhs, %rhs, %lhs_scale, %rhs_scale), kind=kCustom, calls=%fusion_dot, metadata={op_name=\"foo\"}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_INVALID\"}\n+})\";\n+\n class TritonBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   TritonBackendTest()\n@@ -115,6 +132,28 @@ TEST_F(TritonBackendTest, GetSupportedConfigs) {\n   }\n }\n \n+TEST_F(TritonBackendTest, GetSupportedConfigsForScaledDot) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kScaledDotHlo));\n+  HloInstruction* fusion_instr =\n+      module->entry_computation()->root_instruction();\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n+      backend_.GetSupportedConfigs(*fusion_instr);\n+  EXPECT_THAT(configs, absl_testing::IsOk());\n+  EXPECT_GT(configs.value().size(), 0);\n+}\n+\n+TEST_F(TritonBackendTest, GetAndApplyConfigForScaledDot) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kScaledDotHlo));\n+  HloInstruction* fusion_instr =\n+      module->entry_computation()->root_instruction();\n+  absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n+      backend_.GetDefaultConfig(*fusion_instr);\n+  EXPECT_THAT(config, absl_testing::IsOk());\n+  EXPECT_THAT(backend_.ApplyConfig(*fusion_instr, *config.value()), IsOk());\n+}\n+\n TEST_F(TritonBackendTest, GetSupportedConfigsRestrictedDefaultSearch) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kHlo));"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 82,
        "deletions": 14
    }
}