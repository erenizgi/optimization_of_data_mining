{
    "author": "ezhulenev",
    "message": "[xla:gpu] Add support for attaching use payload to CollectiveMultimem + use ranks instead of device ordinals in collective metadata APIs\n\nPiperOrigin-RevId: 840823488",
    "sha": "a29174d1ee4387ccac5bac43d174c550ebde07e2",
    "files": [
        {
            "sha": "1ee066f5e774334631975a63ccc4f5cc81909302",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=a29174d1ee4387ccac5bac43d174c550ebde07e2",
            "patch": "@@ -155,10 +155,10 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n       parameters.size() * clique_key.num_devices() * sizeof(uint64_t);\n   state.metadata = params.executor->Allocate(\n       sizeof(CollectiveKernelMetadata) + param_to_peers_ptrs_size_bytes, 0);\n+\n   return CollectiveMetadataThunk::ConstructCollectiveMetadata(\n-      std::move(parameters), params.stream, clique_key,\n-      state.multicast_device_ptr, params.executor->device_ordinal(),\n-      state.metadata);\n+      clique_key, state.rank, params.stream, std::move(parameters),\n+      state.collective_multimem, state.metadata);\n }\n \n absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {"
        },
        {
            "sha": "2d719e4b8db85265c3e3e14308a57c497b3729aa",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 39,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=a29174d1ee4387ccac5bac43d174c550ebde07e2",
            "patch": "@@ -90,8 +90,8 @@ struct DeviceParameters {\n };\n \n absl::StatusOr<std::vector<DeviceParameters>> SyncLocalDeviceParameters(\n-    const GpuCliqueKey& clique_key, int device_ordinal,\n-    const std::vector<se::DeviceMemoryBase>& parameters) {\n+    const GpuCliqueKey& clique_key, RankId rank,\n+    std::vector<se::DeviceMemoryBase> parameters) {\n   std::vector<DeviceParameters> device_parameters;\n   auto rendezvous_fn = [](absl::Span<const DeviceParameters* const> values) {\n     std::vector<DeviceParameters> values_copy;\n@@ -104,12 +104,12 @@ absl::StatusOr<std::vector<DeviceParameters>> SyncLocalDeviceParameters(\n     return values_copy;\n   };\n \n-  std::string start_rendezvous_key =\n-      absl::StrFormat(\"[%d] Initializing collective metadata for clique %s\",\n-                      device_ordinal, clique_key.ToString());\n+  std::string start_rendezvous_key = absl::StrFormat(\n+      \"[rank=%d] Initializing collective metadata for clique %s\", rank.value(),\n+      clique_key.ToString());\n \n   DeviceParameters params;\n-  params.rank = device_ordinal;\n+  params.rank = rank;\n   params.parameters = std::move(parameters);\n \n   TF_ASSIGN_OR_RETURN(\n@@ -123,39 +123,34 @@ absl::StatusOr<std::vector<DeviceParameters>> SyncLocalDeviceParameters(\n }\n \n absl::StatusOr<std::vector<DeviceParameters>> SyncGlobalDeviceParameters(\n-    const GpuCliqueKey& clique_key, int device_ordinal,\n-    const std::vector<se::DeviceMemoryBase>& parameters) {\n+    const GpuCliqueKey& clique_key, RankId rank,\n+    std::vector<se::DeviceMemoryBase> parameters) {\n   if (!clique_key.is_local()) {\n-    return absl::UnimplementedError(absl::StrCat(\n-        XlaFormatDevice(device_ordinal),\n-        \"Multiprocess collective metadata is not supported yet in clique \",\n-        clique_key.ToString()));\n+    return Unimplemented(\n+        \"[rank=%d] Multiprocess collective metadata is not supported yet in \"\n+        \"clique %s\",\n+        rank.value(), clique_key.ToString());\n   }\n \n   TF_ASSIGN_OR_RETURN(\n       std::vector<DeviceParameters> local_ranks_parameters,\n-      SyncLocalDeviceParameters(clique_key, device_ordinal, parameters));\n+      SyncLocalDeviceParameters(clique_key, rank, std::move(parameters)));\n \n   return local_ranks_parameters;\n }\n \n absl::Status CollectiveMetadataThunk::ConstructCollectiveMetadata(\n-    std::vector<se::DeviceMemoryBase> parameters, se::Stream* stream,\n-    const GpuCliqueKey& clique_key, void* multimem_address_space,\n-    int device_ordinal, se::DeviceMemoryBase destination) {\n+    const GpuCliqueKey& clique_key, RankId rank, se::Stream* stream,\n+    std::vector<se::DeviceMemoryBase> parameters,\n+    std::shared_ptr<CollectiveMultimem> multimem,\n+    se::DeviceMemoryBase destination) {\n   CollectiveKernelMetadata metadata;\n-  metadata.rank = clique_key.rank(GlobalDeviceId(device_ordinal))\n-                      .value_or(RankId(-1))\n-                      .value();\n-  if (metadata.rank == -1) {\n-    return absl::InternalError(\n-        absl::StrFormat(\"Device %d not found in clique %s\", device_ordinal,\n-                        clique_key.ToString()));\n-  }\n-  metadata.multicast_buffer_ptr = multimem_address_space;\n+  metadata.rank = rank.value();\n+  metadata.multicast_buffer_ptr =\n+      multimem ? multimem->mapped_ptr(rank) : nullptr;\n   TF_ASSIGN_OR_RETURN(\n       std::vector<DeviceParameters> device_parameters,\n-      SyncGlobalDeviceParameters(clique_key, device_ordinal, parameters));\n+      SyncGlobalDeviceParameters(clique_key, rank, std::move(parameters)));\n   TF_RET_CHECK(!device_parameters.empty())\n       << \"Not enough devices in the clique.\";\n   const size_t num_parameters = device_parameters[0].parameters.size();\n@@ -227,20 +222,26 @@ absl::Status CollectiveMetadataThunk::Initialize(\n   se::DeviceMemoryBase result_ptr =\n       params.buffer_allocations->GetDeviceAddress(result_);\n \n-  TF_ASSIGN_OR_RETURN(void* multimem_address_space,\n-                      AllocateMultimem(clique_key, params));\n-  return ConstructCollectiveMetadata(\n-      std::move(parameters), params.stream, clique_key, multimem_address_space,\n-      params.executor->device_ordinal(), result_ptr);\n+  GlobalDeviceId global_device_id = params.collective_params->global_device_id;\n+  std::optional<RankId> rank = clique_key.rank(global_device_id);\n+\n+  TF_ASSIGN_OR_RETURN(auto multimem,\n+                      AllocateMultimem(clique_key, *rank, params));\n+\n+  return ConstructCollectiveMetadata(clique_key, *rank, params.stream,\n+                                     std::move(parameters), std::move(multimem),\n+                                     result_ptr);\n }\n \n absl::Status CollectiveMetadataThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   return absl::OkStatus();\n }\n \n-absl::StatusOr<void*> CollectiveMetadataThunk::AllocateMultimem(\n-    const GpuCliqueKey& clique_key, const InitializeParams& params) {\n+absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n+CollectiveMetadataThunk::AllocateMultimem(const GpuCliqueKey& clique_key,\n+                                          RankId rank,\n+                                          const InitializeParams& params) {\n   se::DeviceMemoryBase memory_range;\n   for (const Buffer& parameter : parameters_) {\n     if (parameter.memory_space == xla::Layout::kGenericFastMemorySpace) {\n@@ -258,17 +259,13 @@ absl::StatusOr<void*> CollectiveMetadataThunk::AllocateMultimem(\n     return nullptr;\n   }\n \n-  GlobalDeviceId global_device_id = params.collective_params->global_device_id;\n-\n-  std::optional<RankId> rank = clique_key.rank(global_device_id);\n   TF_ASSIGN_OR_RETURN(std::shared_ptr<CollectiveMultimem> collective_multimem,\n                       CollectiveMultimem::Allocate(params.executor, clique_key,\n-                                                   *rank, memory_range));\n+                                                   rank, memory_range));\n \n   absl::MutexLock lock(mutex_);\n   return (collective_multimem_[params.executor] =\n-              std::move(collective_multimem))\n-      ->mapped_ptr(*rank);\n+              std::move(collective_multimem));\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "9a4344348b568f569390f585446a186018d35d0b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.h",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h?ref=a29174d1ee4387ccac5bac43d174c550ebde07e2",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/collective_multimem.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -62,9 +63,10 @@ class CollectiveMetadataThunk : public Thunk {\n   // All participants should call this method to construct their local\n   // metadata.\n   static absl::Status ConstructCollectiveMetadata(\n-      std::vector<se::DeviceMemoryBase> parameters, se::Stream* stream,\n-      const GpuCliqueKey& clique_key, void* multimem_address_space,\n-      int device_ordinal, se::DeviceMemoryBase destination);\n+      const GpuCliqueKey& clique_key, RankId rank, se::Stream* stream,\n+      std::vector<se::DeviceMemoryBase> parameters,\n+      std::shared_ptr<CollectiveMultimem> multimem,\n+      se::DeviceMemoryBase destination);\n \n   // Calculate the device memory base for the given parameter index.\n   // The size of the returned memory is num_devices pointers.\n@@ -73,8 +75,9 @@ class CollectiveMetadataThunk : public Thunk {\n       int64_t num_devices, int64_t parameter_index);\n \n  private:\n-  absl::StatusOr<void*> AllocateMultimem(const GpuCliqueKey& clique_key,\n-                                         const InitializeParams& params);\n+  absl::StatusOr<std::shared_ptr<CollectiveMultimem>> AllocateMultimem(\n+      const GpuCliqueKey& clique_key, RankId rank,\n+      const InitializeParams& params);\n \n   const CollectiveConfig collective_config_;\n   std::vector<Buffer> parameters_;"
        },
        {
            "sha": "d4d721535200ba7eb59d062ebabe99856e67159a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc?ref=a29174d1ee4387ccac5bac43d174c550ebde07e2",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/collective_multimem.h\"\n \n+#include <any>\n #include <cstdint>\n #include <memory>\n #include <optional>\n@@ -73,6 +74,7 @@ struct AllocateParams {\n   se::StreamExecutor* executor;\n   RankId rank;\n   se::DeviceMemoryBase map_to;\n+  std::any payload;\n };\n \n struct RankCmp {\n@@ -100,7 +102,7 @@ struct MappedPtrFormatter {\n absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n                              const GpuCliqueKey& clique_key, RankId rank,\n-                             se::DeviceMemoryBase map_to) {\n+                             se::DeviceMemoryBase map_to, std::any payload) {\n   VLOG(3) << absl::StrFormat(\n       \"rank=[%d] Allocate collective multimem for clique: %s\", rank.value(),\n       clique_key.ToString());\n@@ -116,7 +118,7 @@ CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n   std::string rendezvous_name = absl::StrFormat(\n       \"CollectiveMultimem::Allocate for clique %s\", clique_key.ToString());\n   AllocateRendezvousKey rendezvous_key = {clique_key};\n-  AllocateParams params = {executor, rank, map_to};\n+  AllocateParams params = {executor, rank, map_to, std::move(payload)};\n \n   // A callback for rendezvous to allocate and map the multicast memory.\n   auto allocate = [&](absl::Span<const AllocateParams*> params)\n@@ -157,6 +159,12 @@ CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n               dynamic_cast<se::gpu::GpuExecutor*>(param->executor)));\n     }\n \n+    // For all participating devices move payloads to the collective multimem.\n+    absl::btree_map<RankId, std::any> payloads;\n+    for (const auto* param : params) {\n+      payloads[param->rank] = std::move(param->payload);\n+    }\n+\n     VLOG(3) << absl::StrFormat(\n         \"Allocated collective multimem for clique: %s; mapped_ptrs: [%s]\",\n         clique_key.ToString(),\n@@ -177,9 +185,9 @@ absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n                              const GpuCliqueKey& clique_key,\n                              GlobalDeviceId global_device_id,\n-                             se::DeviceMemoryBase map_to) {\n+                             se::DeviceMemoryBase map_to, std::any payload) {\n   if (std::optional<RankId> rank = clique_key.rank(global_device_id)) {\n-    return Allocate(executor, clique_key, *rank, map_to);\n+    return Allocate(executor, clique_key, *rank, map_to, std::move(payload));\n   }\n   return InvalidArgument(\"Rank not found for device %v\", global_device_id);\n }"
        },
        {
            "sha": "b1a8e6b5df2d4628f88b6a1fa36b98fbd456978f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.h",
            "status": "modified",
            "additions": 32,
            "deletions": 4,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a29174d1ee4387ccac5bac43d174c550ebde07e2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h?ref=a29174d1ee4387ccac5bac43d174c550ebde07e2",
            "patch": "@@ -16,8 +16,9 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_\n #define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_\n \n+#include <any>\n+#include <functional>\n #include <memory>\n-#include <optional>\n \n #include \"absl/container/btree_map.h\"\n #include \"absl/status/statusor.h\"\n@@ -27,7 +28,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/util.h\"\n+#include \"xla/util.h\"  // IWYU pragma: keep\n \n namespace xla::gpu {\n \n@@ -47,20 +48,29 @@ class CollectiveMultimem {\n   // rank then gets a virtual memory address bound to the multicast memory, and\n   // operations performed via this pointer gets broadcasted to all participating\n   // devices.\n+  //\n+  // The optional `payload` argument is captured by the returned shared pointer\n+  // to allow callers to associate arbitrary data with the collective multimem.\n   static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n       se::StreamExecutor* executor, const GpuCliqueKey& clique_key, RankId rank,\n-      se::DeviceMemoryBase map_to);\n+      se::DeviceMemoryBase map_to, std::any payload = {});\n \n   // Allocates a CollectiveMultimem for the given global device id.\n   static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n       se::StreamExecutor* executor, const GpuCliqueKey& clique_key,\n-      GlobalDeviceId global_device_id, se::DeviceMemoryBase map_to);\n+      GlobalDeviceId global_device_id, se::DeviceMemoryBase map_to,\n+      std::any payload = {});\n \n   const GpuCliqueKey& clique_key() const { return clique_key_; }\n \n   // Returns the device pointer to the multicast memory for the given rank.\n   void* mapped_ptr(RankId rank) const { return mapped_ptrs_.at(rank); }\n \n+  // Returns the payload associated with the given rank. If payload type is not\n+  // the same as `T`, returns an error.\n+  template <typename T>\n+  absl::StatusOr<std::reference_wrapper<T>> payload(RankId rank) const;\n+\n  private:\n   CollectiveMultimem(\n       GpuCliqueKey clique_key, absl::btree_map<RankId, void*> mapped_ptrs,\n@@ -72,10 +82,28 @@ class CollectiveMultimem {\n   // A mapping from a participating rank to the mapped virtual memory pointer.\n   absl::btree_map<RankId, void*> mapped_ptrs_;\n \n+  // A mapping from a participating rank to the payload passed to the Allocate.\n+  absl::btree_map<RankId, std::any> payload_;\n+\n   // All virtual memory pointers are registered with this multicast memory.\n   std::unique_ptr<se::gpu::MulticastMemory> multicast_memory_;\n };\n \n+template <typename T>\n+absl::StatusOr<std::reference_wrapper<T>> CollectiveMultimem::payload(\n+    RankId rank) const {\n+  auto it = payload_.find(rank);\n+  if (it == payload_.end()) {\n+    return NotFound(\"Payload not found for rank %d\", rank.value());\n+  }\n+\n+  if (std::any_cast<T>(&it->second) == nullptr) {\n+    return InvalidArgument(\"Payload type mismatch for rank %d\", rank.value());\n+  }\n+\n+  return std::ref(std::any_cast<T&>(&it->second));\n+}\n+\n }  // namespace xla::gpu\n \n #endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_"
        }
    ],
    "stats": {
        "total": 146,
        "additions": 91,
        "deletions": 55
    }
}