{
    "author": "ermilovmaxim",
    "message": "fix and reenable analytical_latency_estimator_test\n\nPiperOrigin-RevId: 817280140",
    "sha": "9bb4b3f9cdd7d935e9209daf714863d3a738abce",
    "files": [
        {
            "sha": "3bf5c991f37b6d6925e16cf5bafb4855696d97df",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9bb4b3f9cdd7d935e9209daf714863d3a738abce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9bb4b3f9cdd7d935e9209daf714863d3a738abce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=9bb4b3f9cdd7d935e9209daf714863d3a738abce",
            "patch": "@@ -5,7 +5,7 @@ load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n \n # Libraries for performance modeling of HLO.\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n-load(\"//xla/tsl:tsl.bzl\", \"if_google\", \"internal_visibility\")\n+load(\"//xla/tsl:tsl.bzl\", \"if_google\", \"if_oss\", \"internal_visibility\")\n load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n load(\"//xla/tsl/platform:build_config.bzl\", \"tf_proto_library\")\n load(\"//xla/tsl/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\")\n@@ -157,12 +157,7 @@ xla_test(\n         \"amdgpu_any\",\n     ],\n     tags =\n-        if_google(\n-            [\n-                # TODO(b/414617899): Reenable once NVML works\n-                \"manual\",\n-                \"notap\",\n-            ],\n+        if_oss(\n             # TODO(b/435404154): Reenable once this is fixed.\n             [\"no_oss\"],\n         ),\n@@ -177,12 +172,14 @@ xla_test(\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:casts\",\n     ],\n )"
        },
        {
            "sha": "3bdc44d0703f172748ed3d413ccd0687d0bf8d06",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9bb4b3f9cdd7d935e9209daf714863d3a738abce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9bb4b3f9cdd7d935e9209daf714863d3a738abce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc?ref=9bb4b3f9cdd7d935e9209daf714863d3a738abce",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/gpu/alias_info.h\"\n@@ -39,6 +40,7 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/platform/casts.h\"\n \n@@ -162,11 +164,12 @@ ENTRY entry {\n   HloSchedule& module_schedule = hlo_module->schedule();\n   EXPECT_TRUE(hlo_module->has_entry_computation());\n \n+  auto mlir_context = std::make_unique<mlir::MLIRContext>();\n   auto scheduler_config = GetDefaultSchedulerConfig();\n   auto latency_estimator = std::make_unique<AnalyticalLatencyEstimator>(\n       scheduler_config, std::make_unique<ApproximateLatencyEstimator>(),\n       dev_info, HloCostAnalysis::DefaultShapeSize,\n-      hlo_module->entry_computation());\n+      hlo_module->entry_computation(), mlir_context.get());\n   auto alias_info = GetAliasInfo();\n   EXPECT_TRUE(RunScheduler(hlo_module.get(), scheduler_config, alias_info.get(),\n                            std::move(latency_estimator))"
        },
        {
            "sha": "30017a1b483c0b653d045438ddf28e03ed718c24",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9bb4b3f9cdd7d935e9209daf714863d3a738abce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9bb4b3f9cdd7d935e9209daf714863d3a738abce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc?ref=9bb4b3f9cdd7d935e9209daf714863d3a738abce",
            "patch": "@@ -236,7 +236,9 @@ float GetMaxLowLatencyBandwidth(const BandwidthSettings& bandwidth_settings) {\n   auto max_sys_bw = bandwidth_settings.GetMaxSysBwFromGpu(\n       bandwidth_settings.kLowLatencyMaxBandwidths.data());\n   auto it = std::lower_bound(std::begin(speeds), std::end(speeds), max_sys_bw);\n-  CHECK(it != std::cend(speeds));\n+  if (it == speeds.end()) {\n+    return speeds.back();\n+  }\n   return *it;\n }\n \n@@ -379,7 +381,7 @@ RocmBandwidthSettings CreateSettings(\n }  // namespace\n \n /*static*/ bool GpuPerformanceWithCollectiveModel::InitNvml() {\n-#if GOOGLE_CUDA && defined(PLATFORM_POSIX) && !defined(PLATFORM_GOOGLE)\n+#if GOOGLE_CUDA && (defined(PLATFORM_POSIX) || defined(PLATFORM_GOOGLE))\n   void* libhandle = dlopen(\"libnvidia-ml.so.1\", RTLD_NOW);\n   CHECK(libhandle != nullptr) << \"Failed to open libnvidia-ml.so.1\";\n \n@@ -396,7 +398,9 @@ RocmBandwidthSettings CreateSettings(\n        \"nvmlDeviceGetNvLinkCapability\"},\n       {(void**)&xla_nvmlSystemGetNVMLVersion, \"nvmlSystemGetNVMLVersion\"},\n   };\n-#if GOOGLE_CUDA && CUDA_VERSION >= 12040\n+\n+#if GOOGLE_CUDA && CUDA_VERSION >= 12040 && !defined(PLATFORM_GOOGLE)\n+  // Some hosts might still have older driver version(b/414617899).\n   symbols.push_back({(void**)&xla_nvmlDeviceGetHandleByPciBusId_v2,\n                      \"nvmlDeviceGetHandleByPciBusId_v2\"});\n   symbols.push_back({(void**)&xla_nvmlDeviceGetGpuFabricInfoV,"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 15,
        "deletions": 11
    }
}