{
    "author": "terryysun",
    "message": "PR #31795: [GPU] Assign default color to tuples\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31795\n\nüìù Summary of Changes\n[Downstream check](https://github.com/openxla/xla/blob/main/xla/pjrt/pjrt_executable.cc#L288-L299) assumes tuples on default memory space, force assign default color to tuples will get around the check.\n\nüéØ Justification\nNCCL user buffer runs are crashing on MaxText main. This PR fixes the crash.\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nN/A.\n\nüß™ Unit Tests:\nExisting unit tests.\n\nüß™ Execution Tests:\nAdded multiple execution tests.\n\nCopybara import of the project:\n\n--\nc60fe9d62827596eac57df2b480891520b40ab07 by Terry Sun <tesun@nvidia.com>:\n\nassign default color yo tuples\n\n--\n717412a55a94be71afcbb7627f03905c408f8b6a by Terry Sun <tesun@nvidia.com>:\n\nadd constant and polish doc string\n\n--\nc907b2d1ca5a62299b6bfd2534e99c6215313ffd by Terry Sun <tesun@nvidia.com>:\n\nupdate test\n\nMerging this change closes #31795\n\nPiperOrigin-RevId: 818295813",
    "sha": "499c00e520cdbbbdb22fb07261a5cfb6cc190009",
    "files": [
        {
            "sha": "7801d3320c95a3e073112e4b89f0a8af1027678d",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 184,
            "deletions": 0,
            "changes": 184,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/499c00e520cdbbbdb22fb07261a5cfb6cc190009/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/499c00e520cdbbbdb22fb07261a5cfb6cc190009/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=499c00e520cdbbbdb22fb07261a5cfb6cc190009",
            "patch": "@@ -1799,6 +1799,190 @@ TEST(StreamExecutorGpuClientTest, ExecutablePinnedHostOutputMemoryKindTest) {\n   EXPECT_EQ(memory_kinds[0][0], \"pinned_host\");\n }\n \n+TEST(StreamExecutorGpuClientTest,\n+     GetCompiledMemoryStatsWithTupleAndNcclUserBuffers) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client,\n+                          GetStreamExecutorGpuClient(DefaultOptions()));\n+\n+  xla::CompileOptions options;\n+  options.executable_build_options.mutable_debug_options()\n+      ->set_xla_gpu_enable_nccl_user_buffers(true);\n+\n+  constexpr char const* kProgramWithCollectiveAndTuple = R\"(\n+ HloModule test\n+\n+ region_0 {\n+   Arg_0 = f32[] parameter(0)\n+   Arg_1 = f32[] parameter(1)\n+   ROOT add = f32[] add(Arg_0, Arg_1)\n+ }\n+\n+ ENTRY main {\n+   p0 = f32[512,128]{1,0} parameter(0)\n+   p1 = f32[512,32,128]{2,1,0} parameter(1)\n+   p2 = f32[512,8,128]{2,1,0} parameter(2)\n+   p3 = f32[512,14336]{1,0} parameter(3)\n+   p4 = f32[1024]{0} parameter(4)\n+   p5 = f32[1]{0} parameter(5)\n+\n+   // All-gather operations that will use memory space 1 with NCCL user buffers\n+   ag0 = f32[4096,128]{1,0} all-gather(p0), channel_id=1, replica_groups=[1,8]<=[8], dimensions={0}, use_global_device_ids=true\n+   ag1 = f32[4096,32,128]{2,1,0} all-gather(p1), channel_id=2, replica_groups=[1,8]<=[8], dimensions={0}, use_global_device_ids=true\n+   ag2 = f32[4096,8,128]{2,1,0} all-gather(p2), channel_id=3, replica_groups=[1,8]<=[8], dimensions={0}, use_global_device_ids=true\n+   ag3 = f32[4096,14336]{1,0} all-gather(p3), channel_id=4, replica_groups=[1,8]<=[8], dimensions={0}, use_global_device_ids=true\n+\n+   ar0 = f32[1024]{0} all-reduce(p4), channel_id=5, to_apply=region_0\n+   ar1 = f32[1]{0} all-reduce(p5), channel_id=6, to_apply=region_0\n+\n+   // Regular operations with default memory space\n+   add0 = f32[512,128]{1,0} add(p0, p0)\n+   add1 = f32[512,32,128]{2,1,0} add(p1, p1)\n+   add2 = f32[512,8,128]{2,1,0} add(p2, p2)\n+\n+   // Mix of all-gather results (memory space 1) and regular tensors (memory space 0)\n+   ROOT tuple = (f32[4096,128]{1,0}, f32[4096,32,128]{2,1,0}, f32[4096,8,128]{2,1,0}, f32[4096,14336]{1,0},\n+                 f32[1024]{0}, f32[1]{0}, f32[1024]{0}, f32[1]{0},\n+                 f32[512,128]{1,0}, f32[512,32,128]{2,1,0}, f32[512,8,128]{2,1,0}, f32[512,14336]{1,0},\n+                 f32[4096,128]{1,0}, f32[4096,32,128]{2,1,0}, f32[4096,8,128]{2,1,0}, f32[4096,14336]{1,0},\n+                 f32[1024]{0}, f32[1]{0}, f32[1024]{0}, f32[1]{0},\n+                 f32[512,128]{1,0}, f32[512,32,128]{2,1,0}, f32[512,8,128]{2,1,0}, f32[512,14336]{1,0},\n+                 f32[4096,128]{1,0}, f32[4096,32,128]{2,1,0}, f32[4096,8,128]{2,1,0}, f32[4096,14336]{1,0},\n+                 f32[1024]{0}, f32[1]{0}, f32[1024]{0}, f32[1]{0},\n+                 f32[512,128]{1,0}, f32[512,32,128]{2,1,0}, f32[512,8,128]{2,1,0}, f32[512,14336]{1,0},\n+                 f32[4096,128]{1,0}, f32[4096,32,128]{2,1,0}, f32[4096,8,128]{2,1,0}, f32[4096,14336]{1,0},\n+                 f32[1024]{0}, f32[1]{0}, f32[1024]{0}, f32[1]{0},\n+                 f32[512,128]{1,0}, f32[512,32,128]{2,1,0}, f32[512,8,128]{2,1,0}, f32[512,14336]{1,0},\n+                 f32[4096,128]{1,0}, f32[4096,32,128]{2,1,0}, f32[4096,8,128]{2,1,0}, f32[4096,14336]{1,0})\n+                tuple(ag0, ag1, ag2, ag3, ar0, ar1, ar0, ar1,\n+                      p0, p1, p2, p3, ag0, ag1, ag2, ag3,\n+                      ar0, ar1, ar0, ar1, add0, add1, add2, p3,\n+                      ag0, ag1, ag2, ag3, ar0, ar1, ar0, ar1,\n+                      p0, p1, p2, p3, ag0, ag1, ag2, ag3,\n+                      ar0, ar1, ar0, ar1, add0, add1, add2, p3,\n+                      ag0, ag1, ag2, ag3)\n+ }\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto executable,\n+      CompileExecutable(kProgramWithCollectiveAndTuple, *client, options));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto memory_stats, executable->GetExecutable()->GetCompiledMemoryStats());\n+  EXPECT_EQ(memory_stats.output_size_in_bytes, 1764786624);\n+  EXPECT_EQ(memory_stats.host_output_size_in_bytes, 0);\n+  EXPECT_EQ(memory_stats.peak_memory_in_bytes, 1845010888);\n+}\n+\n+TEST(StreamExecutorGpuClientTest, GetCompiledMemoryStatsMixedTuple) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client,\n+                          GetStreamExecutorGpuClient(DefaultOptions()));\n+\n+  xla::CompileOptions options;\n+  options.executable_build_options.mutable_debug_options()\n+      ->set_xla_gpu_enable_nccl_user_buffers(true);\n+\n+  constexpr char const* kSimpleMixedTupleHlo = R\"(\n+HloModule test\n+\n+region_0 {\n+Arg_0 = f32[] parameter(0)\n+Arg_1 = f32[] parameter(1)\n+ROOT add = f32[] add(Arg_0, Arg_1)\n+}\n+\n+ENTRY main {\n+p0 = f32[2]{0} parameter(0)\n+// All-gather across 8 replicas to enlarge dim0.\n+ag = f32[16]{0} all-gather(p0), channel_id=1, replica_groups=[1,8]<=[8], dimensions={0}, use_global_device_ids=true\n+add0 = f32[2]{0} add(p0, p0)\n+ROOT tuple = (f32[16]{0}, f32[2]{0}, f32[2]{0}) tuple(ag, p0, add0)\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto executable,\n+      CompileExecutable(kSimpleMixedTupleHlo, *client, options));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto memory_stats, executable->GetExecutable()->GetCompiledMemoryStats());\n+\n+  EXPECT_EQ(memory_stats.output_size_in_bytes, 104);\n+  EXPECT_EQ(memory_stats.host_output_size_in_bytes, 0);\n+  EXPECT_EQ(memory_stats.peak_memory_in_bytes, 120);\n+}\n+\n+TEST(StreamExecutorGpuClientTest, GetCompiledMemoryStatsMixedTupleNotRoot) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client,\n+                          GetStreamExecutorGpuClient(DefaultOptions()));\n+\n+  xla::CompileOptions options;\n+  options.executable_build_options.mutable_debug_options()\n+      ->set_xla_gpu_enable_nccl_user_buffers(true);\n+\n+  constexpr char const* kMixedTupleNotRootHlo = R\"(\n+HloModule test\n+\n+ENTRY main {\n+p0 = f32[2]{0} parameter(0)\n+ag = f32[16]{0} all-gather(p0), channel_id=1, replica_groups=[1,8]<=[8], dimensions={0}, use_global_device_ids=true\n+add0 = f32[2]{0} add(p0, p0)\n+t = (f32[16]{0}, f32[2]{0}, f32[2]{0}) tuple(ag, p0, add0)\n+ROOT gte0 = f32[16]{0} get-tuple-element(t), index=0\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto executable,\n+      CompileExecutable(kMixedTupleNotRootHlo, *client, options));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto memory_stats, executable->GetExecutable()->GetCompiledMemoryStats());\n+\n+  EXPECT_EQ(memory_stats.output_size_in_bytes, 64);\n+  EXPECT_EQ(memory_stats.host_output_size_in_bytes, 0);\n+  EXPECT_EQ(memory_stats.peak_memory_in_bytes, 80);\n+}\n+\n+TEST(StreamExecutorGpuClientTest, GetCompiledMemoryStatsCountTupleTable) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client,\n+                          GetStreamExecutorGpuClient(DefaultOptions()));\n+\n+  constexpr char const* kManyTuplesHlo = R\"(\n+HloModule test\n+\n+ENTRY main {\n+p0 = f32[1]{0} parameter(0)\n+add0 = f32[1]{0} add(p0, p0)\n+ROOT t = (f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0},\n+         f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0},\n+         f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0},\n+         f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0},\n+         f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0},\n+         f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0},\n+         f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0},\n+         f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0})\n+ tuple(p0, add0, p0, add0,\n+       p0, add0, p0, add0,\n+       p0, add0, p0, add0,\n+       p0, add0, p0, add0,\n+       p0, add0, p0, add0,\n+       p0, add0, p0, add0,\n+       p0, add0, p0, add0,\n+       p0, add0, p0, add0)\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto executable,\n+                          CompileExecutable(kManyTuplesHlo, *client));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto memory_stats, executable->GetExecutable()->GetCompiledMemoryStats());\n+\n+  EXPECT_EQ(memory_stats.output_size_in_bytes, 384);\n+  EXPECT_EQ(memory_stats.host_output_size_in_bytes, 0);\n+  EXPECT_EQ(memory_stats.peak_memory_in_bytes, 388);\n+}\n+\n // Verify the output device memory kind with collective memory space shape\n // when NCCL user buffer is enabled.\n TEST(StreamExecutorGpuClientTest,"
        },
        {
            "sha": "b7c05621afe229221bcdf6bce81a380971b86538",
            "filename": "third_party/xla/xla/service/gpu/gpu_memory_space_assignment.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/499c00e520cdbbbdb22fb07261a5cfb6cc190009/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/499c00e520cdbbbdb22fb07261a5cfb6cc190009/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc?ref=499c00e520cdbbbdb22fb07261a5cfb6cc190009",
            "patch": "@@ -114,6 +114,10 @@ absl::Status AssignColors(bool use_collective_memory, bool use_nvshmem,\n         value->set_color(BufferValue::Color(memory_space));\n         continue;\n       }\n+    } else if (defining_position.shape().IsTuple()) {\n+      // Making sure tuples live in default memory space.\n+      value->set_color((int)MemorySpaceColor::kDefault);\n+      continue;\n     }\n \n     for (const auto& alias :"
        },
        {
            "sha": "f78ef009c8bfecee648fee16f3dad8d0ad838c01",
            "filename": "third_party/xla/xla/service/gpu/gpu_memory_space_assignment_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/499c00e520cdbbbdb22fb07261a5cfb6cc190009/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/499c00e520cdbbbdb22fb07261a5cfb6cc190009/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment_test.cc?ref=499c00e520cdbbbdb22fb07261a5cfb6cc190009",
            "patch": "@@ -210,8 +210,13 @@ TEST_P(GpuMosaicMemorySpaceAssignmentTest, TestMosaicMemorySpaceAssignment) {\n       EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->has_color(), true);\n       EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->color(),\n                 (int)(MosaicContainsNvshmem()\n-                          ? (UseNvshmem() ? MemorySpaceColor::kCollective\n-                                          : MemorySpaceColor::kDefault)\n+                          ? ((UseNvshmem() && !alias_analysis->buffers()[i]\n+                                                   .values()[0]\n+                                                   ->defining_position()\n+                                                   .shape()\n+                                                   .IsTuple())\n+                                 ? MemorySpaceColor::kCollective\n+                                 : MemorySpaceColor::kDefault)\n                           : MemorySpaceColor::kDefault));\n     }\n   }"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 195,
        "deletions": 2
    }
}