{
    "author": "felixwqp",
    "message": "[XLA:GPU] Enable CollectiveCombiner is the HLO is beyond a fast-interconnect domain, more explicitly using  `slice_size` to determine if it's running inside a fast-interconnect domain(NvLink). This will enable combiner for BlackWell.\n\nReverts d759920b6be0ffcc28812cf29f300cb0b522abe8\n\nPiperOrigin-RevId: 800309900",
    "sha": "5f18d711ff4ba03c475f61f94d48c87af211bb29",
    "files": [
        {
            "sha": "d94f43ca606bb34b0d02299ffbd0af94398a10c6",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=5f18d711ff4ba03c475f61f94d48c87af211bb29",
            "patch": "@@ -1223,13 +1223,13 @@ constexpr int kCombineThresholdCount = 256;\n void AddCollectiveCombinerPasses(\n     HloPassPipeline& pipeline, const HloModule& module,\n     const se::DeviceDescription& device_description,\n-    const GpuAliasInfo* alias_info, int pointer_size) {\n+    const GpuAliasInfo* alias_info, int pointer_size,\n+    const GpuCompiler::CompileOptions& options) {\n   const DebugOptions& opts = module.config().debug_options();\n \n   bool enable_heuristic_collective_combining =\n       opts.xla_gpu_experimental_enable_heuristic_collective_combining() &&\n-      GetTopologyType(module.config(), device_description) ==\n-          GPUTopologyType::MULTI_HOST;\n+      !IsNVLinkConnected(module.config(), options.slice_size);\n \n   if (enable_heuristic_collective_combining) {\n     pipeline.AddPass<CollectiveCombinerAnnotator>(device_description,\n@@ -1257,11 +1257,12 @@ void AddCollectiveCombinerPasses(\n \n absl::Status RunPostFusionPasses(\n     HloModule* hlo_module, const se::DeviceDescription& device_description,\n-    const GpuAliasInfo* alias_info, int pointer_size) {\n+    const GpuAliasInfo* alias_info, int pointer_size,\n+    const GpuCompiler::CompileOptions& options) {\n   HloPassPipeline pipeline(\"post-fusion optimization\");\n   pipeline.AddPass<RenameFusions>();\n   AddCollectiveCombinerPasses(pipeline, *hlo_module, device_description,\n-                              alias_info, pointer_size);\n+                              alias_info, pointer_size, options);\n \n   pipeline.AddPass<AllReduceContiguous>();\n \n@@ -1580,7 +1581,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n                                      thread_pool.get_mutable(),\n                                      ShapeSizeBytesFunction()));\n   TF_RETURN_IF_ERROR(RunPostFusionPasses(hlo_module, device_description,\n-                                         alias_info, pointer_size_));\n+                                         alias_info, pointer_size_, options));\n   TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(hlo_module));\n   TF_RETURN_IF_ERROR(RunPostFusionSimplificationPasses(\n       hlo_module,"
        },
        {
            "sha": "fca87e98c358fa3f67b16afd99ee70a6e70ff461",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD?ref=5f18d711ff4ba03c475f61f94d48c87af211bb29",
            "patch": "@@ -104,6 +104,7 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "98484864cbfa0376cebb7ac4d13f086c0df88acd",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 16,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=5f18d711ff4ba03c475f61f94d48c87af211bb29",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -162,23 +163,17 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n   return GPUCommunicationType::UNDEFINED;\n }\n \n-GPUTopologyType GetTopologyType(\n-    const HloModuleConfig& config,\n-    const se::DeviceDescription& device_description) {\n-  se::CudaComputeCapability cc = device_description.cuda_compute_capability();\n-  // TODO: b/390095346 - Use topology information once available at compile\n-  // time.\n-  if (cc.IsHopper()) {\n-    return config.num_partitions() * config.replica_count() > 8\n-               ? GPUTopologyType::MULTI_HOST\n-               : GPUTopologyType::SINGLE_HOST;\n-  }\n-  if (cc.IsAmpere()) {\n-    return config.num_partitions() * config.replica_count() > 16\n-               ? GPUTopologyType::MULTI_HOST\n-               : GPUTopologyType::SINGLE_HOST;\n+bool IsNVLinkConnected(const HloModuleConfig& config,\n+                       int64_t nvlink_slice_size) {\n+  int hlo_device_count = config.num_partitions() * config.replica_count();\n+  if (hlo_device_count <= nvlink_slice_size) {\n+    VLOG(1) << \"NVLink connected: HLO device count \" << hlo_device_count\n+            << \" <= NVLink slice size \" << nvlink_slice_size;\n+    return true;\n   }\n-  return GPUTopologyType::UNKNOWN;\n+  VLOG(1) << \"Not NVLink connected: HLO device count \" << hlo_device_count\n+          << \" > NVLink slice size \" << nvlink_slice_size;\n+  return false;\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "b0b155b1a99c82b3127e7743529d8e00e3e6efe3",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=5f18d711ff4ba03c475f61f94d48c87af211bb29",
            "patch": "@@ -16,6 +16,8 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_COLLECTIVE_OPS_UTILS_H_\n #define XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_COLLECTIVE_OPS_UTILS_H_\n \n+#include <cstdint>\n+\n #include \"absl/status/statusor.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -46,15 +48,8 @@ enum class GPUTopologyType {\n   MULTI_HOST = 2,\n };\n \n-// Returns the given device topology. Currently this function is\n-// heuristic based: it can be the case it will not detect a multi host case when\n-// a user decides to use < 8 GPUs per host. Moreover it tells nothing about how\n-// fast the interconnect between hosts is (Infiniband, NVLINK, DCN, etc.).\n-//\n-// Will return `UNKNOWN` on any platform other than Hopper and Ampere.\n-GPUTopologyType GetTopologyType(\n-    const HloModuleConfig& config,\n-    const se::DeviceDescription& device_description);\n+bool IsNVLinkConnected(const HloModuleConfig& config,\n+                       int64_t nvlink_slice_size);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "aec6ffd78dfe97622d839c78473e3f15b054996a",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 69,
            "changes": 124,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f18d711ff4ba03c475f61f94d48c87af211bb29/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc?ref=5f18d711ff4ba03c475f61f94d48c87af211bb29",
            "patch": "@@ -15,6 +15,8 @@ limitations under the License.\n \n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n \n+#include <cstdint>\n+\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status_matchers.h\"\n@@ -32,65 +34,52 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n+using ::absl_testing::IsOkAndHolds;\n using ::testing::Test;\n-using ::tsl::testing::IsOkAndHolds;\n \n-GPUTopologyType GetTopologyType(se::CudaComputeCapability compute_capability,\n-                                int num_partitions, int replica_count) {\n+bool IsNVLinkConnected(int num_partitions, int replica_count,\n+                       int64_t nvlink_slice_size) {\n   HloModuleConfig config;\n   config.set_num_partitions(num_partitions);\n   config.set_replica_count(replica_count);\n-  se::DeviceDescription device_description;\n-  device_description.set_gpu_compute_capability(compute_capability);\n-  return xla::gpu::GetTopologyType(config, device_description);\n-}\n-\n-TEST(GetTopologyTypeTest, SingleHostSingleDevice) {\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Ampere(),\n-                              /*num_partitions=*/1, /*replica_count=*/1),\n-              GPUTopologyType::SINGLE_HOST);\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Hopper(),\n-                              /*num_partitions=*/1, /*replica_count=*/1),\n-              GPUTopologyType::SINGLE_HOST);\n+  return xla::gpu::IsNVLinkConnected(config, nvlink_slice_size);\n }\n \n-TEST(GetTopologyTypeTest, SingleHostMultiDevices) {\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Ampere(),\n-                              /*num_partitions=*/16, /*replica_count=*/1),\n-              GPUTopologyType::SINGLE_HOST);\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Ampere(),\n-                              /*num_partitions=*/1, /*replica_count=*/16),\n-              GPUTopologyType::SINGLE_HOST);\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Hopper(),\n-                              /*num_partitions=*/8, /*replica_count=*/1),\n-              GPUTopologyType::SINGLE_HOST);\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Hopper(),\n-                              /*num_partitions=*/1, /*replica_count=*/8),\n-              GPUTopologyType::SINGLE_HOST);\n+TEST(IsNVLinkConnectedTest, SingleHostSingleDevice) {\n+  // H100/B200\n+  EXPECT_TRUE(IsNVLinkConnected(\n+      /*num_partitions=*/1, /*replica_count=*/1, /*nvlink_slice_size=*/8));\n+  // A100\n+  EXPECT_TRUE(IsNVLinkConnected(\n+      /*num_partitions=*/1, /*replica_count=*/1, /*nvlink_slice_size=*/16));\n }\n \n-TEST(GetTopologyTypeTest, MultiHosts) {\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Ampere(),\n-                              /*num_partitions=*/32, /*replica_count=*/1),\n-              GPUTopologyType::MULTI_HOST);\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Ampere(),\n-                              /*num_partitions=*/1, /*replica_count=*/32),\n-              GPUTopologyType::MULTI_HOST);\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Hopper(),\n-                              /*num_partitions=*/16, /*replica_count=*/1),\n-              GPUTopologyType::MULTI_HOST);\n-  EXPECT_THAT(GetTopologyType(se::CudaComputeCapability::Hopper(),\n-                              /*num_partitions=*/1, /*replica_count=*/16),\n-              GPUTopologyType::MULTI_HOST);\n+TEST(IsNVLinkConnectedTest, SingleHostMultiDevices) {\n+  // H100/B200\n+  EXPECT_TRUE(IsNVLinkConnected(\n+      /*num_partitions=*/8, /*replica_count=*/1, /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(IsNVLinkConnected(\n+      /*num_partitions=*/1, /*replica_count=*/8, /*nvlink_slice_size=*/8));\n+\n+  // A100\n+  EXPECT_TRUE(IsNVLinkConnected(\n+      /*num_partitions=*/1, /*replica_count=*/16, /*nvlink_slice_size=*/16));\n+  EXPECT_TRUE(IsNVLinkConnected(\n+      /*num_partitions=*/16, /*replica_count=*/1, /*nvlink_slice_size=*/16));\n }\n \n-TEST(GetTopologyTypeTest, NonAmpereAndHopper) {\n-  EXPECT_EQ(GetTopologyType(se::CudaComputeCapability::Volta(),\n-                            /*num_partitions=*/1, /*replica_count=*/1),\n-            GPUTopologyType::UNKNOWN);\n-  EXPECT_EQ(GetTopologyType(se::CudaComputeCapability::Blackwell(),\n-                            /*num_partitions=*/1, /*replica_count=*/1),\n-            GPUTopologyType::UNKNOWN);\n+TEST(IsNVLinkConnectedTest, MultiHosts) {\n+  // H100/B200\n+  EXPECT_FALSE(IsNVLinkConnected(\n+      /*num_partitions=*/16, /*replica_count=*/1, /*nvlink_slice_size=*/8));\n+  EXPECT_FALSE(IsNVLinkConnected(\n+      /*num_partitions=*/1, /*replica_count=*/16, /*nvlink_slice_size=*/8));\n+\n+  // A100\n+  EXPECT_FALSE(IsNVLinkConnected(\n+      /*num_partitions=*/1, /*replica_count=*/32, /*nvlink_slice_size=*/16));\n+  EXPECT_FALSE(IsNVLinkConnected(\n+      /*num_partitions=*/32, /*replica_count=*/1, /*nvlink_slice_size=*/16));\n }\n \n class CommunicationTypeTest : public Test {\n@@ -122,7 +111,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost8Devices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHost4Devices) {\n@@ -145,7 +134,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost4Devices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHost16Devices) {\n@@ -168,7 +157,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost16Devices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n TEST_F(CommunicationTypeTest, DetectRailAlignedAllDevices) {\n@@ -191,7 +180,7 @@ TEST_F(CommunicationTypeTest, DetectRailAlignedAllDevices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n }\n \n TEST_F(CommunicationTypeTest, DetectRailAlignedHalfMesh) {\n@@ -217,7 +206,7 @@ TEST_F(CommunicationTypeTest, DetectRailAlignedHalfMesh) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n }\n \n TEST_F(CommunicationTypeTest, DetectNonRailAligned) {\n@@ -238,10 +227,9 @@ TEST_F(CommunicationTypeTest, DetectNonRailAligned) {\n \n   HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n       module->entry_computation()->root_instruction());\n-  EXPECT_THAT(\n-      CommunicationType(/*num_devices_per_host=*/8, *instr,\n-                        device_info().gpu_compute_capability()),\n-      absl_testing::IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n+  EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHost16DevicesForEmptyReplicaGroups) {\n@@ -262,7 +250,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost16DevicesForEmptyReplicaGroups) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/16, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n TEST_F(CommunicationTypeTest, DetectsRailAligned8DevicesForEmptyReplicaGroups) {\n@@ -283,7 +271,7 @@ TEST_F(CommunicationTypeTest, DetectsRailAligned8DevicesForEmptyReplicaGroups) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n }\n \n TEST_F(CommunicationTypeTest, DetectsNonRailAligned16Devices) {\n@@ -302,10 +290,9 @@ TEST_F(CommunicationTypeTest, DetectsNonRailAligned16Devices) {\n \n   HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n       module->entry_computation()->root_instruction());\n-  EXPECT_THAT(\n-      CommunicationType(/*num_devices_per_host=*/8, *instr,\n-                        device_info().gpu_compute_capability()),\n-      absl_testing::IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n+  EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermute) {\n@@ -325,7 +312,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermute) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n TEST_F(CommunicationTypeTest, DetectsNonRailAlignedCollectivePermute) {\n@@ -344,10 +331,9 @@ TEST_F(CommunicationTypeTest, DetectsNonRailAlignedCollectivePermute) {\n \n   HloChannelInstruction* instr = Cast<HloChannelInstruction>(\n       module->entry_computation()->root_instruction());\n-  EXPECT_THAT(\n-      CommunicationType(/*num_devices_per_host=*/8, *instr,\n-                        device_info().gpu_compute_capability()),\n-      absl_testing::IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n+  EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n }\n \n TEST_F(CommunicationTypeTest, DetectsRailAlignedCollectivePermute) {\n@@ -367,7 +353,7 @@ TEST_F(CommunicationTypeTest, DetectsRailAlignedCollectivePermute) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              absl_testing::IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n }\n \n }  // namespace"
        }
    ],
    "stats": {
        "total": 178,
        "additions": 78,
        "deletions": 100
    }
}