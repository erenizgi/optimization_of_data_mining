{
    "author": "majiddadashi",
    "message": "Add support for int2/int4 in tfl.cast\n\nPiperOrigin-RevId: 820509011",
    "sha": "f67cb87691dfe2bf63f292948b30536e086b6277",
    "files": [
        {
            "sha": "b65c1839862400b1e74de9c47563d832ce6cb283",
            "filename": "RELEASE.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/RELEASE.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/RELEASE.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/RELEASE.md?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -23,6 +23,7 @@\n     * Adds int8 and int16x8 support for SQRT operator.\n     * Adds int16x8 support for EQUAL and NOT_EQUAL operators.\n     * AddsÂ support for int2 type.\n+    * Adds support for int2/int4 in tfl.cast.\n \n ### Bug Fixes and Other Changes\n "
        },
        {
            "sha": "64fc866b2be0554a0e695fc3b0236a085fb7ede8",
            "filename": "tensorflow/compiler/mlir/lite/ir/tfl_ops.td",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -112,6 +112,7 @@ class TFL_VariadicTensorOf<list<Type> allowedRuntimeTypes,\n   Variadic<TensorOf<allowedOpTypes>>,\n   TFL_RuntimeType<Variadic<TensorOf<allowedRuntimeTypes>>>;\n \n+def TFL_I2 : I<2>;\n def TFL_I4 : I<4>;\n def TFL_Int32Or64 : SignlessIntOfWidths<[32, 64]>;\n \n@@ -4072,13 +4073,10 @@ def TFL_CastOp : TFL_Op<\"cast\", [\n   }];\n \n   let arguments = (ins\n-    TFL_TensorOf<[F16, BF16, F32, F64, I1, TFL_I4, I16, UI16, I32, UI32, I64, TFL_Quint8, UI8, I8, Complex<F<32>>]>:$input\n+    TFL_TensorOf<[F16, BF16, F32, F64, I1, TFL_I2, TFL_I4, I16, UI16, I32, UI32, I64, TFL_Quint8, UI8, I8, Complex<F<32>>]>:$input\n   );\n \n-  // TODO(b/393644251): Temporary support for INT4 TFL_CastOp. Runtime\n-  // probably already supports INT4. We should remove the INT4 support here or\n-  // make sure the runtime supports is there, as part of closing the bug.\n-  let results = (outs TFL_TensorOf<[F16, BF16, F32, F64, I1, TFL_I4, I16, UI16, I32, UI32, I64, TFL_Quint8, UI8, I8, Complex<F<32>>]>:$output);\n+  let results = (outs TFL_TensorOf<[F16, BF16, F32, F64, I1, TFL_I2, TFL_I4, I16, UI16, I32, UI32, I64, TFL_Quint8, UI8, I8, Complex<F<32>>]>:$output);\n \n   // TFLite's cast op does not utilize CastOptions, instead derives types\n   // from the TfLiteTensors."
        },
        {
            "sha": "4d6d46e55bb5be2547460f38b89e5b2be27acce1",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/op_version.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -1073,8 +1073,11 @@ int GetBuiltinOperatorVersion(const OpSignature& op_sig) {\n       }\n       return 2;\n     case BuiltinOperator_CAST:\n-      if (op_sig.inputs.at(0).type == kTfLiteBFloat16 ||\n-          op_sig.outputs.at(0).type == kTfLiteBFloat16) {\n+      if (op_sig.inputs.at(0).type == kTfLiteInt2 ||\n+          op_sig.outputs.at(0).type == kTfLiteInt2) {\n+        return 8;\n+      } else if (op_sig.inputs.at(0).type == kTfLiteBFloat16 ||\n+                 op_sig.outputs.at(0).type == kTfLiteBFloat16) {\n         return 7;\n       } else if (op_sig.inputs.at(0).type == kTfLiteInt4 &&\n                  op_sig.outputs.at(0).type == kTfLiteFloat32) {"
        },
        {
            "sha": "641a2e45fb8c246a3c607063cec779aec0abbca4",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/op_version_test.cc",
            "status": "modified",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -1467,4 +1467,72 @@ TEST(OpVersionTest, VersioningSqrtTest) {\n   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16);\n   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);\n }\n+\n+TEST(OpVersionTest, VersioningCastTest) {\n+  OpSignature fake_op_sig = {};\n+  fake_op_sig.op = BuiltinOperator_CAST;\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt2);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 8);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt2);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 8);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteBFloat16);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 7);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteBFloat16);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 7);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt4);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 6);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat64);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat64);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat16);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat16);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt16);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteUInt16);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteUInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);\n+\n+  fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  fake_op_sig.outputs = CreateOpSignatureTensorSpecs(kTfLiteInt32);\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);\n+}\n }  // namespace tflite"
        },
        {
            "sha": "d7e6b7c9a2064c6e44d7f5186ea39c0dba590d6d",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/runtime_version.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -112,6 +112,7 @@ std::string FindMinimumRuntimeVersionForOp(tflite::BuiltinOperator op_code,\n               {{BuiltinOperator_CAST, 5}, \"2.12.0\"},\n               {{BuiltinOperator_CAST, 6}, \"2.15.0\"},\n               {{BuiltinOperator_CAST, 7}, \"2.17.0\"},\n+              {{BuiltinOperator_CAST, 8}, \"2.21.0\"},\n               {{BuiltinOperator_CONCATENATION, 1}, \"1.5.0\"},\n               {{BuiltinOperator_CONCATENATION, 2}, \"1.14.0\"},\n               {{BuiltinOperator_CONCATENATION, 3}, \"2.3.0\"},"
        },
        {
            "sha": "29576e8e06676a7fc44dba9331071e092493a31b",
            "filename": "tensorflow/compiler/mlir/lite/transforms/tf_legalizations/while_loop_outline_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Ftf_legalizations%2Fwhile_loop_outline_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Ftf_legalizations%2Fwhile_loop_outline_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Ftf_legalizations%2Fwhile_loop_outline_pass.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -59,10 +59,10 @@ bool IsCompatibleTypeWithTFLCastOp(Type type) {\n       elemType.isF64())\n     return true;\n \n-  // I1, I4, I8, I16, I32, I64 types are allowed.\n-  if (elemType.isInteger(1) || elemType.isInteger(4) || elemType.isInteger(8) ||\n-      elemType.isInteger(16) || elemType.isInteger(32) ||\n-      elemType.isInteger(64))\n+  // I1, I2, I4, I8, I16, I32, I64 types are allowed.\n+  if (elemType.isInteger(1) || elemType.isInteger(2) || elemType.isInteger(4) ||\n+      elemType.isInteger(8) || elemType.isInteger(16) ||\n+      elemType.isInteger(32) || elemType.isInteger(64))\n     return true;\n \n   // Complex<F<32>> is allowed."
        },
        {
            "sha": "6c7a8b3c7b8e306afa021cff169a9382e6c63203",
            "filename": "tensorflow/lite/core/kernels/register.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -176,7 +176,7 @@ BuiltinOpResolver::BuiltinOpResolver() {\n              /* max_version = */ 2);\n   AddBuiltin(BuiltinOperator_CAST, Register_CAST(),\n              /* min_version = */ 1,\n-             /* max_version = */ 7);\n+             /* max_version = */ 8);\n   AddBuiltin(BuiltinOperator_DEQUANTIZE, Register_DEQUANTIZE(),\n              /* min_version = */ 1,\n              /* max_version = */ 6);"
        },
        {
            "sha": "d07c6bd7281d87e1f8f294a72b38972f39949b41",
            "filename": "tensorflow/lite/kernels/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2FBUILD?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -172,6 +172,8 @@ cc_library(\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/log:absl_check\",\n+        \"@com_google_absl//absl/log:absl_log\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n@@ -1490,11 +1492,14 @@ cc_test(\n     tags = [\"tflite_nnapi\"],\n     deps = [\n         \":cast_test_common\",\n+        \":kernel_util\",\n         \":test_main\",\n         \":test_util\",\n         \"//tensorflow/lite/c:common\",\n         \"//tensorflow/lite/core/c:c_api_types\",\n+        \"//tensorflow/lite/kernels/internal:tensor_utils_no_eigen\",\n         \"//tensorflow/lite/schema:schema_fbs\",\n+        \"@com_google_absl//absl/random\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n         \"@eigen_archive//:eigen3\","
        },
        {
            "sha": "192a552bca4ea211b23161ca1e8c6fdd113f855d",
            "filename": "tensorflow/lite/kernels/cast.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fcast.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fcast.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fcast.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -18,11 +18,14 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <limits>\n+#include <type_traits>\n+#include <vector>\n \n #include \"Eigen/Core\"  // from @eigen_archive\n #include \"tensorflow/lite/core/c/common.h\"\n #include \"tensorflow/lite/core/subgraph.h\"\n #include \"tensorflow/lite/interpreter_options.h\"\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n #include \"tensorflow/lite/kernels/kernel_util.h\"\n #include \"tensorflow/lite/kernels/op_macros.h\"\n@@ -183,6 +186,19 @@ void copyCastToBFloat16(const Eigen::half* in, Eigen::bfloat16* out,\n   });\n }\n \n+TfLiteStatus castInt2ToFloat(TfLiteContext* context, const TfLiteTensor* in,\n+                             TfLiteTensor* out, int num_elements) {\n+  const int8_t* in_data = (const int8_t*)in->data.data;\n+  float* out_data = (float*)out->data.data;\n+  std::vector<int8_t> unpacked_temp(num_elements);\n+  tensor_utils::UnpackPackedIntToInt8(in_data, num_elements, /*bit_width=*/2,\n+                                      unpacked_temp.data());\n+  for (int i = 0; i < num_elements; ++i) {\n+    out_data[i] = static_cast<float>(unpacked_temp[i]);\n+  }\n+  return kTfLiteOk;\n+}\n+\n TfLiteStatus castInt4ToFloat(TfLiteContext* context, const TfLiteTensor* in,\n                              TfLiteTensor* out, int num_elements) {\n   const int8_t* in_data = (const int8_t*)in->data.data;\n@@ -240,6 +256,34 @@ TfLiteStatus castInt4ToFloat(TfLiteContext* context, const TfLiteTensor* in,\n   return kTfLiteOk;\n }\n \n+TfLiteStatus castFloatToInt4(const float* in, TfLiteTensor* out,\n+                             int num_elements) {\n+  const float min_val = -8.0f;\n+  const float max_val = 7.0f;\n+  std::vector<int8_t> unpacked_temp(num_elements);\n+  for (int i = 0; i < num_elements; ++i) {\n+    unpacked_temp[i] =\n+        static_cast<int8_t>(std::max(min_val, std::min(max_val, in[i])));\n+  }\n+  tensor_utils::PackInt8IntoDenseInt(unpacked_temp.data(), num_elements,\n+                                     /*bit_width=*/4, (int8_t*)out->data.data);\n+  return kTfLiteOk;\n+}\n+\n+TfLiteStatus castFloatToInt2(const float* in, TfLiteTensor* out,\n+                             int num_elements) {\n+  const float min_val = -2.0f;\n+  const float max_val = 1.0f;\n+  std::vector<int8_t> unpacked_temp(num_elements);\n+  for (int i = 0; i < num_elements; ++i) {\n+    unpacked_temp[i] =\n+        static_cast<int8_t>(std::max(min_val, std::min(max_val, in[i])));\n+  }\n+  tensor_utils::PackInt8IntoDenseInt(unpacked_temp.data(), num_elements,\n+                                     /*bit_width=*/2, (int8_t*)out->data.data);\n+  return kTfLiteOk;\n+}\n+\n template <typename FromT>\n TfLiteStatus copyToTensor(TfLiteContext* context, const FromT* in,\n                           TfLiteTensor* out, int num_elements) {\n@@ -286,6 +330,20 @@ TfLiteStatus copyToTensor(TfLiteContext* context, const FromT* in,\n       copyCast(in, reinterpret_cast<std::complex<float>*>(out->data.c64),\n                num_elements);\n       break;\n+    case kTfLiteInt4:\n+      if (std::is_same<FromT, float>::value) {\n+        return castFloatToInt4(reinterpret_cast<const float*>(in), out,\n+                               num_elements);\n+      } else {\n+        TF_LITE_UNSUPPORTED_TYPE(context, out->type, \"Cast\");\n+      }\n+    case kTfLiteInt2:\n+      if (std::is_same<FromT, float>::value) {\n+        return castFloatToInt2(reinterpret_cast<const float*>(in), out,\n+                               num_elements);\n+      } else {\n+        TF_LITE_UNSUPPORTED_TYPE(context, out->type, \"Cast\");\n+      }\n     default:\n       // Unsupported type.\n       TF_LITE_UNSUPPORTED_TYPE(context, out->type, \"Cast\");\n@@ -334,6 +392,11 @@ TfLiteStatus EvalImpl(TfLiteContext* context, const TfLiteTensor* input,\n         TF_LITE_UNSUPPORTED_TYPE(context, output->type, \"Cast\");\n       }\n       return castInt4ToFloat(context, input, output, num_elements);\n+    case kTfLiteInt2:\n+      if (output->type != kTfLiteFloat32) {\n+        TF_LITE_UNSUPPORTED_TYPE(context, output->type, \"Cast\");\n+      }\n+      return castInt2ToFloat(context, input, output, num_elements);\n     default:\n       // Unsupported type.\n       TF_LITE_UNSUPPORTED_TYPE(context, input->type, \"Cast\");"
        },
        {
            "sha": "77cc2f3442b1c2300e876734b0981f82981cb175",
            "filename": "tensorflow/lite/kernels/cast_test.cc",
            "status": "modified",
            "additions": 86,
            "deletions": 5,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fcast_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fcast_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fcast_test.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -17,16 +17,18 @@ limitations under the License.\n #include <algorithm>\n #include <complex>\n #include <limits>\n-#include <random>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/random/random.h\"\n #include \"absl/types/span.h\"\n #include \"Eigen/Core\"  // from @eigen_archive\n #include \"tensorflow/lite/c/common.h\"\n #include \"tensorflow/lite/core/c/c_api_types.h\"\n #include \"tensorflow/lite/kernels/cast_test_common.h\"\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n+#include \"tensorflow/lite/kernels/kernel_util.h\"\n #include \"tensorflow/lite/kernels/test_util.h\"\n #include \"tensorflow/lite/schema/schema_generated.h\"\n \n@@ -45,10 +47,10 @@ TEST(CastOpModel, CastInt4ToFloat) {\n \n TEST(CastOpModel, CastInt4ToFloatLarge) {\n   int num_elements = 40;\n-  std::random_device random_device;\n-  auto rng = std::mt19937(random_device());\n-  std::uniform_int_distribution<int8_t> i8dist(-8, 7);\n-  auto i8rng = [&] { return i8dist(rng); };\n+  absl::BitGen bitgen;\n+  auto i8rng = [&] {\n+    return absl::Uniform<int8_t>(absl::IntervalClosed, bitgen, -8, 7);\n+  };\n   std::vector<int8_t> input(num_elements);\n   std::generate(input.begin(), input.end(), i8rng);\n   CastOpModel m({TensorType_INT4, {num_elements}},\n@@ -60,6 +62,85 @@ TEST(CastOpModel, CastInt4ToFloatLarge) {\n   }\n }\n \n+TEST(CastOpModel, CastInt2ToFloat) {\n+  CastOpModel m({TensorType_INT2, {2, 4}}, {TensorType_FLOAT32, {2, 4}});\n+  m.Set2BitInput({1, 0, -1, -2, 1, 0, -1, -2});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  EXPECT_THAT(m.ExtractVector<float>(m.output()),\n+              Pointwise(FloatingPointEq(),\n+                        {1.f, 0.f, -1.f, -2.f, 1.f, 0.f, -1.f, -2.f}));\n+}\n+\n+TEST(CastOpModel, CastInt2ToFloatLarge) {\n+  int num_elements = 40;\n+  absl::BitGen bitgen;\n+  auto i2rng = [&] {\n+    return absl::Uniform<int8_t>(absl::IntervalClosed, bitgen, -2, 1);\n+  };\n+  std::vector<int8_t> input(num_elements);\n+  std::generate(input.begin(), input.end(), i2rng);\n+  CastOpModel m({TensorType_INT2, {num_elements}},\n+                {TensorType_FLOAT32, {num_elements}});\n+  m.Set2BitInput(input);\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  for (int i = 0; i < input.size(); ++i) {\n+    EXPECT_EQ(m.ExtractVector<float>(m.output())[i], input[i]);\n+  }\n+}\n+\n+TEST(CastOpModel, CastFloatToInt4) {\n+  CastOpModel m({TensorType_FLOAT32, {2, 4}}, {TensorType_INT4, {2, 4}});\n+  m.PopulateTensor<float>(m.input(), {1.f, 2.f, 3.f, 4.f, 5.f, 6.f, 7.f, -8.f});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  TfLiteTensor* output = m.GetOutputTensor(0);\n+  int num_elements = NumElements(output);\n+  std::vector<int8_t> unpacked_output(num_elements);\n+  tensor_utils::UnpackPackedIntToInt8(\n+      reinterpret_cast<int8_t*>(output->data.data), num_elements,\n+      /*bit_width=*/4, unpacked_output.data());\n+  EXPECT_THAT(unpacked_output, ElementsAreArray({1, 2, 3, 4, 5, 6, 7, -8}));\n+}\n+\n+TEST(CastOpModel, CastFloatToInt4Clamp) {\n+  CastOpModel m({TensorType_FLOAT32, {1, 4}}, {TensorType_INT4, {1, 4}});\n+  m.PopulateTensor<float>(m.input(), {100.f, -100.f, 7.9f, -8.9f});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  TfLiteTensor* output = m.GetOutputTensor(0);\n+  int num_elements = NumElements(output);\n+  std::vector<int8_t> unpacked_output(num_elements);\n+  tensor_utils::UnpackPackedIntToInt8(\n+      reinterpret_cast<int8_t*>(output->data.data), num_elements,\n+      /*bit_width=*/4, unpacked_output.data());\n+  EXPECT_THAT(unpacked_output, ElementsAreArray({7, -8, 7, -8}));\n+}\n+\n+TEST(CastOpModel, CastFloatToInt2) {\n+  CastOpModel m({TensorType_FLOAT32, {2, 4}}, {TensorType_INT2, {2, 4}});\n+  m.PopulateTensor<float>(m.input(),\n+                          {1.f, 0.f, -1.f, -2.f, 1.f, 0.f, -1.f, -2.f});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  TfLiteTensor* output = m.GetOutputTensor(0);\n+  int num_elements = NumElements(output);\n+  std::vector<int8_t> unpacked_output(num_elements);\n+  tensor_utils::UnpackPackedIntToInt8(\n+      reinterpret_cast<int8_t*>(output->data.data), num_elements,\n+      /*bit_width=*/2, unpacked_output.data());\n+  EXPECT_THAT(unpacked_output, ElementsAreArray({1, 0, -1, -2, 1, 0, -1, -2}));\n+}\n+\n+TEST(CastOpModel, CastFloatToInt2Clamp) {\n+  CastOpModel m({TensorType_FLOAT32, {1, 4}}, {TensorType_INT2, {1, 4}});\n+  m.PopulateTensor<float>(m.input(), {100.f, -100.f, 1.9f, -2.9f});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  TfLiteTensor* output = m.GetOutputTensor(0);\n+  int num_elements = NumElements(output);\n+  std::vector<int8_t> unpacked_output(num_elements);\n+  tensor_utils::UnpackPackedIntToInt8(\n+      reinterpret_cast<int8_t*>(output->data.data), num_elements,\n+      /*bit_width=*/2, unpacked_output.data());\n+  EXPECT_THAT(unpacked_output, ElementsAreArray({1, -2, 1, -2}));\n+}\n+\n TEST(CastOpModel, CastFloatToUint8Infinity) {\n   CastOpModel m({TensorType_FLOAT32, {2}}, {TensorType_UINT8, {2}});\n   m.PopulateTensor<float>(m.input(), {std::numeric_limits<float>::infinity(),"
        },
        {
            "sha": "c0802ca68e15294f20938002eb541e3f6fac91c8",
            "filename": "tensorflow/lite/kernels/cast_test_common.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fcast_test_common.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fcast_test_common.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fcast_test_common.h?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -59,6 +59,10 @@ class CastOpModel : public SingleOpModel {\n     PopulateTensor4bit(input_, 0, f.data(), f.data() + f.size());\n   }\n \n+  void Set2BitInput(absl::Span<const int8_t> data) {\n+    PopulateTensor2bit(input_, 0, data.data(), data.data() + data.size());\n+  }\n+\n   int input() const { return input_; }\n   int output() const { return output_; }\n "
        },
        {
            "sha": "f57c3adf5d7cd1f7f25ed9073432508730cf82fd",
            "filename": "tensorflow/lite/kernels/register_ref.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -377,7 +377,7 @@ BuiltinRefOpResolver::BuiltinRefOpResolver() {\n              /* max_version = */ 2);\n   AddBuiltin(BuiltinOperator_CAST, Register_CAST(),\n              /* min_version = */ 1,\n-             /* max_version = */ 7);\n+             /* max_version = */ 8);\n   AddBuiltin(BuiltinOperator_DEQUANTIZE, Register_DEQUANTIZE_REF(),\n              /* min_version = */ 1,\n              /* max_version = */ 6);"
        },
        {
            "sha": "a15807600ea7bd8cec24501572ad49ad70a79b2a",
            "filename": "tensorflow/lite/kernels/test_util.h",
            "status": "modified",
            "additions": 51,
            "deletions": 24,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Ftest_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f67cb87691dfe2bf63f292948b30536e086b6277/tensorflow%2Flite%2Fkernels%2Ftest_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ftest_util.h?ref=f67cb87691dfe2bf63f292948b30536e086b6277",
            "patch": "@@ -33,22 +33,22 @@ limitations under the License.\n #include <ostream>\n #include <string>\n #include <tuple>\n-#include <type_traits>\n #include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"fp16/fp16.h\"  // from @FP16\n #include \"absl/algorithm/container.h\"\n+#include \"absl/log/absl_check.h\"\n+#include \"absl/log/absl_log.h\"\n #include \"absl/types/span.h\"\n #include \"Eigen/Core\"  // from @eigen_archive\n-#include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\n #include \"tensorflow/lite/core/api/op_resolver.h\"\n #include \"tensorflow/lite/core/c/common.h\"\n #include \"tensorflow/lite/core/interpreter.h\"\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n-#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/utils/sparsity_format_converter.h\"\n #include \"tensorflow/lite/kernels/kernel_util.h\"\n #include \"tensorflow/lite/portable_type_to_tflitetype.h\"\n@@ -57,7 +57,6 @@ limitations under the License.\n #include \"tensorflow/lite/string_util.h\"\n #include \"tensorflow/lite/testing/util.h\"  // IWYU pragma: keep\n #include \"tensorflow/lite/tools/optimize/quantization_utils.h\"\n-#include \"tensorflow/lite/type_to_tflitetype.h\"\n #include \"tensorflow/lite/util.h\"\n #include \"tsl/platform/logging.h\"\n \n@@ -489,14 +488,14 @@ class SingleOpModel {\n             reinterpret_cast<const uint8_t*>(q.data()), q.size());\n         buffers_.push_back(CreateBuffer(builder_, data_buffer));\n       } else if (is_quantized) {\n-        CHECK_EQ(t.type, TensorType_INT8)\n+        ABSL_CHECK_EQ(t.type, TensorType_INT8)\n             << \"The INT8 quantization is only supported for sparsified tensor\";\n         std::vector<int8_t> quantized_output(sparse_data.size());\n         std::vector<float> scales;\n         std::vector<int64_t> zero_points;\n         if (t.per_channel_quantization) {\n-          CHECK_EQ(t.per_channel_quantization_scales.size(),  // NOLINT\n-                   t.per_channel_quantization_offsets.size())\n+          ABSL_CHECK_EQ(t.per_channel_quantization_scales.size(),  // NOLINT\n+                        t.per_channel_quantization_offsets.size())\n               << \"Per channel quantization scales and offsets should have the \"\n                  \"same size\";\n           std::vector<int8_t> temp_data(dense_data.size());\n@@ -703,7 +702,7 @@ class SingleOpModel {\n     TfLiteTensor* t = interpreter_->tensor(index);\n     auto* params =\n         reinterpret_cast<TfLiteAffineQuantization*>(t->quantization.params);\n-    CHECK(t->type == kTfLiteInt32 || t->type == kTfLiteInt64);\n+    ABSL_CHECK(t->type == kTfLiteInt32 || t->type == kTfLiteInt64);\n     if (t->type == kTfLiteInt32) {\n       PerChannelQuantizeBiasPopulateTensor<int32_t>(index, input_data, params);\n     } else {\n@@ -783,7 +782,7 @@ class SingleOpModel {\n   std::vector<T> ExtractVector(int index) const {\n     const T* v = interpreter_->typed_tensor<T>(index);\n     const auto* tensor = interpreter_->tensor(index);\n-    CHECK(v) << \"Could not extract vector at index: \" << index;\n+    ABSL_CHECK(v) << \"Could not extract vector at index: \" << index;\n     int tensor_size;\n     if (tensor->sparsity) {\n       // Getting the size of the sparse buffer this way is based on the\n@@ -815,7 +814,7 @@ class SingleOpModel {\n   // Sets the number of threads available to the interpreter.\n   // Reconstruct the interpreter if reset_interpreter is true.\n   void SetNumThreads(int num_threads, bool reset_interpreter = false) {\n-    CHECK(interpreter_ != nullptr);\n+    ABSL_CHECK(interpreter_ != nullptr);\n     if (reset_interpreter) {\n       // Reconstruct interpreter as number of threads may affect internal\n       // state, e.g. stratch buffer allocation.\n@@ -890,7 +889,7 @@ class SingleOpModel {\n           std::tie(t.scale, t.zero_point) =\n               QuantizationParams<int8_t>(t.min, t.max, kTfLiteInt4);\n         } else {\n-          LOG(FATAL) << \"No support for the requested quantized type\";\n+          ABSL_LOG(FATAL) << \"No support for the requested quantized type\";\n         }\n         t.min = 0;\n         t.max = 0;\n@@ -949,12 +948,12 @@ class SingleOpModel {\n     const float qmax_double = qmax;\n     // 0 should always be a representable value. Let's assume that the initial\n     // min,max range contains 0.\n-    CHECK_LE(f_min, 0);\n-    CHECK_GE(f_max, 0);\n+    ABSL_CHECK_LE(f_min, 0);\n+    ABSL_CHECK_GE(f_max, 0);\n     if (f_min == f_max) {\n       // Special case where the min,max range is a point. Should be {0}.\n-      CHECK_EQ(f_min, 0);\n-      CHECK_EQ(f_max, 0);\n+      ABSL_CHECK_EQ(f_min, 0);\n+      ABSL_CHECK_EQ(f_max, 0);\n       return {scale, zero_point};\n     }\n \n@@ -1003,8 +1002,8 @@ class SingleOpModel {\n \n     // The zero point should always be in the range of quantized value,\n     // // [qmin, qmax].\n-    CHECK_GE(nudged_zero_point, qmin);\n-    CHECK_LE(nudged_zero_point, qmax);\n+    ABSL_CHECK_GE(nudged_zero_point, qmin);\n+    ABSL_CHECK_LE(nudged_zero_point, qmax);\n \n     zero_point = nudged_zero_point;\n     // finally, return the values\n@@ -1028,29 +1027,57 @@ class SingleOpModel {\n \n     if (!v) {\n       auto* t = interpreter_->tensor(index);\n-      CHECK(t) << \"No tensor with index \" << index << \".\";\n-      CHECK(t->data.raw) << \"Empty data for tensor with index \" << index << \".\";\n-      LOG(FATAL) << \"Unknown tensor error.\";\n+      ABSL_CHECK(t) << \"No tensor with index \" << index << \".\";\n+      ABSL_CHECK(t->data.raw)\n+          << \"Empty data for tensor with index \" << index << \".\";\n+      ABSL_LOG(FATAL) << \"Unknown tensor error.\";\n     }\n     absl::c_copy(data, v + offset);\n     PackInt4ValuesDenselyInPlace(v, ElementCount(*tensor_ptr->dims));\n     tensor_ptr->bytes = ((ElementCount(*tensor_ptr->dims) + 1) / 2);\n   }\n \n+  // Partially populates the tensor, starting at the given offset.\n+  void PopulateTensor2bit(int index, int offset, const int8_t* begin,\n+                          const int8_t* end) {\n+    auto data = absl::Span<const int8_t>(begin, end - begin);\n+    TfLiteTensor* tensor_ptr = interpreter_->tensor(index);\n+    uint8_t* v = nullptr;\n+    if (tensor_ptr) {\n+      v = reinterpret_cast<uint8_t*>(tensor_ptr->data.data);\n+    }\n+\n+    if (!v) {\n+      auto* t = interpreter_->tensor(index);\n+      ABSL_CHECK(t) << \"No tensor with index \" << index << \".\";\n+      ABSL_CHECK(t->data.raw)\n+          << \"Empty data for tensor with index \" << index << \".\";\n+      ABSL_LOG(FATAL) << \"Unknown tensor error.\";\n+    }\n+    int num_elements = data.size();\n+    int num_bytes = (num_elements + 3) / 4;\n+    std::vector<int8_t> packed(num_bytes);\n+    tensor_utils::PackInt8IntoDenseInt(data.data(), num_elements,\n+                                       /*bit_width=*/2, packed.data());\n+    memcpy(v + offset, packed.data(), packed.size());\n+    tensor_ptr->bytes = num_bytes;\n+  }\n+\n  private:\n   // Populates the tensor starting at offset using given data.\n   template <typename T, typename Container>\n   void PopulateTensorImpl(int index, int offset, const Container& data) {\n     T* v = interpreter_->typed_tensor<T>(index);\n     if (!v) {\n       auto* t = interpreter_->tensor(index);\n-      CHECK(t) << \"No tensor with index \" << index << \".\";\n-      CHECK(t->data.raw) << \"Empty data for tensor with index \" << index << \".\";\n-      CHECK_EQ(t->type, typeToTfLiteType<T>())\n+      ABSL_CHECK(t) << \"No tensor with index \" << index << \".\";\n+      ABSL_CHECK(t->data.raw)\n+          << \"Empty data for tensor with index \" << index << \".\";\n+      ABSL_CHECK_EQ(t->type, typeToTfLiteType<T>())\n           << \"Type mismatch for tensor with index \" << index << \". Requested \"\n           << TfLiteTypeGetName(typeToTfLiteType<T>()) << \", got \"\n           << TfLiteTypeGetName(t->type) << \".\";\n-      LOG(FATAL) << \"Unknown tensor error.\";\n+      ABSL_LOG(FATAL) << \"Unknown tensor error.\";\n     }\n     absl::c_copy(data, v + offset);\n   }"
        }
    ],
    "stats": {
        "total": 335,
        "additions": 293,
        "deletions": 42
    }
}