{
    "author": "tensorflower-gardener",
    "message": "[TFRT]Update `AddTask` call and add a condition for batch scheduling.\n\n- The task time is not set when padding low priority tasks in PadOpenBatchWithLowPriorityTasks, causing the batch earliest task start time to be 0. This change adds the task start time to the AddTask call.\n- The old batch can be empty when the mixed priority policy is PriorityMerge, causing bad_optional_access error when trying to get the batch earliest task start time. This change make sure that the old batch is not empty before proceeding.\n- Add an empty batch check to MaybeBatchDown. Without the check, MaybeBatchDown would crash on DCHECK_GT(batch_size, 0).\n- Moving the definitions of padding_size_v2 and processed_batch_size_v2 metrics out of the two record functions, which allows unit tests to verify the correct behavior of batching.\n\nPiperOrigin-RevId: 821816706",
    "sha": "03819ab09c5df5712c3332b0c4f15d356f3b0a8c",
    "files": [
        {
            "sha": "ef391875ff39513a876dcf908b6d98c98961bd40",
            "filename": "tensorflow/core/kernels/batching_util/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2FBUILD?ref=03819ab09c5df5712c3332b0c4f15d356f3b0a8c",
            "patch": "@@ -521,13 +521,18 @@ tf_cc_test(\n         \"//tensorflow/core/kernels:batch_kernels\",\n         \"//tensorflow/core/lib/monitoring:cell_reader\",\n         \"//tensorflow/core/platform:notification\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/platform:refcount\",\n         \"@local_tsl//tsl/platform:status\",\n+        \"@local_xla//xla/tsl/lib/monitoring:cell_reader\",\n+        \"@local_xla//xla/tsl/lib/monitoring:test_utils\",\n         \"@local_xla//xla/tsl/platform:criticality\",\n     ],\n )"
        },
        {
            "sha": "10edb6660a70af82d57b167b3766d46ab33897a1",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 15,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc?ref=03819ab09c5df5712c3332b0c4f15d356f3b0a8c",
            "patch": "@@ -95,8 +95,7 @@ void RecordPaddingSize(int32_t padding_size, const string& model_name,\n       ->Add(static_cast<double>(padding_size));\n }\n \n-void RecordPaddingSizeV2(int32_t padding_size, const string& model_name,\n-                         int32_t execution_batch_size, const string& op_name) {\n+std::vector<double> GetBucketLimitsForPaddingSizeV2() {\n   // Bucket containing 0 has bounds [-2/3, 2/3).\n   // Remaining buckets are centered at powers of 2 and have bounds:\n   // [(2/3) * 2^i, (4/3) * 2^i) for i = 1, ..., 13.\n@@ -112,14 +111,20 @@ void RecordPaddingSizeV2(int32_t padding_size, const string& model_name,\n     bucket_limits.push_back(bound);\n     bound *= growth_factor;\n   }\n+  return bucket_limits;\n+}\n \n-  static auto* cell = tensorflow::monitoring::Sampler<3>::New(\n-      {\"/tensorflow/serving/batching/padding_size_v2\",\n-       \"Tracks the padding size distribution on batches by model_name (if \"\n-       \"available).\",\n-       \"model_name\", \"execution_batch_size\", \"op_name\"},\n-      monitoring::Buckets::Explicit(bucket_limits));\n-  cell->GetCell(model_name, absl::StrCat(execution_batch_size), op_name)\n+static auto* padding_size_v2_sampler = tensorflow::monitoring::Sampler<3>::New(\n+    {\"/tensorflow/serving/batching/padding_size_v2\",\n+     \"Tracks the padding size distribution on batches by model_name (if \"\n+     \"available).\",\n+     \"model_name\", \"execution_batch_size\", \"op_name\"},\n+    monitoring::Buckets::Explicit(GetBucketLimitsForPaddingSizeV2()));\n+\n+void RecordPaddingSizeV2(int32_t padding_size, const string& model_name,\n+                         int32_t execution_batch_size, const string& op_name) {\n+  padding_size_v2_sampler\n+      ->GetCell(model_name, absl::StrCat(execution_batch_size), op_name)\n       ->Add(static_cast<double>(padding_size));\n }\n \n@@ -184,15 +189,17 @@ void RecordProcessedBatchSize(int32_t batch_size, const string& model_name,\n   cell->GetCell(model_name, op_name)->Add(static_cast<double>(batch_size));\n }\n \n+static auto* processed_batch_size_v2_counter = monitoring::Counter<3>::New(\n+    \"/tensorflow/serving/batching/processed_batch_size_v2\",\n+    \"Tracks the batch size on processing by model_name and op name (if \"\n+    \"available).\",\n+    \"model_name\", \"op_name\", \"batch_size\");\n+\n // Export the exact number instead of the distribution of processed batch size.\n void RecordProcessedBatchSizeV2(int32_t batch_size, const string& model_name,\n                                 const string& op_name) {\n-  static auto* cell = monitoring::Counter<3>::New(\n-      \"/tensorflow/serving/batching/processed_batch_size_v2\",\n-      \"Tracks the batch size on processing by model_name and op name (if \"\n-      \"available).\",\n-      \"model_name\", \"op_name\", \"batch_size\");\n-  cell->GetCell(model_name, op_name, std::to_string(batch_size))\n+  processed_batch_size_v2_counter\n+      ->GetCell(model_name, op_name, std::to_string(batch_size))\n       ->IncrementBy(1);\n }\n "
        },
        {
            "sha": "28d997426827ba0196e2f4fd375acf262b9c675e",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base_test.cc",
            "status": "modified",
            "additions": 250,
            "deletions": 0,
            "changes": 250,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc?ref=03819ab09c5df5712c3332b0c4f15d356f3b0a8c",
            "patch": "@@ -23,12 +23,18 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/blocking_counter.h\"\n #include \"absl/synchronization/notification.h\"\n #include \"absl/time/clock.h\"\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/tsl/lib/monitoring/cell_reader.h\"\n+#include \"xla/tsl/lib/monitoring/test_utils.h\"\n #include \"xla/tsl/platform/criticality.h\"\n #include \"tensorflow/core/common_runtime/cost_constants.h\"\n #include \"tensorflow/core/common_runtime/cost_measurement.h\"\n@@ -51,12 +57,15 @@ limitations under the License.\n #include \"tensorflow/core/platform/notification.h\"\n #include \"tensorflow/core/public/session_options.h\"\n #include \"tensorflow/core/public/version.h\"\n+#include \"tsl/platform/refcount.h\"\n #include \"tsl/platform/status.h\"\n \n namespace tensorflow {\n namespace serving {\n namespace {\n \n+using ::tensorflow::monitoring::testing::CellReader;\n+using ::tensorflow::monitoring::testing::Histogram;\n using ::testing::Pair;\n using ::testing::UnorderedElementsAre;\n \n@@ -65,6 +74,107 @@ TEST(BatchTaskCriticalityTest, CriticalityDefaultsToCritical) {\n   EXPECT_EQ(batch_task.criticality(), tsl::criticality::Criticality::kCritical);\n }\n \n+struct PriorityTestParams {\n+  std::string test_name;\n+  MixedPriorityBatchingPolicy mixed_priority_batching_policy;\n+  // The expected number of batches for each allowed batch size.\n+  absl::flat_hash_map<int, int> expected_batch_size_count;\n+  // The expected sum of padding sizes for each allowed batch size.\n+  absl::flat_hash_map<int, int> expected_batch_size_padding_sum;\n+};\n+\n+class TestPriorityBatchResourceBase : public BatchResourceBase {\n+ public:\n+  using BatchResourceBase::BatchResourceBase;\n+\n+  std::string DebugString() const override {\n+    return \"TestPriorityBatchResourceBase\";\n+  }\n+\n+ protected:\n+  // Simple function that returns the input tensors as the output tensors.\n+  void ProcessFuncBatchImpl(\n+      const BatchResourceBase::BatchTask& last_task,\n+      absl::Span<const Tensor> inputs, std::vector<Tensor>* combined_outputs,\n+      std::function<void(const absl::Status&)> done) const override {\n+    for (const auto& input : inputs) {\n+      combined_outputs->push_back(input);\n+    }\n+    done(absl::OkStatus());\n+  }\n+};\n+\n+class BatchResourceBaseWithPriorityTest\n+    : public ::testing::TestWithParam<PriorityTestParams> {\n+ protected:\n+  void SetUp() override {\n+    processed_batch_size_v2_reader_ = std::make_unique<CellReader<int64_t>>(\n+        \"/tensorflow/serving/batching/processed_batch_size_v2\");\n+    padding_size_v2_reader_ = std::make_unique<CellReader<Histogram>>(\n+        \"/tensorflow/serving/batching/padding_size_v2\");\n+    // Create device_.\n+    device_ = DeviceFactory::NewDevice(\"CPU\", SessionOptions{},\n+                                       \"/job:a/replica:0/task:0\");\n+    // Create batch_kernel_node_def.\n+    NodeDefBuilder batch_function_builder(\"my_batch_node\", \"BatchFunction\");\n+    batch_function_builder.Attr(\"max_batch_size\", 16);\n+    batch_function_builder.Attr(\"num_batch_threads\", 6);\n+    batch_function_builder.Attr(\"allowed_batch_sizes\", {4, 8, 12, 16});\n+    batch_function_builder.Attr(\"batch_timeout_micros\", 3000000);\n+    batch_function_builder.Attr(\"max_enqueued_batches\", 6);\n+    batch_function_builder.Attr(\"enable_large_batch_splitting\", true);\n+    batch_function_builder.Attr(\"Tin\", {DataType::DT_INT64});\n+    batch_function_builder.Input(std::vector<NodeDefBuilder::NodeOut>{\n+        NodeDefBuilder::NodeOut({\"n1\", 0, DataType::DT_INT64})});\n+    batch_function_builder.Attr(\"Tcaptured\", std::vector<DataType>{});\n+    batch_function_builder.Input(std::vector<NodeDefBuilder::NodeOut>{});\n+    batch_function_builder.Attr(\"Tout\", {DataType::DT_INT64});\n+    NameAttrList f;\n+    f.set_name(\"func_to_batch\");\n+    batch_function_builder.Attr(\"f\", f);\n+    NodeDef batch_kernel_node_def;\n+    CHECK_OK(batch_function_builder.Finalize(&batch_kernel_node_def));\n+\n+    // Create batch_kernel_.\n+    absl::Status op_kernel_creation_status;\n+    batch_kernel_ =\n+        CreateOpKernel(DEVICE_CPU, device_.get(), device_->GetAllocator({}),\n+                       batch_kernel_node_def, TF_GRAPH_DEF_VERSION,\n+                       &op_kernel_creation_status);\n+    CHECK_OK(op_kernel_creation_status);\n+    CHECK(batch_kernel_ != nullptr);\n+\n+    // Create input tensors.\n+    input_tensor_ = Tensor(DataType::DT_INT64, TensorShape({3, 4}));\n+    input_tensor_.flat<int64_t>().setZero();\n+    input_tensor_values_ = {\n+        TensorValue(&input_tensor_),\n+    };\n+\n+    // Fill-in session_metadata_.\n+    session_metadata_.set_name(\"my_model_name\");\n+\n+    // Fill-in params_.\n+    params_.device = device_.get();\n+    params_.op_kernel = batch_kernel_.get();\n+    params_.inputs = input_tensor_values_;\n+    params_.session_metadata = &session_metadata_;\n+\n+    // Create context_.\n+    context_ = std::make_unique<OpKernelContext>(&params_);\n+  }\n+\n+  std::unique_ptr<CellReader<int64_t>> processed_batch_size_v2_reader_;\n+  std::unique_ptr<CellReader<Histogram>> padding_size_v2_reader_;\n+  std::unique_ptr<Device> device_;\n+  std::unique_ptr<OpKernel> batch_kernel_;\n+  Tensor input_tensor_;\n+  std::vector<TensorValue> input_tensor_values_;\n+  SessionMetadata session_metadata_;\n+  OpKernelContext::Params params_;\n+  std::unique_ptr<OpKernelContext> context_;\n+};\n+\n #if defined(PLATFORM_GOOGLE)\n TEST(BatchTaskCriticalityTest, CriticalitySuccessfullyPropagated) {\n   std::vector<BatchResourceBase::BatchTask> batch_tasks;\n@@ -110,6 +220,146 @@ TEST(BatchTaskCriticalityTest, CriticalitySuccessfullyPropagated) {\n   EXPECT_EQ(batch_tasks[4].criticality(),\n             tsl::criticality::Criticality::kCritical);\n }\n+\n+TEST_P(BatchResourceBaseWithPriorityTest, BatchingWithMixedPriorityPolicy) {\n+  std::shared_ptr<SharedBatchScheduler<BatchResourceBase::BatchTask>> batcher;\n+  ASSERT_OK(SharedBatchScheduler<BatchResourceBase::BatchTask>::Create(\n+      SharedBatchScheduler<BatchResourceBase::BatchTask>::Options(), &batcher));\n+  std::vector<int32_t> allowed_batch_sizes = {4, 8, 12, 16};\n+  int max_batch_size = 16;\n+  int64_t batch_timeout = absl::ToInt64Microseconds(absl::Seconds(3));\n+  int num_requests = 6;\n+  // Make the low priority batch timeout longer than the high priority batch\n+  // so the low priority tasks can be padded to the high priority batch instead\n+  // of forming a separate batch.\n+  BatchResourceBase::BatcherT::QueueOptions queue_options =\n+      TestPriorityBatchResourceBase::GetBatcherQueueOptions(\n+          /*num_batch_threads=*/num_requests, /*max_batch_size=*/max_batch_size,\n+          /*batch_timeout_micros=*/batch_timeout,\n+          /*max_enqueued_batches=*/num_requests, allowed_batch_sizes,\n+          /*enable_large_batch_splitting=*/true,\n+          /*disable_padding=*/false, kPadUpPolicy,\n+          /*low_priority_max_batch_size=*/max_batch_size,\n+          /*low_priority_batch_timeout_micros=*/batch_timeout * 3,\n+          /*low_priority_max_enqueued_batches=*/num_requests,\n+          /*low_priority_allowed_batch_sizes=*/allowed_batch_sizes,\n+          /*mixed_priority_batching_policy=*/\n+          GetParam().mixed_priority_batching_policy);\n+  tsl::core::RefCountPtr<BatchResourceBase> batch_resource(\n+      new TestPriorityBatchResourceBase(true, batcher, queue_options,\n+                                        allowed_batch_sizes));\n+\n+  std::vector<std::unique_ptr<OpKernelContext>> contexts;\n+  for (int i = 0; i < num_requests; ++i) {\n+    contexts.push_back(std::make_unique<OpKernelContext>(&params_));\n+  }\n+\n+  absl::BlockingCounter blocking_counter(num_requests);\n+  for (int i = 0; i < num_requests; ++i) {\n+    auto create_batch_task_fn = [&]() {\n+      // The first 3 requests are assigned with the default high priority, while\n+      // the last 3 requests are set to low priority.\n+      std::unique_ptr<BatchResourceBase::BatchTask> batch_task;\n+      if (i >= 3) {\n+        tsl::criticality::ScopedCriticality scoped_criticality(\n+            tsl::criticality::Criticality::kSheddable);\n+        batch_task = std::make_unique<BatchResourceBase::BatchTask>();\n+      } else {\n+        batch_task = std::make_unique<BatchResourceBase::BatchTask>();\n+      }\n+      return batch_task;\n+    };\n+    auto done_callback = [&]() { blocking_counter.DecrementCount(); };\n+    ASSERT_OK(batch_resource->RegisterInput(\n+        /*guid=*/i, contexts[i].get(),\n+        /*batcher_queue_name=*/\"batcher_queue_name\",\n+        /*create_batch_task_fn=*/create_batch_task_fn,\n+        /*done_callback=*/done_callback,\n+        /*forced_warmup_batch_size=*/0));\n+  }\n+  blocking_counter.Wait();\n+\n+  for (const auto& [batch_size, expected_count] :\n+       GetParam().expected_batch_size_count) {\n+    EXPECT_EQ(processed_batch_size_v2_reader_->Delta(\n+                  \"my_model_name\", \"my_batch_node\", absl::StrCat(batch_size)),\n+              expected_count);\n+  }\n+  for (const auto& [batch_size, expected_padding_sum] :\n+       GetParam().expected_batch_size_padding_sum) {\n+    EXPECT_EQ(\n+        padding_size_v2_reader_\n+            ->Delta(\"my_model_name\", absl::StrCat(batch_size), \"my_batch_node\")\n+            .sum(),\n+        expected_padding_sum);\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    BatchResourceBaseWithPriorityTests, BatchResourceBaseWithPriorityTest,\n+    ::testing::ValuesIn<PriorityTestParams>({\n+        // allowed_batch_sizes = {4, 8, 12, 16}.\n+        // 6 requests in total and each request has task size 3.\n+        // 3 requests with high priority and 3 requests with low priority.\n+        // With priority_isolation policy, the high priority tasks and low\n+        // priority tasks are batched separately. There are 2 batches. Each one\n+        // has 3 tasks and total size is 12. Each batch has 3 paddings.\n+        {\n+            \"priority_isolation\",\n+            MixedPriorityBatchingPolicy::kPriorityIsolation,\n+            /*expected_batch_size_count=*/\n+            {{4, 0}, {8, 0}, {12, 2}, {16, 0}},\n+            /*expected_batch_size_padding_sum=*/\n+            {{4, 0}, {8, 0}, {12, 6}, {16, 0}},\n+        },\n+        // With priority_merge policy, high priority tasks and low priority\n+        // tasks are batched together. The total size of all tasks is 18 which\n+        // exceeds the max batch size 16. The last low priority task is split\n+        // into two tasks of size 1 and size 2. There are 2 batches. First batch\n+        // has 6 tasks and total size is 16. No padding for the first batch. The\n+        // second batch has 1 task of size 2 and is padded to size 4.\n+        {\n+            \"priority_merge\",\n+            MixedPriorityBatchingPolicy::kPriorityMerge,\n+            /*expected_batch_size_count=*/\n+            {{4, 1}, {8, 0}, {12, 0}, {16, 1}},\n+            /*expected_batch_size_padding_sum=*/\n+            {{4, 2}, {8, 0}, {12, 0}, {16, 0}},\n+        },\n+        // With padding_with_max_batch_size policy, high priority tasks and low\n+        // priority tasks are batched to the max batch size and there is no\n+        // splitting for low priority tasks. 3 high priority tasks and 2 low\n+        // priority tasks are batched together. The first batch has total size\n+        // of 15 and is padded to size 16. The second batch has 1 low priority\n+        // task of size 3 and is padded to size 4.\n+        {\n+            \"padding_with_max_batch_size\",\n+            MixedPriorityBatchingPolicy::kLowPriorityPaddingWithMaxBatchSize,\n+            /*expected_batch_size_count=*/\n+            {{4, 1}, {8, 0}, {12, 0}, {16, 1}},\n+            /*expected_batch_size_padding_sum=*/\n+            {{4, 1}, {8, 0}, {12, 0}, {16, 1}},\n+        },\n+        // With padding_with_next_allowed_batch_size policy, high priority tasks\n+        // and low priority tasks are batched to the next allowed batch size. 3\n+        // high priority tasks and 1 low priority tasks are batched together.\n+        // The first batch has total size of 12. No padding for batch 1. The\n+        // second batch has 2 low priority tasks (total size 6) and is padded to\n+        // size 8.\n+        {\n+            \"low_priority_padding_with_next_allowed_batch_size\",\n+            MixedPriorityBatchingPolicy::\n+                kLowPriorityPaddingWithNextAllowedBatchSize,\n+            /*expected_batch_size_count=*/\n+            {{4, 0}, {8, 1}, {12, 1}, {16, 0}},\n+            /*expected_batch_size_padding_sum=*/\n+            {{4, 0}, {8, 2}, {12, 0}, {16, 0}},\n+        },\n+    }),\n+    [](const ::testing::TestParamInfo<\n+        BatchResourceBaseWithPriorityTest::ParamType>& info) {\n+      return info.param.test_name;\n+    });\n #endif\n \n class TestTpuCostMeasurement : public CostMeasurement {"
        },
        {
            "sha": "7dabf5628c9980316f0e614e854612f0d982b6dd",
            "filename": "tensorflow/core/kernels/batching_util/batch_scheduler_utils.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.h?ref=03819ab09c5df5712c3332b0c4f15d356f3b0a8c",
            "patch": "@@ -74,6 +74,9 @@ void MaybeBatchDown(Batch<TaskType>& batch,\n                     absl::string_view batch_padding_policy,\n                     ModelBatchStats* model_batch_stats,\n                     std::vector<std::unique_ptr<TaskType>>& out_trimmed_tasks) {\n+  if (batch.empty()) {\n+    return;\n+  }\n   if (batch_padding_policy == kPadUpPolicy) {\n     // This is the default behavior of batch resource when it is given a batch\n     // size that doesn't match any of the allowed batch sizes."
        },
        {
            "sha": "91ff8065a1d8a712de0b29d0bdfa671581b8a30a",
            "filename": "tensorflow/core/kernels/batching_util/batch_scheduler_utils_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils_test.cc?ref=03819ab09c5df5712c3332b0c4f15d356f3b0a8c",
            "patch": "@@ -83,6 +83,23 @@ class FakeTask : public BatchTask {\n   const size_t size_;\n };\n \n+TEST(MaybeBatchDownTest, EmptyBatch) {\n+  Batch<FakeTask> batch;\n+  batch.Close();\n+\n+  std::vector<std::unique_ptr<FakeTask>> out_trimmed_tasks;\n+\n+  MaybeBatchDown(\n+      /* batch= */ batch, /* allowed_batch_sizes= */ {1, 2, 4, 8},\n+      /* disable_padding= */ false,\n+      /* batch_padding_policy= */ kBatchDownPolicy,\n+      /* model_batch_stats= */ nullptr,\n+      /* out_trimmed_tasks= */ out_trimmed_tasks);\n+\n+  EXPECT_TRUE(batch.empty());\n+  EXPECT_TRUE(out_trimmed_tasks.empty());\n+}\n+\n TEST(MaybeBatchDownTest, PadUp) {\n   Batch<FakeTask> batch;\n   batch.AddTask(std::make_unique<FakeTask>(1));"
        },
        {
            "sha": "d458cd86a8e7aabeba8d059aefbde8425cca8bdb",
            "filename": "tensorflow/core/kernels/batching_util/shared_batch_scheduler.h",
            "status": "modified",
            "additions": 40,
            "deletions": 37,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03819ab09c5df5712c3332b0c4f15d356f3b0a8c/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler.h?ref=03819ab09c5df5712c3332b0c4f15d356f3b0a8c",
            "patch": "@@ -1024,7 +1024,7 @@ void Queue<TaskType>::PadOpenBatchWithLowPriorityTasks() {\n           tsl::profiler::ContextType::kSharedBatchScheduler,\n           batches.back()->traceme_context_id());\n \n-      batches.back()->AddTask(std::move(output_tasks[i]));\n+      batches.back()->AddTask(std::move(output_tasks[i]), task_time);\n     }\n   }\n }\n@@ -1200,42 +1200,45 @@ Queue<TaskType>::ScheduleBatch() {\n       // starting a new batch because starting a new batch will close the old\n       // batch, making it read-only.\n       Batch<TaskType>& old_batch = *batches[0];\n-      uint64 old_batch_time = old_batch.EarliestTaskStartTime().value();\n-      std::vector<std::unique_ptr<TaskType>> trimmed_tasks;\n-      MaybeBatchDown(\n-          /* batch= */ old_batch,\n-          /* allowed_batch_sizes= */ options_.allowed_batch_sizes,\n-          /* disable_padding= */ options_.disable_padding,\n-          /* batch_padding_policy= */ options_.batch_padding_policy,\n-          /* model_batch_stats= */ options_.model_batch_stats,\n-          /* out_trimmed_tasks= */ trimmed_tasks);\n-\n-      StartNewBatch();\n-\n-      // Move the trimmed tasks, if any, into the new batch.\n-      Batch<TaskType>& new_batch = *batches[1];\n-      for (std::unique_ptr<TaskType>& task : trimmed_tasks) {\n-        new_batch.AddTask(std::move(task), old_batch_time);\n-      }\n-      if (!new_batch.empty()) {\n-        // TODO - b/325954758: Reconsider the starting time of a trimmed batch.\n-        //\n-        // Ideally, we'd set open_batch_start_time_micros_ to time we received\n-        // the first task in the open batch, but we don't have this information\n-        // here. For now, we're trying as alternative solution that doesn't\n-        // require adding time to each task: assume that requests arrived at a\n-        // steady rate and therefore use a point between the old value of\n-        // open_batch_start_time_micros_ and NOW.\n-        //\n-        // Let's say that originally, the batch had 10 requests, and we want to\n-        // schedule a batch of size 8 and leave 2 requests in the open batch\n-        // (new_batch). Then, variable `position` is 0.8, which means we have to\n-        // set open_batch_start_time_micros_ to be at a position of 80% between\n-        // open_batch_start_time_micros_ and now.\n-        double position = static_cast<double>(old_batch.size()) /\n-                          (old_batch.size() + new_batch.size());\n-        open_batch_start_time_micros_ +=\n-            (env_->NowMicros() - open_batch_start_time_micros_) * position;\n+      if (!old_batch.empty()) {\n+        uint64 old_batch_time = old_batch.EarliestTaskStartTime().value();\n+        std::vector<std::unique_ptr<TaskType>> trimmed_tasks;\n+        MaybeBatchDown(\n+            /* batch= */ old_batch,\n+            /* allowed_batch_sizes= */ options_.allowed_batch_sizes,\n+            /* disable_padding= */ options_.disable_padding,\n+            /* batch_padding_policy= */ options_.batch_padding_policy,\n+            /* model_batch_stats= */ options_.model_batch_stats,\n+            /* out_trimmed_tasks= */ trimmed_tasks);\n+\n+        StartNewBatch();\n+\n+        // Move the trimmed tasks, if any, into the new batch.\n+        Batch<TaskType>& new_batch = *batches[1];\n+        for (std::unique_ptr<TaskType>& task : trimmed_tasks) {\n+          new_batch.AddTask(std::move(task), old_batch_time);\n+        }\n+        if (!new_batch.empty()) {\n+          // TODO - b/325954758: Reconsider the starting time of a trimmed\n+          // batch.\n+          //\n+          // Ideally, we'd set open_batch_start_time_micros_ to time we received\n+          // the first task in the open batch, but we don't have this\n+          // information here. For now, we're trying as alternative solution\n+          // that doesn't require adding time to each task: assume that requests\n+          // arrived at a steady rate and therefore use a point between the old\n+          // value of open_batch_start_time_micros_ and NOW.\n+          //\n+          // Let's say that originally, the batch had 10 requests, and we want\n+          // to schedule a batch of size 8 and leave 2 requests in the open\n+          // batch (new_batch). Then, variable `position` is 0.8, which means we\n+          // have to set open_batch_start_time_micros_ to be at a position of\n+          // 80% between open_batch_start_time_micros_ and now.\n+          double position = static_cast<double>(old_batch.size()) /\n+                            (old_batch.size() + new_batch.size());\n+          open_batch_start_time_micros_ +=\n+              (env_->NowMicros() - open_batch_start_time_micros_) * position;\n+        }\n       }\n     }\n "
        }
    ],
    "stats": {
        "total": 389,
        "additions": 337,
        "deletions": 52
    }
}