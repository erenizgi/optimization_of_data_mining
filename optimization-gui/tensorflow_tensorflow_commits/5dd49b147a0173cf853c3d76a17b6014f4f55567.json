{
    "author": "othakkar",
    "message": "PR #29459: [XLA:CPU][oneDNN] Refactor OneDnnThreadPool to support asynchronous execution using ParallelLoopRunner\n\nImported from GitHub PR https://github.com/openxla/xla/pull/29459\n\n**This PR depends on #28883.**\n\nThis PR refactors `OneDnnThreadPool` to support asynchronous execution via `ParallelLoopRunner`, enabling integration with the thunk runtime and oneDNN's async custom calls.\n\n**Key updates**:\n- Adds a new constructor to `OneDnnThreadPool` that accepts a `ParallelLoopRunner` instance.\n- Updates `onednn_threadpool_test` to use a `ParallelLoopRunner`.\n- When using a runner-backed thread pool, `OneDnnThreadPool` exposes the `ASYNCHRONOUS` flags and implements `wait()` to block on `runner->done_event()`.\n\nThis runner-based interface is necessary for future support of asynchronous oneDNN custom calls through the FFI handler, which will return a `future` (via `runner->done_event()`) to XLAâ€™s async runtime.\nCopybara import of the project:\n\n--\n0a431e8364d16eb3fa5f4bf887b26072af769b01 by Om Thakkar <om.thakkar@intel.com>:\n\nuse runner-based onednn threadpool iface\n\n--\n3b225d6c0ee389cf638f70af7e52bf9a48c7e07b by Om Thakkar <om.thakkar@intel.com>:\n\nremove usage of parallel loop runner\n\n--\n42393c8addc19dfde36efe9ae6b31c1a0f777f03 by Om Thakkar <om.thakkar@intel.com>:\n\nadd a getter for done_event\n\n--\nd135eced61dddcb7f311a4a850ce63e01aaacd3f by Om Thakkar <om.thakkar@intel.com>:\n\naddress review comments for formatting\n\nMerging this change closes #29459\n\nPiperOrigin-RevId: 799988462",
    "sha": "5dd49b147a0173cf853c3d76a17b6014f4f55567",
    "files": [
        {
            "sha": "feba693e0bdd39cb2da825a82e9fd65cd6df8832",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5dd49b147a0173cf853c3d76a17b6014f4f55567/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5dd49b147a0173cf853c3d76a17b6014f4f55567/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD?ref=5dd49b147a0173cf853c3d76a17b6014f4f55567",
            "patch": "@@ -90,6 +90,7 @@ onednn_graph_cc_library(\n onednn_graph_cc_test(\n     name = \"onednn_threadpool_test\",\n     srcs = [\"onednn_threadpool_test.cc\"],\n+    copts = tsl_copts(),\n     deps = [\n         \":onednn_interop\",\n         \":onednn_threadpool\","
        },
        {
            "sha": "f2a4faf2c9a4c236bcde5ce2e63e5844941296b7",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_threadpool.h",
            "status": "modified",
            "additions": 62,
            "deletions": 13,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5dd49b147a0173cf853c3d76a17b6014f4f55567/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5dd49b147a0173cf853c3d76a17b6014f4f55567/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool.h?ref=5dd49b147a0173cf853c3d76a17b6014f4f55567",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n \n+#include \"dnnl_threadpool.hpp\"\n #include \"oneapi/dnnl/dnnl_threadpool_iface.hpp\"\n #include \"xla/backends/cpu/runtime/work_queue.h\"\n \n@@ -28,34 +29,72 @@ limitations under the License.\n \n namespace xla::cpu {\n \n+static tsl::AsyncValueRef<tsl::Chain> OkDoneEventSingleton() {\n+  static tsl::AsyncValueOwningRef<tsl::Chain>* singleton = [] {\n+    auto* storage = new tsl::internal::AsyncValueStorage<tsl::Chain>();\n+    return new tsl::AsyncValueOwningRef<tsl::Chain>(\n+        tsl::MakeAvailableAsyncValueRef<tsl::Chain>(*storage));\n+  }();\n+  return singleton->AsRef();\n+}\n+\n class OneDnnThreadPool final\n     : public dnnl::threadpool_interop::threadpool_iface {\n  public:\n-  explicit OneDnnThreadPool(Eigen::ThreadPoolInterface* thread_pool)\n-      : thread_pool_(thread_pool) {}\n+  explicit OneDnnThreadPool(Eigen::ThreadPoolInterface* thread_pool,\n+                            bool is_async = false)\n+      : thread_pool_(thread_pool), is_async_(is_async) {\n+    if (is_async_) {\n+      done_event_ = OkDoneEventSingleton();\n+      dnnl_threadpool_interop_set_max_concurrency(thread_pool_->NumThreads());\n+    }\n+  }\n \n   int get_num_threads() const final { return thread_pool_->NumThreads(); }\n \n   bool get_in_parallel() const final {\n+    if (is_async_) {\n+      // TODO(intel-tf): this is a temporary fix without which oneDNN runs\n+      // single-threaded.\n+      return false;\n+    }\n     return thread_pool_->CurrentThreadId() >= 0;\n   }\n \n-  uint64_t get_flags() const final { return 0; }\n+  uint64_t get_flags() const final { return is_async_ ? ASYNCHRONOUS : 0; }\n \n #ifdef ENABLE_ONEDNN_ASYNC\n-  // This is a placeholder implementation for the wait method, as we\n-  // need to satisfy the interface requirements of the\n-  // dnnl::threadpool_interop::threadpool_iface with the experimental\n-  // asynchronous runtime support in oneDNN.\n-  // TODO(intel-tf): Implement proper wait logic when thunk runtime\n-  // with oneDNN is enabled.\n-  void wait() final {}\n+  // The wait() method only exists with oneDNN's experimental support for\n+  // asynchronous execution determined by the ENABLE_ONEDNN_ASYNC.\n+  void wait() override {\n+    if (is_async_) {\n+      // While performing asynchronous execution, wait() method is needed to\n+      // notify the user that the output is ready. oneDNN will not call wait()\n+      // inside the library to avoid deadlock.\n+      tsl::BlockUntilReady(done_event_);\n+    }\n+  }\n #endif  // ENABLE_ONEDNN_ASYNC\n \n   void parallel_for(int n, const std::function<void(int, int)>& fn) final {\n-    // It is perfectly safe to block here as Worker implements work stealing\n-    // that guarantees forward progress and deadlock freedom, even if we are\n-    // running in the same thread pool as the Eigen thread_pool.\n+    if (is_async_) {\n+      // If we are using oneDNN with async support, we need to schedule the\n+      // parallel loop using the done_event_. This allows us to return\n+      // immediately and not block the caller thread.\n+      auto parallelize = [this, n, fn](tsl::Chain) {\n+        return Worker::Parallelize(\n+            thread_pool_, thread_pool_->NumThreads(), n,\n+            [fn, n](size_t i) { fn(static_cast<int>(i), n); });\n+      };\n+\n+      done_event_ = done_event_.FlatMap(parallelize);\n+      return;\n+    }\n+\n+    // If we are not using oneDNN with async support, it is perfectly safe to\n+    // block here as Worker implements work stealing that guarantees forward\n+    // progress and deadlock freedom, even if we are running in the same thread\n+    // pool as the Eigen thread_pool.\n     tsl::BlockUntilReady(Worker::Parallelize(thread_pool_,\n                                              thread_pool_->NumThreads(), n,\n                                              [fn, n](size_t i) { fn(i, n); }));\n@@ -65,8 +104,18 @@ class OneDnnThreadPool final\n     thread_pool_ = thread_pool;\n   }\n \n+  tsl::AsyncValueRef<tsl::Chain> done_event() const { return done_event_; }\n+\n  private:\n   Eigen::ThreadPoolInterface* thread_pool_;\n+\n+  // Indicates if we are using oneDNN with async support. TODO(intel-tf): Remove\n+  // this flag when oneDNN supports asynchronous execution by default.\n+  bool is_async_ = false;\n+\n+  // Async value that signals completion of the last scheduled parallel loop.\n+  // This is used only when is_async_ is true.\n+  tsl::AsyncValueRef<tsl::Chain> done_event_;\n };\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "8a204377b5fd987e0d840a313bb93fe9e98999e6",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_threadpool_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5dd49b147a0173cf853c3d76a17b6014f4f55567/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5dd49b147a0173cf853c3d76a17b6014f4f55567/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool_test.cc?ref=5dd49b147a0173cf853c3d76a17b6014f4f55567",
            "patch": "@@ -53,7 +53,12 @@ static absl::StatusOr<dnnl::graph::graph> CreateExpGraph(\n \n TEST(OneDnnThreadPoolTest, Binary) {\n   tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 32);\n+\n+#ifdef ENABLE_ONEDNN_ASYNC\n+  OneDnnThreadPool threadpool(threads.AsEigenThreadPool(), /*is_async=*/true);\n+#else\n   OneDnnThreadPool threadpool(threads.AsEigenThreadPool());\n+#endif  // ENABLE_ONEDNN_ASYNC\n \n   int64_t d0 = 100;\n   int64_t d1 = 1000;\n@@ -100,6 +105,10 @@ TEST(OneDnnThreadPoolTest, Binary) {\n   // Execute compiled oneDNN graph on the CPU stream.\n   compiled_partitions[0].execute(stream, {src}, {dst});\n \n+#ifdef ENABLE_ONEDNN_ASYNC\n+  stream.wait();\n+#endif\n+\n   for (int i = 0; i < num_elements; ++i) {\n     EXPECT_NEAR(dst_data[i], std::exp(1.0f), 1e-5);\n   }"
        }
    ],
    "stats": {
        "total": 85,
        "additions": 72,
        "deletions": 13
    }
}