{
    "author": "ezhulenev",
    "message": "[xla:cpu:nanort] Micro optimizations for buffer construction\n\n- ifrt::Shape is too big to be efficiently passed by value, prefer passing it by reference\n- optimize error handling branches\n\n```\nname                    cpu/op        cpu/op      vs base\nBM_IfRtAddScalars       408.9n ± 1%   365.0n ± 1%  -10.73% (p=0.000 n=40)\nBM_IfRtAddManyScalars   1.219µ ± 1%   1.145µ ± 1%   -6.01% (p=0.000 n=40)\ngeomean                 705.9n        646.6n        -8.40%\n```\n\nPiperOrigin-RevId: 797415610",
    "sha": "89fbeef6b36f75b7a7b43219899f76ce526264e3",
    "files": [
        {
            "sha": "702ce2d73dfccdf486ffafb988e9be9a5a2333d4",
            "filename": "third_party/xla/xla/backends/cpu/nanort/ifrt_client.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 29,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/89fbeef6b36f75b7a7b43219899f76ce526264e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/89fbeef6b36f75b7a7b43219899f76ce526264e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc?ref=89fbeef6b36f75b7a7b43219899f76ce526264e3",
            "patch": "@@ -163,48 +163,45 @@ class NanoArray final : public NanoValue<NanoArray, ifrt::Array> {\n   // need to support deletion of the NanoArray that created the buffer.\n   using DataPtr = std::shared_ptr<void>;\n \n-  NanoArray(NanoIfrtClient* client, ifrt::DType dtype, ifrt::Shape shape,\n+  NanoArray(NanoIfrtClient* client, ifrt::DType dtype, const ifrt::Shape& shape,\n             DataPtr data, ifrt::ShardingRef sharding)\n       : NanoValue<NanoArray, ifrt::Array>(client),\n-        dtype_(std::move(dtype)),\n-        shape_(std::move(shape)),\n+        dtype_(dtype),\n+        shape_(shape),\n         data_(std::move(data)),\n         sharding_(std::move(sharding)) {}\n \n   // Allocates a new array of the given type and shape.\n   static absl::StatusOr<tsl::RCReference<NanoArray>> Allocate(\n-      NanoIfrtClient* client, ifrt::DType dtype, ifrt::Shape shape,\n+      NanoIfrtClient* client, ifrt::DType dtype, const ifrt::Shape& shape,\n       ifrt::ShardingRef sharding) {\n     TF_RET_CHECK(dtype.byte_size().has_value());\n     TF_ASSIGN_OR_RETURN(\n         DataPtr data_ptr,\n         AllocateData(dtype.byte_size().value() * shape.num_elements()));\n-    return tsl::TakeRef(new NanoArray(client, dtype, std::move(shape),\n-                                      std::move(data_ptr),\n+    return tsl::TakeRef(new NanoArray(client, dtype, shape, std::move(data_ptr),\n                                       std::move(sharding)));\n   }\n \n   // Creates an array from a host buffer. The buffer will be used directly\n   // without a copy if the copy semantics allow it and the layout is row major\n   // and dense.\n   static absl::StatusOr<tsl::RCReference<NanoArray>> FromBuffer(\n-      NanoIfrtClient* client, void* data, ifrt::DType dtype, ifrt::Shape shape,\n-      ifrt::ShardingRef sharding,\n+      NanoIfrtClient* client, void* data, ifrt::DType dtype,\n+      const ifrt::Shape& shape, ifrt::ShardingRef sharding,\n       std::optional<absl::Span<const int64_t>> byte_strides, bool make_copy,\n       std::function<void()> on_done_with_host_buffer) {\n-    auto size = dtype.byte_size().value_or(0) * shape.num_elements();\n-    TF_RET_CHECK(size > 0);\n     DataPtr data_ptr;\n \n     bool layout_compatible = LayoutCompatible(dtype, shape, byte_strides);\n     bool aligned = reinterpret_cast<uintptr_t>(data) % MinAlign() == 0;\n \n-    if (!layout_compatible || !aligned) {\n-      // Input is not aligned, or has a weird layout, so we need to copy it.\n-      make_copy = true;\n-    }\n+    // If input is not aligned, or has a weird layout, we need to copy it (or if\n+    // a copy was required by the buffer semantics).\n+    if (ABSL_PREDICT_FALSE(make_copy || !layout_compatible || !aligned)) {\n+      int64_t size = dtype.byte_size().value_or(0) * shape.num_elements();\n+      TF_RET_CHECK(size > 0);\n \n-    if (ABSL_PREDICT_FALSE(make_copy)) {\n       TF_ASSIGN_OR_RETURN(data_ptr, AllocateData(size));\n       if (layout_compatible) {\n         // Input has a compatible layout, so we can just do a memcpy.\n@@ -234,9 +231,9 @@ class NanoArray final : public NanoValue<NanoArray, ifrt::Array> {\n             }\n           });\n     }\n-    TF_RET_CHECK(data_ptr != nullptr);\n-    return tsl::TakeRef(new NanoArray(client, dtype, std::move(shape),\n-                                      std::move(data_ptr),\n+\n+    DCHECK(data_ptr) << \"data_ptr should be allocated\";\n+    return tsl::TakeRef(new NanoArray(client, dtype, shape, std::move(data_ptr),\n                                       std::move(sharding)));\n   }\n \n@@ -634,12 +631,12 @@ class ShardedNanoArray final : public NanoValue<ShardedNanoArray, ifrt::Array> {\n   ABSL_ATTRIBUTE_UNUSED static char ID;  // NOLINT\n \n  private:\n-  ShardedNanoArray(NanoIfrtClient* client, ifrt::DType dtype, ifrt::Shape shape,\n-                   ifrt::ShardingRef sharding,\n+  ShardedNanoArray(NanoIfrtClient* client, ifrt::DType dtype,\n+                   const ifrt::Shape& shape, ifrt::ShardingRef sharding,\n                    std::vector<tsl::RCReference<NanoArray>> shards)\n       : NanoValue<ShardedNanoArray, ifrt::Array>(client),\n-        dtype_(std::move(dtype)),\n-        shape_(std::move(shape)),\n+        dtype_(dtype),\n+        shape_(shape),\n         sharding_(std::move(sharding)),\n         shards_(std::move(shards)) {}\n \n@@ -1071,10 +1068,9 @@ class NanoExecutable final\n       TF_ASSIGN_OR_RETURN(auto ifrt_type,\n                           ifrt::ToDType(result_shapes[i].element_type()));\n       ifrt::Shape ifrt_shape(result_shapes[i].dimensions());\n-      TF_ASSIGN_OR_RETURN(\n-          result_arrays.emplace_back(),\n-          NanoArray::Allocate(client_, ifrt_type, std::move(ifrt_shape),\n-                              output_shardings_[i]));\n+      TF_ASSIGN_OR_RETURN(result_arrays.emplace_back(),\n+                          NanoArray::Allocate(client_, ifrt_type, ifrt_shape,\n+                                              output_shardings_[i]));\n     }\n \n     return result_arrays;\n@@ -1266,9 +1262,8 @@ absl::StatusOr<ifrt::ArrayRef> NanoIfrtClient::MakeArrayFromHostBuffer(\n       make_copy = false;\n       break;\n   }\n-  return NanoArray::FromBuffer(this, const_cast<void*>(data), dtype,\n-                               std::move(shape), std::move(sharding),\n-                               byte_strides, make_copy,\n+  return NanoArray::FromBuffer(this, const_cast<void*>(data), dtype, shape,\n+                               std::move(sharding), byte_strides, make_copy,\n                                std::move(on_done_with_host_buffer));\n }\n "
        }
    ],
    "stats": {
        "total": 53,
        "additions": 24,
        "deletions": 29
    }
}