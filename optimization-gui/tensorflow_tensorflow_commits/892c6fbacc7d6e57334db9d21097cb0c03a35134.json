{
    "author": "ezhulenev",
    "message": "[xla:cpu] Migrate XLA:GPU backend to se::DeviceAddress\n\nPiperOrigin-RevId: 841186984",
    "sha": "892c6fbacc7d6e57334db9d21097cb0c03a35134",
    "files": [
        {
            "sha": "b0a85c6ca377fce051c25e33e5952e9d2e4b105e",
            "filename": "third_party/xla/xla/backends/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2FBUILD?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -26,7 +26,7 @@ cc_library(\n         \"//xla/ffi\",\n         \"//xla/ffi/api:c_api\",\n         \"//xla/ffi/api:c_api_internal\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:scratch_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"@com_google_absl//absl/base:core_headers\","
        },
        {
            "sha": "8cda3c8d28b3e26037cab5b007b0c076ba338f0f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -145,9 +145,9 @@ cc_library(\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n         \"//xla/stream_executor:blas\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n@@ -298,8 +298,8 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:stream_executor_util\",\n         \"//xla/service/gpu/transforms:cudnn_fusion_compiler\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:stream\",\n@@ -518,8 +518,8 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n         \"//xla/service/gpu/autotuning:redzone_buffers\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/gpu:redzone_allocator\",\n@@ -677,7 +677,7 @@ xla_test(\n         \"//xla/service:transfer_manager\",\n         \"//xla/service/gpu:gpu_compiler\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n@@ -844,7 +844,7 @@ xla_cc_binary(\n         \"//xla/service:compiler\",\n         \"//xla/service:gpu_plugin\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\","
        },
        {
            "sha": "b81cf80665db8827000d16671a323d33a04996bc",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -43,7 +43,7 @@ limitations under the License.\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform/platform_object_registry.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -109,7 +109,7 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n       get_codegen_backends(stream_executor, &debug_options, compiler.get(),\n                            &target_config, mlir_context);\n \n-  std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n+  std::unique_ptr<se::DeviceAddressAllocator> allocator =\n       std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n           stream_executor);\n   auto profiler ="
        },
        {
            "sha": "425343686beeaf5afaada4e9ddc1e131567c69c2",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -29,9 +29,9 @@ limitations under the License.\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/stream_executor/blas.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -57,7 +57,7 @@ CublasBackend::GetSupportedConfigs(const HloInstruction& instr) {\n     return configs;\n   }\n \n-  std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n+  std::unique_ptr<se::DeviceAddressAllocator> allocator =\n       std::make_unique<se::StreamExecutorMemoryAllocator>(stream_executor());\n   TF_ASSIGN_OR_RETURN(\n       se::Stream * stream,\n@@ -80,7 +80,7 @@ CublasBackend::GetSupportedConfigs(const HloInstruction& instr) {\n     TF_ASSIGN_OR_RETURN(se::blas::DataType type,\n                         se::gpu::AsBlasDataType(layout.dtype));\n     return se::gpu::MatrixDescriptor{\n-        /*data=*/se::DeviceMemoryBase(), layout.leading_dim_stride,\n+        /*data=*/se::DeviceAddressBase(), layout.leading_dim_stride,\n         layout.batch_stride, type,\n         // BLAS is column-major by default.\n         (layout.order == se::gpu::MatrixLayout::Order::kColumnMajor"
        },
        {
            "sha": "c5c9111d0eac80231473f5100e7beb414a364946",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -41,8 +41,8 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/cudnn_fusion_compiler.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -198,11 +198,11 @@ absl::StatusOr<std::vector<CudnnBackendConfig>> GetAlgorithms(\n       TF_RETURN_IF_ERROR(dnn->GetConvolveRunners(\n           conv_kind, input_type, output_type, stream,\n           gpu_conv_config.input_descriptor,\n-          /*input_data=*/se::DeviceMemoryBase(nullptr),\n+          /*input_data=*/se::DeviceAddressBase(nullptr),\n           gpu_conv_config.filter_descriptor,\n-          /*filter_data=*/se::DeviceMemoryBase(nullptr),\n+          /*filter_data=*/se::DeviceAddressBase(nullptr),\n           gpu_conv_config.output_descriptor,\n-          /*output_data=*/se::DeviceMemoryBase(nullptr),\n+          /*output_data=*/se::DeviceAddressBase(nullptr),\n           gpu_conv_config.conv_desc, use_fallback,\n           /*scratch_allocator=*/nullptr, engine_options, &conv_runners));\n       break;"
        },
        {
            "sha": "81b5135600507cc0168d1a69bd1c5027413262ef",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -37,8 +37,8 @@ limitations under the License.\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n@@ -53,7 +53,7 @@ namespace gpu {\n namespace {\n \n std::vector<ExecutionInput> CreateExecutionInputsFromBuffers(\n-    absl::Span<se::DeviceMemoryBase const> buffers,\n+    absl::Span<se::DeviceAddressBase const> buffers,\n     absl::Span<Shape const> shapes) {\n   CHECK_EQ(buffers.size(), shapes.size());\n   std::vector<ExecutionInput> inputs;\n@@ -90,9 +90,9 @@ int GetScratchBytes(const Executable* executable) {\n \n std::unique_ptr<GpuProfiler> GpuProfiler::Create(\n     se::StreamExecutor* stream_executor, ProfileOptions options,\n-    se::DeviceMemoryAllocator* external_allocator) {\n-  std::unique_ptr<se::DeviceMemoryAllocator> owned_allocator;\n-  se::DeviceMemoryAllocator* active_allocator = external_allocator;\n+    se::DeviceAddressAllocator* external_allocator) {\n+  std::unique_ptr<se::DeviceAddressAllocator> owned_allocator;\n+  se::DeviceAddressAllocator* active_allocator = external_allocator;\n \n   if (active_allocator == nullptr) {\n     owned_allocator ="
        },
        {
            "sha": "76957205d0dbb213bda082659ba6085fac162070",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.h",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n #include \"xla/service/shaped_buffer.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -42,7 +42,7 @@ class GpuProfiler : public Profiler {\n  public:\n   static std::unique_ptr<GpuProfiler> Create(\n       stream_executor::StreamExecutor* stream_executor, ProfileOptions options,\n-      se::DeviceMemoryAllocator* external_allocator = nullptr);\n+      se::DeviceAddressAllocator* external_allocator = nullptr);\n \n   // The input buffers shapes are taken from the attatched HloModule to the\n   // executable.\n@@ -61,8 +61,9 @@ class GpuProfiler : public Profiler {\n \n  private:\n   explicit GpuProfiler(\n-      se::StreamExecutor* stream_executor, se::DeviceMemoryAllocator* allocator,\n-      std::unique_ptr<se::DeviceMemoryAllocator> owned_allocator,\n+      se::StreamExecutor* stream_executor,\n+      se::DeviceAddressAllocator* allocator,\n+      std::unique_ptr<se::DeviceAddressAllocator> owned_allocator,\n       se::Stream* stream, ProfileOptions options)\n       : stream_executor_(stream_executor),\n         allocator_(allocator),\n@@ -75,8 +76,8 @@ class GpuProfiler : public Profiler {\n                                           ExecutionProfile* profile);\n \n   se::StreamExecutor* stream_executor_;\n-  se::DeviceMemoryAllocator* allocator_;\n-  std::unique_ptr<se::DeviceMemoryAllocator> owned_allocator_;\n+  se::DeviceAddressAllocator* allocator_;\n+  std::unique_ptr<se::DeviceAddressAllocator> owned_allocator_;\n   se::Stream* stream_;\n   ProfileOptions options_;\n };"
        },
        {
            "sha": "2bd462e10074d982d6b395841d37ae635028b302",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -43,7 +43,7 @@ limitations under the License.\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/service/transfer_manager.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n@@ -88,7 +88,7 @@ class MockExecutable : public Executable {\n };\n \n absl::StatusOr<ScopedShapedBuffer> CreateTestBuffer(\n-    se::DeviceMemoryAllocator* allocator, se::StreamExecutor* stream_exec,\n+    se::DeviceAddressAllocator* allocator, se::StreamExecutor* stream_exec,\n     se::Stream* stream, int32_t value) {\n   Shape test_shape = ShapeUtil::MakeShape(S32, {});\n   TF_ASSIGN_OR_RETURN(auto* transfer_manager, TransferManager::GetForPlatform(\n@@ -104,7 +104,7 @@ absl::StatusOr<ScopedShapedBuffer> CreateTestBuffer(\n }\n \n absl::StatusOr<ScopedShapedBuffer> CreateTupleTestBuffer(\n-    se::DeviceMemoryAllocator* allocator, se::StreamExecutor* stream_exec,\n+    se::DeviceAddressAllocator* allocator, se::StreamExecutor* stream_exec,\n     se::Stream* stream, int32_t value1, int32_t value2) {\n   Shape test_shape = ShapeUtil::MakeShape(S32, {});\n   Shape test_shape_tuple = ShapeUtil::MakeTupleShape({test_shape, test_shape});\n@@ -133,7 +133,7 @@ class GpuProfilerTest : public HloHardwareIndependentTestBase {\n         std::make_unique<se::StreamExecutorMemoryAllocator>(stream_exec_);\n   }\n   se::StreamExecutor* stream_exec_;\n-  std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n+  std::unique_ptr<se::DeviceAddressAllocator> allocator_;\n };\n \n TEST_F(GpuProfilerTest, CreateInputBuffersAndProfile) {"
        },
        {
            "sha": "2be8a0247fbe2762ea5aa6a28ebf9025dc2f3fe7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -219,7 +219,7 @@ xla_test(\n         \"//xla/service/gpu:gpu_executable\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu/transforms:dynamic_slice_fusion_rewriter\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\","
        },
        {
            "sha": "6b6b860885065c97416f57d781b6005c45609e48",
            "filename": "third_party/xla/xla/backends/gpu/codegen/dynamic_slice_fusion_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fdynamic_slice_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fdynamic_slice_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fdynamic_slice_fusion_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -49,7 +49,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -1022,8 +1022,8 @@ TEST_F(DynamicSliceFusionTest, SlicedOperandAliasingOutput) {\n \n static absl::Status Memcpy(se::Stream* stream, ffi::AnyBuffer src,\n                            ffi::Result<ffi::AnyBuffer> dst) {\n-  se::DeviceMemoryBase dst_mem = dst->device_memory();\n-  se::DeviceMemoryBase src_mem = src.device_memory();\n+  se::DeviceAddressBase dst_mem = dst->device_memory();\n+  se::DeviceAddressBase src_mem = src.device_memory();\n   return stream->MemcpyD2D(&dst_mem, src_mem, src_mem.size());\n }\n \n@@ -1098,13 +1098,13 @@ static absl::Status SubBuffers(\n   //  dst5:  result at tuple index {4}, shape f32[3,128]\n   //  dst6:  result at tuple index {5}, shape f32[96]\n \n-  se::DeviceMemoryBase dst0_mem = dst0->device_memory();\n-  se::DeviceMemoryBase dst1_mem = dst1->device_memory();\n-  se::DeviceMemoryBase dst2_mem = dst2->device_memory();\n-  se::DeviceMemoryBase dst3_mem = dst3->device_memory();\n-  se::DeviceMemoryBase dst4_mem = dst4->device_memory();\n-  se::DeviceMemoryBase dst5_mem = dst5->device_memory();\n-  se::DeviceMemoryBase dst6_mem = dst6->device_memory();\n+  se::DeviceAddressBase dst0_mem = dst0->device_memory();\n+  se::DeviceAddressBase dst1_mem = dst1->device_memory();\n+  se::DeviceAddressBase dst2_mem = dst2->device_memory();\n+  se::DeviceAddressBase dst3_mem = dst3->device_memory();\n+  se::DeviceAddressBase dst4_mem = dst4->device_memory();\n+  se::DeviceAddressBase dst5_mem = dst5->device_memory();\n+  se::DeviceAddressBase dst6_mem = dst6->device_memory();\n \n   TF_RETURN_IF_ERROR(\n       stream->MemcpyD2D(&dst0_mem, src3.device_memory(), 8 * sizeof(float)));\n@@ -1120,7 +1120,7 @@ static absl::Status SubBuffers(\n                                        3 * 128 * sizeof(float)));\n   TF_RETURN_IF_ERROR(\n       stream->MemcpyD2D(&dst6_mem, src6.device_memory(), 64 * sizeof(float)));\n-  stream_executor::DeviceMemoryBase slice =\n+  stream_executor::DeviceAddressBase slice =\n       dst6_mem.GetByteSlice(64 * sizeof(float), 32 * sizeof(float));\n   TF_RETURN_IF_ERROR(\n       stream->MemcpyD2D(&slice, src6.device_memory(), 32 * sizeof(float)));\n@@ -2698,13 +2698,13 @@ static absl::Status SubBuffers2(\n   //  dst5:  result at tuple index {4, 0}, shape f32[5,128]\n   //  dst6:  result at tuple index {4, 1}, shape f32[3,128]\n \n-  se::DeviceMemoryBase dst0_mem = dst0->device_memory();\n-  se::DeviceMemoryBase dst1_mem = dst1->device_memory();\n-  se::DeviceMemoryBase dst2_mem = dst2->device_memory();\n-  se::DeviceMemoryBase dst3_mem = dst3->device_memory();\n-  se::DeviceMemoryBase dst4_mem = dst4->device_memory();\n-  se::DeviceMemoryBase dst5_mem = dst5->device_memory();\n-  se::DeviceMemoryBase dst6_mem = dst6->device_memory();\n+  se::DeviceAddressBase dst0_mem = dst0->device_memory();\n+  se::DeviceAddressBase dst1_mem = dst1->device_memory();\n+  se::DeviceAddressBase dst2_mem = dst2->device_memory();\n+  se::DeviceAddressBase dst3_mem = dst3->device_memory();\n+  se::DeviceAddressBase dst4_mem = dst4->device_memory();\n+  se::DeviceAddressBase dst5_mem = dst5->device_memory();\n+  se::DeviceAddressBase dst6_mem = dst6->device_memory();\n \n   TF_RETURN_IF_ERROR(\n       stream->MemcpyD2D(&dst0_mem, src3.device_memory(), 8 * sizeof(float)));"
        },
        {
            "sha": "975166a9b64b3487304e07b8ef245f35e201f0d1",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -239,7 +239,7 @@ cc_library(\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/runtime:device_id\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -260,7 +260,7 @@ cc_library(\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n         \"@com_google_absl//absl/status\",\n@@ -349,7 +349,7 @@ cc_library(\n         \"//xla/runtime:device_id\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:gpu_stream\",\n@@ -414,7 +414,7 @@ cc_library(\n         \"//xla/runtime:device_id\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -480,7 +480,7 @@ cc_library(\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:gpu_stream\",\n         \"//xla/tsl/concurrency:async_value\",\n@@ -533,7 +533,7 @@ xla_test(\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "05c54e2cf1e022e88c82fc36ae021d3185d6bcb2",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_collectives.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"xla/core/collectives/collectives.h\"\n #include \"xla/core/collectives/collectives_registry.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -57,9 +57,9 @@ stream_executor::Stream* GpuCollectives::Executor::stream() const {\n   return stream_;\n }\n \n-se::DeviceMemoryBase GpuCollectives::Slice(se::DeviceMemoryBase buff,\n-                                           PrimitiveType dtype, size_t offset,\n-                                           size_t count) {\n+se::DeviceAddressBase GpuCollectives::Slice(se::DeviceAddressBase buff,\n+                                            PrimitiveType dtype, size_t offset,\n+                                            size_t count) {\n   size_t multiplier = ShapeUtil::ByteSizeOfPrimitiveType(dtype);\n   return buff.GetByteSlice(offset * multiplier, count * multiplier);\n }"
        },
        {
            "sha": "5c7c51f8f7068d7e9ff66f76162a7b11c32527f0",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_collectives.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n #include \"xla/executable_run_options.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/runtime/device_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -134,8 +134,8 @@ class GpuCollectives : public Collectives {\n \n   // Returns a slice of device memory `buff` containing `count` values of data\n   // type `dtype` starting from `offset`.\n-  static stream_executor::DeviceMemoryBase Slice(\n-      stream_executor::DeviceMemoryBase buff, PrimitiveType dtype,\n+  static stream_executor::DeviceAddressBase Slice(\n+      stream_executor::DeviceAddressBase buff, PrimitiveType dtype,\n       size_t offset, size_t count);\n \n   // TODO(b/410686553): Use smart wrapper instead of void*."
        },
        {
            "sha": "04aa777e0d17f14c423e3dcaa44480ab36125bac",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_communicator.h",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_communicator.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace xla::gpu {\n \n@@ -47,44 +47,44 @@ class GpuCommunicator : public Communicator {\n   virtual Future<> GroupExecute(\n       absl::AnyInvocable<absl::Status(GpuCommunicator*)> f) = 0;\n \n-  virtual absl::Status LaunchAllReduce(se::DeviceMemoryBase send_buffer,\n-                                       se::DeviceMemoryBase recv_buffer,\n+  virtual absl::Status LaunchAllReduce(se::DeviceAddressBase send_buffer,\n+                                       se::DeviceAddressBase recv_buffer,\n                                        PrimitiveType dtype, size_t count,\n                                        ReductionKind reduction_kind,\n                                        const Executor& executor) = 0;\n \n-  virtual absl::Status LaunchBroadcast(se::DeviceMemoryBase send_buffer,\n-                                       se::DeviceMemoryBase recv_buffer,\n+  virtual absl::Status LaunchBroadcast(se::DeviceAddressBase send_buffer,\n+                                       se::DeviceAddressBase recv_buffer,\n                                        PrimitiveType dtype, size_t count,\n                                        RankId root,\n                                        const Executor& executor) = 0;\n \n-  virtual absl::Status LaunchReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                           se::DeviceMemoryBase recv_buffer,\n+  virtual absl::Status LaunchReduceScatter(se::DeviceAddressBase send_buffer,\n+                                           se::DeviceAddressBase recv_buffer,\n                                            PrimitiveType dtype, size_t count,\n                                            ReductionKind reduction_kind,\n                                            const Executor& executor) = 0;\n \n-  virtual absl::Status LaunchAllGather(se::DeviceMemoryBase send_buffer,\n-                                       se::DeviceMemoryBase recv_buffer,\n+  virtual absl::Status LaunchAllGather(se::DeviceAddressBase send_buffer,\n+                                       se::DeviceAddressBase recv_buffer,\n                                        PrimitiveType dtype, size_t count,\n                                        const Executor& executor) = 0;\n \n   virtual absl::Status LaunchAllToAll(\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+      absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+      absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n       PrimitiveType dtype, size_t count, const Executor& executor) = 0;\n \n   virtual absl::Status LaunchCollectivePermute(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+      se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n       PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n       absl::Span<const RankId> target_ranks, const Executor& executor) = 0;\n \n-  virtual absl::Status LaunchSend(se::DeviceMemoryBase send_buffer,\n+  virtual absl::Status LaunchSend(se::DeviceAddressBase send_buffer,\n                                   PrimitiveType dtype, size_t count,\n                                   RankId peer, const Executor& executor) = 0;\n \n-  virtual absl::Status LaunchRecv(se::DeviceMemoryBase recv_buffer,\n+  virtual absl::Status LaunchRecv(se::DeviceAddressBase recv_buffer,\n                                   PrimitiveType dtype, size_t count,\n                                   RankId peer, const Executor& executor) = 0;\n };"
        },
        {
            "sha": "1ed56cb153b71e710979bdfac668f480fb665139",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 36,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_stream.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -327,7 +327,7 @@ absl::StatusOr<size_t> NcclCommunicator::NumRanks() const {\n }\n \n absl::Status NcclCommunicator::RegisterBufferOnce(\n-    se::DeviceMemoryBase buffer_range, int device_ordinal,\n+    se::DeviceAddressBase buffer_range, int device_ordinal,\n     bool use_symmetric_buffer) {\n   bool need_reg = false;\n   {\n@@ -358,7 +358,7 @@ absl::Status NcclCommunicator::RegisterBufferOnce(\n }\n \n absl::StatusOr<std::unique_ptr<Communicator::RegisteredBufferHandle>>\n-NcclCommunicator::RegisterBuffer(stream_executor::DeviceMemoryBase buffer,\n+NcclCommunicator::RegisterBuffer(stream_executor::DeviceAddressBase buffer,\n                                  int device_ordinal,\n                                  bool use_symmetric_buffer) {\n #if (NCCL_VERSION_CODE >= 21901)\n@@ -428,8 +428,8 @@ Future<> NcclCommunicator::GroupExecute(\n   });\n }\n \n-Future<> NcclCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n-                                     se::DeviceMemoryBase recv_buffer,\n+Future<> NcclCommunicator::AllReduce(se::DeviceAddressBase send_buffer,\n+                                     se::DeviceAddressBase recv_buffer,\n                                      PrimitiveType dtype, size_t count,\n                                      ReductionKind reduction_kind,\n                                      const Communicator::Executor& executor) {\n@@ -440,8 +440,8 @@ Future<> NcclCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n   });\n }\n \n-Future<> NcclCommunicator::Broadcast(se::DeviceMemoryBase send_buffer,\n-                                     se::DeviceMemoryBase recv_buffer,\n+Future<> NcclCommunicator::Broadcast(se::DeviceAddressBase send_buffer,\n+                                     se::DeviceAddressBase recv_buffer,\n                                      PrimitiveType dtype, size_t count,\n                                      RankId root, const Executor& executor) {\n   return Execute(\n@@ -451,8 +451,8 @@ Future<> NcclCommunicator::Broadcast(se::DeviceMemoryBase send_buffer,\n       });\n }\n \n-Future<> NcclCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                         se::DeviceMemoryBase recv_buffer,\n+Future<> NcclCommunicator::ReduceScatter(se::DeviceAddressBase send_buffer,\n+                                         se::DeviceAddressBase recv_buffer,\n                                          PrimitiveType dtype, size_t count,\n                                          ReductionKind reduction_kind,\n                                          const Executor& executor) {\n@@ -463,8 +463,8 @@ Future<> NcclCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n   });\n }\n \n-Future<> NcclCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n-                                     se::DeviceMemoryBase recv_buffer,\n+Future<> NcclCommunicator::AllGather(se::DeviceAddressBase send_buffer,\n+                                     se::DeviceAddressBase recv_buffer,\n                                      PrimitiveType dtype, size_t count,\n                                      const Executor& executor) {\n   return Execute([send_buffer, recv_buffer, dtype, count, &executor, this]() {\n@@ -473,16 +473,16 @@ Future<> NcclCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n }\n \n Future<> NcclCommunicator::AllToAll(\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n   return Execute([send_buffers, recv_buffers, dtype, count, &executor, this]() {\n     return LaunchAllToAll(send_buffers, recv_buffers, dtype, count, executor);\n   });\n }\n \n Future<> NcclCommunicator::CollectivePermute(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n   std::vector<RankId> owned_target_ranks(target_ranks.begin(),\n@@ -495,15 +495,15 @@ Future<> NcclCommunicator::CollectivePermute(\n   });\n }\n \n-Future<> NcclCommunicator::Send(se::DeviceMemoryBase send_buffer,\n+Future<> NcclCommunicator::Send(se::DeviceAddressBase send_buffer,\n                                 PrimitiveType dtype, size_t count, RankId peer,\n                                 const Executor& executor) {\n   return Execute([send_buffer, dtype, count, peer, &executor, this]() {\n     return LaunchSend(send_buffer, dtype, count, peer, executor);\n   });\n }\n \n-Future<> NcclCommunicator::Recv(se::DeviceMemoryBase recv_buffer,\n+Future<> NcclCommunicator::Recv(se::DeviceAddressBase recv_buffer,\n                                 PrimitiveType dtype, size_t count, RankId peer,\n                                 const Executor& executor) {\n   return Execute([recv_buffer, dtype, count, peer, &executor, this]() {\n@@ -535,7 +535,7 @@ absl::Status NcclCommunicator::GroupEnd() {\n }\n \n absl::Status NcclCommunicator::LaunchAllReduce(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n     const Communicator::Executor& executor) {\n   if (canceling_.load()) {\n@@ -563,11 +563,9 @@ absl::Status NcclCommunicator::LaunchAllReduce(\n   return absl::OkStatus();\n }\n \n-absl::Status NcclCommunicator::LaunchBroadcast(se::DeviceMemoryBase send_buffer,\n-                                               se::DeviceMemoryBase recv_buffer,\n-                                               PrimitiveType dtype,\n-                                               size_t count, RankId root,\n-                                               const Executor& executor) {\n+absl::Status NcclCommunicator::LaunchBroadcast(\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n+    PrimitiveType dtype, size_t count, RankId root, const Executor& executor) {\n   if (canceling_.load()) {\n     return absl::FailedPreconditionError(\"NcclCommunicator aborted\");\n   }\n@@ -593,7 +591,7 @@ absl::Status NcclCommunicator::LaunchBroadcast(se::DeviceMemoryBase send_buffer,\n }\n \n absl::Status NcclCommunicator::LaunchReduceScatter(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n     const Executor& executor) {\n   if (canceling_.load()) {\n@@ -621,11 +619,9 @@ absl::Status NcclCommunicator::LaunchReduceScatter(\n   return absl::OkStatus();\n }\n \n-absl::Status NcclCommunicator::LaunchAllGather(se::DeviceMemoryBase send_buffer,\n-                                               se::DeviceMemoryBase recv_buffer,\n-                                               PrimitiveType dtype,\n-                                               size_t count,\n-                                               const Executor& executor) {\n+absl::Status NcclCommunicator::LaunchAllGather(\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n+    PrimitiveType dtype, size_t count, const Executor& executor) {\n   if (canceling_.load()) {\n     return absl::FailedPreconditionError(\"NcclCommunicator aborted\");\n   }\n@@ -650,15 +646,15 @@ absl::Status NcclCommunicator::LaunchAllGather(se::DeviceMemoryBase send_buffer,\n }\n \n absl::Status NcclCommunicator::LaunchAllToAll(\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n   if (canceling_.load()) {\n     return absl::FailedPreconditionError(\"NcclCommunicator aborted\");\n   }\n   se::Stream* stream = ToStream(executor);\n \n-  auto buffer_formatter = [](std::string* out, se::DeviceMemoryBase buffer) {\n+  auto buffer_formatter = [](std::string* out, se::DeviceAddressBase buffer) {\n     absl::StrAppendFormat(out, \"%p\", buffer.opaque());\n   };\n \n@@ -689,8 +685,8 @@ absl::Status NcclCommunicator::LaunchAllToAll(\n \n   TF_RETURN_IF_ERROR(GroupStart());\n   for (size_t i = 0; i < send_buffers.size(); ++i) {\n-    se::DeviceMemoryBase send_buffer = send_buffers[i];\n-    se::DeviceMemoryBase recv_buffer = recv_buffers[i];\n+    se::DeviceAddressBase send_buffer = send_buffers[i];\n+    se::DeviceAddressBase recv_buffer = recv_buffers[i];\n \n     XLA_NCCL_RETURN_IF_ERROR(\n         ncclSend(send_buffer.opaque(), ToNcclCount(dtype, count), nccl_dtype, i,\n@@ -705,7 +701,7 @@ absl::Status NcclCommunicator::LaunchAllToAll(\n }\n \n absl::Status NcclCommunicator::LaunchCollectivePermute(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n   if (canceling_.load()) {\n@@ -752,7 +748,7 @@ absl::Status NcclCommunicator::LaunchCollectivePermute(\n   return absl::OkStatus();\n }\n \n-absl::Status NcclCommunicator::LaunchSend(se::DeviceMemoryBase send_buffer,\n+absl::Status NcclCommunicator::LaunchSend(se::DeviceAddressBase send_buffer,\n                                           PrimitiveType dtype, size_t count,\n                                           RankId peer,\n                                           const Executor& executor) {\n@@ -779,7 +775,7 @@ absl::Status NcclCommunicator::LaunchSend(se::DeviceMemoryBase send_buffer,\n   return absl::OkStatus();\n }\n \n-absl::Status NcclCommunicator::LaunchRecv(se::DeviceMemoryBase recv_buffer,\n+absl::Status NcclCommunicator::LaunchRecv(se::DeviceAddressBase recv_buffer,\n                                           PrimitiveType dtype, size_t count,\n                                           RankId peer,\n                                           const Executor& executor) {"
        },
        {
            "sha": "f23b21682c200e3cee3e6fb0d82100057934a462",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.h",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/executor.h\"\n #include \"xla/tsl/platform/env.h\"\n@@ -87,47 +87,47 @@ class NcclCommunicator : public GpuCommunicator {\n   // Since each XLA buffer is a slice into a larger BFCAllocator chunk, first\n   // get the base address of buffer. We will use the base address to keep track\n   // of which chunks we have registered.\n-  absl::Status RegisterBufferOnce(se::DeviceMemoryBase buffer_range,\n+  absl::Status RegisterBufferOnce(se::DeviceAddressBase buffer_range,\n                                   int device_ordinal,\n                                   bool use_symmetric_buffer) final;\n \n   Future<> GroupExecute(\n       absl::AnyInvocable<absl::Status(GpuCommunicator*)> f) final;\n \n-  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllReduce(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, ReductionKind reduction_kind,\n                      const Executor& executor) final;\n \n-  Future<> Broadcast(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> Broadcast(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, RankId root, const Executor& executor) final;\n \n-  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> ReduceScatter(se::DeviceAddressBase send_buffer,\n+                         se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                          size_t count, ReductionKind reduction_kind,\n                          const Executor& executor) final;\n \n-  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllGather(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, const Executor& executor) final;\n \n-  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n                     PrimitiveType dtype, size_t count,\n                     const Executor& executor) final;\n \n-  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n-                             se::DeviceMemoryBase recv_buffer,\n+  Future<> CollectivePermute(se::DeviceAddressBase send_buffer,\n+                             se::DeviceAddressBase recv_buffer,\n                              PrimitiveType dtype, size_t count,\n                              std::optional<RankId> source_rank,\n                              absl::Span<const RankId> target_ranks,\n                              const Executor& executor) final;\n \n-  Future<> Send(se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+  Future<> Send(se::DeviceAddressBase send_buffer, PrimitiveType dtype,\n                 size_t count, RankId peer, const Executor& executor) final;\n \n-  Future<> Recv(se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> Recv(se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                 size_t count, RankId peer, const Executor& executor) final;\n \n   std::string ToString() const final;\n@@ -136,7 +136,7 @@ class NcclCommunicator : public GpuCommunicator {\n \n  private:\n   absl::StatusOr<std::unique_ptr<RegisteredBufferHandle>> RegisterBuffer(\n-      se::DeviceMemoryBase buffer, int device_ordinal,\n+      se::DeviceAddressBase buffer, int device_ordinal,\n       bool use_symmetric_buffer);\n \n   class NcclRegisteredBufferHandle;\n@@ -150,46 +150,46 @@ class NcclCommunicator : public GpuCommunicator {\n   absl::Status GroupStart();\n   absl::Status GroupEnd();\n \n-  absl::Status LaunchAllReduce(se::DeviceMemoryBase send_buffer,\n-                               se::DeviceMemoryBase recv_buffer,\n+  absl::Status LaunchAllReduce(se::DeviceAddressBase send_buffer,\n+                               se::DeviceAddressBase recv_buffer,\n                                PrimitiveType dtype, size_t count,\n                                ReductionKind reduction_kind,\n                                const Executor& executor) final;\n \n-  absl::Status LaunchBroadcast(se::DeviceMemoryBase send_buffer,\n-                               se::DeviceMemoryBase recv_buffer,\n+  absl::Status LaunchBroadcast(se::DeviceAddressBase send_buffer,\n+                               se::DeviceAddressBase recv_buffer,\n                                PrimitiveType dtype, size_t count, RankId root,\n                                const Executor& executor) final;\n \n-  absl::Status LaunchReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                   se::DeviceMemoryBase recv_buffer,\n+  absl::Status LaunchReduceScatter(se::DeviceAddressBase send_buffer,\n+                                   se::DeviceAddressBase recv_buffer,\n                                    PrimitiveType dtype, size_t count,\n                                    ReductionKind reduction_kind,\n                                    const Executor& executor) final;\n \n-  absl::Status LaunchAllGather(se::DeviceMemoryBase send_buffer,\n-                               se::DeviceMemoryBase recv_buffer,\n+  absl::Status LaunchAllGather(se::DeviceAddressBase send_buffer,\n+                               se::DeviceAddressBase recv_buffer,\n                                PrimitiveType dtype, size_t count,\n                                const Executor& executor) final;\n \n   absl::Status LaunchAllToAll(\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+      absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+      absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n       PrimitiveType dtype, size_t count, const Executor& executor) final;\n \n-  absl::Status LaunchCollectivePermute(se::DeviceMemoryBase send_buffer,\n-                                       se::DeviceMemoryBase recv_buffer,\n+  absl::Status LaunchCollectivePermute(se::DeviceAddressBase send_buffer,\n+                                       se::DeviceAddressBase recv_buffer,\n                                        PrimitiveType dtype, size_t count,\n                                        std::optional<RankId> source_rank,\n                                        absl::Span<const RankId> target_ranks,\n                                        const Executor& executor) final;\n \n-  absl::Status LaunchSend(se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n-                          size_t count, RankId peer,\n+  absl::Status LaunchSend(se::DeviceAddressBase send_buffer,\n+                          PrimitiveType dtype, size_t count, RankId peer,\n                           const Executor& executor) final;\n \n-  absl::Status LaunchRecv(se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n-                          size_t count, RankId peer,\n+  absl::Status LaunchRecv(se::DeviceAddressBase recv_buffer,\n+                          PrimitiveType dtype, size_t count, RankId peer,\n                           const Executor& executor) final;\n \n   // Polls the communicator until any pending non-blocking operations are \"done\""
        },
        {
            "sha": "988081b1c94423b8213a624fc860430e22864e3e",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n \n #if TENSORFLOW_USE_ROCM\n@@ -130,7 +130,7 @@ TEST(NcclCommunicator, DoubleAbortFails) {\n TEST(NcclCommunicator, OperationsFailAfterAbort) {\n   for (const bool blocking : {true, false}) {\n     // Declare placeholder variables to make the operations below compile.\n-    se::DeviceMemoryBase buf;\n+    se::DeviceAddressBase buf;\n     PrimitiveType dtype = PrimitiveType::U64;\n     size_t count = 0;\n     ReductionKind rk = ReductionKind::SUM;"
        },
        {
            "sha": "fd14b490160e59fbbabd61df42190c9a0cca2125",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_communicator.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_stream.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -213,7 +213,7 @@ absl::StatusOr<size_t> NvshmemCommunicator::CurrentRank() {\n }\n \n Future<> NvshmemCommunicator::AllReduce(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n     const Communicator::Executor& executor) {\n   if (aborted_) {\n@@ -352,8 +352,8 @@ size_t GetPrimitiveTypeSize(PrimitiveType type) {\n // the actual data transfer between peers.\n absl::Status NvshmemCommunicator::P2P(absl::string_view op_name,\n                                       PrimitiveType type,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      se::DeviceMemoryBase send_buffer,\n+                                      se::DeviceAddressBase recv_buffer,\n+                                      se::DeviceAddressBase send_buffer,\n                                       size_t count, RankId peer,\n                                       const Executor& executor) {\n   if (!op_name.empty() && op_name != \"send\" && op_name != \"recv\") {\n@@ -446,8 +446,8 @@ absl::Status NvshmemCommunicator::P2P(absl::string_view op_name,\n   return absl::OkStatus();\n }\n \n-Future<> NvshmemCommunicator::Send(se::DeviceMemoryBase recv_buffer,\n-                                   se::DeviceMemoryBase send_buffer,\n+Future<> NvshmemCommunicator::Send(se::DeviceAddressBase recv_buffer,\n+                                   se::DeviceAddressBase send_buffer,\n                                    PrimitiveType dtype, size_t count,\n                                    RankId peer, const Executor& executor) {\n   VLOG(1) << \"Send NVSHMEM communicator: \" << ToString();\n@@ -464,8 +464,8 @@ Future<> NvshmemCommunicator::Send(se::DeviceMemoryBase recv_buffer,\n   return absl::OkStatus();\n }\n \n-Future<> NvshmemCommunicator::Recv(se::DeviceMemoryBase recv_buffer,\n-                                   se::DeviceMemoryBase send_buffer,\n+Future<> NvshmemCommunicator::Recv(se::DeviceAddressBase recv_buffer,\n+                                   se::DeviceAddressBase send_buffer,\n                                    PrimitiveType dtype, size_t count,\n                                    RankId peer, const Executor& executor) {\n   VLOG(1) << \"Recv NVSHMEM communicator: \" << ToString();"
        },
        {
            "sha": "152c28032419bdd46517b55da67c7df23576ac26",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_communicator.h",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -56,63 +56,63 @@ class NvshmemCommunicator : public Communicator {\n \n   absl::Status Barrier(const Executor& executor) final;\n \n-  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllReduce(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, ReductionKind reduction_kind,\n                      const Executor& executor) final;\n \n-  Future<> Broadcast(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> Broadcast(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, RankId root,\n                      const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> ReduceScatter(se::DeviceAddressBase send_buffer,\n+                         se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                          size_t count, ReductionKind reduction_kind,\n                          const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllGather(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n                     PrimitiveType dtype, size_t count,\n                     const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n-                             se::DeviceMemoryBase recv_buffer,\n+  Future<> CollectivePermute(se::DeviceAddressBase send_buffer,\n+                             se::DeviceAddressBase recv_buffer,\n                              PrimitiveType dtype, size_t count,\n                              std::optional<RankId> source_rank,\n                              absl::Span<const RankId> target_ranks,\n                              const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  Future<> Send(se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+  Future<> Send(se::DeviceAddressBase send_buffer, PrimitiveType dtype,\n                 size_t count, RankId peer, const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  Future<> Recv(se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> Recv(se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                 size_t count, RankId peer, const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  Future<> Send(se::DeviceMemoryBase recv_buffer,\n-                se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+  Future<> Send(se::DeviceAddressBase recv_buffer,\n+                se::DeviceAddressBase send_buffer, PrimitiveType dtype,\n                 size_t count, RankId peer, const Executor& executor) final;\n \n-  Future<> Recv(se::DeviceMemoryBase recv_buffer,\n-                se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+  Future<> Recv(se::DeviceAddressBase recv_buffer,\n+                se::DeviceAddressBase send_buffer, PrimitiveType dtype,\n                 size_t count, RankId peer, const Executor& executor) final;\n \n   absl::Status Quiet(const Executor& executor) final;\n@@ -123,8 +123,8 @@ class NvshmemCommunicator : public Communicator {\n \n  private:\n   absl::Status P2P(absl::string_view op_name, PrimitiveType type,\n-                   se::DeviceMemoryBase recv_buffer,\n-                   se::DeviceMemoryBase send_buffer, size_t count, RankId peer,\n+                   se::DeviceAddressBase recv_buffer,\n+                   se::DeviceAddressBase send_buffer, size_t count, RankId peer,\n                    const Executor& executor);\n \n   static absl::StatusOr<se::Stream*> ToStream(const Executor& executor);"
        },
        {
            "sha": "16c7ff9316cc2bff32ffaa6485f9ce7b03aa1af6",
            "filename": "third_party/xla/xla/backends/gpu/ffi.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fffi.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fffi.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fffi.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"xla/ffi/api/c_api.h\"\n #include \"xla/ffi/api/c_api_internal.h\"  // IWYU pragma: keep\n #include \"xla/ffi/ffi.h\"  // IWYU pragma: export\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n \n@@ -38,7 +38,7 @@ namespace xla::ffi {\n \n // Type tag binds to one of the following types defined by XLA:GPU runtime:\n struct Stream {};                    //  `se::Stream*`\n-struct Allocator {};                 //  `se::DeviceMemoryAllocator*`\n+struct Allocator {};                 //  `se::DeviceAddressAllocator*`\n struct ScratchAllocator {};          //  `se::OwningScratchAllocator`\n struct CollectiveParams {};          //  `const xla::gpu::CollectiveParams*`\n struct CollectiveCliqueRequests {};  //  `xla::gpu::CollectiveCliqueRequests*`\n@@ -83,7 +83,7 @@ struct CtxDecoding<PlatformStream<T>> {\n \n template <>\n struct CtxDecoding<Allocator> {\n-  using Type = stream_executor::DeviceMemoryAllocator*;\n+  using Type = stream_executor::DeviceAddressAllocator*;\n \n   static std::optional<Type> Decode(const XLA_FFI_Api* api,\n                                     XLA_FFI_ExecutionContext* ctx,"
        },
        {
            "sha": "bbbdeb708afda83f71a4823cc84d8c7998da0184",
            "filename": "third_party/xla/xla/backends/gpu/profiler/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fprofiler%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fprofiler%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fprofiler%2FBUILD?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -82,7 +82,7 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:launch_dimensions\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\","
        },
        {
            "sha": "1a94b5e1be242bd8eab2f7c3e28a489c897a6036",
            "filename": "third_party/xla/xla/backends/gpu/profiler/kernel_name_tracer_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fprofiler%2Fkernel_name_tracer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fprofiler%2Fkernel_name_tracer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fprofiler%2Fkernel_name_tracer_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels_fatbin.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -82,20 +82,20 @@ class KernelNameTracerTest : public ::testing::Test {\n void LaunchAddI32Kernels(stream_executor::StreamExecutor* executor,\n                          stream_executor::Stream* stream) {\n   using AddI32Kernel =\n-      stream_executor::TypedKernel<stream_executor::DeviceMemory<int>,\n-                                   stream_executor::DeviceMemory<int>,\n-                                   stream_executor::DeviceMemory<int>>;\n+      stream_executor::TypedKernel<stream_executor::DeviceAddress<int>,\n+                                   stream_executor::DeviceAddress<int>,\n+                                   stream_executor::DeviceAddress<int>>;\n   TF_ASSERT_OK_AND_ASSIGN(AddI32Kernel add,\n                           stream_executor::gpu::LoadAddI32TestKernel(executor));\n \n   constexpr int64_t kLength = 4;\n   constexpr int64_t kLengthInBytes = sizeof(int32_t) * kLength;\n \n-  stream_executor::DeviceMemory<int32_t> a =\n+  stream_executor::DeviceAddress<int32_t> a =\n       executor->AllocateArray<int32_t>(kLength, 0);\n-  stream_executor::DeviceMemory<int32_t> b =\n+  stream_executor::DeviceAddress<int32_t> b =\n       executor->AllocateArray<int32_t>(kLength, 0);\n-  stream_executor::DeviceMemory<int32_t> c =\n+  stream_executor::DeviceAddress<int32_t> c =\n       executor->AllocateArray<int32_t>(kLength, 0);\n \n   ASSERT_THAT(stream->Memset32(&a, 1, kLengthInBytes), IsOk());\n@@ -121,20 +121,20 @@ void LaunchAddI32Kernels(stream_executor::StreamExecutor* executor,\n void LaunchCommandBufferThunk(stream_executor::StreamExecutor* executor,\n                               stream_executor::Stream* stream) {\n   using AddI32Kernel =\n-      stream_executor::TypedKernel<stream_executor::DeviceMemory<int>,\n-                                   stream_executor::DeviceMemory<int>,\n-                                   stream_executor::DeviceMemory<int>>;\n+      stream_executor::TypedKernel<stream_executor::DeviceAddress<int>,\n+                                   stream_executor::DeviceAddress<int>,\n+                                   stream_executor::DeviceAddress<int>>;\n   TF_ASSERT_OK_AND_ASSIGN(AddI32Kernel add,\n                           stream_executor::gpu::LoadAddI32TestKernel(executor));\n \n   constexpr int64_t kLength = 4;\n   constexpr int64_t kLengthInBytes = sizeof(int32_t) * kLength;\n \n-  stream_executor::DeviceMemory<int32_t> a =\n+  stream_executor::DeviceAddress<int32_t> a =\n       executor->AllocateArray<int32_t>(kLength, 0);\n-  stream_executor::DeviceMemory<int32_t> b =\n+  stream_executor::DeviceAddress<int32_t> b =\n       executor->AllocateArray<int32_t>(kLength, 0);\n-  stream_executor::DeviceMemory<int32_t> c =\n+  stream_executor::DeviceAddress<int32_t> c =\n       executor->AllocateArray<int32_t>(kLength, 0);\n \n   ASSERT_THAT(stream->Memset32(&a, 1, kLengthInBytes), IsOk());"
        },
        {
            "sha": "1ba07e7410f2c2602556dfdcb73a8a9d4369864d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 78,
            "deletions": 78,
            "changes": 156,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -112,7 +112,7 @@ cc_library(\n         \"//xla/service/gpu:stream_executor_util\",\n         \"//xla/service/gpu/kernels:custom_kernel\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n@@ -161,7 +161,7 @@ xla_test(\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:semantic_version\",\n@@ -248,7 +248,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -301,8 +301,8 @@ xla_test(\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -330,7 +330,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",  # build_cleaner: keep\n         \"//xla/service/gpu:buffer_allocations\",  # build_cleaner: keep\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n@@ -374,9 +374,9 @@ xla_test(\n         \"//xla/service/gpu/kernels:custom_kernel\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:command_buffer\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor:launch_dim\",\n@@ -414,9 +414,9 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/stream_executor:command_buffer\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:kernel_spec\",\n@@ -452,7 +452,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -503,8 +503,8 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:gpu_conv_runner\",\n         \"//xla/service/gpu:stream_executor_util\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:lazy_op_runner\",\n         \"//xla/stream_executor:scratch_allocator\",\n@@ -550,7 +550,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:buffer_assignment_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n@@ -594,7 +594,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -644,8 +644,8 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -718,8 +718,8 @@ cc_library(\n         \"//xla/service:custom_call_status_internal\",\n         \"//xla/service:custom_call_target_registry\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -760,7 +760,7 @@ xla_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -794,8 +794,8 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:buffer_assignment_proto_cc\",\n         \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:fft\",\n         \"//xla/stream_executor:scratch_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -838,7 +838,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt\",\n         \"//xla/tsl/platform:statusor\",\n@@ -877,7 +877,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt\",\n         \"//xla/tsl/platform:errors\",\n@@ -909,8 +909,8 @@ xla_test(\n         \"//xla/service/gpu:cublas_cudnn\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream\",\n@@ -948,8 +948,8 @@ cc_library(\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:gpu_transfer_manager\",\n         \"//xla/service/gpu:io_feed_manager\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -993,7 +993,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:launch_dim\",\n@@ -1073,8 +1073,8 @@ cuda_library(\n     deps = [\n         \"//xla:status_macros\",\n         \"//xla:types\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:scratch_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1095,8 +1095,8 @@ cc_library(\n     hdrs = [\"select_k_exec.h\"],\n     deps = [\n         \"//xla:types\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"@com_google_absl//absl/status\",\n     ],\n@@ -1117,7 +1117,7 @@ xla_test(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -1147,8 +1147,8 @@ cc_library(\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n@@ -1194,7 +1194,7 @@ cc_library(\n         \":thunk\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -1268,8 +1268,8 @@ cc_library(\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:stream\",\n@@ -1319,8 +1319,8 @@ xla_test(\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n         \"//xla/service/gpu:ptx_compile_options_from_debug_options\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -1366,7 +1366,7 @@ cc_library(\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n@@ -1399,7 +1399,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:stream\",\n@@ -1440,8 +1440,8 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:stream\",\n@@ -1480,7 +1480,7 @@ cc_library(\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1547,7 +1547,7 @@ cc_library(\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/concurrency:async_value\",\n@@ -1644,7 +1644,7 @@ cc_library(\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/runtime:device_id\",\n         \"//xla/service:rendezvous\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:gpu_executor_header\",\n         \"//xla/stream_executor/gpu:multicast_memory\",\n@@ -1770,7 +1770,7 @@ cc_library(\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/llvm_ir:llvm_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -1840,7 +1840,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:device_id\",\n         \"//xla/service:computation_placer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1870,7 +1870,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:device_id\",\n         \"//xla/service:computation_placer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1919,7 +1919,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:gpu_norm_runner\",\n         \"//xla/service/gpu:gpu_norm_runner_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:lazy_op_runner\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n@@ -1965,7 +1965,7 @@ cc_library(\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:gpu_transfer_manager\",\n         \"//xla/service/gpu:io_feed_manager\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2028,7 +2028,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n@@ -2101,7 +2101,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:device_id\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -2273,7 +2273,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2331,7 +2331,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2423,7 +2423,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:buffer_assignment_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2488,9 +2488,9 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_handle\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:buffer_comparator_kernel\",\n@@ -2518,8 +2518,8 @@ xla_test(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -2539,7 +2539,7 @@ cc_library(\n     deps = [\n         \"//xla:types\",\n         \"//xla:util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -2558,7 +2558,7 @@ xla_test(\n     deps = [\n         \":make_batch_pointers\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -2581,7 +2581,7 @@ cc_library(\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n@@ -2627,8 +2627,8 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -2663,7 +2663,7 @@ cc_library(\n         \"//xla:types\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -2686,8 +2686,8 @@ xla_test(\n         \":ragged_all_to_all\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -2738,7 +2738,7 @@ xla_test(\n         \"//xla:types\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:platform\",\n@@ -2936,7 +2936,7 @@ cc_library(\n         \"//xla/service:computation_placer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2970,7 +2970,7 @@ cc_library(\n         \"//xla/service:computation_placer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -3108,7 +3108,7 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/cpu:cpu_aot_compilation_result\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n@@ -3409,7 +3409,7 @@ cc_library(\n         \":thunk\",\n         \":thunk_id\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -3450,8 +3450,8 @@ xla_test(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:executable\",\n         \"//xla/service/gpu:buffer_allocations\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:mock_stream\",\n         \"//xla/stream_executor:mock_stream_executor\",\n         \"//xla/stream_executor:platform\",\n@@ -3477,7 +3477,7 @@ cc_library(\n         \":thunk\",\n         \"//xla:types\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -3519,7 +3519,7 @@ xla_test(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:executable\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -3575,7 +3575,7 @@ cc_library(\n     srcs = [\"print_buffer_contents.cc\"],\n     hdrs = [\"print_buffer_contents.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:stream\",\n         \"@com_google_absl//absl/log\",\n@@ -3592,7 +3592,7 @@ xla_test(\n     deps = [\n         \":print_buffer_contents\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -3617,7 +3617,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu/kernels:custom_kernel\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:kernel_argument_packing_spec\","
        },
        {
            "sha": "fad1778441935666e95cb03fd4da45f70b3f2c47",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 24,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -74,15 +74,13 @@ static constexpr int64_t kMaxThreadsPerBlock = 512;\n static constexpr int64_t kWarpSize = 32;\n \n template <typename TagType>\n-absl::Status LaunchTypedKernel(TagType, se::Stream* stream,\n-                               const LaunchDimensions& launch_dimensions,\n-                               se::DeviceMemoryBase symmetric_input_buffer,\n-                               se::DeviceMemoryBase local_input_buffer,\n-                               se::DeviceMemoryBase output_buffer, int64_t rank,\n-                               int64_t num_ranks, int64_t num_elements,\n-                               se::DeviceMemoryBase symmetric_signal_buffer,\n-                               uint32_t signal_value,\n-                               se::DeviceMemoryBase metadata) {\n+absl::Status LaunchTypedKernel(\n+    TagType, se::Stream* stream, const LaunchDimensions& launch_dimensions,\n+    se::DeviceAddressBase symmetric_input_buffer,\n+    se::DeviceAddressBase local_input_buffer,\n+    se::DeviceAddressBase output_buffer, int64_t rank, int64_t num_ranks,\n+    int64_t num_elements, se::DeviceAddressBase symmetric_signal_buffer,\n+    uint32_t signal_value, se::DeviceAddressBase metadata) {\n   using ElementType = typename TagType::ElementType;\n   static constexpr bool kIsTwoShot =\n       TagType::kAllReduceStrategy == AllReduceStrategy::kTwoShot;\n@@ -221,20 +219,20 @@ bool IsAllReduceKernelSupported(int64_t num_ranks, int64_t num_elements,\n }\n \n absl::Status RunAllReduceKernel(\n-    se::Stream* stream,                            //\n-    const LaunchDimensions& launch_dimensions,     //\n-    PrimitiveType element_type,                    //\n-    ReductionKind reduction_kind,                  //\n-    AllReduceStrategy all_reduce_strategy,         //\n-    se::DeviceMemoryBase symmetric_input_buffer,   //\n-    se::DeviceMemoryBase local_input_buffer,       //\n-    se::DeviceMemoryBase output_buffer,            //\n-    RankId rank,                                   //\n-    int64_t num_ranks,                             //\n-    int64_t num_elements,                          //\n-    se::DeviceMemoryBase symmetric_signal_buffer,  //\n-    uint32_t signal_value,                         //\n-    se::DeviceMemoryBase metadata) {\n+    se::Stream* stream,                             //\n+    const LaunchDimensions& launch_dimensions,      //\n+    PrimitiveType element_type,                     //\n+    ReductionKind reduction_kind,                   //\n+    AllReduceStrategy all_reduce_strategy,          //\n+    se::DeviceAddressBase symmetric_input_buffer,   //\n+    se::DeviceAddressBase local_input_buffer,       //\n+    se::DeviceAddressBase output_buffer,            //\n+    RankId rank,                                    //\n+    int64_t num_ranks,                              //\n+    int64_t num_elements,                           //\n+    se::DeviceAddressBase symmetric_signal_buffer,  //\n+    uint32_t signal_value,                          //\n+    se::DeviceAddressBase metadata) {\n   if (!IsAllReduceKernelSupported(num_ranks, num_elements, element_type,\n                                   reduction_kind, all_reduce_strategy)) {\n     return absl::InvalidArgumentError("
        },
        {
            "sha": "d9241cc74c6f27eb3df2f3e1e1ffbdcd98871e0e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -22,7 +22,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/types.h\"  // IWYU pragma: keep\n@@ -88,15 +88,15 @@ absl::Status RunAllReduceKernel(\n     PrimitiveType element_type,                      //\n     ReductionKind reduction_kind,                    //\n     se::gpu::AllReduceStrategy all_reduce_strategy,  //\n-    se::DeviceMemoryBase symmetric_input_buffer,     //\n-    se::DeviceMemoryBase local_input_buffer,         //\n-    se::DeviceMemoryBase output_buffer,              //\n+    se::DeviceAddressBase symmetric_input_buffer,    //\n+    se::DeviceAddressBase local_input_buffer,        //\n+    se::DeviceAddressBase output_buffer,             //\n     RankId rank,                                     //\n     int64_t num_ranks,                               //\n     int64_t num_elements,                            //\n-    se::DeviceMemoryBase symmetric_signal_buffer,    //\n+    se::DeviceAddressBase symmetric_signal_buffer,   //\n     uint32_t signal_value,                           //\n-    se::DeviceMemoryBase metadata                    //\n+    se::DeviceAddressBase metadata                   //\n );\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "df6cc9fee38236b15866ff6bf3c963a67317d45b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -44,7 +44,7 @@ limitations under the License.\n #include \"xla/service/hlo_runner.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n@@ -110,10 +110,10 @@ class AllReduceKernelTest : public ::testing::Test,\n     }\n \n     std::vector<std::unique_ptr<se::Stream>> streams;\n-    std::vector<se::DeviceMemoryBase> allocated_buffers;\n-    std::vector<se::DeviceMemoryBase> local_input_buffers;\n-    std::vector<se::DeviceMemoryBase> data_buffers;\n-    std::vector<se::DeviceMemoryBase> signal_flags_buffers;\n+    std::vector<se::DeviceAddressBase> allocated_buffers;\n+    std::vector<se::DeviceAddressBase> local_input_buffers;\n+    std::vector<se::DeviceAddressBase> data_buffers;\n+    std::vector<se::DeviceAddressBase> signal_flags_buffers;\n \n     uint64_t input_size = num_elements * sizeof(T);\n     uint64_t aligned_input_size =\n@@ -149,15 +149,16 @@ class AllReduceKernelTest : public ::testing::Test,\n                                             input_data[i].data(), input_size));\n     }\n \n-    std::vector<se::DeviceMemoryBase> metadata_buffers;\n+    std::vector<se::DeviceAddressBase> metadata_buffers;\n     // One for signal and one for input parameters.\n     constexpr int kNumPeerParameters = 2;\n     size_t param_to_peers_size = sizeof(void*) * kNumPeerParameters * num_ranks;\n     std::vector<void*> param_to_peers_ptrs;\n-    for (const se::DeviceMemoryBase& local_input_buffer : local_input_buffers) {\n+    for (const se::DeviceAddressBase& local_input_buffer :\n+         local_input_buffers) {\n       param_to_peers_ptrs.push_back(local_input_buffer.opaque());\n     }\n-    for (const se::DeviceMemoryBase& signal_flags_buffer :\n+    for (const se::DeviceAddressBase& signal_flags_buffer :\n          signal_flags_buffers) {\n       param_to_peers_ptrs.push_back(signal_flags_buffer.opaque());\n     }\n@@ -182,7 +183,7 @@ class AllReduceKernelTest : public ::testing::Test,\n       metadata_buffers.emplace_back(executors[i]->AllocateArray<uint64_t>(\n           sizeof(CollectiveKernelMetadata) + param_to_peers_size));\n \n-      se::DeviceMemoryBase param_to_peers_ptrs_buffer =\n+      se::DeviceAddressBase param_to_peers_ptrs_buffer =\n           metadata_buffers[i].GetByteSlice(sizeof(CollectiveKernelMetadata),\n                                            param_to_peers_size);\n       metadata.param_to_peers ="
        },
        {
            "sha": "5bad2a67644375abd95fb04af3aa330542e6c849",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\""
        },
        {
            "sha": "c88e31a35b64a7f3e94f9d65527253e6e522be2c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -47,7 +47,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -173,7 +173,7 @@ absl::Status AllToAllStartThunk::Initialize(const InitializeParams& params) {\n       if (config_.has_split_dimension) {\n         buffer_rendezvous_value.buffer = reinterpret_cast<uint64_t>(\n             GpuCollectives::Slice(\n-                se::DeviceMemoryBase(device_buffers[0].destination_buffer),\n+                se::DeviceAddressBase(device_buffers[0].destination_buffer),\n                 device_buffers[0].element_type,\n                 buffer_idx * chunk_element_count, chunk_element_count)\n                 .opaque());\n@@ -301,8 +301,8 @@ absl::Status RunAllToAll(bool has_split_dimension,\n   // in which case inputs are split and outputs concatenated in that dimension\n   // (here, we only support dimension 0), or it takes a list of inputs\n   // and produces a tuple of outputs.\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers;\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers;\n \n   if (has_split_dimension) {\n     TF_RET_CHECK(element_count % num_ranks == 0)\n@@ -389,11 +389,11 @@ absl::Status RunMemCpyAllToAll(bool has_split_dimension,\n       size_t chunk_element_count = buffer.element_count / num_ranks;\n \n       for (int peer = 0; peer < num_ranks; ++peer) {\n-        se::DeviceMemoryBase send_slice = GpuCollectives::Slice(\n+        se::DeviceAddressBase send_slice = GpuCollectives::Slice(\n             buffer.source_buffer, buffer.element_type,\n             peer * chunk_element_count, chunk_element_count);\n-        se::DeviceMemoryBase dst_addr =\n-            se::DeviceMemoryBase((void*)receive_pointer_map[peer]);\n+        se::DeviceAddressBase dst_addr =\n+            se::DeviceAddressBase((void*)receive_pointer_map[peer]);\n         TF_RETURN_IF_ERROR(\n             stream.MemcpyD2D(&dst_addr, send_slice, send_slice.size()));\n       }\n@@ -405,8 +405,8 @@ absl::Status RunMemCpyAllToAll(bool has_split_dimension,\n     for (int peer = 0; peer < num_ranks; ++peer) {\n       auto buffer_idx = (rank.value() + peer) % num_ranks;\n       // double buffer, exchange data with peer\n-      se::DeviceMemoryBase dst_addr =\n-          se::DeviceMemoryBase((void*)receive_pointer_map[peer]);\n+      se::DeviceAddressBase dst_addr =\n+          se::DeviceAddressBase((void*)receive_pointer_map[peer]);\n       TF_RETURN_IF_ERROR(\n           stream.MemcpyD2D(&dst_addr, buffers[buffer_idx].source_buffer,\n                            buffers[buffer_idx].source_buffer.size()));"
        },
        {
            "sha": "606d4346b7c3b7e0711bcf6be977172308cf26cc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -30,9 +30,9 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/shape.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/buffer_comparator_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -51,8 +51,8 @@ struct ComparisonParams {\n   bool run_host_compare = true;\n   const Shape* shape = nullptr;\n   se::Stream* stream = nullptr;\n-  se::DeviceMemoryBase current{};\n-  se::DeviceMemoryBase expected{};\n+  se::DeviceAddressBase current{};\n+  se::DeviceAddressBase expected{};\n };\n \n // Compares two buffers on the GPU.\n@@ -62,7 +62,7 @@ template <typename ElementT>\n static absl::StatusOr<bool> DeviceCompare(const ComparisonParams& params) {\n   se::StreamExecutor* executor = params.stream->parent();\n \n-  se::DeviceMemoryHandle out(executor, executor->AllocateScalar<uint64_t>());\n+  se::DeviceAddressHandle out(executor, executor->AllocateScalar<uint64_t>());\n \n   TF_RETURN_IF_ERROR(\n       params.stream->MemZero(out.address_ptr(), sizeof(uint64_t)));\n@@ -71,8 +71,8 @@ static absl::StatusOr<bool> DeviceCompare(const ComparisonParams& params) {\n                     params.current.size(), params.expected.size());\n   }\n \n-  se::DeviceMemory<ElementT> current_typed(params.current);\n-  se::DeviceMemory<ElementT> expected_typed(params.expected);\n+  se::DeviceAddress<ElementT> current_typed(params.current);\n+  se::DeviceAddress<ElementT> expected_typed(params.expected);\n   uint64_t buffer_size = current_typed.ElementCount();\n \n   TF_ASSIGN_OR_RETURN(\n@@ -95,7 +95,7 @@ static absl::StatusOr<bool> DeviceCompare(const ComparisonParams& params) {\n                    1, 1),\n       dim.thread_counts_per_block());\n \n-  se::DeviceMemory<uint64_t> as_uint64(out.address());\n+  se::DeviceAddress<uint64_t> as_uint64(out.address());\n   TF_RETURN_IF_ERROR(comparison_kernel.Launch(\n       dim.thread_counts_per_block(), dim.block_counts(), params.stream,\n       current_typed, expected_typed, static_cast<float>(params.relative_tol),\n@@ -183,8 +183,8 @@ static absl::StatusOr<bool> CompareEqualParameterized(\n }\n \n absl::StatusOr<bool> BufferComparator::CompareEqual(\n-    se::Stream* stream, const se::DeviceMemoryBase& current,\n-    const se::DeviceMemoryBase& expected) const {\n+    se::Stream* stream, const se::DeviceAddressBase& current,\n+    const se::DeviceAddressBase& expected) const {\n   ComparisonParams params{relative_tol_, verbose_, run_host_compare_, &shape_,\n                           stream,        current,  expected};\n "
        },
        {
            "sha": "ae7749fb0383dc91ee2f10617282c321fbb2050a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n \n namespace xla::gpu {\n@@ -44,9 +44,9 @@ class BufferComparator {\n   //     abs(a - b) / (max(abs(a), abs(b)) + 1) < tolerance\n   //\n   // See the implementation for the tolerance value.\n-  absl::StatusOr<bool> CompareEqual(se::Stream* stream,\n-                                    const se::DeviceMemoryBase& current,\n-                                    const se::DeviceMemoryBase& expected) const;\n+  absl::StatusOr<bool> CompareEqual(\n+      se::Stream* stream, const se::DeviceAddressBase& current,\n+      const se::DeviceAddressBase& expected) const;\n \n  private:\n   Shape shape_;"
        },
        {
            "sha": "6de03d0cdff964a006877784a13630b4311cb6b2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -30,8 +30,8 @@ limitations under the License.\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -60,9 +60,9 @@ class BufferComparatorTest : public testing::Test {\n                            double tolerance) {\n     auto stream = stream_exec_->CreateStream().value();\n \n-    se::DeviceMemoryHandle current_buffer(\n+    se::DeviceAddressHandle current_buffer(\n         stream_exec_, stream_exec_->AllocateArray<ElementType>(current.size()));\n-    se::DeviceMemoryHandle expected_buffer(\n+    se::DeviceAddressHandle expected_buffer(\n         stream_exec_,\n         stream_exec_->AllocateArray<ElementType>(expected.size()));\n \n@@ -105,9 +105,9 @@ class BufferComparatorTest : public testing::Test {\n                           const ElementType& expected,\n                           double tolerance = kDefaultTolerance) {\n     auto stream = stream_exec_->CreateStream().value();\n-    se::DeviceMemoryHandle current_buffer(\n+    se::DeviceAddressHandle current_buffer(\n         stream_exec_, stream_exec_->AllocateScalar<ElementType>());\n-    se::DeviceMemoryHandle expected_buffer(\n+    se::DeviceAddressHandle expected_buffer(\n         stream_exec_, stream_exec_->AllocateScalar<ElementType>());\n \n     CHECK_OK(stream->Memcpy(current_buffer.memory_ptr(), &current,\n@@ -434,12 +434,12 @@ TEST_F(BufferComparatorTest, BF16) {\n \n   auto stream = stream_exec_->CreateStream().value();\n \n-  se::DeviceMemoryHandle lhs(\n+  se::DeviceAddressHandle lhs(\n       stream_exec_,\n       stream_exec_->AllocateArray<Eigen::bfloat16>(element_count));\n   InitializeBuffer(stream.get(), BF16, &rng_state, lhs.memory());\n \n-  se::DeviceMemoryHandle rhs(\n+  se::DeviceAddressHandle rhs(\n       stream_exec_,\n       stream_exec_->AllocateArray<Eigen::bfloat16>(element_count));\n   InitializeBuffer(stream.get(), BF16, &rng_state, rhs.memory());\n@@ -466,7 +466,7 @@ TEST_F(BufferComparatorTest, VeryLargeArray) {\n \n   // We use overlapping lhs and rhs arrays to reduce memory usage, also this\n   // serves as an extra test for possible pointer aliasing problems.\n-  se::DeviceMemoryBase lhs(base.opaque(), n_elems * sizeof(NT)),\n+  se::DeviceAddressBase lhs(base.opaque(), n_elems * sizeof(NT)),\n       rhs(static_cast<NT*>(base.opaque()) + 1, lhs.size());\n \n   constexpr uint32_t pattern = 0xABABABAB;\n@@ -479,7 +479,7 @@ TEST_F(BufferComparatorTest, VeryLargeArray) {\n                               /*run_host_compare*/ false);\n   EXPECT_TRUE(comparator.CompareEqual(stream.get(), lhs, rhs).value());\n \n-  se::DeviceMemoryBase last_word(\n+  se::DeviceAddressBase last_word(\n       static_cast<uint8_t*>(base.opaque()) + (n_elems & ~3), sizeof(uint32_t));\n   // Change only the very last entry of rhs to verify that the whole arrays are\n   // compared (if the grid dimensions are not computed correctly, this might"
        },
        {
            "sha": "d7a4364bc841e3fdd541331f1bcb954bac293a0b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -105,7 +105,7 @@ absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n   const se::ThreadDim thread_dim(\n       executor->GetDeviceDescription().threads_per_block_limit(), 1, 1);\n \n-  se::DeviceMemory<uint8_t> log_ptr(\n+  se::DeviceAddress<uint8_t> log_ptr(\n       params.buffer_allocations->GetDeviceAddress(log_slice_));\n   auto buffer_debug_log =\n       se::gpu::BufferDebugLog<BufferDebugLogEntry>::FromDeviceAddressUnchecked(\n@@ -122,7 +122,7 @@ absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n     const BufferDebugLogEntryId log_entry_id =\n         metadata_store_->AssignId(metadata);\n \n-    se::DeviceMemory<uint8_t> device_buffer(\n+    se::DeviceAddress<uint8_t> device_buffer(\n         params.buffer_allocations->GetDeviceAddress(buffer));\n \n     TF_RETURN_IF_ERROR(kernel->Launch("
        },
        {
            "sha": "47208ddc437641ee34313e454ce110fe30f872c5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -33,8 +33,8 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/service_executable_run_options.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -134,13 +134,13 @@ TEST_F(BuffersDebugChecksumThunkTest, CalculatesChecksums) {\n   BufferAllocations allocations(\n       {executor_->AllocateArray<uint8_t>(kTotalDeviceMemoryBytes)},\n       executor_->device_ordinal(), allocator_.get());\n-  se::DeviceMemoryBase log_mem = allocations.GetDeviceAddress(log_slice);\n-  se::DeviceMemoryBase inputs0_mem = allocations.GetDeviceAddress(inputs[0]);\n-  se::DeviceMemoryBase inputs1_mem = allocations.GetDeviceAddress(inputs[1]);\n+  se::DeviceAddressBase log_mem = allocations.GetDeviceAddress(log_slice);\n+  se::DeviceAddressBase inputs0_mem = allocations.GetDeviceAddress(inputs[0]);\n+  se::DeviceAddressBase inputs1_mem = allocations.GetDeviceAddress(inputs[1]);\n   // Initialize the log in device memory\n   TF_ASSERT_OK_AND_ASSIGN(auto device_log,\n                           BufferDebugLog<BufferDebugLogEntry>::CreateOnDevice(\n-                              *stream_, se::DeviceMemory<uint8_t>(log_mem)));\n+                              *stream_, se::DeviceAddress<uint8_t>(log_mem)));\n   // Fill inputs with some data\n   std::vector<uint32_t> zeros(1024, 0);\n   zeros[123] = 12341234;  // expected checksum for inputs_mem[0]"
        },
        {
            "sha": "6ff174e2a418d2465b4cb5ee97e269193c1b4399",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_float_check_kernel.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -105,7 +105,7 @@ absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n   const se::ThreadDim thread_dim(\n       executor->GetDeviceDescription().threads_per_block_limit(), 1, 1);\n \n-  se::DeviceMemory<uint8_t> log_ptr(\n+  se::DeviceAddress<uint8_t> log_ptr(\n       params.buffer_allocations->GetDeviceAddress(log_slice_));\n   se::gpu::BufferDebugLog<BufferDebugFloatCheckEntry> buffer_debug_log =\n       se::gpu::BufferDebugLog<\n@@ -124,20 +124,20 @@ absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n     const BufferDebugLogEntryId entry_id = metadata_store_->AssignId(metadata);\n \n     PrimitiveType buffer_type = buffer.element_type();\n-    se::DeviceMemoryBase device_buffer =\n+    se::DeviceAddressBase device_buffer =\n         params.buffer_allocations->GetDeviceAddress(buffer);\n     if (buffer_type == PrimitiveType::F32) {\n       VLOG(1) << \"F32 buffer detected with id: \" << entry_id\n               << \" and size: \" << device_buffer.size();\n-      se::DeviceMemory<float> f32_buffer(device_buffer);\n+      se::DeviceAddress<float> f32_buffer(device_buffer);\n       TF_RETURN_IF_ERROR(kernels->f32.Launch(\n           thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n           f32_buffer, f32_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n           buffer_debug_log.GetDeviceEntries()));\n     } else if (buffer_type == PrimitiveType::BF16) {\n       VLOG(1) << \"BF16 buffer detected with id: \" << entry_id\n               << \" and size: \" << device_buffer.size();\n-      se::DeviceMemory<Eigen::bfloat16> bf16_buffer(device_buffer);\n+      se::DeviceAddress<Eigen::bfloat16> bf16_buffer(device_buffer);\n       TF_RETURN_IF_ERROR(kernels->bf16.Launch(\n           thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n           bf16_buffer, bf16_buffer.size(), buffer_debug_log.GetDeviceHeader(),"
        },
        {
            "sha": "dfb933bce2a4eef249913746e1dcd0fa6d06ee60",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/service_executable_run_options.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -125,14 +125,14 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n   BufferAllocations allocations(\n       {executor_->AllocateArray<uint8_t>(kTotalDeviceMemoryBytes)},\n       executor_->device_ordinal(), allocator_.get());\n-  se::DeviceMemoryBase log_mem = allocations.GetDeviceAddress(log_slice);\n-  se::DeviceMemoryBase inputs0_mem = allocations.GetDeviceAddress(inputs[0]);\n-  se::DeviceMemoryBase inputs1_mem = allocations.GetDeviceAddress(inputs[1]);\n+  se::DeviceAddressBase log_mem = allocations.GetDeviceAddress(log_slice);\n+  se::DeviceAddressBase inputs0_mem = allocations.GetDeviceAddress(inputs[0]);\n+  se::DeviceAddressBase inputs1_mem = allocations.GetDeviceAddress(inputs[1]);\n   // Initialize the log in device memory\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto device_log,\n       BufferDebugLog<BufferDebugFloatCheckEntry>::CreateOnDevice(\n-          *stream_, se::DeviceMemory<uint8_t>(log_mem)));\n+          *stream_, se::DeviceAddress<uint8_t>(log_mem)));\n   // Fill inputs with some data\n   {\n     std::vector<Eigen::bfloat16> data(kInputElems, Eigen::bfloat16(0));"
        },
        {
            "sha": "db9b28fc46868e00b55e5a2bdefa484b301a933e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_broadcast_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -79,8 +79,8 @@ absl::Status RunCollectiveBroadcast(std::vector<DeviceBufferPair>& buffers,\n   Future<> future = gpu_comm->GroupExecute(\n       [&buffers, &stream](GpuCommunicator* comm) -> absl::Status {\n         for (auto buffer : buffers) {\n-          se::DeviceMemoryBase src_addr = buffer.source_buffer;\n-          se::DeviceMemoryBase dest_addr = buffer.destination_buffer;\n+          se::DeviceAddressBase src_addr = buffer.source_buffer;\n+          se::DeviceAddressBase dest_addr = buffer.destination_buffer;\n           TF_RETURN_IF_ERROR(comm->LaunchBroadcast(\n               // Always use rank 0 since we always broadcast from the first id\n               // in replica_groups"
        },
        {
            "sha": "345d6148d187135989dc907532950377c9cb7923",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -44,8 +44,8 @@ limitations under the License.*/\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -72,10 +72,10 @@ static constexpr int32_t kAllReduceArgsCount = 6;\n static constexpr int32_t kNumParameters = 2;\n \n // Helper for allocating memory on the device.\n-absl::StatusOr<se::DeviceMemoryHandle> AllocateMemory(\n+absl::StatusOr<se::DeviceAddressHandle> AllocateMemory(\n     se::StreamExecutor* executor, int64_t size,\n     absl::string_view debug_buffer_name) {\n-  se::DeviceMemoryHandle local_buffer_alloc(\n+  se::DeviceAddressHandle local_buffer_alloc(\n       executor,\n       executor->Allocate(\n           size, static_cast<int64_t>(stream_executor::MemoryType::kP2P)));\n@@ -146,7 +146,7 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n       << \"Device \" << params.collective_params->global_device_id\n       << \"is not in the clique.\";\n \n-  std::vector<se::DeviceMemoryBase> parameters{\n+  std::vector<se::DeviceAddressBase> parameters{\n       state.local_buffers_handle.memory(),\n       state.signal_buffers_handle.memory()};\n   TF_RET_CHECK(parameters.size() == kNumParameters);\n@@ -190,12 +190,12 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n           buffers_[0].source_buffer.size(), kXlaAllocatedBufferAlignBytes);\n \n       TF_ASSIGN_OR_RETURN(\n-          se::DeviceMemoryHandle local_buffers_handle,\n+          se::DeviceAddressHandle local_buffers_handle,\n           AllocateMemory(params.executor, kLocalBufferSize * kNumBuffers,\n                          \"Local buffers\"));\n \n       TF_ASSIGN_OR_RETURN(\n-          se::DeviceMemoryHandle signal_buffers_handle,\n+          se::DeviceAddressHandle signal_buffers_handle,\n           AllocateMemory(params.executor, kSignalBufferSize * kNumBuffers,\n                          \"Signal buffers\"));\n \n@@ -287,9 +287,9 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n   }\n   const CollectiveThunk::Buffer& buffer = buffers_[0];\n   const PrimitiveType element_type = collective_config_.operand_element_type[0];\n-  se::DeviceMemoryBase source_buffer =\n+  se::DeviceAddressBase source_buffer =\n       params.buffer_allocations->GetDeviceAddress(buffer.source_buffer);\n-  se::DeviceMemoryBase destination_buffer =\n+  se::DeviceAddressBase destination_buffer =\n       params.buffer_allocations->GetDeviceAddress(buffer.destination_buffer);\n \n   const std::optional<RankId> rank =\n@@ -316,9 +316,9 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n   XLA_VLOG_DEVICE(3, device_ordinal)\n       << \"Performing one-shot all-reduce for clique \" << clique_key.ToString();\n \n-  se::DeviceMemoryBase input_buffer_ptr =\n+  se::DeviceAddressBase input_buffer_ptr =\n       state->remote_buffer_ptrs[buffer_index];\n-  se::DeviceMemoryBase signal_buffer_ptr =\n+  se::DeviceAddressBase signal_buffer_ptr =\n       state->signal_buffer_ptrs[buffer_index];\n   XLA_VLOG_DEVICE(3, device_ordinal)\n       << \"input_buffer_ptr: \" << input_buffer_ptr.opaque()\n@@ -329,12 +329,12 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n       << \"(block x threadsPerBlock)\";\n \n   if (state->kernel != nullptr) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase remote_buffers,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase remote_buffers,\n                         CollectiveMetadataThunk::GetParameterDeviceMemoryBase(\n                             state->metadata, /*num_parameters=*/kNumParameters,\n                             /*num_devices=*/num_devices,\n                             /*parameter_index=*/0));\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase signal_buffers,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase signal_buffers,\n                         CollectiveMetadataThunk::GetParameterDeviceMemoryBase(\n                             state->metadata, /*num_parameters=*/kNumParameters,\n                             /*num_devices=*/num_devices,"
        },
        {
            "sha": "8350a7de83b7c1bc952722fd6efc69a2213a6d24",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -38,8 +38,8 @@ limitations under the License.*/\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -112,20 +112,20 @@ class CollectiveKernelThunk : public Thunk {\n     //   This implies that all GPUs must have finished the first invocation\n     //   before they can sync on the second invocation.\n     // - Alternate back to Buffer 0 on third invocation. And so on.\n-    se::DeviceMemoryHandle local_buffers_handle;\n+    se::DeviceAddressHandle local_buffers_handle;\n \n     // Signal buffers allocated for the collective.\n     // Also double buffered for the same reason as local buffers.\n-    se::DeviceMemoryHandle signal_buffers_handle;\n+    se::DeviceAddressHandle signal_buffers_handle;\n \n     // Pointer to the collective kernel metadata on device.\n-    se::DeviceMemoryBase metadata;\n+    se::DeviceAddressBase metadata;\n \n     // These vectors are merely pointers into the buffer(s) above ordered\n     // by RankId. They are initialized once at the end of Initialize() and never\n     // changed.\n-    std::array<se::DeviceMemoryBase, kNumBuffers> remote_buffer_ptrs;\n-    std::array<se::DeviceMemoryBase, kNumBuffers> signal_buffer_ptrs;\n+    std::array<se::DeviceAddressBase, kNumBuffers> remote_buffer_ptrs;\n+    std::array<se::DeviceAddressBase, kNumBuffers> signal_buffer_ptrs;\n     // Kernel entry for the stream executor.\n     std::unique_ptr<se::Kernel> kernel;\n     uint32_t invocation_count = 0;\n@@ -136,8 +136,8 @@ class CollectiveKernelThunk : public Thunk {\n     // Constructor to make OSS builds happy.\n     StreamState() = default;\n     StreamState(int device_ordinal_arg, RankId rank_arg,\n-                se::DeviceMemoryHandle local_buffers_handle_arg,\n-                se::DeviceMemoryHandle signal_buffers_handle_arg,\n+                se::DeviceAddressHandle local_buffers_handle_arg,\n+                se::DeviceAddressHandle signal_buffers_handle_arg,\n                 std::unique_ptr<se::Kernel> kernel_arg)\n         : device_ordinal(device_ordinal_arg),\n           rank(rank_arg),"
        },
        {
            "sha": "993b4d0cc06b0df50bcc5b577b3aa34c737e1aaf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -43,8 +43,8 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/compilation_provider.h\"\n #include \"xla/stream_executor/cuda/compilation_provider_options.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -227,7 +227,7 @@ absl::StatusOr<std::vector<uint8_t>> CompilePtxToCubin(\n   return std::move(assembly.cubin);\n }\n \n-absl::StatusOr<se::DeviceMemoryBase> RunCollectiveKernelThunk(\n+absl::StatusOr<se::DeviceAddressBase> RunCollectiveKernelThunk(\n     CollectiveKernelThunkMetadata& metadata, se::StreamExecutor* executor,\n     std::vector<uint64_t> input_data, bool emulate_multiprocess = false) {\n   BufferAllocation buffer_allocation(\n@@ -255,12 +255,12 @@ absl::StatusOr<se::DeviceMemoryBase> RunCollectiveKernelThunk(\n       auto collective_params,\n       CollectiveParams::Create(run_options, /*async_streams=*/{},\n                                LocalDeviceId(executor->device_ordinal())));\n-  std::vector<se::DeviceMemoryBase> allocated_buffers = {\n+  std::vector<se::DeviceAddressBase> allocated_buffers = {\n       executor->AllocateArray<uint64_t>(metadata.total_buffer_size)};\n \n-  se::DeviceMemoryBase input_buffer =\n+  se::DeviceAddressBase input_buffer =\n       allocated_buffers[0].GetByteSlice(0, metadata.aligned_input_size_bytes);\n-  se::DeviceMemoryBase output_buffer = allocated_buffers[0].GetByteSlice(\n+  se::DeviceAddressBase output_buffer = allocated_buffers[0].GetByteSlice(\n       metadata.aligned_input_size_bytes, metadata.aligned_input_size_bytes);\n   BufferAllocations buffer_allocations(\n       /*buffers=*/allocated_buffers,\n@@ -306,15 +306,15 @@ absl::StatusOr<se::DeviceMemoryBase> RunCollectiveKernelThunk(\n   return output_buffer;\n }\n \n-std::vector<absl::StatusOr<se::DeviceMemoryBase>>\n+std::vector<absl::StatusOr<se::DeviceAddressBase>>\n RunCollectiveKernelThunkOnDevices(CollectiveKernelThunkMetadata& metadata,\n                                   bool emulate_multiprocess = false) {\n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"device_threads\",\n                                       metadata.num_devices);\n-  std::vector<tsl::Future<se::DeviceMemoryBase>> futures;\n+  std::vector<tsl::Future<se::DeviceAddressBase>> futures;\n   for (int device_number = 0; device_number < metadata.num_devices;\n        ++device_number) {\n-    futures.push_back(tsl::Future<se::DeviceMemoryBase>::MakeOn(\n+    futures.push_back(tsl::Future<se::DeviceAddressBase>::MakeOn(\n         *thread_pool.AsExecutor(),\n         [&metadata, device_number, emulate_multiprocess] {\n           return RunCollectiveKernelThunk(metadata,\n@@ -323,7 +323,7 @@ RunCollectiveKernelThunkOnDevices(CollectiveKernelThunkMetadata& metadata,\n         }));\n   }\n \n-  std::vector<absl::StatusOr<se::DeviceMemoryBase>> results;\n+  std::vector<absl::StatusOr<se::DeviceAddressBase>> results;\n   for (auto& future : futures) {\n     results.push_back(std::move(future).Await());\n   }\n@@ -352,7 +352,7 @@ TEST_P(CollectiveKernelThunkParameterizedTest, ExecutesPtxKernel) {\n \n   se::StreamExecutor* executor0 = GetGpuExecutor(0);\n   TF_ASSERT_OK_AND_ASSIGN(\n-      se::DeviceMemoryBase result_buffer,\n+      se::DeviceAddressBase result_buffer,\n       RunCollectiveKernelThunk(metadata, executor0, input_data));\n \n   std::vector<uint64_t> output_data(kNumElements);\n@@ -379,7 +379,7 @@ TEST(CollectiveKernelThunkTest, MultimemSetupTest) {\n   CollectiveKernelThunkMetadata metadata = CreateCollectiveKernelThunk(\n       /*num_devices=*/kDevicesCount, /*num_elements=*/kNumElements,\n       /*is_multimem_enabled=*/true, /*use_ptx=*/true);\n-  for (absl::StatusOr<se::DeviceMemoryBase> result :\n+  for (absl::StatusOr<se::DeviceAddressBase> result :\n        RunCollectiveKernelThunkOnDevices(metadata)) {\n     TF_ASSERT_OK(result);\n   }\n@@ -391,7 +391,7 @@ TEST(CollectiveKernelThunkTest, MultiprocessTest) {\n   CollectiveKernelThunkMetadata metadata = CreateCollectiveKernelThunk(\n       /*num_devices=*/kDevicesCount, /*num_elements=*/kNumElements,\n       /*is_multimem_enabled=*/false, /*use_ptx=*/true);\n-  for (absl::StatusOr<se::DeviceMemoryBase> result :\n+  for (absl::StatusOr<se::DeviceAddressBase> result :\n        RunCollectiveKernelThunkOnDevices(metadata,\n                                          /*emulate_multiprocess=*/true)) {\n     EXPECT_THAT(result, StatusIs(absl::StatusCode::kUnimplemented));"
        },
        {
            "sha": "ae4757dec337faa71e7ed06e3e985f4c45753061",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -82,7 +82,7 @@ CollectiveConfig CollectiveMetadataThunk::GetCollectiveConfig(\n \n struct DeviceParameters {\n   RankId rank;\n-  std::vector<se::DeviceMemoryBase> parameters;\n+  std::vector<se::DeviceAddressBase> parameters;\n \n   bool operator<(const DeviceParameters& other) const {\n     return rank < other.rank;\n@@ -91,7 +91,7 @@ struct DeviceParameters {\n \n absl::StatusOr<std::vector<DeviceParameters>> SyncLocalDeviceParameters(\n     const GpuCliqueKey& clique_key, RankId rank,\n-    std::vector<se::DeviceMemoryBase> parameters) {\n+    std::vector<se::DeviceAddressBase> parameters) {\n   std::vector<DeviceParameters> device_parameters;\n   auto rendezvous_fn = [](absl::Span<const DeviceParameters* const> values) {\n     std::vector<DeviceParameters> values_copy;\n@@ -124,7 +124,7 @@ absl::StatusOr<std::vector<DeviceParameters>> SyncLocalDeviceParameters(\n \n absl::StatusOr<std::vector<DeviceParameters>> SyncGlobalDeviceParameters(\n     const GpuCliqueKey& clique_key, RankId rank,\n-    std::vector<se::DeviceMemoryBase> parameters) {\n+    std::vector<se::DeviceAddressBase> parameters) {\n   if (!clique_key.is_local()) {\n     return Unimplemented(\n         \"[rank=%d] Multiprocess collective metadata is not supported yet in \"\n@@ -141,9 +141,9 @@ absl::StatusOr<std::vector<DeviceParameters>> SyncGlobalDeviceParameters(\n \n absl::Status CollectiveMetadataThunk::ConstructCollectiveMetadata(\n     const GpuCliqueKey& clique_key, RankId rank, se::Stream* stream,\n-    std::vector<se::DeviceMemoryBase> parameters,\n+    std::vector<se::DeviceAddressBase> parameters,\n     std::shared_ptr<CollectiveMultimem> multimem,\n-    se::DeviceMemoryBase destination) {\n+    se::DeviceAddressBase destination) {\n   CollectiveKernelMetadata metadata;\n   metadata.rank = rank.value();\n   metadata.multicast_buffer_ptr =\n@@ -169,7 +169,7 @@ absl::Status CollectiveMetadataThunk::ConstructCollectiveMetadata(\n \n   const int64_t param_to_peers_ptrs_size =\n       param_to_peers_ptrs.size() * sizeof(void*);\n-  se::DeviceMemoryBase param_to_peers_ptrs_buffer = destination.GetByteSlice(\n+  se::DeviceAddressBase param_to_peers_ptrs_buffer = destination.GetByteSlice(\n       sizeof(CollectiveKernelMetadata), param_to_peers_ptrs_size);\n \n   metadata.param_to_peers =\n@@ -183,9 +183,9 @@ absl::Status CollectiveMetadataThunk::ConstructCollectiveMetadata(\n   return stream->BlockHostUntilDone();\n }\n \n-/* static */ absl::StatusOr<se::DeviceMemoryBase>\n+/* static */ absl::StatusOr<se::DeviceAddressBase>\n CollectiveMetadataThunk::GetParameterDeviceMemoryBase(\n-    const se::DeviceMemoryBase metadata, const int64_t num_parameters,\n+    const se::DeviceAddressBase metadata, const int64_t num_parameters,\n     const int64_t num_devices, const int64_t parameter_index) {\n   TF_RET_CHECK(parameter_index >= 0 && parameter_index < num_parameters)\n       << \"Parameter index \" << parameter_index << \" is out of bounds [0, \"\n@@ -194,7 +194,7 @@ CollectiveMetadataThunk::GetParameterDeviceMemoryBase(\n   // P0R0 P0R1 ... P0Rn P1R0\n   // P1R1 ... P1Rn ... PnRn\n   // Where Pn is the parameter index and Rn is the rank.\n-  se::DeviceMemoryBase ptr_table_base = metadata.GetByteSlice(\n+  se::DeviceAddressBase ptr_table_base = metadata.GetByteSlice(\n       sizeof(CollectiveKernelMetadata),\n       /*size_bytes=*/num_parameters * num_devices * sizeof(void*));\n   return ptr_table_base.GetByteSlice(\n@@ -213,13 +213,13 @@ absl::Status CollectiveMetadataThunk::Initialize(\n                sizeof(CollectiveKernelMetadata) +\n                    num_ranks * parameters_.size() * sizeof(uint64_t));\n \n-  std::vector<se::DeviceMemoryBase> parameters;\n+  std::vector<se::DeviceAddressBase> parameters;\n   parameters.reserve(parameters_.size());\n   for (const Buffer& parameter : parameters_) {\n     parameters.push_back(\n         params.buffer_allocations->GetDeviceAddress(parameter.slice));\n   }\n-  se::DeviceMemoryBase result_ptr =\n+  se::DeviceAddressBase result_ptr =\n       params.buffer_allocations->GetDeviceAddress(result_);\n \n   GlobalDeviceId global_device_id = params.collective_params->global_device_id;\n@@ -242,7 +242,7 @@ absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n CollectiveMetadataThunk::AllocateMultimem(const GpuCliqueKey& clique_key,\n                                           RankId rank,\n                                           const InitializeParams& params) {\n-  se::DeviceMemoryBase memory_range;\n+  se::DeviceAddressBase memory_range;\n   for (const Buffer& parameter : parameters_) {\n     if (parameter.memory_space == xla::Layout::kGenericFastMemorySpace) {\n       TF_ASSIGN_OR_RETURN("
        },
        {
            "sha": "73a36ca88102feb8ff2e1430dc855015c62118ee",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n \n namespace xla::gpu {\n@@ -64,14 +64,14 @@ class CollectiveMetadataThunk : public Thunk {\n   // metadata.\n   static absl::Status ConstructCollectiveMetadata(\n       const GpuCliqueKey& clique_key, RankId rank, se::Stream* stream,\n-      std::vector<se::DeviceMemoryBase> parameters,\n+      std::vector<se::DeviceAddressBase> parameters,\n       std::shared_ptr<CollectiveMultimem> multimem,\n-      se::DeviceMemoryBase destination);\n+      se::DeviceAddressBase destination);\n \n   // Calculate the device memory base for the given parameter index.\n   // The size of the returned memory is num_devices pointers.\n-  static absl::StatusOr<se::DeviceMemoryBase> GetParameterDeviceMemoryBase(\n-      se::DeviceMemoryBase metadata, int64_t num_parameters,\n+  static absl::StatusOr<se::DeviceAddressBase> GetParameterDeviceMemoryBase(\n+      se::DeviceAddressBase metadata, int64_t num_parameters,\n       int64_t num_devices, int64_t parameter_index);\n \n  private:"
        },
        {
            "sha": "bbf2535d5f12167b7534b2ada0a020d3b1fc3e8c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -35,7 +35,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/rendezvous.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n #include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -73,7 +73,7 @@ struct AllocateRendezvousKey {\n struct AllocateParams {\n   se::StreamExecutor* executor;\n   RankId rank;\n-  se::DeviceMemoryBase map_to;\n+  se::DeviceAddressBase map_to;\n   std::any payload;\n };\n \n@@ -102,7 +102,7 @@ struct MappedPtrFormatter {\n absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n                              const GpuCliqueKey& clique_key, RankId rank,\n-                             se::DeviceMemoryBase map_to, std::any payload) {\n+                             se::DeviceAddressBase map_to, std::any payload) {\n   VLOG(3) << absl::StrFormat(\n       \"rank=[%d] Allocate collective multimem for clique: %s\", rank.value(),\n       clique_key.ToString());\n@@ -185,7 +185,7 @@ absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n                              const GpuCliqueKey& clique_key,\n                              GlobalDeviceId global_device_id,\n-                             se::DeviceMemoryBase map_to, std::any payload) {\n+                             se::DeviceAddressBase map_to, std::any payload) {\n   if (std::optional<RankId> rank = clique_key.rank(global_device_id)) {\n     return Allocate(executor, clique_key, *rank, map_to, std::move(payload));\n   }"
        },
        {
            "sha": "20ce764570c53f6ed1e23e118e18ec0078dec5e2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/runtime/device_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/util.h\"  // IWYU pragma: keep\n@@ -53,12 +53,12 @@ class CollectiveMultimem {\n   // to allow callers to associate arbitrary data with the collective multimem.\n   static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n       se::StreamExecutor* executor, const GpuCliqueKey& clique_key, RankId rank,\n-      se::DeviceMemoryBase map_to, std::any payload = {});\n+      se::DeviceAddressBase map_to, std::any payload = {});\n \n   // Allocates a CollectiveMultimem for the given global device id.\n   static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n       se::StreamExecutor* executor, const GpuCliqueKey& clique_key,\n-      GlobalDeviceId global_device_id, se::DeviceMemoryBase map_to,\n+      GlobalDeviceId global_device_id, se::DeviceAddressBase map_to,\n       std::any payload = {});\n \n   const GpuCliqueKey& clique_key() const { return clique_key_; }"
        },
        {
            "sha": "f50e40992f8292d42f9645eb92ff29a8b4bfdb06",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -49,7 +49,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/rendezvous.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -202,15 +202,15 @@ absl::Status CollectivePermuteStartThunk::Initialize(\n     TF_RETURN_IF_ERROR(recv_ptr_map_.InitializeId(current_id));\n \n     if (source_id) {\n-      std::vector<se::DeviceMemoryBase> dest_addrs;\n+      std::vector<se::DeviceAddressBase> dest_addrs;\n       absl::c_transform(device_buffers, std::back_inserter(dest_addrs),\n                         [](const DeviceBufferPair& buffer) {\n                           return buffer.destination_buffer;\n                         });\n       std::vector<void*> dest_opaques;\n       absl::c_transform(\n           dest_addrs, std::back_inserter(dest_opaques),\n-          [](se::DeviceMemoryBase dest_addr) { return dest_addr.opaque(); });\n+          [](se::DeviceAddressBase dest_addr) { return dest_addr.opaque(); });\n       TF_RETURN_IF_ERROR(recv_ptr_map_.PutRecvPtr(current_id, dest_opaques));\n     }\n   }\n@@ -368,7 +368,7 @@ absl::Status RunCollectivePermute(\n   std::optional<int64_t> source_id = source_target.source;\n   std::optional<int64_t> target_id = source_target.target;\n \n-  std::vector<se::DeviceMemoryBase> src_addrs, dest_addrs;\n+  std::vector<se::DeviceAddressBase> src_addrs, dest_addrs;\n   absl::c_transform(\n       buffers, std::back_inserter(src_addrs),\n       [](const DeviceBufferPair& buffer) { return buffer.source_buffer; });\n@@ -431,7 +431,7 @@ absl::Status RunCollectivePermute(\n     // buffer.\n     VLOG(3) << absl::StreamFormat(\"%s : collective-Permute: Issuing MemZero\",\n                                   device_string);\n-    for (se::DeviceMemoryBase& dest_addr : dest_addrs) {\n+    for (se::DeviceAddressBase& dest_addr : dest_addrs) {\n       TF_RETURN_IF_ERROR(stream.MemZero(&dest_addr, dest_addr.size()));\n     }\n   }\n@@ -446,8 +446,8 @@ absl::Status RunCollectivePermute(\n \n     VLOG(3) << current_id << \" initiating memcpy to \" << *target_id;\n     for (uint64_t idx = 0; idx < buffers.size(); ++idx) {\n-      se::DeviceMemoryBase dst_addr =\n-          se::DeviceMemoryBase(recv_ptrs.get().at(idx));\n+      se::DeviceAddressBase dst_addr =\n+          se::DeviceAddressBase(recv_ptrs.get().at(idx));\n       auto src_addr = src_addrs.at(idx);\n       TF_RETURN_IF_ERROR(\n           stream.MemcpyD2D(&dst_addr, src_addr, src_addr.size()));"
        },
        {
            "sha": "f67cca8c784ad3cc4c51ca32540c0c743c5e2f7f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -50,7 +50,7 @@ limitations under the License.\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -214,7 +214,7 @@ absl::StatusOr<std::vector<DeviceBufferPair>> ConvertToDeviceBuffers(\n }\n \n absl::Status MaybeRegisterBuffer(se::StreamExecutor* executor,\n-                                 const se::DeviceMemoryBase& buffer,\n+                                 const se::DeviceAddressBase& buffer,\n                                  Communicator* comm,\n                                  bool use_symmetric_buffer) {\n   TF_ASSIGN_OR_RETURN(auto range, executor->GetMemoryRange(buffer));"
        },
        {
            "sha": "ba936ac9d9d472c1f1e785cc40d694a7b27bb497",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -271,8 +271,8 @@ absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n struct DeviceBufferPair {\n   PrimitiveType element_type;\n   int64_t element_count;\n-  se::DeviceMemoryBase source_buffer;\n-  se::DeviceMemoryBase destination_buffer;\n+  se::DeviceAddressBase source_buffer;\n+  se::DeviceAddressBase destination_buffer;\n   int64_t source_memory_space;\n   int64_t destination_memory_space;\n };"
        },
        {
            "sha": "37428162a4b910b81ae42f4235e7bd9d2d170043",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 39,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -86,7 +86,7 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -935,7 +935,7 @@ absl::StatusOr<se::CommandBuffer*> TracedCommandBuffer::GetOrTraceCommandBuffer(\n     se::Stream* stream, absl::FunctionRef<absl::Status(se::Stream*)> trace,\n     se::StreamPriority priority) {\n   // Collect memory addresses for relevant allocations.\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> allocs;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> allocs;\n   allocs.reserve(allocs_indices_.size());\n   for (auto& index : allocs_indices_) {\n     allocs.emplace_back(buffer_allocation->GetDeviceAddress(index));\n@@ -1097,7 +1097,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> ComputationIdCmd::Record(\n     const Thunk::ExecuteParams& execute_params,\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase dst =\n+  se::DeviceAddressBase dst =\n       execute_params.buffer_allocations->GetDeviceAddress(dest_);\n \n   GlobalDeviceId global_device_id =\n@@ -1193,7 +1193,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> LaunchCmd::Record(\n       tma_metadata_.value_or(se::gpu::TmaMetadata{});\n   for (int idx = 0; idx < args_.size(); ++idx) {\n     const BufferAllocation::Slice& arg = args_[idx];\n-    se::DeviceMemoryBase buf =\n+    se::DeviceAddressBase buf =\n         execute_params.buffer_allocations->GetDeviceAddress(arg);\n     VLOG(5) << \"  Arg: \" << arg << \": \" << buf.opaque();\n \n@@ -1286,9 +1286,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> CustomKernelLaunchCmd::Record(\n                      custom_kernel_.name()));\n   }\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> buffers;\n   for (const BufferAllocation::Slice& arg : args_) {\n-    se::DeviceMemoryBase buf =\n+    se::DeviceAddressBase buf =\n         execute_params.buffer_allocations->GetDeviceAddress(arg);\n     VLOG(5) << \"  Arg: \" << arg << \": \" << buf.opaque();\n     buffers.push_back(buf);\n@@ -1336,9 +1336,9 @@ MemcpyDeviceToDeviceCmd::Record(const Thunk::ExecuteParams& execute_params,\n                                 const RecordParams& record_params,\n                                 RecordAction record_action,\n                                 se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase dst =\n+  se::DeviceAddressBase dst =\n       execute_params.buffer_allocations->GetDeviceAddress(dst_);\n-  se::DeviceMemoryBase src =\n+  se::DeviceAddressBase src =\n       execute_params.buffer_allocations->GetDeviceAddress(src_);\n \n   VLOG(5) << \"MemcpyDeviceToDeviceCmd: num_bytes = \" << num_bytes_;\n@@ -1376,7 +1376,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> MemzeroCmd::Record(\n     const Thunk::ExecuteParams& execute_params,\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase dst =\n+  se::DeviceAddressBase dst =\n       execute_params.buffer_allocations->GetDeviceAddress(dst_);\n \n   VLOG(5) << \"MemzeroCmd:\";\n@@ -1417,7 +1417,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> Memset32Cmd::Record(\n     const Thunk::ExecuteParams& execute_params,\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase dst =\n+  se::DeviceAddressBase dst =\n       execute_params.buffer_allocations->GetDeviceAddress(dst_);\n \n   VLOG(5) << \"Memset32Cmd: bit_pattern=\" << bit_pattern_;\n@@ -1516,7 +1516,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> CaseCmd::Record(\n     const Thunk::ExecuteParams& execute_params,\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase index =\n+  se::DeviceAddressBase index =\n       execute_params.buffer_allocations->GetDeviceAddress(index_);\n \n   VLOG(5) << \"CaseCmd:\";\n@@ -1527,23 +1527,23 @@ absl::StatusOr<const se::CommandBuffer::Command*> CaseCmd::Record(\n       [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n         if (index_is_bool_) {\n           return command_buffer->CreateCase(\n-              se::DeviceMemory<bool>(index),\n+              se::DeviceAddress<bool>(index),\n               CreateCommands(branches_, &execute_params, &record_params),\n               dependencies);\n         }\n         return command_buffer->CreateCase(\n-            se::DeviceMemory<int32_t>(index),\n+            se::DeviceAddress<int32_t>(index),\n             CreateCommands(branches_, &execute_params, &record_params),\n             dependencies);\n       },\n       [&](const se::CommandBuffer::Command* command) {\n         if (index_is_bool_) {\n           return command_buffer->UpdateCase(\n-              command, se::DeviceMemory<bool>(index),\n+              command, se::DeviceAddress<bool>(index),\n               UpdateCommands(branches_, &execute_params, &record_params));\n         }\n         return command_buffer->UpdateCase(\n-            command, se::DeviceMemory<int32_t>(index),\n+            command, se::DeviceAddress<int32_t>(index),\n             UpdateCommands(branches_, &execute_params, &record_params));\n       });\n }\n@@ -1605,7 +1605,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> WhileCmd::Record(\n     const Thunk::ExecuteParams& execute_params,\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase pred =\n+  se::DeviceAddressBase pred =\n       execute_params.buffer_allocations->GetDeviceAddress(pred_);\n \n   VLOG(5) << \"WhileCmd: cond_commands=\" << cond_commands_.size()\n@@ -1669,14 +1669,14 @@ absl::StatusOr<const se::CommandBuffer::Command*> WhileCmd::Record(\n       std::move(record_action),\n       [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n         return command_buffer->CreateWhile(\n-            se::DeviceMemory<bool>(pred),\n+            se::DeviceAddress<bool>(pred),\n             CreateCommands(&cond_commands_, &execute_params, &record_params),\n             CreateCommands(&body_commands_, &execute_params, &record_params),\n             dependencies);\n       },\n       [&](const se::CommandBuffer::Command* command) {\n         return command_buffer->UpdateWhile(\n-            command, se::DeviceMemory<bool>(pred),\n+            command, se::DeviceAddress<bool>(pred),\n             UpdateCommands(&cond_commands_, &execute_params, &record_params),\n             UpdateCommands(&body_commands_, &execute_params, &record_params));\n       });\n@@ -1730,14 +1730,14 @@ absl::StatusOr<const se::CommandBuffer::Command*> GemmCmd::Record(\n     const Thunk::ExecuteParams& execute_params,\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase lhs =\n+  se::DeviceAddressBase lhs =\n       execute_params.buffer_allocations->GetDeviceAddress(lhs_buffer_);\n-  se::DeviceMemoryBase rhs =\n+  se::DeviceAddressBase rhs =\n       execute_params.buffer_allocations->GetDeviceAddress(rhs_buffer_);\n-  se::DeviceMemoryBase out =\n+  se::DeviceAddressBase out =\n       execute_params.buffer_allocations->GetDeviceAddress(output_buffer_);\n \n-  se::DeviceMemoryBase workspace(/*opaque=*/nullptr, /*size=*/0);\n+  se::DeviceAddressBase workspace(/*opaque=*/nullptr, /*size=*/0);\n   if (workspace_.has_value()) {\n     workspace =\n         execute_params.buffer_allocations->GetDeviceAddress(workspace_.value());\n@@ -1867,10 +1867,10 @@ absl::StatusOr<const se::CommandBuffer::Command*> CuDnnCmd::Record(\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n   CHECK(graph_ != nullptr);\n-  std::vector<se::DeviceMemoryBase> operands;\n+  std::vector<se::DeviceAddressBase> operands;\n   operands.reserve(args_.size());\n   for (const BufferAllocation::Slice& arg : args_) {\n-    se::DeviceMemoryBase buf =\n+    se::DeviceAddressBase buf =\n         execute_params.buffer_allocations->GetDeviceAddress(arg);\n     VLOG(5) << \"  Arg: \" << arg << \": \" << buf.opaque();\n     operands.push_back(buf);\n@@ -1884,19 +1884,19 @@ absl::StatusOr<const se::CommandBuffer::Command*> CuDnnCmd::Record(\n         [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n           return command_buffer->CreateDnnGraphCommand(\n               *graph_->get(), *execute_params.stream,\n-              absl::Span<se::DeviceMemoryBase>(operands), dependencies);\n+              absl::Span<se::DeviceAddressBase>(operands), dependencies);\n         },\n         [&](const se::CommandBuffer::Command* command) {\n           return command_buffer->UpdateDnnGraphCommand(\n               command, *graph_->get(), *execute_params.stream,\n-              absl::Span<se::DeviceMemoryBase>(operands));\n+              absl::Span<se::DeviceAddressBase>(operands));\n         });\n   }\n   return RecordTracedCommand(\n       execute_params, record_params, std::move(record_action), command_buffer,\n       [&](se::Stream* stream) {\n         return graph_->get()->Execute(\n-            *stream, absl::Span<se::DeviceMemoryBase>(operands),\n+            *stream, absl::Span<se::DeviceAddressBase>(operands),\n             execute_params.collective_params->local_device_id.value());\n       });\n }\n@@ -2010,34 +2010,34 @@ CustomCallCmd::RecordXlaFfiCall(const Thunk::ExecuteParams& execute_params,\n \n   VLOG(5) << \"CustomCallCmd: target_name=\" << target_name_;\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> arguments;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> arguments;\n   arguments.reserve(operands_.size());\n \n   for (int i = 0; i < operands_.size(); ++i) {\n     const NullableShapedSlice& slice = operands_[i];\n     if (!slice.has_value()) {\n-      arguments.push_back(se::DeviceMemoryBase{});\n+      arguments.push_back(se::DeviceAddressBase{});\n       continue;\n     }\n \n-    se::DeviceMemoryBase buffer =\n+    se::DeviceAddressBase buffer =\n         execute_params.buffer_allocations->GetDeviceAddress(slice->slice);\n     VLOG(5) << \"  Operand \" << i << \": \" << slice->slice << \" (\"\n             << buffer.opaque() << \")\";\n     arguments.push_back(buffer);\n   }\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> results;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> results;\n   results.reserve(results_.size());\n \n   for (int i = 0; i < results_.size(); ++i) {\n     const NullableShapedSlice& slice = results_[i];\n     if (!slice.has_value()) {\n-      results.push_back(se::DeviceMemoryBase{});\n+      results.push_back(se::DeviceAddressBase{});\n       continue;\n     }\n \n-    se::DeviceMemoryBase buffer =\n+    se::DeviceAddressBase buffer =\n         execute_params.buffer_allocations->GetDeviceAddress(slice->slice);\n     VLOG(5) << \"  Result \" << i << \": \" << slice->slice << \" (\"\n             << buffer.opaque() << \")\";\n@@ -2685,8 +2685,8 @@ absl::StatusOr<const se::CommandBuffer::Command*> DynamicSliceFusionCmd::Record(\n \n   const BufferAllocations& orig_allocations =\n       *execute_params.buffer_allocations;\n-  absl::InlinedVector<se::DeviceMemoryBase, 8> slice_buffers(\n-      slices_.size(), se::DeviceMemoryBase());\n+  absl::InlinedVector<se::DeviceAddressBase, 8> slice_buffers(\n+      slices_.size(), se::DeviceAddressBase());\n \n   // Get memory allocation for copying offsets from device.\n   int64_t* offsets_alloc = [&] {\n@@ -2708,7 +2708,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> DynamicSliceFusionCmd::Record(\n \n     // `argument_buffer` will contain the original offset for slice\n     // `argument_slice` within `orig_allocations`\n-    se::DeviceMemoryBase argument_buffer =\n+    se::DeviceAddressBase argument_buffer =\n         orig_allocations.GetDeviceAddress(*slice.embedded_thunk_argument);\n \n     // If argument is not sliced, just use the original buffer.\n@@ -2757,7 +2757,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> DynamicSliceFusionCmd::Record(\n         VLOG(2) << \"  - arg \" << argument_idx << \"[\" << offset_idx\n                 << \"]: transfer offset from device \" << alloc_slice.ToString();\n \n-        se::DeviceMemoryBase offset_src =\n+        se::DeviceAddressBase offset_src =\n             orig_allocations.GetDeviceAddress(alloc_slice);\n         int64_t* offset_dst = &offset_value(argument_idx, offset_idx);\n \n@@ -2895,9 +2895,9 @@ DynamicSliceCopyFusionCmd::Record(const Thunk::ExecuteParams& execute_params,\n                                   const RecordParams& record_params,\n                                   RecordAction record_action,\n                                   se::CommandBuffer* command_buffer) {\n-  se::DeviceMemoryBase src_data =\n+  se::DeviceAddressBase src_data =\n       execute_params.buffer_allocations->GetDeviceAddress(source_buffer_);\n-  se::DeviceMemoryBase dst_data =\n+  se::DeviceAddressBase dst_data =\n       execute_params.buffer_allocations->GetDeviceAddress(destination_buffer_);\n \n   return Handle("
        },
        {
            "sha": "40a5b9cff1a7c167cebb623096fb8ef3a4236ead",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -63,7 +63,7 @@ limitations under the License.\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -579,7 +579,7 @@ class TracedCommandBuffer : public CommandBufferCmd::State {\n   std::vector<BufferAllocation::Index> allocs_indices_;\n \n   struct Entry {\n-    std::vector<se::DeviceMemoryBase> recorded_allocs;\n+    std::vector<se::DeviceAddressBase> recorded_allocs;\n     std::unique_ptr<se::CommandBuffer> command_buffer;\n   };\n   const CommandBufferCmd* trace_cmd_;"
        },
        {
            "sha": "5dffa09d49e184ea724cecd5a9ec46f7390390e6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_test.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 23,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels_fatbin.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -251,9 +251,9 @@ TEST(CommandBufferCmdTest, MemcpyCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -309,9 +309,9 @@ TEST(CommandBufferCmdTest, LaunchCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -381,9 +381,9 @@ TEST(CommandBufferCmdTest, LaunchCmdWithPriority) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -457,9 +457,9 @@ TEST(CommandBufferCmdTest, DynamicSliceCopyFusionCmd) {\n   std::vector<int32_t> a_data = {40, 41, 42, 43, 44, 45, 46, 47};\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memcpy(&a, a_data.data(), byte_length));\n@@ -524,13 +524,13 @@ TEST(TracedCommandBuffer, GetOrUpdateCommandBuffer) {\n     TracedCommandBuffer traced_cmd_buffer(&traced_cmd, buffers,\n                                           /*capacity=*/trace_cache_size);\n \n-    se::DeviceMemoryBase mem0(reinterpret_cast<void*>(0x01234567));\n-    se::DeviceMemoryBase mem1(reinterpret_cast<void*>(0x12345670));\n+    se::DeviceAddressBase mem0(reinterpret_cast<void*>(0x01234567));\n+    se::DeviceAddressBase mem1(reinterpret_cast<void*>(0x12345670));\n \n     se::StreamExecutorMemoryAllocator allocator(executor);\n     BufferAllocations allocations({mem0, mem1}, 0, &allocator);\n \n-    se::DeviceMemory<int32_t> mem = executor->AllocateArray<int32_t>(16, 0);\n+    se::DeviceAddress<int32_t> mem = executor->AllocateArray<int32_t>(16, 0);\n \n     // Count how many times trace callback was called. We also need to record\n     // something on the given stream because we can't leave traced command\n@@ -557,7 +557,7 @@ TEST(TracedCommandBuffer, GetOrUpdateCommandBuffer) {\n \n     // Check that when memory address changes we re-trace the command\n     // buffer.\n-    se::DeviceMemoryBase mem2(reinterpret_cast<void*>(0x23456701));\n+    se::DeviceAddressBase mem2(reinterpret_cast<void*>(0x23456701));\n     allocations = BufferAllocations({mem0, mem2}, 0, &allocator);\n \n     TF_ASSERT_OK_AND_ASSIGN(auto* command_buffer2,\n@@ -608,11 +608,11 @@ TEST(CommandBufferCmdTest, RecordExecutorsWithDependencies) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Device buffers: a, b, c\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> c =\n+  se::DeviceAddress<int32_t> c =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   // Initialize to zero.\n@@ -721,9 +721,12 @@ TEST(CommandBufferCmdTest, NestedChildCmdCreateAndUpdate) {\n   // Prepare device memory for three buffers.\n   int64_t length = 4;\n   int64_t byte_length = sizeof(int32_t) * length;\n-  se::DeviceMemory<int32_t> a = stream_executor->AllocateArray<int32_t>(length);\n-  se::DeviceMemory<int32_t> b = stream_executor->AllocateArray<int32_t>(length);\n-  se::DeviceMemory<int32_t> c = stream_executor->AllocateArray<int32_t>(length);\n+  se::DeviceAddress<int32_t> a =\n+      stream_executor->AllocateArray<int32_t>(length);\n+  se::DeviceAddress<int32_t> b =\n+      stream_executor->AllocateArray<int32_t>(length);\n+  se::DeviceAddress<int32_t> c =\n+      stream_executor->AllocateArray<int32_t>(length);\n \n   // Initialize a = 1s, b = 0s, c = 0s.\n   TF_ASSERT_OK(stream->Memset32(&a, /*pattern=*/1, byte_length));\n@@ -808,9 +811,9 @@ TEST(CommandBufferCmdTest, NestedChildCmdCreateAndUpdate) {\n \n   // Now update: change a and c buffers and record an update on the same command\n   // buffer.\n-  se::DeviceMemory<int32_t> a2 =\n+  se::DeviceAddress<int32_t> a2 =\n       stream_executor->AllocateArray<int32_t>(length);\n-  se::DeviceMemory<int32_t> c2 =\n+  se::DeviceAddress<int32_t> c2 =\n       stream_executor->AllocateArray<int32_t>(length);\n   TF_ASSERT_OK(stream->Memset32(&a2, /*pattern=*/7, byte_length));\n   TF_ASSERT_OK(stream->MemZero(&c2, byte_length));\n@@ -861,8 +864,8 @@ static void BM_GetOrTraceCommandBuffer(benchmark::State& state) {\n       BufferUse::Read(BufferAllocation::Slice(&alloc0, 0, 1024)),\n       BufferUse::Write(BufferAllocation::Slice(&alloc1, 0, 1024))};\n \n-  se::DeviceMemoryBase mem0(reinterpret_cast<void*>(0x01234567));\n-  se::DeviceMemoryBase mem1(reinterpret_cast<void*>(0x12345670));\n+  se::DeviceAddressBase mem0(reinterpret_cast<void*>(0x01234567));\n+  se::DeviceAddressBase mem1(reinterpret_cast<void*>(0x12345670));\n   se::StreamExecutorMemoryAllocator allocator(executor);\n \n   std::array<BufferAllocations, 4> allocations = {"
        },
        {
            "sha": "442397086bbe448ffc27c3dc7a65c43cd327824c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -102,7 +102,7 @@ CommandBufferThunk::ExecutorCommandBuffer::UpdateBufferAllocations(\n   // We check only allocations referenced by commands in a cmd sequence, and\n   // leave every other entry default initialized (nullptr device memory).\n   for (BufferAllocation::Index index : commands.allocs_indices()) {\n-    se::DeviceMemoryBase alloc = allocs->GetDeviceAddress(index);\n+    se::DeviceAddressBase alloc = allocs->GetDeviceAddress(index);\n \n     if (recorded_allocs.size() <= index) {\n       recorded_allocs.resize(index + 1);"
        },
        {
            "sha": "23a0a959ab071763dbb23e5313cc86091abe9b88",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla::gpu {\n@@ -52,7 +52,7 @@ class CommandBufferThunk : public Thunk {\n   // Return the allocation address that was lazilly allocated inside command\n   // buffer. This API is required when the buffers are allocated inside command\n   // buffer but will be consumed by non-command buffer operations.\n-  absl::StatusOr<se::DeviceMemoryBase> GetCommandBufferAllocationAddress(\n+  absl::StatusOr<se::DeviceAddressBase> GetCommandBufferAllocationAddress(\n       const ExecuteParams& params, int64_t index);\n \n   void ForAllThunks(absl::FunctionRef<void(const Thunk*)> fn) const override;\n@@ -96,7 +96,7 @@ class CommandBufferThunk : public Thunk {\n     // execution on a stream. All other pieces of information (like thread\n     // and block sizes) captured by commands at construction time and do not\n     // change.\n-    std::vector<se::DeviceMemoryBase> recorded_allocs ABSL_GUARDED_BY(mutex);\n+    std::vector<se::DeviceAddressBase> recorded_allocs ABSL_GUARDED_BY(mutex);\n \n     // Number of command buffer executions since last update.\n     int64_t num_executions ABSL_GUARDED_BY(mutex) = 0;"
        },
        {
            "sha": "d4a472d5c542e548dcad70b7127bc7aa509a3970",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 49,
            "deletions": 49,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -48,9 +48,9 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels_fatbin.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -159,9 +159,9 @@ TEST(CommandBufferThunkTest, MemcpyCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -224,7 +224,7 @@ TEST(CommandBufferThunkTest, MemzeroCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n \n@@ -269,7 +269,7 @@ TEST(CommandBufferThunkTest, Memset32Cmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -315,7 +315,7 @@ TEST(CommandBufferThunkTest, Memset32CmdCommandBuffersDisabledDuringProfiling) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -373,7 +373,7 @@ TEST(CommandBufferThunkTest, Memset32CmdCommandBuffersEnabledDuringProfiling) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -432,7 +432,7 @@ TEST(CommandBufferThunkTest, Memset32CmdOnDifferentStreams) {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto stream, stream_executor->CreateStream());\n \n-  se::DeviceMemory<int32_t> a = stream_executor->AllocateArray<int32_t>(2, 0);\n+  se::DeviceAddress<int32_t> a = stream_executor->AllocateArray<int32_t>(2, 0);\n   TF_ASSERT_OK(stream->MemZero(&a, 2 * sizeof(int32_t)));\n \n   // Prepare buffer allocations for recording command buffer.\n@@ -478,9 +478,9 @@ TEST(CommandBufferThunkTest, LaunchCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -532,7 +532,7 @@ TEST(CommandBufferThunkTest, LaunchCmd) {\n   ASSERT_EQ(dst, std::vector<int32_t>(4, 42 + 42));\n \n   // Prepare buffer allocation for updating command buffer: c=0\n-  se::DeviceMemory<int32_t> c =\n+  se::DeviceAddress<int32_t> c =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n   TF_ASSERT_OK(stream->MemZero(&c, byte_length));\n \n@@ -582,9 +582,9 @@ TEST(CommandBufferThunkTest, CustomAddKernelLaunchCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -636,7 +636,7 @@ TEST(CommandBufferThunkTest, CustomAddKernelLaunchCmd) {\n   ASSERT_EQ(dst, std::vector<int32_t>(4, 42 + 42));\n \n   // Prepare buffer allocation for updating command buffer: c=0\n-  se::DeviceMemory<int32_t> c =\n+  se::DeviceAddress<int32_t> c =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n   TF_ASSERT_OK(stream->MemZero(&c, byte_length));\n \n@@ -687,18 +687,18 @@ TEST(CommandBufferThunkTest, GemmCmd) {\n   //        1.0, 1.0, 1.0\n   //        1.0, 1.0, 1.0\n   //        1.0, 1.0, 1.0]\n-  se::DeviceMemory<float> lhs = stream_executor->AllocateArray<float>(2 * 4);\n+  se::DeviceAddress<float> lhs = stream_executor->AllocateArray<float>(2 * 4);\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n-  se::DeviceMemory<float> rhs = stream_executor->AllocateArray<float>(4 * 3);\n+  se::DeviceAddress<float> rhs = stream_executor->AllocateArray<float>(4 * 3);\n   std::vector<float> rhs_arr(12, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemory<float> out = stream_executor->AllocateArray<float>(2 * 3);\n+  se::DeviceAddress<float> out = stream_executor->AllocateArray<float>(2 * 3);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       stream_executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n@@ -756,7 +756,7 @@ TEST(CommandBufferThunkTest, GemmCmd) {\n   ASSERT_EQ(dst, std::vector<float>({10, 10, 10, 26, 26, 26}));\n \n   // Prepare buffer allocation for updating command buffer.\n-  se::DeviceMemory<float> updated_out =\n+  se::DeviceAddress<float> updated_out =\n       stream_executor->AllocateArray<float>(2 * 3);\n   TF_ASSERT_OK(stream->MemZero(&updated_out, out_length));\n \n@@ -808,18 +808,18 @@ TEST(CommandBufferThunkTest, ChildGemmCmd) {\n   //        1.0, 1.0, 1.0\n   //        1.0, 1.0, 1.0\n   //        1.0, 1.0, 1.0]\n-  se::DeviceMemory<float> lhs = stream_executor->AllocateArray<float>(2 * 4);\n+  se::DeviceAddress<float> lhs = stream_executor->AllocateArray<float>(2 * 4);\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n-  se::DeviceMemory<float> rhs = stream_executor->AllocateArray<float>(4 * 3);\n+  se::DeviceAddress<float> rhs = stream_executor->AllocateArray<float>(4 * 3);\n   std::vector<float> rhs_arr(12, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemory<float> out = stream_executor->AllocateArray<float>(2 * 3);\n+  se::DeviceAddress<float> out = stream_executor->AllocateArray<float>(2 * 3);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       stream_executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n@@ -885,7 +885,7 @@ TEST(CommandBufferThunkTest, ChildGemmCmd) {\n   ASSERT_EQ(dst, std::vector<float>({10, 10, 10, 26, 26, 26}));\n \n   // Prepare buffer allocation for updating command buffer.\n-  se::DeviceMemory<float> updated_out =\n+  se::DeviceAddress<float> updated_out =\n       stream_executor->AllocateArray<float>(2 * 3);\n   TF_ASSERT_OK(stream->MemZero(&updated_out, out_length));\n \n@@ -938,18 +938,18 @@ TEST(CommandBufferThunkTest, DISABLED_DynamicSliceFusionCmd) {\n   //        1.0, 1.0, 1.0\n   //        1.0, 1.0, 1.0\n   //        1.0, 1.0, 1.0]\n-  se::DeviceMemory<float> lhs = stream_executor->AllocateArray<float>(4 * 4);\n+  se::DeviceAddress<float> lhs = stream_executor->AllocateArray<float>(4 * 4);\n   std::vector<float> lhs_arr{0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n-  se::DeviceMemory<float> rhs = stream_executor->AllocateArray<float>(4 * 3);\n+  se::DeviceAddress<float> rhs = stream_executor->AllocateArray<float>(4 * 3);\n   std::vector<float> rhs_arr(12, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemory<float> out = stream_executor->AllocateArray<float>(2 * 3);\n+  se::DeviceAddress<float> out = stream_executor->AllocateArray<float>(2 * 3);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       stream_executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n@@ -1045,7 +1045,7 @@ TEST(CommandBufferThunkTest, DISABLED_DynamicSliceFusionCmd) {\n   ASSERT_EQ(dst, std::vector<float>({10, 10, 10, 26, 26, 26}));\n \n   // Prepare buffer allocation for updating command buffer.\n-  se::DeviceMemory<float> updated_out =\n+  se::DeviceAddress<float> updated_out =\n       stream_executor->AllocateArray<float>(2 * 3);\n   TF_ASSERT_OK(stream->MemZero(&updated_out, out_length));\n \n@@ -1148,21 +1148,21 @@ TEST(CommandBufferThunkTest, CublasLtCmd) {\n   auto run_cublaslt_test = [&](std::unique_ptr<se::Stream>& stream,\n                                std::vector<float> a_arr,\n                                std::vector<float> result) {\n-    se::DeviceMemory<float> a = stream_executor->AllocateArray<float>(2 * 4);\n+    se::DeviceAddress<float> a = stream_executor->AllocateArray<float>(2 * 4);\n     TF_ASSERT_OK(stream->Memcpy(&a, a_arr.data(), a_length));\n \n-    se::DeviceMemory<float> b = stream_executor->AllocateArray<float>(4 * 3);\n+    se::DeviceAddress<float> b = stream_executor->AllocateArray<float>(4 * 3);\n     std::vector<float> b_arr(12, 1);\n     TF_ASSERT_OK(stream->Memcpy(&b, b_arr.data(), b_length));\n \n-    se::DeviceMemory<float> c = stream_executor->AllocateArray<float>(2 * 3);\n+    se::DeviceAddress<float> c = stream_executor->AllocateArray<float>(2 * 3);\n     std::vector<float> c_arr(6, 1);\n     TF_ASSERT_OK(stream->Memcpy(&c, c_arr.data(), c_length));\n \n-    se::DeviceMemory<float> d = stream_executor->AllocateArray<float>(2 * 3);\n+    se::DeviceAddress<float> d = stream_executor->AllocateArray<float>(2 * 3);\n     TF_ASSERT_OK(stream->MemZero(&d, d_length));\n \n-    se::DeviceMemory<float> workspace =\n+    se::DeviceAddress<float> workspace =\n         stream_executor->AllocateArray<float>(1024 * 1024);\n     TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n@@ -1188,7 +1188,7 @@ TEST(CommandBufferThunkTest, CublasLtCmd) {\n     ASSERT_EQ(dst, result);\n \n     // Prepare buffer allocation for updating command buffer.\n-    se::DeviceMemory<float> updated_d =\n+    se::DeviceAddress<float> updated_d =\n         stream_executor->AllocateArray<float>(2 * 3);\n     TF_ASSERT_OK(stream->MemZero(&updated_d, d_length));\n \n@@ -1236,13 +1236,13 @@ TEST(CommandBufferThunkTest, MultipleLaunchCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=0\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> c =\n+  se::DeviceAddress<int32_t> c =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> d =\n+  se::DeviceAddress<int32_t> d =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n@@ -1311,7 +1311,7 @@ TEST(CommandBufferThunkTest, MultipleLaunchCmd) {\n   BufferAllocation::Slice slice_e(&alloc_e, 0, byte_length);\n \n   // Prepare buffer allocation for updating command buffer: e=0\n-  se::DeviceMemory<int32_t> e =\n+  se::DeviceAddress<int32_t> e =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n   TF_ASSERT_OK(stream->MemZero(&e, byte_length));\n \n@@ -1363,11 +1363,11 @@ TEST(CommandBufferThunkTest, CaseCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: index=0, a=42, b=0\n-  se::DeviceMemory<int32_t> index =\n+  se::DeviceAddress<int32_t> index =\n       stream_executor->AllocateArray<int32_t>(1, 0);\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&index, 0, sizeof(int32_t)));\n@@ -1466,14 +1466,14 @@ TEST(CommandBufferThunkTest, WhileCmd) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: loop_cnt=0, num_iters=10, a=1, b=0\n-  se::DeviceMemory<bool> pred = stream_executor->AllocateArray<bool>(1, 0);\n-  se::DeviceMemory<int32_t> loop_cnt =\n+  se::DeviceAddress<bool> pred = stream_executor->AllocateArray<bool>(1, 0);\n+  se::DeviceAddress<int32_t> loop_cnt =\n       stream_executor->AllocateArray<int32_t>(1, 0);\n-  se::DeviceMemory<int32_t> num_iters =\n+  se::DeviceAddress<int32_t> num_iters =\n       stream_executor->AllocateArray<int32_t>(1, 0);\n-  se::DeviceMemory<int32_t> a =\n+  se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n-  se::DeviceMemory<int32_t> b =\n+  se::DeviceAddress<int32_t> b =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&loop_cnt, 0, sizeof(int32_t)));"
        },
        {
            "sha": "f1077fcf368d3fd97babf350656447c197f6d310",
            "filename": "third_party/xla/xla/backends/gpu/runtime/conditional_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -38,7 +38,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -110,7 +110,7 @@ absl::Status ConditionalThunk::ExecuteOnStream(const ExecuteParams& params) {\n     return handle.get<int32_t>();\n   }();\n \n-  se::DeviceMemoryBase branch_index_address =\n+  se::DeviceAddressBase branch_index_address =\n       params.buffer_allocations->GetDeviceAddress(branch_index_buffer_index_);\n   if (branch_index_is_bool_) {\n     TF_RETURN_IF_ERROR(stream.Memcpy(std::get<bool*>(branch_index_or_pred),"
        },
        {
            "sha": "51cb69840c1197407c3327c595d94a6f98c067f2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_reorder_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_assignment.pb.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -63,17 +63,17 @@ absl::Status ConvolutionReorderThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   const auto& buffer_allocations = *params.buffer_allocations;\n \n-  auto filter_input = se::DeviceMemory<int8_t>(\n+  auto filter_input = se::DeviceAddress<int8_t>(\n       buffer_allocations.GetDeviceAddress(filter_input_));\n-  auto filter_output = se::DeviceMemory<int8_t>(\n+  auto filter_output = se::DeviceAddress<int8_t>(\n       buffer_allocations.GetDeviceAddress(filter_output_));\n \n-  std::optional<se::DeviceMemory<float>> bias_input;\n-  std::optional<se::DeviceMemory<float>> bias_output;\n+  std::optional<se::DeviceAddress<float>> bias_input;\n+  std::optional<se::DeviceAddress<float>> bias_output;\n   if (biases_.has_value()) {\n-    bias_input = se::DeviceMemory<float>(\n+    bias_input = se::DeviceAddress<float>(\n         buffer_allocations.GetDeviceAddress(biases_->bias_input));\n-    bias_output = se::DeviceMemory<float>(\n+    bias_output = se::DeviceAddress<float>(\n         buffer_allocations.GetDeviceAddress(biases_->bias_output));\n   }\n "
        },
        {
            "sha": "3e020680be7d4c81fdfc8f4b18362b5f93714f6d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -28,8 +28,8 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/gpu_conv_runner.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n@@ -85,7 +85,7 @@ GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(\n absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n   const auto& buffer_allocations = *params.buffer_allocations;\n \n-  std::vector<se::DeviceMemoryBase> operand_se_buffers, result_se_buffers;\n+  std::vector<se::DeviceAddressBase> operand_se_buffers, result_se_buffers;\n   operand_se_buffers.reserve(operand_buffers_.size());\n   for (BufferAllocation::Slice buffer : operand_buffers_) {\n     operand_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n@@ -96,7 +96,7 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n     result_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n   }\n \n-  se::DeviceMemoryBase scratch =\n+  se::DeviceAddressBase scratch =\n       buffer_allocations.GetDeviceAddress(scratch_buffer_);\n \n   bool runner_created = false;"
        },
        {
            "sha": "a98f4a05f38dbcc848c6eaebab4b1e8d126f10cd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/copy_thunk.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -51,9 +51,9 @@ DeviceToDeviceCopyThunk::DeviceToDeviceCopyThunk(\n \n absl::Status DeviceToDeviceCopyThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n-  se::DeviceMemoryBase destination_data =\n+  se::DeviceAddressBase destination_data =\n       params.buffer_allocations->GetDeviceAddress(destination_buffer_);\n-  se::DeviceMemoryBase source_data =\n+  se::DeviceAddressBase source_data =\n       params.buffer_allocations->GetDeviceAddress(source_buffer_);\n   VLOG(3) << \"Memcpy D2D of size \" << mem_size_ << \" from \"\n           << source_data.opaque() << \" to \" << destination_data.opaque();\n@@ -182,9 +182,9 @@ DeviceToHostCopyThunk::DeviceToHostCopyThunk(\n \n absl::Status DeviceToHostCopyThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n-  se::DeviceMemoryBase destination_data =\n+  se::DeviceAddressBase destination_data =\n       params.buffer_allocations->GetDeviceAddress(destination());\n-  se::DeviceMemoryBase source_data =\n+  se::DeviceAddressBase source_data =\n       params.buffer_allocations->GetDeviceAddress(source());\n   void* cpu_dst = destination_data.opaque();\n   TF_ASSIGN_OR_RETURN(\n@@ -263,9 +263,9 @@ HostToDeviceCopyThunk::HostToDeviceCopyThunk(\n \n absl::Status HostToDeviceCopyThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n-  se::DeviceMemoryBase destination_data =\n+  se::DeviceAddressBase destination_data =\n       params.buffer_allocations->GetDeviceAddress(destination());\n-  se::DeviceMemoryBase source_data =\n+  se::DeviceAddressBase source_data =\n       params.buffer_allocations->GetDeviceAddress(source());\n   void* cpu_src = source_data.opaque();\n   TF_ASSIGN_OR_RETURN(\n@@ -374,9 +374,9 @@ DynamicMemcpyThunk::DynamicMemcpyThunk(\n       offsets_(std::move(offsets)) {}\n \n absl::Status DynamicMemcpyThunk::ExecuteOnStream(const ExecuteParams& params) {\n-  se::DeviceMemoryBase src_data =\n+  se::DeviceAddressBase src_data =\n       params.buffer_allocations->GetDeviceAddress(source_buffer_);\n-  se::DeviceMemoryBase dst_data =\n+  se::DeviceAddressBase dst_data =\n       params.buffer_allocations->GetDeviceAddress(destination_buffer_);\n \n   int64_t iteration_index = 0;"
        },
        {
            "sha": "b02d8fac14889c38050c8175caf5663f8ed3df6c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cub_sort_thunk.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 29,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcub_sort_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcub_sort_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcub_sort_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -40,8 +40,8 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -60,12 +60,12 @@ uint64_t GetOffsetsSize(int64_t batch_size) {\n }\n \n // Copies segment offsets to the device memory.\n-absl::Status CopyOffsets(se::Stream* stream, se::DeviceMemoryBase scratch,\n+absl::Status CopyOffsets(se::Stream* stream, se::DeviceAddressBase scratch,\n                          int64_t batch_size, int64_t segment_size) {\n   uint64_t offsets_size = GetOffsetsSize(batch_size);\n   char* offsets_buffer =\n       static_cast<char*>(scratch.opaque()) + scratch.size() - offsets_size;\n-  se::DeviceMemoryBase d_offsets(offsets_buffer, offsets_size);\n+  se::DeviceAddressBase d_offsets(offsets_buffer, offsets_size);\n   std::vector<int> h_offsets(batch_size + 1);\n   for (int i = 0; i <= batch_size; ++i) {\n     h_offsets[i] = i * segment_size;\n@@ -80,11 +80,11 @@ class CubSortKeysImpl : public CubSortRunnerInterface {\n                            PrimitiveType type)\n       : sort_keys_fn_(sort_keys_fn), type_(type) {}\n \n-  absl::Status Run(se::DeviceMemoryBase input_keys,\n-                   se::DeviceMemoryBase input_values,\n-                   se::DeviceMemoryBase output_keys,\n-                   se::DeviceMemoryBase output_values,\n-                   se::DeviceMemoryBase scratch, bool descending,\n+  absl::Status Run(se::DeviceAddressBase input_keys,\n+                   se::DeviceAddressBase input_values,\n+                   se::DeviceAddressBase output_keys,\n+                   se::DeviceAddressBase output_values,\n+                   se::DeviceAddressBase scratch, bool descending,\n                    int64_t batch_size, se::Stream* stream) override;\n   absl::Status Run(const Thunk::ExecuteParams& params,\n                    const CubSortThunk* thunk) override;\n@@ -96,12 +96,13 @@ class CubSortKeysImpl : public CubSortRunnerInterface {\n   PrimitiveType type_;\n };\n \n-absl::Status CubSortKeysImpl::Run(se::DeviceMemoryBase input_keys,\n-                                  se::DeviceMemoryBase input_values,\n-                                  se::DeviceMemoryBase output_keys,\n-                                  se::DeviceMemoryBase output_values,\n-                                  se::DeviceMemoryBase scratch, bool descending,\n-                                  int64_t batch_size, se::Stream* stream) {\n+absl::Status CubSortKeysImpl::Run(se::DeviceAddressBase input_keys,\n+                                  se::DeviceAddressBase input_values,\n+                                  se::DeviceAddressBase output_keys,\n+                                  se::DeviceAddressBase output_values,\n+                                  se::DeviceAddressBase scratch,\n+                                  bool descending, int64_t batch_size,\n+                                  se::Stream* stream) {\n   size_t temp_bytes = scratch.size();\n   size_t num_items = input_keys.size() * 8 / primitive_util::BitWidth(type_);\n   CHECK(input_values.is_null());\n@@ -136,10 +137,10 @@ absl::Status CubSortKeysImpl::Run(se::DeviceMemoryBase input_keys,\n absl::Status CubSortKeysImpl::Run(const Thunk::ExecuteParams& params,\n                                   const CubSortThunk* thunk) {\n   const BufferAllocations& allocs = *params.buffer_allocations;\n-  return Run(allocs.GetDeviceAddress(thunk->operand(0)), se::DeviceMemoryBase(),\n-             allocs.GetDeviceAddress(thunk->result(0)), se::DeviceMemoryBase(),\n-             allocs.GetDeviceAddress(thunk->scratch()), thunk->descending(),\n-             thunk->batch_size(), params.stream);\n+  return Run(allocs.GetDeviceAddress(thunk->operand(0)),\n+             se::DeviceAddressBase(), allocs.GetDeviceAddress(thunk->result(0)),\n+             se::DeviceAddressBase(), allocs.GetDeviceAddress(thunk->scratch()),\n+             thunk->descending(), thunk->batch_size(), params.stream);\n }\n \n absl::StatusOr<int64_t> CubSortKeysImpl::GetScratchSize(int64_t num_items,\n@@ -167,11 +168,11 @@ class CubSortPairsImpl : public CubSortRunnerInterface {\n                             PrimitiveType type)\n       : sort_pairs_fn_(sort_pairs_fn), type_(type) {}\n \n-  absl::Status Run(se::DeviceMemoryBase input_keys,\n-                   se::DeviceMemoryBase input_values,\n-                   se::DeviceMemoryBase output_keys,\n-                   se::DeviceMemoryBase output_values,\n-                   se::DeviceMemoryBase scratch, bool descending,\n+  absl::Status Run(se::DeviceAddressBase input_keys,\n+                   se::DeviceAddressBase input_values,\n+                   se::DeviceAddressBase output_keys,\n+                   se::DeviceAddressBase output_values,\n+                   se::DeviceAddressBase scratch, bool descending,\n                    int64_t batch_size, se::Stream* stream) override;\n   absl::Status Run(const Thunk::ExecuteParams& params,\n                    const CubSortThunk* thunk) override;\n@@ -183,11 +184,11 @@ class CubSortPairsImpl : public CubSortRunnerInterface {\n   PrimitiveType type_;\n };\n \n-absl::Status CubSortPairsImpl::Run(se::DeviceMemoryBase input_keys,\n-                                   se::DeviceMemoryBase input_values,\n-                                   se::DeviceMemoryBase output_keys,\n-                                   se::DeviceMemoryBase output_values,\n-                                   se::DeviceMemoryBase scratch,\n+absl::Status CubSortPairsImpl::Run(se::DeviceAddressBase input_keys,\n+                                   se::DeviceAddressBase input_values,\n+                                   se::DeviceAddressBase output_keys,\n+                                   se::DeviceAddressBase output_values,\n+                                   se::DeviceAddressBase scratch,\n                                    bool descending, int64_t batch_size,\n                                    se::Stream* stream) {\n   size_t temp_bytes = scratch.size();"
        },
        {
            "sha": "790ccc0030b8f3c50ee469b1685d48ecddc1a9f5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cub_sort_thunk.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcub_sort_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcub_sort_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcub_sort_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -28,7 +28,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -38,11 +38,11 @@ namespace gpu {\n class CubSortRunnerInterface {\n  public:\n   virtual ~CubSortRunnerInterface() = default;\n-  virtual absl::Status Run(se::DeviceMemoryBase input_keys,\n-                           se::DeviceMemoryBase input_values,\n-                           se::DeviceMemoryBase output_keys,\n-                           se::DeviceMemoryBase output_values,\n-                           se::DeviceMemoryBase scratch, bool descending,\n+  virtual absl::Status Run(se::DeviceAddressBase input_keys,\n+                           se::DeviceAddressBase input_values,\n+                           se::DeviceAddressBase output_keys,\n+                           se::DeviceAddressBase output_values,\n+                           se::DeviceAddressBase scratch, bool descending,\n                            int64_t batch_size, se::Stream* stream) = 0;\n   virtual absl::Status Run(const Thunk::ExecuteParams& params,\n                            const class CubSortThunk* thunk) = 0;"
        },
        {
            "sha": "1e04276113cea07242794f58d6836851e36b1b75",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cuda_command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcuda_command_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcuda_command_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcuda_command_buffer_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -42,9 +42,9 @@ limitations under the License.\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_dnn.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n@@ -156,21 +156,21 @@ TEST(CommandBufferThunkTest, CuDnnCmd) {\n   // Construct a thunk with command sequence.\n   CommandBufferThunk thunk(std::move(executor), Thunk::ThunkInfo());\n \n-  std::vector<se::DeviceMemoryBase> operands;\n+  std::vector<se::DeviceAddressBase> operands;\n   operands.reserve(3);\n \n-  se::DeviceMemory<int8_t> input =\n+  se::DeviceAddress<int8_t> input =\n       stream_executor->AllocateArray<int8_t>(kTotalElements);\n   TF_ASSERT_OK(stream->MemZero(&input, input.size()));\n \n-  se::DeviceMemory<int32_t> output0 =\n+  se::DeviceAddress<int32_t> output0 =\n       stream_executor->AllocateArray<int32_t>(kTotalElements);\n   TF_ASSERT_OK(stream->Memset32(&output0, 123, output0.size()));\n \n   operands.push_back(input);  // multiplying the input by itself\n   operands.push_back(output0);\n \n-  se::DeviceMemoryBase workspace;\n+  se::DeviceAddressBase workspace;\n   if (workspace_size > 0) {\n     workspace = stream_executor->Allocate(workspace_size);\n     operands.push_back(workspace);\n@@ -199,7 +199,7 @@ TEST(CommandBufferThunkTest, CuDnnCmd) {\n   ASSERT_EQ(dst, std::vector<int32_t>(kTotalElements, 0));\n \n   // Prepare buffer allocation for updating command buffer.\n-  se::DeviceMemory<int32_t> output1 =\n+  se::DeviceAddress<int32_t> output1 =\n       stream_executor->AllocateArray<int32_t>(kTotalElements);\n   TF_ASSERT_OK(stream->Memset32(&output1, 456, output1.size()));\n "
        },
        {
            "sha": "df19f2f5ab3c4047af52fac7d9b2f761ee764b9d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cudnn_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcudnn_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcudnn_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcudnn_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_assignment.pb.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -77,7 +77,7 @@ absl::Status CuDnnThunk::ExecuteOnStream(const ExecuteParams& params) {\n   InitializeParams initialize_params;\n   initialize_params.stream = params.stream;\n   TF_RETURN_IF_ERROR(Initialize(initialize_params));\n-  std::vector<se::DeviceMemoryBase> buffer_args;\n+  std::vector<se::DeviceAddressBase> buffer_args;\n   buffer_args.reserve(args_.size());\n   for (const BufferAllocation::Slice& arg : args_) {\n     auto addr = params.buffer_allocations->GetDeviceAddress(arg);\n@@ -90,7 +90,7 @@ absl::Status CuDnnThunk::ExecuteOnStream(const ExecuteParams& params) {\n     buffer_args.push_back(addr);\n   }\n   return graph_->get()->Execute(\n-      *params.stream, absl::Span<se::DeviceMemoryBase>(buffer_args),\n+      *params.stream, absl::Span<se::DeviceAddressBase>(buffer_args),\n       params.collective_params->local_device_id.value());\n }\n "
        },
        {
            "sha": "a8bdbd6719435ac0c55469fc746109f241d386bd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -58,8 +58,8 @@ limitations under the License.\n #include \"xla/service/custom_call_status_internal.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -97,7 +97,7 @@ static absl::StatusOr<ffi::CallFrame> BuildCallFramePrototype(\n     auto elements = absl::c_accumulate(operand->shape.dimensions(), 1ULL,\n                                        std::multiplies<int64_t>());\n     auto dtype_bytes = primitive_util::ByteWidth(operand->shape.element_type());\n-    se::DeviceMemoryBase placeholder_arg(nullptr, elements * dtype_bytes);\n+    se::DeviceAddressBase placeholder_arg(nullptr, elements * dtype_bytes);\n     builder.AddBufferArg(placeholder_arg, operand->shape.element_type(),\n                          operand->shape.dimensions());\n   }\n@@ -115,7 +115,7 @@ static absl::StatusOr<ffi::CallFrame> BuildCallFramePrototype(\n     auto elements = absl::c_accumulate(result->shape.dimensions(), 1ULL,\n                                        std::multiplies<int64_t>());\n     auto dtype_bytes = primitive_util::ByteWidth(result->shape.element_type());\n-    se::DeviceMemoryBase placeholder_ret(nullptr, elements * dtype_bytes);\n+    se::DeviceAddressBase placeholder_ret(nullptr, elements * dtype_bytes);\n     builder.AddBufferRet(placeholder_ret, result->shape.element_type(),\n                          result->shape.dimensions());\n   }\n@@ -374,26 +374,26 @@ CustomCallThunk::BuildCallFrame(\n     const BufferAllocations* absl_nullable buffer_allocations) {\n   auto device_memory = [&](BufferAllocation::Slice slice) {\n     return buffer_allocations ? buffer_allocations->GetDeviceAddress(slice)\n-                              : se::DeviceMemoryBase{};\n+                              : se::DeviceAddressBase{};\n   };\n \n   // Collect arguments buffers.\n-  absl::InlinedVector<se::DeviceMemoryBase, 8> arguments;\n+  absl::InlinedVector<se::DeviceAddressBase, 8> arguments;\n   arguments.reserve(operands_.size());\n   for (auto& operand : operands_) {\n     if (!operand.has_value()) {\n-      arguments.push_back(se::DeviceMemoryBase{});\n+      arguments.push_back(se::DeviceAddressBase{});\n     } else {\n       arguments.push_back(device_memory(operand->slice));\n     }\n   }\n \n   // Collect results buffers.\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> results;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> results;\n   results.reserve(results_.size());\n   for (auto& result : results_) {\n     if (!result.has_value()) {\n-      results.push_back(se::DeviceMemoryBase{});\n+      results.push_back(se::DeviceAddressBase{});\n     } else {\n       results.push_back(device_memory(result->slice));\n     }\n@@ -418,7 +418,7 @@ CallOptions CustomCallThunk::BuildCallOptions(\n     const CollectiveCliques* absl_nullable collective_cliques,\n     const ffi::ExecutionContext* absl_nullable execution_context) {\n   int32_t device_ordinal = -1;\n-  se::DeviceMemoryAllocator* allocator = nullptr;\n+  se::DeviceAddressAllocator* allocator = nullptr;\n   if (buffer_allocations != nullptr) {\n     device_ordinal = buffer_allocations->device_ordinal();\n     allocator = buffer_allocations->memory_allocator();"
        },
        {
            "sha": "0ac5393d5238c00ed053593a37a6ca545353412c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -46,7 +46,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n \n namespace xla {"
        },
        {
            "sha": "8aa5c9853bfcc390882027e872e37fa71f79af00",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -48,7 +48,7 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -405,9 +405,9 @@ TEST(CustomCallThunkTest, ProtoConversion) {\n \n   se::StreamExecutorMemoryAllocator allocator(executor);\n   BufferAllocations device_allocations(\n-      {stream_executor::DeviceMemoryBase(\n+      {stream_executor::DeviceAddressBase(\n            absl::bit_cast<void*>(static_cast<intptr_t>(0xDEADBEEF)), 1024),\n-       stream_executor::DeviceMemoryBase(\n+       stream_executor::DeviceAddressBase(\n            absl::bit_cast<void*>(static_cast<intptr_t>(0xABCDEF)), 1024)},\n       0, &allocator);\n   Thunk::ExecuteParams params = Thunk::ExecuteParams::Create("
        },
        {
            "sha": "53848052fee2ea24673050a54f5ba1d97d436a8e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -35,7 +35,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/kernels/custom_kernel.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -84,9 +84,10 @@ absl::Status CustomKernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n           << custom_kernel_.ToString() << \" as device kernel \"\n           << kernel->name();\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> buffer_args;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> buffer_args;\n   for (const BufferAllocation::Slice& arg : args_) {\n-    se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n+    se::DeviceAddressBase buf =\n+        params.buffer_allocations->GetDeviceAddress(arg);\n     VLOG(3) << \"[\" << device_ordinal << \"]  Arg: alloc #\" << arg.index()\n             << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n             << buf.size() << \"B)\";\n@@ -95,7 +96,7 @@ absl::Status CustomKernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   if (VLOG_IS_ON(100)) {\n     absl::InlinedVector<se::KernelArgument, 4> kernel_args;\n-    for (const se::DeviceMemoryBase& arg : buffer_args) {\n+    for (const se::DeviceAddressBase& arg : buffer_args) {\n       kernel_args.push_back(arg);\n     }\n     PrintBufferContents(params.stream, kernel_args);"
        },
        {
            "sha": "5628f682b31981e6e82773f72a1cd2f76ebd13d1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -51,7 +51,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -277,8 +277,8 @@ absl::Status DynamicSliceThunk::ExecuteOnStream(const ExecuteParams& params) {\n   se::Stream& stream = *params.stream;\n   const BufferAllocations& orig_allocations = *params.buffer_allocations;\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 8> slice_buffers(\n-      slices_.size(), se::DeviceMemoryBase());\n+  absl::InlinedVector<se::DeviceAddressBase, 8> slice_buffers(\n+      slices_.size(), se::DeviceAddressBase());\n \n   // Get memory allocation for copying offsets from device.\n   int64_t* offsets_alloc = [&] {\n@@ -300,7 +300,7 @@ absl::Status DynamicSliceThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n     // `argument_buffer` will contain the original offset for slice\n     // `argument_slice` within `orig_allocations`\n-    se::DeviceMemoryBase argument_buffer =\n+    se::DeviceAddressBase argument_buffer =\n         orig_allocations.GetDeviceAddress(*slice.embedded_thunk_argument);\n \n     // If argument is not sliced, just use the original buffer.\n@@ -350,7 +350,7 @@ absl::Status DynamicSliceThunk::ExecuteOnStream(const ExecuteParams& params) {\n         VLOG(2) << \"  - arg \" << argument_idx << \"[\" << offset_idx\n                 << \"]: transfer offset from device \" << alloc_slice.ToString();\n \n-        se::DeviceMemoryBase offset_src =\n+        se::DeviceAddressBase offset_src =\n             orig_allocations.GetDeviceAddress(alloc_slice);\n         int64_t* offset_dst = &offset_value(argument_idx, offset_idx);\n "
        },
        {
            "sha": "9668eda443f0b3469a954b69b1a7586eadcd9b0b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk_test.cc",
            "status": "modified",
            "additions": 75,
            "deletions": 75,
            "changes": 150,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -53,8 +53,8 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -272,26 +272,26 @@ TEST_F(DynamicSliceThunkTest, SlicedGemm) {\n   // Preparing memory for thunk arguments.\n   // lhs = [1.0, 2.0, 3.0, 4.0,\n   //        5.0, 6.0, 7.0, 8.0]\n-  se::DeviceMemory<float> lhs = executor->AllocateArray<float>(2 * 4);\n+  se::DeviceAddress<float> lhs = executor->AllocateArray<float>(2 * 4);\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n   // rhs = [1.0,\n   //        1.0,\n   //        1.0]\n-  se::DeviceMemory<float> rhs = executor->AllocateArray<float>(3 * 1);\n+  se::DeviceAddress<float> rhs = executor->AllocateArray<float>(3 * 1);\n   std::vector<float> rhs_arr(3, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemory<float> out = executor->AllocateArray<float>(1 * 1);\n+  se::DeviceAddress<float> out = executor->AllocateArray<float>(1 * 1);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n-  se::DeviceMemory<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> lhs_offset_arr{0, 1};\n   TF_ASSERT_OK(\n       stream->Memcpy(&lhs_offset_0, &lhs_offset_arr[0], offset_length));\n@@ -443,7 +443,7 @@ TEST_F(DynamicSliceThunkTest, MultipleSlicedOperandsGemm) {\n   // lhs = [1.0, 2.0, 3.0, 4.0,\n   //        5.0, 6.0, 7.0, 8.0]\n   std::vector<float> arr{1, 2, 3, 4, 5, 6, 7, 8};\n-  se::DeviceMemory<float> lhs = executor->AllocateArray<float>(2 * 4);\n+  se::DeviceAddress<float> lhs = executor->AllocateArray<float>(2 * 4);\n   TF_ASSERT_OK(stream->Memcpy(&lhs, arr.data(), length));\n \n   // Given a `rhs` tensor of shape f32[8,1]{1,0}\n@@ -458,26 +458,26 @@ TEST_F(DynamicSliceThunkTest, MultipleSlicedOperandsGemm) {\n   //        6.0,\n   //        7.0,\n   //        8.0]\n-  se::DeviceMemory<float> rhs = executor->AllocateArray<float>(8);\n+  se::DeviceAddress<float> rhs = executor->AllocateArray<float>(8);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, arr.data(), length));\n \n-  se::DeviceMemory<float> out = executor->AllocateArray<float>(1);\n+  se::DeviceAddress<float> out = executor->AllocateArray<float>(1);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n-  se::DeviceMemory<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> lhs_offset_arr{0, 1};\n   TF_ASSERT_OK(\n       stream->Memcpy(&lhs_offset_0, &lhs_offset_arr[0], offset_length));\n   TF_ASSERT_OK(\n       stream->Memcpy(&lhs_offset_1, &lhs_offset_arr[1], offset_length));\n \n-  se::DeviceMemory<int64_t> rhs_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> rhs_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> rhs_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> rhs_offset_1 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> rhs_offset_arr{2, 0};\n   TF_ASSERT_OK(\n       stream->Memcpy(&rhs_offset_0, &rhs_offset_arr[0], offset_length));\n@@ -512,8 +512,8 @@ TEST_F(DynamicSliceThunkTest, MultipleSlicedOperandsGemm) {\n \n static absl::Status Memcpy(se::Stream* stream, ffi::AnyBuffer src,\n                            ffi::Result<ffi::AnyBuffer> dst) {\n-  se::DeviceMemoryBase dst_mem = dst->device_memory();\n-  se::DeviceMemoryBase src_mem = src.device_memory();\n+  se::DeviceAddressBase dst_mem = dst->device_memory();\n+  se::DeviceAddressBase src_mem = src.device_memory();\n   return stream->MemcpyD2D(&dst_mem, src_mem, src_mem.size());\n }\n \n@@ -612,20 +612,20 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpy) {\n   // s32[1,1,8,8]{3,2,1,0} slice(src), slice={[3:4], [5:6], [2:10], [0:8]}\n \n   // Preparing memory for thunk arguments.\n-  se::DeviceMemory<int32_t> src = executor->AllocateArray<int32_t>(src_count);\n+  se::DeviceAddress<int32_t> src = executor->AllocateArray<int32_t>(src_count);\n   std::vector<int32_t> src_arr(src_count, 0);\n   for (unsigned i = 0; i < src_count; ++i) {\n     src_arr[i] = i;\n   }\n   TF_ASSERT_OK(stream->Memcpy(&src, src_arr.data(), src_length));\n \n-  se::DeviceMemory<int32_t> dst = executor->AllocateArray<int32_t>(dst_count);\n+  se::DeviceAddress<int32_t> dst = executor->AllocateArray<int32_t>(dst_count);\n   TF_ASSERT_OK(stream->MemZero(&dst, dst_length));\n \n-  se::DeviceMemory<int64_t> offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> offset_1 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> offset_2 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> offset_3 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> offset_2 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> offset_3 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> offset_arr{3, 5, 2, 0};\n   TF_ASSERT_OK(stream->Memcpy(&offset_0, &offset_arr[0], offset_length));\n   TF_ASSERT_OK(stream->Memcpy(&offset_1, &offset_arr[1], offset_length));\n@@ -783,20 +783,20 @@ TEST_F(DynamicSliceThunkTest, SlicedOutputMemcpy) {\n   // s32[1,1,2,2]{3,2,1,0} slice(dst), slice={[1:2], [1:2], [0:2], [0:2]}\n \n   // Preparing memory for thunk arguments.\n-  se::DeviceMemory<int32_t> src = executor->AllocateArray<int32_t>(src_count);\n+  se::DeviceAddress<int32_t> src = executor->AllocateArray<int32_t>(src_count);\n   std::vector<int32_t> src_arr(src_count, 0);\n   for (unsigned i = 0; i < src_count; ++i) {\n     src_arr[i] = i;\n   }\n   TF_ASSERT_OK(stream->Memcpy(&src, src_arr.data(), src_length));\n \n-  se::DeviceMemory<int32_t> dst = executor->AllocateArray<int32_t>(dst_count);\n+  se::DeviceAddress<int32_t> dst = executor->AllocateArray<int32_t>(dst_count);\n   TF_ASSERT_OK(stream->MemZero(&dst, dst_length));\n \n-  se::DeviceMemory<int64_t> src_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> src_offset_1 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> src_offset_2 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> src_offset_3 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_2 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_3 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> src_offset_arr{3, 5, 2, 0};\n   TF_ASSERT_OK(\n       stream->Memcpy(&src_offset_0, &src_offset_arr[0], offset_length));\n@@ -807,10 +807,10 @@ TEST_F(DynamicSliceThunkTest, SlicedOutputMemcpy) {\n   TF_ASSERT_OK(\n       stream->Memcpy(&src_offset_3, &src_offset_arr[3], offset_length));\n \n-  se::DeviceMemory<int64_t> dst_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> dst_offset_1 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> dst_offset_2 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> dst_offset_3 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_2 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_3 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> dst_offset_arr{1, 1, 0, 0};\n   TF_ASSERT_OK(\n       stream->Memcpy(&dst_offset_0, &dst_offset_arr[0], offset_length));\n@@ -981,26 +981,26 @@ TEST_F(DynamicSliceThunkTest, SlicedGemmArbitraryArgumentOrder) {\n   // Preparing memory for thunk arguments.\n   // lhs = [1.0, 2.0, 3.0, 4.0,\n   //        5.0, 6.0, 7.0, 8.0]\n-  se::DeviceMemory<float> lhs = executor->AllocateArray<float>(2 * 4);\n+  se::DeviceAddress<float> lhs = executor->AllocateArray<float>(2 * 4);\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n   // rhs = [1.0,\n   //        1.0,\n   //        1.0]\n-  se::DeviceMemory<float> rhs = executor->AllocateArray<float>(3 * 1);\n+  se::DeviceAddress<float> rhs = executor->AllocateArray<float>(3 * 1);\n   std::vector<float> rhs_arr(3, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemory<float> out = executor->AllocateArray<float>(1 * 1);\n+  se::DeviceAddress<float> out = executor->AllocateArray<float>(1 * 1);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n-  se::DeviceMemory<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> lhs_offset_arr{0, 1};\n   TF_ASSERT_OK(\n       stream->Memcpy(&lhs_offset_0, &lhs_offset_arr[0], offset_length));\n@@ -1153,26 +1153,26 @@ TEST_F(DynamicSliceThunkTest, SlicedGemmArbitraryNumberOfArguments) {\n   // Preparing memory for thunk arguments.\n   // lhs = [1.0, 2.0, 3.0, 4.0,\n   //        5.0, 6.0, 7.0, 8.0]\n-  se::DeviceMemory<float> lhs = executor->AllocateArray<float>(2 * 4);\n+  se::DeviceAddress<float> lhs = executor->AllocateArray<float>(2 * 4);\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n   // rhs = [1.0,\n   //        1.0,\n   //        1.0]\n-  se::DeviceMemory<float> rhs = executor->AllocateArray<float>(3 * 1);\n+  se::DeviceAddress<float> rhs = executor->AllocateArray<float>(3 * 1);\n   std::vector<float> rhs_arr(3, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemory<float> out = executor->AllocateArray<float>(1 * 1);\n+  se::DeviceAddress<float> out = executor->AllocateArray<float>(1 * 1);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n-  se::DeviceMemory<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> lhs_offset_arr{0, 1};\n   TF_ASSERT_OK(\n       stream->Memcpy(&lhs_offset_0, &lhs_offset_arr[0], offset_length));\n@@ -1183,7 +1183,7 @@ TEST_F(DynamicSliceThunkTest, SlicedGemmArbitraryNumberOfArguments) {\n   ServiceExecutableRunOptions run_options;\n   se::StreamExecutorMemoryAllocator allocator(executor);\n   BufferAllocations allocations(\n-      {workspace, /*garbage, to be ignored*/ se::DeviceMemoryBase(), out, rhs,\n+      {workspace, /*garbage, to be ignored*/ se::DeviceAddressBase(), out, rhs,\n        lhs_offset_0, lhs_offset_1, /*garbage, to be ignored*/ rhs, lhs},\n       0, &allocator);\n \n@@ -1316,30 +1316,30 @@ TEST_F(DynamicSliceThunkTest, SlicedTupledOperandGemm) {\n   // The `lhs` slice that we want to use will be equivalent to this static\n   // slice op:\n   // f32[1,3]{1,0} slice(lhs), slice={[0:1], [1:4]}\n-  se::DeviceMemory<float> lhs_whole_buffer =\n+  se::DeviceAddress<float> lhs_whole_buffer =\n       executor->AllocateArray<float>(2 * 4 * 3);\n   TF_ASSERT_OK(stream->MemZero(&lhs_whole_buffer, 2 * 4 * 3));\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n-  se::DeviceMemoryBase lhs =\n+  se::DeviceAddressBase lhs =\n       lhs_whole_buffer.GetByteSlice(lhs_length, lhs_length);\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n   // rhs = [1.0,\n   //        1.0,\n   //        1.0]\n-  se::DeviceMemory<float> rhs = executor->AllocateArray<float>(3 * 1);\n+  se::DeviceAddress<float> rhs = executor->AllocateArray<float>(3 * 1);\n   std::vector<float> rhs_arr(3, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemory<float> out = executor->AllocateArray<float>(1 * 1);\n+  se::DeviceAddress<float> out = executor->AllocateArray<float>(1 * 1);\n   TF_ASSERT_OK(stream->MemZero(&out, out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n-  se::DeviceMemory<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> lhs_offset_arr{0, 1};\n   TF_ASSERT_OK(\n       stream->Memcpy(&lhs_offset_0, &lhs_offset_arr[0], offset_length));\n@@ -1491,20 +1491,20 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpyOOB) {\n   // s32[1,1,2,2]{3,2,1,0} slice(dst), slice={[1:2], [1:2], [0:2], [0:2]}\n \n   // Preparing memory for thunk arguments.\n-  se::DeviceMemory<int32_t> src = executor->AllocateArray<int32_t>(src_count);\n+  se::DeviceAddress<int32_t> src = executor->AllocateArray<int32_t>(src_count);\n   std::vector<int32_t> src_arr(src_count, 0);\n   for (unsigned i = 0; i < src_count; ++i) {\n     src_arr[i] = i;\n   }\n   TF_ASSERT_OK(stream->Memcpy(&src, src_arr.data(), src_length));\n \n-  se::DeviceMemory<int32_t> dst = executor->AllocateArray<int32_t>(dst_count);\n+  se::DeviceAddress<int32_t> dst = executor->AllocateArray<int32_t>(dst_count);\n   TF_ASSERT_OK(stream->MemZero(&dst, dst_length));\n \n-  se::DeviceMemory<int64_t> src_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> src_offset_1 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> src_offset_2 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> src_offset_3 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_2 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> src_offset_3 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> src_ref_offset_arr{3, 5, 2, 0};\n   std::vector<int64_t> src_offset_arr{3, 5, 2, -3};\n   TF_ASSERT_OK(\n@@ -1516,10 +1516,10 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpyOOB) {\n   TF_ASSERT_OK(\n       stream->Memcpy(&src_offset_3, &src_offset_arr[3], offset_length));\n \n-  se::DeviceMemory<int64_t> dst_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> dst_offset_1 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> dst_offset_2 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> dst_offset_3 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_2 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> dst_offset_3 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> dst_ref_offset_arr{1, 1, 0, 0};\n   std::vector<int64_t> dst_offset_arr{3, 2, 5, -4};\n   TF_ASSERT_OK(\n@@ -1692,30 +1692,30 @@ TEST_F(DynamicSliceThunkTest, SlicedOperandsSameBufferGemm) {\n   // The `lhs` slice that we want to use will be equivalent to this static\n   // slice op:\n   // f32[1,3]{1,0} slice(lhs), slice={[0:1], [1:4]}\n-  se::DeviceMemory<float> buffer =\n+  se::DeviceAddress<float> buffer =\n       executor->AllocateArray<float>(lhs_length + rhs_length + out_length);\n   TF_ASSERT_OK(stream->MemZero(&buffer, lhs_length + rhs_length + out_length));\n \n-  se::DeviceMemoryBase lhs = buffer.GetByteSlice(0, lhs_length);\n+  se::DeviceAddressBase lhs = buffer.GetByteSlice(0, lhs_length);\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(&lhs, lhs_arr.data(), lhs_length));\n \n   // rhs = [1.0,\n   //        1.0,\n   //        1.0]\n-  se::DeviceMemoryBase rhs = buffer.GetByteSlice(lhs_length, rhs_length);\n+  se::DeviceAddressBase rhs = buffer.GetByteSlice(lhs_length, rhs_length);\n   std::vector<float> rhs_arr(3, 1);\n   TF_ASSERT_OK(stream->Memcpy(&rhs, rhs_arr.data(), rhs_length));\n \n-  se::DeviceMemoryBase out =\n+  se::DeviceAddressBase out =\n       buffer.GetByteSlice(lhs_length + rhs_length, out_length);\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       executor->AllocateArray<float>(1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(&workspace, 1024 * 1024));\n \n-  se::DeviceMemory<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n-  se::DeviceMemory<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_0 = executor->AllocateArray<int64_t>(1);\n+  se::DeviceAddress<int64_t> lhs_offset_1 = executor->AllocateArray<int64_t>(1);\n   std::vector<int64_t> lhs_offset_arr{0, 1};\n   TF_ASSERT_OK(\n       stream->Memcpy(&lhs_offset_0, &lhs_offset_arr[0], offset_length));\n@@ -1914,7 +1914,7 @@ TEST_F(DynamicSliceThunkTest,\n   // Preparing memory for thunk arguments.\n   // lhs = [1.0, 2.0, 3.0, 4.0,\n   //        5.0, 6.0, 7.0, 8.0]\n-  se::DeviceMemory<float> lhs =\n+  se::DeviceAddress<float> lhs =\n       executor->AllocateArray<float>(/*element_count=*/2 * 4);\n   std::vector<float> lhs_arr{1, 2, 3, 4, 5, 6, 7, 8};\n   TF_ASSERT_OK(stream->Memcpy(/*gpu_dst=*/&lhs, /*host_src=*/lhs_arr.data(),\n@@ -1924,17 +1924,17 @@ TEST_F(DynamicSliceThunkTest,\n   //        3.0,\n   //        2.0,\n   //        1.0]\n-  se::DeviceMemory<float> rhs =\n+  se::DeviceAddress<float> rhs =\n       executor->AllocateArray<float>(/*element_count=*/4 * 1);\n   std::vector<float> rhs_arr{4, 3, 2, 1};\n   TF_ASSERT_OK(stream->Memcpy(/*gpu_dst=*/&rhs, /*host_src=*/rhs_arr.data(),\n                               /*size=*/rhs_length));\n \n-  se::DeviceMemory<float> out =\n+  se::DeviceAddress<float> out =\n       executor->AllocateArray<float>(/*element_count=*/1 * 1);\n   TF_ASSERT_OK(stream->MemZero(/*location=*/&out, /*size=*/out_length));\n \n-  se::DeviceMemory<float> workspace =\n+  se::DeviceAddress<float> workspace =\n       executor->AllocateArray<float>(/*element_count=*/1024 * 1024);\n   TF_ASSERT_OK(stream->MemZero(/*location=*/&workspace, /*size=*/1024 * 1024));\n "
        },
        {
            "sha": "a2cf3258f202ed3c0d6d2998fb637d56b0a4cf2e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/fft_thunk.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -34,8 +34,8 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/fft.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -146,12 +146,12 @@ absl::Status FftThunk::ExecuteOnStream(const ExecuteParams& params) {\n       &fft_plan_cache_, params.stream, buffer_allocations.memory_allocator());\n }\n \n-absl::Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,\n-                    se::DeviceMemoryBase output, const Shape& output_shape,\n+absl::Status RunFft(se::DeviceAddressBase input, const Shape& input_shape,\n+                    se::DeviceAddressBase output, const Shape& output_shape,\n                     se::fft::Type fft_type, absl::Span<const int64_t> fft_len,\n                     int device_ordinal, FftPlanCache* fft_plan_cache,\n                     se::Stream* stream,\n-                    se::DeviceMemoryAllocator* memory_allocator) {\n+                    se::DeviceAddressAllocator* memory_allocator) {\n   VLOG(3) << \"FFT type: \" << FftTypeToString(fft_type);\n   VLOG(3) << \"Input shape: \" << ShapeUtil::HumanStringWithLayout(input_shape);\n   VLOG(3) << \"Output shape: \" << ShapeUtil::HumanStringWithLayout(output_shape);\n@@ -209,20 +209,20 @@ absl::Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,\n   bool launch_ok;\n   switch (fft_type) {\n     case se::fft::Type::kC2CForward: {\n-      se::DeviceMemory<complex64> input_data(input);\n-      se::DeviceMemory<complex64> output_data(output);\n+      se::DeviceAddress<complex64> input_data(input);\n+      se::DeviceAddress<complex64> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       break;\n     }\n     case se::fft::Type::kZ2ZForward: {\n-      se::DeviceMemory<complex128> input_data(input);\n-      se::DeviceMemory<complex128> output_data(output);\n+      se::DeviceAddress<complex128> input_data(input);\n+      se::DeviceAddress<complex128> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       break;\n     }\n     case se::fft::Type::kC2CInverse: {\n-      se::DeviceMemory<complex64> input_data(input);\n-      se::DeviceMemory<complex64> output_data(output);\n+      se::DeviceAddress<complex64> input_data(input);\n+      se::DeviceAddress<complex64> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       if (launch_ok) {\n         TF_ASSIGN_OR_RETURN(auto blas, GetBlas(stream));\n@@ -233,8 +233,8 @@ absl::Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,\n       break;\n     }\n     case se::fft::Type::kZ2ZInverse: {\n-      se::DeviceMemory<complex128> input_data(input);\n-      se::DeviceMemory<complex128> output_data(output);\n+      se::DeviceAddress<complex128> input_data(input);\n+      se::DeviceAddress<complex128> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       if (launch_ok) {\n         TF_ASSIGN_OR_RETURN(auto blas, GetBlas(stream));\n@@ -245,20 +245,20 @@ absl::Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,\n       break;\n     }\n     case se::fft::Type::kR2C: {\n-      se::DeviceMemory<float> input_data(input);\n-      se::DeviceMemory<complex64> output_data(output);\n+      se::DeviceAddress<float> input_data(input);\n+      se::DeviceAddress<complex64> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       break;\n     }\n     case se::fft::Type::kD2Z: {\n-      se::DeviceMemory<double> input_data(input);\n-      se::DeviceMemory<complex128> output_data(output);\n+      se::DeviceAddress<double> input_data(input);\n+      se::DeviceAddress<complex128> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       break;\n     }\n     case se::fft::Type::kC2R: {\n-      se::DeviceMemory<complex64> input_data(input);\n-      se::DeviceMemory<float> output_data(output);\n+      se::DeviceAddress<complex64> input_data(input);\n+      se::DeviceAddress<float> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       if (launch_ok) {\n         TF_ASSIGN_OR_RETURN(auto blas, GetBlas(stream));\n@@ -269,8 +269,8 @@ absl::Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,\n       break;\n     }\n     case se::fft::Type::kZ2D: {\n-      se::DeviceMemory<complex128> input_data(input);\n-      se::DeviceMemory<double> output_data(output);\n+      se::DeviceAddress<complex128> input_data(input);\n+      se::DeviceAddress<double> output_data(output);\n       launch_ok = fft->DoFft(stream, fft_plan.get(), input_data, &output_data);\n       if (launch_ok) {\n         TF_ASSIGN_OR_RETURN(auto blas, GetBlas(stream));"
        },
        {
            "sha": "7f8af58d62444e01a4337ffa2a6edf37d28b8e34",
            "filename": "third_party/xla/xla/backends/gpu/runtime/fft_thunk.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -31,8 +31,8 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/fft.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -111,12 +111,12 @@ class FftThunk : public Thunk {\n   const Shape output_shape_;\n };\n \n-absl::Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,\n-                    se::DeviceMemoryBase output, const Shape& output_shape,\n+absl::Status RunFft(se::DeviceAddressBase input, const Shape& input_shape,\n+                    se::DeviceAddressBase output, const Shape& output_shape,\n                     se::fft::Type fft_type,\n                     absl::Span<const int64_t> fft_length, int device_ordinal,\n                     FftPlanCache* fft_plan_cache, se::Stream* stream,\n-                    se::DeviceMemoryAllocator* memory_allocator);\n+                    se::DeviceAddressAllocator* memory_allocator);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "e756b11003ec092b8394775209d819b0a729d872",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gemm_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgemm_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgemm_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgemm_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -28,7 +28,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -54,7 +54,7 @@ GemmThunk::GemmThunk(ThunkInfo thunk_info, GemmConfig config,\n absl::Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {\n   VLOG(3) << \"Running GEMM thunk\";\n   const BufferAllocations& allocs = *params.buffer_allocations;\n-  se::DeviceMemoryBase workspace(/*opaque=*/nullptr, /*size=*/0);\n+  se::DeviceAddressBase workspace(/*opaque=*/nullptr, /*size=*/0);\n   if (workspace_.has_value()) {\n     workspace = allocs.GetDeviceAddress(workspace_.value());\n   }"
        },
        {
            "sha": "fdc7d63891d797bb1b55d6366893b69956e03e18",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -93,7 +93,7 @@ absl::Status CublasLtMatmulThunk::ExecuteOnStreamInternal(\n   VLOG(3) << \"Running cublas_lt matmul thunk\";\n   const BufferAllocations& allocs = *params.buffer_allocations;\n \n-  se::DeviceMemoryBase bias, a_scale, b_scale, c_scale, d_scale, d_amax, aux,\n+  se::DeviceAddressBase bias, a_scale, b_scale, c_scale, d_scale, d_amax, aux,\n       workspace;\n   if (bias_.allocation() != nullptr) {\n     bias = allocs.GetDeviceAddress(bias_);"
        },
        {
            "sha": "ccdf653ca1862e6554f616b225f190183c1fdaf2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -47,8 +47,8 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n@@ -169,7 +169,7 @@ class GpuBlasLtThunkBuilder {\n   }\n \n   std::unique_ptr<BufferAllocations> buffer_allocations() {\n-    std::vector<se::DeviceMemoryBase> buffers(mem_buffers_.size());\n+    std::vector<se::DeviceAddressBase> buffers(mem_buffers_.size());\n     for (size_t i = 0; i < buffers.size(); i++) {\n       buffers[i] = *mem_buffers_[i];\n     }"
        },
        {
            "sha": "732462ab004c75296c388f430429648ca17d2919",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_execute_thunk_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -44,7 +44,7 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n@@ -114,8 +114,8 @@ TEST(HostExecuteStartThunkTest, SingleArgSingleResult) {\n   TF_ASSERT_OK_AND_ASSIGN(auto hlo_module,\n                           ParseAndReturnUnverifiedModule(kHloModule, {}));\n \n-  se::DeviceMemoryBase arg = stream_executor->Allocate(1 * sizeof(int32_t));\n-  se::DeviceMemoryBase result = stream_executor->Allocate(1 * sizeof(int32_t));\n+  se::DeviceAddressBase arg = stream_executor->Allocate(1 * sizeof(int32_t));\n+  se::DeviceAddressBase result = stream_executor->Allocate(1 * sizeof(int32_t));\n \n   TF_ASSERT_OK(stream->Memset32(&arg, 5, 4));\n   TF_ASSERT_OK(stream->MemZero(&result, 4));\n@@ -184,10 +184,12 @@ TEST(HostExecuteStartThunkTest, MultiArgMultipleResult) {\n   TF_ASSERT_OK_AND_ASSIGN(auto hlo_module,\n                           ParseAndReturnUnverifiedModule(kHloModule, {}));\n \n-  se::DeviceMemoryBase arg0 = stream_executor->Allocate(1 * sizeof(int32_t));\n-  se::DeviceMemoryBase arg1 = stream_executor->Allocate(1 * sizeof(int32_t));\n-  se::DeviceMemoryBase result0 = stream_executor->Allocate(1 * sizeof(int32_t));\n-  se::DeviceMemoryBase result1 = stream_executor->Allocate(1 * sizeof(int32_t));\n+  se::DeviceAddressBase arg0 = stream_executor->Allocate(1 * sizeof(int32_t));\n+  se::DeviceAddressBase arg1 = stream_executor->Allocate(1 * sizeof(int32_t));\n+  se::DeviceAddressBase result0 =\n+      stream_executor->Allocate(1 * sizeof(int32_t));\n+  se::DeviceAddressBase result1 =\n+      stream_executor->Allocate(1 * sizeof(int32_t));\n \n   TF_ASSERT_OK(stream->Memset32(&arg0, 5, 4));\n   TF_ASSERT_OK(stream->Memset32(&arg1, 3, 4));\n@@ -277,10 +279,10 @@ TEST(HostExecuteStartThunkTest, ArgAndResultPinnedOnHost) {\n       auto result_memory_allocation,\n       stream_executor->HostMemoryAllocate(1 * sizeof(int32_t)));\n \n-  se::DeviceMemoryBase arg(arg_memory_allocation->opaque(),\n-                           arg_memory_allocation->size());\n-  se::DeviceMemoryBase result(result_memory_allocation->opaque(),\n-                              result_memory_allocation->size());\n+  se::DeviceAddressBase arg(arg_memory_allocation->opaque(),\n+                            arg_memory_allocation->size());\n+  se::DeviceAddressBase result(result_memory_allocation->opaque(),\n+                               result_memory_allocation->size());\n \n   // Prepare buffer allocations for recording command buffer.\n   BufferAllocation alloc_arg(/*index=*/0, 4, /*color=*/0);\n@@ -352,10 +354,10 @@ TEST(HostExecuteStartThunkTest, ArgAndResultInSharedMemory) {\n       auto result_memory_allocation,\n       unified_memory_allocator->Allocate(1 * sizeof(int32_t)));\n \n-  se::DeviceMemoryBase arg(arg_memory_allocation->opaque(),\n-                           arg_memory_allocation->size());\n-  se::DeviceMemoryBase result(result_memory_allocation->opaque(),\n-                              result_memory_allocation->size());\n+  se::DeviceAddressBase arg(arg_memory_allocation->opaque(),\n+                            arg_memory_allocation->size());\n+  se::DeviceAddressBase result(result_memory_allocation->opaque(),\n+                               result_memory_allocation->size());\n \n   // Prepare buffer allocations for recording command buffer.\n   BufferAllocation alloc_arg(/*index=*/0, 4, /*color=*/0);\n@@ -415,8 +417,8 @@ TEST(HostExecuteStartThunkTest, ArgAndResultNonRegisteredHostMemory) {\n   alignas(xla::cpu::Align()) int32_t arg_value = 5;\n   alignas(xla::cpu::Align()) int32_t result_value = 0;\n \n-  se::DeviceMemoryBase arg(&arg_value, sizeof(int32_t));\n-  se::DeviceMemoryBase result(&result_value, sizeof(int32_t));\n+  se::DeviceAddressBase arg(&arg_value, sizeof(int32_t));\n+  se::DeviceAddressBase result(&result_value, sizeof(int32_t));\n \n   // Prepare buffer allocations for recording command buffer.\n   BufferAllocation alloc_arg(/*index=*/0, 4, /*color=*/0);\n@@ -484,8 +486,8 @@ TEST(HostExecuteStartThunkTest, TestErrorPropagationFromExecuteEvent) {\n   int32_t arg_value = 5;\n   int32_t result_value = 0;\n \n-  se::DeviceMemoryBase arg(&arg_value, sizeof(int32_t));\n-  se::DeviceMemoryBase result(&result_value, sizeof(int32_t));\n+  se::DeviceAddressBase arg(&arg_value, sizeof(int32_t));\n+  se::DeviceAddressBase result(&result_value, sizeof(int32_t));\n \n   // Prepare buffer allocations for recording command buffer.\n   BufferAllocation alloc_arg(/*index=*/0, 4, /*color=*/0);"
        },
        {
            "sha": "3ef5ab1130c09fba1745ae4b1730c9de4df80731",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_send_recv_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -34,7 +34,7 @@ limitations under the License.\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -186,7 +186,7 @@ absl::Status HostSendThunk::ExecuteOnStream(const ExecuteParams& params) {\n     stream = params.stream;\n   }\n \n-  se::DeviceMemoryBase src =\n+  se::DeviceAddressBase src =\n       params.buffer_allocations->GetDeviceAddress(buffer_);\n \n   // Send buffer to a handler registered with the executable.\n@@ -382,7 +382,7 @@ absl::Status HostRecvThunk::ExecuteOnStream(const ExecuteParams& params) {\n     stream = params.stream;\n   }\n \n-  se::DeviceMemoryBase dst =\n+  se::DeviceAddressBase dst =\n       params.buffer_allocations->GetDeviceAddress(buffer_);\n \n   // Recv buffer from a handler registered with the run options."
        },
        {
            "sha": "49926e8e76b98ddc80313c3b4b6fe16bb8bd6ad7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/infeed_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Finfeed_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Finfeed_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Finfeed_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -33,8 +33,8 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -52,15 +52,15 @@ absl::Status InfeedThunk::ExecuteOnStream(const ExecuteParams& params) {\n   const BufferAllocations& buffer_allocations = *params.buffer_allocations;\n \n   VLOG(2) << \"Infeeding to GPU\";\n-  ShapeTree<se::DeviceMemoryHandle> source_buffers =\n+  ShapeTree<se::DeviceAddressHandle> source_buffers =\n       GpuTransferManager::GetOrCreateInfeedManager(stream.parent())\n           ->BlockingGetNextDestination();\n \n   size_t index = 0;\n   for (auto& source : source_buffers.leaves()) {\n     // Assert that the shapes are compatible.\n     const ShapeIndex& shape_index = source.first;\n-    se::DeviceMemoryHandle& buffer = source.second;\n+    se::DeviceAddressHandle& buffer = source.second;\n     const Shape& source_shape =\n         ShapeUtil::GetSubshape(source_buffers.shape(), shape_index);\n     TF_RET_CHECK(\n@@ -69,7 +69,7 @@ absl::Status InfeedThunk::ExecuteOnStream(const ExecuteParams& params) {\n         << ShapeUtil::HumanStringWithLayout(source_shape)\n         << \" and infeed dest buffer shape \"\n         << ShapeUtil::HumanStringWithLayout(dest_slices_[index].shape);\n-    se::DeviceMemoryBase dest_address =\n+    se::DeviceAddressBase dest_address =\n         buffer_allocations.GetDeviceAddress(dest_slices_[index++].slice);\n     TF_RETURN_IF_ERROR(\n         stream.Memcpy(&dest_address, buffer.memory(), buffer.memory().size()));"
        },
        {
            "sha": "c96d35c77717a79ed693ca6adcbb528926a5b30a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n@@ -217,7 +217,7 @@ absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n     int device_ordinal = executor->device_ordinal();\n     XLA_VLOG_DEVICE(3, device_ordinal) << \"Launching \" << kernel->name();\n     for (const auto& [idx, arg] : llvm::enumerate(args_)) {\n-      se::DeviceMemoryBase buf =\n+      se::DeviceAddressBase buf =\n           params.buffer_allocations->GetDeviceAddress(arg);\n       XLA_VLOG_DEVICE(3, device_ordinal)\n           << \"Arg: alloc #\" << arg.index() << \", offset: \" << arg.offset()"
        },
        {
            "sha": "3af6ad766ccf95876ef33ac6f23a7e3377433daf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/make_batch_pointers.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n #include <cstddef>\n \n #include \"absl/status/status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/gpu/make_batch_pointers_kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -31,9 +31,9 @@ limitations under the License.\n namespace xla::gpu {\n \n absl::Status MakeBatchPointers(se::Stream* stream,\n-                               se::DeviceMemoryBase base_ptr,\n+                               se::DeviceAddressBase base_ptr,\n                                size_t stride_bytes, size_t n,\n-                               se::DeviceMemoryBase ptrs_out) {\n+                               se::DeviceAddressBase ptrs_out) {\n   se::StreamExecutor* executor = stream->parent();\n   size_t threads_per_block = [&] {\n     if (executor->GetPlatform()->id() =="
        },
        {
            "sha": "795cb9b8b437b7c6e02f9112efaebb59c28b7c37",
            "filename": "third_party/xla/xla/backends/gpu/runtime/make_batch_pointers.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n #include <cstddef>\n \n #include \"absl/status/status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/types.h\"  // IWYU pragma: keep\n \n@@ -50,9 +50,9 @@ namespace xla::gpu {\n //    need to allocate the host memory as pinned, one alloc per stream.  Then\n //    we'd need to manage this memory without leaks.  This becomes complex!\n absl::Status MakeBatchPointers(se::Stream* stream,\n-                               se::DeviceMemoryBase base_ptr,\n+                               se::DeviceAddressBase base_ptr,\n                                size_t stride_bytes, size_t n,\n-                               se::DeviceMemoryBase ptrs_out);\n+                               se::DeviceAddressBase ptrs_out);\n \n }  // namespace xla::gpu\n "
        },
        {
            "sha": "e0685b6161ec05c212a566d826a08a1d505e27a6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/make_batch_pointers_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -47,8 +47,8 @@ TEST(MakeBatchPointersTest, Basic) {\n \n   // We don't care what `base` points to, we only need a pointer to a buffer\n   // that we can use as a base.\n-  stream_executor::DeviceMemory<char> base = executor->AllocateScalar<char>();\n-  stream_executor::DeviceMemory<void*> ptrs_out =\n+  stream_executor::DeviceAddress<char> base = executor->AllocateScalar<char>();\n+  stream_executor::DeviceAddress<void*> ptrs_out =\n       executor->AllocateArray<void*>(8);\n \n   constexpr int kStride = 13;"
        },
        {
            "sha": "a370a04753620b2f71a891ad6d808f41185c696c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/memset_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmemset_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmemset_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmemset_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -22,14 +22,14 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n \n absl::Status MemzeroThunk::ExecuteOnStream(const ExecuteParams& params) {\n-  se::DeviceMemoryBase dest_data =\n+  se::DeviceAddressBase dest_data =\n       params.buffer_allocations->GetDeviceAddress(dest_);\n   return params.stream->MemZero(&dest_data, dest_data.size());\n }\n@@ -55,7 +55,7 @@ absl::StatusOr<ThunkProto> MemzeroThunk::ToProto() const {\n \n absl::Status Memset32BitValueThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n-  se::DeviceMemoryBase dest_data =\n+  se::DeviceAddressBase dest_data =\n       params.buffer_allocations->GetDeviceAddress(dest_);\n   return params.stream->Memset32(&dest_data, value_, dest_data.size());\n }"
        },
        {
            "sha": "153d4f6e9e7048317c9fd7605645cd9f29d65e2f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/norm_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/gpu_norm_runner.h\"\n #include \"xla/service/gpu/gpu_norm_runner.pb.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -99,14 +99,14 @@ NormRunner& NormThunk::GetOrCreateRunner(\n absl::Status NormThunk::ExecuteOnStream(const ExecuteParams& params) {\n   const auto& buffer_allocations = *params.buffer_allocations;\n \n-  se::DeviceMemoryBase x_se_buffer =\n+  se::DeviceAddressBase x_se_buffer =\n       buffer_allocations.GetDeviceAddress(x_buffer_);\n-  se::DeviceMemoryBase scale_se_buffer =\n+  se::DeviceAddressBase scale_se_buffer =\n       buffer_allocations.GetDeviceAddress(scale_buffer_);\n-  se::DeviceMemoryBase y_or_dx_se_buffer =\n+  se::DeviceAddressBase y_or_dx_se_buffer =\n       buffer_allocations.GetDeviceAddress(y_or_dx_buffer_);\n \n-  std::optional<se::DeviceMemoryBase> bias_se_buffer, expectation_se_buffer,\n+  std::optional<se::DeviceAddressBase> bias_se_buffer, expectation_se_buffer,\n       norm_factor_se_buffer, dy_se_buffer, dscale_se_buffer, dbias_se_buffer;\n   if (bias_buffer_) {\n     bias_se_buffer = buffer_allocations.GetDeviceAddress(bias_buffer_.value());\n@@ -125,7 +125,7 @@ absl::Status NormThunk::ExecuteOnStream(const ExecuteParams& params) {\n         buffer_allocations.GetDeviceAddress(dbias_buffer_.value());\n   }\n \n-  se::DeviceMemoryBase scratch =\n+  se::DeviceAddressBase scratch =\n       buffer_allocations.GetDeviceAddress(scratch_buffer_);\n \n   RunNormOptions opts;"
        },
        {
            "sha": "6dac0d9b4a935a4bc0fd2f2942064e866b0af5bd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_permute_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -45,7 +45,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -161,7 +161,7 @@ absl::Status RunCollectivePermute(P2PConfig::SourceTargetMapEntry source_target,\n   std::optional<int64_t> source_id = source_target.source;\n   std::optional<int64_t> target_id = source_target.target;\n \n-  std::vector<se::DeviceMemoryBase> src_addrs, dest_addrs;\n+  std::vector<se::DeviceAddressBase> src_addrs, dest_addrs;\n   absl::c_transform(\n       buffers, std::back_inserter(src_addrs),\n       [](const DeviceBufferPair& buffer) { return buffer.source_buffer; });"
        },
        {
            "sha": "7c30a373e730c1d002131e3625b9ccc430d688ba",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_send_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_send_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_send_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_send_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -120,8 +120,8 @@ absl::Status NvshmemSendThunk::RunNvshmemCollective(const ExecuteParams& params,\n   if (recv_buffer_status.ok()) {\n     void* recv_buffer_ptr = recv_buffer_status.value();\n     VLOG(3) << \"Using existing receive buffer for send: \" << recv_buffer_ptr;\n-    buffer.destination_buffer =\n-        se::DeviceMemoryBase(recv_buffer_ptr, buffer.destination_buffer.size());\n+    buffer.destination_buffer = se::DeviceAddressBase(\n+        recv_buffer_ptr, buffer.destination_buffer.size());\n   } else {\n     VLOG(3) << \"No receive buffer found\";\n   }"
        },
        {
            "sha": "bb9259d19a7cf6ad438c23368eaf588252e07326",
            "filename": "third_party/xla/xla/backends/gpu/runtime/outfeed_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Foutfeed_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Foutfeed_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Foutfeed_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -103,7 +103,7 @@ absl::Status OutfeedThunk::ExecuteOnStream(const ExecuteParams& params) {\n     if (!source_slice.allocation()) {\n       return Internal(\"outfeed source missing buffer allocation\");\n     }\n-    se::DeviceMemoryBase data_address =\n+    se::DeviceAddressBase data_address =\n         buffer_allocations.GetDeviceAddress(source_slice);\n \n     // TODO(b/111309141): Run this on a separate stream so it doesn't block"
        },
        {
            "sha": "4c39fa2cf7f1799a04965713f58f507435fb20b9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/print_buffer_contents.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/stream.h\"\n \n@@ -43,7 +43,7 @@ void PrintBufferContents(stream_executor::Stream*, int input_idx,\n }\n \n void PrintBufferContents(stream_executor::Stream* stream, int input_idx,\n-                         stream_executor::DeviceMemoryBase buf) {\n+                         stream_executor::DeviceAddressBase buf) {\n   auto host_buffer = std::make_unique<char[]>(buf.size());\n   CHECK_OK(stream->Memcpy(host_buffer.get(), buf, buf.size()));\n   CHECK_OK(stream->BlockHostUntilDone());"
        },
        {
            "sha": "bf5635b4eb386a00e9991038ac47ed3371973201",
            "filename": "third_party/xla/xla/backends/gpu/runtime/print_buffer_contents_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"absl/log/scoped_mock_log.h\"\n #include \"absl/strings/ascii.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -46,7 +46,7 @@ TEST(PrintBufferContentsTest, PrintBufferContents) {\n \n   auto stream = executor->CreateStream().value();\n \n-  stream_executor::DeviceMemory<int> arg1 =\n+  stream_executor::DeviceAddress<int> arg1 =\n       executor->AllocateArray<int32_t>(10, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&arg1, 0x12345678, 10 * sizeof(int32_t)));"
        },
        {
            "sha": "c41efa553a26a298f1098b40c762f8a54096d115",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n #include \"xla/primitive_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/gpu/ragged_all_to_all_kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -44,13 +44,13 @@ template <int64_t kVectorSize>\n absl::Status LaunchTypedKernel(\n     se::Stream* stream, se::StreamExecutor* executor,\n     const se::ThreadDim& thread_dims, const se::BlockDim& block_dims,\n-    se::DeviceMemoryBase input_buffer,\n+    se::DeviceAddressBase input_buffer,\n     const std::array<void*,\n                      stream_executor::gpu::kMaxNumRaggedAllToAllOutputPtrs>&\n         output_ptrs,\n-    se::DeviceMemoryBase input_offsets_buffer,\n-    se::DeviceMemoryBase send_sizes_buffer,\n-    se::DeviceMemoryBase output_offsets_buffer, int64_t num_updates_per_output,\n+    se::DeviceAddressBase input_offsets_buffer,\n+    se::DeviceAddressBase send_sizes_buffer,\n+    se::DeviceAddressBase output_offsets_buffer, int64_t num_updates_per_output,\n     int64_t num_row_elements) {\n   TF_ASSIGN_OR_RETURN(\n       auto kernel,\n@@ -75,11 +75,11 @@ bool IsRaggedAllToAllKernelSupported(int64_t num_outputs,\n \n absl::Status RunRaggedAllToAllKernel(\n     se::Stream* stream, PrimitiveType element_type,\n-    se::DeviceMemoryBase input_buffer,\n-    absl::Span<const se::DeviceMemoryBase> output_buffers,\n-    se::DeviceMemoryBase input_offsets_buffer,\n-    se::DeviceMemoryBase send_sizes_buffer,\n-    se::DeviceMemoryBase output_offsets_buffer, int64_t num_outputs,\n+    se::DeviceAddressBase input_buffer,\n+    absl::Span<const se::DeviceAddressBase> output_buffers,\n+    se::DeviceAddressBase input_offsets_buffer,\n+    se::DeviceAddressBase send_sizes_buffer,\n+    se::DeviceAddressBase output_offsets_buffer, int64_t num_outputs,\n     int64_t num_updates_per_output, int64_t num_input_rows,\n     int64_t num_row_elements) {\n   if (output_buffers.size() >"
        },
        {
            "sha": "f5e76e79275291958cf4ea82210ba4b976c66832",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/types.h\"  // IWYU pragma: keep\n #include \"xla/xla_data.pb.h\"\n@@ -49,11 +49,11 @@ bool IsRaggedAllToAllKernelSupported(int64_t num_outputs,\n // the case.\n absl::Status RunRaggedAllToAllKernel(\n     se::Stream* stream, PrimitiveType element_type,\n-    se::DeviceMemoryBase input_buffer,\n-    absl::Span<const se::DeviceMemoryBase> output_buffers,\n-    se::DeviceMemoryBase input_offsets_buffer,\n-    se::DeviceMemoryBase send_sizes_buffer,\n-    se::DeviceMemoryBase output_offsets_buffer, int64_t num_outputs,\n+    se::DeviceAddressBase input_buffer,\n+    absl::Span<const se::DeviceAddressBase> output_buffers,\n+    se::DeviceAddressBase input_offsets_buffer,\n+    se::DeviceAddressBase send_sizes_buffer,\n+    se::DeviceAddressBase output_offsets_buffer, int64_t num_outputs,\n     int64_t num_updates_per_output, int64_t num_input_rows,\n     int64_t num_row_elements);\n }  // namespace xla::gpu"
        },
        {
            "sha": "e1de91cda5ea162c0e790610b2015f4a8584bbdc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -24,8 +24,8 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/types/span.h\"\n #include \"xla/primitive_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -83,24 +83,24 @@ TEST_F(RaggedAllToAllKernelTest, SimpleKernelTest) {\n   constexpr int64_t num_row_elements = 2;\n   constexpr int64_t n = num_input_rows * num_row_elements;\n \n-  stream_executor::DeviceMemoryHandle input_buffer(\n+  stream_executor::DeviceAddressHandle input_buffer(\n       executor, executor->AllocateArray<T>(n));\n \n-  std::vector<stream_executor::DeviceMemoryHandle> output_buffers;\n+  std::vector<stream_executor::DeviceAddressHandle> output_buffers;\n   for (int64_t i = 0; i < num_outputs; ++i) {\n     output_buffers.emplace_back(executor, executor->AllocateArray<T>(n));\n     ASSERT_TRUE(!output_buffers[i].memory().is_null());\n     TF_ASSERT_OK(\n         stream->MemZero(output_buffers[i].memory_ptr(), n * sizeof(T)));\n   }\n \n-  stream_executor::DeviceMemoryHandle input_offsets_buffer(\n+  stream_executor::DeviceAddressHandle input_offsets_buffer(\n       executor,\n       executor->AllocateArray<int64_t>(num_outputs * num_update_per_output));\n-  stream_executor::DeviceMemoryHandle send_sizes_buffer(\n+  stream_executor::DeviceAddressHandle send_sizes_buffer(\n       executor,\n       executor->AllocateArray<int64_t>(num_outputs * num_update_per_output));\n-  stream_executor::DeviceMemoryHandle output_offsets_buffer(\n+  stream_executor::DeviceAddressHandle output_offsets_buffer(\n       executor,\n       executor->AllocateArray<int64_t>(num_outputs * num_update_per_output));\n \n@@ -126,7 +126,7 @@ TEST_F(RaggedAllToAllKernelTest, SimpleKernelTest) {\n                               output_offsets.data(),\n                               output_offsets.size() * sizeof(int64_t)));\n \n-  std::vector<se::DeviceMemoryBase> output_buffers_span;\n+  std::vector<se::DeviceAddressBase> output_buffers_span;\n   for (auto& output_buffer : output_buffers) {\n     output_buffers_span.push_back(output_buffer.memory());\n   }"
        },
        {
            "sha": "f6d65a9a110d136d58cd07923237e821d225fde0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -49,8 +49,8 @@ limitations under the License.\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -108,8 +108,8 @@ absl::Status LoadRaggedTensorMetadata(\n \n // Runs AllToAll on a buffer that contains ragged tensor metadata.\n absl::Status RunAllToAllOnIndexBuffer(\n-    const se::DeviceMemoryBase& source_buffer, int64_t num_updates_per_replica,\n-    const se::DeviceMemoryBase& destination_buffer, PrimitiveType element_type,\n+    const se::DeviceAddressBase& source_buffer, int64_t num_updates_per_replica,\n+    const se::DeviceAddressBase& destination_buffer, PrimitiveType element_type,\n     se::Stream& stream, Communicator& comm) {\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm.NumRanks());\n \n@@ -119,10 +119,10 @@ absl::Status RunAllToAllOnIndexBuffer(\n        &destination_buffer, &stream](GpuCommunicator* comm) -> absl::Status {\n         for (int peer = 0; peer < num_ranks; ++peer) {\n           int64_t offset = peer * num_updates_per_replica;\n-          se::DeviceMemoryBase send_slice =\n+          se::DeviceAddressBase send_slice =\n               GpuCollectives::Slice(source_buffer, element_type, offset,\n                                     /*count=*/num_updates_per_replica);\n-          se::DeviceMemoryBase recv_slice =\n+          se::DeviceAddressBase recv_slice =\n               GpuCollectives::Slice(destination_buffer, element_type, offset,\n                                     /*count=*/num_updates_per_replica);\n           TF_RETURN_IF_ERROR(comm->LaunchSend(send_slice, element_type,\n@@ -144,7 +144,7 @@ absl::Status RunRaggedAllToAll(\n     int64_t ragged_row_element_size, int64_t num_total_updates,\n     const std::vector<DeviceBufferPair>& original_buffers, se::Stream& stream,\n     Communicator& comm, absl::Span<int64_t* const> ragged_metadata_allocs,\n-    const se::DeviceMemoryBase& output_offsets_device_buffer,\n+    const se::DeviceAddressBase& output_offsets_device_buffer,\n     bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n   XLA_VLOG_DEVICE(3, device_ordinal)\n@@ -182,18 +182,18 @@ absl::Status RunRaggedAllToAll(\n        &stream](GpuCommunicator* comm) -> absl::Status {\n         PrimitiveType element_type = buffers[0].element_type;\n \n-        se::DeviceMemoryBase input_buffer = buffers[0].source_buffer;\n-        se::DeviceMemoryBase output_buffer = buffers[1].destination_buffer;\n+        se::DeviceAddressBase input_buffer = buffers[0].source_buffer;\n+        se::DeviceAddressBase output_buffer = buffers[1].destination_buffer;\n \n         for (int64_t i = 0; i < num_updates_per_replica; ++i) {\n           for (int peer = 0; peer < num_ranks; ++peer) {\n             int64_t idx = peer * num_updates_per_replica + i;\n-            se::DeviceMemoryBase send_slice = GpuCollectives::Slice(\n+            se::DeviceAddressBase send_slice = GpuCollectives::Slice(\n                 input_buffer, element_type,\n                 input_offsets[idx] * ragged_row_element_size,\n                 send_sizes[idx] * ragged_row_element_size);\n \n-            se::DeviceMemoryBase recv_slice = GpuCollectives::Slice(\n+            se::DeviceAddressBase recv_slice = GpuCollectives::Slice(\n                 output_buffer, element_type,\n                 output_offsets[idx] * ragged_row_element_size,\n                 recv_sizes[idx] * ragged_row_element_size);\n@@ -218,7 +218,7 @@ absl::Status RunRaggedAllToAll(\n // Contains the values that are passed between host threads with rendezvous.\n struct RendezvousValue {\n   RankId rank;\n-  se::DeviceMemoryBase output_buffer;\n+  se::DeviceAddressBase output_buffer;\n   se::Event* start_event;\n   se::Event* end_event;\n \n@@ -234,7 +234,7 @@ absl::StatusOr<std::shared_ptr<std::vector<RendezvousValue>>>\n RendezvousBeforeKernelStart(absl::string_view name,\n                             const GpuCliqueKey& clique_key, RankId rank,\n                             int64_t num_ranks,\n-                            const se::DeviceMemoryBase& output_buffer,\n+                            const se::DeviceAddressBase& output_buffer,\n                             se::Stream& stream, se::Event* start_event,\n                             se::Event* end_event) {\n   RendezvousValue rendezvous_value;\n@@ -321,8 +321,8 @@ absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n \n   PrimitiveType element_type = buffers[0].element_type;\n \n-  se::DeviceMemoryBase input_buffer = buffers[0].source_buffer;\n-  se::DeviceMemoryBase output_buffer = buffers[1].destination_buffer;\n+  se::DeviceAddressBase input_buffer = buffers[0].source_buffer;\n+  se::DeviceAddressBase output_buffer = buffers[1].destination_buffer;\n \n   TF_ASSIGN_OR_RETURN(\n       std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n@@ -332,7 +332,7 @@ absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n \n   const int64_t num_updates_per_replica = config_.num_total_updates / num_ranks;\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> output_ptrs;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> output_ptrs;\n   for (auto& value : *rendezvous_values) {\n     output_ptrs.push_back(value.output_buffer);\n   }\n@@ -432,7 +432,7 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n     state->host_buffer_allocs.push_back(std::move(alloc));\n   }\n \n-  state->output_offsets_device_buffer = se::DeviceMemoryHandle{\n+  state->output_offsets_device_buffer = se::DeviceAddressHandle{\n       executor,\n       executor->Allocate(config_.num_total_updates * sizeof(int64_t))};\n "
        },
        {
            "sha": "6a48a5fac956b05a19442b3f5636d43f5f413685",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -89,7 +89,7 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n         host_buffer_allocs;\n \n     // Device memory buffer for output offsets.\n-    se::DeviceMemoryHandle output_offsets_device_buffer;\n+    se::DeviceAddressHandle output_offsets_device_buffer;\n \n     // Event to synchronize streams on different devices at the start of the\n     // kernel."
        },
        {
            "sha": "d70cd11c7f72da43b1b53bdaeae3a46459801f5c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/recv_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -104,7 +104,7 @@ absl::StatusOr<bool> RecvThunk::RunCollective(const ExecuteParams& params,\n           << hlo_name_ << \")\";\n \n   const std::optional<int64_t> source_id = source_target.source;\n-  se::DeviceMemoryBase dest_addr = buffer.destination_buffer;\n+  se::DeviceAddressBase dest_addr = buffer.destination_buffer;\n \n   VLOG(3) << absl::StreamFormat(\"[%d] %s : id = %d, source_id = %d\",\n                                 device_ordinal, device_string, current_id,"
        },
        {
            "sha": "5bc1c70c02f9ea3703c625960bbf803bd793bd1f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_exec.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -19,8 +19,8 @@ limitations under the License.\n #include <cstdint>\n \n #include \"absl/status/status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n \n namespace xla::gpu {\n@@ -42,11 +42,11 @@ namespace xla::gpu {\n //   absl::Status indicating success or failure of the operation.\n template <typename T>\n absl::Status select_k_exec(int device_ordinal,\n-                           ::stream_executor::DeviceMemoryAllocator* allocator,\n+                           ::stream_executor::DeviceAddressAllocator* allocator,\n                            ::stream_executor::Stream* stream,\n-                           ::stream_executor::DeviceMemoryBase data_in,\n-                           ::stream_executor::DeviceMemoryBase data_out,\n-                           ::stream_executor::DeviceMemoryBase indices_out,\n+                           ::stream_executor::DeviceAddressBase data_in,\n+                           ::stream_executor::DeviceAddressBase data_out,\n+                           ::stream_executor::DeviceAddressBase indices_out,\n                            std::uint32_t batch, std::uint32_t n,\n                            std::uint32_t k);\n "
        },
        {
            "sha": "5de001e791eafc41273552495f5312f694cff41b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_exec_raft.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -37,8 +37,8 @@ limitations under the License.\n #include \"raft/matrix/select_k_types.hpp\"\n #include \"xla/backends/gpu/runtime/select_k_exec.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -54,19 +54,19 @@ namespace {\n class OwningScratchAllocator {\n  public:\n   OwningScratchAllocator(int device_ordinal,\n-                         se::DeviceMemoryAllocator* allocator)\n+                         se::DeviceAddressAllocator* allocator)\n       : device_ordinal_(device_ordinal), allocator_(allocator) {}\n \n   OwningScratchAllocator(OwningScratchAllocator&&) = default;\n   OwningScratchAllocator& operator=(OwningScratchAllocator&&) = default;\n \n   // Allocate memory and track ownership\n-  absl::StatusOr<se::DeviceMemory<uint8_t>> AllocateBytes(int64_t byte_size) {\n-    TF_ASSIGN_OR_RETURN(se::OwningDeviceMemory buffer,\n+  absl::StatusOr<se::DeviceAddress<uint8_t>> AllocateBytes(int64_t byte_size) {\n+    TF_ASSIGN_OR_RETURN(se::ScopedDeviceAddress<uint8_t> buffer,\n                         allocator_->Allocate(device_ordinal_, byte_size,\n                                              /*retry_on_failure=*/false));\n \n-    se::DeviceMemory<uint8_t> res = *buffer;\n+    se::DeviceAddress<uint8_t> res = *buffer;\n     void* raw_ptr = res.opaque();\n     buffers_.emplace(raw_ptr, std::move(buffer));\n     return res;\n@@ -82,31 +82,31 @@ class OwningScratchAllocator {\n     return absl::NotFoundError(\"Pointer not found\");\n   }\n \n-  se::DeviceMemoryAllocator* get_allocator() const { return allocator_; }\n+  se::DeviceAddressAllocator* get_allocator() const { return allocator_; }\n \n-  void set_allocator(se::DeviceMemoryAllocator* allocator) {\n+  void set_allocator(se::DeviceAddressAllocator* allocator) {\n     allocator_ = allocator;\n   }\n \n  private:\n   int device_ordinal_;\n-  se::DeviceMemoryAllocator* allocator_;\n+  se::DeviceAddressAllocator* allocator_;\n   // key = raw device pointer, value = owning memory object\n-  absl::flat_hash_map<void*, se::OwningDeviceMemory> buffers_;\n+  absl::flat_hash_map<void*, se::ScopedDeviceAddress<uint8_t>> buffers_;\n };\n \n // Custom RMM memory resource backed by StreamExecutor allocator\n class XlaDeviceMemoryResource : public rmm::mr::device_memory_resource {\n  public:\n   XlaDeviceMemoryResource(int device_ordinal,\n-                          se::DeviceMemoryAllocator* allocator)\n+                          se::DeviceAddressAllocator* allocator)\n       : scratch_allocator_(device_ordinal, allocator) {}\n \n-  se::DeviceMemoryAllocator* get_allocator() const {\n+  se::DeviceAddressAllocator* get_allocator() const {\n     return scratch_allocator_.get_allocator();\n   }\n \n-  void set_allocator(se::DeviceMemoryAllocator* allocator) {\n+  void set_allocator(se::DeviceAddressAllocator* allocator) {\n     scratch_allocator_.set_allocator(allocator);\n   }\n \n@@ -150,7 +150,7 @@ struct RaftStreamResource : public se::Stream::Resource {\n   // Returns:\n   //   Unique pointer to an initialized RaftStreamResource.\n   static std::unique_ptr<RaftStreamResource> Create(\n-      int device_ordinal, se::DeviceMemoryAllocator* allocator,\n+      int device_ordinal, se::DeviceAddressAllocator* allocator,\n       cudaStream_t cuda_stream) {\n     // Assign our custom AllocatorForRaft for this device\n     auto handle = std::make_unique<RaftStreamResource>();\n@@ -253,10 +253,10 @@ SelectAlgo choose_select_k_algorithm<nv_bfloat16>(uint32_t rows, uint32_t cols,\n // Host-side entry point for raft select_k\n template <typename T>\n absl::Status select_k_exec(int device_ordinal,\n-                           se::DeviceMemoryAllocator* allocator,\n-                           se::Stream* stream, se::DeviceMemoryBase data_in,\n-                           se::DeviceMemoryBase data_out,\n-                           se::DeviceMemoryBase indices_out,\n+                           se::DeviceAddressAllocator* allocator,\n+                           se::Stream* stream, se::DeviceAddressBase data_in,\n+                           se::DeviceAddressBase data_out,\n+                           se::DeviceAddressBase indices_out,\n                            std::uint32_t batch, std::uint32_t n,\n                            std::uint32_t k) {\n   // Pick the most suitable algorithm\n@@ -326,23 +326,23 @@ absl::Status select_k_exec(int device_ordinal,\n }\n \n // Explicit instantiations for supported types\n-template absl::Status select_k_exec<float>(int, se::DeviceMemoryAllocator*,\n-                                           se::Stream*, se::DeviceMemoryBase,\n-                                           se::DeviceMemoryBase,\n-                                           se::DeviceMemoryBase, std::uint32_t,\n+template absl::Status select_k_exec<float>(int, se::DeviceAddressAllocator*,\n+                                           se::Stream*, se::DeviceAddressBase,\n+                                           se::DeviceAddressBase,\n+                                           se::DeviceAddressBase, std::uint32_t,\n                                            std::uint32_t, std::uint32_t);\n \n template absl::Status select_k_exec<nv_bfloat16>(\n-    int, se::DeviceMemoryAllocator*, se::Stream*, se::DeviceMemoryBase,\n-    se::DeviceMemoryBase, se::DeviceMemoryBase, std::uint32_t, std::uint32_t,\n+    int, se::DeviceAddressAllocator*, se::Stream*, se::DeviceAddressBase,\n+    se::DeviceAddressBase, se::DeviceAddressBase, std::uint32_t, std::uint32_t,\n     std::uint32_t);\n \n // Explicit specializations for xla::bfloat16\n template <>\n absl::Status select_k_exec<::xla::bfloat16>(\n-    int device_ordinal, se::DeviceMemoryAllocator* allocator,\n-    se::Stream* stream, se::DeviceMemoryBase data_in,\n-    se::DeviceMemoryBase data_out, se::DeviceMemoryBase indices_out,\n+    int device_ordinal, se::DeviceAddressAllocator* allocator,\n+    se::Stream* stream, se::DeviceAddressBase data_in,\n+    se::DeviceAddressBase data_out, se::DeviceAddressBase indices_out,\n     std::uint32_t batch, std::uint32_t n, std::uint32_t k) {\n   // Sanity check: Eigen::bfloat16 and nv_bfloat16 must be binary-compatible\n   static_assert(sizeof(::xla::bfloat16) == sizeof(nv_bfloat16),"
        },
        {
            "sha": "623a0dd23c61f9894f5e43c678a25f48739861ff",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_exec_raft_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/select_k_exec.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -114,11 +114,11 @@ void RunSelectKTest() {\n   topk.resize(k);\n \n   // Allocate device memory for input and outputs\n-  se::DeviceMemory<T> d_data_in =\n+  se::DeviceAddress<T> d_data_in =\n       stream_executor->AllocateArray<T>(batch * n, 0);\n-  se::DeviceMemory<T> d_data_out =\n+  se::DeviceAddress<T> d_data_out =\n       stream_executor->AllocateArray<T>(batch * k, 0);\n-  se::DeviceMemory<uint32_t> d_indices_out =\n+  se::DeviceAddress<uint32_t> d_indices_out =\n       stream_executor->AllocateArray<uint32_t>(batch * k, 0);\n \n   // Copy host to device"
        },
        {
            "sha": "a1b109ce3d667e1a4624c1ad459971130a58685b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_exec_stub.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_stub.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -17,8 +17,8 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"xla/backends/gpu/runtime/select_k_exec.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/types.h\"  // IWYU pragma: keep\n \n@@ -27,26 +27,26 @@ namespace se = ::stream_executor;\n \n template <typename T>\n absl::Status select_k_exec(int device_ordinal,\n-                           se::DeviceMemoryAllocator* allocator,\n-                           se::Stream* stream, se::DeviceMemoryBase data_in,\n-                           se::DeviceMemoryBase data_out,\n-                           se::DeviceMemoryBase indices_out,\n+                           se::DeviceAddressAllocator* allocator,\n+                           se::Stream* stream, se::DeviceAddressBase data_in,\n+                           se::DeviceAddressBase data_out,\n+                           se::DeviceAddressBase indices_out,\n                            std::uint32_t batch, std::uint32_t n,\n                            std::uint32_t k) {\n   return absl::UnimplementedError(\n       \"select_k_exec is not implemented on this platform\");\n }\n \n // Explicit instantiations for supported dtypes.\n-template absl::Status select_k_exec<float>(int, se::DeviceMemoryAllocator*,\n-                                           se::Stream*, se::DeviceMemoryBase,\n-                                           se::DeviceMemoryBase,\n-                                           se::DeviceMemoryBase, std::uint32_t,\n+template absl::Status select_k_exec<float>(int, se::DeviceAddressAllocator*,\n+                                           se::Stream*, se::DeviceAddressBase,\n+                                           se::DeviceAddressBase,\n+                                           se::DeviceAddressBase, std::uint32_t,\n                                            std::uint32_t, std::uint32_t);\n \n template absl::Status select_k_exec<::xla::bfloat16>(\n-    int, se::DeviceMemoryAllocator*, se::Stream*, se::DeviceMemoryBase,\n-    se::DeviceMemoryBase, se::DeviceMemoryBase, std::uint32_t, std::uint32_t,\n+    int, se::DeviceAddressAllocator*, se::Stream*, se::DeviceAddressBase,\n+    se::DeviceAddressBase, se::DeviceAddressBase, std::uint32_t, std::uint32_t,\n     std::uint32_t);\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "a53066008232a95ba44e74e1797a15d2381ced74",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -35,8 +35,8 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/types.h\"\n@@ -76,16 +76,17 @@ absl::Status SelectKThunk::ExecuteOnStream(const ExecuteParams& params) {\n   VLOG(3) << \"Launching \" << ToString(0);\n \n   // Map buffer slices to device memory.\n-  absl::InlinedVector<se::DeviceMemoryBase, 3> buffer_args;\n+  absl::InlinedVector<se::DeviceAddressBase, 3> buffer_args;\n   for (const BufferAllocation::Slice& arg : args_) {\n-    se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n+    se::DeviceAddressBase buf =\n+        params.buffer_allocations->GetDeviceAddress(arg);\n     VLOG(3) << \"  Arg: alloc #\" << arg.index() << \", offset: \" << arg.offset()\n             << \": \" << buf.opaque() << \" (\" << buf.size() << \"B)\";\n     buffer_args.push_back(buf);\n   }\n \n   int device_ordinal = params.buffer_allocations->device_ordinal();\n-  se::DeviceMemoryAllocator* allocator =\n+  se::DeviceAddressAllocator* allocator =\n       params.buffer_allocations->memory_allocator();\n   se::Stream* stream = params.stream;\n "
        },
        {
            "sha": "53d39d7d5e30938f22ec56ffcc129abe33f18723",
            "filename": "third_party/xla/xla/backends/gpu/runtime/send_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -104,7 +104,7 @@ absl::StatusOr<bool> SendThunk::RunCollective(const ExecuteParams& params,\n           << hlo_name_ << \")\";\n \n   const std::optional<int64_t> target_id = source_target.target;\n-  se::DeviceMemoryBase src_addr = buffer.source_buffer;\n+  se::DeviceAddressBase src_addr = buffer.source_buffer;\n \n   VLOG(3) << absl::StreamFormat(\"[%d] %s : id = %d, target_id = %d\",\n                                 device_ordinal, device_string, current_id,"
        },
        {
            "sha": "a842be9f462c83a8b57ab28665fc01c860a3e0d1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/topk_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftopk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftopk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftopk_test.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -28,7 +28,7 @@ limitations under the License.\n #include \"absl/strings/ascii.h\"\n #include \"absl/strings/substitute.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/kernel_serialization_check.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_args.h\"\n@@ -97,11 +97,11 @@ TEST_P(TopKKernelTest, TopKFloat) {\n   const auto [n_kb, k, batch_size, offset] = GetParam();\n   const size_t n = n_kb * 1024 + offset;\n \n-  se::DeviceMemory<T> input_buffer =\n+  se::DeviceAddress<T> input_buffer =\n       executor->AllocateArray<T>(n * batch_size, 0);\n-  se::DeviceMemory<T> output_values =\n+  se::DeviceAddress<T> output_values =\n       executor->AllocateArray<T>(k * batch_size, 0);\n-  se::DeviceMemory<uint32_t> output_indices =\n+  se::DeviceAddress<uint32_t> output_indices =\n       executor->AllocateArray<uint32_t>(k * batch_size, 0);\n \n   auto source = RandomVec<T>(n * batch_size);\n@@ -119,7 +119,7 @@ TEST_P(TopKKernelTest, TopKFloat) {\n \n   // Launch topk kernel with device memory arguments.\n   se::KernelArgsDeviceMemoryArray arr(\n-      std::vector<se::DeviceMemoryBase>(\n+      std::vector<se::DeviceAddressBase>(\n           {input_buffer, output_values, output_indices}),\n       custom_kernel->shared_memory_bytes());\n   TF_ASSERT_OK(kernel->Launch(custom_kernel->thread_dims(),\n@@ -151,11 +151,11 @@ TEST_P(TopKKernelTest, TopKPackedNegative) {\n   const auto [n_kb, k, batch_size, offset] = GetParam();\n   const size_t n = n_kb * 1024 + offset;\n \n-  se::DeviceMemory<T> input_buffer =\n+  se::DeviceAddress<T> input_buffer =\n       executor->AllocateArray<T>(n * batch_size, 0);\n-  se::DeviceMemory<T> output_values =\n+  se::DeviceAddress<T> output_values =\n       executor->AllocateArray<T>(k * batch_size, 0);\n-  se::DeviceMemory<uint32_t> output_indices =\n+  se::DeviceAddress<uint32_t> output_indices =\n       executor->AllocateArray<uint32_t>(k * batch_size, 0);\n \n   auto source = RandomVecNegative<T>(n * batch_size);\n@@ -173,7 +173,7 @@ TEST_P(TopKKernelTest, TopKPackedNegative) {\n \n   // Launch topk kernel with device memory arguments.\n   se::KernelArgsDeviceMemoryArray arr(\n-      std::vector<se::DeviceMemoryBase>(\n+      std::vector<se::DeviceAddressBase>(\n           {input_buffer, output_values, output_indices}),\n       custom_kernel->shared_memory_bytes());\n   TF_ASSERT_OK(kernel->Launch(custom_kernel->thread_dims(),"
        },
        {
            "sha": "c6d36f675715eb8b703e0f44b7bcd8a0b53ad785",
            "filename": "third_party/xla/xla/backends/gpu/runtime/triangular_solve_thunk.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 26,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -152,9 +152,9 @@ absl::StatusOr<ThunkProto> TriangularSolveThunk::ToProto() const {\n   return proto;\n }\n \n-absl::Status RunTriangularSolve(se::DeviceMemoryBase a_data,\n-                                se::DeviceMemoryBase b_data,\n-                                se::DeviceMemoryBase temp_data,\n+absl::Status RunTriangularSolve(se::DeviceAddressBase a_data,\n+                                se::DeviceAddressBase b_data,\n+                                se::DeviceAddressBase temp_data,\n                                 se::blas::UpperLower uplo, se::blas::Side side,\n                                 se::blas::Diagonal unit_diagonal,\n                                 se::blas::Transpose transpose_a,\n@@ -179,34 +179,34 @@ absl::Status RunTriangularSolve(se::DeviceMemoryBase a_data,\n   if (batch_size == 1) {\n     switch (type) {\n       case F32: {\n-        se::DeviceMemory<float> b_data_typed(b_data);\n+        se::DeviceAddress<float> b_data_typed(b_data);\n         launch_ok = blas->DoBlasTrsm(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0f, se::DeviceMemory<float>(a_data), lda, &b_data_typed,\n-            ldb);\n+            /*alpha=*/1.0f, se::DeviceAddress<float>(a_data), lda,\n+            &b_data_typed, ldb);\n         break;\n       }\n       case F64: {\n-        se::DeviceMemory<double> b_data_typed(b_data);\n+        se::DeviceAddress<double> b_data_typed(b_data);\n         launch_ok = blas->DoBlasTrsm(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0, se::DeviceMemory<double>(a_data), lda, &b_data_typed,\n-            ldb);\n+            /*alpha=*/1.0, se::DeviceAddress<double>(a_data), lda,\n+            &b_data_typed, ldb);\n         break;\n       }\n       case C64: {\n-        se::DeviceMemory<std::complex<float>> b_data_typed(b_data);\n+        se::DeviceAddress<std::complex<float>> b_data_typed(b_data);\n         launch_ok = blas->DoBlasTrsm(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0f, se::DeviceMemory<std::complex<float>>(a_data), lda,\n+            /*alpha=*/1.0f, se::DeviceAddress<std::complex<float>>(a_data), lda,\n             &b_data_typed, ldb);\n         break;\n       }\n       case C128: {\n-        se::DeviceMemory<std::complex<double>> b_data_typed(b_data);\n+        se::DeviceAddress<std::complex<double>> b_data_typed(b_data);\n         launch_ok = blas->DoBlasTrsm(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0, se::DeviceMemory<std::complex<double>>(a_data), lda,\n+            /*alpha=*/1.0, se::DeviceAddress<std::complex<double>>(a_data), lda,\n             &b_data_typed, ldb);\n         break;\n       }\n@@ -221,9 +221,9 @@ absl::Status RunTriangularSolve(se::DeviceMemoryBase a_data,\n     int64_t batch_pointers_bytes = sizeof(void*) * batch_size;\n     TF_RET_CHECK(temp_data.size() >= 2 * batch_pointers_bytes);\n     void** temp_base = reinterpret_cast<void**>(temp_data.opaque());\n-    se::DeviceMemoryBase a_pointers(temp_base, batch_pointers_bytes);\n-    se::DeviceMemoryBase b_pointers(temp_base + batch_size,\n-                                    batch_pointers_bytes);\n+    se::DeviceAddressBase a_pointers(temp_base, batch_pointers_bytes);\n+    se::DeviceAddressBase b_pointers(temp_base + batch_size,\n+                                     batch_pointers_bytes);\n \n     TF_RETURN_IF_ERROR(MakeBatchPointers(stream, a_data, a_batch_stride,\n                                          batch_size, a_pointers));\n@@ -232,35 +232,36 @@ absl::Status RunTriangularSolve(se::DeviceMemoryBase a_data,\n \n     switch (type) {\n       case F32: {\n-        se::DeviceMemory<float*> typed_b_pointers(b_pointers);\n+        se::DeviceAddress<float*> typed_b_pointers(b_pointers);\n         launch_ok = blas->DoBlasTrsmBatched(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0f, se::DeviceMemory<float*>(a_pointers), lda,\n+            /*alpha=*/1.0f, se::DeviceAddress<float*>(a_pointers), lda,\n             &typed_b_pointers, ldb, batch_size);\n         break;\n       }\n       case F64: {\n-        se::DeviceMemory<double*> typed_b_pointers(b_pointers);\n+        se::DeviceAddress<double*> typed_b_pointers(b_pointers);\n         launch_ok = blas->DoBlasTrsmBatched(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0f, se::DeviceMemory<double*>(a_pointers), lda,\n+            /*alpha=*/1.0f, se::DeviceAddress<double*>(a_pointers), lda,\n             &typed_b_pointers, ldb, batch_size);\n         break;\n       }\n       case C64: {\n-        se::DeviceMemory<std::complex<float>*> typed_b_pointers(b_pointers);\n+        se::DeviceAddress<std::complex<float>*> typed_b_pointers(b_pointers);\n         launch_ok = blas->DoBlasTrsmBatched(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0f, se::DeviceMemory<std::complex<float>*>(a_pointers),\n+            /*alpha=*/1.0f, se::DeviceAddress<std::complex<float>*>(a_pointers),\n             lda, &typed_b_pointers, ldb, batch_size);\n         break;\n       }\n       case C128: {\n-        se::DeviceMemory<std::complex<double>*> typed_b_pointers(b_pointers);\n+        se::DeviceAddress<std::complex<double>*> typed_b_pointers(b_pointers);\n         launch_ok = blas->DoBlasTrsmBatched(\n             stream, side, uplo, transpose_a, unit_diagonal, m, n,\n-            /*alpha=*/1.0f, se::DeviceMemory<std::complex<double>*>(a_pointers),\n-            lda, &typed_b_pointers, ldb, batch_size);\n+            /*alpha=*/1.0f,\n+            se::DeviceAddress<std::complex<double>*>(a_pointers), lda,\n+            &typed_b_pointers, ldb, batch_size);\n         break;\n       }\n       default:"
        },
        {
            "sha": "88c05ca23a226d55c70c5a1574f09a65ae10c23d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/triangular_solve_thunk.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -87,9 +87,9 @@ class TriangularSolveThunk : public Thunk {\n   const int64_t b_batch_stride_;\n };\n \n-absl::Status RunTriangularSolve(se::DeviceMemoryBase a_data,\n-                                se::DeviceMemoryBase b_data,\n-                                se::DeviceMemoryBase temp_data,\n+absl::Status RunTriangularSolve(se::DeviceAddressBase a_data,\n+                                se::DeviceAddressBase b_data,\n+                                se::DeviceAddressBase temp_data,\n                                 se::blas::UpperLower uplo, se::blas::Side side,\n                                 se::blas::Diagonal unit_diagonal,\n                                 se::blas::Transpose transpose_a,"
        },
        {
            "sha": "511f94f91e8edd3bbb1bb758536276d57ce1c42d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/while_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -38,7 +38,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -149,7 +149,7 @@ absl::Status WhileThunk::ExecuteOnStream(const ExecuteParams& params) {\n   }\n   TF_ASSIGN_OR_RETURN(HostMemoryPool::Handle handle, pool->Acquire());\n   bool* condition_result = handle.get<bool>();\n-  se::DeviceMemoryBase condition_result_data =\n+  se::DeviceAddressBase condition_result_data =\n       params.buffer_allocations->GetDeviceAddress(\n           condition_result_buffer_index_);\n "
        },
        {
            "sha": "6b9515523cf0c14f0a849d88ae4fa4354f2d0bb3",
            "filename": "third_party/xla/xla/backends/interpreter/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2FBUILD?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -15,7 +15,7 @@ cc_library(\n         \"//xla/service:generic_transfer_manager\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:transfer_manager\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream_executor_h\",\n     ],\n     alwayslink = True,  # Contains per-platform transfer manager registration\n@@ -87,8 +87,8 @@ cc_library(\n         \"//xla/service:maybe_owning_device_memory\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:transfer_manager\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -156,8 +156,8 @@ cc_library(\n     hdrs = [\"executor.h\"],\n     deps = [\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:generic_memory_allocation\",\n         \"//xla/stream_executor:generic_memory_allocator\","
        },
        {
            "sha": "d8a9ac91c7d39ffb55ad5b3e529fd0680e67cef0",
            "filename": "third_party/xla/xla/backends/interpreter/executable_base.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutable_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutable_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutable_base.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -39,8 +39,8 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -172,7 +172,7 @@ absl::StatusOr<ExecutionOutput> InterpreterExecutableBase::ExecuteAsyncOnStream(\n absl::StatusOr<ExecutionOutput>\n InterpreterExecutableBase::AllocateOutputMemoryWithInputReuse(\n     const Shape& shape, const HloInputOutputAliasConfig& alias_config,\n-    se::DeviceMemoryAllocator* allocator,\n+    se::DeviceAddressAllocator* allocator,\n     std::vector<ExecutionInput>* arguments, se::Stream* stream) {\n   TF_RETURN_IF_ERROR(alias_config.ForEachAliasWithStatus(\n       [&](const ShapeIndex& output_index,\n@@ -201,7 +201,7 @@ InterpreterExecutableBase::AllocateOutputMemoryWithInputReuse(\n   ExecutionOutput result(shape, allocator, executor->device_ordinal());\n   for (auto& pair : result.MutableResult()->buffers()) {\n     const ShapeIndex& result_index = pair.first;\n-    se::DeviceMemoryBase& result_buffer = pair.second;\n+    se::DeviceAddressBase& result_buffer = pair.second;\n     int64_t allocation_bytes =\n         transfer_manager->GetByteSizeRequirement(ShapeUtil::GetSubshape(\n             result.Result().on_device_shape(), result_index));\n@@ -218,7 +218,7 @@ InterpreterExecutableBase::AllocateOutputMemoryWithInputReuse(\n       MaybeOwningDeviceMemory* device_memory =\n           input.MutableBuffer(alias->parameter_index);\n       if (auto owning = device_memory->Release()) {\n-        se::DeviceMemoryBase device_memory_base = owning->Release();\n+        se::DeviceAddressBase device_memory_base = owning->Release();\n         *device_memory = device_memory_base;\n         result_buffer = device_memory_base;\n         result.AddAliasedIndex(result_index);"
        },
        {
            "sha": "b6bcfa75f16d975d514d1126d6f0739ee497ae58",
            "filename": "third_party/xla/xla/backends/interpreter/executable_base.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutable_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutable_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutable_base.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/service/executable.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/xla.pb.h\"\n namespace xla {\n@@ -55,7 +55,7 @@ class InterpreterExecutableBase : public Executable {\n  private:\n   absl::StatusOr<ExecutionOutput> AllocateOutputMemoryWithInputReuse(\n       const Shape& shape, const HloInputOutputAliasConfig& alias_config,\n-      se::DeviceMemoryAllocator* allocator,\n+      se::DeviceAddressAllocator* allocator,\n       std::vector<ExecutionInput>* arguments, stream_executor::Stream* stream);\n \n   InterpreterExecutableBase(const InterpreterExecutableBase&) = delete;"
        },
        {
            "sha": "93ac837ab069fdb3c6be67a3c6914e9c0af8ce06",
            "filename": "third_party/xla/xla/backends/interpreter/executor.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutor.cc?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -24,8 +24,8 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/host/host_stream.h\"\n #include \"xla/stream_executor/stream.h\"\n "
        },
        {
            "sha": "0171dc692be4c8b6d18350e1d0e6c88a52a9b745",
            "filename": "third_party/xla/xla/backends/interpreter/executor.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fexecutor.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -28,8 +28,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/generic_memory_allocation.h\"\n #include \"xla/stream_executor/generic_memory_allocator.h\""
        },
        {
            "sha": "4924df7932ebb8512d0c71ceac72ec75e1721454",
            "filename": "third_party/xla/xla/backends/interpreter/interpreter_transfer_manager.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Finterpreter_transfer_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/892c6fbacc7d6e57334db9d21097cb0c03a35134/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Finterpreter_transfer_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Finterpreter_transfer_manager.h?ref=892c6fbacc7d6e57334db9d21097cb0c03a35134",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n #include \"xla/service/generic_transfer_manager.h\"\n #include \"xla/service/shaped_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla {\n@@ -37,7 +37,7 @@ class InterpreterTransferManager : public GenericTransferManager {\n \n   bool CanBufferBeAccessedNow(\n       se::StreamExecutor* executor,\n-      const se::DeviceMemoryBase& device_buffer) const override {\n+      const se::DeviceAddressBase& device_buffer) const override {\n     return true;\n   }\n "
        }
    ],
    "stats": {
        "total": 1943,
        "additions": 974,
        "deletions": 969
    }
}