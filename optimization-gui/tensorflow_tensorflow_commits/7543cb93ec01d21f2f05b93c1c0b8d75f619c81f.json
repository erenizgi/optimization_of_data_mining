{
    "author": "loislo",
    "message": "[XLA:GPU] Add a pass to convert unsupported types like F8E8M0NU to i8\n\nFor some reason Triton does not support some types like F8E8M0NU and uses i8 instead.\nLets implement a rewriter that converts these types.\n\nPiperOrigin-RevId: 805419707",
    "sha": "7543cb93ec01d21f2f05b93c1c0b8d75f619c81f",
    "files": [
        {
            "sha": "e36914358abce2495e27591740e6a5027d586e2c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=7543cb93ec01d21f2f05b93c1c0b8d75f619c81f",
            "patch": "@@ -35,6 +35,7 @@ cc_library(\n         \"generalize_kernel_signature.cc\",\n         \"int4_passes.cc\",\n         \"round_f32_to_tf32_for_tf32_dot_pass.cc\",\n+        \"triton_xla_convert_unsupported_types.cc\",\n         \"triton_xla_extract_insert_to_triton_pass.cc\",\n         \"triton_xla_fold_transpose_pass.cc\",\n         \"triton_xla_lower_atomics_pass.cc\","
        },
        {
            "sha": "07d23cf8817e838b6969b2db1bfb37e6c1459ecb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=7543cb93ec01d21f2f05b93c1c0b8d75f619c81f",
            "patch": "@@ -41,6 +41,7 @@ std::unique_ptr<mlir::Pass> CreateRoundF32ToTF32ForTf32DotRewritePass();\n std::unique_ptr<mlir::Pass> CreateExtractTmaInfoPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLALowerGetTidPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLALowerAtomicsPass();\n+std::unique_ptr<mlir::Pass> CreateTritonXLAConvertUnsupportedTypesPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "69bc0a861e24a3e0661518709cd4c23e009fe57d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=7543cb93ec01d21f2f05b93c1c0b8d75f619c81f",
            "patch": "@@ -129,4 +129,25 @@ def TritonXLALowerAtomicsPass\n   let constructor = \"CreateTritonXLALowerAtomicsPass()\";\n }\n \n+def TritonXLAConvertUnsupportedTypesPass\n+    : Pass<\"convert-unsupported-types\", \"mlir::ModuleOp\"> {\n+  let summary = \"Converts types unsupported by Triton into their supported equivalents.\";\n+  let description = [{\n+    This pass converts types unsupported by Triton into their supported\n+    equivalents.\n+\n+    This is essentially useful to support e.g. flavors of float8 types that\n+    Triton decides to represent as int8s from the very beginning, but that have\n+    both a proper representation in HLO and upstream MLIR.\n+\n+    The important invariant of this pass is that the types it converts to have\n+    the same bitwidth as the original type.\n+\n+    This pass may not be replaced by a simple `TypeConverter`, because the type\n+    replacement it performs do not preserve semantics in a vacuum (but may for\n+    specific ops, such as loads, stores, or bitcasts).\n+  }];\n+  let constructor = \"CreateTritonXLAConvertUnsupportedTypesPass()\";\n+}\n+\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "e6a10be19072288bba905b0f2e77b2e43dc0844f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_convert_unsupported_types.mlir",
            "status": "added",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir?ref=7543cb93ec01d21f2f05b93c1c0b8d75f619c81f",
            "patch": "@@ -0,0 +1,20 @@\n+// RUN: xla-opt --split-input-file --convert-unsupported-types --canonicalize  %s | FileCheck %s\n+\n+module {\n+  // CHECK:   func.func @triton_fn(%arg0: !tt.ptr<f8E4M3FN>, %arg1: !tt.ptr<i8>, %arg2: !tt.ptr<f8E4M3FN>, %arg3: !tt.ptr<i8>, %arg4: !tt.ptr<f32>) {\n+  func.func @triton_fn(%arg0: !tt.ptr<f8E4M3FN>, %arg1: !tt.ptr<f8E8M0FNU>, %arg2: !tt.ptr<f8E4M3FN>, %arg3: !tt.ptr<f8E8M0FNU>, %arg4: !tt.ptr<f32>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32>\n+    %extracted_tile = triton_xla.extract from %arg0 as memref<64x512xf8E4M3FN, #triton_xla.layout<[1, 0]>> [0, 0] [16, 32] [1, 1] : tensor<16x32xf8E4M3FN>\n+    // CHECK: %[[arg_0:.*]] = triton_xla.extract from %arg0 as memref<64x512xf8E4M3FN, #triton_xla.layout<[1, 0]>> [0, 0] [16, 32] [1, 1] : tensor<16x32xf8E4M3FN>\n+    %extracted_tile_0 = triton_xla.extract from %arg1 as memref<64x16xf8E8M0FNU, #triton_xla.layout<[1, 0]>> [0, 0] [16, 1] [1, 1] : tensor<16x1xf8E8M0FNU>\n+    // CHECK: %[[arg_1:.*]] = triton_xla.extract from %arg1 as memref<64x16xi8, #triton_xla.layout<[1, 0]>> [0, 0] [16, 1] [1, 1] : tensor<16x1xi8>\n+    %extracted_tile_1 = triton_xla.extract from %arg2 as memref<512x64xf8E4M3FN, #triton_xla.layout<[1, 0]>> [0, 0] [32, 16] [1, 1] : tensor<32x16xf8E4M3FN>\n+    // CHECK: %[[arg_2:.*]] = triton_xla.extract from %arg2 as memref<512x64xf8E4M3FN, #triton_xla.layout<[1, 0]>> [0, 0] [32, 16] [1, 1] : tensor<32x16xf8E4M3FN>\n+    %extracted_tile_2 = triton_xla.extract from %arg3 as memref<16x64xf8E8M0FNU, #triton_xla.layout<[1, 0]>> [0, 0] [1, 16] [1, 1] : tensor<1x16xf8E8M0FNU>\n+    // CHECK: %[[arg_3:.*]] = triton_xla.extract from %arg3 as memref<16x64xi8, #triton_xla.layout<[1, 0]>> [0, 0] [1, 16] [1, 1] : tensor<1x16xi8>\n+    %16 = arith.bitcast %extracted_tile_0 : tensor<16x1xf8E8M0FNU> to tensor<16x1xi8>\n+    %17 = arith.bitcast %extracted_tile_2 : tensor<1x16xf8E8M0FNU> to tensor<1x16xi8>\n+    %18 = tt.dot_scaled %extracted_tile scale %16, %extracted_tile_1 scale %17, %cst lhs = e4m3 rhs = e4m3 {fastMath = true} : tensor<16x32xf8E4M3FN>, tensor<16x1xi8> * tensor<32x16xf8E4M3FN>, tensor<1x16xi8> -> tensor<16x16xf32>\n+    return\n+  }\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "4e0f3d64ee0065762e53545b547f1e9c23bb996b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_convert_unsupported_types.cc",
            "status": "added",
            "additions": 163,
            "deletions": 0,
            "changes": 163,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7543cb93ec01d21f2f05b93c1c0b8d75f619c81f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc?ref=7543cb93ec01d21f2f05b93c1c0b8d75f619c81f",
            "patch": "@@ -0,0 +1,163 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+    http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/SCF/Transforms/Patterns.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n+\n+namespace mlir::triton::xla {\n+\n+#define GEN_PASS_DEF_TRITONXLACONVERTUNSUPPORTEDTYPESPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+class UnsupportedTypesConverter : public TypeConverter {\n+ public:\n+  UnsupportedTypesConverter() : TypeConverter() {\n+    // Fallback to the no conversion for all other types.\n+    addConversion([](Type type) -> std::optional<Type> { return type; });\n+\n+    // Convert F8E8M0FNUType to i8. This is a workaround for the fact that\n+    // Triton doesn't support F8E8M0FNUType natively.\n+    addConversion([](Float8E8M0FNUType type) -> std::optional<Type> {\n+      return IntegerType::get(type.getContext(), 8);\n+    });\n+\n+    // Helper conversions for the nontrivial types.\n+    addConversion([this](Type type) -> std::optional<Type> {\n+      if (auto shaped_type = dyn_cast<ShapedType>(type)) {\n+        Type new_type = convertType(shaped_type.getElementType());\n+        return shaped_type.clone(new_type);\n+      }\n+      return std::nullopt;\n+    });\n+    addConversion([this](Type type) -> std::optional<Type> {\n+      if (auto pointer_type = dyn_cast<triton::PointerType>(type)) {\n+        Type new_type = convertType(pointer_type.getPointeeType());\n+        return triton::PointerType::get(new_type,\n+                                        pointer_type.getAddressSpace());\n+      }\n+      return std::nullopt;\n+    });\n+    addConversion([this](Type type) -> std::optional<Type> {\n+      if (auto func_type = dyn_cast<FunctionType>(type)) {\n+        SmallVector<Type> new_inputs = convertTypes(func_type.getInputs());\n+        SmallVector<Type> new_results = convertTypes(func_type.getResults());\n+        if (new_inputs != func_type.getInputs() ||\n+            new_results != func_type.getResults()) {\n+          return FunctionType::get(func_type.getContext(), new_inputs,\n+                                   new_results);\n+        }\n+      }\n+      return std::nullopt;\n+    });\n+  }\n+\n+  // Helper method to convert a range of types.\n+  SmallVector<Type> convertTypes(ArrayRef<Type> types) {\n+    SmallVector<Type> new_types;\n+    for (auto type : types) {\n+      new_types.push_back(convertType(type));\n+    }\n+    return new_types;\n+  }\n+};\n+\n+struct RewriteF8ToI8ConversionPattern final : ConversionPattern {\n+  RewriteF8ToI8ConversionPattern(const TypeConverter& converter,\n+                                 MLIRContext* ctx)\n+      : ConversionPattern::ConversionPattern(\n+            converter, Pattern::MatchAnyOpTypeTag(), 1, ctx) {}\n+\n+  LogicalResult matchAndRewrite(\n+      Operation* op, ArrayRef<Value> operands,\n+      ConversionPatternRewriter& rewriter) const override {\n+    if (getTypeConverter()->isLegal(op)) {\n+      return failure();\n+    }\n+\n+    if (!isa<ExtractOp, InsertOp, arith::BitcastOp>(op)) {\n+      return failure();\n+    }\n+\n+    const TypeConverter* converter = getTypeConverter();\n+    SmallVector<Type> result_types;\n+    if (failed(converter->convertTypes(op->getResultTypes(), result_types))) {\n+      // Note to anyone looking for this error message: this is a \"can't\n+      // happen\". If you're seeing it, there's a bug.\n+      return op->emitOpError(\"The op is not legal but type conversion failed.\");\n+    }\n+    Operation* replacement = rewriter.create(\n+        op->getLoc(), op->getName().getIdentifier(), operands, result_types,\n+        op->getAttrs(), op->getSuccessors(), /*regions=*/{});\n+    rewriter.replaceOp(op, replacement);\n+    return success();\n+  }\n+};\n+\n+class TritonXLAConvertUnsupportedTypesPass\n+    : public impl::TritonXLAConvertUnsupportedTypesPassBase<\n+          TritonXLAConvertUnsupportedTypesPass> {\n+ public:\n+  using Base::Base;\n+\n+ private:\n+  void runOnOperation() override {\n+    auto* ctx = &getContext();\n+    auto module = getOperation();\n+    UnsupportedTypesConverter converter;\n+\n+    ConversionTarget target(*ctx);\n+    target.markUnknownOpDynamicallyLegal([&](Operation* op) {\n+      if (auto func_op = dyn_cast<func::FuncOp>(op)) {\n+        return converter.isLegal(func_op.getFunctionType());\n+      }\n+      return converter.isLegal(op);\n+    });\n+\n+    RewritePatternSet patterns(ctx);\n+    patterns.add<RewriteF8ToI8ConversionPattern>(converter,\n+                                                 patterns.getContext());\n+    scf::populateSCFStructuralTypeConversions(converter, patterns);\n+    populateFunctionOpInterfaceTypeConversionPattern<mlir::func::FuncOp>(\n+        patterns, converter);\n+\n+    if (failed(applyPartialConversion(module, target, std::move(patterns)))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<Pass> CreateTritonXLAConvertUnsupportedTypesPass() {\n+  return std::make_unique<TritonXLAConvertUnsupportedTypesPass>();\n+}\n+\n+}  // namespace mlir::triton::xla"
        }
    ],
    "stats": {
        "total": 206,
        "additions": 206,
        "deletions": 0
    }
}