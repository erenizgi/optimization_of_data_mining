{
    "author": "ezhulenev",
    "message": "[xla:cpu:ynn] Implement SlinkyThreadPool on top of WorkQueue and Worker APIs\n\nRemove `work_queue` and `worker` that were originally forked from `xla::cpu::WorkQueue` and `Worker`\n\nPiperOrigin-RevId: 823179793",
    "sha": "7ba33178571e22400f943f60848825419bd5b2f0",
    "files": [
        {
            "sha": "1c4f449b24b8c4c86bdad9e06625e6a3aaed6814",
            "filename": "third_party/xla/xla/backends/cpu/runtime/work_queue.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h?ref=7ba33178571e22400f943f60848825419bd5b2f0",
            "patch": "@@ -55,6 +55,8 @@ class WorkQueue {\n \n   size_t num_partitions() const { return partitions_.size(); }\n \n+  bool IsEmpty() const { return empty_.load(std::memory_order_relaxed); }\n+\n  private:\n   friend class Worker;\n \n@@ -67,9 +69,8 @@ class WorkQueue {\n     size_t end;\n   };\n \n-  // An empty work queue flag to stop worker threads from looping through all\n-  // partitions looking for work.\n-  bool IsEmpty() const { return empty_.load(std::memory_order_relaxed); }\n+  // Sets an empty work queue flag to stop worker threads from looping through\n+  // all partitions looking for work.\n   void SetEmpty() { empty_.store(true, std::memory_order_relaxed); }\n \n   // Notify that one of the workers switched to the work stealing mode."
        },
        {
            "sha": "2def41e34f05f76cca6bf234ce7eee16427e5518",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/BUILD",
            "status": "modified",
            "additions": 30,
            "deletions": 11,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD?ref=7ba33178571e22400f943f60848825419bd5b2f0",
            "patch": "@@ -14,6 +14,35 @@ package_group(\n     ],\n )\n \n+cc_library(\n+    name = \"slinky_threadpool\",\n+    srcs = [\"slinky_threadpool.cc\"],\n+    hdrs = [\"slinky_threadpool.h\"],\n+    deps = [\n+        \"//xla/backends/cpu/runtime:work_queue\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:fixed_array\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@eigen_archive//:eigen3\",\n+        \"@slinky//slinky/base:thread_pool\",\n+    ],\n+)\n+\n+ynn_cc_test(\n+    name = \"slinky_threadpool_test\",\n+    srcs = [\"slinky_threadpool_test.cc\"],\n+    deps = [\n+        \":slinky_threadpool\",\n+        \"//xla/tsl/platform:env\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@slinky//slinky/base:thread_pool\",\n+    ],\n+)\n+\n cc_library(\n     name = \"ynn_interop\",\n     srcs = [\"ynn_interop.cc\"],\n@@ -35,6 +64,7 @@ cc_library(\n     srcs = [\"ynn_threadpool.cc\"],\n     hdrs = [\"ynn_threadpool.h\"],\n     deps = [\n+        \":slinky_threadpool\",\n         \":ynn_interop\",\n         \"@XNNPACK//ynnpack:ynnpack_h\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -49,17 +79,6 @@ cc_library(\n     ],\n )\n \n-ynn_cc_test(\n-    name = \"ynn_threadpool_test\",\n-    srcs = [\"ynn_threadpool_test.cc\"],\n-    deps = [\n-        \":ynn_threadpool\",\n-        \"//xla/tsl/platform:env\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@slinky//slinky/base:thread_pool\",\n-    ],\n-)\n-\n cc_library(\n     name = \"ynn_fusion_thunk\",\n     srcs = [\"ynn_fusion_thunk.cc\"],"
        },
        {
            "sha": "41cf103c05ce4a0f790d752d002866e02d8aa6f3",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/slinky_threadpool.cc",
            "status": "added",
            "additions": 416,
            "deletions": 0,
            "changes": 416,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.cc?ref=7ba33178571e22400f943f60848825419bd5b2f0",
            "patch": "@@ -0,0 +1,416 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/runtime/ynnpack/slinky_threadpool.h\"\n+\n+#include <algorithm>\n+#include <atomic>\n+#include <cassert>\n+#include <cstddef>\n+#include <cstdint>\n+#include <deque>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/base/optimization.h\"\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"slinky/base/function_ref.h\"\n+#include \"slinky/base/ref_count.h\"\n+#include \"slinky/base/thread_pool.h\"\n+#include \"xla/backends/cpu/runtime/work_queue.h\"\n+\n+#define EIGEN_USE_THREADS\n+#include \"Eigen/ThreadPool\"\n+#include \"unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace xla::cpu {\n+\n+//===----------------------------------------------------------------------===//\n+// Task\n+//===----------------------------------------------------------------------===//\n+\n+namespace {\n+\n+// Running a task can result in three states:\n+//\n+//   kPending:  The task is still being processed by the worker threads.\n+//   kComplete: The caller thread is the one who completed the task.\n+//   kDone:     The task is done and all work items have been processed, however\n+//              the caller thread did't process any work items.\n+//\n+// We need this state to signal the waiter thread just once, from a thread that\n+// completed the task.S\n+enum class TaskState { kPending, kComplete, kDone };\n+\n+class Task final : public SlinkyThreadPool::task {\n+ public:\n+  Task(SlinkyThreadPool::task_body body, size_t num_work_items,\n+       size_t num_partitions);\n+\n+  // Runs this task by processing work items in the current thread.\n+  TaskState Run();\n+\n+  // Returns the number of workers that are currently working on this task.\n+  int64_t num_workers() const;\n+\n+  bool is_empty_work_queue() const;\n+  bool done() const final;\n+\n+ private:\n+  SlinkyThreadPool::task_body body_;\n+  WorkQueue work_queue_;\n+\n+  ABSL_CACHELINE_ALIGNED std::atomic<size_t> worker_index_;\n+  ABSL_CACHELINE_ALIGNED std::atomic<size_t> pending_work_items_;\n+};\n+}  // namespace\n+\n+Task::Task(SlinkyThreadPool::task_body body, size_t num_work_items,\n+           size_t num_partitions)\n+    : body_(std::move(body)),\n+      work_queue_(num_work_items, num_partitions),\n+      worker_index_(0),\n+      pending_work_items_(num_work_items) {}\n+\n+TaskState Task::Run() {\n+  // If we have more workers joining the task than the number of partitions,\n+  // then we have to wrap around to the first partition.\n+  size_t worker_index = worker_index_.fetch_add(1, std::memory_order_relaxed);\n+  if (ABSL_PREDICT_FALSE(worker_index >= work_queue_.num_partitions())) {\n+    worker_index %= work_queue_.num_partitions();\n+  }\n+\n+  // Each worker processes the body using its own copy of the task.\n+  Worker w(worker_index, &work_queue_);\n+  size_t num_processed_work_items = 0;\n+\n+  if (std::optional<size_t> item = w.Pop()) {\n+    SlinkyThreadPool::task_body body = body_;\n+\n+    do {\n+      body(*item);\n+      ++num_processed_work_items;\n+    } while ((item = w.Pop()).has_value());\n+  }\n+\n+  // The number of pending work items should never go below zero.\n+  size_t previous_work_items = pending_work_items_.fetch_sub(\n+      num_processed_work_items, std::memory_order_acq_rel);\n+  DCHECK_GE(previous_work_items, num_processed_work_items);\n+\n+  // Task is done if we have no more work items to process. Task is complete if\n+  // we are the one who processed the last work item.\n+  bool is_done = previous_work_items == num_processed_work_items;\n+  bool is_complete = is_done && num_processed_work_items > 0;\n+\n+  return is_complete ? TaskState::kComplete\n+         : is_done   ? TaskState::kDone\n+                     : TaskState::kPending;\n+}\n+\n+int64_t Task::num_workers() const {\n+  return worker_index_.load(std::memory_order_relaxed);\n+}\n+\n+bool Task::is_empty_work_queue() const { return work_queue_.IsEmpty(); }\n+\n+bool Task::done() const {\n+  return pending_work_items_.load(std::memory_order_acquire) == 0;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// SlinkyThreadPool::Impl\n+//===----------------------------------------------------------------------===//\n+\n+// We keep a stack of tasks that are currently being processed by current\n+// thread, to avoid recursive calls.\n+static thread_local std::vector<const Task*> task_stack;  // NOLINT\n+\n+class SlinkyThreadPool::Impl : public slinky::ref_counted<Impl> {\n+ public:\n+  explicit Impl(Eigen::ThreadPoolInterface* threadpool);\n+\n+  // Enqueues a new task into the queue and returns a reference to it.\n+  slinky::ref_count<Task> Enqueue(SlinkyThreadPool::task_body body,\n+                                  size_t num_work_items, size_t num_partitions);\n+\n+  // Work on the single task and return the state of the task.\n+  TaskState WorkOnTask(Task* task);\n+\n+  // Work on all tasks in the queue. Returns when Run out of tasks to process.\n+  void WorkOnTasks(const absl::Condition& condition);\n+\n+  void Await(const absl::Condition& condition);\n+  void AtomicCall(slinky::function_ref<void()> t);\n+\n+  // Returns true if we can schedule more workers into the underlying scheduler.\n+  bool CanScheduleWorkers() const;\n+\n+  // Schedules the given number of workers for the given task. Worker scheduling\n+  // uses recursive work splitting and early exit if the task does not need any\n+  // more workers, of if we reached the maximum number of scheduled workers.\n+  void ScheduleWorkers(int64_t num_workers, slinky::ref_count<Task> task);\n+\n+  size_t thread_count() const { return thread_count_; }\n+\n+ private:\n+  friend class slinky::ref_counted<Impl>;\n+  static void destroy(Impl* ptr) { delete ptr; }\n+\n+  // A state of the work scheduling for a given task.\n+  struct ScheduleState : public slinky::ref_counted<ScheduleState> {\n+    ScheduleState(int64_t remaining_workers, slinky::ref_count<Task> task,\n+                  slinky::ref_count<Impl> impl)\n+        : remaining_workers(remaining_workers),\n+          task(std::move(task)),\n+          impl(std::move(impl)) {}\n+\n+    static void destroy(ScheduleState* ptr) { delete ptr; }\n+\n+    std::atomic<int64_t> remaining_workers;\n+    slinky::ref_count<Task> task;\n+    slinky::ref_count<Impl> impl;\n+  };\n+\n+  // Worker scheduling function for the underlying scheduler.\n+  template <bool release_impl_ref>\n+  static void ScheduleWorkers(ScheduleState* context);\n+\n+  // Dequeues a pending task from the queue.\n+  slinky::ref_count<Task> Dequeue();\n+\n+  // Signals all waiter threads waiting on the waiter mutex.\n+  void SignalWaiters();\n+\n+  Eigen::ThreadPoolInterface* threadpool_;\n+  size_t thread_count_;\n+\n+  std::deque<slinky::ref_count<Task>> tasks_ ABSL_GUARDED_BY(tasks_mutex_);\n+\n+  // A mutex for guarding mutable state accessed concurrently.\n+  ABSL_CACHELINE_ALIGNED absl::Mutex tasks_mutex_;\n+\n+  // A mutex for signalling threads waiting on the tasks or conditions.\n+  ABSL_CACHELINE_ALIGNED absl::Mutex waiter_mutex_;\n+};\n+\n+SlinkyThreadPool::Impl::Impl(Eigen::ThreadPoolInterface* threadpool)\n+    : threadpool_(threadpool),\n+      thread_count_(threadpool_ ? threadpool_->NumThreads() : 0) {}\n+\n+slinky::ref_count<Task> SlinkyThreadPool::Impl::Enqueue(\n+    SlinkyThreadPool::task_body body, size_t num_work_items,\n+    size_t num_partitions) {\n+  slinky::ref_count<Task> task(\n+      new Task(std::move(body), num_work_items, num_partitions));\n+\n+  absl::MutexLock lock(tasks_mutex_);\n+  return tasks_.emplace_back(std::move(task));\n+}\n+\n+slinky::ref_count<Task> SlinkyThreadPool::Impl::Dequeue() {\n+  absl::MutexLock lock(tasks_mutex_);\n+\n+  for (auto i = tasks_.begin(); i != tasks_.end();) {\n+    slinky::ref_count<Task>& task = *i;\n+\n+    // Task doesn't have any more work items to process.\n+    if (ABSL_PREDICT_FALSE(task->is_empty_work_queue())) {\n+      i = tasks_.erase(i);\n+      continue;\n+    }\n+\n+    // Don't Run the same task multiple times on the same thread.\n+    if (ABSL_PREDICT_FALSE(absl::c_contains(task_stack, &*task))) {\n+      ++i;\n+      continue;\n+    }\n+\n+    return task;\n+  }\n+\n+  return nullptr;\n+}\n+\n+TaskState SlinkyThreadPool::Impl::WorkOnTask(Task* task) {\n+  DCHECK(absl::c_find(task_stack, task) == task_stack.end());\n+\n+  task_stack.push_back(task);\n+  TaskState state = task->Run();\n+  task_stack.pop_back();\n+\n+  // If we are the one who completed the task, we signal the waiters to wake upS\n+  // any threads that are waiting for the task completion. If the task was\n+  // completed by another worker, we do nothing to avoid the cost of waking up\n+  // the same thread multiple times.\n+  if (ABSL_PREDICT_FALSE(state == TaskState::kComplete)) {\n+    SignalWaiters();\n+  }\n+\n+  return state;\n+}\n+\n+void SlinkyThreadPool::Impl::WorkOnTasks(const absl::Condition& condition) {\n+  while (slinky::ref_count<Task> task = Dequeue()) {\n+    WorkOnTask(&*task);\n+\n+    if (ABSL_PREDICT_TRUE(condition.Eval())) {\n+      return;\n+    }\n+  }\n+}\n+\n+void SlinkyThreadPool::Impl::Await(const absl::Condition& condition) {\n+  if (ABSL_PREDICT_FALSE(!condition.Eval())) {\n+    absl::MutexLock lock(waiter_mutex_);\n+    waiter_mutex_.Await(condition);\n+  }\n+}\n+\n+void SlinkyThreadPool::Impl::SignalWaiters() {\n+  absl::MutexLock lock(waiter_mutex_);\n+}\n+\n+void SlinkyThreadPool::Impl::AtomicCall(slinky::function_ref<void()> t) {\n+  absl::MutexLock lock(waiter_mutex_);\n+  t();\n+}\n+\n+bool SlinkyThreadPool::Impl::CanScheduleWorkers() const {\n+  // One reference is owned by the parent SlinkyThreadPool, every other\n+  // reference is owned by a worker scheduled into the underlying scheduler.\n+  return ref_count() < 1 + thread_count();\n+}\n+\n+void SlinkyThreadPool::Impl::ScheduleWorkers(int64_t num_workers,\n+                                             slinky::ref_count<Task> task) {\n+  if (ABSL_PREDICT_TRUE(num_workers > 0 && CanScheduleWorkers())) {\n+    slinky::ref_count<ScheduleState> state(\n+        new ScheduleState(num_workers - 1, std::move(task), {this}));\n+    threadpool_->Schedule([state = state.take()]() {\n+      ScheduleWorkers</*release_impl_ref=*/false>(state);\n+    });\n+  }\n+}\n+\n+template <bool release_impl_ref>\n+void SlinkyThreadPool::Impl::ScheduleWorkers(ScheduleState* context) {\n+  auto state = slinky::ref_count<ScheduleState>::assume(context);\n+\n+  // We recursively keep scheduling workers into the underlying scheduler.\n+  // This is more efficient than scheduling them sequentially from a single\n+  // thread, because workers can start processing the task sooner and we\n+  // distribute thread wake-ups evenly across underlying threads.\n+  static constexpr int32_t kNumRecursiveWorkers = 2;\n+\n+  for (size_t i = 0; i < kNumRecursiveWorkers; ++i) {\n+    bool schedule_worker =\n+        state->impl->CanScheduleWorkers() &&\n+        !state->task->is_empty_work_queue() &&\n+        state->remaining_workers.fetch_sub(1, std::memory_order_relaxed) > 0;\n+\n+    if (ABSL_PREDICT_TRUE(!schedule_worker)) {\n+      break;\n+    }\n+\n+    // Add +1 reference to account for the scheduled worker, as we use `impl`\n+    // reference count to track the number of active workers.\n+    state->impl->add_ref();\n+    state->impl->threadpool_->Schedule(\n+        [state = slinky::ref_count<ScheduleState>(state).take()]() {\n+          SlinkyThreadPool::Impl::ScheduleWorkers</*release_impl_ref=*/true>(\n+              state);\n+        });\n+  }\n+\n+  // Keep processing tasks from the queue until we are out of tasks.\n+  static constexpr bool kFalse = false;\n+  state->impl->WorkOnTasks(absl::Condition(&kFalse));\n+\n+  // One `impl` reference implicitly owned by the `state`, every additional\n+  // reference is added and released explicitly by the worker task.\n+  if constexpr (release_impl_ref) {\n+    state->impl->release();\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// SlinkyThreadPool\n+//===----------------------------------------------------------------------===//\n+\n+SlinkyThreadPool::SlinkyThreadPool(Eigen::ThreadPoolDevice* device)\n+    : impl_(new Impl(device ? device->getPool() : nullptr)) {}\n+\n+SlinkyThreadPool::SlinkyThreadPool(Eigen::ThreadPoolInterface* threadpool)\n+    : impl_(new Impl(threadpool)) {}\n+\n+SlinkyThreadPool::~SlinkyThreadPool() = default;\n+\n+slinky::ref_count<SlinkyThreadPool::task> SlinkyThreadPool::enqueue(\n+    size_t n, task_body t, int32_t max_workers) {\n+  CHECK_GE(max_workers, n);\n+\n+  // Don't create more partitions than the number of threads. Also make sure\n+  // that we have at least one partition (if we don't have a scheduler).\n+  size_t num_partitions = std::min<size_t>(n, thread_count());\n+  num_partitions = std::max<size_t>(1, num_partitions);\n+\n+  auto task = impl_->Enqueue(std::move(t), n, num_partitions);\n+\n+  // If we don't have any worker threads, we return a task to the caller, and\n+  // assume that the caller will wait on it.\n+  if (ABSL_PREDICT_FALSE(impl_->thread_count() == 0)) {\n+    return task;\n+  }\n+\n+  // We assume that the caller will immediately start working on the task, so we\n+  // need to schedule workers only for the remaining number of partitions.\n+  impl_->ScheduleWorkers(/*num_workers=*/num_partitions - 1, task);\n+\n+  return task;\n+}\n+\n+void SlinkyThreadPool::wait_for(task* t) {\n+  Task* task = static_cast<Task*>(t);\n+  TaskState state = impl_->WorkOnTask(task);\n+\n+  // If the task is complete or done, we are immediately done with waiting.\n+  if (ABSL_PREDICT_TRUE(state == TaskState::kComplete ||\n+                        state == TaskState::kDone)) {\n+    return;\n+  }\n+\n+  // Switch to the work stealing mode and work on other tasks in the queue\n+  // until the given task is done.\n+  impl_->WorkOnTasks(absl::Condition(task, &Task::done));\n+  impl_->Await(absl::Condition(task, &Task::done));\n+}\n+\n+void SlinkyThreadPool::wait_for(predicate_ref condition) {\n+  impl_->WorkOnTasks(absl::Condition(&condition));\n+  impl_->Await(absl::Condition(&condition));\n+}\n+\n+void SlinkyThreadPool::atomic_call(slinky::function_ref<void()> t) {\n+  impl_->AtomicCall(t);\n+}\n+\n+int SlinkyThreadPool::thread_count() const { return impl_->thread_count(); }\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "f3f6a7ee24c7831a53f4fe4736cb54c257f1c002",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/slinky_threadpool.h",
            "status": "added",
            "additions": 61,
            "deletions": 0,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.h?ref=7ba33178571e22400f943f60848825419bd5b2f0",
            "patch": "@@ -0,0 +1,61 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_YNNPACK_SLINKY_THREADPOOL_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_YNNPACK_SLINKY_THREADPOOL_H_\n+\n+#include <cstddef>\n+#include <cstdint>\n+\n+#include \"slinky/base/function_ref.h\"\n+#include \"slinky/base/ref_count.h\"\n+#include \"slinky/base/thread_pool.h\"\n+\n+namespace Eigen {\n+struct ThreadPoolDevice;\n+class ThreadPoolInterface;\n+}  // namespace Eigen\n+\n+namespace xla::cpu {\n+\n+// This is an implementation of slinky::thread_pool, using absl::Mutex for\n+// synchronization, and dispatches work to Eigen::ThreadPoolInterface.\n+class SlinkyThreadPool final : public slinky::thread_pool {\n+ public:\n+  explicit SlinkyThreadPool(Eigen::ThreadPoolDevice* device);\n+  explicit SlinkyThreadPool(Eigen::ThreadPoolInterface* threadpool);\n+  ~SlinkyThreadPool() final;\n+\n+  SlinkyThreadPool(SlinkyThreadPool&&) = default;\n+  SlinkyThreadPool& operator=(SlinkyThreadPool&&) = default;\n+\n+  slinky::ref_count<task> enqueue(size_t n, task_body t,\n+                                  int32_t max_workers) final;\n+\n+  void wait_for(task* t) final;\n+  void wait_for(predicate_ref condition) final;\n+\n+  void atomic_call(slinky::function_ref<void()> t) final;\n+\n+  int thread_count() const final;\n+\n+ private:\n+  class Impl;\n+  slinky::ref_count<Impl> impl_;\n+};\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_YNNPACK_SLINKY_THREADPOOL_H_"
        },
        {
            "sha": "90b5f365fc46ad4b3fa17aa56bc4ad6bdd95537b",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/slinky_threadpool_test.cc",
            "status": "renamed",
            "additions": 20,
            "deletions": 34,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool_test.cc?ref=7ba33178571e22400f943f60848825419bd5b2f0",
            "patch": "@@ -13,95 +13,81 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/slinky_threadpool.h\"\n \n #include <array>\n #include <atomic>\n #include <cstddef>\n #include <cstdint>\n+#include <vector>\n \n #include <gtest/gtest.h>\n #include \"slinky/base/thread_pool.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n \n-namespace Eigen {\n-class ThreadPoolInterface;\n-}  // namespace Eigen\n-\n namespace xla::cpu {\n \n-TEST(YnnThreadpoolImpl, inline_scheduling) {\n-  auto ynn_threadpool =\n-      CreateYnnThreadpool(static_cast<Eigen::ThreadPoolInterface*>(nullptr));\n-  auto thread_pool =\n-      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n+TEST(SlinkyThreadPoolTest, InlineScheduling) {\n+  SlinkyThreadPool thread_pool(\n+      static_cast<Eigen::ThreadPoolInterface*>(nullptr));\n \n   static constexpr size_t size = 10000;\n \n   std::vector<int32_t> data(size, 0);\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool->parallel_for(size, inc);\n+  thread_pool.parallel_for(size, inc);\n \n   std::vector<int32_t> expected(size, 1);\n   EXPECT_EQ(data, expected);\n }\n \n-TEST(YnnThreadpoolImpl, single_loop) {\n+TEST(SlinkyThreadPoolTest, SingleLoop) {\n   tsl::thread::ThreadPool test_thread_pool(tsl::Env::Default(), \"test\", 4);\n-  auto ynn_threadpool =\n-      CreateYnnThreadpool(test_thread_pool.AsEigenThreadPool());\n-  auto thread_pool =\n-      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n+  SlinkyThreadPool thread_pool(test_thread_pool.AsEigenThreadPool());\n \n   static constexpr size_t size = 10000;\n \n   std::vector<int32_t> data(size, 0);\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool->parallel_for(size, inc);\n+  thread_pool.parallel_for(size, inc);\n \n   std::vector<int32_t> expected(size, 1);\n   EXPECT_EQ(data, expected);\n }\n \n-TEST(YnnThreadpoolImpl, loop_chain) {\n+TEST(SlinkyThreadPoolTest, LoopChain) {\n   tsl::thread::ThreadPool test_thread_pool(tsl::Env::Default(), \"test\", 4);\n-  auto ynn_threadpool =\n-      CreateYnnThreadpool(test_thread_pool.AsEigenThreadPool());\n-  auto thread_pool =\n-      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n+  SlinkyThreadPool thread_pool(test_thread_pool.AsEigenThreadPool());\n \n   static constexpr size_t size = 10000;\n \n   std::vector<int32_t> data(size, 0);\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool->parallel_for(size, inc);\n-  thread_pool->parallel_for(size, inc);\n-  thread_pool->parallel_for(size, inc);\n-  thread_pool->parallel_for(size, inc);\n-  thread_pool->parallel_for(size, inc);\n+  thread_pool.parallel_for(size, inc);\n+  thread_pool.parallel_for(size, inc);\n+  thread_pool.parallel_for(size, inc);\n+  thread_pool.parallel_for(size, inc);\n+  thread_pool.parallel_for(size, inc);\n \n   std::vector<int32_t> expected(size, 5);\n   EXPECT_EQ(data, expected);\n }\n \n-TEST(YnnThreadpoolImpl, nested_loops) {\n+TEST(SlinkyThreadPoolTest, NestedLoops) {\n   tsl::thread::ThreadPool test_thread_pool(tsl::Env::Default(), \"test\", 4);\n-  auto ynn_threadpool =\n-      CreateYnnThreadpool(test_thread_pool.AsEigenThreadPool());\n-  auto thread_pool =\n-      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n+  SlinkyThreadPool thread_pool(test_thread_pool.AsEigenThreadPool());\n \n   static constexpr size_t size = 100;\n \n   std::array<std::atomic<int32_t>, size> data = {{0}};\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool->parallel_for(\n-      size, [&](size_t i) { thread_pool->parallel_for(size, inc); });\n+  thread_pool.parallel_for(\n+      size, [&](size_t i) { thread_pool.parallel_for(size, inc); });\n \n   for (size_t i = 0; i < size; ++i) {\n     EXPECT_EQ(data[i], size);",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_threadpool_test.cc"
        },
        {
            "sha": "11027e4b90adee4a3a37e2fbe5a0098ebc8c582f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_threadpool.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 557,
            "changes": 561,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7ba33178571e22400f943f60848825419bd5b2f0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc?ref=7ba33178571e22400f943f60848825419bd5b2f0",
            "patch": "@@ -13,580 +13,27 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include <algorithm>\n-#include <atomic>\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n+\n #include <cassert>\n-#include <cstddef>\n-#include <cstdint>\n-#include <deque>\n-#include <optional>\n-#include <utility>\n-#include <vector>\n \n #include \"ynnpack/include/ynnpack.h\"\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/base/optimization.h\"\n-#include \"absl/base/thread_annotations.h\"\n-#include \"absl/container/fixed_array.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n-#include \"absl/synchronization/mutex.h\"\n-#include \"slinky/base/function_ref.h\"\n-#include \"slinky/base/ref_count.h\"\n-#include \"slinky/base/thread_pool.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/slinky_threadpool.h\"\n #include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n-#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n \n #define EIGEN_USE_THREADS\n #include \"Eigen/ThreadPool\"\n #include \"unsupported/Eigen/CXX11/Tensor\"\n \n namespace xla::cpu {\n \n-namespace {\n-\n-// This is an implementation of slinky::thread_pool, using absl::Mutex for\n-// synchronization, and dispatches work to Eigen::ThreadPoolInterface.\n-class YnnThreadpoolImpl final : public slinky::thread_pool {\n- public:\n-  explicit YnnThreadpoolImpl(Eigen::ThreadPoolDevice* device);\n-  explicit YnnThreadpoolImpl(Eigen::ThreadPoolInterface* threadpool);\n-  ~YnnThreadpoolImpl() final;\n-\n-  YnnThreadpoolImpl(YnnThreadpoolImpl&&) = delete;\n-  YnnThreadpoolImpl& operator=(YnnThreadpoolImpl&&) = delete;\n-\n-  slinky::ref_count<task> enqueue(size_t n, task_body t,\n-                                  int32_t max_workers) final;\n-\n-  void wait_for(task* t) final;\n-  void wait_for(predicate_ref condition) final;\n-\n-  void atomic_call(slinky::function_ref<void()> t) final;\n-\n-  int thread_count() const final;\n-\n- private:\n-  class impl;\n-  slinky::ref_count<impl> impl_;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-// work_queue\n-//===----------------------------------------------------------------------===//\n-\n-// Forward declare.\n-class worker;\n-\n-// A work queue that partitions `num_work_items` work items into\n-// `num_partitions` partitions processed by parallel workers.\n-class work_queue {\n- public:\n-  work_queue(size_t num_work_items, size_t num_partitions);\n-\n-  // Returns the next work item in the given partition. Returns std::nullopt\n-  // if the partition is complete.\n-  std::optional<size_t> pop_work_item(size_t partition_index);\n-\n-  // Return the partition [begin, end) work item range.\n-  std::pair<size_t, size_t> partition_range(size_t partition_index) const;\n-\n-  size_t num_partitions() const { return partitions_.size(); }\n-\n-  // If work queue is empty, it means that all work items are being processed by\n-  // the workers, and the task will be done once all workers complete.\n-  bool is_empty() const { return empty_.load(std::memory_order_relaxed); }\n-\n- private:\n-  friend class worker;\n-\n-  // Work items partition tracking the next work item to process.\n-  struct partition {\n-    void initialize(size_t begin, size_t end);\n-\n-    // Tracks index of the next work item in the assigned partition.\n-    ABSL_CACHELINE_ALIGNED std::atomic<size_t> index;\n-    size_t begin;\n-    size_t end;\n-  };\n-\n-  void set_empty() { empty_.store(true, std::memory_order_relaxed); }\n-\n-  absl::FixedArray<partition, 32> partitions_;\n-  ABSL_CACHELINE_ALIGNED std::atomic<bool> empty_;\n-};\n-\n-}  // namespace\n-\n-void work_queue::partition::initialize(size_t begin, size_t end) {\n-  index.store(begin, std::memory_order_relaxed);\n-  this->begin = begin;\n-  this->end = end;\n-}\n-\n-work_queue::work_queue(size_t num_work_items, size_t num_partitions)\n-    : partitions_(num_partitions), empty_(num_work_items == 0) {\n-  size_t partition_size = num_work_items / num_partitions;\n-  size_t rem_work = num_work_items % num_partitions;\n-  for (size_t i = 0, begin = 0, end = 0; i < num_partitions; ++i, begin = end) {\n-    end = begin + partition_size + ((i < rem_work) ? 1 : 0);\n-    partitions_[i].initialize(begin, end);\n-  }\n-}\n-\n-std::optional<size_t> work_queue::pop_work_item(size_t partition_index) {\n-  DCHECK(partition_index < partitions_.size()) << \"Invalid partition index\";\n-  partition& partition = partitions_.data()[partition_index];\n-\n-  // Check if partition is already empty.\n-  if (size_t index = partition.index.load(std::memory_order_relaxed);\n-      ABSL_PREDICT_FALSE(index >= partition.end)) {\n-    return std::nullopt;\n-  }\n-\n-  // Try to acquire the next work item in the partition.\n-  size_t index = partition.index.fetch_add(1, std::memory_order_relaxed);\n-  return ABSL_PREDICT_FALSE(index >= partition.end) ? std::nullopt\n-                                                    : std::make_optional(index);\n-}\n-\n-std::pair<size_t, size_t> work_queue::partition_range(\n-    size_t partition_index) const {\n-  DCHECK(partition_index < partitions_.size()) << \"Invalid partition index\";\n-  return {partitions_[partition_index].begin, partitions_[partition_index].end};\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// worker\n-//===----------------------------------------------------------------------===//\n-\n-namespace {\n-\n-// Worker processes work items from the work queue starting from the assigned\n-// work partition. Once the assigned partition is complete it tries to pop\n-// the work item from the next partition. Once the work queue is empty (the\n-// worker wraps around to the initial partition) it returns and empty work item.\n-class worker {\n- public:\n-  worker(size_t partition_index, work_queue* queue);\n-\n-  std::optional<size_t> pop_work_item();\n-\n- private:\n-  size_t initial_partition_index_;\n-  size_t partition_index_;\n-  work_queue* queue_;\n-};\n-\n-}  // namespace\n-\n-worker::worker(size_t partition_index, work_queue* queue)\n-    : initial_partition_index_(partition_index),\n-      partition_index_(partition_index),\n-      queue_(queue) {}\n-\n-std::optional<size_t> worker::pop_work_item() {\n-  std::optional<size_t> work = queue_->pop_work_item(partition_index_);\n-  if (ABSL_PREDICT_TRUE(work)) {\n-    return work;\n-  }\n-\n-  while (!work.has_value() && !queue_->is_empty()) {\n-    // Wrap around to the first partition.\n-    if (ABSL_PREDICT_FALSE(++partition_index_ >= queue_->num_partitions())) {\n-      partition_index_ = 0;\n-    }\n-\n-    // We checked all partitions and got back to the partition we started from.\n-    if (ABSL_PREDICT_FALSE(partition_index_ == initial_partition_index_)) {\n-      queue_->set_empty();\n-      break;\n-    }\n-\n-    work = queue_->pop_work_item(partition_index_);\n-  }\n-\n-  return work;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// task_impl\n-//===----------------------------------------------------------------------===//\n-\n-namespace {\n-\n-// Running a task can result in three states:\n-//\n-//   kPending:  The task is still being processed by the worker threads.\n-//   kComplete: The caller thread is the one who completed the task.\n-//   kDone:     The task is done and all work items have been processed, however\n-//              the caller thread did't process any work items.\n-//\n-// We need this state to signal the waiter thread just once, from a thread that\n-// completed the task.\n-enum class task_state { kPending, kComplete, kDone };\n-\n-class task_impl final : public YnnThreadpoolImpl::task {\n- public:\n-  task_impl(YnnThreadpoolImpl::task_body body, size_t num_work_items,\n-            size_t num_partitions);\n-\n-  // Runs this task by process work items in the current thread.\n-  task_state run();\n-\n-  int64_t num_workers() const;\n-  bool is_empty_work_queue() const;\n-  bool done() const final;\n-\n- private:\n-  YnnThreadpoolImpl::task_body body_;\n-  work_queue work_queue_;\n-\n-  ABSL_CACHELINE_ALIGNED std::atomic<size_t> worker_index_;\n-  ABSL_CACHELINE_ALIGNED std::atomic<size_t> pending_work_items_;\n-};\n-\n-task_impl::task_impl(YnnThreadpoolImpl::task_body body, size_t num_work_items,\n-                     size_t num_partitions)\n-    : body_(std::move(body)),\n-      work_queue_(num_work_items, num_partitions),\n-      worker_index_(0),\n-      pending_work_items_(num_work_items) {}\n-\n-task_state task_impl::run() {\n-  // If we have more workers joining the task than the number of partitions,\n-  // then we have to wrap around to the first partition.\n-  size_t worker_index = worker_index_.fetch_add(1, std::memory_order_relaxed);\n-  if (ABSL_PREDICT_FALSE(worker_index >= work_queue_.num_partitions())) {\n-    worker_index %= work_queue_.num_partitions();\n-  }\n-\n-  // Each worker processes the body using its own copy of the task.\n-  worker w(worker_index, &work_queue_);\n-  size_t num_processed_work_items = 0;\n-\n-  if (std::optional<size_t> item = w.pop_work_item()) {\n-    YnnThreadpoolImpl::task_body body = body_;\n-\n-    do {\n-      body(*item);\n-      ++num_processed_work_items;\n-    } while ((item = w.pop_work_item()).has_value());\n-  }\n-\n-  // The number of pending work items should never go below zero.\n-  size_t previous_work_items = pending_work_items_.fetch_sub(\n-      num_processed_work_items, std::memory_order_acq_rel);\n-  DCHECK_GE(previous_work_items, num_processed_work_items);\n-\n-  // Task is done if we have no more work items to process. Task is complete if\n-  // we are the one who processed the last work item.\n-  bool is_done = previous_work_items == num_processed_work_items;\n-  bool is_complete = is_done && num_processed_work_items > 0;\n-\n-  return is_complete ? task_state::kComplete\n-         : is_done   ? task_state::kDone\n-                     : task_state::kPending;\n-}\n-\n-int64_t task_impl::num_workers() const {\n-  return worker_index_.load(std::memory_order_relaxed);\n-}\n-\n-bool task_impl::is_empty_work_queue() const { return work_queue_.is_empty(); }\n-\n-bool task_impl::done() const {\n-  return pending_work_items_.load(std::memory_order_acquire) == 0;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// YnnThreadpoolImpl::impl\n-//===----------------------------------------------------------------------===//\n-\n-// We keep a stack of tasks that are currently being processed by current\n-// thread, to avoid recursive calls.\n-static thread_local std::vector<const task_impl*> task_stack;  // NOLINT\n-\n-class YnnThreadpoolImpl::impl : public slinky::ref_counted<impl> {\n- public:\n-  explicit impl(Eigen::ThreadPoolInterface* threadpool);\n-\n-  // Work on the single task and return the state of the task.\n-  task_state work_on_task(task_impl* task);\n-\n-  // Work on all tasks in the queue. Returns when run out of tasks to process.\n-  void work_on_tasks(const absl::Condition& condition);\n-\n-  // Enqueues a new task into the queue and returns a reference to it.\n-  slinky::ref_count<task_impl> enqueue(YnnThreadpoolImpl::task_body body,\n-                                       size_t num_work_items,\n-                                       size_t num_partitions);\n-\n-  void await(const absl::Condition& condition);\n-\n-  void atomic_call(slinky::function_ref<void()> t);\n-\n-  // Returns true if we can schedule more workers into the underlying scheduler.\n-  bool can_schedule_workers() const;\n-\n-  // Schedules the given number of workers for the given task. Worker scheduling\n-  // uses recursive work splitting and early exit if the task does not need any\n-  // more workers, of if we reached the maximum number of scheduled workers.\n-  void schedule_workers(int64_t num_workers, slinky::ref_count<task_impl> task);\n-\n-  size_t thread_count() const { return thread_count_; }\n-\n- private:\n-  friend class slinky::ref_counted<impl>;\n-  static void destroy(impl* ptr) { delete ptr; }\n-\n-  // A state of the work scheduling for a given task.\n-  struct schedule_state : public slinky::ref_counted<schedule_state> {\n-    schedule_state(int64_t remaining_workers, slinky::ref_count<task_impl> task,\n-                   slinky::ref_count<impl> impl)\n-        : remaining_workers(remaining_workers),\n-          task(std::move(task)),\n-          impl(std::move(impl)) {}\n-\n-    static void destroy(schedule_state* ptr) { delete ptr; }\n-\n-    std::atomic<int64_t> remaining_workers;\n-    slinky::ref_count<task_impl> task;\n-    slinky::ref_count<impl> impl;\n-  };\n-\n-  // Worker scheduling function for the underlying scheduler.\n-  template <bool release_impl_ref>\n-  static void schedule_workers(schedule_state* context);\n-\n-  // Dequeues a pending task from the queue.\n-  slinky::ref_count<task_impl> dequeue();\n-\n-  // Signals all waiter threads waiting on the waiter mutex.\n-  void signal_waiters();\n-\n-  Eigen::ThreadPoolInterface* threadpool_;\n-  size_t thread_count_;\n-\n-  std::deque<slinky::ref_count<task_impl>> tasks_ ABSL_GUARDED_BY(tasks_mutex_);\n-\n-  // A mutex for guarding mutable state accessed concurrently.\n-  ABSL_CACHELINE_ALIGNED absl::Mutex tasks_mutex_;\n-\n-  // A mutex for signalling threads waiting on the tasks or conditions.\n-  ABSL_CACHELINE_ALIGNED absl::Mutex waiter_mutex_;\n-};\n-\n-YnnThreadpoolImpl::impl::impl(Eigen::ThreadPoolInterface* threadpool)\n-    : threadpool_(threadpool),\n-      thread_count_(threadpool_ ? threadpool_->NumThreads() : 0) {}\n-\n-slinky::ref_count<task_impl> YnnThreadpoolImpl::impl::enqueue(\n-    YnnThreadpoolImpl::task_body body, size_t num_work_items,\n-    size_t num_partitions) {\n-  slinky::ref_count<task_impl> task(\n-      new task_impl(std::move(body), num_work_items, num_partitions));\n-\n-  absl::MutexLock lock(tasks_mutex_);\n-  return tasks_.emplace_back(std::move(task));\n-}\n-\n-slinky::ref_count<task_impl> YnnThreadpoolImpl::impl::dequeue() {\n-  absl::MutexLock lock(tasks_mutex_);\n-\n-  for (auto i = tasks_.begin(); i != tasks_.end();) {\n-    slinky::ref_count<task_impl>& task = *i;\n-\n-    // Task doesn't have any more work items to process.\n-    if (ABSL_PREDICT_FALSE(task->is_empty_work_queue())) {\n-      i = tasks_.erase(i);\n-      continue;\n-    }\n-\n-    // Don't run the same task multiple times on the same thread.\n-    if (ABSL_PREDICT_FALSE(absl::c_contains(task_stack, &*task))) {\n-      ++i;\n-      continue;\n-    }\n-\n-    return task;\n-  }\n-\n-  return nullptr;\n-}\n-\n-task_state YnnThreadpoolImpl::impl::work_on_task(task_impl* task) {\n-  DCHECK(absl::c_find(task_stack, task) == task_stack.end());\n-\n-  task_stack.push_back(task);\n-  task_state state = task->run();\n-  task_stack.pop_back();\n-\n-  // If we are the one who completed the task, we signal the waiters to wake upS\n-  // any threads that are waiting for the task completion. If the task was\n-  // completed by another worker, we do nothing to avoid the cost of waking up\n-  // the same thread multiple times.\n-  if (ABSL_PREDICT_FALSE(state == task_state::kComplete)) {\n-    signal_waiters();\n-  }\n-\n-  return state;\n-}\n-\n-void YnnThreadpoolImpl::impl::work_on_tasks(const absl::Condition& condition) {\n-  while (slinky::ref_count<task_impl> task = dequeue()) {\n-    work_on_task(&*task);\n-\n-    if (ABSL_PREDICT_TRUE(condition.Eval())) {\n-      return;\n-    }\n-  }\n-}\n-\n-void YnnThreadpoolImpl::impl::await(const absl::Condition& condition) {\n-  if (ABSL_PREDICT_FALSE(!condition.Eval())) {\n-    absl::MutexLock lock(waiter_mutex_);\n-    waiter_mutex_.Await(condition);\n-  }\n-}\n-\n-void YnnThreadpoolImpl::impl::signal_waiters() {\n-  absl::MutexLock lock(waiter_mutex_);\n-}\n-\n-void YnnThreadpoolImpl::impl::atomic_call(slinky::function_ref<void()> t) {\n-  absl::MutexLock lock(waiter_mutex_);\n-  t();\n-}\n-\n-bool YnnThreadpoolImpl::impl::can_schedule_workers() const {\n-  // One reference is owned by the parent YnnThreadpoolImpl, every other\n-  // reference is owned by a worker scheduled into the underlying scheduler.\n-  return ref_count() < 1 + thread_count();\n-}\n-\n-void YnnThreadpoolImpl::impl::schedule_workers(\n-    int64_t num_workers, slinky::ref_count<task_impl> task) {\n-  if (ABSL_PREDICT_TRUE(num_workers > 0 && can_schedule_workers())) {\n-    slinky::ref_count<schedule_state> state(\n-        new schedule_state(num_workers - 1, std::move(task), {this}));\n-    threadpool_->Schedule([state = state.take()]() {\n-      schedule_workers</*release_impl_ref=*/false>(state);\n-    });\n-  }\n-}\n-\n-template <bool release_impl_ref>\n-void YnnThreadpoolImpl::impl::schedule_workers(schedule_state* context) {\n-  auto state = slinky::ref_count<schedule_state>::assume(context);\n-\n-  // We recursively keep scheduling workers into the underlying scheduler.\n-  // This is more efficient than scheduling them sequentially from a single\n-  // thread, because workers can start processing the task sooner and we\n-  // distribute thread wake-ups evenly across underlying threads.\n-  static constexpr int32_t kNumRecursiveWorkers = 2;\n-\n-  for (size_t i = 0; i < kNumRecursiveWorkers; ++i) {\n-    bool schedule_worker =\n-        state->impl->can_schedule_workers() &&\n-        !state->task->is_empty_work_queue() &&\n-        state->remaining_workers.fetch_sub(1, std::memory_order_relaxed) > 0;\n-\n-    if (ABSL_PREDICT_TRUE(!schedule_worker)) {\n-      break;\n-    }\n-\n-    // Add +1 reference to account for the scheduled worker, as we use `impl`\n-    // reference count to track the number of active workers.\n-    state->impl->add_ref();\n-    state->impl->threadpool_->Schedule(\n-        [state = slinky::ref_count<schedule_state>(state).take()]() {\n-          YnnThreadpoolImpl::impl::schedule_workers</*release_impl_ref=*/true>(\n-              state);\n-        });\n-  }\n-\n-  // Keep processing tasks from the queue until we are out of tasks.\n-  static constexpr bool kFalse = false;\n-  state->impl->work_on_tasks(absl::Condition(&kFalse));\n-\n-  // One `impl` reference implicitly owned by the `state`, every additional\n-  // reference is added and released explicitly by the worker task.\n-  if constexpr (release_impl_ref) {\n-    state->impl->release();\n-  }\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// YnnThreadpoolImpl\n-//===----------------------------------------------------------------------===//\n-\n-YnnThreadpoolImpl::YnnThreadpoolImpl(Eigen::ThreadPoolDevice* device)\n-    : impl_(new impl(device ? device->getPool() : nullptr)) {}\n-\n-YnnThreadpoolImpl::YnnThreadpoolImpl(Eigen::ThreadPoolInterface* threadpool)\n-    : impl_(new impl(threadpool)) {}\n-\n-YnnThreadpoolImpl::~YnnThreadpoolImpl() = default;\n-\n-slinky::ref_count<YnnThreadpoolImpl::task> YnnThreadpoolImpl::enqueue(\n-    size_t n, task_body t, int32_t max_workers) {\n-  CHECK_GE(max_workers, n);\n-\n-  // Don't create more partitions than the number of threads. Also make sure\n-  // that we have at least one partition (if we don't have a scheduler).\n-  size_t num_partitions = std::min<size_t>(n, thread_count());\n-  num_partitions = std::max<size_t>(1, num_partitions);\n-\n-  auto task = impl_->enqueue(std::move(t), n, num_partitions);\n-\n-  // If we don't have any worker threads, we return a task to the caller, and\n-  // assume that the caller will wait on it.\n-  if (ABSL_PREDICT_FALSE(impl_->thread_count() == 0)) {\n-    return task;\n-  }\n-\n-  // We assume that the caller will immediately start working on the task, so we\n-  // need to schedule workers only for the remaining number of partitions.\n-  impl_->schedule_workers(/*num_workers=*/num_partitions - 1, task);\n-\n-  return task;\n-}\n-\n-void YnnThreadpoolImpl::wait_for(task* t) {\n-  task_impl* task = static_cast<task_impl*>(t);\n-  task_state state = impl_->work_on_task(task);\n-\n-  // If the task is complete or done, we are immediately done with waiting.\n-  if (ABSL_PREDICT_TRUE(state == task_state::kComplete ||\n-                        state == task_state::kDone)) {\n-    return;\n-  }\n-\n-  // Switch to the work stealing mode and work on other tasks in the queue\n-  // until the given task is done.\n-  impl_->work_on_tasks(absl::Condition(task, &task_impl::done));\n-  impl_->await(absl::Condition(task, &task_impl::done));\n-}\n-\n-void YnnThreadpoolImpl::wait_for(predicate_ref condition) {\n-  impl_->work_on_tasks(absl::Condition(&condition));\n-  impl_->await(absl::Condition(&condition));\n-}\n-\n-void YnnThreadpoolImpl::atomic_call(slinky::function_ref<void()> t) {\n-  impl_->atomic_call(t);\n-}\n-\n-int YnnThreadpoolImpl::thread_count() const { return impl_->thread_count(); }\n-\n-}  // namespace\n-\n absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n     Eigen::ThreadPoolInterface* threadpool) {\n   return CreateYnnThreadpool([&](ynn_threadpool_t* ynn_threadpool) {\n     *ynn_threadpool =\n-        reinterpret_cast<ynn_threadpool_t>(new YnnThreadpoolImpl(threadpool));\n+        reinterpret_cast<ynn_threadpool_t>(new SlinkyThreadPool(threadpool));\n     return ynn_status_success;\n   });\n }"
        }
    ],
    "stats": {
        "total": 1140,
        "additions": 535,
        "deletions": 605
    }
}