{
    "author": "mrguenther",
    "message": "Integrate StableHLO at openxla/stablehlo@baaf7475\n\nPiperOrigin-RevId: 823160034",
    "sha": "96c1b6c0a67c3f32f56a7501a9bbe2b87bd9535c",
    "files": [
        {
            "sha": "55268d3a2c6741988620adcdb955c2bc2836139a",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 35,
            "deletions": 1379,
            "changes": 1414,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96c1b6c0a67c3f32f56a7501a9bbe2b87bd9535c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96c1b6c0a67c3f32f56a7501a9bbe2b87bd9535c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=96c1b6c0a67c3f32f56a7501a9bbe2b87bd9535c",
            "patch": "@@ -1,542 +1,34 @@\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir b/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n---- stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n-+++ stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n-@@ -1,5 +1,6 @@\n- // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg --split-input-file --canonicalize | FileCheck %s\n- // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg=\"enable-primitive-ops=true\" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-PRIMITIVE\n-+// RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg=\"capture-scalar-inputs=false\" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-NO-CAPTURE\n- \n- // CHECK: #map = affine_map<(d0, d1) -> (d0, d1)>\n- // CHECK-LABEL: func @float_add\n-@@ -534,6 +535,19 @@\n-   %0 = \"stablehlo.sign\"(%arg0) : (tensor<2x2xcomplex<f32>>)\n-                           -> tensor<2x2xcomplex<f32>>\n-   func.return %0 : tensor<2x2xcomplex<f32>>\n-+}\n-+\n-+// -----\n-+\n-+// CHECK-LABEL: func @float_tan\n-+// CHECK-PRIMITIVE-LABEL: func @float_tan\n-+func.func @float_tan(%arg0: tensor<2x2xf32>) -> tensor<2x2xf32> {\n-+  // CHECK: linalg.generic\n-+  // CHECK: tan\n-+  // CHECK-PRIMITIVE: linalg.map\n-+  // CHECK-PRIMITIVE: tan\n-+  %0 = \"stablehlo.tan\"(%arg0) : (tensor<2x2xf32>) -> tensor<2x2xf32>\n-+  func.return %0 : tensor<2x2xf32>\n- }\n- \n- // -----\n-@@ -926,6 +940,23 @@\n- // CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {\n- // CHECK-PRIMITIVE:        %[[RES:.*]] = arith.select %[[PRED_ELEM]], %[[LHS_]], %[[RHS_]] : f32\n- // CHECK-PRIMITIVE:        linalg.yield %[[RES]]\n-+\n-+// CHECK-NO-CAPTURE:      #[[SCALAR_MAP:.*]] = affine_map<(d0, d1) -> ()>\n-+// CHECK-NO-CAPTURE:      #[[ID_MAP:.*]] = affine_map<(d0, d1) -> (d0, d1)>\n-+// CHECK-NO-CAPTURE:      func @select_scalar_pred_dyn\n-+// CHECK-NO-CAPTURE-SAME:  (%[[PRED:.*]]: tensor<i1>, %[[LHS:.*]]: tensor<2x?xf32>, %[[RHS:.*]]: tensor<2x?xf32>)\n-+// CHECK-NO-CAPTURE-DAG:  %[[C1:.*]] = arith.constant 1\n-+// CHECK-NO-CAPTURE-DAG:  %[[DIM:.*]] =  tensor.dim %[[LHS]], %[[C1]]\n-+// CHECK-NO-CAPTURE-DAG:  %[[DST:.*]] = tensor.empty(%[[DIM]])\n-+// CHECK-NO-CAPTURE:      linalg.generic\n-+// CHECK-NO-CAPTURE-SAME:   indexing_maps = [#[[SCALAR_MAP]], #[[ID_MAP]], #[[ID_MAP]], #[[ID_MAP]]]\n-+// CHECK-NO-CAPTURE-SAME:   iterator_types = [\"parallel\", \"parallel\"]\n-+// CHECK-NO-CAPTURE-SAME:   ins(%[[PRED]], %[[LHS]], %[[RHS]] : tensor<i1>, tensor<2x?xf32>, tensor<2x?xf32>)\n-+// CHECK-NO-CAPTURE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)\n-+// CHECK-NO-CAPTURE-SAME:   {someattr}\n-+// CHECK-NO-CAPTURE:      ^bb0(%[[PRED_:.*]]: i1, %[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %{{.*}}: f32):\n-+// CHECK-NO-CAPTURE:        %[[RES:.*]] = arith.select %[[PRED_]], %[[LHS_]], %[[RHS_]] : f32\n-+// CHECK-NO-CAPTURE:        linalg.yield %[[RES]]\n- \n- // -----\n- \n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp\n---- stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp\n-@@ -140,12 +140,11 @@\n-   // (any sign-op, or an integral abs-op).\n-   // TODO(peiming, ajcbik): these all can potentially be optimized by applying\n-   // value transform on sparse_tenosr.value memref\n--  if (isa<mlir::stablehlo::SignOp>(op) || isa<mlir::stablehlo::NegOp>(op) ||\n-+  if (isa<mlir::stablehlo::SignOp, mlir::stablehlo::NegOp,\n-+          mlir::stablehlo::TanOp>(op) ||\n-       (isa<mlir::stablehlo::AbsOp>(op) && hasIntegralShapeType(op)) ||\n--      isa<chlo::AsinOp>(op) || isa<chlo::AsinhOp>(op) ||\n--      isa<chlo::AtanOp>(op) || isa<chlo::AtanhOp>(op) ||\n--      isa<chlo::BesselI1eOp>(op) || isa<chlo::SinhOp>(op) ||\n--      isa<chlo::TanOp>(op)) {\n-+      isa<chlo::AsinOp, chlo::AsinhOp, chlo::AtanOp, chlo::AtanhOp,\n-+          chlo::BesselI1eOp, chlo::SinhOp, chlo::TanOp>(op)) {\n-     if (!sparse_tensor::getSparseTensorEncoding(op->getResult(0).getType()) &&\n-         !sparse_tensor::getSparseTensorEncoding(op->getOperand(0).getType()))\n-       return Value();\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h b/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h\n---- stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h\n-@@ -153,14 +153,11 @@\n-   using FOp = ::mlir::math::SinOp;\n-   using COp = ::mlir::complex::SinOp;\n- };\n--// FIXME(Jakub)\n--/*\n- template <>\n- struct StablehloToScalarOp<stablehlo::TanOp> {\n-   using FOp = ::mlir::math::TanOp;\n-   using COp = ::mlir::complex::TanOp;\n- };\n--*/\n- template <>\n- struct StablehloToScalarOp<stablehlo::Atan2Op> {\n-   using FOp = ::mlir::math::Atan2Op;\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td b/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td\n---- stablehlo/stablehlo/conversions/linalg/transforms/Passes.td\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/Passes.td\n-@@ -39,7 +39,11 @@\n-                  Option<\"enableSparseOps\", \"enable-sparse-ops\", \"bool\",\n-                         /*default=*/\"false\",\n-                         \"Lower to Sparse Tensor ops (sparse_tensor.concatenate)\"\n--                        \"when possible, instead of linalg.generic\">];\n-+                        \"when possible, instead of linalg.generic\">,\n-+                 Option<\"captureScalarInputs\", \"capture-scalar-inputs\", \"bool\",\n-+                        /*default=*/\"true\",\n-+                        \"Capture scalar inputs in generic ops instead of\"\n-+                        \"passing as tensor-scalar argument.\">];\n- }\n- \n- #endif  // STABLEHLO_TO_LINALG_PASSES\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h b/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h\n---- stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h\n-@@ -26,11 +26,12 @@\n- //===----------------------------------------------------------------------===//\n- \n- /// Populates the patterns that convert from StableHLO to Linalg on tensors.\n--void populateStablehloToLinalgConversionPatterns(MLIRContext *context,\n--                                                 TypeConverter &typeConverter,\n--                                                 RewritePatternSet *patterns,\n-+void populateStablehloToLinalgConversionPatterns(MLIRContext* context,\n-+                                                 TypeConverter& typeConverter,\n-+                                                 RewritePatternSet* patterns,\n-                                                  bool enablePrimitiveOps,\n--                                                 bool enableSparseOps);\n-+                                                 bool enableSparseOps,\n-+                                                 bool captureScalarInputs);\n- \n- //===----------------------------------------------------------------------===//\n- // Fine-grained patterns used by the implementation.\n-@@ -39,8 +40,9 @@\n- /// Populates the patterns that convert from elementwise StableHLO ops to Linalg\n- /// on tensors.\n- void populatePointwiseStablehloToLinalgConversionPatterns(\n--    MLIRContext *context, TypeConverter &typeConverter,\n--    RewritePatternSet *patterns, bool enablePrimitiveOps);\n-+    MLIRContext* context, TypeConverter& typeConverter,\n-+    RewritePatternSet* patterns, bool enablePrimitiveOps,\n-+    bool captureScalarInputs);\n- \n- /// Populates the patterns that convert from convolution StableHLO ops to Linalg\n- /// on tensors.\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n---- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-@@ -2634,7 +2634,8 @@\n- \n-     RewritePatternSet patterns_(context);\n-     populateStablehloToLinalgConversionPatterns(\n--        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps);\n-+        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps,\n-+        captureScalarInputs);\n-     patterns = std::move(patterns_);\n- \n-     return success();\n-@@ -2657,7 +2658,8 @@\n-                                                  TypeConverter& typeConverter,\n-                                                  RewritePatternSet* patterns,\n-                                                  bool enablePrimitiveOps,\n--                                                 bool enableSparseOps) {\n-+                                                 bool enableSparseOps,\n-+                                                 bool captureScalarInputs) {\n-   // clang-format off\n-   patterns->add<ConcatenateConverter>(typeConverter, context,\n-                                       enablePrimitiveOps);\n-@@ -2680,7 +2682,8 @@\n-       >(typeConverter, context);\n- \n-   detail::populatePointwiseStablehloToLinalgConversionPatterns(\n--      context, typeConverter, patterns, enablePrimitiveOps);\n-+      context, typeConverter, patterns, enablePrimitiveOps,\n-+      captureScalarInputs);\n- \n-   if (enableSparseOps) {\n-     patterns->add<SparseConcatenateConverter>(typeConverter, context);\n diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n --- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n +++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n-@@ -145,6 +145,7 @@\n-       ScalarHloToArithmeticPattern<mlir::stablehlo::SineOp>,\n-       ScalarHloToArithmeticPattern<mlir::stablehlo::SqrtOp>,\n-       ScalarHloToArithmeticPattern<mlir::stablehlo::SubtractOp>,\n-+      ScalarHloToArithmeticPattern<mlir::stablehlo::TanOp>,\n-       ScalarHloToArithmeticPattern<mlir::stablehlo::TanhOp>,\n-       ScalarHloToArithmeticPattern<mlir::stablehlo::XorOp>>(typeConverter,\n-                                                             context, filterFn);\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp\n---- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp\n-@@ -23,6 +23,7 @@\n- \n- #include \"llvm/ADT/STLExtras.h\"\n- #include \"llvm/ADT/SmallVector.h\"\n-+#include \"llvm/Support/Debug.h\"\n- #include \"mlir/Dialect/Linalg/IR/Linalg.h\"\n- #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n- #include \"mlir/IR/AffineMap.h\"\n-@@ -43,6 +44,8 @@\n- #include \"stablehlo/conversions/linalg/transforms/Rewriters.h\"\n- #include \"stablehlo/dialect/StablehloOps.h\"\n- \n-+#define DEBUG_TYPE \"stablehlo-conversions\"\n-+\n- namespace mlir::stablehlo {\n- namespace {\n- int64_t getRank(Value v) { return cast<ShapedType>(v.getType()).getRank(); }\n-@@ -142,6 +145,11 @@\n- struct PointwiseToLinalgMapConverter : OpConversionPattern<OpTy> {\n-   using OpConversionPattern<OpTy>::OpConversionPattern;\n-   using OpAdaptor = typename OpTy::Adaptor;\n-+\n-+  PointwiseToLinalgMapConverter(TypeConverter& typeConverter,\n-+                                MLIRContext* context, bool captureScalarInputs)\n-+      : OpConversionPattern<OpTy>(typeConverter, context),\n-+        captureScalarInputs(captureScalarInputs) {}\n- \n-   virtual FailureOr<Operation *> createLinalgOp(\n-       OpTy &op, ConversionPatternRewriter &rewriter,\n-@@ -190,8 +198,11 @@\n-             rewriter, loc, cast<TypedValue<ShapedType>>(input),\n-             cast<ShapedType>(emptyTensor.getType())));\n-         scalarInputs.push_back(nullptr);\n-+      } else if (captureScalarInputs) {\n-+        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));\n-       } else {\n--        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));\n-+        mappedInputs.push_back(input);\n-+        scalarInputs.push_back(nullptr);\n-       }\n-     }\n- \n-@@ -202,6 +213,8 @@\n-     rewriter.replaceOp(op, (*mapOp)->getResults());\n-     return success();\n-   }\n-+\n-+  bool captureScalarInputs;\n- };\n- \n- /// Converts a HLO operation to a linalg.generic op that contains the\n-@@ -211,12 +224,12 @@\n-   using PointwiseToLinalgMapConverter<OpTy>::PointwiseToLinalgMapConverter;\n-   using OpAdaptor = typename OpTy::Adaptor;\n- \n--  FailureOr<Operation *> createLinalgOp(OpTy &op,\n--                                        ConversionPatternRewriter &rewriter,\n--                                        ArrayRef<Value> mappedInputs,\n--                                        ArrayRef<Value> scalarVals,\n--                                        Value emptyTensor,\n--                                        int64_t maxRank) const override {\n-+  FailureOr<Operation*> createLinalgOp(OpTy& op,\n-+                                       ConversionPatternRewriter& rewriter,\n-+                                       ArrayRef<Value> mappedInputs,\n-+                                       ArrayRef<Value> scalarVals,\n-+                                       Value emptyTensor,\n-+                                       int64_t maxRank) const override {\n-     // Create indexing maps.\n-     AffineMap scalarMap = AffineMap::get(maxRank, 0, rewriter.getContext());\n-     AffineMap idMap = rewriter.getMultiDimIdentityMap(maxRank);\n-@@ -225,10 +238,10 @@\n-       maps.push_back(isScalar(v) ? scalarMap : idMap);\n-     maps.push_back(idMap);\n-     bool failed = false;\n--    Operation *linalgOp = rewriter.create<linalg::GenericOp>(\n-+    Operation* linalgOp = rewriter.create<linalg::GenericOp>(\n-         op.getLoc(), emptyTensor.getType(), mappedInputs, emptyTensor, maps,\n-         getNParallelLoopsAttrs(maxRank),\n--        [&](OpBuilder &nestedBuilder, Location /*nested_loc*/,\n-+        [&](OpBuilder& nestedBuilder, Location /*nested_loc*/,\n-             ValueRange args) {\n-           Type innerResultTy = getElementTypeOrSelf(emptyTensor);\n-           auto argvec =\n-@@ -253,8 +266,9 @@\n- \n- namespace detail {\n- void populatePointwiseStablehloToLinalgConversionPatterns(\n--    MLIRContext *context, TypeConverter &typeConverter,\n--    RewritePatternSet *patterns, bool enablePrimitiveOps) {\n-+    MLIRContext* context, TypeConverter& typeConverter,\n-+    RewritePatternSet* patterns, bool enablePrimitiveOps,\n-+    bool captureScalarInputs) {\n-   if (enablePrimitiveOps) {\n-     patterns->add<\n-         PointwiseToLinalgMapConverter<mlir::stablehlo::AbsOp>,\n-@@ -301,12 +315,12 @@\n-         PointwiseToLinalgMapConverter<mlir::stablehlo::SineOp>,\n-         PointwiseToLinalgMapConverter<mlir::stablehlo::SqrtOp>,\n-         PointwiseToLinalgMapConverter<mlir::stablehlo::SubtractOp>,\n-+        PointwiseToLinalgMapConverter<mlir::stablehlo::TanOp>,\n-         PointwiseToLinalgMapConverter<mlir::stablehlo::TanhOp>,\n--        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(typeConverter,\n--                                                               context);\n-+        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(\n-+        typeConverter, context, captureScalarInputs);\n-     return;\n-   }\n--\n-   patterns\n-       ->add<PointwiseToLinalgConverter<mlir::stablehlo::AbsOp>,\n-             PointwiseToLinalgConverter<mlir::stablehlo::AddOp>,\n-@@ -352,9 +366,10 @@\n-             PointwiseToLinalgConverter<mlir::stablehlo::SineOp>,\n-             PointwiseToLinalgConverter<mlir::stablehlo::SqrtOp>,\n-             PointwiseToLinalgConverter<mlir::stablehlo::SubtractOp>,\n-+            PointwiseToLinalgConverter<mlir::stablehlo::TanOp>,\n-             PointwiseToLinalgConverter<mlir::stablehlo::TanhOp>,\n--            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(typeConverter,\n--                                                                context);\n-+            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(\n-+          typeConverter, context, captureScalarInputs);\n- }\n- }  // namespace detail\n- }  // namespace mlir::stablehlo\n-diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp\n---- stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp\n-+++ stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp\n-@@ -40,7 +40,7 @@\n- \n- namespace {\n- \n--Value buildRescaleMultiplier(bool scale32, OpBuilder& builder, Location loc,\n-+Value buildRescaleMultiplier(bool scale32, OpBuilder &builder, Location loc,\n-                              ArrayRef<int32_t> multipliers) {\n-   if (scale32) {\n-     return tosa::getConstTensorInt<int32_t>(builder, loc, multipliers);\n-@@ -51,7 +51,7 @@\n- }\n- \n- // create a tosa rescale op and return its result value\n--Value buildRescale(PatternRewriter& rewriter, Location loc,\n-+Value buildRescale(PatternRewriter &rewriter, Location loc,\n-                    ShapedType outputType, Value inputVal, int32_t multiplier,\n-                    int32_t shift, int64_t inputZp, int64_t outputZp,\n-                    bool doubleRound, bool scale32, bool perChannel) {\n-@@ -85,7 +85,7 @@\n- }\n- \n- // Creates TOSA rescale op with int32 output\n--Value buildRescaleToInt32(PatternRewriter& rewriter, Location loc,\n-+Value buildRescaleToInt32(PatternRewriter &rewriter, Location loc,\n-                           Value inputVal, double inputScale, int64_t inputZp) {\n-   auto inputType = cast<ShapedType>(inputVal.getType());\n-   auto outputType = inputType.clone(rewriter.getI32Type());\n-@@ -103,7 +103,7 @@\n- }\n- \n- // Creates TOSA rescale op with int32 input\n--Value buildRescaleFromInt32(PatternRewriter& rewriter, Location loc,\n-+Value buildRescaleFromInt32(PatternRewriter &rewriter, Location loc,\n-                             ShapedType outputType, Value inputVal,\n-                             double outputScale, int64_t outputZp) {\n-   // Input should be int32 type\n-@@ -124,14 +124,14 @@\n- }\n- \n- using UnaryRescaleScalesFn =\n--    void (*)(const quant::UniformQuantizedType& operandQType,\n--             const quant::UniformQuantizedType& resultQType,\n--             double& operandRescaleScale, double& resultRescaleScale);\n--\n--void GetUnaryRescaleScales(const quant::UniformQuantizedType& operandQType,\n--                           const quant::UniformQuantizedType& resultQType,\n--                           double& operandRescaleScale,\n--                           double& resultRescaleScale) {\n-+    void (*)(const quant::UniformQuantizedType &operandQType,\n-+             const quant::UniformQuantizedType &resultQType,\n-+             double &operandRescaleScale, double &resultRescaleScale);\n-+\n-+void GetUnaryRescaleScales(const quant::UniformQuantizedType &operandQType,\n-+                           const quant::UniformQuantizedType &resultQType,\n-+                           double &operandRescaleScale,\n-+                           double &resultRescaleScale) {\n-   double operandScale = operandQType.getScale();\n-   double resultScale = resultQType.getScale();\n- \n-@@ -145,7 +145,7 @@\n- \n- template <typename StablehloOp>\n- LogicalResult matchAndRewriteUnaryOp(\n--    StablehloOp op, PatternRewriter& rewriter,\n-+    StablehloOp op, PatternRewriter &rewriter,\n-     UnaryRescaleScalesFn rescaleScalesFn = GetUnaryRescaleScales) {\n-   Value operand = op.getOperand();\n-   Value result = op.getResult();\n-@@ -190,21 +190,21 @@\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::AbsOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteUnaryOp(op, rewriter);\n- }\n- \n- using BinaryRescaleScalesFn = void (*)(\n--    const quant::UniformQuantizedType& lhsQType,\n--    const quant::UniformQuantizedType& rhsQType,\n--    const quant::UniformQuantizedType& resultQType, double& lhsRescaleScale,\n--    double& rhsRescaleScale, double& resultRescaleScale);\n--\n--void GetAddSubRescaleScales(const quant::UniformQuantizedType& lhsQType,\n--                            const quant::UniformQuantizedType& rhsQType,\n--                            const quant::UniformQuantizedType& resultQType,\n--                            double& lhsRescaleScale, double& rhsRescaleScale,\n--                            double& resultRescaleScale) {\n-+    const quant::UniformQuantizedType &lhsQType,\n-+    const quant::UniformQuantizedType &rhsQType,\n-+    const quant::UniformQuantizedType &resultQType, double &lhsRescaleScale,\n-+    double &rhsRescaleScale, double &resultRescaleScale);\n-+\n-+void GetAddSubRescaleScales(const quant::UniformQuantizedType &lhsQType,\n-+                            const quant::UniformQuantizedType &rhsQType,\n-+                            const quant::UniformQuantizedType &resultQType,\n-+                            double &lhsRescaleScale, double &rhsRescaleScale,\n-+                            double &resultRescaleScale) {\n-   // 1. Rescale inputs to scale = 2.0 x max(lhs.scale, rhs.scale)\n-   // 2. Extra left shift to input to increase precision\n-   // Where input_shift = 20 if input is 8-bit\n-@@ -230,11 +230,11 @@\n-       maxScale2x / (resultScale * static_cast<double>(1 << inputShift));\n- }\n- \n--void GetMulDivRescaleScales(const quant::UniformQuantizedType& lhsQType,\n--                            const quant::UniformQuantizedType& rhsQType,\n--                            const quant::UniformQuantizedType& resultQType,\n--                            double& lhsRescaleScale, double& rhsRescaleScale,\n--                            double& resultRescaleScale) {\n-+void GetMulDivRescaleScales(const quant::UniformQuantizedType &lhsQType,\n-+                            const quant::UniformQuantizedType &rhsQType,\n-+                            const quant::UniformQuantizedType &resultQType,\n-+                            double &lhsRescaleScale, double &rhsRescaleScale,\n-+                            double &resultRescaleScale) {\n-   double lhsScale = lhsQType.getScale();\n-   double rhsScale = rhsQType.getScale();\n-   double resultScale = resultQType.getScale();\n-@@ -248,11 +248,11 @@\n-   resultRescaleScale = lhsScale * rhsScale / resultScale;\n- }\n- \n--void GetMinMaxRescaleScales(const quant::UniformQuantizedType& lhsQType,\n--                            const quant::UniformQuantizedType& rhsQType,\n--                            const quant::UniformQuantizedType& resultQType,\n--                            double& lhsRescaleScale, double& rhsRescaleScale,\n--                            double& resultRescaleScale) {\n-+void GetMinMaxRescaleScales(const quant::UniformQuantizedType &lhsQType,\n-+                            const quant::UniformQuantizedType &rhsQType,\n-+                            const quant::UniformQuantizedType &resultQType,\n-+                            double &lhsRescaleScale, double &rhsRescaleScale,\n-+                            double &resultRescaleScale) {\n-   // 1. Rescale inputs to scale = max(lhs.scale, rhs.scale)\n-   // 2. Extra left shift to input to increase precision\n-   // Where input_shift = 20 if input is 8-bit\n-@@ -280,7 +280,7 @@\n- }\n- \n- template <typename StablehloOp>\n--LogicalResult matchAndRewriteBinaryOp(StablehloOp op, PatternRewriter& rewriter,\n-+LogicalResult matchAndRewriteBinaryOp(StablehloOp op, PatternRewriter &rewriter,\n-                                       BinaryRescaleScalesFn rescaleScalesFn) {\n-   Value lhs = op.getLhs();\n-   Value rhs = op.getRhs();\n-@@ -339,37 +339,37 @@\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::AddOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteBinaryOp(op, rewriter, GetAddSubRescaleScales);\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::SubtractOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteBinaryOp(op, rewriter, GetAddSubRescaleScales);\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::MulOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteBinaryOp(op, rewriter, GetMulDivRescaleScales);\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::DivOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteBinaryOp(op, rewriter, GetMulDivRescaleScales);\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::MinOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteBinaryOp(op, rewriter, GetMinMaxRescaleScales);\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::MaxOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteBinaryOp(op, rewriter, GetMinMaxRescaleScales);\n- }\n- \n- LogicalResult matchAndRewriteCompareOp(stablehlo::CompareOp op,\n--                                       PatternRewriter& rewriter) {\n-+                                       PatternRewriter &rewriter) {\n-   Value lhs = op.getLhs();\n-   Value rhs = op.getRhs();\n-   Value result = op.getResult();\n-@@ -429,7 +429,7 @@\n- }\n- \n- LogicalResult matchAndRewriteOp(stablehlo::CompareOp op,\n--                                PatternRewriter& rewriter) {\n-+                                PatternRewriter &rewriter) {\n-   return matchAndRewriteCompareOp(op, rewriter);\n- }\n- \n-@@ -438,7 +438,7 @@\n-     : public OpRewritePattern<StablehloOpType> {\n-   using OpRewritePattern<StablehloOpType>::OpRewritePattern;\n-   LogicalResult matchAndRewrite(StablehloOpType op,\n--                                PatternRewriter& rewriter) const override {\n-+                                PatternRewriter &rewriter) const override {\n-     return matchAndRewriteOp(op, rewriter);\n-   }\n- };\n-@@ -446,7 +446,7 @@\n- struct StablehloQuantLegalizeToTosaRescalePass\n-     : impl::StablehloQuantLegalizeToTosaRescalePassBase<\n-           StablehloQuantLegalizeToTosaRescalePass> {\n--  LogicalResult initialize(MLIRContext* ctx) override {\n-+  LogicalResult initialize(MLIRContext *ctx) override {\n-     RewritePatternSet patternList(ctx);\n-     populateStablehloQuantLegalizeToTosaRescalePatterns(&patternList, ctx);\n-     patterns = std::move(patternList);\n-@@ -468,7 +468,7 @@\n- }  // namespace\n- \n- void populateStablehloQuantLegalizeToTosaRescalePatterns(\n--    RewritePatternSet* patterns, MLIRContext* context) {\n-+    RewritePatternSet *patterns, MLIRContext *context) {\n-   // unary ops\n-   patterns->addWithLabel<QuantizedStablehloOpConversion<stablehlo::AbsOp>>(\n-       {\"StablehloQuantAbsOp\"}, context);\n+@@ -33,6 +33,7 @@\n+ \n+ template <typename OpTy>\n+ struct ScalarHloToFuncPatterns final : OpConversionPattern<OpTy> {\n++  // NOLINTNEXTLINE(clang-diagnostic-shadow-field)\n+   ScalarHloToFuncPatterns(TypeConverter& typeConverter, MLIRContext* context,\n+                           PatternBenefit benefit = 1)\n+       : OpConversionPattern<OpTy>(typeConverter, context, benefit) {}\n+@@ -51,6 +52,7 @@\n+ template <typename OpTy>\n+ struct ScalarHloToArithmeticPattern final : OpConversionPattern<OpTy> {\n+   ScalarHloToArithmeticPattern(\n++      // NOLINTNEXTLINE(clang-diagnostic-shadow-field)\n+       TypeConverter& typeConverter, MLIRContext* context,\n+       llvm::function_ref<bool(Operation*)> filterFn = nullptr,\n+       PatternBenefit benefit = 1)\n+diff --ruN a/stablehlo/stablehlo/dialect/Base.td b/stablehlo/stablehlo/dialect/Base.td\n+--- stablehlo/stablehlo/dialect/Base.td\n++++ stablehlo/stablehlo/dialect/Base.td\n+@@ -152,7 +152,7 @@\n+     AnyTypeOf<[HLO_PerAxisQuantizedSignedInt, HLO_PerAxisQuantizedUnsignedInt], \"per-axis integer quantized\">;\n+ \n+ // Token type.\n+-def HLO_Token : Type<CPred<\"::llvm::isa<::mlir::stablehlo::TokenType>($_self)\">, \"token\">;\n++def HLO_Token : Type<CPred<\"::llvm::isa<TokenType>($_self)\">, \"token\">;\n+ \n+ // Any integer tensor types\n+ def HLO_IntTensor : RankedTensorOf<[HLO_Int]>;\n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp\n --- stablehlo/stablehlo/dialect/StablehloOps.cpp\n +++ stablehlo/stablehlo/dialect/StablehloOps.cpp\n@@ -548,97 +40,6 @@ diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/\n  #define GET_OP_CLASSES\n  #include \"stablehlo/dialect/StablehloOps.cpp.inc\"\n  \n-diff --ruN a/stablehlo/stablehlo/integrations/c/VhloDialect.h b/stablehlo/stablehlo/integrations/c/VhloDialect.h\n---- stablehlo/stablehlo/integrations/c/VhloDialect.h\n-+++ stablehlo/stablehlo/integrations/c/VhloDialect.h\n-@@ -13,7 +13,7 @@\n- #ifndef STABLEHLO_INTEGRATIONS_C_VHLO_DIALECT_H\n- #define STABLEHLO_INTEGRATIONS_C_VHLO_DIALECT_H\n- \n--#include \"mlir-c/RegisterEverything.h\"\n-+#include \"mlir-c/IR.h\"\n- \n- #ifdef __cplusplus\n- extern \"C\" {\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp\n-@@ -110,6 +110,43 @@\n-   }\n- }\n- \n-+bool IsBoolean(ElementType elementType) {\n-+  MLIRContext ctx;\n-+  return getElementType(ctx, elementType).isInteger(1);\n-+}\n-+\n-+bool IsComplex(ElementType elementType) {\n-+  MLIRContext ctx;\n-+  auto type = dyn_cast<ComplexType>(getElementType(ctx, elementType));\n-+  return !!type;\n-+}\n-+\n-+bool IsFloat(ElementType elementType) {\n-+  MLIRContext ctx;\n-+  return getElementType(ctx, elementType).isFloat();\n-+}\n-+\n-+bool IsInteger(ElementType elementType, bool includeBool = false) {\n-+  MLIRContext ctx;\n-+  Type type = getElementType(ctx, elementType);\n-+  return type.isInteger() && (includeBool || !IsBoolean(elementType));\n-+}\n-+\n-+bool IsSignedInteger(ElementType elementType) {\n-+  MLIRContext ctx;\n-+  Type type = getElementType(ctx, elementType);\n-+\n-+  // Note that this is not the same as `type.isSignedInteger()`. Signed integers\n-+  // are not used in StableHLO.\n-+  return type.isSignlessInteger() && !IsBoolean(elementType);\n-+}\n-+\n-+bool IsUnsignedInteger(ElementType elementType) {\n-+  MLIRContext ctx;\n-+  return getElementType(ctx, elementType).isUnsignedInteger() &&\n-+         !IsBoolean(elementType);\n-+}\n-+\n- RankedTensorType makeTensorType(MLIRContext& ctx, ArrayRef<int64_t> shape,\n-                                 ElementType elementType) {\n-   return makeTensorType(ctx, shape, getElementType(ctx, elementType));\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h\n---- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h\n-+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h\n-@@ -18,7 +18,6 @@\n- \n- #include <complex>\n- #include <cstdint>\n--#include <source_location>\n- #include <type_traits>\n- #include <vector>\n- \n-@@ -68,6 +67,20 @@\n-   // clang-format on\n- };\n- \n-+bool IsBoolean(ElementType elementType);\n-+\n-+bool IsComplex(ElementType elementType);\n-+\n-+bool IsFloat(ElementType elementType);\n-+\n-+bool IsInteger(ElementType elementType, bool includeBool);\n-+\n-+// In StableHLO, we refer to signed integer as the MLIR's equivalent signless\n-+// integer. StableHLO does not have a notion of signless integers like MLIR.\n-+bool IsSignedInteger(ElementType elementType);\n-+\n-+bool IsUnsignedInteger(ElementType elementType);\n-+\n- Type getElementType(MLIRContext& ctx, ElementType elementType);\n- \n- // Build a ranked tensor type with an element type of ElementType.\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n --- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n +++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n@@ -843,79 +244,6 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n +  return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>\n  }\n  \n- // -----\n-@@ -529,28 +532,15 @@\n- // IotaOp\n- \n- // CHECK-LABEL: func @eval_iota\n--func.func @eval_iota() -> (tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {\n--  // CHECK-NOT: stablehlo.iota\n--  // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<\n--  // CHECK-SAME: {{\\[\\[}}[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],\n--  // CHECK-SAME: {{\\[}}[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],\n--  // CHECK-SAME: {{\\[}}[2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2]]]> : tensor<3x4x5xi32>\n--\n--  // CHECK: [[RESULT1:%.*]] = stablehlo.constant dense<\n--  // CHECK-SAME: {{\\[\\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],\n--  // CHECK-SAME: {{\\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],\n--  // CHECK-SAME: {{\\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]]]> : tensor<3x4x5xi32>\n--\n--  // CHECK: [[RESULT2:%.*]] = stablehlo.constant dense<\n--  // CHECK-SAME: {{\\[\\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],\n--  // CHECK-SAME: {{\\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],\n--  // CHECk-SAME: {{\\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]]> : tensor<3x4x5xi32>\n--\n-+func.func @eval_iota() -> (tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {\n-+  // CHECK:      [[RESULT0:%.*]] = stablehlo.constant dense<0> : tensor<1xi32>\n-+  // CHECK-NEXT: [[RESULT1:%.*]] = stablehlo.iota dim = 1 : tensor<3x4x5xi32>\n-+  // CHECK-NEXT: [[RESULT2:%.*]] = stablehlo.iota dim = 2 : tensor<3x4x5xi32>\n-   // CHECK: return [[RESULT0]], [[RESULT1]], [[RESULT2]]\n--  %0 = stablehlo.iota dim = 0 : tensor<3x4x5xi32>\n-+  %0 = stablehlo.iota dim = 0 : tensor<1xi32>\n-   %1 = stablehlo.iota dim = 1 : tensor<3x4x5xi32>\n-   %2 = stablehlo.iota dim = 2 : tensor<3x4x5xi32>\n--  func.return %0, %1, %2 : tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>\n-+  func.return %0, %1, %2 : tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>\n- }\n- \n- // -----\n-@@ -596,6 +586,37 @@\n-   // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\\[\\[1, 2\\], \\[3, 4\\]\\]}}> : tensor<2x2xi32>\n-   // CHECK-NEXT: return [[CST1]], [[CST2]]\n-   return %0, %1 : tensor<1xi32>, tensor<2x2xi32>\n-+}\n-+\n-+// -----\n-+\n-+////////\n-+// SliceOp / DynamicSliceOp\n-+\n-+// CHECK-LABEL: @slice_fold\n-+func.func @slice_fold(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {\n-+  %c = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5]]> : tensor<6x1xi32>\n-+  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>\n-+  // CHECK: stablehlo.constant dense<2> : tensor<1x1xi32>\n-+  return %0 : tensor<1x1xi32>\n-+}\n-+\n-+// CHECK-LABEL: @slice_fold_splat\n-+func.func @slice_fold_splat(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {\n-+  %c = stablehlo.constant dense<1> : tensor<6x1xi32>\n-+  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>\n-+  // CHECK: stablehlo.constant dense<1> : tensor<1x1xi32>\n-+  return %0 : tensor<1x1xi32>\n-+}\n-+\n-+// CHECK-LABEL: @dynamic_slice_fold\n-+func.func @dynamic_slice_fold(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<1x1xi32> {\n-+  %0 = stablehlo.constant dense<256> : tensor<6x1xi32>\n-+  %1 = \"stablehlo.dynamic_slice\"(%0, %arg0, %arg1) <{slice_sizes = array<i64: 1, 1>}> : (tensor<6x1xi32>, tensor<i32>, tensor<i32>) -> tensor<1x1xi32>\n-+\n-+  // CHECK: %[[RESULT:.*]] = stablehlo.constant dense<256> : tensor<1x1xi32>\n-+  // CHECK: return %[[RESULT]]\n-+  return %1 : tensor<1x1xi32>\n- }\n- \n  // -----\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n@@ -1021,533 +349,6 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b\n    func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {\n      return %arg1 : tensor<?xf32>\n    }\n-diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n---- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n-+++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n-@@ -73,7 +73,7 @@\n- template <typename FromOpTy, typename ToOpTy>\n- struct HloNaryElementwiseAdaptor {\n-   static ToOpTy createOp(FromOpTy fromOp, Type resultType,\n--                         ValueRange broadcastedOperands, OpBuilder& builder) {\n-+                         ValueRange broadcastedOperands, OpBuilder &builder) {\n-     return builder.create<ToOpTy>(fromOp.getLoc(), resultType,\n-                                   broadcastedOperands);\n-   }\n-@@ -118,7 +118,7 @@\n- struct HloCompareAdaptor {\n-   static mlir::stablehlo::CompareOp createOp(\n-       mlir::chlo::BroadcastCompareOp fromOp, Type resultType,\n--      ValueRange broadcastedOperands, OpBuilder& builder) {\n-+      ValueRange broadcastedOperands, OpBuilder &builder) {\n-     auto chloDirection = fromOp.getComparisonDirection();\n-     auto hloDirection = toStableHloComparisonDirection(chloDirection);\n-     if (!hloDirection) return nullptr;\n-@@ -140,9 +140,9 @@\n- // to take a ChloOpTy, NonBroadcastingOpTy, and an Adaptor as templated values.\n- template <template <typename, typename, typename> typename Pattern,\n-           typename... ConstructorArgs>\n--static void populateForBroadcastingBinaryOp(MLIRContext* context,\n--                                            RewritePatternSet* patterns,\n--                                            ConstructorArgs&&... args) {\n-+static void populateForBroadcastingBinaryOp(MLIRContext *context,\n-+                                            RewritePatternSet *patterns,\n-+                                            ConstructorArgs &&...args) {\n- #define POPULATE_BCAST(ChloOp, HloOp)                                          \\\n-   patterns                                                                     \\\n-       ->add<Pattern<ChloOp, HloOp, HloNaryElementwiseAdaptor<ChloOp, HloOp>>>( \\\n-@@ -179,21 +179,21 @@\n-       context, args...);\n- }\n- \n--static Value getConstantLikeMaxFiniteValue(OpBuilder& b, Location loc,\n-+static Value getConstantLikeMaxFiniteValue(OpBuilder &b, Location loc,\n-                                            Value val) {\n-   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));\n-   return getConstantLike(\n-       b, loc, llvm::APFloat::getLargest(ty.getFloatSemantics()), val);\n- }\n- \n--static Value getConstantLikeInfValue(OpBuilder& b, Location loc, Value val,\n-+static Value getConstantLikeInfValue(OpBuilder &b, Location loc, Value val,\n-                                      bool negative) {\n-   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));\n-   return getConstantLike(\n-       b, loc, llvm::APFloat::getInf(ty.getFloatSemantics(), negative), val);\n- }\n- \n--static Value getConstantLikeSmallestNormalizedValue(OpBuilder& b, Location loc,\n-+static Value getConstantLikeSmallestNormalizedValue(OpBuilder &b, Location loc,\n-                                                     Value val) {\n-   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));\n-   return getConstantLike(\n-@@ -239,7 +239,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       ChloOpTy op, typename ChloOpTy::Adaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     // Only rewrite for statically determinable non-broadcasting cases.\n-     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());\n-     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());\n-@@ -329,7 +329,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       ChloOpTy op, typename ChloOpTy::Adaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     // Only support ranked operands.\n-     Value lhs = adaptor.getLhs();\n-     Value rhs = adaptor.getRhs();\n-@@ -413,7 +413,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::ConstantLikeOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     auto resultTy = cast<ShapedType>(op.getType());\n- \n-     // Unranked uses are not supported.\n-@@ -445,7 +445,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::BroadcastSelectOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     // Only support ranked operands.\n-     Value pred = adaptor.getPred();\n-     Value onTrue = adaptor.getOnTrue();\n-@@ -533,7 +533,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::ConstantOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, op.getValue());\n-     return success();\n-   }\n-@@ -541,7 +541,7 @@\n- \n- template <typename FTy>\n- static Value materializeChebyshevPolynomialApproximation(\n--    OpBuilder& rewriter, Location loc, Value x, ArrayRef<FTy> coefficients) {\n-+    OpBuilder &rewriter, Location loc, Value x, ArrayRef<FTy> coefficients) {\n-   Value b0 = getConstantLike(rewriter, loc, 0.0, x);\n-   Value b1 = getConstantLike(rewriter, loc, 0.0, x);\n-   Value b2 = getConstantLike(rewriter, loc, 0.0, x);\n-@@ -561,7 +561,7 @@\n- }\n- \n- template <typename FTy>\n--static Value materializeBesselI1eApproximation(OpBuilder& rewriter,\n-+static Value materializeBesselI1eApproximation(OpBuilder &rewriter,\n-                                                Location loc, Value x,\n-                                                ArrayRef<FTy> kI1eCoeffsA,\n-                                                ArrayRef<FTy> kI1eCoeffsB) {\n-@@ -594,7 +594,7 @@\n-       loc, rewriter.create<mlir::stablehlo::SignOp>(loc, x), select);\n- }\n- \n--Value materializeBesselI1eApproximationF32(OpBuilder& rewriter, Location loc,\n-+Value materializeBesselI1eApproximationF32(OpBuilder &rewriter, Location loc,\n-                                            ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&\n-@@ -620,7 +620,7 @@\n-                                                   kI1eCoeffsB);\n- }\n- \n--static Value materializeBesselI1eApproximationF64(OpBuilder& rewriter,\n-+static Value materializeBesselI1eApproximationF64(OpBuilder &rewriter,\n-                                                   Location loc,\n-                                                   ValueRange args) {\n-   Value x = args.front();\n-@@ -663,10 +663,10 @@\n-                                                    kI1eCoeffsA, kI1eCoeffsB);\n- }\n- \n--static Value materializeWithUpcast(ConversionPatternRewriter& rewriter,\n-+static Value materializeWithUpcast(ConversionPatternRewriter &rewriter,\n-                                    Location loc, ValueRange args,\n-                                    FloatType minPrecisionTy,\n--                                   Value callback(OpBuilder&, Location,\n-+                                   Value callback(OpBuilder &, Location,\n-                                                   ValueRange)) {\n-   Type originalTy = getElementTypeOrSelf(args.front().getType());\n-   auto floatOriginalTy = dyn_cast<FloatType>(originalTy);\n-@@ -699,7 +699,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::BesselI1eOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     Location loc = op.getLoc();\n-     Value x = adaptor.getOperand();\n-     Type ty = cast<ShapedType>(x.getType()).getElementType();\n-@@ -725,7 +725,7 @@\n- };\n- \n- template <typename FTy>\n--static Value materializePolynomialApproximation(OpBuilder& rewriter,\n-+static Value materializePolynomialApproximation(OpBuilder &rewriter,\n-                                                 Location loc, Value x,\n-                                                 ArrayRef<FTy> coefficients) {\n-   if (coefficients.empty()) return getConstantLike(rewriter, loc, 0.0, x);\n-@@ -746,7 +746,7 @@\n- // argument and derive the final approximation for all |x| >= 1.\n- // This implementation is based on Cephes.\n- static Value materializeErfcApproximationF64ForMagnituteGeOne(\n--    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {\n-+    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&\n-          \"expect f64 element type\");\n-@@ -831,7 +831,7 @@\n- // Precondition is |x| <= 1. Use erfc approximation, otherwise.\n- // This implementation is based on Cephes.\n- static Value materializeErfApproximationF64ForMagnituteLeOne(\n--    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {\n-+    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&\n-          \"expect f64 element type\");\n-@@ -856,7 +856,7 @@\n- }\n- \n- // This implementation is based on Cephes.\n--static Value materializeErfApproximationF64(ConversionPatternRewriter& rewriter,\n-+static Value materializeErfApproximationF64(ConversionPatternRewriter &rewriter,\n-                                             Location loc, ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&\n-@@ -884,7 +884,7 @@\n- }\n- \n- static Value materializeErfcApproximationF64(\n--    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {\n-+    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&\n-          \"expect f64 element type\");\n-@@ -916,7 +916,7 @@\n- // argument and derive the final approximation for all |x| >= 1.\n- // This implementation is based on Cephes.\n- static Value materializeErfcApproximationF32ForMagnitudeGeOne(\n--    OpBuilder& rewriter, Location loc, ValueRange args) {\n-+    OpBuilder &rewriter, Location loc, ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&\n-          \"expect f32 element type\");\n-@@ -982,7 +982,7 @@\n- // Precondition is |x| <= 1. Use erfc approximation, otherwise.\n- // This implementation is based on Cephes.\n- static Value materializeErfApproximationF32ForMagnitudeLeOne(\n--    OpBuilder& rewriter, Location loc, ValueRange args) {\n-+    OpBuilder &rewriter, Location loc, ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&\n-          \"expect f32 element type\");\n-@@ -1001,7 +1001,7 @@\n- }\n- \n- // This is the same approximation as used in Eigen.\n--static Value materializeErfApproximationF32(OpBuilder& rewriter, Location loc,\n-+static Value materializeErfApproximationF32(OpBuilder &rewriter, Location loc,\n-                                             ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&\n-@@ -1038,7 +1038,7 @@\n-                                                    erf, ubErf);\n- }\n- \n--static Value materializeErfcApproximationF32(OpBuilder& rewriter, Location loc,\n-+static Value materializeErfcApproximationF32(OpBuilder &rewriter, Location loc,\n-                                              ValueRange args) {\n-   Value x = args.front();\n-   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&\n-@@ -1070,7 +1070,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::ErfOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     Location loc = op.getLoc();\n-     Value x = adaptor.getOperand();\n-     Type ty = cast<ShapedType>(x.getType()).getElementType();\n-@@ -1098,7 +1098,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::ErfcOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     Location loc = op.getLoc();\n-     Value x = adaptor.getOperand();\n-     Type ty = cast<ShapedType>(x.getType()).getElementType();\n-@@ -1121,7 +1121,7 @@\n-   }\n- };\n- \n--static Value erfInv32(OpBuilder& b, Location loc, ValueRange args) {\n-+static Value erfInv32(OpBuilder &b, Location loc, ValueRange args) {\n-   constexpr int kDegree = 9;\n-   constexpr std::array<float, 9> wLessThan5Constants = {\n-       2.81022636e-08f,  3.43273939e-07f, -3.5233877e-06f,\n-@@ -1178,7 +1178,7 @@\n-       result);\n- }\n- \n--static Value erfInv64(ConversionPatternRewriter& b, Location loc,\n-+static Value erfInv64(ConversionPatternRewriter &b, Location loc,\n-                       ValueRange args) {\n-   constexpr std::array<double, 23> wLessThan625Constants = {\n-       -3.6444120640178196996e-21, -1.685059138182016589e-19,\n-@@ -1298,7 +1298,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::ErfInvOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     Location loc = op.getLoc();\n-     if (op.getType().getElementType().isF64()) {\n-       rewriter.replaceOp(op, erfInv64(rewriter, loc, adaptor.getOperands()));\n-@@ -1338,7 +1338,7 @@\n- //   with   t(z) = z + kLanczosGamma + 1/2\n- //          a(z) = kBaseLanczosCoeff\n- //                   + sum(k = 1, n, kLanczosCoefficients[i] / (z + k))\n--Value materializeLgamma(OpBuilder& rewriter, Location loc, ValueRange args) {\n-+Value materializeLgamma(OpBuilder &rewriter, Location loc, ValueRange args) {\n-   // If the input is less than 0.5 use Euler's reflection formula.\n-   //   gamma(x) = pi / (sin(pi * x) * gamma(1 - x))\n-   // Let z be\n-@@ -1485,7 +1485,7 @@\n- // +/-89.4159851, due to rounding error when computing x +/- log(1/2).  The\n- // correct answer of 3.40281961e+38 (0x7f7fffec) is very close to max-float, so\n- // we deem this acceptable.\n--static Value materializeCoshApproximation(OpBuilder& rewriter, Location loc,\n-+static Value materializeCoshApproximation(OpBuilder &rewriter, Location loc,\n-                                           ValueRange operands) {\n-   mlir::chlo::CoshOp::Adaptor transformed(operands);\n-   Value x = transformed.getOperand();\n-@@ -1504,7 +1504,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::CoshOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     rewriter.replaceOp(\n-         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),\n-                                   rewriter.getF32Type(),\n-@@ -1523,7 +1523,7 @@\n- //          a(z) = kBaseLanczosCoeff\n- //                   + sum(k = 1, n, kLanczosCoefficients[i] / (z + k))\n- //          a'(z) = - sum(k = 1, n, kLanczosCoefficients[i] / (z + k) / (z + k))\n--Value materializeDigamma(OpBuilder& rewriter, Location loc, ValueRange args) {\n-+Value materializeDigamma(OpBuilder &rewriter, Location loc, ValueRange args) {\n-   // If the input is less than 0.5 use Euler's reflection formula.\n-   //   digamma(x) = digamma(1 - x) - pi * cot(pi * x)\n-   // Let z be\n-@@ -1630,14 +1630,14 @@\n- \n- namespace {\n- \n--static Value getConstantLikeSmallestFiniteValue(OpBuilder& b, Location loc,\n-+static Value getConstantLikeSmallestFiniteValue(OpBuilder &b, Location loc,\n-                                                 Value val) {\n-   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));\n-   return getConstantLike(\n-       b, loc, llvm::APFloat::getSmallest(ty.getFloatSemantics()), val);\n- }\n- \n--static Value materializeZeta(OpBuilder& rewriter, Location loc,\n-+static Value materializeZeta(OpBuilder &rewriter, Location loc,\n-                              ValueRange args) {\n-   // Implementation ported from:\n-   // https://github.com/openxla/xla/blob/7a067a7b88d2ffb15b1dc5e3c06f701a15f0391d/xla/client/lib/math.cc#L1912-L1917\n-@@ -1790,7 +1790,7 @@\n- \n- }  // namespace\n- \n--Value materializePolygamma(OpBuilder& rewriter, Location loc, ValueRange args) {\n-+Value materializePolygamma(OpBuilder &rewriter, Location loc, ValueRange args) {\n-   mlir::chlo::PolygammaOp::Adaptor transformed(args);\n-   Value n = transformed.getN();\n-   Value x = transformed.getX();\n-@@ -1840,7 +1840,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::LgammaOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     FloatType minPrecisionTy = rewriter.getF32Type();\n-     rewriter.replaceOp(\n-         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),\n-@@ -1854,7 +1854,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::DigammaOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     FloatType minPrecisionTy = rewriter.getF32Type();\n-     rewriter.replaceOp(\n-         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),\n-@@ -1863,7 +1863,7 @@\n-   }\n- };\n- \n--static Value materializeNextAfter(ConversionPatternRewriter& rewriter,\n-+static Value materializeNextAfter(ConversionPatternRewriter &rewriter,\n-                                   Location loc, ValueRange operands) {\n-   mlir::chlo::NextAfterOp::Adaptor transformed(operands);\n-   Value x = transformed.getX();\n-@@ -1957,7 +1957,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::NextAfterOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     rewriter.replaceOp(\n-         op, materializeNextAfter(rewriter, op.getLoc(), adaptor.getOperands()));\n-     return success();\n-@@ -1969,7 +1969,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::PolygammaOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     Location loc = op.getLoc();\n-     FloatType minPrecisionTy = rewriter.getF32Type();\n-     rewriter.replaceOp(\n-@@ -1989,7 +1989,7 @@\n- // +/-89.4159851, due to rounding error when computing x +/- log(1/2).  The\n- // correct answer of 3.40281961e+38 (0x7f7fffec) is very close to max-float, so\n- // we deem this acceptable.\n--static Value materializeSinhApproximationForLargeX(OpBuilder& rewriter,\n-+static Value materializeSinhApproximationForLargeX(OpBuilder &rewriter,\n-                                                    Location loc,\n-                                                    ValueRange operands) {\n-   mlir::chlo::SinhOp::Adaptor transformed(operands);\n-@@ -2007,7 +2007,7 @@\n- // Express `sinh` as\n- //   sinh(x) = (e^x - e^-x) / 2                     if |x| < 1\n- //           = e^(x + log(1/2)) - e^(-x + log(1/2)) otherwise.\n--static Value materializeSinhApproximation(OpBuilder& rewriter, Location loc,\n-+static Value materializeSinhApproximation(OpBuilder &rewriter, Location loc,\n-                                           ValueRange operands) {\n-   Value largeSinhResult =\n-       materializeSinhApproximationForLargeX(rewriter, loc, operands);\n-@@ -2043,7 +2043,7 @@\n- namespace {\n- \n- ArrayAttr convertPrecisionConfig(mlir::ArrayAttr precisionConfig,\n--                                 ConversionPatternRewriter& rewriter) {\n-+                                 ConversionPatternRewriter &rewriter) {\n-   std::vector<Attribute> precisions;\n-   for (Attribute precision : precisionConfig.getValue()) {\n-     switch (dyn_cast<mlir::chlo::PrecisionAttr>(precision).getValue()) {\n-@@ -2077,7 +2077,7 @@\n- // In this implementation, the IR size increases by a factor of g. If this\n- // becomes a problem, we can try adding stablehlo.while to reduce the IR size.\n- LogicalResult handleRaggedDotMode1(mlir::chlo::RaggedDotOp op,\n--                                   ConversionPatternRewriter& rewriter) {\n-+                                   ConversionPatternRewriter &rewriter) {\n-   Value lhs = op.getLhs();\n-   Value rhs = op.getRhs();\n-   chlo::RaggedDotDimensionNumbersAttr raggedDotDimensionNumbers =\n-@@ -2231,7 +2231,7 @@\n- //   group_sizes : [g]\n- //   result : [g, b, m, n]\n- LogicalResult handleRaggedDotMode2(mlir::chlo::RaggedDotOp op,\n--                                   ConversionPatternRewriter& rewriter) {\n-+                                   ConversionPatternRewriter &rewriter) {\n-   return failure();\n- }\n- \n-@@ -2241,7 +2241,7 @@\n- //   group_sizes : [g]\n- //   result : [b, m, n]\n- LogicalResult handleRaggedDotMode3(mlir::chlo::RaggedDotOp op,\n--                                   ConversionPatternRewriter& rewriter) {\n-+                                   ConversionPatternRewriter &rewriter) {\n-   return failure();\n- }\n- \n-@@ -2254,7 +2254,7 @@\n-   // dimension.\n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::RaggedDotOp op, OpAdaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     if (op.getLhs().getType().getRank() < op.getRhs().getType().getRank()) {\n-       return handleRaggedDotMode1(op, rewriter);\n-     } else if (op.getLhs().getType().getRank() <\n-@@ -2271,7 +2271,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::SinhOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     Value x = adaptor.getOperand();\n-     if (isa<ComplexType>(cast<ShapedType>(x.getType()).getElementType())) {\n-       rewriter.replaceOp(op, materializeSinhApproximationForLargeX(\n-@@ -2321,7 +2321,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::TopKOp op, OpAdaptor /*adaptor*/,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     auto operandType = dyn_cast<RankedTensorType>(op.getOperand().getType());\n-     if (!operandType) return failure();\n-     int64_t operandRank = operandType.getRank();\n-@@ -2436,7 +2436,7 @@\n- \n-   LogicalResult matchAndRewrite(\n-       mlir::chlo::ZetaOp op, OpAdaptor adaptor,\n--      ConversionPatternRewriter& rewriter) const override {\n-+      ConversionPatternRewriter &rewriter) const override {\n-     Location loc = op.getLoc();\n-     FloatType minPrecisionTy = rewriter.getF32Type();\n-     rewriter.replaceOp(\n-@@ -2452,7 +2452,7 @@\n- \n- struct ChloLegalizeToStablehloPass final\n-     : impl::ChloLegalizeToStablehloPassBase<ChloLegalizeToStablehloPass> {\n--  LogicalResult initialize(MLIRContext* context) override {\n-+  LogicalResult initialize(MLIRContext *context) override {\n-     target = std::make_shared<ConversionTarget>(*context);\n-     target->addIllegalDialect<chlo::ChloDialect>();\n-     target->addLegalDialect<mlir::stablehlo::StablehloDialect,\n-@@ -2482,8 +2482,8 @@\n- }  // namespace\n- \n- namespace {\n--static void populateChloBroadcastingPatterns(MLIRContext* context,\n--                                             RewritePatternSet* patterns) {\n-+static void populateChloBroadcastingPatterns(MLIRContext *context,\n-+                                             RewritePatternSet *patterns) {\n-   // Instantiate conversion templates for conforming binary elementwise ops\n-   // that do not have different dtypes between operands and results and do\n-   // not have special attributes that need to be preserved.\n-@@ -2496,8 +2496,8 @@\n-   patterns->add<ConvertConstantLikeOp, ConvertSelectOp>(context);\n- }\n- \n--static void populateChloDecompositionPatterns(MLIRContext* context,\n--                                              RewritePatternSet* patterns) {\n-+static void populateChloDecompositionPatterns(MLIRContext *context,\n-+                                              RewritePatternSet *patterns) {\n-   populateWithGenerated(*patterns);\n-   patterns\n-       ->add<ConvertConstantOp, ConvertBesselI1eOp, ConvertCoshOp,\n-@@ -2508,8 +2508,8 @@\n- }\n- }  // namespace\n- \n--void populateChloToStablehloPatterns(MLIRContext* context,\n--                                     RewritePatternSet* patterns) {\n-+void populateChloToStablehloPatterns(MLIRContext *context,\n-+                                     RewritePatternSet *patterns) {\n-   populateChloBroadcastingPatterns(context, patterns);\n-   populateChloDecompositionPatterns(context, patterns);\n- }\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp\n --- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp\n +++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp\n@@ -1560,71 +361,10 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehl\n                                << \"\\n  curr=\" << key.toString()\n                                << \"\\n  prev=\" << prevKey.toString();\n    }\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.td b/stablehlo/stablehlo/transforms/optimization/Passes.td\n---- stablehlo/stablehlo/transforms/optimization/Passes.td\n-+++ stablehlo/stablehlo/transforms/optimization/Passes.td\n-@@ -23,14 +23,14 @@\n-          \"explicit MLIR `MemoryEffects`. Notably, this means `func.call` ops \"\n-          \"will be assumed pure.\">,\n-   Option<\"foldOpElementLimit\", \"fold-op-element-limit\", \"int64_t\",\n--         /*default=*/\"1\",\n-+         /*default=*/\"65536\",\n-          \"Folding an op into a constant can sometimes come at the cost of \"\n-          \"memory overhead. (This occurs if the op's inputs are reused, meaning \"\n-          \"that they can't be deleted after the op is folded to a constant, or \"\n--         \"when folding operations like `iota` whose outputs take up more \"\n-+         \"when folding operations like `concat` whose outputs take up more \"\n-          \"memory than their inputs.) In such cases, this config option sets an \"\n-          \"upper limit on how many elements an op's result may have before the \"\n--         \"op is no longer folded.\">,\n-+         \"op is no longer folded. Splat folds are exempt from this limit.\">,\n-   Option<\"optimizeFloat\", \"optimize-float\", \"bool\", /*default=*/\"true\",\n-          \"Allow float optimizations that, though mathematically equivalent, \"\n-          \"may result in slightly different quantization of floating-point \"\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -74,12 +74,39 @@\n- \n- static constexpr StablehloAggressiveFolderPassOptions kDefaultOptions;\n- \n-+APSInt getAPSInt(Type type, uint64_t value) {\n-+  unsigned numBits;\n-+  bool isUnsigned;\n-+  if (auto integerType = dyn_cast<IntegerType>(type)) {\n-+    numBits = integerType.getWidth();\n-+    // Signless types are treated as signed, per StableHLO convention.\n-+    isUnsigned = integerType.isUnsignedInteger();\n-+  } else {\n-+    llvm::report_fatal_error(\"expected integer type\");\n-+  }\n-+  return APSInt(\n-+      {/*numBits=*/numBits, value, /*isSigned=*/false, /*implicitTrunc=*/true},\n-+      /*isUnsigned=*/isUnsigned);\n-+}\n-+\n- template <typename T>\n- APSInt getAPSInt(unsigned bitWidth, T value, bool isSigned) {\n-   return APSInt({/*numBits=*/bitWidth, static_cast<uint64_t>(value),\n-                  /*isSigned=*/isSigned,\n-                  /*implicitTrunc=*/true},\n-                 /*isUnsigned=*/!isSigned);\n-+}\n-+\n-+APFloat getAPFloat(\n-+    Type type, double value,\n-+    llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {\n-+  auto floatType = dyn_cast<FloatType>(type);\n-+  if (!floatType) llvm::report_fatal_error(\"expected float type\");\n-+\n-+  APFloat result(value);\n-+  bool unusedLosesInfo = false;\n-+  result.convert(floatType.getFloatSemantics(), roundingMode, &unusedLosesInfo);\n-+  return result;\n- }\n- \n- LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,\n-@@ -530,10 +557,15 @@\n+@@ -530,10 +530,15 @@\n    using FoldOpRewritePattern<OpType>::matchAndRewrite;\n    using FoldOpRewritePattern<OpType>::options;\n  \n@@ -1642,7 +382,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n        return success();\n      return rewriter.notifyMatchFailure(op, \"skipping fold of shape op dtype\");\n    }\n-@@ -605,7 +637,8 @@\n+@@ -605,7 +610,8 @@\n                                  PatternRewriter& rewriter) const override {\n      auto resultType = op.getType();\n      if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||\n@@ -1652,7 +392,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n        return failure();\n  \n      SplatElementsAttr cstAttr;\n-@@ -1104,7 +1137,7 @@\n+@@ -1104,7 +1110,7 @@\n          failed(validateShapeFoldDtype(rewriter, op, resultType)))\n        return failure();\n  \n@@ -1661,98 +401,13 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n      if (!matchPattern(op.getOperand(), m_Constant(&attr)))\n        return rewriter.notifyMatchFailure(op, \"expected constant operand\");\n      rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(resultType));\n-@@ -1256,21 +1289,48 @@\n-       return rewriter.notifyMatchFailure(\n-           op, \"expected operand with static ranked tensor type\");\n- \n--    ElementsAttr els;\n-+    DenseElementsAttr els;\n-     if (!matchPattern(operand, m_Constant(&els)))\n-       return rewriter.notifyMatchFailure(\n-           op, \"expected constant integer or float operand\");\n- \n-+    // Short circuit on splat resizes\n-+    if (els.isSplat()) {\n-+      rewriter.replaceOpWithNewOp<ConstantOp>(op, els.resizeSplat(resultType));\n-+      return success();\n-+    }\n-+\n-     DenseElementsAttr resAttr;\n--    if (auto data = els.tryGetValues<APInt>())\n-+    if (auto data = els.tryGetValues<APInt>(); succeeded(data))\n-       resAttr = sliceType(op, *data);\n--    else if (auto data = els.tryGetValues<APFloat>())\n-+    else if (auto data = els.tryGetValues<APFloat>(); succeeded(data))\n-       resAttr = sliceType(op, *data);\n-     else\n-       return rewriter.notifyMatchFailure(op.getLoc(),\n-                                          \"unsupported element type\");\n- \n-     rewriter.replaceOpWithNewOp<ConstantOp>(op, resAttr);\n-+    return success();\n-+  }\n-+};\n-+\n-+// Pattern: dynamic_slice(splat_cst, start, end) -> resized_splat_cst\n-+struct FoldDynamicSliceOpPattern : public FoldOpRewritePattern<DynamicSliceOp> {\n-+  using FoldOpRewritePattern::FoldOpRewritePattern;\n-+\n-+  LogicalResult matchAndRewrite(DynamicSliceOp op,\n-+                                PatternRewriter& rewriter) const override {\n-+    auto resultType = op.getType();\n-+    if (failed(validateStaticShapeResult(rewriter, op, resultType)))\n-+      return failure();\n-+\n-+    SplatElementsAttr inputSplatAttr;\n-+    if (!matchPattern(op.getOperand(), m_Constant(&inputSplatAttr)) ||\n-+        !inputSplatAttr)\n-+      return rewriter.notifyMatchFailure(op, \"Input must be a splat constant.\");\n-+\n-+    rewriter.replaceOpWithNewOp<ConstantOp>(\n-+        op, inputSplatAttr.resizeSplat(resultType));\n-     return success();\n-   }\n- };\n-@@ -1482,6 +1542,14 @@\n-       rewriter.replaceOpWithNewOp<ConstantOp>(\n-           op, DenseIntElementsAttr::get(resultType, values));\n-       return success();\n-+    }\n-+\n-+    // TODO: Support more iota folding, but doing so currently causes OOMs,\n-+    // so this pattern needs to be enabled more carefully.\n-+    if (outputSize != 1) {\n-+      return rewriter.notifyMatchFailure(\n-+          op, \"expected output size to be 1, but got: \" +\n-+                  std::to_string(outputSize));\n-     }\n- \n-     int64_t sequences = 1;\n-@@ -1881,6 +1949,7 @@\n-   patterns->add<FoldConcatenateOpPattern>(context, options, benefit);\n-   patterns->add<FoldConvertOpPattern>(context, options, benefit);\n-   patterns->add<FoldDivOpPattern>(context, options, benefit);\n-+  patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);\n-   patterns->add<FoldGetDimensionSizeOpPattern>(context, options, benefit);\n-   patterns->add<FoldMaxOpPattern>(context, options, benefit);\n-   patterns->add<FoldMinOpPattern>(context, options, benefit);\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n-@@ -331,7 +331,7 @@\n- DenseI64ArrayAttr getInvertedBroadcastDimensions(OpBuilder& b,\n-                                                  ArrayRef<int64_t> dims) {\n-   SmallVector<int64_t> permutation(dims.size());\n--  for (size_t i = 0; i < dims.size(); ++i) {\n-+  for (auto i = 0; i < dims.size(); ++i) {\n-     permutation[dims[i]] = i;\n-   }\n-   return b.getDenseI64ArrayAttr(permutation);\n-@@ -1308,6 +1308,17 @@\n- //////////////////////////////////\n+@@ -1309,6 +1309,17 @@\n  // TransposeOp\n  /////////////////////////////////\n-+\n+ \n +DenseI64ArrayAttr getMergedTransposePermutation(OpBuilder& b,\n +                                                ArrayRef<int64_t> childPerm,\n +                                                ArrayRef<int64_t> parentPerm) {\n@@ -1763,9 +418,10 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n +  }\n +  return b.getDenseI64ArrayAttr(mergedPerm);\n +}\n- \n++\n  // Pattern: transpose(X, [no_mem_layout_change...]) -> reshape(X)\n  struct TransposeIsReshape final : SimplifyOpRewritePattern<TransposeOp> {\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td"
        },
        {
            "sha": "b8b4fbe69122f8596def4f8b2af9e5c7e2eae43c",
            "filename": "third_party/xla/third_party/stablehlo/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96c1b6c0a67c3f32f56a7501a9bbe2b87bd9535c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96c1b6c0a67c3f32f56a7501a9bbe2b87bd9535c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl?ref=96c1b6c0a67c3f32f56a7501a9bbe2b87bd9535c",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n     # LINT.IfChange\n-    STABLEHLO_COMMIT = \"0a4440a5c8de45c4f9649bf3eb4913bf3f97da0d\"\n-    STABLEHLO_SHA256 = \"f1620aafc2b6d730e2ee9c33b35a59a2656a11eed10b1ef8049f175eb4fbdd9c\"\n+    STABLEHLO_COMMIT = \"baaf7475f8925cb0c5f9580408b3c0385f888487\"\n+    STABLEHLO_SHA256 = \"c4b96f94d9d4aaa8b2dc88104579aab662aa33d59b79e77a9b75c8e0af3d9461\"\n     # LINT.ThenChange(Google-internal path)\n \n     tf_http_archive("
        }
    ],
    "stats": {
        "total": 1418,
        "additions": 37,
        "deletions": 1381
    }
}