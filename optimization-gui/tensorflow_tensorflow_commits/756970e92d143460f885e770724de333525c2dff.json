{
    "author": "mooskagh",
    "message": "[XLA] Introduce --xla_dump_emitter_re flag to control which emitter dumps to dump\n\nThis is symmetric to --xla_dump_hlo_pass_re to be able to dump ones without dumping the others.\n\nPiperOrigin-RevId: 836600695",
    "sha": "756970e92d143460f885e770724de333525c2dff",
    "files": [
        {
            "sha": "491bfaef3b17d4d56a7d8df8f5bcfd28a4c3f0b2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=756970e92d143460f885e770724de333525c2dff",
            "patch": "@@ -180,7 +180,7 @@ absl::Status RunPassPipeline(mlir::ModuleOp module, const HloModule& hlo_module,\n                              absl::string_view entry_function_name) {\n   bool should_dump_mlir_passes =\n       DumpingEnabledForHloModule(hlo_module) &&\n-      DumpingEnabledForHloPass(\"mlir-fusion-emitter\",\n+      DumpingEnabledForEmitter(\"mlir-fusion\",\n                                hlo_module.config().debug_options());\n \n   std::string mlir_passes_dump_result;"
        },
        {
            "sha": "063b46efad574ec65ad5b9fe32b28835352e18cf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=756970e92d143460f885e770724de333525c2dff",
            "patch": "@@ -1656,7 +1656,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n \n   const auto debug_options = fusion->GetModule()->config().debug_options();\n \n-  if (DumpingEnabledForHloModule(*hlo_computation->parent())) {\n+  if (DumpingEnabledForHloModule(*hlo_computation->parent()) &&\n+      DumpingEnabledForEmitter(\"triton-fusion\", debug_options)) {\n     auto suffix = absl::StrCat(fusion->name(), \".before_validation.ttir.txt\");\n     DumpToFileInDirOrStdout(\n         *hlo_computation->parent(), \"\", suffix,\n@@ -1679,7 +1680,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n                               ->config()\n                               .debug_options()\n                               .xla_gpu_unsupported_annotate_with_emitter_loc());\n-  if (DumpingEnabledForHloModule(*hlo_computation->parent())) {\n+  if (DumpingEnabledForHloModule(*hlo_computation->parent()) &&\n+      DumpingEnabledForEmitter(\"triton-fusion\", debug_options)) {\n     std::string suffix = absl::StrCat(fusion->name(), \".ttir.txt\");\n     DumpToFileInDirOrStdout(\n         *hlo_computation->parent(), \"\", suffix,\n@@ -1749,8 +1751,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   bool should_dump_mlir_passes =\n       hlo_config.debug_options().xla_enable_dumping() &&\n       DumpingEnabledForHloModule(hlo_module) &&\n-      DumpingEnabledForHloPass(\"triton-fusion-emitter\",\n-                               hlo_config.debug_options());\n+      DumpingEnabledForEmitter(\"triton-fusion\", hlo_config.debug_options());\n \n   mlir::PassManager pm(&mlir_context);\n   pm.enableVerifier(should_verify);\n@@ -1787,7 +1788,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n       }\n     } else {\n       LOG(ERROR)\n-          << \"--xla_dump_hlo_pass_re=triton-fusion-emitter is set, but neither \"\n+          << \"--xla_dump_emitter_re=triton-fusion is set, but neither \"\n           << \"the environment variable TEST_UNDECLARED_OUTPUTS_DIR nor the \"\n           << \"flag --xla_dump_to is set, so the llvm dumps are disabled.\";\n     }"
        },
        {
            "sha": "8d03115831853ca8da2dcc6f60813aa73180b01f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=756970e92d143460f885e770724de333525c2dff",
            "patch": "@@ -384,7 +384,7 @@ ENTRY e {\n   }\n   DebugOptions debug_options = verified_module->config().debug_options();\n   debug_options.set_xla_dump_to(output_directory);\n-  debug_options.set_xla_dump_hlo_pass_re(\"triton-fusion-emitter\");\n+  debug_options.set_xla_dump_emitter_re(\"triton-fusion\");\n   verified_module->mutable_config().set_debug_options(debug_options);\n \n   EXPECT_TRUE(RunAndCompare(std::move(verified_module),"
        },
        {
            "sha": "fd559444de3c9ebe587ec9c4e4f5cdd3c711bafb",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=756970e92d143460f885e770724de333525c2dff",
            "patch": "@@ -1412,6 +1412,13 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       \"If specified, dumps HLO before and after optimization passes which \"\n       \"match this regular expression, in addition to dumping at the very \"\n       \"beginning and end of compilation.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_dump_emitter_re\",\n+      string_setter_for(&DebugOptions::set_xla_dump_emitter_re),\n+      debug_options->xla_dump_emitter_re(),\n+      \"If specified, dumps debug logs (e.g. IR like LLVM or MLIR) before and \"\n+      \"after emitters which match this regular expression, in addition to \"\n+      \"dumping at the very beginning and end of compilation.\"));\n   flag_list->push_back(\n       tsl::Flag(\"xla_dump_include_timestamp\",\n                 bool_setter_for(&DebugOptions::set_xla_dump_include_timestamp),"
        },
        {
            "sha": "5a2f963624881e219a56b7de7e78582c5770dfa1",
            "filename": "third_party/xla/xla/service/dump.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fservice%2Fdump.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fservice%2Fdump.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fdump.cc?ref=756970e92d143460f885e770724de333525c2dff",
            "patch": "@@ -211,6 +211,7 @@ struct CanonicalDebugOptions {\n         return RE2::PartialMatch(module_name, pattern);\n       };\n     } else if (!opts.xla_dump_hlo_pass_re().empty() ||\n+               !opts.xla_dump_emitter_re().empty() ||\n                !opts.xla_dump_to().empty() || output_format_specified) {\n       should_dump_module = [](string_view) { return true; };\n     } else {\n@@ -228,6 +229,15 @@ struct CanonicalDebugOptions {\n       should_dump_pass = [](string_view) { return false; };\n     }\n \n+    if (!opts.xla_dump_emitter_re().empty()) {\n+      std::string pattern = opts.xla_dump_emitter_re();\n+      should_dump_emitter = [pattern](string_view emitter_name) {\n+        return RE2::PartialMatch(emitter_name, pattern);\n+      };\n+    } else {\n+      should_dump_emitter = [](string_view) { return false; };\n+    }\n+\n     // Initialize should_dump_pipeline. If the option was not specified, dump\n     // all pipelines. Otherwise dump only those pipelines that user asked for\n     // explicitly.\n@@ -252,6 +262,7 @@ struct CanonicalDebugOptions {\n                       \"is not set, so cannot dump anywhere.\";\n         should_dump_module = [](string_view) { return false; };\n         should_dump_pass = [](string_view) { return false; };\n+        should_dump_emitter = [](string_view) { return false; };\n         should_dump_pipeline = [](string_view) { return false; };\n       }\n     }\n@@ -270,6 +281,7 @@ struct CanonicalDebugOptions {\n   std::string dump_to;\n   std::function<bool(string_view module_name)> should_dump_module;\n   std::function<bool(string_view pass_name)> should_dump_pass;\n+  std::function<bool(string_view emitter_name)> should_dump_emitter;\n   std::function<bool(string_view pipeline_name)> should_dump_pipeline;\n \n   // dump_ir isn't present here because this file is mostly concerned with\n@@ -1016,6 +1028,11 @@ bool DumpingEnabledForHloPass(string_view hlo_pass_name,\n   return CanonicalDebugOptions(opts).should_dump_pass(hlo_pass_name);\n }\n \n+bool DumpingEnabledForEmitter(string_view emitter_name,\n+                              const DebugOptions& opts) {\n+  return CanonicalDebugOptions(opts).should_dump_emitter(emitter_name);\n+}\n+\n bool DumpingToStdout(const DebugOptions& opts) {\n   return CanonicalDebugOptions(opts).dumping_to_stdout();\n }"
        },
        {
            "sha": "76fe27c938f5f26900bc859e264dcec91b669cf8",
            "filename": "third_party/xla/xla/service/dump.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fservice%2Fdump.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fservice%2Fdump.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fdump.h?ref=756970e92d143460f885e770724de333525c2dff",
            "patch": "@@ -190,6 +190,10 @@ bool DumpingEnabledForHloModule(absl::string_view hlo_module_name,\n bool DumpingEnabledForHloPass(absl::string_view hlo_pass_name,\n                               const DebugOptions& opts);\n \n+// Returns true if we should dump data for an emitter.\n+bool DumpingEnabledForEmitter(absl::string_view emitter_name,\n+                              const DebugOptions& opts);\n+\n inline bool DumpingEnabledForHloModule(const HloModule& module) {\n   return DumpingEnabledForHloModule(module.name(),\n                                     module.config().debug_options());"
        },
        {
            "sha": "23e19c4c711ad7d81a8f2e4e1425439c2e39c3a3",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/756970e92d143460f885e770724de333525c2dff/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=756970e92d143460f885e770724de333525c2dff",
            "patch": "@@ -1116,6 +1116,10 @@ message DebugOptions {\n   // match this regular expression.  Set to .* to dump before/after all passes.\n   optional string xla_dump_hlo_pass_re = 111;\n \n+  // If specified, dumps debug logs (e.g. IR like LLVM or MLIR) before and\n+  // after emitters that match this regular expression.\n+  optional string xla_dump_emitter_re = 433;\n+\n   // Specifies the format that HLO is dumped in.  Multiple of these may be\n   // specified.\n   optional bool xla_dump_hlo_as_text = 112;\n@@ -1312,7 +1316,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 433\n+  // Next id: 434\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 49,
        "additions": 41,
        "deletions": 8
    }
}