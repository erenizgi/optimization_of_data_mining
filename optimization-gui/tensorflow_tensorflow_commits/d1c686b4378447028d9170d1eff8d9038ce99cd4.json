{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Move buffer registration register to the NcclCommunicator.\n\nInstead of using a static memory to memorize registered buffers it's better to keep this logic withing a NCCL communicator.\nThis change also removes unnecessary memory registrations in operations which don't use NCCL (like ragged-all-to-all) and adds a device number to a few logging statements.\n\nPiperOrigin-RevId: 802511708",
    "sha": "d1c686b4378447028d9170d1eff8d9038ce99cd4",
    "files": [
        {
            "sha": "769319c3b490667d77fea0df49ec377cd7fc754a",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -351,6 +351,7 @@ cc_library(\n         \"//xla/service:global_device_id\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n         \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:gpu_stream\",\n@@ -360,7 +361,9 @@ cc_library(\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/debugging:leak_check\",\n         \"@com_google_absl//absl/functional:any_invocable\","
        },
        {
            "sha": "492b2598ac92c2f897e80b1bcacbe72ddc5453c4",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 90,
            "deletions": 47,
            "changes": 137,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n+#include <tuple>\n #include <utility>\n #include <vector>\n \n@@ -31,6 +32,7 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n@@ -181,11 +183,12 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n  public:\n   NcclRegisteredBufferHandle(NcclCommunicator& comm, void* handle,\n                              tsl::AsyncValue::Executor* executor,\n-                             bool symmetric_handle)\n+                             bool symmetric_handle, int device_ordinal)\n       : comm_(comm),\n         handle_(handle),\n         executor_(),\n-        symmetric_handle_(symmetric_handle) {}\n+        symmetric_handle_(symmetric_handle),\n+        device_ordinal_(device_ordinal) {}\n \n   ~NcclRegisteredBufferHandle() override {\n     if (auto status = Unregister(); !status.ok()) {\n@@ -195,13 +198,14 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n \n   absl::Status Unregister() final {\n     VLOG(3) << absl::StreamFormat(\n-        \"Deregister buffer for NCCL communicator; handle=%p; comm=%p\", handle_,\n-        comm_.comm_);\n+        \"[%d] Deregister buffer for NCCL communicator; handle=%p; comm=%p\",\n+        device_ordinal_, handle_, comm_.comm_);\n     if (!symmetric_handle_) {\n #if (NCCL_VERSION_CODE >= 21901)\n       auto f = [this]() -> absl::Status {\n         if (comm_.canceling_.load()) {\n-          return FailedPrecondition(\"NcclCommunicator aborted\");\n+          return FailedPrecondition(\"[%d] NcclCommunicator aborted\",\n+                                    device_ordinal_);\n         }\n         XLA_NCCL_RETURN_IF_ERROR(ncclCommDeregister(comm_.comm(), handle_));\n         return comm_.PollUntilDone();\n@@ -211,17 +215,20 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n       }\n       return BlockAndGet(tsl::MakeAsyncValueRef(*executor_, f));\n #else\n-      return Unimplemented(\"NCCL version does not support ncclCommDeregister\");\n+      return Unimplemented(\n+          \"[%d] NCCL version does not support ncclCommDeregister\",\n+          device_ordinal_);\n #endif  // NCCL_VERSION_CODE >= 21901\n     } else {\n       VLOG(3) << absl::StreamFormat(\n-          \"Deregister symmetric buffer for NCCL communicator; handle=%p; \"\n+          \"[%d] Deregister symmetric buffer for NCCL communicator; handle=%p; \"\n           \"comm=%p\",\n-          handle_, comm_.comm_);\n+          device_ordinal_, handle_, comm_.comm_);\n #if (NCCL_VERSION_CODE >= 22700)\n       auto f = [this]() -> absl::Status {\n         if (comm_.canceling_.load()) {\n-          return FailedPrecondition(\"NcclCommunicator aborted\");\n+          return FailedPrecondition(\"[%d] NcclCommunicator aborted\",\n+                                    device_ordinal_);\n         }\n         XLA_NCCL_RETURN_IF_ERROR(\n             ncclCommWindowDeregister(comm_.comm(), *(ncclWindow_t*)(handle_)));\n@@ -233,7 +240,8 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n       return BlockAndGet(tsl::MakeAsyncValueRef(*executor_, f));\n #else\n       return Unimplemented(\n-          \"NCCL version does not support ncclCommWindowDeregister\");\n+          \"[%d] NCCL version does not support ncclCommWindowDeregister\",\n+          device_ordinal_);\n #endif  // NCCL_VERSION_CODE >= 22700\n     }\n   }\n@@ -243,6 +251,7 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n   void* handle_;\n   tsl::AsyncValue::Executor* executor_;\n   bool symmetric_handle_;\n+  int device_ordinal_;\n };\n \n //==-----------------------------------------------------------------------===//\n@@ -347,24 +356,53 @@ absl::StatusOr<size_t> NcclCommunicator::NumRanks() const {\n   }));\n }\n \n-absl::StatusOr<std::unique_ptr<Communicator::RegisteredBufferHandle>>\n-NcclCommunicator::RegisterBuffer(stream_executor::DeviceMemoryBase buffer) {\n-  return RegisterBuffer(buffer, /*use_symmetric_buffer=*/false);\n+absl::Status NcclCommunicator::RegisterBufferOnce(\n+    se::DeviceMemoryBase buffer, se::DeviceMemoryBase buffer_range,\n+    int device_ordinal, bool use_symmetric_buffer) {\n+  bool need_reg = false;\n+  {\n+    absl::MutexLock lock(&registered_buffers_.mu);\n+    if (!registered_buffers_.records_to_handles.contains(\n+            {buffer.size(), buffer.opaque()})) {\n+      need_reg = true;\n+    } else {\n+      VLOG(5) << \"[\" << device_ordinal << \"] Buffer: \" << buffer.opaque()\n+              << \" with size: \" << buffer.size()\n+              << \" with range: \" << buffer_range.opaque()\n+              << \" is already registered.\";\n+    }\n+  }\n+  if (need_reg) {\n+    VLOG(5) << \"[\" << device_ordinal << \"] Registering \" << buffer.opaque()\n+            << \" with size: \" << buffer.size()\n+            << \" and with range: \" << buffer_range.opaque()\n+            << \", is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n+    // Symmetric buffer registration is a collective operation,\n+    // we need to do that before locking on a global.\n+    TF_ASSIGN_OR_RETURN(auto handle, RegisterBuffer(buffer, device_ordinal,\n+                                                    use_symmetric_buffer));\n+    absl::MutexLock lock(&registered_buffers_.mu);\n+    registered_buffers_\n+        .records_to_handles[std::make_tuple(buffer.size(), buffer.opaque())] =\n+        std::move(handle);\n+  }\n+  return absl::OkStatus();\n }\n \n absl::StatusOr<std::unique_ptr<Communicator::RegisteredBufferHandle>>\n NcclCommunicator::RegisterBuffer(stream_executor::DeviceMemoryBase buffer,\n+                                 int device_ordinal,\n                                  bool use_symmetric_buffer) {\n #if (NCCL_VERSION_CODE >= 21901)\n   using Handle = std::unique_ptr<Communicator::RegisteredBufferHandle>;\n \n   if (!use_symmetric_buffer) {\n-    return BlockAndGet(\n-        Execute<Handle>([&buffer, this]() -> absl::StatusOr<Handle> {\n+    return BlockAndGet(Execute<Handle>(\n+        [&buffer, device_ordinal, this]() -> absl::StatusOr<Handle> {\n           VLOG(3) << absl::StreamFormat(\n-              \"Register buffer for NCCL communicator; buffer=%p; size=%d; \"\n+              \"[%d] Register buffer for NCCL communicator; buffer=%p; size=%d; \"\n               \"comm=%p\",\n-              buffer.opaque(), buffer.size(), comm_);\n+              device_ordinal, buffer.opaque(), buffer.size(), comm_);\n           if (canceling_.load()) {\n             return absl::FailedPreconditionError(\"NcclCommunicator aborted\");\n           }\n@@ -375,34 +413,39 @@ NcclCommunicator::RegisterBuffer(stream_executor::DeviceMemoryBase buffer,\n             TF_RETURN_IF_ERROR(PollUntilDone());\n           }\n           return std::make_unique<NcclRegisteredBufferHandle>(\n-              *this, handle, executor_.get(), /*symmetric_buffer= */ false);\n+              *this, handle, executor_.get(), /*symmetric_buffer= */ false,\n+              device_ordinal);\n         }));\n #else\n-  return Unimplemented(\"NCCL version does not support ncclCommRegister\");\n+  return Unimplemented(\"[%d] NCCL version does not support ncclCommRegister\",\n+                       device_ordinal);\n #endif  // NCCL_VERSION_CODE >= 21901\n   } else {\n #if (NCCL_VERSION_CODE >= 22700)\n-    return BlockAndGet(\n-        Execute<Handle>([&buffer, this]() -> absl::StatusOr<Handle> {\n-          VLOG(3) << absl::StreamFormat(\n-              \"Register symmetric buffer for NCCL communicator; buffer=%p; \"\n-              \"size=%d; \"\n-              \"comm=%p\",\n-              buffer.opaque(), buffer.size(), comm_);\n-          void* handle = nullptr;\n-          XLA_NCCL_RETURN_IF_ERROR(ncclGroupStart());\n-          XLA_NCCL_RETURN_IF_ERROR(ncclCommWindowRegister(\n-              comm_, buffer.opaque(), buffer.size(), (ncclWindow_t*)&handle,\n-              NCCL_WIN_COLL_SYMMETRIC));\n-          XLA_NCCL_RETURN_IF_ERROR(ncclGroupEnd());\n-          if (group_nesting_level_ == 0) {\n-            TF_RETURN_IF_ERROR(PollUntilDone());\n-          }\n-          return std::make_unique<NcclRegisteredBufferHandle>(\n-              *this, handle, executor_.get(), /*symmetric_buffer= */ true);\n-        }));\n+    return BlockAndGet(Execute<Handle>([&buffer, device_ordinal,\n+                                        this]() -> absl::StatusOr<Handle> {\n+      VLOG(3) << absl::StreamFormat(\n+          \"[%d] Register symmetric buffer for NCCL communicator; buffer=%p; \"\n+          \"size=%d; \"\n+          \"comm=%p\",\n+          device_ordinal, buffer.opaque(), buffer.size(), comm_);\n+      void* handle = nullptr;\n+      XLA_NCCL_RETURN_IF_ERROR(ncclGroupStart());\n+      XLA_NCCL_RETURN_IF_ERROR(ncclCommWindowRegister(\n+          comm_, buffer.opaque(), buffer.size(), (ncclWindow_t*)&handle,\n+          NCCL_WIN_COLL_SYMMETRIC));\n+      XLA_NCCL_RETURN_IF_ERROR(ncclGroupEnd());\n+      if (group_nesting_level_ == 0) {\n+        TF_RETURN_IF_ERROR(PollUntilDone());\n+      }\n+      return std::make_unique<NcclRegisteredBufferHandle>(\n+          *this, handle, executor_.get(), /*symmetric_buffer= */ true,\n+          device_ordinal);\n+    }));\n #else\n-  return Unimplemented(\"NCCL version does not support ncclCommWindowRegister\");\n+  return Unimplemented(\n+      \"[%d] NCCL version does not support ncclCommWindowRegister\",\n+      device_ordinal);\n #endif  // NCCL_VERSION_CODE >= 22700\n   }\n }\n@@ -529,7 +572,7 @@ absl::Status NcclCommunicator::LaunchAllReduce(\n   se::Stream* stream = ToStream(executor);\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL AllReduce operation on device #%d; send_buffer=%p; \"\n+      \"[%d] Launch NCCL AllReduce operation; send_buffer=%p; \"\n       \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%s; comm=%p; \"\n       \"stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n@@ -559,7 +602,7 @@ absl::Status NcclCommunicator::LaunchBroadcast(se::DeviceMemoryBase send_buffer,\n   se::Stream* stream = ToStream(executor);\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL Broadcast operation on device #%d; send_buffer=%p; \"\n+      \"[%d] Launch NCCL Broadcast operation; send_buffer=%p; \"\n       \"recv_buffer=%p; dtype=%s; count=%d; root=%d; comm=%p; \"\n       \"stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n@@ -587,7 +630,7 @@ absl::Status NcclCommunicator::LaunchReduceScatter(\n   se::Stream* stream = ToStream(executor);\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL ReduceScatter operation on device #%d; send_buffer=%p; \"\n+      \"[%d] Launch NCCL ReduceScatter operation; send_buffer=%p; \"\n       \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%s; comm=%p; \"\n       \"stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n@@ -617,7 +660,7 @@ absl::Status NcclCommunicator::LaunchAllGather(se::DeviceMemoryBase send_buffer,\n   se::Stream* stream = ToStream(executor);\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL AllGather operation on device #%d; send_buffer=%p; \"\n+      \"[%d] Launch NCCL AllGather operation; send_buffer=%p; \"\n       \"recv_buffer=%p; dtype=%s; count=%d; comm=%p; stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n       recv_buffer.opaque(), primitive_util::LowercasePrimitiveTypeName(dtype),\n@@ -648,7 +691,7 @@ absl::Status NcclCommunicator::LaunchAllToAll(\n   };\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL AllToAll operation on device #%d; send_buffers=[%s]; \"\n+      \"[%d] Launch NCCL AllToAll operation; send_buffers=[%s]; \"\n       \"recv_buffers=[%s]; dtype=%s; count=%d; comm=%p; stream=%p\",\n       stream->parent()->device_ordinal(),\n       absl::StrJoin(send_buffers, \", \", buffer_formatter),\n@@ -703,7 +746,7 @@ absl::Status NcclCommunicator::LaunchCollectivePermute(\n   };\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL CollectivePermute operation on device #%d; send_buffer=%p; \"\n+      \"[%d] Launch NCCL CollectivePermute operation; send_buffer=%p; \"\n       \"recv_buffer=%p; dtype=%s; source_rank=%s; target_ranks=[%s]; count=%d; \"\n       \"comm=%p; stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n@@ -747,7 +790,7 @@ absl::Status NcclCommunicator::LaunchSend(se::DeviceMemoryBase send_buffer,\n   se::Stream* stream = ToStream(executor);\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL Send operation on device #%d; send_buffer=%p; dtype=%s; \"\n+      \"[%d] Launch NCCL Send operation; send_buffer=%p; dtype=%s; \"\n       \"count=%d; peer=%d; comm=%p; stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n       primitive_util::LowercasePrimitiveTypeName(dtype), count, peer.value(),\n@@ -774,7 +817,7 @@ absl::Status NcclCommunicator::LaunchRecv(se::DeviceMemoryBase recv_buffer,\n   se::Stream* stream = ToStream(executor);\n \n   VLOG(3) << absl::StreamFormat(\n-      \"Launch NCCL Recv operation on device #%d; recv_buffer=%p; dtype=%s; \"\n+      \"[%d] Launch NCCL Recv operation; recv_buffer=%p; dtype=%s; \"\n       \"count=%d; peer=%d; comm=%p; stream=%p\",\n       stream->parent()->device_ordinal(), recv_buffer.opaque(),\n       primitive_util::LowercasePrimitiveTypeName(dtype), count, peer.value(),"
        },
        {
            "sha": "90c54b0a5055039a100190444fb24666255c1fb4",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.h",
            "status": "modified",
            "additions": 26,
            "deletions": 5,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -21,13 +21,17 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n+#include <tuple>\n #include <utility>\n \n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n #include \"xla/core/collectives/communicator.h\"\n@@ -79,11 +83,13 @@ class NcclCommunicator : public GpuCommunicator {\n   absl::Status HealthCheck() const final;\n   absl::StatusOr<size_t> NumRanks() const final;\n \n-  absl::StatusOr<std::unique_ptr<RegisteredBufferHandle>> RegisterBuffer(\n-      se::DeviceMemoryBase buffer) final;\n-\n-  absl::StatusOr<std::unique_ptr<RegisteredBufferHandle>> RegisterBuffer(\n-      se::DeviceMemoryBase buffer, bool use_symmetric_buffer) final;\n+  // Since each XLA buffer is a slice into a larger BFCAllocator chunk, first\n+  // get the base address of buffer. We will use the base address to keep track\n+  // of which chunks we have registered.\n+  absl::Status RegisterBufferOnce(se::DeviceMemoryBase buffer,\n+                                  se::DeviceMemoryBase buffer_range,\n+                                  int device_ordinal,\n+                                  bool use_symmetric_buffer) final;\n \n   tsl::AsyncValueRef<Communicator::Event> GroupExecute(\n       absl::AnyInvocable<absl::Status(GpuCommunicator*)> f) final;\n@@ -134,6 +140,10 @@ class NcclCommunicator : public GpuCommunicator {\n   ncclComm_t comm() const { return comm_; }\n \n  private:\n+  absl::StatusOr<std::unique_ptr<RegisteredBufferHandle>> RegisterBuffer(\n+      se::DeviceMemoryBase buffer, int device_ordinal,\n+      bool use_symmetric_buffer);\n+\n   class NcclRegisteredBufferHandle;\n \n   explicit NcclCommunicator(ncclComm_t comm,\n@@ -227,6 +237,17 @@ class NcclCommunicator : public GpuCommunicator {\n \n   // Nesting level of current NCCL group\n   int group_nesting_level_ = 0;\n+\n+  // Keep track of which communicators we have registered for already.\n+  // Each ncclMemAlloc'd buffer needs to be registered once per comm.\n+  struct RegisteredBuffers {\n+    absl::Mutex mu;\n+    // Pointer to the registered buffer handle.\n+    absl::flat_hash_map<std::tuple<size_t, void*>,\n+                        std::unique_ptr<Communicator::RegisteredBufferHandle>>\n+        records_to_handles ABSL_GUARDED_BY(mu);\n+  };\n+  RegisteredBuffers registered_buffers_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "01f90a51f78ccae48e4b0a9c0f9024e32cfd6d47",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -152,7 +152,7 @@ TEST(NcclCommunicator, OperationsFailAfterAbort) {\n     ASSERT_THAT((*comm)->Abort(), absl_testing::IsOk());\n     AssertAborted((*comm)->HealthCheck());\n     AssertAborted((*comm)->NumRanks().status());\n-    AssertAborted((*comm)->RegisterBuffer(buf).status());\n+    AssertAborted((*comm)->RegisterBufferOnce(buf, buf, 0, false));\n     AssertEventAborted(\n         (*comm)->AllReduce(buf, buf, dtype, count, rk, executor));\n     AssertEventAborted("
        },
        {
            "sha": "a26ece5e80a68384e92e3a8c702141eb574ff7ad",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_gather_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -109,8 +109,7 @@ absl::Status RunAllGather(std::vector<DeviceBufferPair>& buffers,\n                           se::Stream& stream, Communicator* comm,\n                           bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Performing all-gather from device ordinal: \" << device_ordinal;\n+  VLOG(3) << \"[\" << device_ordinal << \"] Performing all-gather\";\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm,\n                                           use_symmetric_buffer));\n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);"
        },
        {
            "sha": "07822b776729b8a82defa712d29b7b1838db5be5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -87,8 +87,7 @@ absl::Status RunAllReduce(ReductionKind reduction_kind,\n                           se::Stream& stream, Communicator* comm,\n                           bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Performing all-reduce from device ordinal: \" << device_ordinal;\n+  VLOG(3) << \"[\" << device_ordinal << \"] Performing all-reduce\";\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm,\n                                           use_symmetric_buffer));\n \n@@ -105,8 +104,7 @@ absl::Status RunAllReduce(ReductionKind reduction_kind,\n         return absl::OkStatus();\n       });\n   tsl::BlockUntilReady(event);\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Done performing all-reduce for ordinal: \" << device_ordinal;\n+  VLOG(3) << \"[\" << device_ordinal << \"] Done performing all-reduce\";\n   if (event.IsError()) {\n     return event.GetError();\n   }\n@@ -237,8 +235,7 @@ absl::Status RunReduceScatter(ReductionKind reduction_kind,\n                               se::Stream& stream, Communicator* comm,\n                               bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"Performing reduce-scatter from device ordinal: \"\n-          << device_ordinal;\n+  VLOG(3) << \"[\" << device_ordinal << \"] Performing reduce-scatter\";\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm,\n                                           use_symmetric_buffer));\n \n@@ -263,7 +260,7 @@ absl::Status RunReduceScatter(ReductionKind reduction_kind,\n         return absl::OkStatus();\n       });\n   tsl::BlockUntilReady(event);\n-  VLOG(3) << \"Done performing reduce-scatter for ordinal: \" << device_ordinal;\n+  VLOG(3) << \"[\" << device_ordinal << \"] Done performing reduce-scatter\";\n   if (event.IsError()) {\n     return event.GetError();\n   }"
        },
        {
            "sha": "e971ceee922c7e9f97120d893717b5f55bed9cf7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -382,8 +382,6 @@ absl::Status RunCollectivePermute(\n   int device_ordinal = stream.parent()->device_ordinal();\n   VLOG(3) << \"Performing collective permute from device ordinal: \"\n           << device_ordinal << \" current_id \" << current_id;\n-  TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm,\n-                                          use_symmetric_buffer));\n \n   std::optional<int64_t> source_id = source_target.source;\n   std::optional<int64_t> target_id = source_target.target;\n@@ -424,6 +422,8 @@ absl::Status RunCollectivePermute(\n         }\n       }\n     } else {\n+      TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm,\n+                                              use_symmetric_buffer));\n       auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n       tsl::AsyncValueRef<Communicator::Event> event = gpu_comm->GroupExecute(\n           [source_rank, &buffers, &src_addrs, &dest_addrs, &target_ranks,"
        },
        {
            "sha": "bb5bba58957fcb9eaf3e65ded0134cf70b1aa6bc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 57,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -351,71 +351,24 @@ absl::StatusOr<std::vector<DeviceBufferPair>> ConvertToDeviceBuffers(\n   return device_buffers;\n }\n \n-absl::Status RegisterBufferOnce(se::StreamExecutor* executor,\n-                                Communicator* comm, se::DeviceMemoryBase buffer,\n-                                bool use_symmetric_buffer) {\n-  // Keep track of which communicators we have registered for already.\n-  // Each ncclMemAlloc'd buffer needs to be registered once per comm.\n-  struct RegisteredBuffers {\n-    absl::Mutex mu;\n-    // Device ordinal, communicator, and base pointer address.\n-    absl::flat_hash_set<std::tuple<int, uint64_t, Communicator*, void*>> records\n-        ABSL_GUARDED_BY(mu);\n-    // Buffers could be deregistered with ncclCommDeregister.\n-    std::vector<std::unique_ptr<Communicator::RegisteredBufferHandle>> handles\n-        ABSL_GUARDED_BY(mu);\n-  };\n-  static auto& all_registered = *new RegisteredBuffers;\n-\n-  // Since each XLA buffer is a slice into a larger BFCAllocator chunk, first\n-  // get the base address of buffer. We will use the base address to keep track\n-  // of which chunks we have registered.\n-  TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase base_buffer,\n-                      executor->GetMemoryRange(buffer));\n-  bool need_reg = false;\n-  {\n-    absl::MutexLock lock(&all_registered.mu);\n-    if (!all_registered.records.contains({executor->device_ordinal(),\n-                                          buffer.size(), comm,\n-                                          buffer.opaque()})) {\n-      need_reg = true;\n-    } else {\n-      VLOG(5) << \"[\" << executor->device_ordinal()\n-              << \"] Buffer: \" << buffer.opaque()\n-              << \" with size: \" << buffer.size()\n-              << \" and base pointer: \" << base_buffer.opaque()\n-              << \" is already registered.\";\n-    }\n-  }\n-  if (need_reg) {\n-    VLOG(5) << \"[\" << executor->device_ordinal() << \"] Registering \"\n-            << buffer.opaque() << \" with size: \" << buffer.size()\n-            << \" and base pointer: \" << base_buffer.opaque()\n-            << \", is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n-    // Symmetric buffer registration is a collective operation,\n-    // we need to do that before locking on a global.\n-    TF_ASSIGN_OR_RETURN(auto handle,\n-                        comm->RegisterBuffer(buffer, use_symmetric_buffer));\n-    absl::MutexLock lock(&all_registered.mu);\n-    all_registered.handles.push_back(std::move(handle));\n-    all_registered.records.insert(\n-        {executor->device_ordinal(), buffer.size(), comm, buffer.opaque()});\n-  }\n-  return absl::OkStatus();\n-}\n-\n absl::Status MaybeRegisterBuffers(se::StreamExecutor* executor,\n                                   const std::vector<DeviceBufferPair>& buffers,\n                                   Communicator* comm,\n                                   bool use_symmetric_buffer) {\n   for (int i = 0; i < buffers.size(); ++i) {\n     if (buffers[i].source_memory_space == kCollectiveMemorySpaceColor) {\n-      TF_RETURN_IF_ERROR(RegisterBufferOnce(\n-          executor, comm, buffers[i].source_buffer, use_symmetric_buffer));\n+      TF_ASSIGN_OR_RETURN(auto range,\n+                          executor->GetMemoryRange(buffers[i].source_buffer));\n+      TF_RETURN_IF_ERROR(comm->RegisterBufferOnce(\n+          buffers[i].source_buffer, range, executor->device_ordinal(),\n+          use_symmetric_buffer));\n     }\n     if (buffers[i].destination_memory_space == kCollectiveMemorySpaceColor) {\n-      TF_RETURN_IF_ERROR(RegisterBufferOnce(\n-          executor, comm, buffers[i].destination_buffer, use_symmetric_buffer));\n+      TF_ASSIGN_OR_RETURN(\n+          auto range, executor->GetMemoryRange(buffers[i].destination_buffer));\n+      TF_RETURN_IF_ERROR(comm->RegisterBufferOnce(\n+          buffers[i].destination_buffer, range, executor->device_ordinal(),\n+          use_symmetric_buffer));\n     }\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "9ceb364e0d649bcefe7e29c26547db4027eefcc5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -153,9 +153,6 @@ absl::Status RunRaggedAllToAll(\n   VLOG(3) << \"[\" << device_ordinal\n           << \"] Performing ragged-all-to-all from device ordinal: \"\n           << device_ordinal;\n-  TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), original_buffers,\n-                                          comm, use_symmetric_buffer));\n-\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm->NumRanks());\n \n   std::vector<DeviceBufferPair> buffers = original_buffers;\n@@ -325,8 +322,6 @@ absl::Status RunMemCpyRaggedAllToAll(\n     se::Event* start_event, se::Event* end_event) {\n   int device_ordinal = stream.parent()->device_ordinal();\n   VLOG(3) << \"[\" << device_ordinal << \"] Performing mem-copy-ragged-all-to-all\";\n-  TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm));\n-\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm->NumRanks());\n \n   PrimitiveType element_type = buffers[0].element_type;"
        },
        {
            "sha": "0e1e35b14e20af6fd88298a133b38f1e3d29e8cf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/recv_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -99,9 +99,6 @@ absl::StatusOr<bool> RecvThunk::RunCollective(const ExecuteParams& params,\n           << CollectiveOpGroupModeToString(config_.config.group_mode) << \" (\"\n           << hlo_name_ << \")\";\n \n-  TF_RETURN_IF_ERROR(\n-      MaybeRegisterBuffers(stream.parent(), {buffer}, comm_handle.comm));\n-\n   const std::optional<int64_t> source_id = source_target.source;\n   se::DeviceMemoryBase dest_addr = buffer.destination_buffer;\n \n@@ -132,6 +129,8 @@ absl::StatusOr<bool> RecvThunk::RunCollective(const ExecuteParams& params,\n       ++(*counter);\n     }\n     if (should_run) {\n+      TF_RETURN_IF_ERROR(\n+          MaybeRegisterBuffers(stream.parent(), {buffer}, comm_handle.comm));\n       auto event = comm_handle.comm->Recv(\n           dest_addr, buffer.element_type, buffer.element_count,\n           RankId(*source_id), GpuCollectives::On(stream));"
        },
        {
            "sha": "403e401793c1f04c01848e2072b14d09d068367c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/send_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -99,9 +99,6 @@ absl::StatusOr<bool> SendThunk::RunCollective(const ExecuteParams& params,\n           << CollectiveOpGroupModeToString(config_.config.group_mode) << \" (\"\n           << hlo_name_ << \")\";\n \n-  TF_RETURN_IF_ERROR(\n-      MaybeRegisterBuffers(stream.parent(), {buffer}, comm_handle.comm));\n-\n   const std::optional<int64_t> target_id = source_target.target;\n   se::DeviceMemoryBase src_addr = buffer.source_buffer;\n \n@@ -133,6 +130,8 @@ absl::StatusOr<bool> SendThunk::RunCollective(const ExecuteParams& params,\n     }\n \n     if (should_run) {\n+      TF_RETURN_IF_ERROR(\n+          MaybeRegisterBuffers(stream.parent(), {buffer}, comm_handle.comm));\n       auto event = comm_handle.comm->Send(\n           src_addr, buffer.element_type, buffer.element_count,\n           RankId(*target_id), GpuCollectives::On(stream));"
        },
        {
            "sha": "af5fd0e8d22b3bc70d1e0dd4d1be445faca3832b",
            "filename": "third_party/xla/xla/core/collectives/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -73,6 +73,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:platform\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "b186df079d55cb38821f5b5c3fcda5283bb3d40a",
            "filename": "third_party/xla/xla/core/collectives/communicator.h",
            "status": "modified",
            "additions": 9,
            "deletions": 21,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1c686b4378447028d9170d1eff8d9038ce99cd4/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h?ref=d1c686b4378447028d9170d1eff8d9038ce99cd4",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/platform.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/chain.h\"\n #include \"xla/util.h\"\n@@ -66,30 +67,17 @@ class Communicator {\n     virtual absl::Status Unregister() = 0;\n   };\n \n-  // Register `buffer` for efficient collective operations (i.e. on NCCL backend\n-  // it registers the buffer for zero-copy collective operations).\n-  virtual absl::StatusOr<std::unique_ptr<RegisteredBufferHandle>>\n-  RegisterBuffer(stream_executor::DeviceMemoryBase buffer) {\n+  // Register `buffer` which can be a slice within a bigger `buffer_range` once\n+  // for efficient collective operations (i.e. on NCCL backend it registers the\n+  // buffer for zero-copy collective operations).\n+  //\n+  virtual absl::Status RegisterBufferOnce(se::DeviceMemoryBase buffer,\n+                                          se::DeviceMemoryBase buffer_range,\n+                                          int device_ordinal,\n+                                          bool use_symmetric_buffer) {\n     return Unimplemented(\"User-managed buffer registration is not supported\");\n   }\n \n-  // Register `buffer` for efficient collective operations (i.e. on NCCL backend\n-  // it registers the buffer for zero-copy collective operations).\n-  // If `use_symmetric_buffer` is true, the buffer is registered as a symmetric\n-  // buffer.\n-  virtual absl::StatusOr<std::unique_ptr<RegisteredBufferHandle>>\n-  RegisterBuffer(stream_executor::DeviceMemoryBase buffer,\n-                 bool use_symmetric_buffer) {\n-    return Unimplemented(\"User-managed buffer registration is not supported\");\n-  }\n-\n-  // Register `buffer` for efficient collective operations (i.e. on NVSHMEM\n-  // backend it registers the buffer for unregistered nvshmem buffers).\n-  virtual absl::Status RegisterBuffer(void* addr, size_t length) {\n-    return absl::UnimplementedError(\n-        \"User-managed buffer registration is not supported\");\n-  }\n-\n   // Abort any uncompleted operations and destroys the underlying communicator\n   // object. It is undefined behavior to use the communicator after calling\n   // this method."
        }
    ],
    "stats": {
        "total": 304,
        "additions": 151,
        "deletions": 153
    }
}