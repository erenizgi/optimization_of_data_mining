{
    "author": "majiddadashi",
    "message": "Update LowerQuantAnnotationsPass to use greedy rewriter\n\nIn addition, the rewrite pattern responsible for rewriting the fake quant composite now explicitly replaces the fake quant composite op instead of relying on the driver to kill the trivially dead op after all its uses replaced.\n\nPiperOrigin-RevId: 809268429",
    "sha": "70aed422873364804c3538ec57abc94fcaf2ebea",
    "files": [
        {
            "sha": "915db8f68675504ae3fe90b58d74c0f642825642",
            "filename": "tensorflow/compiler/mlir/lite/tests/lower_quant_annotations.mlir",
            "status": "added",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70aed422873364804c3538ec57abc94fcaf2ebea/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Flower_quant_annotations.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70aed422873364804c3538ec57abc94fcaf2ebea/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Flower_quant_annotations.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Flower_quant_annotations.mlir?ref=70aed422873364804c3538ec57abc94fcaf2ebea",
            "patch": "@@ -0,0 +1,25 @@\n+// RUN: litert-opt %s --tfl-lower-quant-annotations | FileCheck %s\n+\n+func.func private @XlaCallModule_quant.fake_quant.impl_0(tensor<1x28x28x3xf32>) -> tensor<1x28x28x3xf32>\n+func.func private @XlaCallModule_quant.fake_quant.impl_5_0(tensor<2x1x1x1xf32>) -> tensor<2x1x1x1xf32>\n+func.func private @XlaCallModule_quant.fake_quant.impl_17_0(tensor<1x30x30x2xf32>) -> tensor<1x30x30x2xf32>\n+// CHECK-LABEL: func.func @serving_default\n+func.func @serving_default(%arg0: tensor<1x28x28x3xf32>) -> (tensor<1x30x30x2xf32>) {\n+  %cst = arith.constant dense<[[0, 0], [1, 1], [1, 1], [0, 0]]> : tensor<4x2xi32>\n+  %cst_0 = arith.constant dense<[0, 2, 3, 1]> : tensor<4xi32>\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<2xf32>\n+  %cst_2 = arith.constant dense<[[[[1.0]]], [[[2.0]]]]> : tensor<2x1x1x1xf32>\n+  // CHECK: %[[QUANT0:.+]] = \"tfl.quantize\"(%arg0) <{qtype = tensor<1x28x28x3x!quant.uniform<i8:f32, 0.0039197271689772606:-128>>}> : (tensor<1x28x28x3xf32>) -> tensor<1x28x28x3x!quant.uniform<i8:f32, 0.0039197271689772606:-128>>\n+  // CHECK: %[[DEQUANT0:.+]] = \"tfl.dequantize\"(%[[QUANT0]]) : (tensor<1x28x28x3x!quant.uniform<i8:f32, 0.0039197271689772606:-128>>) -> tensor<1x28x28x3xf32>\n+  %0 = stablehlo.composite \"quant.fake_quant\" %arg0 {composite_attributes = {dtype = \"i8\", narrow_range = false, scale = dense<0.00391972717> : tensor<1xf32>, zero_point = dense<-128> : tensor<1xi32>}, decomposition = @XlaCallModule_quant.fake_quant.impl_0} : (tensor<1x28x28x3xf32>) -> tensor<1x28x28x3xf32>\n+  // CHECK: %[[QUANT1:.+]] = \"tfl.quantize\"(%{{.+}}) <{qtype = tensor<2x1x1x1x!quant.uniform<i8<-127:127>:f32:0, {0.0058756377547979355,0.0049431771039962769}>>}> : (tensor<2x1x1x1xf32>) -> tensor<2x1x1x1x!quant.uniform<i8<-127:127>:f32:0, {0.0058756377547979355,0.0049431771039962769}>>\n+  // CHECK: %[[DEQUANT1:.+]] = \"tfl.dequantize\"(%[[QUANT1]]) : (tensor<2x1x1x1x!quant.uniform<i8<-127:127>:f32:0, {0.0058756377547979355,0.0049431771039962769}>>) -> tensor<2x1x1x1xf32>\n+  %1 = stablehlo.composite \"quant.fake_quant\" %cst_2 {composite_attributes = {dtype = \"i8\", narrow_range = true, quantization_dimension = 0 : i32, scale = dense<[0.00587563775, 0.0049431771]> : tensor<2xf32>}, decomposition = @XlaCallModule_quant.fake_quant.impl_5_0} : (tensor<2x1x1x1xf32>) -> tensor<2x1x1x1xf32>\n+  %2 = \"tfl.transpose\"(%1, %cst_0) : (tensor<2x1x1x1xf32>, tensor<4xi32>) -> tensor<2x1x1x1xf32>\n+  %3 = \"tfl.pad\"(%0, %cst) : (tensor<1x28x28x3xf32>, tensor<4x2xi32>) -> tensor<1x30x30x3xf32>\n+  %4 = \"tfl.conv_2d\"(%3, %2, %cst_1) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x30x30x3xf32>, tensor<2x1x1x1xf32>, tensor<2xf32>) -> tensor<1x30x30x2xf32>\n+  // CHECK-OFF: %[[QUANT2:.+]] = \"tfl.quantize\"(%{{.+}}) <{qtype = tensor<1x30x30x2x!quant.uniform<i8:f32, 0.018049469217658043:8>>}> : (tensor<1x30x30x2xf32>) -> tensor<1x30x30x2x!quant.uniform<i8:f32, 0.018049469217658043:8>>\n+  // CHECK-OFF: %[[DEQUANT2:.+]] = \"tfl.dequantize\"(%[[QUANT2]]) : (tensor<1x30x30x2x!quant.uniform<i8:f32, 0.018049469217658043:8>>) -> tensor<1x30x30x2xf32>\n+  %5 = stablehlo.composite \"quant.fake_quant\" %4 {composite_attributes = {dtype = \"i8\", narrow_range = false, scale = dense<0.0180494692> : tensor<1xf32>, zero_point = dense<8> : tensor<1xi32>}, decomposition = @XlaCallModule_quant.fake_quant.impl_17_0} : (tensor<1x30x30x2xf32>) -> tensor<1x30x30x2xf32>\n+  return %5 : tensor<1x30x30x2xf32>\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "4773e749014829c718202fc11abdeaee015401f0",
            "filename": "tensorflow/compiler/mlir/lite/transforms/lower_quant_annotations_pass.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 41,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70aed422873364804c3538ec57abc94fcaf2ebea/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Flower_quant_annotations_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70aed422873364804c3538ec57abc94fcaf2ebea/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Flower_quant_annotations_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Flower_quant_annotations_pass.cc?ref=70aed422873364804c3538ec57abc94fcaf2ebea",
            "patch": "@@ -108,7 +108,7 @@ class RewriteQuantizeCompositeOp\n                                 /*input=*/op.getOperand(0),\n                                 /*qtype=*/TypeAttr::get(output_type));\n \n-    rewriter.replaceAllOpUsesWith(op, tfl_quantize_op.getOutput());\n+    rewriter.replaceOp(op, tfl_quantize_op.getOutput());\n     return success();\n   }\n };\n@@ -245,7 +245,7 @@ class RewriteDequantizeCompositeOp\n     TFL::DequantizeOp tfl_dequantize_op =\n         TFL::DequantizeOp::create(rewriter, composite_op.getLoc(), output_type,\n                                   /*input=*/tfl_quantize_input);\n-    rewriter.replaceAllOpUsesWith(composite_op, tfl_dequantize_op.getOutput());\n+    rewriter.replaceOp(composite_op, tfl_dequantize_op.getOutput());\n \n     return success();\n   }\n@@ -256,14 +256,9 @@ class RewriteFakeQuantCompositeOp\n   using OpRewritePattern<stablehlo::CompositeOp>::OpRewritePattern;\n \n  public:\n-  explicit RewriteFakeQuantCompositeOp(MLIRContext* context)\n-      : OpRewritePattern<stablehlo::CompositeOp>(context) {\n-    setHasBoundedRewriteRecursion();\n-  }\n-\n   LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n                                 PatternRewriter& rewriter) const final {\n-    if (op.getName() != \"quant.fake_quant\") {\n+    if (op.getName() != \"quant.fake_quant\" || IsDrqFakeQuant(op)) {\n       return failure();\n     }\n \n@@ -318,7 +313,7 @@ class RewriteFakeQuantCompositeOp\n     TFL::DequantizeOp tfl_dequantize_op = TFL::DequantizeOp::create(\n         rewriter, op.getLoc(), output_type, /*input=*/tfl_quantize_op);\n \n-    rewriter.replaceAllOpUsesWith(op, tfl_dequantize_op.getOutput());\n+    rewriter.replaceOp(op, tfl_dequantize_op.getOutput());\n     return success();\n   }\n };\n@@ -328,7 +323,7 @@ class RemovePreventGradient : public OpRewritePattern<TF::PreventGradientOp> {\n \n   LogicalResult matchAndRewrite(TF::PreventGradientOp op,\n                                 PatternRewriter& rewriter) const final {\n-    rewriter.replaceAllOpUsesWith(op, op.getInput());\n+    rewriter.replaceOp(op, op.getInput());\n     return success();\n   }\n };\n@@ -338,7 +333,7 @@ class RemoveIdentity : public OpRewritePattern<TF::IdentityOp> {\n \n   LogicalResult matchAndRewrite(TF::IdentityOp op,\n                                 PatternRewriter& rewriter) const final {\n-    rewriter.replaceAllOpUsesWith(op, op.getInput());\n+    rewriter.replaceOp(op, op.getInput());\n     return success();\n   }\n };\n@@ -361,10 +356,17 @@ class UpdateFunctionOutputType : public OpRewritePattern<func::ReturnOp> {\n     }\n \n     auto return_op_types = return_op.getOperandTypes();\n+    auto current_func_type = func_op.getFunctionType();\n+\n+    // If the function's result types already match the return op's\n+    // operand types, report failure so the rewriter converges.\n+    if (current_func_type.getResults() == return_op_types) {\n+      return failure();\n+    }\n+\n     rewriter.startOpModification(func_op);\n     auto new_func_type = mlir::FunctionType::get(\n-        func_op.getContext(), func_op.getFunctionType().getInputs(),\n-        return_op_types);\n+        func_op.getContext(), current_func_type.getInputs(), return_op_types);\n     func_op.setFunctionType(new_func_type);\n     rewriter.finalizeOpModification(func_op);\n \n@@ -400,34 +402,8 @@ void LowerQuantAnnotationsPass::runOnOperation() {\n   patterns.add<RewriteQuantizeCompositeOp, RewriteDequantizeCompositeOp,\n                RewriteFakeQuantCompositeOp>(&ctx);\n \n-  ConversionTarget target(getContext());\n-  target.addLegalDialect<func::FuncDialect>();\n-  target.addLegalDialect<TF::TensorFlowDialect>();\n-  target.addLegalDialect<TFL::TensorFlowLiteDialect>();\n-  target.addLegalDialect<quant::QuantDialect>();\n-  target.addLegalDialect<arith::ArithDialect>();\n-\n-  // Declare all the MHLO ops as legal except for the quantization composites we\n-  // want to lower.\n-  target.addDynamicallyLegalDialect<stablehlo::StablehloDialect>(\n-      [](Operation* op) {\n-        auto mhlo_op = dyn_cast_or_null<stablehlo::CompositeOp>(op);\n-        if (!mhlo_op) {\n-          return true;\n-        }\n-        // DRQ fake quant survives this pass to be later fused into the DRQ\n-        // kernel. We cannot lower this to Q-DQ since scale/zero_point are\n-        // unknown.\n-        if (IsDrqFakeQuant(mhlo_op)) {\n-          return true;\n-        }\n-        return mhlo_op.getName() != \"quant.quantize\" &&\n-               mhlo_op.getName() != \"quant.dequantize\" &&\n-               mhlo_op.getName() != \"quant.fake_quant\";\n-      });\n-\n-  if (failed(applyPartialConversion(getOperation(), target,\n-                                    std::move(patterns)))) {\n+  if (failed(\n+          applyPatternsGreedily(module, std::move(patterns), greedy_config))) {\n     getOperation().emitError(\"Composite lowering pass failed.\");\n     signalPassFailure();\n   }"
        }
    ],
    "stats": {
        "total": 83,
        "additions": 42,
        "deletions": 41
    }
}