{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Update parameter extract, dot, pad & concat to emit 0D tensors.\n\nOne of a chain of commits to remove the special casing for 0D tensors so that the triton specific requirement is moved to the lowering stage, e.g. the upcoming XTile::CPU backend doesn't have such a requirement, and consistently using tensors makes it simpler.\n\nPiperOrigin-RevId: 828921635",
    "sha": "d116e676076ed29b1caa2473367e6a7cf4f50822",
    "files": [
        {
            "sha": "6b76cf017ad500c3ba387b87820158a98c521a50",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 27,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d116e676076ed29b1caa2473367e6a7cf4f50822/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d116e676076ed29b1caa2473367e6a7cf4f50822/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=d116e676076ed29b1caa2473367e6a7cf4f50822",
            "patch": "@@ -352,16 +352,14 @@ TensorValue Iota(EmitterLocOpBuilder b, int32_t limit) {\n   return b.create<stablehlo::IotaOp>(type, /*iota_dimension=*/0);\n }\n \n-ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder b,\n-                                    const TileInfo& tile_info, Value arg) {\n+TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n+                                 const TileInfo& tile_info, Value arg) {\n   auto tensor_type = mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n                                                  tile_info.storage_type());\n \n-  mlir::Value extracted_tensor = b.create<xla::xtile::ExtractTileOp>(\n+  return b.create<xla::xtile::ExtractTileOp>(\n       tensor_type, arg, tile_info.offsets(), tile_info.padded_tile_sizes(),\n       tile_info.tile_strides());\n-\n-  return MakeScalarOrTensor(b, extracted_tensor);\n }\n \n absl::StatusOr<TensorValue> EmitScope(\n@@ -869,7 +867,7 @@ absl::StatusOr<TensorValue> CanonicalizeDotOperand(\n   return operand;\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitDot(\n+absl::StatusOr<TensorValue> EmitDot(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n@@ -1033,10 +1031,10 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n         result, EmitTiledReshape(b, padded_tile_sizes, MakeTensor(b, result)));\n   }\n \n-  return MakeScalarOrTensor(b, result);\n+  return MakeTensor(b, result);\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n+absl::StatusOr<TensorValue> EmitScaledDot(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n@@ -1186,10 +1184,10 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n         result, EmitTiledReshape(b, padded_tile_sizes, MakeTensor(b, result)));\n   }\n \n-  return MakeScalarOrTensor(b, result);\n+  return MakeTensor(b, result);\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n+absl::StatusOr<TensorValue> EmitConcatenate(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_concatenate,\n@@ -1294,10 +1292,10 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n \n   b.setInsertionPointAfter(if_ops.front());\n \n-  return ScalarOrTensor(if_ops.front().getResult(0));\n+  return mlir::cast<TensorValue>(if_ops.front().getResult(0));\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitPad(\n+absl::StatusOr<TensorValue> EmitPad(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const TiledHloInstruction& tiled_pad,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n@@ -1348,15 +1346,15 @@ absl::StatusOr<ScalarOrTensor> EmitPad(\n     mask = mask ? b.create<arith::AndIOp>(mask, cmp) : cmp;\n   }\n   if (!mask) {\n-    return MakeScalarOrTensor(b, values[tiled_operand]);\n+    return values[tiled_operand];\n   }\n   const TiledHloInstruction* padding_value = tiled_pad.operand(1);\n \n   TensorValue pad_value_splat =\n       Splat(b, values[padding_value], padded_tile_sizes);\n-  auto result = ScalarOrTensor(\n-      b.create<arith::SelectOp>(mask, values[tiled_operand], pad_value_splat));\n-  return result;\n+  return mlir::cast<TensorValue>(\n+      b.create<arith::SelectOp>(mask, values[tiled_operand], pad_value_splat)\n+          .getResult());\n }\n \n absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n@@ -1383,7 +1381,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n         TileInfo tile_info,\n         TileInfo::Construct(b, pid, GetRuntimeValues(tiled_hlo, values),\n                             tiled_hlo));\n-    ScalarOrTensor parameter =\n+    TensorValue parameter =\n         EmitParameterExtract(b, tile_info, fn.getArgument(arg_index));\n \n     // Some types are stored using different types, e.g. i1 is stored in memory\n@@ -1403,30 +1401,39 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n             \"while lowering \",\n             fusion->called_computation()->ToString()));\n       }\n-      parameter = ScalarOrTensor(\n-          Cast(b, parameter.UnwrapUnsafe(), expected_element_type));\n+      parameter =\n+          mlir::cast<TensorValue>(Cast(b, parameter, expected_element_type));\n     }\n \n-    return parameter;\n+    return MakeScalarOrTensor(b, parameter);\n   }\n \n   if (hlo->opcode() == HloOpcode::kConcatenate) {\n-    return EmitConcatenate(b, device_info, fusion, tiled_hlo,\n-                           block_level_parameters, fn, pid, values);\n+    TF_ASSIGN_OR_RETURN(\n+        TensorValue result,\n+        EmitConcatenate(b, device_info, fusion, tiled_hlo,\n+                        block_level_parameters, fn, pid, values));\n+    return MakeScalarOrTensor(b, result);\n   }\n \n   if (hlo->opcode() == HloOpcode::kPad) {\n-    return EmitPad(b, device_info, tiled_hlo, values, pid);\n+    TF_ASSIGN_OR_RETURN(TensorValue result,\n+                        EmitPad(b, device_info, tiled_hlo, values, pid));\n+    return MakeScalarOrTensor(b, result);\n   }\n \n   if (hlo->opcode() == HloOpcode::kDot) {\n-    return EmitDot(b, device_info, fusion, tiled_hlo, block_level_parameters,\n-                   fn, pid, values);\n+    TF_ASSIGN_OR_RETURN(TensorValue result,\n+                        EmitDot(b, device_info, fusion, tiled_hlo,\n+                                block_level_parameters, fn, pid, values));\n+    return MakeScalarOrTensor(b, result);\n   }\n \n   if (hlo->opcode() == HloOpcode::kScaledDot) {\n-    return EmitScaledDot(b, device_info, fusion, tiled_hlo,\n-                         block_level_parameters, fn, pid, values);\n+    TF_ASSIGN_OR_RETURN(TensorValue result,\n+                        EmitScaledDot(b, device_info, fusion, tiled_hlo,\n+                                      block_level_parameters, fn, pid, values));\n+    return MakeScalarOrTensor(b, result);\n   }\n \n   if (hlo->opcode() == HloOpcode::kConstant) {"
        }
    ],
    "stats": {
        "total": 61,
        "additions": 34,
        "deletions": 27
    }
}