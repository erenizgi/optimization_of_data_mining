{
    "author": "sergachev",
    "message": "PR #31018: [NFC] Fix various typos.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31018\n\nCopybara import of the project:\n\n--\nb13132ab8f5331f80d5c8a9d3d5d5a18f242f9f3 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[NFC] Fix various typos.\n\nMerging this change closes #31018\n\nPiperOrigin-RevId: 804797296",
    "sha": "def1256b27bc3dcec8dbd4234b56fa26c2d720a0",
    "files": [
        {
            "sha": "36f2a93f023a9902d2c1d501f19b37f79001dc48",
            "filename": "third_party/xla/third_party/gpus/cuda/build_defs.bzl.tpl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fthird_party%2Fgpus%2Fcuda%2Fbuild_defs.bzl.tpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fthird_party%2Fgpus%2Fcuda%2Fbuild_defs.bzl.tpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fgpus%2Fcuda%2Fbuild_defs.bzl.tpl?ref=def1256b27bc3dcec8dbd4234b56fa26c2d720a0",
            "patch": "@@ -11,7 +11,7 @@ def if_cuda(if_true, if_false = []):\n     })\n \n def if_cuda_clang(if_true, if_false = []):\n-   \"\"\"Shorthand for select()'ing on wheteher we're building with cuda-clang.\n+   \"\"\"Shorthand for select()'ing on whether we're building with cuda-clang.\n \n     Returns a select statement which evaluates to if_true if we're building\n     with cuda-clang.  Otherwise, the select statement evaluates to if_false.\n@@ -31,7 +31,7 @@ def if_cuda_exec(if_true, if_false = []):\n     return if_cuda(if_true, if_false)\n \n def cuda_compiler(if_cuda_clang, if_nvcc, neither = []):\n-    \"\"\"Shorthand for select()'ing on wheteher we're building with cuda-clang or nvcc.\n+    \"\"\"Shorthand for select()'ing on whether we're building with cuda-clang or nvcc.\n \n      Returns a select statement which evaluates to if_cuda_clang if we're building\n      with cuda-clang, if_nvcc if we're building with NVCC.\n@@ -48,7 +48,7 @@ def cuda_compiler(if_cuda_clang, if_nvcc, neither = []):\n         return neither\n \n def if_cuda_clang_opt(if_true, if_false = []):\n-   \"\"\"Shorthand for select()'ing on wheteher we're building with cuda-clang\n+   \"\"\"Shorthand for select()'ing on whether we're building with cuda-clang\n    in opt mode.\n \n     Returns a select statement which evaluates to if_true if we're building"
        },
        {
            "sha": "d72d33a2499199ab46d16cecb5cb435ed2195071",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=def1256b27bc3dcec8dbd4234b56fa26c2d720a0",
            "patch": "@@ -681,7 +681,7 @@ class LaunchCmd : public CommandBufferCmd {\n };\n \n //===----------------------------------------------------------------------===//\n-// CustomKenelLaunchCmd\n+// CustomKernelLaunchCmd\n //===----------------------------------------------------------------------===//\n \n class CustomKernelLaunchCmd : public CommandBufferCmd {"
        },
        {
            "sha": "673fb14936edd4ebe69a1dd14bf3d91493e051f5",
            "filename": "third_party/xla/xla/hlo/tools/hlo_opt/opt_main.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_main.cc?ref=def1256b27bc3dcec8dbd4234b56fa26c2d720a0",
            "patch": "@@ -135,7 +135,7 @@ absl::StatusOr<std::vector<std::unique_ptr<HloModule>>> GetModules(\n             \"'// ---'\");\n       } else {\n         return absl::InternalError(\n-            \"'// ---' separator found in input, but -split-input-file not \"\n+            \"'// ---' separator found in input, but --split-input-file not \"\n             \"specified\");\n       }\n     }"
        },
        {
            "sha": "a1e65609f6ac59dd72b009d6219de5fc73fedf78",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc?ref=def1256b27bc3dcec8dbd4234b56fa26c2d720a0",
            "patch": "@@ -87,8 +87,8 @@ int64_t GetPaddedTileSize(absl::Span<int64_t const> tile_sizes) {\n //\n // Spilling almost always causes significant performance regressions, so this\n // heuristic tries to be safe and increase recall at the cost of precision.\n-bool DoesTileFitsInRegisters(int64_t tile_size,\n-                             const se::DeviceDescription& device_info) {\n+bool DoesTileFitInRegisters(int64_t tile_size,\n+                            const se::DeviceDescription& device_info) {\n   // This is a conservative estimate to make sure that we don't get a tile that\n   // is too big and results in register spills.\n   //\n@@ -141,8 +141,8 @@ bool DoesComputationFitInRegisters(\n     const se::DeviceDescription& device_info) {\n   // Check that output tiles fit in registers.\n   for (const TiledHloInstruction* root : tiled_hlo_computation.GetRoots()) {\n-    if (!DoesTileFitsInRegisters(GetPaddedTileSize(root->tile_sizes()),\n-                                 device_info)) {\n+    if (!DoesTileFitInRegisters(GetPaddedTileSize(root->tile_sizes()),\n+                                device_info)) {\n       return false;\n     }\n   }\n@@ -154,8 +154,8 @@ bool DoesComputationFitInRegisters(\n     // Iota is not an operand, but usually needs to be materialized in\n     // registers.\n     if ((is_operand || tiled_hlo->hlo()->opcode() == HloOpcode::kIota) &&\n-        !DoesTileFitsInRegisters(GetPaddedTileSize(tiled_hlo->tile_sizes()),\n-                                 device_info)) {\n+        !DoesTileFitInRegisters(GetPaddedTileSize(tiled_hlo->tile_sizes()),\n+                                device_info)) {\n       return false;\n     }\n   }"
        },
        {
            "sha": "000910c6fd2e5be44e1b732adddda3296ca3dea7",
            "filename": "third_party/xla/xla/service/spmd/fft_handler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/def1256b27bc3dcec8dbd4234b56fa26c2d720a0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc?ref=def1256b27bc3dcec8dbd4234b56fa26c2d720a0",
            "patch": "@@ -235,7 +235,7 @@ HloInstruction* GetFinalFftUsingCollectivePermute(\n       ShapeUtil::ChangeElementType(partition_id->shape(),\n                                    hlo->shape().element_type()),\n       partition_id));\n-  // Buid while loop body.\n+  // Build while loop body.\n   SpmdBuilder body_b(\"fft_collective_permute_body\", hlo);\n   auto param = body_b.AddInstruction(HloInstruction::CreateParameter(\n       /*parameter_number=*/0,"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 12,
        "deletions": 12
    }
}