{
    "author": "tomhennigan",
    "message": "Optimize even sharding path in `HloSharding::Disassemble`.\n\nPiperOrigin-RevId: 840220652",
    "sha": "53a29d6863b22496958d53c97794fa159924d7df",
    "files": [
        {
            "sha": "f1886472ad0564f95fbc85c28069d245a14cc053",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 22,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/53a29d6863b22496958d53c97794fa159924d7df/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/53a29d6863b22496958d53c97794fa159924d7df/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc?ref=53a29d6863b22496958d53c97794fa159924d7df",
            "patch": "@@ -213,29 +213,38 @@ HloSharding::Disassemble(\n     is_even_sharding = true;\n   }\n \n-  const absl::Span<Device* const> devices = devices_->devices();\n-  if (is_even_sharding) {\n-    // Fast path for even sharding.\n-    TF_ASSIGN_OR_RETURN(xla::ifrt::Shape shard_shape, GetShardShape(shape));\n-    std::vector<std::pair<Shape, ShardingRef>> result;\n-    if (single_device_shard_semantics ==\n-        SingleDeviceShardSemantics::kAllShards) {\n-      result.reserve(devices_->size());\n-    } else {\n-      result.reserve(devices_->AddressableDeviceList()->size());\n-    }\n-    for (int i = 0; i < devices_->size(); ++i) {\n-      if (single_device_shard_semantics ==\n-              SingleDeviceShardSemantics::kAllShards ||\n-          devices[i]->IsAddressable()) {\n-        result.push_back({\n-            shard_shape,\n-            SingleDeviceSharding::Create(devices[i], memory_kind_),\n-        });\n-      }\n-    }\n-    return result;\n+  return is_even_sharding\n+             ? DisassembleEven(shape, single_device_shard_semantics)\n+             : DisassembleUneven(shape, single_device_shard_semantics);\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n+HloSharding::DisassembleEven(\n+    const Shape& shape,\n+    SingleDeviceShardSemantics single_device_shard_semantics) const {\n+  // Fast path for even sharding.\n+  TF_ASSIGN_OR_RETURN(xla::ifrt::Shape shard_shape, GetShardShape(shape));\n+  std::vector<std::pair<Shape, ShardingRef>> result;\n+  DeviceList* device_list;\n+  if (single_device_shard_semantics == SingleDeviceShardSemantics::kAllShards) {\n+    device_list = devices_.get();\n+  } else {\n+    device_list = devices_->AddressableDeviceList();\n+  }\n+  result.reserve(device_list->size());\n+  for (Device* device : device_list->devices()) {\n+    result.push_back({\n+        shard_shape,\n+        SingleDeviceSharding::Create(device, memory_kind_),\n+    });\n   }\n+  return result;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n+HloSharding::DisassembleUneven(\n+    const Shape& shape,\n+    SingleDeviceShardSemantics single_device_shard_semantics) const {\n   // Slow path that uses `IndexDomains()` to handle uneven sharding.\n   TF_ASSIGN_OR_RETURN(\n       std::vector<IndexDomain> index_domains,\n@@ -247,6 +256,7 @@ HloSharding::Disassemble(\n   } else {\n     result.reserve(devices_->AddressableDeviceList()->size());\n   }\n+  const absl::Span<Device* const> devices = devices_->devices();\n   for (int i = 0; i < index_domains.size(); ++i) {\n     if (single_device_shard_semantics ==\n             SingleDeviceShardSemantics::kAllShards ||"
        },
        {
            "sha": "0c75e1bf3556fc65812a135d8404118176b363da",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding.h",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/53a29d6863b22496958d53c97794fa159924d7df/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/53a29d6863b22496958d53c97794fa159924d7df/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.h?ref=53a29d6863b22496958d53c97794fa159924d7df",
            "patch": "@@ -98,6 +98,14 @@ class HloSharding final\n \n   void Hash(absl::HashState state) const override;\n \n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> DisassembleEven(\n+      const Shape& shape,\n+      SingleDeviceShardSemantics single_device_shard_semantics) const;\n+\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> DisassembleUneven(\n+      const Shape& shape,\n+      SingleDeviceShardSemantics single_device_shard_semantics) const;\n+\n   xla::HloSharding xla_hlo_sharding_;\n \n   // Cached hash. 0 indicates the hash needs to be computed and cached."
        }
    ],
    "stats": {
        "total": 62,
        "additions": 40,
        "deletions": 22
    }
}