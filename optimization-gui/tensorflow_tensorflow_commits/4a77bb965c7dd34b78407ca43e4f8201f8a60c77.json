{
    "author": "ChromeHearts",
    "message": "Improve resharding adapter to support in the case when Table stacking layouts have changed.\n\nPiperOrigin-RevId: 804460722",
    "sha": "4a77bb965c7dd34b78407ca43e4f8201f8a60c77",
    "files": [
        {
            "sha": "1af261667bbc1f54754c46f168ef0aa919ea8a84",
            "filename": "tensorflow/python/tpu/tpu_embedding_v3_checkpoint_adapter.py",
            "status": "modified",
            "additions": 92,
            "deletions": 50,
            "changes": 142,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a77bb965c7dd34b78407ca43e4f8201f8a60c77/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a77bb965c7dd34b78407ca43e4f8201f8a60c77/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter.py?ref=4a77bb965c7dd34b78407ca43e4f8201f8a60c77",
            "patch": "@@ -150,13 +150,13 @@ def _unshard_from_sc_to_cpu(\n   logging.vlog(\n       1,\n       \"To unshuffle_from_sc_to_cpu on stacked_table.shape: %s\",\n-      stacked_table[0].shape,\n+      stacked_table.shape,\n   )\n   ret_tensors = []\n \n   for layout in from_shard_layouts:\n     padded_table = tpu_embedding_v3_utils.unshuffle_from_sc_to_cpu(\n-        stacked_table[0],\n+        stacked_table,\n         num_sparse_cores=layout.num_sparse_cores,\n         offset_in_shard=layout.sparse_core_shard_row_offset,\n         size_in_shard=layout.unsharded_padded_shape[0]\n@@ -279,20 +279,19 @@ class EmbeddingReshardCallback(checkpoint_adapter.ReshardCallback):\n   def __init__(\n       self,\n       object_local_name: str,\n-      from_shard_layouts: Sequence[\n-          sparse_core_layout_pb2.SparseCoreTableLayout\n-      ],  # table name to layout\n-      to_shard_layouts: Sequence[\n-          sparse_core_layout_pb2.SparseCoreTableLayout\n-      ],  # table name to layout\n+      from_shard_layouts: Mapping[\n+          str, Sequence[sparse_core_layout_pb2.SparseCoreTableLayout]\n+      ],\n+      to_shard_layouts: Sequence[sparse_core_layout_pb2.SparseCoreTableLayout],\n   ):\n     \"\"\"Initializes  Reshard callback.\n \n     Args:\n       object_local_name:  The local name of the object being restored.\n-      from_shard_layouts: layouts as in checkpoint being restored from.\n-      to_shard_layouts: target layouts as specified in the embedding being\n-        restored.\n+      from_shard_layouts: A dictionary in stacked table name to a list of its\n+        consituent table layouts.  The layouts are coming from the checkpoint\n+        being restored.\n+      to_shard_layouts: a list of target layouts that will be resharded to.\n     \"\"\"\n     logging.info(\"Creating EmbeddingReshardCallback for %s\", object_local_name)\n     self._object_local_name = object_local_name\n@@ -322,55 +321,76 @@ def update_restore_inputs(\n       restore_v2 op will usually be passed to reshard method of this class to\n       get the final resharded value.\n     \"\"\"\n-    logging.vlog(\n-        1,\n-        \"Updating restore v2 inputs for %s[%s]: %s\",\n-        checkpoint_key,\n-        self._object_local_name,\n-        shape_and_slice_spec,\n-    )\n-\n+    keys = []\n     slices = []\n+    for stacked_name, table_layouts in self._from_shard_layouts.items():\n+      key = checkpoint_key.replace(self._object_local_name, stacked_name)\n+      keys.append(key)\n+\n+      # use the first layout get the full shape of the stacked table\n+      first_layout = table_layouts[0]\n+      full_vocab_size = (\n+          first_layout.total_rows_per_sparse_core_shard\n+          * first_layout.num_sparse_cores\n+      )\n+      stack_dim = first_layout.unsharded_padded_shape[1]\n+      full_shape = [full_vocab_size, stack_dim]\n+      slices.append(\n+          _shard_info_str(\n+              full_shape,\n+              trackable_base.ShardInfo(offset=[0, 0], shape=full_shape),\n+          )\n+      )\n \n-    # use the first layout get the full shape of the stacked table\n-    first_layout = self._from_shard_layouts[0]\n-    full_vocab_size = (\n-        first_layout.total_rows_per_sparse_core_shard\n-        * first_layout.num_sparse_cores\n-    )\n-    stack_dim = first_layout.unsharded_padded_shape[1]\n-    full_shape = [full_vocab_size, stack_dim]\n-    logging.vlog(\n-        1,\n-        \"Read checkpoint_key %s: %s\",\n+    logging.info(\n+        \"Updating restore v2 inputs for %s[%s]:%s to stacked_tables: [%s],\"\n+        \" slices: [%s]\",\n         checkpoint_key,\n-        full_shape,\n+        self._object_local_name,\n+        shape_and_slice_spec,\n+        \", \".join(keys),\n+        \", \".join(slices),\n     )\n \n-    slices.append(\n-        _shard_info_str(\n-            full_shape,\n-            trackable_base.ShardInfo(offset=[0, 0], shape=full_shape),\n-        )\n-    )\n-    return ([checkpoint_key], slices)\n+    return (keys, slices)\n \n   def reshard(\n-      self, checkpoint_values: tensor.Tensor, shape_and_slice: str\n+      self,\n+      checkpoint_values: Sequence[tensor.Tensor],\n+      shape_and_slice: str,\n   ) -> tensor.Tensor:\n     # unshard\n     stime = time.time()\n-    logging.vlog(\n-        1,\n-        \"EmbeddingReshardCallback: starting to reshard [%s]\",\n+    logging.info(\n+        \"EmbeddingReshardCallback: starting to reshard [%s],\"\n+        \" from checkpoint_value with shapes: %s\",\n         self._object_local_name,\n+        \", \".join([str(t.shape) for t in checkpoint_values]),\n     )\n-    unsharded_tensors = _unshard_from_sc_to_cpu(\n-        checkpoint_values, self._from_shard_layouts\n-    )\n \n+    unsharded_tables = dict()\n+\n+    for stacked_table, layouts in zip(\n+        checkpoint_values,\n+        list(self._from_shard_layouts.values()),\n+    ):\n+      logging.info(\n+          \"Unshard sc_to_cpu stacked_table: %s, shape: %s, no. of constituent\"\n+          \" tables: %d\",\n+          layouts[0].stacked_table_name,\n+          stacked_table.shape,\n+          len(layouts),\n+      )\n+\n+      unsharded_tensors = _unshard_from_sc_to_cpu(stacked_table, layouts)\n+      for unshared_tensor, layout in zip(unsharded_tensors, layouts):\n+        unsharded_tables[layout.table_name] = unshared_tensor\n+\n+    required_tables = [\n+        unsharded_tables[layout.table_name] for layout in self._to_shard_layouts\n+    ]\n     ret = _shard_from_cpu_to_sc(\n-        unsharded_tensors, shape_and_slice, self._to_shard_layouts\n+        required_tables, shape_and_slice, self._to_shard_layouts\n     )\n \n     etime = time.time()\n@@ -385,7 +405,16 @@ def reshard(\n def _reorg_layouts(\n     layouts: Sequence[sparse_core_layout_pb2.SparseCoreTableLayout],\n ) -> Mapping[str, Sequence[sparse_core_layout_pb2.SparseCoreTableLayout]]:\n-  \"\"\"Reorg the layouts to be in the order of the logical table.\"\"\"\n+  \"\"\"Reorg the layouts to be in the order of the logical table.\n+\n+    Building a Dict[StackedTableName, SortedList[TableLayout]]\n+\n+  Args:\n+    layouts: The layouts to be reorged.\n+\n+  Returns:\n+    A dict of stacked table name to sorted list of table layouts.\n+  \"\"\"\n   stacked_name_to_table_names = collections.defaultdict(list)\n   for layout in layouts:\n     stacked_name_to_table_names[layout.stacked_table_name].append(layout)\n@@ -470,12 +499,25 @@ def initialize_reshard_callbacks(\n     # Reshard to different SC Layout\n     from_layouts = _reorg_layouts(list(self._checkpoint_layouts.values()))\n     to_layouts = _reorg_layouts(list(embedding_layouts.values()))\n-    for stacked_name in from_layouts.keys():\n-      logging.info(\"Creating resharding plan for %s\", stacked_name)\n+    for stacked_name, table_layouts in to_layouts.items():\n+      # look for required stacked tables\n+      required_stacked_tables = dict()\n+      for table_layout in table_layouts:\n+        for from_stacked_name, from_table_layouts in from_layouts.items():\n+          if table_layout.table_name in {\n+              layout.table_name for layout in from_table_layouts\n+          }:\n+            required_stacked_tables[from_stacked_name] = from_table_layouts\n+\n+      logging.info(\n+          \"Creating resharding plan for %s, required stacked_tables: %s\",\n+          stacked_name,\n+          \", \".join(required_stacked_tables.keys()),\n+      )\n       self._checkpoint_to_reshard_callback[stacked_name] = (\n           EmbeddingReshardCallback(\n               object_local_name=stacked_name,\n-              from_shard_layouts=from_layouts[stacked_name],\n+              from_shard_layouts=required_stacked_tables,\n               to_shard_layouts=to_layouts[stacked_name],\n           )\n       )"
        },
        {
            "sha": "7f9252550efcff83e696107ca84f08fca3d87e37",
            "filename": "tensorflow/python/tpu/tpu_embedding_v3_checkpoint_adapter_test.py",
            "status": "modified",
            "additions": 217,
            "deletions": 2,
            "changes": 219,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a77bb965c7dd34b78407ca43e4f8201f8a60c77/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a77bb965c7dd34b78407ca43e4f8201f8a60c77/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter_test.py?ref=4a77bb965c7dd34b78407ca43e4f8201f8a60c77",
            "patch": "@@ -376,12 +376,12 @@ def test_adapt_to_different_sharded_stacked(self):\n     callback = sc_to_sc_adapter.get_reshard_callback(\"one_two_three\")\n     self.assertEqual(callback.object_name(), \"one_two_three\")\n     updated_keys, updated_slices = callback.update_restore_inputs(\n-        \"path/to/embedding/one_two/in/checkpoint\", \"24 8 6,12:0,8\"\n+        \"path/to/embedding/one_two_three/in/checkpoint\", \"24 8 6,12:0,8\"\n     )\n     self.assertAllEqual(\n         updated_keys,\n         [\n-            \"path/to/embedding/one_two/in/checkpoint\",\n+            \"path/to/embedding/one_two_three/in/checkpoint\",\n         ],\n     )\n     self.assertAllEqual(\n@@ -454,6 +454,221 @@ def test_adapt_to_different_sharded_stacked(self):\n         callback.reshard([one_two_three], \"32 8 8,16:0,8\"),\n     )\n \n+  def test_adapt_to_different_sc_table_stacking(self):\n+\n+    source_layouts = {\n+        \"one\": create_layout(\n+            tables_name=\"one\",\n+            stacked_table_name=\"one_two\",\n+            num_sparse_cores=4,\n+            num_partitions=2,\n+            unsharded_shape=(6, 5),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=4,\n+        ),\n+        \"two\": create_layout(\n+            tables_name=\"two\",\n+            stacked_table_name=\"one_two\",\n+            num_sparse_cores=4,\n+            num_partitions=2,\n+            unsharded_shape=(7, 4),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=2,\n+            shard_rotation=1,\n+            total_rows_per_sparse_core_shard=4,\n+        ),\n+        \"three\": create_layout(\n+            tables_name=\"three\",\n+            stacked_table_name=\"three\",\n+            num_sparse_cores=4,\n+            num_partitions=2,\n+            unsharded_shape=(15, 3),\n+            unsharded_padded_shape=(16, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=4,\n+        ),\n+    }\n+    src_layouts_pb = sparse_core_layout_pb2.SparseCoreTableLayouts()\n+    src_layouts_pb.tables.extend(source_layouts.values())\n+\n+    sc_to_sc_adapter = (\n+        tpu_embedding_v3_checkpoint_adapter.TpuEmbeddingV3CheckpointAdapter(\n+            layouts=src_layouts_pb\n+        )\n+    )\n+\n+    target_layouts = {\n+        \"one\": create_layout(\n+            tables_name=\"one\",\n+            stacked_table_name=\"one\",\n+            num_sparse_cores=8,\n+            num_partitions=4,\n+            unsharded_shape=(6, 5),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=1,\n+        ),\n+        \"two\": create_layout(\n+            tables_name=\"two\",\n+            stacked_table_name=\"two_three\",\n+            num_sparse_cores=8,\n+            num_partitions=4,\n+            unsharded_shape=(7, 4),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=3,\n+        ),\n+        \"three\": create_layout(\n+            tables_name=\"three\",\n+            stacked_table_name=\"two_three\",\n+            num_sparse_cores=8,\n+            num_partitions=4,\n+            unsharded_shape=(15, 3),\n+            unsharded_padded_shape=(16, 8),\n+            row_offset=1,\n+            shard_rotation=1,\n+            total_rows_per_sparse_core_shard=3,\n+        ),\n+    }\n+\n+    # this take a mapping[str, sparse_core_layout_pb2.SparseCoreTableLayout]\n+    sc_to_sc_adapter.initialize_reshard_callbacks(target_layouts)\n+\n+    src_one_two = tf_constant([\n+        # shard 0\n+        [0, 0, 0, 0, 0, 0, 0, 0],\n+        [4, 4, 4, 4, 4, 0, 0, 0],\n+        [13, 13, 13, 13, 0, 0, 0, 0],\n+        [0, 0, 0, 0, 0, 0, 0, 0],\n+        # shard 1\n+        [1, 1, 1, 1, 1, 0, 0, 0],\n+        [5, 5, 5, 5, 5, 0, 0, 0],\n+        [10, 10, 10, 10, 0, 0, 0, 0],\n+        [14, 14, 14, 14, 0, 0, 0, 0],\n+        # shard 2\n+        [2, 2, 2, 2, 2, 0, 0, 0],\n+        [6, 6, 6, 6, 6, 0, 0, 0],\n+        [11, 11, 11, 11, 0, 0, 0, 0],\n+        [15, 15, 15, 15, 0, 0, 0, 0],\n+        # shard 3\n+        [3, 3, 3, 3, 3, 0, 0, 0],\n+        [7, 7, 7, 7, 7, 0, 0, 0],\n+        [12, 12, 12, 12, 0, 0, 0, 0],\n+        [16, 16, 16, 16, 0, 0, 0, 0],\n+    ])\n+\n+    src_three = tf_constant([\n+        # shard 0\n+        [100, 100, 100, 0, 0, 0, 0, 0],\n+        [104, 104, 104, 0, 0, 0, 0, 0],\n+        [108, 108, 108, 0, 0, 0, 0, 0],\n+        [112, 112, 112, 0, 0, 0, 0, 0],\n+        # shard 1\n+        [101, 101, 101, 0, 0, 0, 0, 0],\n+        [105, 105, 105, 0, 0, 0, 0, 0],\n+        [109, 109, 109, 0, 0, 0, 0, 0],\n+        [113, 113, 113, 0, 0, 0, 0, 0],\n+        # shard 2\n+        [102, 102, 102, 0, 0, 0, 0, 0],\n+        [106, 106, 106, 0, 0, 0, 0, 0],\n+        [110, 110, 110, 0, 0, 0, 0, 0],\n+        [114, 114, 114, 0, 0, 0, 0, 0],\n+        # shard 3\n+        [103, 103, 103, 0, 0, 0, 0, 0],\n+        [107, 107, 107, 0, 0, 0, 0, 0],\n+        [111, 111, 111, 0, 0, 0, 0, 0],\n+        [0, 0, 0, 0, 0, 0, 0, 0],\n+    ])\n+\n+    with self.subTest(\"one\"):\n+      callback = sc_to_sc_adapter.get_reshard_callback(\"one\")\n+      self.assertEqual(callback.object_name(), \"one\")\n+      updated_keys, updated_slices = callback.update_restore_inputs(\n+          \"path/to/embedding/one/in/checkpoint\", \"8 8 2,3:0,8\"\n+      )\n+\n+      self.assertAllEqual(\n+          updated_keys,\n+          [\n+              \"path/to/embedding/one_two/in/checkpoint\",\n+          ],\n+      )\n+      self.assertAllEqual(\n+          updated_slices,\n+          [\"16 8 0,16:0,8\"],\n+      )\n+\n+      self.assertAllEqual(\n+          tf_constant([\n+              [2, 2, 2, 2, 2, 0, 0, 0],\n+              [3, 3, 3, 3, 3, 0, 0, 0],\n+              [4, 4, 4, 4, 4, 0, 0, 0],\n+              [5, 5, 5, 5, 5, 0, 0, 0],\n+          ]),\n+          callback.reshard([src_one_two], \"8 8 2,4:0,8\"),\n+      )\n+\n+    self.assertAllEqual(\n+        tf_constant([\n+            [4, 4, 4, 4, 4, 0, 0, 0],\n+            [5, 5, 5, 5, 5, 0, 0, 0],\n+            [0, 0, 0, 0, 0, 0, 0, 0],\n+            [0, 0, 0, 0, 0, 0, 0, 0],\n+        ]),\n+        callback.reshard([src_one_two], \"8 8 4,4:0,8\"),\n+    )\n+\n+    with self.subTest(\"two_three\"):\n+      callback = sc_to_sc_adapter.get_reshard_callback(\"two_three\")\n+      self.assertEqual(callback.object_name(), \"two_three\")\n+      updated_keys, updated_slices = callback.update_restore_inputs(\n+          \"path/to/embedding/two_three/in/checkpoint\", \"24 8 8,6:0,8\"\n+      )\n+      self.assertAllEqual(\n+          updated_keys,\n+          [\n+              \"path/to/embedding/one_two/in/checkpoint\",\n+              \"path/to/embedding/three/in/checkpoint\",\n+          ],\n+      )\n+      self.assertAllEqual(\n+          updated_slices,\n+          [\"16 8 0,16:0,8\", \"16 8 0,16:0,8\"],\n+      )\n+\n+    self.assertAllEqual(\n+        tf_constant([\n+            #  shard 2\n+            [12, 12, 12, 12, 0, 0, 0, 0],\n+            [101, 101, 101, 0, 0, 0, 0, 0],\n+            [109, 109, 109, 0, 0, 0, 0, 0],\n+            #  shard 3\n+            [13, 13, 13, 13, 0, 0, 0, 0],\n+            [102, 102, 102, 0, 0, 0, 0, 0],\n+            [110, 110, 110, 0, 0, 0, 0, 0],\n+        ]),\n+        callback.reshard([src_one_two, src_three], \"24 8 6,6:0,8\"),\n+    )\n+\n+    self.assertAllEqual(\n+        tf_constant([\n+            #  shard 6\n+            [16, 16, 16, 16, 0, 0, 0, 0],\n+            [105, 105, 105, 0, 0, 0, 0, 0],\n+            [113, 113, 113, 0, 0, 0, 0, 0],\n+            #  shard 7\n+            [0, 0, 0, 0, 0, 0, 0, 0],\n+            [106, 106, 106, 0, 0, 0, 0, 0],\n+            [114, 114, 114, 0, 0, 0, 0, 0],\n+        ]),\n+        callback.reshard([src_one_two, src_three], \"24 8 18,6:0,8\"),\n+    )\n+\n \n if __name__ == \"__main__\":\n   v2_compat.enable_v2_behavior()"
        }
    ],
    "stats": {
        "total": 361,
        "additions": 309,
        "deletions": 52
    }
}