{
    "author": "EusebioDM",
    "message": "Add proto [de]serialization for `NormThunk`\n\nWe can re-create the GpuNormConfig from the GpuNormDescriptor, so we don't serialize the actual config. Also the cache and mutex don't need to be serialized.\n\nConsidered adding some fancy logic to get rid of all the repeated `if (has_field) { ASSIGN... }` lines, but in the end the more verbose but simpler code seemed more readable.\n\nPiperOrigin-RevId: 816654916",
    "sha": "502f86a0850038a1e6f6012dbf8176e91ff4a81e",
    "files": [
        {
            "sha": "328eccb386f92ef9f36a9729adb1ffeea39a5589",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -1540,6 +1540,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:gpu_norm_runner\",\n+        \"//xla/service/gpu:gpu_norm_runner_proto_cc\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:lazy_op_runner\",\n         \"//xla/stream_executor:stream\",\n@@ -1551,6 +1552,22 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"norm_thunk_test\",\n+    srcs = [\"norm_thunk_test.cc\"],\n+    deps = [\n+        \":norm_thunk\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n     ],\n )\n \n@@ -2256,6 +2273,8 @@ tf_proto_library(\n         \":dynamic_slice_thunk_proto\",\n         \"//xla:xla_data_proto\",\n         \"//xla/service:buffer_assignment_proto\",\n+        \"//xla/service/gpu:backend_configs\",\n+        \"//xla/service/gpu:gpu_norm_runner_proto\",\n         \"//xla/service/gpu:launch_dimensions_proto\",\n         \"//xla/stream_executor:launch_dim_proto\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt_proto\",\n@@ -2289,6 +2308,7 @@ cc_library(\n         \":infeed_thunk\",\n         \":kernel_thunk\",\n         \":memset_thunk\",\n+        \":norm_thunk\",\n         \":replica_id_thunk\",\n         \":sequential_thunk\",\n         \":thunk\","
        },
        {
            "sha": "cdb60a3ff5a8c1a04dfcb9441a550e05d9efd379",
            "filename": "third_party/xla/xla/backends/gpu/runtime/norm_thunk.cc",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -23,9 +23,11 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/gpu_norm_runner.h\"\n+#include \"xla/service/gpu/gpu_norm_runner.pb.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -148,5 +150,94 @@ absl::Status NormThunk::Initialize(const InitializeParams& params) {\n   return lazy_runner->GetOrCreateRunner(ln_config, params.stream).status();\n }\n \n+absl::StatusOr<std::unique_ptr<NormThunk>> NormThunk::FromProto(\n+    ThunkInfo thunk_info, const NormThunkProto& proto,\n+    absl::Span<const BufferAllocation> buffer_allocations) {\n+  TF_ASSIGN_OR_RETURN(GpuNormDescriptor descriptor,\n+                      GpuNormDescriptor::FromProto(proto.norm_descriptor()));\n+\n+  TF_ASSIGN_OR_RETURN(auto x, BufferAllocation::Slice::FromProto(\n+                                  proto.x(), buffer_allocations));\n+  TF_ASSIGN_OR_RETURN(auto scale, BufferAllocation::Slice::FromProto(\n+                                      proto.scale(), buffer_allocations));\n+  TF_ASSIGN_OR_RETURN(auto y_or_dx, BufferAllocation::Slice::FromProto(\n+                                        proto.y_or_dx(), buffer_allocations));\n+  std::optional<BufferAllocation::Slice> bias;\n+  if (proto.has_bias()) {\n+    TF_ASSIGN_OR_RETURN(bias, BufferAllocation::Slice::FromProto(\n+                                  proto.bias(), buffer_allocations));\n+  }\n+  std::optional<BufferAllocation::Slice> expectation;\n+  if (proto.has_expectation()) {\n+    TF_ASSIGN_OR_RETURN(expectation,\n+                        BufferAllocation::Slice::FromProto(proto.expectation(),\n+                                                           buffer_allocations));\n+  }\n+  std::optional<BufferAllocation::Slice> norm_factor;\n+  if (proto.has_norm_factor()) {\n+    TF_ASSIGN_OR_RETURN(norm_factor,\n+                        BufferAllocation::Slice::FromProto(proto.norm_factor(),\n+                                                           buffer_allocations));\n+  }\n+  std::optional<BufferAllocation::Slice> dy;\n+  if (proto.has_dy()) {\n+    TF_ASSIGN_OR_RETURN(\n+        dy, BufferAllocation::Slice::FromProto(proto.dy(), buffer_allocations));\n+  }\n+  std::optional<BufferAllocation::Slice> dscale;\n+  if (proto.has_dscale()) {\n+    TF_ASSIGN_OR_RETURN(dscale, BufferAllocation::Slice::FromProto(\n+                                    proto.dscale(), buffer_allocations));\n+  }\n+  std::optional<BufferAllocation::Slice> dbias;\n+  if (proto.has_dbias()) {\n+    TF_ASSIGN_OR_RETURN(dbias, BufferAllocation::Slice::FromProto(\n+                                   proto.dbias(), buffer_allocations));\n+  }\n+  TF_ASSIGN_OR_RETURN(auto scratch, BufferAllocation::Slice::FromProto(\n+                                        proto.scratch(), buffer_allocations));\n+\n+  return Create(std::move(thunk_info), descriptor, x, scale, y_or_dx, bias,\n+                expectation, norm_factor, dy, dscale, dbias, scratch);\n+}\n+\n+absl::StatusOr<ThunkProto> NormThunk::ToProto() const {\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+\n+  NormThunkProto* norm_proto = proto.mutable_norm_thunk();\n+  *norm_proto->mutable_norm_descriptor() = descriptor_.ToProto();\n+\n+  TF_ASSIGN_OR_RETURN(*norm_proto->mutable_x(), x_buffer_.ToProto());\n+  TF_ASSIGN_OR_RETURN(*norm_proto->mutable_scale(), scale_buffer_.ToProto());\n+  TF_ASSIGN_OR_RETURN(*norm_proto->mutable_y_or_dx(),\n+                      y_or_dx_buffer_.ToProto());\n+  if (bias_buffer_.has_value()) {\n+    TF_ASSIGN_OR_RETURN(*norm_proto->mutable_bias(), bias_buffer_->ToProto());\n+  }\n+  if (expectation_buffer_.has_value()) {\n+    TF_ASSIGN_OR_RETURN(*norm_proto->mutable_expectation(),\n+                        expectation_buffer_->ToProto());\n+  }\n+  if (norm_factor_buffer_.has_value()) {\n+    TF_ASSIGN_OR_RETURN(*norm_proto->mutable_norm_factor(),\n+                        norm_factor_buffer_->ToProto());\n+  }\n+  if (dy_buffer_.has_value()) {\n+    TF_ASSIGN_OR_RETURN(*norm_proto->mutable_dy(), dy_buffer_->ToProto());\n+  }\n+  if (dscale_buffer_.has_value()) {\n+    TF_ASSIGN_OR_RETURN(*norm_proto->mutable_dscale(),\n+                        dscale_buffer_->ToProto());\n+  }\n+  if (dbias_buffer_.has_value()) {\n+    TF_ASSIGN_OR_RETURN(*norm_proto->mutable_dbias(), dbias_buffer_->ToProto());\n+  }\n+  TF_ASSIGN_OR_RETURN(*norm_proto->mutable_scratch(),\n+                      scratch_buffer_.ToProto());\n+\n+  return proto;\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "d5468ec62c0140995e01903e749ad55250e6e909",
            "filename": "third_party/xla/xla/backends/gpu/runtime/norm_thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.h?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/gpu_norm_runner.h\"\n@@ -53,6 +54,12 @@ class NormThunk : public Thunk {\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n \n+  static absl::StatusOr<std::unique_ptr<NormThunk>> FromProto(\n+      ThunkInfo thunk_info, const NormThunkProto& proto,\n+      absl::Span<const BufferAllocation> buffer_allocations);\n+\n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n  private:\n   NormThunk(ThunkInfo thunk_info, GpuNormConfig config,\n             GpuNormDescriptor descriptor, BufferAllocation::Slice x,\n@@ -77,6 +84,9 @@ class NormThunk : public Thunk {\n   BufferAllocation::Slice scratch_buffer_;\n   NormRunner& GetOrCreateRunner(const stream_executor::Stream*);\n \n+  // Technically this is only needed during initialization to create the\n+  // GpuNormConfig, but the actual GpuNormConfig is hard to serialize. So we\n+  // keep the descriptor around for serialization purposes.\n   GpuNormDescriptor descriptor_;\n   GpuNormConfig config_;\n   absl::Mutex mu_;"
        },
        {
            "sha": "27b32a2945a63bff8044955453555b80bc4eeaab",
            "filename": "third_party/xla/xla/backends/gpu/runtime/norm_thunk_test.cc",
            "status": "added",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk_test.cc?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -0,0 +1,112 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/norm_thunk.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::tsl::proto_testing::EqualsProto;\n+using ::tsl::proto_testing::ParseTextProtoOrDie;\n+\n+TEST(NormThunkTest, ProtoRoundTrip) {\n+  auto proto = ParseTextProtoOrDie<ThunkProto>(R\"pb(\n+    thunk_info {\n+      profile_annotation: \"norm_thunk_profile\"\n+      execution_stream_id: 0\n+    }\n+    norm_thunk {\n+      norm_descriptor {\n+        backend_config {\n+          epsilon: 0.001\n+          kind: LAYER_FWD_INFER\n+          algorithm { algo_id: 0 is_cudnn_frontend: true }\n+        }\n+        x_shape {\n+          element_type: F32\n+          dimensions: [ 2, 3 ]\n+          layout {\n+            minor_to_major: [ 1, 0 ]\n+            tail_padding_alignment_in_elements: 1\n+          }\n+          is_dynamic_dimension: [ false, false ]\n+        }\n+        scale_shape {\n+          element_type: F32\n+          dimensions: [ 3 ]\n+          layout {\n+            minor_to_major: [ 0 ]\n+            tail_padding_alignment_in_elements: 1\n+          }\n+          is_dynamic_dimension: false\n+        }\n+        bias_shape {\n+          element_type: F32\n+          dimensions: [ 3 ]\n+          layout {\n+            minor_to_major: [ 0 ]\n+            tail_padding_alignment_in_elements: 1\n+          }\n+          is_dynamic_dimension: false\n+        }\n+        y_or_dx_shape {\n+          element_type: F32\n+          dimensions: [ 2, 3 ]\n+          layout {\n+            minor_to_major: [ 1, 0 ]\n+            tail_padding_alignment_in_elements: 1\n+          }\n+          is_dynamic_dimension: [ false, false ]\n+        }\n+        scratch_size: 1024\n+      }\n+      x { offset: 0 size: 24 buffer_allocation_index: 0 }\n+      scale { offset: 0 size: 12 buffer_allocation_index: 1 }\n+      y_or_dx { offset: 0 size: 24 buffer_allocation_index: 2 }\n+      bias { offset: 0 size: 12 buffer_allocation_index: 3 }\n+      scratch { offset: 0 size: 1024 buffer_allocation_index: 4 }\n+    }\n+  )pb\");\n+\n+  std::vector<BufferAllocation> buffer_allocations;\n+  buffer_allocations.emplace_back(/*index=*/0, /*size=*/24, /*color=*/0);\n+  buffer_allocations.emplace_back(/*index=*/1, /*size=*/12, /*color=*/0);\n+  buffer_allocations.emplace_back(/*index=*/2, /*size=*/24, /*color=*/0);\n+  buffer_allocations.emplace_back(/*index=*/3, /*size=*/12, /*color=*/0);\n+  buffer_allocations.emplace_back(/*index=*/4, /*size=*/1024, /*color=*/0);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Thunk::ThunkInfo thunk_info,\n+                          Thunk::ThunkInfo::FromProto(proto.thunk_info()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<NormThunk> thunk,\n+      NormThunk::FromProto(thunk_info, proto.norm_thunk(), buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n+  EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "46ee11139601964a10f949544fa79847123c7ec9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -19,6 +19,8 @@ package xla.gpu;\n \n import \"xla/backends/gpu/runtime/dynamic_slice_thunk.proto\";\n import \"xla/service/buffer_assignment.proto\";\n+import \"xla/service/gpu/backend_configs.proto\";\n+import \"xla/service/gpu/gpu_norm_runner.proto\";\n import \"xla/service/gpu/launch_dimensions.proto\";\n import \"xla/stream_executor/gpu/gpu_blas_lt.proto\";\n import \"xla/stream_executor/gpu/tma_metadata.proto\";\n@@ -170,6 +172,20 @@ message CublasLtMatmulThunkProto {\n   optional xla.buffer_assignment.BufferAllocationSliceProto workspace = 16;\n }\n \n+message NormThunkProto {\n+  GpuNormDescriptorProto norm_descriptor = 1;\n+  xla.buffer_assignment.BufferAllocationSliceProto x = 2;\n+  xla.buffer_assignment.BufferAllocationSliceProto scale = 3;\n+  xla.buffer_assignment.BufferAllocationSliceProto y_or_dx = 4;\n+  optional xla.buffer_assignment.BufferAllocationSliceProto bias = 5;\n+  optional xla.buffer_assignment.BufferAllocationSliceProto expectation = 6;\n+  optional xla.buffer_assignment.BufferAllocationSliceProto norm_factor = 7;\n+  optional xla.buffer_assignment.BufferAllocationSliceProto dy = 8;\n+  optional xla.buffer_assignment.BufferAllocationSliceProto dscale = 9;\n+  optional xla.buffer_assignment.BufferAllocationSliceProto dbias = 10;\n+  xla.buffer_assignment.BufferAllocationSliceProto scratch = 11;\n+}\n+\n message ThunkProto {\n   ThunkInfoProto thunk_info = 1;\n \n@@ -196,6 +212,7 @@ message ThunkProto {\n     InfeedThunkProto infeed_thunk = 21;\n     CublasLtMatmulThunkProto cublas_lt_matmul_thunk = 22;\n     OutfeedThunkProto outfeed_thunk = 23;\n+    NormThunkProto norm_thunk = 24;\n   }\n }\n "
        },
        {
            "sha": "2bd108df72cce05ff44bd4e889f34ba3ec4ed630",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/infeed_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/memset_thunk.h\"\n+#include \"xla/backends/gpu/runtime/norm_thunk.h\"\n #include \"xla/backends/gpu/runtime/replica_id_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -157,6 +158,10 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProto(\n                                           thunk_proto.cublas_lt_matmul_thunk(),\n                                           buffer_allocations);\n   }\n+  if (thunk_proto.has_norm_thunk()) {\n+    return NormThunk::FromProto(std::move(thunk_info), thunk_proto.norm_thunk(),\n+                                buffer_allocations);\n+  }\n \n   std::optional<absl::string_view> unsupported_thunk_type =\n       GetStoredThunkTypeName(thunk_proto);"
        },
        {
            "sha": "ee48a34a16a9b2ef1b0947cf7b0770b32bb0979b",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -1206,6 +1206,7 @@ cc_library(\n     deps = [\n         \":backend_configs_cc\",\n         \":cublas_cudnn\",\n+        \":gpu_norm_runner_proto_cc\",\n         \":stream_executor_util\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n@@ -1220,6 +1221,28 @@ cc_library(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"gpu_norm_runner_test\",\n+    srcs = [\"gpu_norm_runner_test.cc\"],\n+    deps = [\n+        \":gpu_norm_runner\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+tf_proto_library(\n+    name = \"gpu_norm_runner_proto\",\n+    srcs = [\"gpu_norm_runner.proto\"],\n+    deps = [\n+        \":backend_configs\",\n+        \"//xla:xla_data_proto\",\n+    ],\n+)\n+\n tf_proto_library(\n     name = \"fusion_process_dump_proto\",\n     srcs = [\"fusion_process_dump.proto\"],"
        },
        {
            "sha": "2a8890d8b7d88f5fc09e6a13ff6e49bbf42a1867",
            "filename": "third_party/xla/xla/service/gpu/gpu_norm_runner.cc",
            "status": "modified",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.cc?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -19,8 +19,11 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/service/gpu/gpu_norm_runner.pb.h\"\n+#include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/lazy_op_runner.h\"\n@@ -76,5 +79,71 @@ absl::Status RunGpuNorm(const gpu::GpuNormConfig& config,\n   return (*runner)(stream, options.profile_result, scratch_memory, operands);\n }\n \n+GpuNormDescriptorProto GpuNormDescriptor::ToProto() const {\n+  GpuNormDescriptorProto proto;\n+  *proto.mutable_backend_config() = backend_config;\n+  *proto.mutable_x_shape() = x_shape.ToProto();\n+  *proto.mutable_scale_shape() = scale_shape.ToProto();\n+  if (bias_shape.has_value()) {\n+    *proto.mutable_bias_shape() = bias_shape->ToProto();\n+  }\n+  *proto.mutable_y_or_dx_shape() = y_or_dx_shape.ToProto();\n+  if (expectation_shape.has_value()) {\n+    *proto.mutable_expectation_shape() = expectation_shape->ToProto();\n+  }\n+  if (norm_factor_shape.has_value()) {\n+    *proto.mutable_norm_factor_shape() = norm_factor_shape->ToProto();\n+  }\n+  if (dy_shape.has_value()) {\n+    *proto.mutable_dy_shape() = dy_shape->ToProto();\n+  }\n+  if (dscale_shape.has_value()) {\n+    *proto.mutable_dscale_shape() = dscale_shape->ToProto();\n+  }\n+  if (dbias_shape.has_value()) {\n+    *proto.mutable_dbias_shape() = dbias_shape->ToProto();\n+  }\n+  proto.set_scratch_size(scratch_size);\n+  return proto;\n+}\n+\n+absl::StatusOr<GpuNormDescriptor> GpuNormDescriptor::FromProto(\n+    const GpuNormDescriptorProto& proto) {\n+  GpuNormDescriptor descriptor;\n+  descriptor.backend_config = proto.backend_config();\n+\n+  TF_ASSIGN_OR_RETURN(descriptor.x_shape, Shape::FromProto(proto.x_shape()));\n+  TF_ASSIGN_OR_RETURN(descriptor.scale_shape,\n+                      Shape::FromProto(proto.scale_shape()));\n+  if (proto.has_bias_shape()) {\n+    TF_ASSIGN_OR_RETURN(descriptor.bias_shape,\n+                        Shape::FromProto(proto.bias_shape()));\n+  }\n+  TF_ASSIGN_OR_RETURN(descriptor.y_or_dx_shape,\n+                      Shape::FromProto(proto.y_or_dx_shape()));\n+  if (proto.has_expectation_shape()) {\n+    TF_ASSIGN_OR_RETURN(descriptor.expectation_shape,\n+                        Shape::FromProto(proto.expectation_shape()));\n+  }\n+  if (proto.has_norm_factor_shape()) {\n+    TF_ASSIGN_OR_RETURN(descriptor.norm_factor_shape,\n+                        Shape::FromProto(proto.norm_factor_shape()));\n+  }\n+  if (proto.has_dy_shape()) {\n+    TF_ASSIGN_OR_RETURN(descriptor.dy_shape,\n+                        Shape::FromProto(proto.dy_shape()));\n+  }\n+  if (proto.has_dscale_shape()) {\n+    TF_ASSIGN_OR_RETURN(descriptor.dscale_shape,\n+                        Shape::FromProto(proto.dscale_shape()));\n+  }\n+  if (proto.has_dbias_shape()) {\n+    TF_ASSIGN_OR_RETURN(descriptor.dbias_shape,\n+                        Shape::FromProto(proto.dbias_shape()));\n+  }\n+  descriptor.scratch_size = proto.scratch_size();\n+  return descriptor;\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "23c9f5dc5cdb1949bc8e175a9cd13ca0c473831d",
            "filename": "third_party/xla/xla/service/gpu/gpu_norm_runner.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.h?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/service/gpu/gpu_norm_runner.pb.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -66,6 +67,11 @@ struct GpuNormDescriptor {\n   std::optional<Shape> dscale_shape;\n   std::optional<Shape> dbias_shape;\n   size_t scratch_size;\n+\n+  static absl::StatusOr<GpuNormDescriptor> FromProto(\n+      const GpuNormDescriptorProto& proto);\n+\n+  GpuNormDescriptorProto ToProto() const;\n };\n \n // Structure to describe static properties of a fused norm op."
        },
        {
            "sha": "f76cb2415d94988845f821fb71be6c8541fb604b",
            "filename": "third_party/xla/xla/service/gpu/gpu_norm_runner.proto",
            "status": "added",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner.proto?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -0,0 +1,35 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+syntax = \"proto3\";\n+\n+package xla.gpu;\n+\n+import \"xla/service/gpu/backend_configs.proto\";\n+import \"xla/xla_data.proto\";\n+\n+message GpuNormDescriptorProto {\n+  xla.gpu.CudnnNormBackendConfig backend_config = 1;\n+  xla.ShapeProto x_shape = 2;\n+  xla.ShapeProto scale_shape = 3;\n+  optional xla.ShapeProto bias_shape = 4;\n+  xla.ShapeProto y_or_dx_shape = 5;\n+  optional xla.ShapeProto expectation_shape = 6;\n+  optional xla.ShapeProto norm_factor_shape = 7;\n+  optional xla.ShapeProto dy_shape = 8;\n+  optional xla.ShapeProto dscale_shape = 9;\n+  optional xla.ShapeProto dbias_shape = 10;\n+  int64 scratch_size = 11;\n+}"
        },
        {
            "sha": "0bb36068ac87af6cb71ccc8789f605cfdb6dafce",
            "filename": "third_party/xla/xla/service/gpu/gpu_norm_runner_test.cc",
            "status": "added",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502f86a0850038a1e6f6012dbf8176e91ff4a81e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_norm_runner_test.cc?ref=502f86a0850038a1e6f6012dbf8176e91ff4a81e",
            "patch": "@@ -0,0 +1,83 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/gpu_norm_runner.h\"\n+\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+using ::tsl::proto_testing::EqualsProto;\n+using ::tsl::proto_testing::ParseTextProtoOrDie;\n+\n+TEST(GpuNormRunnerTest, GpuNormDescriptorToFromProto) {\n+  auto descriptor_proto = ParseTextProtoOrDie<GpuNormDescriptorProto>(R\"pb(\n+    backend_config {\n+      epsilon: 0.001\n+      kind: LAYER_FWD_INFER\n+      algorithm { algo_id: 0 is_cudnn_frontend: true }\n+    }\n+    x_shape {\n+      element_type: F32\n+      dimensions: [ 2, 3 ]\n+      layout {\n+        minor_to_major: [ 1, 0 ]\n+        tail_padding_alignment_in_elements: 1\n+      }\n+      is_dynamic_dimension: [ false, false ]\n+    }\n+    scale_shape {\n+      element_type: F32\n+      dimensions: [ 3 ]\n+      layout {\n+        minor_to_major: [ 0 ]\n+        tail_padding_alignment_in_elements: 1\n+      }\n+      is_dynamic_dimension: false\n+    }\n+    bias_shape {\n+      element_type: F32\n+      dimensions: [ 3 ]\n+      layout {\n+        minor_to_major: [ 0 ]\n+        tail_padding_alignment_in_elements: 1\n+      }\n+      is_dynamic_dimension: false\n+    }\n+    y_or_dx_shape {\n+      element_type: F32\n+      dimensions: [ 2, 3 ]\n+      layout {\n+        minor_to_major: [ 1, 0 ]\n+        tail_padding_alignment_in_elements: 1\n+      }\n+      is_dynamic_dimension: [ false, false ]\n+    }\n+    scratch_size: 1024\n+  )pb\");\n+\n+  TF_ASSERT_OK_AND_ASSIGN(GpuNormDescriptor descriptor,\n+                          GpuNormDescriptor::FromProto(descriptor_proto));\n+  EXPECT_THAT(descriptor.ToProto(), EqualsProto(descriptor_proto));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 471,
        "additions": 471,
        "deletions": 0
    }
}