{
    "author": "Varcho",
    "message": "[ReplicaGroupV3][Refactor][5/n] Update partitioner code to use CollectiveDeviceList (V1 replica group) in place of vector<vector<int>>.\n\nPiperOrigin-RevId: 848093189",
    "sha": "44359ffdde6f016a4aca25bceb8a90cee3780c82",
    "files": [
        {
            "sha": "e380f36be9305207792128c5f6cbe5cbeca99db1",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=44359ffdde6f016a4aca25bceb8a90cee3780c82",
            "patch": "@@ -293,7 +293,7 @@ DotDimensionIndexMapping ComputeDimensionIndexMapping(\n                                   output_to_lhs_indices, output_to_rhs_indices};\n }\n \n-std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n+CollectiveDeviceList GetPartitionGroupsForReplication(\n     const HloSharding& sharding, absl::Span<const int64_t> replication_dims) {\n   int64_t group_size = 1;\n   for (int64_t i : replication_dims) {\n@@ -312,7 +312,7 @@ std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n         }\n         partition_groups[group_id].push_back(partition);\n       });\n-  return partition_groups;\n+  return CollectiveDeviceList(partition_groups);\n }\n \n // Returns true iff all of the following conditions are simultaneously true:\n@@ -3395,18 +3395,16 @@ bool PrioritizeContractingDimensionsPartitioning(\n        other_non_contracting_dims) {\n     ag_replication_dims.push_back(lhs_matching_iterations ? dim.rhs : dim.lhs);\n   }\n+\n   auto all_gather_subgroups =\n       GetPartitionGroupsForReplication(other_sharding, ag_replication_dims);\n   auto reduce_scatter_subgroups = GetPartitionGroupsForReplication(\n       outer_output_tmp_sharding, output_slice_dims);\n   const double all_gather_time_in_ms = visitor->GetCommunicationTimeInMilliSec(\n-      all_gather_bytes,\n-      CollectiveDeviceList(visitor->CreateReplicaGroups(all_gather_subgroups)));\n+      all_gather_bytes, all_gather_subgroups);\n   const double reduce_scatter_time_in_ms =\n-      visitor->GetCommunicationTimeInMilliSec(\n-          reduce_scatter_bytes,\n-          CollectiveDeviceList(\n-              visitor->CreateReplicaGroups(reduce_scatter_subgroups)));\n+      visitor->GetCommunicationTimeInMilliSec(reduce_scatter_bytes,\n+                                              reduce_scatter_subgroups);\n \n   Shape other_original_shape = other_hlo->shape();\n   *other_hlo->mutable_shape() ="
        },
        {
            "sha": "34c599c3ac955745fb0b49c694dbeb86a44b2861",
            "filename": "third_party/xla/xla/service/spmd/dot_handler_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc?ref=44359ffdde6f016a4aca25bceb8a90cee3780c82",
            "patch": "@@ -138,8 +138,8 @@ HloModule test\n ENTRY main {\n   Arg_0 = bf16[2048,24576]{1,0} parameter(0), sharding={devices=[1,4]<=[4]}\n   Arg_1 = bf16[24576,98304]{1,0} parameter(1), sharding={devices=[4,1]<=[4]}\n-  ROOT dot = bf16[2048,98304]{1,0} dot(Arg_0, Arg_1), \n-    lhs_contracting_dims={1}, rhs_contracting_dims={0}, \n+  ROOT dot = bf16[2048,98304]{1,0} dot(Arg_0, Arg_1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     sharding={devices=[1,4]<=[4]}\n }\n )\";\n@@ -183,9 +183,9 @@ HloModule test\n ENTRY main {\n   Arg_0 = bf16[8,2048,256]{2,1,0} parameter(0), sharding={devices=[4,1,1]<=[4]}\n   Arg_1 = bf16[8,256,512]{2,1,0} parameter(1), sharding={devices=[4,1,1]<=[4]}\n-  ROOT dot = bf16[8,2048,512]{2,1,0} dot(Arg_0, Arg_1), \n+  ROOT dot = bf16[8,2048,512]{2,1,0} dot(Arg_0, Arg_1),\n     lhs_batch_dims={0}, rhs_batch_dims={0},\n-    lhs_contracting_dims={2}, rhs_contracting_dims={1}, \n+    lhs_contracting_dims={2}, rhs_contracting_dims={1},\n     sharding={devices=[4,1,1]<=[4]}\n }\n )\";\n@@ -218,8 +218,8 @@ HloModule test\n ENTRY main {\n   Arg_0 = bf16[128,256]{1,0} parameter(0), sharding={devices=[1,16]<=[16]}\n   Arg_1 = bf16[256,512]{1,0} parameter(1), sharding={devices=[16,1]<=[16]}\n-  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1), \n-    lhs_contracting_dims={1}, rhs_contracting_dims={0}, \n+  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     sharding={devices=[1,16]<=[16]}\n }\n )\";\n@@ -243,8 +243,8 @@ HloModule test\n ENTRY main {\n   Arg_0 = bf16[128,256]{1,0} parameter(0), sharding={devices=[1,32]<=[32]}\n   Arg_1 = bf16[256,512]{1,0} parameter(1), sharding={devices=[32,1]<=[32]}\n-  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1), \n-    lhs_contracting_dims={1}, rhs_contracting_dims={0}, \n+  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     sharding={devices=[1,32]<=[32]}\n }\n )\";\n@@ -269,8 +269,8 @@ HloModule test\n ENTRY main {\n   Arg_0 = bf16[128,256]{1,0} parameter(0), sharding={devices=[1,64]<=[64]}\n   Arg_1 = bf16[256,512]{1,0} parameter(1), sharding={devices=[64,1]<=[64]}\n-  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1), \n-    lhs_contracting_dims={1}, rhs_contracting_dims={0}, \n+  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     sharding={devices=[1,64]<=[64]}\n }\n )\";\n@@ -296,8 +296,8 @@ HloModule test\n ENTRY main {\n   Arg_0 = bf16[128,256]{1,0} parameter(0), sharding={devices=[1,8]<=[8]}\n   Arg_1 = bf16[256,512]{1,0} parameter(1), sharding={devices=[8,1]<=[8]}\n-  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1), \n-    lhs_contracting_dims={1}, rhs_contracting_dims={0}, \n+  ROOT dot = bf16[128,512]{1,0} dot(Arg_0, Arg_1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     sharding={devices=[1,8]<=[8]}\n }\n )\";"
        },
        {
            "sha": "fc233f51925933865f7d84ebbf7c0feb51c088e8",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=44359ffdde6f016a4aca25bceb8a90cee3780c82",
            "patch": "@@ -1750,14 +1750,13 @@ PartitionedHlo PartitionedHlo::ReshardWithAllToAll(\n     VLOG(5) << \"Falling back to creating all-to-all with replica groups V1 \"\n                \"(list of vectors).\";\n     // The order of ids in the group must follow the temp_target sharding.\n-    std::vector<std::vector<int64_t>> groups =\n-        GetPartitionGroupsAcrossTargetDims(temp_target, {target_dim},\n-                                           {group_size});\n+    CollectiveDeviceList groups = GetPartitionGroupsAcrossTargetDims(\n+        temp_target, {target_dim}, {group_size});\n     // After the reshape, it is guaranteed to have at least 3 dimensions.\n     all_to_all =\n         state_.collective_ops_creator.create_cross_partition_all_to_all(\n-            state_.b, {reshape}, groups, (*state_.next_channel_id)++,\n-            target_dim);\n+            state_.b, {reshape}, groups.flattened_replica_groups(),\n+            (*state_.next_channel_id)++, target_dim);\n   }\n   CHECK_NE(all_to_all, nullptr);\n \n@@ -1939,12 +1938,12 @@ PartitionedHlo PartitionedHlo::TryMultipleSourceTargetDims(\n   } else {\n     VLOG(5) << \"Falling back to creating all-to-all with replica groups V1 \"\n                \"(list of vectors).\";\n-    std::vector<std::vector<int64_t>> groups =\n-        GetPartitionGroupsAcrossTargetDims(temp_target, eligible_target_dims,\n-                                           group_sizes);\n+    CollectiveDeviceList groups = GetPartitionGroupsAcrossTargetDims(\n+        temp_target, eligible_target_dims, group_sizes);\n     all_to_all =\n         state_.collective_ops_creator.create_cross_partition_all_to_all(\n-            state_.b, {reshape_1}, groups, (*state_.next_channel_id)++, 0);\n+            state_.b, {reshape_1}, groups.flattened_replica_groups(),\n+            (*state_.next_channel_id)++, 0);\n   }\n   // Step 3. Split sharding axes to multiple dimensions\n   // 1. reshape_2 (8,16,8,16,8) -> (2,4,16,8,16,8)\n@@ -5209,9 +5208,12 @@ SpmdPartitioner::AllGatherShardsInternal(\n         auto partition_subgroups =\n             GetPartitionGroupsForReplication(sharding, {*it});\n         result_shape.set_dimensions(\n-            *it, result_shape.dimensions(*it) * partition_subgroups[0].size());\n+            *it, result_shape.dimensions(*it) *\n+                     partition_subgroups.num_devices_per_group());\n         result = collectives_creator.create_cross_partition_all_gather(\n-            b, result, result_shape, partition_subgroups, (*next_channel_id)++,\n+            b, result, result_shape,\n+            partition_subgroups.flattened_replica_groups(),\n+            (*next_channel_id)++,\n             /*all_gather_dimension=*/*it);\n       }\n     }\n@@ -5247,10 +5249,10 @@ SpmdPartitioner::AllGatherShardsInternal(\n   } else {\n     auto partition_subgroups =\n         GetPartitionGroupsForReplication(sharding, selected_dims);\n-    shape[0] *= partition_subgroups[0].size();\n+    shape[0] *= partition_subgroups.num_devices_per_group();\n     result = collectives_creator.create_cross_partition_all_gather(\n         b, result, ShapeUtil::MakeShape(operand->shape().element_type(), shape),\n-        partition_subgroups, (*next_channel_id)++,\n+        partition_subgroups.flattened_replica_groups(), (*next_channel_id)++,\n         /*all_gather_dimension=*/0);\n   }\n   ag = result;\n@@ -5339,7 +5341,8 @@ HloInstruction* SpmdPartitioner::AllReduceAlongShardingDimsInternal(\n     auto partition_subgroups =\n         GetPartitionGroupsForReplication(sharding, selected_dims);\n     return collectives_creator.create_cross_partition_all_reduce(\n-        b, operand, reduction, partition_subgroups, (*next_channel_id)++);\n+        b, operand, reduction, partition_subgroups.flattened_replica_groups(),\n+        (*next_channel_id)++);\n   }\n \n   auto result = operand;\n@@ -5362,7 +5365,8 @@ HloInstruction* SpmdPartitioner::AllReduceAlongShardingDimsInternal(\n       auto partition_subgroups =\n           GetPartitionGroupsForReplication(sharding, {*it});\n       result = collectives_creator.create_cross_partition_all_reduce(\n-          b, result, reduction, partition_subgroups, (*next_channel_id)++);\n+          b, result, reduction, partition_subgroups.flattened_replica_groups(),\n+          (*next_channel_id)++);\n     }\n   }\n   return result;"
        },
        {
            "sha": "c2b418440c1cc0d581511691843a27a7d9669f65",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=44359ffdde6f016a4aca25bceb8a90cee3780c82",
            "patch": "@@ -2895,7 +2895,7 @@ HloInstruction* PadDataFromWindowReshard(\n   return sharded_data;\n }\n \n-std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n+CollectiveDeviceList GetPartitionGroupsForReplication(\n     const HloSharding& sharding, absl::Span<const int64_t> replication_dims) {\n   absl::Span<const int64_t> sharding_dims = sharding.dimensions();\n   DCHECK_GE(sharding_dims.size(), replication_dims.size());\n@@ -2939,10 +2939,10 @@ std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n         DCHECK_LT(group_id, partition_groups.size());\n         partition_groups[group_id].push_back(partition);\n       });\n-  return partition_groups;\n+  return CollectiveDeviceList(partition_groups);\n }\n \n-std::vector<std::vector<int64_t>> GetPartitionGroupsAcrossTargetDims(\n+CollectiveDeviceList GetPartitionGroupsAcrossTargetDims(\n     const HloSharding& sharding, std::vector<int64_t> target_dims,\n     std::vector<int64_t> group_sizes) {\n   CHECK(target_dims.size() == group_sizes.size());\n@@ -2966,7 +2966,7 @@ std::vector<std::vector<int64_t>> GetPartitionGroupsAcrossTargetDims(\n     }\n     groups[group_id].push_back(device);\n   });\n-  return groups;\n+  return CollectiveDeviceList(groups);\n }\n \n std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsAcrossTargetDims("
        },
        {
            "sha": "ff3ffa5e80d7f6ea21c3f3a31b95097033778f7d",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.h?ref=44359ffdde6f016a4aca25bceb8a90cee3780c82",
            "patch": "@@ -595,13 +595,13 @@ HloInstruction* PadDataFromWindowReshard(\n \n // Generates partition groups (groups of devices that will communicate via a\n // collective) from sharding and provided replication_dims.\n-std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n+CollectiveDeviceList GetPartitionGroupsForReplication(\n     const HloSharding& sharding, absl::Span<const int64_t> replication_dims);\n \n // Generates partition groups (groups of devices that will communicate via a\n // collective) across provided target dims with provided group sizes in vector\n // of vector format (legacy format).\n-std::vector<std::vector<int64_t>> GetPartitionGroupsAcrossTargetDims(\n+CollectiveDeviceList GetPartitionGroupsAcrossTargetDims(\n     const HloSharding& sharding, std::vector<int64_t> target_dims,\n     std::vector<int64_t> group_sizes);\n "
        },
        {
            "sha": "06e90b3061a8e3fe42e7bc555f0bd3215047e786",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44359ffdde6f016a4aca25bceb8a90cee3780c82/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util_test.cc?ref=44359ffdde6f016a4aca25bceb8a90cee3780c82",
            "patch": "@@ -77,21 +77,21 @@ TEST(SPMDPartitionerUtilTest, PartialReplicateReshardCompatibleSharding2) {\n \n TEST(SPMDPartitionerUtilTest, GetPartitionGroupsForReplication) {\n   HloSharding sharding = HloSharding::IotaTile({2, 2, 2});\n-  std::vector<std::vector<int64_t>> actual_partition_groups =\n+  CollectiveDeviceList actual_partition_groups =\n       GetPartitionGroupsForReplication(sharding, {1});\n   std::vector<std::vector<int64_t>> expected_partition_groups = {\n       {0, 2}, {1, 3}, {4, 6}, {5, 7}};\n-  EXPECT_THAT(actual_partition_groups,\n+  EXPECT_THAT(actual_partition_groups.flattened_replica_groups(),\n               testing::ContainerEq(expected_partition_groups));\n }\n \n TEST(SPMDPartitionerUtilTest, GetPartitionGroupsForReplication2) {\n   HloSharding sharding = HloSharding::IotaTile({2, 2, 2}, {2, 2, 2}, {0, 2, 1});\n-  std::vector<std::vector<int64_t>> actual_partition_groups =\n+  CollectiveDeviceList actual_partition_groups =\n       GetPartitionGroupsForReplication(sharding, {0, 2});\n   std::vector<std::vector<int64_t>> expected_partition_groups = {{0, 2, 4, 6},\n                                                                  {1, 3, 5, 7}};\n-  EXPECT_THAT(actual_partition_groups,\n+  EXPECT_THAT(actual_partition_groups.flattened_replica_groups(),\n               testing::ContainerEq(expected_partition_groups));\n }\n "
        }
    ],
    "stats": {
        "total": 92,
        "additions": 47,
        "deletions": 45
    }
}