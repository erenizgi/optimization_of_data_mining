{
    "author": "ezhulenev",
    "message": "[xla:pjrt] Migtate PjRt interpreter to xla::Future and xla::Promise\n\nPiperOrigin-RevId: 812102331",
    "sha": "241a71590be928b02c3bf7c614c3848f1986ea9f",
    "files": [
        {
            "sha": "25477d26ab422262acd851c99bb12f6acf41682b",
            "filename": "third_party/xla/xla/pjrt/interpreter/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/241a71590be928b02c3bf7c614c3848f1986ea9f/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/241a71590be928b02c3bf7c614c3848f1986ea9f/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2FBUILD?ref=241a71590be928b02c3bf7c614c3848f1986ea9f",
            "patch": "@@ -14,6 +14,7 @@ cc_library(\n     hdrs = [\"interpreter_client.h\"],\n     visibility = internal_visibility([\"//xla:friends\"]),\n     deps = [\n+        \"//xla:future\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n         \"//xla:util\","
        },
        {
            "sha": "27d7fab4b7e2a904a59bd74b10c17ffb4c71744b",
            "filename": "third_party/xla/xla/pjrt/interpreter/interpreter_client.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/241a71590be928b02c3bf7c614c3848f1986ea9f/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/241a71590be928b02c3bf7c614c3848f1986ea9f/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.cc?ref=241a71590be928b02c3bf7c614c3848f1986ea9f",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"xla/client/executable_build_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/hlo/evaluator/hlo_evaluator.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -48,7 +49,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_common.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/utils.h\"\n #include \"xla/service/batchnorm_expander.h\"\n #include \"xla/service/computation_placer.h\"\n@@ -188,7 +188,7 @@ absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>>\n InterpreterLoadedExecutable::Execute(\n     absl::Span<const std::vector<PjRtBuffer*>> argument_handles,\n     const ExecuteOptions& options,\n-    std::optional<std::vector<PjRtFuture<>>>& returned_futures) const {\n+    std::optional<std::vector<Future<>>>& returned_futures) const {\n   if (device_assignment_ == nullptr) {\n     return absl::InvalidArgumentError(\n         \"Execute expects a non-null device_assignment\");\n@@ -207,7 +207,7 @@ InterpreterLoadedExecutable::Execute(\n                         addressable_devices_.size()));\n   }\n \n-  std::optional<PjRtFuture<>> returned_future;\n+  std::optional<Future<>> returned_future;\n   TF_ASSIGN_OR_RETURN(\n       std::vector<std::unique_ptr<PjRtBuffer>> replica_result,\n       ExecuteSharded(argument_handles[0], addressable_devices_[0], options,\n@@ -218,15 +218,15 @@ InterpreterLoadedExecutable::Execute(\n     CHECK(returned_future.has_value())\n         << \"returned_future must be set because ExecuteSharded was called with \"\n            \"fill_future=true.\";\n-    returned_futures = std::vector<PjRtFuture<>>({*std::move(returned_future)});\n+    returned_futures = std::vector<Future<>>({*std::move(returned_future)});\n   }\n   return result;\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n InterpreterLoadedExecutable::ExecuteSharded(\n     absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-    const ExecuteOptions& options, std::optional<PjRtFuture<>>& returned_future,\n+    const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n     bool fill_future) const {\n   if (device_assignment_ == nullptr) {\n     return absl::InvalidArgumentError(\n@@ -274,7 +274,7 @@ InterpreterLoadedExecutable::ExecuteSharded(\n   // Shrink the generated dynamic shape into static shape.\n   result_literal = result_literal.ToStatic();\n   if (fill_future) {\n-    returned_future = PjRtFuture<>(absl::OkStatus());\n+    returned_future = Future<>(absl::OkStatus());\n   }\n \n   TF_ASSIGN_OR_RETURN(PjRtMemorySpace * memory_space,\n@@ -306,7 +306,7 @@ InterpreterLoadedExecutable::ExecuteSharded(\n absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n InterpreterLoadedExecutable::ExecutePortable(\n     absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-    const ExecuteOptions& options, std::optional<PjRtFuture<>>& returned_future,\n+    const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n     bool fill_future) const {\n   return absl::UnimplementedError(\"ExecutePortable is not implemented\");\n }"
        },
        {
            "sha": "409db1b20e4d4ef1b26031dcd2e49ebd7e1ab6bf",
            "filename": "third_party/xla/xla/pjrt/interpreter/interpreter_client.h",
            "status": "modified",
            "additions": 15,
            "deletions": 20,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/241a71590be928b02c3bf7c614c3848f1986ea9f/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/241a71590be928b02c3bf7c614c3848f1986ea9f/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.h?ref=241a71590be928b02c3bf7c614c3848f1986ea9f",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/hlo/evaluator/hlo_evaluator.h\"\n #include \"xla/hlo/evaluator/hlo_evaluator_interface.h\"\n@@ -198,8 +199,8 @@ class InterpreterLiteralWrapperBuffer final : public PjRtBuffer {\n         \"InterpreterLiteralWrapperBuffer.\");\n   }\n \n-  PjRtFuture<> ToLiteral(MutableLiteralBase* literal) override {\n-    return PjRtFuture<>(ShapeUtil::ForEachSubshapeWithStatus(\n+  Future<> ToLiteral(MutableLiteralBase* literal) override {\n+    return Future<>(ShapeUtil::ForEachSubshapeWithStatus(\n         literal_.shape(),\n         [&](const Shape& subshape, const ShapeIndex& index) -> absl::Status {\n           if (!subshape.IsArray()) {\n@@ -220,15 +221,14 @@ class InterpreterLiteralWrapperBuffer final : public PjRtBuffer {\n         }));\n   }\n \n-  PjRtFuture<> LazyToLiteral(\n-      absl::AnyInvocable<PjRtFuture<MutableLiteralBase*>() &&> generator)\n-      override {\n+  Future<> LazyToLiteral(\n+      absl::AnyInvocable<Future<MutableLiteralBase*>() &&> generator) override {\n     // Underlying buffer is always ready, so we can immediately call the\n     // generator.\n-    PjRtFuture<MutableLiteralBase*> future = std::move(generator)();\n+    Future<MutableLiteralBase*> future = std::move(generator)();\n     const absl::StatusOr<MutableLiteralBase*>& literal = future.Await();\n     if (!literal.ok()) {\n-      return PjRtFuture<>(literal.status());\n+      return Future<>(literal.status());\n     }\n     return ToLiteral(*literal);\n   }\n@@ -237,9 +237,9 @@ class InterpreterLiteralWrapperBuffer final : public PjRtBuffer {\n     return literal_.size_bytes();\n   }\n \n-  PjRtFuture<> CopyRawToHost(void* dst, int64_t offset,\n-                             int64_t transfer_size) override {\n-    return PjRtFuture<>(absl::UnimplementedError(\n+  Future<> CopyRawToHost(void* dst, int64_t offset,\n+                         int64_t transfer_size) override {\n+    return Future<>(absl::UnimplementedError(\n         \"CopyRawToHost not supported by InterpreterLiteralWrapperBuffer.\"));\n   }\n \n@@ -268,15 +268,13 @@ class InterpreterLiteralWrapperBuffer final : public PjRtBuffer {\n         \"InterpreterLiteralWrapperBuffer.\");\n   }\n \n-  void CopyToRemoteDevice(PjRtFuture<std::string> serialized_descriptor,\n+  void CopyToRemoteDevice(Future<std::string> serialized_descriptor,\n                           RemoteSendCallback on_done) override {\n     LOG(ERROR) << \"InterpreterLiteralWrapperBuffer::CopyToRemoteDevice was \"\n                   \"called but is not implemented.\";\n   }\n \n-  PjRtFuture<> GetReadyFuture() override {\n-    return PjRtFuture<>(absl::OkStatus());\n-  }\n+  Future<> GetReadyFuture() override { return Future<>(absl::OkStatus()); }\n \n   bool IsOnCpu() const override { return true; }\n \n@@ -357,19 +355,16 @@ class InterpreterLoadedExecutable final : public PjRtLoadedExecutable {\n   absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>> Execute(\n       absl::Span<const std::vector<PjRtBuffer*>> argument_handles,\n       const ExecuteOptions& options,\n-      std::optional<std::vector<PjRtFuture<>>>& returned_futures)\n-      const override;\n+      std::optional<std::vector<Future<>>>& returned_futures) const override;\n \n   absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>> ExecuteSharded(\n       absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-      const ExecuteOptions& options,\n-      std::optional<PjRtFuture<>>& returned_future,\n+      const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n       bool fill_future) const override;\n \n   absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>> ExecutePortable(\n       absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-      const ExecuteOptions& options,\n-      std::optional<PjRtFuture<>>& returned_future,\n+      const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n       bool fill_future) const override;\n \n   void Delete() override { hlo_module_ = nullptr; }"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 23,
        "deletions": 27
    }
}