{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 848079100",
    "sha": "55e5af9a8e7ec18403da7788043618a66af72b98",
    "files": [
        {
            "sha": "04aef1c88095da1620487e1aee28ee02c5311136",
            "filename": "tensorflow/core/kernels/bias_op_gpu.cu.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 44,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbias_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbias_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbias_op_gpu.cu.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -57,23 +57,23 @@ struct AccumulatorType<Eigen::bfloat16> {\n // Definition of the GPU implementations declared in bias_op.cc.\n \n template <typename T>\n-__global__ void BiasNHWCKernel(int32 nthreads, const T* __restrict__ input,\n+__global__ void BiasNHWCKernel(int32_t nthreads, const T* __restrict__ input,\n                                const T* __restrict__ bias,\n-                               T* __restrict__ output, int32 bias_size) {\n+                               T* __restrict__ output, int32_t bias_size) {\n   GPU_1D_KERNEL_LOOP(index, nthreads) {\n-    int32 bias_offset = index % bias_size;\n+    int32_t bias_offset = index % bias_size;\n     output[index] = ldg(input + index) + ldg(bias + bias_offset);\n   }\n }\n \n template <typename T>\n-__global__ void BiasNCHWKernel(int32 nthreads, const T* __restrict__ input,\n+__global__ void BiasNCHWKernel(int32_t nthreads, const T* __restrict__ input,\n                                const T* __restrict__ bias,\n-                               T* __restrict__ output, int32 bias_size,\n-                               int32 image_size) {\n+                               T* __restrict__ output, int32_t bias_size,\n+                               int32_t image_size) {\n   GPU_1D_KERNEL_LOOP(index, nthreads) {\n-    int32 index2 = index / image_size;\n-    int32 bias_offset = index2 % bias_size;\n+    int32_t index2 = index / image_size;\n+    int32_t bias_offset = index2 % bias_size;\n     output[index] = ldg(input + index) + ldg(bias + bias_offset);\n   }\n }\n@@ -82,11 +82,12 @@ __global__ void BiasNCHWKernel(int32 nthreads, const T* __restrict__ input,\n // dimension.\n template <typename T>\n void BiasGPU<T>::compute(const GPUDevice& d, const T* input, const T* bias,\n-                         T* output, int32 batch, int32 height, int32 width,\n-                         int depth, int32 channel, TensorFormat data_format) {\n-  const int32 bias_size = channel;\n-  const int32 image_size = height * width * depth;\n-  const int32 total_count = batch * bias_size * image_size;\n+                         T* output, int32_t batch, int32_t height,\n+                         int32_t width, int depth, int32_t channel,\n+                         TensorFormat data_format) {\n+  const int32_t bias_size = channel;\n+  const int32_t image_size = height * width * depth;\n+  const int32_t total_count = batch * bias_size * image_size;\n   if (total_count == 0) {\n     return;\n   }\n@@ -109,76 +110,76 @@ void BiasGPU<T>::compute(const GPUDevice& d, const T* input, const T* bias,\n \n // A naive implementation that is functional on all cases.\n template <typename T>\n-__global__ void BiasGradNHWC_Naive(int32 nthreads,\n+__global__ void BiasGradNHWC_Naive(int32_t nthreads,\n                                    const T* __restrict__ output_backprop,\n                                    T* __restrict__ bias_backprop,\n-                                   int32 bias_size) {\n+                                   int32_t bias_size) {\n   GPU_1D_KERNEL_LOOP(index, nthreads) {\n-    int32 bias_offset = index % bias_size;\n+    int32_t bias_offset = index % bias_size;\n     GpuAtomicAdd(bias_backprop + bias_offset, ldg(output_backprop + index));\n   }\n }\n \n // A naive implementation that is functional on all cases.\n template <typename T>\n-__global__ void BiasGradNCHW_Naive(int32 nthreads,\n+__global__ void BiasGradNCHW_Naive(int32_t nthreads,\n                                    const T* __restrict__ output_backprop,\n                                    T* __restrict__ bias_backprop,\n-                                   int32 bias_size, int32 image_size) {\n+                                   int32_t bias_size, int32_t image_size) {\n   GPU_1D_KERNEL_LOOP(index, nthreads) {\n-    int32 index2 = index / image_size;\n-    int32 bias_offset = index2 % bias_size;\n+    int32_t index2 = index / image_size;\n+    int32_t bias_offset = index2 % bias_size;\n     GpuAtomicAdd(bias_backprop + bias_offset, ldg(output_backprop + index));\n   }\n }\n \n template <typename T>\n __global__ void BiasGradNHWC_SharedAtomics(\n-    int32 nthreads, const T* __restrict__ output_backprop,\n-    T* __restrict__ bias_backprop, int32 bias_size) {\n+    int32_t nthreads, const T* __restrict__ output_backprop,\n+    T* __restrict__ bias_backprop, int32_t bias_size) {\n   typedef typename AccumulatorType<T>::type AccT;\n   GPU_DYNAMIC_SHARED_MEM_DECL(8, char, s_buf);\n   AccT* s_data = reinterpret_cast<AccT*>(s_buf);\n-  for (int32 index = threadIdx.x; index < bias_size; index += blockDim.x) {\n+  for (int32_t index = threadIdx.x; index < bias_size; index += blockDim.x) {\n     s_data[index] = AccT(0);\n   }\n   __syncthreads();\n \n-  for (int32 index = blockIdx.x * blockDim.x + threadIdx.x; index < nthreads;\n+  for (int32_t index = blockIdx.x * blockDim.x + threadIdx.x; index < nthreads;\n        index += blockDim.x * gridDim.x) {\n-    int32 bias_offset = index % bias_size;\n+    int32_t bias_offset = index % bias_size;\n     GpuAtomicAdd(s_data + bias_offset, AccT(ldg(output_backprop + index)));\n   }\n   __syncthreads();\n \n-  for (int32 index = threadIdx.x; index < bias_size; index += blockDim.x) {\n+  for (int32_t index = threadIdx.x; index < bias_size; index += blockDim.x) {\n     GpuAtomicAdd(bias_backprop + index, T(s_data[index]));\n   }\n }\n \n template <typename T>\n __global__ void BiasGradNCHW_SharedAtomics(\n     const T* __restrict__ output_backprop, T* __restrict__ bias_backprop,\n-    int32 batch, int32 bias_size, int32 image_size, int group_size) {\n+    int32_t batch, int32_t bias_size, int32_t image_size, int group_size) {\n   // Initialize the shared memory.\n   typedef typename AccumulatorType<T>::type AccT;\n-  const int32 kSDataSize = 32;\n+  const int32_t kSDataSize = 32;\n   __shared__ AccT s_data[kSDataSize];\n-  for (int32 index = threadIdx.x; index < kSDataSize; index += blockDim.x) {\n+  for (int32_t index = threadIdx.x; index < kSDataSize; index += blockDim.x) {\n     s_data[index] = AccT(0);\n   }\n   __syncthreads();\n \n   // Accumulate all the values within this thread. They all have the same bias\n   // index.\n-  int32 bias_index = blockIdx.x % bias_size;\n-  int32 group_index = blockIdx.x / bias_size;\n-  int32 total_count = batch * image_size;\n+  int32_t bias_index = blockIdx.x % bias_size;\n+  int32_t group_index = blockIdx.x / bias_size;\n+  int32_t total_count = batch * image_size;\n   AccT sum(0);\n-  for (int32 index = group_index * blockDim.x + threadIdx.x;\n+  for (int32_t index = group_index * blockDim.x + threadIdx.x;\n        index < total_count; index += blockDim.x * group_size) {\n-    int32 image_offset = index % image_size;\n-    int32 batch = index / image_size;\n+    int32_t image_offset = index % image_size;\n+    int32_t batch = index / image_size;\n     T val = ldg(output_backprop +\n                 (batch * bias_size + bias_index) * image_size + image_offset);\n     sum += AccT(val);\n@@ -192,11 +193,11 @@ __global__ void BiasGradNCHW_SharedAtomics(\n \n   // Accumulate the results in the shared memory into the first element.\n   // No syncthreads is needed since this is only in the same warp.\n-  int32 thread_index = threadIdx.x;\n+  int32_t thread_index = threadIdx.x;\n #if GOOGLE_CUDA\n   if (thread_index < 32) {\n     AccT data = s_data[thread_index];\n-    for (int32 delta = warpSize / 2; delta > 0; delta /= 2) {\n+    for (int32_t delta = warpSize / 2; delta > 0; delta /= 2) {\n       data += GpuShuffleXorSync(kCudaWarpAll, data, delta);\n     }\n     if (thread_index == 0) {\n@@ -219,20 +220,20 @@ __global__ void BiasGradNCHW_SharedAtomics(\n \n template <typename T>\n void BiasGradGPU<T>::compute(const GPUDevice& d, const T* output_backprop,\n-                             T* bias_backprop, int32 batch, int32 height,\n-                             int32 width, int32 depth, int32 channel,\n+                             T* bias_backprop, int32_t batch, int32_t height,\n+                             int32_t width, int32_t depth, int32_t channel,\n                              TensorFormat data_format) {\n-  const int32 bias_size = channel;\n-  const int32 image_size = height * width * depth;\n-  const int32 total_count = batch * bias_size * image_size;\n+  const int32_t bias_size = channel;\n+  const int32_t image_size = height * width * depth;\n+  const int32_t total_count = batch * bias_size * image_size;\n   if (total_count == 0) {\n     return;\n   }\n-  static constexpr int32 kWarpSize = 32;\n+  static constexpr int32_t kWarpSize = 32;\n   GpuLaunchConfig config = GetGpuLaunchConfig(total_count, d);\n \n   const int max_shared_memory_size = d.sharedMemPerBlock() / 2;\n-  int32 shared_memory_size = 0;\n+  int32_t shared_memory_size = 0;\n   if (data_format == FORMAT_NHWC) {\n     shared_memory_size = bias_size * sizeof(typename AccumulatorType<T>::type);\n   }"
        },
        {
            "sha": "60f17e6de240de53583d846b5aea12a53ff91139",
            "filename": "tensorflow/core/kernels/bias_op_gpu.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbias_op_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbias_op_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbias_op_gpu.h?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -68,12 +68,12 @@ class BiasGradGPUProfileResult {\n   }\n   BiasAddGradGPUMode algorithm() const { return algorithm_; }\n   void set_algorithm(BiasAddGradGPUMode val) { algorithm_ = val; }\n-  uint64 elapsed_time() const { return elapsed_time_; }\n-  void set_elapsed_time(uint64 val) { elapsed_time_ = val; }\n+  uint64_t elapsed_time() const { return elapsed_time_; }\n+  void set_elapsed_time(uint64_t val) { elapsed_time_ = val; }\n \n  private:\n   BiasAddGradGPUMode algorithm_ = BiasAddGradGPUMode::kInvalid;\n-  uint64 elapsed_time_ = std::numeric_limits<uint64>::max();\n+  uint64_t elapsed_time_ = std::numeric_limits<uint64_t>::max();\n };\n \n }  // namespace tensorflow"
        },
        {
            "sha": "4d309f6f6286eae667c01cdfa2f99e1ef1b7dfe3",
            "filename": "tensorflow/core/kernels/bincount_op_gpu.cu.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbincount_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbincount_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbincount_op_gpu.cu.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -36,19 +36,19 @@ namespace functor {\n \n template <typename Tidx, typename T>\n struct BincountFunctor<GPUDevice, Tidx, T, false> {\n-  static Status Compute(OpKernelContext* context,\n-                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n-                        const typename TTypes<T, 1>::ConstTensor& weights,\n-                        typename TTypes<T, 1>::Tensor& output,\n-                        const Tidx num_bins) {\n+  static absl::Status Compute(OpKernelContext* context,\n+                              const typename TTypes<Tidx, 1>::ConstTensor& arr,\n+                              const typename TTypes<T, 1>::ConstTensor& weights,\n+                              typename TTypes<T, 1>::Tensor& output,\n+                              const Tidx num_bins) {\n     if (weights.size() != 0) {\n       return errors::Unimplemented(\n           \"Weights are not yet supported by the GPU implementation of Bincount.\"\n           \" Please use unsorted_segment_sum instead or put Bincount inside\"\n           \" tf.function(jit_compile=True).\");\n     }\n     if (output.size() == 0) {\n-      return OkStatus();\n+      return absl::OkStatus();\n     }\n     if (tensorflow::OpDeterminismRequired()) {\n       // TODO(reedwm): Is this really nondeterministic?\n@@ -88,11 +88,11 @@ struct BincountFunctor<GPUDevice, Tidx, T, false> {\n     }\n     Tensor temp_storage;\n     TF_RETURN_IF_ERROR(context->allocate_temp(\n-        DataTypeToEnum<int8>::value,\n+        DataTypeToEnum<int8_t>::value,\n         TensorShape({static_cast<int64_t>(temp_storage_bytes)}),\n         &temp_storage));\n \n-    void* d_temp_storage = temp_storage.flat<int8>().data();\n+    void* d_temp_storage = temp_storage.flat<int8_t>().data();\n     // The second HistogramEven is to actual run with d_temp_storage\n     // allocated with temp_storage_bytes.\n     err = gpuprim::DeviceHistogram::HistogramEven(\n@@ -109,7 +109,7 @@ struct BincountFunctor<GPUDevice, Tidx, T, false> {\n       return errors::Internal(\n           \"Could not launch HistogramEven: \", GpuGetErrorString(err), \".\");\n     }\n-    return OkStatus();\n+    return absl::OkStatus();\n   }\n };\n \n@@ -126,11 +126,11 @@ __global__ void BincountReduceKernel(const Tidx* in, T* out, const int nthreads,\n \n template <typename Tidx, typename T>\n struct BincountFunctor<GPUDevice, Tidx, T, true> {\n-  static Status Compute(OpKernelContext* context,\n-                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n-                        const typename TTypes<T, 1>::ConstTensor& weights,\n-                        typename TTypes<T, 1>::Tensor& output,\n-                        const Tidx num_bins) {\n+  static absl::Status Compute(OpKernelContext* context,\n+                              const typename TTypes<Tidx, 1>::ConstTensor& arr,\n+                              const typename TTypes<T, 1>::ConstTensor& weights,\n+                              typename TTypes<T, 1>::Tensor& output,\n+                              const Tidx num_bins) {\n     const int nthreads = arr.dimension(0);\n \n     auto d = context->eigen_gpu_device();\n@@ -206,11 +206,11 @@ __global__ void BincountColReduceSharedKernel(const Tidx* in, const T* weights,\n \n template <typename Tidx, typename T, bool binary_count>\n struct BincountReduceFunctor<GPUDevice, Tidx, T, binary_count> {\n-  static Status Compute(OpKernelContext* context,\n-                        const typename TTypes<Tidx, 2>::ConstTensor& in,\n-                        const typename TTypes<T, 2>::ConstTensor& weights,\n-                        typename TTypes<T, 2>::Tensor& out,\n-                        const Tidx num_bins) {\n+  static absl::Status Compute(OpKernelContext* context,\n+                              const typename TTypes<Tidx, 2>::ConstTensor& in,\n+                              const typename TTypes<T, 2>::ConstTensor& weights,\n+                              typename TTypes<T, 2>::Tensor& out,\n+                              const Tidx num_bins) {\n     const int num_rows = in.dimension(0);\n     const int num_cols = in.dimension(1);\n "
        },
        {
            "sha": "e58902ddfccc21481e84cadb60a382cba3613176",
            "filename": "tensorflow/core/kernels/broadcast_to_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbroadcast_to_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbroadcast_to_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbroadcast_to_op.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -149,11 +149,11 @@ TF_CALL_float8_e4m3fn(REGISTER_KERNEL);\n // registration requires all int32 inputs and outputs to be in host memory.\n REGISTER_KERNEL_BUILDER(Name(\"BroadcastTo\")\n                             .Device(DEVICE_GPU)\n-                            .TypeConstraint<int32>(\"T\")\n+                            .TypeConstraint<int32_t>(\"T\")\n                             .HostMemory(\"input\")\n                             .HostMemory(\"shape\")\n                             .HostMemory(\"output\"),\n-                        BroadcastToOp<CPUDevice, int32>);\n+                        BroadcastToOp<CPUDevice, int32_t>);\n #endif\n #if defined(PLUGGABLE_DEVICE_SUPPORTED_MACOS)\n REGISTER_KERNEL_BUILDER(Name(\"BroadcastTo\")"
        },
        {
            "sha": "d69244d0c67cadcd5aeb18264fa68e39018c8f22",
            "filename": "tensorflow/core/kernels/bucketize_op_gpu.cu.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbucketize_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fbucketize_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbucketize_op_gpu.cu.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -34,18 +34,19 @@ typedef Eigen::GpuDevice GPUDevice;\n \n template <typename T, bool useSharedMem>\n __global__ void BucketizeCustomKernel(\n-    const int32 size_in, const T* __restrict__ in, const int32 size_boundaries,\n-    GpuDeviceArrayStruct<float> boundaries_array, int32* __restrict__ out) {\n+    const int32_t size_in, const T* __restrict__ in,\n+    const int32_t size_boundaries, GpuDeviceArrayStruct<float> boundaries_array,\n+    int32_t* __restrict__ out) {\n   const float* boundaries = GetGpuDeviceArrayOnDevice(&boundaries_array);\n \n   GPU_DYNAMIC_SHARED_MEM_DECL(sizeof(float), unsigned char, shared_mem);\n   float* shared_mem_boundaries = reinterpret_cast<float*>(shared_mem);\n \n   if (useSharedMem) {\n-    int32 lidx = threadIdx.y * blockDim.x + threadIdx.x;\n-    int32 blockSize = blockDim.x * blockDim.y;\n+    int32_t lidx = threadIdx.y * blockDim.x + threadIdx.x;\n+    int32_t blockSize = blockDim.x * blockDim.y;\n \n-    for (int32 i = lidx; i < size_boundaries; i += blockSize) {\n+    for (int32_t i = lidx; i < size_boundaries; i += blockSize) {\n       shared_mem_boundaries[i] = boundaries[i];\n     }\n \n@@ -56,11 +57,11 @@ __global__ void BucketizeCustomKernel(\n \n   GPU_1D_KERNEL_LOOP(i, size_in) {\n     T value = in[i];\n-    int32 bucket = 0;\n-    int32 count = size_boundaries;\n+    int32_t bucket = 0;\n+    int32_t count = size_boundaries;\n     while (count > 0) {\n-      int32 l = bucket;\n-      int32 step = count / 2;\n+      int32_t l = bucket;\n+      int32_t step = count / 2;\n       l += step;\n       if (!(value < static_cast<T>(boundaries[l]))) {\n         bucket = ++l;\n@@ -78,10 +79,10 @@ namespace functor {\n template <typename T>\n struct BucketizeFunctor<GPUDevice, T> {\n   // PRECONDITION: boundaries_vector must be sorted.\n-  static Status Compute(OpKernelContext* context,\n-                        const typename TTypes<T, 1>::ConstTensor& input,\n-                        const std::vector<float>& boundaries_vector,\n-                        typename TTypes<int32, 1>::Tensor& output) {\n+  static absl::Status Compute(OpKernelContext* context,\n+                              const typename TTypes<T, 1>::ConstTensor& input,\n+                              const std::vector<float>& boundaries_vector,\n+                              typename TTypes<int32_t, 1>::Tensor& output) {\n     const GPUDevice& d = context->eigen_device<GPUDevice>();\n \n     GpuDeviceArrayOnHost<float> boundaries_array(context,\n@@ -93,8 +94,8 @@ struct BucketizeFunctor<GPUDevice, T> {\n     TF_RETURN_IF_ERROR(boundaries_array.Finalize());\n \n     GpuLaunchConfig config = GetGpuLaunchConfig(input.size(), d);\n-    int32 shared_mem_size = sizeof(float) * boundaries_vector.size();\n-    const int32 kMaxSharedMemBytes = 16384;\n+    int32_t shared_mem_size = sizeof(float) * boundaries_vector.size();\n+    const int32_t kMaxSharedMemBytes = 16384;\n     if (shared_mem_size < d.sharedMemPerBlock() &&\n         shared_mem_size < kMaxSharedMemBytes) {\n       TF_CHECK_OK(GpuLaunchKernel(BucketizeCustomKernel<T, true>,\n@@ -108,7 +109,7 @@ struct BucketizeFunctor<GPUDevice, T> {\n           config.thread_per_block, 0, d.stream(), input.size(), input.data(),\n           boundaries_vector.size(), boundaries_array.data(), output.data()));\n     }\n-    return OkStatus();\n+    return absl::OkStatus();\n   }\n };\n }  // namespace functor"
        },
        {
            "sha": "98e35f138363d511216efe8173e7d9c9271b55dd",
            "filename": "tensorflow/core/kernels/cast_op.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 35,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcast_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcast_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcast_op.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -184,10 +184,10 @@ class GpuCastOp : public CastOpBase {\n   }\n \n  private:\n-  Status Prepare() {\n+  absl::Status Prepare() {\n     if (external_src_dtype_ == external_dst_dtype_) {\n       work_ = nullptr;  // Identity\n-      return OkStatus();\n+      return absl::OkStatus();\n     }\n     if (src_dtype_ == DT_BOOL) {\n       work_ = GetGpuCastFromBool(dst_dtype_);\n@@ -228,7 +228,7 @@ class GpuCastOp : public CastOpBase {\n     } else if (src_dtype_ == DT_UINT4) {\n       work_ = GetGpuCastFromUint4(dst_dtype_);\n     }\n-    return work_ == nullptr ? Unimplemented() : OkStatus();\n+    return work_ == nullptr ? Unimplemented() : absl::OkStatus();\n   }\n };\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n@@ -263,14 +263,14 @@ CURRY_TYPES2(REGISTER_CAST_GPU, std::complex<float>);\n CURRY_TYPES2(REGISTER_CAST_GPU, std::complex<double>);\n #else\n REGISTER_CAST_GPU(bool, bfloat16);\n-REGISTER_CAST_GPU(int8, bfloat16);\n-REGISTER_CAST_GPU(int16, bfloat16);\n-REGISTER_CAST_GPU(int32, bfloat16);\n-REGISTER_CAST_GPU(int64, bfloat16);\n-REGISTER_CAST_GPU(uint8, bfloat16);\n-REGISTER_CAST_GPU(uint16, bfloat16);\n-REGISTER_CAST_GPU(uint32, bfloat16);\n-REGISTER_CAST_GPU(uint64, bfloat16);\n+REGISTER_CAST_GPU(int8_t, bfloat16);\n+REGISTER_CAST_GPU(int16_t, bfloat16);\n+REGISTER_CAST_GPU(int32_t, bfloat16);\n+REGISTER_CAST_GPU(int64_t, bfloat16);\n+REGISTER_CAST_GPU(uint8_t, bfloat16);\n+REGISTER_CAST_GPU(uint16_t, bfloat16);\n+REGISTER_CAST_GPU(uint32_t, bfloat16);\n+REGISTER_CAST_GPU(uint64_t, bfloat16);\n REGISTER_CAST_GPU(Eigen::half, bfloat16);\n REGISTER_CAST_GPU(float, bfloat16);\n REGISTER_CAST_GPU(double, bfloat16);\n@@ -301,43 +301,43 @@ REGISTER_CAST_GPU(float8_e4m3fn, float8_e5m2);\n REGISTER_CAST_GPU(float8_e4m3fn, float8_e4m3fn);\n \n REGISTER_CAST_GPU(int4, int4);\n-REGISTER_CAST_GPU(int4, int8);\n-REGISTER_CAST_GPU(int4, int16);\n-REGISTER_CAST_GPU(int4, int32);\n+REGISTER_CAST_GPU(int4, int8_t);\n+REGISTER_CAST_GPU(int4, int16_t);\n+REGISTER_CAST_GPU(int4, int32_t);\n REGISTER_CAST_GPU(int4, int64_t);\n REGISTER_CAST_GPU(int4, uint4);\n-REGISTER_CAST_GPU(int4, uint8);\n-REGISTER_CAST_GPU(int4, uint16);\n-REGISTER_CAST_GPU(int4, uint32);\n+REGISTER_CAST_GPU(int4, uint8_t);\n+REGISTER_CAST_GPU(int4, uint16_t);\n+REGISTER_CAST_GPU(int4, uint32_t);\n REGISTER_CAST_GPU(int4, uint64_t);\n \n-REGISTER_CAST_GPU(int8, int4);\n-REGISTER_CAST_GPU(int16, int4);\n-REGISTER_CAST_GPU(int32, int4);\n+REGISTER_CAST_GPU(int8_t, int4);\n+REGISTER_CAST_GPU(int16_t, int4);\n+REGISTER_CAST_GPU(int32_t, int4);\n REGISTER_CAST_GPU(int64_t, int4);\n REGISTER_CAST_GPU(uint4, int4);\n-REGISTER_CAST_GPU(uint8, int4);\n-REGISTER_CAST_GPU(uint16, int4);\n-REGISTER_CAST_GPU(uint32, int4);\n+REGISTER_CAST_GPU(uint8_t, int4);\n+REGISTER_CAST_GPU(uint16_t, int4);\n+REGISTER_CAST_GPU(uint32_t, int4);\n REGISTER_CAST_GPU(uint64_t, int4);\n \n-REGISTER_CAST_GPU(uint4, int8);\n-REGISTER_CAST_GPU(uint4, int16);\n-REGISTER_CAST_GPU(uint4, int32);\n+REGISTER_CAST_GPU(uint4, int8_t);\n+REGISTER_CAST_GPU(uint4, int16_t);\n+REGISTER_CAST_GPU(uint4, int32_t);\n REGISTER_CAST_GPU(uint4, int64_t);\n REGISTER_CAST_GPU(uint4, uint4);\n-REGISTER_CAST_GPU(uint4, uint8);\n-REGISTER_CAST_GPU(uint4, uint16);\n-REGISTER_CAST_GPU(uint4, uint32);\n+REGISTER_CAST_GPU(uint4, uint8_t);\n+REGISTER_CAST_GPU(uint4, uint16_t);\n+REGISTER_CAST_GPU(uint4, uint32_t);\n REGISTER_CAST_GPU(uint4, uint64_t);\n \n-REGISTER_CAST_GPU(int8, uint4);\n-REGISTER_CAST_GPU(int16, uint4);\n-REGISTER_CAST_GPU(int32, uint4);\n+REGISTER_CAST_GPU(int8_t, uint4);\n+REGISTER_CAST_GPU(int16_t, uint4);\n+REGISTER_CAST_GPU(int32_t, uint4);\n REGISTER_CAST_GPU(int64_t, uint4);\n-REGISTER_CAST_GPU(uint8, uint4);\n-REGISTER_CAST_GPU(uint16, uint4);\n-REGISTER_CAST_GPU(uint32, uint4);\n+REGISTER_CAST_GPU(uint8_t, uint4);\n+REGISTER_CAST_GPU(uint16_t, uint4);\n+REGISTER_CAST_GPU(uint32_t, uint4);\n REGISTER_CAST_GPU(uint64_t, uint4);\n \n #undef REGISTER_CAST_GPU"
        },
        {
            "sha": "5f5552edd519ca4b2715e92f30ebd5ba5266079c",
            "filename": "tensorflow/core/kernels/cast_op_impl_int64.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcast_op_impl_int64.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcast_op_impl_int64.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcast_op_impl_int64.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -38,7 +38,7 @@ CastFunctorType GetCpuCastFromInt64(DataType dst_dtype) {\n     (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\n CastFunctorType GetGpuCastFromInt64(DataType dst_dtype) {\n #if defined(MLIR_GENERATED_GPU_KERNELS_ENABLED)\n-  CAST_CASE(GPUDevice, int64, bfloat16);\n+  CAST_CASE(GPUDevice, int64_t, bfloat16);\n #else\n   CURRY_TYPES3(CAST_CASE, GPUDevice, int64);\n #endif"
        },
        {
            "sha": "31ceecab9a84ee6b30de8ff4f1736f93bc817602",
            "filename": "tensorflow/core/kernels/check_numerics_op_gpu.cu.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcheck_numerics_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcheck_numerics_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcheck_numerics_op_gpu.cu.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -38,10 +38,10 @@ typedef Eigen::GpuDevice GPUDevice;\n template <typename T>\n __global__ void CheckNumericsKernel(const T* __restrict__ data, int size,\n                                     int abnormal_detected[2]) {\n-  const int32 thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n-  const int32 total_thread_count = gridDim.x * blockDim.x;\n+  const int32_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n+  const int32_t total_thread_count = gridDim.x * blockDim.x;\n \n-  int32 offset = thread_id;\n+  int32_t offset = thread_id;\n \n   while (offset < size) {\n     if (isnan(data[offset])) {\n@@ -61,10 +61,10 @@ __global__ void CheckNumericsKernel(const T* __restrict__ data, int size,\n template <typename T>\n __global__ void CheckNumericsKernelV2(const T* __restrict__ data, int size,\n                                       int abnormal_detected[3]) {\n-  const int32 thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n-  const int32 total_thread_count = gridDim.x * blockDim.x;\n+  const int32_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n+  const int32_t total_thread_count = gridDim.x * blockDim.x;\n \n-  int32 offset = thread_id;\n+  int32_t offset = thread_id;\n \n   while (offset < size) {\n     if (isnan(data[offset])) {\n@@ -85,8 +85,8 @@ template <typename T>\n struct CheckNumericsLaunch {\n   void Run(const GPUDevice& d, const T* data, int size,\n            int abnormal_detected[2]) {\n-    const int32 block_size = d.maxGpuThreadsPerBlock();\n-    const int32 num_blocks =\n+    const int32_t block_size = d.maxGpuThreadsPerBlock();\n+    const int32_t num_blocks =\n         (d.getNumGpuMultiProcessors() * d.maxGpuThreadsPerMultiProcessor()) /\n         block_size;\n \n@@ -103,8 +103,8 @@ template <typename T>\n struct CheckNumericsLaunchV2 {\n   void Run(const GPUDevice& d, const T* data, int size,\n            int abnormal_detected[3]) {\n-    const int32 block_size = d.maxGpuThreadsPerBlock();\n-    const int32 num_blocks =\n+    const int32_t block_size = d.maxGpuThreadsPerBlock();\n+    const int32_t num_blocks =\n         (d.getNumGpuMultiProcessors() * d.maxGpuThreadsPerMultiProcessor()) /\n         block_size;\n "
        },
        {
            "sha": "9e69fb3611560228fe3c47eaed2785cee380ddaa",
            "filename": "tensorflow/core/kernels/collective_nccl.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -22,10 +22,11 @@ limitations under the License.\n \n namespace tensorflow {\n \n-NcclBase::NcclBase(CollectiveType type, const string& name)\n+NcclBase::NcclBase(CollectiveType type, const std::string& name)\n     : type_(type), name_(name), col_ctx_(nullptr), col_params_(nullptr) {}\n \n-Status NcclBase::InitializeCollectiveParams(CollectiveParams* col_params) {\n+absl::Status NcclBase::InitializeCollectiveParams(\n+    CollectiveParams* col_params) {\n   if (type_ != col_params->instance.type) {\n     return errors::Internal(\"Expected initialized type \", type_,\n                             \" to match type in CollectiveParams \",\n@@ -60,10 +61,10 @@ Status NcclBase::InitializeCollectiveParams(CollectiveParams* col_params) {\n                             \", expected name \", expected_name);\n   }\n \n-  return OkStatus();\n+  return absl::OkStatus();\n }\n \n-Status NcclBase::InitializeCollectiveContext(\n+absl::Status NcclBase::InitializeCollectiveContext(\n     std::shared_ptr<CollectiveContext> col_ctx) {\n   col_ctx_ = col_ctx;\n   col_params_ = col_ctx->col_params.get();"
        },
        {
            "sha": "26a096fa3f8bb443910f1975599927f6310ad5ba",
            "filename": "tensorflow/core/kernels/collective_nccl.h",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl.h?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -22,19 +22,20 @@ namespace tensorflow {\n \n class NcclBase : public CollectiveImplementationInterface {\n  public:\n-  explicit NcclBase(CollectiveType type, const string& name);\n+  explicit NcclBase(CollectiveType type, const std::string& name);\n   ~NcclBase() override = default;\n \n   // No-op for this collective implementation.\n-  Status InitializeCollectiveParams(CollectiveParams* col_params) override;\n+  absl::Status InitializeCollectiveParams(\n+      CollectiveParams* col_params) override;\n \n   // Initializes the device objects and device localities.\n-  Status InitializeCollectiveContext(\n+  absl::Status InitializeCollectiveContext(\n       std::shared_ptr<CollectiveContext> col_ctx) override;\n \n  protected:\n   const CollectiveType type_;\n-  const string name_;\n+  const std::string name_;\n   std::shared_ptr<CollectiveContext> col_ctx_;\n   const CollectiveParams* col_params_;  // Not owned\n };"
        },
        {
            "sha": "58cdf8afd024857c79e49e3a57514eb4d59a336d",
            "filename": "tensorflow/core/kernels/concat_lib_gpu.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -74,8 +74,9 @@ void ConcatGPU(\n         inputs_flat,\n     Tensor* output, typename TTypes<T, 2>::Tensor* output_flat) {\n   if (inputs_flat.size() < 16) {\n-    if (output->NumElements() < std::numeric_limits<int32>::max()) {\n-      ConcatGPUSlice<T, int32>(c->eigen_gpu_device(), inputs_flat, output_flat);\n+    if (output->NumElements() < std::numeric_limits<int32_t>::max()) {\n+      ConcatGPUSlice<T, int32_t>(c->eigen_gpu_device(), inputs_flat,\n+                                 output_flat);\n     } else {\n       ConcatGPUSlice<T, int64_t>(c->eigen_gpu_device(), inputs_flat,\n                                  output_flat);\n@@ -84,8 +85,8 @@ void ConcatGPU(\n     // Switching indexing to int64 might cause performance issues.\n     // Hence, we keep int32 indexing in the GPU kernel unless we need to\n     // switch to int64.\n-    if (output->NumElements() < std::numeric_limits<int32>::max()) {\n-      ConcatGPUCall<T, int32>(c, inputs_flat, output_flat);\n+    if (output->NumElements() < std::numeric_limits<int32_t>::max()) {\n+      ConcatGPUCall<T, int32_t>(c, inputs_flat, output_flat);\n     } else {\n       ConcatGPUCall<T, int64_t>(c, inputs_flat, output_flat);\n     }"
        },
        {
            "sha": "58b6957a120f2a2ec8c36d6de9be7ef9161e6714",
            "filename": "tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu_impl.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu_impl.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu_impl.cu.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -126,7 +126,7 @@ void ConcatGPUSlice(\n     Eigen::array<IntType, 2> size;\n     size[0] = inputs_flat[i]->dimension(0);\n     size[1] = inputs_flat[i]->dimension(1);\n-    if (std::is_same<IntType, int32>::value) {\n+    if (std::is_same<IntType, int32_t>::value) {\n       To32Bit(*output).slice(offset, size).device(gpu_device) =\n           To32Bit(*inputs_flat[i]);\n     } else {\n@@ -159,7 +159,7 @@ void ConcatGPUImpl(const Eigen::GpuDevice& gpu_device,\n     // on most processors\n     // possibly due to decreasing occupancy\n     // 4096 inputs is a lot, most code will take the smem path\n-    const int32 kMaxSmemBytesPerformance = 16384;\n+    const int32_t kMaxSmemBytesPerformance = 16384;\n     if (smem_usage < smem_max && smem_usage < kMaxSmemBytesPerformance) {\n       TF_CHECK_OK(GpuLaunchKernel(\n           concat_variable_kernel<T, IntType, true>, config.block_count,"
        },
        {
            "sha": "ecd815f3e7e8a28f6643ca02a63e413b1cf7e5a7",
            "filename": "tensorflow/core/kernels/conv_grad_input_ops.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 30,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fconv_grad_input_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fconv_grad_input_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconv_grad_input_ops.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -45,7 +45,7 @@ template struct LaunchConv2DBackpropInputOp<CPUDevice, double>;\n \n // A dummy type to group forward backward data autotune results together.\n struct ConvBackwardDataAutotuneGroup {\n-  static string name() { return \"ConvBwdData\"; }\n+  static std::string name() { return \"ConvBwdData\"; }\n };\n \n typedef AutotuneSingleton<ConvBackwardDataAutotuneGroup, ConvParameters,\n@@ -56,14 +56,14 @@ typedef AutotuneSingleton<ConvBackwardDataAutotuneGroup, ConvParameters,\n // Computes backprop input using Eigen::SpatialConvolutionBackwardInput on GPU\n // for int32 inputs.\n template <>\n-struct LaunchConv2DBackpropInputOp<GPUDevice, int32> {\n+struct LaunchConv2DBackpropInputOp<GPUDevice, int32_t> {\n   void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                   const Tensor& out_backprop, const Tensor& filter,\n                   int row_dilation, int col_dilation, int row_stride,\n                   int col_stride, const Padding& padding,\n                   const std::vector<int64_t>& explicit_paddings,\n                   Tensor* in_backprop, TensorFormat data_format) {\n-    LaunchConv2DBackpropInputOpImpl<GPUDevice, int32> launcher;\n+    LaunchConv2DBackpropInputOpImpl<GPUDevice, int32_t> launcher;\n     launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,\n              row_dilation, col_dilation, row_stride, col_stride, padding,\n              explicit_paddings, in_backprop, data_format);\n@@ -82,8 +82,8 @@ void LaunchConv2DBackpropInputOpGpuImpl(\n   using se::dnn::AlgorithmDesc;\n   using se::dnn::ProfileResult;\n \n-  std::vector<int32> strides(4, 1);\n-  std::vector<int32> dilations(4, 1);\n+  std::vector<int32_t> strides(4, 1);\n+  std::vector<int32_t> dilations(4, 1);\n   auto input_h = GetTensorDimIndex(data_format, 'H');\n   auto input_w = GetTensorDimIndex(data_format, 'W');\n   strides[input_h] = row_stride;\n@@ -144,10 +144,10 @@ void LaunchConv2DBackpropInputOpGpuImpl(\n       dims.spatial_dims[0].stride == 1 && dims.spatial_dims[1].stride == 1 &&\n       data_format == FORMAT_NHWC && (padding == VALID || padding == SAME)) {\n     // 1x1 filter, so call cublas directly.\n-    const uint64 m = dims.batch_size * dims.spatial_dims[0].input_size *\n-                     dims.spatial_dims[1].input_size;\n-    const uint64 k = dims.out_depth;\n-    const uint64 n = dims.in_depth;\n+    const uint64_t m = dims.batch_size * dims.spatial_dims[0].input_size *\n+                       dims.spatial_dims[1].input_size;\n+    const uint64_t k = dims.out_depth;\n+    const uint64_t n = dims.in_depth;\n \n     auto a_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(),\n                                 out_backprop.template flat<T>().size());\n@@ -172,10 +172,10 @@ void LaunchConv2DBackpropInputOpGpuImpl(\n              data_format == FORMAT_NHWC) {\n     // The input data and filter have the same height/width, and we are not\n     // using grouped convolution, so call cublas directly.\n-    const uint64 m = dims.batch_size;\n-    const uint64 k = dims.out_depth;\n-    const uint64 n = dims.spatial_dims[0].input_size *\n-                     dims.spatial_dims[1].input_size * dims.in_depth;\n+    const uint64_t m = dims.batch_size;\n+    const uint64_t k = dims.out_depth;\n+    const uint64_t n = dims.spatial_dims[0].input_size *\n+                       dims.spatial_dims[1].input_size * dims.in_depth;\n \n     auto a_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(),\n                                 out_backprop.template flat<T>().size());\n@@ -279,7 +279,8 @@ void LaunchConv2DBackpropInputOpGpuImpl(\n   //   (2) NHWC -> OHWI\n \n   Tensor transformed_filter;\n-  const auto transform_filter = [&](FilterTensorFormat dst_format) -> Status {\n+  const auto transform_filter =\n+      [&](FilterTensorFormat dst_format) -> absl::Status {\n     VLOG(4) << \"Transform filter tensor from \" << ToString(FORMAT_HWIO)\n             << \" to \" << ToString(dst_format);\n \n@@ -297,7 +298,7 @@ void LaunchConv2DBackpropInputOpGpuImpl(\n         To32Bit(filter.tensor<T, 4>()),\n         To32Bit(transformed_filter.tensor<T, 4>()));\n \n-    return OkStatus();\n+    return absl::OkStatus();\n   };\n \n   if (compute_data_format == FORMAT_NCHW) {\n@@ -391,7 +392,7 @@ void LaunchConv2DBackpropInputOpGpuImpl(\n   auto autotune_entry = std::move(entry_or).value();\n \n   DnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize, ctx);\n-  Status cudnn_launch_status =\n+  absl::Status cudnn_launch_status =\n       LaunchAutotunedConv(autotune_entry, &scratch_allocator,\n                           se::dnn::ConvolutionKind::BACKWARD_DATA, stream,\n                           input_desc, in_backprop_ptr, filter_desc, filter_ptr,\n@@ -531,23 +532,23 @@ DECLARE_GPU_SPEC(double);\n #undef DECLARE_GPU_SPEC\n \n template <>\n-void SpatialConvolutionBackwardInputFunc<GPUDevice, int32>::operator()(\n-    const GPUDevice&, typename TTypes<int32, 4>::Tensor,\n-    typename TTypes<int32, 4>::ConstTensor,\n-    typename TTypes<int32, 4>::ConstTensor, Eigen::DenseIndex,\n+void SpatialConvolutionBackwardInputFunc<GPUDevice, int32_t>::operator()(\n+    const GPUDevice&, typename TTypes<int32_t, 4>::Tensor,\n+    typename TTypes<int32_t, 4>::ConstTensor,\n+    typename TTypes<int32_t, 4>::ConstTensor, Eigen::DenseIndex,\n     Eigen::DenseIndex, Eigen::DenseIndex, Eigen::DenseIndex);\n extern template struct SpatialConvolutionBackwardInputFunc<GPUDevice, int32>;\n \n template <>\n void SpatialConvolutionBackwardInputWithExplicitPaddingFunc<\n-    GPUDevice, int32>::operator()(const GPUDevice&,\n-                                  typename TTypes<int32, 4>::Tensor,\n-                                  typename TTypes<int32, 4>::ConstTensor,\n-                                  typename TTypes<int32, 4>::ConstTensor,\n-                                  Eigen::DenseIndex, Eigen::DenseIndex,\n-                                  Eigen::DenseIndex, Eigen::DenseIndex,\n-                                  Eigen::DenseIndex, Eigen::DenseIndex,\n-                                  Eigen::DenseIndex, Eigen::DenseIndex);\n+    GPUDevice, int32_t>::operator()(const GPUDevice&,\n+                                    typename TTypes<int32_t, 4>::Tensor,\n+                                    typename TTypes<int32_t, 4>::ConstTensor,\n+                                    typename TTypes<int32_t, 4>::ConstTensor,\n+                                    Eigen::DenseIndex, Eigen::DenseIndex,\n+                                    Eigen::DenseIndex, Eigen::DenseIndex,\n+                                    Eigen::DenseIndex, Eigen::DenseIndex,\n+                                    Eigen::DenseIndex, Eigen::DenseIndex);\n extern template struct SpatialConvolutionBackwardInputWithExplicitPaddingFunc<\n     GPUDevice, int32>;\n \n@@ -575,9 +576,9 @@ REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")\n                         Conv2DBackpropInputOp<GPUDevice, Eigen::bfloat16>);\n REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")\n                             .Device(DEVICE_GPU)\n-                            .TypeConstraint<int32>(\"T\")\n+                            .TypeConstraint<int32_t>(\"T\")\n                             .HostMemory(\"input_sizes\"),\n-                        Conv2DBackpropInputOp<GPUDevice, int32>);\n+                        Conv2DBackpropInputOp<GPUDevice, int32_t>);\n \n // To be used inside depthwise_conv_grad_op.cc.\n // TODO(reedwm): Move this and the definition to depthwise_conv_grad_op.cc."
        },
        {
            "sha": "d344bb09da1c39ca351c915aa64c7e27ded914e2",
            "filename": "tensorflow/core/kernels/cudnn_pooling_gpu.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcudnn_pooling_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcudnn_pooling_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcudnn_pooling_gpu.h?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -39,9 +39,9 @@ class DnnPooling3dOp {\n  public:\n   static void Compute(OpKernelContext* context,\n                       se::dnn::PoolingMode pooling_mode,\n-                      const std::array<int64, 3>& size,\n-                      const std::array<int64, 3>& stride,\n-                      const std::array<int64, 3>& padding,\n+                      const std::array<int64_t, 3>& size,\n+                      const std::array<int64_t, 3>& stride,\n+                      const std::array<int64_t, 3>& padding,\n                       TensorFormat data_format, const Tensor& tensor_in,\n                       Tensor* output);\n };\n@@ -53,10 +53,10 @@ class DnnPooling3dGradOp {\n  public:\n   static void Compute(OpKernelContext* context,\n                       se::dnn::PoolingMode pooling_mode,\n-                      const std::array<int64, 3>& window,\n-                      const std::array<int64, 3>& stride,\n-                      const std::array<int64, 3>& padding,\n-                      const std::array<int64, 3>& output_size,\n+                      const std::array<int64_t, 3>& window,\n+                      const std::array<int64_t, 3>& stride,\n+                      const std::array<int64_t, 3>& padding,\n+                      const std::array<int64_t, 3>& output_size,\n                       TensorFormat data_format, const Tensor& out_backprop,\n                       const TensorShape& tensor_in_shape,\n                       const Tensor* tensor_in, const Tensor* tensor_out,"
        },
        {
            "sha": "f22d3bd3db7c96941992a12a77f2e1e2fde0d128",
            "filename": "tensorflow/core/kernels/cwise_op_clip_gpu.cu.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcwise_op_clip_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcwise_op_clip_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcwise_op_clip_gpu.cu.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -24,35 +24,35 @@ limitations under the License.\n namespace tensorflow {\n \n template <typename T>\n-__global__ void UnaryClipCustomKernel(const int32 size_in,\n-                                      const T *__restrict__ in0,\n-                                      const T *__restrict__ in1,\n-                                      const T *__restrict__ in2,\n-                                      T *__restrict__ out) {\n+__global__ void UnaryClipCustomKernel(const int32_t size_in,\n+                                      const T* __restrict__ in0,\n+                                      const T* __restrict__ in1,\n+                                      const T* __restrict__ in2,\n+                                      T* __restrict__ out) {\n   GPU_1D_KERNEL_LOOP(i, size_in) {\n     T value = in2[0] < in0[i] ? in2[0] : in0[i];\n     out[i] = value < in1[0] ? in1[0] : value;\n   }\n }\n \n template <typename T>\n-__global__ void BinaryRightClipCustomKernel(const int32 size_in,\n-                                            const T *__restrict__ in0,\n-                                            const T *__restrict__ in1,\n-                                            const T *__restrict__ in2,\n-                                            T *__restrict__ out) {\n+__global__ void BinaryRightClipCustomKernel(const int32_t size_in,\n+                                            const T* __restrict__ in0,\n+                                            const T* __restrict__ in1,\n+                                            const T* __restrict__ in2,\n+                                            T* __restrict__ out) {\n   GPU_1D_KERNEL_LOOP(i, size_in) {\n     T value = in2[i] < in0[i] ? in2[i] : in0[i];\n     out[i] = value < in1[0] ? in1[0] : value;\n   }\n }\n \n template <typename T>\n-__global__ void BinaryLeftClipCustomKernel(const int32 size_in,\n-                                           const T *__restrict__ in0,\n-                                           const T *__restrict__ in1,\n-                                           const T *__restrict__ in2,\n-                                           T *__restrict__ out) {\n+__global__ void BinaryLeftClipCustomKernel(const int32_t size_in,\n+                                           const T* __restrict__ in0,\n+                                           const T* __restrict__ in1,\n+                                           const T* __restrict__ in2,\n+                                           T* __restrict__ out) {\n   GPU_1D_KERNEL_LOOP(i, size_in) {\n     T value = in2[0] < in0[i] ? in2[0] : in0[i];\n     out[i] = value < in1[i] ? in1[i] : value;"
        },
        {
            "sha": "5ef7a4008c8728f08c218c331e34df420214ffb6",
            "filename": "tensorflow/core/kernels/cwise_op_select.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcwise_op_select.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fcwise_op_select.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcwise_op_select.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -289,7 +289,7 @@ REGISTER_SELECT_GPU(bool);\n REGISTER_SELECT_GPU(Eigen::half);\n REGISTER_SELECT_GPU(float);\n REGISTER_SELECT_GPU(double);\n-REGISTER_SELECT_GPU(int32);\n+REGISTER_SELECT_GPU(int32_t);\n REGISTER_SELECT_GPU(int64_t);\n REGISTER_SELECT_GPU(complex64);\n REGISTER_SELECT_GPU(complex128);"
        },
        {
            "sha": "db7cf3f31f78495de2c3cd2dabf187b58562a17e",
            "filename": "tensorflow/core/kernels/depthwise_conv_grad_op.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fdepthwise_conv_grad_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55e5af9a8e7ec18403da7788043618a66af72b98/tensorflow%2Fcore%2Fkernels%2Fdepthwise_conv_grad_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdepthwise_conv_grad_op.cc?ref=55e5af9a8e7ec18403da7788043618a66af72b98",
            "patch": "@@ -560,7 +560,7 @@ class DepthwiseConv2dNativeBackpropInputOp : public OpKernel {\n                 errors::InvalidArgument(\"Sliding window strides field must \"\n                                         \"specify 4 dimensions\"));\n \n-    string data_format;\n+    std::string data_format;\n     OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n     OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                 errors::InvalidArgument(\"Invalid data format\"));\n@@ -619,7 +619,7 @@ class DepthwiseConv2dNativeBackpropInputOp : public OpKernel {\n             \"Conv2DBackpropInput: input_sizes input must be 1-dim, not \",\n             input_sizes.dims()));\n     TensorShape input_shape;\n-    const int32* in_sizes_data = input_sizes.template flat<int32>().data();\n+    const int32_t* in_sizes_data = input_sizes.template flat<int32_t>().data();\n \n     for (int i = 0; i < input_sizes.NumElements(); ++i) {\n       OP_REQUIRES(context, in_sizes_data[i] >= 0,\n@@ -695,7 +695,7 @@ class DepthwiseConv2dNativeBackpropInputOp : public OpKernel {\n   bool use_cudnn_grouped_conv_;\n \n  private:\n-  std::vector<int32> strides_;\n+  std::vector<int32_t> strides_;\n   Padding padding_;\n   std::vector<int64_t> explicit_paddings_;\n   TensorFormat data_format_;\n@@ -1071,7 +1071,7 @@ class DepthwiseConv2dNativeBackpropFilterOp : public OpKernel {\n                 errors::InvalidArgument(\"Sliding window strides field must \"\n                                         \"specify 4 dimensions\"));\n \n-    string data_format;\n+    std::string data_format;\n     OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n     OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                 errors::InvalidArgument(\"Invalid data format\"));\n@@ -1129,7 +1129,8 @@ class DepthwiseConv2dNativeBackpropFilterOp : public OpKernel {\n             \"Conv2DBackpropFilter: filter_sizes input must be 1-dim, not \",\n             filter_sizes.dims()));\n     TensorShape filter_shape;\n-    const int32* filter_sizes_data = filter_sizes.template flat<int32>().data();\n+    const int32_t* filter_sizes_data =\n+        filter_sizes.template flat<int32_t>().data();\n     for (int i = 0; i < filter_sizes.NumElements(); ++i) {\n       OP_REQUIRES(context, filter_sizes_data[i] >= 0,\n                   errors::InvalidArgument(\"Dimension \", i,\n@@ -1249,7 +1250,7 @@ class DepthwiseConv2dNativeBackpropFilterOp : public OpKernel {\n   bool use_cudnn_grouped_conv_;\n \n  private:\n-  std::vector<int32> strides_;\n+  std::vector<int32_t> strides_;\n   Padding padding_;\n   std::vector<int64_t> explicit_paddings_;\n   TensorFormat data_format_;"
        }
    ],
    "stats": {
        "total": 413,
        "additions": 210,
        "deletions": 203
    }
}