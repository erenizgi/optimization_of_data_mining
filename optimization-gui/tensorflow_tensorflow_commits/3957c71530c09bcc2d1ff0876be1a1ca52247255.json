{
    "author": "GleasonK",
    "message": "Don't change function signatures in rewrite pass\n\nPiperOrigin-RevId: 808792209",
    "sha": "3957c71530c09bcc2d1ff0876be1a1ca52247255",
    "files": [
        {
            "sha": "d2c8485cf77d50e382d6df092ea7a3836380adf2",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_legalize_quant_composite.cpp",
            "status": "modified",
            "additions": 95,
            "deletions": 205,
            "changes": 300,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3957c71530c09bcc2d1ff0876be1a1ca52247255/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_legalize_quant_composite.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3957c71530c09bcc2d1ff0876be1a1ca52247255/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_legalize_quant_composite.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_legalize_quant_composite.cpp?ref=3957c71530c09bcc2d1ff0876be1a1ca52247255",
            "patch": "@@ -67,14 +67,13 @@ namespace {\n  *   - Requires exactly one operand and one result.\n  *   - Operand type must be a float type.\n  *   - Result type must be an integer type.\n- *   - All users of the quantize op must be either quant.dequantize composite\n- *   ops or func::ReturnOps.\n+ *   - All users of the quantize op must be quant.dequantize composites.\n  *\n  * - **quant.dequantize:**\n  *   - Requires exactly one operand and one result.\n  *   - Legalization is supported only if the operand is:\n- *     - The only user of a block argument, or\n- *     - Defined by a stablehlo::UniformQuantizeOp.\n+ *     - quant.quantize composite op\n+ *     - quantized type.\n  *   - Operand type must be an integer type or a quant::QuantizedType. The later\n  *      case is for the case where the operand is defined by a\n  *      stablehlo::UniformQuantizeOp.\n@@ -83,71 +82,29 @@ namespace {\n  * - Any other composite op name is unsupported.\n  **/\n \n-llvm::LogicalResult isSupportedQuantCompositeOp(stablehlo::CompositeOp op) {\n-  if (op.getName() != \"quant.quantize\" && op.getName() != \"quant.dequantize\" &&\n-      op.getName() != \"quant.fake_quant\") {\n-    return failure();\n-  }\n-\n-  if (op.getNumOperands() != 1 || op.getNumResults() != 1) {\n-    return failure();\n-  }\n-\n-  if (op.getName() == \"quant.fake_quant\") {\n-    if (op.getOperand(0).getType() != op.getType(0)) {\n-      return failure();\n-    }\n-    if (!isa<FloatType>(getElementTypeOrSelf(op.getType(0)))) {\n-      return failure();\n-    }\n-\n-    auto dtypeAttr = llvm::dyn_cast_or_null<TypeAttr>(\n-        op.getCompositeAttributes().get(\"dtype\"));\n-    if (dtypeAttr == nullptr) {\n-      return failure();\n-    }\n-    return success();\n-  }\n-\n-  if (op.getName() == \"quant.quantize\") {\n-    if (!isa<FloatType>(getElementTypeOrSelf(op.getOperand(0).getType()))) {\n-      return failure();\n-    }\n-    if (!isa<IntegerType>(getElementTypeOrSelf(op.getType(0)))) {\n-      return failure();\n-    }\n-    for (auto* user : op->getUsers()) {\n-      bool isFedToDequantizeComposite =\n-          isa<stablehlo::CompositeOp>(user) &&\n-          cast<stablehlo::CompositeOp>(user).getName() == \"quant.dequantize\";\n-      bool isFedToReturnOp = isa<func::ReturnOp>(user);\n-      if (!isFedToDequantizeComposite && !isFedToReturnOp) {\n-        return failure();\n-      }\n-    }\n+bool isRewritableQuantizeCompositeOp(stablehlo::CompositeOp op) {\n+  return op.getName() == \"quant.quantize\" && op.getNumOperands() == 1 &&\n+         op.getNumResults() == 1 &&\n+         isa<FloatType>(getElementTypeOrSelf(op.getOperand(0).getType())) &&\n+         isa<IntegerType>(getElementTypeOrSelf(op.getType(0)));\n+}\n \n-    return success();\n-  }\n+bool isRewritableDequantizeCompositeOp(stablehlo::CompositeOp op) {\n+  return op.getName() == \"quant.dequantize\" && op.getNumOperands() == 1 &&\n+         op.getNumResults() == 1 &&\n+         (isa<IntegerType>(getElementTypeOrSelf(op.getOperand(0).getType())) ||\n+          isa<quant::QuantizedType>(\n+              getElementTypeOrSelf(op.getOperand(0).getType()))) &&\n+         isa<FloatType>(getElementTypeOrSelf(op.getType(0)));\n+}\n \n-  // op.getName() == \"quant.dequantize\n-  bool isOnlyUserOfBlockArgument =\n-      isa<BlockArgument>(op.getOperand(0)) &&\n-      cast<BlockArgument>(op.getOperand(0)).hasOneUse();\n-  bool isDefinedByQuantizeOp =\n-      op.getOperand(0).getDefiningOp() != nullptr &&\n-      mlir::isa<stablehlo::UniformQuantizeOp>(op.getOperand(0).getDefiningOp());\n-  if (!isOnlyUserOfBlockArgument && !isDefinedByQuantizeOp) {\n-    return failure();\n-  }\n-  if (!isa<IntegerType>(getElementTypeOrSelf(op.getOperand(0).getType())) &&\n-      !isa<quant::QuantizedType>(\n-          getElementTypeOrSelf(op.getOperand(0).getType()))) {\n-    return failure();\n-  }\n-  if (!isa<FloatType>(getElementTypeOrSelf(op.getType(0)))) {\n-    return failure();\n-  }\n-  return success();\n+bool isRewritableFakeQuantCompositeOp(stablehlo::CompositeOp op) {\n+  return op.getName() == \"quant.fake_quant\" && op.getNumOperands() == 1 &&\n+         op.getNumResults() == 1 &&\n+         op.getOperand(0).getType() == op.getType(0) &&\n+         isa<FloatType>(getElementTypeOrSelf(op.getType(0))) &&\n+         llvm::dyn_cast_or_null<TypeAttr>(\n+             op.getCompositeAttributes().get(\"dtype\"));\n }\n \n /**\n@@ -224,44 +181,34 @@ LogicalResult getQuantCompositeAttributes(\n   return success();\n }\n \n-class RewriteQuantizeCompositeOp\n-    : public OpRewritePattern<stablehlo::CompositeOp> {\n-  using OpRewritePattern<stablehlo::CompositeOp>::OpRewritePattern;\n-\n-  LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n-                                PatternRewriter& rewriter) const final {\n-    if (op.getName() != \"quant.quantize\") {\n-      return failure();\n-    }\n-\n-    SmallVector<double> scales;\n-    SmallVector<int64_t> zeroPoints;\n-    int32_t quantizedDimension;\n-    int64_t storageTypeMin;\n-    int64_t storageTypeMax;\n-    std::string dtypeStr;\n-    Type storageType;\n-\n-    if (failed(getQuantCompositeAttributes(op, scales, zeroPoints,\n-                                           quantizedDimension, storageTypeMin,\n-                                           storageTypeMax, storageType))) {\n-      return failure();\n-    }\n-\n-    Type expressedType = getElementTypeOrSelf(op.getInputs().front().getType());\n-    Type quantizedElementType = stablehlo::getQuantizedElementType(\n-        op.getLoc(), storageType, expressedType, scales, zeroPoints,\n-        quantizedDimension, storageTypeMin, storageTypeMax);\n-    RankedTensorType outputQuantizedType = RankedTensorType::get(\n-        llvm::cast<ShapedType>(op.getResults().front().getType()).getShape(),\n-        quantizedElementType);\n-    auto stablehloQuantizeOp = rewriter.create<stablehlo::UniformQuantizeOp>(\n-        op.getLoc(), outputQuantizedType,\n-        /*input=*/op.getOperand(0));\n-    rewriter.replaceAllOpUsesWith(op, stablehloQuantizeOp.getResult());\n-    return success();\n+FailureOr<stablehlo::UniformQuantizeOp> buildUniformQuantizeOp(\n+    stablehlo::CompositeOp op, PatternRewriter& rewriter) {\n+  SmallVector<double> scales;\n+  SmallVector<int64_t> zeroPoints;\n+  int32_t quantizedDimension;\n+  int64_t storageTypeMin;\n+  int64_t storageTypeMax;\n+  std::string dtypeStr;\n+  Type storageType;\n+\n+  if (failed(getQuantCompositeAttributes(op, scales, zeroPoints,\n+                                         quantizedDimension, storageTypeMin,\n+                                         storageTypeMax, storageType))) {\n+    return rewriter.notifyMatchFailure(op,\n+                                       \"Failed to get quantization attributes\");\n   }\n-};\n+\n+  Type expressedType = getElementTypeOrSelf(op.getInputs().front().getType());\n+  Type quantizedElementType = stablehlo::getQuantizedElementType(\n+      op.getLoc(), storageType, expressedType, scales, zeroPoints,\n+      quantizedDimension, storageTypeMin, storageTypeMax);\n+  RankedTensorType outputQuantizedType = RankedTensorType::get(\n+      llvm::cast<ShapedType>(op.getResults().front().getType()).getShape(),\n+      quantizedElementType);\n+  return stablehlo::UniformQuantizeOp::create(rewriter, op.getLoc(),\n+                                              outputQuantizedType,\n+                                              /*input=*/op.getOperand(0));\n+}\n \n class RewriteDequantizeCompositeOp\n     : public OpRewritePattern<stablehlo::CompositeOp> {\n@@ -270,7 +217,11 @@ class RewriteDequantizeCompositeOp\n   LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n                                 PatternRewriter& rewriter) const final {\n     if (op.getName() != \"quant.dequantize\") {\n-      return failure();\n+      return rewriter.notifyMatchFailure(op, \"Not a dequantize composite op\");\n+    }\n+    if (!isRewritableDequantizeCompositeOp(op)) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"Not a rewritable dequantize composite op\");\n     }\n \n     SmallVector<double> scales;\n@@ -287,54 +238,33 @@ class RewriteDequantizeCompositeOp\n       return failure();\n     }\n \n-    Type expressedType =\n-        getElementTypeOrSelf(op.getResults().front().getType());\n-    Type quantizedElementType = stablehlo::getQuantizedElementType(\n-        op.getLoc(), storageType, expressedType, scales, zeroPoints,\n-        quantizedDimension, storageTypeMin, storageTypeMax);\n-    auto quantizedType =\n-        llvm::cast<ShapedType>(op.getResults().front().getType())\n-            .clone(quantizedElementType);\n-\n-    // If the operand of the dequantize compposite op defined by block\n-    // argument, we need to create a new block argument with the quantized type.\n-    // Otherwise, we can directly use the composite op's operand.\n+    // If operand is already quantized, rewrite\n     Value quantizedInput = op.getOperand(0);\n-    if (isa<BlockArgument>(op.getOperand(0))) {\n-      auto funcOp = op->getParentOfType<func::FuncOp>();\n-      if (funcOp == nullptr) {\n-        return rewriter.notifyMatchFailure(op,\n-                                           \"Failed to find enclosing function\");\n-      }\n-      SmallVector<Type> newFuncInputTypes;\n-      auto funcInputTypes = funcOp.getFunctionType().getInputs();\n-      int updatedArgIdx = -1;\n-      for (auto [i, arg] : llvm::enumerate(funcOp.getArguments())) {\n-        if (arg == quantizedInput) {\n-          newFuncInputTypes.push_back(quantizedType);\n-          updatedArgIdx = i;\n-        } else {\n-          newFuncInputTypes.push_back(funcInputTypes[i]);\n-        }\n-      }\n-      rewriter.modifyOpInPlace(funcOp, [&]() {\n-        funcOp.setType(rewriter.getFunctionType(\n-            newFuncInputTypes, funcOp.getFunctionType().getResults()));\n-      });\n-      funcOp.getBody()\n-          .front()\n-          .getArgument(updatedArgIdx)\n-          .setType(quantizedType);\n-      quantizedInput = funcOp.getBody().front().getArgument(updatedArgIdx);\n+    if (isa<quant::QuantizedType>(\n+            getElementTypeOrSelf(op.getOperand(0).getType()))) {\n+      rewriter.replaceOpWithNewOp<stablehlo::UniformDequantizeOp>(\n+          op, op.getType(0),\n+          /*input=*/quantizedInput);\n+      return success();\n     }\n \n-    auto stablehloDeQuantizeOp =\n-        rewriter.create<stablehlo::UniformDequantizeOp>(\n-            op.getLoc(), op.getType(0),\n-            /*input=*/quantizedInput);\n-    rewriter.eraseOp(op);\n-    rewriter.replaceAllOpUsesWith(op, stablehloDeQuantizeOp.getResult());\n+    // Otherwise the operand must be a composite op\n+    auto quantizeCompositeOp =\n+        quantizedInput.getDefiningOp<stablehlo::CompositeOp>();\n+    if (!quantizeCompositeOp ||\n+        !isRewritableQuantizeCompositeOp(quantizeCompositeOp)) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"Operand is not quantized or a quantize composite op\");\n+    }\n+\n+    auto quantizeOp = buildUniformQuantizeOp(quantizeCompositeOp, rewriter);\n+    if (failed(quantizeOp)) {\n+      return rewriter.notifyMatchFailure(op, \"Failed to build quantize op\");\n+    }\n \n+    rewriter.replaceOpWithNewOp<stablehlo::UniformDequantizeOp>(\n+        op, op.getType(0),\n+        /*input=*/quantizeOp->getResult());\n     return success();\n   }\n };\n@@ -352,7 +282,11 @@ class RewriteFakeQuantCompositeOp\n   LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n                                 PatternRewriter& rewriter) const final {\n     if (op.getName() != \"quant.fake_quant\") {\n-      return failure();\n+      return rewriter.notifyMatchFailure(op, \"Not a fake quant composite op\");\n+    }\n+    if (!isRewritableFakeQuantCompositeOp(op)) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"Not a rewritable fake quant composite op\");\n     }\n \n     SmallVector<double> scales;\n@@ -377,32 +311,9 @@ class RewriteFakeQuantCompositeOp\n         llvm::cast<ShapedType>(op.getType(0)).getShape(), quantizedElementType);\n     auto stablehloQuantizeOp = rewriter.create<stablehlo::UniformQuantizeOp>(\n         op.getLoc(), quantizedType, /*input=*/op.getOperand(0));\n-    auto stablehloDeQuantizeOp =\n-        rewriter.create<stablehlo::UniformDequantizeOp>(\n-            op.getLoc(), op.getType(0),\n-            /*input=*/stablehloQuantizeOp.getResult());\n-    rewriter.replaceAllOpUsesWith(op, stablehloDeQuantizeOp.getResult());\n-    return success();\n-  }\n-};\n-\n-/**\n- * When there is a quantize op at the output, the return op's operand is a\n- * quantized tensor. However, the function's return type is still a simple\n- * integer. This pattern makes sure the function's signature is updated so\n- * that it's return type conforms the operand of its return op.\n- */\n-struct UpdateFunctionTypePattern : public OpRewritePattern<func::ReturnOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-  LogicalResult matchAndRewrite(func::ReturnOp op,\n-                                PatternRewriter& rewriter) const override {\n-    auto funcOp = op->getParentOfType<func::FuncOp>();\n-    if (funcOp == nullptr) {\n-      return rewriter.notifyMatchFailure(op,\n-                                         \"Failed to find enclosing function\");\n-    }\n-    funcOp.setType(rewriter.getFunctionType(funcOp.getArgumentTypes(),\n-                                            op.getOperandTypes()));\n+    rewriter.replaceOpWithNewOp<stablehlo::UniformDequantizeOp>(\n+        op, op.getType(0),\n+        /*input=*/stablehloQuantizeOp.getResult());\n     return success();\n   }\n };\n@@ -413,39 +324,18 @@ class StablehloLegalizeQuantCompositePass\n  public:\n   void runOnOperation() override {\n     MLIRContext& ctx = getContext();\n-    auto module = getOperation();\n-\n     RewritePatternSet patterns(&ctx);\n-    patterns.add<RewriteQuantizeCompositeOp, RewriteDequantizeCompositeOp,\n-                 RewriteFakeQuantCompositeOp>(&ctx);\n-\n-    ConversionTarget target(getContext());\n-    target.addLegalDialect<func::FuncDialect>();\n-    target.addLegalDialect<quant::QuantDialect>();\n-\n-    // Declare all the MHLO ops as legal except for the quantization\n-    // composites we want to lower.\n-    target.addDynamicallyLegalDialect<stablehlo::StablehloDialect>(\n-        [](Operation* op) {\n-          auto compositeOp = dyn_cast_or_null<stablehlo::CompositeOp>(op);\n-          if (!compositeOp) {\n-            return true;\n-          }\n-          return failed(isSupportedQuantCompositeOp(compositeOp));\n-        });\n-\n-    if (failed(applyPartialConversion(getOperation(), target,\n-                                      std::move(patterns)))) {\n+    patterns.add<RewriteDequantizeCompositeOp, RewriteFakeQuantCompositeOp>(\n+        &ctx);\n+\n+    GreedyRewriteConfig config;\n+    config.enableFolding(false);\n+    config.setMaxIterations(3);\n+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns),\n+                                     config))) {\n       getOperation().emitError(\"Composite lowering pass failed.\");\n       signalPassFailure();\n     }\n-\n-    GreedyRewriteConfig greedyRewriteConfig;\n-    RewritePatternSet cleanupPatterns(&ctx);\n-    cleanupPatterns.add<UpdateFunctionTypePattern>(&ctx);\n-\n-    (void)applyPatternsGreedily(module, std::move(cleanupPatterns),\n-                                greedyRewriteConfig);\n   }\n };\n "
        },
        {
            "sha": "7ff575c77733167314a93269ea62f5ea9e3ab19c",
            "filename": "third_party/xla/xla/mlir_hlo/tests/stablehlo_ext/stablehlo_legalize_quant_composite.mlir",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3957c71530c09bcc2d1ff0876be1a1ca52247255/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fstablehlo_legalize_quant_composite.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3957c71530c09bcc2d1ff0876be1a1ca52247255/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fstablehlo_legalize_quant_composite.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fstablehlo_legalize_quant_composite.mlir?ref=3957c71530c09bcc2d1ff0876be1a1ca52247255",
            "patch": "@@ -18,8 +18,7 @@ func.func private @quant.fake_quant.impl(%arg0: tensor<2x2xf32>) -> tensor<2x2xf\n \n // CHECK-LABEL: func @quantize_feed_to_return\n func.func @quantize_feed_to_return(%arg0: tensor<2x2xf32>) -> tensor<2x2xi8> {\n-  // CHECK: %[[TEMP_0:.*]] = stablehlo.uniform_quantize %arg0 : (tensor<2x2xf32>) -> tensor<2x2x!quant.uniform<i8:f32:1, {0.0016193275805562735,0.0016197443474084139}>>\n-  // CHECK: return %[[TEMP_2:.*]]\n+  // CHECK: \"quant.quantize\"\n   %0 = stablehlo.composite \"quant.quantize\" %arg0 {composite_attributes = {dtype = i8, quantization_dimension = 1 : i32, scale = dense<[0.00161932758, 0.0016197443]> : tensor<2xf32>, zero_point = dense<0> : tensor<2xi64>, storage_type_min=-128, storage_type_max=127}, decomposition = @quant.quant.impl} : (tensor<2x2xf32>) -> tensor<2x2xi8>\n   return %0 : tensor<2x2xi8>\n }\n@@ -33,10 +32,8 @@ func.func private @quant.quant.impl(%arg0: tensor<2x2xf32>) -> tensor<2x2xi8> {\n // -----\n \n // CHECK-LABEL: func @args_feeding_dequantize\n-// CHECK-SAME:      arg0: tensor<2x2x!quant.uniform\n func.func @args_feeding_dequantize(%arg0: tensor<2x2xi8>) -> tensor<2x2xf32> {\n-  // CHECK:         %[[TEMP_0:.*]] = stablehlo.uniform_dequantize %arg0\n-  // CHECK-NEXT:    return %[[TEMP_1:.*]] : tensor<2x2xf32>\n+  // CHECK: quant.dequantize\n   %0 = stablehlo.composite \"quant.dequantize\" %arg0 {composite_attributes = {dtype = i8, quantization_dimension = 1 : i32, scale = dense<[0.00161932758, 0.0016197443]> : tensor<2xf32>, zero_point = dense<0> : tensor<2xi64>, storage_type_min=-128, storage_type_max=127}, decomposition = @quant.dequant.impl} : (tensor<2x2xi8>) -> tensor<2x2xf32>\n   return %0 : tensor<2x2xf32>\n }\n@@ -109,9 +106,10 @@ func.func private @quant.dequant.impl(%arg0: tensor<2x2xi8>) -> tensor<2x2xf32>\n \n // CHECK-LABEL: func @quantize_feeding_dequantize_and_return\n func.func @quantize_feeding_dequantize_and_return(%arg0: tensor<2x2xf32>) -> (tensor<2x2xf32> , tensor<2x2xi8>) {\n+  // CHECK: %[[COMP:.*]] = stablehlo.composite \"quant.quantize\"\n   // CHECK: %[[TEMP_0:.*]] = stablehlo.uniform_quantize %arg0\n   // CHECK: %[[TEMP_1:.*]] = stablehlo.uniform_dequantize %[[TEMP_0]]\n-  // CHECK: return %[[TEMP_1]], %[[TEMP_0]] : tensor<2x2xf32>, tensor<2x2x!quant.uniform\n+  // CHECK: return %[[TEMP_1]], %[[COMP]] : tensor<2x2xf32>, tensor<2x2xi8>\n   %0 = stablehlo.composite \"quant.quantize\" %arg0 {composite_attributes = {dtype = i8, quantization_dimension = 1 : i32, scale = dense<[0.00161932758, 0.0016197443]> : tensor<2xf32>, zero_point = dense<0> : tensor<2xi64>, storage_type_min=-128, storage_type_max=127}, decomposition = @quant.quant.impl} : (tensor<2x2xf32>) -> tensor<2x2xi8>\n   %1 = stablehlo.composite \"quant.dequantize\" %0 {composite_attributes = {dtype = i8, quantization_dimension = 1 : i32, scale = dense<[0.00161932758, 0.0016197443]> : tensor<2xf32>, zero_point = dense<0> : tensor<2xi64>, storage_type_min=-128, storage_type_max=127}, decomposition = @quant.dequant.impl} : (tensor<2x2xi8>) -> tensor<2x2xf32>\n   return %1, %0 : tensor<2x2xf32> , tensor<2x2xi8>"
        }
    ],
    "stats": {
        "total": 310,
        "additions": 99,
        "deletions": 211
    }
}