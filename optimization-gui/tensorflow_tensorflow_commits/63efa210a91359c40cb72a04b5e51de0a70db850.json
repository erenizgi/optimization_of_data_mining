{
    "author": "bchetioui",
    "message": "[XLA:GPU] Make setting `--xla_gpu_unsupported_generic_triton_emitter_features` into a no-op.\n\nDelete all the underlying code.\n\nPiperOrigin-RevId: 836140782",
    "sha": "63efa210a91359c40cb72a04b5e51de0a70db850",
    "files": [
        {
            "sha": "bdb247e4e3f577716c6d6067d33c0b0d4286de29",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -102,16 +102,6 @@ class GpuCodegenBackend : public CodegenBackend {\n     // Avoid using GPU graphs as we don't want to measure graph construction\n     // time.\n     debug_options.clear_xla_gpu_enable_command_buffer();\n-    // Make sure to use the generic Triton emitter for everything.\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n     // Avoid using async dot as we don't want to measure event overheads.\n     debug_options.set_xla_gpu_async_dot(false);\n     debug_options.set_xla_embed_ir_in_executable(false);"
        },
        {
            "sha": "5a475834096ffb7cb8862338c3f8a8340f4a5cf9",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -213,18 +213,6 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonBackend::RunHloPasses(\n \n   NestGemmFusion nest_gemm_fusion(gpu_device_info, mlir_context_);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(hlo_module.get()).status());\n-\n-  bool is_legacy_gemm_disabled = absl::c_contains(\n-      debug_options().xla_gpu_unsupported_generic_triton_emitter_features(),\n-      DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n-  bool is_triton_gemm_fusion =\n-      IsGpuFusionKind(*hlo_module->entry_computation()->root_instruction(),\n-                      kTritonGemmFusionKind);\n-  if (is_legacy_gemm_disabled && is_triton_gemm_fusion) {\n-    return absl::InternalError(\n-        absl::StrCat(\"Unexpected \", kTritonGemmFusionKind,\n-                     \" fusion: \", hlo_module->ToString()));\n-  }\n   return hlo_module;\n }\n "
        },
        {
            "sha": "4bbcc293b50719363a2c9fe338aff26c3f491ed1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include <string>\n #include <tuple>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include <gmock/gmock.h>\n@@ -81,16 +80,6 @@ class AlgorithmTest : public GpuCodegenTest {\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n     debug_options.set_xla_gpu_autotune_level(0);\n-    // TODO(b/393299275): remove when these flags are on by default.\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n     return debug_options;\n   }\n "
        },
        {
            "sha": "a733c8b505ad765370af92c57d1839f4139b1570",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -87,15 +87,6 @@ class TritonTest : public GpuCodegenTest {\n  public:\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n-    auto* emitter_opts =\n-        debug_options\n-            .mutable_xla_gpu_unsupported_generic_triton_emitter_features();\n-    emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n-    emitter_opts->Add(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    emitter_opts->Add(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n     debug_options.set_xla_gpu_autotune_level(0);\n     return debug_options;\n   }\n@@ -160,14 +151,6 @@ class TritonGemmTest : public TritonTest {\n     debug_options.set_xla_gpu_enable_split_k_autotuning(false);\n     // Always rewrite Gemms with Triton regardless of size.\n     debug_options.set_xla_gpu_gemm_rewrite_size_threshold(0);\n-    // TODO(b/393299275): remove when generic emitter is fully enabled.\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n     return debug_options;\n   }\n "
        },
        {
            "sha": "8e5f55c4c5d6dd8f0cc0443b4178421a22315db9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -4867,14 +4867,6 @@ class TritonScaledDotGemmTest\n     debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n     debug_options.set_xla_gpu_autotune_level(0);\n     debug_options.set_xla_gpu_cublas_fallback(false);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n     return debug_options;\n   }\n };\n@@ -5062,8 +5054,6 @@ class TritonScaledDotTest : public TritonEmitterTest {\n     debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n     debug_options.set_xla_gpu_autotune_level(0);\n     debug_options.set_xla_gpu_cublas_fallback(false);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n     return debug_options;\n   }\n "
        },
        {
            "sha": "293ab253d8a93a0bcf914dce4d55b8efe7a73b06",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 31,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -326,15 +326,6 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n       DebugOptions::PARTITIONING_ALGORITHM_NOOP);\n \n   opts.set_xla_gpu_enable_triton_gemm(true);\n-  opts.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-  opts.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-      DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-  opts.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-      DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n-  opts.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-      DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-  opts.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-      DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n   opts.set_xla_gpu_unsupported_enable_triton_multi_output_fusion(true);\n   opts.set_xla_gpu_enable_cudnn_int8x32_convolution_reordering(true);\n   opts.set_xla_gpu_triton_gemm_any(true);\n@@ -871,17 +862,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n         return true;\n       };\n \n-  auto xla_gpu_generic_triton_emitter_features_to_string =\n-      [](google::protobuf::RepeatedField<int> values) -> std::string {\n-    struct Formatter {\n-      void operator()(std::string* out, int type) const {\n-        absl::StrAppend(out,\n-                        DebugOptions::GenericTritonEmitterFeature_Name(type));\n-      }\n-    };\n-    return absl::StrJoin(values, \",\", Formatter());\n-  };\n-\n   // Custom \"sub-parser\" for xla_gpu_experimental_autotune_cache_mode.\n   auto detection_mode = [](DebugOptions* debug_options,\n                            const std::string& value)\n@@ -1974,19 +1954,11 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n                 bool_setter_for(&DebugOptions::set_xla_gpu_enable_triton_gemm),\n                 debug_options->xla_gpu_enable_triton_gemm(),\n                 \"[Stable] Whether to use Triton-based matrix multiplication.\"));\n+  // TODO(b/393299275): remove. The flag is left here as a no-op to migrate\n+  // users separately from deleting the underlying functionality.\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_unsupported_generic_triton_emitter_features\",\n-      SetterForRepeatedEnum<DebugOptions::GenericTritonEmitterFeature>(\n-          \"xla_gpu_unsupported_generic_triton_emitter_features\",\n-          /*enum_prefix=*/\"GENERIC_TRITON_EMITTER_\",\n-          &DebugOptions::GenericTritonEmitterFeature_Parse,\n-          debug_options\n-              ->mutable_xla_gpu_unsupported_generic_triton_emitter_features()),\n-      xla_gpu_generic_triton_emitter_features_to_string(\n-          debug_options->xla_gpu_unsupported_generic_triton_emitter_features()),\n-      \"Comma-separated list of individual features of generic Triton emitter. \"\n-      \"Use +/- prefix to modify the default list, or list features to enable \"\n-      \"explicitly - that will override the defaults.\"));\n+      noop_flag_setter<std::string>, \"\", \"[Deprecated, do not use].\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_unsupported_enable_triton_multi_output_fusion\",\n       bool_setter_for("
        },
        {
            "sha": "717394aded86d4fa16799a7314a66f2b9c328ca1",
            "filename": "third_party/xla/xla/debug_options_parsers_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 53,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -388,59 +388,6 @@ TEST(ParseRepeatedEnumModifiersTest, Invalid) {\n               StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n-TEST(ParseRepeatedEnumFlagsTest, GenericTritonEmitterFeatures) {\n-  DebugOptions debug_options = DefaultDebugOptionsIgnoringFlags();\n-  const auto& enabled_features =\n-      debug_options.xla_gpu_unsupported_generic_triton_emitter_features();\n-\n-  // Check default setting.\n-  ASSERT_THAT(\n-      enabled_features,\n-      testing::UnorderedElementsAre(\n-          DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM,\n-          DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM,\n-          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES,\n-          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION));\n-\n-  // Initialize the flag objects.\n-  std::vector<tsl::Flag> flag_objects;\n-  MakeDebugOptionsFlags(&flag_objects, &debug_options);\n-\n-  // Adding options.\n-  SetXlaFlagsEnvVar(\n-      \"--xla_gpu_unsupported_generic_triton_emitter_features=\"\n-      \"-allow_all_gemm_shapes\");\n-  ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n-  EXPECT_THAT(\n-      enabled_features,\n-      testing::UnorderedElementsAre(\n-          DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM,\n-          DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM,\n-          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION));\n-\n-  // Overwriting options.\n-  SetXlaFlagsEnvVar(\n-      \"--xla_gpu_unsupported_generic_triton_emitter_features=disable_legacy_\"\n-      \"gemm,allow_all_ops_in_gemm_fusion\");\n-  ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n-  EXPECT_THAT(\n-      enabled_features,\n-      testing::UnorderedElementsAre(\n-          DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM,\n-          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION));\n-\n-  // More adding/removing options. Do not add duplicates.\n-  SetXlaFlagsEnvVar(\n-      \"--xla_gpu_unsupported_generic_triton_emitter_features=-disable_legacy_\"\n-      \"gemm,-unspecified,+enable_nested_gemm,+allow_all_ops_in_gemm_fusion\");\n-  ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n-  EXPECT_THAT(\n-      enabled_features,\n-      testing::UnorderedElementsAre(\n-          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION,\n-          DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM));\n-}\n-\n TEST(ParseRepeatedEnumFlagsTest, CommandBufferCmdType) {\n   DebugOptions debug_options = DefaultDebugOptionsIgnoringFlags();\n "
        },
        {
            "sha": "b617bd80b09abd1f4c3d3e8f0523185791e16c5a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -124,6 +124,7 @@ cc_library(\n     deps = if_cuda_is_configured([\":gemm_fusion_autotuner_cuda\"]) + if_rocm_is_configured([\n         \":gemm_fusion_autotuner_rocm\",\n     ]) + [\n+        \":autotune_cache_key\",\n         \":autotuner_compile_util\",\n         \":autotuner_pass\",\n         \":autotuner_status_key\",\n@@ -237,6 +238,7 @@ xla_test(\n         \"no_mac\",\n     ],\n     deps = [\n+        \":autotune_cache_key\",\n         \":autotuner_util\",\n         \":gemm_fusion_autotuner\",\n         \"//xla:autotune_results_proto_cc\",\n@@ -274,7 +276,6 @@ xla_test(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "44d5275d18bae42d00050bb50f6dbd02d47b5464",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -68,6 +68,7 @@ limitations under the License.\n #include \"xla/service/compiler.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n #include \"xla/service/gpu/autotuning/autotuner_compile_util.h\"\n #include \"xla/service/gpu/autotuning/autotuner_pass.h\"\n #include \"xla/service/gpu/autotuning/autotuner_status_key.h\"\n@@ -97,12 +98,10 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/matmul_indexing_utils.h\"\n #include \"xla/service/shaped_buffer.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler_helpers.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n@@ -120,7 +119,6 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/path.h\"\n-#include \"tsl/platform/protobuf.h\"\n #include \"tsl/profiler/lib/scoped_annotation.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n \n@@ -346,17 +344,6 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n \n   NestGemmFusion nest_gemm_fusion(gpu_device_info, mlir_context);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(new_module.get()).status());\n-  bool is_legacy_gemm_disabled = absl::c_contains(\n-      debug_opts.xla_gpu_unsupported_generic_triton_emitter_features(),\n-      DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n-  bool is_triton_gemm_fusion =\n-      IsGpuFusionKind(*new_module->entry_computation()->root_instruction(),\n-                      kTritonGemmFusionKind);\n-  if (is_legacy_gemm_disabled && is_triton_gemm_fusion) {\n-    return absl::InternalError(\n-        absl::StrCat(\"Unexpected \", kTritonGemmFusionKind,\n-                     \" fusion: \", new_module->ToString()));\n-  }\n   return new_module;\n }\n \n@@ -1092,13 +1079,6 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n             fusion, opts, mlir_context_,\n             allow_filtering_kernels_spilling_registers);\n       });\n-      if (absl::c_contains(\n-              debug_options_\n-                  .xla_gpu_unsupported_generic_triton_emitter_features(),\n-              DebugOptions::\n-                  GENERIC_TRITON_EMITTER_MUST_ACCEPT_ALL_AUTOTUNER_CONFIGS)) {\n-        return executable_or;\n-      }\n       absl::StatusCode code = executable_or.status().code();\n       // TODO(b/447113513): we should not silently ignore that wide range of\n       // errors as we might hide real regressions and drop the optimal"
        },
        {
            "sha": "614b2e3e4d1e49a5a5a4e01daf99fe35cfe1fa76",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n-#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -54,6 +53,7 @@ limitations under the License.\n #include \"xla/service/call_inliner.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n@@ -1837,8 +1837,6 @@ class GemmFusionAutotunerEnableTma : public GemmFusionAutotunerTest {\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options =\n         GemmFusionAutotunerTest::GetDebugOptionsForTest();\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n     debug_options.set_xla_gpu_experimental_enable_triton_tma(true);\n     return debug_options;\n   }"
        },
        {
            "sha": "7f9f0729f75906d5a9bb32ac8530e5f128199741",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -1759,10 +1759,6 @@ TEST_F(PassOrderTest, GemmRewriterRunsAfterDotNormalizer) {\n TEST_F(PassOrderTest, NestGemmFusionRunsAfterGemmFusionAutotuner) {\n   // NestGemmFusion expect to see __triton_gemm custom call with a backend\n   // config created by gemm_fusion_autotuner.\n-  DebugOptions options = GetDebugOptionsForTest();\n-  options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-      DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-  SetDebugOptions(options);\n   VerifyPassOrder(\"gemm-fusion-autotuner\", \"nest_gemm_fusion\");\n }\n "
        },
        {
            "sha": "5f65f922a9395192e39c9da42159acf3ea87ed76",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -1946,7 +1946,6 @@ cc_library(\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/model:triton_emitter_constraints\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tools:hlo_extractor\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "05d23ba75ec1f7c0ecd68178587e070d86ba6258",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 122,
            "changes": 134,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -65,7 +65,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tools/hlo_extractor.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -1099,15 +1098,6 @@ absl::Status TryHoistBitcastsInComputationToCallers(HloInstruction* dot,\n   return absl::OkStatus();\n }\n \n-bool IsFeatureEnabled(const HloModule* module,\n-                      DebugOptions::GenericTritonEmitterFeature feature) {\n-  return absl::c_contains(\n-      module->config()\n-          .debug_options()\n-          .xla_gpu_unsupported_generic_triton_emitter_features(),\n-      feature);\n-}\n-\n class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n  public:\n   explicit NestGemmFusionVisitor(\n@@ -1118,76 +1108,22 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n         device_description_(device_description) {}\n \n  private:\n-  absl::Status AcceptDotOperand(const HloInstruction* operand,\n-                                absl::Span<const int64_t> batch_dims,\n-                                absl::Span<const int64_t> contracting_dims) {\n-    if (contracting_dims.size() != 1) {\n-      return absl::InternalError(absl::StrCat(\n-          \"Expected operand with exactly one contracting dimension, got \",\n-          contracting_dims.size()));\n-    }\n-\n-    TF_ASSIGN_OR_RETURN(\n-        std::vector<int64_t> non_contracting_dimensions,\n-        GetNonContractingDims(operand->shape(), batch_dims, contracting_dims));\n-    if (non_contracting_dimensions.size() != 1) {\n-      return absl::InternalError(absl::StrCat(\n-          \"Expected operand with exactly one non-contracting dimension, got \",\n-          non_contracting_dimensions.size()));\n-    }\n-\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status AcceptDotInstruction(const HloDotInstruction* dot) {\n-    if (IsFeatureEnabled(\n-            dot->GetModule(),\n-            DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES)) {\n-      return absl::OkStatus();\n-    }\n-    const HloInstruction* lhs = dot->operand(0);\n-    const HloInstruction* rhs = dot->operand(1);\n-    auto dims = dot->dot_dimension_numbers();\n-    TF_RETURN_IF_ERROR(AcceptDotOperand(lhs, dims.lhs_batch_dimensions(),\n-                                        dims.lhs_contracting_dimensions()));\n-    TF_RETURN_IF_ERROR(AcceptDotOperand(rhs, dims.rhs_batch_dimensions(),\n-                                        dims.rhs_contracting_dimensions()));\n-    return absl::OkStatus();\n-  }\n-\n   absl::Status AcceptNestedInstruction(const HloInstruction* instruction) {\n     if (instruction->IsElementwise()) {\n       return absl::OkStatus();\n     }\n-    switch (instruction->opcode()) {\n-      case HloOpcode::kBroadcast:\n-      case HloOpcode::kConstant:\n-      case HloOpcode::kPad:\n-      case HloOpcode::kParameter:\n-        return absl::OkStatus();\n-      case HloOpcode::kFusion:\n-        return AcceptResultingFusion(Cast<HloFusionInstruction>(instruction));\n-      case HloOpcode::kScaledDot:\n-        if (instruction->GetModule()\n-                ->config()\n-                .debug_options()\n-                .xla_gpu_experimental_scaled_dot_with_triton()) {\n-          return absl::OkStatus();\n-        }\n-        return absl::InternalError(\"Scaled dot with Triton is not enabled.\");\n-      case HloOpcode::kDot:\n-        return AcceptDotInstruction(Cast<HloDotInstruction>(instruction));\n-      default:\n-        if (!IsFeatureEnabled(\n-                instruction->GetModule(),\n-                DebugOptions::\n-                    GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION)) {\n-          return absl::InternalError(absl::StrCat(\n-              \"Instruction \", HloOpcodeString(instruction->opcode()),\n-              \" is not allowed in nested GEMM fusion.\"));\n-        }\n-        return absl::OkStatus();\n+    const DebugOptions& debug_options =\n+        instruction->GetModule()->config().debug_options();\n+    if (instruction->opcode() == HloOpcode::kScaledDot &&\n+        !debug_options.xla_gpu_experimental_scaled_dot_with_triton()) {\n+      return absl::InternalError(\"Scaled dot with Triton is not enabled.\");\n+    }\n+\n+    if (instruction->opcode() == HloOpcode::kFusion) {\n+      return AcceptResultingFusion(Cast<HloFusionInstruction>(instruction));\n     }\n+\n+    return absl::OkStatus();\n   }\n \n   // Checks whether all operations are from the \"tested\" set that we confirmed\n@@ -1261,42 +1197,7 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n         return absl::OkStatus();\n       }\n     }\n-    {\n-      // Symbolic tile analysis and nesting do not support all HLOs yet and\n-      // might leave the module in an invalid state. To avoid that we first dry\n-      // run the rewrite on an extracted module.\n-      // TODO(b/393299275): remove dry-run once we can handle all HLOs.\n-      std::unique_ptr<HloModule> extracted_module =\n-          ExtractInstructionIntoNewModule(*fusion);\n-      extracted_module->mutable_config().set_debug_options(\n-          fusion->GetModule()->config().debug_options());\n-      HloComputation* entry = extracted_module->entry_computation();\n-      HloFusionInstruction* extracted_fusion =\n-          Cast<HloFusionInstruction>(entry->root_instruction());\n-      if (extracted_fusion == nullptr) {\n-        return absl::InternalError(absl::StrCat(\n-            \"Failed to create a cloned module for fusion \", fusion->name()));\n-      }\n-      std::unique_ptr<CallGraph> cloned_call_graph =\n-          CallGraph::Build(extracted_module.get(), {});\n-      absl::Status status =\n-          RewriteFusion(extracted_fusion, cloned_call_graph.get());\n-      if (!status.ok()) {\n-        VLOG(2) << \"Failed to rewrite the fusion \" << fusion->ToString()\n-                << \" in a cloned module: \" << status;\n-        if (IsFeatureEnabled(\n-                fusion->GetModule(),\n-                DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n-          // As legacy emitter is disabled we are doomed to fail now, returning\n-          // the dry run result failure as it is a better diagnostic.\n-          return status;\n-        }\n-        return absl::OkStatus();\n-      }\n-    }\n-    absl::Status status = RewriteFusion(fusion, call_graph_);\n-    VLOG(2) << \"RewriteFusion \" << fusion->name() << \": \" << status;\n-    return status;\n+    return RewriteFusion(fusion, call_graph_);\n   }\n \n  private:\n@@ -1325,17 +1226,6 @@ absl::StatusOr<bool> NestGemmFusion::RunOnModule(\n absl::StatusOr<bool> NestGemmFusion::RunImpl(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  VLOG(2) << \"--xla_gpu_unsupported_generic_triton_emitter_features=\"\n-          << absl::StrJoin(\n-                 module->config()\n-                     .debug_options()\n-                     .xla_gpu_unsupported_generic_triton_emitter_features(),\n-                 \",\");\n-  if (!IsFeatureEnabled(\n-          module, DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM)) {\n-    VLOG(1) << \"Generic Triton emitter for gemms is disabled, exiting\";\n-    return false;\n-  }\n   return RunOnModule(module, execution_threads);\n }\n "
        },
        {
            "sha": "51df59db187a8f65448e143279770eb936c6d5bb",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -77,19 +77,6 @@ class NestGemmFusionTest : public HloHardwareIndependentTestBase {\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(\n           se::GpuComputeCapability{se::CudaComputeCapability::Ampere()})};\n   mlir::MLIRContext mlir_context_;\n-\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions options =\n-        HloHardwareIndependentTestBase::GetDebugOptionsForTest();\n-    auto* emitter_opts =\n-        options.mutable_xla_gpu_unsupported_generic_triton_emitter_features();\n-    emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    emitter_opts->Add(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    emitter_opts->Add(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n-    return options;\n-  }\n };\n \n TEST_F(NestGemmFusionTest, BasicTest) {"
        },
        {
            "sha": "5e52217a2ed11b7fbd09c64b13cec8dd56d9a81d",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 2,
            "deletions": 30,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/63efa210a91359c40cb72a04b5e51de0a70db850/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=63efa210a91359c40cb72a04b5e51de0a70db850",
            "patch": "@@ -317,27 +317,6 @@ message DebugOptions {\n     LHS = 2;\n   }\n \n-  // Options for the generic Triton emitter.\n-  // Set with xla_gpu_unsupported_generic_triton_emitter_features.\n-  enum GenericTritonEmitterFeature {\n-    // No specfic meaning, zero value for protobuf best practices.\n-    GENERIC_TRITON_EMITTER_UNSPECIFIED = 0;\n-    // Enable nest_gemm_fusion pass to convert gemms to be emitted by the\n-    // generic Triton emitter.\n-    GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM = 1;\n-    // Disable legacy GEMM emitter, that might lead to crashes if GEMM is not\n-    // supported by the generic emitter.\n-    GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM = 2;\n-    // Do not restrict which ops can be present in the GEMM fusion.\n-    GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION = 3;\n-    // Do not restrict the shapes of the operands and the result of the dot\n-    // instruction.\n-    GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES = 4;\n-    // Fail in autotuner if any of the configs are not supported.\n-    // Otherwise, the autotuner will silenly ignore configs that are regected.\n-    GENERIC_TRITON_EMITTER_MUST_ACCEPT_ALL_AUTOTUNER_CONFIGS = 5;\n-  }\n-\n   // Experimental optimizations for SPMD-based pipeline parallelism on GPU.\n   enum PipelineParallelismOptLevel {\n     PIPELINE_PARALLELISM_OPT_LEVEL_DISABLE = 0;\n@@ -962,14 +941,6 @@ message DebugOptions {\n   // TODO(b/390559452): Remove the flag once the feature is stable.\n   optional bool xla_gpu_unsupported_enable_triton_multi_output_fusion = 382;\n \n-  // Controls smaller knobs of the generic Triton emitter.\n-  // Do not rely on this flag currently supported values as they are subject to\n-  // change. Check 'debug_option_flags' for description.\n-  // TODO(b/393299275): remove the flag once we fully switched to the new\n-  // emitter.\n-  repeated GenericTritonEmitterFeature\n-      xla_gpu_unsupported_generic_triton_emitter_features = 398;\n-\n   // Internal debug/testing flag to override the number of devices in the fast\n   // interconnect domain. Default is 0, which means the number of devices is not\n   // overridden.\n@@ -1467,8 +1438,9 @@ message DebugOptions {\n   // xla_allow_get_default_platform\n   // xla_gpu_ensure_minor_dot_contraction_dims\n   // xla_gpu_unsafe_pipelined_loop_annotator\n+  // xla_gpu_unsupported_generic_triton_emitter_features\n   reserved 5, 117, 133, 139, 176, 178, 180, 193, 214, 194, 221, 242, 206, 320,\n-      325, 326, 332, 361, 270, 229, 271, 279, 218, 369, 371, 249, 309;\n+      325, 326, 332, 361, 270, 229, 271, 279, 218, 369, 371, 249, 309, 398;\n }\n \n // Contains flags which affects the GPU compilation result."
        }
    ],
    "stats": {
        "total": 360,
        "additions": 21,
        "deletions": 339
    }
}