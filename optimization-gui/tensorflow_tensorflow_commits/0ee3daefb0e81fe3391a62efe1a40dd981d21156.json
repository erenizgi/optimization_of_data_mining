{
    "author": "pineapplejuice233",
    "message": "Enable placing TPU input on corresponding local CPU devices.\n\nPiperOrigin-RevId: 819472209",
    "sha": "0ee3daefb0e81fe3391a62efe1a40dd981d21156",
    "files": [
        {
            "sha": "1f6fd6a89962676f487ee67f6864e72f8248806b",
            "filename": "tensorflow/python/distribute/device_util.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0ee3daefb0e81fe3391a62efe1a40dd981d21156/tensorflow%2Fpython%2Fdistribute%2Fdevice_util.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0ee3daefb0e81fe3391a62efe1a40dd981d21156/tensorflow%2Fpython%2Fdistribute%2Fdevice_util.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fdistribute%2Fdevice_util.py?ref=0ee3daefb0e81fe3391a62efe1a40dd981d21156",
            "patch": "@@ -145,12 +145,16 @@ def current():\n   return d\n \n \n-def get_host_for_device(device):\n+def get_host_for_device(device, device_index=0):\n   \"\"\"Returns the corresponding host device for the given device.\"\"\"\n   spec = tf_device.DeviceSpec.from_string(device)\n   return tf_device.DeviceSpec(\n-      job=spec.job, replica=spec.replica, task=spec.task,\n-      device_type=\"CPU\", device_index=0).to_string()\n+      job=spec.job,\n+      replica=spec.replica,\n+      task=spec.task,\n+      device_type=\"CPU\",\n+      device_index=device_index,\n+  ).to_string()\n \n \n def local_devices_from_num_gpus(num_gpus):"
        },
        {
            "sha": "c3e1f17f715554e058d73d490dbe57a303a26c4c",
            "filename": "tensorflow/python/distribute/tpu_strategy.py",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0ee3daefb0e81fe3391a62efe1a40dd981d21156/tensorflow%2Fpython%2Fdistribute%2Ftpu_strategy.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0ee3daefb0e81fe3391a62efe1a40dd981d21156/tensorflow%2Fpython%2Fdistribute%2Ftpu_strategy.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fdistribute%2Ftpu_strategy.py?ref=0ee3daefb0e81fe3391a62efe1a40dd981d21156",
            "patch": "@@ -907,7 +907,6 @@ def __init__(\n \n         tpu_devices.append(replica_devices)\n       self._tpu_devices = np.array(tpu_devices, dtype=object)\n-\n     self._host_device = device_util.get_host_for_device(self._tpu_devices[0][0])\n \n     # Preload the data onto the TPUs. Currently we always preload onto logical\n@@ -957,6 +956,48 @@ def __init__(\n     # to match IteratorGetNext's device with the TPUExecute device.\n     self._enable_data_reorder = False\n \n+  def _place_input_on_local_cpu_devices(self):\n+    \"\"\"Place input on local CPU devices.\n+\n+    For example, if the tpu_devices are:\n+    '/job:worker/replica:0/task:0/device:TPU:0',\n+    '/job:worker/replica:0/task:1/device:TPU:0',\n+    '/job:worker/replica:0/task:1/device:TPU:1',\n+    '/job:worker/replica:0/task:0/device:TPU:1',\n+\n+\n+    the host_input_worker_devices will be:\n+    {\n+        '/job:worker/replica:0/task:0/device:CPU:0': [\n+            '/job:worker/replica:0/task:0/device:TPU:0',\n+        ],\n+        '/job:worker/replica:0/task:1/device:CPU:0', [\n+            '/job:worker/replica:0/task:1/device:TPU:0',\n+        ],\n+        '/job:worker/replica:0/task:1/device:CPU:1': [\n+            '/job:worker/replica:0/task:1/device:TPU:1',\n+        ],\n+        '/job:worker/replica:0/task:0/device:CPU:1': [\n+            '/job:worker/replica:0/task:0/device:TPU:1',\n+        ],\n+    }\n+    This will make sure that the input is placed on the corresponding host CPU\n+    device if the device assignment is set.\n+    \"\"\"\n+    self._device_input_worker_devices = collections.OrderedDict()\n+    self._host_input_worker_devices = collections.OrderedDict()\n+    for tpu_device in self._tpu_devices[:, 0]:\n+      host_device = device_util.get_host_for_device(\n+          tpu_device,\n+          device_index=tf_device.DeviceSpec.from_string(\n+              tpu_device\n+          ).device_index,\n+      )\n+      self._device_input_worker_devices.setdefault(host_device, [])\n+      self._device_input_worker_devices[host_device].append(tpu_device)\n+      self._host_input_worker_devices.setdefault(host_device, [])\n+      self._host_input_worker_devices[host_device].append(host_device)\n+\n   def _get_replica_order(self):\n     \"\"\"Get the replica order based on the tpu device order.\n "
        }
    ],
    "stats": {
        "total": 53,
        "additions": 49,
        "deletions": 4
    }
}