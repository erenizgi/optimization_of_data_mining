{
    "author": "tensorflower-gardener",
    "message": "Refactor: Improve code style and readability in GPU transforms\n\nbased on clang-tidy: add curly braces, remove redundant const_cast and\nstd::string() conversion, else after return, use absl::StrContains for\nstring searching.\nPiperOrigin-RevId: 805835128",
    "sha": "7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
    "files": [
        {
            "sha": "380b36668e8272854a54fd0af7f85d41289cd41e",
            "filename": "third_party/xla/xla/service/gpu/transforms/add_tracking_suffix_to_instruction_names.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fadd_tracking_suffix_to_instruction_names.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fadd_tracking_suffix_to_instruction_names.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fadd_tracking_suffix_to_instruction_names.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -40,8 +40,9 @@ absl::StatusOr<bool> AddTrackingSuffixToInstructionNames::Run(\n       if (instruction->opcode() == HloOpcode::kParameter ||\n           instruction->opcode() == HloOpcode::kCustomCall ||\n           instruction->opcode() == HloOpcode::kFusion ||\n-          !instruction->IsFusible())\n+          !instruction->IsFusible()) {\n         continue;\n+      }\n \n       auto new_name = absl::StrCat(instruction->name(), \".0\");\n       module->SetAndUniquifyInstrName(instruction, new_name);"
        },
        {
            "sha": "3e16716d04ddd1bfe1fb8afe966b033c76b6611e",
            "filename": "third_party/xla/xla/service/gpu/transforms/async_wrapper_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fasync_wrapper_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fasync_wrapper_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fasync_wrapper_test.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -42,7 +42,9 @@ class AsyncWrapperTest : public HloTestBase {};\n int CountAsyncInstructions(HloComputation* computation) {\n   int count = 0;\n   for (const HloInstruction* instruction : computation->instructions()) {\n-    if (instruction->IsAsynchronous()) ++count;\n+    if (instruction->IsAsynchronous()) {\n+      ++count;\n+    }\n   }\n   return count;\n }"
        },
        {
            "sha": "74941b15d8d8d45ca8181215e6a8db2872ce3074",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 26,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -184,7 +184,8 @@ static HloInstruction* FindAsyncDoneCommand(const HloInstruction* start) {\n           start)) {\n     CHECK(start->users().size() == 1);  // NOLINT, checked by HLO verifier\n     return start->users().front();\n-  } else if (HloPredicateIsOp<HloOpcode::kAsyncStart>(start)) {\n+  }\n+  if (HloPredicateIsOp<HloOpcode::kAsyncStart>(start)) {\n     return start->async_chain_done();\n   }\n \n@@ -299,14 +300,13 @@ static bool IsCommand(const HloInstruction* hlo,\n       if (config_name ==\n           kDynamicSliceFusionWithStaticAddressComputationConfigName) {\n         return IsCommand(hero, config) || IsAsyncStartCommand(hero, config);\n-      } else {\n-        // DynamicSliceFusionRewriter currently only rewrites for dynamic slice\n-        // fusion with constant or loop iteration offset values, which are all\n-        // supported by command buffer.\n-        return (config.enabled_commands.contains(\n-                    DebugOptions::DYNAMIC_SLICE_FUSION) &&\n-                (IsCommand(hero, config) || IsAsyncStartCommand(hero, config)));\n       }\n+      // DynamicSliceFusionRewriter currently only rewrites for dynamic slice\n+      // fusion with constant or loop iteration offset values, which are all\n+      // supported by command buffer.\n+      return (config.enabled_commands.contains(\n+                  DebugOptions::DYNAMIC_SLICE_FUSION) &&\n+              (IsCommand(hero, config) || IsAsyncStartCommand(hero, config)));\n     }\n \n     // Cuda has a bug that when the cuda kernel's parameter size is larger than\n@@ -331,24 +331,29 @@ static bool IsCommand(const HloInstruction* hlo,\n     return config.enabled_commands.contains(DebugOptions::FUSION);\n   }\n \n-  if (auto* sort = DynCast<HloSortInstruction>(hlo))\n+  if (auto* sort = DynCast<HloSortInstruction>(hlo)) {\n     return config.enabled_commands.contains(DebugOptions::FUSION);\n+  }\n \n-  if (HloPredicateIsOp<HloOpcode::kCopy>(hlo))\n+  if (HloPredicateIsOp<HloOpcode::kCopy>(hlo)) {\n     return config.enabled_commands.contains(DebugOptions::FUSION);\n+  }\n \n   if (HloPredicateIsOp<HloOpcode::kPartitionId, HloOpcode::kReplicaId>(hlo)) {\n     return config.enabled_commands.contains(DebugOptions::FUSION);\n   }\n \n-  if (auto* custom_call = DynCast<HloCustomCallInstruction>(hlo))\n+  if (auto* custom_call = DynCast<HloCustomCallInstruction>(hlo)) {\n     return IsCommand(custom_call, config);\n+  }\n \n-  if (HloPredicateIsOp<HloOpcode::kWhile>(hlo))\n+  if (HloPredicateIsOp<HloOpcode::kWhile>(hlo)) {\n     return IsCommand<HloOpcode::kWhile>(hlo, config);\n+  }\n \n-  if (HloPredicateIsOp<HloOpcode::kConditional>(hlo))\n+  if (HloPredicateIsOp<HloOpcode::kConditional>(hlo)) {\n     return IsCommand<HloOpcode::kConditional>(hlo, config);\n+  }\n \n   return false;\n }\n@@ -481,25 +486,31 @@ CommandBufferScheduling::CollectCommandBufferSequences(\n   auto check_dynamic_slice_operand_not_from_seq =\n       [&](const HloInstructionSequence& seq, const HloInstruction* inst) {\n         if (!config.enabled_commands.contains(\n-                DebugOptions::DYNAMIC_SLICE_FUSION))\n+                DebugOptions::DYNAMIC_SLICE_FUSION)) {\n           return true;\n+        }\n         const auto* fusion = DynCast<HloFusionInstruction>(inst);\n-        if (!fusion) return true;\n+        if (!fusion) {\n+          return true;\n+        }\n \n         auto gpu_config = fusion->backend_config<GpuBackendConfig>();\n         const FusionBackendConfig& backend_config =\n             gpu_config->fusion_backend_config();\n         const auto& custom_config = backend_config.custom_fusion_config();\n         if (custom_config.name() !=\n-            kDynamicSliceFusionWithDynamicAddressComputationConfigName)\n+            kDynamicSliceFusionWithDynamicAddressComputationConfigName) {\n           return true;\n+        }\n \n         auto* fused_computation = fusion->called_computation();\n         return !absl::c_any_of(\n             fused_computation->instructions(), [&](const HloInstruction* inst) {\n               const auto* dynamic_inst =\n                   DynCast<HloDynamicIndexInstruction>(inst);\n-              if (!dynamic_inst) return false;\n+              if (!dynamic_inst) {\n+                return false;\n+              }\n               for (auto* operand : dynamic_inst->index_operands()) {\n                 const auto* param = DynCast<HloParameterInstruction>(operand);\n                 const auto* fusion_operand =\n@@ -655,7 +666,9 @@ absl::StatusOr<bool> CommandBufferScheduling::MoveParametersAndConstantsToFront(\n   schedule.set_sequence(computation, new_sequence);\n   for (auto [old_i, new_i] :\n        llvm::zip(sequence.instructions(), new_sequence.instructions())) {\n-    if (old_i != new_i) return true;\n+    if (old_i != new_i) {\n+      return true;\n+    }\n   }\n   return false;\n }\n@@ -690,8 +703,9 @@ absl::StatusOr<CommandBuffer> CommandBufferScheduling::PrepareCommandBuffer(\n   auto mapped_operands = [&](HloInstruction* instr) {\n     absl::InlinedVector<HloInstruction*, 4> operands;\n     for (HloInstruction* operand : instr->operands()) {\n-      if (auto it = inst_mapping.find(operand); it != inst_mapping.end())\n+      if (auto it = inst_mapping.find(operand); it != inst_mapping.end()) {\n         operands.push_back(it->second);\n+      }\n     }\n     return operands;\n   };\n@@ -700,10 +714,14 @@ absl::StatusOr<CommandBuffer> CommandBufferScheduling::PrepareCommandBuffer(\n   for (HloInstruction* inst : instructions) {\n     for (HloInstruction* operand : inst->operands()) {\n       // We already mapped instruction to a parameter.\n-      if (parameters.contains(operand)) continue;\n+      if (parameters.contains(operand)) {\n+        continue;\n+      }\n \n       // Operand instruction is a part of the command buffer.\n-      if (in_command_buffer.contains(operand)) continue;\n+      if (in_command_buffer.contains(operand)) {\n+        continue;\n+      }\n \n       // Create a new parameter for value defined outside of a command buffer.\n       int64_t parameter_id = parameters.size();\n@@ -782,8 +800,9 @@ absl::StatusOr<CommandBuffer> CommandBufferScheduling::PrepareCommandBuffer(\n absl::StatusOr<HloComputation*> CommandBufferScheduling::RewriteCommandBuffer(\n     HloComputation* parent, const HloInstructionSequence& seq,\n     CommandBuffer command_buffer) {\n-  if (command_buffer.results.empty())\n+  if (command_buffer.results.empty()) {\n     return absl::InternalError(\"command buffer results must not be empty\");\n+  }\n \n   // If we have more than one result we return them as tuple, and get individual\n   // values using `get-tuple-element` instructions. Otherwise we simply return\n@@ -796,7 +815,9 @@ absl::StatusOr<HloComputation*> CommandBufferScheduling::RewriteCommandBuffer(\n   } else {\n     absl::InlinedVector<Shape, 4> shapes;\n     shapes.reserve(command_buffer.results.size());\n-    for (auto* res : command_buffer.results) shapes.push_back(res->shape());\n+    for (auto* res : command_buffer.results) {\n+      shapes.push_back(res->shape());\n+    }\n     cmd_buffer_result_shape = ShapeUtil::MakeTupleShape(shapes);\n   }\n \n@@ -909,7 +930,9 @@ absl::StatusOr<bool> CommandBufferScheduling::Run(\n   // compared to a regular execution. Some operations (i.e. async collectives)\n   // can't be captured into command buffers, and forming too large command\n   // buffers too early can impact async operations scheduling.\n-  if (!module->has_schedule()) return Internal(\"module is not scheduled\");\n+  if (!module->has_schedule()) {\n+    return Internal(\"module is not scheduled\");\n+  }\n \n   const DebugOptions& debug_options = module->config().debug_options();\n \n@@ -980,11 +1003,14 @@ absl::StatusOr<bool> CommandBufferScheduling::Run(\n   for (HloComputation* comp : order) {\n     // Skip special computations that do not have lowering to thunks.\n     if (comp->IsFusionComputation() || comp->IsAsyncComputation() ||\n-        !comp->caller_instructions(HloOpcode::kCustomCall).empty())\n+        !comp->caller_instructions(HloOpcode::kCustomCall).empty()) {\n       continue;\n+    }\n \n     // Skip computations that already part of command buffers.\n-    if (processed_command_buffers.contains(comp)) continue;\n+    if (processed_command_buffers.contains(comp)) {\n+      continue;\n+    }\n \n     TF_ASSIGN_OR_RETURN(bool changed_, MoveParametersAndConstantsToFront(comp));\n     changed |= changed_;"
        },
        {
            "sha": "11b62590e3fcb31d7f6b551a12a006e1357abe92",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_custom_call_compiler.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_custom_call_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_custom_call_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_custom_call_compiler.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -491,19 +491,21 @@ absl::StatusOr<se::gpu::CudnnGraph> BuildGraphForCustomCallToBlockScaledDot(\n }\n \n absl::StatusOr<se::gpu::CudnnGraph> HloCustomCallToCuDnnGraph(\n-    se::dnn::DnnSupport &dnn_support, HloCustomCallInstruction *custom_call) {\n+    se::dnn::DnnSupport& dnn_support, HloCustomCallInstruction* custom_call) {\n   if (IsFwdCustomCallTofMHA(*custom_call)) {\n     return BuildGraphForCustomCallToForwardFMHA(dnn_support, custom_call);\n-  } else if (IsFwdCustomCallTofMHAF8(*custom_call)) {\n+  }\n+  if (IsFwdCustomCallTofMHAF8(*custom_call)) {\n     return BuildGraphForCustomCallToForwardFMHAF8(dnn_support, custom_call);\n-  } else if (IsBwdCustomCallTofMHA(*custom_call)) {\n+  }\n+  if (IsBwdCustomCallTofMHA(*custom_call)) {\n     return BuildGraphForCustomCallToBackwardFMHA(dnn_support, custom_call);\n-  } else if (IsBwdCustomCallTofMHAF8(*custom_call)) {\n+  }\n+  if (IsBwdCustomCallTofMHAF8(*custom_call)) {\n     return BuildGraphForCustomCallToBackwardFMHAF8(dnn_support, custom_call);\n-  } else {\n-    TF_RET_CHECK(IsCustomCallToBlockScaledDot(*custom_call));\n-    return BuildGraphForCustomCallToBlockScaledDot(dnn_support, custom_call);\n   }\n+  TF_RET_CHECK(IsCustomCallToBlockScaledDot(*custom_call));\n+  return BuildGraphForCustomCallToBlockScaledDot(dnn_support, custom_call);\n }\n \n class CuDnnCustomCallVisitor : public DfsHloRewriteVisitor {"
        },
        {
            "sha": "4bba22892d962b1e99a957ea11df2d650e6cea81",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_simplify_padding.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_simplify_padding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_simplify_padding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_simplify_padding.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -176,9 +176,10 @@ std::optional<int64_t> NumTrailingZeroOutputFeatures(HloInstruction* conv) {\n     VLOG(2) << \"Success: Weights is a pad; padding on output feature dim is \"\n             << padding_config.edge_padding_high();\n     return padding_config.edge_padding_high();\n-  } else if (const HloInstruction * pad; Match(\n-                 weights, m::Reshape(m::Pad(&pad, m::Op(),\n-                                            m::ConstantEffectiveScalar(0))))) {\n+  }\n+  if (const HloInstruction* pad;\n+      Match(weights,\n+            m::Reshape(m::Pad(&pad, m::Op(), m::ConstantEffectiveScalar(0))))) {\n     // Check that the reshape merely adds a VECT_C to the kernel input features.\n     // That is, we reshape from [I,O,H,W] (in some order) to [I/k,k,O,H,W] (in\n     // the same order) for some constant k (probably 32).  Then check how much\n@@ -225,7 +226,8 @@ std::optional<int64_t> NumTrailingZeroOutputFeatures(HloInstruction* conv) {\n                \"feature dim is \"\n             << padding_config.edge_padding_high();\n     return padding_config.edge_padding_high();\n-  } else if (Match(weights, m::Constant())) {\n+  }\n+  if (Match(weights, m::Constant())) {\n     // Iterate backwards over `weights` to find the index of the first nonzero\n     // value.\n     //"
        },
        {
            "sha": "313c2527356ed515d63436db32845770b524871f",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_simplify_padding_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_simplify_padding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_simplify_padding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_simplify_padding_test.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -383,7 +383,9 @@ TEST_F(CudnnSimplifyPaddingTest, PaddedConstantWeight) {\n                                   m::Op())));\n     SetConstantValue<int8_t>(\n         weights, [](absl::Span<const int64_t> dims, int8_t old_val) -> int8_t {\n-          if (dims[3] < 6) return 1;\n+          if (dims[3] < 6) {\n+            return 1;\n+          }\n           return 0;\n         });\n   }"
        },
        {
            "sha": "c10bd10e2b3ab3193eed78b712dc7a2fe87efe41",
            "filename": "third_party/xla/xla/service/gpu/transforms/custom_kernel_fusion_rewriter.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcustom_kernel_fusion_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcustom_kernel_fusion_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcustom_kernel_fusion_rewriter.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -64,7 +64,9 @@ GetPatternReplacements(const CustomKernelFusionPattern::Match& match) {\n \n   for (HloInstruction* instr : match.instructions()) {\n     for (HloInstruction* user : instr->users()) {\n-      if (instr == match.root() || instructions_set.contains(user)) continue;\n+      if (instr == match.root() || instructions_set.contains(user)) {\n+        continue;\n+      }\n \n       if (match.HasReplacement(instr)) {\n         requires_replacement.insert(instr);\n@@ -178,7 +180,9 @@ absl::StatusOr<HloInstruction*> CreateFusionInstruction(\n   TF_RETURN_IF_ERROR(fusion->set_backend_config(std::move(gpu_config)));\n \n   // If we don't have workspace we can return constructed fusion instruction.\n-  if (match.workspace_size_bytes() == 0) return fusion;\n+  if (match.workspace_size_bytes() == 0) {\n+    return fusion;\n+  }\n \n   // Otherwise have to get result corresponding to the original value;\n   return parent->AddInstruction(\n@@ -199,14 +203,18 @@ absl::StatusOr<bool> CustomKernelFusionRewriter::Run(\n     }\n   }\n \n-  if (matches.empty()) return false;\n+  if (matches.empty()) {\n+    return false;\n+  }\n \n   for (const CustomKernelFusionPattern::Match& match : matches) {\n     VLOG(2) << \"Matched custom kernel fusion \" << match.config().name()\n             << \"; root instruction: \" << match.instructions().back()->name();\n \n     auto replacememts = GetPatternReplacements(match);\n-    if (!replacememts.has_value()) continue;\n+    if (!replacememts.has_value()) {\n+      continue;\n+    }\n \n     auto captures = GetPatternCaptures(match);\n "
        },
        {
            "sha": "d56c5be5ef4d667b2ec38d0690ef7a38baed6040",
            "filename": "third_party/xla/xla/service/gpu/transforms/dynamic_slice_fusion_rewriter.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -78,16 +78,19 @@ using InstructionSet = absl::flat_hash_set<HloInstruction*>;\n \n bool IsCustomCall(const HloInstruction* hlo, absl::string_view platform_name) {\n   auto* custom_call = DynCast<HloCustomCallInstruction>(hlo);\n-  if (custom_call == nullptr) return false;\n+  if (custom_call == nullptr) {\n+    return false;\n+  }\n \n   // TODO(vuson): properly handle token by following\n   // `LhloDialectEmitter::EmitCustomCallOp`'s `CreateOperands` logic for\n   // `LhloDialectEmitter::EmitFusionOp`'s `RewriteFusionOperand`\n   if (custom_call->shape().IsTuple() &&\n       absl::c_any_of(\n           custom_call->shape().tuple_shapes(),\n-          [&](const Shape& sub_shape) { return sub_shape.IsToken(); }))\n+          [&](const Shape& sub_shape) { return sub_shape.IsToken(); })) {\n     return false;\n+  }\n \n   const std::string call_target_name = custom_call->custom_call_target();\n \n@@ -154,8 +157,9 @@ absl::Status CreateRootTuple(\n       elements.push_back(gte);\n     }\n   }\n-  if (elements.size() > 1)\n+  if (elements.size() > 1) {\n     builder.AddInstruction(HloInstruction::CreateTuple(elements));\n+  }\n \n   return absl::OkStatus();\n }\n@@ -231,9 +235,9 @@ absl::StatusOr<HloInstruction*> CreateFusionInstruction(\n       *gpu_config.mutable_fusion_backend_config();\n   backend_config.set_kind(\"__custom_fusion\");\n   CustomFusionConfig config;\n-  config.set_name(std::string(\n+  config.set_name(\n       dynamic ? kDynamicSliceFusionWithDynamicAddressComputationConfigName\n-              : kDynamicSliceFusionWithStaticAddressComputationConfigName));\n+              : kDynamicSliceFusionWithStaticAddressComputationConfigName);\n   *backend_config.mutable_custom_fusion_config() = config;\n   TF_RETURN_IF_ERROR(fusion->set_backend_config(std::move(gpu_config)));\n \n@@ -253,7 +257,9 @@ absl::StatusOr<bool> DynamicSliceFusionRewriter::Run(\n   std::unique_ptr<CallGraph> call_graph = CallGraph::Build(module);\n   // Collect all potential custom call matches in the non-fusion computations.\n   for (HloComputation* computation : module->computations()) {\n-    if (computation->IsFusionComputation()) continue;\n+    if (computation->IsFusionComputation()) {\n+      continue;\n+    }\n     for (HloInstruction* instr : computation->instructions()) {\n       if ((HloPredicateIsOp<HloOpcode::kReduceScatter>(instr)) ||\n           IsLegacyCublasMatmul(*instr) || IsCustomCall(instr, platform_name_)) {\n@@ -283,7 +289,9 @@ absl::StatusOr<bool> DynamicSliceFusionRewriter::Run(\n     }\n   }\n \n-  if (matches.empty()) return false;\n+  if (matches.empty()) {\n+    return false;\n+  }\n \n   for (HloInstruction* hero : matches) {\n     auto& paths = matches_kv[hero];\n@@ -316,8 +324,8 @@ absl::StatusOr<bool> DynamicSliceFusionRewriter::Run(\n \n     HloComputation* parent = hero->parent();\n     if (fusion->shape().IsTuple()) {\n-      TF_RETURN_IF_ERROR(parent->ReplaceInstructionWithDifferentShape(\n-          const_cast<HloInstruction*>(hero), fusion));\n+      TF_RETURN_IF_ERROR(\n+          parent->ReplaceInstructionWithDifferentShape(hero, fusion));\n       for (auto& sliced_user_path : sliced_user_paths) {\n         auto old_gte =\n             Cast<HloGetTupleElementInstruction>(sliced_user_path.front());\n@@ -328,7 +336,7 @@ absl::StatusOr<bool> DynamicSliceFusionRewriter::Run(\n             parent->ReplaceInstruction(sliced_user_path.back(), gte));\n       }\n     } else {\n-      auto* instr_to_be_replaced = const_cast<HloInstruction*>(hero);\n+      HloInstruction* instr_to_be_replaced = hero;\n       if (sliced_user_paths.empty()) {\n         // The only case where a tuple-shaped original hero op is fused into a\n         // non-tuple-shaped fusion is there's only one element of the original"
        },
        {
            "sha": "d0a32e6fc30ed3f5ee8abe2bb81b8c6dad737c86",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_dynamic_memcpy_rewriter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_dynamic_memcpy_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_dynamic_memcpy_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_dynamic_memcpy_rewriter.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -288,7 +288,7 @@ absl::StatusOr<bool> FusionDynamicMemcpyRewriter::Run(\n     TF_ASSIGN_OR_RETURN(auto backend_config,\n                         fusion->backend_config<GpuBackendConfig>());\n     auto* fusion_config = backend_config.mutable_fusion_backend_config();\n-    fusion_config->set_kind(std::string(kDynamicMemcpyFusionKind));\n+    fusion_config->set_kind(kDynamicMemcpyFusionKind);\n     auto* memcpy_config = fusion_config->mutable_dynamic_memcpy_config();\n \n     if (descriptor->src_dynamic_offsets.size() +"
        },
        {
            "sha": "acbb37127cb1996d69a0ee74ee754aa9998ab12a",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_swap_operands.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 8,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_swap_operands.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_swap_operands.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_swap_operands.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -67,7 +67,9 @@ HloDotInstruction* MakeDotWithSwappedOperands(HloInstruction* dot) {\n   std::vector<int64_t> out_shape_permutation;\n   out_shape_permutation.reserve(dot->shape().dimensions().size());\n   auto fill_permutation = [&](int64_t count, int64_t start) {\n-    while (count--) out_shape_permutation.push_back(start++);\n+    while (count--) {\n+      out_shape_permutation.push_back(start++);\n+    }\n   };\n   // The output shape of a dot is batch dimensions, then lhs non-contracting,\n   // then rhs non-contracting. Batch dimensions stay where they were. and\n@@ -155,15 +157,21 @@ absl::StatusOr<int64_t> GetNonContractingDimsNumElements(\n // powers of two.\n absl::StatusOr<bool> ShouldSwapOperands(const HloInstruction* instr) {\n   const HloDotInstruction* dot = DynCast<HloDotInstruction>(instr);\n-  if (dot == nullptr) return false;\n+  if (dot == nullptr) {\n+    return false;\n+  }\n   const bool lhs_has_code = HasCodeGeneratingInstructions(dot->operand(0));\n   const bool rhs_has_code = HasCodeGeneratingInstructions(dot->operand(1));\n   TF_ASSIGN_OR_RETURN(const int64_t lhs_size, GetNonContractingDimsNumElements(\n                                                   dot, /*operand_index=*/0));\n   TF_ASSIGN_OR_RETURN(const int64_t rhs_size, GetNonContractingDimsNumElements(\n                                                   dot, /*operand_index=*/1));\n-  if (lhs_size < 64 && rhs_size >= 64) return true;\n-  if (!lhs_has_code && rhs_has_code && rhs_size >= 64) return true;\n+  if (lhs_size < 64 && rhs_size >= 64) {\n+    return true;\n+  }\n+  if (!lhs_has_code && rhs_has_code && rhs_size >= 64) {\n+    return true;\n+  }\n   return false;\n }\n \n@@ -183,12 +191,18 @@ absl::StatusOr<bool> EmitterCanHandleSwappedOperands(\n absl::StatusOr<bool> MaybeSwapOperands(HloComputation* computation) {\n   HloInstruction* dot =\n       hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n-  if (dot == nullptr) return false;\n+  if (dot == nullptr) {\n+    return false;\n+  }\n   TF_ASSIGN_OR_RETURN(const bool should_swap_operands, ShouldSwapOperands(dot));\n-  if (!should_swap_operands) return false;\n+  if (!should_swap_operands) {\n+    return false;\n+  }\n   TF_ASSIGN_OR_RETURN(const bool can_handle_swapped_operands,\n                       EmitterCanHandleSwappedOperands(dot));\n-  if (!can_handle_swapped_operands) return false;\n+  if (!can_handle_swapped_operands) {\n+    return false;\n+  }\n   TF_RETURN_IF_ERROR(SwapDotOperandsInFusion(computation));\n   return true;\n }\n@@ -201,7 +215,9 @@ absl::StatusOr<bool> GemmFusionSwapOperands::Run(\n   bool any_changed = false;\n   for (HloComputation* computation :\n        module->MakeComputationPostOrder(execution_threads)) {\n-    if (!IsTritonFusedComputation(*computation)) continue;\n+    if (!IsTritonFusedComputation(*computation)) {\n+      continue;\n+    }\n     TF_ASSIGN_OR_RETURN(const bool changed, MaybeSwapOperands(computation));\n     any_changed |= changed;\n   }"
        },
        {
            "sha": "cdd232ac49f07350ff9afe7df14990ddc3445418",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 23,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -221,8 +221,8 @@ std::optional<InstrPath> FindF8SubgraphRecursive(\n       subgraph->emplace_back(std::make_pair(instr, 0));\n     }\n     return subgraph;\n-  } else if (HloPredicateIsOp<HloOpcode::kMultiply, HloOpcode::kSelect>(\n-                 instr)) {\n+  }\n+  if (HloPredicateIsOp<HloOpcode::kMultiply, HloOpcode::kSelect>(instr)) {\n     for (int k = 0; k < 2; ++k) {\n       // Iterate over operands 0 and 1 for multiply and operands 1 and 2 for\n       // select.\n@@ -501,7 +501,9 @@ auto BcastConstScalarNear(double value) {\n             xla::Cast<const HloConstantInstruction>(instr)\n                 ->literal()\n                 .GetAsDouble({});\n-        if (!actual.has_value()) return false;\n+        if (!actual.has_value()) {\n+          return false;\n+        }\n         double epsilon;\n         switch (instr->shape().element_type()) {\n           case F16:\n@@ -1512,18 +1514,21 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       // since abs(ReLU(x)) = ReLU(x).\n       TF_ASSIGN_OR_RETURN(auto gpu_config,\n                           existing_gemm->backend_config<GpuBackendConfig>());\n-      const GemmBackendConfig &config = gpu_config.gemm_backend_config();\n+      const GemmBackendConfig& config = gpu_config.gemm_backend_config();\n       for (int i = 0; i < gemm_users.size(); ++i) {\n-        HloInstruction *maybe_reduce = nullptr;\n+        HloInstruction* maybe_reduce = nullptr;\n         if (gemm_users[i]->opcode() == HloOpcode::kAbs) {\n-          if (gemm_users[i]->users().size() != 1) continue;\n+          if (gemm_users[i]->users().size() != 1) {\n+            continue;\n+          }\n           maybe_reduce = gemm_users[i]->users()[0];\n         } else {\n           // If there is no Abs instruction, relu is required as epilogue to\n           // ensure all values are nonnegative.\n           if (config.epilogue() != GemmBackendConfig::BIAS_RELU &&\n-              config.epilogue() != GemmBackendConfig::RELU)\n+              config.epilogue() != GemmBackendConfig::RELU) {\n             continue;\n+          }\n           maybe_reduce = gemm_users[i];\n         }\n \n@@ -1563,12 +1568,11 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n                    \"conflicts with the existing fusion of the addition of a \"\n                    \"matrix bias with element type other than BF16 or F16.\";\n         return absl::OkStatus();\n-      } else {\n-        // Turn off the output to operand aliasing, since the fp8 output and\n-        // bf16/fp16 bias have different sizes.\n-        xla::Cast<HloCustomCallInstruction>(existing_gemm)\n-            ->set_output_to_operand_aliasing({});\n       }\n+      // Turn off the output to operand aliasing, since the fp8 output and\n+      // bf16/fp16 bias have different sizes.\n+      xla::Cast<HloCustomCallInstruction>(existing_gemm)\n+          ->set_output_to_operand_aliasing({});\n     }\n \n     // If necessary, invert the scaling factor of D and convert to F32. When no\n@@ -1843,7 +1847,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         if (bias_type == BF16) {\n           return output_type == F8E4M3FN || output_type == F8E5M2 ||\n                  output_type == F32 || output_type == BF16;\n-        } else if (bias_type == F16) {\n+        }\n+        if (bias_type == F16) {\n           return output_type == F16 || output_type == F8E4M3FN ||\n                  output_type == F8E5M2;\n         }\n@@ -2097,7 +2102,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         PrimitiveType::C64, PrimitiveType::C128};\n     // legacy cublas has a defined set of combinations of types that it\n     // supports. Figure out the computeType and scaleType.\n-    if (!absl::c_linear_search(supported_type, output_type)) return false;\n+    if (!absl::c_linear_search(supported_type, output_type)) {\n+      return false;\n+    }\n     TF_ASSIGN_OR_RETURN(const se::blas::DataType output_dtype,\n                         se::gpu::AsBlasDataType(output_type));\n     TF_ASSIGN_OR_RETURN(\n@@ -2186,7 +2193,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         PrimitiveType::BF16,       PrimitiveType::F32,\n         PrimitiveType::S32,        PrimitiveType::F64,\n         PrimitiveType::C64,        PrimitiveType::C128};\n-    if (!absl::c_linear_search(supported_type, output_type)) return false;\n+    if (!absl::c_linear_search(supported_type, output_type)) {\n+      return false;\n+    }\n     // cublasLt has a defined set of combinations of types that it supports.\n     // Figure out the computeType and scaleType.\n     TF_ASSIGN_OR_RETURN(const se::blas::DataType output_dtype,\n@@ -2195,8 +2204,10 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         backend_config.precision_config().operand_precision());\n     const PrecisionConfig::Algorithm algorithm =\n         backend_config.precision_config().algorithm();\n-    if (!algorithm_util::IsSupportedByCublasOrCublasLt(algorithm, gpu_version_))\n+    if (!algorithm_util::IsSupportedByCublasOrCublasLt(algorithm,\n+                                                       gpu_version_)) {\n       return false;\n+    }\n \n     TF_ASSIGN_OR_RETURN(\n         const se::blas::ComputationType compute_type,\n@@ -2481,13 +2492,12 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       output_f16_shape.set_element_type(F16);\n       HloInstruction *f16_dot =\n           instr->AddInstruction(instr->CloneWithNewShape(output_f16_shape));\n-      HloInstruction *convert_to_f8 = instr->AddInstruction(\n+      HloInstruction* convert_to_f8 = instr->AddInstruction(\n           HloInstruction::CreateConvert(instr->shape(), f16_dot));\n       TF_RETURN_IF_ERROR(ReplaceInstruction(instr, convert_to_f8));\n       return f16_dot;\n-    } else {\n-      return instr;\n     }\n+    return instr;\n   }\n };\n \n@@ -2584,11 +2594,10 @@ class GemmWorkspaceRewriteVisitor : public DfsHloRewriteVisitor {\n         TF_RETURN_IF_ERROR(ReplaceInstruction(user_get_tuple, get_output));\n       }\n       return absl::OkStatus();\n-    } else {\n-      HloInstruction *get_output = instr->AddInstruction(\n-          HloInstruction::CreateGetTupleElement(new_call, 0));\n-      return ReplaceInstruction(instr, get_output);\n     }\n+    HloInstruction* get_output = instr->AddInstruction(\n+        HloInstruction::CreateGetTupleElement(new_call, 0));\n+    return ReplaceInstruction(instr, get_output);\n   }\n \n  private:"
        },
        {
            "sha": "e92c007be33507b69dd80468301b1eead51d8b2d",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -122,15 +122,6 @@ ENTRY AddDotsFunc {\n }\n )\";\n \n-  ErrorSpec error_spec = [&] {\n-    DebugOptions debug_options = GetDebugOptionsForTest();\n-    if (debug_options.xla_gpu_enable_cublaslt()) {\n-      return ErrorSpec{1e-3, 1e-3};\n-    } else {\n-      return ErrorSpec{1e-3, 1e-3};\n-    }\n-  }();\n-\n   auto get_module = [&]() {\n     HloModuleConfig config;\n     DebugOptions debug_options = GetDebugOptionsForTest();\n@@ -153,7 +144,7 @@ ENTRY AddDotsFunc {\n     )\");\n   TF_ASSERT_OK(filecheck_result.status());\n   EXPECT_TRUE(filecheck_result.value());\n-  EXPECT_TRUE(RunAndCompare(*get_module(), error_spec));\n+  EXPECT_TRUE(RunAndCompare(*get_module(), ErrorSpec{1e-3, 1e-3}));\n }\n \n TEST_F(GemmRewriteTest, BF16GemmCodeGen) {"
        },
        {
            "sha": "0935bda4697ba16b544b93e84c22350d4f01821c",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test_lib.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -71,9 +71,8 @@ stream_executor::GpuComputeCapability\n GemmRewriteTestBase::CudaHopperOrRocmCapability() {\n   if (IsCuda()) {\n     return se::CudaComputeCapability::Hopper();\n-  } else {\n-    return std::get<se::RocmComputeCapability>(Capability());\n   }\n+  return std::get<se::RocmComputeCapability>(Capability());\n }\n \n DebugOptions GemmRewriteTestBase::GetDebugOptionsForTest() const {"
        },
        {
            "sha": "b802a5c23bdfe7c047c4f18bbc05cc2a4ba72030",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -151,19 +151,21 @@ HeuristicLayoutAssignment(const HloInstruction* instr,\n         // TODO(b/383560056): find the right filter for 3D convolutions. 3D\n         // convolutions also have a much smaller surface of support. We filter\n         // them out completely as well for now.\n-      } else if (num_spatial_dimensions > 2) {\n+      }\n+      if (num_spatial_dimensions > 2) {\n         VLOG(2) << \"Using NHWC for \" << num_spatial_dimensions << \"D conv \"\n                 << instr->ToString() << \" on \" << cc->ToString();\n         return kAllNCHW;\n-      } else {\n-        return kAllNHWC;\n       }\n+      return kAllNHWC;\n     }\n   }\n \n   const auto* rocm_compute_capability =\n       std::get_if<se::RocmComputeCapability>(&gpu_version);\n-  if (rocm_compute_capability && input_ty == F16) return kAllNHWC;\n+  if (rocm_compute_capability && input_ty == F16) {\n+    return kAllNHWC;\n+  }\n \n   // If we're not Volta or not fp16/bfloat16, or not conv2D, the decision is\n   // easy: Use NCHW.\n@@ -747,7 +749,9 @@ absl::Status GpuLayoutAssignment::SetOperandMajorToMinorLayout(\n     std::initializer_list<absl::Span<const int64_t>> dim_groups,\n     bool mandatory) {\n   size_t size = 0;\n-  for (auto group : dim_groups) size += group.size();\n+  for (auto group : dim_groups) {\n+    size += group.size();\n+  }\n   std::vector<int64_t> major_to_minor;\n   major_to_minor.reserve(size);\n   for (const auto& group : dim_groups) {"
        },
        {
            "sha": "fcaf7702ee78c1ab07f968dbb3c08733bdfae1c9",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -62,30 +62,38 @@ const HloSliceInstruction* FindUniqueSlice(const HloInstruction* parent,\n                                            const HloInstruction* instr) {\n   if (const auto* slice = DynCast<HloSliceInstruction>(instr)) {\n     return slice;\n-  } else if (const auto* fusion = DynCast<HloFusionInstruction>(instr)) {\n+  }\n+  if (const auto* fusion = DynCast<HloFusionInstruction>(instr)) {\n     const HloSliceInstruction* result = nullptr;\n     for (size_t i = 0; i < fusion->operand_count(); ++i) {\n       if (fusion->operand(i) == parent) {\n         // Parameter used more than once -> there's no unique slice.\n-        if (result) return nullptr;\n+        if (result) {\n+          return nullptr;\n+        }\n \n         auto* called_param = fusion->fused_parameter(i);\n-        if (called_param->user_count() != 1) return nullptr;\n+        if (called_param->user_count() != 1) {\n+          return nullptr;\n+        }\n \n         result = FindUniqueSlice(called_param, called_param->users()[0]);\n-        if (!result) return nullptr;\n+        if (!result) {\n+          return nullptr;\n+        }\n       }\n     }\n     return result;\n-  } else {\n-    return nullptr;\n   }\n+  return nullptr;\n }\n \n FusionDecision ParameterSlicesAreNonOverlapping(const HloInstruction& instr1,\n                                                 const HloInstruction& instr2,\n                                                 const HloInstruction* parent) {\n-  if (parent->shape().IsTuple()) return FusionDecision::Allow();\n+  if (parent->shape().IsTuple()) {\n+    return FusionDecision::Allow();\n+  }\n   // Allow MOF if the parameter is small, even if there's no overlap. 1024 bytes\n   // were arbitrarily chosen as the threshold.\n   if (ShapeUtil::ByteSizeOfElements(parent->shape()) < 1024) {\n@@ -94,7 +102,9 @@ FusionDecision ParameterSlicesAreNonOverlapping(const HloInstruction& instr1,\n \n   const HloSliceInstruction* slice1 = FindUniqueSlice(parent, &instr1);\n   const HloSliceInstruction* slice2 = FindUniqueSlice(parent, &instr2);\n-  if (!slice1 || !slice2) return FusionDecision::Allow();\n+  if (!slice1 || !slice2) {\n+    return FusionDecision::Allow();\n+  }\n \n   // TODO(jreiffers): Check strides as well.\n   auto& starts1 = slice1->slice_starts();"
        },
        {
            "sha": "f6eaf21471346fa7f8d3d476693dcfebffe351ef",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -143,7 +143,7 @@ absl::Status FuseInstructionsForConsumer(HloInstruction& root,\n   TF_ASSIGN_OR_RETURN(auto gpu_config,\n                       fusion->backend_config<GpuBackendConfig>());\n   gpu_config.mutable_fusion_backend_config()->set_kind(\n-      std::string(kTritonNestedGemmFusionKind));\n+      kTritonNestedGemmFusionKind);\n   TF_RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n \n   for (int64_t operand_index : consumer.OperandIndices(&root)) {\n@@ -289,7 +289,7 @@ absl::Status MakeNestedFusionFromGemmFusion(HloFusionInstruction* fusion,\n   FusionBackendConfig& backend_config =\n       *gpu_config.mutable_fusion_backend_config();\n   backend_config.clear_triton_gemm_config();\n-  backend_config.set_kind(std::string(kTritonNestedGemmFusionKind));\n+  backend_config.set_kind(kTritonNestedGemmFusionKind);\n \n   TF_ASSIGN_OR_RETURN(\n       BlockLevelParameters block_level_parameters,"
        },
        {
            "sha": "8f52be65f112deaf9b1fe5ad6eac6ee8cb274d03",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -251,7 +251,7 @@ absl::StatusOr<HloFusionInstruction*> MakeFusionForDiamond(\n                       normalization_fusion->backend_config<GpuBackendConfig>());\n   FusionBackendConfig& backend_config =\n       *gpu_config.mutable_fusion_backend_config();\n-  backend_config.set_kind(std::string(kTritonFusionKind));\n+  backend_config.set_kind(kTritonFusionKind);\n   TF_RETURN_IF_ERROR(normalization_fusion->set_backend_config(gpu_config));\n   return xla::Cast<HloFusionInstruction>(normalization_fusion);\n }"
        },
        {
            "sha": "74b9cc129deb4e15c1f3909a4226ad673225329e",
            "filename": "third_party/xla/xla/service/gpu/transforms/splitk_rewriter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -309,7 +309,9 @@ absl::StatusOr<HloInstruction*> SplitKDimensionOfDot(HloDotInstruction* src_dot,\n   auto shift_dimension = [](tsl::protobuf::RepeatedField<int64_t>* dims,\n                             int64_t idx) {\n     absl::c_for_each(*dims, [idx](int64_t& dim) {\n-      if (dim >= idx) dim++;\n+      if (dim >= idx) {\n+        dim++;\n+      }\n     });\n   };\n   shift_dimension(new_dnums.mutable_lhs_contracting_dimensions(), lhs_k_idx);"
        },
        {
            "sha": "a09061e6ff1df74ed3616b89fb39b362525b0212",
            "filename": "third_party/xla/xla/service/gpu/transforms/topk_specializer.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -92,7 +92,7 @@ class SpecializeTopkVisitor : public DfsHloRewriteVisitor {\n \n     if (auto small_topk = SmallBufferOptimization(topk); small_topk.ok()) {\n       return ReplaceInstruction(topk, *small_topk);\n-    } else {\n+    } else {  // NOLINT(readability-else-after-return)\n       VLOG(2) << \"Small TopK optimization doesn't match: \"\n               << small_topk.status();\n     }"
        },
        {
            "sha": "9d7e017a93c0147be5864fa7bd9cc7a7334f9a3e",
            "filename": "third_party/xla/xla/service/gpu/transforms/topk_splitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_splitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_splitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_splitter.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -75,7 +75,9 @@ class TopkSplitterVisitor : public DfsHloRewriteVisitor {\n     if (n % kRequiredAlignment != 0) {\n       return absl::OkStatus();\n     }\n-    if (n <= split_threshold_) return absl::OkStatus();\n+    if (n <= split_threshold_) {\n+      return absl::OkStatus();\n+    }\n     int new_batch =\n         std::min(absl::bit_floor(n / split_threshold_), kMaximumBatchSize);\n     int new_n = n / new_batch;"
        },
        {
            "sha": "d551058516345c5b4038a78a6e8cc3d163376313",
            "filename": "third_party/xla/xla/service/gpu/transforms/windowed_einsum_handler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fwindowed_einsum_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b75c02262698bd6938d8a67d2b366d9f67d2ffd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fwindowed_einsum_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fwindowed_einsum_handler.cc?ref=7b75c02262698bd6938d8a67d2b366d9f67d2ffd",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n@@ -709,9 +710,8 @@ absl::Status PostProcessUnrolledLoop(HloInstruction* loop, int64_t stream_id) {\n     }\n   }\n   if (partial_accumulations.size() > 0 &&\n-      while_body->name().find(\n-          WindowedEinsumHandler::kWindowedEinsumAgLoopName) !=\n-          std::string::npos) {\n+      absl::StrContains(while_body->name(),\n+                        WindowedEinsumHandler::kWindowedEinsumAgLoopName)) {\n     TF_RETURN_IF_ERROR(\n         MoveAccumulationOutsideLoop(partial_accumulations, while_body, loop));\n   }"
        }
    ],
    "stats": {
        "total": 324,
        "additions": 204,
        "deletions": 120
    }
}