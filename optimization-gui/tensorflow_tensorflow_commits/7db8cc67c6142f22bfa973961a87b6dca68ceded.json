{
    "author": "apivovarov",
    "message": "Migrate from std::algorithms to absl::c_ algorithms\n\nThis change replaces uses of std::find, std::find_if, std::transform, std::sort, std::next_permutation, std::equal, and std::iota with their absl::c_ counterparts, which operate directly on containers. This simplifies the code by removing the need to pass begin() and end() iterators.\n\nAlso includes minor refactoring of if/else structures and updates function signatures for clarity.\n\nPiperOrigin-RevId: 834467507",
    "sha": "7db8cc67c6142f22bfa973961a87b6dca68ceded",
    "files": [
        {
            "sha": "241f2b58a15ffc8fef0bfa2a786f3eb588ac78a2",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -967,18 +967,20 @@ cc_library(\n     deps = [\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/functional:function_ref\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:logging\",\n     ],\n )\n@@ -1253,7 +1255,6 @@ cc_library(\n         \":hlo_buffer\",\n         \":hlo_cost_analysis\",\n         \":hlo_value\",\n-        \"//xla:debug_options_flags\",\n         \"//xla:shape_util\",\n         \"//xla:side_effect_util\",\n         \"//xla:status_macros\",\n@@ -1299,7 +1300,7 @@ xla_cc_test(\n         \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/hlo/testlib:test_helpers\",\n+        \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/hlo/transforms/collectives:async_collective_creator\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n@@ -2234,12 +2235,12 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/transforms/expanders:op_expander_pass\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -3401,6 +3402,7 @@ xla_cc_test(\n     srcs = [\"hlo_module_test.cc\"],\n     deps = [\n         \":buffer_value\",\n+        \":compilation_environments\",\n         \":computation_placer_hdr\",\n         \":hlo_module_config\",\n         \":test_compilation_environment_proto_cc\",\n@@ -3422,6 +3424,7 @@ xla_cc_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/lib/strings:proto_serialization\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -3430,7 +3433,6 @@ xla_cc_test(\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n         \"@local_tsl//tsl/platform:casts\",\n-        \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n \n@@ -3591,6 +3593,9 @@ cc_library(\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "2ee5b15a2aa538ff26466642acdb926d34259404",
            "filename": "third_party/xla/xla/service/call_graph.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcall_graph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcall_graph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcall_graph.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -18,20 +18,27 @@ limitations under the License.\n #include <deque>\n #include <memory>\n #include <queue>\n+#include <string>\n+#include <utility>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/memory/memory.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/map_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n-#include \"tsl/platform/errors.h\"\n \n namespace xla {\n \n@@ -587,8 +594,7 @@ absl::flat_hash_set<const T*> CallGraph::NearestCommonAncestorsHelper(\n         return nearest_common_ancestors.contains(nca);\n       })) {\n     absl::erase_if(nearest_common_ancestors, [&starting_nodes](const T* nca) {\n-      return std::find(starting_nodes.begin(), starting_nodes.end(), nca) ==\n-             starting_nodes.end();\n+      return absl::c_find(starting_nodes, nca) == starting_nodes.end();\n     });\n   }\n "
        },
        {
            "sha": "6bac08d12cc9c8c1ddb554fda55d0691f319067e",
            "filename": "third_party/xla/xla/service/conditional_code_motion.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fconditional_code_motion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fconditional_code_motion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fconditional_code_motion.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -1043,9 +1043,7 @@ class MoveOperandIntoBranch {\n         VLOG(1) << \"matching_tuple_indices: \"\n                 << matching_tuple_indices[matching_index][0] << \"\\n\";\n         if (matching_tuple_indices[matching_index].end() ==\n-            std::find(matching_tuple_indices[matching_index].begin(),\n-                      matching_tuple_indices[matching_index].end(),\n-                      tuple_index)) {\n+            absl::c_find(matching_tuple_indices[matching_index], tuple_index)) {\n           continue;\n         }\n         for (HloInstruction* param_user : param_users) {\n@@ -1115,8 +1113,7 @@ class MoveOperandIntoBranch {\n       }\n       while (repl_count < new_operands.size()) {\n         HloInstruction* new_input = new_operands[repl_count++];\n-        auto new_input_in_user = std::find(user->operands().begin(),\n-                                           user->operands().end(), new_input);\n+        auto new_input_in_user = absl::c_find(user->operands(), new_input);\n         int64_t opd_index = (new_input_in_user == user->operands().end())\n                                 ? user->operand_count()\n                                 : new_input_in_user - user->operands().begin();\n@@ -1943,9 +1940,8 @@ ConditionalCodeMotion::Decision ConditionalCodeMotion::ConsiderCodeMotion(\n                           ? Decision::Direction::kMoveOutOfBranch\n                           : Decision::Direction::kMoveIntoBranch,\n                       benefit);\n-    } else {\n-      connect.clear_recently_visited();\n     }\n+    connect.clear_recently_visited();\n   } else {\n     connect.AddNewBoundaries(new_boundaries);\n   }"
        },
        {
            "sha": "a54d1a3a1892db60cd6ea133f431aedb0251eefe",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -942,6 +942,7 @@ cc_library(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/llvm_ir:kernel_support_library\",\n         \"//xla/tsl/lib/math:math_util\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/numeric:bits\",\n         \"@com_google_absl//absl/strings\",\n@@ -1127,10 +1128,12 @@ xla_cc_test(\n         \"//xla/service:transpose_folding\",\n         \"//xla/tests:test_utils\",\n         \"//xla/tests:xla_internal_test_main\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "c626bb5e99e9d538dcb9e686fef226cf56e8fb81",
            "filename": "third_party/xla/xla/service/cpu/cpu_instruction_fusion_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_instruction_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_instruction_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_instruction_fusion_test.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,14 +15,15 @@ limitations under the License.\n \n #include \"xla/service/cpu/cpu_instruction_fusion.h\"\n \n-#include <algorithm>\n #include <memory>\n #include <set>\n #include <string>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -35,8 +36,8 @@ limitations under the License.\n #include \"xla/service/transpose_folding.h\"\n #include \"xla/shape.h\"\n #include \"xla/tests/test_utils.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace op = xla::testing::opcode_matchers;\n \n@@ -276,9 +277,8 @@ class OpcodeFusionTest : public InstructionFusionTest {\n     EXPECT_EQ(root->fusion_kind(), fusion_kind);\n \n     std::vector<HloOpcode> fused_opcodes(root->fused_instruction_count());\n-    std::transform(root->fused_instructions().begin(),\n-                   root->fused_instructions().end(), fused_opcodes.begin(),\n-                   [](const HloInstruction* hlo) { return hlo->opcode(); });\n+    absl::c_transform(root->fused_instructions(), fused_opcodes.begin(),\n+                      [](const HloInstruction* hlo) { return hlo->opcode(); });\n \n     EXPECT_EQ(\n         std::multiset<HloOpcode>(fused_opcodes.begin(), fused_opcodes.end()),"
        },
        {
            "sha": "d7a0594c05f29749bc9c6cbd05584d21c5158027",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n+#include <tuple>\n #include <type_traits>\n #include <utility>\n #include <vector>\n@@ -75,7 +76,6 @@ limitations under the License.\n #include \"xla/layout.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/literal.h\"\n-#include \"xla/literal_util.h\"\n #include \"xla/map_util.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -3634,9 +3634,8 @@ llvm_ir::IrArray IrEmitter::GetIrArrayFor(const HloInstruction* hlo) {\n std::vector<llvm_ir::IrArray> IrEmitter::GetIrArraysForOperandsOf(\n     const HloInstruction* hlo) {\n   std::vector<llvm_ir::IrArray> arrays;\n-  std::transform(\n-      hlo->operands().begin(), hlo->operands().end(),\n-      std::back_inserter(arrays),\n+  absl::c_transform(\n+      hlo->operands(), std::back_inserter(arrays),\n       [&](const HloInstruction* operand) { return GetIrArrayFor(operand); });\n   return arrays;\n }"
        },
        {
            "sha": "c67c09b6cbfae30ed9c846fe508b380d38d07a00",
            "filename": "third_party/xla/xla/service/cpu/onednn_matmul.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/service/cpu/onednn_matmul.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <cstring>\n #include <initializer_list>\n@@ -180,9 +179,8 @@ std::unique_ptr<matmul::primitive_desc> CreateMatMulPrimDesc(\n   TransposeIfNecessary(matmul_config.result().tensor().dimensions(), false,\n                        output_md);\n   std::vector<memory::desc> fused_mds;\n-  std::transform(fused_shapes.begin(), fused_shapes.end(),\n-                 std::back_inserter(fused_mds),\n-                 [](const Shape& shape) { return ShapeToMemDesc(shape); });\n+  absl::c_transform(fused_shapes, std::back_inserter(fused_mds),\n+                    [](const Shape& shape) { return ShapeToMemDesc(shape); });\n   return CreateMatMulPrimDesc(engine(engine::kind::cpu, 0), input_md,\n                               weights_md, output_md, fused_mds, matmul_config);\n }\n@@ -218,9 +216,8 @@ CreateOneDnnPrimDesc<dnnl::matmul::primitive_desc>(HloInstruction* instr) {\n   auto fused_operands =\n       HloInstruction::InstructionVector(operands.begin() + 2, operands.end());\n   std::vector<Shape> fused_shapes;\n-  std::transform(fused_operands.begin(), fused_operands.end(),\n-                 std::back_inserter(fused_shapes),\n-                 [](const HloInstruction* instr) { return instr->shape(); });\n+  absl::c_transform(fused_operands, std::back_inserter(fused_shapes),\n+                    [](const HloInstruction* instr) { return instr->shape(); });\n \n   return CreateMatMulPrimDesc(input_shape, weight_shape, output_shape,\n                               fused_shapes, matmul_config);"
        },
        {
            "sha": "d894307fac1b4d5eb2ba4c8c662cd68fd13c7d93",
            "filename": "third_party/xla/xla/service/cpu/tiled_dot_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftiled_dot_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftiled_dot_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftiled_dot_emitter.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,13 +15,13 @@ limitations under the License.\n \n #include \"xla/service/cpu/tiled_dot_emitter.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <iterator>\n #include <string>\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/numeric/bits.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -565,9 +565,8 @@ void RowMajorMatrixVectorProductEmitter::EmitOuterLoopBody(llvm::Value* row,\n                         &scalar_accumulators);\n \n   std::vector<llvm::Value*> accumulator_values;\n-  std::transform(\n-      vector_accumulators.begin(), vector_accumulators.end(),\n-      std::back_inserter(accumulator_values),\n+  absl::c_transform(\n+      vector_accumulators, std::back_inserter(accumulator_values),\n       [](const VectorVariable& vector_var) { return vector_var.Get(); });\n \n   std::vector<llvm::Value*> horizontal_sums;"
        },
        {
            "sha": "3fca25bc77c6f4d9af0c628d3e2aee534e2cb359",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -1397,15 +1397,16 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:test\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n-        \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:status_matchers\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "18e16a3a8309610f06208dc5b0bd526814aecf41",
            "filename": "third_party/xla/xla/service/gpu/cudnn_support_utils_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcudnn_support_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcudnn_support_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcudnn_support_utils_test.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/service/gpu/cudnn_support_utils.h\"\n \n-#include <algorithm>\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n@@ -24,6 +23,10 @@ limitations under the License.\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -35,19 +38,14 @@ limitations under the License.\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/status_matchers.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n namespace {\n \n-using ::tsl::testing::IsOkAndHolds;\n-\n class CudnnSupportUtilsTest : public HloHardwareIndependentTestBase {\n  public:\n   // Gets the custom call with `target` from the `module`. Expects that there is\n@@ -388,11 +386,11 @@ TEST_P(ReorderFilterRank4Test, InferTransposeRank4) {\n }\n \n std::vector<std::string> GeneratePermutations(std::string input_dims) {\n-  std::sort(input_dims.begin(), input_dims.end());\n+  absl::c_sort(input_dims);\n   std::vector<std::string> permutations;\n   do {\n     permutations.push_back(input_dims);\n-  } while (std::next_permutation(input_dims.begin(), input_dims.end()));\n+  } while (absl::c_next_permutation(input_dims));\n   return permutations;\n }\n "
        },
        {
            "sha": "d9209c4e0a76687c4b98261546ad88b5afcea2ee",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -169,9 +169,9 @@ xla_test(\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n@@ -666,12 +666,13 @@ cc_library(\n         \"//xla/codegen/tiling:affine_map_evaluator\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n+        \"//xla/hlo/analysis:interval\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service/gpu:gpu_fusible\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/stream_executor:device_description\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\","
        },
        {
            "sha": "b4f282ed9efab75ad869d8d0084848d717f702e5",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,14 +15,14 @@ limitations under the License.\n \n #include \"xla/service/gpu/model/analytical_latency_estimator.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <functional>\n #include <memory>\n #include <utility>\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -39,7 +39,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/platform/casts.h\"\n \n@@ -51,10 +50,10 @@ namespace {\n \n int64_t GetInstructionIndexInSchedule(\n     absl::Span<HloInstruction* const> schedule, absl::string_view hlo_name) {\n-  return std::find_if(schedule.begin(), schedule.end(),\n-                      [hlo_name](HloInstruction* instruction) {\n-                        return instruction->name() == hlo_name;\n-                      }) -\n+  return absl::c_find_if(schedule,\n+                         [hlo_name](HloInstruction* instruction) {\n+                           return instruction->name() == hlo_name;\n+                         }) -\n          schedule.begin();\n }\n "
        },
        {
            "sha": "833fa98b9816bb34874071a2e10cff1bd0cfe1b6",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n@@ -35,12 +36,13 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/codegen/tiling/affine_map_evaluator.h\"\n+#include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/analysis/interval.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/layout.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n@@ -345,7 +347,7 @@ std::vector<Interval> FindIntervals(\n   FindAllIndices(expr, 0, 0, dimension_ranges, symbol_ranges, &dimensions,\n                  &symbols, &linear_indices);\n \n-  std::sort(linear_indices.begin(), linear_indices.end());\n+  absl::c_sort(linear_indices);\n   linear_indices.erase(\n       std::unique(linear_indices.begin(), linear_indices.end()),\n       linear_indices.end());"
        },
        {
            "sha": "62484812fb8bd5767bd768ee0fc3274647f9acca",
            "filename": "third_party/xla/xla/service/gpu/model/experimental/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -70,6 +70,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:matmul_indexing_utils\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "1bca382ae81b715d3b96c839ad949f00182ffa7a",
            "filename": "third_party/xla/xla/service/gpu/model/experimental/symbolic_tile_propagation.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Fsymbolic_tile_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Fsymbolic_tile_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fexperimental%2Fsymbolic_tile_propagation.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include <string>\n #include <utility>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -83,8 +84,7 @@ SymbolicTiles PropagateTileToOutputForBroadcastOp(\n   dim_tiles.reserve(output_rank);\n   for (auto [output_dim_id, output_dim] :\n        llvm::enumerate(output_shape.dimensions())) {\n-    auto bcast_dim =\n-        std::find(bcast_dims.begin(), bcast_dims.end(), output_dim_id);\n+    auto bcast_dim = absl::c_find(bcast_dims, output_dim_id);\n     // If the dimension is not a broadcast dimension, create a tile that covers\n     // the entire dimension.\n     if (bcast_dim == bcast_dims.end()) {"
        },
        {
            "sha": "05925be364da61dc1240e2bd0edd61df27439213",
            "filename": "third_party/xla/xla/service/gpu/model/hlo_op_profiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiler.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -252,7 +252,7 @@ class CuptiKernelTracer : public HloOpProfiler::KernelTracer,\n       LOG(ERROR) << \"No kernel events\";\n       return 0;\n     }\n-    std::sort(kernel_times_ns_.begin(), kernel_times_ns_.end());\n+    absl::c_sort(kernel_times_ns_);\n     auto i = kernel_times_ns_.size() / 2;\n     // Return median value if number of values is odd.\n     if (kernel_times_ns_.size() % 2 != 0) {"
        },
        {
            "sha": "44fb9236ccb415283841b4160645adc54970ab31",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -380,22 +380,19 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n-        \"@com_google_absl//absl/functional:overload\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@llvm-project//llvm:Support\",\n     ],\n )\n \n@@ -1934,6 +1931,7 @@ cc_library(\n         \"//xla/codegen/tiling:symbolic_tile\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n         \"//xla/codegen/tiling:symbolic_tiled_hlo_instruction\",\n+        \"//xla/codegen/tiling:tiling_specification\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\","
        },
        {
            "sha": "beae7f36fea13491dd1806ea6ce7d32345568eff",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -101,7 +101,6 @@ cc_library(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "39660341c0605d254294057260c9cc2ad940bf42",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n \n-#include <algorithm>\n #include <cstddef>\n #include <cstdint>\n #include <optional>\n@@ -37,7 +36,6 @@ limitations under the License.\n #include \"xla/hlo/ir/replica_group.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n-#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -81,8 +79,8 @@ bool SameParticipantCounts(const absl::flat_hash_map<int64_t, size_t>& lhs,\n   for (const auto& [_, v] : rhs) {\n     rhs_counts.push_back(v);\n   }\n-  std::sort(lhs_counts.begin(), lhs_counts.end());\n-  std::sort(rhs_counts.begin(), rhs_counts.end());\n+  absl::c_sort(lhs_counts);\n+  absl::c_sort(rhs_counts);\n   return lhs_counts == rhs_counts;\n }\n "
        },
        {
            "sha": "236ee1dd1a5d17eceee1f163cecbcbcc9063f687",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -22,14 +22,12 @@ limitations under the License.\n #include <memory>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n-#include \"absl/functional/overload.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -54,7 +52,6 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n@@ -462,7 +459,7 @@ CommandBufferScheduling::CollectCommandBufferSequences(\n   // are captured by the same command buffer.\n   auto collect_async_region = [&](const HloInstruction* start) {\n     auto get_index = [&](const HloInstruction* inst) -> size_t {\n-      auto it = std::find(instructions.begin(), instructions.end(), inst);\n+      auto it = absl::c_find(instructions, inst);\n       return std::distance(instructions.begin(), it);\n     };\n "
        },
        {
            "sha": "4e05b9ebbe5fa43620096865af303c46aa35205a",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fused_conv_rewriter.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 14,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -25,7 +25,6 @@ limitations under the License.\n #include <string>\n #include <tuple>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -386,11 +385,9 @@ class GraphString {\n \n     // Insert op in front of its first use as an operand in graph_ or at the end\n     // of graph_ if not an operand of another op.\n-    auto pos = std::find_if(\n-        graph_.begin(), graph_.end(), [op](OpDescriptor graph_op) -> bool {\n-          return std::find(graph_op.operands.begin(), graph_op.operands.end(),\n-                           op) != graph_op.operands.end();\n-        });\n+    auto pos = absl::c_find_if(graph_, [op](OpDescriptor graph_op) -> bool {\n+      return absl::c_find(graph_op.operands, op) != graph_op.operands.end();\n+    });\n     pos = graph_.insert(pos, OpDescriptor{op, element_type, op_name, operands});\n \n     // If necessary, move the operands of the op already in the graph in front\n@@ -450,19 +447,17 @@ class GraphString {\n     auto op_filter = [&](OpDescriptor graph_op) -> bool {\n       if (op_name.empty()) {\n         return graph_op.instr->unique_id() == op->unique_id();\n-      } else {\n-        return graph_op.instr->unique_id() == op->unique_id() &&\n-               graph_op.name == op_name;\n       }\n+      return graph_op.instr->unique_id() == op->unique_id() &&\n+             graph_op.name == op_name;\n     };\n-    return std::find_if(graph_.begin(), graph_.end(), op_filter) !=\n-           graph_.end();\n+    return absl::c_find_if(graph_, op_filter) != graph_.end();\n   }\n \n   std::vector<HloInstruction*> Operands(HloInstruction* op) const {\n-    auto op_it = std::find_if(\n-        graph_.begin(), graph_.end(),\n-        [op](OpDescriptor graph_op) -> bool { return op == graph_op.instr; });\n+    auto op_it = absl::c_find_if(graph_, [op](OpDescriptor graph_op) -> bool {\n+      return op == graph_op.instr;\n+    });\n     if (op_it != graph_.end()) {\n       return op_it->operands;\n     }"
        },
        {
            "sha": "f7a0e33958be541a7b638aa40caffaf20e49a24a",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_norm_rewriter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -967,8 +967,7 @@ class CudnnNormRewriterVisitor : public DfsHloRewriteVisitor {\n       std::vector<int64_t> non_norm_dims;\n       for (int64_t x_dim = 0; x_dim < x.instr()->shape().dimensions().size();\n            ++x_dim) {\n-        if (std::find(norm_dims.begin(), norm_dims.end(), x_dim) ==\n-            norm_dims.end()) {\n+        if (absl::c_find(norm_dims, x_dim) == norm_dims.end()) {\n           non_norm_dims.push_back(x_dim);\n         }\n       }"
        },
        {
            "sha": "a4dd62b7278bdc73a52c0ee590afdf1ea0d252f6",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,8 +15,6 @@ limitations under the License.\n \n #include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n \n-#include <algorithm>\n-#include <cstddef>\n #include <cstdint>\n #include <deque>\n #include <memory>\n@@ -44,6 +42,7 @@ limitations under the License.\n #include \"xla/codegen/tiling/symbolic_tile.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n #include \"xla/codegen/tiling/symbolic_tiled_hlo_instruction.h\"\n+#include \"xla/codegen/tiling/tiling_specification.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -59,6 +58,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/model/triton_emitter_constraints.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/matmul_indexing_utils.h\"\n@@ -1398,7 +1398,7 @@ absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n           << computation->root_instruction()->shape().ToString();\n   llvm::SmallVector<int64_t> output_tile_sizes = get_tile_sizes(out_rank);\n \n-  std::sort(output_tile_sizes.begin(), output_tile_sizes.end());\n+  absl::c_sort(output_tile_sizes);\n \n   const TilingSpecification& tiling_specification =\n       analysis.GetTilingSpecification();\n@@ -1445,8 +1445,7 @@ absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n     VLOG(4) << \"mapped_dot_tile_sizes: \"\n             << absl::StrJoin(mapped_dot_tile_sizes, \",\")\n             << \" != \" << absl::StrJoin(expected_dot_tile_sizes, \",\");\n-  } while (std::next_permutation(output_tile_sizes.begin(),\n-                                 output_tile_sizes.end()));\n+  } while (absl::c_next_permutation(output_tile_sizes));\n \n   return absl::InternalError(absl::StrCat(\n       \"Couldn't find output tile sizes that satisfy \", tiled_dot.ToString()));"
        },
        {
            "sha": "a29e284375c4e4e9e8d3ca4ee96969e7425bdb14",
            "filename": "third_party/xla/xla/service/gpu/transforms/tree_reduction_rewriter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftree_reduction_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftree_reduction_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftree_reduction_rewriter.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -289,8 +289,7 @@ class ReductionRewriterVisitor : public DfsHloRewriteVisitor {\n     // Compute dimensions to reduce for inner reduction.\n     absl::InlinedVector<int64_t, 2> inner_reduce_dims(\n         sorted_dims_to_reduce.begin(), sorted_dims_to_reduce.end());\n-    auto split_dim_it = std::find(inner_reduce_dims.begin(),\n-                                  inner_reduce_dims.end(), split_params.dim);\n+    auto split_dim_it = absl::c_find(inner_reduce_dims, split_params.dim);\n     *split_dim_it += 1;\n \n     // Compute dimension to reduce for outer reduction."
        },
        {
            "sha": "20dc56cf80a375933c362a1aaaf2a433dc8cbdd0",
            "filename": "third_party/xla/xla/service/hlo_module_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_test.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/hlo/ir/hlo_module.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <memory>\n #include <optional>\n@@ -24,6 +23,7 @@ limitations under the License.\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n@@ -46,6 +46,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_matchers.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/buffer_value.h\"\n+#include \"xla/service/compilation_environments.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/test_compilation_environment.pb.h\"\n@@ -58,7 +59,6 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n-#include \"tsl/platform/protobuf.h\"\n \n namespace xla {\n \n@@ -173,9 +173,8 @@ TEST_F(HloModuleTest, CloneFrontendAttributes) {\n   frontend_attributes.mutable_map()->emplace(\"attribute1\", \"attribute1_value\");\n   module->set_frontend_attributes(frontend_attributes);\n   std::unique_ptr<HloModule> clone = module->Clone();\n-  bool areEqual = std::equal(\n-      frontend_attributes.map().begin(), frontend_attributes.map().end(),\n-      clone->frontend_attributes().map().begin(),\n+  bool areEqual = absl::c_equal(\n+      frontend_attributes.map(), clone->frontend_attributes().map(),\n       [](const auto& kv1, const auto& kv2) {\n         return kv1.first == kv2.first && kv1.second == kv2.second;\n       });"
        },
        {
            "sha": "e2a5cd29821fb24bb0c407b7138c6d8de2b670d3",
            "filename": "third_party/xla/xla/service/hlo_verifier.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 33,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -93,10 +93,9 @@ absl::Status CheckUnaryOpWithResultAccuracy(HloInstruction* unary) {\n   if (unary->has_result_accuracy()) {\n     if (IsUnaryOpWithResultAccuracy(unary->opcode())) {\n       return absl::OkStatus();\n-    } else {\n-      return Internal(\"Unary op with result accuracy is not supported for %s\",\n-                      HloOpcodeString(opcode));\n     }\n+    return Internal(\"Unary op with result accuracy is not supported for %s\",\n+                    HloOpcodeString(opcode));\n   }\n   return absl::OkStatus();\n }\n@@ -642,15 +641,14 @@ absl::Status ShapeVerifier::HandleAllToAll(HloInstruction* hlo) {\n         hlo, ShapeInference::InferAllToAllShape(\n                  hlo->operand(0)->shape(), *all_to_all->split_dimension(),\n                  *all_to_all->split_dimension(), split_count));\n-  } else {\n-    TF_RET_CHECK(hlo->operand_count() == split_count);\n-    std::vector<const Shape*> operand_shapes;\n-    for (const HloInstruction* operand : hlo->operands()) {\n-      operand_shapes.push_back(&operand->shape());\n-    }\n-    return CheckShape(hlo,\n-                      ShapeInference::InferAllToAllTupleShape(operand_shapes));\n   }\n+  TF_RET_CHECK(hlo->operand_count() == split_count);\n+  std::vector<const Shape*> operand_shapes;\n+  for (const HloInstruction* operand : hlo->operands()) {\n+    operand_shapes.push_back(&operand->shape());\n+  }\n+  return CheckShape(hlo,\n+                    ShapeInference::InferAllToAllTupleShape(operand_shapes));\n }\n \n absl::Status ShapeVerifier::HandleRaggedAllToAll(HloInstruction* hlo) {\n@@ -842,16 +840,15 @@ absl::Status CheckDuplicatedSourceOrTarget(\n             \"Source %d appears more than once in instruction's source-target \"\n             \"pairs: %s\",\n             p.first, collective_permute->ToString());\n-      } else {\n-        return Internal(\n-            \"Source %d appears more than %d times in instruction's \"\n-            \"source-target \"\n-            \"pairs: %s\",\n-            p.first, allowed_seen_count, collective_permute->ToString());\n       }\n-    } else {\n-      seen_source_to_targets[p.first].push_back(p.second);\n+      return Internal(\n+          \"Source %d appears more than %d times in instruction's \"\n+          \"source-target \"\n+          \"pairs: %s\",\n+          p.first, allowed_seen_count, collective_permute->ToString());\n     }\n+    seen_source_to_targets[p.first].push_back(p.second);\n+\n     TF_RET_CHECK(p.second >= 0)\n         << \"Target \" << p.second\n         << \" in the instruction's source-target pair must be >= 0 : \"\n@@ -867,16 +864,14 @@ absl::Status CheckDuplicatedSourceOrTarget(\n             \"Target %d appears more than once in instruction's source-target \"\n             \"pairs: %s\",\n             p.second, collective_permute->ToString());\n-      } else {\n-        return Internal(\n-            \"Target %d appears more than %d times in instruction's \"\n-            \"source-target \"\n-            \"pairs: %s\",\n-            p.second, allowed_seen_count, collective_permute->ToString());\n       }\n-    } else {\n-      seen_target_to_sources[p.second].push_back(p.first);\n+      return Internal(\n+          \"Target %d appears more than %d times in instruction's \"\n+          \"source-target \"\n+          \"pairs: %s\",\n+          p.second, allowed_seen_count, collective_permute->ToString());\n     }\n+    seen_target_to_sources[p.second].push_back(p.first);\n   }\n   return absl::OkStatus();\n }\n@@ -3647,9 +3642,9 @@ absl::Status InstructionVerifier::HandleTranspose(HloInstruction* transpose) {\n   TF_RET_CHECK(shape.dimensions().size() == transpose->dimensions().size());\n   TF_RET_CHECK(shape.dimensions().size() ==\n                transpose->operand(0)->shape().dimensions().size());\n-  TF_RET_CHECK(std::equal(\n-      shape.dimensions().begin(), shape.dimensions().end(),\n-      Permute(operand->shape().dimensions(), transpose->dimensions()).begin()))\n+  TF_RET_CHECK(absl::c_equal(\n+      shape.dimensions(),\n+      Permute(operand->shape().dimensions(), transpose->dimensions())))\n       << \"shape: \" << shape << \", operand->shape(): \" << shape\n       << \", dimensions: {\" << absl::StrJoin(transpose->dimensions(), \", \")\n       << \"}\";\n@@ -3845,7 +3840,7 @@ absl::StatusOr<bool> HloVerifier::RunImpl(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   auto disabled = module->config().debug_options().xla_disable_hlo_passes();\n-  if (std::find(disabled.begin(), disabled.end(), name()) != disabled.end()) {\n+  if (absl::c_find(disabled, name()) != disabled.end()) {\n     return false;\n   }\n   auto status_or_changed = [&]() -> absl::StatusOr<bool> {\n@@ -3900,9 +3895,8 @@ absl::StatusOr<bool> HloVerifier::RunImpl(\n           *module, [this](const Shape& shape) -> int64_t {\n             if (target_metadata_->GetVerifierOpts().IsLayoutSensitive()) {\n               return target_metadata_->GetVerifierOpts().ShapeSize(shape);\n-            } else {\n-              return 0;\n             }\n+            return 0;\n           }));\n     }\n "
        },
        {
            "sha": "eae8d0ee1eb506fa333551df987cd0b7f981bb75",
            "filename": "third_party/xla/xla/service/latency_hiding_scheduler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -43,7 +43,6 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n #include \"xla/hlo/analysis/hlo_reachability.h\"\n@@ -1826,8 +1825,7 @@ DefaultSchedulerCore::FindAndExtractBestNodeAvailable(\n           sched_state.ongoing_annotation = -1;\n         }\n         // Remove this annotation from ready_annotations if it's there.\n-        auto it = std::find(sched_state.ready_annotations.begin(),\n-                            sched_state.ready_annotations.end(), annotation);\n+        auto it = absl::c_find(sched_state.ready_annotations, annotation);\n         if (it != sched_state.ready_annotations.end()) {\n           sched_state.ready_annotations.erase(it);\n         }\n@@ -2171,8 +2169,7 @@ absl::Status DefaultSchedulerCore::ScheduleAnnotation(\n       continue;\n     }\n     // Delete the node from the ready set.\n-    auto node_it = std::find(sched_state->ready_set.begin(),\n-                             sched_state->ready_set.end(), node);\n+    auto node_it = absl::c_find(sched_state->ready_set, node);\n     TF_RET_CHECK(node_it != sched_state->ready_set.end())\n         << \"Couldn't find the annotated node in ready set: \"\n         << node->GetInstr().name();\n@@ -2260,8 +2257,7 @@ absl::StatusOr<HloGraphNode::TimeCost> DefaultSchedulerCore::ScheduleNode(\n   // was there.\n   if (sched_state->config.enable_selective_resources &&\n       n->ReleasesSelectiveResource()) {\n-    auto it = std::find(sched_state->selective_resource_releasers.begin(),\n-                        sched_state->selective_resource_releasers.end(), n);\n+    auto it = absl::c_find(sched_state->selective_resource_releasers, n);\n     // Perform sanity check node was in selective_resources_releasers.\n     if (it == sched_state->selective_resource_releasers.end()) {\n       LOG(WARNING) << \"Selective resource releasers list does not contain node \"\n@@ -3066,8 +3062,7 @@ DefaultSchedulerCore::GetNumResourcesNeededForAnnotation(\n         // assuming maximum overlapping, where the resources used by the\n         // async-done ops need to be accumulated.\n         const HloInstruction* start = instr->operand(0);\n-        if (std::find(instrs.begin(), instrs.end(), start) == instrs.end() ||\n-            get_max_resources) {\n+        if (absl::c_find(instrs, start) == instrs.end() || get_max_resources) {\n           num_resources_needed[resource] += usage;\n           continue;\n         }"
        },
        {
            "sha": "9a3dd47c32949b463302559d06267a9cb56861f5",
            "filename": "third_party/xla/xla/service/latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler_test.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/hlo/testlib/test_helpers.h\"\n+#include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/hlo/transforms/collectives/async_collective_creator.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -63,7 +63,7 @@ constexpr int kMaxConcurrentAsyncCollectivePermutes = 5;\n \n int PositionInVector(absl::Span<HloInstruction* const> vec,\n                      const HloInstruction* element) {\n-  return std::distance(vec.begin(), std::find(vec.begin(), vec.end(), element));\n+  return std::distance(vec.begin(), absl::c_find(vec, element));\n }\n \n bool MaxConcurrentCollectivePermutesBelowThreshold("
        },
        {
            "sha": "1524a372a6c8b30255cce2777d2206903b47cfcd",
            "filename": "third_party/xla/xla/service/layout_assignment.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/service/layout_assignment.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <deque>\n #include <iterator>\n@@ -58,6 +57,9 @@ limitations under the License.\n #include \"xla/shape_layout.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -455,9 +457,8 @@ absl::Status LayoutAssignment::SetInstructionLayout(\n         if (subshape.IsArray()) {\n           return SetBufferLayout(layout, *buffers[0], mandatory,\n                                  /*dfs=*/true, priority);\n-        } else {\n-          return absl::OkStatus();\n         }\n+        return absl::OkStatus();\n       });\n }\n \n@@ -497,9 +498,8 @@ absl::Status LayoutAssignment::SetInstructionLayout(\n         if (subshape.IsArray() && subshape.has_layout()) {\n           return SetBufferLayout(subshape.layout(), *buffers[0], mandatory,\n                                  /*dfs=*/dfs, priority);\n-        } else {\n-          return absl::OkStatus();\n         }\n+        return absl::OkStatus();\n       }));\n   VLOG(3) << \"Setting operand layout?\\n\";\n   if (shape_with_layout.IsArray() &&\n@@ -3024,10 +3024,10 @@ absl::Status LayoutAssignment::Init(HloModule* module) {\n     std::vector<HloInstruction*> copies_to_remove(added_copies_.begin(),\n                                                   added_copies_.end());\n     // Ensure determinism.\n-    std::sort(copies_to_remove.begin(), copies_to_remove.end(),\n-              [](const HloInstruction* a, const HloInstruction* b) {\n-                return a->unique_id() < b->unique_id();\n-              });\n+    absl::c_sort(copies_to_remove,\n+                 [](const HloInstruction* a, const HloInstruction* b) {\n+                   return a->unique_id() < b->unique_id();\n+                 });\n     for (HloInstruction* instruction : copies_to_remove) {\n       VLOG(5) << \"Removing added copy: \" << instruction->ToString();\n       HloComputation* computation = instruction->parent();"
        },
        {
            "sha": "fd8d2c5595c039fcd77ea3deedeb4d885862e0e9",
            "filename": "third_party/xla/xla/service/llvm_ir/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -283,6 +283,7 @@ cc_library(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:status\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "51d8ef32a247ca4d0e988ddc5d30ea5168820a32",
            "filename": "third_party/xla/xla/service/llvm_ir/kernel_support_library.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fkernel_support_library.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fkernel_support_library.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fkernel_support_library.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,13 +15,13 @@ limitations under the License.\n \n #include \"xla/service/llvm_ir/kernel_support_library.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <functional>\n #include <iterator>\n #include <memory>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -110,9 +110,8 @@ void KernelSupportLibrary::EmitAndCallOutlinedKernel(\n   if (!function) {\n     VLOG(2) << \"Generating kernel for \" << kernel_name;\n     std::vector<llvm::Type*> arg_types;\n-    std::transform(sanitized_args.begin(), sanitized_args.end(),\n-                   std::back_inserter(arg_types),\n-                   [](llvm::Value* arg) { return arg->getType(); });\n+    absl::c_transform(sanitized_args, std::back_inserter(arg_types),\n+                      [](llvm::Value* arg) { return arg->getType(); });\n \n     auto* function_type =\n         llvm::FunctionType::get(b->getVoidTy(), arg_types, /*isVarArg=*/false);"
        },
        {
            "sha": "9d6e4ddaf12f6a7abaaf6d370ab202da39e6fa0e",
            "filename": "third_party/xla/xla/service/memory_space_assignment/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -127,7 +127,6 @@ xla_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:status\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -141,7 +140,6 @@ xla_test(\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n         \"@local_tsl//tsl/platform:protobuf\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "9905941f42919b1828d3a0f3d053f5a7d592306d",
            "filename": "third_party/xla/xla/service/memory_space_assignment/algorithm.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 21,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -157,9 +157,8 @@ bool LooksLikeAnActivation(const HloInstruction* inst, bool permissive_mode) {\n           user = user->parent()->FusionInstruction();\n           if (LooksLikeAnActivation(user, permissive_mode)) {\n             return true;\n-          } else {\n-            break;\n           }\n+          break;\n         }\n         return true;\n       case HloOpcode::kDynamicUpdateSlice:\n@@ -854,13 +853,12 @@ bool MsaAlgorithm::IsUseAllowedInAlternateMemory(const AllocationValue& value,\n                 << \", parameter time = \" << parameter_time\n                 << \", min use time = \" << min_use_time;\n         return true;\n-      } else {\n-        VLOG(4) << \"Conditional allocation not allowed in alternate memory for \"\n-                   \"computation = \"\n-                << called_computation->name()\n-                << \", parameter time = \" << parameter_time\n-                << \", min use time = \" << min_use_time;\n       }\n+      VLOG(4) << \"Conditional allocation not allowed in alternate memory for \"\n+                 \"computation = \"\n+              << called_computation->name()\n+              << \", parameter time = \" << parameter_time\n+              << \", min use time = \" << min_use_time;\n     }\n     return false;\n   }\n@@ -1511,8 +1509,7 @@ std::vector<const HloValue*> MsaAlgorithm::GenerateJointProcessedValues(\n       const HloValue& next_value = alias_analysis_.dataflow_analysis()\n                                        .GetValueSet(inst)\n                                        .GetUniqueValue();\n-      if (std::find(worklist.begin(), worklist.end(), &next_value) ==\n-          worklist.end()) {\n+      if (absl::c_find(worklist, &next_value) == worklist.end()) {\n         worklist.push_back(&next_value);\n       }\n     };\n@@ -1781,9 +1778,8 @@ void FixAllocationSequenceAfterPostAllocationTransformation(\n       std::remove_if(\n           allocations->begin(), allocations->end(),\n           [transformation_info](const std::unique_ptr<Allocation>& allocation) {\n-            return std::find(transformation_info.to_be_removed.begin(),\n-                             transformation_info.to_be_removed.end(),\n-                             allocation->defining_position().instruction) !=\n+            return absl::c_find(transformation_info.to_be_removed,\n+                                allocation->defining_position().instruction) !=\n                    transformation_info.to_be_removed.end();\n           }),\n       allocations->end());\n@@ -4421,11 +4417,15 @@ void MsaAlgorithm::MaybeCreateMirroredParentAllocationForWhileUse(\n         preferred_offset_for_computation) {\n   const HloUse& hlo_use = use.hlo_use;\n \n-  if (hlo_use.instruction->opcode() != HloOpcode::kWhile) return;\n+  if (hlo_use.instruction->opcode() != HloOpcode::kWhile) {\n+    return;\n+  }\n \n   Allocation* aliased_allocation =\n       GetLiveAllocationAt(*allocation_value.allocation_sequence(), use_time);\n-  if (aliased_allocation->memory_space() != MemorySpace::kAlternate) return;\n+  if (aliased_allocation->memory_space() != MemorySpace::kAlternate) {\n+    return;\n+  }\n \n   const auto& instruction_schedule = hlo_live_range_.instruction_schedule();\n   if (options_.enable_while_redundant_eviction_elimination &&\n@@ -6647,13 +6647,12 @@ bool MsaAlgorithm::ViolatesMaximumOutstandingAsyncCopies(\n                              num_additional_copies;\n     return num_prefetches >=\n            options_.max_outstanding_prefetches + extra_async_copy_limit;\n-  } else {\n-    int64_t num_evictions = eviction_interval_tree_.NumChunksOverlappingInTime(\n-                                inclusive_start_time, end_time) +\n-                            num_additional_copies;\n-    return num_evictions >=\n-           options_.max_outstanding_evictions + extra_async_copy_limit;\n   }\n+  int64_t num_evictions = eviction_interval_tree_.NumChunksOverlappingInTime(\n+                              inclusive_start_time, end_time) +\n+                          num_additional_copies;\n+  return num_evictions >=\n+         options_.max_outstanding_evictions + extra_async_copy_limit;\n }\n \n AllocationResult MsaAlgorithm::ForceAlternateMemoryAllocationForMinTime("
        },
        {
            "sha": "6ebf01e72259d0c405287766a8b49b055153cda9",
            "filename": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,8 +15,6 @@ limitations under the License.\n \n #include \"xla/service/memory_space_assignment/memory_space_assignment.h\"\n \n-#include <stdbool.h>\n-\n #include <algorithm>\n #include <cstdint>\n #include <functional>\n@@ -53,6 +51,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/hlo/utils/hlo_live_range.h\"\n@@ -87,12 +86,10 @@ limitations under the License.\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/status.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/protobuf.h\"  // IWYU pragma: keep\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace memory_space_assignment {\n@@ -510,8 +507,7 @@ ENTRY entry {\n       module->schedule().sequence(module->entry_computation());\n   auto find_index = [&](const HloInstruction* instruction) {\n     return std::distance(sequence.instructions().begin(),\n-                         std::find(sequence.instructions().begin(),\n-                                   sequence.instructions().end(), instruction));\n+                         absl::c_find(sequence.instructions(), instruction));\n   };\n   int64_t copy_done_time = find_index(copy_done);\n   int64_t negate9_time = find_index(negate9);\n@@ -1501,8 +1497,7 @@ ENTRY entry {\n           request_modifier_module->entry_computation());\n   auto find_index = [&](const HloInstruction* instruction) {\n     return std::distance(sequence.instructions().begin(),\n-                         std::find(sequence.instructions().begin(),\n-                                   sequence.instructions().end(), instruction));\n+                         absl::c_find(sequence.instructions(), instruction));\n   };\n \n   int negate4_index = find_index(negate4);"
        },
        {
            "sha": "c6121b02c2957d940d38bd3a2c9955ee98765d29",
            "filename": "third_party/xla/xla/service/scatter_determinism_expander.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -15,11 +15,11 @@ limitations under the License.\n \n #include \"xla/service/scatter_determinism_expander.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <optional>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/strings/str_format.h\"\n@@ -38,10 +38,9 @@ limitations under the License.\n #include \"xla/service/scatter_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n \n@@ -736,9 +735,8 @@ absl::StatusOr<HloInstruction*> ScatterDeterminismExpander::ExpandInstruction(\n     std::vector<int64_t> actual_update_window_dims(num_operand_dims);\n     int update_dim_index = 0;\n     for (int i = 0; i < num_operand_dims; ++i) {\n-      if (std::find(dim_numbers.inserted_window_dims().begin(),\n-                    dim_numbers.inserted_window_dims().end(),\n-                    i) != dim_numbers.inserted_window_dims().end()) {\n+      if (absl::c_find(dim_numbers.inserted_window_dims(), i) !=\n+          dim_numbers.inserted_window_dims().end()) {\n         actual_update_window_dims[i] = 1;\n       } else {\n         actual_update_window_dims[i] ="
        },
        {
            "sha": "3bda8f55781ebcab611ef8b808e03a0e307ee946",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #include <deque>\n #include <functional>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <string>\n #include <tuple>\n@@ -732,7 +731,7 @@ std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n     lhs_tile_assignment_array->Reshape(lhs_tile_assignment_dimensions);\n \n     std::vector<int64_t> transpose_order(lhs_tile_assignment_dimensions.size());\n-    std::iota(transpose_order.begin(), transpose_order.end(), 0);\n+    absl::c_iota(transpose_order, 0);\n     transpose_order.back() = lhs_windowing_dim;\n     transpose_order[lhs_windowing_dim] =\n         lhs_tile_assignment_dimensions.size() - 1;"
        },
        {
            "sha": "faa2d9c923dd8c77c7485993f4c60475a2099c5e",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -93,10 +93,9 @@ std::string SpmdLogger::MakeReport() {\n   absl::StrAppend(&report,\n                   \"\\n\\n***** SPMD memory during transformation *****\\n\");\n \n-  std::sort(entries_.begin(), entries_.end(),\n-            [](auto const& entry0, auto const& entry1) {\n-              return entry0.first > entry1.first;\n-            });\n+  absl::c_sort(entries_, [](auto const& entry0, auto const& entry1) {\n+    return entry0.first > entry1.first;\n+  });\n   for (int64_t i = 0;\n        i < std::min<int64_t>(report_instruction_count_, entries_.size()); ++i) {\n     absl::StrAppend(&report, \"\\n  \",\n@@ -176,22 +175,22 @@ template <typename F>\n     }\n   }\n \n-  const auto add_report = [&](std::vector<HloInstruction*>* insts) {\n-    std::sort(insts->begin(), insts->end(),\n-              [](const HloInstruction* inst0, const HloInstruction* inst1) {\n-                return ShapeSizeInBytes(inst0->shape()) >\n-                       ShapeSizeInBytes(inst1->shape());\n-              });\n+  const auto add_report = [&](std::vector<HloInstruction*>& insts) {\n+    absl::c_sort(insts,\n+                 [](const HloInstruction* inst0, const HloInstruction* inst1) {\n+                   return ShapeSizeInBytes(inst0->shape()) >\n+                          ShapeSizeInBytes(inst1->shape());\n+                 });\n     for (int64_t i = 0;\n-         i < std::min<int64_t>(report_instruction_count, insts->size()); ++i) {\n+         i < std::min<int64_t>(report_instruction_count, insts.size()); ++i) {\n       absl::StrAppend(&report, \"  \",\n                       tsl::strings::HumanReadableNumBytes(\n-                          ShapeSizeInBytes((*insts)[i]->shape())),\n-                      \" : \", (*insts)[i]->ToString(), \"\\n\");\n+                          ShapeSizeInBytes(insts[i]->shape())),\n+                      \" : \", insts[i]->ToString(), \"\\n\");\n     }\n   };\n \n-  add_report(&instructions);\n+  add_report(instructions);\n   return report;\n }\n \n@@ -677,7 +676,7 @@ PartitionedHlo PartitionedHlo::ReshardNoCache(\n   if (target.ReplicateOnLastTileDim()) {\n     std::vector<int64_t> group_dims(target.tile_assignment().num_dimensions() -\n                                     1);\n-    std::iota(group_dims.begin(), group_dims.end(), 0);\n+    absl::c_iota(group_dims, 0);\n     auto target_grouped =\n         hlo_sharding_util::GroupShardingOnDims(target, group_dims);\n     auto partially_sharded = PerGroupSliceFromReplicated(\n@@ -1339,7 +1338,7 @@ PartitionedHlo PartitionedHlo::Replicate() const {\n \n   // 'Tiled' to 'Replicated'.\n   std::vector<int64_t> all_dims(shape.dimensions().size());\n-  std::iota(all_dims.begin(), all_dims.end(), 0);\n+  absl::c_iota(all_dims, 0);\n   HloInstruction* result = ReplicatePartial(all_dims);\n   result->set_sharding(HloSharding::Replicate());\n   return update_cache(PartitionedHlo(result, base_shape_, state_));\n@@ -1683,7 +1682,7 @@ HloSharding GetAllToAllSharding(const HloSharding& source_sharding,\n     }\n \n     std::vector<int> permutation(shape_1_dims.size());\n-    std::iota(permutation.begin(), permutation.end(), 0);\n+    absl::c_iota(permutation, 0);\n     std::swap(permutation[added_source_dim], permutation[added_target_dim]);\n     std::vector<int64_t> shape_2_dims(result.dimensions().begin(),\n                                       result.dimensions().end());\n@@ -1995,7 +1994,7 @@ PartitionedHlo PartitionedHlo::TryMultipleSourceTargetDims(\n       HloInstruction::CreateReshape(transpose_0->shape(), all_to_all));\n \n   std::vector<int64_t> permutation_1(base_shape_.dimensions().size());\n-  std::iota(permutation_1.begin(), permutation_1.end(), num_eligible_dims);\n+  absl::c_iota(permutation_1, num_eligible_dims);\n   for (int64_t i = 0; i < num_eligible_dims; ++i) {\n     auto it = absl::c_find(permutation_1,\n                            eligible_source_dims[i] + num_eligible_dims);\n@@ -2268,7 +2267,7 @@ std::optional<PartitionedHlo> PartitionedHlo::TryComplexReshardHandling(\n             << sharding().ToString();\n     std::vector<int64_t> transpose_dims(\n         sharding().tile_assignment().num_dimensions(), 0);\n-    std::iota(transpose_dims.begin(), transpose_dims.end(), 0);\n+    absl::c_iota(transpose_dims, 0);\n     std::swap(transpose_dims[first_different_dimension], transpose_dims.back());\n     auto intermediate_sharding =\n         hlo_sharding_util::TransposeSharding(sharding(), transpose_dims);\n@@ -4601,7 +4600,7 @@ absl::Status SpmdPartitioningVisitor::HandleRng(HloInstruction* hlo) {\n   } else {\n     std::vector<int64_t> group_dims(\n         hlo->sharding().tile_assignment().num_dimensions() - 1);\n-    std::iota(group_dims.begin(), group_dims.end(), 0);\n+    absl::c_iota(group_dims, 0);\n     auto sharding_grouped =\n         hlo_sharding_util::GroupShardingOnDims(hlo->sharding(), group_dims);\n     auto per_group_state = CreatePerGroupPartitioningState("
        },
        {
            "sha": "fad9a726ad738d11a56bd10eee1996875ee0a967",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7db8cc67c6142f22bfa973961a87b6dca68ceded/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=7db8cc67c6142f22bfa973961a87b6dca68ceded",
            "patch": "@@ -2974,7 +2974,7 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsAcrossTargetDims(\n   }\n \n   std::vector<int> transpose_dims(reshape_dimensions.size());\n-  std::iota(transpose_dims.begin(), transpose_dims.end(), 0);\n+  absl::c_iota(transpose_dims, 0);\n   for (int64_t loc : target_dim_locations) {\n     if (auto it = absl::c_find(transpose_dims, loc);\n         it != transpose_dims.end()) {\n@@ -3037,13 +3037,13 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsForReplication(\n   // into a tile assignment with dims [M, N], where M is the number of replica\n   // groups and N is the size of each replica group.\n   std::vector<int> transpose_dims(sharding.tile_assignment().num_dimensions());\n-  std::iota(transpose_dims.begin(), transpose_dims.end(), 0);\n+  absl::c_iota(transpose_dims, 0);\n \n   // Sorting is not necessary but is done to match the non-optimized equivalent\n   // function.\n   std::vector<int> replication_dims_sorted(replication_dims.begin(),\n                                            replication_dims.end());\n-  std::sort(replication_dims_sorted.begin(), replication_dims_sorted.end());\n+  absl::c_sort(replication_dims_sorted);\n   for (int64_t i : replication_dims_sorted) {\n     if (auto it = absl::c_find(transpose_dims, i); it != transpose_dims.end()) {\n       transpose_dims.erase(it);"
        }
    ],
    "stats": {
        "total": 415,
        "additions": 191,
        "deletions": 224
    }
}