{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 809667447",
    "sha": "189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
    "files": [
        {
            "sha": "16ac74fb95049c3551fa31f78cfb103b5b7d80fa",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/gpu_event.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Fgpu_event.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Fgpu_event.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Fgpu_event.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -46,7 +46,7 @@ tsl::AsyncValueRef<GpuEvent> AfterAll(\n   for (auto& event : events) {\n     event.AndThen([state, event = event.AsPtr()]() {\n       if (event.IsError()) {\n-        absl::MutexLock lock(&state->mutex);\n+        absl::MutexLock lock(state->mutex);\n         state->error_status = event.GetError();\n       }\n "
        },
        {
            "sha": "aadbded1e2e3b029f1c4d3478ff083ee3679a151",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_async_host_to_device_transfer_manager.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -168,7 +168,7 @@ TfrtGpuAsyncHostToDeviceTransferManager::\n     ~TfrtGpuAsyncHostToDeviceTransferManager() {\n   for (int buffer_index = 0; buffer_index < transfers_in_flight_.size();\n        buffer_index++) {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     // Make sure we don't leave dangling pointers in cleanup routines even\n     // if the client lets the object go out of scope.\n     mu_.Await(absl::Condition(\n@@ -196,7 +196,7 @@ absl::Status TfrtGpuAsyncHostToDeviceTransferManager::TransferLiteralToBuffer(\n \n   tsl::AsyncValueRef<GpuDeviceMemory> buffer;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n \n     DCHECK_LT(buffer_index, buffer_ptrs_.size());\n     if (last_transfer_started_[buffer_index]) {\n@@ -282,7 +282,7 @@ TfrtGpuAsyncHostToDeviceTransferManager::TransferRawDataToSubBuffer(\n \n   se::DeviceMemoryBase sub_buffer;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     DCHECK_LT(buffer_index, buffer_ptrs_.size());\n     if (last_transfer_started_[buffer_index]) {\n       return InvalidArgument(\n@@ -360,7 +360,7 @@ TfrtGpuAsyncHostToDeviceTransferManager::TransferRawDataToSubBuffer(\n void TfrtGpuAsyncHostToDeviceTransferManager::SetBufferError(\n     int buffer_index, absl::Status error) {\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     // For a given buffer_index, SetBufferError can't be called twice, or\n     // called after the last transfer has been enqueued.\n     CHECK(!definition_events_[buffer_index].IsConcrete());\n@@ -388,7 +388,7 @@ void TfrtGpuAsyncHostToDeviceTransferManager::CleanUp(\n     tsl::profiler::TraceMe traceme(\n         \"TfrtGpuAsyncHostToDeviceTransferManager::CleanUp\");\n \n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n \n     bool last_transfer_started = last_transfer_started_[buffer_index];\n     size_t transfers_in_flight = transfers_in_flight_[buffer_index];"
        },
        {
            "sha": "4479525d4798d31472da21bff011b576154a7250",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_async_host_to_device_transfer_manager.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.h?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -85,7 +85,7 @@ class TfrtGpuAsyncHostToDeviceTransferManager final\n   PjRtDevice* device() const override { return device_; }\n \n   std::unique_ptr<PjRtBuffer> RetrieveBuffer(int buffer_index) override {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     DCHECK_LT(buffer_index, buffers_.size());\n     return std::move(buffers_[buffer_index]);\n   };"
        },
        {
            "sha": "ba7370a970857ad1c8277eeb4927f25bd4292aaf",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -110,7 +110,7 @@ absl::StatusOr<size_t> TfrtGpuBuffer::GetOnDeviceSizeInBytes() const {\n \n TrackedGpuDeviceBuffer* TfrtGpuBuffer::AcquireUsage(\n     tsl::AsyncValueRef<GpuEvent> usage_event) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (!tracked_device_buffer_) {\n     return nullptr;\n   }\n@@ -169,7 +169,7 @@ absl::StatusOr<Shape> TfrtGpuBuffer::logical_on_device_shape() {\n \n PjRtFuture<> TfrtGpuBuffer::GetReadyFuture() {\n   VLOG(4) << \"TfrtGpuBuffer::GetReadyFuture\";\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (!tracked_device_buffer_) {\n     return PjRtFuture<>(InvalidArgument(\n         \"GetReadyFuture() called on deleted or donated buffer\"));\n@@ -259,7 +259,7 @@ bool TfrtGpuBuffer::IsOnCpu() const {\n }\n \n const tsl::AsyncValueRef<GpuDeviceMemory>& TfrtGpuBuffer::GetBufferPtr() const {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   return tracked_device_buffer_->buffer();\n }\n \n@@ -283,7 +283,7 @@ TfrtGpuBuffer::AcquireExternalReference() {\n     tsl::AsyncValueRef<GpuDeviceMemory> data_;\n   };\n \n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (tracked_device_buffer_ == nullptr) {\n     return InvalidArgument(\"Buffer has been deleted or donated.\");\n   }\n@@ -390,7 +390,7 @@ PjRtFuture<> TfrtGpuBuffer::ToLiteralHelper(\n             options.permutation = permutation;\n             options.input_layout = TransposePlan::Striding{byte_strides};\n             {\n-              absl::MutexLock lock(&client->transpose_mu_);\n+              absl::MutexLock lock(client->transpose_mu_);\n               absl::StatusOr<std::shared_ptr<TransposePlan>> t =\n                   client->transpose_cache_.GetOrCreate(options);\n               if (!t.ok()) {\n@@ -681,7 +681,7 @@ void TfrtGpuBuffer::Delete() {\n   std::unique_ptr<TrackedGpuDeviceBuffer> device_buffer;\n   tsl::AsyncValueRef<GpuEvent> external_references_dropped_event;\n   {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     device_buffer = ReleaseBufferLocked();\n     if (device_buffer == nullptr) {\n       return;\n@@ -724,7 +724,7 @@ void TfrtGpuBuffer::Delete() {\n }\n \n bool TfrtGpuBuffer::IsDeleted() const {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   return tracked_device_buffer_ == nullptr;\n }\n \n@@ -852,7 +852,7 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> TfrtGpuBuffer::CopyToMemorySpace(\n }\n \n void TfrtGpuBuffer::DropExternalReference() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   CHECK_GT(external_reference_counter_, 0);\n   --external_reference_counter_;\n   if (external_reference_counter_ == 0) {\n@@ -866,7 +866,7 @@ absl::StatusOr<std::unique_ptr<TrackedGpuDeviceBuffer>> TfrtGpuBuffer::Release(\n   tsl::BlockUntilReady(donation_event);\n   std::unique_ptr<TrackedGpuDeviceBuffer> device_buffer;\n   {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     device_buffer = ReleaseBufferLocked();\n   }\n   if (device_buffer == nullptr) {\n@@ -906,7 +906,7 @@ std::unique_ptr<TrackedGpuDeviceBuffer> TfrtGpuBuffer::ReleaseBufferLocked() {\n \n absl::StatusOr<TfrtGpuBuffer::DonationTransaction>\n TfrtGpuBuffer::AcquireDonation() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n \n   if (tracked_device_buffer_ == nullptr) {\n     return InvalidArgument(\"Donation requested for invalid buffer\");"
        },
        {
            "sha": "8ad6060370949f30f843e501b05d9f6241d489bf",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.h?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -171,7 +171,7 @@ class TfrtGpuBuffer final : public PjRtBuffer {\n       ABSL_LOCKS_EXCLUDED(mu_);\n \n   tsl::AsyncValueRef<bool> GetDonationEvent() {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     return donation_event_;\n   }\n "
        },
        {
            "sha": "ba3e19e3e4e9b4178630cddb06d327c46685702a",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -813,7 +813,7 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> TfrtGpuClient::BufferFromHostBuffer(\n     options.dims = dims;\n     options.permutation = permutation;\n     options.input_layout = TransposePlan::Striding{*byte_strides};\n-    absl::MutexLock lock(&transpose_mu_);\n+    absl::MutexLock lock(transpose_mu_);\n     TF_ASSIGN_OR_RETURN(transpose, transpose_cache_.GetOrCreate(options));\n   }\n \n@@ -1070,7 +1070,7 @@ absl::Status TfrtGpuClient::DmaMap(void* data, size_t buffer_size) {\n     return absl::InternalError(absl::StrFormat(\n         \"Failed to register host memory at address: %ps\", data));\n   }\n-  absl::MutexLock lock(&dma_maps_mutex_);\n+  absl::MutexLock lock(dma_maps_mutex_);\n   dma_maps_.insert({data, buffer_size});\n   return absl::OkStatus();\n }\n@@ -1086,13 +1086,13 @@ absl::Status TfrtGpuClient::DmaUnmap(void* data) {\n     return absl::InternalError(absl::StrFormat(\n         \"Failed to unregister host memory at address: %ps\", data));\n   }\n-  absl::MutexLock lock(&dma_maps_mutex_);\n+  absl::MutexLock lock(dma_maps_mutex_);\n   dma_maps_.erase(data);\n   return absl::OkStatus();\n }\n \n bool TfrtGpuClient::IsDmaMapped(const void* data_start, int64_t transfer_size) {\n-  absl::MutexLock lock(&dma_maps_mutex_);\n+  absl::MutexLock lock(dma_maps_mutex_);\n   if (dma_maps_.empty()) {\n     return false;\n   }"
        },
        {
            "sha": "b6114a3afa2cbb41955fe38331a416452bedd400",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -523,7 +523,7 @@ TEST(TfrtGpuClientTest, DonateWithControlDependency) {\n       ShapeUtil::DeviceShapeToHostShape(blocked_buffer->on_device_shape()));\n   bool got_literal = false;\n   blocked_buffer->ToLiteral(result_literal.get()).OnReady([&](absl::Status s) {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     TF_ASSERT_OK(s);\n     got_literal = true;\n   });\n@@ -534,7 +534,7 @@ TEST(TfrtGpuClientTest, DonateWithControlDependency) {\n   EXPECT_TRUE(future.IsReady());\n \n   {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     mu.Await(absl::Condition(&got_literal));\n   }\n \n@@ -706,7 +706,7 @@ TEST(TfrtGpuClientTest, ToLiteralAsync) {\n   // Literal is not ready.\n   buffer->LazyToLiteral([f = std::move(literal_future)]() { return f; })\n       .OnReady([&](absl::Status s) {\n-        absl::MutexLock l(&mu);\n+        absl::MutexLock l(mu);\n         TF_ASSERT_OK(s);\n         got_literal = true;\n       });\n@@ -717,7 +717,7 @@ TEST(TfrtGpuClientTest, ToLiteralAsync) {\n   literal_promise.Set(literal.get());\n \n   {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     mu.Await(absl::Condition(&got_literal));\n   }\n \n@@ -865,7 +865,7 @@ TEST(TfrtGpuClientTest, ToLiteralAsyncBeforeBufferReady) {\n   bool got_literal = false;\n \n   buffer->ToLiteral(literal.get()).OnReady([&](absl::Status s) {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     TF_ASSERT_OK(s);\n     got_literal = true;\n   });\n@@ -878,7 +878,7 @@ TEST(TfrtGpuClientTest, ToLiteralAsyncBeforeBufferReady) {\n   buffer.reset();\n \n   {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     mu.Await(absl::Condition(&got_literal));\n   }\n \n@@ -928,12 +928,12 @@ TEST(TfrtGpuClientTest, FromHostAsync) {\n     literals.push_back(std::make_shared<Literal>(\n         ShapeUtil::DeviceShapeToHostShape(buffer->on_device_shape())));\n     buffer->ToLiteral(literals.back().get()).OnReady([&](absl::Status s) {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       TF_ASSERT_OK(s);\n       ++got_literal_count;\n     });\n     buffer->GetReadyFuture().OnReady([&](absl::Status s) {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       TF_ASSERT_OK(s);\n       ++got_callback_count;\n     });\n@@ -945,7 +945,7 @@ TEST(TfrtGpuClientTest, FromHostAsync) {\n       return got_literal_count == src_literals.size() &&\n              got_callback_count == src_literals.size();\n     };\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     mu.Await(absl::Condition(&done));\n   }\n \n@@ -1107,7 +1107,7 @@ TEST(TfrtGpuClientTest, CreateMixOfErrorBuffers) {\n       TF_ASSERT_OK(transfer_manager->TransferLiteralToBuffer(i, src_literals[i],\n                                                              [&]() {}));\n       buffer->GetReadyFuture().OnReady([&](absl::Status s) {\n-        absl::MutexLock l(&mu);\n+        absl::MutexLock l(mu);\n         TF_ASSERT_OK(s);\n         ++got_callback_count;\n       });\n@@ -1116,7 +1116,7 @@ TEST(TfrtGpuClientTest, CreateMixOfErrorBuffers) {\n       transfer_manager->SetBufferError(i, error);\n       buffer->GetReadyFuture().OnReady(\n           [error, &mu, &got_callback_count](absl::Status s) {\n-            absl::MutexLock l(&mu);\n+            absl::MutexLock l(mu);\n             EXPECT_THAT(s.message(), HasSubstr(error.message()));\n             ++got_callback_count;\n           });\n@@ -1126,7 +1126,7 @@ TEST(TfrtGpuClientTest, CreateMixOfErrorBuffers) {\n \n   {\n     auto done = [&]() { return got_callback_count == src_literals.size(); };\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     QCHECK(mu.AwaitWithTimeout(absl::Condition(&done), absl::Seconds(60)));\n   }\n }"
        },
        {
            "sha": "854537b3d3b0354a419851cd4aa13ec814c1b5e2",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_device.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -160,7 +160,7 @@ absl::Status TfrtGpuDevice::TransferFromOutfeed(\n }\n \n int TfrtGpuDevice::GetNewPrngSeed() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   int x = 0;\n   do {\n     x = prng_seed_distribution_(prng_seed_generator_);\n@@ -245,7 +245,7 @@ absl::StatusOr<tsl::AllocatorStats> TfrtGpuDevice::GetAllocatorStats() const {\n \n tsl::AsyncValueRef<GpuEvent> TfrtGpuDevice::SetLastCollectiveLaunchEvent(\n     tsl::AsyncValueRef<GpuEvent> event) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   VLOG(3) << \"SetLastCollectiveLaunchEvent: IsAvailable: \"\n           << event.IsAvailable() << \"; pointer: \" << event.GetAsyncValue()\n           << \"Old Event: IsAvailable: \""
        },
        {
            "sha": "403257350d3375e7a9902071fab139c9e3169897",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_executable.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -114,7 +114,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n                                           {{\"channel_id\", channel_id_}});\n     });\n \n-    absl::ReleasableMutexLock lock(&mu_);\n+    absl::ReleasableMutexLock lock(mu_);\n \n     VLOG(4) << \"Add chunk to a H2D channel #\" << channel_id_ << \": \"\n             << \"size=\" << chunk.size() << \", \"\n@@ -978,7 +978,7 @@ TfrtGpuExecutable::Execute(\n               }\n             }\n \n-            absl::MutexLock lock(&mu);\n+            absl::MutexLock lock(mu);\n             --running;\n             if (!statusor.ok()) {\n               if (failed == 0) {\n@@ -1000,7 +1000,7 @@ TfrtGpuExecutable::Execute(\n         mu.AssertHeld();\n         return running == 0;\n       };\n-      absl::MutexLock lock(&mu);\n+      absl::MutexLock lock(mu);\n       mu.Await(absl::Condition(&done_running));\n     }\n "
        },
        {
            "sha": "5143661fae172c4c3b7277f7f3542700bf89bade",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/189979cd3f646f374d2bcbe1f37fbb08b793ca5b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc?ref=189979cd3f646f374d2bcbe1f37fbb08b793ca5b",
            "patch": "@@ -319,7 +319,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n                                           {{\"channel_id\", channel_id_}});\n     });\n \n-    absl::ReleasableMutexLock lock(&mu_);\n+    absl::ReleasableMutexLock lock(mu_);\n \n     VLOG(4) << \"Add chunk to a H2D channel #\" << channel_id_ << \": \"\n             << \"size=\" << chunk.size() << \", \""
        }
    ],
    "stats": {
        "total": 80,
        "additions": 40,
        "deletions": 40
    }
}