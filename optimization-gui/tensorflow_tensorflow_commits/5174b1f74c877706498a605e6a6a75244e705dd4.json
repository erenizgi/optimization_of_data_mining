{
    "author": "akuegel",
    "message": "[XLA:GPU] Add Sort Fusion kind and corresponding FusionInterface.\n\nPiperOrigin-RevId: 846138786",
    "sha": "5174b1f74c877706498a605e6a6a75244e705dd4",
    "files": [
        {
            "sha": "702d1850efef92e48448343e27d7ddef0161a727",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -297,6 +297,7 @@ cc_library(\n         \":cudnn\",\n         \":custom\",\n         \":fusion_emitter\",\n+        \":sort\",\n         \"//xla:shape_util\",\n         \"//xla/backends/gpu/codegen/emitters:concatenate\",\n         \"//xla/backends/gpu/codegen/emitters:in_place_dynamic_update_slice\",\n@@ -316,3 +317,27 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n     ],\n )\n+\n+cc_library(\n+    name = \"sort\",\n+    srcs = [\"sort.cc\"],\n+    hdrs = [\"sort.h\"],\n+    deps = [\n+        \":fusion_emitter\",\n+        \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/gpu/codegen/llvm:llvm_emitter\",\n+        \"//xla/backends/gpu/runtime:copy_thunk\",\n+        \"//xla/backends/gpu/runtime:shaped_slice\",\n+        \"//xla/backends/gpu/runtime:thunk\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emitter_context\",\n+        \"//xla/tsl/platform:status_macros\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)"
        },
        {
            "sha": "ebce9ff67d9355c0725d586c39043f71ff356881",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/emitters/scatter.h\"\n #include \"xla/backends/gpu/codegen/emitters/transpose.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n+#include \"xla/backends/gpu/codegen/sort.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion.h\"\n #include \"xla/codegen/ir_emission_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -121,6 +122,9 @@ std::unique_ptr<FusionInterface> GetFusionEmitter(\n     case HloFusionAnalysis::EmitterFusionKind::kConcatenate: {\n       return std::make_unique<ConcatenateFusion>(analysis);\n     }\n+    case HloFusionAnalysis::EmitterFusionKind::kSort: {\n+      return std::make_unique<SortFusion>();\n+    }\n     case HloFusionAnalysis::EmitterFusionKind::kTriton:\n       return std::make_unique<TritonFusion>(analysis);\n     case HloFusionAnalysis::EmitterFusionKind::kCuDnn:"
        },
        {
            "sha": "00d63c372342efcd017232d54a74b2b51c58bc68",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2FBUILD?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -54,7 +54,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "3837599da053fb33723597749baf04d38e23172d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/llvm_emitter.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 32,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -89,10 +89,10 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/fingerprint.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -363,7 +363,7 @@ absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n                                    llvm::Value* output) {\n   TF_RET_CHECK(computation.num_parameters() > 0);\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       llvm::Function * emitted_function,\n       IrEmitter(&ir_emitter_context, llvm_module, /*is_nested=*/true)\n           .CodegenNestedComputation(computation));\n@@ -461,7 +461,7 @@ absl::StatusOr<llvm::Function*> IrEmitter::CodegenNestedComputation(\n     return function;\n   }\n \n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       EmitConstants(module_, ir_emitter_context_, nested_computation));\n   std::vector<const HloInstruction*> io_hlos;\n   std::vector<llvm::Type*> argument_types;\n@@ -522,7 +522,7 @@ absl::StatusOr<llvm::Function*> IrEmitter::CodegenNestedComputation(\n   }\n   bindings_.EmitBasePointersForHlos(io_hlos, non_io_hlos);\n \n-  TF_RETURN_IF_ERROR(nested_computation.root_instruction()->Accept(this));\n+  RETURN_IF_ERROR(nested_computation.root_instruction()->Accept(this));\n   b_.SetInsertPoint(ret_instr);\n \n   // Function epilogue: copy the output value back.\n@@ -576,7 +576,7 @@ absl::Status IrEmitter::EmitTargetElementLoop(\n   if (hlo.shape().IsTuple()) {\n     std::vector<llvm_ir::IrArray> target_arrays =\n         ConstructIrArrayForOutputs(hlo);\n-    TF_RETURN_IF_ERROR(\n+    RETURN_IF_ERROR(\n         llvm_ir::LoopEmitter(element_generator, target_arrays, &b_).EmitLoop());\n     llvm_ir::EmitTuple(GetIrArray(hlo, hlo), target_arrays, &b_);\n     return absl::OkStatus();\n@@ -599,13 +599,13 @@ absl::StatusOr<KernelThunkInfo> BuildKernelThunkForNonFusionOp(\n     IrEmitter& ir_emitter, const LaunchDimensions& launch_dimensions) {\n   std::string suggested_kernel_name(hlo->name());\n \n-  TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n-                      emitters::KernelArguments::Create(\n-                          buffer_assignment, GetDefaultBufferAlignment(), hlo));\n+  ASSIGN_OR_RETURN(auto kernel_arguments,\n+                   emitters::KernelArguments::Create(\n+                       buffer_assignment, GetDefaultBufferAlignment(), hlo));\n \n   VLOG(3) << \"Generating (without reuse check): \" << suggested_kernel_name;\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       llvm::Function * kernel,\n       BuildKernelPrototype(llvm_module, gpu_device_info, suggested_kernel_name,\n                            sanitized_kernel_name, kernel_arguments,\n@@ -867,22 +867,25 @@ absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n     LaunchDimensions launch_dimensions = xor_masks.size() > 1\n                                              ? tiled_launch_dimensions\n                                              : standard_launch_dimensions;\n-    TF_ASSIGN_OR_RETURN(\n-        KernelThunkInfo kernel_thunk_info,\n-        BuildKernelThunkForNonFusionOp(\n-            llvm_module, sort, ir_emitter_context->buffer_assignment(),\n-            ir_emitter_context->GetNextThunkId(),\n-            ir_emitter_context->gpu_device_info(),\n-            ir_emitter_context->GetSanitizedUniqueName(op_name), ir_emitter,\n-            launch_dimensions));\n+    bool is_fusion = sort->parent()->IsFusionComputation();\n+    const HloInstruction* hlo_with_buffers =\n+        is_fusion ? sort->parent()->FusionInstruction() : sort;\n+    ASSIGN_OR_RETURN(KernelThunkInfo kernel_thunk_info,\n+                     BuildKernelThunkForNonFusionOp(\n+                         llvm_module, hlo_with_buffers,\n+                         ir_emitter_context->buffer_assignment(),\n+                         ir_emitter_context->GetNextThunkId(),\n+                         ir_emitter_context->gpu_device_info(),\n+                         ir_emitter_context->GetSanitizedUniqueName(op_name),\n+                         ir_emitter, launch_dimensions));\n     thunks.push_back(std::move(kernel_thunk_info.thunk));\n \n     // The first `operand_count()` elements of `ir_arrays` are the input\n     // operands and the rest are the output arrays. Inputs are aliases with\n     // outputs, so we need to pass only the outputs to the in-place sort kernel.\n     auto output_arrays_span =\n         absl::Span<const llvm_ir::IrArray>(kernel_thunk_info.ir_arrays)\n-            .subspan(sort->operand_count());\n+            .subspan(hlo_with_buffers->operand_count());\n \n     auto* comparator = sort->called_computations().front();\n     auto* builder = ir_emitter.builder();\n@@ -911,17 +914,17 @@ absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n       }\n       if (xor_mask >= tile_size) {\n         if (!xor_masks.empty()) {\n-          TF_RETURN_IF_ERROR(emit_kernel(xor_masks));\n+          RETURN_IF_ERROR(emit_kernel(xor_masks));\n           xor_masks.clear();\n         }\n-        TF_RETURN_IF_ERROR(emit_kernel({xor_mask}));\n+        RETURN_IF_ERROR(emit_kernel({xor_mask}));\n       } else {\n         xor_masks.push_back(xor_mask);\n       }\n     }\n   }\n   if (!xor_masks.empty()) {\n-    TF_RETURN_IF_ERROR(emit_kernel(xor_masks));\n+    RETURN_IF_ERROR(emit_kernel(xor_masks));\n   }\n   return thunks;\n }\n@@ -941,7 +944,7 @@ absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n   LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n       input_shape, ir_emitter_context->gpu_device_info(), {kUnrollFactor});\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       KernelThunkInfo kernel_thunk_info,\n       BuildKernelThunkForNonFusionOp(\n           llvm_module, hlo, ir_emitter_context->buffer_assignment(),\n@@ -1067,10 +1070,10 @@ absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n   };\n \n   const Shape& data_shape = hlo->shape().tuple_shapes(0);\n-  TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n-                                         launch_dimensions,\n-                                         ir_emitter.builder(), {kUnrollFactor})\n-                         .EmitLoop(ir_name, index_ty));\n+  RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n+                                      launch_dimensions, ir_emitter.builder(),\n+                                      {kUnrollFactor})\n+                      .EmitLoop(ir_name, index_ty));\n   return thunk_sequence;\n }\n \n@@ -1089,7 +1092,7 @@ absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n       input_shape, ir_emitter_context->gpu_device_info(), {kUnrollFactor});\n   llvm::Type* index_ty = GetIndexTypeForKernel(\n       hlo, launch_dimensions.launch_bound(), ir_emitter.builder());\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       KernelThunkInfo kernel_thunk_info,\n       BuildKernelThunkForNonFusionOp(\n           llvm_module, hlo, ir_emitter_context->buffer_assignment(),\n@@ -1205,10 +1208,10 @@ absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n     return absl::OkStatus();\n   };\n \n-  TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n-                                         launch_dimensions,\n-                                         ir_emitter.builder(), {kUnrollFactor})\n-                         .EmitLoop(ir_name, index_ty));\n+  RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n+                                      launch_dimensions, ir_emitter.builder(),\n+                                      {kUnrollFactor})\n+                      .EmitLoop(ir_name, index_ty));\n   return thunk_sequence;\n }\n \n@@ -1222,7 +1225,7 @@ absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateStateLLVMIR(\n   auto& b = *ir_emitter.builder();\n   // Emit a kernel to increment the global state for Philox RNG\n   // algorithm.\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       KernelThunkInfo kernel_thunk_info,\n       BuildKernelThunkForNonFusionOp(\n           llvm_module, hlo, ir_emitter_context->buffer_assignment(),"
        },
        {
            "sha": "35c0049f09486cd3a9621279f5af915ebc684e5c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/sort.cc",
            "status": "added",
            "additions": 114,
            "deletions": 0,
            "changes": 114,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fsort.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fsort.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fsort.cc?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -0,0 +1,114 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/backends/gpu/codegen/sort.h\"\n+\n+#include <cstdint>\n+#include <iterator>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n+#include \"xla/backends/gpu/codegen/llvm/llvm_emitter.h\"\n+#include \"xla/backends/gpu/runtime/copy_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/layout.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/ir_emitter_context.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+absl::StatusOr<FusionEmissionResult> SortFusion::Emit(\n+    IrEmitterContext& ir_emitter_context,\n+    const HloFusionInstruction& fusion) const {\n+  std::vector<BufferAllocation::Slice> src_buffers;\n+  std::vector<BufferAllocation::Slice> dst_buffers;\n+  std::vector<Shape> src_shapes;\n+  src_buffers.reserve(fusion.operand_count());\n+  dst_buffers.reserve(fusion.operand_count());\n+  src_shapes.reserve(fusion.operand_count());\n+  const HloSortInstruction* sort =\n+      Cast<HloSortInstruction>(fusion.fused_expression_root());\n+  Shape keys_shape = sort->operand(0)->shape();\n+  for (int64_t i = 0; i < sort->operand_count(); ++i) {\n+    // We assume that the layout of all involved operands and\n+    // outputs is the same.\n+    TF_RET_CHECK(LayoutUtil::LayoutsInShapesEqual(\n+        keys_shape, sort->operand(i)->shape(),\n+        Layout::Equal().IgnoreMemorySpace().IgnoreElementSize()));\n+    ShapeIndex shape_index =\n+        sort->operand_count() > 1 ? ShapeIndex({i}) : ShapeIndex({});\n+    TF_RET_CHECK(LayoutUtil::LayoutsInShapesEqual(\n+        keys_shape, ShapeUtil::GetSubshape(sort->shape(), shape_index),\n+        Layout::Equal().IgnoreMemorySpace().IgnoreElementSize()));\n+    // We expect only parameters or iotas as operand of sort.\n+    if (HloPredicateIsOp<HloOpcode::kParameter>(sort->operand(i))) {\n+      const HloInstruction* src_instr =\n+          fusion.operand(sort->operand(i)->parameter_number());\n+      ASSIGN_OR_RETURN(\n+          BufferAllocation::Slice slice,\n+          ir_emitter_context.buffer_assignment().GetUniqueSlice(src_instr, {}));\n+      src_buffers.push_back(slice);\n+      src_shapes.push_back(sort->operand(i)->shape());\n+      ASSIGN_OR_RETURN(slice,\n+                       ir_emitter_context.buffer_assignment().GetUniqueSlice(\n+                           &fusion, shape_index));\n+      dst_buffers.push_back(slice);\n+    } else {\n+      TF_RET_CHECK(HloPredicateIsOp<HloOpcode::kIota>(sort->operand(i)));\n+    }\n+  }\n+\n+  FusionEmissionResult result;\n+  for (int i = 0; i < src_buffers.size(); ++i) {\n+    if (src_buffers[i] != dst_buffers[i]) {\n+      result.thunks.emplace_back(std::make_unique<DeviceToDeviceCopyThunk>(\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              &fusion, ir_emitter_context.GetNextThunkId()),\n+          /*source_buffer=*/ShapedSlice{src_buffers[i], src_shapes[i]},\n+          /*destination_buffer=*/ShapedSlice{dst_buffers[i], src_shapes[i]},\n+          /*mem_size=*/src_buffers[i].size()));\n+    }\n+  }\n+  std::string op_name(sort->name());\n+  result.module = ir_emitter_context.CreateLLVMModule(op_name);\n+  ASSIGN_OR_RETURN(\n+      ThunkSequence sort_thunks,\n+      EmitBitonicSortLLVMIR(sort, result.module.get(), &ir_emitter_context));\n+  result.thunks.insert(result.thunks.end(),\n+                       std::make_move_iterator(sort_thunks.begin()),\n+                       std::make_move_iterator(sort_thunks.end()));\n+  return result;\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "3125278f529c9e5c86d8b698f7a5b5a42f6083ef",
            "filename": "third_party/xla/xla/backends/gpu/codegen/sort.h",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fsort.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fsort.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fsort.h?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -0,0 +1,40 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef XLA_BACKENDS_GPU_CODEGEN_SORT_H_\n+#define XLA_BACKENDS_GPU_CODEGEN_SORT_H_\n+\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/gpu/ir_emitter_context.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// A fusion consisting of a sort op with operands which are either parameters or\n+// iotas.\n+class SortFusion : public FusionInterface {\n+ public:\n+  SortFusion() = default;\n+\n+  absl::StatusOr<FusionEmissionResult> Emit(\n+      IrEmitterContext& ir_emitter_context,\n+      const HloFusionInstruction& fusion) const final;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_CODEGEN_SORT_H_"
        },
        {
            "sha": "5f81a4124013774bda4e1f333957491d1d96745e",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -2788,7 +2788,6 @@ xla_cc_test(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/tests:xla_internal_test_main\",\n-        \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_googletest//:gtest\",\n     ],"
        },
        {
            "sha": "feedcbdf5a4e6b095f72c02db1d72ab78c872560",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -182,6 +182,10 @@ HloFusionAnalysis::EmitterFusionKind GetEmitterFusionKind(\n     return HloFusionAnalysis::EmitterFusionKind::kScatter;\n   }\n \n+  if (fusion_roots[0].opcode() == HloOpcode::kSort) {\n+    return HloFusionAnalysis::EmitterFusionKind::kSort;\n+  }\n+\n   if (UseConcatenateFusion(fusion_roots, fusion_heroes)) {\n     return HloFusionAnalysis::EmitterFusionKind::kConcatenate;\n   }"
        },
        {
            "sha": "78dba963b95ac1f9db199ab86f0face69a0f4b99",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -50,6 +50,7 @@ class HloFusionAnalysis {\n     kScatter,\n     kCuDnn,\n     kDynamicMemcpy,\n+    kSort,\n   };\n \n   // Precomputed information about inputs (arguments) and outputs (roots) of the"
        },
        {
            "sha": "7a02fcddf8ecf6751e2114b7a8922d8868992b42",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis_test.cc",
            "status": "modified",
            "additions": 49,
            "deletions": 17,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis_test.cc?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n \n namespace xla::gpu {\n@@ -34,7 +33,7 @@ using ::tsl::proto_testing::EqualsProto;\n class HloFusionAnalysisTest : public HloHardwareIndependentTestBase {};\n \n TEST_F(HloFusionAnalysisTest, DoesNotPeekOutsideBoundary) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -64,7 +63,7 @@ TEST_F(HloFusionAnalysisTest, DoesNotPeekOutsideBoundary) {\n }\n \n TEST_F(HloFusionAnalysisTest, ReductionWithMultipleUsers) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -100,7 +99,7 @@ TEST_F(HloFusionAnalysisTest, ReductionWithMultipleUsers) {\n }\n \n TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusion) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -133,7 +132,7 @@ TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusion) {\n }\n \n TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusionPartiallyFused) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -166,7 +165,7 @@ TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusionPartiallyFused) {\n }\n \n TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusionPartiallyFusedInConsumer) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -197,7 +196,7 @@ TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusionPartiallyFusedInConsumer) {\n }\n \n TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusionPartiallyFusedInBoth) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -234,7 +233,7 @@ TEST_F(HloFusionAnalysisTest, ReductionEpilogueFusionPartiallyFusedInBoth) {\n }\n \n TEST_F(HloFusionAnalysisTest, ReduceMultiOutputFusionWithTransposeBitcast) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -266,7 +265,7 @@ TEST_F(HloFusionAnalysisTest, ReduceMultiOutputFusionWithTransposeBitcast) {\n }\n \n TEST_F(HloFusionAnalysisTest, InvalidReduceMultiOutputFusion) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -303,7 +302,7 @@ TEST_F(HloFusionAnalysisTest, InvalidDevice) {\n   // Verifies that an analysis can be created even with an invalid/empty device\n   // info, and that the emitter type is determined correctly.\n   // Don't rely on this behavior.\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     add {\n@@ -320,7 +319,7 @@ TEST_F(HloFusionAnalysisTest, InvalidDevice) {\n     })\"));\n \n   stream_executor::GpuDeviceInfoProto device_info_proto;\n-  TF_ASSERT_OK_AND_ASSIGN(\n+  ASSERT_OK_AND_ASSIGN(\n       auto device_info,\n       stream_executor::DeviceDescription::FromProto(device_info_proto));\n   device_info.set_threads_per_warp(32);\n@@ -333,7 +332,7 @@ TEST_F(HloFusionAnalysisTest, InvalidDevice) {\n }\n \n TEST_F(HloFusionAnalysisTest, ConcatFusion) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n \n     fused_computation {\n@@ -360,8 +359,41 @@ TEST_F(HloFusionAnalysisTest, ConcatFusion) {\n             HloFusionAnalysis::EmitterFusionKind::kConcatenate);\n }\n \n+TEST_F(HloFusionAnalysisTest, SortFusion) {\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+    HloModule test_module\n+\n+    less_than {\n+      lhs.0 = f32[] parameter(0)\n+      rhs.0 = f32[] parameter(1)\n+      lhs.1 = s32[] parameter(2)\n+      rhs.1 = s32[] parameter(3)\n+      ROOT lt = pred[] compare(lhs.0, rhs.0), direction=LT\n+    }\n+\n+    fused_computation {\n+      p0 = f32[256] parameter(0)\n+      iota = s32[256] iota(), iota_dimension=0\n+      ROOT sort = (f32[256], s32[256]) sort(p0, iota), dimensions={0}, to_apply=less_than, is_stable=false\n+    }\n+\n+    ENTRY main {\n+      p = f32[256] parameter(0)\n+      ROOT fusion = (f32[256], s32[256]) fusion(p), kind=kInput, calls=fused_computation\n+    })\"));\n+\n+  auto device_info = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+\n+  auto* root = module->entry_computation()->root_instruction();\n+  auto analysis = HloFusionAnalysis::Create(\n+      FusionBackendConfig::default_instance(),\n+      HloFusionAdaptor::ForInstruction(root), &device_info);\n+  EXPECT_EQ(analysis.emitter_fusion_kind(),\n+            HloFusionAnalysis::EmitterFusionKind::kSort);\n+}\n+\n TEST_F(HloFusionAnalysisTest, ExtractValidGpuBackendConfig) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule module\n \n     fused_computation.1 {\n@@ -399,7 +431,7 @@ TEST_F(HloFusionAnalysisTest, ExtractValidGpuBackendConfig) {\n \n TEST_F(HloFusionAnalysisTest,\n        InvalidGpuBackendConfig_SingleInstruction_Ignored) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule module\n \n     ENTRY entry {\n@@ -418,7 +450,7 @@ TEST_F(HloFusionAnalysisTest,\n \n TEST_F(HloFusionAnalysisTest,\n        InvalidGpuBackendConfig_ProducerConsumer_Ignored) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule module\n \n     fused_computation {\n@@ -444,7 +476,7 @@ TEST_F(HloFusionAnalysisTest,\n }\n \n TEST_F(HloFusionAnalysisTest, ConcatenateFusion) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule module\n \n     fusion {\n@@ -475,7 +507,7 @@ TEST_F(HloFusionAnalysisTest, ConcatenateFusion) {\n }\n \n TEST_F(HloFusionAnalysisTest, ConcatenateFusionFallbackToLoop) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     HloModule module\n \n     fusion {"
        },
        {
            "sha": "e8b6fad5b6d45c015e99339432b719beb7d3a48c",
            "filename": "third_party/xla/xla/service/gpu/tests/sorting_test.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsorting_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsorting_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsorting_test.cc?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -146,6 +146,36 @@ TEST_F(SortingTest, PackedElementType) {\n   EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{1e-5, 1e-5}));\n }\n \n+TEST_F(SortingTest, SortFusionWithIotaOperand) {\n+  const char* hlo_text = R\"(\n+    HloModule module\n+\n+    sorting_computation {\n+      %lhs_key = s32[] parameter(0)\n+      %rhs_key = s32[] parameter(1)\n+      %lhs_index = s32[] parameter(2)\n+      %rhs_index = s32[] parameter(3)\n+      %lt_key = pred[] compare(%lhs_key, %rhs_key), direction=LT\n+      %gt_key = pred[] compare(%rhs_key, %lhs_key), direction=LT\n+      %eq_key = pred[] compare(%lt_key, %gt_key), direction=EQ\n+      %lt_index = pred[] compare(%lhs_index, %rhs_index), direction=LT\n+      ROOT res = pred[] select(%eq_key, %lt_index, %lt_key)\n+    }\n+\n+    sort_fusion {\n+      p0 = s32[16384]{0} parameter(0)\n+      iota = s32[16384]{0} iota(), iota_dimension=0\n+      ROOT sort = (s32[16384]{0}, s32[16384]{0}) sort(p0, iota), dimensions={0}, is_stable=true, to_apply=sorting_computation\n+    }\n+\n+    ENTRY main {\n+      p = s32[16384]{0} parameter(0)\n+      ROOT fusion = (s32[16384]{0}, s32[16384]{0}) fusion(p), kind=kInput, calls=sort_fusion\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(hlo_text, ErrorSpec{1e-5, 1e-5}));\n+}\n+\n // Test that verifies the IgnoreMemorySpace option works correctly\n TEST_F(SortingTest, LayoutsInShapesEqualWithIgnoreMemorySpace) {\n   const char* hlo_text = R\"("
        },
        {
            "sha": "3630635a713d2e1ec062e246ab88b266517d310e",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5174b1f74c877706498a605e6a6a75244e705dd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc?ref=5174b1f74c877706498a605e6a6a75244e705dd4",
            "patch": "@@ -1300,6 +1300,7 @@ HloInstruction::FusionKind PriorityFusion::ChooseKind(\n     case HloFusionAnalysis::EmitterFusionKind::kReduction:\n     case HloFusionAnalysis::EmitterFusionKind::kTranspose:\n     case HloFusionAnalysis::EmitterFusionKind::kScatter:\n+    case HloFusionAnalysis::EmitterFusionKind::kSort:\n       return HloInstruction::FusionKind::kInput;\n   }\n }"
        }
    ],
    "stats": {
        "total": 355,
        "additions": 304,
        "deletions": 51
    }
}