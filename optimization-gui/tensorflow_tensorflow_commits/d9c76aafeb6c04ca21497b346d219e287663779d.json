{
    "author": "felixwqp",
    "message": "Adjust the collective-permute cross host type to `MULTI_HOST_NON_WORLD_LEVEL` only.\n\nPiperOrigin-RevId: 826327580",
    "sha": "d9c76aafeb6c04ca21497b346d219e287663779d",
    "files": [
        {
            "sha": "f088b3fce0fa388007cb50df9f9cc383149972be",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d9c76aafeb6c04ca21497b346d219e287663779d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d9c76aafeb6c04ca21497b346d219e287663779d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD?ref=d9c76aafeb6c04ca21497b346d219e287663779d",
            "patch": "@@ -105,7 +105,9 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "54fc006c3ad9a15a811f63120814a72dc5007187",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 49,
            "changes": 120,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d9c76aafeb6c04ca21497b346d219e287663779d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d9c76aafeb6c04ca21497b346d219e287663779d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=d9c76aafeb6c04ca21497b346d219e287663779d",
            "patch": "@@ -18,11 +18,12 @@ limitations under the License.\n #include <algorithm>\n #include <cstddef>\n #include <cstdint>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -43,7 +44,24 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-struct CommunicationMetadata {\n+// Computes a map from source node ID to a set of target node IDs for a\n+// collective-permute instruction. A node ID is computed by dividing the device\n+// (replica) ID by the number of devices per host.\n+absl::flat_hash_map<int64_t, absl::flat_hash_set<int64_t>>\n+GetSourceToTargetsNodeMap(const HloCollectivePermuteInstruction& instr,\n+                          int num_devices_per_host) {\n+  absl::flat_hash_map<int64_t, absl::flat_hash_set<int64_t>>\n+      source_to_targets_node_map;\n+  for (const auto& [source, target] : instr.source_target_pairs()) {\n+    int64_t source_node = source / num_devices_per_host;\n+    int64_t target_node = target / num_devices_per_host;\n+    source_to_targets_node_map[source_node].insert(target_node);\n+  }\n+  return source_to_targets_node_map;\n+}\n+\n+struct CollectiveMetadata {\n+  // map for ops with `replica_groups`, e.g. all-gather.\n   absl::flat_hash_map<int64_t, size_t> node_to_participant_count;\n   int num_devices_per_host;\n   int64_t replica_count;\n@@ -66,49 +84,31 @@ bool SameParticipantCounts(const absl::flat_hash_map<int64_t, size_t>& lhs,\n   return lhs_counts == rhs_counts;\n }\n \n-absl::StatusOr<CommunicationMetadata> CommunicationContext(\n-    const HloChannelInstruction& instr, int num_devices_per_host) {\n+absl::StatusOr<CollectiveMetadata> CommunicationContext(\n+    const HloCollectiveInstruction& instr, int num_devices_per_host) {\n   absl::flat_hash_map<int64_t, size_t> node_to_participant_count;\n \n-  if (const HloCollectiveInstruction* collective =\n-          DynCast<HloCollectiveInstruction>(&instr)) {\n-    for (const ReplicaGroup& replica_group :\n-         collective->device_list().replica_groups()) {\n-      absl::flat_hash_map<int64_t, size_t> buffer;\n-      for (int64_t rank : replica_group.replica_ids()) {\n-        int64_t node_id = rank / num_devices_per_host;\n-        buffer[node_id]++;\n-      }\n-      if (!node_to_participant_count.empty() &&\n-          !SameParticipantCounts(buffer, node_to_participant_count)) {\n-        return absl::FailedPreconditionError(\n-            absl::StrCat(\"Non homogenous replica group: \",\n-                         collective->device_list().ToString()));\n-      }\n-      if (node_to_participant_count.empty()) {\n-        node_to_participant_count = buffer;\n-      }\n+  for (const ReplicaGroup& replica_group :\n+       instr.device_list().replica_groups()) {\n+    absl::flat_hash_map<int64_t, size_t> buffer;\n+    for (int64_t rank : replica_group.replica_ids()) {\n+      int64_t node_id = rank / num_devices_per_host;\n+      buffer[node_id]++;\n     }\n-  } else if (const HloCollectivePermuteInstruction* collective_permute =\n-                 DynCast<HloCollectivePermuteInstruction>(&instr)) {\n-    for (const auto& [source, target] :\n-         collective_permute->source_target_pairs()) {\n-      int64_t source_node = source / num_devices_per_host;\n-      int64_t target_node = target / num_devices_per_host;\n-      node_to_participant_count[source_node]++;\n-      node_to_participant_count[target_node]++;\n+    if (!node_to_participant_count.empty() &&\n+        !SameParticipantCounts(buffer, node_to_participant_count)) {\n+      return absl::FailedPreconditionError(absl::StrCat(\n+          \"Non homogenous replica group: \", instr.device_list().ToString()));\n+    }\n+    if (node_to_participant_count.empty()) {\n+      node_to_participant_count = buffer;\n     }\n-  } else {\n-    return absl::FailedPreconditionError(\n-        \"Cannot determine communication context for non-collective channel \"\n-        \"instruction\");\n   }\n-\n-  return CommunicationMetadata{node_to_participant_count, num_devices_per_host,\n-                               instr.GetModule()->config().replica_count()};\n+  return CollectiveMetadata{node_to_participant_count, num_devices_per_host,\n+                            instr.GetModule()->config().replica_count()};\n }\n \n-bool IsSingleHost(const CommunicationMetadata& pattern) {\n+bool IsSingleHost(const CollectiveMetadata& pattern) {\n   if (pattern.node_to_participant_count.size() == 1) {\n     return true;\n   }\n@@ -117,7 +117,7 @@ bool IsSingleHost(const CommunicationMetadata& pattern) {\n          pattern.replica_count <= pattern.num_devices_per_host;\n }\n \n-bool IsWorldLevelCommunication(const CommunicationMetadata& pattern) {\n+bool IsWorldLevelCommunication(const CollectiveMetadata& pattern) {\n   if (!IsSingleHost(pattern) && pattern.node_to_participant_count.empty()) {\n     return true;\n   }\n@@ -128,7 +128,7 @@ bool IsWorldLevelCommunication(const CommunicationMetadata& pattern) {\n       });\n }\n \n-bool IsNonWorldLevelCommunication(const CommunicationMetadata& pattern) {\n+bool IsNonWorldLevelCommunication(const CollectiveMetadata& pattern) {\n   return !IsSingleHost(pattern) && !IsWorldLevelCommunication(pattern);\n }\n \n@@ -149,16 +149,38 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n     return absl::FailedPreconditionError(\"Only CUDA is supported.\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(CommunicationMetadata comm,\n-                      CommunicationContext(instr, num_devices_per_host));\n-  if (IsSingleHost(comm)) {\n+  if (const auto* collective = DynCast<HloCollectiveInstruction>(&instr)) {\n+    TF_ASSIGN_OR_RETURN(\n+        CollectiveMetadata comm,\n+        CommunicationContext(*collective, num_devices_per_host));\n+    if (IsSingleHost(comm)) {\n+      return GPUCommunicationType::SINGLE_HOST;\n+    }\n+    if (IsWorldLevelCommunication(comm)) {\n+      return GPUCommunicationType::MULTI_HOST_WORLD_LEVEL;\n+    }\n+    if (IsNonWorldLevelCommunication(comm)) {\n+      return GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL;\n+    }\n+  } else if (const auto* collective_permute =\n+                 DynCast<HloCollectivePermuteInstruction>(&instr)) {\n+    const auto source_to_targets_node_map =\n+        GetSourceToTargetsNodeMap(*collective_permute, num_devices_per_host);\n+    for (const auto& [source_node, target_node_set] :\n+         source_to_targets_node_map) {\n+      if (target_node_set.size() > 1) {\n+        return GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL;\n+      }\n+      CHECK_EQ(target_node_set.size(), 1);\n+      if (source_node != *target_node_set.begin()) {\n+        return GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL;\n+      }\n+    }\n     return GPUCommunicationType::SINGLE_HOST;\n-  }\n-  if (IsWorldLevelCommunication(comm)) {\n-    return GPUCommunicationType::MULTI_HOST_WORLD_LEVEL;\n-  }\n-  if (IsNonWorldLevelCommunication(comm)) {\n-    return GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL;\n+  } else {\n+    return absl::FailedPreconditionError(\n+        \"Cannot determine communication type for non-collective channel \"\n+        \"instruction\");\n   }\n \n   return GPUCommunicationType::UNDEFINED;"
        },
        {
            "sha": "838cc5b2e64a5827718dec0c6244310b83bd2dbe",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 49,
            "deletions": 1,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d9c76aafeb6c04ca21497b346d219e287663779d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d9c76aafeb6c04ca21497b346d219e287663779d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc?ref=d9c76aafeb6c04ca21497b346d219e287663779d",
            "patch": "@@ -266,6 +266,26 @@ TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermute) {\n               IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n+TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermuteSinglePair) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=8\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={{0,7},{7,0}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloChannelInstruction* instr = Cast<HloChannelInstruction>(\n+      module->entry_computation()->root_instruction());\n+  EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+}\n+\n TEST_F(CommunicationTypeTest, DetectNonWorldLevelCollectivePermute) {\n   absl::string_view kHlo = R\"(\n     HloModule m, num_partitions=16\n@@ -304,7 +324,35 @@ TEST_F(CommunicationTypeTest, DetectWorldLevelCollectivePermute) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_WORLD_LEVEL));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL));\n+}\n+\n+TEST_F(CommunicationTypeTest, DetectsCrossHostCollectivePermuteMixed) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=16\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+       source_target_pairs={{0,7},\n+                            {0,8},\n+                            {1,9},\n+                            {2,10},\n+                            {3,11},\n+                            {4,12},\n+                            {5,13},\n+                            {6,14},\n+                            {7,15}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloChannelInstruction* instr = Cast<HloChannelInstruction>(\n+      module->entry_computation()->root_instruction());\n+  EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL));\n }\n \n }  // namespace"
        }
    ],
    "stats": {
        "total": 172,
        "additions": 122,
        "deletions": 50
    }
}