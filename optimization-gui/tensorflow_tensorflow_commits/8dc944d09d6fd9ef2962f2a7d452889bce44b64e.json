{
    "author": "yimeisun123",
    "message": "PR #30328: [XLA:CPU][oneDNN] Update on the criterias for rewriting Dot to oneDNN Matmul\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30328\n\nUpdate on Dot to oneDNN Matmul rewriting in terms of data type, shapes, and canonical order.\nCopybara import of the project:\n\n--\n5aeeeecc5d4fdfc1bfc8b20c17a0b5e3337f68e4 by Yimei Sun <yimei.sun@intel.com>:\n\n[XLA:CPU][oneDNN] Update on the criterias for rewriting Dot to oneDNN Matmul\n\nMerging this change closes #30328\n\nPiperOrigin-RevId: 804883242",
    "sha": "8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
    "files": [
        {
            "sha": "bbcafbe7ec217952c4a79ff20fadf6327eb7b995",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -45,6 +45,7 @@ onednn_graph_cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/cpu/runtime:dot_lib\",\n         \"//xla/backends/cpu/runtime/onednn:onednn_interop\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/tsl/mkl:onednn\",\n@@ -65,17 +66,21 @@ onednn_graph_cc_library(\n     deps = [\"//xla/tsl/mkl:onednn\"],\n )\n \n-cc_library(\n+onednn_graph_cc_library(\n     name = \"onednn_support\",\n     srcs = [\"onednn_support.cc\"],\n     hdrs = [\"onednn_support.h\"],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/cpu/codegen:target_machine_features\",\n         \"//xla/backends/cpu/runtime:dot_lib\",\n+        \"//xla/tsl/mkl:onednn\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@local_tsl//tsl/platform:platform_port\",\n     ],\n )\n "
        },
        {
            "sha": "4c426a18c12cd6cc147da7613ae79c9a610be97e",
            "filename": "third_party/xla/xla/backends/cpu/codegen/target_machine_features.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.cc?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -39,6 +39,8 @@ TargetMachineFeatures::TargetMachineFeatures(\n   if (target_machine_) {\n     has_avx512bf16_ = absl::StrContains(\n         target_machine_->getTargetFeatureString().str(), \"+avx512bf16\");\n+    has_avx512fp16_ = absl::StrContains(\n+        target_machine_->getTargetFeatureString().str(), \"+avx512fp16\");\n   }\n }\n "
        },
        {
            "sha": "8725d22d4fddb990c3c03e1f6c045bf1e82a41c3",
            "filename": "third_party/xla/xla/backends/cpu/codegen/target_machine_features.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.h?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -72,6 +72,7 @@ class TargetMachineFeatures {\n   virtual std::string get_target_feature_string() const;\n \n   virtual bool has_avx512bf16() const { return has_avx512bf16_; }\n+  virtual bool has_avx512fp16() const { return has_avx512fp16_; }\n \n  private:\n   llvm::TargetTransformInfo* GetTargetTransformInfoFor(\n@@ -85,6 +86,7 @@ class TargetMachineFeatures {\n \n   // Store availability of popular features here for efficient checks.\n   bool has_avx512bf16_ = false;\n+  bool has_avx512fp16_ = false;\n };\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "54757c444b77d4eaad62b47bffb60e9555b5066c",
            "filename": "third_party/xla/xla/backends/cpu/codegen/target_machine_features_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features_test.cc?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -26,42 +26,44 @@ limitations under the License.\n namespace xla::cpu {\n namespace {\n \n-struct Avx512Bf16TestSpec {\n+struct Avx512Bf16Fp16TestSpec {\n   std::string cpu_name;\n   std::string features;\n   bool has_avx512bf16;\n+  bool has_avx512fp16;\n };\n \n-class Avx512Bf16Test\n+class Avx512Bf16Fp16Test\n     : public TargetMachineTestBase,\n-      public ::testing::WithParamInterface<Avx512Bf16TestSpec> {\n+      public ::testing::WithParamInterface<Avx512Bf16Fp16TestSpec> {\n  public:\n   static std::string Name(\n-      const ::testing::TestParamInfo<Avx512Bf16TestSpec>& info) {\n+      const ::testing::TestParamInfo<Avx512Bf16Fp16TestSpec>& info) {\n     return info.param.cpu_name;\n   }\n };\n \n-TEST_P(Avx512Bf16Test, CheckAvailability) {\n-  Avx512Bf16TestSpec spec = GetParam();\n+TEST_P(Avx512Bf16Fp16Test, CheckAvailability) {\n+  Avx512Bf16Fp16TestSpec spec = GetParam();\n   const char* triple_string = \"x86_64-unknown-linux-gnu\";\n   std::unique_ptr<TargetMachineFeatures> features =\n       CreateTargetMachineFeatures(triple_string, spec.cpu_name, spec.features);\n   EXPECT_EQ(features->has_avx512bf16(), spec.has_avx512bf16);\n+  EXPECT_EQ(features->has_avx512fp16(), spec.has_avx512fp16);\n }\n \n-std::vector<Avx512Bf16TestSpec> GetAvx512Bf16TestSpecs() {\n-  return std::vector<Avx512Bf16TestSpec>{\n-      Avx512Bf16TestSpec{\"znver3\", \"+avx,+avx2\", false},\n-      Avx512Bf16TestSpec{\"sapphirerapids\",\n-                         \"+avx512vnni,+avx512bf16,+amx-bf16,+amx-int8,\"\n-                         \"+amx-tile,+amx-transpose\",\n-                         true}};\n+std::vector<Avx512Bf16Fp16TestSpec> GetAvx512Bf16Fp16TestSpecs() {\n+  return std::vector<Avx512Bf16Fp16TestSpec>{\n+      Avx512Bf16Fp16TestSpec{\"znver3\", \"+avx,+avx2\", false, false},\n+      Avx512Bf16Fp16TestSpec{\"sapphirerapids\",\n+                             \"+avx512vnni,+avx512bf16,+avx512fp16,+amx-bf16,\"\n+                             \"+amx-int8,+amx-tile,+amx-transpose\",\n+                             true, true}};\n }\n \n-INSTANTIATE_TEST_SUITE_P(Avx512Bf16Suite, Avx512Bf16Test,\n-                         ::testing::ValuesIn(GetAvx512Bf16TestSpecs()),\n-                         Avx512Bf16Test::Name);\n+INSTANTIATE_TEST_SUITE_P(Avx512Bf16Fp16Suite, Avx512Bf16Fp16Test,\n+                         ::testing::ValuesIn(GetAvx512Bf16Fp16TestSpecs()),\n+                         Avx512Bf16Fp16Test::Name);\n \n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "3789386749862b99011c16ee38405fb02186f051",
            "filename": "third_party/xla/xla/backends/cpu/onednn_emitter.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_emitter.cc?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/cpu/onednn_fusion.h\"\n #include \"xla/backends/cpu/onednn_support.h\"\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"\n #include \"xla/backends/cpu/runtime/onednn/onednn_interop.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -51,6 +52,12 @@ static absl::StatusOr<dnnl::graph::logical_tensor::data_type> OneDnnDatatype(\n   switch (type) {\n     case F32:\n       return dnnl::graph::logical_tensor::data_type::f32;\n+    case F16:\n+      return dnnl::graph::logical_tensor::data_type::f16;\n+    case BF16:\n+      return dnnl::graph::logical_tensor::data_type::bf16;\n+    case PRED:\n+      return dnnl::graph::logical_tensor::data_type::boolean;\n     default:\n       return InvalidArgument(\"Unsupported oneDNN data type: %s\",\n                              primitive_util::LowercasePrimitiveTypeName(type));\n@@ -185,7 +192,7 @@ static absl::StatusOr<dnnl::graph::logical_tensor> DefineBinaryOp(\n static absl::StatusOr<dnnl::graph::logical_tensor> DefineMatMul(\n     dnnl::graph::graph& graph, size_t op_id, LogicalTensorMap& logical_tensors,\n     const HloInstruction* instr) {\n-  // Verify that this Dot is supported by XNNPACK.\n+  // Verify that this Dot is supported by oneDNN.\n   const DotDimensionNumbers& dnums = instr->dot_dimension_numbers();\n   const Shape& lhs_shape = instr->operand(0)->shape();\n   const Shape& rhs_shape = instr->operand(1)->shape();\n@@ -215,6 +222,19 @@ static absl::StatusOr<dnnl::graph::logical_tensor> DefineMatMul(\n                                 lhs.get_id(), rhs.get_id(), output.get_id());\n \n   dnnl::graph::op op(op_id, matmul_op, {lhs, rhs}, {output});\n+\n+  TF_ASSIGN_OR_RETURN(DotShape dot_shape,\n+                      GetDotShape(dnums, lhs_shape, rhs_shape, instr->shape()));\n+  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n+                      GetDotCanonicalDims(dnums, dot_shape));\n+\n+  if (!dot_canonical_dims.lhs_canonical) {\n+    op.set_attr<bool>(dnnl::graph::op::attr::transpose_a, true);\n+  }\n+  if (!dot_canonical_dims.rhs_canonical) {\n+    op.set_attr<bool>(dnnl::graph::op::attr::transpose_b, true);\n+  }\n+\n   ONEDNN_RETURN_IF_ERROR(graph.add_op(op));\n \n   return output;"
        },
        {
            "sha": "396e681bcc700ddafd863948236747c91cf85e3d",
            "filename": "third_party/xla/xla/backends/cpu/onednn_support.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 11,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.cc?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -15,31 +15,98 @@ limitations under the License.\n \n #include \"xla/backends/cpu/onednn_support.h\"\n \n+#include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n+#include \"dnnl.hpp\"  // NOLINT: for DNNL_MAX_NDIMS\n+#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/backends/cpu/runtime/dot_lib.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/cpu_info.h\"\n \n namespace xla::cpu {\n \n+bool IsOneDnnSupportedDType(PrimitiveType dtype) {\n+  using tsl::port::CPUFeature;\n+  switch (dtype) {\n+    case F32:\n+      return true;\n+    case BF16:\n+      return TestCPUFeature(CPUFeature::AVX512F) ||\n+             TestCPUFeature(CPUFeature::AVX_NE_CONVERT) ||\n+             TestCPUFeature(CPUFeature::AMX_BF16);\n+    case F16:\n+      return (TestCPUFeature(CPUFeature::AVX512BW) &&\n+              (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n+               TestCPUFeature(CPUFeature::AMX_FP16))) ||\n+             TestCPUFeature(CPUFeature::AVX_NE_CONVERT);\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool IsOneDnnSupportedDType(PrimitiveType dtype,\n+                            const TargetMachineFeatures* cpu_features) {\n+  if (dtype == F32) {\n+    return true;\n+  }\n+\n+  if (cpu_features == nullptr) {\n+    return IsOneDnnSupportedDType(dtype);\n+  }\n+\n+  if (dtype == BF16) {\n+    return cpu_features->has_avx512bf16();\n+  }\n+  if (dtype == F16) {\n+    return cpu_features->has_avx512fp16();\n+  }\n+\n+  return false;\n+}\n+\n absl::StatusOr<bool> IsOneDnnDotSupported(\n     const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n-    const Shape& rhs_shape, const Shape& out_shape) {\n-  // TODO(penporn): Support other element types.\n-  if (lhs_shape.element_type() != F32 || rhs_shape.element_type() != F32 ||\n-      out_shape.element_type() != F32) {\n+    const Shape& rhs_shape, const Shape& out_shape,\n+    const TargetMachineFeatures* cpu_features) {\n+  if (lhs_shape.element_type() != rhs_shape.element_type() ||\n+      lhs_shape.element_type() != out_shape.element_type()) {\n+    return false;\n+  }\n+  if (!IsOneDnnSupportedDType(out_shape.element_type(), cpu_features)) {\n+    return false;\n+  }\n+\n+  if (ShapeUtil::IsZeroElementArray(lhs_shape) ||\n+      ShapeUtil::IsZeroElementArray(rhs_shape) ||\n+      ShapeUtil::IsZeroElementArray(out_shape)) {\n+    return false;\n+  }\n+\n+  // NOLINTNEXTLINE: Use dnnl.hpp for DNNL_MAX_NDIMS for now.\n+  if (lhs_shape.dimensions_size() > DNNL_MAX_NDIMS ||\n+      rhs_shape.dimensions_size() > DNNL_MAX_NDIMS ||\n+      lhs_shape.dimensions_size() != rhs_shape.dimensions_size()) {\n     return false;\n   }\n \n-  TF_ASSIGN_OR_RETURN(DotShape dot_shape, GetDotShape(dot_dimensions, lhs_shape,\n-                                                      rhs_shape, out_shape));\n+  auto dot_shape_result =\n+      GetDotShape(dot_dimensions, lhs_shape, rhs_shape, out_shape);\n+  if (!dot_shape_result.ok()) {\n+    VLOG(2) << \"GetDotShape Error: \" << dot_shape_result.status();\n+    return false;\n+  }\n+  DotShape dot_shape = dot_shape_result.value();\n \n-  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n-                      GetDotCanonicalDims(dot_dimensions, dot_shape));\n+  auto dot_canonical_result = GetDotCanonicalDims(dot_dimensions, dot_shape);\n+  if (!dot_canonical_result.ok()) {\n+    VLOG(2) << \"GetDotCanonicalDims Error: \" << dot_canonical_result.status();\n+    return false;\n+  }\n+  DotCanonicalDims dot_canonical_dims = dot_canonical_result.value();\n \n-  // Restrict support to no transposes and row-major layouts for now.\n-  return dot_canonical_dims.lhs_canonical && dot_canonical_dims.rhs_canonical &&\n-         !dot_canonical_dims.lhs_column_major &&\n+  // Restrict support to row-major layouts.\n+  return !dot_canonical_dims.lhs_column_major &&\n          !dot_canonical_dims.rhs_column_major;\n }\n "
        },
        {
            "sha": "3903950609085562ab8a621e072cf9d86de25a53",
            "filename": "third_party/xla/xla/backends/cpu/onednn_support.h",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.h?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -21,18 +21,24 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n \n inline constexpr absl::string_view kOneDnnFusionKind = \"__onednn_fusion\";\n \n+bool IsOneDnnSupportedDType(PrimitiveType dtype);\n+bool IsOneDnnSupportedDType(PrimitiveType dtype,\n+                            const TargetMachineFeatures* cpu_features);\n+\n // Returns true if the dot operation is supported by oneDNN. Returns an error\n // if the dot operation shape is invalid.\n absl::StatusOr<bool> IsOneDnnDotSupported(\n     const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n-    const Shape& rhs_shape, const Shape& out_shape);\n+    const Shape& rhs_shape, const Shape& out_shape,\n+    const TargetMachineFeatures* cpu_features = nullptr);\n \n }  // namespace xla::cpu\n "
        },
        {
            "sha": "94bc86212e9d71c3df42593db7478be36dda9052",
            "filename": "third_party/xla/xla/backends/cpu/transforms/dot_library_rewriter_test.cc",
            "status": "modified",
            "additions": 68,
            "deletions": 4,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter_test.cc?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -120,7 +120,7 @@ class CpuLibraryTest : public TargetMachineTestBase {\n     EXPECT_EQ(fusion->fusion_kind(), HloInstruction::FusionKind::kCustom);\n \n     // Adjust the expected values if a convert is auto-inserted.\n-    if (spec.out_dtype == \"bf16\" &&\n+    if (!use_onednn && spec.out_dtype == \"bf16\" &&\n         hlo_query::FindInstruction(fusion->fused_instructions_computation(),\n                                    HloOpcode::kDot)) {\n       ++expected.num_instructions_in_fused_computation;\n@@ -156,7 +156,14 @@ class CpuLibraryFullParamTest\n   bool IsDotEnabledOnCPU() {\n     DotRewriteTestSpec spec = GetParam();\n     bool bf16_dot_supported = absl::StrContains(spec.features, \"+avx512bf16\");\n-    return spec.in_dtype != \"bf16\" || bf16_dot_supported;\n+    bool fp16_dot_supported = absl::StrContains(spec.features, \"+avx512fp16\");\n+    if (spec.in_dtype == \"bf16\") {\n+      return bf16_dot_supported;\n+    }\n+    if (spec.in_dtype == \"f16\") {\n+      return fp16_dot_supported;\n+    }\n+    return true;\n   }\n };\n \n@@ -198,6 +205,61 @@ TEST_P(CpuLibraryFullParamTest, MatMul) {\n   RunTest(hlo_template, {HloOpcode::kDot, 2, 3, IsDotEnabledOnCPU()});\n }\n \n+TEST_P(CpuLibraryFullParamTest, MatMulTransposeRHS) {\n+  const absl::string_view hlo_template = R\"(\n+    HloModule matmul\n+\n+    ENTRY %main {\n+      %input = $in_dtype[32,8,128,64]{3,2,1,0} parameter(0)\n+      %weight = $in_dtype[32,8,128,64]{3,2,1,0} parameter(1)\n+      ROOT %dot = $out_dtype[32,8,128,128]{3,2,1,0} dot(%input, %weight),\n+                  lhs_batch_dims={0,1}, lhs_contracting_dims={3},\n+                  rhs_batch_dims={0,1}, rhs_contracting_dims={3}\n+    })\";\n+\n+  RunTest(hlo_template, {HloOpcode::kDot, 2, 3, IsDotEnabledOnCPU()});\n+}\n+\n+TEST_P(CpuLibraryFullParamTest, MatMulTransposeLHS) {\n+  const absl::string_view hlo_template = R\"(\n+    HloModule matmul\n+\n+    ENTRY %main {\n+      %input = $in_dtype[32,8,128,64]{3,2,1,0} parameter(0)\n+      %weight = $in_dtype[32,8,128,64]{3,2,1,0} parameter(1)\n+      ROOT %dot = $out_dtype[32,8,64,64]{3,2,1,0} dot(%input, %weight),\n+                  lhs_batch_dims={0,1}, lhs_contracting_dims={2},\n+                  rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    })\";\n+\n+  DotRewriteTestSpec spec = GetParam();\n+  FusionProperties expected = {HloOpcode::kDot, 0, 0, false};\n+  if (spec.lib == \"onednn\" && IsDotEnabledOnCPU()) {\n+    expected = FusionProperties{HloOpcode::kDot, 2, 3, true};\n+  }\n+  RunTest(hlo_template, expected);\n+}\n+\n+TEST_P(CpuLibraryFullParamTest, MatMulDimSizeUnqual) {\n+  const absl::string_view hlo_template = R\"(\n+    HloModule matmul\n+\n+    ENTRY %main {\n+      %input = $in_dtype[1,16,256,256]{3,2,1,0} parameter(0)\n+      %weight = $in_dtype[1,16,256]{2,1,0} parameter(1)\n+      ROOT %dot = $out_dtype[1,16,256]{2,1,0} dot(%input, %weight),\n+                  lhs_batch_dims={0,1}, lhs_contracting_dims={3},\n+                  rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    })\";\n+\n+  DotRewriteTestSpec spec = GetParam();\n+  FusionProperties expected = {HloOpcode::kDot, 0, 0, false};\n+  if (spec.lib == \"xnn\" && IsDotEnabledOnCPU()) {\n+    expected = FusionProperties{HloOpcode::kDot, 2, 3, true};\n+  }\n+  RunTest(hlo_template, expected);\n+}\n+\n TEST_P(CpuLibraryFullParamTest, MatMulAndAdd) {\n   const absl::string_view hlo_template = R\"(\n     HloModule matmul\n@@ -363,7 +425,8 @@ std::vector<DotRewriteTestSpec> GetDotRewriteTestSpecs() {\n   absl::flat_hash_map<std::string, std::string> cpu_to_features = {\n       {\"znver3\", \"+avx,+avx2\"},\n       {\"sapphirerapids\",\n-       \"+avx512vnni,+avx512bf16,+amx-bf16,+amx-int8,+amx-tile,+amx-transpose\"},\n+       \"+avx512vnni,+avx512bf16,+amx-bf16,+avx512fp16,+amx-int8,+amx-tile,+amx-\"\n+       \"transpose\"},\n   };\n \n   // Input and output data types to test per each library + CPU combination.\n@@ -382,7 +445,8 @@ std::vector<DotRewriteTestSpec> GetDotRewriteTestSpecs() {\n \n #if XLA_ONEDNN_USE_GRAPH_API\n   // Don't test oneDNN if we don't build with it.\n-  dtype_map[{\"onednn\", \"sapphirerapids\"}] = {{\"f32\", \"f32\"}};\n+  dtype_map[{\"onednn\", \"sapphirerapids\"}] = {\n+      {\"f32\", \"f32\"}, {\"bf16\", \"bf16\"}, {\"f16\", \"f16\"}};\n   fusion_modes[\"onednn\"] = {\"dot\"};\n #endif  // XLA_ONEDNN_USE_GRAPH_API\n "
        },
        {
            "sha": "7b033f7ca946ec7153b3656cadf9c296e56afcf7",
            "filename": "third_party/xla/xla/backends/cpu/transforms/onednn_matcher.h",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fonednn_matcher.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8dc944d09d6fd9ef2962f2a7d452889bce44b64e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fonednn_matcher.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fonednn_matcher.h?ref=8dc944d09d6fd9ef2962f2a7d452889bce44b64e",
            "patch": "@@ -51,11 +51,19 @@ class OneDnnMatcher : public LibraryMatcher {\n     if (!SupportedOps().contains(instr->opcode())) {\n       return false;\n     }\n-    // Assume all ops are supported as long as all inputs and output are F32.\n-    return instr->shape().element_type() == F32 &&\n+    if (instr->opcode() == HloOpcode::kDot) {\n+      return IsOneDnnDotSupported(\n+          instr->dot_dimension_numbers(), instr->operand(0)->shape(),\n+          instr->operand(1)->shape(), instr->shape(), target_machine_features_);\n+    }\n+\n+    return IsOneDnnSupportedDType(instr->shape().element_type(),\n+                                  target_machine_features_) &&\n            std::all_of(instr->operands().begin(), instr->operands().end(),\n-                       [](const HloInstruction* operand) {\n-                         return operand->shape().element_type() == F32;\n+                       [this](const HloInstruction* operand) {\n+                         return IsOneDnnSupportedDType(\n+                             operand->shape().element_type(),\n+                             target_machine_features_);\n                        });\n   }\n "
        }
    ],
    "stats": {
        "total": 252,
        "additions": 214,
        "deletions": 38
    }
}