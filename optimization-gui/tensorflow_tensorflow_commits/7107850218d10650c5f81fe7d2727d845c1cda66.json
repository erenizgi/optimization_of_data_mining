{
    "author": "tensorflower-gardener",
    "message": "Reverts 9a31dea9ee6fa86752dfb8adc4290f783dc95e6b\n\nPiperOrigin-RevId: 835318451",
    "sha": "7107850218d10650c5f81fe7d2727d845c1cda66",
    "files": [
        {
            "sha": "42e2e7ccb5086a9ec1efd06ac488a42d15cd351b",
            "filename": "tensorflow/compiler/mlir/tfrt/tests/sink_in_invariant_ops.mlir",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7107850218d10650c5f81fe7d2727d845c1cda66/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7107850218d10650c5f81fe7d2727d845c1cda66/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir?ref=7107850218d10650c5f81fe7d2727d845c1cda66",
            "patch": "@@ -195,28 +195,6 @@ func.func @sink_in_stateful_call(%arg0: tensor<i32> {tf_saved_model.index_path =\n   func.return %2 : tensor<i32>\n }\n \n-// Test VarHandleOp getting sinked when it is used by the called function and returned by the called function.\n-\n-// CHECK: func private @func_use_and_return_varhandle([[arg0:.+]]: tensor<!tf_type.resource<tensor<i32>>>)\n-func.func private @func_use_and_return_varhandle(%arg0: tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) {\n-  // CHECK: tf.VarHandleOp\n-  // CHECK-NEXT: tf.ReadVariableOp\n-  %0 = \"tf.ReadVariableOp\"(%arg0) {device = \"cpu\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n-\n-  func.return %0, %arg0 : tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>\n-}\n-\n-// CHECK-LABEL: func @sink_in_stateful_call_varhandle_return\n-func.func @sink_in_stateful_call_varhandle_return(%arg0: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n-  attributes {tf_saved_model.exported_names = [\"test_sink_in_stateful_call_varhandle_return\"]} {\n-  // CHECK: tf.VarHandleOp\n-  %0 = \"tf.VarHandleOp\"() {container = \"\", shared_name = \"x\"} : () -> tensor<!tf_type.resource<tensor<i32>>>\n-  // CHECK: \"tf.StatefulPartitionedCall\"(%0)\n-  %1:2 = \"tf.StatefulPartitionedCall\"(%0) {device = \"/CPU:0\", config = \"\", config_proto = \"\", executor_type = \"\", f = @func_use_and_return_varhandle} : (tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>)\n-  %2 = \"tf.AddV2\"(%arg0, %1#0) {device = \"/CPU:0\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\n-  func.return %2 : tensor<i32>\n-}\n-\n // CHECK-LABEL: func @sink_in_if\n func.func @sink_in_if(%arg0: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n   attributes {tf_saved_model.exported_names = [\"test_sink_in_if\"]} {\n@@ -396,54 +374,3 @@ func.func @nested_sink_in_if(%arg: tensor<i32> {tf_saved_model.index_path = [\"in\n }\n \n }\n-\n-// -----\n-\n-module attributes {tf_saved_model.semantics} {\n-\n-// Test sinks crossing nested tf.While and BatchFunction, while the sinkable ops are only copied at the target.\n-\n-// CHECK-LABEL: func private @batched_function\n-func.func private @batched_function(%arg0: tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n-  attributes {tf._input_shapes = [#tf_type.shape<1x3>, #tf_type.shape<*>], tf.signature.is_stateful} {\n-  // CHECK: tf.VarHandleOp\n-  // CHECK-NEXT: tf.ReadVariableOp\n-  %1 = \"tf.ReadVariableOp\"(%arg0) {device = \"/device:CPU:0\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n-  %2 = \"tf.Identity\"(%1) {device = \"/device:CPU:0\"} : (tensor<i32>) -> tensor<i32>\n-  func.return %2 : tensor<i32>\n-}\n-\n-// CHECK-LABEL: func private @while_cond_func\n-func.func private @while_cond_func(\n-    %arg0: tensor<i32>,\n-    %arg1: tensor<i32>,\n-    %arg: tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32> {\n-  // CHECK: [[handle:%.*]] = \"tf.VarHandleOp\"()\n-  // CHECK: \"tf.ReadVariableOp\"([[handle]])\n-  %0 = \"tf.ReadVariableOp\"(%arg) {device = \"cpu\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n-  func.return %0 : tensor<i32>\n-}\n-\n-// CHECK-LABEL: func private @while_body_func\n-func.func private @while_body_func(\n-    %arg0: tensor<i32>,\n-    %arg1: tensor<i32>,\n-    %arg2: tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) {\n-  // CHECK: \"tf.BatchFunction\"(%arg2)\n-  %0 = \"tf.BatchFunction\"(%arg2) {allowed_batch_sizes = [6], batch_timeout_micros = 100000 : i64, batching_queue = \"\", container = \"\", device = \"/device:CPU:0\", enable_large_batch_splitting = false, f = @batched_function, max_batch_size = 6 : i64, max_enqueued_batches = 10 : i64, num_batch_threads = 1 : i64, operandSegmentSizes = array<i32: 1, 0>, shared_name = \"batch/\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n-  func.return %0, %arg0, %arg2 : tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>\n-}\n-\n-// CHECK-LABEL: func @nested_sink_in_while_and_batch_functions\n-func.func @nested_sink_in_while_and_batch_functions(%arg: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n-  attributes {tf_saved_model.exported_names = [\"test_sink_in_while_and_batch_functions\"]} {\n-  // CHECK: [[handle:%.*]] = \"tf.VarHandleOp\"()\n-  %handle = \"tf.VarHandleOp\"() {container = \"\", shared_name = \"x\"} : () -> tensor<!tf_type.resource<tensor<i32>>>\n-  // CHECK: [[cond:%.*]] = \"tf.Const\"()\n-  %cond = \"tf.Const\"() {device = \"/CPU:0\", value = dense<0> : tensor<i32>} : () -> tensor<i32>\n-  // CHECK: \"tf.While\"([[cond]], [[cond]], [[handle]])\n-  %x:3 = \"tf.While\"(%cond, %cond, %handle) {body = @while_body_func, cond = @while_cond_func, is_stateless = false, parallel_iterations = 10 : i64, shape_invariant} : (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>)\n-  func.return %x#0 : tensor<i32>\n-}\n-\n-}"
        },
        {
            "sha": "4615c521edb0593a2016fe08a9902d42f6d9d88f",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/sink_in_invariant_ops.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 26,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7107850218d10650c5f81fe7d2727d845c1cda66/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7107850218d10650c5f81fe7d2727d845c1cda66/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc?ref=7107850218d10650c5f81fe7d2727d845c1cda66",
            "patch": "@@ -49,28 +49,15 @@ bool IsSinkCandidate(mlir::Operation *op) {\n // Check if the op is allowed to be sinked. We are being conservative here to\n // whilelist very limited set of ops here.\n struct AllowSinkHelper {\n-  explicit AllowSinkHelper(mlir::Operation* sinked_op, mlir::Operation* user,\n-                           int arg_index) {\n+  explicit AllowSinkHelper(mlir::Operation *op, int arg_index) {\n     if (llvm::isa<mlir::TF::BatchFunctionOp,\n-                  mlir::TF::StatefulPartitionedCallOp>(user)) {\n+                  mlir::TF::StatefulPartitionedCallOp>(op)) {\n       allow_sink_to = true;\n       callee_arg_index = arg_index;\n       return;\n     }\n \n-    // We tend to limit this support on WhileOp to only VarHandleOp to satisfy\n-    // IFRT lowering requirements.\n-    // Sinking other invariants like ConstOp is error-prone because it requires\n-    // non-trivial effort to avoid sinking Consts when they are used by cond\n-    // function and we don't need such support.\n-    if (llvm::isa<mlir::TF::VarHandleOp>(sinked_op) &&\n-        llvm::isa<mlir::TF::WhileOp>(user)) {\n-      allow_sink_to = true;\n-      callee_arg_index = arg_index;\n-      return;\n-    }\n-\n-    if (llvm::isa<mlir::TF::IfOp>(user) && arg_index > 0) {\n+    if (llvm::isa<mlir::TF::IfOp>(op) && arg_index > 0) {\n       allow_sink_to = true;\n       callee_arg_index = arg_index - 1;\n       return;\n@@ -120,8 +107,7 @@ void FindSinkTarget(\n   for (mlir::OpOperand &use : value.getUses()) {\n     auto *user = use.getOwner();\n \n-    AllowSinkHelper helper(original.getDefiningOp(), user,\n-                           use.getOperandNumber());\n+    AllowSinkHelper helper(user, use.getOperandNumber());\n \n     if (helper.allow_sink_to) {\n       auto values = FindValueInCallees(symbol_table, symbol_users, user,\n@@ -130,14 +116,6 @@ void FindSinkTarget(\n         FindSinkTarget(symbol_table, symbol_users, original, value, targets);\n       }\n     } else if (value != original) {\n-      // If the sinked op is directly used by ReturnOp, we don't sink it.\n-      // One example is for tf.WhileOp, the input and output of the cond\n-      // function and the body function must be the same. If the cond function\n-      // has an input of type tf.VarHandleOp and it just return the VarHandleOp,\n-      // we don't need to sink it.\n-      if (llvm::isa<mlir::func::ReturnOp>(user)) {\n-        continue;\n-      }\n       targets[&use].insert(original);\n     }\n   }"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 4,
        "deletions": 99
    }
}