{
    "author": "nputikhin",
    "message": "[XLA:GPU] Refactor GEMM fusion pass planing loop to use builder\n\nBy splitting out the plan building logic we make the loop itself more compact and understandable.\n\nPiperOrigin-RevId: 827889906",
    "sha": "b79778ea146065d43d7c1c4dc500e95845c61ee2",
    "files": [
        {
            "sha": "0a30847b67b21055541c338b2e51539ead612cd3",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion.cc",
            "status": "modified",
            "additions": 111,
            "deletions": 74,
            "changes": 185,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b79778ea146065d43d7c1c4dc500e95845c61ee2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b79778ea146065d43d7c1c4dc500e95845c61ee2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc?ref=b79778ea146065d43d7c1c4dc500e95845c61ee2",
            "patch": "@@ -106,11 +106,6 @@ class AdjacencyList {\n   std::vector<std::vector<NodeId>> adj_;\n };\n \n-struct HloAndDimOrder {\n-  const HloInstruction* original_hlo = nullptr;\n-  DimensionOrder dim_order;\n-};\n-\n struct HloAndIterSpec {\n   const HloInstruction* original_hlo;\n   TensorIterationSpec iter_spec;\n@@ -278,6 +273,87 @@ std::optional<DimOrdersAndReqs> GetUserDimOrdersAndCombinedReqsIfProfitable(\n       std::get<DotRequirements>(combined_reqs)};\n }\n \n+class FusionPlanBuilder {\n+ public:\n+  // Builds and returns the FusionPlan. Clears internal state.\n+  FusionPlan BuildPlan() {\n+    FusionPlan fusion_plan;\n+    for (auto& [node_id, node] : node_map_) {\n+      CHECK(node.should_fuse.has_value());\n+      fusion_plan.map[node_id] =\n+          NodeFusionPlan{node.original_hlo, *node.should_fuse};\n+    }\n+\n+    node_map_.clear();\n+    node_reuse_map_.clear();\n+    fusion_plan.graph = std::move(graph_);\n+    return fusion_plan;\n+  }\n+\n+  void ReserveSpaceForOutNeighbors(AdjacencyList::NodeId node_id,\n+                                   size_t count) {\n+    graph_.ReserveSpaceForOutNeighbors(node_id, count);\n+  }\n+\n+  void AddArc(AdjacencyList::NodeId from, AdjacencyList::NodeId to) {\n+    graph_.AddArc(from, to);\n+  }\n+\n+  const HloInstruction* GetOriginalHlo(AdjacencyList::NodeId node_id) const {\n+    return node_map_.at(node_id).original_hlo;\n+  }\n+\n+  const DimensionOrder& GetDimOrder(AdjacencyList::NodeId node_id) const {\n+    return node_map_.at(node_id).dim_order;\n+  }\n+\n+  // Inserts a node for the given HLO and `dim_order` unless already exists.\n+  // Returns the node id and a bool indicating if a new node was inserted.\n+  std::pair<AdjacencyList::NodeId, bool> InsertNode(\n+      const HloInstruction& hlo, const DimensionOrder& dim_order) {\n+    HloAndIterSpec reuse_key{&hlo, dim_order.ToTensorIterationSpec()};\n+\n+    // Attempt to insert a placeholder. If the key already exists, inserted is\n+    // false.\n+    auto [it, inserted] = node_reuse_map_.insert({reuse_key, -1});\n+    if (!inserted) {\n+      return {it->second, false};\n+    }\n+\n+    // Key was not present. Create the node and update the map.\n+    AdjacencyList::NodeId node_id = graph_.AddNode();\n+    it->second = node_id;\n+    CHECK(node_map_\n+              .insert({node_id,\n+                       Node{&hlo, dim_order, /*should_fuse=*/std::nullopt}})\n+              .second);\n+    return {node_id, true};\n+  }\n+\n+  // Assigns fusion decision for the specified node.\n+  // The node must not have an already assigned decision.\n+  void SetShouldFuseNode(AdjacencyList::NodeId node_id, bool should_fuse) {\n+    Node& node = node_map_.at(node_id);\n+    CHECK(!node.should_fuse.has_value());\n+    node.should_fuse = should_fuse;\n+  }\n+\n+ private:\n+  AdjacencyList graph_;\n+\n+  struct Node {\n+    const HloInstruction* original_hlo;\n+    DimensionOrder dim_order;\n+    std::optional<bool> should_fuse;\n+  };\n+  absl::flat_hash_map<AdjacencyList::NodeId, Node> node_map_;\n+\n+  // Allows reusing nodes when multiple instructions iterate over the same HLO\n+  // using the same iteration spec. In that case we don't duplicate the\n+  // instruction in the fusion.\n+  absl::flat_hash_map<HloAndIterSpec, AdjacencyList::NodeId> node_reuse_map_;\n+};\n+\n // Builds the fusion map and the requirements which can later be used to\n // actually fuse that subgraph.\n FusionPlanAndRequirements BuildFusionPlanTowardOperands(\n@@ -288,61 +364,32 @@ FusionPlanAndRequirements BuildFusionPlanTowardOperands(\n     const DotRequirements& requirements_so_far) {\n   CHECK(!max_params.has_value() || max_params.value() >= 1);\n \n-  // The graph describing the structure of the fusion that we build - nodes\n-  // corresponding to the instructions and arcs pointing from users to operands.\n-  // We can build and modify this graph easily without the need to create\n-  // HloInstructions at this point.\n-  AdjacencyList graph;\n-  // Stores the original HLO and the dimension order for each node. This is a\n-  // temporary map which is used when processing the nodes in this function.\n-  absl::flat_hash_map<AdjacencyList::NodeId, HloAndDimOrder>\n-      hlo_and_dim_order_map;\n-  // Stores the information needed to build the fused HLO for each node (what\n-  // was the original HLO and whether we should fuse it or create a parameter).\n-  // This is one of the outputs of this function.\n-  absl::flat_hash_map<AdjacencyList::NodeId, NodeFusionPlan> fusion_plan_map;\n-  // Allows reusing nodes when multiple instructions iterate over the same HLO\n-  // using the same iteration spec. In that case we don't duplicate the\n-  // instruction in the fusion.\n-  absl::flat_hash_map<HloAndIterSpec, AdjacencyList::NodeId> node_reuse_map;\n+  FusionPlanBuilder fusion_builder;\n+\n   // The requirements imposed by the fusion choices made in this function,\n-  // combined with the existing requirements. This is one of the outputs of this\n-  // function.\n+  // combined with the existing requirements. This is one of the outputs of\n+  // this function.\n   DotRequirements combined_reqs = requirements_so_far;\n \n-  auto get_or_create_fusion_node =\n-      [&](const HloInstruction& hlo, const DimensionOrder& dim_order,\n-          bool* is_new_node = nullptr) -> AdjacencyList::NodeId {\n-    HloAndIterSpec reuse_key = {&hlo, dim_order.ToTensorIterationSpec()};\n-    if (auto it = node_reuse_map.find(reuse_key); it != node_reuse_map.end()) {\n-      if (is_new_node != nullptr) {\n-        *is_new_node = false;\n-      }\n-      return it->second;\n-    }\n-    AdjacencyList::NodeId node_id = graph.AddNode();\n-    CHECK(hlo_and_dim_order_map.insert({node_id, {&hlo, dim_order}}).second);\n-    CHECK(node_reuse_map.insert({reuse_key, node_id}).second);\n-    if (is_new_node != nullptr) {\n-      *is_new_node = true;\n-    }\n-    return node_id;\n-  };\n   AdjacencyList::NodeId root =\n-      get_or_create_fusion_node(root_hlo, root_dim_order);\n+      fusion_builder.InsertNode(root_hlo, root_dim_order).first;\n \n   // Nodes at the fusion edge that can either get fused too or become parameters\n   // of the fusion. Used to track the number of parameters.\n   absl::flat_hash_set<AdjacencyList::NodeId> inputs({root});\n+\n   std::queue<AdjacencyList::NodeId> queue({root});\n   int64_t num_requeued = 0;\n+\n   // BFS\n+  // If all queued instructions are re-queued, they all exceed the parameter\n+  // limit, so stop fusing.\n   while (queue.size() > num_requeued) {\n     AdjacencyList::NodeId node_id = queue.front();\n     queue.pop();\n-    const HloAndDimOrder& hlo_and_dim_order = hlo_and_dim_order_map.at(node_id);\n-    const HloInstruction& original_hlo = *hlo_and_dim_order.original_hlo;\n-    const DimensionOrder& dim_order = hlo_and_dim_order.dim_order;\n+    const HloInstruction& original_hlo =\n+        *fusion_builder.GetOriginalHlo(node_id);\n+    const DimensionOrder& dim_order = fusion_builder.GetDimOrder(node_id);\n \n     // Watch the total number of fusion parameters.\n     if (max_params.has_value() &&\n@@ -355,55 +402,45 @@ FusionPlanAndRequirements BuildFusionPlanTowardOperands(\n       continue;\n     }\n     num_requeued = 0;\n+\n     if (original_hlo.opcode() == HloOpcode::kParameter) {\n-      CHECK(fusion_plan_map\n-                .insert({node_id, {&original_hlo, /*should_fuse=*/false}})\n-                .second);\n+      fusion_builder.SetShouldFuseNode(node_id, false);\n       continue;\n     }\n+\n     auto opt_result = GetOperandDimOrdersAndCombinedReqsIfProfitable(\n         original_hlo, dim_order, properties, gpu_version, combined_reqs);\n     if (!opt_result.has_value()) {\n-      CHECK(fusion_plan_map\n-                .insert({node_id, {&original_hlo, /*should_fuse=*/false}})\n-                .second);\n+      fusion_builder.SetShouldFuseNode(node_id, false);\n       continue;\n     }\n+\n     const DimOrderMap operand_dim_orders = std::move(opt_result->dim_orders);\n     combined_reqs = std::move(opt_result->requirements);\n+\n     inputs.erase(node_id);\n-    graph.ReserveSpaceForOutNeighbors(node_id, original_hlo.operand_count());\n-    for (int64_t i = 0; i < original_hlo.operand_count(); ++i) {\n-      const HloInstruction& operand = *original_hlo.operand(i);\n-      const DimensionOrder& operand_dim_order = operand_dim_orders.at(&operand);\n-      bool is_new_node = false;\n-      AdjacencyList::NodeId operand_node_id =\n-          get_or_create_fusion_node(operand, operand_dim_order, &is_new_node);\n-      graph.AddArc(node_id, operand_node_id);\n+    fusion_builder.ReserveSpaceForOutNeighbors(node_id,\n+                                               original_hlo.operand_count());\n+    for (const HloInstruction* operand : original_hlo.operands()) {\n+      const DimensionOrder& operand_dim_order = operand_dim_orders.at(operand);\n+      auto [operand_node_id, is_new_node] =\n+          fusion_builder.InsertNode(*operand, operand_dim_order);\n+      fusion_builder.AddArc(node_id, operand_node_id);\n       if (is_new_node) {\n-        VLOG(6) << \"Enqueueing \" << operand.ToString() << \":\"\n+        VLOG(6) << \"Enqueueing \" << operand->ToString() << \":\"\n                 << operand_dim_order.ToString();\n         inputs.insert(operand_node_id);\n         queue.push(operand_node_id);\n       }\n     }\n-    CHECK(\n-        fusion_plan_map.insert({node_id, {&original_hlo, /*should_fuse=*/true}})\n-            .second);\n+    fusion_builder.SetShouldFuseNode(node_id, true);\n   }\n   // Handle the remaining requeued items.\n-  while (!queue.empty()) {\n+  for (; !queue.empty(); queue.pop()) {\n     AdjacencyList::NodeId node_id = queue.front();\n-    queue.pop();\n-\n-    const HloAndDimOrder& hlo_and_dim_order = hlo_and_dim_order_map.at(node_id);\n-    CHECK(fusion_plan_map\n-              .insert({node_id,\n-                       {hlo_and_dim_order.original_hlo, /*should_fuse=*/false}})\n-              .second);\n+    fusion_builder.SetShouldFuseNode(node_id, false);\n   }\n-  return {{std::move(graph), std::move(fusion_plan_map)},\n-          std::move(combined_reqs)};\n+  return {fusion_builder.BuildPlan(), std::move(combined_reqs)};\n }\n \n // Builds the HLO instructions for the fusion represented by `fusion_plan`,"
        }
    ],
    "stats": {
        "total": 185,
        "additions": 111,
        "deletions": 74
    }
}