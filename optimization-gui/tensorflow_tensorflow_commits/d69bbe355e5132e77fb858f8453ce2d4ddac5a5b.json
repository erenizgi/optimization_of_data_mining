{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 826028412",
    "sha": "d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
    "files": [
        {
            "sha": "31f1aeedd9850e11335d60542dec4e5b192ba33a",
            "filename": "tensorflow/compiler/jit/build_xla_ops_pass.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fbuild_xla_ops_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fbuild_xla_ops_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fbuild_xla_ops_pass.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -132,7 +132,7 @@ void MergeOutgoingDataEdges(const Scope& s, Node* old_node, Node* new_node,\n     if (merged_output.node() == nullptr) {\n       Output new_output(new_node, oidx);\n       if (debugging_opts.print_outputs) {\n-        string cpu_device = \"/job:localhost/replica:0/task:0/device:CPU:0\";\n+        std::string cpu_device = \"/job:localhost/replica:0/task:0/device:CPU:0\";\n         ops::Print print_op(s.WithOpName(\"print_\", oidx)\n                                 .WithDevice(cpu_device)\n                                 .WithAssignedDevice(cpu_device),\n@@ -298,7 +298,8 @@ absl::StatusOr<Node*> ReplaceFunctionCallWithPartitionedCall(\n     const GraphOptimizationPassOptions& options,\n     const FunctionLibraryDefinition& flib_def, Node* n, Graph* g,\n     const NameAttrList& func, const Scope& root) {\n-  string config_string = options.session_options->config.SerializeAsString();\n+  std::string config_string =\n+      options.session_options->config.SerializeAsString();\n \n   int input_count = absl::c_count_if(\n       n->in_edges(), [](const Edge* e) { return !e->IsControlEdge(); });\n@@ -346,7 +347,8 @@ absl::StatusOr<Node*> ReplaceFunctionCallWithPartitionedCall(\n \n absl::StatusOr<jit::DeviceId> InferDeviceForCluster(\n     jit::DeviceInfoCache* device_info_cache, Node* n,\n-    const string& function_name, const FunctionLibraryDefinition& flib_def) {\n+    const std::string& function_name,\n+    const FunctionLibraryDefinition& flib_def) {\n   const FunctionDef* func_def = flib_def.Find(function_name);\n   TF_RET_CHECK(func_def) << \"Could not find \" << function_name;\n \n@@ -485,7 +487,8 @@ absl::Status ReplaceNodeWithXlaCompileAndXlaRun(\n     requires_compilation = true;\n   }\n \n-  string device_name_str = string(device_info_cache->GetNameFor(device));\n+  std::string device_name_str =\n+      std::string(device_info_cache->GetNameFor(device));\n \n   absl::Status status;\n   Scope root = NewInternalScope(g, &status, /*refiner=*/nullptr)"
        },
        {
            "sha": "6b90557df4b86f6e7c0a90afd3cdd5d874dc5cd1",
            "filename": "tensorflow/compiler/jit/build_xla_ops_pass_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fbuild_xla_ops_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fbuild_xla_ops_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fbuild_xla_ops_pass_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -85,8 +85,8 @@ absl::Status BuildXlaOps(const Scope& s, const FunctionDefLibrary& fdef_lib,\n   return absl::OkStatus();\n }\n \n-absl::Status MakeXlaCompiledKernel(Graph* graph, const string& callee_name,\n-                                   const string& node_name,\n+absl::Status MakeXlaCompiledKernel(Graph* graph, const std::string& callee_name,\n+                                   const std::string& node_name,\n                                    int num_constant_args, int num_resource_args,\n                                    Node** result) {\n   NodeDef call_node;\n@@ -99,27 +99,30 @@ absl::Status MakeXlaCompiledKernel(Graph* graph, const string& callee_name,\n   return absl::OkStatus();\n }\n \n-absl::Status MakeXlaCompiledKernel(Graph* graph, const string& callee_name,\n-                                   const string& node_name, Node** result) {\n+absl::Status MakeXlaCompiledKernel(Graph* graph, const std::string& callee_name,\n+                                   const std::string& node_name,\n+                                   Node** result) {\n   return MakeXlaCompiledKernel(graph, callee_name, node_name,\n                                /*num_constant_args=*/0, /*num_resource_args=*/0,\n                                result);\n }\n \n-Node* MakeWrite(const Scope& scope, Output value_to_write, const string& id) {\n+Node* MakeWrite(const Scope& scope, Output value_to_write,\n+                const std::string& id) {\n   Output var_handle = ops::VarHandleOp(scope.WithOpName(\"Var_\" + id), DT_FLOAT,\n                                        TensorShape({}));\n   ops::AssignVariableOp assign_op(scope.WithOpName(\"Assignee_\" + id),\n                                   var_handle, value_to_write);\n   return assign_op.operation.node();\n }\n \n-Node* MakeWrite(const Scope& scope, const string& id) {\n+Node* MakeWrite(const Scope& scope, const std::string& id) {\n   return MakeWrite(\n       scope, ops::Const(scope.WithOpName(\"ValueToAssign\" + id), 1.0f), id);\n }\n \n-FunctionDefLibrary CreateFunctionDefLibWithConstFunction(const string& name) {\n+FunctionDefLibrary CreateFunctionDefLibWithConstFunction(\n+    const std::string& name) {\n   FunctionDefLibrary fdef_lib;\n   FunctionDef func = FunctionDefHelper::Create(\n       /*function_name=*/name, /*in_def=*/{}, /*out_def=*/{\"out: float\"},"
        },
        {
            "sha": "4164efc65a8f4c5c1e94550094c32a11de046642",
            "filename": "tensorflow/compiler/jit/clone_constants_for_better_clustering.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fclone_constants_for_better_clustering.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fclone_constants_for_better_clustering.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fclone_constants_for_better_clustering.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -36,27 +36,29 @@ class CloneConstantsForBetterClusteringPassImpl {\n \n  private:\n   absl::Status CloneSmallConstantInputs(\n-      const absl::flat_hash_set<string>& name_set, Node* n);\n-  string GenerateUniqueName(const absl::flat_hash_set<string>& name_set,\n-                            absl::string_view prefix);\n-  absl::StatusOr<Node*> CloneNode(const absl::flat_hash_set<string>& name_set,\n-                                  Node* n);\n+      const absl::flat_hash_set<std::string>& name_set, Node* n);\n+  std::string GenerateUniqueName(\n+      const absl::flat_hash_set<std::string>& name_set,\n+      absl::string_view prefix);\n+  absl::StatusOr<Node*> CloneNode(\n+      const absl::flat_hash_set<std::string>& name_set, Node* n);\n \n   Graph* graph_;\n   int unique_name_counter_;\n };\n \n-string CloneConstantsForBetterClusteringPassImpl::GenerateUniqueName(\n-    const absl::flat_hash_set<string>& name_set, absl::string_view prefix) {\n-  string candidate;\n+std::string CloneConstantsForBetterClusteringPassImpl::GenerateUniqueName(\n+    const absl::flat_hash_set<std::string>& name_set,\n+    absl::string_view prefix) {\n+  std::string candidate;\n   do {\n     candidate = absl::StrCat(prefix, \"/clone_\", unique_name_counter_++);\n   } while (name_set.contains(candidate));\n   return candidate;\n }\n \n absl::StatusOr<Node*> CloneConstantsForBetterClusteringPassImpl::CloneNode(\n-    const absl::flat_hash_set<string>& name_set, Node* n) {\n+    const absl::flat_hash_set<std::string>& name_set, Node* n) {\n   NodeDef new_in_def = n->def();\n   new_in_def.clear_input();\n   new_in_def.set_name(GenerateUniqueName(name_set, new_in_def.name()));\n@@ -112,7 +114,7 @@ bool IsInPlaceOp(absl::string_view op_name) {\n \n absl::Status\n CloneConstantsForBetterClusteringPassImpl::CloneSmallConstantInputs(\n-    const absl::flat_hash_set<string>& name_set, Node* n) {\n+    const absl::flat_hash_set<std::string>& name_set, Node* n) {\n   std::vector<const Edge*> in_edges;\n   // Get the edges and sort them so we clone in a deterministic order.\n   absl::c_copy(n->in_edges(), std::back_inserter(in_edges));\n@@ -142,7 +144,7 @@ CloneConstantsForBetterClusteringPassImpl::CloneSmallConstantInputs(\n }\n \n absl::Status CloneConstantsForBetterClusteringPassImpl::Run() {\n-  absl::flat_hash_set<string> name_set;\n+  absl::flat_hash_set<std::string> name_set;\n   absl::c_transform(graph_->nodes(), std::inserter(name_set, name_set.begin()),\n                     [](Node* n) { return n->name(); });\n   std::vector<Node*> nodes;"
        },
        {
            "sha": "20a3d98be1d0f2338ef621a58678d50e90668311",
            "filename": "tensorflow/compiler/jit/cluster_scoping_pass.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcluster_scoping_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcluster_scoping_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fcluster_scoping_pass.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -51,8 +51,8 @@ class ClusterScopingPassImpl {\n   size_t unique_scope_id_;\n };\n \n-std::optional<string> GetXlaInternalScope(Node* node) {\n-  string scope;\n+std::optional<std::string> GetXlaInternalScope(Node* node) {\n+  std::string scope;\n   if (GetNodeAttr(node->attrs(), kXlaInternalScopeAttr, &scope).ok()) {\n     return scope;\n   }\n@@ -85,8 +85,8 @@ void SetXlaInternalScope(Node* node, absl::string_view scope) {\n //  Node_X (scope \"stage\") -> Stage\n //\n void AddOrAppendXlaInternalScope(Node* node, absl::string_view suffix) {\n-  string updated_scope;\n-  std::optional<string> cur_scope = GetXlaInternalScope(node);\n+  std::string updated_scope;\n+  std::optional<std::string> cur_scope = GetXlaInternalScope(node);\n   if (cur_scope == std::nullopt) {\n     updated_scope = std::string(suffix);\n   } else {\n@@ -96,7 +96,7 @@ void AddOrAppendXlaInternalScope(Node* node, absl::string_view suffix) {\n }\n \n void ClusterScopingPassImpl::AddScopeToAllTransitivePredecessors(Node* start) {\n-  const string unique_suffix = absl::StrCat(\"_\", GetUniqueScopeId());\n+  const std::string unique_suffix = absl::StrCat(\"_\", GetUniqueScopeId());\n \n   std::vector<Node*> starts;\n   starts.push_back(start);\n@@ -106,7 +106,7 @@ void ClusterScopingPassImpl::AddScopeToAllTransitivePredecessors(Node* start) {\n }\n \n void ClusterScopingPassImpl::AddScopeToAllTransitiveSuccessors(Node* start) {\n-  const string unique_suffix = absl::StrCat(\"_\", GetUniqueScopeId());\n+  const std::string unique_suffix = absl::StrCat(\"_\", GetUniqueScopeId());\n \n   std::vector<Node*> starts;\n   starts.push_back(start);"
        },
        {
            "sha": "66cc10775992a3f1734547cf5aa264ad8d27fe38",
            "filename": "tensorflow/compiler/jit/cluster_scoping_pass_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcluster_scoping_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcluster_scoping_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fcluster_scoping_pass_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -45,10 +45,11 @@ absl::Status ClusterScoping(std::unique_ptr<Graph>* graph) {\n   return pass.Run(opt_options);\n }\n \n-absl::flat_hash_map<string, string> GetXlaInternalScopes(const Graph& graph) {\n-  absl::flat_hash_map<string, string> scopes;\n+absl::flat_hash_map<std::string, std::string> GetXlaInternalScopes(\n+    const Graph& graph) {\n+  absl::flat_hash_map<std::string, std::string> scopes;\n   for (Node* node : graph.nodes()) {\n-    string scope;\n+    std::string scope;\n     if (GetNodeAttr(node->attrs(), kXlaInternalScopeAttr, &scope).ok()) {\n       scopes[node->name()] = scope;\n     }\n@@ -63,7 +64,7 @@ absl::flat_hash_map<string, string> GetXlaInternalScopes(const Graph& graph) {\n   return scopes;\n }\n \n-Node* BuildStageNode(GraphDefBuilder& builder, string name,\n+Node* BuildStageNode(GraphDefBuilder& builder, std::string name,\n                      std::initializer_list<DataType> dtypes,\n                      absl::Span<const ops::NodeOut> values) {\n   auto opts = builder.opts()"
        },
        {
            "sha": "6c77648817f808c5b535d855d48f04349eb370ae",
            "filename": "tensorflow/compiler/jit/compilability_check_util.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcompilability_check_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcompilability_check_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fcompilability_check_util.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -172,7 +172,7 @@ RecursiveCompilabilityChecker::FindUncompilableNodes(\n }\n \n bool RecursiveCompilabilityChecker::HasXLAKernel(\n-    const Node& node, string* uncompilable_reason) const {\n+    const Node& node, std::string* uncompilable_reason) const {\n   // There is a SymbolicGradient kernel on the XLA_JIT device, but the gradient\n   // is really a kind of function call and will be handled by\n   // IsCompilableCall().\n@@ -424,7 +424,7 @@ bool RecursiveCompilabilityChecker::IsCompilableNode(\n     return false;\n   }\n \n-  string uncompilable_reason;\n+  std::string uncompilable_reason;\n   if (IsFunctionCall(*lib_runtime->GetFunctionLibraryDefinition(), node)) {\n     if (!IsCompilableCall(node.def(), lib_runtime, stack_trace,\n                           encapsulating_function, uncompilable_nodes)) {"
        },
        {
            "sha": "7d6741529ebd08d2401529ce37e5b55527419329",
            "filename": "tensorflow/compiler/jit/compilability_check_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcompilability_check_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fcompilability_check_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fcompilability_check_util.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -262,7 +262,7 @@ class RecursiveCompilabilityChecker {\n   }\n \n   bool HasXLAKernel(const Node& node,\n-                    string* uncompilable_reason = nullptr) const;\n+                    std::string* uncompilable_reason = nullptr) const;\n \n   static void MaybeMarkUncompilableNode(\n       const absl::string_view reason,"
        },
        {
            "sha": "fa546e3543e3587e7c514506164ab1ff022f6877",
            "filename": "tensorflow/compiler/jit/deadness_analysis.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -123,7 +123,7 @@ class Predicate {\n  public:\n   enum class Kind { kAnd, kOr, kNot, kAndRecurrence, kSymbol, kIntSymbol };\n \n-  virtual string ToString() const = 0;\n+  virtual std::string ToString() const = 0;\n \n   // An ID assigned to the Predicate at construction time.  Conceptually like a\n   // pointer, except that it is stable across runs.\n@@ -156,12 +156,12 @@ class AndPredicate : public Predicate {\n   explicit AndPredicate(int64_t id, std::vector<Predicate*> operands)\n       : Predicate(id), operands_(std::move(operands)) {}\n \n-  string ToString() const override {\n+  std::string ToString() const override {\n     if (operands().empty()) {\n       return \"#true\";\n     }\n \n-    std::vector<string> operands_str;\n+    std::vector<std::string> operands_str;\n     std::transform(operands().begin(), operands().end(),\n                    std::back_inserter(operands_str),\n                    [](Predicate* pred) { return pred->ToString(); });\n@@ -186,12 +186,12 @@ class OrPredicate : public Predicate {\n   explicit OrPredicate(int64_t id, std::vector<Predicate*> operands)\n       : Predicate(id), operands_(std::move(operands)) {}\n \n-  string ToString() const override {\n+  std::string ToString() const override {\n     if (operands().empty()) {\n       return \"#false\";\n     }\n \n-    std::vector<string> operands_str;\n+    std::vector<std::string> operands_str;\n     std::transform(operands().begin(), operands().end(),\n                    std::back_inserter(operands_str),\n                    [](Predicate* pred) { return pred->ToString(); });\n@@ -215,7 +215,7 @@ class NotPredicate : public Predicate {\n   explicit NotPredicate(int64_t id, Predicate* operand)\n       : Predicate(id), operands_({operand}) {}\n \n-  string ToString() const override {\n+  std::string ToString() const override {\n     return absl::StrCat(\"~\", operand()->ToString());\n   }\n \n@@ -251,14 +251,14 @@ class NotPredicate : public Predicate {\n class AndRecurrencePredicate : public Predicate {\n  public:\n   explicit AndRecurrencePredicate(int64_t id, Predicate* start, Predicate* step,\n-                                  std::vector<string> frame)\n+                                  std::vector<std::string> frame)\n       : Predicate(id), operands_({start, step}), frame_(std::move(frame)) {}\n \n   Predicate* start() const { return operands_[0]; }\n   Predicate* step() const { return operands_[1]; }\n-  absl::Span<const string> frame() const { return frame_; }\n+  absl::Span<const std::string> frame() const { return frame_; }\n \n-  string ToString() const override {\n+  std::string ToString() const override {\n     return absl::StrCat(\"{\", start()->ToString(), \",&,\", step()->ToString(),\n                         \"}<\", absl::StrJoin(frame(), \";\"), \">\");\n   }\n@@ -271,7 +271,7 @@ class AndRecurrencePredicate : public Predicate {\n \n  private:\n   std::array<Predicate*, 2> operands_;\n-  std::vector<string> frame_;\n+  std::vector<std::string> frame_;\n };\n \n // Represents an uninterpreted symbol in a logical predicate.\n@@ -286,7 +286,7 @@ class SymbolPredicate : public Predicate {\n         tensor_id_(std::move(tensor_id)),\n         must_be_true_(must_be_true) {}\n \n-  string ToString() const override {\n+  std::string ToString() const override {\n     return must_be_true() ? absl::StrCat(\"*\", tensor_id_.ToString())\n                           : tensor_id_.ToString();\n   }\n@@ -320,7 +320,7 @@ class IntSymbolPredicate : public Predicate {\n         tensor_id_(std::move(tensor_id)),\n         must_have_value_(must_have_value) {}\n \n-  string ToString() const override {\n+  std::string ToString() const override {\n     return must_have_value().has_value()\n                ? absl::StrCat(tensor_id_.ToString(), \"=\", *must_have_value_)\n                : tensor_id_.ToString();\n@@ -396,7 +396,7 @@ class PredicateFactory {\n   }\n \n   Predicate* MakeAndRecurrencePredicate(Predicate* start, Predicate* step,\n-                                        std::vector<string> frame) {\n+                                        std::vector<std::string> frame) {\n     SignatureForAndRec signature(start, step, std::move(frame));\n     auto it = interned_and_rec_instances_.find(signature);\n     if (it != interned_and_rec_instances_.end()) {\n@@ -463,8 +463,8 @@ class PredicateFactory {\n       Tensor tensor(proto->dtype());\n       TF_RET_CHECK(tensor.FromProto(*proto));\n \n-      *predicate = tensor.scalar<int32>()() == *must_have_value ? MakeTrue()\n-                                                                : MakeFalse();\n+      *predicate = tensor.scalar<int32_t>()() == *must_have_value ? MakeTrue()\n+                                                                  : MakeFalse();\n       return absl::OkStatus();\n     }\n     SignatureForIntSymbol signature = {tensor_id, must_have_value};\n@@ -559,9 +559,9 @@ class PredicateFactory {\n       std::pair<Predicate::Kind, absl::Span<Predicate* const>>;\n   using SignatureForNot = Predicate*;\n   using SignatureForAndRec =\n-      std::tuple<Predicate*, Predicate*, std::vector<string>>;\n+      std::tuple<Predicate*, Predicate*, std::vector<std::string>>;\n   using SignatureForSymbol = std::pair<SafeTensorId, bool>;\n-  using SignatureForIntSymbol = std::pair<SafeTensorId, std::optional<int32>>;\n+  using SignatureForIntSymbol = std::pair<SafeTensorId, std::optional<int32_t>>;\n \n   struct HashSignatureForAndOr {\n     size_t operator()(const SignatureForAndOr& signature) const {\n@@ -586,7 +586,7 @@ class PredicateFactory {\n           SafeTensorId::Hasher()(signature.first),\n           Hash64Combine(\n               ::tensorflow::hash<bool>()(signature.second.has_value()),\n-              ::tensorflow::hash<int32>()(\n+              ::tensorflow::hash<int32_t>()(\n                   signature.second.has_value() ? *signature.second : 0)));\n     }\n   };\n@@ -830,8 +830,8 @@ class DeadnessAnalysisImpl : public DeadnessAnalysis {\n   absl::StatusOr<DeadnessAnalysis::DeadnessPredicate> GetPredicateFor(\n       Node* n, int oidx) const override;\n   void Print() const override;\n-  absl::flat_hash_map<TensorId, string, TensorId::Hasher> PredicateMapAsString()\n-      const;\n+  absl::flat_hash_map<TensorId, std::string, TensorId::Hasher>\n+  PredicateMapAsString() const;\n \n  private:\n   enum class EdgeKind { kDataAndControl, kDataOnly, kControlOnly };\n@@ -958,7 +958,7 @@ absl::Status DeadnessAnalysisImpl::HandleSwitch(\n     for (int i = 0; i < n->num_outputs() - 1; i++) {\n       TF_RETURN_IF_ERROR(predicate_factory_.MakeSymbolPredicate(\n           pred_edge->src(), pred_edge->src_output(),\n-          /*must_have_value=*/std::optional<int32>(i), &branch_pred));\n+          /*must_have_value=*/std::optional<int32_t>(i), &branch_pred));\n       input_preds.push_back(branch_pred);\n       SetPredicate(n, i, predicate_factory_.MakeAndPredicate(input_preds),\n                    should_revisit);\n@@ -982,7 +982,7 @@ absl::Status DeadnessAnalysisImpl::HandleSwitch(\n \n namespace {\n absl::Status CreateMultipleNextIterationInputsError(Node* merge) {\n-  std::vector<string> backedges;\n+  std::vector<std::string> backedges;\n   for (const Edge* backedge : merge->in_edges()) {\n     if (backedge->src()->IsNextIteration()) {\n       backedges.push_back(absl::StrCat(\"  \", SummarizeNode(*backedge->src())));\n@@ -1058,7 +1058,7 @@ Predicate* DeduceStepPredicate(PredicateFactory* predicate_factory,\n \n absl::Status GetFullFrame(const Node* n,\n                           absl::Span<const ControlFlowInfo> cfi_infos,\n-                          std::vector<string>* frame) {\n+                          std::vector<std::string>* frame) {\n   int depth = 0;\n   for (const ControlFlowInfo* cfi_iter = &cfi_infos[n->id()]; !n->IsSource();\n        n = cfi_iter->parent_frame, cfi_iter = &cfi_infos[n->id()]) {\n@@ -1174,7 +1174,7 @@ absl::Status DeadnessAnalysisImpl::HandleMerge(\n \n         Predicate* start =\n             predicate_factory_.MakeOrPredicate(non_recurrent_inputs);\n-        std::vector<string> frame;\n+        std::vector<std::string> frame;\n         TF_RETURN_IF_ERROR(GetFullFrame(n, control_flow_info_, &frame));\n         Predicate* and_rec = predicate_factory_.MakeAndRecurrencePredicate(\n             start, step, std::move(frame));\n@@ -1358,7 +1358,7 @@ absl::Status DeadnessAnalysisImpl::GetFrameBasedTopologicalOrder(\n // nested while, as there is no clean cut for separating them in the topological\n // order.\n absl::Status DeadnessAnalysisImpl::Populate(bool enable_optimistic) {\n-  std::vector<string> unreachable_nodes;\n+  std::vector<std::string> unreachable_nodes;\n   // Compute the loop structure of the graph.\n   TF_RETURN_IF_ERROR(\n       BuildControlFlowInfo(&graph_, &control_flow_info_, &unreachable_nodes));\n@@ -1582,9 +1582,9 @@ DeadnessAnalysis::~DeadnessAnalysis() {}\n   return absl::OkStatus();\n }\n \n-absl::flat_hash_map<TensorId, string, TensorId::Hasher>\n+absl::flat_hash_map<TensorId, std::string, TensorId::Hasher>\n DeadnessAnalysisImpl::PredicateMapAsString() const {\n-  absl::flat_hash_map<TensorId, string, TensorId::Hasher> result;\n+  absl::flat_hash_map<TensorId, std::string, TensorId::Hasher> result;\n   for (const auto& kv_pair : predicate_map_) {\n     CHECK(result.insert({kv_pair.first, kv_pair.second->ToString()}).second);\n   }\n@@ -1603,7 +1603,7 @@ absl::Status ComputePredicates(const Graph& graph,\n \n }  // namespace deadness_analysis_internal\n \n-string DeadnessAnalysis::DebugString(DeadnessPredicate predicate) const {\n+std::string DeadnessAnalysis::DebugString(DeadnessPredicate predicate) const {\n   return static_cast<Predicate*>(predicate.pred_)->ToString();\n }\n "
        },
        {
            "sha": "1cd394154faf36a8bf7d328afa3f9a2bd7c2416e",
            "filename": "tensorflow/compiler/jit/deadness_analysis.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -81,7 +81,7 @@ class DeadnessAnalysis {\n   virtual void Print() const = 0;\n   virtual ~DeadnessAnalysis();\n \n-  string DebugString(DeadnessPredicate predicate) const;\n+  std::string DebugString(DeadnessPredicate predicate) const;\n \n   // Run the deadness analysis over `graph` and returns an error or a populated\n   // instance of DeadnessAnalysis in `result`."
        },
        {
            "sha": "569cdeadae735e9e6cac82d49d3abc7be9cb155c",
            "filename": "tensorflow/compiler/jit/deadness_analysis_internal.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis_internal.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis_internal.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis_internal.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -24,7 +24,8 @@ namespace deadness_analysis_internal {\n \n // Returns a map describing the predicate each Tensor was mapped to.  For\n // testing purposes only.\n-using PredicateMapTy = absl::flat_hash_map<TensorId, string, TensorId::Hasher>;\n+using PredicateMapTy =\n+    absl::flat_hash_map<TensorId, std::string, TensorId::Hasher>;\n absl::Status ComputePredicates(const Graph& graph,\n                                PredicateMapTy* out_predicate_map,\n                                bool enable_optimistic = true);"
        },
        {
            "sha": "fd7d93b3772f5f764bf8b8d38a803182e09cfe4a",
            "filename": "tensorflow/compiler/jit/deadness_analysis_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdeadness_analysis_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -61,7 +61,7 @@ absl::Status AnalyzeDeadness(Graph* graph,\n   return DeadnessAnalysis::Run(*graph, result);\n }\n \n-ops::Switch CreateSwitch(const Scope& root, const string& prefix) {\n+ops::Switch CreateSwitch(const Scope& root, const std::string& prefix) {\n   Output value = ops::Placeholder(root.WithOpName(prefix + \"/value\"), DT_FLOAT);\n   Output predicate =\n       ops::Placeholder(root.WithOpName(prefix + \"/pred\"), DT_BOOL);\n@@ -76,7 +76,7 @@ void VLogGraphIfAsked(const Graph& graph) {\n   if (VLOG_IS_ON(3)) {\n     GraphDef graph_def;\n     graph.ToGraphDef(&graph_def);\n-    string serialized;\n+    std::string serialized;\n     ::tensorflow::protobuf::TextFormat::PrintToString(graph_def, &serialized);\n     LOG(INFO) << serialized;\n   }\n@@ -127,8 +127,8 @@ struct InductionVarInfo {\n //    +-----> |     Exit      |\n //            +---------------+\n InductionVarInfo CreateInductionVariable(const Scope& root,\n-                                         const string& prefix,\n-                                         const string& frame_name,\n+                                         const std::string& prefix,\n+                                         const std::string& frame_name,\n                                          const Output& initial_value) {\n   Output enter_initial_value = ops::internal::Enter(\n       root.WithOpName(prefix + \"/enter\"), initial_value, frame_name);\n@@ -158,8 +158,8 @@ InductionVarInfo CreateInductionVariable(const Scope& root,\n }\n \n InductionVarInfo CreateInductionVariable(const Scope& root,\n-                                         const string& prefix,\n-                                         const string& frame_name,\n+                                         const std::string& prefix,\n+                                         const std::string& frame_name,\n                                          int32_t init) {\n   return CreateInductionVariable(\n       root, prefix, frame_name,\n@@ -201,7 +201,7 @@ struct DependentInductionVar {\n };\n \n DependentInductionVar CreateDependentLoopInvariantValue(\n-    const Scope& root, const string& prefix, const string& frame_name,\n+    const Scope& root, const std::string& prefix, const std::string& frame_name,\n     const Output& loop_cond, const Output& value) {\n   Output enter_value = ops::internal::Enter(root.WithOpName(prefix + \"/enter\"),\n                                             value, frame_name);\n@@ -218,7 +218,7 @@ DependentInductionVar CreateDependentLoopInvariantValue(\n }\n \n DependentInductionVar CreateDependentLoopInvariantValue(\n-    const Scope& root, const string& prefix, const string& frame_name,\n+    const Scope& root, const std::string& prefix, const std::string& frame_name,\n     const Output& loop_cond, int32_t value) {\n   return CreateDependentLoopInvariantValue(\n       root, prefix, frame_name, loop_cond,"
        },
        {
            "sha": "8288b44e7f1c1da2cff6df4595537672b9c052eb",
            "filename": "tensorflow/compiler/jit/device_compilation_cluster_signature.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_cluster_signature.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_cluster_signature.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_cluster_signature.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -65,9 +65,9 @@ struct SignatureNotEqual {\n // Functor that incrementally computes a Signature's hash given its current hash\n // and one of its args.\n struct SignatureHashCombiner {\n-  explicit SignatureHashCombiner(const uint64 h) : h(h) {}\n-  uint64 h;\n-  uint64 operator()(const Tensor& arg) {\n+  explicit SignatureHashCombiner(const uint64_t h) : h(h) {}\n+  uint64_t h;\n+  uint64_t operator()(const Tensor& arg) {\n     h = Hash64Combine(h, std::hash<int>()(static_cast<int>(arg.dtype())));\n     h = Hash64Combine(\n         h, Hash64(arg.tensor_data().data(), arg.tensor_data().size()));\n@@ -76,7 +76,7 @@ struct SignatureHashCombiner {\n     }\n     return h;\n   }\n-  uint64 operator()(const TensorTypeAndShape& arg) {\n+  uint64_t operator()(const TensorTypeAndShape& arg) {\n     h = Hash64Combine(h, std::hash<int>()(static_cast<int>(arg.first)));\n     h = Hash64Combine(h, std::hash<int>()(arg.second.size()));\n     for (int dim : arg.second) {\n@@ -108,8 +108,8 @@ bool Signature::operator==(const Signature& other) const {\n   return true;\n }\n \n-uint64 Signature::Hash::operator()(const Signature& signature) const {\n-  uint64 h = std::hash<string>()(signature.name);\n+uint64_t Signature::Hash::operator()(const Signature& signature) const {\n+  uint64_t h = std::hash<std::string>()(signature.name);\n   for (const auto& arg : signature.args) {\n     h = std::visit(SignatureHashCombiner(h), arg);\n   }"
        },
        {
            "sha": "721c1d3b78c50e92cb5da8f3bc21d864688ec1f1",
            "filename": "tensorflow/compiler/jit/device_compilation_cluster_signature.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_cluster_signature.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_cluster_signature.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_cluster_signature.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -58,7 +58,8 @@ struct DeviceCompilationClusterSignature {\n   bool operator==(const DeviceCompilationClusterSignature& other) const;\n \n   struct Hash {\n-    uint64 operator()(const DeviceCompilationClusterSignature& signature) const;\n+    uint64_t operator()(\n+        const DeviceCompilationClusterSignature& signature) const;\n   };\n \n   // Returns a human-readable description of the signature."
        },
        {
            "sha": "ec161293b7643d7b0dec1d0b0a32e658b4aa10ad",
            "filename": "tensorflow/compiler/jit/device_compilation_profiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_compilation_profiler.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -107,7 +107,7 @@ absl::Status DeviceCompilationProfiler::RegisterCompilation(\n       cluster_compile_stats_.emplace(function.name(), ClusterCompileStats{})\n           .first;\n \n-  const uint64 compile_time_s = compile_time_us / 1.0e6;\n+  const uint64_t compile_time_s = compile_time_us / 1.0e6;\n   it->second.compile_count++;\n   it->second.cumulative_compile_time_us += compile_time_us;\n   VLOG(1) << \"Compiled \" << function_name << \" \" << it->second.compile_count"
        },
        {
            "sha": "a9f2418282c414dee95661abe9f1255add4cfa66",
            "filename": "tensorflow/compiler/jit/device_compiler.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_compiler.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -137,7 +137,7 @@ class DeviceCompiler : public ResourceBase {\n     return compiler_client_.get();\n   }\n \n-  string DebugString() const override;\n+  std::string DebugString() const override;\n \n  private:\n   // Common implementation of Compile and CompileSingleOp. The `OpKernelContext`\n@@ -259,7 +259,7 @@ DeviceCompiler<ExecutableType, ClientType>::~DeviceCompiler() {\n }\n \n template <typename ExecutableType, typename ClientType>\n-string DeviceCompiler<ExecutableType, ClientType>::DebugString() const {\n+std::string DeviceCompiler<ExecutableType, ClientType>::DebugString() const {\n   return \"DeviceCompiler\";\n }\n \n@@ -331,7 +331,7 @@ DeviceCompiler<ExecutableType, ClientType>::CompileStrict(\n     CompileScope scope, OpKernelContext* ctx,\n     DeviceCompilationProfiler* profiler, mutex* mu) {\n   tensorflow::Env* env = tensorflow::Env::Default();\n-  const uint64 compile_start_us = env->NowMicros();\n+  const uint64_t compile_start_us = env->NowMicros();\n \n   TfGraphToHloCompiler compiler(options);\n   cache_value.compile_state = DeviceCompileState::kCompiled;\n@@ -385,8 +385,8 @@ DeviceCompiler<ExecutableType, ClientType>::CompileStrict(\n   // Finalize the cache to release the XlaComputation after it was compiled.\n   cache_->Finalize();\n \n-  const uint64 compile_end_us = env->NowMicros();\n-  const uint64 compile_time_us = compile_end_us - compile_start_us;\n+  const uint64_t compile_end_us = env->NowMicros();\n+  const uint64_t compile_time_us = compile_end_us - compile_start_us;\n \n   device_compiler_internal::LogOnceXlaCompiledFirstCluster();\n   TF_RETURN_IF_ERROR(profiler->RegisterCompilation(\n@@ -496,7 +496,7 @@ absl::Status DeviceCompiler<ExecutableType, ClientType>::CompileImpl(\n \n   profiler->RegisterExecution(function);\n \n-  string human_signature;\n+  std::string human_signature;\n   if (VLOG_IS_ON(2)) {\n     human_signature = VLOG_IS_ON(3) ? signature.HumanString() : function.name();\n     VLOG(2) << \"DeviceCompilationClusterSignature: \" << human_signature;"
        },
        {
            "sha": "749110be186311170ff7a082a70793bbd5ec2160",
            "filename": "tensorflow/compiler/jit/device_compiler_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_compiler_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -139,7 +139,7 @@ class MockXlaDeviceExecutablePersistor\n             Config{testing::TmpDir(), false, \"xla\"},\n             DeviceType(DEVICE_CPU_XLA_JIT)) {}\n   MOCK_METHOD(absl::Status, TryToPersistExecutable,\n-              (uint64, const std::string&, const XlaCompiler::Options&,\n+              (uint64_t, const std::string&, const XlaCompiler::Options&,\n                const XlaCompiler::CompilationResult&,\n                const xla::LocalExecutable&,\n                (DeviceCompilerClient<xla::LocalExecutable, xla::LocalClient>*)),\n@@ -425,7 +425,7 @@ TEST_F(DeviceCompilerTest, CompileFailedToLoadFromPersistentCache) {\n       &xla_executable));\n \n   // Corrupt the file which contains the serialized executable.\n-  std::vector<string> files;\n+  std::vector<std::string> files;\n   TF_ASSERT_OK(Env::Default()->GetChildren(testing::TmpDir(), &files));\n   std::string const* serialized_executable_filename = nullptr;\n   for (const auto& file : files) {"
        },
        {
            "sha": "33bba30f3db3e19667b2ca9404e6968aaa3e1710",
            "filename": "tensorflow/compiler/jit/device_context_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_context_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_context_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_context_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -38,7 +38,7 @@ static bool Initialized = [] {\n \n class DeviceContextTest : public ::testing::Test {\n  public:\n-  void SetDevice(const string& device_type) {\n+  void SetDevice(const std::string& device_type) {\n     auto& rollout_config = GetXlaOpsCommonFlags()->tf_xla_use_device_api;\n     rollout_config.AllowForDeviceInXlaLaunch(DeviceType(device_type));\n     rollout_config.AllowForDeviceInXlaCompileOnDemand(DeviceType(device_type));"
        },
        {
            "sha": "5a64b078e1a93c2baa4bae6a368a691f1b0ef20b",
            "filename": "tensorflow/compiler/jit/device_executable_persistor.h",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_executable_persistor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_executable_persistor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_executable_persistor.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -96,7 +96,7 @@ class DeviceExecutablePersistor {\n   // TODO(b/255826209): Take in Signature instead of hash and string once cache\n   // is refactored.\n   std::optional<StatusOr<std::unique_ptr<ExecutableType>>> TryToLoadExecutable(\n-      uint64 signature_hash, const std::string& signature_str,\n+      uint64_t signature_hash, const std::string& signature_str,\n       const XlaCompiler::Options& options,\n       const XlaCompiler::CompilationResult& compilation_result,\n       DeviceCompilerClient<ExecutableType, ClientType>* client) const;\n@@ -107,7 +107,7 @@ class DeviceExecutablePersistor {\n   // TODO(b/255826209): Take in Signature instead hash and string once cache\n   // is refactored.\n   virtual absl::Status TryToPersistExecutable(\n-      uint64 signature_hash, const std::string& signature_str,\n+      uint64_t signature_hash, const std::string& signature_str,\n       const XlaCompiler::Options& options,\n       const XlaCompiler::CompilationResult& compilation_result,\n       const ExecutableType& executable,\n@@ -123,15 +123,15 @@ class DeviceExecutablePersistor {\n   // Returns a cache key proto that identifies an entry in the compilation\n   // cache.\n   XlaSerializedCacheKey BuildSerializedCacheKey(\n-      uint64 signature_hash, const xla::HloModuleProto& hlo_module) const;\n+      uint64_t signature_hash, const xla::HloModuleProto& hlo_module) const;\n \n   XlaSerializedCacheKey BuildSerializedCacheKey(\n-      uint64 signature_hash, const xla::HloModuleProto& hlo_module,\n+      uint64_t signature_hash, const xla::HloModuleProto& hlo_module,\n       bool compiled_using_pjrt) const;\n \n   // Serializes the signature and its corresponding entry to a proto message.\n   absl::StatusOr<XlaSerializedCacheEntry> SerializeEntry(\n-      uint64 signature_hash, const XlaCompiler::Options& options,\n+      uint64_t signature_hash, const XlaCompiler::Options& options,\n       const XlaCompiler::CompilationResult& compilation_result,\n       const ExecutableType& executable,\n       DeviceCompilerClient<ExecutableType, ClientType>* compiler_client) const;\n@@ -189,7 +189,7 @@ std::string DeviceExecutablePersistor<ExecutableType, ClientType>::GetFilePath(\n template <typename ExecutableType, typename ClientType>\n XlaSerializedCacheKey\n DeviceExecutablePersistor<ExecutableType, ClientType>::BuildSerializedCacheKey(\n-    uint64 signature_hash, const xla::HloModuleProto& hlo_module,\n+    uint64_t signature_hash, const xla::HloModuleProto& hlo_module,\n     bool compiled_using_pjrt) const {\n   XlaSerializedCacheKey key;\n   key.set_signature_fingerprint(signature_hash);\n@@ -203,7 +203,7 @@ DeviceExecutablePersistor<ExecutableType, ClientType>::BuildSerializedCacheKey(\n template <typename ExecutableType, typename ClientType>\n XlaSerializedCacheKey\n DeviceExecutablePersistor<ExecutableType, ClientType>::BuildSerializedCacheKey(\n-    uint64 signature_hash, const xla::HloModuleProto& hlo_module) const {\n+    uint64_t signature_hash, const xla::HloModuleProto& hlo_module) const {\n   return BuildSerializedCacheKey(signature_hash, hlo_module, false);\n }\n \n@@ -212,7 +212,7 @@ DeviceExecutablePersistor<ExecutableType, ClientType>::BuildSerializedCacheKey(\n template <>\n inline XlaSerializedCacheKey\n DeviceExecutablePersistor<xla::PjRtLoadedExecutable, xla::PjRtClient>::\n-    BuildSerializedCacheKey(uint64 signature_hash,\n+    BuildSerializedCacheKey(uint64_t signature_hash,\n                             const xla::HloModuleProto& hlo_module) const {\n   return BuildSerializedCacheKey(signature_hash, hlo_module, true);\n }\n@@ -305,7 +305,7 @@ DeviceExecutablePersistor<ExecutableType, ClientType>::SaveSerializedEntry(\n template <typename ExecutableType, typename ClientType>\n absl::StatusOr<XlaSerializedCacheEntry>\n DeviceExecutablePersistor<ExecutableType, ClientType>::SerializeEntry(\n-    uint64 signature_hash, const XlaCompiler::Options& options,\n+    uint64_t signature_hash, const XlaCompiler::Options& options,\n     const XlaCompiler::CompilationResult& compilation_result,\n     const ExecutableType& executable,\n     DeviceCompilerClient<ExecutableType, ClientType>* compiler_client) const {\n@@ -340,7 +340,7 @@ DeviceExecutablePersistor<ExecutableType, ClientType>::SerializeEntry(\n template <typename ExecutableType, typename ClientType>\n std::optional<StatusOr<std::unique_ptr<ExecutableType>>>\n DeviceExecutablePersistor<ExecutableType, ClientType>::TryToLoadExecutable(\n-    uint64 signature_hash, const std::string& signature_str,\n+    uint64_t signature_hash, const std::string& signature_str,\n     const XlaCompiler::Options& options,\n     const XlaCompiler::CompilationResult& compilation_result,\n     DeviceCompilerClient<ExecutableType, ClientType>* compiler_client) const {\n@@ -376,7 +376,7 @@ DeviceExecutablePersistor<ExecutableType, ClientType>::TryToLoadExecutable(\n template <typename ExecutableType, typename ClientType>\n absl::Status\n DeviceExecutablePersistor<ExecutableType, ClientType>::TryToPersistExecutable(\n-    uint64 signature_hash, const std::string& signature_str,\n+    uint64_t signature_hash, const std::string& signature_str,\n     const XlaCompiler::Options& options,\n     const XlaCompiler::CompilationResult& compilation_result,\n     const ExecutableType& executable,"
        },
        {
            "sha": "62cfd4c1b8e0b791230d0fe7c0b3a33058097723",
            "filename": "tensorflow/compiler/jit/device_executable_persistor_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_executable_persistor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_executable_persistor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_executable_persistor_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -222,7 +222,7 @@ absl::StatusOr<XlaSerializedCacheEntry> ReadCacheEntryFromFile(\n }\n \n XlaSerializedCacheKey CreateCacheKey(\n-    uint64 signature_hash,\n+    uint64_t signature_hash,\n     const XlaCompiler::CompilationResult& compilation_result,\n     const DeviceType& device_type, const std::string& persistence_prefix,\n     bool compiled_using_pjrt = false) {"
        },
        {
            "sha": "1979aec5bcf0c3c7eda600f9d6c294267731f699",
            "filename": "tensorflow/compiler/jit/device_util.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_util.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -44,7 +44,7 @@ void DeviceSet::UnionWith(const DeviceSet& other) {\n }\n \n bool DeviceSet::IsEmpty() const {\n-  return absl::c_all_of(storage_, [&](uint64 val) { return val == 0; });\n+  return absl::c_all_of(storage_, [&](uint64_t val) { return val == 0; });\n }\n \n absl::StatusOr<DeviceId> DeviceInfoCache::GetIdFor(absl::string_view name) {\n@@ -56,15 +56,15 @@ absl::StatusOr<DeviceId> DeviceInfoCache::GetIdFor(absl::string_view name) {\n   }\n \n   int new_id = names_.size();\n-  names_.push_back(string(name));\n+  names_.push_back(std::string(name));\n   id_to_device_type_.push_back(std::make_unique<DeviceType>(\"\"));\n   DeviceType* device_type = id_to_device_type_.back().get();\n   TF_RETURN_IF_ERROR(DeviceNameToDeviceType(names_.back(), device_type));\n \n   is_cpu_.push_back(device_type->type_string() == DEVICE_CPU);\n   is_gpu_.push_back(device_type->type_string() == DEVICE_GPU);\n \n-  name_to_id_.emplace(string(name), DeviceId(new_id));\n+  name_to_id_.emplace(std::string(name), DeviceId(new_id));\n \n   const XlaOpRegistry::DeviceRegistration* compilation_device;\n   if (!XlaOpRegistry::GetCompilationDevice(device_type->type(),\n@@ -76,18 +76,18 @@ absl::StatusOr<DeviceId> DeviceInfoCache::GetIdFor(absl::string_view name) {\n   return DeviceId(new_id);\n }\n \n-string DeviceInfoCache::DebugString(const DeviceSet& device_set) const {\n-  std::vector<string> names;\n+std::string DeviceInfoCache::DebugString(const DeviceSet& device_set) const {\n+  std::vector<std::string> names;\n   device_set.ForEach([&](DeviceId device_id) {\n-    names.push_back(string(GetNameFor(device_id)));\n+    names.push_back(std::string(GetNameFor(device_id)));\n     return true;\n   });\n \n   return absl::StrCat(\"[\", absl::StrJoin(names, \",\"), \"]\");\n }\n }  // namespace jit\n \n-absl::Status DeviceNameToDeviceType(const string& device,\n+absl::Status DeviceNameToDeviceType(const std::string& device,\n                                     DeviceType* device_type) {\n   DeviceNameUtils::ParsedName parsed;\n   if (!DeviceNameUtils::ParseFullName(device, &parsed)) {"
        },
        {
            "sha": "fa862aac88c3942dd13fc92e5be365cc1c9e6888",
            "filename": "tensorflow/compiler/jit/device_util.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_util.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -75,9 +75,9 @@ class DeviceSet {\n     // iterator if this ends up being used widely.\n     for (int word_index = 0, end = storage_.size(); word_index < end;\n          word_index++) {\n-      uint64 word = storage_[word_index];\n+      uint64_t word = storage_[word_index];\n       while (word != 0) {\n-        uint64 only_lowest_bit_set = word & -word;\n+        uint64_t only_lowest_bit_set = word & -word;\n         // The number of trailing zeros in a non-zero word is the index of the\n         // least significant 1.\n         int bit_index = absl::countr_zero(word);\n@@ -90,7 +90,7 @@ class DeviceSet {\n   }\n \n  private:\n-  absl::InlinedVector<uint64, 1> storage_;\n+  absl::InlinedVector<uint64_t, 1> storage_;\n \n   const int kWordSize = 64;\n };\n@@ -131,25 +131,25 @@ class DeviceInfoCache {\n     return std::cref(*id_to_device_type_[device_id.id()]);\n   }\n \n-  string DebugString(const DeviceSet& device_set) const;\n+  std::string DebugString(const DeviceSet& device_set) const;\n \n  private:\n-  absl::flat_hash_map<string, DeviceId> name_to_id_;\n+  absl::flat_hash_map<std::string, DeviceId> name_to_id_;\n \n   // These fields are populated for a device in GetIdFor, *before* we give out a\n   // DeviceId.\n   std::vector<const XlaOpRegistry::DeviceRegistration*>\n       id_to_compilation_device_;\n   std::vector<std::unique_ptr<DeviceType>> id_to_device_type_;\n-  std::vector<string> names_;\n+  std::vector<std::string> names_;\n   std::vector<bool> is_cpu_;\n   std::vector<bool> is_gpu_;\n };\n \n }  // namespace jit\n \n // Returns the DeviceType corresponding to 'device'.\n-absl::Status DeviceNameToDeviceType(const string& device,\n+absl::Status DeviceNameToDeviceType(const std::string& device,\n                                     DeviceType* device_type);\n \n // Picks the device for which XLA should compile a cluster that contains"
        },
        {
            "sha": "be58292f9316866723f408dbf21d16e3e685be5d",
            "filename": "tensorflow/compiler/jit/device_util_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fdevice_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fdevice_util_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -23,7 +23,7 @@ namespace {\n \n absl::Status PickDeviceHelper(bool allow_mixing_unknown_and_cpu,\n                               absl::Span<const absl::string_view> device_names,\n-                              string* result) {\n+                              std::string* result) {\n   jit::DeviceInfoCache cache;\n   jit::DeviceSet device_set;\n   for (absl::string_view name : device_names) {\n@@ -34,14 +34,14 @@ absl::Status PickDeviceHelper(bool allow_mixing_unknown_and_cpu,\n   TF_ASSIGN_OR_RETURN(\n       jit::DeviceId result_id,\n       PickDeviceForXla(cache, device_set, allow_mixing_unknown_and_cpu));\n-  *result = string(cache.GetNameFor(result_id));\n+  *result = std::string(cache.GetNameFor(result_id));\n   return absl::OkStatus();\n }\n \n void CheckPickDeviceResult(absl::string_view expected_result,\n                            bool allow_mixing_unknown_and_cpu,\n                            absl::Span<const absl::string_view> inputs) {\n-  string result;\n+  std::string result;\n   TF_ASSERT_OK(PickDeviceHelper(allow_mixing_unknown_and_cpu, inputs, &result))\n       << \"inputs = [\" << absl::StrJoin(inputs, \", \")\n       << \"], allow_mixing_unknown_and_cpu=\" << allow_mixing_unknown_and_cpu\n@@ -51,7 +51,7 @@ void CheckPickDeviceResult(absl::string_view expected_result,\n \n void CheckPickDeviceHasError(bool allow_mixing_unknown_and_cpu,\n                              absl::Span<const absl::string_view> inputs) {\n-  string result;\n+  std::string result;\n   EXPECT_FALSE(\n       PickDeviceHelper(allow_mixing_unknown_and_cpu, inputs, &result).ok());\n }\n@@ -110,10 +110,10 @@ void SimpleRoundTripTestForDeviceSet(int num_devices) {\n   jit::DeviceSet device_set;\n   jit::DeviceInfoCache device_info_cache;\n \n-  std::vector<string> expected_devices, actual_devices;\n+  std::vector<std::string> expected_devices, actual_devices;\n \n   for (int i = 0; i < num_devices; i++) {\n-    string device_name =\n+    std::string device_name =\n         absl::StrCat(\"/job:localhost/replica:0/task:0/device:XPU:\", i);\n     TF_ASSERT_OK_AND_ASSIGN(jit::DeviceId device_id,\n                             device_info_cache.GetIdFor(device_name));\n@@ -122,7 +122,8 @@ void SimpleRoundTripTestForDeviceSet(int num_devices) {\n   }\n \n   device_set.ForEach([&](jit::DeviceId device_id) {\n-    actual_devices.push_back(string(device_info_cache.GetNameFor(device_id)));\n+    actual_devices.push_back(\n+        std::string(device_info_cache.GetNameFor(device_id)));\n     return true;\n   });\n "
        },
        {
            "sha": "6e7d16de16a4f63002c42e7d1dd60c86888052ff",
            "filename": "tensorflow/compiler/jit/encapsulate_subgraphs_pass.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 47,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -115,7 +115,7 @@ void MarkGuaranteedConstants(\n }\n \n struct OutputInputTensorPairHasher {\n-  uint64 operator()(std::pair<OutputTensor, InputTensor> const& s) const {\n+  uint64_t operator()(std::pair<OutputTensor, InputTensor> const& s) const {\n     return Hash64Combine(OutputTensor::Hash()(s.first),\n                          InputTensor::Hash()(s.second));\n   }\n@@ -128,7 +128,7 @@ static const char* const kRetValOp = \"_Retval\";\n \n class Encapsulator {\n  public:\n-  Encapsulator(string group_attribute, Graph const* graph_in)\n+  Encapsulator(std::string group_attribute, Graph const* graph_in)\n       : group_attribute_(std::move(group_attribute)), graph_in_(graph_in) {}\n \n   // Find subgraphs marked with 'group_attribute', and build a new\n@@ -182,7 +182,7 @@ class Encapsulator {\n     // 'reuse_existing_functions' is set, use an existing function with the same\n     // name, if any.  If 'rewrite_subgraph_fn' is set, it is applied to the\n     // subgraph before function conversion.\n-    absl::Status BuildFunctionDef(const string& name_in,\n+    absl::Status BuildFunctionDef(const std::string& name_in,\n                                   const RewriteSubgraphFn& rewrite_subgraph_fn,\n                                   bool reuse_existing_functions,\n                                   FunctionLibraryDefinition* library);\n@@ -226,7 +226,7 @@ class Encapsulator {\n         const absl::flat_hash_map<const Node*, Node*>& node_images);\n \n     // Creates the sequencer node if it doesn't exist, adding it to graph_out.\n-    absl::Status MakeSequencingNode(const string& subgraph_name,\n+    absl::Status MakeSequencingNode(const std::string& subgraph_name,\n                                     Graph* graph_out);\n \n     // If there is a sequencer node, adds a control edge from the sequencer to\n@@ -243,14 +243,14 @@ class Encapsulator {\n \n     // Which device are these nodes on? Used to assign a device to the call\n     // node.\n-    string device_;\n+    std::string device_;\n \n     // NodeDef for the function call node.\n     NodeDef call_node_def_;\n \n     // Name that is used for the call node. This may not be\n     // call_node_def_.name() if the client supplies a rewrite lambda.\n-    string function_def_name_;\n+    std::string function_def_name_;\n \n     // Placeholder node simulating the host compute key in the output graph.\n     // Not owned.\n@@ -275,15 +275,15 @@ class Encapsulator {\n     // Set of node names that are the source of a control output of the\n     // subgraph. We store strings here so that we can tolerate nodes being\n     // removed from the graph.\n-    absl::flat_hash_set<string> control_output_nodes_;\n+    absl::flat_hash_set<std::string> control_output_nodes_;\n \n     // NoOp node in the output graph that is sequenced after the call node.\n     Node* sequencer_ = nullptr;\n   };\n \n   // Returns the key attribute associated with a node in attr. Sets either\n   // result to the empty string if the respective attribute is not found.\n-  absl::Status GetFunctionNameAttr(Node const* node, string* attr) const;\n+  absl::Status GetFunctionNameAttr(Node const* node, std::string* attr) const;\n \n   // Copies edges local to a subgraph. Adds _Arg and _Retval nodes to\n   // subgraphs for data edges that cross subgraph boundaries.\n@@ -308,36 +308,35 @@ class Encapsulator {\n   // a subgraph boundary it is the output of a call node, otherwise it is a node\n   // in the output graph.\n   absl::Status FindOutputImageOfEdgeSrc(\n-      const string& src_func_id, const string& dst_func_id,\n+      const std::string& src_func_id, const std::string& dst_func_id,\n       const absl::flat_hash_map<const Node*, Node*>& node_images,\n       const Node* original_src_node, Node** src_image);\n \n   // Finds an edge source slot in the output graph. If the edge crosses a\n   // subgraph boundary it is a slot on the output of a call node, otherwise it\n   // is a slot on a node in the output graph.\n-  int FindOutputSlotOfEdgeSrc(const string& src_func_id,\n-                              const string& dst_func_id,\n-                              const Edge* edge);\n+  int FindOutputSlotOfEdgeSrc(const std::string& src_func_id,\n+                              const std::string& dst_func_id, const Edge* edge);\n \n   // Finds the image of an edge destination in the output graph. If the edge\n   // crosses a subgraph boundary it is the input of a call node, otherwise it is\n   // a node in the output graph.\n   absl::Status FindOutputImageOfEdgeDst(\n-      const string& src_func_id, const string& dst_func_id,\n+      const std::string& src_func_id, const std::string& dst_func_id,\n       const absl::flat_hash_map<const Node*, Node*>& node_images,\n       const Node* original_dst_node, Node** dst_image);\n \n   // Finds an edge destination slot in the output graph. If the edge crosses a\n   // subgraph boundary it is a slot on the input of a call node, otherwise it is\n   // a slot on a node in the output graph.\n-  int FindOutputSlotOfEdgeDst(const string& src_func_id,\n-                              const string& dst_func_id,\n-                              const Edge* edge);\n+  int FindOutputSlotOfEdgeDst(const std::string& src_func_id,\n+                              const std::string& dst_func_id, const Edge* edge);\n \n   // Copies a single edge to the output graph. The edge is either entirely\n   // within the output graph, or crosses into or out of a compiled subgraph.\n   absl::Status CopyEdgeToOutputGraph(\n-      const Edge* edge, const string& src_func_id, const string& dst_func_id,\n+      const Edge* edge, const std::string& src_func_id,\n+      const std::string& dst_func_id,\n       const absl::flat_hash_map<const Node*, Node*>& node_images,\n       Graph* graph_out,\n       absl::flat_hash_set<std::pair<OutputTensor, InputTensor>,\n@@ -358,10 +357,10 @@ class Encapsulator {\n       absl::flat_hash_map<const Node*, Node*>* node_images,\n       FunctionLibraryDefinition* library);\n \n-  const string group_attribute_;\n+  const std::string group_attribute_;\n   const Graph* graph_in_;\n \n-  absl::flat_hash_map<string, Subgraph> subgraphs_;\n+  absl::flat_hash_map<std::string, Subgraph> subgraphs_;\n \n   Encapsulator(const Encapsulator&) = delete;\n   void operator=(const Encapsulator&) = delete;\n@@ -374,19 +373,20 @@ namespace {\n // including clusters that are not present in the ancestors map. has_successors\n // is the set of clusters that are ancestors of some other cluster.\n void TopologicalClusterSort(\n-    const absl::flat_hash_set<string>& clusters,\n-    const absl::flat_hash_set<string>& has_successors,\n-    const absl::flat_hash_map<string, absl::flat_hash_set<string>>& ancestors,\n-    std::vector<string>* sorted) {\n+    const absl::flat_hash_set<std::string>& clusters,\n+    const absl::flat_hash_set<std::string>& has_successors,\n+    const absl::flat_hash_map<std::string, absl::flat_hash_set<std::string>>&\n+        ancestors,\n+    std::vector<std::string>* sorted) {\n   // The nodes are placed in 'sorted' in topological order.\n   sorted->clear();\n   // We don't use the standard DFS because we are not operating on Node*\n   // objects.\n   struct Work {\n-    string cluster;\n+    std::string cluster;\n     bool leave;\n   };\n-  std::set<string> visited;\n+  std::set<std::string> visited;\n   std::vector<Work> stack;\n   // Seed the processing list with clusters that have no successors.\n   for (const auto& cluster : clusters) {\n@@ -523,7 +523,7 @@ absl::Status Encapsulator::Subgraph::RecordResult(\n }\n \n absl::Status Encapsulator::Subgraph::MakeSequencingNode(\n-    const string& subgraph_name, Graph* graph_out) {\n+    const std::string& subgraph_name, Graph* graph_out) {\n   if (sequencer_ == nullptr) {\n     NodeDef seq_def;\n     // TODO(shikharagarwal): What source node should we use for errors?\n@@ -547,11 +547,11 @@ void Encapsulator::Subgraph::ConnectSequencerToCallNode(Graph* graph_out) {\n }\n \n absl::Status Encapsulator::Subgraph::BuildFunctionDef(\n-    const string& name_in, const RewriteSubgraphFn& rewrite_subgraph_fn,\n+    const std::string& name_in, const RewriteSubgraphFn& rewrite_subgraph_fn,\n     bool reuse_existing_functions, FunctionLibraryDefinition* library) {\n   // name_in is copied here because name may be modified below if\n   // rewrite_subgraph_fn is true.\n-  string name = name_in;\n+  std::string name = name_in;\n   call_node_def_.set_op(name);\n   call_node_def_.set_name(name);\n   call_node_def_.set_device(device_);\n@@ -596,7 +596,7 @@ absl::Status Encapsulator::Subgraph::BuildFunctionDef(\n   function_def_name_ = name;\n \n   FunctionDef fdef;\n-  auto lookup = [this](const Node* node) -> std::optional<string> {\n+  auto lookup = [this](const Node* node) -> std::optional<std::string> {\n     if (control_output_nodes_.contains(node->name())) {\n       return std::make_optional(node->name());\n     }\n@@ -625,7 +625,7 @@ absl::Status Encapsulator::Subgraph::BuildFunctionDef(\n \n absl::Status Encapsulator::Subgraph::ReplaceFunctionDef(\n     FunctionLibraryDefinition* library) {\n-  const string& name = function_def_name_;\n+  const std::string& name = function_def_name_;\n \n   FunctionDef fdef;\n   TF_RETURN_IF_ERROR(GraphToFunctionDef(*graph_, name, &fdef));\n@@ -654,7 +654,7 @@ absl::Status Encapsulator::Subgraph::AddFunctionCallNode(\n }\n \n absl::Status Encapsulator::GetFunctionNameAttr(Node const* node,\n-                                               string* attr) const {\n+                                               std::string* attr) const {\n   AttrSlice attrs = node->attrs();\n   attr->clear();\n   for (const auto& node_attr : attrs) {\n@@ -667,12 +667,12 @@ absl::Status Encapsulator::GetFunctionNameAttr(Node const* node,\n   return absl::OkStatus();\n }\n \n-bool IsInSubgraph(const string& func_id) { return !func_id.empty(); }\n+bool IsInSubgraph(const std::string& func_id) { return !func_id.empty(); }\n \n absl::Status Encapsulator::CopySubgraphNodes(\n     absl::flat_hash_map<const Node*, Node*>* node_images) {\n   for (Node* node : graph_in_->op_nodes()) {\n-    string func_id;\n+    std::string func_id;\n     TF_RETURN_IF_ERROR(GetFunctionNameAttr(node, &func_id));\n     if (!IsInSubgraph(func_id)) continue;\n \n@@ -688,9 +688,9 @@ absl::Status Encapsulator::CopySubgraphEdges(\n     const absl::flat_hash_map<const Node*, Node*>& node_images,\n     std::vector<std::pair<const Node*, Node*>>* src_arg_pairs) {\n   for (const Edge* edge : graph_in_->edges()) {\n-    string src_func_id;\n+    std::string src_func_id;\n     TF_RETURN_IF_ERROR(GetFunctionNameAttr(edge->src(), &src_func_id));\n-    string dst_func_id;\n+    std::string dst_func_id;\n     TF_RETURN_IF_ERROR(GetFunctionNameAttr(edge->dst(), &dst_func_id));\n     Node* src_image = gtl::FindWithDefault(node_images, edge->src(), nullptr);\n     Node* dst_image = gtl::FindWithDefault(node_images, edge->dst(), nullptr);\n@@ -793,7 +793,7 @@ absl::Status Encapsulator::BuildFunctionDefs(\n     const RewriteSubgraphFn& rewrite_subgraph_fn, bool reuse_existing_functions,\n     FunctionLibraryDefinition* library) {\n   for (auto& subgraph_entry : subgraphs_) {\n-    string name = subgraph_entry.first;\n+    std::string name = subgraph_entry.first;\n     Subgraph& subgraph = subgraph_entry.second;\n     TF_RETURN_IF_ERROR(subgraph.BuildFunctionDef(\n         name, rewrite_subgraph_fn, reuse_existing_functions, library));\n@@ -804,7 +804,7 @@ absl::Status Encapsulator::BuildFunctionDefs(\n absl::Status Encapsulator::CopyNodesToOutputGraph(\n     Graph* graph_out, absl::flat_hash_map<const Node*, Node*>* node_images) {\n   for (Node* node : graph_in_->op_nodes()) {\n-    string func_id;\n+    std::string func_id;\n     TF_RETURN_IF_ERROR(GetFunctionNameAttr(node, &func_id));\n \n     // Don't copy nodes that are going to be encapsulated.\n@@ -829,7 +829,7 @@ absl::Status Encapsulator::AddFunctionCallNodes(\n }\n \n absl::Status Encapsulator::FindOutputImageOfEdgeSrc(\n-    const string& src_func_id, const string& dst_func_id,\n+    const std::string& src_func_id, const std::string& dst_func_id,\n     const absl::flat_hash_map<const Node*, Node*>& node_images,\n     const Node* original_src_node, Node** src_image) {\n   if (IsInSubgraph(src_func_id)) {\n@@ -844,8 +844,8 @@ absl::Status Encapsulator::FindOutputImageOfEdgeSrc(\n   return absl::OkStatus();\n }\n \n-int Encapsulator::FindOutputSlotOfEdgeSrc(const string& src_func_id,\n-                                          const string& dst_func_id,\n+int Encapsulator::FindOutputSlotOfEdgeSrc(const std::string& src_func_id,\n+                                          const std::string& dst_func_id,\n                                           const Edge* edge) {\n   if (IsInSubgraph(src_func_id)) {\n     const Subgraph& src_subgraph = subgraphs_.at(src_func_id);\n@@ -860,7 +860,7 @@ int Encapsulator::FindOutputSlotOfEdgeSrc(const string& src_func_id,\n }\n \n absl::Status Encapsulator::FindOutputImageOfEdgeDst(\n-    const string& src_func_id, const string& dst_func_id,\n+    const std::string& src_func_id, const std::string& dst_func_id,\n     const absl::flat_hash_map<const Node*, Node*>& node_images,\n     const Node* original_dst_node, Node** dst_image) {\n   if (IsInSubgraph(dst_func_id)) {\n@@ -875,8 +875,8 @@ absl::Status Encapsulator::FindOutputImageOfEdgeDst(\n   return absl::OkStatus();\n }\n \n-int Encapsulator::FindOutputSlotOfEdgeDst(const string& src_func_id,\n-                                          const string& dst_func_id,\n+int Encapsulator::FindOutputSlotOfEdgeDst(const std::string& src_func_id,\n+                                          const std::string& dst_func_id,\n                                           const Edge* edge) {\n   if (IsInSubgraph(dst_func_id)) {\n     const Subgraph& dst_subgraph = subgraphs_.at(dst_func_id);\n@@ -891,7 +891,8 @@ int Encapsulator::FindOutputSlotOfEdgeDst(const string& src_func_id,\n }\n \n absl::Status Encapsulator::CopyEdgeToOutputGraph(\n-    const Edge* edge, const string& src_func_id, const string& dst_func_id,\n+    const Edge* edge, const std::string& src_func_id,\n+    const std::string& dst_func_id,\n     const absl::flat_hash_map<const Node*, Node*>& node_images,\n     Graph* graph_out,\n     absl::flat_hash_set<std::pair<OutputTensor, InputTensor>,\n@@ -943,9 +944,9 @@ absl::Status Encapsulator::AddEdgesToOutputGraph(\n       edges_added;\n \n   for (const Edge* edge : graph_in_->edges()) {\n-    string src_func_id;\n+    std::string src_func_id;\n     TF_RETURN_IF_ERROR(GetFunctionNameAttr(edge->src(), &src_func_id));\n-    string dst_func_id;\n+    std::string dst_func_id;\n     TF_RETURN_IF_ERROR(GetFunctionNameAttr(edge->dst(), &dst_func_id));\n \n     // Ignore edges that are strictly contained within one subgraph, unless\n@@ -1091,7 +1092,7 @@ absl::Status Encapsulator::BuildOutputGraph(\n }  // anonymous namespace\n \n absl::Status EncapsulateSubgraphsInFunctions(\n-    string group_attribute, const Graph& graph_in,\n+    std::string group_attribute, const Graph& graph_in,\n     const RewriteSubgraphFn& rewrite_subgraph_fn, bool reuse_existing_functions,\n     std::unique_ptr<Graph>* graph_out, FunctionLibraryDefinition* library) {\n   Encapsulator encapsulator(std::move(group_attribute),"
        },
        {
            "sha": "ed2c9ef45a2c16bdc364dac0317d370a88066ae3",
            "filename": "tensorflow/compiler/jit/encapsulate_subgraphs_pass.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -73,7 +73,7 @@ typedef std::function<absl::Status(\n // dep from B. Originally D must run after C, post-transformation this\n // dependency is lost.\n absl::Status EncapsulateSubgraphsInFunctions(\n-    string group_attribute, const Graph& graph_in,\n+    std::string group_attribute, const Graph& graph_in,\n     const RewriteSubgraphFn& rewrite_subgraph_fn, bool reuse_existing_functions,\n     std::unique_ptr<Graph>* graph_out, FunctionLibraryDefinition* library);\n "
        },
        {
            "sha": "94b136a02b99cfe96c7133870fb1dd69b7b33ae1",
            "filename": "tensorflow/compiler/jit/encapsulate_subgraphs_pass_test.cc",
            "status": "modified",
            "additions": 123,
            "deletions": 114,
            "changes": 237,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_subgraphs_pass_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -46,7 +46,7 @@ const char* const kXlaHostTransferSequencerAttr =\n     \"_xla_host_transfer_sequencer\";\n \n absl::Status AddGraphDefToFunctionLibrary(\n-    const GraphDefBuilder& graphdef_builder, const string& name_suffix,\n+    const GraphDefBuilder& graphdef_builder, const std::string& name_suffix,\n     FunctionDefLibrary* library) {\n   GraphDef graphdef;\n   TF_RETURN_IF_ERROR(graphdef_builder.ToGraphDef(&graphdef));\n@@ -64,13 +64,14 @@ absl::Status AddGraphDefToFunctionLibrary(\n }\n \n template <class Tkey, class Tvalue>\n-bool EqualProtoMap(const ::tensorflow::protobuf::Map<Tkey, Tvalue>& a,\n-                   const ::tensorflow::protobuf::Map<Tkey, Tvalue>& b,\n-                   const std::function<string(const Tkey&)>& key_to_string,\n-                   const std::function<string(const Tvalue&)>& value_to_string,\n-                   const std::function<bool(const Tkey&, const Tvalue&,\n-                                            const Tvalue&)>& compare,\n-                   const string& map_name, string* diff) {\n+bool EqualProtoMap(\n+    const ::tensorflow::protobuf::Map<Tkey, Tvalue>& a,\n+    const ::tensorflow::protobuf::Map<Tkey, Tvalue>& b,\n+    const std::function<std::string(const Tkey&)>& key_to_string,\n+    const std::function<std::string(const Tvalue&)>& value_to_string,\n+    const std::function<bool(const Tkey&, const Tvalue&, const Tvalue&)>&\n+        compare,\n+    const std::string& map_name, std::string* diff) {\n   for (const auto& elt_a : a) {\n     const auto iter = b.find(elt_a.first);\n     if (iter == b.end()) {\n@@ -106,7 +107,7 @@ bool EqualProtoMap(const ::tensorflow::protobuf::Map<Tkey, Tvalue>& a,\n }\n \n bool EqualFunctionNodeDef(const NodeDef& a, const NodeDef& b,\n-                          const string& diff_preamble, string* diff) {\n+                          const std::string& diff_preamble, std::string* diff) {\n   if (a.op() != b.op()) {\n     if (diff) {\n       *diff = absl::StrCat(diff_preamble, \" mismatch for node \", a.name(),\n@@ -131,8 +132,8 @@ bool EqualFunctionNodeDef(const NodeDef& a, const NodeDef& b,\n     }\n     return false;\n   }\n-  std::unordered_set<string> control_input_a;\n-  std::unordered_set<string> control_input_b;\n+  std::unordered_set<std::string> control_input_a;\n+  std::unordered_set<std::string> control_input_b;\n   for (int i = 0; i < a.input_size(); ++i) {\n     if (absl::StartsWith(a.input(i), \"^\")) {\n       if (!absl::StartsWith(b.input(i), \"^\")) {\n@@ -164,17 +165,17 @@ bool EqualFunctionNodeDef(const NodeDef& a, const NodeDef& b,\n     }\n     return false;\n   }\n-  return EqualProtoMap<string, AttrValue>(\n-      a.attr(), b.attr(), [](const string& s) { return s; },\n+  return EqualProtoMap<std::string, AttrValue>(\n+      a.attr(), b.attr(), [](const std::string& s) { return s; },\n       [](const AttrValue& v) { return v.DebugString(); },\n-      [](const string& key, const AttrValue& av, const AttrValue& bv) {\n+      [](const std::string& key, const AttrValue& av, const AttrValue& bv) {\n         if (key == \"ancestors\") {\n           // The ancestors are added from a set so the order is unpredictable;\n           // just compare set equality not list equality.\n-          std::unordered_set<string> a_set(av.list().s().begin(),\n-                                           av.list().s().end());\n-          std::unordered_set<string> b_set(bv.list().s().begin(),\n-                                           bv.list().s().end());\n+          std::unordered_set<std::string> a_set(av.list().s().begin(),\n+                                                av.list().s().end());\n+          std::unordered_set<std::string> b_set(bv.list().s().begin(),\n+                                                bv.list().s().end());\n           return a_set == b_set;\n         } else {\n           return av.DebugString() == bv.DebugString();\n@@ -184,7 +185,7 @@ bool EqualFunctionNodeDef(const NodeDef& a, const NodeDef& b,\n }\n \n bool EqualFunctionDef(const FunctionDef& a, const FunctionDef& b,\n-                      string* diff) {\n+                      std::string* diff) {\n   if (a.signature().DebugString() != b.signature().DebugString()) {\n     if (diff) {\n       *diff =\n@@ -194,22 +195,21 @@ bool EqualFunctionDef(const FunctionDef& a, const FunctionDef& b,\n     }\n     return false;\n   }\n-  if (!EqualProtoMap<string, AttrValue>(\n-          a.attr(), b.attr(), [](const string& s) { return s; },\n+  if (!EqualProtoMap<std::string, AttrValue>(\n+          a.attr(), b.attr(), [](const std::string& s) { return s; },\n           [](const AttrValue& v) { return v.DebugString(); },\n-          [](const string& key, const AttrValue& av, const AttrValue& bv) {\n+          [](const std::string& key, const AttrValue& av, const AttrValue& bv) {\n             return av.DebugString() == bv.DebugString();\n           },\n           absl::StrCat(\"attr mismatch for function \", a.signature().name()),\n           diff)) {\n     return false;\n   }\n-  if (!EqualProtoMap<string, string>(\n-          a.ret(), b.ret(), [](const string& s) { return s; },\n-          [](const string& s) { return s; },\n-          [](const string& key, const string& av, const string& bv) {\n-            return av == bv;\n-          },\n+  if (!EqualProtoMap<std::string, std::string>(\n+          a.ret(), b.ret(), [](const std::string& s) { return s; },\n+          [](const std::string& s) { return s; },\n+          [](const std::string& key, const std::string& av,\n+             const std::string& bv) { return av == bv; },\n           absl::StrCat(\"ret mismatch for function \", a.signature().name()),\n           diff)) {\n     return false;\n@@ -257,8 +257,9 @@ bool EqualFunctionDef(const FunctionDef& a, const FunctionDef& b,\n }\n \n bool EqualFunctionDefLibrary(const FunctionDefLibrary& expected,\n-                             const FunctionDefLibrary& actual, string* diff) {\n-  std::unordered_map<string, const FunctionDef*> actual_index;\n+                             const FunctionDefLibrary& actual,\n+                             std::string* diff) {\n+  std::unordered_map<std::string, const FunctionDef*> actual_index;\n   for (const FunctionDef& function : actual.function()) {\n     actual_index[function.signature().name()] = &function;\n   }\n@@ -343,7 +344,7 @@ REGISTER_OP(\"AddNLikeTest\")\n     .SetIsAggregate();\n \n Node* Sequencer(const GraphDefBuilder::Options& opts,\n-                const string& call_node_name) {\n+                const std::string& call_node_name) {\n   if (opts.HaveError()) return nullptr;\n   NodeBuilder node_builder(opts.GetNameForOp(\"NoOp\"), \"NoOp\",\n                            opts.op_registry());\n@@ -383,7 +384,7 @@ Node* KeyPlaceholderShape(const GraphDefBuilder::Options& opts) {\n   return KnownShapeBase(DT_STRING, {2}, opts);\n }\n \n-Node* KeyPlaceholder(const string& call_node,\n+Node* KeyPlaceholder(const std::string& call_node,\n                      const GraphDefBuilder::Options& opts) {\n   if (opts.HaveError()) return nullptr;\n   NodeBuilder node_builder(absl::StrCat(call_node, \"_key_placeholder\"),\n@@ -396,15 +397,16 @@ Node* KeyPlaceholder(const string& call_node,\n       .FinalizeBuilder(&node_builder);\n }\n \n-Node* RecvAtHost(ops::NodeOut key_input, const string& cluster,\n-                 const string& new_func_name, const string& oc_cluster,\n+Node* RecvAtHost(ops::NodeOut key_input, const std::string& cluster,\n+                 const std::string& new_func_name,\n+                 const std::string& oc_cluster,\n                  absl::Span<const DataType> dtypes,\n                  const GraphDefBuilder::Options& opts) {\n   if (opts.HaveError()) return nullptr;\n-  string key = absl::StrCat(\"host_compute_channel_\", cluster, \"_\",\n-                            new_func_name, \"_\", oc_cluster);\n-  string name = absl::StrCat(\"outside_compilation_\", cluster, \"_\",\n-                             new_func_name, \"_\", oc_cluster, \"_recv\");\n+  std::string key = absl::StrCat(\"host_compute_channel_\", cluster, \"_\",\n+                                 new_func_name, \"_\", oc_cluster);\n+  std::string name = absl::StrCat(\"outside_compilation_\", cluster, \"_\",\n+                                  new_func_name, \"_\", oc_cluster, \"_recv\");\n   NodeBuilder node_builder(opts.WithName(name).GetNameForOp(\"_XlaRecvAtHost\"),\n                            \"_XlaRecvAtHost\", opts.op_registry());\n   node_builder.Input(std::move(key_input));\n@@ -416,15 +418,16 @@ Node* RecvAtHost(ops::NodeOut key_input, const string& cluster,\n       .FinalizeBuilder(&node_builder);\n }\n \n-Node* SendFromHost(ops::NodeOut key_input, const string& cluster,\n-                   const string& new_func_name, const string& oc_cluster,\n+Node* SendFromHost(ops::NodeOut key_input, const std::string& cluster,\n+                   const std::string& new_func_name,\n+                   const std::string& oc_cluster,\n                    const std::vector<ops::NodeOut>& inputs,\n                    const GraphDefBuilder::Options& opts) {\n   if (opts.HaveError()) return nullptr;\n-  string key = absl::StrCat(\"host_compute_channel_\", cluster, \"_\",\n-                            new_func_name, \"_\", oc_cluster);\n-  string name = absl::StrCat(\"outside_compilation_\", cluster, \"_\",\n-                             new_func_name, \"_\", oc_cluster, \"_send\");\n+  std::string key = absl::StrCat(\"host_compute_channel_\", cluster, \"_\",\n+                                 new_func_name, \"_\", oc_cluster);\n+  std::string name = absl::StrCat(\"outside_compilation_\", cluster, \"_\",\n+                                  new_func_name, \"_\", oc_cluster, \"_send\");\n   NodeBuilder node_builder(opts.WithName(name).GetNameForOp(\"_XlaSendFromHost\"),\n                            \"_XlaSendFromHost\", opts.op_registry());\n   node_builder.Input(inputs);\n@@ -477,8 +480,9 @@ Node* RetOp(int index, ops::NodeOut a, const GraphDefBuilder::Options& opts) {\n   return opts.FinalizeBuilder(&node_builder);\n }\n \n-absl::Status Encapsulate(GraphDef* graphdef, FunctionDefLibrary* library,\n-                         const std::vector<string>& encapsulated_functions) {\n+absl::Status Encapsulate(\n+    GraphDef* graphdef, FunctionDefLibrary* library,\n+    const std::vector<std::string>& encapsulated_functions) {\n   absl::Status s;\n   // Convert the GraphDef to a Graph\n   std::unique_ptr<FunctionLibraryDefinition> lib_def(\n@@ -512,7 +516,7 @@ absl::Status Encapsulate(GraphDef* graphdef, FunctionDefLibrary* library,\n                                       &graph_out, lib_def.get());\n   if (!s.ok()) return s;\n \n-  std::unordered_map<string, XlaClusterInfo> clusters;\n+  std::unordered_map<std::string, XlaClusterInfo> clusters;\n   for (const auto& func : encapsulated_functions) {\n     Node* xla_computation_node;\n     for (Node* n : graph_out->nodes()) {\n@@ -527,7 +531,7 @@ absl::Status Encapsulate(GraphDef* graphdef, FunctionDefLibrary* library,\n     func_name_attrs.set_name(func);\n     clusters.emplace(func,\n                      XlaClusterInfo{func, func_name_attrs, xla_computation_node,\n-                                    std::map<string, int>{}});\n+                                    std::map<std::string, int>{}});\n   }\n   bool modified;\n   s = ExtractOutsideCompilation(\"_encapsulate\", \"_outside\", clusters,\n@@ -551,7 +555,7 @@ absl::Status Encapsulate(GraphDef* graphdef, FunctionDefLibrary* library,\n }\n \n absl::Status Encapsulate(GraphDef* graphdef, FunctionDefLibrary* library) {\n-  std::vector<string> encapsulated_functions;\n+  std::vector<std::string> encapsulated_functions;\n   return Encapsulate(graphdef, library, encapsulated_functions);\n }\n \n@@ -698,8 +702,8 @@ TEST(EncapsulateSubgraphsTest, TwoFunctions) {\n }\n \n // Returns a vector of node names in 'graph', sorted by name.\n-std::vector<string> GraphNodes(const Graph& graph) {\n-  std::vector<string> nodes;\n+std::vector<std::string> GraphNodes(const Graph& graph) {\n+  std::vector<std::string> nodes;\n   for (const auto& node : graph.nodes()) {\n     if (!node->IsSource() && !node->IsSink()) {\n       nodes.push_back(node->name());\n@@ -710,8 +714,9 @@ std::vector<string> GraphNodes(const Graph& graph) {\n }\n \n // Returns a sorted vector of (src, dst) edges in 'graph'.\n-std::vector<std::pair<string, string>> GraphEdges(const Graph& graph) {\n-  std::vector<std::pair<string, string>> edges;\n+std::vector<std::pair<std::string, std::string>> GraphEdges(\n+    const Graph& graph) {\n+  std::vector<std::pair<std::string, std::string>> edges;\n   for (const Edge* edge : graph.edges()) {\n     if (edge->src()->IsSource() || edge->dst()->IsSink()) continue;\n     edges.emplace_back(\n@@ -742,18 +747,19 @@ TEST(EncapsulateSubgraphsTest, InputDeduplication) {\n       /*rewrite_subgraph_fn=*/{},\n       /*reuse_existing_functions=*/false, &graph, &library));\n \n-  std::vector<string> expected_nodes = {\"cluster1\", \"cluster2\", \"mul\", \"x\"};\n+  std::vector<std::string> expected_nodes = {\"cluster1\", \"cluster2\", \"mul\",\n+                                             \"x\"};\n   EXPECT_EQ(expected_nodes, GraphNodes(*graph));\n \n-  std::vector<std::pair<string, string>> expected_edges = {\n+  std::vector<std::pair<std::string, std::string>> expected_edges = {\n       {\"cluster1:0\", \"cluster2:0\"},\n       {\"cluster1:0\", \"mul:0\"},\n       {\"cluster2:0\", \"mul:1\"},\n       {\"x:0\", \"cluster1:0\"}};\n   EXPECT_EQ(expected_edges, GraphEdges(*graph));\n }\n \n-const Node* FindNodeByName(const Graph& graph, const string& name) {\n+const Node* FindNodeByName(const Graph& graph, const std::string& name) {\n   for (const Node* node : graph.nodes()) {\n     if (node->name() == name) return node;\n   }\n@@ -889,7 +895,7 @@ TEST(EncapsulateSubgraphsTest, OneFunctionOneOutside) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -931,7 +937,7 @@ TEST(EncapsulateSubgraphsTest, OneFunctionOneOutside) {\n            {\"C:o:0\", \"c:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -941,7 +947,7 @@ TEST(EncapsulateSubgraphsTest, OneFunctionOneOutside) {\n             {\"shapes\", absl::Span<const DataType>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}},\n            {\"c\"}},\n@@ -1025,7 +1031,7 @@ TEST(EncapsulateSubgraphsTest, OneFunctionTwoOutside) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -1102,7 +1108,7 @@ TEST(EncapsulateSubgraphsTest, OneFunctionTwoOutside) {\n            {\"F:o:0\", \"D:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O2\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1112,8 +1118,9 @@ TEST(EncapsulateSubgraphsTest, OneFunctionTwoOutside) {\n             {\"shapes\", absl::Span<const DataType>({})},\n             {\"_outside_compilation_subgraph\", \"O2\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\",\n-                                       \"outside_compilation_O1_host_compute\"})},\n+             absl::Span<const std::string>(\n+                 {\"_xla_token_arg_node\",\n+                  \"outside_compilation_O1_host_compute\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O2_host_compute\"}},\n            {\"F\", \"outside_compilation_O1_host_compute\"}},\n@@ -1122,7 +1129,7 @@ TEST(EncapsulateSubgraphsTest, OneFunctionTwoOutside) {\n            {\"C:o:0\", \"D:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1132,7 +1139,7 @@ TEST(EncapsulateSubgraphsTest, OneFunctionTwoOutside) {\n             {\"shapes\", absl::Span<const DataType>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}},\n            {\"D\"}},\n@@ -1235,7 +1242,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutside) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\", \"F2\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\", \"F2\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -1262,7 +1269,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutside) {\n            {\"C:o:0\", \"D:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1273,7 +1280,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutside) {\n              absl::Span<const TensorShapeProto>({shape_proto_expected})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}},\n            {\"D\"}},\n@@ -1295,7 +1302,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutside) {\n            {\"d_0_arg\", \"G:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F2_F2_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1306,7 +1313,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutside) {\n              absl::Span<const TensorShapeProto>({shape_proto_expected})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n       },\n@@ -1409,7 +1416,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutsideDependencyFromOutside) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\", \"F2\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\", \"F2\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -1432,7 +1439,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutsideDependencyFromOutside) {\n            {\"C:o:0\", \"D:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1443,7 +1450,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutsideDependencyFromOutside) {\n              absl::Span<const TensorShapeProto>({shape_proto_expected})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}},\n            {\"D\"}},\n@@ -1462,7 +1469,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutsideDependencyFromOutside) {\n            {\"G:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F2_F2_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1473,7 +1480,7 @@ TEST(EncapsulateSubgraphsTest, TwoFunctionsTwoOutsideDependencyFromOutside) {\n              absl::Span<const TensorShapeProto>({shape_proto_expected})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n       },\n@@ -1556,7 +1563,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoInputs) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -1578,7 +1585,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoInputs) {\n            {\"a_0_arg\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1589,7 +1596,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoInputs) {\n              absl::Span<const TensorShapeProto>({shape_proto_expected})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n       },\n@@ -1652,7 +1659,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationControlInput) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -1674,7 +1681,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationControlInput) {\n            {\"a_0_arg\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1685,7 +1692,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationControlInput) {\n              absl::Span<const TensorShapeProto>({shape_proto_expected})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}},\n            {\"D\"}},\n@@ -1748,7 +1755,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoOutputs) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -1785,7 +1792,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoOutputs) {\n            {\"D:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1795,7 +1802,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoOutputs) {\n             {\"shapes\", absl::Span<const TensorShapeProto>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n       },\n@@ -1858,7 +1865,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationControlOutput) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -1899,7 +1906,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationControlOutput) {\n            {\"D:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -1909,7 +1916,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationControlOutput) {\n             {\"shapes\", absl::Span<const TensorShapeProto>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n       },\n@@ -1978,7 +1985,7 @@ TEST(EncapsulateSubgraphsTest,\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -2037,7 +2044,7 @@ TEST(EncapsulateSubgraphsTest,\n            {\"a_0_arg\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -2047,15 +2054,15 @@ TEST(EncapsulateSubgraphsTest,\n             {\"shapes\", absl::Span<const TensorShapeProto>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n           {{\"outside_compilation_O2_host_compute\"},\n            \"XlaHostCompute\",\n            {\"F:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O2\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -2065,8 +2072,9 @@ TEST(EncapsulateSubgraphsTest,\n             {\"shapes\", absl::Span<const TensorShapeProto>({})},\n             {\"_outside_compilation_subgraph\", \"O2\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\",\n-                                       \"outside_compilation_O1_host_compute\"})},\n+             absl::Span<const std::string>(\n+                 {\"_xla_token_arg_node\",\n+                  \"outside_compilation_O1_host_compute\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O2_host_compute\"}},\n            {\"outside_compilation_O1_host_compute\"}},\n@@ -2149,7 +2157,7 @@ TEST(EncapsulateSubgraphsTest,\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -2189,7 +2197,7 @@ TEST(EncapsulateSubgraphsTest,\n            {\"a_0_arg\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O2\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -2199,8 +2207,9 @@ TEST(EncapsulateSubgraphsTest,\n             {\"shapes\", absl::Span<const TensorShapeProto>({})},\n             {\"_outside_compilation_subgraph\", \"O2\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\",\n-                                       \"outside_compilation_O1_host_compute\"})},\n+             absl::Span<const std::string>(\n+                 {\"_xla_token_arg_node\",\n+                  \"outside_compilation_O1_host_compute\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O2_host_compute\"}},\n            {\"outside_compilation_O1_host_compute\"}},\n@@ -2209,7 +2218,7 @@ TEST(EncapsulateSubgraphsTest,\n            {\"D:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -2219,7 +2228,7 @@ TEST(EncapsulateSubgraphsTest,\n             {\"shapes\", absl::Span<const TensorShapeProto>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n       },\n@@ -2303,7 +2312,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationClusterDependency) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -2340,7 +2349,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationClusterDependency) {\n         {\"D:o:0\"},\n         {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n          {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-         {\"ancestors\", absl::Span<const string>({})},\n+         {\"ancestors\", absl::Span<const std::string>({})},\n          {\"key\", \"host_compute_channel_F1_F1_O1\"},\n          {\"send_key\", \"\"},\n          {\"recv_key\", \"\"},\n@@ -2350,15 +2359,15 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationClusterDependency) {\n          {\"shapes\", absl::Span<const TensorShapeProto>({})},\n          {\"_outside_compilation_subgraph\", \"O1\"},\n          {\"_xla_token_input_nodes\",\n-          absl::Span<const string>({\"_xla_token_arg_node\"})},\n+          absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n          {\"_xla_original_oc_node_name\",\n           \"outside_compilation_O1_host_compute\"}}},\n        {{\"outside_compilation_O2_host_compute\"},\n         \"XlaHostCompute\",\n         {\"D:o:0\"},\n         {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n          {\"Toutputs\", absl::Span<const DataType>({})},\n-         {\"ancestors\", absl::Span<const string>({})},\n+         {\"ancestors\", absl::Span<const std::string>({})},\n          {\"key\", \"host_compute_channel_F1_F1_O2\"},\n          {\"send_key\", \"\"},\n          {\"recv_key\", \"\"},\n@@ -2368,7 +2377,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationClusterDependency) {\n          {\"shapes\", absl::Span<const TensorShapeProto>({})},\n          {\"_outside_compilation_subgraph\", \"O2\"},\n          {\"_xla_token_input_nodes\",\n-          absl::Span<const string>(\n+          absl::Span<const std::string>(\n               {\"_xla_token_arg_node\", \"outside_compilation_O1_host_compute\"})},\n          {\"_xla_original_oc_node_name\", \"outside_compilation_O2_host_compute\"}},\n         {\"outside_compilation_O1_host_compute\"}},\n@@ -2377,7 +2386,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationClusterDependency) {\n         {\"D:o:0\"},\n         {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n          {\"Toutputs\", absl::Span<const DataType>({})},\n-         {\"ancestors\", absl::Span<const string>({})},\n+         {\"ancestors\", absl::Span<const std::string>({})},\n          {\"key\", \"host_compute_channel_F1_F1_O3\"},\n          {\"send_key\", \"\"},\n          {\"recv_key\", \"\"},\n@@ -2387,9 +2396,9 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationClusterDependency) {\n          {\"shapes\", absl::Span<const TensorShapeProto>({})},\n          {\"_outside_compilation_subgraph\", \"O3\"},\n          {\"_xla_token_input_nodes\",\n-          absl::Span<const string>({\"_xla_token_arg_node\",\n-                                    \"outside_compilation_O1_host_compute\",\n-                                    \"outside_compilation_O2_host_compute\"})},\n+          absl::Span<const std::string>(\n+              {\"_xla_token_arg_node\", \"outside_compilation_O1_host_compute\",\n+               \"outside_compilation_O2_host_compute\"})},\n          {\"_xla_original_oc_node_name\", \"outside_compilation_O3_host_compute\"}},\n         {\"outside_compilation_O1_host_compute\",\n          \"outside_compilation_O2_host_compute\"}}},\n@@ -2470,7 +2479,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoInputsOrOutputs) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -2507,7 +2516,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoInputsOrOutputs) {\n            {\"a_0_arg\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -2517,7 +2526,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationNoInputsOrOutputs) {\n             {\"shapes\", absl::Span<const TensorShapeProto>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}}},\n       },\n@@ -2586,7 +2595,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationShapeInference) {\n     TF_EXPECT_OK(b1.ToGraphDef(&graphdef));\n   }\n \n-  std::vector<string> encapsulated_functions{\"F1\"};\n+  std::vector<std::string> encapsulated_functions{\"F1\"};\n   TF_EXPECT_OK(Encapsulate(&graphdef, &library, encapsulated_functions));\n \n   FunctionDefLibrary library_expected;\n@@ -2627,7 +2636,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationShapeInference) {\n            {\"c_0_arg\", \"c:o:0\"},\n            {{\"Tinputs\", absl::Span<const DataType>({DT_FLOAT, DT_FLOAT})},\n             {\"Toutputs\", absl::Span<const DataType>({DT_FLOAT})},\n-            {\"ancestors\", absl::Span<const string>({})},\n+            {\"ancestors\", absl::Span<const std::string>({})},\n             {\"key\", \"host_compute_channel_F1_F1_O1\"},\n             {\"send_key\", \"\"},\n             {\"recv_key\", \"\"},\n@@ -2637,7 +2646,7 @@ TEST(EncapsulateSubgraphsTest, OutsideCompilationShapeInference) {\n             {\"shapes\", absl::Span<const DataType>({})},\n             {\"_outside_compilation_subgraph\", \"O1\"},\n             {\"_xla_token_input_nodes\",\n-             absl::Span<const string>({\"_xla_token_arg_node\"})},\n+             absl::Span<const std::string>({\"_xla_token_arg_node\"})},\n             {\"_xla_original_oc_node_name\",\n              \"outside_compilation_O1_host_compute\"}},\n            {\"c\"}},"
        },
        {
            "sha": "445ca63c05ad664c9dd7e2e9ed0182df0ae5482e",
            "filename": "tensorflow/compiler/jit/encapsulate_util.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 24,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_util.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -36,7 +36,8 @@ namespace {\n \n // Returns string attribute value for the node if the attribute is present,\n // otherwise returns empty optional value.\n-std::optional<string> GetStringAttr(const Node& n, const string& attr_name) {\n+std::optional<std::string> GetStringAttr(const Node& n,\n+                                         const std::string& attr_name) {\n   auto attr = n.attrs().Find(attr_name);\n   if (!attr) {\n     return std::nullopt;\n@@ -47,8 +48,8 @@ std::optional<string> GetStringAttr(const Node& n, const string& attr_name) {\n \n // Adds a value to the node's list attribute.\n template <typename T>\n-absl::Status AppendToListAttr(Node* n, const string& attr_name,\n-                              const string& value) {\n+absl::Status AppendToListAttr(Node* n, const std::string& attr_name,\n+                              const std::string& value) {\n   std::vector<T> attr_value;\n   absl::Status s = GetNodeAttr(n->attrs(), attr_name, &attr_value);\n   if (!s.ok() && s.code() != error::NOT_FOUND) {\n@@ -63,15 +64,15 @@ absl::Status AppendToListAttr(Node* n, const string& attr_name,\n \n // Replaces attribute value.\n template <typename T>\n-void ReplaceAttr(Node* n, const string& attr_name, const T& value) {\n+void ReplaceAttr(Node* n, const std::string& attr_name, const T& value) {\n   n->ClearAttr(attr_name);\n   n->AddAttr(attr_name, value);\n }\n \n // Step 1 for `PreprocessEdgesBetweenOutsideCompilations`. See comments of\n // `PreprocessEdgesBetweenOutsideCompilations` for details.\n absl::Status PreprocessControlEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name) {\n+    Graph* g, const std::string& outside_compilation_attr_name) {\n   // Gather edges to remove. We should not remove the edge while iterating.\n   std::vector<const Edge*> edges_to_remove;\n   for (const Edge* e : g->edges()) {\n@@ -89,7 +90,7 @@ absl::Status PreprocessControlEdgesBetweenOutsideCompilations(\n         // Case 1a: outside compilation to outside compilation control edge.\n         edges_to_remove.push_back(e);\n \n-        TF_RETURN_IF_ERROR(AppendToListAttr<string>(\n+        TF_RETURN_IF_ERROR(AppendToListAttr<std::string>(\n             e->dst(), kXlaControlDependenciesWithinXlaClusterAttrName,\n             e->src()->name()));\n       }\n@@ -111,7 +112,7 @@ absl::Status PreprocessControlEdgesBetweenOutsideCompilations(\n // Step 2 for `PreprocessEdgesBetweenOutsideCompilations`. See comments of\n // `PreprocessEdgesBetweenOutsideCompilations` for details.\n absl::Status PreprocessDataEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name) {\n+    Graph* g, const std::string& outside_compilation_attr_name) {\n   // Gather edges between outside compilation and host computation. Notice that\n   // we do not store `Edge*` directly because we remove some nodes while adding\n   // Identity nodes, and those Edge pointers might be invalidated.\n@@ -138,7 +139,7 @@ absl::Status PreprocessDataEdgesBetweenOutsideCompilations(\n \n   // Remove the edge from host to outside compilation. Add a placeholder as\n   // outside compilation node input.\n-  std::map<std::pair<string, int>, Node*> placeholders;\n+  std::map<std::pair<std::string, int>, Node*> placeholders;\n   for (int i = 0, end = edges.size(); i < end; i++) {\n     Node* dst = g->FindNodeId(edges[i].dst_node_id);\n     const Edge* e;\n@@ -148,15 +149,15 @@ absl::Status PreprocessDataEdgesBetweenOutsideCompilations(\n     g->RemoveEdge(e);\n \n     // Find or create placeholder node.\n-    string new_name =\n+    std::string new_name =\n         absl::StrCat(src->name(), \"_oc_to_oc_placeholder_\", src_output);\n     auto placeholder_index = std::make_pair(src->name(), src_output);\n     auto iter = placeholders.find(placeholder_index);\n     Node* placeholder_node;\n     if (iter == placeholders.end()) {\n       NodeDefBuilder placeholder_builder(new_name, \"Placeholder\");\n       placeholder_builder.Attr(\"dtype\", src->output_type(src_output));\n-      string outside_compilation_attr;\n+      std::string outside_compilation_attr;\n       TF_RETURN_IF_ERROR(GetNodeAttr(dst->attrs(),\n                                      outside_compilation_attr_name,\n                                      &outside_compilation_attr));\n@@ -195,7 +196,7 @@ absl::Status PreprocessDataEdgesBetweenOutsideCompilations(\n // Step 1 for `PostprocessEdgesBetweenOutsideCompilations`. See comments of\n // `PostprocessEdgesBetweenOutsideCompilations` for details.\n absl::Status PostprocessDataEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name) {\n+    Graph* g, const std::string& outside_compilation_attr_name) {\n   // Gather all outside compilation to outside compilation nodes.\n   std::vector<Node*> placeholder_nodes;\n   for (Node* n : g->nodes()) {\n@@ -208,7 +209,7 @@ absl::Status PostprocessDataEdgesBetweenOutsideCompilations(\n   // Remove the placeholder nodes, and reconnect original edge.\n   auto node_name_index = g->BuildNodeNameIndex();\n   for (auto n : placeholder_nodes) {\n-    string node_name;\n+    std::string node_name;\n     int node_src_output;\n     TF_RETURN_IF_ERROR(GetNodeAttr(\n         n->attrs(), kOutsideCompilationOriginalNodeAttrName, &node_name));\n@@ -271,12 +272,12 @@ absl::Status PostprocessDataEdgesBetweenOutsideCompilations(\n // Step 2 for `PostprocessEdgesBetweenOutsideCompilations`. See comments of\n // `PostprocessEdgesBetweenOutsideCompilations` for details.\n absl::Status PostprocessControlEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name) {\n+    Graph* g, const std::string& outside_compilation_attr_name) {\n   auto node_name_index = g->BuildNodeNameIndex();\n \n   // Reconnect outside compilation to outside compilation control edge.\n   for (Node* n : g->nodes()) {\n-    std::vector<string> control_deps;\n+    std::vector<std::string> control_deps;\n     absl::Status s =\n         GetNodeAttr(n->attrs(), kXlaControlDependenciesWithinXlaClusterAttrName,\n                     &control_deps);\n@@ -288,7 +289,7 @@ absl::Status PostprocessControlEdgesBetweenOutsideCompilations(\n       }\n     } else {\n       n->ClearAttr(kXlaControlDependenciesWithinXlaClusterAttrName);\n-      for (const string& control_input : control_deps) {\n+      for (const std::string& control_input : control_deps) {\n         auto iter = node_name_index.find(control_input);\n         if (iter == node_name_index.end()) {\n           return errors::Internal(\"Cannot find original node for \",\n@@ -342,11 +343,11 @@ absl::Status PerformStaticShapeInferenceBeforeEncapsulation(Graph* g) {\n }\n \n absl::StatusOr<\n-    std::unique_ptr<absl::flat_hash_map<string, std::vector<string>>>>\n+    std::unique_ptr<absl::flat_hash_map<std::string, std::vector<std::string>>>>\n OutsideCompilationClusterDependencies(\n-    const Graph* g, const string& outside_compilation_attr_name) {\n+    const Graph* g, const std::string& outside_compilation_attr_name) {\n   auto cluster_deps = std::make_unique<\n-      absl::flat_hash_map<string, absl::flat_hash_set<string>>>();\n+      absl::flat_hash_map<std::string, absl::flat_hash_set<std::string>>>();\n \n   for (const Edge* e : g->edges()) {\n     auto src_outside_compilation =\n@@ -360,18 +361,18 @@ OutsideCompilationClusterDependencies(\n       if (dst_deps_it == cluster_deps->end()) {\n         cluster_deps->insert(std::make_pair(\n             *dst_outside_compilation,\n-            absl::flat_hash_set<string>({*src_outside_compilation})));\n+            absl::flat_hash_set<std::string>({*src_outside_compilation})));\n       } else {\n         dst_deps_it->second.insert(*src_outside_compilation);\n       }\n     }\n   }\n \n-  auto cluster_deps_ordered =\n-      std::make_unique<absl::flat_hash_map<string, std::vector<string>>>();\n+  auto cluster_deps_ordered = std::make_unique<\n+      absl::flat_hash_map<std::string, std::vector<std::string>>>();\n \n   for (auto it = cluster_deps->begin(); it != cluster_deps->end(); it++) {\n-    std::vector<string> ordered_deps(it->second.begin(), it->second.end());\n+    std::vector<std::string> ordered_deps(it->second.begin(), it->second.end());\n     std::sort(ordered_deps.begin(), ordered_deps.end());\n     cluster_deps_ordered->insert(std::make_pair(it->first, ordered_deps));\n   }\n@@ -380,7 +381,7 @@ OutsideCompilationClusterDependencies(\n }\n \n absl::Status PreprocessEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name) {\n+    Graph* g, const std::string& outside_compilation_attr_name) {\n   // Remove edges from source node to outside compilation nodes, and edges\n   // from outside compilation nodes to sink node.\n   std::vector<const Edge*> edges_to_remove;\n@@ -406,7 +407,7 @@ absl::Status PreprocessEdgesBetweenOutsideCompilations(\n }\n \n absl::Status PostprocessEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name) {\n+    Graph* g, const std::string& outside_compilation_attr_name) {\n   TF_RETURN_IF_ERROR(PostprocessDataEdgesBetweenOutsideCompilations(\n       g, outside_compilation_attr_name));\n   TF_RETURN_IF_ERROR(PostprocessControlEdgesBetweenOutsideCompilations("
        },
        {
            "sha": "81ab31c79dcda23824d4f176a1135c230b0e63a9",
            "filename": "tensorflow/compiler/jit/encapsulate_util.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_util.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -95,31 +95,31 @@ struct XlaClusterInfo {\n   // without losing aggregate initialization, which allows us to get rid of\n   // the constructor definitions again.\n   XlaClusterInfo() {}\n-  XlaClusterInfo(const string& cluster_name,\n+  XlaClusterInfo(const std::string& cluster_name,\n                  const NameAttrList& func_name_attrs, Node* node,\n-                 const std::map<string, int>& host_compute_core)\n+                 const std::map<std::string, int>& host_compute_core)\n       : cluster_name(cluster_name),\n         func_name_attrs(func_name_attrs),\n         node(node),\n         host_compute_core(host_compute_core) {}\n   // XLA cluster name. It might be different from `func_name`.\n-  const string cluster_name;\n+  const std::string cluster_name;\n   // Name and attributes of XLA computation function.\n   const NameAttrList func_name_attrs;\n   // The XLA computation node in the graph.\n   Node* node;\n   // A mapping from outside compilation cluster name to its device assignment.\n-  const std::map<string, int> host_compute_core;\n+  const std::map<std::string, int> host_compute_core;\n };\n \n // Finds dependencies between outside compilation clusters, including both data\n // dependencies and control dependencies. cluster_deps maps the name name of an\n // outside compilation cluster to a set of names of outside compilation clusters\n // that it depends on.\n absl::StatusOr<\n-    std::unique_ptr<absl::flat_hash_map<string, std::vector<string>>>>\n+    std::unique_ptr<absl::flat_hash_map<std::string, std::vector<std::string>>>>\n OutsideCompilationClusterDependencies(\n-    const Graph* g, const string& outside_compilation_attr_name);\n+    const Graph* g, const std::string& outside_compilation_attr_name);\n \n // Preprocesses edges within the same XLA cluster. It will perform the following\n // operations in order:\n@@ -135,7 +135,7 @@ OutsideCompilationClusterDependencies(\n // 2.  For data edges between different outside compilations, remove the edge\n //     and create a Placeholder node as dst node's input.\n absl::Status PreprocessEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name);\n+    Graph* g, const std::string& outside_compilation_attr_name);\n \n // Postprocesses edges within the same XLA cluster. This function reverts what\n // `PreprocessEdgesBetweenOutsideCompilations` did. It will perform the\n@@ -149,7 +149,7 @@ absl::Status PreprocessEdgesBetweenOutsideCompilations(\n // `PreprocessEdgesBetweenOutsideCompilations` step 1b are not handled here.\n // They are handled in `RewriteOutsideCompilationSubgraphFn`.\n absl::Status PostprocessEdgesBetweenOutsideCompilations(\n-    Graph* g, const string& outside_compilation_attr_name);\n+    Graph* g, const std::string& outside_compilation_attr_name);\n }  // namespace tensorflow\n \n #endif  // TENSORFLOW_COMPILER_JIT_ENCAPSULATE_UTIL_H_"
        },
        {
            "sha": "8ba114040103633bd424c6aad07ca323ccd7c7db",
            "filename": "tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_xla_computations_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_xla_computations_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_xla_computations_pass.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -46,7 +46,7 @@ const char* const kXlaClusterOutput = \"XlaClusterOutput\";\n \n bool IsCpuGpuCompile(const Graph* graph) {\n   for (Node* n : graph->nodes()) {\n-    string name;\n+    std::string name;\n     // Only consider nodes being compiled.\n     if (!TryGetNodeAttr(n->attrs(), kXlaClusterIdAttr, &name)) continue;\n     // Early return for any node with a device that is not a CPU or GPU.\n@@ -185,7 +185,7 @@ absl::Status RewriteSubgraph(\n   // Uniquify the function name by computing a fingerprint of the function.\n   // Nondeterminism in serialization would not lead to incorrect results, but\n   // may cause spurious cache misses.\n-  TF_ASSIGN_OR_RETURN(uint64 fingerprint, FingerprintGraph(*graph));\n+  TF_ASSIGN_OR_RETURN(uint64_t fingerprint, FingerprintGraph(*graph));\n   VLOG(1) << \"Subgraph fingerprint:\" << fingerprint;\n   call_def->set_op(absl::StrCat(call_def->op(), \"_\", fingerprint));\n   return absl::OkStatus();\n@@ -360,7 +360,8 @@ absl::Status RewriteSubgraph(\n /*static*/ absl::Status EncapsulateXlaComputationsPass::BuildXlaLaunchOps(\n     Graph* graph) {\n   const auto is_xla_launch_node = [](const Node& node) -> absl::StatusOr<bool> {\n-    const string& name = GetNodeAttrString(node.attrs(), kXlaClusterIdAttr);\n+    const std::string& name =\n+        GetNodeAttrString(node.attrs(), kXlaClusterIdAttr);\n     return !name.empty();\n   };\n "
        },
        {
            "sha": "acd5319cf8ed16379f408863aaafdd708ce74dd4",
            "filename": "tensorflow/compiler/jit/encapsulate_xla_computations_pass_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_xla_computations_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_xla_computations_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fencapsulate_xla_computations_pass_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -34,7 +34,7 @@ limitations under the License.\n namespace tensorflow {\n \n static std::unique_ptr<Graph> MakeOuterGraph(\n-    const FunctionLibraryDefinition& flib_def, const string& function) {\n+    const FunctionLibraryDefinition& flib_def, const std::string& function) {\n   Scope scope = Scope::NewRootScope().ExitOnError();\n   TF_EXPECT_OK(scope.graph()->AddFunctionLibrary(flib_def.ToProto()));\n \n@@ -143,7 +143,7 @@ TEST(EncapsulateXlaComputations, DeterministicEncapsulate) {\n   // Test that control edge insertion order doesn't affect the cache key\n   // (cluster name) generated by TPU encapsulate pass.\n   auto get_serialized_graph = [](bool control_input_reversed,\n-                                 bool operand_reversed) -> string {\n+                                 bool operand_reversed) -> std::string {\n     FunctionLibraryDefinition flib_def(OpRegistry::Global(),\n                                        FunctionDefLibrary());\n     std::unique_ptr<Graph> graph(new Graph(&flib_def));\n@@ -250,8 +250,8 @@ TEST(EncapsulateXlaComputations, Encapsulate) {\n \n   TF_ASSERT_OK(EncapsulateXlaComputationsPass::Encapsulate(&graph, &flib_def));\n \n-  std::unordered_map<string, Node*> index = graph->BuildNodeNameIndex();\n-  string function = index.at(\"launch0\")->type_string();\n+  std::unordered_map<std::string, Node*> index = graph->BuildNodeNameIndex();\n+  std::string function = index.at(\"launch0\")->type_string();\n \n   // Tests the outer graph is as expected.\n   {\n@@ -285,9 +285,9 @@ TEST(EncapsulateXlaComputations, Encapsulate) {\n   // function. Encapsulation should be deterministic to avoid recompilation.\n   TF_ASSERT_OK(\n       EncapsulateXlaComputationsPass::Encapsulate(&graph_copy, &flib_def));\n-  std::unordered_map<string, Node*> index_copy =\n+  std::unordered_map<std::string, Node*> index_copy =\n       graph_copy->BuildNodeNameIndex();\n-  string function_copy = index_copy.at(\"launch0\")->type_string();\n+  std::string function_copy = index_copy.at(\"launch0\")->type_string();\n   EXPECT_EQ(function, function_copy);\n }\n "
        },
        {
            "sha": "05514f00bd29d5f6be90cbbee8a3c7788f169224",
            "filename": "tensorflow/compiler/jit/extract_outside_compilation_pass.cc",
            "status": "modified",
            "additions": 179,
            "deletions": 147,
            "changes": 326,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -42,7 +42,7 @@ namespace {\n \n // Control return mapping function for outside compilation host graphs.\n // All nodes with kXlaHasHostTransfer attribute are control outputs.\n-std::optional<string> HostGraphControlRetMapping(const Node* n) {\n+std::optional<std::string> HostGraphControlRetMapping(const Node* n) {\n   if (HasNodeAttr(n->def(), kXlaHasHostTransferAttrName)) {\n     return n->name();\n   }\n@@ -52,7 +52,7 @@ std::optional<string> HostGraphControlRetMapping(const Node* n) {\n // Add a key placeholder node to the graph. The key placeholder node will be\n // used as input for XlaRecvAtHost/XlaSendFromHost nodes.\n absl::StatusOr<Node*> AddHostComputeKeyPlaceholder(\n-    const string& xla_cluster_name, Graph* g) {\n+    const std::string& xla_cluster_name, Graph* g) {\n   NodeDef key_def;\n   NodeDefBuilder builder(absl::StrCat(xla_cluster_name, \"_key_placeholder\"),\n                          \"Placeholder\");\n@@ -74,7 +74,8 @@ bool IsKeyPlaceholderNode(const Node& n) {\n }\n \n // Returns nodes with given type.\n-std::vector<Node*> GatherNodesWithType(const Graph& g, const string& type) {\n+std::vector<Node*> GatherNodesWithType(const Graph& g,\n+                                       const std::string& type) {\n   std::vector<Node*> result;\n   for (Node* n : g.nodes()) {\n     if (n->type_string() == type) {\n@@ -105,7 +106,7 @@ absl::Status GetArgDataTypes(const std::vector<Node*>& arg_nodes,\n \n // Builds XlaRecvAtHost node.\n absl::StatusOr<Node*> BuildRecvAtHostNode(\n-    Graph* g, const string& oc_cluster_name,\n+    Graph* g, const std::string& oc_cluster_name,\n     const std::vector<DataType>& recv_at_host_dtypes, Node* key_placeholder) {\n   NodeDefBuilder recv_at_host_builder(\n       absl::StrCat(\"outside_compilation_\", oc_cluster_name, \"_recv\"),\n@@ -128,7 +129,7 @@ absl::StatusOr<Node*> BuildRecvAtHostNode(\n \n // Builds XlaRecvAtHost node, and replaces all _Arg nodes with it.\n absl::StatusOr<Node*> ReplaceArgNodesWithRecvAtHostNode(\n-    Graph* g, const string& oc_cluster_name,\n+    Graph* g, const std::string& oc_cluster_name,\n     std::vector<DataType>* recv_at_host_dtypes, Node* key_placeholder) {\n   // TODO(b/77601805): use out nodes for source node, instead of traversing all\n   // nodes.\n@@ -205,7 +206,7 @@ absl::Status GetRetDataTypes(const std::vector<Node*>& ret_nodes,\n \n // Builds XlaSendFromHost node.\n absl::StatusOr<Node*> BuildSendFromHostNode(\n-    Graph* g, const string& oc_cluster_name,\n+    Graph* g, const std::string& oc_cluster_name,\n     const std::vector<Node*>& ret_nodes,\n     const std::vector<DataType>& send_from_host_dtypes, Node* key_placeholder) {\n   NodeDefBuilder send_from_host_builder(\n@@ -245,7 +246,7 @@ absl::StatusOr<Node*> BuildSendFromHostNode(\n \n // Builds XlaSendFromHost node, and replaces all _Retval nodes with it.\n absl::StatusOr<Node*> ReplaceRetNodesWithSendFromHostNode(\n-    Graph* g, const string& oc_cluster_name,\n+    Graph* g, const std::string& oc_cluster_name,\n     std::vector<DataType>* send_from_host_dtypes, Node* key_placeholder) {\n   // TODO(b/77601805): use in nodes for sink node, instead of traversing all\n   // nodes.\n@@ -299,16 +300,17 @@ std::optional<std::vector<PartialTensorShape>> GetInferredInputShapes(\n   return results;\n }\n \n-string host_compute_node_name(const string& original_oc_name) {\n+std::string host_compute_node_name(const std::string& original_oc_name) {\n   return absl::StrCat(\"outside_compilation_\", original_oc_name,\n                       \"_host_compute\");\n }\n \n // Builds XlaHostCompute NodeDef from the outside compilation call node.\n absl::StatusOr<NodeDef> BuildXlaHostComputeNodeDef(\n-    const Node* call_node, const std::map<string, int>& host_compute_core,\n-    const absl::flat_hash_map<string, std::vector<string>>& cluster_deps) {\n-  string original_oc_name;\n+    const Node* call_node, const std::map<std::string, int>& host_compute_core,\n+    const absl::flat_hash_map<std::string, std::vector<std::string>>&\n+        cluster_deps) {\n+  std::string original_oc_name;\n   TF_RETURN_IF_ERROR(GetNodeAttr(\n       call_node->attrs(), \"_outside_compilation_subgraph\", &original_oc_name));\n   NodeDefBuilder host_compute_builder(host_compute_node_name(original_oc_name),\n@@ -341,7 +343,7 @@ absl::StatusOr<NodeDef> BuildXlaHostComputeNodeDef(\n   // according to their host-side graph dependency. This can cause deadlock.\n   // Therefore, we hint XLA what the correct ordering of these clusters should\n   // be to avoid deadlocks.\n-  std::vector<string> xla_token_input_nodes;\n+  std::vector<std::string> xla_token_input_nodes;\n   xla_token_input_nodes.emplace_back(kXlaTokenArgNodeName);\n   auto cluster_deps_it = cluster_deps.find(original_oc_name);\n   if (cluster_deps_it != cluster_deps.end()) {\n@@ -376,8 +378,10 @@ absl::StatusOr<NodeDef> BuildXlaHostComputeNodeDef(\n \n // Replace outside compilation function call node with XlaHostCompute node.\n TF_ATTRIBUTE_NOINLINE absl::StatusOr<Node*> ReplaceOutsideCompilationCallNode(\n-    Graph* g, Node* call_node, const std::map<string, int>& host_compute_core,\n-    const absl::flat_hash_map<string, std::vector<string>>& cluster_deps) {\n+    Graph* g, Node* call_node,\n+    const std::map<std::string, int>& host_compute_core,\n+    const absl::flat_hash_map<std::string, std::vector<std::string>>&\n+        cluster_deps) {\n   // Build XlaHostCompute NodeDef.\n   TF_ASSIGN_OR_RETURN(\n       NodeDef node_def,\n@@ -405,16 +409,17 @@ absl::Status ResetDeviceOrdinalToPlaceholderValue(Graph* g) {\n       n->ClearAttr(\"device_ordinal\");\n       n->AddAttr(\"device_ordinal\", device_ordinal_value);\n     } else if (n->IsIfNode()) {\n-      for (const string& attr_name :\n-           std::vector<string>{\"then_branch\", \"else_branch\"}) {\n+      for (const std::string& attr_name :\n+           std::vector<std::string>{\"then_branch\", \"else_branch\"}) {\n         NameAttrList branch_func;\n         TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), attr_name, &branch_func));\n         (*branch_func.mutable_attr())[\"_device_ordinal\"] = device_ordinal_value;\n         n->ClearAttr(attr_name);\n         n->AddAttr(attr_name, branch_func);\n       }\n     } else if (n->IsWhileNode()) {\n-      for (const string& attr_name : std::vector<string>{\"cond\", \"body\"}) {\n+      for (const std::string& attr_name :\n+           std::vector<std::string>{\"cond\", \"body\"}) {\n         NameAttrList branch_func;\n         TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), attr_name, &branch_func));\n         (*branch_func.mutable_attr())[\"_device_ordinal\"] = device_ordinal_value;\n@@ -448,11 +453,12 @@ bool HasLiftedArgs(const FunctionDef& function_def) {\n absl::StatusOr<std::vector<std::pair<Node*, Node*>>>\n LiftedArgsAndOutsideCompilationNodesInFunctionBody(\n     const FunctionBody& function_body,\n-    const std::unordered_map<string, Node*>& outside_compilation_attr_to_node) {\n+    const std::unordered_map<std::string, Node*>&\n+        outside_compilation_attr_to_node) {\n   std::vector<std::pair<Node*, Node*>>\n       lifted_arg_nodes_and_outside_compilation_nodes;\n   for (Node* n : function_body.graph->op_nodes()) {\n-    string oc_cluster;\n+    std::string oc_cluster;\n     if (n->type_string() == \"Placeholder\" &&\n         GetNodeAttr(n->def(), kXlaLiftedArgOutsideCompilationAttrName,\n                     &oc_cluster)\n@@ -471,7 +477,7 @@ LiftedArgsAndOutsideCompilationNodesInFunctionBody(\n absl::StatusOr<std::vector<DataType>> UpdateTypesAttribute(\n     const std::vector<std::pair<Node*, Node*>>&\n         lifted_arg_nodes_and_outside_compilation_nodes,\n-    const string& type_attr_name, Node* n) {\n+    const std::string& type_attr_name, Node* n) {\n   std::vector<DataType> data_types;\n   data_types.reserve(lifted_arg_nodes_and_outside_compilation_nodes.size());\n   TF_RETURN_IF_ERROR(GetNodeAttr(n->def(), type_attr_name, &data_types));\n@@ -578,7 +584,8 @@ absl::Status AddFunctionWithNewName(const std::string& new_name,\n // Reconnect outside compilation lifted arguments in a functional While node to\n // its outside compilation tensor sources.\n absl::Status PostprocessLiftedArgsForWhile(\n-    const std::unordered_map<string, Node*>& outside_compilation_attr_to_node,\n+    const std::unordered_map<std::string, Node*>&\n+        outside_compilation_attr_to_node,\n     Graph* g, Node* n, FunctionLibraryDefinition* fld) {\n   TF_RET_CHECK(n->IsWhileNode());\n \n@@ -687,7 +694,8 @@ absl::Status PostprocessLiftedArgsForWhile(\n }\n \n absl::Status PostprocessLiftedArgsForIf(\n-    const std::unordered_map<string, Node*>& outside_compilation_attr_to_node,\n+    const std::unordered_map<std::string, Node*>&\n+        outside_compilation_attr_to_node,\n     Graph* g, Node* n, FunctionLibraryDefinition* fld) {\n   TF_RET_CHECK(n->IsIfNode());\n \n@@ -826,7 +834,8 @@ absl::Status PostprocessLiftedArgsForIf(\n }\n \n absl::Status PostprocessLiftedArgsForCall(\n-    const std::unordered_map<string, Node*>& outside_compilation_attr_to_node,\n+    const std::unordered_map<std::string, Node*>&\n+        outside_compilation_attr_to_node,\n     Graph* g, Node* n, FunctionLibraryDefinition* fld) {\n   const FunctionDef* fdef = fld->Find(n->type_string());\n   TF_RET_CHECK(fdef);\n@@ -924,12 +933,12 @@ absl::Status PostprocessLiftedArgsForCall(\n \n // Creates a mapping from outside compilation cluster name to lifted argument\n // placeholder.\n-absl::StatusOr<std::unordered_map<string, Node*>> OutsideCompilationAttrToNode(\n-    const Graph& g) {\n-  std::unordered_map<string, Node*> outside_compilation_attr_to_node;\n+absl::StatusOr<std::unordered_map<std::string, Node*>>\n+OutsideCompilationAttrToNode(const Graph& g) {\n+  std::unordered_map<std::string, Node*> outside_compilation_attr_to_node;\n   for (Node* n : g.op_nodes()) {\n     bool is_lifted_arg;\n-    string outside_compilation_attr;\n+    std::string outside_compilation_attr;\n     if (TryGetNodeAttr(n->def(), kXlaIsLiftedArgAttrName, &is_lifted_arg) &&\n         TryGetNodeAttr(n->def(), \"_xla_outside_compilation\",\n                        &outside_compilation_attr)) {\n@@ -988,8 +997,9 @@ absl::Status PostprocessLiftedArgs(Graph* g, FunctionLibraryDefinition* fld) {\n //    replace this node with compilation result node.\n // 3) all outside compilation graphs.\n absl::Status ConstructHostGraph(\n-    const string& xla_cluster_name, const string& outside_compilation_attr_name,\n-    const std::vector<string>& outside_compilation_host_graphs,\n+    const std::string& xla_cluster_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::vector<std::string>& outside_compilation_host_graphs,\n     FunctionLibraryDefinition* fld, std::unique_ptr<Graph>* host_graph) {\n   host_graph->reset(new Graph(fld));\n \n@@ -1013,15 +1023,15 @@ absl::Status ConstructHostGraph(\n   //    XlaSendFromHost, If/While nodes containing\n   //    XlaRecvAtHost/XlaSendFromHost) to sequencer node.\n   // c) Clear node_def.device(), so device placer won't get confused.\n-  for (const string& host_func : outside_compilation_host_graphs) {\n+  for (const std::string& host_func : outside_compilation_host_graphs) {\n     VLOG(4) << \"Expanding host graph \" << host_func;\n     // Temporarily use \"0\" as \"_device_ordinal\". It will be reset to placeholder\n     // value after we expanded all host graphs. We cannot just use placeholder\n     // value here because FunctionDef instantiation does not allow placeholder\n     // value for attributes.\n     AttrValue device_ordinal_attr;\n     device_ordinal_attr.set_i(0);\n-    protobuf::Map<string, AttrValue> attrs;\n+    protobuf::Map<std::string, AttrValue> attrs;\n     attrs[\"_device_ordinal\"] = device_ordinal_attr;\n     std::unique_ptr<FunctionBody> host_fbody;\n     const FunctionDef* host_fdef = fld->Find(host_func);\n@@ -1123,18 +1133,17 @@ absl::Status ConstructHostGraph(\n \n // Expand XLA computation's outside compilation host side graph into main graph.\n // Add a control edge between sequencer node and the XLA computation node.\n-absl::Status ExpandHostGraphIntoMainGraph(Graph* main_graph,\n-                                          FunctionLibraryDefinition* fld,\n-                                          const string& host_graph_func_name,\n-                                          Node* xla_computation_node,\n-                                          Node* pivot_node) {\n+absl::Status ExpandHostGraphIntoMainGraph(\n+    Graph* main_graph, FunctionLibraryDefinition* fld,\n+    const std::string& host_graph_func_name, Node* xla_computation_node,\n+    Node* pivot_node) {\n   // Temporarily use \"0\" as \"_device_ordinal\". It will be rewritten with the\n   // correct value in a later pass. We cannot just use placeholder value here\n   // because FunctionDef instantiation does not allow placeholder value for\n   // attributes.\n   AttrValue device_ordinal_attr;\n   device_ordinal_attr.set_i(0);\n-  protobuf::Map<string, AttrValue> attrs;\n+  protobuf::Map<std::string, AttrValue> attrs;\n   attrs[\"_device_ordinal\"] = device_ordinal_attr;\n   std::unique_ptr<FunctionBody> fbody;\n   const FunctionDef* host_graph_func = fld->Find(host_graph_func_name);\n@@ -1207,12 +1216,12 @@ absl::Status ExpandHostGraphIntoMainGraph(Graph* main_graph,\n // 2) Remove control edges.\n // 3) Prune nodes that are not useful for shape inference.\n absl::Status RewriteShapeInferenceGraph(\n-    const string& shape_inference_graph_name, Graph* host_graph,\n+    const std::string& shape_inference_graph_name, Graph* host_graph,\n     Node* pivot_node, FunctionLibraryDefinition* fld) {\n   // Use \"0\" as \"_device_ordinal\". It does not matter for shape inference.\n   AttrValue device_ordinal_attr;\n   device_ordinal_attr.set_i(0);\n-  protobuf::Map<string, AttrValue> attrs;\n+  protobuf::Map<std::string, AttrValue> attrs;\n   attrs[\"_device_ordinal\"] = device_ordinal_attr;\n   std::unique_ptr<FunctionBody> fbody;\n   const FunctionDef* shape_inference_graph =\n@@ -1338,13 +1347,13 @@ void SetMaximalSharding(NodeDefBuilder& node_builder) {\n \n // Builds XlaSendToHost node which sends cond predicate to host.\n TF_ATTRIBUTE_NOINLINE absl::StatusOr<Node*> BuildSendIfPredNode(\n-    const string& name, const string& host_transfer_key, Node* pred_node,\n-    Graph* g) {\n+    const std::string& name, const std::string& host_transfer_key,\n+    Node* pred_node, Graph* g) {\n   NodeDefBuilder send_pred_builder(name, \"XlaSendToHost\");\n   send_pred_builder.Attr(\"Tinput\", DT_BOOL);\n   send_pred_builder.Attr(\"key\", absl::StrCat(host_transfer_key, \"_dtoh_0\"));\n   send_pred_builder.Attr(kXlaTokenInputNodesAttrName,\n-                         std::vector<string>{kXlaTokenArgNodeName});\n+                         std::vector<std::string>{kXlaTokenArgNodeName});\n   send_pred_builder.Attr(kXlaOriginalOutsideCompilationNodeName, name);\n   SetMaximalSharding(send_pred_builder);\n   send_pred_builder.Input(pred_node->name(), 0, DT_BOOL);\n@@ -1356,14 +1365,14 @@ TF_ATTRIBUTE_NOINLINE absl::StatusOr<Node*> BuildSendIfPredNode(\n }\n \n // Replaces key placeholder node with an _Arg node.\n-absl::Status ReplaceKeyPlaceholderWithArgNode(const string& xla_cluster_name,\n-                                              const string& func_name,\n-                                              FunctionLibraryDefinition* fld) {\n+absl::Status ReplaceKeyPlaceholderWithArgNode(\n+    const std::string& xla_cluster_name, const std::string& func_name,\n+    FunctionLibraryDefinition* fld) {\n   // Temporarily use \"0\" as \"_device_ordinal\". It will be reset to placeholder\n   // value after rewriting.\n   AttrValue device_ordinal_attr;\n   device_ordinal_attr.set_i(0);\n-  protobuf::Map<string, AttrValue> attrs;\n+  protobuf::Map<std::string, AttrValue> attrs;\n   attrs[\"_device_ordinal\"] = device_ordinal_attr;\n   std::unique_ptr<FunctionBody> fbody;\n   const FunctionDef* func = fld->Find(func_name);\n@@ -1404,14 +1413,15 @@ absl::Status ReplaceKeyPlaceholderWithArgNode(const string& xla_cluster_name,\n \n // Builds host side graph for If node.\n TF_ATTRIBUTE_NOINLINE absl::Status BuildHostGraphForIfNode(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const string& if_node_name, const string& host_transfer_key,\n-    const string& host_graph_func_name, FunctionLibraryDefinition* fld,\n-    const string& then_branch_host_func_name,\n-    const string& else_branch_host_func_name) {\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name, const std::string& if_node_name,\n+    const std::string& host_transfer_key,\n+    const std::string& host_graph_func_name, FunctionLibraryDefinition* fld,\n+    const std::string& then_branch_host_func_name,\n+    const std::string& else_branch_host_func_name) {\n   Graph host_graph(fld);\n-  string outside_compilation_name = absl::StrCat(\"oc_if_\", if_node_name);\n+  std::string outside_compilation_name = absl::StrCat(\"oc_if_\", if_node_name);\n   AttrValue device_ordinal_value;\n   device_ordinal_value.set_placeholder(\"_device_ordinal\");\n \n@@ -1484,7 +1494,7 @@ TF_ATTRIBUTE_NOINLINE absl::Status BuildHostGraphForIfNode(\n \n // Rewrites loop cond to add a node which sends loop cond to host.\n TF_ATTRIBUTE_NOINLINE absl::Status AddSendLoopPredToLoopCond(\n-    const string& cond_xla_func_name, const string& host_transfer_key,\n+    const std::string& cond_xla_func_name, const std::string& host_transfer_key,\n     NameAttrList* loop_cond_func, FunctionLibraryDefinition* fld,\n     Node* while_node) {\n   // Instantiate the loop cond function.\n@@ -1523,7 +1533,7 @@ TF_ATTRIBUTE_NOINLINE absl::Status AddSendLoopPredToLoopCond(\n   send_loop_cond_builder.Attr(\"key\",\n                               absl::StrCat(host_transfer_key, \"_dtoh_0\"));\n   send_loop_cond_builder.Attr(kXlaTokenInputNodesAttrName,\n-                              std::vector<string>{kXlaTokenArgNodeName});\n+                              std::vector<std::string>{kXlaTokenArgNodeName});\n   send_loop_cond_builder.Attr(kXlaOriginalOutsideCompilationNodeName,\n                               send_loop_cond_builder.node_name());\n   SetMaximalSharding(send_loop_cond_builder);\n@@ -1560,18 +1570,21 @@ TF_ATTRIBUTE_NOINLINE absl::Status AddSendLoopPredToLoopCond(\n \n // Rewrites while loop cond function for host.\n absl::Status RewriteHostWhileLoopCond(\n-    const string& cond_host_func_name, const string& while_node_name,\n-    const string& host_transfer_key, const string& xla_cluster_attr_name,\n-    const string& xla_cluster_name, const string& outside_compilation_attr_name,\n-    const string& outside_compilation_name, FunctionLibraryDefinition* fld) {\n+    const std::string& cond_host_func_name, const std::string& while_node_name,\n+    const std::string& host_transfer_key,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& xla_cluster_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& outside_compilation_name,\n+    FunctionLibraryDefinition* fld) {\n   // Replace key placeholder node with _Arg node.\n   TF_RETURN_IF_ERROR(ReplaceKeyPlaceholderWithArgNode(\n       xla_cluster_name, cond_host_func_name, fld));\n \n   // Instantiate cond function.\n   AttrValue device_ordinal_temp_value;\n   device_ordinal_temp_value.set_i(0);\n-  protobuf::Map<string, AttrValue> attrs;\n+  protobuf::Map<std::string, AttrValue> attrs;\n   attrs[\"_device_ordinal\"] = device_ordinal_temp_value;\n   std::unique_ptr<FunctionBody> cond_fbody;\n   const FunctionDef* cond_host_func = fld->Find(cond_host_func_name);\n@@ -1634,18 +1647,21 @@ absl::Status RewriteHostWhileLoopCond(\n \n // Rewrites while loop body function for host.\n absl::Status RewriteHostWhileLoopBody(\n-    const string& body_host_func_name, const string& while_node_name,\n-    const string& host_transfer_key, const string& xla_cluster_attr_name,\n-    const string& xla_cluster_name, const string& outside_compilation_attr_name,\n-    const string& outside_compilation_name, FunctionLibraryDefinition* fld) {\n+    const std::string& body_host_func_name, const std::string& while_node_name,\n+    const std::string& host_transfer_key,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& xla_cluster_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& outside_compilation_name,\n+    FunctionLibraryDefinition* fld) {\n   // Replace key placeholder node with _Arg node.\n   TF_RETURN_IF_ERROR(ReplaceKeyPlaceholderWithArgNode(\n       xla_cluster_name, body_host_func_name, fld));\n \n   // Instantiate body function.\n   AttrValue device_ordinal_temp_value;\n   device_ordinal_temp_value.set_i(0);\n-  protobuf::Map<string, AttrValue> attrs;\n+  protobuf::Map<std::string, AttrValue> attrs;\n   attrs[\"_device_ordinal\"] = device_ordinal_temp_value;\n   std::unique_ptr<FunctionBody> body_fbody;\n   const FunctionDef* body_host_func = fld->Find(body_host_func_name);\n@@ -1692,13 +1708,16 @@ absl::Status RewriteHostWhileLoopBody(\n \n // Builds host side graph for while node.\n TF_ATTRIBUTE_NOINLINE absl::Status BuildHostGraphForWhileNode(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const string& while_node_name, const string& host_transfer_key,\n-    const string& host_graph_func_name, FunctionLibraryDefinition* fld,\n-    const string& cond_host_func_name, const string& body_host_func_name) {\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name, const std::string& while_node_name,\n+    const std::string& host_transfer_key,\n+    const std::string& host_graph_func_name, FunctionLibraryDefinition* fld,\n+    const std::string& cond_host_func_name,\n+    const std::string& body_host_func_name) {\n   Graph host_graph(fld);\n-  string outside_compilation_name = absl::StrCat(\"oc_while_\", while_node_name);\n+  std::string outside_compilation_name =\n+      absl::StrCat(\"oc_while_\", while_node_name);\n \n   // Step 1: add key placeholder node.\n   TF_ASSIGN_OR_RETURN(\n@@ -1759,10 +1778,12 @@ TF_ATTRIBUTE_NOINLINE absl::Status BuildHostGraphForWhileNode(\n \n // Builds host graph for func call nodes.\n absl::Status BuildHostGraphForFuncCallNode(\n-    const string& xla_cluster_attr_name, const string& xla_cluster_name,\n-    const string& outside_compilation_attr_name,\n-    const string& func_call_node_name, const string& func_call_host_func_name,\n-    const string& host_graph_func_name, FunctionLibraryDefinition* fld) {\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& xla_cluster_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& func_call_node_name,\n+    const std::string& func_call_host_func_name,\n+    const std::string& host_graph_func_name, FunctionLibraryDefinition* fld) {\n   Graph host_graph(fld);\n   AttrValue device_ordinal_value;\n   device_ordinal_value.set_placeholder(\"_device_ordinal\");\n@@ -1807,18 +1828,19 @@ absl::Status BuildHostGraphForFuncCallNode(\n }\n \n TF_ATTRIBUTE_NOINLINE absl::Status ExtractOutsideCompilationForFuncCallNode(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const std::map<string, int>& host_compute_core, Graph* g, Node* n,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name,\n+    const std::map<std::string, int>& host_compute_core, Graph* g, Node* n,\n     FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n-    std::vector<string>* host_graphs,\n-    std::vector<string>* shape_inference_graphs,\n+    std::vector<std::string>* host_graphs,\n+    std::vector<std::string>* shape_inference_graphs,\n     bool* has_outside_compilation) {\n   bool func_has_outside_compilation = false;\n   NameAttrList func;\n   if (fld->Contains(n->type_string())) {\n     func.set_name(n->type_string());\n-    typedef protobuf::Map<string, AttrValue> AttrMap;\n+    typedef protobuf::Map<std::string, AttrValue> AttrMap;\n     *func.mutable_attr() = AttrMap(n->attrs().begin(), n->attrs().end());\n   } else if (n->IsPartitionedCall()) {\n     TF_RETURN_IF_ERROR(GetNodeAttr(n->def(), \"f\", &func));\n@@ -1827,16 +1849,16 @@ TF_ATTRIBUTE_NOINLINE absl::Status ExtractOutsideCompilationForFuncCallNode(\n     func.set_name(FunctionLibraryDefinition::kGradientOp);\n     *func.mutable_attr() = n->def().attr();\n   }\n-  string canonical_func_name;\n+  std::string canonical_func_name;\n   if (func.name() == FunctionLibraryDefinition::kGradientOp) {\n     NameAttrList forward_func;\n     TF_RETURN_IF_ERROR(GetNodeAttr(n->def(), \"f\", &forward_func));\n     canonical_func_name = absl::StrCat(\"gradient_\", forward_func.name());\n   } else {\n     canonical_func_name = func.name();\n   }\n-  string new_func_name = absl::StrCat(canonical_func_name, \"_oc\");\n-  string host_func_name =\n+  std::string new_func_name = absl::StrCat(canonical_func_name, \"_oc\");\n+  std::string host_func_name =\n       absl::StrCat(\"oc_func_call_host_\", canonical_func_name);\n   TF_RETURN_IF_ERROR(ExtractOutsideCompilationForFunction(\n       xla_cluster_attr_name, outside_compilation_attr_name, xla_cluster_name,\n@@ -1876,11 +1898,11 @@ TF_ATTRIBUTE_NOINLINE absl::Status ExtractOutsideCompilationForFuncCallNode(\n   TF_RETURN_IF_ERROR(replace_builder->Finalize(replace_def.get()));\n   TF_ASSIGN_OR_RETURN(Node * replace, ReplaceNode(g, n, *replace_def));\n   replace->AddAttr(kXlaTokenInputNodesAttrName,\n-                   std::vector<string>{kXlaTokenArgNodeName});\n+                   std::vector<std::string>{kXlaTokenArgNodeName});\n   replace->AddAttr(kXlaOriginalOutsideCompilationNodeName, replace->name());\n \n   // Build host side graph for the function call.\n-  string oc_host_graph_name =\n+  std::string oc_host_graph_name =\n       absl::StrCat(\"oc_func_host_graph_\", replace->name());\n   TF_RETURN_IF_ERROR(BuildHostGraphForFuncCallNode(\n       xla_cluster_attr_name, xla_cluster_name, outside_compilation_attr_name,\n@@ -1893,12 +1915,13 @@ TF_ATTRIBUTE_NOINLINE absl::Status ExtractOutsideCompilationForFuncCallNode(\n }\n \n absl::Status ExtractOutsideCompilationForIfNode(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const std::map<string, int>& host_compute_core, Graph* g, Node* n,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name,\n+    const std::map<std::string, int>& host_compute_core, Graph* g, Node* n,\n     FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n-    std::vector<string>* host_graphs,\n-    std::vector<string>* shape_inference_graphs,\n+    std::vector<std::string>* host_graphs,\n+    std::vector<std::string>* shape_inference_graphs,\n     bool* has_outside_compilation) {\n   // Instantiate \"then_branch\" and \"else_branch\".\n   NameAttrList then_branch, else_branch;\n@@ -1908,12 +1931,14 @@ absl::Status ExtractOutsideCompilationForIfNode(\n   // Extract outside compilation for then_branch and else_branch.\n   bool then_branch_has_outside_compilation = false;\n   bool else_branch_has_outside_compilation = false;\n-  string then_branch_host_func_name =\n-             absl::StrCat(\"oc_then_branch_host_if_\", then_branch.name()),\n-         else_branch_host_func_name =\n-             absl::StrCat(\"oc_else_branch_host_if_\", else_branch.name());\n-  string then_branch_xla_func_name = absl::StrCat(then_branch.name(), \"_oc\"),\n-         else_branch_xla_func_name = absl::StrCat(else_branch.name(), \"_oc\");\n+  std::string then_branch_host_func_name =\n+                  absl::StrCat(\"oc_then_branch_host_if_\", then_branch.name()),\n+              else_branch_host_func_name =\n+                  absl::StrCat(\"oc_else_branch_host_if_\", else_branch.name());\n+  std::string then_branch_xla_func_name =\n+                  absl::StrCat(then_branch.name(), \"_oc\"),\n+              else_branch_xla_func_name =\n+                  absl::StrCat(else_branch.name(), \"_oc\");\n   TF_RETURN_IF_ERROR(ExtractOutsideCompilationForFunction(\n       xla_cluster_attr_name, outside_compilation_attr_name, xla_cluster_name,\n       then_branch, then_branch_xla_func_name, then_branch_host_func_name,\n@@ -1946,7 +1971,7 @@ absl::Status ExtractOutsideCompilationForIfNode(\n   }\n   n->AddAttr(kXlaOriginalOutsideCompilationNodeName, n->name());\n \n-  string host_transfer_key = absl::StrCat(\"oc_if_pred_\", n->name());\n+  std::string host_transfer_key = absl::StrCat(\"oc_if_pred_\", n->name());\n \n   // XLA computation: add a SendToHost node to send cond predicate.\n   Node* pred_node;\n@@ -1956,7 +1981,7 @@ absl::Status ExtractOutsideCompilationForIfNode(\n       BuildSendIfPredNode(absl::StrCat(\"send_oc_if_pred_\", n->name()),\n                           host_transfer_key, pred_node, g));\n   n->AddAttr(kXlaTokenInputNodesAttrName,\n-             std::vector<string>{send_pred_node->name()});\n+             std::vector<std::string>{send_pred_node->name()});\n \n   // Add a control edge from `send_pred_node` to If node, so XlaCompiler will\n   // visit If node after `send_pred_node`, thus the token output for\n@@ -1969,7 +1994,7 @@ absl::Status ExtractOutsideCompilationForIfNode(\n   // we need to create a no-op host graph.\n   if (!then_branch_has_outside_compilation) {\n     std::unique_ptr<Graph> then_branch_host_graph(new Graph(fld));\n-    std::vector<string> then_branch_host_graphs;\n+    std::vector<std::string> then_branch_host_graphs;\n     TF_RETURN_IF_ERROR(ConstructHostGraph(\n         xla_cluster_name, outside_compilation_attr_name,\n         then_branch_host_graphs, fld, &then_branch_host_graph));\n@@ -1986,7 +2011,7 @@ absl::Status ExtractOutsideCompilationForIfNode(\n   }\n   if (!else_branch_has_outside_compilation) {\n     std::unique_ptr<Graph> else_branch_host_graph(new Graph(fld));\n-    std::vector<string> else_branch_host_graphs;\n+    std::vector<std::string> else_branch_host_graphs;\n     TF_RETURN_IF_ERROR(ConstructHostGraph(\n         xla_cluster_name, outside_compilation_attr_name,\n         else_branch_host_graphs, fld, &else_branch_host_graph));\n@@ -2001,7 +2026,7 @@ absl::Status ExtractOutsideCompilationForIfNode(\n       TF_RETURN_IF_ERROR(fld->AddFunctionDef(else_branch_host_fdef));\n     }\n   }\n-  string oc_host_graph_name = absl::StrCat(\"oc_if_host_graph_\", n->name());\n+  std::string oc_host_graph_name = absl::StrCat(\"oc_if_host_graph_\", n->name());\n   TF_RETURN_IF_ERROR(BuildHostGraphForIfNode(\n       xla_cluster_attr_name, outside_compilation_attr_name, xla_cluster_name,\n       n->name(), host_transfer_key, oc_host_graph_name, fld,\n@@ -2012,12 +2037,13 @@ absl::Status ExtractOutsideCompilationForIfNode(\n }\n \n absl::Status ExtractOutsideCompilationForWhileNode(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const std::map<string, int>& host_compute_core, Graph* g, Node* n,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name,\n+    const std::map<std::string, int>& host_compute_core, Graph* g, Node* n,\n     FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n-    std::vector<string>* host_graphs,\n-    std::vector<string>* shape_inference_graphs,\n+    std::vector<std::string>* host_graphs,\n+    std::vector<std::string>* shape_inference_graphs,\n     bool* has_outside_compilation) {\n   // Instantiate \"cond\" and \"body\".\n   NameAttrList cond, body;\n@@ -2027,10 +2053,12 @@ absl::Status ExtractOutsideCompilationForWhileNode(\n   // Extract outside compilation for cond and body.\n   bool cond_has_outside_compilation = false;\n   bool body_has_outside_compilation = false;\n-  string cond_host_func_name = absl::StrCat(\"oc_cond_host_while_\", cond.name()),\n-         body_host_func_name = absl::StrCat(\"oc_body_host_while_\", body.name());\n-  string cond_xla_func_name = absl::StrCat(cond.name(), \"_oc\"),\n-         body_xla_func_name = absl::StrCat(body.name(), \"_oc\");\n+  std::string cond_host_func_name =\n+                  absl::StrCat(\"oc_cond_host_while_\", cond.name()),\n+              body_host_func_name =\n+                  absl::StrCat(\"oc_body_host_while_\", body.name());\n+  std::string cond_xla_func_name = absl::StrCat(cond.name(), \"_oc\"),\n+              body_xla_func_name = absl::StrCat(body.name(), \"_oc\");\n   TF_RETURN_IF_ERROR(ExtractOutsideCompilationForFunction(\n       xla_cluster_attr_name, outside_compilation_attr_name, xla_cluster_name,\n       cond, cond_xla_func_name, cond_host_func_name, host_compute_core, flr,\n@@ -2060,19 +2088,19 @@ absl::Status ExtractOutsideCompilationForWhileNode(\n   }\n   n->AddAttr(kXlaOriginalOutsideCompilationNodeName, n->name());\n \n-  string host_transfer_key = absl::StrCat(\"oc_while_pred_\", n->name());\n+  std::string host_transfer_key = absl::StrCat(\"oc_while_pred_\", n->name());\n \n   // XLA computation: rewrite cond function to add a SendToHost node to send\n   // loop predicate.\n   TF_RETURN_IF_ERROR(AddSendLoopPredToLoopCond(\n       cond_xla_func_name, host_transfer_key, &cond, fld, n));\n   n->AddAttr(kXlaTokenInputNodesAttrName,\n-             std::vector<string>{kXlaTokenArgNodeName});\n+             std::vector<std::string>{kXlaTokenArgNodeName});\n \n   // Build host side graph for the \"While\" node.\n   if (!cond_has_outside_compilation) {\n     std::unique_ptr<Graph> cond_host_graph(new Graph(fld));\n-    std::vector<string> host_graphs;\n+    std::vector<std::string> host_graphs;\n     TF_RETURN_IF_ERROR(ConstructHostGraph(xla_cluster_name,\n                                           outside_compilation_attr_name,\n                                           host_graphs, fld, &cond_host_graph));\n@@ -2088,7 +2116,7 @@ absl::Status ExtractOutsideCompilationForWhileNode(\n   }\n   if (!body_has_outside_compilation) {\n     std::unique_ptr<Graph> body_host_graph(new Graph(fld));\n-    std::vector<string> host_graphs;\n+    std::vector<std::string> host_graphs;\n     TF_RETURN_IF_ERROR(ConstructHostGraph(xla_cluster_name,\n                                           outside_compilation_attr_name,\n                                           host_graphs, fld, &body_host_graph));\n@@ -2102,7 +2130,8 @@ absl::Status ExtractOutsideCompilationForWhileNode(\n       TF_RETURN_IF_ERROR(fld->AddFunctionDef(body_host_fdef));\n     }\n   }\n-  string oc_host_graph_name = absl::StrCat(\"oc_while_host_graph_\", n->name());\n+  std::string oc_host_graph_name =\n+      absl::StrCat(\"oc_while_host_graph_\", n->name());\n   TF_RETURN_IF_ERROR(BuildHostGraphForWhileNode(\n       xla_cluster_attr_name, outside_compilation_attr_name, xla_cluster_name,\n       n->name(), host_transfer_key, oc_host_graph_name, fld,\n@@ -2113,11 +2142,13 @@ absl::Status ExtractOutsideCompilationForWhileNode(\n }\n \n absl::Status ExtractOutsideCompilationForNodesWithAssociatedFunctions(\n-    Graph* g, const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const std::map<string, int>& host_compute_core, FunctionLibraryRuntime* flr,\n-    FunctionLibraryDefinition* fld, std::vector<string>* host_graphs,\n-    std::vector<string>* shape_inference_graphs,\n+    Graph* g, const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name,\n+    const std::map<std::string, int>& host_compute_core,\n+    FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n+    std::vector<std::string>* host_graphs,\n+    std::vector<std::string>* shape_inference_graphs,\n     bool* has_outside_compilation) {\n   std::vector<Node*> if_nodes, while_nodes, func_call_nodes;\n   for (Node* n : g->nodes()) {\n@@ -2155,7 +2186,7 @@ absl::Status ExtractOutsideCompilationForNodesWithAssociatedFunctions(\n }\n \n absl::Status CopyOutsideCompilationConstNodes(\n-    Graph* g, const string& outside_compilation_attr_name) {\n+    Graph* g, const std::string& outside_compilation_attr_name) {\n   for (Node* n : g->op_nodes()) {\n     if (!n->IsConstant() ||\n         !HasNodeAttr(n->def(), outside_compilation_attr_name)) {\n@@ -2205,8 +2236,8 @@ absl::Status RewriteOutsideCompilationSubgraphFn::operator()(\n     const std::vector<OutputTensor>& arg_source_tensors,\n     std::unique_ptr<Graph>* graph, std::vector<int>* input_permutation,\n     std::vector<int>* output_permutation, NodeDef* node_def) {\n-  string old_name = node_def->op();\n-  string new_name =\n+  std::string old_name = node_def->op();\n+  std::string new_name =\n       absl::StrCat(xla_cluster_name_, \"_\", new_function_name_, \"_\", old_name);\n   node_def->set_op(new_name);\n   node_def->set_name(new_name);\n@@ -2290,14 +2321,14 @@ absl::Status RewriteOutsideCompilationSubgraphFn::operator()(\n     AddNodeAttr(\"shape_inference_graph\", shape_inference_graph, node_def);\n     AddNodeAttr(\"shapes\", *shapes, node_def);\n   } else {\n-    string shape_inference_func_name =\n+    std::string shape_inference_func_name =\n         absl::StrCat(\"_outside_compilation_shape_inference_\", new_name);\n     NameAttrList shape_inference_graph;\n     shape_inference_graph.set_name(shape_inference_func_name);\n     AddNodeAttr(\"shape_inference_graph\", shape_inference_graph, node_def);\n     AddNodeAttr(\"shapes\", std::vector<TensorShapeProto>{}, node_def);\n   }\n-  AddNodeAttr(\"ancestors\", std::vector<string>{}, node_def);\n+  AddNodeAttr(\"ancestors\", std::vector<std::string>{}, node_def);\n   AddNodeAttr(\"Tinputs\", recv_at_host_dtypes, node_def);\n   AddNodeAttr(\"Toutputs\", send_from_host_dtypes, node_def);\n   AddNodeAttr(\"key\", absl::StrCat(\"host_compute_channel_\", new_name), node_def);\n@@ -2306,15 +2337,16 @@ absl::Status RewriteOutsideCompilationSubgraphFn::operator()(\n }\n \n absl::Status ExtractOutsideCompilationForFunction(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const NameAttrList& func_name_attrs, const string& new_func_name,\n-    const string& host_graph_func_name,\n-    const std::map<string, int>& host_compute_core, FunctionLibraryRuntime* flr,\n-    FunctionLibraryDefinition* fld, std::vector<string>* shape_inference_graphs,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name, const NameAttrList& func_name_attrs,\n+    const std::string& new_func_name, const std::string& host_graph_func_name,\n+    const std::map<std::string, int>& host_compute_core,\n+    FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n+    std::vector<std::string>* shape_inference_graphs,\n     bool* has_outside_compilation) {\n   // Convert the function to graph.\n-  const string& func_name = func_name_attrs.name();\n+  const std::string& func_name = func_name_attrs.name();\n   FunctionLibraryRuntime::Handle handle;\n   TF_RETURN_IF_ERROR(\n       flr->Instantiate(func_name, AttrSlice(&func_name_attrs.attr()), &handle));\n@@ -2345,8 +2377,8 @@ absl::Status ExtractOutsideCompilationForFunction(\n   }\n \n   std::unique_ptr<Graph> graph_out;\n-  std::vector<string> outside_compilation_host_graphs;\n-  std::vector<string> shape_inference_graphs_to_rewrite;\n+  std::vector<std::string> outside_compilation_host_graphs;\n+  std::vector<std::string> shape_inference_graphs_to_rewrite;\n   if (*has_outside_compilation) {\n     // Copy outside compilation Const nodes with non outside compilation users.\n     TF_RETURN_IF_ERROR(CopyOutsideCompilationConstNodes(\n@@ -2404,7 +2436,7 @@ absl::Status ExtractOutsideCompilationForFunction(\n         }\n       }\n     }\n-    std::map<string, Node*> host_compute_nodes;\n+    std::map<std::string, Node*> host_compute_nodes;\n     for (Node* n : outside_compilation_nodes) {\n       auto host_compute_node_or = ReplaceOutsideCompilationCallNode(\n           graph_out.get(), n, host_compute_core, *cluster_deps);\n@@ -2416,11 +2448,11 @@ absl::Status ExtractOutsideCompilationForFunction(\n     // them so XlaCompiler can handle them in correct order.\n     for (const auto& iter : host_compute_nodes) {\n       Node* host_compute_node = iter.second;\n-      std::vector<string> token_input_node_names;\n+      std::vector<std::string> token_input_node_names;\n       TF_RETURN_IF_ERROR(GetNodeAttr(host_compute_node->def(),\n                                      kXlaTokenInputNodesAttrName,\n                                      &token_input_node_names));\n-      for (const string& node_name : token_input_node_names) {\n+      for (const std::string& node_name : token_input_node_names) {\n         if (node_name == kXlaTokenArgNodeName) {\n           continue;\n         }\n@@ -2459,15 +2491,15 @@ absl::Status ExtractOutsideCompilationForFunction(\n     // Shape inference graphs might contain Placeholder nodes for outside\n     // compilation to outside compilation edges. Rewrite shape inference graphs\n     // to remove such nodes.\n-    for (const string& shape_inference_graph :\n+    for (const std::string& shape_inference_graph :\n          shape_inference_graphs_to_rewrite) {\n       TF_RETURN_IF_ERROR(\n           RewriteShapeInferenceGraph(shape_inference_graph, host_graph.get(),\n                                      /*pivot_node=*/nullptr, fld));\n     }\n \n     // Remove the outside compilation graphs from function library.\n-    for (const string& func : outside_compilation_host_graphs) {\n+    for (const std::string& func : outside_compilation_host_graphs) {\n       TF_RETURN_IF_ERROR(fld->RemoveFunction(func));\n     }\n \n@@ -2499,9 +2531,9 @@ absl::Status ExtractOutsideCompilationForFunction(\n }\n \n absl::Status ExtractOutsideCompilation(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name,\n-    const std::unordered_map<string, XlaClusterInfo>& clusters, Graph* g,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::unordered_map<std::string, XlaClusterInfo>& clusters, Graph* g,\n     FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n     bool* modified) {\n   if (VLOG_IS_ON(4)) {\n@@ -2511,14 +2543,14 @@ absl::Status ExtractOutsideCompilation(\n   *modified = false;\n   auto node_name_index = g->BuildNodeNameIndex();\n   for (auto& iter : clusters) {\n-    string xla_cluster_name = iter.first;\n+    std::string xla_cluster_name = iter.first;\n     Node* n = iter.second.node;\n     auto const& func_name_attrs = iter.second.func_name_attrs;\n     auto const& host_compute_core = iter.second.host_compute_core;\n \n-    std::vector<string> shape_inference_graphs;\n+    std::vector<std::string> shape_inference_graphs;\n     bool has_outside_compilation;\n-    string host_graph_func_name =\n+    std::string host_graph_func_name =\n         absl::StrCat(\"oc_host_graph_\", xla_cluster_name);\n     TF_RETURN_IF_ERROR(ExtractOutsideCompilationForFunction(\n         xla_cluster_attr_name, outside_compilation_attr_name, xla_cluster_name,\n@@ -2528,7 +2560,7 @@ absl::Status ExtractOutsideCompilation(\n     *modified |= has_outside_compilation;\n \n     if (has_outside_compilation) {\n-      string pivot_name = absl::StrCat(xla_cluster_name, \"/pivot\");\n+      std::string pivot_name = absl::StrCat(xla_cluster_name, \"/pivot\");\n       Node* pivot_node = node_name_index[pivot_name];\n       TF_RETURN_IF_ERROR(ExpandHostGraphIntoMainGraph(\n           g, fld, host_graph_func_name, n, pivot_node));"
        },
        {
            "sha": "c1697fcb4cde0dd5134de4883d4da9de9063e05b",
            "filename": "tensorflow/compiler/jit/extract_outside_compilation_pass.h",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -44,9 +44,9 @@ namespace tensorflow {\n class RewriteOutsideCompilationSubgraphFn {\n  public:\n   RewriteOutsideCompilationSubgraphFn(\n-      const string& xla_cluster_attr_name,\n-      const string& outside_compilation_attr_name,\n-      const string& xla_cluster_name, const string& new_function_name)\n+      const std::string& xla_cluster_attr_name,\n+      const std::string& outside_compilation_attr_name,\n+      const std::string& xla_cluster_name, const std::string& new_function_name)\n       : xla_cluster_attr_name_(xla_cluster_attr_name),\n         outside_compilation_attr_name_(outside_compilation_attr_name),\n         xla_cluster_name_(xla_cluster_name),\n@@ -59,10 +59,10 @@ class RewriteOutsideCompilationSubgraphFn {\n                           NodeDef* node_def);\n \n  private:\n-  string xla_cluster_attr_name_;\n-  string outside_compilation_attr_name_;\n-  string xla_cluster_name_;\n-  string new_function_name_;\n+  std::string xla_cluster_attr_name_;\n+  std::string outside_compilation_attr_name_;\n+  std::string xla_cluster_name_;\n+  std::string new_function_name_;\n };\n \n // For an XLA computation function, replace all outside compilations with\n@@ -88,22 +88,23 @@ class RewriteOutsideCompilationSubgraphFn {\n // has_outside_compilation: a bool indicating whether this function has any\n //   outside compilation nodes.\n absl::Status ExtractOutsideCompilationForFunction(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name, const string& xla_cluster_name,\n-    const NameAttrList& func_name_attrs, const string& new_func_name,\n-    const string& host_graph_func_name,\n-    const std::map<string, int>& host_compute_core, FunctionLibraryRuntime* flr,\n-    FunctionLibraryDefinition* fld, std::vector<string>* shape_inference_graphs,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::string& xla_cluster_name, const NameAttrList& func_name_attrs,\n+    const std::string& new_func_name, const std::string& host_graph_func_name,\n+    const std::map<std::string, int>& host_compute_core,\n+    FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n+    std::vector<std::string>* shape_inference_graphs,\n     bool* has_outside_compilation);\n \n // Rewrites XLA computation in `clusters` to replace outside compilation nodes\n // with XlaHostCompute, and moves those outside compilations into `g`. If shapes\n // of outside compilation outputs cannot be determined now, we will store shape\n // inference graph into `fld`.\n absl::Status ExtractOutsideCompilation(\n-    const string& xla_cluster_attr_name,\n-    const string& outside_compilation_attr_name,\n-    const std::unordered_map<string, XlaClusterInfo>& clusters, Graph* g,\n+    const std::string& xla_cluster_attr_name,\n+    const std::string& outside_compilation_attr_name,\n+    const std::unordered_map<std::string, XlaClusterInfo>& clusters, Graph* g,\n     FunctionLibraryRuntime* flr, FunctionLibraryDefinition* fld,\n     bool* modified);\n "
        },
        {
            "sha": "1a6441a80726a0843239812ea847448d4af1787f",
            "filename": "tensorflow/compiler/jit/extract_outside_compilation_pass_test.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 40,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fextract_outside_compilation_pass_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -236,14 +236,14 @@ class ExtractOutsideCompilationForFunctionTest : public ::testing::Test {\n   }\n \n   absl::Status ExtractOutsideCompilationTest(\n-      const string &xla_cluster_attr_name,\n-      const string &outside_compilation_attr_name,\n-      const string &xla_cluster_name, const NameAttrList &func_name_attrs,\n-      const string &new_func_name, const string &host_graph_func_name,\n-      const std::map<string, int> &host_compute_core,\n-      FunctionLibraryDefinition *fld,\n-      std::vector<string> *shape_inference_graphs,\n-      bool *has_outside_compilation) {\n+      const std::string& xla_cluster_attr_name,\n+      const std::string& outside_compilation_attr_name,\n+      const std::string& xla_cluster_name, const NameAttrList& func_name_attrs,\n+      const std::string& new_func_name, const std::string& host_graph_func_name,\n+      const std::map<std::string, int>& host_compute_core,\n+      FunctionLibraryDefinition* fld,\n+      std::vector<std::string>* shape_inference_graphs,\n+      bool* has_outside_compilation) {\n     OptimizerOptions opts;\n     pflr_ = std::make_unique<ProcessFunctionLibraryRuntime>(\n         device_mgr_.get(), Env::Default(), /*config=*/nullptr,\n@@ -288,9 +288,9 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, Basic) {\n   }\n   FunctionLibraryDefinition fld(OpRegistry::Global(), fdl);\n \n-  protobuf::Map<string, tensorflow::AttrValue> attrs;\n-  std::map<string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n-  std::vector<string> shape_inference_graphs;\n+  protobuf::Map<std::string, tensorflow::AttrValue> attrs;\n+  std::map<std::string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n+  std::vector<std::string> shape_inference_graphs;\n   bool has_outside_compilation;\n   NameAttrList name_attrs;\n   name_attrs.set_name(\"cluster\");\n@@ -342,7 +342,7 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, Basic) {\n   std::unique_ptr<FunctionBody> host_fbody;\n   AttrValue device_ordinal_temp_value;\n   device_ordinal_temp_value.set_i(0);\n-  protobuf::Map<string, AttrValue> host_func_attrs;\n+  protobuf::Map<std::string, AttrValue> host_func_attrs;\n   host_func_attrs[\"_device_ordinal\"] = device_ordinal_temp_value;\n   TF_CHECK_OK(FunctionDefToBodyHelper(\n       *fld.Find(\"host_graph\"), AttrSlice(&host_func_attrs), &fld, &host_fbody));\n@@ -406,9 +406,9 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, NoHostGraph) {\n   }\n   FunctionLibraryDefinition fld(OpRegistry::Global(), fdl);\n \n-  protobuf::Map<string, tensorflow::AttrValue> attrs;\n-  std::map<string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n-  std::vector<string> shape_inference_graphs;\n+  protobuf::Map<std::string, tensorflow::AttrValue> attrs;\n+  std::map<std::string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n+  std::vector<std::string> shape_inference_graphs;\n   bool has_outside_compilation;\n   NameAttrList name_attrs;\n   name_attrs.set_name(\"cluster\");\n@@ -481,9 +481,9 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, OutsideCompilationInIf) {\n   }\n   FunctionLibraryDefinition fld(OpRegistry::Global(), fdl);\n \n-  protobuf::Map<string, tensorflow::AttrValue> attrs;\n-  std::map<string, int> host_compute_core;\n-  std::vector<string> shape_inference_graphs;\n+  protobuf::Map<std::string, tensorflow::AttrValue> attrs;\n+  std::map<std::string, int> host_compute_core;\n+  std::vector<std::string> shape_inference_graphs;\n   bool has_outside_compilation;\n   NameAttrList name_attrs;\n   name_attrs.set_name(\"cluster\");\n@@ -498,7 +498,7 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, OutsideCompilationInIf) {\n     std::unique_ptr<FunctionBody> host_fbody;\n     AttrValue device_ordinal_temp_value;\n     device_ordinal_temp_value.set_i(0);\n-    protobuf::Map<string, AttrValue> host_func_attrs;\n+    protobuf::Map<std::string, AttrValue> host_func_attrs;\n     host_func_attrs[\"_device_ordinal\"] = device_ordinal_temp_value;\n     TF_CHECK_OK(FunctionDefToBodyHelper(*fld.Find(\"host_graph\"),\n                                         AttrSlice(&host_func_attrs), &fld,\n@@ -568,7 +568,7 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, OutsideCompilationInIf) {\n     // _xla_token_input_nodes.\n     Node *if_node = node_name_index[\"if\"];\n     EXPECT_NE(if_node, nullptr);\n-    std::vector<string> token_inputs;\n+    std::vector<std::string> token_inputs;\n     TF_CHECK_OK(\n         GetNodeAttr(if_node->def(), \"_xla_token_input_nodes\", &token_inputs));\n     EXPECT_THAT(token_inputs, ::testing::ElementsAre(\"send_oc_if_pred_if\"));\n@@ -631,9 +631,9 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, OutsideCompilationInWhile) {\n   }\n   FunctionLibraryDefinition fld(OpRegistry::Global(), fdl);\n \n-  protobuf::Map<string, tensorflow::AttrValue> attrs;\n-  std::map<string, int> host_compute_core;\n-  std::vector<string> shape_inference_graphs;\n+  protobuf::Map<std::string, tensorflow::AttrValue> attrs;\n+  std::map<std::string, int> host_compute_core;\n+  std::vector<std::string> shape_inference_graphs;\n   bool has_outside_compilation;\n   NameAttrList name_attrs;\n   name_attrs.set_name(\"cluster\");\n@@ -648,7 +648,7 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, OutsideCompilationInWhile) {\n     std::unique_ptr<FunctionBody> host_fbody;\n     AttrValue device_ordinal_temp_value;\n     device_ordinal_temp_value.set_i(0);\n-    protobuf::Map<string, AttrValue> host_func_attrs;\n+    protobuf::Map<std::string, AttrValue> host_func_attrs;\n     host_func_attrs[\"_device_ordinal\"] = device_ordinal_temp_value;\n     TF_CHECK_OK(FunctionDefToBodyHelper(*fld.Find(\"host_graph\"),\n                                         AttrSlice(&host_func_attrs), &fld,\n@@ -767,9 +767,9 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, OutsideCompilationInFunction) {\n     TF_CHECK_OK(fld.AddFunctionDef(*xla_fdef));\n   }\n \n-  protobuf::Map<string, tensorflow::AttrValue> attrs;\n-  std::map<string, int> host_compute_core;\n-  std::vector<string> shape_inference_graphs;\n+  protobuf::Map<std::string, tensorflow::AttrValue> attrs;\n+  std::map<std::string, int> host_compute_core;\n+  std::vector<std::string> shape_inference_graphs;\n   bool has_outside_compilation;\n   NameAttrList name_attrs;\n   name_attrs.set_name(\"cluster\");\n@@ -784,7 +784,7 @@ TEST_F(ExtractOutsideCompilationForFunctionTest, OutsideCompilationInFunction) {\n     std::unique_ptr<FunctionBody> host_fbody;\n     AttrValue device_ordinal_temp_value;\n     device_ordinal_temp_value.set_i(0);\n-    protobuf::Map<string, AttrValue> host_func_attrs;\n+    protobuf::Map<std::string, AttrValue> host_func_attrs;\n     host_func_attrs[\"_device_ordinal\"] = device_ordinal_temp_value;\n     TF_CHECK_OK(FunctionDefToBodyHelper(*fld.Find(\"host_graph\"),\n                                         AttrSlice(&host_func_attrs), &fld,\n@@ -873,9 +873,9 @@ TEST_F(ExtractOutsideCompilationForFunctionTest,\n   }\n   FunctionLibraryDefinition fld(OpRegistry::Global(), fdl);\n \n-  protobuf::Map<string, tensorflow::AttrValue> attrs;\n-  std::map<string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n-  std::vector<string> shape_inference_graphs;\n+  protobuf::Map<std::string, tensorflow::AttrValue> attrs;\n+  std::map<std::string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n+  std::vector<std::string> shape_inference_graphs;\n   bool has_outside_compilation;\n   NameAttrList name_attrs;\n   name_attrs.set_name(\"cluster\");\n@@ -898,14 +898,15 @@ TEST_F(ExtractOutsideCompilationForFunctionTest,\n   EXPECT_NE(host_compute_1, nullptr);\n \n   // Check XlaHostCompute nodes' \"_xla_token_input_nodes\" attr.\n-  std::vector<string> token_input_nodes;\n+  std::vector<std::string> token_input_nodes;\n   TF_CHECK_OK(GetNodeAttr(AttrSlice(host_compute_0->attrs()),\n                           \"_xla_token_input_nodes\", &token_input_nodes));\n \n-  std::vector<string> expected_token_input_nodes_0({\"_xla_token_arg_node\"});\n+  std::vector<std::string> expected_token_input_nodes_0(\n+      {\"_xla_token_arg_node\"});\n   EXPECT_EQ(token_input_nodes, expected_token_input_nodes_0);\n   token_input_nodes.clear();\n-  std::vector<string> expected_token_input_nodes_1(\n+  std::vector<std::string> expected_token_input_nodes_1(\n       {\"_xla_token_arg_node\", \"outside_compilation_0_host_compute\"});\n   TF_CHECK_OK(GetNodeAttr(AttrSlice(host_compute_1->attrs()),\n                           \"_xla_token_input_nodes\", &token_input_nodes));\n@@ -955,9 +956,9 @@ TEST_F(ExtractOutsideCompilationForFunctionTest,\n   }\n   FunctionLibraryDefinition fld(OpRegistry::Global(), fdl);\n \n-  protobuf::Map<string, tensorflow::AttrValue> attrs;\n-  std::map<string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n-  std::vector<string> shape_inference_graphs;\n+  protobuf::Map<std::string, tensorflow::AttrValue> attrs;\n+  std::map<std::string, int> host_compute_core = {{\"0\", 1}, {\"1\", 0}};\n+  std::vector<std::string> shape_inference_graphs;\n   bool has_outside_compilation;\n   NameAttrList name_attrs;\n   name_attrs.set_name(\"cluster\");\n@@ -980,14 +981,15 @@ TEST_F(ExtractOutsideCompilationForFunctionTest,\n   EXPECT_NE(host_compute_1, nullptr);\n \n   // Check XlaHostCompute nodes' \"_xla_token_input_nodes\" attr.\n-  std::vector<string> token_input_nodes;\n+  std::vector<std::string> token_input_nodes;\n   TF_CHECK_OK(GetNodeAttr(AttrSlice(host_compute_0->attrs()),\n                           \"_xla_token_input_nodes\", &token_input_nodes));\n \n-  std::vector<string> expected_token_input_nodes_0({\"_xla_token_arg_node\"});\n+  std::vector<std::string> expected_token_input_nodes_0(\n+      {\"_xla_token_arg_node\"});\n   EXPECT_EQ(token_input_nodes, expected_token_input_nodes_0);\n   token_input_nodes.clear();\n-  std::vector<string> expected_token_input_nodes_1(\n+  std::vector<std::string> expected_token_input_nodes_1(\n       {\"_xla_token_arg_node\", \"outside_compilation_0_host_compute\"});\n   TF_CHECK_OK(GetNodeAttr(AttrSlice(host_compute_1->attrs()),\n                           \"_xla_token_input_nodes\", &token_input_nodes));"
        },
        {
            "sha": "a0a0d45736f1e82bb812b83cb36417c9e370e2e2",
            "filename": "tensorflow/compiler/jit/flags.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fflags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fflags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fflags.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -46,7 +46,7 @@ std::vector<Flag>* jitrt_flag_list;\n std::vector<Flag>* flag_list;\n absl::once_flag flags_init;\n \n-bool SetterForXlaAutoJitFlag(const string& value) {\n+bool SetterForXlaAutoJitFlag(const std::string& value) {\n   int32_t opt_level;\n   // We need to use the mark_for_compilation_flags directly here instead of\n   // going via GetMarkForCompilationPassFlags() to avoid infinite recursion. The\n@@ -81,7 +81,7 @@ bool SetterForXlaAutoJitFlag(const string& value) {\n   return true;\n }\n \n-bool SetterForXlaCallModuleDisabledChecks(const string& value) {\n+bool SetterForXlaCallModuleDisabledChecks(const std::string& value) {\n   auto directives = absl::StrSplit(value, ',', absl::SkipEmpty());\n   call_module_flags->disabled_checks.insert(directives.begin(),\n                                             directives.end());\n@@ -231,7 +231,7 @@ void AllocateAndParseFlags() {\n   mark_for_compilation_flags->xla_auto_jit_flag.optimization_level_general = 0;\n   mark_for_compilation_flags->tf_xla_min_cluster_size = 4;\n   mark_for_compilation_flags->tf_xla_max_cluster_size =\n-      std::numeric_limits<int32>::max();\n+      std::numeric_limits<int32_t>::max();\n   mark_for_compilation_flags->tf_xla_clustering_debug = false;\n   mark_for_compilation_flags->tf_xla_cpu_global_jit = false;\n   mark_for_compilation_flags->tf_xla_clustering_fuel =\n@@ -463,7 +463,7 @@ void ResetFlags() {\n \n }  // namespace\n \n-bool SetXlaAutoJitFlagFromFlagString(const string& value) {\n+bool SetXlaAutoJitFlagFromFlagString(const std::string& value) {\n   absl::call_once(flags_init, &AllocateAndParseFlags);\n   return SetterForXlaAutoJitFlag(value);\n }"
        },
        {
            "sha": "96154b892ae5b0e0c23019b967406f14aa115469",
            "filename": "tensorflow/compiler/jit/flags.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fflags.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fflags.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fflags.h?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -41,32 +41,32 @@ struct XlaAutoJitFlag {\n   // `optimization_level_general` applies.\n   //\n   // Experimental.\n-  int32 optimization_level_single_gpu;\n-  int32 optimization_level_general;\n+  int32_t optimization_level_single_gpu;\n+  int32_t optimization_level_general;\n };\n \n // Sets the xla_auto_jit_flag based on the given flag string. Supported syntax\n // is:\n // <number>: sets general and single_gpu setting to the provided number.\n // single-gpu(<number>): sets the single_gpu setting to the provided number.\n-bool SetXlaAutoJitFlagFromFlagString(const string& value);\n+bool SetXlaAutoJitFlagFromFlagString(const std::string& value);\n \n // Flags associated with the XLA bridge's mark_for_compilation_pass module.\n struct MarkForCompilationPassFlags {\n   XlaAutoJitFlag xla_auto_jit_flag;\n \n   // Minimum number of operators in an XLA compilation. Ignored for operators\n   // placed on an XLA device or operators explicitly marked for compilation.\n-  int32 tf_xla_min_cluster_size;\n+  int32_t tf_xla_min_cluster_size;\n \n   // Maximum number of operators in an XLA compilation.\n-  int32 tf_xla_max_cluster_size;\n+  int32_t tf_xla_max_cluster_size;\n \n   // If non-empty, limit XLA clustering to the following TF operations.\n-  string tf_xla_ops_to_cluster;\n+  std::string tf_xla_ops_to_cluster;\n \n   // If non-empty, remove following operations from XLA clustering excludelist.\n-  string tf_xla_cluster_exclude_ops;\n+  std::string tf_xla_cluster_exclude_ops;\n \n   // Dump graphs during XLA compilation.\n   bool tf_xla_clustering_debug;\n@@ -110,7 +110,7 @@ struct MarkForCompilationPassFlags {\n   bool tf_xla_disable_strict_signature_checks;\n \n   // Specifies the persistance cache prefix. Default is \"xla_compile_cache\"\n-  string tf_xla_persistent_cache_prefix;\n+  std::string tf_xla_persistent_cache_prefix;\n };\n \n // Flags associated with XLA Sparse Core."
        },
        {
            "sha": "1b0239c35509708a75658a457abfcde53c301f06",
            "filename": "tensorflow/compiler/jit/force_xla_constants_on_host_pass_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fforce_xla_constants_on_host_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fforce_xla_constants_on_host_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fforce_xla_constants_on_host_pass_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -95,7 +95,7 @@ TEST(ForceXlaConstantsOnHostPassTest, Simple) {\n     if (CanCreateXlaKernel(node->def())) {\n       EXPECT_FALSE(found);\n       found = true;\n-      std::vector<int32> hostmem_attr;\n+      std::vector<int32_t> hostmem_attr;\n       EXPECT_TRUE(TryGetNodeAttr(node->def(), \"_input_hostmem\", &hostmem_attr));\n       EXPECT_EQ(hostmem_attr.size(), 1);\n       EXPECT_EQ(hostmem_attr[0], 1);"
        },
        {
            "sha": "03a7d1081b8b539337e4eebfc535e23b6377ba7a",
            "filename": "tensorflow/compiler/jit/increase_dynamism_for_auto_jit_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fincrease_dynamism_for_auto_jit_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fincrease_dynamism_for_auto_jit_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fincrease_dynamism_for_auto_jit_pass.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -93,7 +93,7 @@ std::vector<int64_t> IntTensorAsVector(const Tensor& t) {\n   result.reserve(t.NumElements());\n   for (int i = 0; i < t.NumElements(); i++) {\n     int64_t element = t.dtype() == DT_INT32\n-                          ? static_cast<int64_t>(t.flat<int32>()(i))\n+                          ? static_cast<int64_t>(t.flat<int32_t>()(i))\n                           : t.flat<int64_t>()(i);\n     result.push_back(element);\n   }\n@@ -251,14 +251,14 @@ absl::Status ComputeSliceSize(const Scope& host_scope,\n absl::Status ConvertTensorFlowSliceToStaticShapedSlice(\n     Graph* g, Node* slice, const SliceInputs& slice_inputs,\n     absl::string_view cluster_name, Node** result) {\n-  string host_name;\n+  std::string host_name;\n   TF_RETURN_IF_ERROR(DeviceNameUtils::DeviceNameToCpuDeviceName(\n       slice->assigned_device_name(), &host_name));\n \n   absl::Status status;\n   Scope main_scope =\n       NewInternalScope(g, &status, /*refiner=*/nullptr)\n-          .WithXlaCluster(string(cluster_name))\n+          .WithXlaCluster(std::string(cluster_name))\n           .NewSubScope(absl::StrCat(slice->name(), \"/static_shaped_slice\"));\n   Scope host_scope = main_scope.WithAssignedDevice(host_name);\n \n@@ -286,7 +286,7 @@ absl::Status ConvertTensorFlowSliceToStaticShapedSlice(\n \n   TF_RETURN_IF_ERROR(main_scope.status());\n \n-  std::vector<string> compile_time_const_inputs;\n+  std::vector<std::string> compile_time_const_inputs;\n   compile_time_const_inputs.push_back(\"size\");\n   (*result)->AddAttr(kXlaCompileTimeConstantInputsAttr,\n                      compile_time_const_inputs);"
        },
        {
            "sha": "6a8523a7d4c893e170b606cf03f2b9e7466e7df2",
            "filename": "tensorflow/compiler/jit/increase_dynamism_for_auto_jit_pass_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fincrease_dynamism_for_auto_jit_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d69bbe355e5132e77fb858f8453ce2d4ddac5a5b/tensorflow%2Fcompiler%2Fjit%2Fincrease_dynamism_for_auto_jit_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fincrease_dynamism_for_auto_jit_pass_test.cc?ref=d69bbe355e5132e77fb858f8453ce2d4ddac5a5b",
            "patch": "@@ -66,7 +66,8 @@ class FakeDevice : public Device {\n \n   Allocator* GetAllocator(AllocatorAttributes attr) override { return nullptr; }\n \n-  static std::unique_ptr<Device> Make(const string& name, const string& type) {\n+  static std::unique_ptr<Device> Make(const std::string& name,\n+                                      const std::string& type) {\n     DeviceAttributes device_attributes;\n     device_attributes.set_name(name);\n     device_attributes.set_device_type(DeviceType(type).type());\n@@ -100,7 +101,7 @@ absl::Status IncreaseDynamismForAutoJit(const Scope& s,\n \n   // Scope::ToGraph seems to drop assigned devices, probably because it goes\n   // through a GraphDef.  So explicitly maintain the device assignment.\n-  std::unordered_map<string, string> assigned_device_names;\n+  std::unordered_map<std::string, std::string> assigned_device_names;\n   for (Node* n : s.graph()->nodes()) {\n     assigned_device_names[n->name()] = n->assigned_device_name();\n   }\n@@ -149,7 +150,7 @@ TEST(SliceToDynamicSliceRewriteTest, Basic) {\n                    Inputs(m_slice_size_0, Const(static_cast<int64_t>(500)),\n                           Const(zero_32))));\n \n-  std::vector<string> compile_time_constant_inputs;\n+  std::vector<std::string> compile_time_constant_inputs;\n   compile_time_constant_inputs.push_back(\"size\");\n   auto m_dynamic_slice = NodeWith(\n       Op(\"Slice\"), AssignedDevice(kDeviceName),"
        }
    ],
    "stats": {
        "total": 1158,
        "additions": 609,
        "deletions": 549
    }
}