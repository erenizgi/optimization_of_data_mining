{
    "author": "tensorflower-gardener",
    "message": "Use absl::Status factory methods for constructing errors, replacing equivalent tensorflow::errors:: calls.\n\nPiperOrigin-RevId: 823954871",
    "sha": "b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
    "files": [
        {
            "sha": "6cf8994f6fa3913e1d02eedf286f8fbdfc6af3b1",
            "filename": "tensorflow/core/kernels/BUILD",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2FBUILD?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -1067,7 +1067,10 @@ tf_kernel_library(\n tf_kernel_library(\n     name = \"gather_op\",\n     prefix = \"gather_op\",\n-    deps = ARRAY_DEPS,\n+    deps = ARRAY_DEPS + [\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+    ],\n )\n \n tf_kernel_library(\n@@ -1357,6 +1360,8 @@ tf_kernel_library(\n     srcs = [\"ragged_gather_op.cc\"],\n     deps = [\n         \"//tensorflow/core:framework\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n \n@@ -2744,6 +2749,8 @@ tf_kernel_library(\n         \":gather_functor\",\n         \":gpu_prim_hdrs\",\n         \"//tensorflow/core:framework_internal\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n \n@@ -2963,6 +2970,7 @@ tf_kernel_library(\n         \"//tensorflow/core:lib_internal\",\n         \"//tensorflow/core/framework:bounds_check\",\n         \"//tensorflow/core/util:determinism_for_kernels\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/synchronization\",\n     ],\n@@ -5462,6 +5470,8 @@ tf_kernel_library(\n     deps = STATE_DEPS + [\n         \":loose_headers\",\n         \"//tensorflow/core/util:determinism_for_kernels\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n \n@@ -5471,6 +5481,8 @@ tf_kernel_library(\n     deps = STATE_DEPS + [\n         \"//tensorflow/core/framework:op_requires\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n \n@@ -5886,6 +5898,8 @@ tf_kernel_library(\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core/framework:bounds_check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n         \"@eigen_archive//:eigen3\",\n     ],\n )\n@@ -6512,7 +6526,6 @@ filegroup(\n         \"variable_ops.h\",\n         \"variant_ops_util.cc\",\n         \"variant_ops_util.h\",\n-    ] + [\n         \"//tensorflow/c/kernels:portable_all_op_kernels\",\n         \"//tensorflow/core/kernels/image:non_max_suppression_op.cc\",\n         \"//tensorflow/core/kernels/image:non_max_suppression_op.h\","
        },
        {
            "sha": "aca59d5acd369eada5290c8969986cf9fad7efc9",
            "filename": "tensorflow/core/kernels/count_ops.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 40,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fcount_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fcount_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcount_ops.cc?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -19,6 +19,8 @@ limitations under the License.\n #define EIGEN_USE_THREADS\n \n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n@@ -114,9 +116,9 @@ class DenseCount : public OpKernel {\n     OP_REQUIRES(context,\n                 TensorShapeUtils::IsVector(data.shape()) ||\n                     TensorShapeUtils::IsMatrix(data.shape()),\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Input must be a 1 or 2-dimensional tensor. Got: \",\n-                    data.shape().DebugString()));\n+                    data.shape().DebugString())));\n \n     // Ensure all values are non-negative.\n     const auto data_values = data.flat<T>();\n@@ -125,15 +127,15 @@ class DenseCount : public OpKernel {\n         (data_values >= static_cast<T>(0)).all();\n     OP_REQUIRES(\n         context, nonnegative(),\n-        errors::InvalidArgument(\"Input values must all be non-negative\"));\n+        absl::InvalidArgumentError(\"Input values must all be non-negative\"));\n \n     if (use_weights) {\n       OP_REQUIRES(\n           context, weights.shape() == data.shape(),\n-          errors::InvalidArgument(\n+          absl::InvalidArgumentError(absl::StrCat(\n               \"Weights and data must have the same shape. Weight shape: \",\n               weights.shape().DebugString(),\n-              \"; data shape: \", data.shape().DebugString()));\n+              \"; data shape: \", data.shape().DebugString())));\n     }\n \n     bool is_1d = TensorShapeUtils::IsVector(data.shape());\n@@ -143,7 +145,7 @@ class DenseCount : public OpKernel {\n     int num_batch_elements = 1;\n     for (int i = 0; i < num_batch_dimensions; ++i) {\n       OP_REQUIRES(context, data.shape().dim_size(i) != 0,\n-                  errors::InvalidArgument(\n+                  absl::InvalidArgumentError(\n                       \"Invalid input: Shapes dimension cannot be 0.\"));\n       num_batch_elements *= data.shape().dim_size(i);\n     }\n@@ -202,29 +204,31 @@ class SparseCount : public OpKernel {\n     bool use_weights = weights.NumElements() > 0;\n \n     OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Input indices must be a 2-dimensional tensor. Got: \",\n-                    indices.shape().DebugString()));\n+                    indices.shape().DebugString())));\n     OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n-                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n-                                        values.shape().DebugString()));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Input values must be a vector. Got: \",\n+                                 values.shape().DebugString())));\n     OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n-                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n-                                        shape.shape().DebugString()));\n-    OP_REQUIRES(context,\n-                values.shape().dim_size(0) == indices.shape().dim_size(0),\n-                errors::InvalidArgument(\n-                    \"Number of values must match first dimension of indices.\",\n-                    \"Got \", values.shape().dim_size(0),\n-                    \" values, indices shape: \", indices.shape().DebugString()));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Input shape must be a vector. Got: \",\n+                                 shape.shape().DebugString())));\n+    OP_REQUIRES(\n+        context, values.shape().dim_size(0) == indices.shape().dim_size(0),\n+        absl::InvalidArgumentError(absl::StrCat(\n+            \"Number of values must match first dimension of indices.\", \"Got \",\n+            values.shape().dim_size(0),\n+            \" values, indices shape: \", indices.shape().DebugString())));\n     OP_REQUIRES(\n         context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n-        errors::InvalidArgument(\n+        absl::InvalidArgumentError(absl::StrCat(\n             \"Number of dimensions must match second dimension of indices.\",\n             \"Got \", shape.shape().dim_size(0),\n-            \" dimensions, indices shape: \", indices.shape().DebugString()));\n+            \" dimensions, indices shape: \", indices.shape().DebugString())));\n     OP_REQUIRES(context, shape.NumElements() > 0,\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(\n                     \"The shape argument requires at least one element.\"));\n     // Validate indices: each index must be valid for the corresponding\n     // dimension. This could be possibly done better.\n@@ -237,10 +241,10 @@ class SparseCount : public OpKernel {\n         OP_REQUIRES(\n             context,\n             indices_values(i, j) >= 0 && indices_values(i, j) < shape_vector(j),\n-            errors::InvalidArgument(\n+            absl::InvalidArgumentError(absl::StrCat(\n                 \"Invalid index value at \", i, \": dimension \", j, \" has value \",\n                 indices_values(i, j), \" which is not in [0, \", shape_vector(j),\n-                \") (as given by dense shape \", shape.DebugString()));\n+                \") (as given by dense shape \", shape.DebugString())));\n       }\n     }\n \n@@ -251,23 +255,23 @@ class SparseCount : public OpKernel {\n         (values_values >= static_cast<T>(0)).all();\n     OP_REQUIRES(\n         context, nonnegative(),\n-        errors::InvalidArgument(\"Input values must all be non-negative\"));\n+        absl::InvalidArgumentError(\"Input values must all be non-negative\"));\n \n     if (use_weights) {\n       OP_REQUIRES(\n           context, weights.shape() == values.shape(),\n-          errors::InvalidArgument(\n+          absl::InvalidArgumentError(absl::StrCat(\n               \"Weights and values must have the same shape. Weight shape: \",\n               weights.shape().DebugString(),\n-              \"; values shape: \", values.shape().DebugString()));\n+              \"; values shape: \", values.shape().DebugString())));\n     }\n \n     bool is_1d = shape.NumElements() == 1;\n     int num_batches = is_1d ? 1 : shape_vector(0);\n-    OP_REQUIRES(\n-        context, 0 < num_batches && num_batches < kMaxBatches,\n-        errors::InvalidArgument(\"Cannot allocate \", num_batches,\n-                                \" batches, is the dense shape too wide?\"));\n+    OP_REQUIRES(context, 0 < num_batches && num_batches < kMaxBatches,\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Cannot allocate \", num_batches,\n+                                 \" batches, is the dense shape too wide?\")));\n \n     const auto weight_values = weights.flat<W>();\n \n@@ -279,11 +283,11 @@ class SparseCount : public OpKernel {\n       int batch = is_1d ? 0 : indices_values(idx, 0);\n       if (batch >= num_batches) {\n         OP_REQUIRES(context, batch < num_batches,\n-                    errors::InvalidArgument(\n+                    absl::InvalidArgumentError(absl::StrCat(\n                         \"Indices value along the first dimension must be \",\n                         \"lower than the first index of the shape.\", \"Got \",\n                         batch, \" as batch and \", num_batches,\n-                        \" as the first dimension of the shape.\"));\n+                        \" as the first dimension of the shape.\")));\n       }\n       const auto& value = values_values(idx);\n       if (maxlength_ < 0 || value < maxlength_) {\n@@ -332,10 +336,10 @@ class RaggedCount : public OpKernel {\n     if (use_weights) {\n       OP_REQUIRES(\n           context, weights.shape() == values.shape(),\n-          errors::InvalidArgument(\n+          absl::InvalidArgumentError(absl::StrCat(\n               \"Weights and values must have the same shape. Weight shape: \",\n               weights.shape().DebugString(),\n-              \"; values shape: \", values.shape().DebugString()));\n+              \"; values shape: \", values.shape().DebugString())));\n     }\n \n     const auto splits_values = splits.flat<int64_t>();\n@@ -346,23 +350,23 @@ class RaggedCount : public OpKernel {\n \n     OP_REQUIRES(\n         context, num_batches > 0,\n-        errors::InvalidArgument(\n+        absl::InvalidArgumentError(\n             \"Must provide at least 2 elements for the splits argument\"));\n     OP_REQUIRES(context, splits_values(0) == 0,\n-                errors::InvalidArgument(\"Splits must start with 0, not with \",\n-                                        splits_values(0)));\n+                absl::InvalidArgumentError(absl::StrCat(\n+                    \"Splits must start with 0, not with \", splits_values(0))));\n     OP_REQUIRES(context, splits_values(num_batches) == num_values,\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Splits must end with the number of values, got \",\n-                    splits_values(num_batches), \" instead of \", num_values));\n+                    splits_values(num_batches), \" instead of \", num_values)));\n \n     // Ensure all values are non-negative.\n     Eigen::TensorFixedSize<bool, Eigen::Sizes<>, Eigen::RowMajor> nonnegative;\n     nonnegative.device(context->eigen_cpu_device()) =\n         (values_values >= static_cast<T>(0)).all();\n     OP_REQUIRES(\n         context, nonnegative(),\n-        errors::InvalidArgument(\"Input values must all be non-negative\"));\n+        absl::InvalidArgumentError(\"Input values must all be non-negative\"));\n \n     auto per_batch_counts = BatchedMap<W>(num_batches);\n     T max_value = 0;"
        },
        {
            "sha": "366f67f13c87695a6e601aad78eb7cde99e39cc4",
            "filename": "tensorflow/core/kernels/dynamic_partition_op.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fdynamic_partition_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fdynamic_partition_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdynamic_partition_op.cc?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -16,6 +16,9 @@ limitations under the License.\n // See docs in ../ops/data_flow_ops.cc.\n \n #include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/bounds_check.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n@@ -47,10 +50,10 @@ class DynamicPartitionOp_Shared : public OpKernel {\n     OP_REQUIRES(\n         c,\n         TensorShapeUtils::StartsWith((*data)->shape(), (*partitions)->shape()),\n-        errors::InvalidArgument(\n+        absl::InvalidArgumentError(absl::StrCat(\n             \"data.shape must start with partitions.shape, \",\n             \"got data.shape = \", (*data)->shape().DebugString(),\n-            \", partitions.shape = \", (*partitions)->shape().DebugString()));\n+            \", partitions.shape = \", (*partitions)->shape().DebugString())));\n \n     // Count how many occurrences of each partition id we have in partitions\n     absl::InlinedVector<int, 32UL> partition_count(num_partitions_);\n@@ -59,9 +62,9 @@ class DynamicPartitionOp_Shared : public OpKernel {\n     for (int64_t i = 0; i < N; i++) {\n       const int32_t p = internal::SubtleMustCopy(e_partitions(i));\n       OP_REQUIRES(c, FastBoundsCheck(p, num_partitions_),\n-                  errors::InvalidArgument(\n+                  absl::InvalidArgumentError(absl::StrCat(\n                       \"partitions\", SliceDebugString((*partitions)->shape(), i),\n-                      \" = \", p, \" is not in [0, \", num_partitions_, \")\"));\n+                      \" = \", p, \" is not in [0, \", num_partitions_, \")\")));\n       partition_count[p]++;\n     }\n \n@@ -111,14 +114,14 @@ class DynamicPartitionOp : public DynamicPartitionOp_Shared {\n       }\n       for (int64_t i = 0; i < N; i++) {\n         const int32_t p = internal::SubtleMustCopy(e_partitions(i));\n-        OP_REQUIRES(\n-            c, FastBoundsCheck(p, num_partitions_),\n-            errors::InvalidArgument(\"indices[\", i, \"] is out of range\"));\n+        OP_REQUIRES(c, FastBoundsCheck(p, num_partitions_),\n+                    absl::InvalidArgumentError(\n+                        absl::StrCat(\"indices[\", i, \"] is out of range\")));\n         auto oi = output_index[p];\n         OP_REQUIRES(c, FastBoundsCheck(oi, out_vec[p].size()),\n-                    errors::InvalidArgument(\n+                    absl::InvalidArgumentError(absl::StrCat(\n                         \"out_vec[\", p, \"] size: \", out_vec[p].size(),\n-                        \" is not LTE output_index[\", p, \"] : \", oi));\n+                        \" is not LTE output_index[\", p, \"] : \", oi)));\n         out_vec[p](oi) = data_flat(i);\n         output_index[p]++;\n       }\n@@ -139,15 +142,16 @@ class DynamicPartitionOp : public DynamicPartitionOp_Shared {\n       for (int64_t i = 0; i < N; i++) {\n         // outputs[p][output_index[p]++] = data[i]\n         const int32_t p = internal::SubtleMustCopy(e_partitions(i));\n-        OP_REQUIRES(\n-            c, FastBoundsCheck(p, num_partitions_),\n-            errors::InvalidArgument(\"indices[\", i,\n-                                    \"] has been asynchronously overwritten and \"\n-                                    \"is no longer in range!\"));\n+        OP_REQUIRES(c, FastBoundsCheck(p, num_partitions_),\n+                    absl::InvalidArgumentError(absl::StrCat(\n+                        \"indices[\", i,\n+                        \"] has been asynchronously overwritten and \"\n+                        \"is no longer in range!\")));\n         auto oi = output_index[p];\n-        OP_REQUIRES(c, FastBoundsCheck(oi, out_flat[p].dimension(0)),\n-                    errors::InvalidArgument(\"Size of output_index: \", oi,\n-                                            \" is no longer in range.\"));\n+        OP_REQUIRES(\n+            c, FastBoundsCheck(oi, out_flat[p].dimension(0)),\n+            absl::InvalidArgumentError(absl::StrCat(\n+                \"Size of output_index: \", oi, \" is no longer in range.\")));\n         Eigen::DSizes<Eigen::DenseIndex, 2> out_indices(oi, 0);\n         Eigen::DSizes<Eigen::DenseIndex, 2> data_indices(i, 0);\n         out_flat[p].slice(out_indices, sizes) ="
        },
        {
            "sha": "768eb74fb9f2cd4ce08a7c827308410cf7909b7a",
            "filename": "tensorflow/core/kernels/gather_op.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 29,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fgather_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fgather_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fgather_op.cc?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -15,11 +15,16 @@ limitations under the License.\n \n // See docs in ../ops/array_ops.cc.\n \n+#include <limits>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/bounds_check.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/framework/variant.h\"\n #include \"tensorflow/core/framework/variant_encode_decode.h\"\n #include \"tensorflow/core/kernels/gather_functor.h\"\n@@ -56,7 +61,7 @@ class GatherOp : public OpKernel {\n     const Tensor& indices = c->input(1);\n     OP_REQUIRES(\n         c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n-        errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n+        absl::InvalidArgumentError(\"params must be at least 1 dimensional\"));\n \n     // GatherV2 added an axis argument. For backwards compatibility with Gather,\n     // fall back to axis 0 if the op does not have an axis input.\n@@ -66,26 +71,26 @@ class GatherOp : public OpKernel {\n       axis_is_set = true;\n       const Tensor& axis_tensor = c->input(2);\n       OP_REQUIRES(c, TensorShapeUtils::IsScalar(axis_tensor.shape()),\n-                  errors::InvalidArgument(\"axis must be scalar\"));\n+                  absl::InvalidArgumentError(\"axis must be scalar\"));\n \n       if (axis_tensor.dtype() == DT_INT32) {\n         axis = axis_tensor.scalar<int32>()();\n       } else if (axis_tensor.dtype() == DT_INT64) {\n         axis = axis_tensor.scalar<int64_t>()();\n       } else {\n         OP_REQUIRES(c, false,\n-                    errors::InvalidArgument(\"axis must be int32 or int64.\"));\n+                    absl::InvalidArgumentError(\"axis must be int32 or int64.\"));\n       }\n     }\n     // special case to avoid checkfail when axis = kint64max.\n     OP_REQUIRES(c, axis < kint64max,\n                 absl::InvalidArgumentError(\"axis must be less than kint64max\"));\n \n     int64_t min_params_dim = axis < 0 ? -axis : axis + 1;\n-    OP_REQUIRES(\n-        c, params.dims() >= min_params_dim,\n-        errors::InvalidArgument(\"Shape must be at least rank \", min_params_dim,\n-                                \" but is rank \", params.dims()));\n+    OP_REQUIRES(c, params.dims() >= min_params_dim,\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Shape must be at least rank \", min_params_dim,\n+                                 \" but is rank \", params.dims())));\n \n     if (axis < 0) {\n       axis = params.dims() + axis;\n@@ -96,43 +101,45 @@ class GatherOp : public OpKernel {\n     if (batch_dims != 0) {\n       OP_REQUIRES(c,\n                   batch_dims >= -indices.dims() && batch_dims <= indices.dims(),\n-                  errors::InvalidArgument(\"Expected batch_dims in the range [\",\n-                                          -indices.dims(), \", \", indices.dims(),\n-                                          \"], but got \", batch_dims));\n+                  absl::InvalidArgumentError(absl::StrCat(\n+                      \"Expected batch_dims in the range [\", -indices.dims(),\n+                      \", \", indices.dims(), \"], but got \", batch_dims)));\n \n       if (batch_dims < 0) {\n         batch_dims = indices.dims() + batch_dims;\n       }\n \n       if (!axis_is_set) axis = batch_dims;\n \n-      OP_REQUIRES(c, batch_dims < params.dims(),\n-                  errors::InvalidArgument(\"batch_dims (\", batch_dims,\n-                                          \") must be less than rank(params) (\",\n-                                          params.dims(), \").\"));\n-\n-      OP_REQUIRES(c, axis >= batch_dims,\n-                  errors::InvalidArgument(\"batch_dims (\", batch_dims,\n-                                          \") must be less than or equal to \",\n-                                          \"axis (\", axis, \").\"));\n+      OP_REQUIRES(\n+          c, batch_dims < params.dims(),\n+          absl::InvalidArgumentError(absl::StrCat(\n+              \"batch_dims (\", batch_dims, \") must be less than rank(params) (\",\n+              params.dims(), \").\")));\n+\n+      OP_REQUIRES(\n+          c, axis >= batch_dims,\n+          absl::InvalidArgumentError(absl::StrCat(\n+              \"batch_dims (\", batch_dims, \") must be less than or equal to \",\n+              \"axis (\", axis, \").\")));\n       for (int i = 0; i < batch_dims; ++i) {\n         OP_REQUIRES(c, params.dim_size(i) == indices.dim_size(i),\n-                    errors::InvalidArgument(\n+                    absl::InvalidArgumentError(absl::StrCat(\n                         \"params.shape[\", i, \"]: \", params.dim_size(i),\n                         \" should be equal to indices.shape[\", i,\n-                        \"]: \", indices.dim_size(i)));\n+                        \"]: \", indices.dim_size(i))));\n       }\n     }\n \n     // Check that we have enough index space\n     int64_t gather_dim_size = params.dim_size(axis);\n     const int64_t N = indices.NumElements();\n-    OP_REQUIRES(\n-        c, gather_dim_size <= std::numeric_limits<Index>::max(),\n-        errors::InvalidArgument(\"params.shape[\", axis, \"] too large for \",\n-                                DataTypeString(DataTypeToEnum<Index>::v()),\n-                                \" indexing: \", gather_dim_size, \" > \",\n-                                std::numeric_limits<Index>::max()));\n+    OP_REQUIRES(c, gather_dim_size <= std::numeric_limits<Index>::max(),\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"params.shape[\", axis, \"] too large for \",\n+                                 DataTypeString(DataTypeToEnum<Index>::v()),\n+                                 \" indexing: \", gather_dim_size, \" > \",\n+                                 std::numeric_limits<Index>::max())));\n \n     // The result shape is params.shape[:axis] + indices.shape[batch_dims:] +\n     // params.shape[axis + 1:].\n@@ -182,9 +189,9 @@ class GatherOp : public OpKernel {\n     }\n     OP_REQUIRES(\n         c, bad_i < 0,\n-        errors::InvalidArgument(\n+        absl::InvalidArgumentError(absl::StrCat(\n             \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n-            indices_flat(bad_i), \" is not in [0, \", gather_dim_size, \")\"));\n+            indices_flat(bad_i), \" is not in [0, \", gather_dim_size, \")\")));\n   }\n \n  private:"
        },
        {
            "sha": "7e218941a49e9a695e77d70a89393a932321b0a6",
            "filename": "tensorflow/core/kernels/ragged_gather_op.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 11,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fragged_gather_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fragged_gather_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_gather_op.cc?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -17,6 +17,8 @@ limitations under the License.\n #include <string>\n #include <vector>\n \n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n@@ -64,20 +66,21 @@ class RaggedGatherOpBase : public OpKernel {\n                                                 &params_nested_splits_in));\n     OP_REQUIRES(\n         context, params_nested_splits_in.size() > 0,\n-        errors::InvalidArgument(\"params_nested_splits must be non empty\"));\n+        absl::InvalidArgumentError(\"params_nested_splits must be non empty\"));\n \n     const Tensor& params_dense_values_in =\n         context->input(params_nested_splits_in.size());\n     const Tensor& indices_in =\n         context->input(params_nested_splits_in.size() + 1);\n \n-    OP_REQUIRES(context, params_nested_splits_in[0].dims() > 0,\n-                errors::InvalidArgument(\"Split tensors must not be scalars\"));\n+    OP_REQUIRES(\n+        context, params_nested_splits_in[0].dims() > 0,\n+        absl::InvalidArgumentError(\"Split tensors must not be scalars\"));\n     SPLITS_TYPE num_params = params_nested_splits_in[0].dim_size(0) - 1;\n     OP_REQUIRES_OK(context, ValidateIndices(indices_in, num_params));\n \n     OP_REQUIRES(context, params_dense_values_in.dims() > 0,\n-                errors::InvalidArgument(\"params.rank must be nonzero\"));\n+                absl::InvalidArgumentError(\"params.rank must be nonzero\"));\n     SPLITS_TYPE num_params_dense_values = params_dense_values_in.dim_size(0);\n \n     // Calculate the `splits`, and store the value slices that we need to\n@@ -106,9 +109,9 @@ class RaggedGatherOpBase : public OpKernel {\n     for (SPLITS_TYPE i = 0; i < indices.size(); ++i) {\n       SPLITS_TYPE index = indices(i);\n       if (index < 0 || index >= num_params) {\n-        return errors::InvalidArgument(\n-            \"indices\", SliceDebugString(indices_in.shape(), i), \" = \", index,\n-            \" is not in [0, \", num_params, \")\");\n+        return absl::InvalidArgumentError(\n+            absl::StrCat(\"indices\", SliceDebugString(indices_in.shape(), i),\n+                         \" = \", index, \" is not in [0, \", num_params, \")\"));\n       }\n     }\n     return absl::OkStatus();\n@@ -201,18 +204,18 @@ class RaggedGatherOpBase : public OpKernel {\n                                    ? num_params_dense_values\n                                    : params_nested_splits[dim + 1].size();\n       if (splits.size() == 0) {\n-        return errors::InvalidArgument(\"Ragged splits may not be empty\");\n+        return absl::InvalidArgumentError(\"Ragged splits may not be empty\");\n       }\n       if (splits(0) < 0) {\n-        return errors::InvalidArgument(\"Ragged splits must be non-negative\");\n+        return absl::InvalidArgumentError(\"Ragged splits must be non-negative\");\n       }\n       if (splits(splits.size() - 1) > last_split) {\n-        return errors::InvalidArgument(\n+        return absl::InvalidArgumentError(\n             \"Ragged splits must not point past values\");\n       }\n       for (int i = 1; i < splits.size(); ++i) {\n         if (splits(i - 1) > splits(i)) {\n-          return errors::InvalidArgument(\"Ragged splits must be sorted\");\n+          return absl::InvalidArgumentError(\"Ragged splits must be sorted\");\n         }\n       }\n     }"
        },
        {
            "sha": "f414b43df992b8e33025b1f2163f8e76695d51f1",
            "filename": "tensorflow/core/kernels/resource_variable_ops.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 75,
            "changes": 153,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fresource_variable_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fresource_variable_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fresource_variable_ops.cc?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -45,11 +45,6 @@ limitations under the License.\n //   (use_locking=false), we never copy even if the variable's\n //   reference count is >1.\n \n-#include \"absl/status/status.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/synchronization/notification.h\"\n-#include \"tensorflow/core/framework/op_requires.h\"\n-#include \"tensorflow/core/framework/types.pb.h\"\n #define EIGEN_USE_THREADS\n \n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n@@ -58,18 +53,25 @@ limitations under the License.\n #include \"tensorflow/core/platform/stream_executor.h\"\n #endif\n \n+#include <limits>\n #include <memory>\n #include <type_traits>\n #include <vector>\n \n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n+#include \"absl/synchronization/notification.h\"\n #include \"tensorflow/core/common_runtime/device.h\"\n #include \"tensorflow/core/framework/bounds_check.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/resource_mgr.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n #include \"tensorflow/core/framework/variant_op_registry.h\"\n #include \"tensorflow/core/kernels/dense_update_functor.h\"\n #include \"tensorflow/core/kernels/gather_functor.h\"\n@@ -139,7 +141,8 @@ absl::Status CopyVariable(int output_idx, OpKernelContext* ctx,\n       TF_CALL_uint2(HANDLER);\n #undef HANDLER\n       default:\n-        return errors::Internal(\"Unsupported dtype\", t->dtype());\n+        return absl::InternalError(\n+            absl::StrCat(\"Unsupported dtype: \", DataTypeString(t->dtype())));\n     }\n   }\n   return absl::OkStatus();\n@@ -152,12 +155,12 @@ void ReadVariableOp::Compute(OpKernelContext* ctx) {\n   const ResourceHandle& handle = HandleFromInput(ctx, 0);\n   const auto status = LookupResource(ctx, handle, &variable);\n   OP_REQUIRES(ctx, status.ok(),\n-              errors::FailedPrecondition(\n+              absl::FailedPreconditionError(absl::StrCat(\n                   \"Could not find variable \", handle.name(), \". \",\n                   \"This could mean that the variable has been deleted. \",\n                   \"In TF1, it can also mean the variable is uninitialized. \",\n                   \"Debug info: container=\", handle.container(),\n-                  \", status error message=\", status.message()));\n+                  \", status error message=\", status.message())));\n \n   tf_shared_lock ml(*variable->mu());\n   // We're acquiring a reference to the underlying buffer while\n@@ -167,9 +170,9 @@ void ReadVariableOp::Compute(OpKernelContext* ctx) {\n   if (!variable->copy_on_read_mode.load()) {\n     OP_REQUIRES(\n         ctx, dtype_ == t->dtype(),\n-        errors::InvalidArgument(\n+        absl::InvalidArgumentError(absl::StrCat(\n             \"Trying to read variable with wrong dtype. Expected \",\n-            DataTypeString(dtype_), \" got \", DataTypeString(t->dtype())));\n+            DataTypeString(dtype_), \" got \", DataTypeString(t->dtype()))));\n     ctx->set_output(0, *t);\n   } else {\n     OP_REQUIRES_OK(ctx, CopyVariable(0, ctx, t));\n@@ -181,9 +184,9 @@ ReadVariablesOp::ReadVariablesOp(OpKernelConstruction* c) : OpKernel(c) {\n   OP_REQUIRES_OK(c, c->GetAttr(\"N\", &n));\n   OP_REQUIRES_OK(c, c->GetAttr(\"dtypes\", &dtypes_));\n   OP_REQUIRES(c, n == dtypes_.size(),\n-              errors::InvalidArgument(\n+              absl::InvalidArgumentError(absl::StrCat(\n                   \"Mismatched number of arguments to ReadVariablesOp (\", n,\n-                  \" vs. \", dtypes_.size(), \")\"));\n+                  \" vs. \", dtypes_.size(), \")\")));\n }\n \n void ReadVariablesOp::Compute(OpKernelContext* ctx) {\n@@ -203,22 +206,22 @@ void ReadVariablesOp::Compute(OpKernelContext* ctx) {\n   }\n \n   OP_REQUIRES(ctx, uninitialized_vars.empty(),\n-              errors::FailedPrecondition(\n+              absl::FailedPreconditionError(absl::StrCat(\n                   \"In ReadVariablesOp the following variables were \"\n                   \"found uninitialized: \",\n-                  absl::StrJoin(uninitialized_vars, \", \")));\n+                  absl::StrJoin(uninitialized_vars, \", \"))));\n \n   for (size_t i = 0; i < dtypes_.size(); ++i) {\n     // We're acquiring a reference to the underlying buffer while\n     // holding a shared lock to guarantee ordering of reads and\n     // writes.\n     tf_shared_lock ml(*variables[i]->mu());\n     OP_REQUIRES(ctx, dtypes_[i] == variables[i]->tensor()->dtype(),\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Trying to read variable \", handles[i]->name(),\n                     \" from Container: \", handles[i]->container(),\n                     \" with wrong dtype. Expected \", DataTypeString(dtypes_[i]),\n-                    \" got \", DataTypeString(variables[i]->tensor()->dtype())));\n+                    \" got \", DataTypeString(variables[i]->tensor()->dtype()))));\n     if (variables[i]->copy_on_read_mode.load()) {\n       OP_REQUIRES_OK(ctx, CopyVariable(i, ctx, variables[i]->tensor()));\n     } else {\n@@ -383,12 +386,12 @@ void DisableCopyOnReadOp::Compute(OpKernelContext* ctx) {\n   const ResourceHandle& handle = HandleFromInput(ctx, 0);\n   const auto status = LookupResource(ctx, handle, &variable);\n   OP_REQUIRES(ctx, status.ok(),\n-              errors::FailedPrecondition(\n+              absl::FailedPreconditionError(absl::StrCat(\n                   \"Could not find variable \", handle.name(), \". \",\n                   \"This could mean that the variable has been deleted. \",\n                   \"In TF1, it can also mean the variable is uninitialized. \",\n                   \"Debug info: container=\", handle.container(),\n-                  \", status error message=\", status.message()));\n+                  \", status error message=\", status.message())));\n   // If the variable is currently in copy-on-read mode, its refcount is 1\n   if (variable->copy_on_read_mode.load()) {\n     // Obtain an exclusive lock on the variable and change the access mode\n@@ -420,10 +423,10 @@ class AssignVariableOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     OP_REQUIRES(context, dtype_ == context->input(1).dtype(),\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Variable and value dtypes don't match; respectively, \",\n                     DataTypeString(dtype_), \" and \",\n-                    DataTypeString(context->input(1).dtype())));\n+                    DataTypeString(context->input(1).dtype()))));\n     core::RefCountPtr<Var> variable;\n     const Tensor& value = context->input(1);\n     // Note: every resource-variable-manipulating op assumes copy-on-write\n@@ -454,20 +457,20 @@ class AssignVariableOp : public OpKernel {\n                 (variable->tensor()->dtype() == DT_INVALID &&\n                  !variable->is_initialized) ||\n                     variable->tensor()->dtype() == dtype_,\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Trying to assign variable with wrong dtype. Expected \",\n                     DataTypeString(variable->tensor()->dtype()), \" got \",\n-                    DataTypeString(dtype_)));\n+                    DataTypeString(dtype_))));\n     if (validate_shape_) {\n       OP_REQUIRES(\n           context,\n           (!variable->is_initialized ||\n            variable->tensor()->shape().IsSameSize(value.shape())),\n-          errors::InvalidArgument(\n+          absl::InvalidArgumentError(absl::StrCat(\n               \"Trying to assign to variable with tensor with wrong shape.\"\n               \" Expected \",\n               variable->tensor()->shape().DebugString(), \" got \",\n-              value.shape().DebugString()));\n+              value.shape().DebugString())));\n     }\n     if (variable->copy_on_read_mode.load()) {\n       AllocatorAttributes attr;\n@@ -496,9 +499,10 @@ class AssignVariableOp<Device, Variant> : public OpKernel {\n  public:\n   explicit AssignVariableOp(OpKernelConstruction* c) : OpKernel(c) {\n     OP_REQUIRES_OK(c, c->GetAttr(\"dtype\", &dtype_));\n-    OP_REQUIRES(c, dtype_ == DT_VARIANT,\n-                errors::Internal(\"Variant kernel called with dtype: \",\n-                                 DataTypeString(dtype_)));\n+    OP_REQUIRES(\n+        c, dtype_ == DT_VARIANT,\n+        absl::InternalError(absl::StrCat(\"Variant kernel called with dtype: \",\n+                                         DataTypeString(dtype_))));\n   }\n \n   void Compute(OpKernelContext* context) override {\n@@ -532,10 +536,10 @@ class AssignVariableOp<Device, Variant> : public OpKernel {\n \n     mutex_lock ml(*variable->mu());\n     OP_REQUIRES(context, variable->tensor()->dtype() == DT_VARIANT,\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Trying to assign variable with wrong dtype. Expected \",\n                     DataTypeString(variable->tensor()->dtype()), \" got \",\n-                    DataTypeString(DT_VARIANT)));\n+                    DataTypeString(DT_VARIANT))));\n     variable->is_initialized = true;\n     *variable->tensor() = Tensor(DT_VARIANT, value.shape());\n \n@@ -633,7 +637,7 @@ class AssignUpdateVariableOp : public OpKernel {\n     OP_REQUIRES_OK(context, ValidateAssignUpdateVariableOpShapes(\n                                 var_tensor->shape(), value.shape()));\n     OP_REQUIRES(context, var_tensor->dtype() == value.dtype(),\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(\n                     \"DType of variable handle and value does not match.\"));\n     OP_REQUIRES_OK(\n         context, PrepareToUpdateVariable<Device, T>(\n@@ -746,21 +750,21 @@ class ResourceGatherOp : public OpKernel {\n     const Tensor& indices = c->input(1);\n     OP_REQUIRES(\n         c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n-        errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n-    OP_REQUIRES(\n-        c, params.shape().dims() >= batch_dims_,\n-        errors::InvalidArgument(\"params must have at least \", batch_dims_,\n-                                \" (batch_dims) dimensions but it has shape \",\n-                                params.shape().DebugString()));\n+        absl::InvalidArgumentError(\"params must be at least 1 dimensional\"));\n+    OP_REQUIRES(c, params.shape().dims() >= batch_dims_,\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"params must have at least \", batch_dims_,\n+                                 \" (batch_dims) dimensions but it has shape \",\n+                                 params.shape().DebugString())));\n \n     // Check that we have enough index space\n     const int64_t N = indices.NumElements();\n-    OP_REQUIRES(\n-        c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n-        errors::InvalidArgument(\"params.shape[0] too large for \",\n-                                DataTypeString(DataTypeToEnum<Index>::v()),\n-                                \" indexing: \", params.dim_size(0), \" > \",\n-                                std::numeric_limits<Index>::max()));\n+    OP_REQUIRES(c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"params.shape[0] too large for \",\n+                                 DataTypeString(DataTypeToEnum<Index>::v()),\n+                                 \" indexing: \", params.dim_size(0), \" > \",\n+                                 std::numeric_limits<Index>::max())));\n \n     // The result shape is params.shape[:batch_dims] +\n     // indices.shape[batch_dims:] + params.shape[batch_dims+1:].\n@@ -817,11 +821,11 @@ class ResourceGatherOp : public OpKernel {\n       functor::GatherFunctor<Device, T, Index> functor;\n       int64_t bad_i = functor(c, params_flat, indices_flat, out_flat);\n \n-      OP_REQUIRES(\n-          c, bad_i < 0,\n-          errors::InvalidArgument(\n-              \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n-              indices_flat(bad_i), \" is not in [0, \", params.dim_size(0), \")\"));\n+      OP_REQUIRES(c, bad_i < 0,\n+                  absl::InvalidArgumentError(absl::StrCat(\n+                      \"indices\", SliceDebugString(indices.shape(), bad_i),\n+                      \" = \", indices_flat(bad_i), \" is not in [0, \",\n+                      params.dim_size(0), \")\")));\n     }\n   }\n \n@@ -838,8 +842,8 @@ class ResourceGatherOp : public OpKernel {\n     }\n     OP_REQUIRES(\n         ctx, batch_size != 0,\n-        errors::InvalidArgument(\n-            \"Inner size of indices would result in batch_size of 0 and a \",\n+        absl::InvalidArgumentError(\n+            \"Inner size of indices would result in batch_size of 0 and a \"\n             \"division by 0 in the implementation. This is illegal\"));\n \n     auto indices_flat = indices->flat<Index>();\n@@ -1014,7 +1018,7 @@ Status CopyTensorToHost(OpKernelContext* c, const Tensor& device_tensor,\n   TF_RETURN_IF_ERROR(stream->Memcpy(host_tensor->flat<T>().data(), device_ptr,\n                                     device_tensor.NumElements() * sizeof(T)));\n   if (!stream) {\n-    return errors::Internal(\"Failed to copy indices to host\");\n+    return absl::InternalError(\"Failed to copy indices to host\");\n   }\n   return OkStatus();\n }\n@@ -1027,9 +1031,9 @@ template <typename T, typename Index, scatter_op::UpdateOp Op>\n Status DoScatterOnCpu(OpKernelContext* c, Tensor* params, const Tensor& indices,\n                       const Tensor& updates, Index num_indices) {\n   if (!DataTypeCanUseMemcpy(params->dtype())) {\n-    return errors::Unimplemented(\n+    return absl::UnimplementedError(absl::StrCat(\n         \"GPU Scatter ops for dtype \", DataTypeString(params->dtype()),\n-        \" do not yet have a deterministic implementation\");\n+        \" do not yet have a deterministic implementation\"));\n   }\n   auto stream = c->op_device_context()->stream();\n \n@@ -1050,7 +1054,7 @@ Status DoScatterOnCpu(OpKernelContext* c, Tensor* params, const Tensor& indices,\n   TF_RETURN_IF_ERROR(stream->Memcpy(&params_ptr, host_params.flat<T>().data(),\n                                     host_params.NumElements() * sizeof(T)));\n   if (!stream) {\n-    return errors::Internal(\"Failed to copy params to device\");\n+    return absl::InternalError(\"Failed to copy params to device\");\n   }\n   // Deallocate host_params' buffer once the host-to-device copy is complete.\n   // host_params is captured by value in the lambda so that its buffer is only\n@@ -1090,27 +1094,27 @@ absl::Status DoScatter(OpKernelContext* c, Tensor* params,\n       const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                   params_flat, update, indices_flat);\n       if (bad_i >= 0) {\n-        return errors::InvalidArgument(\n+        return absl::InvalidArgumentError(absl::StrCat(\n             \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n-            indices_flat(bad_i), \" is not in [0, \", params->dim_size(0), \")\");\n+            indices_flat(bad_i), \" is not in [0, \", params->dim_size(0), \")\"));\n       }\n     } else {\n       int64_t num_updates = updates.NumElements();\n       if (!TensorShapeUtils::StartsWith(updates.shape(), indices.shape())) {\n-        return errors::InvalidArgument(\n+        return absl::InvalidArgumentError(absl::StrCat(\n             \"The shape of indices (\", indices.shape().DebugString(),\n             \") must be a prefix of the shape of updates (\",\n-            updates.shape().DebugString(), \")\");\n+            updates.shape().DebugString(), \")\"));\n       }\n       auto updates_flat =\n           updates.shaped<T, 2>({num_indices, num_updates / num_indices});\n       functor::ScatterFunctor<Device, T, Index, op> functor;\n       const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                   params_flat, updates_flat, indices_flat);\n       if (bad_i >= 0) {\n-        return errors::InvalidArgument(\n+        return absl::InvalidArgumentError(absl::StrCat(\n             \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n-            indices_flat(bad_i), \" is not in [0, \", params->dim_size(0), \")\");\n+            indices_flat(bad_i), \" is not in [0, \", params->dim_size(0), \")\"));\n       }\n     }\n   }\n@@ -1138,7 +1142,7 @@ class ResourceScatterUpdateOp : public OpKernel {\n     // Check data type of update and resource to scatter.\n     const DataType update_dtype = c->input(2).dtype();\n     OP_REQUIRES(c, v->tensor()->dtype() == update_dtype,\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(\n                     \"DType of scatter resource and updates does not match.\"));\n \n     OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n@@ -1166,33 +1170,32 @@ class ResourceScatterUpdateOp : public OpKernel {\n     OP_REQUIRES(c,\n                 updates.dims() == 0 ||\n                     updates.dims() == indices.dims() + params->dims() - 1,\n-                errors::InvalidArgument(\n+                absl::InvalidArgumentError(absl::StrCat(\n                     \"Must have updates.shape = indices.shape + \"\n                     \"params.shape[1:] or updates.shape = [], got \",\n                     \"updates.shape \", updates.shape().DebugString(),\n                     \", indices.shape \", indices.shape().DebugString(),\n-                    \", params.shape \", params->shape().DebugString()));\n+                    \", params.shape \", params->shape().DebugString())));\n \n     // Check that we have enough index space\n     const int64_t N_big = indices.NumElements();\n-    OP_REQUIRES(\n-        c, N_big <= std::numeric_limits<Index>::max(),\n-        errors::InvalidArgument(\"indices has too many elements for \",\n-                                DataTypeString(DataTypeToEnum<Index>::v()),\n-                                \" indexing: \", N_big, \" > \",\n-                                std::numeric_limits<Index>::max()));\n+    OP_REQUIRES(c, N_big <= std::numeric_limits<Index>::max(),\n+                absl::InvalidArgumentError(absl::StrCat(\n+                    \"indices has too many elements for \",\n+                    DataTypeString(DataTypeToEnum<Index>::v()), \" indexing: \",\n+                    N_big, \" > \", std::numeric_limits<Index>::max())));\n     const Index N = static_cast<Index>(N_big);\n-    OP_REQUIRES(\n-        c, params->dim_size(0) <= std::numeric_limits<Index>::max(),\n-        errors::InvalidArgument(\"params.shape[0] too large for \",\n-                                DataTypeString(DataTypeToEnum<Index>::v()),\n-                                \" indexing: \", params->dim_size(0), \" > \",\n-                                std::numeric_limits<Index>::max()));\n+    OP_REQUIRES(c, params->dim_size(0) <= std::numeric_limits<Index>::max(),\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"params.shape[0] too large for \",\n+                                 DataTypeString(DataTypeToEnum<Index>::v()),\n+                                 \" indexing: \", params->dim_size(0), \" > \",\n+                                 std::numeric_limits<Index>::max())));\n \n     // Prevent division by 0\n     if (isCPUDevice<Device>() && op == tensorflow::scatter_op::UpdateOp::DIV) {\n       OP_REQUIRES(c, ValidateInput<T>(updates),\n-                  errors::InvalidArgument(\"updates must not contain 0\"));\n+                  absl::InvalidArgumentError(\"updates must not contain 0\"));\n     }\n \n     if (N > 0) {"
        },
        {
            "sha": "44364ee6c4cd8ad7e8474721b2b0eaff44bf71d2",
            "filename": "tensorflow/core/kernels/scatter_op.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 27,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fscatter_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Fscatter_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fscatter_op.cc?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -15,9 +15,14 @@ limitations under the License.\n \n // See docs in ../ops/state_ops.cc.\n \n+#include <limits>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/scatter_functor.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n #include \"tensorflow/core/platform/types.h\"\n@@ -50,17 +55,18 @@ static bool ValidShapes(const Tensor& params, const Tensor& updates,\n static void DoValidationChecking(OpKernelContext* c, const Tensor& params,\n                                  const Tensor& indices, const Tensor& updates) {\n   OP_REQUIRES(c, params.IsInitialized(),\n-              errors::FailedPrecondition(\"Null ref for params\"));\n+              absl::FailedPreconditionError(\"Null ref for params\"));\n   OP_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n-              errors::InvalidArgument(\"params must be at least 1-D, got shape \",\n-                                      params.shape().DebugString()));\n-  OP_REQUIRES(\n-      c, ValidShapes(params, updates, indices),\n-      errors::InvalidArgument(\"Must have updates.shape = indices.shape + \"\n-                              \"params.shape[1:] or updates.shape = [], got \",\n-                              \"updates.shape \", updates.shape().DebugString(),\n-                              \", indices.shape \", indices.shape().DebugString(),\n-                              \", params.shape \", params.shape().DebugString()));\n+              absl::InvalidArgumentError(\n+                  absl::StrCat(\"params must be at least 1-D, got shape \",\n+                               params.shape().DebugString())));\n+  OP_REQUIRES(c, ValidShapes(params, updates, indices),\n+              absl::InvalidArgumentError(absl::StrCat(\n+                  \"Must have updates.shape = indices.shape + \"\n+                  \"params.shape[1:] or updates.shape = [], got \",\n+                  \"updates.shape \", updates.shape().DebugString(),\n+                  \", indices.shape \", indices.shape().DebugString(),\n+                  \", params.shape \", params.shape().DebugString())));\n }\n \n template <typename Device, typename T, typename Index, scatter_op::UpdateOp op>\n@@ -76,7 +82,7 @@ class ScatterUpdateOp : public OpKernel {\n     if (std::is_same<Device, GPUDevice>::value) {\n       OP_REQUIRES(\n           c, !OpDeterminismRequired(),\n-          errors::Unimplemented(\n+          absl::UnimplementedError(\n               \"Determinism is not yet supported in GPU implementation of \"\n               \"Scatter ops with ref inputs. Consider using resource variables \"\n               \"instead if you want to run Scatter when op determinism is \"\n@@ -106,19 +112,18 @@ class ScatterUpdateOp : public OpKernel {\n \n     // Check that we have enough index space\n     const int64_t N_big = indices.NumElements();\n-    OP_REQUIRES(\n-        c, N_big <= std::numeric_limits<Index>::max(),\n-        errors::InvalidArgument(\"indices has too many elements for \",\n-                                DataTypeString(DataTypeToEnum<Index>::v()),\n-                                \" indexing: \", N_big, \" > \",\n-                                std::numeric_limits<Index>::max()));\n+    OP_REQUIRES(c, N_big <= std::numeric_limits<Index>::max(),\n+                absl::InvalidArgumentError(absl::StrCat(\n+                    \"indices has too many elements for \",\n+                    DataTypeString(DataTypeToEnum<Index>::v()), \" indexing: \",\n+                    N_big, \" > \", std::numeric_limits<Index>::max())));\n     const Index N = static_cast<Index>(indices.NumElements());\n-    OP_REQUIRES(\n-        c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n-        errors::InvalidArgument(\"params.shape[0] too large for \",\n-                                DataTypeString(DataTypeToEnum<Index>::v()),\n-                                \" indexing: \", params.dim_size(0), \" > \",\n-                                std::numeric_limits<Index>::max()));\n+    OP_REQUIRES(c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"params.shape[0] too large for \",\n+                                 DataTypeString(DataTypeToEnum<Index>::v()),\n+                                 \" indexing: \", params.dim_size(0), \" > \",\n+                                 std::numeric_limits<Index>::max())));\n \n     // We always return the input ref.\n     c->forward_ref_input_to_ref_output(0, 0);\n@@ -133,10 +138,10 @@ class ScatterUpdateOp : public OpKernel {\n         const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                     params_flat, update, indices_flat);\n         OP_REQUIRES(c, bad_i < 0,\n-                    errors::InvalidArgument(\n+                    absl::InvalidArgumentError(absl::StrCat(\n                         \"indices\", SliceDebugString(indices.shape(), bad_i),\n                         \" = \", indices_flat(bad_i), \" is not in [0, \",\n-                        params.dim_size(0), \")\"));\n+                        params.dim_size(0), \")\")));\n       } else {\n         auto updates_flat =\n             updates.shaped<T, 2>({N, updates.NumElements() / N});\n@@ -145,10 +150,10 @@ class ScatterUpdateOp : public OpKernel {\n         const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                     params_flat, updates_flat, indices_flat);\n         OP_REQUIRES(c, bad_i < 0,\n-                    errors::InvalidArgument(\n+                    absl::InvalidArgumentError(absl::StrCat(\n                         \"indices\", SliceDebugString(indices.shape(), bad_i),\n                         \" = \", indices_flat(bad_i), \" is not in [0, \",\n-                        params.dim_size(0), \")\"));\n+                        params.dim_size(0), \")\")));\n       }\n     }\n   }"
        },
        {
            "sha": "bcd527929ba6d5e7d6999526c30229f054b72cf3",
            "filename": "tensorflow/core/kernels/training_ops.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Ftraining_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b553f7951d5b0928bf5ff326b657ab9a9aaecadc/tensorflow%2Fcore%2Fkernels%2Ftraining_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Ftraining_ops.cc?ref=b553f7951d5b0928bf5ff326b657ab9a9aaecadc",
            "patch": "@@ -18,6 +18,8 @@ limitations under the License.\n \n #include <algorithm>  // NOLINT\n \n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/framework/bounds_check.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n@@ -3450,9 +3452,9 @@ class SparseApplyKerasMomentumOp : public OpKernel {\n         momentum.scalar<T>(), use_nesterov_);\n     OP_REQUIRES(\n         ctx, bad_i < 0,\n-        errors::InvalidArgument(\n+        absl::InvalidArgumentError(absl::StrCat(\n             \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n-            indices_flat(bad_i), \" is not in [0, \", var.dim_size(0), \")\"));\n+            indices_flat(bad_i), \" is not in [0, \", var.dim_size(0), \")\")));\n \n     MaybeForwardRefInputToRefOutput(ctx, 0, 0);\n   }"
        }
    ],
    "stats": {
        "total": 447,
        "additions": 244,
        "deletions": 203
    }
}