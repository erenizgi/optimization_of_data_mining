{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 809695206",
    "sha": "c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e",
    "files": [
        {
            "sha": "e74a8b920d867e86995ab6d427c8386fa0f6c574",
            "filename": "third_party/xla/xla/stream_executor/gpu/context_map.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcontext_map.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcontext_map.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcontext_map.h?ref=c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e",
            "patch": "@@ -40,15 +40,15 @@ class ContextMap {\n       : find_device_ordinal_(std::move(find_device_ordinal)) {}\n   // Returns whether context is a member of the live set.\n   bool Has(GpuContext context) {\n-    absl::ReaderMutexLock lock(&mutex_);\n+    absl::ReaderMutexLock lock(mutex_);\n     return gpu_context_to_context_type_map_.find(context) !=\n            gpu_context_to_context_type_map_.end();\n   }\n \n   // Adds context to the live set, or returns it if it's already present.\n   ContextType* Add(GpuContext context, int device_ordinal) {\n     CHECK_NE(context, nullptr);\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n \n     auto insert_result = gpu_context_to_context_type_map_.insert(\n         std::make_pair(context, nullptr));\n@@ -63,7 +63,7 @@ class ContextMap {\n \n   // Removes context from the live set.\n   void Remove(GpuContext context) {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     CHECK_NE(context, nullptr);\n     auto it = gpu_context_to_context_type_map_.find(context);\n     CHECK(it != gpu_context_to_context_type_map_.end()) << context;\n@@ -82,7 +82,7 @@ class ContextMap {\n \n   // Returns the context associated to that ptr.\n   GpuContext GetAnyContext(void* ptr) {\n-    absl::ReaderMutexLock lock(&mutex_);\n+    absl::ReaderMutexLock lock(mutex_);\n     int device_ordinal = find_device_ordinal_(ptr);\n     CHECK_EQ(ordinal_to_type_map_.count(device_ordinal), 1);\n     CHECK(!ordinal_to_type_map_.at(device_ordinal).empty())"
        },
        {
            "sha": "f122d628eb362e3030275645b6cbb32a8f9bb875",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_blas_lt.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_blas_lt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_blas_lt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_blas_lt.cc?ref=c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e",
            "patch": "@@ -288,7 +288,7 @@ DataType GetScaleType(DataType c_type, ComputationType computation_type) {\n \n absl::StatusOr<BlasLt::MatmulPlan*> BlasLt::GetOrCreateMatmulPlan(\n     const std::string& key, PlanCreateFunc create) {\n-  absl::MutexLock lock(&plan_cache_mu_);  // double mutex ???\n+  absl::MutexLock lock(plan_cache_mu_);  // double mutex ???\n   auto res = plan_cache_.emplace(key, MatmulPlanPtr{});\n   // New entry inserted: always create a new matmul plan if key is empty,\n   // this is used by command_buffer_thunk test.\n@@ -301,12 +301,12 @@ absl::StatusOr<BlasLt::MatmulPlan*> BlasLt::GetOrCreateMatmulPlan(\n }\n \n void BlasLt::ClearMatmulPlanCache() {\n-  absl::MutexLock lock(&plan_cache_mu_);\n+  absl::MutexLock lock(plan_cache_mu_);\n   plan_cache_.clear();\n }\n \n size_t BlasLt::GetMatmulPlanCacheSize() const {\n-  absl::MutexLock lock(&plan_cache_mu_);\n+  absl::MutexLock lock(plan_cache_mu_);\n   return plan_cache_.size();\n }\n "
        },
        {
            "sha": "9159a5e2611c1249455a556536c5ea841e7a4d99",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_cudamallocasync_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_cudamallocasync_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_cudamallocasync_allocator.cc?ref=c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e",
            "patch": "@@ -443,25 +443,25 @@ bool GpuCudaMallocAsyncAllocator::TracksAllocationSizes() const {\n \n size_t GpuCudaMallocAsyncAllocator::RequestedSize(const void* ptr) const {\n   if (!stats_ || !ptr) return 0;\n-  absl::MutexLock l(&mutex_);\n+  absl::MutexLock l(mutex_);\n   return size_map_.at(ptr);\n }\n \n size_t GpuCudaMallocAsyncAllocator::AllocatedSize(const void* ptr) const {\n   if (!stats_ || !ptr) return 0;\n-  absl::MutexLock l(&mutex_);\n+  absl::MutexLock l(mutex_);\n   return size_map_.at(ptr);\n }\n \n std::optional<tsl::AllocatorStats> GpuCudaMallocAsyncAllocator::GetStats() {\n   if (!stats_) return std::nullopt;\n-  absl::MutexLock l(&mutex_);\n+  absl::MutexLock l(mutex_);\n   return *stats_;\n }\n \n bool GpuCudaMallocAsyncAllocator::ClearStats() {\n   if (!stats_) return false;\n-  absl::MutexLock l(&mutex_);\n+  absl::MutexLock l(mutex_);\n   stats_->num_allocs = 0;\n   stats_->peak_bytes_in_use = stats_->bytes_in_use;\n   stats_->largest_alloc_size = 0;"
        },
        {
            "sha": "31bb0e71b496ff08b74cf454bf1b61b77589aa8b",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_executor.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h?ref=c435cdb28ea7a8e4f2a3a4cb91d692b08e22ea8e",
            "patch": "@@ -45,12 +45,12 @@ class GpuExecutor : public StreamExecutorCommon {\n   int device_ordinal() const override { return device_ordinal_; };\n \n   absl::StatusOr<std::vector<ApiTrace>> ExtractApiTrace() override {\n-    absl::MutexLock lock(&logger_mu_);\n+    absl::MutexLock lock(logger_mu_);\n     return std::move(argument_logs_);\n   }\n \n   absl::Status RecordApiTrace(ApiTrace call) override {\n-    absl::MutexLock lock(&logger_mu_);\n+    absl::MutexLock lock(logger_mu_);\n     if (std::holds_alternative<GemmCallTrace>(call) &&\n         (argument_logging_mode_ & kLogGemm)) {\n       argument_logs_.push_back(call);\n@@ -59,7 +59,7 @@ class GpuExecutor : public StreamExecutorCommon {\n   }\n \n   bool SetArgumentLoggingMode(uint64_t mode) override {\n-    absl::MutexLock lock(&logger_mu_);\n+    absl::MutexLock lock(logger_mu_);\n     argument_logging_mode_ = mode;\n     return true;\n   }"
        }
    ],
    "stats": {
        "total": 28,
        "additions": 14,
        "deletions": 14
    }
}