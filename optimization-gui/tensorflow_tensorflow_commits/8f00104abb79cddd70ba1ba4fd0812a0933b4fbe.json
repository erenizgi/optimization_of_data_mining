{
    "author": "KanishAnand",
    "message": "Implement `HloSharding::dimensions()` for `NamedSharding`\n\nPiperOrigin-RevId: 843143103",
    "sha": "8f00104abb79cddd70ba1ba4fd0812a0933b4fbe",
    "files": [
        {
            "sha": "02673f27e7ff1eff042bdaad31d478b6abe63f77",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f00104abb79cddd70ba1ba4fd0812a0933b4fbe/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f00104abb79cddd70ba1ba4fd0812a0933b4fbe/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=8f00104abb79cddd70ba1ba4fd0812a0933b4fbe",
            "patch": "@@ -506,6 +506,9 @@ class HloSharding {\n \n   // Returns all sharding dimensions.\n   absl::Span<const int64_t> dimensions() const {\n+    if (UseNamedShardingLeaf()) {\n+      return named_sharding_->dimensions();\n+    }\n     return tile_assignment().dimensions();\n   }\n "
        },
        {
            "sha": "0795df6588a397290b38b3df420c5883f1acb0c6",
            "filename": "third_party/xla/xla/hlo/ir/named_sharding.h",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f00104abb79cddd70ba1ba4fd0812a0933b4fbe/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f00104abb79cddd70ba1ba4fd0812a0933b4fbe/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h?ref=8f00104abb79cddd70ba1ba4fd0812a0933b4fbe",
            "patch": "@@ -76,7 +76,12 @@ class NamedSharding {\n         dim_shardings_(CanonicalizedDimShardings(dim_shardings)),\n         replicated_axes_(replicated_axes.begin(), replicated_axes.end()),\n         unreduced_axes_(unreduced_axes.begin(), unreduced_axes.end()),\n-        metadata_(metadata.begin(), metadata.end()) {}\n+        metadata_(metadata.begin(), metadata.end()) {\n+    sharded_sizes_.reserve(dim_shardings_.size());\n+    for (const DimensionSharding& dim_sharding : dim_shardings_) {\n+      sharded_sizes_.push_back(dim_sharding.getShardedSize(mesh_));\n+    }\n+  }\n \n   const Mesh& mesh() const { return mesh_; }\n   absl::Span<const DimensionSharding> dim_shardings() const {\n@@ -94,6 +99,9 @@ class NamedSharding {\n     return dim_shardings_[dim].getShardedSize(mesh_);\n   }\n \n+  // Returns all sharding dimensions.\n+  absl::Span<const int64_t> dimensions() const { return sharded_sizes_; }\n+\n   // Returns the total number of devices used by sharding.\n   int64_t num_devices() const {\n     return mesh_.device_assignment().num_elements();\n@@ -154,6 +162,13 @@ class NamedSharding {\n   std::vector<AxisRef> replicated_axes_;\n   std::vector<AxisRef> unreduced_axes_;\n   std::vector<OpMetadata> metadata_;\n+\n+  // Stores sharded sizes for each dimension. Required to maintain backward\n+  // compatibility with existing `HloSharding::dimensions()` implementation\n+  // returning a span.\n+  // Once we make API change for `HloSharding::dimensions()` to return a vector,\n+  // we can remove this field.\n+  std::vector<int64_t> sharded_sizes_;\n };\n \n // Contains test only helper functions."
        },
        {
            "sha": "19764b7042f3b4a2fd849209d75fa03265e7b328",
            "filename": "third_party/xla/xla/hlo/ir/named_sharding_test.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f00104abb79cddd70ba1ba4fd0812a0933b4fbe/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f00104abb79cddd70ba1ba4fd0812a0933b4fbe/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding_test.cc?ref=8f00104abb79cddd70ba1ba4fd0812a0933b4fbe",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/hlo/ir/named_sharding.h\"\n \n+#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"xla/hlo/ir/mesh_and_axis.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -23,6 +24,7 @@ namespace xla {\n namespace {\n \n using DimensionSharding = NamedSharding::DimensionSharding;\n+using ::testing::ElementsAre;\n \n TEST(NamedShardingTest, CanonicalizedDimShardings) {\n   Mesh mesh_abcd({2, 4}, {\"a\", \"b\"});\n@@ -151,6 +153,24 @@ TEST(NamedShardingTest, Dimension) {\n   EXPECT_EQ(empty_sharding.num_dimensions(), 0);\n }\n \n+TEST(NamedShardingTest, Dimensions) {\n+  Mesh mesh({2, 4, 3, 8}, {\"a\", \"b\", \"c\", \"d\"});\n+\n+  AxisRef axis_a(0);\n+  AxisRef axis_b(1, {2, 2});\n+  AxisRef axis_c(2);\n+  AxisRef axis_d(3, {4, 2});\n+\n+  DimensionSharding ds_ab({axis_a, axis_b}, /*is_closed=*/true);\n+  DimensionSharding ds_dc({axis_d, axis_c}, /*is_closed=*/true);\n+\n+  NamedSharding sharding(mesh, /*dim_shardings=*/{ds_ab, ds_dc});\n+  EXPECT_THAT(sharding.dimensions(), ElementsAre(2 * 2, 2 * 3));\n+\n+  NamedSharding empty_sharding(mesh, /*dim_shardings=*/{});\n+  EXPECT_THAT(empty_sharding.dimensions(), ElementsAre());\n+}\n+\n TEST(NamedShardingTest, NumDevices) {\n   Mesh mesh({2, 4, 3, 8}, {\"a\", \"b\", \"c\", \"d\"});\n   NamedSharding sharding(mesh, {});"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 39,
        "deletions": 1
    }
}