{
    "author": "ezhulenev",
    "message": "[xla] Keep HloInstruction::shape_ as std::shared_ptr\n\nShape size is around 500 bytes. There is a very small number of unique shapes in any given HloModule, by keeping them as shared pointers and canonicalizing similar to LiteralCanonicalizer, we can significantly reduce the memory size of HloModules that we keep in memory.\n\nPiperOrigin-RevId: 800625639",
    "sha": "3b005eae2b5654ae28630d2f9c07208bcc1ea6e4",
    "files": [
        {
            "sha": "b060aa90c8adddbac960cac482d78666d234a4ec",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3b005eae2b5654ae28630d2f9c07208bcc1ea6e4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3b005eae2b5654ae28630d2f9c07208bcc1ea6e4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc?ref=3b005eae2b5654ae28630d2f9c07208bcc1ea6e4",
            "patch": "@@ -2367,12 +2367,13 @@ void HloInstruction::set_single_sharding(const HloSharding& sharding) {\n void HloInstruction::SetupDerivedInstruction(\n     HloInstruction* derived_instruction) const {\n   if (sharding_ != nullptr &&\n-      ShapeUtil::CompatibleKind(shape_, derived_instruction->shape())) {\n+      ShapeUtil::CompatibleKind(shape(), derived_instruction->shape())) {\n     // Only copy sharding if the tuple tree shape of the two instruction is\n     // compatible because copying it between differently shaped instructions\n     // can produce invalid shardings.\n     derived_instruction->set_sharding(*sharding_);\n-  } else if (!ShapeUtil::CompatibleKind(shape_, derived_instruction->shape())) {\n+  } else if (!ShapeUtil::CompatibleKind(shape(),\n+                                        derived_instruction->shape())) {\n     derived_instruction->clear_sharding();\n   }\n   derived_instruction->set_metadata(metadata());\n@@ -2897,7 +2898,7 @@ std::unique_ptr<HloInstruction> HloInstruction::CloneWithNewShape(\n std::unique_ptr<HloInstruction> HloInstruction::Clone(\n     const std::string& suffix, HloCloneContext* context) const {\n   std::unique_ptr<HloInstruction> clone =\n-      CloneWithNewShape(shape_, suffix, context);\n+      CloneWithNewShape(shape(), suffix, context);\n   return clone;\n }\n \n@@ -3870,7 +3871,7 @@ bool HloInstruction::IsElementwiseImpl(\n     return operand_idx.has_value() && operand_idx.value() == 0;\n   }\n   if (opcode_ == HloOpcode::kBitcastConvert &&\n-      primitive_util::BitWidth(shape_.element_type()) !=\n+      primitive_util::BitWidth(shape().element_type()) !=\n           primitive_util::BitWidth(operands_[0]->shape().element_type())) {\n     return false;\n   }\n@@ -4374,7 +4375,7 @@ HloInstructionProto HloInstruction::ToProto() const {\n   proto.set_id(unique_id_);\n   proto.set_name(name_);\n   *proto.mutable_opcode() = std::string(HloOpcodeString(opcode_));\n-  *proto.mutable_shape() = shape_.ToProto();\n+  *proto.mutable_shape() = shape().ToProto();\n   for (const HloInstruction* operand : operands_) {\n     proto.add_operand_ids(operand->unique_id_64_bits());\n   }\n@@ -4486,9 +4487,9 @@ HloInstruction::HloInstruction(HloOpcode opcode, const Shape& shape)\n       cleaned_up_(false),\n       marked_as_dead_(false),\n       is_root_(false),\n-      shape_(shape),\n+      shape_(std::make_shared<Shape>(shape)),\n       name_(HloOpcodeString(opcode)) {\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(shape_));\n+  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(*shape_));\n }\n \n template <typename HloInstructionPtr>\n@@ -4923,8 +4924,6 @@ absl::Status HloInstruction::AcceptWithOperandOrder(\n   return absl::OkStatus();\n }\n \n-const Shape& HloInstruction::shape() const { return shape_; }\n-\n absl::InlinedVector<int64_t, 4> HloInstruction::OperandIndices(\n     const HloInstruction* operand) const {\n   const size_t num_operands = operand_count();\n@@ -5088,8 +5087,8 @@ HloInstruction::ReshapeMerelyInsertsOrDeletes1SizedDimensions() const {\n   if (HloOpcode::kReshape != opcode_) {\n     return std::nullopt;\n   }\n-  return ShapeUtil::InsertedOrDeleted1SizedDimensions(operand(0)->shape_,\n-                                                      shape_);\n+  return ShapeUtil::InsertedOrDeleted1SizedDimensions(operand(0)->shape(),\n+                                                      shape());\n }\n \n absl::string_view ToString(HloInstruction::FusionKind kind) {"
        },
        {
            "sha": "a44c4d7a7810f85d61ba5bfa3640a8e8fbaf9355",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.h",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3b005eae2b5654ae28630d2f9c07208bcc1ea6e4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3b005eae2b5654ae28630d2f9c07208bcc1ea6e4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h?ref=3b005eae2b5654ae28630d2f9c07208bcc1ea6e4",
            "patch": "@@ -1208,10 +1208,19 @@ class HloInstruction {\n   virtual bool HasSideEffect() const;\n \n   // Returns the result shape of this instruction.\n-  const Shape& shape() const;\n+  const Shape& shape() const {\n+    DCHECK(shape_) << \"Instruction shape must be set\";\n+    return *shape_;\n+  }\n \n   // Returns the (mutable) result shape of this instruction.\n-  Shape* mutable_shape() { return &shape_; }\n+  Shape* mutable_shape() {\n+    DCHECK(shape_) << \"Instruction shape must be set\";\n+    if (shape_.use_count() > 1) {\n+      shape_ = std::make_shared<Shape>(*shape_);\n+    }\n+    return &*shape_;\n+  }\n \n   // Returns the ith operand to this instruction.\n   const HloInstruction* operand(int64_t i) const;\n@@ -2673,7 +2682,7 @@ class HloInstruction {\n   std::shared_ptr<const HloSharding> sharding_;\n \n   // Result shape of this instruction.\n-  Shape shape_;\n+  std::shared_ptr<Shape> shape_;\n \n   // The backend-specific configuration for how a backend should compile this\n   // HLO. See the documentation on backend_config()."
        }
    ],
    "stats": {
        "total": 36,
        "additions": 22,
        "deletions": 14
    }
}