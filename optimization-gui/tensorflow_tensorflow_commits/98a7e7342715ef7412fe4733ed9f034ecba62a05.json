{
    "author": "seherellis",
    "message": "Rollback of PR #26196\nNot reviewed by the scheduling team. Sorry for the trouble.\n\nReverts 1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff\n\nPiperOrigin-RevId: 842788005",
    "sha": "98a7e7342715ef7412fe4733ed9f034ecba62a05",
    "files": [
        {
            "sha": "2afc4787298d11c9ad7bc4e1c336b93994bde726",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=98a7e7342715ef7412fe4733ed9f034ecba62a05",
            "patch": "@@ -582,12 +582,7 @@ absl::Status RunLatencyHidingSchedulerPasses(\n     pipeline.AddPass<PGLEAccuracyChecker>(\n         dynamic_cast<ProfileGuidedLatencyEstimator&>(*estimator));\n   }\n-  // If overlap limit is set to be greater than 1 and the default t-short size\n-  // estimator is used we will tell LHS to extend async-done intervals as much\n-  // as possible to start collectives as early as possible.\n-  if (config.parallel_collective_overlap_limit > 1) {\n-    config.prioritize_compute_over_async_start = true;\n-  }\n+\n   auto async_tracker = std::make_unique<GpuAsyncTracker>(config);\n \n   std::shared_ptr<const SchedulingContext> scheduling_context ="
        },
        {
            "sha": "28eabcb1cd768080fc86ddd3e3de4e0664939d72",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 86,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc?ref=98a7e7342715ef7412fe4733ed9f034ecba62a05",
            "patch": "@@ -75,8 +75,6 @@ class GpuLatencyHidingSchedulerBaseTest\n     DebugOptions& options = module->mutable_config().mutable_debug_options();\n     options.set_xla_gpu_experimental_parallel_collective_overlap_limit(\n         num_parallel_resources);\n-    options.set_xla_gpu_enable_analytical_sol_latency_estimator(false);\n-\n     options.set_xla_gpu_pgle_accuracy_checker(strictness);\n \n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, /*pointer_size=*/8,\n@@ -1043,89 +1041,5 @@ TEST_F(GpuLatencyHidingSchedulerBaseTest, ParallelThreadsShouldBeScheduled) {\n   TF_EXPECT_OK(ScheduleModule(module.get()));\n }\n \n-TEST_F(GpuLatencyHidingSchedulerBaseTest,\n-       MultipleParallelAsyncsExtendedOverAllComputes) {\n-  absl::string_view kHloModule = R\"(\n-HloModule m\n-reduce {\n-x = f32[] parameter(0)\n-y = f32[] parameter(1)\n-ROOT _ = f32[] add(x, y)\n-}\n-ENTRY main {\n-p0 = f32[] parameter(0)\n-p1 = f32[2] parameter(1)\n-p2 = f32[2] parameter(2)\n-p3 = f32[2] parameter(3)\n-p4 = f32[2] parameter(4)\n-p5 = f32[2] parameter(5)\n-p6 = f32[2] parameter(6)\n-ar_0 = f32[] all-reduce-start(p0), to_apply=reduce\n-ar_1 = f32[] all-reduce-done(ar_0)\n-add_2 = f32[2] add(p1, p6)\n-\n-ar_2 = f32[2] all-reduce-start(add_2), to_apply=reduce\n-ar_3 = f32[2] all-reduce-done(ar_2)\n-add_3 = f32[2] add(p1, p3)\n-\n-rs_0 = ((f32[2]), f32[1]) reduce-scatter-start(add_3), to_apply=reduce,\n-dimensions={0}\n-rs_1 = f32[1] reduce-scatter-done(rs_0)\n-add_0 = f32[2] add(p1, p2)\n-div_0 = f32[2] divide(p3, p4)\n-mul_0 = f32[2] multiply(p4, p5)\n-ROOT _ = (f32[], f32[2], f32[1], f32[2], f32[2], f32[2]) tuple(ar_1, ar_3, rs_1, add_0, div_0, mul_0)\n-}\n-)\";\n-  absl::string_view kFdoProfile = \"\";\n-\n-  auto config = GetModuleConfig(kFdoProfile);\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kHloModule, config));\n-\n-  TF_EXPECT_OK(ScheduleModule(module.get(), /*num_parallel_resources=*/16));\n-  auto schedule = module->schedule();\n-  std::vector<HloInstruction*> instruction_sequence =\n-      schedule.sequence(module->entry_computation()).instructions();\n-  // With a lot of parallel resources and default latency estimator,\n-  // LHS will try to extend all asyncs as much as possible.\n-  // We expect all computes to be wrapped within all async start-done\n-  // intervals.\n-  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"add_2\") >\n-                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n-              GetIndexByName(instruction_sequence, \"add_3\") >\n-                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n-              GetIndexByName(instruction_sequence, \"add_2\") <\n-                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n-              GetIndexByName(instruction_sequence, \"add_3\") <\n-                  GetIndexByName(instruction_sequence, \"ar_1\"));\n-\n-  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"add_0\") >\n-                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n-              GetIndexByName(instruction_sequence, \"add_0\") >\n-                  GetIndexByName(instruction_sequence, \"rs_0\") &&\n-              GetIndexByName(instruction_sequence, \"add_0\") <\n-                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n-              GetIndexByName(instruction_sequence, \"add_0\") <\n-                  GetIndexByName(instruction_sequence, \"rs_1\"));\n-\n-  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"div_0\") >\n-                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n-              GetIndexByName(instruction_sequence, \"div_0\") >\n-                  GetIndexByName(instruction_sequence, \"rs_0\") &&\n-              GetIndexByName(instruction_sequence, \"div_0\") <\n-                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n-              GetIndexByName(instruction_sequence, \"div_0\") <\n-                  GetIndexByName(instruction_sequence, \"rs_1\"));\n-  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"mul_0\") >\n-                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n-              GetIndexByName(instruction_sequence, \"mul_0\") >\n-                  GetIndexByName(instruction_sequence, \"rs_0\") &&\n-              GetIndexByName(instruction_sequence, \"mul_0\") <\n-                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n-              GetIndexByName(instruction_sequence, \"mul_0\") <\n-                  GetIndexByName(instruction_sequence, \"rs_1\"));\n-}\n-\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "62995d048976f9f4e46a2a369b04911456fe342a",
            "filename": "third_party/xla/xla/service/latency_hiding_scheduler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc?ref=98a7e7342715ef7412fe4733ed9f034ecba62a05",
            "patch": "@@ -1262,29 +1262,6 @@ class ReadySetLt {\n     return std::nullopt;\n   }\n \n-  inline std::optional<bool> DelayAsyncStartCandidateCondition(\n-      DefaultSchedulerCore::ScheduleCandidate& a,\n-      DefaultSchedulerCore::ScheduleCandidate& b, const HloGraphNode* a_node,\n-      const HloGraphNode* b_node, const char** reason) const {\n-    bool a_has_async_resource =\n-        a_node->DoesReleaseAnyResource() && !IsResourceConstrained(a, a_node);\n-    bool b_has_async_resource =\n-        b_node->DoesReleaseAnyResource() && !IsResourceConstrained(b, b_node);\n-\n-    CMP_EXPLICIT(!a_has_async_resource, !b_has_async_resource,\n-                 \"kDelayAsyncStartForCompute\");\n-    if (a_has_async_resource && b_has_async_resource) {\n-      // If 2 nodes are both async nodes, we prioritize the one\n-      // with more depth to free up more computes to overlap\n-      // with the one with less depth which can be launched\n-      // early\n-      CMP_EXPLICIT(a_node->GetDepth() > b_node->GetDepth(),\n-                   b_node->GetDepth() > a_node->GetDepth(),\n-                   \"kDelayAsyncStartForDepth\");\n-    }\n-    return std::nullopt;\n-  }\n-\n   // The comparison here implements the priority for the nodes in the ready\n   // set. The function compares a and b in a series of prioritized\n   // comparisons. As soon as it finds one that is not equal, it stops.  If\n@@ -1394,16 +1371,6 @@ class ReadySetLt {\n                    AsyncDepth0CandidateCondition(b, bn), \"kStartAtZeroDepth\");\n     }\n \n-    if (sched_state_.config.aggressive_scheduling_policies &&\n-        sched_state_.config.prioritize_compute_over_async_start) {\n-      // If an instruction releasing a resource is not resource constrained,\n-      // delay it as much as possible.\n-      if (auto value =\n-              DelayAsyncStartCandidateCondition(a, b, an, bn, reason)) {\n-        return *value;\n-      }\n-    }\n-\n     auto a_readytime = an->GetReadyTime();\n     auto b_readytime = bn->GetReadyTime();\n     if (a_readytime != b_readytime) {  // Quick test to avoid lots of work"
        },
        {
            "sha": "01630bbaa5bf5e89abe824b350daa2d09bddb187",
            "filename": "third_party/xla/xla/service/latency_hiding_scheduler.h",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98a7e7342715ef7412fe4733ed9f034ecba62a05/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.h?ref=98a7e7342715ef7412fe4733ed9f034ecba62a05",
            "patch": "@@ -149,9 +149,6 @@ struct SchedulerConfig {\n   bool use_real_cost_model = false;\n   bool aggressive_scheduling_policies = false;\n   bool prioritize_async_depth_over_stall = false;\n-\n-  bool prioritize_compute_over_async_start = false;\n-\n   bool enable_release_start_policy = false;\n   bool resource_sharing = false;\n   bool resource_serializing = false;"
        }
    ],
    "stats": {
        "total": 129,
        "additions": 1,
        "deletions": 128
    }
}