{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Register memory range only once.\n\nPiperOrigin-RevId: 803021720",
    "sha": "62930700b980b41cf2f604f5b28b502e77df57cc",
    "files": [
        {
            "sha": "ac652f4a19e75a85bb37b755d5f5040ed3e894da",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=62930700b980b41cf2f604f5b28b502e77df57cc",
            "patch": "@@ -357,33 +357,31 @@ absl::StatusOr<size_t> NcclCommunicator::NumRanks() const {\n }\n \n absl::Status NcclCommunicator::RegisterBufferOnce(\n-    se::DeviceMemoryBase buffer, se::DeviceMemoryBase buffer_range,\n-    int device_ordinal, bool use_symmetric_buffer) {\n+    se::DeviceMemoryBase buffer_range, int device_ordinal,\n+    bool use_symmetric_buffer) {\n   bool need_reg = false;\n   {\n     absl::MutexLock lock(&registered_buffers_.mu);\n-    if (!registered_buffers_.records_to_handles.contains(\n-            {buffer.size(), buffer.opaque()})) {\n+    if (!registered_buffers_.range_to_handle.contains(buffer_range.opaque())) {\n       need_reg = true;\n     } else {\n-      VLOG(5) << \"[\" << device_ordinal << \"] Buffer: \" << buffer.opaque()\n-              << \" with size: \" << buffer.size()\n-              << \" with range: \" << buffer_range.opaque()\n+      VLOG(5) << \"[\" << device_ordinal\n+              << \"] Buffer range: \" << buffer_range.opaque()\n+              << \" with size: \" << buffer_range.size()\n               << \" is already registered.\";\n     }\n   }\n   if (need_reg) {\n-    VLOG(5) << \"[\" << device_ordinal << \"] Registering \" << buffer.opaque()\n-            << \" with size: \" << buffer.size()\n-            << \" and with range: \" << buffer_range.opaque()\n+    VLOG(5) << \"[\" << device_ordinal << \"] Registering \"\n+            << buffer_range.opaque() << \" with size: \" << buffer_range.size()\n             << \", is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n     // Symmetric buffer registration is a collective operation,\n     // we need to do that before locking on a global.\n-    TF_ASSIGN_OR_RETURN(auto handle, RegisterBuffer(buffer, device_ordinal,\n-                                                    use_symmetric_buffer));\n+    TF_ASSIGN_OR_RETURN(\n+        auto handle,\n+        RegisterBuffer(buffer_range, device_ordinal, use_symmetric_buffer));\n     absl::MutexLock lock(&registered_buffers_.mu);\n-    registered_buffers_\n-        .records_to_handles[std::make_tuple(buffer.size(), buffer.opaque())] =\n+    registered_buffers_.range_to_handle[buffer_range.opaque()] =\n         std::move(handle);\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "ed0947ed6529594eb4b81e4fd8483755ffc3ec72",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.h",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h?ref=62930700b980b41cf2f604f5b28b502e77df57cc",
            "patch": "@@ -86,8 +86,7 @@ class NcclCommunicator : public GpuCommunicator {\n   // Since each XLA buffer is a slice into a larger BFCAllocator chunk, first\n   // get the base address of buffer. We will use the base address to keep track\n   // of which chunks we have registered.\n-  absl::Status RegisterBufferOnce(se::DeviceMemoryBase buffer,\n-                                  se::DeviceMemoryBase buffer_range,\n+  absl::Status RegisterBufferOnce(se::DeviceMemoryBase buffer_range,\n                                   int device_ordinal,\n                                   bool use_symmetric_buffer) final;\n \n@@ -242,10 +241,10 @@ class NcclCommunicator : public GpuCommunicator {\n   // Each ncclMemAlloc'd buffer needs to be registered once per comm.\n   struct RegisteredBuffers {\n     absl::Mutex mu;\n-    // Pointer to the registered buffer handle.\n-    absl::flat_hash_map<std::tuple<size_t, void*>,\n+    // Buffer range to the registered buffer handle.\n+    absl::flat_hash_map<void*,\n                         std::unique_ptr<Communicator::RegisteredBufferHandle>>\n-        records_to_handles ABSL_GUARDED_BY(mu);\n+        range_to_handle ABSL_GUARDED_BY(mu);\n   };\n   RegisteredBuffers registered_buffers_;\n };"
        },
        {
            "sha": "8172412d2f3f70b35e8c876aa0f97d9173688e56",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc?ref=62930700b980b41cf2f604f5b28b502e77df57cc",
            "patch": "@@ -152,7 +152,7 @@ TEST(NcclCommunicator, OperationsFailAfterAbort) {\n     ASSERT_THAT((*comm)->Abort(), absl_testing::IsOk());\n     AssertAborted((*comm)->HealthCheck());\n     AssertAborted((*comm)->NumRanks().status());\n-    AssertAborted((*comm)->RegisterBufferOnce(buf, buf, 0, false));\n+    AssertAborted((*comm)->RegisterBufferOnce(buf, 0, false));\n     AssertEventAborted(\n         (*comm)->AllReduce(buf, buf, dtype, count, rk, executor));\n     AssertEventAborted("
        },
        {
            "sha": "33bdf47056e5002445bff8f7f8c3c4c9ba018f80",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 10,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=62930700b980b41cf2f604f5b28b502e77df57cc",
            "patch": "@@ -351,24 +351,35 @@ absl::StatusOr<std::vector<DeviceBufferPair>> ConvertToDeviceBuffers(\n   return device_buffers;\n }\n \n+absl::Status MaybeRegisterBuffer(se::StreamExecutor* executor,\n+                                 const se::DeviceMemoryBase& buffer,\n+                                 Communicator* comm,\n+                                 bool use_symmetric_buffer) {\n+  TF_ASSIGN_OR_RETURN(auto range, executor->GetMemoryRange(buffer));\n+  VLOG(1) << \"[\" << executor->device_ordinal()\n+          << \"] Registering range: \" << range.opaque()\n+          << \" with size: \" << range.size()\n+          << \" for buffer: \" << buffer.opaque()\n+          << \" with size: \" << buffer.size()\n+          << \" is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n+  // If the collective memory buffer is a slice of a larger preallocated buffer,\n+  // we need to register the entire preallocated buffer once.\n+  return comm->RegisterBufferOnce(range, executor->device_ordinal(),\n+                                  use_symmetric_buffer);\n+}\n+\n absl::Status MaybeRegisterBuffers(se::StreamExecutor* executor,\n                                   const std::vector<DeviceBufferPair>& buffers,\n                                   Communicator* comm,\n                                   bool use_symmetric_buffer) {\n   for (int i = 0; i < buffers.size(); ++i) {\n     if (buffers[i].source_memory_space == kCollectiveMemorySpaceColor) {\n-      TF_ASSIGN_OR_RETURN(auto range,\n-                          executor->GetMemoryRange(buffers[i].source_buffer));\n-      TF_RETURN_IF_ERROR(comm->RegisterBufferOnce(\n-          buffers[i].source_buffer, range, executor->device_ordinal(),\n-          use_symmetric_buffer));\n+      TF_RETURN_IF_ERROR(MaybeRegisterBuffer(executor, buffers[i].source_buffer,\n+                                             comm, use_symmetric_buffer));\n     }\n     if (buffers[i].destination_memory_space == kCollectiveMemorySpaceColor) {\n-      TF_ASSIGN_OR_RETURN(\n-          auto range, executor->GetMemoryRange(buffers[i].destination_buffer));\n-      TF_RETURN_IF_ERROR(comm->RegisterBufferOnce(\n-          buffers[i].destination_buffer, range, executor->device_ordinal(),\n-          use_symmetric_buffer));\n+      TF_RETURN_IF_ERROR(MaybeRegisterBuffer(\n+          executor, buffers[i].destination_buffer, comm, use_symmetric_buffer));\n     }\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "901eda21d2d50a564e4392ab69304a8d6e90b932",
            "filename": "third_party/xla/xla/core/collectives/communicator.h",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62930700b980b41cf2f604f5b28b502e77df57cc/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h?ref=62930700b980b41cf2f604f5b28b502e77df57cc",
            "patch": "@@ -67,12 +67,10 @@ class Communicator {\n     virtual absl::Status Unregister() = 0;\n   };\n \n-  // Register `buffer` which can be a slice within a bigger `buffer_range` once\n-  // for efficient collective operations (i.e. on NCCL backend it registers the\n-  // buffer for zero-copy collective operations).\n+  // Register `buffer_range` once for efficient collective operations (i.e. on\n+  // NCCL backend it registers the buffer for zero-copy collective operations).\n   //\n-  virtual absl::Status RegisterBufferOnce(se::DeviceMemoryBase buffer,\n-                                          se::DeviceMemoryBase buffer_range,\n+  virtual absl::Status RegisterBufferOnce(se::DeviceMemoryBase buffer_range,\n                                           int device_ordinal,\n                                           bool use_symmetric_buffer) {\n     return Unimplemented(\"User-managed buffer registration is not supported\");"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 41,
        "deletions": 35
    }
}