{
    "author": "dimvar",
    "message": "PR #32229: Changes to the compute capabilities to account for the two different Blackwell Edge GPUs.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32229\n\nRTX PRO 6000 has CC 12.0.\nSpark has CC 12.1.\n\nRemoved the IsAtLeastBlackwellPro method because there is no guarantee that future data center GPUs will have CC higher than 12.0.\n\nAlso skipped the latency estimator test on Edge GPUs because it uses the collective performance model and crashes here:\nhttps://github.com/openxla/xla/blob/784702574ee3f2df06116e7f7e4b837150c8694a/xla/service/gpu/model/gpu_collective_performance_model.cc#L239\nCopybara import of the project:\n\n--\nca47c656de78f8c5385dcf77b7454d7adc774203 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nSome Spark fixes.                                                                                                                                                                                                                                                             Rename kBlackwellPro to kBlackwell12, as the sm_12x compute capabilities also include Spark.\n\nFix the latency estimator test and the gemm fusion autotuner test for Spark.\n\nRemoved the IsAtLeastBlackwellPro method because there is no guarantee that future\ndata center GPUs will have CC higher than 12.0.\n\nMerging this change closes #32229\n\nPiperOrigin-RevId: 817600860",
    "sha": "d0850b6100850af33fc48c0f53417ae324950a67",
    "files": [
        {
            "sha": "62915b7c319ea75b0187eebc7765597ec223af44",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=d0850b6100850af33fc48c0f53417ae324950a67",
            "patch": "@@ -330,6 +330,7 @@ TEST_F(StatelessAutotunerTest, CublasFallbackForBf16Bf16F32Algorithm) {\n                \"Hopper\";\n         break;\n       case se::CudaComputeCapability::kBlackwell:\n+      case se::CudaComputeCapability::kBlackwell_12:\n         EXPECT_TRUE(hasCublasConfig(configs))\n             << \"There should be a cublas fallback for dot_bf16_bf16_f32 on \"\n                \"Blackwell\";"
        },
        {
            "sha": "7c660761734a3dff053720c8a796e8da321d7c5f",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc?ref=d0850b6100850af33fc48c0f53417ae324950a67",
            "patch": "@@ -121,6 +121,12 @@ TEST_F(AnalyticalLatencyHidingSchedulerTest, TestAnalyticalLatencyEstimator) {\n       if (!c.IsAtLeast(se::CudaComputeCapability::kPascal)) {\n         GTEST_SKIP() << \"This test is for Pascal+ GPUs.\";\n       }\n+      if (c.major == 12 && c.minor == 1) {\n+        // Skip this test for Spark. Because of the AllReduce, the test uses\n+        // gpu_collective_performance_model, which only makes sense in a\n+        // datacenter network setting.\n+        GTEST_SKIP() << \"This test is for datacenter GPUs.\";\n+      }\n     } else if (!std::is_same_v<stream_executor::RocmComputeCapability, cc>) {\n       GTEST_SKIP() << \"This test is for Pascal+ GPUs.\";\n     }"
        },
        {
            "sha": "1cb117ac7f0bbdeedf3605d994ae2e422b012f4e",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_compute_capability.h",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h?ref=d0850b6100850af33fc48c0f53417ae324950a67",
            "patch": "@@ -70,7 +70,7 @@ struct CudaComputeCapability {\n     kAmpere = 8,\n     kHopper = 9,\n     kBlackwell = 10,\n-    kBlackwellPro = 12\n+    kBlackwell_12 = 12\n   };\n \n   constexpr CudaComputeCapability() = default;\n@@ -138,20 +138,6 @@ struct CudaComputeCapability {\n                                  FeatureExtension::kForwardCompatibleFeatures};\n   }\n \n-  // Includes all GPUs with compute capability 12.x. When comparing with\n-  // `IsAtLeast` this will true for all compute capabilities of 12.0 or higher.\n-  constexpr static CudaComputeCapability BlackwellPro() {\n-    return CudaComputeCapability{kBlackwellPro, 0, FeatureExtension::kNone};\n-  }\n-\n-  // Includes all GPUs with compute capability 12.x. When comparing with\n-  // `IsAtLeast` this will true for all 12.x compute capabilities but not for\n-  // compute capabilities with a higher major version.\n-  constexpr static CudaComputeCapability BlackwellProGenerationOnly() {\n-    return CudaComputeCapability{kBlackwellPro, 0,\n-                                 FeatureExtension::kForwardCompatibleFeatures};\n-  }\n-\n   // Returns true if the compute capability is at least\n   // `other_major.other_minor`. It is equivalent to\n   // this->SupportsAllFeaturesOf(CudaComputeCapability{other_major,\n@@ -183,10 +169,6 @@ struct CudaComputeCapability {\n     return major >= CudaComputeCapabilities::kBlackwell;\n   }\n \n-  bool IsAtLeastBlackwellPro() const {\n-    return major >= CudaComputeCapabilities::kBlackwellPro;\n-  }\n-\n   bool IsPascal() const { return major == CudaComputeCapabilities::kPascal; }\n \n   bool IsVolta() const { return major == CudaComputeCapabilities::kVolta; }\n@@ -204,10 +186,6 @@ struct CudaComputeCapability {\n     return major == CudaComputeCapabilities::kBlackwell;\n   }\n \n-  bool IsBlackwellPro() const {\n-    return major == CudaComputeCapabilities::kBlackwellPro;\n-  }\n-\n   // Returns true if a kernel compiled for compute capability `other` can be run\n   // on a GPU with compute capability `this`.\n   bool SupportsAllFeaturesOf(const CudaComputeCapability& other) const {"
        },
        {
            "sha": "71fade462cfe39798d3e0ef5c8b0e06d82212f93",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_compute_capability_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d0850b6100850af33fc48c0f53417ae324950a67/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability_test.cc?ref=d0850b6100850af33fc48c0f53417ae324950a67",
            "patch": "@@ -195,21 +195,6 @@ TEST(CudaComputeCapabilityTest, IsAtLeastMethods) {\n       CudaComputeCapability(10, 0, FeatureExtension::kAcceleratedFeatures)\n           .IsAtLeastBlackwell());\n   EXPECT_TRUE(CudaComputeCapability(10, 1).IsAtLeastBlackwell());\n-\n-  // IsAtLeastBlackwellPro (sm_120)\n-  EXPECT_FALSE(CudaComputeCapability(11, 0).IsAtLeastBlackwellPro());\n-  EXPECT_FALSE(\n-      CudaComputeCapability(11, 0, FeatureExtension::kForwardCompatibleFeatures)\n-          .IsAtLeastBlackwellPro());\n-  EXPECT_TRUE(CudaComputeCapability(12, 0).IsAtLeastBlackwellPro());\n-  EXPECT_TRUE(\n-      CudaComputeCapability(12, 0, FeatureExtension::kAcceleratedFeatures)\n-          .IsAtLeastBlackwellPro());\n-  EXPECT_TRUE(CudaComputeCapability(12, 0).IsAtLeastBlackwellPro());\n-  EXPECT_TRUE(CudaComputeCapability(12, 0).IsAtLeastBlackwellPro());\n-  EXPECT_TRUE(\n-      CudaComputeCapability(12, 0, FeatureExtension::kForwardCompatibleFeatures)\n-          .IsAtLeastBlackwellPro());\n }\n \n TEST(CudaComputeCapabilityTest, FromProtoWithFeatureExtensionSpecified) {"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 8,
        "deletions": 38
    }
}