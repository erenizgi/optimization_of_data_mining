{
    "author": "sergey-kozub",
    "message": "PR #32738: [XLA:GPU] Allow cuDNN scaled dot fusions in the gemm autotuner\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32738\n\nüìù Summary of Changes\nAllow selecting cuDNN gemm configs when autotuning scaled dot fusions.\n\nüéØ Justification\ncuDNN has a kernel for block scaled dot operations, this PR enables it in the autotuner.\nNote: XLA flag `--xla_gpu_experimental_scaled_dot_with_triton` is required to enable this.\n\nüöÄ Kind of Contribution\n‚ú® New Feature\n‚ö°Ô∏è Performance Improvement\n\nCopybara import of the project:\n\n--\n29b518309a7e2681edc8d4ac5b982dbb7222c134 by Sergey Kozub <skozub@nvidia.com>:\n\n[XLA:GPU] Allow cuDNN scaled dot fusions in the gemm autotuner\n\nMerging this change closes #32738\n\nPiperOrigin-RevId: 833238074",
    "sha": "7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
    "files": [
        {
            "sha": "f3713676400a0b570e407d77002e3e9f9dc3c1a5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -1250,15 +1250,18 @@ ENTRY main {\n       backend_config={\"fusion_backend_config\":{kind:\"__cudnn$fusion\"}}\n })\";\n   EXPECT_TRUE(*RunCuDnnFileCheck(kHloText, R\"(\n+CHECK: \"intermediate_data_type\": \"FLOAT\"\n CHECK: \"nodes\"\n CHECK: {\n CHECK: \"block_size\": [{{[[:space:]]*32[[:space:]]*}}]\n+CHECK: \"compute_data_type\": \"FLOAT\"\n CHECK: \"X\": \"lhs\"\n CHECK: \"scale\": \"lhs_scale\"\n CHECK: \"Y\": \"result_lhs_dq\"\n CHECK: \"tag\": \"BLOCK_SCALE_DEQUANTIZE\"\n CHECK: {\n CHECK: \"block_size\": [{{[[:space:]]*32[[:space:]]*}}]\n+CHECK: \"compute_data_type\": \"FLOAT\"\n CHECK: \"X\": \"rhs\"\n CHECK: \"scale\": \"rhs_scale\"\n CHECK: \"Y\": \"result_rhs_dq\""
        },
        {
            "sha": "4bad6eeb6043a14d3476606626fc0924b4615109",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -60,6 +60,7 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:stream_executor_util\",\n+        \"//xla/service/gpu/transforms:block_scaling_rewriter\",\n         \"//xla/service/gpu/transforms:cudnn_fusion_compiler\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n@@ -170,6 +171,7 @@ cc_library(\n         \"//xla/service/gpu/kernels:custom_kernel\",\n         \"//xla/service/gpu/kernels:custom_kernel_fusion\",\n         \"//xla/service/gpu/kernels:custom_kernel_fusion_pattern\",\n+        \"//xla/service/gpu/transforms:block_scaling_rewriter\",\n         \"//xla/service/gpu/transforms:custom_kernel_fusion_rewriter\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:fusion_wrapper\","
        },
        {
            "sha": "610d0475c75f99980a85de5d4dafd4b2da90329e",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 4,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -84,6 +84,7 @@ limitations under the License.\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/split_k_gemm_rewriter.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n+#include \"xla/service/gpu/transforms/block_scaling_rewriter.h\"\n #include \"xla/service/gpu/transforms/custom_kernel_fusion_rewriter.h\"\n #include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n #include \"xla/service/gpu/transforms/fusion_wrapper.h\"\n@@ -456,16 +457,25 @@ absl::StatusOr<std::unique_ptr<HloModule>> CuDnnFusionExtractor(\n   tsl::profiler::TraceMe traceme(\"CuDnnFusionExtractor\");\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n                       FusionExtractor(fusion, debug_opts));\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+\n+  // Swizzle scale tensors for block scaled dot.\n+  HloInstruction* scaled_dot = hlo_query::GetFirstInstructionWithOpcode(\n+      *root->called_computations()[0], HloOpcode::kScaledDot);\n+  if (scaled_dot != nullptr) {\n+    TF_ASSIGN_OR_RETURN(root, CudnnScaledDotHelper::AddScaleSwizzle(\n+                                  Cast<HloFusionInstruction>(root)));\n+  }\n \n+  // Update backend config of the root fusion.\n   GpuBackendConfig gpu_config;\n   FusionBackendConfig& backend_config =\n       *gpu_config.mutable_fusion_backend_config();\n   backend_config.set_kind(std::string(kCuDnnFusionKind));\n   // Provided a plan ID the autotuner just compiles one plan.\n   backend_config.mutable_cudnn_fusion_config()->set_plan_id(plan_id);\n-  TF_RETURN_IF_ERROR(\n-      module->entry_computation()->root_instruction()->set_backend_config(\n-          gpu_config));\n+  TF_RETURN_IF_ERROR(root->set_backend_config(gpu_config));\n+\n   return module;\n }\n \n@@ -764,10 +774,16 @@ absl::Status GemmFusionAutotunerRewriterVisitor::HandleFusion(\n \n   // Autotune result has a cuDNN fusion.\n   CHECK(autotune_result.has_algorithm());\n+  if (fusion_backend_config.kind() == kTritonScaledDotFusionKind) {\n+    TF_ASSIGN_OR_RETURN(fusion_instr,\n+                        CudnnScaledDotHelper::AddScaleSwizzle(\n+                            Cast<HloFusionInstruction>(fusion_instr)));\n+  }\n   fusion_backend_config.set_kind(kCuDnnFusionKind);\n   fusion_backend_config.mutable_cudnn_fusion_config()->set_plan_id(\n       autotune_result.algorithm().algo_id());\n   TF_RETURN_IF_ERROR(fusion_instr->set_backend_config(gpu_config));\n+\n   MarkAsChanged();\n   return absl::OkStatus();\n }\n@@ -907,7 +923,9 @@ GemmFusionAutotunerImpl::GenerateDotConfigs(const HloFusionInstruction& fusion,\n     }\n \n     // Add lib (e.g. cuDNN) plans, if available.\n-    if (AddLibConfigs(fusion, dot, configs)) return configs;\n+    if (AddLibConfigs(fusion, dot, configs)) {\n+      return configs;\n+    }\n   }\n \n   // Add CustomKernelFusion (Cutlass) configs, if available.\n@@ -941,6 +959,10 @@ GemmFusionAutotunerImpl::GenerateScaledDotConfigs(\n       IsAutotuningEnabled() && !config_.IsDeviceless()) {\n     // Add cuBLAS reference config, if available.\n     configs.push_back(CuBlasConfig{});\n+    // Add lib (e.g. cuDNN) plans, if available.\n+    if (AddLibConfigs(fusion, dot, configs)) {\n+      return configs;\n+    }\n   }\n \n   // TODO(b/436988479): fine tune the search space."
        },
        {
            "sha": "3771d94f4f82d568e7b833e83a176119edca64bb",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -207,7 +207,7 @@ class GemmFusionAutotunerImpl {\n   }\n \n   bool AddLibConfigs(const HloFusionInstruction& fusion,\n-                     const HloDotInstruction* dot,\n+                     const HloInstruction* dot,\n                      std::vector<BackendConfig>& configs);\n \n   std::vector<TritonGemmConfig> GetDefaultTritonConfigs() const;"
        },
        {
            "sha": "c530e8714c52b686656b9a18b5c5190850c6f45b",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 9,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n+#include \"xla/service/gpu/transforms/block_scaling_rewriter.h\"\n #include \"xla/service/gpu/transforms/cudnn_fusion_compiler.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n \n@@ -49,23 +50,32 @@ int GetCuDnnPlanCount(const HloInstruction& hlo,\n }\n \n bool GemmFusionAutotunerImpl::AddLibConfigs(\n-    const HloFusionInstruction& fusion, const HloDotInstruction* dot,\n+    const HloFusionInstruction& fusion, const HloInstruction* dot,\n     std::vector<BackendConfig>& configs) {\n   // Add cuDNN plans, if available.\n   stream_executor::CudaComputeCapability cc =\n       *GetComputeCapability().cuda_compute_capability();\n-  bool is_cudnn_enabled =\n-      !config_.IsDeviceless() &&\n-      GetDnnVersionInfoOrDefault(config_.GetExecutor()).major_version() >= 9 &&\n+  auto dnn_version = GetDnnVersionInfoOrDefault(\n+      !config_.IsDeviceless() ? config_.GetExecutor() : nullptr);\n+\n+  bool is_cudnn_fusion = IsGpuFusionKind(fusion, kCuDnnFusionKind);\n+  bool is_supported_triton_dot_fusion =\n+      IsGpuFusionKind(fusion, kTritonGemmFusionKind) &&\n+      dnn_version.major_version() >= 9 &&\n+      algorithm_util::IsSupportedByCudnn(dot->precision_config().algorithm()) &&\n       ((cc.IsAtLeastAmpere() &&\n         debug_options_.xla_gpu_cudnn_gemm_fusion_level() > 1) ||\n        (cc.IsAtLeastBlackwell() &&\n         debug_options_.xla_gpu_cudnn_gemm_fusion_level() > 0));\n-  if ((IsGpuFusionKind(fusion, kCuDnnFusionKind) && IsAutotuningEnabled()) ||\n-      (IsGpuFusionKind(fusion, kTritonGemmFusionKind) && is_cudnn_enabled &&\n-       algorithm_util::IsSupportedByCudnn(\n-           dot->precision_config().algorithm()) &&\n-       IsAutotuningEnabled())) {\n+  bool is_supported_triton_scaled_dot_fusion =\n+      IsGpuFusionKind(fusion, kTritonScaledDotFusionKind) &&\n+      dnn_version >= kCudnnSupportsBlockScaledDot &&\n+      CudnnScaledDotHelper::IsSupported(Cast<HloScaledDotInstruction>(dot)) &&\n+      cc.IsAtLeastBlackwell();\n+\n+  if (IsAutotuningEnabled() &&\n+      (is_cudnn_fusion || is_supported_triton_dot_fusion ||\n+       is_supported_triton_scaled_dot_fusion)) {\n     const int plan_count = GetCuDnnPlanCount(fusion, config_);\n     for (int plan_id = 0; plan_id < plan_count; ++plan_id) {\n       configs.push_back(CuDnnConfig{plan_id});"
        },
        {
            "sha": "b1ac498f97cd344add2b30a8f45b8b4910743945",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_rocm.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -28,7 +28,7 @@ namespace gpu {\n const int64_t GemmFusionAutotunerImpl::BLAS_GEMM_DEFAULT = HIPBLAS_GEMM_DEFAULT;\n \n bool GemmFusionAutotunerImpl::AddLibConfigs(\n-    const HloFusionInstruction& fusion, const HloDotInstruction* dot,\n+    const HloFusionInstruction& fusion, const HloInstruction* dot,\n     std::vector<BackendConfig>& configs) {\n   return false;\n }"
        },
        {
            "sha": "c800cb212c5a432b583c4bb7d77959d16564a250",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -950,6 +950,41 @@ ENTRY e {\n )\");\n }\n \n+TEST_F(GemmFusionAutotunerTest, AutotuneScaledDotCuDnnFusion) {\n+  if (GpuComputeComp().IsRocm() ||\n+      GetDebugOptionsForTest()\n+          .xla_gpu_experimental_disable_binary_libraries()) {\n+    GTEST_SKIP() << \"Not supported on ROCm or with binary libraries disabled.\";\n+  }\n+  if (!GetCudaComputeCapability().IsAtLeastBlackwell()) {\n+    GTEST_SKIP() << \"Not supported on pre-Blackwell GPUs.\";\n+  }\n+  const std::string kHlo = R\"(\n+fusion1 {\n+  %lhs = f8e4m3fn[4,192,224] parameter(0)\n+  %rhs = f8e4m3fn[4,256,224] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,192,7] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,256,7] parameter(3)\n+  ROOT %result = f32[4,192,256] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_batch_dims={0}, rhs_batch_dims={0},\n+      lhs_contracting_dims={2}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY e {\n+  %lhs = f8e4m3fn[4,192,224] parameter(0)\n+  %rhs = f8e4m3fn[4,256,224] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,192,7] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,256,7] parameter(3)\n+  ROOT _ = f32[4,192,256] fusion(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      kind=kCustom, calls=fusion1,\n+      backend_config={\"fusion_backend_config\": {kind: \"__cudnn$fusion\"}}\n+})\";\n+\n+  CheckTritonAutotuning(kHlo, R\"(\n+// CHECK: \"plan_id\":\n+)\");\n+}\n+\n // TODO(b/281489442): Write a testcase called\n // `SkipConfigsProducingDeviantResults` or similar.\n "
        },
        {
            "sha": "909437ec51974e5e66f0e73a48e2c5ea4ea74a6c",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -331,9 +331,12 @@ xla_cc_test(\n     deps = [\n         \":block_scaling_rewriter\",\n         \"//xla:error_spec\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n+        \"//xla/hlo/testlib:filecheck\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n@@ -745,6 +748,7 @@ cc_library(\n     srcs = if_cuda_is_configured([\"cudnn_fusion_compiler.cc\"]),\n     hdrs = if_cuda_is_configured([\"cudnn_fusion_compiler.h\"]),\n     deps = if_cuda_is_configured([\n+        \":block_scaling_rewriter\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:cudnn_support_utils\",\n         \"//xla/service/gpu:ir_emission_utils\","
        },
        {
            "sha": "3efcad134688c2ddaa60a8264201d8b5536fe6be",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter.cc",
            "status": "modified",
            "additions": 193,
            "deletions": 39,
            "changes": 232,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -263,6 +263,12 @@ enum class CudnnMxType {\n \n CudnnMxType GetCudnnMxType(const Shape& input_shape, const Shape& scale_shape,\n                            std::optional<int64_t> block_size) {\n+  // Non-default layout is not supported.\n+  if (!LayoutUtil::IsMonotonicWithDim0Major(input_shape.layout()) ||\n+      !LayoutUtil::IsMonotonicWithDim0Major(scale_shape.layout())) {\n+    return CudnnMxType::UNSUPPORTED_TYPE;\n+  }\n+\n   // Determine the block size from shapes, unless explicitly given.\n   int64_t actual_block_size =\n       block_size.has_value()\n@@ -307,7 +313,8 @@ bool IsSupportedByCudnn(CudnnMxType lhs, CudnnMxType rhs) {\n \n // Reshape inputs to shapes compatible with cuDNN.\n absl::StatusOr<std::tuple<XlaOp, XlaOp, int64_t>> BuildCudnnScaledDotInput(\n-    XlaOp input_op, XlaOp scale_op, std::optional<int64_t> block_size) {\n+    XlaOp input_op, XlaOp scale_op, std::optional<int64_t> block_size,\n+    bool pad_input) {\n   // Get shapes from the inputs.\n   XlaBuilder& builder = *input_op.builder();\n   TF_ASSIGN_OR_RETURN(Shape input_shape, builder.GetShape(input_op));\n@@ -330,12 +337,6 @@ absl::StatusOr<std::tuple<XlaOp, XlaOp, int64_t>> BuildCudnnScaledDotInput(\n   TF_RET_CHECK(size_noncontracting <= scale_noncontracting);\n   TF_RET_CHECK(rank == 2 || scale_shape.dimensions(0) == batch_size);\n \n-  // Reshape inputs, if necessary.\n-  if (rank != 3) {\n-    input_op = Reshape(input_op, {1, size_noncontracting, size_contracting});\n-    scale_op = Reshape(scale_op, {1, scale_noncontracting, scale_contracting});\n-  }\n-\n   // cuDNN kernel imposes constraints on the input shape sizes.\n   const int64_t kInputNonContractingTileSize = 128;\n   const int64_t kScaleContractingTileSize = 4;\n@@ -350,9 +351,9 @@ absl::StatusOr<std::tuple<XlaOp, XlaOp, int64_t>> BuildCudnnScaledDotInput(\n         RoundUpTo(scale_contracting, kScaleContractingTileSize);\n \n     // Pad input tensor, if necessary.\n-    if (size_noncontracting != padded_noncontracting) {\n-      PaddingConfig input_padding_config = MakeNoPaddingConfig(/*rank=*/3);\n-      input_padding_config.mutable_dimensions(1)->set_edge_padding_high(\n+    if (size_noncontracting != padded_noncontracting && pad_input) {\n+      PaddingConfig input_padding_config = MakeNoPaddingConfig(rank);\n+      input_padding_config.mutable_dimensions(rank - 2)->set_edge_padding_high(\n           padded_noncontracting - size_noncontracting);\n       input_op = Pad(input_op, Zero(&builder, input_shape.element_type()),\n                      input_padding_config);\n@@ -361,10 +362,10 @@ absl::StatusOr<std::tuple<XlaOp, XlaOp, int64_t>> BuildCudnnScaledDotInput(\n     // Pad scale tensor, if necessary.\n     if (scale_noncontracting != padded_noncontracting ||\n         scale_contracting != padded_contracting) {\n-      PaddingConfig scale_padding_config = MakeNoPaddingConfig(/*rank=*/3);\n-      scale_padding_config.mutable_dimensions(1)->set_edge_padding_high(\n+      PaddingConfig scale_padding_config = MakeNoPaddingConfig(rank);\n+      scale_padding_config.mutable_dimensions(rank - 2)->set_edge_padding_high(\n           padded_noncontracting - scale_noncontracting);\n-      scale_padding_config.mutable_dimensions(2)->set_edge_padding_high(\n+      scale_padding_config.mutable_dimensions(rank - 1)->set_edge_padding_high(\n           padded_contracting - scale_contracting);\n       scale_op = Pad(scale_op, Zero(&builder, scale_shape.element_type()),\n                      scale_padding_config);\n@@ -378,8 +379,8 @@ absl::StatusOr<std::tuple<XlaOp, XlaOp, int64_t>> BuildCudnnScaledDotInput(\n   // using non-vectorized loads or using an extra shared memory buffer).\n   // https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-a-layout-1x\n   TF_ASSIGN_OR_RETURN(Shape scale_valid_shape, builder.GetShape(scale_op));\n-  int64_t scale_rows = scale_valid_shape.dimensions(1);\n-  int64_t scale_cols = scale_valid_shape.dimensions(2);\n+  int64_t scale_rows = scale_valid_shape.dimensions(rank - 2);\n+  int64_t scale_cols = scale_valid_shape.dimensions(rank - 1);\n   scale_op =\n       Reshape(scale_op, {batch_size, scale_rows / kInputNonContractingTileSize,\n                          4, 32, scale_cols / kScaleContractingTileSize,\n@@ -402,23 +403,27 @@ absl::StatusOr<XlaOp> BuildCudnnScaledDot(XlaOp lhs_input, XlaOp rhs_input,\n       cudnn_version >= kCudnnSupportsBlockScaledDotWithGlobalScale;\n \n   // Get inputs from parameters.\n-  TF_ASSIGN_OR_RETURN(\n-      auto lhs_ops_and_size,\n-      BuildCudnnScaledDotInput(lhs_input, lhs_scale, block_size));\n+  TF_ASSIGN_OR_RETURN(auto lhs_ops_and_size,\n+                      BuildCudnnScaledDotInput(lhs_input, lhs_scale, block_size,\n+                                               /*pad_input=*/true));\n   auto [lhs_input_op, lhs_scale_op, lhs_size] = lhs_ops_and_size;\n \n-  TF_ASSIGN_OR_RETURN(\n-      auto rhs_ops_and_size,\n-      BuildCudnnScaledDotInput(rhs_input, rhs_scale, block_size));\n+  TF_ASSIGN_OR_RETURN(auto rhs_ops_and_size,\n+                      BuildCudnnScaledDotInput(rhs_input, rhs_scale, block_size,\n+                                               /*pad_input=*/true));\n   auto [rhs_input_op, rhs_scale_op, rhs_size] = rhs_ops_and_size;\n \n   // Calculate output shape.\n   XlaBuilder& builder = *lhs_input.builder();\n   TF_ASSIGN_OR_RETURN(Shape lhs_shape, builder.GetShape(lhs_input_op));\n   TF_ASSIGN_OR_RETURN(Shape rhs_shape, builder.GetShape(rhs_input_op));\n-  Shape result_shape = ShapeUtil::MakeShape(\n-      result_type, {lhs_shape.dimensions(0), lhs_shape.dimensions(1),\n-                    rhs_shape.dimensions(1)});\n+  int rank = lhs_shape.dimensions().size();\n+  std::vector<int64_t> result_dims{lhs_shape.dimensions(rank - 2),\n+                                   rhs_shape.dimensions(rank - 2)};\n+  if (rank == 3) {\n+    result_dims.insert(result_dims.begin(), lhs_shape.dimensions(0));\n+  }\n+  Shape result_shape = ShapeUtil::MakeShape(result_type, result_dims);\n   Shape scratch_shape = ShapeUtil::MakeShape(PrimitiveType::U8, {0});\n   Shape output_shape = ShapeUtil::MakeTupleShape({result_shape, scratch_shape});\n \n@@ -440,10 +445,13 @@ absl::StatusOr<XlaOp> BuildCudnnScaledDot(XlaOp lhs_input, XlaOp rhs_input,\n   }\n \n   // Slice the result, if necessary.\n-  if (lhs_size != lhs_shape.dimensions(1) ||\n-      rhs_size != rhs_shape.dimensions(1)) {\n-    std::vector<int64_t> limit{lhs_shape.dimensions(0), lhs_size, rhs_size};\n-    result = Slice(result, {0, 0, 0}, limit, {1, 1, 1});\n+  if (lhs_size != lhs_shape.dimensions(rank - 2) ||\n+      rhs_size != rhs_shape.dimensions(rank - 2)) {\n+    std::vector<int64_t> start(rank, 0);\n+    std::vector<int64_t> strides(rank, 1);\n+    result_dims[rank - 2] = lhs_size;\n+    result_dims[rank - 1] = rhs_size;\n+    result = Slice(result, start, result_dims, strides);\n   }\n   return result;\n }\n@@ -609,22 +617,82 @@ absl::StatusOr<HloInstruction*> ExpandBlockScaledDotCustomCall(\n   // Build replacement instruction sequence.\n   XlaBuilder builder(std::string(instruction->name()));\n   auto operands = absl::MakeSpan(instruction->operands());\n-  TF_ASSIGN_OR_RETURN(\n-      XlaOp block_scaled_dot,\n+  TF_RETURN_IF_ERROR(\n       BuildBlockScaledDot(builder, operands[0], operands[1], operands[2],\n                           operands.size() >= 4 ? operands[3] : nullptr,\n                           operands.size() == 5 ? operands[4] : nullptr, dnums,\n-                          result_type, block_size, std::move(cudnn_version)));\n+                          result_type, block_size, std::move(cudnn_version))\n+          .status());\n+  return ExpandInstructionUsingBuilder(builder, instruction);\n+}\n+\n+// ----- cuDNN scale swizzling\n+\n+absl::StatusOr<HloComputation*> CreateScaleSwizzleComputation(\n+    const HloInstruction* input, const HloInstruction* scale) {\n+  // Create XLA builder and parameters.\n+  std::string name = absl::StrCat(scale->name(), \"_swizzle\");\n+  XlaBuilder builder(name);\n+  XlaOp input_op = Parameter(&builder, 0, input->shape(), \"input\");\n+  XlaOp scale_op = Parameter(&builder, 1, scale->shape(), \"scale\");\n+\n+  // Build swizzle computation.\n+  TF_ASSIGN_OR_RETURN(\n+      auto ops_and_size,\n+      BuildCudnnScaledDotInput(input_op, scale_op, /*block_size=*/std::nullopt,\n+                               /*pad_input=*/false));\n+  auto [result_input_op, result_scale_op, _] = ops_and_size;\n+  Tuple(&builder, {result_input_op, result_scale_op});\n \n-  // Reshape to the expected output shape.\n-  // This should only happen when a unit-sized dimension is added by the pass.\n-  TF_ASSIGN_OR_RETURN(Shape result_shape, builder.GetShape(block_scaled_dot));\n-  if (result_shape != instruction->shape()) {\n-    CHECK_EQ(ShapeUtil::ElementsIn(instruction->shape()),\n-             ShapeUtil::ElementsIn(result_shape));\n-    Reshape(instruction->shape(), block_scaled_dot);\n+  TF_ASSIGN_OR_RETURN(XlaComputation xla_computation, builder.Build());\n+  TF_ASSIGN_OR_RETURN(\n+      HloComputation * computation,\n+      XlaComputationToHloComputation(xla_computation, input->GetModule()));\n+\n+  for (HloInstruction* instr : computation->instructions()) {\n+    // Replace reshapes with bitcasts (post layout assignment).\n+    if (instr->opcode() == HloOpcode::kReshape) {\n+      TF_RETURN_IF_ERROR(computation->ReplaceInstruction(\n+          instr, computation->AddInstruction(HloInstruction::CreateBitcast(\n+                     instr->shape(), instr->mutable_operand(0)))));\n+    }\n+    // Fix transpose layouts (generated as no-ops).\n+    if (instr->opcode() == HloOpcode::kTranspose) {\n+      *instr->mutable_shape()->mutable_layout() =\n+          LayoutUtil::GetDefaultLayoutForShape(instr->shape());\n+    }\n   }\n-  return ExpandInstructionUsingBuilder(builder, instruction);\n+  return computation;\n+}\n+\n+absl::Status SliceScaledDotOperands(HloInstruction* scaled_dot) {\n+  // Create scaled dot operation with noncontracting dimensions sliced.\n+  int rank = scaled_dot->shape().dimensions().size();\n+  HloComputation* computation = scaled_dot->parent();\n+\n+  // Create slice operations for LHS/RHS.\n+  std::vector<HloInstruction*> new_operands(scaled_dot->operands().begin(),\n+                                            scaled_dot->operands().end());\n+  for (int i = 0; i < 2; ++i) {\n+    const Shape& input_shape = scaled_dot->operand(i)->shape();\n+    const Shape& scale_shape = scaled_dot->operand(i + 2)->shape();\n+    if (input_shape.dimensions(rank - 2) != scale_shape.dimensions(rank - 2)) {\n+      std::vector<int64_t> start(rank, 0);\n+      std::vector<int64_t> strides(rank, 1);\n+      std::vector<int64_t> limit(input_shape.dimensions().begin(),\n+                                 input_shape.dimensions().end());\n+      limit[rank - 1] = scale_shape.dimensions(rank - 1);\n+      new_operands[i + 2] =\n+          computation->AddInstruction(HloInstruction::CreateSlice(\n+              ShapeUtil::MakeShape(scale_shape.element_type(), limit),\n+              scaled_dot->mutable_operand(i + 2), start, limit, strides));\n+    }\n+  }\n+\n+  // Replace scaled dot instruction operands.\n+  HloInstruction* new_scaled_dot = computation->AddInstruction(\n+      scaled_dot->CloneWithNewOperands(scaled_dot->shape(), new_operands));\n+  return computation->ReplaceInstruction(scaled_dot, new_scaled_dot);\n }\n \n }  // namespace\n@@ -652,4 +720,90 @@ absl::StatusOr<HloInstruction*> BlockScalingRewriter::ExpandInstruction(\n              << instruction->custom_call_target();\n }\n \n+bool CudnnScaledDotHelper::IsSupported(\n+    const HloScaledDotInstruction* scaled_dot) {\n+  const HloInstruction* lhs_input = scaled_dot->operand(0);\n+  const HloInstruction* rhs_input = scaled_dot->operand(1);\n+  const HloInstruction* lhs_scale = scaled_dot->operand(2);\n+  const HloInstruction* rhs_scale = scaled_dot->operand(3);\n+\n+  // Input fusion is not supported, as the underlying kernel reads from HBM.\n+  auto is_parameter = [](const HloInstruction* instr, int index) {\n+    return instr->opcode() == HloOpcode::kParameter &&\n+           instr->parameter_number() == index && instr->user_count() == 1;\n+  };\n+  if (!is_parameter(lhs_input, 0) || !is_parameter(rhs_input, 1) ||\n+      !is_parameter(lhs_scale, 2) || !is_parameter(rhs_scale, 3)) {\n+    return false;\n+  }\n+\n+  // The dot dimension numbers must have fixed order: batch dimension first\n+  // (if present) and contracting dimension last.\n+  const DotDimensionNumbers& dnums = scaled_dot->dot_dimension_numbers();\n+  int rank = lhs_input->shape().dimensions().size();\n+  if (dnums.lhs_contracting_dimensions()[0] != rank - 1 ||\n+      dnums.rhs_contracting_dimensions()[0] != rank - 1 ||\n+      (rank == 3 && (dnums.lhs_batch_dimensions()[0] != 0 ||\n+                     dnums.rhs_batch_dimensions()[0] != 0))) {\n+    return false;\n+  }\n+\n+  // cuDNN kernel supports a subset of block scaled types.\n+  return IsSupportedByCudnn(\n+      GetCudnnMxType(lhs_input->shape(), lhs_scale->shape(), std::nullopt),\n+      GetCudnnMxType(rhs_input->shape(), rhs_scale->shape(), std::nullopt));\n+}\n+\n+absl::StatusOr<HloInstruction*> CudnnScaledDotHelper::AddScaleSwizzle(\n+    HloFusionInstruction* fusion) {\n+  HloComputation* parent = fusion->parent();\n+  int rank = fusion->shape().dimensions().size();\n+\n+  // Add swizzling to LHS/RHS.\n+  std::vector<HloInstruction*> swizzled_operands(4);\n+  for (int i = 0; i < 2; ++i) {\n+    TF_ASSIGN_OR_RETURN(HloComputation * swizzle_computation,\n+                        CreateScaleSwizzleComputation(fusion->operand(i),\n+                                                      fusion->operand(i + 2)));\n+    HloInstruction* call = parent->AddInstruction(HloInstruction::CreateCall(\n+        swizzle_computation->root_instruction()->shape(),\n+        {fusion->mutable_operand(i), fusion->mutable_operand(i + 2)},\n+        swizzle_computation));\n+    for (int j = 0; j < 2; ++j) {\n+      swizzled_operands[i + j * 2] =\n+          parent->AddInstruction(HloInstruction::CreateGetTupleElement(\n+              call->shape().tuple_shapes(j), call, j));\n+    }\n+  }\n+\n+  // Update fusion computation parameter shapes, if needed.\n+  HloComputation* computation = fusion->fused_instructions_computation();\n+  bool need_slicing = false;\n+  for (int i = 0; i < 4; ++i) {\n+    HloInstruction* param = computation->parameter_instruction(i);\n+    const Shape& swizzled_shape = swizzled_operands[i]->shape();\n+    Shape* param_shape = param->mutable_shape();\n+    if (*param_shape != swizzled_shape) {\n+      need_slicing |= param_shape->dimensions(rank - 2) !=\n+                      swizzled_shape.dimensions(rank - 2);\n+      *param_shape = swizzled_shape;\n+    }\n+  }\n+\n+  // Replace scaled dot if any inputs need slicing.\n+  if (need_slicing) {\n+    HloInstruction* scaled_dot =\n+        computation->parameter_instruction(0)->users()[0];\n+    TF_RETURN_IF_ERROR(SliceScaledDotOperands(scaled_dot));\n+  }\n+\n+  // Create new fusion with the swizzled operands.\n+  HloInstruction* new_fusion =\n+      parent->AddInstruction(HloInstruction::CreateFusion(\n+          computation->root_instruction()->shape(), fusion->fusion_kind(),\n+          swizzled_operands, fusion->fused_instructions_computation()));\n+  TF_RETURN_IF_ERROR(parent->ReplaceInstruction(fusion, new_fusion));\n+  return new_fusion;\n+}\n+\n }  // namespace xla::gpu"
        },
        {
            "sha": "5cc1c31c0b40ccf2b560ab1f942520d4fd04e058",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter.h",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.h?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/transforms/expanders/op_expander_pass.h\"\n #include \"xla/stream_executor/dnn.h\"\n \n@@ -98,6 +99,18 @@ class BlockScalingRewriter : public OpExpanderPass {\n   se::dnn::VersionInfo cudnn_version_;\n };\n \n+// Helper class for building cuDNN scaled dot operations.\n+class CudnnScaledDotHelper {\n+ public:\n+  // Check if the scaled dot fusion is supported by cuDNN.\n+  static bool IsSupported(const HloScaledDotInstruction* scaled_dot);\n+\n+  // Extract scale tensor swizzling from the block scaled dot fusion into\n+  // separate computations.\n+  static absl::StatusOr<HloInstruction*> AddScaleSwizzle(\n+      HloFusionInstruction* fusion);\n+};\n+\n }  // namespace xla::gpu\n \n #endif  // XLA_SERVICE_GPU_TRANSFORMS_BLOCK_SCALING_REWRITER_H_"
        },
        {
            "sha": "6b15e98ae18c08cf229b286dcdaff402cd9ec2a1",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter_test.cc",
            "status": "modified",
            "additions": 238,
            "deletions": 13,
            "changes": 251,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -18,9 +18,12 @@ limitations under the License.\n #include <utility>\n \n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/error_spec.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n+#include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -336,26 +339,21 @@ ENTRY main {\n   BlockScalingRewriter pass(kCudnnSupportsBlockScaledDot);\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs:%.+]] = f8e4m3fn[128,96]{1,0} parameter(0)\n-  CHECK: [[lhs_rs:%.+]] = f8e4m3fn[1,128,96]{2,1,0} reshape([[lhs]])\n   CHECK: [[rhs:%.+]] = f8e4m3fn[120,96]{1,0} parameter(1)\n-  CHECK: [[rhs_rs:%.+]] = f8e4m3fn[1,120,96]{2,1,0} reshape([[rhs]])\n-  CHECK: [[rhs_pad:%.+]] = f8e4m3fn[1,128,96]{2,1,0} pad([[rhs_rs]], {{.+}}), padding=0_0x0_8x0_0\n+  CHECK: [[rhs_pad:%.+]] = f8e4m3fn[128,96]{1,0} pad([[rhs]], {{.+}}), padding=0_8x0_0\n   CHECK: [[lhs_scale:%.+]] = f8e8m0fnu[128,3]{1,0} parameter(2)\n-  CHECK: [[lhs_scale_rs:%.+]] = f8e8m0fnu[1,128,3]{2,1,0} reshape([[lhs_scale]])\n-  CHECK: [[lhs_scale_pad:%.+]] = f8e8m0fnu[1,128,4]{2,1,0} pad([[lhs_scale_rs]], {{.+}}), padding=0_0x0_0x0_1\n+  CHECK: [[lhs_scale_pad:%.+]] = f8e8m0fnu[128,4]{1,0} pad([[lhs_scale]], {{.+}}), padding=0_0x0_1\n   CHECK: [[lhs_scale_rs2:%.+]] = f8e8m0fnu[1,1,4,32,1,4]{5,4,3,2,1,0} reshape([[lhs_scale_pad]])\n   CHECK: [[lhs_scale_tr2:%.+]] = f8e8m0fnu[1,1,1,32,4,4]{5,2,3,4,1,0} transpose([[lhs_scale_rs2]]), dimensions={0,1,4,3,2,5}\n-  CHECK: [[lhs_scale_swizzle:%.+]] = f8e8m0fnu[1,128,4]{2,1,0} reshape([[lhs_scale_tr2]])\n+  CHECK: [[lhs_scale_swizzle:%.+]] = f8e8m0fnu[128,4]{1,0} reshape([[lhs_scale_tr2]])\n   CHECK: [[rhs_scale:%.+]] = f8e8m0fnu[120,3]{1,0} parameter(3)\n-  CHECK: [[rhs_scale_rs:%.+]] = f8e8m0fnu[1,120,3]{2,1,0} reshape([[rhs_scale]])\n-  CHECK: [[rhs_scale_pad:%.+]] = f8e8m0fnu[1,128,4]{2,1,0} pad([[rhs_scale_rs]], {{.+}}), padding=0_0x0_8x0_1\n+  CHECK: [[rhs_scale_pad:%.+]] = f8e8m0fnu[128,4]{1,0} pad([[rhs_scale]], {{.+}}), padding=0_8x0_1\n   CHECK: [[rhs_scale_rs2:%.+]] = f8e8m0fnu[1,1,4,32,1,4]{5,4,3,2,1,0} reshape([[rhs_scale_pad]])\n   CHECK: [[rhs_scale_tr2:%.+]] = f8e8m0fnu[1,1,1,32,4,4]{5,2,3,4,1,0} transpose([[rhs_scale_rs2]]), dimensions={0,1,4,3,2,5}\n-  CHECK: [[rhs_scale_swizzle:%.+]] = f8e8m0fnu[1,128,4]{2,1,0} reshape([[rhs_scale_tr2]])\n-  CHECK: [[call:%.+]] = ({{.+}}) custom-call([[lhs_rs]], [[rhs_pad]], [[lhs_scale_swizzle]], [[rhs_scale_swizzle]])\n-  CHECK: [[gte:%.+]] = f16[1,128,128]{2,1,0} get-tuple-element([[call]]), index=0\n-  CHECK: [[slice:%.+]] = f16[1,128,120]{2,1,0} slice([[gte]]), slice={[0:1], [0:128], [0:120]}\n-  CHECK: ROOT {{.+}} = f16[128,120]{1,0} reshape([[slice]])\n+  CHECK: [[rhs_scale_swizzle:%.+]] = f8e8m0fnu[128,4]{1,0} reshape([[rhs_scale_tr2]])\n+  CHECK: [[call:%.+]] = ({{.+}}) custom-call([[lhs]], [[rhs_pad]], [[lhs_scale_swizzle]], [[rhs_scale_swizzle]])\n+  CHECK: [[gte:%.+]] = f16[128,128]{1,0} get-tuple-element([[call]]), index=0\n+  CHECK: ROOT {{.+}} = f16[128,120]{1,0} slice([[gte]]), slice={[0:128], [0:120]}\n })\");\n }\n \n@@ -394,5 +392,232 @@ ENTRY main {\n })\");\n }\n \n+TEST_F(BlockScalingRewriterTest, CudnnFusionSupportedE4M3) {\n+  constexpr absl::string_view hlo_string = R\"(\n+ENTRY main {\n+  %lhs = f8e4m3fn[128,256] parameter(0)\n+  %rhs = f8e4m3fn[128,256] parameter(1)\n+  %lhs_scale = f8e8m0fnu[128,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[128,8] parameter(3)\n+  ROOT %result = f16[128,128] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  EXPECT_TRUE(CudnnScaledDotHelper::IsSupported(Cast<HloScaledDotInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+}\n+\n+TEST_F(BlockScalingRewriterTest, CudnnFusionUnsupportedE5M2) {\n+  constexpr absl::string_view hlo_string = R\"(\n+ENTRY main {\n+  %lhs = f8e5m2[128,256] parameter(0)\n+  %rhs = f8e5m2[128,256] parameter(1)\n+  %lhs_scale = f8e8m0fnu[128,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[128,8] parameter(3)\n+  ROOT %result = f16[128,128] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  EXPECT_FALSE(CudnnScaledDotHelper::IsSupported(Cast<HloScaledDotInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+}\n+\n+TEST_F(BlockScalingRewriterTest, CudnnFusionUnsupportedDimensions) {\n+  constexpr absl::string_view hlo_string = R\"(\n+ENTRY main {\n+  %lhs = f8e4m3fn[128,256] parameter(0)\n+  %rhs = f8e4m3fn[256,128] parameter(1)\n+  %lhs_scale = f8e8m0fnu[128,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[8,128] parameter(3)\n+  ROOT %result = f16[128,128] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  EXPECT_FALSE(CudnnScaledDotHelper::IsSupported(Cast<HloScaledDotInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+}\n+\n+TEST_F(BlockScalingRewriterTest, CudnnFusionUnupportedLayout) {\n+  constexpr absl::string_view hlo_string = R\"(\n+HloModule test\n+\n+ENTRY main {\n+  %lhs = f8e4m3fn[16,128]{0,1} parameter(0)\n+  %rhs = f8e4m3fn[32,128]{1,0} parameter(1)\n+  %lhs_scale = f8e8m0fnu[16,4]{1,0} parameter(2)\n+  %rhs_scale = f8e8m0fnu[32,4]{1,0} parameter(3)\n+  ROOT %result = f16[16,32]{1,0} scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  EXPECT_FALSE(CudnnScaledDotHelper::IsSupported(Cast<HloScaledDotInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+}\n+\n+TEST_F(BlockScalingRewriterTest, CudnnFusionUnsupportedInputs) {\n+  constexpr absl::string_view hlo_string = R\"(\n+ENTRY main {\n+  %lhs = f8e4m3fn[128] parameter(0)\n+  %lhs_bc = f8e4m3fn[128,256] broadcast(%lhs), dimensions={0}\n+  %rhs = f8e4m3fn[128,256] parameter(1)\n+  %lhs_scale = f8e8m0fnu[128,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[128,8] parameter(3)\n+  ROOT %result = f16[128,128] scaled-dot(%lhs_bc, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  EXPECT_FALSE(CudnnScaledDotHelper::IsSupported(Cast<HloScaledDotInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+}\n+\n+TEST_F(BlockScalingRewriterTest, CudnnFusionSwizzleSimple) {\n+  constexpr absl::string_view hlo_string = R\"(\n+fusion {\n+  %lhs = f8e4m3fn[4,384,256] parameter(0)\n+  %rhs = f8e4m3fn[4,512,256] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,384,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,512,8] parameter(3)\n+  ROOT %result = f32[4,384,512] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_batch_dims={0}, rhs_batch_dims={0},\n+      lhs_contracting_dims={2}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY main {\n+  %lhs = f8e4m3fn[4,384,256] parameter(0)\n+  %rhs = f8e4m3fn[4,512,256] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,384,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,512,8] parameter(3)\n+  ROOT %result = f32[4,384,512] fusion(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      kind=kCustom, calls=fusion,\n+      backend_config={\"fusion_backend_config\":{\"kind\":\"__cudnn$fusion\"}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  ASSERT_IS_OK(CudnnScaledDotHelper::AddScaleSwizzle(Cast<HloFusionInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+\n+  constexpr absl::string_view expected = R\"(\n+  // %lhs_scale.1_swizzle.1\n+  CHECK: [[lhs_bc:%.+]] = f8e8m0fnu[4,3,4,32,2,4]{5,4,3,2,1,0} bitcast({{.+}})\n+  CHECK: [[lhs_tr:%.+]] = f8e8m0fnu[4,3,2,32,4,4]{5,4,3,2,1,0} transpose([[lhs_bc]]), dimensions={0,1,4,3,2,5}\n+  CHECK: {{.+}} = f8e8m0fnu[4,384,8]{2,1,0} bitcast([[lhs_tr]])\n+  // %rhs_scale.1_swizzle.1\n+  CHECK: [[rhs_bc:%.+]] = f8e8m0fnu[4,4,4,32,2,4]{5,4,3,2,1,0} bitcast({{.+}})\n+  CHECK: [[rhs_tr:%.+]] = f8e8m0fnu[4,4,2,32,4,4]{5,4,3,2,1,0} transpose([[rhs_bc]]), dimensions={0,1,4,3,2,5}\n+  CHECK: {{.+}} = f8e8m0fnu[4,512,8]{2,1,0} bitcast([[rhs_tr]])\n+  // %fusion\n+  CHECK: [[lhs:%.+]] = f8e4m3fn[4,384,256]{2,1,0} parameter(0)\n+  CHECK: [[rhs:%.+]] = f8e4m3fn[4,512,256]{2,1,0} parameter(1)\n+  CHECK: [[lhs_scale:%.+]] = f8e8m0fnu[4,384,8]{2,1,0} parameter(2)\n+  CHECK: [[rhs_scale:%.+]] = f8e8m0fnu[4,512,8]{2,1,0} parameter(3)\n+  CHECK: {{.+}} = f32[4,384,512]{2,1,0} scaled-dot([[lhs]], [[rhs]], [[lhs_scale]], [[rhs_scale]])\n+)\";\n+  EXPECT_THAT(RunFileCheck(test_module->ToString(), expected),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(BlockScalingRewriterTest, CudnnFusionSwizzlePadContracting) {\n+  constexpr absl::string_view hlo_string = R\"(\n+fusion {\n+  %lhs = f8e4m3fn[4,384,224] parameter(0)\n+  %rhs = f8e4m3fn[4,512,224] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,384,7] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,512,7] parameter(3)\n+  ROOT %result = f32[4,384,512] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_batch_dims={0}, rhs_batch_dims={0},\n+      lhs_contracting_dims={2}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY main {\n+  %lhs = f8e4m3fn[4,384,224] parameter(0)\n+  %rhs = f8e4m3fn[4,512,224] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,384,7] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,512,7] parameter(3)\n+  ROOT %result = f32[4,384,512] fusion(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      kind=kCustom, calls=fusion,\n+      backend_config={\"fusion_backend_config\":{\"kind\":\"__cudnn$fusion\"}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  ASSERT_IS_OK(CudnnScaledDotHelper::AddScaleSwizzle(Cast<HloFusionInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+\n+  constexpr absl::string_view expected = R\"(\n+  // %lhs_scale.1_swizzle.1\n+  CHECK: [[lhs_pad:%.+]] = f8e8m0fnu[4,384,8]{2,1,0} pad({{.+}}, {{.+}}), padding=0_0x0_0x0_1\n+  CHECK: [[lhs_bc:%.+]] = f8e8m0fnu[4,3,4,32,2,4]{5,4,3,2,1,0} bitcast([[lhs_pad]])\n+  CHECK: [[lhs_tr:%.+]] = f8e8m0fnu[4,3,2,32,4,4]{5,4,3,2,1,0} transpose([[lhs_bc]]), dimensions={0,1,4,3,2,5}\n+  CHECK: {{.+}} = f8e8m0fnu[4,384,8]{2,1,0} bitcast([[lhs_tr]])\n+  // %rhs_scale.1_swizzle.1\n+  CHECK: [[rhs_pad:%.+]] = f8e8m0fnu[4,512,8]{2,1,0} pad({{.+}}, {{.+}}), padding=0_0x0_0x0_1\n+  CHECK: [[rhs_bc:%.+]] = f8e8m0fnu[4,4,4,32,2,4]{5,4,3,2,1,0} bitcast([[rhs_pad]])\n+  CHECK: [[rhs_tr:%.+]] = f8e8m0fnu[4,4,2,32,4,4]{5,4,3,2,1,0} transpose([[rhs_bc]]), dimensions={0,1,4,3,2,5}\n+  CHECK: {{.+}} = f8e8m0fnu[4,512,8]{2,1,0} bitcast([[rhs_tr]])\n+  // %fusion\n+  CHECK: [[lhs:%.+]] = f8e4m3fn[4,384,224]{2,1,0} parameter(0)\n+  CHECK: [[rhs:%.+]] = f8e4m3fn[4,512,224]{2,1,0} parameter(1)\n+  CHECK: [[lhs_scale:%.+]] = f8e8m0fnu[4,384,8]{2,1,0} parameter(2)\n+  CHECK: [[rhs_scale:%.+]] = f8e8m0fnu[4,512,8]{2,1,0} parameter(3)\n+  CHECK: {{.+}} = f32[4,384,512]{2,1,0} scaled-dot([[lhs]], [[rhs]], [[lhs_scale]], [[rhs_scale]])\n+)\";\n+  EXPECT_THAT(RunFileCheck(test_module->ToString(), expected),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(BlockScalingRewriterTest, CudnnFusionSwizzlePadNoncontracting) {\n+  constexpr absl::string_view hlo_string = R\"(\n+fusion {\n+  %lhs = f8e4m3fn[4,320,256] parameter(0)\n+  %rhs = f8e4m3fn[4,448,256] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,320,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,448,8] parameter(3)\n+  ROOT %result = f32[4,320,448] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      lhs_batch_dims={0}, rhs_batch_dims={0},\n+      lhs_contracting_dims={2}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY main {\n+  %lhs = f8e4m3fn[4,320,256] parameter(0)\n+  %rhs = f8e4m3fn[4,448,256] parameter(1)\n+  %lhs_scale = f8e8m0fnu[4,320,8] parameter(2)\n+  %rhs_scale = f8e8m0fnu[4,448,8] parameter(3)\n+  ROOT %result = f32[4,320,448] fusion(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      kind=kCustom, calls=fusion,\n+      backend_config={\"fusion_backend_config\":{\"kind\":\"__cudnn$fusion\"}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  ASSERT_IS_OK(CudnnScaledDotHelper::AddScaleSwizzle(Cast<HloFusionInstruction>(\n+      test_module->entry_computation()->root_instruction())));\n+\n+  constexpr absl::string_view expected = R\"(\n+  // %lhs_scale.1_swizzle.1\n+  CHECK: [[lhs_pad:%.+]] = f8e8m0fnu[4,384,8]{2,1,0} pad({{.+}}, {{.+}}), padding=0_0x0_64x0_0\n+  CHECK: [[lhs_bc:%.+]] = f8e8m0fnu[4,3,4,32,2,4]{5,4,3,2,1,0} bitcast([[lhs_pad]])\n+  CHECK: [[lhs_tr:%.+]] = f8e8m0fnu[4,3,2,32,4,4]{5,4,3,2,1,0} transpose([[lhs_bc]]), dimensions={0,1,4,3,2,5}\n+  CHECK: {{.+}} = f8e8m0fnu[4,384,8]{2,1,0} bitcast([[lhs_tr]])\n+  // %rhs_scale.1_swizzle.1\n+  CHECK: [[rhs_pad:%.+]] = f8e8m0fnu[4,512,8]{2,1,0} pad({{.+}}, {{.+}}), padding=0_0x0_64x0_0\n+  CHECK: [[rhs_bc:%.+]] = f8e8m0fnu[4,4,4,32,2,4]{5,4,3,2,1,0} bitcast([[rhs_pad]])\n+  CHECK: [[rhs_tr:%.+]] = f8e8m0fnu[4,4,2,32,4,4]{5,4,3,2,1,0} transpose([[rhs_bc]]), dimensions={0,1,4,3,2,5}\n+  CHECK: {{.+}} = f8e8m0fnu[4,512,8]{2,1,0} bitcast([[rhs_tr]])\n+  // %fusion\n+  CHECK: [[lhs:%.+]] = f8e4m3fn[4,320,256]{2,1,0} parameter(0)\n+  CHECK: [[rhs:%.+]] = f8e4m3fn[4,448,256]{2,1,0} parameter(1)\n+  CHECK: [[lhs_scale:%.+]] = f8e8m0fnu[4,384,8]{2,1,0} parameter(2)\n+  CHECK: [[lhs_slice:%.+]] = f8e8m0fnu[4,320,8]{2,1,0} slice([[lhs_scale]]), slice={[0:4], [0:320], [0:8]}\n+  CHECK: [[rhs_scale:%.+]] = f8e8m0fnu[4,512,8]{2,1,0} parameter(3)\n+  CHECK: [[rhs_slice:%.+]] = f8e8m0fnu[4,448,8]{2,1,0} slice([[rhs_scale]]), slice={[0:4], [0:448], [0:8]}\n+  CHECK: {{.+}} = f32[4,320,448]{2,1,0} scaled-dot([[lhs]], [[rhs]], [[lhs_slice]], [[rhs_slice]])\n+)\";\n+  EXPECT_THAT(RunFileCheck(test_module->ToString(), expected),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "1286045abe505a5b41be1329c02855cf1f6e5630",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fusion_compiler.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"xla/service/gpu/cudnn_support_utils.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n+#include \"xla/service/gpu/transforms/block_scaling_rewriter.h\"\n #include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/matmul_indexing_utils.h\"\n #include \"xla/shape_util.h\"\n@@ -533,6 +534,9 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n   VLOG(5) << fusion.ToString();\n   VLOG(5) << computation.ToString();\n   graph::Graph graph;\n+  // Intermediate data type is needed for `block_scale_dequantize` graph nodes.\n+  graph.set_intermediate_data_type(cudnn_frontend::DataType_t::FLOAT);\n+\n   std::vector<HloInstruction*> instructions =\n       computation.MakeInstructionPostOrder();\n   absl::flat_hash_map<const HloInstruction*,\n@@ -733,30 +737,25 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n       if (!compute_dtype.has_value()) {\n         return std::nullopt;\n       }\n-      const auto& dimension_numbers = hlo->dot_dimension_numbers();\n       std::array<std::shared_ptr<graph::Tensor_attributes>, 2> dot_operands;\n       for (int i = 0; i < 2; ++i) {\n-        const Shape& input_shape = hlo->operand(i)->shape();\n         const Shape& scale_shape = hlo->operand(i + 2)->shape();\n-        int dim = i == 0 ? dimension_numbers.lhs_contracting_dimensions(0)\n-                         : dimension_numbers.rhs_contracting_dimensions(0);\n-        int block_size =\n-            input_shape.dimensions(dim) / scale_shape.dimensions(dim);\n-\n+        int block_size = scale_shape.element_type() == F8E8M0FNU\n+                             ? BlockScalingRewriter::kBlockSizeMXFP8\n+                             : BlockScalingRewriter::kBlockSizeNVFP4;\n         auto scale = operand(i + 2);\n         scale->set_reordering_type(fe::TensorReordering_t::F8_128x4);\n         auto dq_attrs = graph::Block_scale_dequantize_attributes()\n                             .set_block_size(block_size)\n-                            .set_compute_data_type(fe::DataType_t::FLOAT);\n+                            .set_compute_data_type(*compute_dtype);\n         dot_operands[i] =\n             graph.block_scale_dequantize(operand(i), scale, dq_attrs);\n         dot_operands[i]->set_name(\n             absl::StrCat(hlo->name(), i == 0 ? \"_lhs\" : \"_rhs\", \"_dq\"));\n       }\n-      hlo_to_cudnn[hlo] =\n-          graph.matmul(dot_operands[0], dot_operands[1],\n-                       graph::Matmul_attributes().set_compute_data_type(\n-                           compute_dtype.value()));\n+      hlo_to_cudnn[hlo] = graph.matmul(\n+          dot_operands[0], dot_operands[1],\n+          graph::Matmul_attributes().set_compute_data_type(*compute_dtype));\n     } else if (HloPredicateIsOp<HloOpcode::kConvolution>(hlo)) {\n       // translate conv windows to cudnn conv attr\n       const Window& window = DynCast<HloConvolutionInstruction>(hlo)->window();"
        },
        {
            "sha": "04bdead284265555bc5efb4f00e2f2ae3f6e785a",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=7fd6f3e6aa2836d6cfda6ea8c891fc6ac1fcf960",
            "patch": "@@ -4630,7 +4630,7 @@ absl::StatusOr<CudnnGraph> GetCudnnBlockScaledDotOperationGraph(\n                         desc.GetPhysicalDimensionsMajorToMinor());\n     std::vector<int64_t> strides = desc.GetPhysicalStridesMajorToMinor();\n     if (dimensions.size() == 2) {\n-      dimensions.insert(dimensions.begin(), 1);\n+      dimensions.insert(dimensions.begin(), 1);  // Batch dimension is implicit.\n       strides.insert(strides.begin(), dimensions[1] * dimensions[2]);\n     }\n     CHECK_EQ(dimensions.size(), 3);\n@@ -4670,7 +4670,7 @@ absl::StatusOr<CudnnGraph> GetCudnnBlockScaledDotOperationGraph(\n     d_tensor->set_uid(next_uid());\n     d_tensor->set_is_virtual(false);\n   } else {\n-    std::vector<int64_t> scalar(lhs_data.ndims(), 1);\n+    std::vector<int64_t> scalar(3, 1);  // Batch dimension is implicit.\n     auto scale_attr = Tensor_attributes()\n                           .set_uid(next_uid())\n                           .set_dim(scalar)"
        }
    ],
    "stats": {
        "total": 629,
        "additions": 548,
        "deletions": 81
    }
}