{
    "author": "tensorflower-gardener",
    "message": "[TFRT]Fix scope of batcher queue options initialization.\n\nThe logic for setting `max_execution_batch_size` and `allowed_batch_sizes` was incorrectly nested within the scope of `enable_large_batch_splitting` is true. In this change, we\n\n- Set `max_execution_batch_size` and `allowed_batch_sizes` unconditionally.\n- Add unit tests in BatchResourceBaseWithPriorityTest\n    - The previous code would lead to a different batch scheduling for the test case `low_priority_padding_with_next_allowed_batch_size_disable_splitting` when enable_large_batch_splitting is false.\n    - With previous code, when splitting is disabled, the allowed_batch_sizes is not set, so GetLowPriorityTasksForPadding will return an empty queue and no low priority tasks are able to pad. In this case, the batching result would be like:\n        - batch_1 has 3 high priority tasks with total size 9 and padded to size 12\n        - batch_2 has 3 low priority tasks with total size 9 and padded to size 12.\n- Add unit tests in BatchResourceBaseMaxExecutionBatchSizeTest\n    - Tests different batch scheduling behavior when splitting is enabled/disabled and allowed_batch_sizes is empty/non-empty.\n\nPiperOrigin-RevId: 823267289",
    "sha": "c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e",
    "files": [
        {
            "sha": "cdd9af962f346e964a7e96afddcf074ae1c778ed",
            "filename": "tensorflow/core/kernels/batching_util/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2FBUILD?ref=c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e",
            "patch": "@@ -531,6 +531,7 @@ tf_cc_test(\n         \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:refcount\",\n         \"@local_tsl//tsl/platform:status\",\n+        \"@local_xla//xla/tsl/lib/core:status_test_util\",\n         \"@local_xla//xla/tsl/lib/monitoring:cell_reader\",\n         \"@local_xla//xla/tsl/lib/monitoring:test_utils\",\n         \"@local_xla//xla/tsl/platform:criticality\","
        },
        {
            "sha": "3c131832e12ced9ac6451ea404d13f97829d6f59",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc?ref=c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e",
            "patch": "@@ -608,18 +608,17 @@ BatchResourceBase::GetBatcherQueueOptions(\n       return SplitInputTask(input_task, open_batch_remaining_slot,\n                             max_batch_size, output_tasks);\n     };\n-\n-    if (allowed_batch_sizes.empty()) {\n-      batcher_queue_options.max_execution_batch_size = max_batch_size;\n-      batcher_queue_options.high_priority_queue_options\n-          .max_execution_batch_size = max_batch_size;\n-    } else {\n-      batcher_queue_options.max_execution_batch_size =\n-          *allowed_batch_sizes.rbegin();\n-      batcher_queue_options.high_priority_queue_options\n-          .max_execution_batch_size = *allowed_batch_sizes.rbegin();\n-      batcher_queue_options.allowed_batch_sizes = allowed_batch_sizes;\n-    }\n+  }\n+  if (allowed_batch_sizes.empty()) {\n+    batcher_queue_options.max_execution_batch_size = max_batch_size;\n+    batcher_queue_options.high_priority_queue_options.max_execution_batch_size =\n+        max_batch_size;\n+  } else {\n+    batcher_queue_options.max_execution_batch_size =\n+        *allowed_batch_sizes.rbegin();\n+    batcher_queue_options.high_priority_queue_options.max_execution_batch_size =\n+        *allowed_batch_sizes.rbegin();\n+    batcher_queue_options.allowed_batch_sizes = allowed_batch_sizes;\n   }\n   batcher_queue_options.disable_padding = disable_padding;\n "
        },
        {
            "sha": "0a46a96020288e2f9abb2a3ec79ca94c19e169d8",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base_test.cc",
            "status": "modified",
            "additions": 245,
            "deletions": 11,
            "changes": 256,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc?ref=c3c6653a985a5fd7594f7d1ad8c4d4e26980a29e",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"absl/time/clock.h\"\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/lib/monitoring/cell_reader.h\"\n #include \"xla/tsl/lib/monitoring/test_utils.h\"\n #include \"xla/tsl/platform/criticality.h\"\n@@ -76,20 +77,19 @@ TEST(BatchTaskCriticalityTest, CriticalityDefaultsToCritical) {\n \n struct PriorityTestParams {\n   std::string test_name;\n+  bool enable_large_batch_splitting;\n   MixedPriorityBatchingPolicy mixed_priority_batching_policy;\n   // The expected number of batches for each allowed batch size.\n   absl::flat_hash_map<int, int> expected_batch_size_count;\n   // The expected sum of padding sizes for each allowed batch size.\n   absl::flat_hash_map<int, int> expected_batch_size_padding_sum;\n };\n \n-class TestPriorityBatchResourceBase : public BatchResourceBase {\n+class TestBatchResourceBase : public BatchResourceBase {\n  public:\n   using BatchResourceBase::BatchResourceBase;\n \n-  std::string DebugString() const override {\n-    return \"TestPriorityBatchResourceBase\";\n-  }\n+  std::string DebugString() const override { return \"TestBatchResourceBase\"; }\n \n  protected:\n   // Simple function that returns the input tensors as the output tensors.\n@@ -223,7 +223,7 @@ TEST(BatchTaskCriticalityTest, CriticalitySuccessfullyPropagated) {\n \n TEST_P(BatchResourceBaseWithPriorityTest, BatchingWithMixedPriorityPolicy) {\n   std::shared_ptr<SharedBatchScheduler<BatchResourceBase::BatchTask>> batcher;\n-  ASSERT_OK(SharedBatchScheduler<BatchResourceBase::BatchTask>::Create(\n+  TF_ASSERT_OK(SharedBatchScheduler<BatchResourceBase::BatchTask>::Create(\n       SharedBatchScheduler<BatchResourceBase::BatchTask>::Options(), &batcher));\n   std::vector<int32_t> allowed_batch_sizes = {4, 8, 12, 16};\n   int max_batch_size = 16;\n@@ -233,11 +233,12 @@ TEST_P(BatchResourceBaseWithPriorityTest, BatchingWithMixedPriorityPolicy) {\n   // so the low priority tasks can be padded to the high priority batch instead\n   // of forming a separate batch.\n   BatchResourceBase::BatcherT::QueueOptions queue_options =\n-      TestPriorityBatchResourceBase::GetBatcherQueueOptions(\n+      TestBatchResourceBase::GetBatcherQueueOptions(\n           /*num_batch_threads=*/num_requests, /*max_batch_size=*/max_batch_size,\n           /*batch_timeout_micros=*/batch_timeout,\n           /*max_enqueued_batches=*/num_requests, allowed_batch_sizes,\n-          /*enable_large_batch_splitting=*/true,\n+          /*enable_large_batch_splitting=*/\n+          GetParam().enable_large_batch_splitting,\n           /*disable_padding=*/false, kPadUpPolicy,\n           /*low_priority_max_batch_size=*/max_batch_size,\n           /*low_priority_batch_timeout_micros=*/batch_timeout * 3,\n@@ -246,8 +247,8 @@ TEST_P(BatchResourceBaseWithPriorityTest, BatchingWithMixedPriorityPolicy) {\n           /*mixed_priority_batching_policy=*/\n           GetParam().mixed_priority_batching_policy);\n   tsl::core::RefCountPtr<BatchResourceBase> batch_resource(\n-      new TestPriorityBatchResourceBase(true, batcher, queue_options,\n-                                        allowed_batch_sizes));\n+      new TestBatchResourceBase(true, batcher, queue_options,\n+                                allowed_batch_sizes));\n \n   std::vector<std::unique_ptr<OpKernelContext>> contexts;\n   for (int i = 0; i < num_requests; ++i) {\n@@ -270,7 +271,7 @@ TEST_P(BatchResourceBaseWithPriorityTest, BatchingWithMixedPriorityPolicy) {\n       return batch_task;\n     };\n     auto done_callback = [&]() { blocking_counter.DecrementCount(); };\n-    ASSERT_OK(batch_resource->RegisterInput(\n+    TF_ASSERT_OK(batch_resource->RegisterInput(\n         /*guid=*/i, contexts[i].get(),\n         /*batcher_queue_name=*/\"batcher_queue_name\",\n         /*create_batch_task_fn=*/create_batch_task_fn,\n@@ -306,6 +307,7 @@ INSTANTIATE_TEST_SUITE_P(\n         // has 3 tasks and total size is 12. Each batch has 3 paddings.\n         {\n             \"priority_isolation\",\n+            /*enable_large_batch_splitting=*/true,\n             MixedPriorityBatchingPolicy::kPriorityIsolation,\n             /*expected_batch_size_count=*/\n             {{4, 0}, {8, 0}, {12, 2}, {16, 0}},\n@@ -319,13 +321,28 @@ INSTANTIATE_TEST_SUITE_P(\n         // has 6 tasks and total size is 16. No padding for the first batch. The\n         // second batch has 1 task of size 2 and is padded to size 4.\n         {\n-            \"priority_merge\",\n+            \"priority_merge_enable_splitting\",\n+            /*enable_large_batch_splitting=*/true,\n             MixedPriorityBatchingPolicy::kPriorityMerge,\n             /*expected_batch_size_count=*/\n             {{4, 1}, {8, 0}, {12, 0}, {16, 1}},\n             /*expected_batch_size_padding_sum=*/\n             {{4, 2}, {8, 0}, {12, 0}, {16, 0}},\n         },\n+        // With priority_merge policy, high priority tasks and low priority\n+        // tasks are batched together. Since splitting is disabled, there are 2\n+        // batches. First batch has 5 tasks, total size is 15 and is padded to\n+        // size 16. The second batch has 1 low priority task of size 3 and is\n+        // padded to size 4.\n+        {\n+            \"priority_merge_disable_splitting\",\n+            /*enable_large_batch_splitting=*/false,\n+            MixedPriorityBatchingPolicy::kPriorityMerge,\n+            /*expected_batch_size_count=*/\n+            {{4, 1}, {8, 0}, {12, 0}, {16, 1}},\n+            /*expected_batch_size_padding_sum=*/\n+            {{4, 1}, {8, 0}, {12, 0}, {16, 1}},\n+        },\n         // With padding_with_max_batch_size policy, high priority tasks and low\n         // priority tasks are batched to the max batch size and there is no\n         // splitting for low priority tasks. 3 high priority tasks and 2 low\n@@ -334,6 +351,7 @@ INSTANTIATE_TEST_SUITE_P(\n         // task of size 3 and is padded to size 4.\n         {\n             \"padding_with_max_batch_size\",\n+            /*enable_large_batch_splitting=*/true,\n             MixedPriorityBatchingPolicy::kLowPriorityPaddingWithMaxBatchSize,\n             /*expected_batch_size_count=*/\n             {{4, 1}, {8, 0}, {12, 0}, {16, 1}},\n@@ -348,6 +366,19 @@ INSTANTIATE_TEST_SUITE_P(\n         // size 8.\n         {\n             \"low_priority_padding_with_next_allowed_batch_size\",\n+            /*enable_large_batch_splitting=*/true,\n+            MixedPriorityBatchingPolicy::\n+                kLowPriorityPaddingWithNextAllowedBatchSize,\n+            /*expected_batch_size_count=*/\n+            {{4, 0}, {8, 1}, {12, 1}, {16, 0}},\n+            /*expected_batch_size_padding_sum=*/\n+            {{4, 0}, {8, 2}, {12, 0}, {16, 0}},\n+        },\n+        // Same as above but disabled large batch splitting.\n+        {\n+            \"low_priority_padding_with_next_allowed_batch_size_disable_\"\n+            \"splitting\",\n+            /*enable_large_batch_splitting=*/false,\n             MixedPriorityBatchingPolicy::\n                 kLowPriorityPaddingWithNextAllowedBatchSize,\n             /*expected_batch_size_count=*/\n@@ -1006,6 +1037,209 @@ TEST_F(BatchResourceBaseTest, ConfiguredBatchPaddingPolicyMetric) {\n   my_batch_resource->Unref();\n }\n \n+struct MaxExecutionBatchSizeTestParams {\n+  std::string test_name;\n+  bool enable_large_batch_splitting;\n+  int max_batch_size;\n+  std::vector<int> allowed_batch_sizes;\n+  absl::flat_hash_map<int, int> expected_batch_size_and_count;\n+  absl::flat_hash_map<int, int> expected_batch_size_and_padding_sum;\n+};\n+\n+class BatchResourceBaseMaxExecutionBatchSizeTest\n+    : public ::testing::TestWithParam<MaxExecutionBatchSizeTestParams> {\n+ protected:\n+  void SetUp() override {\n+    processed_batch_size_v2_reader_ = std::make_unique<CellReader<int64_t>>(\n+        \"/tensorflow/serving/batching/processed_batch_size_v2\");\n+    padding_size_v2_reader_ = std::make_unique<CellReader<Histogram>>(\n+        \"/tensorflow/serving/batching/padding_size_v2\");\n+    // Create device_.\n+    device_ = DeviceFactory::NewDevice(\"CPU\", SessionOptions{},\n+                                       \"/job:a/replica:0/task:0\");\n+    // Create batch_kernel_node_def.\n+    NodeDefBuilder batch_function_builder(\"my_batch_node\", \"BatchFunction\");\n+    batch_function_builder.Attr(\"max_batch_size\", GetParam().max_batch_size);\n+    batch_function_builder.Attr(\"num_batch_threads\", 6);\n+    batch_function_builder.Attr(\"allowed_batch_sizes\",\n+                                GetParam().allowed_batch_sizes);\n+    batch_function_builder.Attr(\"batch_timeout_micros\", 3000000);\n+    batch_function_builder.Attr(\"max_enqueued_batches\", 6);\n+    batch_function_builder.Attr(\"enable_large_batch_splitting\",\n+                                GetParam().enable_large_batch_splitting);\n+    batch_function_builder.Attr(\"Tin\", {DataType::DT_INT64});\n+    batch_function_builder.Input(std::vector<NodeDefBuilder::NodeOut>{\n+        NodeDefBuilder::NodeOut({\"n1\", 0, DataType::DT_INT64})});\n+    batch_function_builder.Attr(\"Tcaptured\", std::vector<DataType>{});\n+    batch_function_builder.Input(std::vector<NodeDefBuilder::NodeOut>{});\n+    batch_function_builder.Attr(\"Tout\", {DataType::DT_INT64});\n+    NameAttrList f;\n+    f.set_name(\"func_to_batch\");\n+    batch_function_builder.Attr(\"f\", f);\n+    NodeDef batch_kernel_node_def;\n+    CHECK_OK(batch_function_builder.Finalize(&batch_kernel_node_def));\n+\n+    // Create batch_kernel_.\n+    absl::Status op_kernel_creation_status;\n+    batch_kernel_ =\n+        CreateOpKernel(DEVICE_CPU, device_.get(), device_->GetAllocator({}),\n+                       batch_kernel_node_def, TF_GRAPH_DEF_VERSION,\n+                       &op_kernel_creation_status);\n+    CHECK_OK(op_kernel_creation_status);\n+    CHECK(batch_kernel_ != nullptr);\n+\n+    // Create input tensors.\n+    input_tensor_ = Tensor(DataType::DT_INT64, TensorShape({1, 4}));\n+    input_tensor_.flat<int64_t>().setZero();\n+    input_tensor_values_ = {\n+        TensorValue(&input_tensor_),\n+    };\n+\n+    // Fill-in session_metadata_.\n+    session_metadata_.set_name(\"my_model_name\");\n+\n+    // Fill-in params_.\n+    params_.device = device_.get();\n+    params_.op_kernel = batch_kernel_.get();\n+    params_.inputs = input_tensor_values_;\n+    params_.session_metadata = &session_metadata_;\n+\n+    // Create context_.\n+    context_ = std::make_unique<OpKernelContext>(&params_);\n+  }\n+\n+  std::unique_ptr<CellReader<int64_t>> processed_batch_size_v2_reader_;\n+  std::unique_ptr<CellReader<Histogram>> padding_size_v2_reader_;\n+  std::unique_ptr<Device> device_;\n+  std::unique_ptr<OpKernel> batch_kernel_;\n+  Tensor input_tensor_;\n+  std::vector<TensorValue> input_tensor_values_;\n+  SessionMetadata session_metadata_;\n+  OpKernelContext::Params params_;\n+  std::unique_ptr<OpKernelContext> context_;\n+};\n+\n+TEST_P(BatchResourceBaseMaxExecutionBatchSizeTest,\n+       MaxExecutionBatchSizeIsRespected) {\n+  std::shared_ptr<SharedBatchScheduler<BatchResourceBase::BatchTask>> batcher;\n+  TF_ASSERT_OK(SharedBatchScheduler<BatchResourceBase::BatchTask>::Create(\n+      SharedBatchScheduler<BatchResourceBase::BatchTask>::Options(), &batcher));\n+  int64_t batch_timeout = absl::ToInt64Microseconds(absl::Seconds(3));\n+  int num_requests = 10;\n+  BatchResourceBase::BatcherT::QueueOptions queue_options =\n+      TestBatchResourceBase::GetBatcherQueueOptions(\n+          /*num_batch_threads=*/num_requests,\n+          /*max_batch_size=*/GetParam().max_batch_size,\n+          /*batch_timeout_micros=*/batch_timeout,\n+          /*max_enqueued_batches=*/num_requests, GetParam().allowed_batch_sizes,\n+          /*enable_large_batch_splitting=*/\n+          GetParam().enable_large_batch_splitting,\n+          /*disable_padding=*/false);\n+  tsl::core::RefCountPtr<BatchResourceBase> batch_resource(\n+      new TestBatchResourceBase(true, batcher, queue_options,\n+                                GetParam().allowed_batch_sizes));\n+\n+  std::vector<std::unique_ptr<OpKernelContext>> contexts;\n+  for (int i = 0; i < num_requests; ++i) {\n+    contexts.push_back(std::make_unique<OpKernelContext>(&params_));\n+  }\n+\n+  absl::BlockingCounter blocking_counter(num_requests);\n+  for (int i = 0; i < num_requests; ++i) {\n+    auto create_batch_task_fn = [&]() {\n+      return std::make_unique<BatchResourceBase::BatchTask>();\n+    };\n+    auto done_callback = [&]() { blocking_counter.DecrementCount(); };\n+    TF_ASSERT_OK(batch_resource->RegisterInput(\n+        /*guid=*/i, contexts[i].get(),\n+        /*batcher_queue_name=*/\"batcher_queue_name\",\n+        /*create_batch_task_fn=*/create_batch_task_fn,\n+        /*done_callback=*/done_callback,\n+        /*forced_warmup_batch_size=*/0));\n+  }\n+  blocking_counter.Wait();\n+\n+  for (const auto& [batch_size, expected_count] :\n+       GetParam().expected_batch_size_and_count) {\n+    EXPECT_EQ(processed_batch_size_v2_reader_->Delta(\n+                  \"my_model_name\", \"my_batch_node\", absl::StrCat(batch_size)),\n+              expected_count);\n+  }\n+  for (const auto& [batch_size, expected_padding_sum] :\n+       GetParam().expected_batch_size_and_padding_sum) {\n+    EXPECT_EQ(\n+        padding_size_v2_reader_\n+            ->Delta(\"my_model_name\", absl::StrCat(batch_size), \"my_batch_node\")\n+            .sum(),\n+        expected_padding_sum);\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    BatchResourceBaseMaxExecutionBatchSizeTests,\n+    BatchResourceBaseMaxExecutionBatchSizeTest,\n+    testing::ValuesIn<MaxExecutionBatchSizeTestParams>({\n+        // There are 10 requests and each request has task size 1. When batch\n+        // splitting is enabled and allowed_batch_sizes is empty, the\n+        // max_execution_batch_size is assigned by the max_batch_size 16. Since\n+        // allowed_batch_sizes is empty, any batch size <= 16 is allowed.\n+        // Therefore an input batch of size 10 is processed directly with no\n+        // padding.\n+        {\n+            \"batch_splitting_enabled_and_allowed_batch_sizes_empty\",\n+            /*enable_large_batch_splitting=*/true,\n+            /*max_batch_size=*/16,\n+            /*allowed_batch_sizes=*/{},\n+            /*expected_batch_size_and_count=*/{{10, 1}},\n+            /*expected_batch_size_and_padding_sum=*/{{10, 0}},\n+        },\n+        // Same requests as above. With batch splitting disabled,\n+        // max_execution_batch_size is set by input_batch_size_limit, which\n+        // inherits its value from max_batch_size (16). Since\n+        // allowed_batch_sizes is empty, any batch size <= 16 is permitted.\n+        // Therefore, an input batch of size 10 is processed directly with no\n+        // padding.\n+        {\n+            \"batch_splitting_disabled_and_allowed_batch_sizes_empty\",\n+            /*enable_large_batch_splitting=*/false,\n+            /*max_batch_size=*/16,\n+            /*allowed_batch_sizes=*/{},\n+            /*expected_batch_size_and_count=*/{{10, 1}},\n+            /*expected_batch_size_and_padding_sum=*/{{10, 0}},\n+        },\n+        // Same requests as above. When batch splitting is enabled and\n+        // allowed_batch_sizes is not empty, the max_execution_batch_size is\n+        // assigned to the largest allowed_batch_size 8. There are two batches.\n+        // The first batch has 8 requests with total size 8, no padding. The\n+        // second batch has 2 requests with total size 2, padding to size 4\n+        // with 2 paddings.\n+        {\n+            \"batch_splitting_enabled_and_allowed_batch_sizes_not_empty\",\n+            /*enable_large_batch_splitting=*/true,\n+            /*max_batch_size=*/16,\n+            /*allowed_batch_sizes=*/{4, 8},\n+            /*expected_batch_size_and_count=*/{{4, 1}, {8, 1}},\n+            /*expected_batch_size_and_padding_sum=*/{{4, 2}, {8, 0}},\n+        },\n+        // Same requests as above. When batch splitting is disabled, the\n+        // max_execution_batch_size is assigned to the max_batch_size 16. Since\n+        // allowed_batch_sizes is not empty and the padding policy is pad up,\n+        // there is one batch of total size 10 which is padded to size 16 with 6\n+        // paddings.\n+        {\n+            \"batch_splitting_disabled_and_allowed_batch_sizes_not_empty\",\n+            /*enable_large_batch_splitting=*/false,\n+            /*max_batch_size=*/16,\n+            /*allowed_batch_sizes=*/{4, 8, 16},\n+            /*expected_batch_size_and_count=*/{{4, 0}, {8, 0}, {16, 1}},\n+            /*expected_batch_size_and_padding_sum=*/{{4, 0}, {8, 0}, {16, 6}},\n+        },\n+    }),\n+    [](const ::testing::TestParamInfo<\n+        BatchResourceBaseMaxExecutionBatchSizeTest::ParamType>& info) {\n+      return info.param.test_name;\n+    });\n+\n }  // namespace\n }  // namespace serving\n }  // namespace tensorflow"
        }
    ],
    "stats": {
        "total": 280,
        "additions": 257,
        "deletions": 23
    }
}