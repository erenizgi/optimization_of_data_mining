{
    "author": "sergachev",
    "message": "PR #34227: [GPU] Remove no longer necessary workaround for cuDNN convolutions.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34227\n\nüìù Summary of Changes\nDo not add unnecessary side inputs to convolution graphs.\n\nüéØ Justification\nThe workaround is no longer necessary with modern cuDNN versions and hinders performance.\n\nüöÄ Kind of Contribution\n‚ö°Ô∏è Performance Improvement, ‚ôªÔ∏è Cleanup\n\nüìä Benchmark (for Performance Improvements)\n\\-\n\nüß™ Unit Tests:\n\\-\n\nüß™ Execution Tests:\n\\-\n\nCopybara import of the project:\n\n--\n4ec6685a8f7cc1a25d9ee8d9645156a0ebb2c594 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Remove no longer necessary workaround for cuDNN convolutions.\n\nMerging this change closes #34227\n\nPiperOrigin-RevId: 836409759",
    "sha": "a76b67446a972396ff8ddc30b5d9ef1ca6f919a2",
    "files": [
        {
            "sha": "d3ccf8a6497a1ffd3f1ede23983ae1a3aef08baa",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a76b67446a972396ff8ddc30b5d9ef1ca6f919a2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a76b67446a972396ff8ddc30b5d9ef1ca6f919a2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=a76b67446a972396ff8ddc30b5d9ef1ca6f919a2",
            "patch": "@@ -3461,13 +3461,12 @@ GetGenericCudnnOperationGraph(\n bool SideInputNeeded(dnn::ActivationMode activation_mode, double conv_scale,\n                      double side_input_scale) {\n   // Cudnn uses precompiled kernels to perform the Conv-Add-BiasAdd-Act when the\n-  // activation is Relu or Identity and this requires the \"side_input\" for the\n+  // activation is Relu and this requires the \"side_input\" for the\n   // Add. For other activations, cudnn uses the runtime-compiled kernels.\n   // However, for this case, we need to drop the Add node and use\n   // Conv-BiasAdd-Act pattern to trigger the correct cudnn path.\n   // TODO(kaixih@nvidia): We should remove this WAR when the cudnn fixes it.\n-  bool check_activation = activation_mode == dnn::ActivationMode::kNone ||\n-                          activation_mode == dnn::ActivationMode::kRelu;\n+  bool check_activation = activation_mode == dnn::ActivationMode::kRelu;\n   bool check_scale = conv_scale != 1.0 || side_input_scale != 0.0;\n   return check_activation || check_scale;\n }"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 2,
        "deletions": 3
    }
}