{
    "author": "KanishAnand",
    "message": "Add `Proto` suffix to proto types to avoid name conflicts with corresponding cpp types as both are under `xla` namespace.\n\n#hloshardingv3\n\nPiperOrigin-RevId: 818005071",
    "sha": "d979afadb7d4c512522b4f2ed224b0c1ec9476eb",
    "files": [
        {
            "sha": "c86fd53d5bfc00a295548a5259960f22214fc89e",
            "filename": "third_party/xla/xla/xla_data.proto",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d979afadb7d4c512522b4f2ed224b0c1ec9476eb/third_party%2Fxla%2Fxla%2Fxla_data.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d979afadb7d4c512522b4f2ed224b0c1ec9476eb/third_party%2Fxla%2Fxla%2Fxla_data.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla_data.proto?ref=d979afadb7d4c512522b4f2ed224b0c1ec9476eb",
            "patch": "@@ -934,7 +934,7 @@ message StatisticsViz {\n //   * All of its elements must be non-negative.\n //   * `device_ids` should not be equal to `iota(product(axis_sizes))`.\n //   * Sorted `device_ids` must be `iota(product(axis_sizes))`.\n-message Mesh {\n+message MeshProto {\n   message MeshAxis {\n     string name = 1;\n     int64 size = 2;\n@@ -950,7 +950,7 @@ message Mesh {\n //\n // Constraints:\n // - `axis_index` is a valid index into `mesh.axes`.\n-message AxisRef {\n+message AxisRefProto {\n   // When splitting a full axis into n sub-axes, the axis is reshaped into\n   // [k_1,...,k_n], and the ith sub-axis can be expressed by the product of\n   // all axis sizes to its left `m=prod(k_1,...,k_(i-1))` (aka pre-size) and\n@@ -985,31 +985,31 @@ message AxisRef {\n // This is corresponding to mlir sharding representation `Sdy_TensorSharding`\n // (https://github.com/openxla/shardy/blob/main/shardy/dialect/sdy/ir/attrs.td)\n // used in Shardy.\n-message NamedSharding {\n+message NamedShardingProto {\n   // Describes how a tensor dimension is sharded using indices into\n   // mesh.axes, from major to minor, and a boolean indicating whether\n   // the dimension can be further sharded.\n   message DimensionSharding {\n     // List of axes to shard a tensor dimension on from major to minor. The\n     // dimension should have at least one axis if it is closed.\n-    repeated AxisRef axes = 1;\n+    repeated AxisRefProto axes = 1;\n     // If true, this dimension is \"closed\" and can't be further sharded.\n     bool is_closed = 2;\n   }\n \n   // If this is non-empty, then all the other fields are ignored and this\n   // represents a tuple sharding.\n-  repeated NamedSharding tuple_shardings = 1;\n+  repeated NamedShardingProto tuple_shardings = 1;\n \n-  Mesh mesh = 2;\n+  MeshProto mesh = 2;\n   // The dimension shardings tell us for each dimension of the tensor, along\n   // which axes it is sharded from major to minor.\n   repeated DimensionSharding dim_shardings = 3;\n   // All axes in this list are explicitly replicated.\n-  repeated AxisRef replicated_axes = 4;\n+  repeated AxisRefProto replicated_axes = 4;\n   // A sharding can have unreduced axes, meaning the tensor is unreduced\n   // along these axes.\n-  repeated AxisRef unreduced_axes = 5;\n+  repeated AxisRefProto unreduced_axes = 5;\n \n   // This field is used to track the source of this sharding, usually derived\n   // from instructions. Multple metadata may be populated if sharding is\n@@ -1114,7 +1114,7 @@ message OpSharding {\n   // Note that instead of reusing OpSharding's fields like metadata, we have\n   // separate fields in NamedSharding to treat it as a standalone message which\n   // is more clear and will help in future cleanup.\n-  optional NamedSharding named_sharding = 14;\n+  optional NamedShardingProto named_sharding = 14;\n }\n // LINT.ThenChange()\n "
        }
    ],
    "stats": {
        "total": 18,
        "additions": 9,
        "deletions": 9
    }
}