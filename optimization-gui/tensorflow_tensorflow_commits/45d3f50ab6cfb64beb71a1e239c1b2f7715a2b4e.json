{
    "author": "tensorflower-gardener",
    "message": "#sdy Export unreduced as HLOSharding instead of frontend attributes.\n\nPiperOrigin-RevId: 800293168",
    "sha": "45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e",
    "files": [
        {
            "sha": "828fa836f853dec7626d0c57c8d7878067fe33cc",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc?ref=45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e",
            "patch": "@@ -912,6 +912,9 @@ absl::Status HloSharding::ValidateNonTuple(\n   if (proto.type() == OpSharding::MANUAL) {\n     return std::move(Manual(metadata).SetShardGroupFromProto(proto));\n   }\n+  if (proto.type() == OpSharding::UNREDUCED) {\n+    return std::move(Unreduced(metadata).SetShardGroupFromProto(proto));\n+  }\n   if (proto.type() == OpSharding::UNKNOWN) {\n     return std::move(Unknown(metadata).SetShardGroupFromProto(proto));\n   }\n@@ -1040,6 +1043,8 @@ OpSharding HloSharding::ToProto() const {\n     result.set_type(OpSharding::MAXIMAL);\n   } else if (IsManual()) {\n     result.set_type(OpSharding::MANUAL);\n+  } else if (IsUnreduced()) {\n+    result.set_type(OpSharding::UNREDUCED);\n   } else if (IsUnknown()) {\n     result.set_type(OpSharding::UNKNOWN);\n   } else {"
        },
        {
            "sha": "cf35647770efb66d52edfd4564a269ded1878fc7",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/export_shardings.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 6,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc?ref=45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e",
            "patch": "@@ -168,10 +168,6 @@ LogicalResult exportFunc(FuncOp funcOp, const SymbolTable& symbolTable,\n \n     if (ArrayRef<TensorShardingAttr> shardings = mlir::sdy::getShardings(op);\n         !shardings.empty()) {\n-      if (allShardingsUnreduced(shardings)) {\n-        setFrontendAttribute(op, kHasUnreducedAxes,\n-                             builder.getStringAttr(\"true\"));\n-      }\n       setHloShardingAttr(op, shardings, getMeshAttr, manualAxes);\n       op->removeAttr(kShardingAttr);\n     } else if (addMissingShardingToControlFlow &&\n@@ -312,6 +308,11 @@ HloSharding convertToHloSharding(\n   if (mesh.getAxes().size() == manualAxes.size()) {\n     return HloSharding::Manual();\n   }\n+  // TODO(b/438306205): Remove this check once we support both unreduced and\n+  // manual axes in subgroup sharding.\n+  CHECK(sdySharding.getUnreducedAxes().empty() || manualAxes.empty())\n+      << \"Only one of unreduced and manual axes can be present: \"\n+      << mlir::sdy::attributeToString(sdySharding);\n \n   // Iterate the dim shardings.\n   for (auto [index, dimSharding] :\n@@ -333,6 +334,16 @@ HloSharding convertToHloSharding(\n     }\n   }\n \n+  // Iterate the unreduced axes.\n+  if (!sdySharding.getUnreducedAxes().empty()) {\n+    types.push_back(OpSharding::UNREDUCED);\n+    int64_t& unreducedDim = tileAssignmentDims.emplace_back(1);\n+    for (AxisRefAttr unreducedAxis : sdySharding.getUnreducedAxes()) {\n+      unreducedDim *= unreducedAxis.getSize(mesh);\n+      axisRefToShardedPos[unreducedAxis] = shardedPos++;\n+    }\n+  }\n+\n   // We will add all axes and let canonicalization merge adjacent axes.\n   SmallVector<AxisRefAttr> meshAxisRefs = getOrderedAxisRefs(sdySharding, mesh);\n   SmallVector<int64_t> reshapeDims(meshAxisRefs.size());\n@@ -345,11 +356,11 @@ HloSharding convertToHloSharding(\n \n     auto shardedPosIt = axisRefToShardedPos.find(axisRef);\n     if (shardedPosIt == axisRefToShardedPos.end()) {\n-      // Axis is replicated\n+      // Axis is replicated.\n       transposePerm[replicatedPos++] = axisIndex;\n       totalReplicatedSize *= axisRef.getSize(mesh);\n     } else {\n-      // Axis is sharded or manual\n+      // Axis is sharded, manual, or unreduced.\n       transposePerm[shardedPosIt->second] = axisIndex;\n     }\n   }"
        },
        {
            "sha": "3f678553d9ac5c9996f55300e600d980b276d7b2",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/stablehlo_export_pipeline.mlir",
            "status": "modified",
            "additions": 39,
            "deletions": 20,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_pipeline.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_pipeline.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_pipeline.mlir?ref=45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e",
            "patch": "@@ -455,10 +455,9 @@ func.func @all_reduce_input_no_unreduced_axes(%arg0: tensor<8x8xf32> {sdy.shardi\n }\n \n // CHECK-LABEL: func @all_reduce_input_with_unreduced_axes\n-// CHECK-SAME:  (%arg0: tensor<8x8xf32> {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dim_replicate}\"}) -> tensor<8x8xf32> {\n-// CHECK-NOT:   mhlo.frontend_attributes\n+// CHECK-SAME:  (%arg0: tensor<8x8xf32> {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"}) -> tensor<8x8xf32> {\n func.func @all_reduce_input_with_unreduced_axes(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{\"j\"}, {}], unreduced={\"i\"}>}) -> tensor<8x8xf32> {\n-  // CHECK-NEXT: %[[COPY_0:.*]] = mhlo.copy %arg0 {mhlo.frontend_attributes = {xla.sdy.has_unreduced_axes = \"true\"}, mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dim_replicate}\"\n+  // CHECK-NEXT: %[[COPY_0:.*]] = mhlo.copy %arg0 {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"}\n   // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_0]]) {mhlo.sharding = \"{manual}\"}\n   // CHECK-NEXT: %[[ALL_REDUCE:.*]] = \"stablehlo.all_reduce\"(%1) <{\n   // CHECK-SAME:   channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>,\n@@ -475,35 +474,36 @@ func.func @all_reduce_input_with_unreduced_axes(%arg0: tensor<8x8xf32> {sdy.shar\n }\n \n //===----------------------------------------------------------------------===//\n-// Unreduced frontend attribute tests\n+// Unreduced sharding tests\n //===----------------------------------------------------------------------===//\n \n // CHECK-LABEL: func @unreduced_func_input\n-func.func @unreduced_func_input(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{}, {}], unreduced={\"j\"}>}, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32> {\n+// CHECK-SAME:  (%arg0: tensor<4x8xf32> {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={unreduced}}\"}, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32> {\n+func.func @unreduced_func_input(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{\"i\"}, {}], unreduced={\"j\"}>}, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32> {\n   // CHECK-NEXT: %[[MUL:.*]] = stablehlo.multiply %arg0, %arg1\n-  // CHECK-NOT:  mhlo.frontend_attributes\n+  // CHECK-NOT:  mhlo.sharding\n   // CHECK-NEXT: return %[[MUL]]\n   %0 = stablehlo.multiply %arg0, %arg1 : tensor<4x8xf32>\n   return %0 : tensor<4x8xf32>\n }\n \n // CHECK-LABEL: func @unreduced_op\n func.func @unreduced_op(%arg0: tensor<4x64x16xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{}, {\"i\", \"j\"}, {}]>}) -> tensor<4x16xf32> {\n-  // CHECK:      %[[REDUCE:.*]] = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1] {mhlo.frontend_attributes = {xla.sdy.has_unreduced_axes = \"true\"}\n-  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[REDUCE]], %[[REDUCE]] {mhlo.frontend_attributes = {xla.sdy.has_unreduced_axes = \"true\"}\n+  // CHECK:      %[[REDUCE:.*]] = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1] {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"}\n+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[REDUCE]], %[[REDUCE]] {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"}\n   %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n-  %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\"}>]>} : (tensor<4x64x16xf32>, tensor<f32>) -> tensor<4x16xf32>\n-  %2 = stablehlo.add %1, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\"}>]>} : tensor<4x16xf32>\n-  %3 = sdy.all_reduce {\"i\"} %2 out_sharding=<@mesh_5, [{}, {}]> : tensor<4x16xf32>\n+  %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{\"j\"}, {}], unreduced={\"i\"}>]>} : (tensor<4x64x16xf32>, tensor<f32>) -> tensor<4x16xf32>\n+  %2 = stablehlo.add %1, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{\"j\"}, {}], unreduced={\"i\"}>]>} : tensor<4x16xf32>\n+  %3 = sdy.all_reduce {\"i\"} %2 out_sharding=<@mesh_5, [{\"j\"}, {}]> : tensor<4x16xf32>\n   return %3 : tensor<4x16xf32>\n }\n \n // CHECK-LABEL: func @no_unreduced_op\n func.func @no_unreduced_op(%arg0: tensor<4x64x16xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{}, {\"i\", \"j\"}, {}]>}) -> tensor<4x16xf32> {\n   // CHECK:      %[[REDUCE:.*]] = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1]\n-  // CHECK-NOT:  mhlo.frontend_attributes\n+  // CHECK-NOT:  mhlo.sharding = \"{unreduced}\"\n   // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[REDUCE]], %[[REDUCE]]\n-  // CHECK-NOT:  mhlo.frontend_attributes\n+  // CHECK-NOT:  mhlo.sharding = \"{unreduced}\"\n   // CHECK-NEXT: return %[[ADD]]\n   %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n   %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}]>]>} : (tensor<4x64x16xf32>, tensor<f32>) -> tensor<4x16xf32>\n@@ -513,22 +513,41 @@ func.func @no_unreduced_op(%arg0: tensor<4x64x16xf32> {sdy.sharding = #sdy.shard\n \n // CHECK-LABEL: func @both_results_unreduced\n func.func @both_results_unreduced(%arg0: tensor<4x64x16xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{}, {\"i\", \"j\"}, {}]>}, %arg1: tensor<4x64x16xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{}, {\"i\", \"j\"}, {}]>}) -> (tensor<4x16xf32>, tensor<4x16xf32>) {\n-  // CHECK:      %[[REDUCE:.*]]:2 = stablehlo.reduce(%arg0 init: %cst), (%arg1 init: %cst) across dimensions = [1] {mhlo.frontend_attributes = {xla.sdy.has_unreduced_axes = \"true\"}\n-  // CHECK:      %[[ADD0:.*]] = stablehlo.add %[[REDUCE]]#0, %[[REDUCE]]#0 {mhlo.frontend_attributes = {xla.sdy.has_unreduced_axes = \"true\"}\n-  // CHECK-NEXT: %[[ADD1:.*]] = stablehlo.add %[[REDUCE]]#1, %[[REDUCE]]#1 {mhlo.frontend_attributes = {xla.sdy.has_unreduced_axes = \"true\"}\n-  // CHECK:      return %[[ADD0]], %[[ADD1]]\n+  // CHECK:      %cst = stablehlo.constant\n+  // CHECK-NEXT: %[[REDUCE:.*]]:2 = stablehlo.reduce(%arg0 init: %cst), (%arg1 init: %cst) across dimensions = [1]\n+  // CHECK-SAME{LITERAL}: {mhlo.sharding = \"{{unreduced}, {unreduced}}\"}\n+  // CHECK-SAME:  : (tensor<4x64x16xf32>, tensor<4x64x16xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x16xf32>, tensor<4x16xf32>)\n+  // CHECK: %[[ADD0:.*]] = stablehlo.add %[[REDUCE]]#0, %[[REDUCE]]#0 {mhlo.sharding = \"{unreduced}\"}\n+  // CHECK-NEXT: %[[ADD1:.*]] = stablehlo.add %[[REDUCE]]#1, %[[REDUCE]]#1 {mhlo.sharding = \"{unreduced}\"}\n+  // CHECK-NEXT: return %[[ADD0]], %[[ADD1]]\n   %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n-  %1:2 = stablehlo.reduce(%arg0 init: %0), (%arg1 init: %0) across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\"}>, <@mesh_5, [{}, {}], unreduced={\"i\"}>]>} : (tensor<4x64x16xf32>, tensor<4x64x16xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x16xf32>, tensor<4x16xf32>)\n+  %1:2 = stablehlo.reduce(%arg0 init: %0), (%arg1 init: %0) across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\", \"j\"}>, <@mesh_5, [{}, {}], unreduced={\"i\", \"j\"}>]>} : (tensor<4x64x16xf32>, tensor<4x64x16xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x16xf32>, tensor<4x16xf32>)\n     reducer(%arg2: tensor<f32>, %arg4: tensor<f32>) (%arg3: tensor<f32>, %arg5: tensor<f32>)  {\n       %2 = stablehlo.add %arg2, %arg4 : tensor<f32>\n       %3 = stablehlo.add %arg3, %arg5 : tensor<f32>\n       stablehlo.return %2, %3 : tensor<f32>, tensor<f32>\n     }\n-  %2 = stablehlo.add %1#0, %1#0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\"}>]>} : tensor<4x16xf32>\n-  %3 = stablehlo.add %1#1, %1#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\"}>]>} : tensor<4x16xf32>\n+  %2 = stablehlo.add %1#0, %1#0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\", \"j\"}>]>} : tensor<4x16xf32>\n+  %3 = stablehlo.add %1#1, %1#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{}, {}], unreduced={\"i\", \"j\"}>]>} : tensor<4x16xf32>\n   return %2, %3 : tensor<4x16xf32>, tensor<4x16xf32>\n }\n \n+func.func @unreduced_sub_axis(%arg0: tensor<4x64x16xf32> {sdy.sharding = #sdy.sharding<@mesh_2, [{}, {\"x\", \"y\"}, {}]>}) -> tensor<4x16xf32> {\n+  %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n+  // CHECK:      %[[REDUCE:.*]] = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1]\n+  // CHECK-SAME{LITERAL}: {mhlo.sharding = \"{devices=[4,1,2,4]<=[8,4]T(1,0) last_tile_dims={unreduced, replicated}}\"}\n+  %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_2, [{\"y\"}, {}], unreduced={\"x\":(1)2}>]>} : (tensor<4x64x16xf32>, tensor<f32>) -> tensor<4x16xf32>\n+  return %1 : tensor<4x16xf32>\n+}\n+\n+func.func @unreduced_canonicalization(%arg0: tensor<4x64x16xf32> {sdy.sharding = #sdy.sharding<@mesh_2, [{}, {\"x\"}, {}]>}) -> tensor<4x16xf32> {\n+  %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n+  // CHECK:      %[[REDUCE:.*]] = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1]\n+  // CHECK-SAME{LITERAL}: {mhlo.sharding = \"{devices=[1,1,8,4]<=[2,4,4]T(0,2,1) last_tile_dims={unreduced, replicated}}\"}\n+  %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_2, [{}, {}], unreduced={\"x\":(1)2, \"y\"}>]>} : (tensor<4x64x16xf32>, tensor<f32>) -> tensor<4x16xf32>\n+  return %1 : tensor<4x16xf32>\n+}\n+\n \n // CHECK-LABEL: func private @foo\n // CHECK-SAME:    %arg0: tensor<4x2xi32> {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"}"
        },
        {
            "sha": "93e6a80f08136b835f48eae0a3d50c8802b1d0a6",
            "filename": "third_party/xla/xla/service/spmd/shardy/utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc?ref=45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e",
            "patch": "@@ -326,6 +326,7 @@ SmallVector<AxisRefAttr> getOrderedAxisRefs(Attribute shardingOrAxisList,\n     for (DimensionShardingAttr dimSharding : sharding.getDimShardings()) {\n       consumeAxisRefList(dimSharding.getAxes());\n     }\n+    consumeAxisRefList(sharding.getUnreducedAxes());\n   } else {\n     consumeAxisRefList(\n         mlir::cast<AxisRefListAttr>(shardingOrAxisList).getValue());"
        },
        {
            "sha": "944153f5f81526bf169bbba8e6ae715cdd36c9d1",
            "filename": "third_party/xla/xla/service/spmd/shardy/utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.h?ref=45d3f50ab6cfb64beb71a1e239c1b2f7715a2b4e",
            "patch": "@@ -147,9 +147,10 @@ std::string duplicateShardingsAtIndices(\n     const llvm::BitVector& indicesToDuplicate);\n \n // Return all axes or sub-axes in the `mesh`, such that sub-axes are derived\n-// from `shardingOrAxisList` and sorted by their order in the mesh. For example,\n-// given mesh <\"x\"=2, \"y\"=16, \"z\"=4> and axis refs [{\"x\"}, {\"y\":2(2)}], we\n-// would return [\"x\", \"y\":1(2), \"y\":2(2), \"y\":4(4), \"z\"].\n+// from `shardingOrAxisList` (including unreduced axes but not replicated)\n+// and sorted by their order in the mesh. For example, given mesh <\"x\"=2,\n+// \"y\"=16, \"z\"=4> and axis refs [{\"x\"}, {\"y\":2(2)}], we would return [\"x\",\n+// \"y\":1(2), \"y\":2(2), \"y\":4(4), \"z\"].\n mlir::SmallVector<mlir::sdy::AxisRefAttr> getOrderedAxisRefs(\n     mlir::Attribute shardingOrAxisList, mlir::sdy::MeshAttr mesh);\n "
        }
    ],
    "stats": {
        "total": 95,
        "additions": 66,
        "deletions": 29
    }
}