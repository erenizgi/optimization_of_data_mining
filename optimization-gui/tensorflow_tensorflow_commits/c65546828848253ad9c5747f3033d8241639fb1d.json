{
    "author": "sfvaroglu",
    "message": "PR #31375: [XLA:GPU] Add NVLink domain check to CollectiveBackendAssigner\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31375\n\nüìù Summary of Changes\nThis PR updates the CollectiveBackendAssigner pass to account for NVLink domain connectivity when deciding between NVSHMEM and DEFAULT backends. It does this by adding a slice_size parameter to the compilation pipeline and introducing an IsIntraNVLinkDomain check.\n\nüéØ Justification\nThe CollectiveBackendAssigner now uses NVSHMEM not only for single-host scenarios, but also when all devices are within the same NVLink domain.\n\nüöÄ Kind of Contribution\n‚ö°Ô∏è Performance Improvement, üß™ Tests\n\nüìä Benchmark (for Performance Improvements)\nH100\n|  | NVSHMEM enabled | NVSHMEM disabled |\n|----------|----------|----------|\n| llama31_8b_fp8_1x8    | 1095330 us   | 1093816 us    |\n| llama31_8b_bf16_2x8    | 1368948 us   | 1370896 us   |\n| llama31_8b_fp8_2x8    | 1096447 us   | 1092437 us   |\n| llama31_70b_fp8_16x8    | 9723821 us   | 9707544 us    |\n\nüß™ Unit Tests:\nAdded unit tests to xla/service/gpu/transforms/collectives/collective_backend_assigner_test.cc\n\nüß™ Execution Tests:\nTested with llama3-8b on 2 GB200 nodes (fsdp = 8). The average step time in NVSHMEM case was 3.69s (vs. 3.76s in the default case).\nCopybara import of the project:\n\n--\na02b77cec9622314af01ae481d0fb28b149f1b45 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd NVLink domain check to CollectiveBackendAssigner\n\nMerging this change closes #31375\n\nPiperOrigin-RevId: 826649437",
    "sha": "c65546828848253ad9c5747f3033d8241639fb1d",
    "files": [
        {
            "sha": "fee384e0dabf464874735cb3fc7bbe110958df67",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -975,7 +975,7 @@ absl::Status RunCollectiveOptimizationPasses(\n \n   if (debug_options.xla_gpu_experimental_enable_nvshmem()) {\n     collectives_pipeline.AddPass<CollectiveBackendAssigner>(\n-        gpu_version, num_visible_devices_per_process);\n+        gpu_version, num_visible_devices_per_process, options.slice_size);\n   }\n \n   if (debug_options.xla_gpu_unsupported_enable_ragged_all_to_all_decomposer()) {"
        },
        {
            "sha": "caab9b91f0e07484c63e4433d51795b31bfea1cc",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -136,6 +136,7 @@ cc_library(\n     srcs = [\"gpu_collective_combiner_utils.cc\"],\n     hdrs = [\"gpu_collective_combiner_utils.h\"],\n     deps = [\n+        \":collective_ops_utils\",\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:collective_ops_utils\","
        },
        {
            "sha": "6cb55d45d1289ab146cf772d8479b32ad9bdc2ee",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_backend_assigner.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.cc?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -92,14 +92,17 @@ absl::StatusOr<bool> CollectiveBackendAssigner::Run(\n           GPUCommunicationType comm_type,\n           GetCommunicationType(instr, num_visible_devices_per_process_,\n                                gpu_version_));\n+      int64_t shape_size = GetShapeSize(instr->shape());\n       VLOG(1) << \"CollectiveBackendAssigner: comm_type=\"\n-              << static_cast<int>(comm_type)\n-              << \" shape_size=\" << GetShapeSize(instr->shape())\n-              << \" threshold_in_bytes_=\" << threshold_in_bytes_;\n-      bool use_nvshmem = (num_visible_devices_per_process_ == 1 ||\n-                          comm_type == GPUCommunicationType::SINGLE_HOST) &&\n-                         (!IsAllReduceOp(instr) ||\n-                          GetShapeSize(instr->shape()) < threshold_in_bytes_);\n+              << static_cast<int>(comm_type) << \" shape_size=\" << shape_size\n+              << \" threshold_in_bytes_=\" << threshold_in_bytes_\n+              << \" slice_size_=\" << slice_size_;\n+      bool use_nvshmem =\n+          (num_visible_devices_per_process_ == 1 ||\n+           comm_type == GPUCommunicationType::SINGLE_HOST ||\n+           (slice_size_ > 0 &&\n+            IsIntraNVLinkDomain(module->config(), slice_size_))) &&\n+          (!IsAllReduceOp(instr) || shape_size < threshold_in_bytes_);\n       if (!use_nvshmem) {\n         continue;\n       }"
        },
        {
            "sha": "a47a00e4cbb1c81868905802770f417571156819",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_backend_assigner.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.h?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -43,11 +43,12 @@ class CollectiveBackendAssigner : public HloModulePass {\n  public:\n   explicit CollectiveBackendAssigner(\n       const se::GpuComputeCapability& gpu_version,\n-      int num_visible_devices_per_process,\n+      int num_visible_devices_per_process, int64_t slice_size = 0,\n       int64_t threshold_in_bytes = kDefaultThresholdInBytes)\n       : gpu_version_(gpu_version),\n         num_visible_devices_per_process_(num_visible_devices_per_process),\n-        threshold_in_bytes_(threshold_in_bytes) {}\n+        threshold_in_bytes_(threshold_in_bytes),\n+        slice_size_(slice_size) {}\n \n   absl::string_view name() const override {\n     return \"collective-backend-assigner\";\n@@ -61,6 +62,7 @@ class CollectiveBackendAssigner : public HloModulePass {\n   se::GpuComputeCapability gpu_version_;\n   int num_visible_devices_per_process_;\n   int64_t threshold_in_bytes_;\n+  int64_t slice_size_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "3a8b20d1d68dcf790cb6fe64a1a757a455904ad2",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_backend_assigner_test.cc",
            "status": "modified",
            "additions": 109,
            "deletions": 8,
            "changes": 117,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner_test.cc?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -38,11 +38,13 @@ using ::tsl::testing::IsOkAndHolds;\n \n class CollectiveBackendAssignerTest : public HloHardwareIndependentTestBase {\n  protected:\n-  absl::StatusOr<bool> RunCollectiveBackendAssigner(HloModule* module) {\n+  absl::StatusOr<bool> RunCollectiveBackendAssigner(HloModule* module,\n+                                                    int num_devices_per_host,\n+                                                    int64_t slice_size = 0) {\n     se::GpuComputeCapability gpu_version = se::CudaComputeCapability(8, 0);\n-    return RunHloPass(\n-        CollectiveBackendAssigner(gpu_version, /*num_devices_per_host=*/1),\n-        module);\n+    return RunHloPass(CollectiveBackendAssigner(\n+                          gpu_version, num_devices_per_host, slice_size),\n+                      module);\n   }\n \n   absl::StatusOr<CollectiveBackendConfig_CollectiveBackend>\n@@ -70,7 +72,9 @@ TEST_F(CollectiveBackendAssignerTest, SmallAllReduceUsesNvshmem) {\n   )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n-  EXPECT_THAT(RunCollectiveBackendAssigner(module.get()),\n+\n+  EXPECT_THAT(RunCollectiveBackendAssigner(\n+                  module.get(), /*num_devices_per_host=*/1, /*slice_size=*/0),\n               absl_testing::IsOkAndHolds(true));\n \n   const HloInstruction* all_reduce =\n@@ -96,7 +100,9 @@ TEST_F(CollectiveBackendAssignerTest, LargeAllReduceUsesDefault) {\n   )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n-  EXPECT_THAT(RunCollectiveBackendAssigner(module.get()),\n+\n+  EXPECT_THAT(RunCollectiveBackendAssigner(\n+                  module.get(), /*num_devices_per_host=*/1, /*slice_size=*/0),\n               absl_testing::IsOkAndHolds(false));\n \n   const HloInstruction* all_reduce =\n@@ -117,7 +123,9 @@ TEST_F(CollectiveBackendAssignerTest, SmallCollectivePermuteUsesNvshmem) {\n   )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n-  EXPECT_THAT(RunCollectiveBackendAssigner(module.get()),\n+\n+  EXPECT_THAT(RunCollectiveBackendAssigner(\n+                  module.get(), /*num_devices_per_host=*/1, /*slice_size=*/0),\n               absl_testing::IsOkAndHolds(true));\n \n   const HloInstruction* permute =\n@@ -138,7 +146,9 @@ TEST_F(CollectiveBackendAssignerTest, LargeCollectivePermuteUsesNvshmem) {\n   )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n-  EXPECT_THAT(RunCollectiveBackendAssigner(module.get()),\n+\n+  EXPECT_THAT(RunCollectiveBackendAssigner(\n+                  module.get(), /*num_devices_per_host=*/1, /*slice_size=*/0),\n               absl_testing::IsOkAndHolds(true));\n \n   const HloInstruction* permute =\n@@ -147,6 +157,97 @@ TEST_F(CollectiveBackendAssignerTest, LargeCollectivePermuteUsesNvshmem) {\n               absl_testing::IsOkAndHolds(CollectiveBackendConfig::NVSHMEM));\n }\n \n+TEST_F(CollectiveBackendAssignerTest, IntraNvlinkDomainUsesNvshmem) {\n+  absl::string_view kHloText = R\"(\n+    HloModule m\n+\n+    add {\n+      lhs = f32[] parameter(0)\n+      rhs = f32[] parameter(1)\n+      ROOT add = f32[] add(lhs, rhs)\n+    }\n+\n+    ENTRY main {\n+      p0 = f32[1024,1024] parameter(0)\n+      ROOT result = f32[1024,1024] all-reduce(p0), to_apply=add, replica_groups={{0,1}}, channel_id=5\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n+  module->mutable_config().set_num_partitions(2);\n+  module->mutable_config().set_replica_count(2);\n+\n+  EXPECT_THAT(RunCollectiveBackendAssigner(\n+                  module.get(), /*num_devices_per_host=*/2, /*slice_size=*/4),\n+              absl_testing::IsOkAndHolds(true));\n+\n+  const HloInstruction* all_reduce =\n+      module->entry_computation()->root_instruction();\n+  EXPECT_THAT(GetCollectiveBackendConfig(all_reduce),\n+              absl_testing::IsOkAndHolds(CollectiveBackendConfig::NVSHMEM));\n+}\n+\n+TEST_F(CollectiveBackendAssignerTest,\n+       IntraNvlinkDomainLargeAllReduceUsesDefault) {\n+  absl::string_view kHloText = R\"(\n+    HloModule m\n+\n+    add {\n+      lhs = f32[] parameter(0)\n+      rhs = f32[] parameter(1)\n+      ROOT add = f32[] add(lhs, rhs)\n+    }\n+\n+    ENTRY main {\n+      p0 = f32[8192,8192] parameter(0)\n+      ROOT result = f32[8192,8192] all-reduce(p0), to_apply=add, replica_groups={{0,1}}, channel_id=8\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n+  module->mutable_config().set_num_partitions(2);\n+  module->mutable_config().set_replica_count(2);\n+\n+  EXPECT_THAT(RunCollectiveBackendAssigner(\n+                  module.get(), /*num_devices_per_host=*/2, /*slice_size=*/4),\n+              absl_testing::IsOkAndHolds(false));\n+\n+  const HloInstruction* all_reduce =\n+      module->entry_computation()->root_instruction();\n+  EXPECT_THAT(GetCollectiveBackendConfig(all_reduce),\n+              absl_testing::IsOkAndHolds(CollectiveBackendConfig::DEFAULT));\n+}\n+\n+TEST_F(CollectiveBackendAssignerTest, NonIntraNvlinkDomainUsesDefault) {\n+  absl::string_view kHloText = R\"(\n+    HloModule m\n+\n+    add {\n+      lhs = f32[] parameter(0)\n+      rhs = f32[] parameter(1)\n+      ROOT add = f32[] add(lhs, rhs)\n+    }\n+\n+    ENTRY main {\n+      p0 = f32[1024,1024] parameter(0)\n+      ROOT result = f32[1024,1024] all-reduce(p0), to_apply=add, channel_id=13\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n+  module->mutable_config().set_num_partitions(1);\n+  module->mutable_config().set_replica_count(4);\n+\n+  EXPECT_THAT(RunCollectiveBackendAssigner(\n+                  module.get(), /*num_devices_per_host=*/2, /*slice_size=*/2),\n+              absl_testing::IsOkAndHolds(false));\n+\n+  const HloInstruction* all_reduce =\n+      module->entry_computation()->root_instruction();\n+  EXPECT_THAT(GetCollectiveBackendConfig(all_reduce),\n+              absl_testing::IsOkAndHolds(CollectiveBackendConfig::DEFAULT));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "f4bdf504e8e7aa0d9155b396f550af4eac7dc7a2",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -186,5 +186,13 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n   return GPUCommunicationType::UNDEFINED;\n }\n \n+bool IsIntraNVLinkDomain(const HloModuleConfig& config, int64_t slice_size) {\n+  int device_count = config.num_partitions() * config.replica_count();\n+  bool is_intra = device_count <= slice_size;\n+  VLOG(1) << \"IsIntraNVLinkDomain: device_count=\" << device_count\n+          << \" slice_size=\" << slice_size << \" is_intra=\" << is_intra;\n+  return is_intra;\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "c4ed3803a57c77d5577ed0cac802f4235864f6ef",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {\n@@ -46,6 +47,9 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n // Returns true if instruction is a synchronous collective op.\n bool IsGPUSyncCollective(const HloInstruction& instr);\n \n+// Returns true if all devices are within the same NVLink domain (slice).\n+bool IsIntraNVLinkDomain(const HloModuleConfig& config, int64_t slice_size);\n+\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "52ea29f9a7ac53f6aaf16aa8c7e94acb0f126ced",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/gpu_collective_combiner_utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65546828848253ad9c5747f3033d8241639fb1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.cc?ref=c65546828848253ad9c5747f3033d8241639fb1d",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -76,17 +77,17 @@ bool EnableHeuristicCollectiveCombining(\n   if (!cc.IsAtLeastAmpere()) {\n     return false;\n   }\n-  int hlo_device_count = config.num_partitions() * config.replica_count();\n-  if (hlo_device_count <= nvlink_slice_size) {\n+  if (IsIntraNVLinkDomain(config, nvlink_slice_size)) {\n     VLOG(1) << \"Disabled heuristic collective combining for intra-NVLink \"\n                \"domain communication: HLO device count \"\n-            << hlo_device_count << \" <= NVLink slice size \"\n-            << nvlink_slice_size;\n+            << (config.num_partitions() * config.replica_count())\n+            << \" <= NVLink slice size \" << nvlink_slice_size;\n     return false;\n   }\n   VLOG(1) << \"Enabled heuristic collective combining for inter-NVLink domain \"\n              \"communication: HLO device count \"\n-          << hlo_device_count << \" > NVLink slice size \" << nvlink_slice_size;\n+          << (config.num_partitions() * config.replica_count())\n+          << \" > NVLink slice size \" << nvlink_slice_size;\n   return true;\n }\n "
        }
    ],
    "stats": {
        "total": 166,
        "additions": 143,
        "deletions": 23
    }
}