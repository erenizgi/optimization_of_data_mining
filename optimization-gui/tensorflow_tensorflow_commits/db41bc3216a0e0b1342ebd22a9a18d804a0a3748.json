{
    "author": "loislo",
    "message": "[XLA:GPU] add the test case for the `scaled-dot` HLO.\n\nNow we have the test but still do not have the triton lowering support for it.\n\nPiperOrigin-RevId: 802953434",
    "sha": "db41bc3216a0e0b1342ebd22a9a18d804a0a3748",
    "files": [
        {
            "sha": "42224eabe3074f8ddedb375082ca2f0897a5cf2c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/db41bc3216a0e0b1342ebd22a9a18d804a0a3748/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/db41bc3216a0e0b1342ebd22a9a18d804a0a3748/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=db41bc3216a0e0b1342ebd22a9a18d804a0a3748",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/codegen/triton/support.h\"\n \n+#include <algorithm>\n #include <array>\n #include <cstdint>\n #include <iterator>\n@@ -124,6 +125,10 @@ bool DoesOpSupportType(HloOpcode opcode, PrimitiveType type) {\n       return type == F32 || type == F64;\n     case HloOpcode::kDot:\n       return type != PRED;\n+    case HloOpcode::kScaledDot:\n+      static constexpr std::array types = {F4E2M1FN, F8E4M3FN, F8E5M2, BF16};\n+      return std::any_of(types.begin(), types.end(),\n+                         [&](auto t) { return t == type; });\n     case HloOpcode::kBatchNormInference:\n     case HloOpcode::kBatchNormTraining:\n     case HloOpcode::kBatchNormGrad:\n@@ -2316,6 +2321,39 @@ INSTANTIATE_TEST_SUITE_P(\n         ::testing::ValuesIn(AllDevicesToTest())),\n     DotPrecisionAlgorithmTestName);\n \n+class ScaledDotTest : public TritonSupportTest,\n+                      public ::testing::WithParamInterface<PrimitiveType> {};\n+\n+TEST_P(ScaledDotTest, ScaledDotOperandTypes) {\n+  const std::string kHloTestTemplate = R\"(\n+HloModule ScaledDotOperandTypes\n+\n+ENTRY triton_computation {\n+  lhs = $0[16, 32] parameter(0)\n+  lhs_scale = f8e8m0fnu[16, 1] parameter(1)\n+  rhs = $0[32, 16] parameter(2)\n+  rhs_scale = f8e8m0fnu[1, 16] parameter(3)\n+  ROOT dot = f32[16, 16] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+      lhs_contracting_dims={1},\n+      rhs_contracting_dims={0}\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      TestedInstruction ti,\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, GetParam(),\n+                                     HloOpcode::kScaledDot,\n+                                     /*use_nested_gemm_fusions=*/true));\n+  RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 16},\n+                 se::CudaComputeCapability::Hopper());\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ScaledDotTest, ScaledDotTest,\n+    ::testing::ValuesIn(AllOpSupportedTypes(HloOpcode::kScaledDot)),\n+    [](const ::testing::TestParamInfo<PrimitiveType>& info) {\n+      return primitive_util::LowercasePrimitiveTypeName(info.param);\n+    });\n+\n class FusionKindsTest\n     : public TritonSupportTest,\n       public ::testing::WithParamInterface<"
        }
    ],
    "stats": {
        "total": 38,
        "additions": 38,
        "deletions": 0
    }
}