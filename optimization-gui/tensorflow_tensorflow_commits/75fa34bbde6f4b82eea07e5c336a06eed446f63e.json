{
    "author": "serach24",
    "message": "PR #32231: Support forward conv with dilation and add basic heuristic for differ‚Ä¶\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32231\n\nüìù Summary of Changes\nThe changes enable native support for forward convolutions with window dilation in XLA's GPU backend. Previously, all dilated convolutions were treated as non-canonical and required explicit padding materialization. Now, forward convolutions with window dilation (but not base dilation) are preserved and handled natively by cuDNN, avoiding unnecessary padding overhead.\n\nüéØ Justification\nPerformance Problem: JAX shows 15-23x slower performance than PyTorch for dilated convolutions (33.5ms vs 1.4ms at dilation rate 2). This is because XLA materializes dilated convolutions as padded convolutions instead of using cuDNN's native support.\nSolution: Allow forward convolutions with window dilation to bypass padding materialization and use cuDNN's native dilated convolution kernels directly.\n\nüöÄ Kind of Contribution\nPerformance Improvement\n\nüìä Benchmark (for Performance Improvements)\ndilation 1:\n\tprev: 1.08 ms\n\tnow: 1.07 ms\ndilation 2:\n\tprev: 25.79 ms\n\tnow: 0.91 ms\ndilation 1024:\n\tprev: 26.24 ms\n\tnow: 2.34 ms\n\nCopybara import of the project:\n\n--\nb5a38df2ed4715b43fc8ca8d652005a35290d47e by Chenhao Jiang <chenhaoj@nvidia.com>:\n\nSupport forward conv with dilation and add basic heuristic for differentiating forward/backward\n\nMerging this change closes #32231\n\nPiperOrigin-RevId: 822482265",
    "sha": "75fa34bbde6f4b82eea07e5c336a06eed446f63e",
    "files": [
        {
            "sha": "2ff88bf1ea9ee4b5d4ff6d1fd4cc8225c18fcfd6",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_padding_legalization.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization.cc?ref=75fa34bbde6f4b82eea07e5c336a06eed446f63e",
            "patch": "@@ -52,7 +52,7 @@ bool IsForwardConvolutionCanonical(const HloInstruction& conv) {\n         conv.custom_call_target() == kCudnnConvForwardGraphCallTarget);\n   return window_util::HasSymmetricPadding(conv.window()) &&\n          !window_util::HasNegativePadding(conv.window()) &&\n-         !window_util::HasDilation(conv.window());\n+         !window_util::HasBaseDilation(conv.window());\n }\n \n // If the (positive and negative) padding on the input operand of a convolution\n@@ -139,8 +139,10 @@ HloInstruction* MaybePaddedAndSlicedInput(\n // operand.\n HloInstruction* MaybePaddedKernel(const Window& conv_window,\n                                   const ConvolutionDimensionNumbers& conv_dnums,\n-                                  HloInstruction* kernel) {\n-  if (!window_util::HasWindowDilation(conv_window)) {\n+                                  HloInstruction* kernel,\n+                                  bool preserve_window_dilation = false) {\n+  if (!window_util::HasWindowDilation(conv_window) ||\n+      preserve_window_dilation) {\n     return kernel;\n   }\n \n@@ -172,6 +174,12 @@ bool ConvPaddingLegalization::CanonicalizeForwardConvolution(\n     return false;\n   }\n \n+  bool has_window_dilation = window_util::HasWindowDilation(conv->window());\n+  bool preserve_window_dilation =\n+      has_window_dilation && window_util::HasSymmetricPadding(conv->window()) &&\n+      !window_util::HasNegativePadding(conv->window()) &&\n+      !window_util::HasBaseDilation(conv->window());\n+\n   // Insert slices and/or pads between the convolution and its input and/or\n   // kernel operand.\n   Window new_conv_window = conv->window();\n@@ -180,17 +188,17 @@ bool ConvPaddingLegalization::CanonicalizeForwardConvolution(\n       conv->mutable_operand(0));\n   HloInstruction* new_kernel =\n       MaybePaddedKernel(new_conv_window, conv->convolution_dimension_numbers(),\n-                        conv->mutable_operand(1));\n+                        conv->mutable_operand(1), preserve_window_dilation);\n \n-  // Remove the window dilation from convolution's window field. These paddings\n-  // are made explicit with the pads inserted by MaybePaddedKernel().\n   for (size_t i = 0; i < new_conv_window.dimensions_size(); ++i) {\n     WindowDimension* dim = new_conv_window.mutable_dimensions(i);\n \n     // The size of the kernel may have changed so update the Window to match.\n     dim->set_size(new_kernel->shape().dimensions(\n         conv->convolution_dimension_numbers().kernel_spatial_dimensions(i)));\n-    dim->set_window_dilation(1);\n+    if (!preserve_window_dilation) {\n+      dim->set_window_dilation(1);\n+    }\n   }\n \n   // The conv CustomCall returns a tuple (conv_result, scratch_buffer).  Extract"
        },
        {
            "sha": "89181cbebf01bbacdfa9ec76abaa707cec0da59a",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_padding_legalization_test.cc",
            "status": "modified",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization_test.cc?ref=75fa34bbde6f4b82eea07e5c336a06eed446f63e",
            "patch": "@@ -91,6 +91,74 @@ ENTRY %convolution (operand f64[2,2,2,3]{3,2,1,0}) -> (f64[2,2,4,4]{3,2,1,0}, u8\n   EXPECT_TRUE(ShapeUtil::Equal(conv->shape(), expected_conv_shape));\n }\n \n+TEST_F(ConvPaddingLegalizationTest, ForwardConvolveWithWindowDilation) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule convolution_module\n+ENTRY %convolution (input f32[1,3,5,5]{3,2,1,0}, kernel f32[3,3,3,3]{3,2,1,0}) -> (f32[1,3,5,5]{3,2,1,0}, u8[0]) {\n+  %input = f32[1,3,5,5]{3,2,1,0} parameter(0)\n+  %kernel = f32[3,3,3,3]{3,2,1,0} parameter(1)\n+  ROOT %custom-call = (f32[1,3,5,5]{3,2,1,0}, u8[0]{0}) custom-call(%input, %kernel), window={size=3x3 pad=2_2x2_2 rhs_dilate=2x2}, dim_labels=bf01_01io->bf01, custom_call_target=\"__cudnn$convForward\"\n+}\n+                                               )\")\n+                    .value();\n+  EXPECT_FALSE(ConvPaddingLegalization().Run(module.get()).value());\n+  auto root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              GmockMatch(m::CustomCall({kCudnnConvForwardCallTarget},\n+                                       m::Parameter(0), m::Parameter(1))));\n+  for (int i = 0; i < 2; ++i) {\n+    const WindowDimension& dim = root->window().dimensions(i);\n+    EXPECT_EQ(2, dim.window_dilation());\n+    EXPECT_EQ(3, dim.size());\n+  }\n+}\n+\n+TEST_F(ConvPaddingLegalizationTest,\n+       ForwardConvolveWithWindowDilationAndAsymmetricPadding) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule convolution_module\n+ENTRY %convolution (input f32[1,3,5,5]{3,2,1,0}, kernel f32[3,3,3,3]{3,2,1,0}) -> (f32[1,3,5,5]{3,2,1,0}, u8[0]) {\n+  %input = f32[1,3,5,5]{3,2,1,0} parameter(0)\n+  %kernel = f32[3,3,3,3]{3,2,1,0} parameter(1)\n+  ROOT %custom-call = (f32[1,3,5,5]{3,2,1,0}, u8[0]{0}) custom-call(%input, %kernel), window={size=3x3 pad=1_2x1_2 rhs_dilate=2x2}, dim_labels=bf01_01io->bf01, custom_call_target=\"__cudnn$convForward\"\n+}\n+                                               )\")\n+                    .value();\n+  ASSERT_TRUE(ConvPaddingLegalization().Run(module.get()).value());\n+  auto root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              GmockMatch(m::CustomCall({kCudnnConvForwardCallTarget},\n+                                       m::Pad(m::Parameter(0), m::Op()),\n+                                       m::Pad(m::Parameter(1), m::Op()))));\n+  for (int i = 0; i < 2; ++i) {\n+    const WindowDimension& dim = root->window().dimensions(i);\n+    EXPECT_EQ(1, dim.window_dilation());\n+  }\n+}\n+\n+TEST_F(ConvPaddingLegalizationTest,\n+       ForwardConvolveWithWindowDilationAndBaseDilation) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule convolution_module\n+ENTRY %convolution (input f32[1,3,5,5]{3,2,1,0}, kernel f32[3,3,3,3]{3,2,1,0}) -> (f32[1,3,9,9]{3,2,1,0}, u8[0]) {\n+  %input = f32[1,3,5,5]{3,2,1,0} parameter(0)\n+  %kernel = f32[3,3,3,3]{3,2,1,0} parameter(1)\n+  ROOT %custom-call = (f32[1,3,9,9]{3,2,1,0}, u8[0]{0}) custom-call(%input, %kernel), window={size=3x3 pad=2_2x2_2 rhs_dilate=2x2 lhs_dilate=2x2}, dim_labels=bf01_01io->bf01, custom_call_target=\"__cudnn$convForward\"\n+}\n+                                               )\")\n+                    .value();\n+  ASSERT_TRUE(ConvPaddingLegalization().Run(module.get()).value());\n+  auto root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              GmockMatch(m::CustomCall({kCudnnConvForwardCallTarget},\n+                                       m::Pad(m::Parameter(0), m::Op()),\n+                                       m::Pad(m::Parameter(1), m::Op()))));\n+  for (int i = 0; i < 2; ++i) {\n+    const WindowDimension& dim = root->window().dimensions(i);\n+    EXPECT_EQ(1, dim.window_dilation());\n+  }\n+}\n+\n }  // anonymous namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "d5048180682480108d524ff8890830cb15ad4e88",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_rewriter.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc?ref=75fa34bbde6f4b82eea07e5c336a06eed446f63e",
            "patch": "@@ -138,6 +138,42 @@ bool MaybeConv1dToConv2d(HloInstruction* conv) {\n   return false;\n }\n \n+bool LooksLikeForwardConvolution(const HloInstruction* conv) {\n+  const ConvolutionDimensionNumbers& dnums =\n+      conv->convolution_dimension_numbers();\n+  const Shape& lhs_shape = conv->operand(0)->shape();\n+  const Shape& rhs_shape = conv->operand(1)->shape();\n+  const Shape& result_shape = conv->shape();\n+\n+  // Compare batch and output feature counts. Backward-filter convolutions swap\n+  // these, so matching values are a strong signal that this is a forward\n+  // convolution, even if it has dilation.\n+  int64_t lhs_batches = lhs_shape.dimensions(dnums.input_batch_dimension());\n+  int64_t result_batches =\n+      result_shape.dimensions(dnums.output_batch_dimension());\n+  if (lhs_batches != result_batches) {\n+    return false;\n+  }\n+\n+  int64_t rhs_output_features =\n+      rhs_shape.dimensions(dnums.kernel_output_feature_dimension());\n+  int64_t result_output_features =\n+      result_shape.dimensions(dnums.output_feature_dimension());\n+  if (rhs_output_features != result_output_features) {\n+    return false;\n+  }\n+\n+  for (int i = 0; i < dnums.kernel_spatial_dimensions_size(); ++i) {\n+    int64_t kdim = rhs_shape.dimensions(dnums.kernel_spatial_dimensions(i));\n+    int64_t odim = result_shape.dimensions(dnums.output_spatial_dimensions(i));\n+    if (kdim > odim) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n bool CanImplementAsGpuForwardConv(HloInstruction* conv) {\n   const ConvolutionDimensionNumbers& dnums =\n       conv->convolution_dimension_numbers();\n@@ -191,6 +227,12 @@ ConvolutionMatch MatchBackwardFilter(HloInstruction* conv) {\n   //              Convolution\n   //                 conv\n   CHECK_EQ(HloOpcode::kConvolution, conv->opcode());\n+  if (LooksLikeForwardConvolution(conv)) {\n+    VLOG(1) << \"Convolution \" << conv->ToString()\n+            << \" looks like a forward convolution; skipping backward filter \"\n+               \"rewrite.\";\n+    return std::nullopt;\n+  }\n \n   // Step 2: match paddings and dimension numbers of the forward convolution.\n   const ConvolutionDimensionNumbers& conv_dnums ="
        },
        {
            "sha": "0b1d98628e80f40083bc0b9b33c691660bef309f",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_rewriter_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75fa34bbde6f4b82eea07e5c336a06eed446f63e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter_test.cc?ref=75fa34bbde6f4b82eea07e5c336a06eed446f63e",
            "patch": "@@ -754,6 +754,27 @@ TEST_F(ConvRewriterTest, TestConv1dBackwardInputPatternMatch) {\n                   0)));\n }\n \n+TEST_F(ConvRewriterTest, ForwardConvolutionWithWindowDilation) {\n+  // Forward convolution with window dilation should be preserved and not\n+  // misclassified as backward filter convolution.\n+  const std::string module_str = absl::StrFormat(R\"(\n+    HloModule Test\n+\n+    ENTRY Test {\n+      input = f32[8,128,32,32] parameter(0)\n+      filter = f32[3,3,128,128] parameter(1)\n+      ROOT conv = f32[8,128,32,32] convolution(input, filter), window={size=3x3 pad=2_2x2_2 rhs_dilate=2x2}, dim_labels=bf01_01io->bf01\n+    })\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+\n+  EXPECT_TRUE(RunPass(m.get()));\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::GetTupleElement(\n+                  m::CustomCall({kCudnnConvForwardCallTarget}, m::Parameter(0),\n+                                m::Parameter(1)),\n+                  0)));\n+}\n+\n TEST_F(ConvRewriterTest, TestInvalidTypes) {\n   const std::string module_str = absl::StrFormat(R\"(\n     HloModule Test"
        }
    ],
    "stats": {
        "total": 153,
        "additions": 146,
        "deletions": 7
    }
}