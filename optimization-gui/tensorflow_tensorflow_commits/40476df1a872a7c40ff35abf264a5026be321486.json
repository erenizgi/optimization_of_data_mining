{
    "author": "dsharletg",
    "message": "Enable f32 dots by default in YNNPACK\n\nWe expect this to be a small speedup of f32 dots by wall clock time, but a significant improvement in CPU time (~30%).\n\nThis is the last of the 3 major datatypes for YNNPACK to be enabled (f32, bf16, int8).\n\nPiperOrigin-RevId: 840344698",
    "sha": "40476df1a872a7c40ff35abf264a5026be321486",
    "files": [
        {
            "sha": "4eda97a255f752f37f1697380e37610b825a549b",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/40476df1a872a7c40ff35abf264a5026be321486/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/40476df1a872a7c40ff35abf264a5026be321486/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc?ref=40476df1a872a7c40ff35abf264a5026be321486",
            "patch": "@@ -99,6 +99,10 @@ absl::StatusOr<ynn_binary_operator> YnnBinaryOperator(const HloOpcode& opcode) {\n }\n \n bool IsLayoutSupportedByYnn(const Shape& shape) {\n+  if (shape.dimensions().size() > YNN_MAX_TENSOR_RANK) {\n+    // TODO(b/460602165): We should eliminate this limitation.\n+    return false;\n+  }\n   return !shape.has_layout() || LayoutUtil::HasDescendingLayout(shape.layout());\n }\n \n@@ -154,9 +158,7 @@ absl::StatusOr<bool> IsDotSupportedByYnn(\n   static const absl::NoDestructor<absl::flat_hash_set<\n       std::tuple<PrimitiveType, PrimitiveType, PrimitiveType>>>\n       kAllowedTypes({\n-          // TODO(b/452693819): We plan to enable this in stages, starting with\n-          // int8, and enable f32 later.\n-          // {F32, F32, F32},\n+          {F32, F32, F32},\n           // TODO(b/449998002): We don't have fast fp16 kernels yet.\n           // {F16, F16, F32},\n           {BF16, BF16, F32},"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 5,
        "deletions": 3
    }
}