{
    "author": "ZixuanJiang",
    "message": "Refactor spmd partitioner.\n\nPiperOrigin-RevId: 822689391",
    "sha": "4d53eda2feea6dba76c2e592057300838dd1023f",
    "files": [
        {
            "sha": "0e41129ecf809aa177c35816390b5f25cd90cd37",
            "filename": "third_party/xla/xla/service/spmd/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD?ref=4d53eda2feea6dba76c2e592057300838dd1023f",
            "patch": "@@ -89,7 +89,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@com_google_absl//absl/utility\",\n         \"@local_tsl//tsl/platform:numbers\",\n     ],\n )"
        },
        {
            "sha": "ba8907371471640a19725bad53e79ba8f5a85c9a",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 24,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=4d53eda2feea6dba76c2e592057300838dd1023f",
            "patch": "@@ -2467,10 +2467,9 @@ SpmdPartitioningVisitor::MakePartitioningState() {\n     state.collective_ops_creator = *visiting_collective_ops_creator_;\n     state.partition_id = *visiting_partition_id_;\n     return CreatePerGroupPartitioningState(state, *device_groups_, &b_);\n-  } else {\n-    state.collective_ops_creator = collective_ops_creator_;\n-    state.partition_id = partition_id_;\n   }\n+  state.collective_ops_creator = collective_ops_creator_;\n+  state.partition_id = partition_id_;\n   return state;\n }\n \n@@ -3323,7 +3322,8 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n           output_shard_size * split_factor);\n       return operand.state().b->AddInstruction(HloInstruction::CreateReshape(\n           output_shard_shape, reshard_operand->sharded_input));\n-    } else if (output_dim_size % input_dim_size == 0) {\n+    }\n+    if (output_dim_size % input_dim_size == 0) {\n       // Merge dims.\n       int64_t merge_factor = output_dim_size / input_dim_size;\n       // First reshape locally. (The sharded dimension could include padded\n@@ -5054,22 +5054,20 @@ SPMDCollectiveOpsCreator GetDefaultCollectiveOpsCreator(int64_t num_partitions,\n           // If the src/dst pairs are empty, then the collective permute\n           // just initializes the output to zero.\n           return CreateZero(operand->shape(), b);\n-        } else {\n-          // A collective-permute is a copy if all pairs are \"identity\" and\n-          // all partitions are listed.\n-          bool is_copy =\n-              src_dst_pairs.size() == num_partitions &&\n-              absl::c_all_of(src_dst_pairs,\n-                             [](const std::pair<int64_t, int64_t>& pair) {\n-                               return pair.first == pair.second;\n-                             });\n-          if (is_copy) {\n-            return operand;\n-          } else {\n-            return b->AddInstruction(HloInstruction::CreateCollectivePermute(\n-                operand->shape(), operand, src_dst_pairs, channel_id));\n-          }\n         }\n+        // A collective-permute is a copy if all pairs are \"identity\" and\n+        // all partitions are listed.\n+        bool is_copy =\n+            src_dst_pairs.size() == num_partitions &&\n+            absl::c_all_of(src_dst_pairs,\n+                           [](const std::pair<int64_t, int64_t>& pair) {\n+                             return pair.first == pair.second;\n+                           });\n+        if (is_copy) {\n+          return operand;\n+        }\n+        return b->AddInstruction(HloInstruction::CreateCollectivePermute(\n+            operand->shape(), operand, src_dst_pairs, channel_id));\n       },\n       [create_all_to_all_list_of_lists](\n           SpmdBuilder* b, absl::Span<HloInstruction* const> operands,\n@@ -5309,13 +5307,13 @@ HloInstruction* SpmdPartitioner::AllReduceAlongShardingDimsInternal(\n           .create_cross_partition_all_reduce_with_iota_device_list(\n               b, operand, reduction, partition_group_list.value(),\n               (*next_channel_id)++);\n-    } else {\n-      auto partition_subgroups =\n-          GetPartitionGroupsForReplication(sharding, selected_dims);\n-      return collectives_creator.create_cross_partition_all_reduce(\n-          b, operand, reduction, partition_subgroups, (*next_channel_id)++);\n     }\n+    auto partition_subgroups =\n+        GetPartitionGroupsForReplication(sharding, selected_dims);\n+    return collectives_creator.create_cross_partition_all_reduce(\n+        b, operand, reduction, partition_subgroups, (*next_channel_id)++);\n   }\n+\n   auto result = operand;\n   for (auto it = selected_dims.rbegin(); it != selected_dims.rend(); ++it) {\n     if (sharding.tile_assignment().dim(*it) == 1) {"
        },
        {
            "sha": "ff006de8d320202c569104d9e457dff097e3e540",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=4d53eda2feea6dba76c2e592057300838dd1023f",
            "patch": "@@ -158,8 +158,6 @@ class SpmdBuilder : public HloComputation::Builder {\n     instructions_[hlo];\n   }\n \n-  HloInstruction* visiting_hlo() const { return visiting_hlo_; }\n-\n   // Wrapper of queries to broadcast_dims_.\n   std::optional<const absl::flat_hash_set<int64_t>*> BroadcastDimsForCreatedHlo(\n       const HloInstruction* hlo) {\n@@ -370,7 +368,7 @@ class SpmdPartitioner : public HloModulePass {\n   }\n \n   // Update module's parameter and output sharding information, based on the\n-  // sharding information of the module's parameters and outptuts.\n+  // sharding information of the module's parameters and outputs.\n   static void RecordInputsOutputsSharding(HloModule* module);\n \n   int64_t num_partitions() const { return num_partitions_; }\n@@ -443,7 +441,6 @@ class SpmdPartitioner : public HloModulePass {\n \n   SpmdPartitionerOptions options_;\n   SPMDCollectiveOpsCreator collective_ops_creator_;\n-  std::vector<std::vector<int64_t>> device_groups_;\n   absl::flat_hash_set<absl::string_view> execution_threads_;\n };\n \n@@ -722,6 +719,7 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n \n   absl::Status DefaultAction(HloInstruction* hlo) override;\n \n+  // go/keep-sorted start\n   absl::Status HandleAllReduce(HloInstruction* hlo) override;\n   absl::Status HandleBitcastConvert(HloInstruction* hlo) override;\n   absl::Status HandleBroadcast(HloInstruction* hlo) override;\n@@ -760,6 +758,7 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n   absl::Status HandleTriangularSolve(HloInstruction* hlo) override;\n   absl::Status HandleTuple(HloInstruction* hlo) override;\n   absl::Status HandleWhile(HloInstruction* hlo) override;\n+  // go/keep-sorted end\n \n   // Implementation of dot partitioning given DotGeneralDimsMapping.\n   template <typename CreateShardedFunctor>"
        },
        {
            "sha": "3d081c779215726a11881e988133c3ff7e78ca32",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=4d53eda2feea6dba76c2e592057300838dd1023f",
            "patch": "@@ -2534,10 +2534,9 @@ HloSharding CreateMatchingShardingOnDims(\n   if (to_be_partially_replicated) {\n     return AlignShardingOnDims(HloSharding::PartialTile(tgt_tile_assignment),\n                                target_dims, source_sharding, source_dims);\n-  } else {\n-    return AlignShardingOnDims(HloSharding::Tile(tgt_tile_assignment),\n-                               target_dims, source_sharding, source_dims);\n   }\n+  return AlignShardingOnDims(HloSharding::Tile(tgt_tile_assignment),\n+                             target_dims, source_sharding, source_dims);\n }\n \n std::optional<GatherScatterParallelDimSharding>\n@@ -2909,8 +2908,8 @@ std::vector<std::vector<int64_t>> GetPartitionGroupsAcrossTargetDims(\n       [&](absl::Span<const int64_t> indices, int64_t device) {\n         int64_t group_id = 0;\n         for (int64_t dim = 0; dim < indices.size(); ++dim) {\n-          auto it = absl::c_find(target_dims, dim);\n-          if (it != target_dims.end()) {\n+          if (auto it = absl::c_find(target_dims, dim);\n+              it != target_dims.end()) {\n             int64_t group_size =\n                 group_sizes[std::distance(target_dims.begin(), it)];\n             group_id *= sharding.tile_assignment().dim(dim) / group_size;\n@@ -2963,8 +2962,7 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsAcrossTargetDims(\n   std::vector<int64_t> target_dim_locations;\n   for (int64_t dim = 0; dim < sharding.tile_assignment().num_dimensions();\n        ++dim) {\n-    auto it = std::find(target_dims.begin(), target_dims.end(), dim);\n-    if (it != target_dims.end()) {\n+    if (auto it = absl::c_find(target_dims, dim); it != target_dims.end()) {\n       int64_t current_val = sharding.tile_assignment().dim(dim);\n       int64_t group_size = group_sizes[std::distance(target_dims.begin(), it)];\n       reshape_dimensions.push_back(current_val / group_size);\n@@ -2978,8 +2976,8 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsAcrossTargetDims(\n   std::vector<int> transpose_dims(reshape_dimensions.size());\n   std::iota(transpose_dims.begin(), transpose_dims.end(), 0);\n   for (int64_t loc : target_dim_locations) {\n-    auto it = std::find(transpose_dims.begin(), transpose_dims.end(), loc);\n-    if (it != transpose_dims.end()) {\n+    if (auto it = absl::c_find(transpose_dims, loc);\n+        it != transpose_dims.end()) {\n       transpose_dims.erase(it);\n       transpose_dims.push_back(loc);\n     }\n@@ -3047,8 +3045,7 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsForReplication(\n                                            replication_dims.end());\n   std::sort(replication_dims_sorted.begin(), replication_dims_sorted.end());\n   for (int64_t i : replication_dims_sorted) {\n-    auto it = std::find(transpose_dims.begin(), transpose_dims.end(), i);\n-    if (it != transpose_dims.end()) {\n+    if (auto it = absl::c_find(transpose_dims, i); it != transpose_dims.end()) {\n       transpose_dims.erase(it);\n       transpose_dims.push_back(i);\n     }"
        },
        {
            "sha": "b54e6b87e3981c8192af8ccf255676c108f49613",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.h",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d53eda2feea6dba76c2e592057300838dd1023f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.h?ref=4d53eda2feea6dba76c2e592057300838dd1023f",
            "patch": "@@ -17,45 +17,35 @@ limitations under the License.\n #define XLA_SERVICE_SPMD_SPMD_PARTITIONER_UTIL_H_\n \n #include <algorithm>\n-#include <cstddef>\n #include <cstdint>\n #include <initializer_list>\n #include <limits>\n #include <memory>\n #include <optional>\n #include <string>\n-#include <tuple>\n #include <type_traits>\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_replace.h\"\n #include \"absl/types/span.h\"\n-#include \"absl/utility/utility.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/ir/replica_group.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n-#include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/hlo/utils/hlo_sharding_util.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/spmd/spmd_partitioner.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -822,9 +812,6 @@ template <typename Arg, IsHloModulePointer<Arg> = 0>\n std::decay_t<Arg> FakeHloModule(Arg&& module, HloModule* fake_module) {\n   return fake_module;\n }\n-template <class T>\n-using decay_rvalue_reference_t =\n-    std::conditional_t<std::is_rvalue_reference<T>::value, std::decay_t<T>, T>;\n \n // Modifies SpmdPartitioningVisitor* type objects.\n template <typename Arg, IsSpmdPartitioningVisitorPointer<Arg> = 0>"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 33,
        "deletions": 53
    }
}