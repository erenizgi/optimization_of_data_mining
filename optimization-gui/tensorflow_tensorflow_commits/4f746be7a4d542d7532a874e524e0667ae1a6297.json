{
    "author": "beckerhe",
    "message": "Move CustomKernelThunk into its own file\n\nCustomKernelThunk is currently declared in kernel_thunk.h and this change moves it into its own file custom_kernel_thunk.h. The same is done for the implementation (kernel_thunk.cc to custom_kernel_thunk.cc and the tests).\n\nIt also moves the shared logic (PrintBufferContents function) into its own file and adds tests.\n\nPiperOrigin-RevId: 833204437",
    "sha": "4f746be7a4d542d7532a874e524e0667ae1a6297",
    "files": [
        {
            "sha": "6a4f9c91b0aa6f393a4453b0906c641904bd7549",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -132,6 +132,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:copy_thunk\",\n         \"//xla/backends/gpu/runtime:custom_call_target\",\n         \"//xla/backends/gpu/runtime:custom_call_thunk\",\n+        \"//xla/backends/gpu/runtime:custom_kernel_thunk\",\n         \"//xla/backends/gpu/runtime:dynamic_slice_thunk\",\n         \"//xla/backends/gpu/runtime:gemm_thunk\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\","
        },
        {
            "sha": "7d947e72c9a59375a90d45021f09f24cab2000c4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_target.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\""
        },
        {
            "sha": "e1932a91b5688cae4eb14bc93481daabc94bb229",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 94,
            "deletions": 4,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -193,6 +193,7 @@ cc_library(\n         \":copy_thunk\",\n         \":cudnn_thunk\",\n         \":custom_call_thunk\",\n+        \":custom_kernel_thunk\",\n         \":dynamic_slice_thunk\",\n         \":gemm_thunk\",\n         \":gpublas_lt_matmul_thunk\",\n@@ -962,20 +963,19 @@ cc_library(\n     srcs = [\"kernel_thunk.cc\"],\n     hdrs = [\"kernel_thunk.h\"],\n     deps = [\n+        \":print_buffer_contents\",\n         \":thunk\",\n-        \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n-        \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/service/gpu/kernels:custom_kernel\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -989,7 +989,6 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n@@ -3272,6 +3271,97 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"print_buffer_contents\",\n+    srcs = [\"print_buffer_contents.cc\"],\n+    hdrs = [\"print_buffer_contents.h\"],\n+    deps = [\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel_args\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"print_buffer_contents_test\",\n+    srcs = [\"print_buffer_contents_test.cc\"],\n+    backends = [\"gpu\"],\n+    deps = [\n+        \":print_buffer_contents\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel_args\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"@com_google_absl//absl/log:scoped_mock_log\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"custom_kernel_thunk\",\n+    srcs = [\"custom_kernel_thunk.cc\"],\n+    hdrs = [\"custom_kernel_thunk.h\"],\n+    deps = [\n+        \":print_buffer_contents\",\n+        \":thunk\",\n+        \":thunk_id\",\n+        \"//xla/codegen/emitters:kernel_arguments\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/runtime:buffer_use\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/kernels:custom_kernel\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"custom_kernel_thunk_test\",\n+    srcs = [\"custom_kernel_thunk_test.cc\"],\n+    deps = [\n+        \":custom_kernel_thunk\",\n+        \":thunk\",\n+        \":thunk_id\",\n+        \"//xla:literal\",\n+        \"//xla:shape_util\",\n+        \"//xla/codegen/emitters:kernel_arguments\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/runtime:buffer_use\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu/kernels:custom_kernel\",\n+        \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"buffer_debug_log_entry_metadata_store_test\",\n     srcs = [\"buffer_debug_log_entry_metadata_store_test.cc\"],"
        },
        {
            "sha": "d2e50af0dfb45b8989e96764e3805ae80f9e2d15",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\""
        },
        {
            "sha": "499eed76c2d7045e1c83d91c1c5dcfd4540067d7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk.cc",
            "status": "added",
            "additions": 128,
            "deletions": 0,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -0,0 +1,128 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/print_buffer_contents.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/runtime/buffer_use.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/kernels/custom_kernel.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+\n+CustomKernelThunk::CustomKernelThunk(\n+    const HloInstruction* instr, CustomKernel custom_kernel,\n+    const emitters::KernelArguments& kernel_arguments, ThunkId thunk_id)\n+    : Thunk(Kind::kCustomKernel,\n+            Thunk::ThunkInfo::WithProfileAnnotation(instr, thunk_id)),\n+      args_(kernel_arguments.GetArgumentBufferSlices()),\n+      written_(kernel_arguments.GetArgumentOutputFlags()),\n+      custom_kernel_(std::move(custom_kernel)) {}\n+\n+std::string CustomKernelThunk::ToString(int indent) const {\n+  return custom_kernel_.ToString();\n+}\n+\n+absl::Status CustomKernelThunk::Initialize(const InitializeParams& params) {\n+  absl::MutexLock lock(mutex_);\n+\n+  if (!kernel_cache_.contains(params.executor)) {\n+    TF_ASSIGN_OR_RETURN(\n+        std::unique_ptr<se::Kernel> kernel,\n+        params.executor->LoadKernel(custom_kernel_.kernel_spec()));\n+    kernel_cache_.emplace(params.executor, std::move(kernel));\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+absl::Status CustomKernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n+  se::StreamExecutor* executor = params.stream->parent();\n+\n+  se::Kernel* kernel = [&] {\n+    absl::MutexLock lock(mutex_);\n+    return kernel_cache_[executor].get();\n+  }();\n+\n+  int device_ordinal = executor->device_ordinal();\n+  VLOG(3) << \"[\" << device_ordinal << \"] Launching \"\n+          << custom_kernel_.ToString() << \" as device kernel \"\n+          << kernel->name();\n+\n+  absl::InlinedVector<se::DeviceMemoryBase, 4> buffer_args;\n+  for (const BufferAllocation::Slice& arg : args_) {\n+    se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n+    VLOG(3) << \"[\" << device_ordinal << \"]  Arg: alloc #\" << arg.index()\n+            << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n+            << buf.size() << \"B)\";\n+    buffer_args.push_back(buf);\n+  }\n+\n+  if (VLOG_IS_ON(100)) {\n+    absl::InlinedVector<se::KernelArgument, 4> kernel_args;\n+    for (const se::DeviceMemoryBase& arg : buffer_args) {\n+      kernel_args.push_back(arg);\n+    }\n+    PrintBufferContents(params.stream, kernel_args);\n+  }\n+\n+  se::KernelArgsDeviceMemoryArray args(buffer_args,\n+                                       custom_kernel_.shared_memory_bytes());\n+\n+  return kernel->Launch(custom_kernel_.thread_dims(),\n+                        custom_kernel_.block_dims(),\n+                        custom_kernel_.cluster_dims(), params.stream, args);\n+}\n+\n+Thunk::BufferUses CustomKernelThunk::buffer_uses() const {\n+  Thunk::BufferUses buffers;\n+  buffers.reserve(args_.size());\n+  for (int i = 0; i < args_.size(); ++i) {\n+    // We assume that any buffer is either an input or an output of the\n+    // kernel, and inout buffers are represented as 2 separate arguments.\n+    if (written_[i]) {\n+      buffers.push_back(BufferUse::Write(args_[i]));\n+    } else {\n+      buffers.push_back(BufferUse::Read(args_[i]));\n+    }\n+  }\n+  return buffers;\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "3664958644cbbf20fd8fd3e5df0646a56b20ebcc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk.h",
            "status": "added",
            "additions": 94,
            "deletions": 0,
            "changes": 94,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -0,0 +1,94 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_CUSTOM_KERNEL_THUNK_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_CUSTOM_KERNEL_THUNK_H_\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/kernels/custom_kernel.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// CustomKernelThunk loads and executes kernels defined by a custom kernel\n+// (which in practice means hand written CUDA C++ kernel), instead of a kernel\n+// compiled by XLA and loaded from an executable source.\n+class CustomKernelThunk : public Thunk {\n+ public:\n+  CustomKernelThunk(const HloInstruction* inst, CustomKernel custom_kernel,\n+                    const emitters::KernelArguments& kernel_arguments,\n+                    ThunkId thunk_id);\n+\n+  std::string ToString(int indent) const override;\n+\n+  absl::Status Initialize(const InitializeParams& params) override;\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+\n+  const CustomKernel& custom_kernel() const { return custom_kernel_; }\n+\n+  const std::vector<BufferAllocation::Slice>& arguments() const {\n+    return args_;\n+  }\n+\n+  absl::string_view custom_kernel_name() const { return custom_kernel_.name(); }\n+\n+  const std::vector<bool>& written() const { return written_; }\n+\n+  LaunchDimensions launch_dimensions() const {\n+    return LaunchDimensions(custom_kernel_.block_dims(),\n+                            custom_kernel_.thread_dims());\n+  }\n+\n+  int64_t shmem_bytes() const { return custom_kernel_.shared_memory_bytes(); }\n+\n+  BufferUses buffer_uses() const override;\n+\n+ private:\n+  // Buffer slices passed to the kernel as arguments.\n+  std::vector<BufferAllocation::Slice> args_;\n+\n+  // args_[i] is written iff (written_[i] == true).\n+  std::vector<bool> written_;\n+\n+  CustomKernel custom_kernel_;\n+\n+  // Loaded kernels for each `StreamExecutor`.\n+  mutable absl::Mutex mutex_;\n+  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<se::Kernel>>\n+      kernel_cache_ ABSL_GUARDED_BY(mutex_);\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_CUSTOM_KERNEL_THUNK_H_"
        },
        {
            "sha": "c5401bfa47c6ef0b078366373de8d35d128854bf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk_test.cc",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -0,0 +1,82 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/runtime/buffer_use.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/kernels/custom_kernel.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+TEST(CustomKernelThunkTest, BufferUsesReturnsCorrectBuffers) {\n+  CustomKernel kernel(\n+      /*name=*/\"\",\n+      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n+          /*ptx=*/\"\", /*kernel_name=*/\"\", /*arity=*/0),\n+      se::BlockDim(), se::ThreadDim(), /*shared_memory_bytes=*/0);\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n+  BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n+  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n+  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n+  arg0.set_written(false);\n+  arg1.set_written(true);\n+  emitters::KernelArguments kernel_arguments({arg0, arg1});\n+  auto hlo = HloInstruction::CreateConstant(Literal());\n+  CustomKernelThunk thunk(hlo.get(), kernel, kernel_arguments, ThunkId{0});\n+\n+  Thunk::BufferUses buffers = thunk.buffer_uses();\n+\n+  ASSERT_THAT(buffers, testing::UnorderedElementsAre(BufferUse::Read(slice0),\n+                                                     BufferUse::Write(slice1)));\n+}\n+\n+TEST(CustomKernelThunkTest, BufferUsesReturnsBuffersInConsistentOrder) {\n+  CustomKernel kernel(\n+      /*name=*/\"\",\n+      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n+          /*ptx=*/\"\", /*kernel_name=*/\"\", /*arity=*/0),\n+      se::BlockDim(), se::ThreadDim(), /*shared_memory_bytes=*/0);\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n+  BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n+  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n+  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n+  arg0.set_written(false);\n+  arg1.set_written(true);\n+  emitters::KernelArguments kernel_arguments({arg0, arg1});\n+  auto hlo = HloInstruction::CreateConstant(Literal());\n+  CustomKernelThunk thunk(hlo.get(), kernel, kernel_arguments, ThunkId{0});\n+\n+  Thunk::BufferUses buffers1 = thunk.buffer_uses();\n+  Thunk::BufferUses buffers2 = thunk.buffer_uses();\n+\n+  ASSERT_THAT(buffers1, testing::ContainerEq(buffers2));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "01f6576bc21c4fc9f04b938f2ed66c508cf582b9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 136,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -15,13 +15,11 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n \n-#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <optional>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/container/inlined_vector.h\"\n@@ -32,20 +30,19 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"xla/backends/gpu/runtime/print_buffer_contents.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -60,27 +57,6 @@ using tsl::profiler::TraceMeLevel;\n namespace xla {\n namespace gpu {\n \n-Thunk::BufferUses BufferUseFromKernelArguments(\n-    absl::Span<const BufferAllocation::Slice> args,\n-    const std::vector<bool>& written) {\n-  Thunk::BufferUses buffers;\n-  buffers.reserve(args.size());\n-  for (int i = 0; i < args.size(); ++i) {\n-    // We assume that any buffer is either an input or an output of the\n-    // kernel, and inout buffers are represented as 2 separate arguments.\n-    if (written[i]) {\n-      buffers.push_back(BufferUse::Write(args[i]));\n-    } else {\n-      buffers.push_back(BufferUse::Read(args[i]));\n-    }\n-  }\n-  return buffers;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// KernelThunk\n-//===----------------------------------------------------------------------===//\n-\n KernelThunk::KernelThunk(Thunk::ThunkInfo thunk_info, std::string kernel_name,\n                          const emitters::KernelArguments& kernel_arguments,\n                          LaunchDimensions launch_dimensions,\n@@ -189,45 +165,6 @@ absl::Status KernelThunk::Initialize(const InitializeParams& params) {\n   return absl::OkStatus();\n }\n \n-void PrintBufferContents(se::Stream*, int input_idx, se::TensorMap tensor_map) {\n-  VLOG(100) << \"TENSOR_MAP(\" << input_idx << \") = \";\n-  for (std::byte element : tensor_map.storage) {\n-    VLOG(100) << absl::StrFormat(\"%x \", static_cast<unsigned>(element));\n-  }\n-}\n-\n-void PrintBufferContents(se::Stream* stream, int input_idx,\n-                         se::DeviceMemoryBase buf) {\n-  auto host_buffer = std::make_unique<char[]>(buf.size());\n-  CHECK_OK(stream->Memcpy(host_buffer.get(), buf, buf.size()));\n-  CHECK_OK(stream->BlockHostUntilDone());\n-\n-  std::string buffer_contents;\n-  for (int i = 0; i < buf.size(); ++i) {\n-    absl::StrAppendFormat(&buffer_contents, \"%x \",\n-                          static_cast<unsigned>(host_buffer[i]));\n-  }\n-  VLOG(100) << \"BUF(\" << input_idx << \") = \" << buffer_contents;\n-}\n-\n-void PrintBufferContents(se::Stream*, int input_idx, int64_t int_arg) {\n-  VLOG(100) << \"INT(\" << input_idx << \") = \";\n-  VLOG(100) << absl::StrFormat(\"%x \", int_arg);\n-}\n-\n-static void PrintBufferContents(\n-    se::Stream* stream, absl::Span<const se::KernelArgument> kernel_args) {\n-  for (const auto& [input_idx, arg] : llvm::enumerate(kernel_args)) {\n-    // pre-cpp-20-compat(P0588R1): Capturing structured bindings in lambdas is\n-    // ill-formed.\n-    std::visit(\n-        [&stream, &input_idx = input_idx](auto const& arg) {\n-          PrintBufferContents(stream, input_idx, arg);\n-        },\n-        arg);\n-  }\n-}\n-\n absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n   TraceMe trace(\n       [] { return TraceMeEncode(\"KernelThunk::ExecuteOnStream\", {}); },\n@@ -308,79 +245,18 @@ absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n }\n \n Thunk::BufferUses KernelThunk::buffer_uses() const {\n-  return BufferUseFromKernelArguments(absl::MakeConstSpan(args_), written_);\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// CustomKernelThunk\n-//===----------------------------------------------------------------------===//\n-\n-CustomKernelThunk::CustomKernelThunk(\n-    const HloInstruction* instr, CustomKernel custom_kernel,\n-    const emitters::KernelArguments& kernel_arguments, ThunkId thunk_id)\n-    : Thunk(Kind::kCustomKernel,\n-            Thunk::ThunkInfo::WithProfileAnnotation(instr, thunk_id)),\n-      args_(kernel_arguments.GetArgumentBufferSlices()),\n-      written_(kernel_arguments.GetArgumentOutputFlags()),\n-      custom_kernel_(std::move(custom_kernel)) {}\n-\n-std::string CustomKernelThunk::ToString(int indent) const {\n-  return custom_kernel_.ToString();\n-}\n-\n-absl::Status CustomKernelThunk::Initialize(const InitializeParams& params) {\n-  absl::MutexLock lock(mutex_);\n-\n-  if (!kernel_cache_.contains(params.executor)) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<se::Kernel> kernel,\n-        params.executor->LoadKernel(custom_kernel_.kernel_spec()));\n-    kernel_cache_.emplace(params.executor, std::move(kernel));\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n-absl::Status CustomKernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n-  se::StreamExecutor* executor = params.stream->parent();\n-\n-  se::Kernel* kernel = [&] {\n-    absl::MutexLock lock(mutex_);\n-    return kernel_cache_[executor].get();\n-  }();\n-\n-  int device_ordinal = executor->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal << \"] Launching \"\n-          << custom_kernel_.ToString() << \" as device kernel \"\n-          << kernel->name();\n-\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> buffer_args;\n-  for (const BufferAllocation::Slice& arg : args_) {\n-    se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n-    VLOG(3) << \"[\" << device_ordinal << \"]  Arg: alloc #\" << arg.index()\n-            << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n-            << buf.size() << \"B)\";\n-    buffer_args.push_back(buf);\n-  }\n-\n-  if (VLOG_IS_ON(100)) {\n-    absl::InlinedVector<se::KernelArgument, 4> kernel_args;\n-    for (const se::DeviceMemoryBase& arg : buffer_args) {\n-      kernel_args.push_back(arg);\n+  Thunk::BufferUses buffers;\n+  buffers.reserve(args_.size());\n+  for (int i = 0; i < args_.size(); ++i) {\n+    // We assume that any buffer is either an input or an output of the\n+    // kernel, and inout buffers are represented as 2 separate arguments.\n+    if (written_[i]) {\n+      buffers.push_back(BufferUse::Write(args_[i]));\n+    } else {\n+      buffers.push_back(BufferUse::Read(args_[i]));\n     }\n-    PrintBufferContents(params.stream, kernel_args);\n   }\n-\n-  se::KernelArgsDeviceMemoryArray args(buffer_args,\n-                                       custom_kernel_.shared_memory_bytes());\n-\n-  return kernel->Launch(custom_kernel_.thread_dims(),\n-                        custom_kernel_.block_dims(),\n-                        custom_kernel_.cluster_dims(), params.stream, args);\n-}\n-\n-Thunk::BufferUses CustomKernelThunk::buffer_uses() const {\n-  return BufferUseFromKernelArguments(absl::MakeConstSpan(args_), written_);\n+  return buffers;\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "cd7212fffad2c99e26fb630210d86b7bd90b69bc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 63,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -26,16 +26,12 @@ limitations under the License.\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -46,19 +42,13 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n-class GpuExecutable;\n-\n // TODO(ezhulenev): Unify KernelThunk and CustomKernelThunk as they are very\n // similar. XLA:GPU should use more of kernel loading APIs provided by\n // StreamExecutor out of the box and less custom kernel loading solutions.\n //\n // Today KernelThunk is required for lowering to XLA runtime, and\n // CustomKernelThunk is only supported for thunk execution.\n-\n-//===----------------------------------------------------------------------===//\n-// KernelThunk\n-//===----------------------------------------------------------------------===//\n-\n+//\n // This class stores everything that StreamExecutor needs for launching a\n // kernel. It implements the ExecuteOnStream interface for GpuExecutable to\n // invoke the corresponding kernel.\n@@ -141,58 +131,6 @@ class KernelThunk : public Thunk {\n       kernel_cache_ ABSL_GUARDED_BY(mutex_);\n };\n \n-//===----------------------------------------------------------------------===//\n-// CustomKernelThunk\n-//===----------------------------------------------------------------------===//\n-\n-// CustomKernelThunk loads and executes kernels defined by a custom kernel\n-// (which in practice means hand written CUDA C++ kernel), instead of a kernel\n-// compiled by XLA and loaded from an executable source.\n-class CustomKernelThunk : public Thunk {\n- public:\n-  CustomKernelThunk(const HloInstruction* inst, CustomKernel custom_kernel,\n-                    const emitters::KernelArguments& kernel_arguments,\n-                    ThunkId thunk_id);\n-\n-  std::string ToString(int indent) const override;\n-\n-  absl::Status Initialize(const InitializeParams& params) override;\n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n-\n-  const CustomKernel& custom_kernel() const { return custom_kernel_; }\n-\n-  const std::vector<BufferAllocation::Slice>& arguments() const {\n-    return args_;\n-  }\n-\n-  absl::string_view custom_kernel_name() const { return custom_kernel_.name(); }\n-\n-  const std::vector<bool>& written() const { return written_; }\n-\n-  LaunchDimensions launch_dimensions() const {\n-    return LaunchDimensions(custom_kernel_.block_dims(),\n-                            custom_kernel_.thread_dims());\n-  }\n-\n-  int64_t shmem_bytes() const { return custom_kernel_.shared_memory_bytes(); }\n-\n-  BufferUses buffer_uses() const override;\n-\n- private:\n-  // Buffer slices passed to the kernel as arguments.\n-  std::vector<BufferAllocation::Slice> args_;\n-\n-  // args_[i] is written iff (written_[i] == true).\n-  std::vector<bool> written_;\n-\n-  CustomKernel custom_kernel_;\n-\n-  // Loaded kernels for each `StreamExecutor`.\n-  mutable absl::Mutex mutex_;\n-  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<se::Kernel>>\n-      kernel_cache_ ABSL_GUARDED_BY(mutex_);\n-};\n-\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "2c2b35364d1a51a4c2f89380e8577c83e16e7d65",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -308,51 +308,6 @@ TEST(KernelThunkTest, BufferUsesReturnsBuffersInConsistentOrder) {\n   ASSERT_THAT(buffers1, testing::ContainerEq(buffers2));\n }\n \n-TEST(CustomKernelThunkTest, BufferUsesReturnsCorrectBuffers) {\n-  CustomKernel kernel(\n-      /*name=*/\"\",\n-      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n-          /*ptx=*/\"\", /*kernel_name=*/\"\", /*arity=*/0),\n-      se::BlockDim(), se::ThreadDim(), /*shared_memory_bytes=*/0);\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n-  BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n-  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n-  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n-  arg0.set_written(false);\n-  arg1.set_written(true);\n-  emitters::KernelArguments kernel_arguments({arg0, arg1});\n-  auto hlo = HloInstruction::CreateConstant(Literal());\n-  CustomKernelThunk thunk(hlo.get(), kernel, kernel_arguments, ThunkId{0});\n-\n-  Thunk::BufferUses buffers = thunk.buffer_uses();\n-\n-  ASSERT_THAT(buffers, testing::UnorderedElementsAre(BufferUse::Read(slice0),\n-                                                     BufferUse::Write(slice1)));\n-}\n-\n-TEST(CustomKernelThunkTest, BufferUsesReturnsBuffersInConsistentOrder) {\n-  CustomKernel kernel(\n-      /*name=*/\"\",\n-      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n-          /*ptx=*/\"\", /*kernel_name=*/\"\", /*arity=*/0),\n-      se::BlockDim(), se::ThreadDim(), /*shared_memory_bytes=*/0);\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n-  BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n-  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n-  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n-  arg0.set_written(false);\n-  arg1.set_written(true);\n-  emitters::KernelArguments kernel_arguments({arg0, arg1});\n-  auto hlo = HloInstruction::CreateConstant(Literal());\n-  CustomKernelThunk thunk(hlo.get(), kernel, kernel_arguments, ThunkId{0});\n-\n-  Thunk::BufferUses buffers1 = thunk.buffer_uses();\n-  Thunk::BufferUses buffers2 = thunk.buffer_uses();\n-\n-  ASSERT_THAT(buffers1, testing::ContainerEq(buffers2));\n-}\n \n class KernelThunkTmaPTXTest : public ::testing::TestWithParam<bool> {\n  public:"
        },
        {
            "sha": "30180fadd8d863df448363193ed397e338189ebf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/print_buffer_contents.cc",
            "status": "added",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -0,0 +1,77 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/print_buffer_contents.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <variant>\n+\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n+#include \"xla/stream_executor/stream.h\"\n+\n+namespace xla::gpu {\n+\n+namespace {\n+void PrintBufferContents(stream_executor::Stream*, int input_idx,\n+                         stream_executor::TensorMap tensor_map) {\n+  std::string formatted_contents;\n+  for (std::byte element : tensor_map.storage) {\n+    absl::StrAppendFormat(&formatted_contents, \"%02x \",\n+                          static_cast<unsigned>(element));\n+  }\n+  LOG(INFO) << \"TENSOR_MAP(\" << input_idx << \") = \" << formatted_contents;\n+}\n+\n+void PrintBufferContents(stream_executor::Stream* stream, int input_idx,\n+                         stream_executor::DeviceMemoryBase buf) {\n+  auto host_buffer = std::make_unique<char[]>(buf.size());\n+  CHECK_OK(stream->Memcpy(host_buffer.get(), buf, buf.size()));\n+  CHECK_OK(stream->BlockHostUntilDone());\n+\n+  std::string buffer_contents;\n+  for (int i = 0; i < buf.size(); ++i) {\n+    absl::StrAppendFormat(&buffer_contents, \"%02x \",\n+                          static_cast<unsigned>(host_buffer[i]));\n+  }\n+  LOG(INFO) << \"BUF(\" << input_idx << \") = \" << buffer_contents;\n+}\n+\n+void PrintBufferContents(stream_executor::Stream*, int input_idx,\n+                         int64_t int_arg) {\n+  LOG(INFO) << \"INT(\" << input_idx\n+            << \") = \" << absl::StrFormat(\"%#08x\", int_arg);\n+}\n+}  // namespace\n+\n+void PrintBufferContents(\n+    stream_executor::Stream* stream,\n+    absl::Span<const stream_executor::KernelArgument> kernel_args) {\n+  for (int input_idx = 0; input_idx < kernel_args.size(); ++input_idx) {\n+    const stream_executor::KernelArgument& arg = kernel_args[input_idx];\n+    std::visit(\n+        [&](auto const& arg) { PrintBufferContents(stream, input_idx, arg); },\n+        arg);\n+  }\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "3ced52a0aee03b00c82404c5e552638dd22ac8f5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/print_buffer_contents.h",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents.h?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -0,0 +1,32 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_PRINT_BUFFER_CONTENTS_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_PRINT_BUFFER_CONTENTS_H_\n+\n+#include \"absl/types/span.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n+#include \"xla/stream_executor/stream.h\"\n+\n+namespace xla::gpu {\n+\n+// Prints the contents of the buffer arguments in `kernel_args` to LOG(INFO).\n+void PrintBufferContents(\n+    stream_executor::Stream* stream,\n+    absl::Span<const stream_executor::KernelArgument> kernel_args);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_PRINT_BUFFER_CONTENTS_H_"
        },
        {
            "sha": "102cabd07835d4de0c8fc9f11394442d1f99d248",
            "filename": "third_party/xla/xla/backends/gpu/runtime/print_buffer_contents_test.cc",
            "status": "added",
            "additions": 79,
            "deletions": 0,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fprint_buffer_contents_test.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -0,0 +1,79 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/print_buffer_contents.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/scoped_mock_log.h\"\n+#include \"absl/strings/ascii.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+using ::testing::_;\n+using ::testing::HasSubstr;\n+\n+TEST(PrintBufferContentsTest, PrintBufferContents) {\n+  auto name =\n+      absl::AsciiStrToUpper(PlatformUtil::CanonicalPlatformName(\"gpu\").value());\n+  stream_executor::Platform* platform =\n+      stream_executor::PlatformManager::PlatformWithName(name).value();\n+  stream_executor::StreamExecutor* executor =\n+      platform->ExecutorForDevice(0).value();\n+\n+  auto stream = executor->CreateStream().value();\n+\n+  stream_executor::DeviceMemory<int> arg1 =\n+      executor->AllocateArray<int32_t>(10, 0);\n+\n+  TF_ASSERT_OK(stream->Memset32(&arg1, 0x12345678, 10 * sizeof(int32_t)));\n+  TF_ASSERT_OK(stream->BlockHostUntilDone());\n+\n+  std::vector<stream_executor::KernelArgument> kernel_args;\n+  kernel_args.push_back(arg1);\n+  stream_executor::TensorMap tensor_map;\n+  for (int i = 0; i < 128; ++i) {\n+    tensor_map.storage[i] = static_cast<std::byte>(i);\n+  }\n+  kernel_args.push_back(tensor_map);\n+  kernel_args.push_back(0x123456789);\n+\n+  absl::ScopedMockLog log{absl::MockLogDefault::kIgnoreUnexpected};\n+  EXPECT_CALL(log, Log(_, _, HasSubstr(\"BUF(0) = 78 56 34 12 78 56 34 12\")));\n+  EXPECT_CALL(\n+      log,\n+      Log(_, _,\n+          HasSubstr(\"TENSOR_MAP(1) = 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d \"\n+                    \"0e 0f 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f\")));\n+  EXPECT_CALL(log, Log(_, _, HasSubstr(\"INT(2) = 0x123456789\")));\n+\n+  log.StartCapturingLogs();\n+  PrintBufferContents(stream.get(), kernel_args);\n+  log.StopCapturingLogs();\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "bb519a7814b95b0230e6ccf78c2190cf33e72286",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -354,6 +354,7 @@ cc_library(\n         \":gpu_constants\",\n         \":ir_emitter_context\",\n         \":kernel_call\",\n+        \"//xla/backends/gpu/runtime:custom_kernel_thunk\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/hlo/ir:hlo\",\n@@ -497,6 +498,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:cudnn_thunk\",\n         \"//xla/backends/gpu/runtime:custom_call_target\",\n         \"//xla/backends/gpu/runtime:custom_call_thunk\",\n+        \"//xla/backends/gpu/runtime:custom_kernel_thunk\",\n         \"//xla/backends/gpu/runtime:fft_thunk\",\n         \"//xla/backends/gpu/runtime:gemm_thunk\",\n         \"//xla/backends/gpu/runtime:gpublas_lt_matmul_thunk\","
        },
        {
            "sha": "0627f6788ae89fe8e258845bb53853c3acd571ec",
            "filename": "third_party/xla/xla/service/gpu/custom_kernel_emitter_cuda.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\""
        },
        {
            "sha": "96b42f629c427233ab9a5413717ed9407bc62b3c",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4f746be7a4d542d7532a874e524e0667ae1a6297/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=4f746be7a4d542d7532a874e524e0667ae1a6297",
            "patch": "@@ -89,6 +89,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/cub_sort_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n+#include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/fft_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\""
        }
    ],
    "stats": {
        "total": 854,
        "additions": 606,
        "deletions": 248
    }
}