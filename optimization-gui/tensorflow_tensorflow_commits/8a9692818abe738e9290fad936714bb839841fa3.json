{
    "author": "akuegel",
    "message": "Use GetInPlaceInputOutputPairs from AliasInfo instead of HloDataflowAnalysis.\n\nPiperOrigin-RevId: 837082195",
    "sha": "8a9692818abe738e9290fad936714bb839841fa3",
    "files": [
        {
            "sha": "70f69508aec264bbda0c2ee6ea7ea89fab7169ca",
            "filename": "third_party/xla/xla/hlo/tools/hlo_opt/opt_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -248,7 +248,8 @@ void OptProvider::RegisterAllHardwareIndependentPasses() {\n   RegisterPass<BFloat16ConversionFolding>(\n       /*bfloat16_support=*/bfloat16_support);\n   RegisterPass<BFloat16MixedPrecisionRemoval>();\n-  RegisterPass<BFloat16Propagation>(/*bfloat16_support=*/bfloat16_support);\n+  RegisterPass<BFloat16Propagation>(/*bfloat16_support=*/bfloat16_support,\n+                                    alias_info_.get());\n   RegisterPass<BatchDotSimplification>();\n   RegisterPass<BroadcastCanonicalizer>();\n   RegisterPass<CholeskyExpander>();"
        },
        {
            "sha": "9624571f9201752d4d67440ef974f39b6289c03e",
            "filename": "third_party/xla/xla/hlo/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2FBUILD?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -28,6 +28,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/analysis:hlo_operand_index\",\n         \"//xla/hlo/ir:hlo\",\n@@ -61,6 +62,7 @@ xla_cc_test(\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:test\","
        },
        {
            "sha": "5c80a87f29c2ab0c60ede022b103e4a4269f46d2",
            "filename": "third_party/xla/xla/hlo/transforms/bfloat16_propagation.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/analysis/hlo_operand_index.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -50,8 +51,9 @@ limitations under the License.\n \n namespace xla {\n \n-BFloat16Propagation::BFloat16Propagation(const FloatSupport* bfloat16_support)\n-    : bfloat16_support_(bfloat16_support) {\n+BFloat16Propagation::BFloat16Propagation(const FloatSupport* bfloat16_support,\n+                                         const AliasInfo* alias_info)\n+    : bfloat16_support_(bfloat16_support), alias_info_(alias_info) {\n   DCHECK_EQ(bfloat16_support->LowPrecisionType(), BF16);\n }\n \n@@ -744,7 +746,7 @@ bool BFloat16Propagation::ResolveInconsistencyOfAliasingBuffersHelper(\n         // HloAliasAnalysis (e.g., their computation graphs may not have been\n         // flattened yet).\n         for (const auto& operand_and_output_index :\n-             HloDataflowAnalysis::GetInPlaceInputOutputPairs(hlo)) {\n+             alias_info_->GetInPlaceInputOutputPairs(hlo)) {\n           if (operand_and_output_index.second == index) {\n             const HloOperandIndex& operand_index =\n                 operand_and_output_index.first;"
        },
        {
            "sha": "b547418e180a92ad4c1974f862c9cbaa58dfdcd1",
            "filename": "third_party/xla/xla/hlo/transforms/bfloat16_propagation.h",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation.h?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -66,7 +67,8 @@ namespace xla {\n // pass.\n class BFloat16Propagation : public HloModulePass {\n  public:\n-  explicit BFloat16Propagation(const FloatSupport* bfloat16_support);\n+  BFloat16Propagation(const FloatSupport* bfloat16_support,\n+                      const AliasInfo* alias_info);\n \n   ~BFloat16Propagation() override = default;\n \n@@ -84,6 +86,8 @@ class BFloat16Propagation : public HloModulePass {\n  protected:\n   const FloatSupport* bfloat16_support_;\n \n+  const AliasInfo* alias_info_;\n+\n   // Runs the pass on the given module. Returns whether the module was changed\n   // (precision reductions were added).\n   absl::StatusOr<bool> RunImpl("
        },
        {
            "sha": "c2dc7c227113969f0c2fc8013cb91a3e7da1fc0d",
            "filename": "third_party/xla/xla/hlo/transforms/bfloat16_propagation_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fbfloat16_propagation_test.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/comparison_util.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -82,7 +83,7 @@ class BFloat16PropagationTest : public HloHardwareIndependentTestBase {\n   // module is changed after this pass.\n   bool PropagatePrecision(HloModule* module) {\n     TestBFloat16Support bfloat16_support;\n-    BFloat16Propagation propagation(&bfloat16_support);\n+    BFloat16Propagation propagation(&bfloat16_support, &alias_info_);\n     absl::StatusOr<bool> result = propagation.Run(module);\n     EXPECT_IS_OK(result.status());\n     return result.value();\n@@ -108,6 +109,7 @@ class BFloat16PropagationTest : public HloHardwareIndependentTestBase {\n     return HloInstruction::CreateDot(shape, lhs, rhs, dot_dnums,\n                                      DefaultPrecisionConfig(2));\n   }\n+  AliasInfo alias_info_;\n };\n \n // Tests that BF16 can propagate through select over non-tuple buffers, but not"
        },
        {
            "sha": "57b1cfa96e20f0c8a36bb15479c8e274c43df41a",
            "filename": "third_party/xla/xla/service/copy_insertion.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -1183,7 +1183,7 @@ absl::Status CopyInsertion::AddCopiesToResolveInterference(\n         // have been copied.\n         absl::flat_hash_set<int64_t> copied_operands;\n         for (const auto& operand_and_output_index :\n-             HloDataflowAnalysis::GetInPlaceInputOutputPairs(\n+             alias_info_->GetInPlaceInputOutputPairs(\n                  // Input/output buffer aliasing analysis needs to be done\n                  // directly with the wrapped instruction when the compiler sees\n                  // an async box."
        },
        {
            "sha": "95270c1204d62a4447c8d59d3809e6918c9d82f9",
            "filename": "third_party/xla/xla/service/copy_removal.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -346,8 +346,9 @@ bool ComputeRelativeLocation::ForceRuntimeOrder(\n                \"kBeforeStart or kAfterEnd\";\n     return false;\n   }\n-  auto ModifiesNonCopy = [](HloInstruction* instr, const HloInstruction* op) {\n-    auto in_place = HloDataflowAnalysis::GetInPlaceInputOutputPairs(instr);\n+  auto ModifiesNonCopy = [this](HloInstruction* instr,\n+                                const HloInstruction* op) {\n+    auto in_place = alias_info_->GetInPlaceInputOutputPairs(instr);\n     if (in_place.empty()) {\n       return false;\n     }\n@@ -435,7 +436,7 @@ bool ComputeRelativeLocation::InstructionCanIntercept(\n     // If the instruction only uses the value, it can intercept only if it\n     // modifies the buffer in place.\n     for (const auto& operand_and_output_index :\n-         HloDataflowAnalysis::GetInPlaceInputOutputPairs(instr)) {\n+         alias_info_->GetInPlaceInputOutputPairs(instr)) {\n       const HloOperandIndex& operand_index = operand_and_output_index.first;\n       if (region.contains(\n               instr->mutable_operand(operand_index.operand_number))) {\n@@ -1293,7 +1294,7 @@ bool CopyRemover::ValuesInterfere(const ValueNode* src, const ValueNode* dest,\n   VLOG(5) << \"    ValuesInterfere destination live range:\\n\"\n           << dest_live_range.ToString();\n \n-  ComputeRelativeLocation relative_location_analysis(ordering_);\n+  ComputeRelativeLocation relative_location_analysis(ordering_, alias_info_);\n   auto rel1 = relative_location_analysis.ComputeBetweenLiveRangeRegions(\n       src_live_range, dest_live_range);\n   VLOG(3) << \"    ValuesInterfere - location of dest in relation to src: \";"
        },
        {
            "sha": "ac1e7b65c367bed4c66fb390a4a899486f2022bb",
            "filename": "third_party/xla/xla/service/copy_removal.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.h?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -289,8 +289,8 @@ class Relation {\n class ComputeRelativeLocation {\n  public:\n   typedef LiveRangeRegions::InstructionEntry InstructionEntry;\n-  explicit ComputeRelativeLocation(HloOrdering* ordering)\n-      : ordering_(ordering) {\n+  ComputeRelativeLocation(HloOrdering* ordering, const AliasInfo* alias_info)\n+      : ordering_(ordering), alias_info_(alias_info) {\n     VLOG(3) << \"New analysis\";\n   }\n \n@@ -352,6 +352,7 @@ class ComputeRelativeLocation {\n                                                 HloInstruction* instr2);\n \n   HloOrdering* ordering_;\n+  const AliasInfo* alias_info_;\n   absl::flat_hash_map<\n       HloInstruction*,\n       absl::flat_hash_map<HloInstruction*, Relation::RuntimeOrder>>"
        },
        {
            "sha": "bab8705a9ec90474a1d3cf64f83af0eb11c9c3ca",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -1589,6 +1589,7 @@ cc_library(\n     srcs = [\"fusion_pipeline.cc\"],\n     hdrs = [\"fusion_pipeline.h\"],\n     deps = [\n+        \":alias_info\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n@@ -1605,6 +1606,7 @@ cc_library(\n         \"//xla/service/gpu/transforms:priority_fusion\",\n         \"//xla/service/gpu/transforms:variadic_op_splitter\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/tsl/platform:env\",\n         \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:env\",\n     ],\n@@ -2852,6 +2854,7 @@ cc_library(\n     name = \"alias_info\",\n     srcs = [\"alias_info.cc\"],\n     hdrs = [\"alias_info.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n         \":backend_configs_cc\",\n         \":hlo_fusion_analysis\",\n@@ -2929,6 +2932,7 @@ cc_library(\n     hdrs = [\"gpu_fusible.h\"],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n+        \":alias_info\",\n         \":backend_configs_cc\",\n         \":hlo_fusion_analysis\",\n         \":ir_emission_utils\",\n@@ -2964,6 +2968,7 @@ xla_cc_test(\n         \"nomsan\",\n     ],\n     deps = [\n+        \":alias_info\",\n         \":gpu_device_info_for_tests\",\n         \":gpu_fusible\",\n         \":hlo_fusion_analysis\","
        },
        {
            "sha": "ce4176081976e6df486616b224091eb6f0ee6e1d",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n #include \"xla/service/cpu_gpu_shape_verifier.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/transforms/multi_output_fusion.h\"\n #include \"xla/service/gpu/transforms/priority_fusion.h\"\n@@ -34,6 +35,7 @@ limitations under the License.\n #include \"xla/service/hlo_verifier.h\"\n #include \"xla/service/layout_assignment.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/xla.pb.h\"\n #include \"tsl/platform/threadpool.h\"\n \n@@ -43,7 +45,7 @@ namespace gpu {\n HloPassPipeline FusionPipeline(\n     const DebugOptions& debug_options,\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n-    tsl::thread::ThreadPool* thread_pool,\n+    const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool,\n     const se::DeviceDescription& gpu_device_info,\n     mlir::MLIRContext* mlir_context) {\n   HloPassFix<HloPassPipeline> fusion(\"fusion\");\n@@ -72,8 +74,8 @@ HloPassPipeline FusionPipeline(\n       /*is_layout_sensitive=*/true, /*ignore_control_dependencies=*/false,\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);\n   fusion.AddPass<HloDCE>();\n-  fusion.AddPass<MultiOutputFusion>(gpu_device_info, shape_size_bytes_function,\n-                                    mlir_context);\n+  fusion.AddPass<MultiOutputFusion>(gpu_device_info, alias_info,\n+                                    shape_size_bytes_function, mlir_context);\n   fusion.AddPass<HloCSE>(\n       /*is_layout_sensitive=*/true, /*ignore_control_dependencies=*/false,\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);"
        },
        {
            "sha": "64b4ceea176562bee6e30702e05bdf85d2bec2b8",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/xla.pb.h\"\n@@ -31,7 +32,7 @@ namespace gpu {\n HloPassPipeline FusionPipeline(\n     const DebugOptions& debug_options,\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n-    tsl::thread::ThreadPool* thread_pool,\n+    const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool,\n     const se::DeviceDescription& gpu_device_info,\n     mlir::MLIRContext* mlir_context);\n "
        },
        {
            "sha": "882165e03b675537b069c4eda753c4fe0e73a39b",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -1014,6 +1014,7 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n                              const Compiler::GpuTargetConfig& gpu_target_config,\n                              tsl::thread::ThreadPool* thread_pool,\n                              HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n+                             const GpuAliasInfo* alias_info,\n                              mlir::MLIRContext* mlir_context) {\n   const se::DeviceDescription& gpu_device_info =\n       gpu_target_config.device_description;\n@@ -1024,7 +1025,7 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n \n   TF_RETURN_IF_ERROR(\n       FusionPipeline(hlo_module->config().debug_options(), shape_size_fn,\n-                     thread_pool, gpu_device_info, mlir_context)\n+                     alias_info, thread_pool, gpu_device_info, mlir_context)\n           .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n           .status());\n \n@@ -1437,9 +1438,9 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   TF_RETURN_IF_ERROR(\n       RunDynamicSliceFusionPasses(hlo_module, /*platform_id=*/PlatformId()));\n \n-  TF_RETURN_IF_ERROR(RunFusionPasses(hlo_module, gpu_target_config,\n-                                     thread_pool.get_mutable(),\n-                                     ShapeSizeBytesFunction(), &mlir_context_));\n+  TF_RETURN_IF_ERROR(\n+      RunFusionPasses(hlo_module, gpu_target_config, thread_pool.get_mutable(),\n+                      ShapeSizeBytesFunction(), alias_info, &mlir_context_));\n   TF_RETURN_IF_ERROR(RunPostFusionPasses(hlo_module, device_description,\n                                          alias_info, pointer_size_, options,\n                                          &mlir_context_));\n@@ -1665,7 +1666,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       pipeline.AddPass<HloDCE>();\n       pipeline.AddPass<SoftmaxRewriterTriton>(\n           gpu_target_config.device_description, ShapeSizeBytesFunction(),\n-          &mlir_context_,\n+          alias_info, &mlir_context_,\n           /*only_fuse_if_profitable=*/true);\n     }\n "
        },
        {
            "sha": "5f665644c5719e142316faf44746187d96fe14d6",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/permutation_util.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n@@ -528,7 +529,8 @@ FusionDecision CanEmitInputFusedScatter(const HloInstruction& producer,\n }\n \n FusionDecision IsProducerMultiOutputFusible(\n-    const HloInstruction& producer, const se::DeviceDescription& device_info) {\n+    const HloInstruction& producer, const GpuAliasInfo* alias_info,\n+    const se::DeviceDescription& device_info) {\n   // Skip multiple output fusion. It's not yet supported.\n   if (producer.IsMultiOutputFusion()) {\n     return FusionDecision::Forbid(\"Producer is a multi-output fusion\");\n@@ -558,7 +560,7 @@ FusionDecision IsProducerMultiOutputFusible(\n   // is in-place. (We can relax this restriction by establishing an explicit\n   // contract that describes what multi-output fusion scenarios are supported by\n   // codegen and then changing this check to allow exactly those fusions).\n-  if (!HloDataflowAnalysis::GetInPlaceInputOutputPairs(&producer).empty()) {\n+  if (!alias_info->GetInPlaceInputOutputPairs(&producer).empty()) {\n     return FusionDecision::Forbid(\"In-place operations are present\");\n   }\n "
        },
        {
            "sha": "a658e42f5e2d95d68fd9aae1c2dbf84fe16853f7",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.h?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/instruction_fusion.h\"\n@@ -179,7 +180,8 @@ FusionDecision CanEmitInputFusedScatter(const HloInstruction& producer,\n // That is, the root tuple of the multi-output fusion will contain the results\n // of both, the producer and consumer.\n FusionDecision IsProducerMultiOutputFusible(\n-    const HloInstruction& producer, const se::DeviceDescription& device_info);\n+    const HloInstruction& producer, const GpuAliasInfo* alias_info,\n+    const se::DeviceDescription& device_info);\n // Whether `instr` is a candidate for sibling fusion or as a consumer in\n // a producer-consumer multi-output fusion.\n bool IsFusibleAsMultiOutputFusionRoot(const HloInstruction& instr,"
        },
        {
            "sha": "d1ad49adfd127805fa770b09314eae97e8ab9cb9",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/instruction_fusion.h\"\n@@ -52,6 +53,7 @@ class GpuFusibleTest : public HloHardwareIndependentTestBase {\n  public:\n   void SetUp() override {\n     TF_ASSERT_OK_AND_ASSIGN(device_description_, MakeDeviceDescription());\n+    alias_info_ = std::make_unique<GpuAliasInfo>(device_description_);\n   }\n \n   DebugOptions GetDebugOptionsForTest() const override {\n@@ -70,7 +72,7 @@ class GpuFusibleTest : public HloHardwareIndependentTestBase {\n \n   FusionDecision IsProducerMultiOutputFusible(\n       const HloInstruction& producer) const {\n-    return ::xla::gpu::IsProducerMultiOutputFusible(producer,\n+    return ::xla::gpu::IsProducerMultiOutputFusible(producer, alias_info_.get(),\n                                                     device_description_);\n   }\n \n@@ -97,6 +99,7 @@ class GpuFusibleTest : public HloHardwareIndependentTestBase {\n \n  private:\n   se::DeviceDescription device_description_;\n+  std::unique_ptr<GpuAliasInfo> alias_info_;\n };\n \n const char kModulePrefix[] = R\"("
        },
        {
            "sha": "bd069a29384d5576ac5dcd03da04352f9c2b2d23",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -1869,6 +1869,7 @@ cc_library(\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_graph_dumper\",\n         \"//xla/service:instruction_fusion\",\n+        \"//xla/service/gpu:alias_info\",\n         \"//xla/service/gpu:gpu_fusible\",\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n         \"//xla/service/gpu/model:gpu_performance_model\",\n@@ -1904,8 +1905,10 @@ xla_cc_test(\n         \"//xla/hlo/testlib:pattern_matcher_gmock\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:pattern_matcher\",\n+        \"//xla/service/gpu:alias_info\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:gpu_fusible\",\n+        \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:IR\",\n@@ -2524,6 +2527,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:instruction_fusion\",\n+        \"//xla/service/gpu:alias_info\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:fusion_pipeline\",\n         \"//xla/service/gpu:ir_emission_utils\",\n@@ -2564,6 +2568,7 @@ xla_cc_test(\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:instruction_fusion\",\n         \"//xla/service:pattern_matcher\",\n+        \"//xla/service/gpu:alias_info\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/stream_executor:device_description\","
        },
        {
            "sha": "fb5ea23e9c09fb19acdc59b9e382364e74c98590",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n@@ -230,7 +231,7 @@ FusionDecision ProducerCandidateIsFusible(\n std::vector<HloInstruction*> GetProducerConsumerMultiOutputFusionCandidates(\n     const HloInstruction* producer, const HloDfsReachability& reachability,\n     FusionInfoCache* fusion_info_cache,\n-    const se::DeviceDescription& device_info,\n+    const se::DeviceDescription& device_info, const GpuAliasInfo* alias_info,\n     GpuPerformanceModelOwning& gpu_performance_model,\n     GpuHloCostAnalysis* cost_analysis) {\n   std::vector<HloInstruction*> fusion_candidates;\n@@ -241,7 +242,7 @@ std::vector<HloInstruction*> GetProducerConsumerMultiOutputFusionCandidates(\n \n   // If the producer is not a valid candidate for MOF, no need to check any of\n   // its users.\n-  if (!IsProducerMultiOutputFusible(*producer, device_info)) {\n+  if (!IsProducerMultiOutputFusible(*producer, alias_info, device_info)) {\n     return fusion_candidates;\n   }\n \n@@ -453,7 +454,7 @@ absl::StatusOr<bool> MultiOutputFusion::DoMultiOutputFusion() {\n     // multi-output fusion will occur before the current op in the order of\n     // traversal, and hence, not get into the way of subsequent fusion attempts.\n     const auto candidates = GetProducerConsumerMultiOutputFusionCandidates(\n-        producer, *reachability_, &fusion_info_cache, device_info_,\n+        producer, *reachability_, &fusion_info_cache, device_info_, alias_info_,\n         gpu_performance_model, &cost_analysis);\n     auto* consumer_for_fusion = SelectPreferredFusionCandidate(candidates);\n     if (consumer_for_fusion == nullptr) {"
        },
        {
            "sha": "593112a91ff38f4c1ba1ffa7b442c4067c17270e",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.h",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -96,10 +97,11 @@ namespace gpu {\n class MultiOutputFusion : public HloModulePass {\n  public:\n   explicit MultiOutputFusion(\n-      const se::DeviceDescription& device_info,\n+      const se::DeviceDescription& device_info, const GpuAliasInfo* alias_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n       mlir::MLIRContext* mlir_context)\n       : device_info_(device_info),\n+        alias_info_(alias_info),\n         shape_size_function_(shape_size_function),\n         mlir_context_(mlir_context) {}\n \n@@ -129,6 +131,7 @@ class MultiOutputFusion : public HloModulePass {\n   std::unique_ptr<HloDfsReachability> reachability_;\n \n   se::DeviceDescription device_info_;\n+  const GpuAliasInfo* alias_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_function_;\n   mlir::MLIRContext* mlir_context_;\n };"
        },
        {
            "sha": "70639728160517aff8474bbfb6e9fbce33ba7bcf",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -30,12 +30,14 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -46,14 +48,16 @@ namespace m = ::xla::match;\n \n class MultiOutputFusionTest : public HloHardwareIndependentTestBase {\n  public:\n-  MultiOutputFusion mof_{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n+  se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n+  GpuAliasInfo alias_info_{device_info_};\n+  MultiOutputFusion mof_{device_info_, &alias_info_,\n                          HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n \n   void CheckMultiOutputFusion(absl::string_view hlo,\n                               std::optional<absl::string_view> expected) {\n     RunAndFilecheckHloRewrite(\n         hlo,\n-        MultiOutputFusion{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n+        MultiOutputFusion{device_info_, &alias_info_,\n                           HloCostAnalysis::DefaultShapeSize, &mlir_context_},\n         expected);\n   }"
        },
        {
            "sha": "ef7b6269cacb51980b2a6d2381c728605a23379f",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 16,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/layout_util.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/fusion_pipeline.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n@@ -269,7 +270,7 @@ absl::StatusOr<HloFusionInstruction*> MakeFusionForDiamond(\n absl::Status RunFusionPipeline(\n     HloModule* module, const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    MLIRContext* mlir_context) {\n+    const GpuAliasInfo* alias_info, MLIRContext* mlir_context) {\n   HloPassPipeline reduction_pipeline(\"reduction_pipeline\");\n   // Passes that run after SoftmaxRewriterTriton and before PriorityFusion and\n   // transform reductions.\n@@ -282,7 +283,8 @@ absl::Status RunFusionPipeline(\n   TF_RETURN_IF_ERROR(reduction_pipeline.Run(module).status());\n \n   return FusionPipeline(module->config().debug_options(), shape_size,\n-                        /*thread_pool=*/nullptr, device_info, mlir_context)\n+                        alias_info, /*thread_pool=*/nullptr, device_info,\n+                        mlir_context)\n       .Run(module)\n       .status();\n }\n@@ -301,14 +303,14 @@ EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n     const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    MLIRContext* mlir_context) {\n+    const GpuAliasInfo* alias_info, MLIRContext* mlir_context) {\n   auto new_module = ExtractComputationIntoNewModule(\n       *fusion->fused_instructions_computation());\n \n   // After this call, the `new_module` will have instruction fused without\n   // SoftmaxRewriterTriton.\n   TF_RETURN_IF_ERROR(RunFusionPipeline(new_module.get(), device_info,\n-                                       shape_size, mlir_context));\n+                                       shape_size, alias_info, mlir_context));\n \n   VLOG(3) << \"priority fusion module: \" << new_module->ToString();\n \n@@ -348,7 +350,8 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n+    const GpuAliasInfo* alias_info, MLIRContext* mlir_context,\n+    bool use_cost_model_to_evaluate_fusions) {\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(normalization_fusion);\n \n   TF_ASSIGN_OR_RETURN(\n@@ -365,10 +368,10 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n       std::get<TiledRunTimeData>(std::move(tiled_runtime_data_or));\n \n   if (use_cost_model_to_evaluate_fusions) {\n-    TF_ASSIGN_OR_RETURN(\n-        absl::Duration run_time_without_softmax_rewriter,\n-        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n-            normalization_fusion, device_info, shape_size, mlir_context));\n+    TF_ASSIGN_OR_RETURN(absl::Duration run_time_without_softmax_rewriter,\n+                        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n+                            normalization_fusion, device_info, shape_size,\n+                            alias_info, mlir_context));\n \n     VLOG(2) << \"run time estimate if normalization diamond fused together: \"\n             << tiled_runtime_data.runtime_data.exec_time;\n@@ -400,18 +403,19 @@ absl::StatusOr<bool> MaybeFuseDiamondImpl(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n+    const GpuAliasInfo* alias_info, MLIRContext* mlir_context,\n+    bool use_cost_model_to_evaluate_fusions) {\n   TF_ASSIGN_OR_RETURN(HloFusionInstruction * normalization_fusion,\n                       MakeFusionForDiamond(diamond));\n   HloInstruction* root = diamond.root;\n \n   VLOG(2) << \"MaybeFuseDiamondImpl: \" << normalization_fusion->ToString();\n \n-  TF_ASSIGN_OR_RETURN(\n-      FusionDecision fusion_decision,\n-      DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n-          normalization_fusion, indexing_performance_model, device_info,\n-          shape_size, mlir_context, use_cost_model_to_evaluate_fusions));\n+  TF_ASSIGN_OR_RETURN(FusionDecision fusion_decision,\n+                      DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n+                          normalization_fusion, indexing_performance_model,\n+                          device_info, shape_size, alias_info, mlir_context,\n+                          use_cost_model_to_evaluate_fusions));\n \n   if (!fusion_decision.CanFuse()) {\n     VLOG(2) << \"Not fusing: \" << fusion_decision.Explain();\n@@ -639,7 +643,7 @@ absl::StatusOr<bool> SoftmaxRewriterTriton::MaybeFuseNormalizationDiamond(\n       &device_info_, &fusion_analysis_cache, shape_size_, mlir_context_);\n \n   return MaybeFuseDiamondImpl(diamond, indexing_performance_model, device_info_,\n-                              shape_size_, mlir_context_,\n+                              shape_size_, alias_info_, mlir_context_,\n                               use_cost_model_to_evaluate_fusions_);\n }\n "
        },
        {
            "sha": "315623af25a736a1ef9d16d5c785de450ec3f041",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -51,10 +52,12 @@ class SoftmaxRewriterTriton : public HloModulePass {\n  public:\n   explicit SoftmaxRewriterTriton(const se::DeviceDescription& device_info,\n                                  HloCostAnalysis::ShapeSizeFunction shape_size,\n+                                 const GpuAliasInfo* alias_info,\n                                  mlir::MLIRContext* mlir_context,\n                                  bool only_fuse_if_profitable = false)\n       : device_info_(device_info),\n         shape_size_(shape_size),\n+        alias_info_(alias_info),\n         use_cost_model_to_evaluate_fusions_(only_fuse_if_profitable),\n         mlir_context_(mlir_context) {}\n \n@@ -105,6 +108,7 @@ class SoftmaxRewriterTriton : public HloModulePass {\n  private:\n   const se::DeviceDescription& device_info_;\n   const HloCostAnalysis::ShapeSizeFunction shape_size_;\n+  const GpuAliasInfo* alias_info_;\n   bool use_cost_model_to_evaluate_fusions_;\n   mlir::MLIRContext* mlir_context_;\n };"
        },
        {
            "sha": "f50392dc9d638538c1a1dc1c6a2ca2b992aab3b8",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 9,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a9692818abe738e9290fad936714bb839841fa3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc?ref=8a9692818abe738e9290fad936714bb839841fa3",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -64,8 +65,10 @@ class SoftmaxRewriterTritonTest\n  protected:\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   mlir::MLIRContext mlir_context_;\n-  SoftmaxRewriterTriton fusion_rewriter_{\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n+  GpuAliasInfo alias_info_{device_info_};\n+  SoftmaxRewriterTriton fusion_rewriter_{device_info_,\n+                                         HloCostAnalysis::DefaultShapeSize,\n+                                         &alias_info_, &mlir_context_};\n };\n \n TEST_F(SoftmaxRewriterTritonTest, CanFuseSingleNormalizationF32) {\n@@ -565,7 +568,7 @@ ENTRY main {\n       SoftmaxRewriterTriton(\n           TestGpuDeviceInfo::RTXA6000DeviceInfo(\n               se::CudaComputeCapability{se::CudaComputeCapability::kVolta, 0}),\n-          HloCostAnalysis::DefaultShapeSize, &mlir_context_)\n+          HloCostAnalysis::DefaultShapeSize, &alias_info_, &mlir_context_)\n           .Run(module.get()),\n       absl_testing::StatusIs(\n           tsl::error::FAILED_PRECONDITION,\n@@ -594,7 +597,7 @@ ENTRY main {\n \n   EXPECT_TRUE(SoftmaxRewriterTriton(TestGpuDeviceInfo::AMDMI210DeviceInfo(),\n                                     HloCostAnalysis::DefaultShapeSize,\n-                                    &mlir_context_)\n+                                    &alias_info_, &mlir_context_)\n                   .Run(module.get())\n                   .ok());\n }\n@@ -679,8 +682,9 @@ ENTRY main {\n }\n )\";\n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n-  SoftmaxRewriterTriton fusion_rewriter(\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n+  SoftmaxRewriterTriton fusion_rewriter(device_info_,\n+                                        HloCostAnalysis::DefaultShapeSize,\n+                                        &alias_info_, &mlir_context_);\n   EXPECT_FALSE(fusion_rewriter_.Run(module.get()).value());\n }\n \n@@ -827,7 +831,8 @@ ENTRY main {\n \n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n   SoftmaxRewriterTriton softmax_rewriter_triton(\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &alias_info_,\n+      &mlir_context_);\n   int unmatched = 0, matched = 0;\n   for (HloInstruction* instruction :\n        module->entry_computation()->MakeInstructionPostOrder()) {\n@@ -1082,7 +1087,8 @@ ENTRY main {\n     // Verify that SoftmaxRewriterTriton without Cost Model will fuse the\n     // normalization diamond.\n     SoftmaxRewriterTriton fusion_rewriter_without_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n+        device_info_, HloCostAnalysis::DefaultShapeSize, &alias_info_,\n+        &mlir_context_,\n         /*only_fuse_if_profitable=*/false};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n@@ -1097,7 +1103,8 @@ ENTRY main {\n     // SoftmaxRewriterTriton with Cost Model will discard the normalization\n     // diamond, because row size is too large.\n     SoftmaxRewriterTriton fusion_rewriter_with_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n+        device_info_, HloCostAnalysis::DefaultShapeSize, &alias_info_,\n+        &mlir_context_,\n         /*only_fuse_if_profitable=*/true};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();"
        }
    ],
    "stats": {
        "total": 171,
        "additions": 114,
        "deletions": 57
    }
}