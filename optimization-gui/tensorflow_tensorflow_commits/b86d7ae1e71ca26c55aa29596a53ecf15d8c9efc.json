{
    "author": "IllogicalMoose",
    "message": "Disable failing XLA tests on the b200 backend.\n\nPiperOrigin-RevId: 807384671",
    "sha": "b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
    "files": [
        {
            "sha": "f0ea23e1a0a05800ec90a8e411e83169bad60e1c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -74,6 +74,10 @@ cc_library(\n xla_test(\n     name = \"block_level_emitter_test\",\n     srcs = [\"block_level_emitter_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\n         \"a100\",\n         \"h100\",\n@@ -517,6 +521,10 @@ cc_library(\n xla_test(\n     name = \"triton_test\",\n     srcs = [\"triton_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\n         \"a100\",\n         \"h100\",\n@@ -606,6 +614,10 @@ cc_library(\n xla_test(\n     name = \"native_emitter_test\",\n     srcs = [\"native_emitter_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\n         \"a100\",\n         \"h100\","
        },
        {
            "sha": "ff9d4b3083b7132ea179f9588df5ad3101c58131",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -79,6 +79,10 @@ cc_library(\n xla_test(\n     name = \"cudnn_test\",\n     srcs = [\"cudnn_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     tags = [\"cuda-only\"],\n     deps = ["
        },
        {
            "sha": "292aa60b478fec72190afd72cb567e8970d87d46",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -932,6 +932,10 @@ xla_test(\n xla_test(\n     name = \"fusion_emitter_parametrized_legacy_test\",\n     srcs = if_gpu_is_configured([\"fusion_emitter_parametrized_legacy_test.cc\"]),\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\n         \"a100\",\n         \"h100\","
        },
        {
            "sha": "26178d4ddc5334add65bedc9bd6f459d1904570d",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -461,6 +461,10 @@ cc_library(\n xla_test(\n     name = \"nccl_communicator_test\",\n     srcs = [\"nccl_communicator_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     local_defines =\n         if_rocm_is_configured(["
        },
        {
            "sha": "7ac2822fbcf9247bf28bf549efb28462721da360",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -845,6 +845,10 @@ cc_library(\n xla_test(\n     name = \"kernel_thunk_test\",\n     srcs = [\"kernel_thunk_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     deps = [\n         \":command_buffer_cmd\",\n@@ -2480,21 +2484,21 @@ xla_test(\n     srcs = [\"command_buffer_conversion_pass_test.cc\"],\n     backends = [\"gpu\"],\n     deps = [\n+        \":all_gather_thunk\",\n+        \":collective_thunk\",\n         \":command_buffer_conversion_pass\",\n+        \":command_buffer_thunk\",\n+        \":conditional_thunk\",\n+        \":copy_thunk\",\n+        \":cudnn_thunk\",\n+        \":custom_call_thunk\",\n+        \":gemm_thunk\",\n+        \":replica_id_thunk\",\n+        \":sequential_thunk\",\n+        \":thunk\",\n+        \":while_thunk\",\n         \"//xla:shape_util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n-        \"//xla/backends/gpu/runtime:all_gather_thunk\",\n-        \"//xla/backends/gpu/runtime:collective_thunk\",\n-        \"//xla/backends/gpu/runtime:command_buffer_thunk\",\n-        \"//xla/backends/gpu/runtime:conditional_thunk\",\n-        \"//xla/backends/gpu/runtime:copy_thunk\",\n-        \"//xla/backends/gpu/runtime:cudnn_thunk\",\n-        \"//xla/backends/gpu/runtime:custom_call_thunk\",\n-        \"//xla/backends/gpu/runtime:gemm_thunk\",\n-        \"//xla/backends/gpu/runtime:replica_id_thunk\",\n-        \"//xla/backends/gpu/runtime:sequential_thunk\",\n-        \"//xla/backends/gpu/runtime:thunk\",\n-        \"//xla/backends/gpu/runtime:while_thunk\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:hlo_module_config\","
        },
        {
            "sha": "5e850ec827f3ffd7ef7734d0f38885539ddbddfc",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -1720,6 +1720,10 @@ cc_library(\n xla_test(\n     name = \"gpu_compiler_test\",\n     srcs = [\"gpu_compiler_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     data = [\"gpu_compiler_test_autotune_db.textproto\"],\n     deps = [\n@@ -2004,6 +2008,10 @@ xla_test(\n     srcs = [\n         \"ptx_compilation_test.cc\",\n     ],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\n         \"gpu\",\n     ],\n@@ -2919,6 +2927,10 @@ xla_cc_test(\n xla_test(\n     name = \"determinism_test\",\n     srcs = [\"determinism_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     deps = [\n         \"//xla:literal\","
        },
        {
            "sha": "a3070a8db9e56bd25e117e249bda4787a6146e97",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -207,6 +207,10 @@ xla_test(\n     name = \"gemm_fusion_autotuner_test\",\n     timeout = \"long\",\n     srcs = [\"gemm_fusion_autotuner_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\n         \"a100\",\n         \"h100\","
        },
        {
            "sha": "dfc52f008f410077ea9d22cf541b69b8000305b7",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -207,6 +207,10 @@ xla_test(\n     name = \"gpu_spmd_e2e_compile_test\",\n     size = \"small\",\n     srcs = [\"gpu_spmd_e2e_compile_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     deps = [\n         \":gpu_codegen_test\",\n@@ -302,6 +306,10 @@ xla_test(\n     srcs = [\n         \"gpu_compilation_parallelism_test.cc\",\n     ],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     deps = [\n         \":gpu_codegen_test\",\n@@ -315,6 +323,10 @@ xla_test(\n xla_test(\n     name = \"gpu_copy_test\",\n     srcs = [\"gpu_copy_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     deps = [\n         \":gpu_codegen_test\",\n@@ -796,6 +808,10 @@ xla_test(\n xla_test(\n     name = \"gpu_fused_mha_test\",\n     srcs = [\"gpu_fused_mha_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\n         \"a100\",\n         \"h100\","
        },
        {
            "sha": "1b245903c4c9a9767d4a65bcc8aec9a36b8f0678",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -567,6 +567,10 @@ xla_cc_test(\n xla_test(\n     name = \"gpu_kernel_test\",\n     srcs = [\"gpu_kernel_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     deps = [\n         \":gpu_test_kernels\","
        },
        {
            "sha": "8c2d5f8af2d984e89f55847251e9ceec8c3d6ffc",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=b86d7ae1e71ca26c55aa29596a53ecf15d8c9efc",
            "patch": "@@ -1008,6 +1008,10 @@ xla_test(\n     name = \"dot_operation_test\",\n     timeout = \"long\",\n     srcs = [\"dot_operation_test.cc\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     shard_count = 20,\n     tags = [\n         \"optonly\",\n@@ -1064,6 +1068,10 @@ xla_test(\n     name = \"dot_operation_test_autotune_disabled\",\n     srcs = [\"dot_operation_test.cc\"],\n     args = [\"--xla_gpu_autotune_level=0\"],\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     backends = [\"gpu\"],\n     shard_count = 20,\n     tags = [\n@@ -1273,6 +1281,10 @@ xla_test(\n             \"--xla_cpu_multi_thread_eigen=false\",\n         ],\n     },\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n     shard_count = 20,\n     tags = [\n         \"optonly\","
        }
    ],
    "stats": {
        "total": 100,
        "additions": 88,
        "deletions": 12
    }
}