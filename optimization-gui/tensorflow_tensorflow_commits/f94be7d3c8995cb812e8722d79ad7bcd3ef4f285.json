{
    "author": "loislo",
    "message": "[XLA:GPU] fix scaled-dot emitter for handling 3d tensors with batch dimension.\n\nThe initial test set for scaled-dot missed to cover the test case with batch dims.\nIn this cl we fix that. We test that optimized hlo still has scaled-dot instruction and the final triton ir got dot_scaled instruction.\n\nIt:\na) drops the special handling of tile dims for scaled-dot op.\nb) adds a new test case for 3D scaled-dot operations.\nc) adds `ReshapeOp` and `TransOp` to `TritonXLAConvertUnsupportedTypesPass`.\nPiperOrigin-RevId: 815045762",
    "sha": "f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
    "files": [
        {
            "sha": "31cda4fbc3683e9a04bd6301b7fa812d72ced034",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
            "patch": "@@ -32,7 +32,7 @@ namespace xla::gpu {\n void CreateTritonXlaPipeline(\n     mlir::OpPassManager* pm,\n     const stream_executor::GpuComputeCapability& gpu_cc, bool rewrite_int4,\n-    bool allow_tma, bool convert_unsupported_types) {\n+    bool allow_tma) {\n   pm->addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n \n@@ -43,10 +43,6 @@ void CreateTritonXlaPipeline(\n     pm->addPass(mlir::triton::xla::CreateInt4ToPackedInt4RewritePass(\n         /*enable_bf16x2=*/is_at_least_hopper));\n   }\n-  if (convert_unsupported_types) {\n-    pm->addPass(\n-        mlir::triton::xla::CreateTritonXLAConvertUnsupportedTypesPass());\n-  }\n \n   pm->addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n       /*allow_tma=*/allow_tma && is_at_least_hopper));"
        },
        {
            "sha": "e43939888106f3da4b37adf9549bda8eab4ba106",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h?ref=f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
            "patch": "@@ -26,7 +26,7 @@ namespace xla::gpu {\n void CreateTritonXlaPipeline(\n     mlir::OpPassManager* pm,\n     const stream_executor::GpuComputeCapability& gpu_cc, bool rewrite_int4,\n-    bool allow_tma, bool convert_unsupported_types);\n+    bool allow_tma);\n \n // Creates a Triton compilation pipeline.\n //"
        },
        {
            "sha": "8e341120d06fd0ffd837ea81285069b195842938",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc?ref=f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
            "patch": "@@ -39,8 +39,7 @@ TEST(CompilationPipelineTest, UnswitchLoopsAfterLICM) {\n   mlir::PassManager pm(&ctx);\n \n   CreateTritonXlaPipeline(&pm, stream_executor::CudaComputeCapability(),\n-                          /*rewrite_int4=*/false, /*allow_tma=*/true,\n-                          /*convert_unsupported_types=*/true);\n+                          /*rewrite_int4=*/false, /*allow_tma=*/true);\n \n   std::vector<std::string> pass_names;\n   for (const mlir::Pass& pass : pm.getPasses()) {"
        },
        {
            "sha": "53d917aae793e53444f46ae340ed6510a91a4d14",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 24,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
            "patch": "@@ -869,11 +869,11 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n // Returns `shape` without all its unit dimensions, as well as the index of the\n // remaining dimensions in the original `shape`.\n std::pair<SmallVector<int64_t>, SmallVector<int64_t>> CollapseUnitDims(\n-    llvm::ArrayRef<int64_t> shape, bool scaled_dot = false) {\n+    llvm::ArrayRef<int64_t> shape) {\n   SmallVector<int64_t> shape_without_unit_dims;\n   SmallVector<int64_t> non_unit_dims_indices;\n   for (auto [i, size] : llvm::enumerate(shape)) {\n-    if (size != 1 || scaled_dot) {\n+    if (size != 1) {\n       shape_without_unit_dims.push_back(size);\n       non_unit_dims_indices.push_back(i);\n     }\n@@ -892,12 +892,11 @@ enum class DotOperandSide { kLhs, kRhs };\n absl::StatusOr<Value> CanonicalizeDotOperand(EmitterLocOpBuilder b,\n                                              Value operand,\n                                              int64_t contracting_dim_idx,\n-                                             DotOperandSide side,\n-                                             bool scaled_dot = false) {\n+                                             DotOperandSide side) {\n   llvm::ArrayRef<int64_t> shape =\n       mlir::cast<ShapedType>(operand.getType()).getShape();\n   auto [shape_without_unit_dims, non_unit_dims_indices] =\n-      CollapseUnitDims(shape, scaled_dot);\n+      CollapseUnitDims(shape);\n \n   if (shape_without_unit_dims.size() != 2) {\n     return absl::FailedPreconditionError(\n@@ -1111,19 +1110,23 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n   SmallVector<int64_t> padded_tile_sizes =\n       GetPaddedTileSizes(tiled_hlo_dot.tile_sizes());\n \n+  SmallVector<int64_t, 2> padded_tile_sizes_no_unit_dims =\n+      CollapseUnitDims(padded_tile_sizes).first;\n+\n   // Sanity check: Triton historically did not support non-2D dots (and still\n   // doesn't support arbitrary nD dots), so we require that the dot is tiled\n   // with exactly two non-unit tile sizes. This anyway matches the hardware's\n   // expectations, so seems like a reasonable requirement.\n   // TODO(b/393299275): this needs to be enforced in tiling.\n-  if (padded_tile_sizes.size() != 2) {\n+  if (padded_tile_sizes_no_unit_dims.size() != 2) {\n     return absl::FailedPreconditionError(\n         \"Expected dot to be tiled with exactly two non-unit tile sizes\");\n   }\n \n   Type accumulator_type = b.getF32Type();\n   Value accumulator =\n-      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes).UnwrapTensor();\n+      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims)\n+          .UnwrapTensor();\n \n   TF_ASSIGN_OR_RETURN(int64_t loop_iteration_count,\n                       GetDotLoopIterationCount(tiled_hlo_dot));\n@@ -1199,20 +1202,18 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n \n     // Canonicalize the dot operands to match Triton's/the hardware's\n     // expectations.\n+    TF_ASSIGN_OR_RETURN(lhs,\n+                        CanonicalizeDotOperand(b, lhs, lhs_contracting_dim_idx,\n+                                               DotOperandSide::kLhs));\n     TF_ASSIGN_OR_RETURN(\n-        lhs, CanonicalizeDotOperand(b, lhs, lhs_contracting_dim_idx,\n-                                    DotOperandSide::kLhs, /*scaled_dot=*/true));\n-    TF_ASSIGN_OR_RETURN(\n-        lhs_scale,\n-        CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n-                               DotOperandSide::kLhs, /*scaled_dot=*/true));\n-    TF_ASSIGN_OR_RETURN(\n-        rhs, CanonicalizeDotOperand(b, rhs, rhs_contracting_dim_idx,\n-                                    DotOperandSide::kRhs, /*scaled_dot=*/true));\n+        lhs_scale, CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n+                                          DotOperandSide::kLhs));\n+    TF_ASSIGN_OR_RETURN(rhs,\n+                        CanonicalizeDotOperand(b, rhs, rhs_contracting_dim_idx,\n+                                               DotOperandSide::kRhs));\n     TF_ASSIGN_OR_RETURN(\n-        rhs_scale,\n-        CanonicalizeDotOperand(b, rhs_scale, rhs_contracting_dim_idx,\n-                               DotOperandSide::kRhs, /*scaled_dot=*/true));\n+        rhs_scale, CanonicalizeDotOperand(b, rhs_scale, rhs_contracting_dim_idx,\n+                                          DotOperandSide::kRhs));\n \n     TF_ASSIGN_OR_RETURN(\n         Value acc_next,\n@@ -1232,6 +1233,13 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n     result = Cast(b, result, dot_output_type);\n   }\n \n+  if (padded_tile_sizes.size() != padded_tile_sizes_no_unit_dims.size()) {\n+    TF_ASSIGN_OR_RETURN(\n+        ScalarOrTensor wrapped_result,\n+        EmitTiledReshape(b, padded_tile_sizes, ScalarOrTensor(result)));\n+    result = wrapped_result.UnwrapTensor();\n+  }\n+\n   return ScalarOrTensor(result);\n }\n \n@@ -2042,6 +2050,17 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n         ExtractInstructionIntoNewModule(*fusion)->ToString());\n   }\n \n+  if (debug_options.xla_gpu_experimental_scaled_dot_with_triton()) {\n+    // Convert unsupported types before verification.\n+    mlir::PassManager pm(&mlir_context);\n+    pm.addPass(mlir::triton::xla::CreateTritonXLAConvertUnsupportedTypesPass());\n+    if (mlir::failed(pm.run(triton_module.get()))) {\n+      return CreateInternalError(\n+          \"Failed to fix unsupported types in Triton module for fusion:\",\n+          fusion, *triton_module);\n+    }\n+  }\n+\n   if (mlir::failed(mlir::verify(*triton_module))) {\n     return CreateInternalError(\n         \"Failed to verify Triton module for fusion:\", fusion, *triton_module);\n@@ -2177,11 +2196,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   }\n \n   CreateTritonXlaPipeline(&pm, gpu_cc, /*rewrite_int4=*/is_xla_fusion,\n-                          block_level_parameters.is_tma_allowed,\n-                          /*convert_unsupported_types=*/\n-                          hlo_module.config()\n-                              .debug_options()\n-                              .xla_gpu_experimental_scaled_dot_with_triton());\n+                          block_level_parameters.is_tma_allowed);\n \n   int num_warps = block_level_parameters.num_warps;\n   int num_ctas = block_level_parameters.num_ctas;"
        },
        {
            "sha": "3760183d32da4ae35a4108983fbaf9c9bb88faee",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 97,
            "deletions": 6,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
            "patch": "@@ -43,7 +43,9 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/literal.h\"\n@@ -82,6 +84,12 @@ class TritonEmitterTest : public GpuCodegenTest {\n         ->GetDeviceDescription()\n         .gpu_compute_capability();\n   }\n+  stream_executor::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n };\n \n class TmaParameterizedTritonEmitterTest\n@@ -3981,12 +3989,6 @@ class TritonScaledDotGemmTest\n         DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n     return debug_options;\n   }\n-  stream_executor::CudaComputeCapability GetCudaComputeCapability() {\n-    return backend()\n-        .default_stream_executor()\n-        ->GetDeviceDescription()\n-        .cuda_compute_capability();\n-  }\n };\n \n TEST_P(TritonScaledDotGemmTest,\n@@ -4174,6 +4176,95 @@ INSTANTIATE_TEST_SUITE_P(\n                                          \"bf16[128,256]\", \"f8E5M2\"}),\n     ScaleDotTestParams::ToString);\n \n+class TritonScaledDotTest : public TritonEmitterTest {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = TritonEmitterTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n+    debug_options.set_xla_gpu_autotune_level(0);\n+    debug_options.set_xla_gpu_cublas_fallback(false);\n+    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n+    return debug_options;\n+  }\n+\n+  HloComputation* GetFirstComputationWithInstruction(const HloModule& module,\n+                                                     HloOpcode opcode) const {\n+    for (const auto& computation : module.computations()) {\n+      for (const auto& instruction : computation->instructions()) {\n+        if (instruction->opcode() == opcode) {\n+          return computation;\n+        }\n+      }\n+    }\n+    return nullptr;\n+  }\n+};\n+\n+TEST_F(TritonScaledDotTest, ScaledDotWithBatchGetFusedAndExecutedCorrectly) {\n+  if (!GetCudaComputeCapability().IsAtLeastHopper()) {\n+    GTEST_SKIP() << \"Skipping test for pre-Hopper GPUs.\";\n+  }\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule ScaledDotWithBatchGetFusedAndExecutedCorrectly\n+\n+ENTRY e {\n+  lhs = f8e4m3fn[3,128,128] parameter(0)\n+  lhs_scale = f8e8m0fnu[3,128,4] parameter(1)\n+  rhs = f8e4m3fn[3,128,128] parameter(2)\n+  rhs_scale = f8e8m0fnu[3,128,4 ] parameter(3)\n+  ROOT _ = bf16[3,128,128] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_batch_dims={0},\n+    rhs_batch_dims={0},\n+    lhs_contracting_dims={2},\n+    rhs_contracting_dims={2}\n+}\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloTextTemplate));\n+  TF_ASSERT_OK_AND_ASSIGN(auto optimized_module,\n+                          GetOptimizedModule(std::move(module)));\n+  constexpr absl::string_view kExpectedOptimizedHLO = R\"(\n+    CHECK: fusion\n+    CHECK: ROOT {{.*}} scaled-dot\n+    CHECK: ENTRY\n+    CHECK: __triton_nested_gemm_fusion\n+  )\";\n+  EXPECT_THAT(RunFileCheck(optimized_module->ToString(), kExpectedOptimizedHLO),\n+              true);\n+  for (const auto& computation : optimized_module->computations()) {\n+    for (const auto& instruction : computation->instructions()) {\n+      if (instruction->opcode() == HloOpcode::kScaledDot) {\n+        LOG(INFO) << \"Instruction: \" << instruction->name();\n+      }\n+    }\n+  }\n+\n+  HloComputation* scaled_dot_computation = GetFirstComputationWithInstruction(\n+      *optimized_module, HloOpcode::kScaledDot);\n+  constexpr absl::string_view kExpectedTritonIr = R\"(\n+      CHECK: tt.dot_scaled\n+      CHECK: tensor<16x128xf8E4M3FN>, tensor<16x4xi8>\n+      CHECK: tensor<128x16xf8E4M3FN>, tensor<16x4xi8>\n+      CHECK: -> tensor<16x16xf32>\n+  )\";\n+  EXPECT_THAT(CreateTritonIrAndFileCheck(*scaled_dot_computation,\n+                                         /*block_level_parameters=*/\n+                                         {\n+                                             {{1, 16, 16}},\n+                                             4,\n+                                             1,\n+                                             1,\n+                                             false,\n+                                         },\n+                                         kExpectedTritonIr),\n+              absl_testing::IsOk());\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "da132c9fffd89ae10b706b5b0ea385f7328d89d7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_convert_unsupported_types.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc?ref=f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace mlir::triton::xla {\n@@ -96,6 +97,8 @@ class TritonXLAConvertUnsupportedTypesPass\n     RewritePatternSet patterns(ctx);\n     patterns.add<GenericOpConversionPattern<ExtractOp>,\n                  GenericOpConversionPattern<InsertOp>,\n+                 GenericOpConversionPattern<ReshapeOp>,\n+                 GenericOpConversionPattern<TransOp>,\n                  GenericOpConversionPattern<arith::BitcastOp>>(converter, ctx);\n     scf::populateSCFStructuralTypeConversions(converter, patterns);\n     populateFunctionOpInterfaceTypeConversionPattern<func::FuncOp>(patterns,"
        },
        {
            "sha": "600280386a8a525c4e12bb1f4d2c6210865f4449",
            "filename": "third_party/xla/xla/service/gpu/tests/xla-opt.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f94be7d3c8995cb812e8722d79ad7bcd3ef4f285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc?ref=f94be7d3c8995cb812e8722d79ad7bcd3ef4f285",
            "patch": "@@ -47,8 +47,6 @@ struct TritonPipelineOptions\n   Option<std::string> target{*this, \"target\", llvm::cl::init(\"8.0\")};\n   Option<bool> rewrite_int4{*this, \"rewrite-int4\", llvm::cl::init(true)};\n   Option<bool> allow_tma{*this, \"allow-tma\", llvm::cl::init(false)};\n-  Option<bool> convert_unsupported_types{*this, \"convert-unsupported-types\",\n-                                         llvm::cl::init(true)};\n   Option<int> num_warps{*this, \"num-warps\", llvm::cl::init(4)};\n   Option<int> num_ctas{*this, \"num-ctas\", llvm::cl::init(1)};\n   Option<int> num_stages{*this, \"num-stages\", llvm::cl::init(3)};\n@@ -71,8 +69,7 @@ mlir::PassPipelineRegistration<TritonPipelineOptions>\n             gpu_cc = rocm_cc;\n           }\n           xla::gpu::CreateTritonXlaPipeline(&pm, gpu_cc, options.rewrite_int4,\n-                                            options.allow_tma,\n-                                            options.convert_unsupported_types);\n+                                            options.allow_tma);\n           xla::gpu::CreateTritonPipeline(&pm, gpu_cc, options.num_warps,\n                                          options.num_ctas, options.num_stages,\n                                          cluster_info);"
        }
    ],
    "stats": {
        "total": 185,
        "additions": 143,
        "deletions": 42
    }
}