{
    "author": "alankelly",
    "message": "Remove dynamically quantized FC and Conv2D from latest ops.\n\nYou might ask why dynamically quantized ops, which accumulate into int32 and not float, could have implementation dependent differences, since int accumulation is exact and commutative, unlike float. The differences come from how TFLite and XNNPack quantize the LHS of GEMM op. TFLite rounds to nearest with ties to Away. XNNPACK rounds to nearest with ties to even. IEEE 754 states: \"Round to nearest, ties to even\" is the default for binary floating point and the recommended default for decimal.\n\nPiperOrigin-RevId: 797657284",
    "sha": "99ca84c474c7d05d04e49eec5a2e6a27c0dc3137",
    "files": [
        {
            "sha": "d5b60bd83b702317dff803b3b2ffe51a50803f93",
            "filename": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/99ca84c474c7d05d04e49eec5a2e6a27c0dc3137/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fxnnpack_delegate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/99ca84c474c7d05d04e49eec5a2e6a27c0dc3137/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fxnnpack_delegate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fxnnpack_delegate.cc?ref=99ca84c474c7d05d04e49eec5a2e6a27c0dc3137",
            "patch": "@@ -3611,8 +3611,7 @@ class Subgraph {\n         logging_context, output_tensor, 4, node->outputs->data[0],\n         BuiltinOperator_CONV_2D, node_index));\n \n-    bool dynamically_quantized = (delegate.enable_latest_operators() &&\n-                                  (input_tensor.type == kTfLiteFloat32 &&\n+    bool dynamically_quantized = ((input_tensor.type == kTfLiteFloat32 &&\n                                    filter_tensor.type == kTfLiteInt8));\n     if (input_tensor.type != output_tensor.type ||\n         ((input_tensor.type != filter_tensor.type) && !dynamically_quantized)) {\n@@ -4533,8 +4532,7 @@ class Subgraph {\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, output_tensor,\n                                        node->outputs->data[0], node_index));\n \n-    bool dynamically_quantized = (delegate.enable_latest_operators() &&\n-                                  (input_tensor.type == kTfLiteFloat32 &&\n+    bool dynamically_quantized = ((input_tensor.type == kTfLiteFloat32 &&\n                                    (filter_tensor.type == kTfLiteInt4 ||\n                                     filter_tensor.type == kTfLiteInt8)));\n     bool supported_srq = (input_tensor.type == kTfLiteInt8 &&"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 2,
        "deletions": 4
    }
}