{
    "author": "gerlero",
    "message": "PR #32115: [XLA:CPU] Make rendezvous timeouts configurable via flags\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32115\n\nüìù Summary of Changes\nAs discussed in https://github.com/openxla/xla/pull/26921#issuecomment-3299642657, add configurable options for setting the timeout (and also the time before the \"stuck\" warning shows) when running in parallel on CPU.\n\nüéØ Justification\nConfigurable timeouts are required for longer parallel workloads with uneven work distributions (i.e., the inputs on some CPUs take longer to process than on others).\n\nNote: I've set the default values to follow the equivalent GPU case. These are a bit longer than the current hardcoded values, but it made sense to me to have the same defaults regardless of the device type.\n\nCloses https://github.com/jax-ml/jax/issues/31818\n\nüöÄ Kind of Contribution\nPlease remove what does not apply:\n‚ú® New Feature\n\nüìä Benchmark (for Performance Improvements)\nN/A\n\nüß™ Unit Tests:\nNone\n\nüß™ Execution Tests:\nNone\nCopybara import of the project:\n\n--\nbaf8dbeef0874ef4e77a2e6d27354e59e7948fb9 by Gabriel Gerlero <gerlero@users.noreply.github.com>:\n\n[XLA:CPU] Make rendezvous timeouts configurable via flags\n\nMerging this change closes #32115\n\nPiperOrigin-RevId: 814751310",
    "sha": "efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d",
    "files": [
        {
            "sha": "91644b74268c2b46fb26f3b3840b913909ec0e53",
            "filename": "third_party/xla/xla/backends/cpu/collectives/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD?ref=efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d",
            "patch": "@@ -133,6 +133,7 @@ cc_library(\n     hdrs = [\"in_process_communicator.h\"],\n     deps = [\n         \":cpu_collectives\",\n+        \"//xla:debug_options_flags\",\n         \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:util\","
        },
        {
            "sha": "698a11d6d4be7ce22213f55f1f998f7c24ebe06e",
            "filename": "third_party/xla/xla/backends/cpu/collectives/in_process_communicator.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 17,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc?ref=efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -37,6 +36,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/debug_options_flags.h\"\n #include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n@@ -51,8 +51,23 @@ limitations under the License.\n namespace xla::cpu {\n namespace {\n \n-static absl::Duration kWarnStuckTimeout = absl::Seconds(5);\n-static absl::Duration kTerminateTimeout = absl::Seconds(10);\n+static absl::Duration WarnStuckTimeout() {\n+  static const absl::Duration warn_stuck_timeout = []() {\n+    int64_t timeout = xla::GetDebugOptionsFromFlags()\n+                          .xla_cpu_collective_call_warn_stuck_seconds();\n+    return timeout >= 0 ? absl::Seconds(timeout) : absl::InfiniteDuration();\n+  }();\n+  return warn_stuck_timeout;\n+}\n+\n+static absl::Duration TerminateTimeout() {\n+  static const absl::Duration terminate_timeout = []() {\n+    int64_t timeout = xla::GetDebugOptionsFromFlags()\n+                          .xla_cpu_collective_call_terminate_timeout_seconds();\n+    return timeout >= 0 ? absl::Seconds(timeout) : absl::InfiniteDuration();\n+  }();\n+  return terminate_timeout;\n+}\n \n // In-process collective operation participants.\n //\n@@ -410,11 +425,11 @@ Future<> InProcessCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n   std::string name = absl::StrCat(\"all reduce \", key.ToString());\n   AllReduceParticipant partiticipant{rank_, send_buffer, recv_buffer};\n \n-  TF_ASSIGN_OR_RETURN(\n-      auto op, Rendezvous<OpParticipants<AllReduceParticipant>>(\n-                   name, key, partiticipant, key.num_local_participants,\n-                   CollectParticipants<AllReduceParticipant>, kWarnStuckTimeout,\n-                   kTerminateTimeout));\n+  TF_ASSIGN_OR_RETURN(auto op,\n+                      Rendezvous<OpParticipants<AllReduceParticipant>>(\n+                          name, key, partiticipant, key.num_local_participants,\n+                          CollectParticipants<AllReduceParticipant>,\n+                          WarnStuckTimeout(), TerminateTimeout()));\n \n   TF_RETURN_IF_ERROR(\n       op->Invoke(AllReduceOp, rank_, dtype, count, reduction_kind));\n@@ -437,7 +452,7 @@ Future<> InProcessCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n                       Rendezvous<OpParticipants<ReduceScatterParticipant>>(\n                           name, key, partiticipant, key.num_local_participants,\n                           CollectParticipants<ReduceScatterParticipant>,\n-                          kWarnStuckTimeout, kTerminateTimeout));\n+                          WarnStuckTimeout(), TerminateTimeout()));\n \n   TF_RETURN_IF_ERROR(\n       op->Invoke(ReduceScatterOp, rank_, dtype, count, reduction_kind));\n@@ -460,7 +475,7 @@ Future<> InProcessCommunicator::CollectivePermute(\n                       Rendezvous<OpParticipants<CollectivePermuteParticipant>>(\n                           name, key, partiticipant, key.num_local_participants,\n                           CollectParticipants<CollectivePermuteParticipant>,\n-                          kWarnStuckTimeout, kTerminateTimeout));\n+                          WarnStuckTimeout(), TerminateTimeout()));\n \n   size_t num_bytes = count * primitive_util::ByteWidth(dtype);\n \n@@ -484,8 +499,8 @@ Future<> InProcessCommunicator::AllToAll(\n   TF_ASSIGN_OR_RETURN(\n       auto op, Rendezvous<OpParticipants<AllToAllParticipant>>(\n                    name, key, partiticipant, key.num_local_participants,\n-                   CollectParticipants<AllToAllParticipant>, kWarnStuckTimeout,\n-                   kTerminateTimeout));\n+                   CollectParticipants<AllToAllParticipant>, WarnStuckTimeout(),\n+                   TerminateTimeout()));\n \n   size_t num_bytes = count * primitive_util::ByteWidth(dtype);\n \n@@ -504,11 +519,11 @@ Future<> InProcessCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n   std::string name = absl::StrCat(\"all gather \", key.ToString());\n   AllGatherParticipant partiticipant{rank_, send_buffer, recv_buffer};\n \n-  TF_ASSIGN_OR_RETURN(\n-      auto op, Rendezvous<OpParticipants<AllGatherParticipant>>(\n-                   name, key, partiticipant, key.num_local_participants,\n-                   CollectParticipants<AllGatherParticipant>, kWarnStuckTimeout,\n-                   kTerminateTimeout));\n+  TF_ASSIGN_OR_RETURN(auto op,\n+                      Rendezvous<OpParticipants<AllGatherParticipant>>(\n+                          name, key, partiticipant, key.num_local_participants,\n+                          CollectParticipants<AllGatherParticipant>,\n+                          WarnStuckTimeout(), TerminateTimeout()));\n \n   size_t num_bytes = count * primitive_util::ByteWidth(dtype);\n "
        },
        {
            "sha": "c1e46873977336012384a1efb7c6e9bdabc24227",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d",
            "patch": "@@ -452,6 +452,9 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n       DebugOptions::UNSTABLE_REDUCTION_DETECTION_MODE_NONE);\n   opts.set_xla_gpu_experimental_scaled_dot_with_triton(false);\n   opts.set_xla_gpu_experimental_use_raft_select_k(false);\n+\n+  opts.set_xla_cpu_collective_call_warn_stuck_seconds(20);\n+  opts.set_xla_cpu_collective_call_terminate_timeout_seconds(40);\n   return opts;\n }\n \n@@ -2563,6 +2566,19 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n           &DebugOptions::set_xla_gpu_experimental_scaled_dot_with_triton),\n       debug_options->xla_gpu_experimental_scaled_dot_with_triton(),\n       \"If true, use the Triton emitter for scaled dot.\"));\n+\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_cpu_collective_call_warn_stuck_timeout_seconds\",\n+      int32_setter_for(\n+          &DebugOptions::set_xla_cpu_collective_call_warn_stuck_seconds),\n+      debug_options->xla_cpu_collective_call_warn_stuck_seconds(),\n+      \"Set timeout for Collective Call Rendezvous stuck warning\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_cpu_collective_call_terminate_timeout_seconds\",\n+      int32_setter_for(\n+          &DebugOptions::set_xla_cpu_collective_call_terminate_timeout_seconds),\n+      debug_options->xla_cpu_collective_call_terminate_timeout_seconds(),\n+      \"Set timeout for Collective Call Rendezvous termination\"));\n }  // NOLINT(readability/fn_size)\n \n // Allocates flag_values and flag_objects; this function must not be called more"
        },
        {
            "sha": "ddb33ad82028cbc74beb432f46206a945b12b23b",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=efa0eaaf9260c2ff5e94ee3f22756824f2a2e50d",
            "patch": "@@ -162,6 +162,13 @@ message DebugOptions {\n     XNN_GRAPH_FUSION_MODE_BYPASS_COST_MODEL = 3;\n   }\n \n+  // The number of seconds to wait before terminating a rendezvous call\n+  optional int32 xla_cpu_collective_call_terminate_timeout_seconds = 417;\n+\n+  // The number of seconds to wait before warning about a rendezvous call that\n+  // has not yet timed out.\n+  optional int32 xla_cpu_collective_call_warn_stuck_seconds = 418;\n+\n   // Use region analysis in copy insertion pass.\n   optional bool xla_cpu_copy_insertion_use_region_analysis = 337;\n \n@@ -1351,7 +1358,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 417\n+  // Next id: 419\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 75,
        "additions": 57,
        "deletions": 18
    }
}