{
    "author": "ezhulenev",
    "message": "[xla:gpu] Switch CollectiveMetadataThunk to GpuCliqueRendezvous\n\nPiperOrigin-RevId: 841783861",
    "sha": "79cd71f875f583563fcbfeb04ad0660ed5f997fa",
    "files": [
        {
            "sha": "fcb87afc21d28e323f9e713a00cc6e73ac011d43",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_rendezvous.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.cc?ref=79cd71f875f583563fcbfeb04ad0660ed5f997fa",
            "patch": "@@ -65,8 +65,8 @@ struct RankFormatter {\n }  // namespace\n \n GpuCliqueRendezvous::GpuCliqueRendezvous(\n-    GpuCliqueKey clique_key, absl::btree_map<RankId, std::any> state)\n-    : clique_key_(std::move(clique_key)), state_(std::move(state)) {}\n+    GpuCliqueKey clique_key, absl::btree_map<RankId, std::any> values)\n+    : clique_key_(std::move(clique_key)), values_(std::move(values)) {}\n \n absl::StatusOr<std::shared_ptr<GpuCliqueRendezvous>> GpuCliqueRendezvous::Join(\n     const GpuCliqueKey& clique_key, RankId rank, std::any data) {"
        },
        {
            "sha": "623cd7d8513fd72e5d42c36268165ae3da59f027",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_rendezvous.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.h?ref=79cd71f875f583563fcbfeb04ad0660ed5f997fa",
            "patch": "@@ -42,15 +42,15 @@ class GpuCliqueRendezvous {\n   static absl::StatusOr<std::shared_ptr<GpuCliqueRendezvous>> Join(\n       const GpuCliqueKey& clique_key, RankId rank, std::any data);\n \n-  // Returns the clique key associated with this data.\n+  // Returns the clique key associated with this rendezvous object.\n   const GpuCliqueKey& clique_key() const { return clique_key_; }\n \n-  // Returns the state associated with the given rank. If state type is not\n-  // the same as `T`, returns an error.\n+  // Returns the value at the given rank. If value type is not the same as `T`,\n+  // returns an error.\n   template <typename T>\n-  absl::StatusOr<std::reference_wrapper<const T>> state(RankId rank) const {\n-    auto it = state_.find(rank);\n-    if (it == state_.end()) {\n+  absl::StatusOr<std::reference_wrapper<const T>> at(RankId rank) const {\n+    auto it = values_.find(rank);\n+    if (it == values_.end()) {\n       return NotFound(\"Data not found for rank %d\", rank.value());\n     }\n \n@@ -64,10 +64,10 @@ class GpuCliqueRendezvous {\n \n  private:\n   GpuCliqueRendezvous(GpuCliqueKey clique_key,\n-                      absl::btree_map<RankId, std::any> state);\n+                      absl::btree_map<RankId, std::any> values);\n \n   GpuCliqueKey clique_key_;\n-  absl::btree_map<RankId, std::any> state_;\n+  absl::btree_map<RankId, std::any> values_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "ef8a2ce09383c0a1d63a52fa116cbefbca39b384",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_rendezvous_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous_test.cc?ref=79cd71f875f583563fcbfeb04ad0660ed5f997fa",
            "patch": "@@ -48,8 +48,8 @@ TEST(GpuCliqueRendezvousTest, TwoParticipants) {\n \n       GpuCliqueRendezvous& data = **rendezvous;\n       ASSERT_EQ(data.clique_key(), key);\n-      ASSERT_EQ(*data.state<int32_t>(RankId(0)), 0);\n-      ASSERT_EQ(*data.state<int32_t>(RankId(1)), 1);\n+      ASSERT_EQ(*data.at<int32_t>(RankId(0)), 0);\n+      ASSERT_EQ(*data.at<int32_t>(RankId(1)), 1);\n     };\n   };\n "
        },
        {
            "sha": "c40f279f9212dd3d133b06fbbdb2ed762f1e3152",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=79cd71f875f583563fcbfeb04ad0660ed5f997fa",
            "patch": "@@ -2010,6 +2010,7 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_rendezvous\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:device_id\","
        },
        {
            "sha": "e65d7760e8981fbfc4f96b3d0af42c11ef85624f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc?ref=79cd71f875f583563fcbfeb04ad0660ed5f997fa",
            "patch": "@@ -394,7 +394,7 @@ TEST(CollectiveKernelThunkTest, MultiprocessTest) {\n   for (absl::StatusOr<se::DeviceAddressBase> result :\n        RunCollectiveKernelThunkOnDevices(metadata,\n                                          /*emulate_multiprocess=*/true)) {\n-    EXPECT_THAT(result, StatusIs(absl::StatusCode::kUnimplemented));\n+    EXPECT_THAT(result, StatusIs(absl::StatusCode::kInvalidArgument));\n   }\n }\n "
        },
        {
            "sha": "a5e44c890f34fee80cc44cca28ec6c9386df64cf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 82,
            "changes": 105,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79cd71f875f583563fcbfeb04ad0660ed5f997fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=79cd71f875f583563fcbfeb04ad0660ed5f997fa",
            "patch": "@@ -15,31 +15,27 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n \n+#include <any>\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <optional>\n-#include <string>\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"absl/types/span.h\"\n #include \"google/protobuf/repeated_ptr_field.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_rendezvous.h\"\n #include \"xla/backends/gpu/runtime/collective_multimem.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/layout.h\"\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/rendezvous.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n@@ -80,98 +76,43 @@ CollectiveConfig CollectiveMetadataThunk::GetCollectiveConfig(\n   return config;\n }\n \n-struct DeviceParameters {\n-  RankId rank;\n-  std::vector<se::DeviceAddressBase> parameters;\n-\n-  bool operator<(const DeviceParameters& other) const {\n-    return rank < other.rank;\n-  }\n-};\n-\n-absl::StatusOr<std::vector<DeviceParameters>> SyncLocalDeviceParameters(\n-    const GpuCliqueKey& clique_key, RankId rank,\n-    std::vector<se::DeviceAddressBase> parameters) {\n-  std::vector<DeviceParameters> device_parameters;\n-  auto rendezvous_fn = [](absl::Span<const DeviceParameters* const> values) {\n-    std::vector<DeviceParameters> values_copy;\n-    for (const auto& value : values) {\n-      values_copy.push_back(*value);\n-    }\n-    // Sort to make sure that values are in the same order as the\n-    // devices are ordered in the communicator.\n-    absl::c_sort(values_copy);\n-    return values_copy;\n-  };\n-\n-  std::string start_rendezvous_key = absl::StrFormat(\n-      \"[rank=%d] Initializing collective metadata for clique %s\", rank.value(),\n-      clique_key.ToString());\n-\n-  DeviceParameters params;\n-  params.rank = rank;\n-  params.parameters = std::move(parameters);\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::shared_ptr<std::vector<DeviceParameters>> local_ranks_parameters,\n-      Rendezvous<std::vector<DeviceParameters>>(\n-          /*name=*/start_rendezvous_key, /*key=*/clique_key,\n-          /*value=*/params,\n-          /*num_threads=*/clique_key.num_local_participants(), rendezvous_fn));\n-  return std::vector<DeviceParameters>(local_ranks_parameters->begin(),\n-                                       local_ranks_parameters->end());\n-}\n-\n-absl::StatusOr<std::vector<DeviceParameters>> SyncGlobalDeviceParameters(\n-    const GpuCliqueKey& clique_key, RankId rank,\n-    std::vector<se::DeviceAddressBase> parameters) {\n-  if (!clique_key.is_local()) {\n-    return Unimplemented(\n-        \"[rank=%d] Multiprocess collective metadata is not supported yet in \"\n-        \"clique %s\",\n-        rank.value(), clique_key.ToString());\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<DeviceParameters> local_ranks_parameters,\n-      SyncLocalDeviceParameters(clique_key, rank, std::move(parameters)));\n-\n-  return local_ranks_parameters;\n-}\n-\n absl::Status CollectiveMetadataThunk::ConstructCollectiveMetadata(\n     const GpuCliqueKey& clique_key, RankId rank, se::Stream* stream,\n     std::vector<se::DeviceAddressBase> parameters,\n     std::shared_ptr<CollectiveMultimem> multimem,\n     se::DeviceAddressBase destination) {\n-  CollectiveKernelMetadata metadata;\n-  metadata.rank = rank.value();\n-  metadata.multicast_buffer_ptr =\n-      multimem ? multimem->mapped_ptr(rank) : nullptr;\n+  size_t num_parameters = parameters.size();\n+\n+  using DeviceParameters = std::vector<se::DeviceAddressBase>;\n+\n+  // Exchange device parameters with all ranks in the clique.\n   TF_ASSIGN_OR_RETURN(\n-      std::vector<DeviceParameters> device_parameters,\n-      SyncGlobalDeviceParameters(clique_key, rank, std::move(parameters)));\n-  TF_RET_CHECK(!device_parameters.empty())\n-      << \"Not enough devices in the clique.\";\n-  const size_t num_parameters = device_parameters[0].parameters.size();\n-  for (const auto& value : device_parameters) {\n-    TF_RET_CHECK(value.parameters.size() == num_parameters);\n-  }\n+      auto device_parameters,\n+      GpuCliqueRendezvous::Join(clique_key, rank, std::move(parameters)));\n \n+  // Collect pointers to device buffers from all participating ranks.\n   std::vector<void*> param_to_peers_ptrs;\n-  param_to_peers_ptrs.reserve(device_parameters.size() * num_parameters);\n-  for (int peer = 0; peer < device_parameters.size(); ++peer) {\n-    for (int param = 0; param < num_parameters; ++param) {\n-      param_to_peers_ptrs.push_back(\n-          device_parameters[peer].parameters[param].opaque());\n+  for (auto peer = RankId(0); peer < RankId(clique_key.num_devices()); ++peer) {\n+    TF_ASSIGN_OR_RETURN(const DeviceParameters& peer_parameters,\n+                        device_parameters->at<DeviceParameters>(peer));\n+    for (se::DeviceAddressBase peer_parameter : peer_parameters) {\n+      param_to_peers_ptrs.push_back(peer_parameter.opaque());\n     }\n   }\n \n+  // Check that all participants have the same number of parameters.\n+  TF_RET_CHECK(param_to_peers_ptrs.size() ==\n+               num_parameters * clique_key.num_local_participants());\n+\n   const int64_t param_to_peers_ptrs_size =\n       param_to_peers_ptrs.size() * sizeof(void*);\n   se::DeviceAddressBase param_to_peers_ptrs_buffer = destination.GetByteSlice(\n       sizeof(CollectiveKernelMetadata), param_to_peers_ptrs_size);\n \n+  CollectiveKernelMetadata metadata;\n+  metadata.rank = rank.value();\n+  metadata.multicast_buffer_ptr =\n+      multimem ? multimem->mapped_ptr(rank) : nullptr;\n   metadata.param_to_peers =\n       reinterpret_cast<void**>(param_to_peers_ptrs_buffer.opaque());\n "
        }
    ],
    "stats": {
        "total": 132,
        "additions": 37,
        "deletions": 95
    }
}