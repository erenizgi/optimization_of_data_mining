{
    "author": "nurmukhametov",
    "message": "PR #35211: [ROCm] Reimplement register spilling detection\n\nImported from GitHub PR https://github.com/openxla/xla/pull/35211\n\nReplace amd_comgr library with LLVM's native API to find NT_AMDGPU_METADATA note sections and extract the stack usage and register spill counts from there.\n\nAdd detection for dynamic stack usage.\n\nAdd VLOG(2) dumps for per-kernel stats as well as register counts.\n\nChange the logic of discarding the module. The module is discarded only if the stack is used, i.e., either .private_segment_fixed_size is not zero or .uses_dynamic_stack is true. There are examples where there are SGPR spills, but they are saved to VGPRs and not to the stack.\n\nAdd tests in amdgpu_register_spilling_test.cc which cover cases where no spills, VGPR-only spills, SGPR-only spills, or dynamic stack usage occur. For that, the following LLVM IR inputs are added:\n- amdgpu_no_spills.ll: Simple kernel with minimal register usage\n- amdgpu_vgpr_spills.ll: High VGPR pressure with limited VGPRs (64)\n- amdgpu_sgpr_spills.ll: High SGPR pressure with limited SGPRs (32)\n- amdgpu_dynamic_stack.ll: Indirect function call requiring dynamic stack\nCopybara import of the project:\n\n--\nb83efc6a7addcfe617459280d1cea22cd8d0c4c8 by Aleksei Nurmukhametov <anurmukh@amd.com>:\n\n[ROCm] Reimplement register spilling detection\n\nReplace amd_comgr library with LLVM's native API to find\nNT_AMDGPU_METADATA note sections and extract the stack usage and\nregister spill counts from there.\n\nAdd detection for dynamic stack usage.\n\nAdd VLOG(2) dumps for per-kernel stats as well as register counts.\n\nChange the logic of discarding the module. The module is discarded only\nif the stack is used, i.e., either .private_segment_fixed_size is not\nzero or .uses_dynamic_stack is true. There are examples where there are\nSGPR spills, but they are saved to VGPRs and not to the stack.\n\nAdd tests in amdgpu_register_spilling_test.cc which cover cases where no\nspills, VGPR-only spills, SGPR-only spills, or dynamic stack usage\noccur. For that, the following LLVM IR inputs are added:\n- amdgpu_no_spills.ll: Simple kernel with minimal register usage\n- amdgpu_vgpr_spills.ll: High VGPR pressure with limited VGPRs (64)\n- amdgpu_sgpr_spills.ll: High SGPR pressure with limited SGPRs (32)\n- amdgpu_dynamic_stack.ll: Indirect function call requiring dynamic\n  stack\n\nMerging this change closes #35211\n\nPiperOrigin-RevId: 845742402",
    "sha": "a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
    "files": [
        {
            "sha": "de7d5421af6ffa44089269e8bf2909f91a6a38cb",
            "filename": "third_party/xla/third_party/gpus/rocm/BUILD.tpl",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fthird_party%2Fgpus%2Frocm%2FBUILD.tpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fthird_party%2Fgpus%2Frocm%2FBUILD.tpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fgpus%2Frocm%2FBUILD.tpl?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -593,7 +593,6 @@ alias(\n         threshold = 71000,\n         value = rocm_version_number(),\n     ),\n-    visibility = [\"//visibility:public\"],\n )\n \n cc_library("
        },
        {
            "sha": "fbd35f6609e59799f09e7dcec2b39b1e6cfe3b88",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/BUILD",
            "status": "modified",
            "additions": 30,
            "deletions": 2,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -199,6 +199,7 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@llvm-project//llvm:AMDGPUAsmParser\",  # buildcleaner: keep\n         \"@llvm-project//llvm:Analysis\",\n+        \"@llvm-project//llvm:BinaryFormat\",\n         \"@llvm-project//llvm:BitReader\",\n         \"@llvm-project//llvm:BitWriter\",\n         \"@llvm-project//llvm:CodeGen\",\n@@ -207,13 +208,12 @@ cc_library(\n         \"@llvm-project//llvm:Linker\",\n         \"@llvm-project//llvm:MC\",\n         \"@llvm-project//llvm:ObjCARC\",  # buildcleaner: keep\n+        \"@llvm-project//llvm:Object\",\n         \"@llvm-project//llvm:Passes\",\n         \"@llvm-project//llvm:Scalar\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:Target\",\n         \"@llvm-project//llvm:TargetParser\",\n-        \"@local_config_rocm//rocm:amd_comgr\",\n-        \"@local_config_rocm//rocm:rocm_headers\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:random\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n@@ -313,6 +313,34 @@ xla_cc_test(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"amdgpu_register_spilling_test\",\n+    size = \"small\",\n+    srcs = [\"amdgpu_register_spilling_test.cc\"],\n+    data = [\n+        \"tests_data/amdgpu_dynamic_stack.ll\",\n+        \"tests_data/amdgpu_no_spills.ll\",\n+        \"tests_data/amdgpu_sgpr_spills.ll\",\n+        \"tests_data/amdgpu_vgpr_spills.ll\",\n+    ],\n+    tags = [\n+        \"gpu\",\n+        \"rocm-only\",\n+    ],\n+    deps = [\n+        \":amdgpu_backend\",\n+        \":load_ir_module\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/stream_executor:device_description\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@local_tsl//tsl/platform:path\",\n+        \"@local_tsl//tsl/platform:test\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"load_ir_module_test\",\n     size = \"small\","
        },
        {
            "sha": "fe72d8217551064e83e22d87dd666dbafccf2ab3",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/amdgpu_backend.cc",
            "status": "modified",
            "additions": 253,
            "deletions": 116,
            "changes": 369,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -40,14 +40,15 @@ limitations under the License.\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"amd_comgr/amd_comgr.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringSet.h\"\n #include \"llvm/Analysis/CGSCCPassManager.h\"\n #include \"llvm/Analysis/LazyCallGraph.h\"\n #include \"llvm/Analysis/LoopAnalysisManager.h\"\n #include \"llvm/Analysis/TargetLibraryInfo.h\"\n #include \"llvm/Analysis/TargetTransformInfo.h\"\n+#include \"llvm/BinaryFormat/ELF.h\"\n+#include \"llvm/BinaryFormat/MsgPackDocument.h\"\n #include \"llvm/Bitcode/BitcodeReader.h\"\n #include \"llvm/Bitcode/BitcodeWriter.h\"\n #include \"llvm/CodeGen/CommandFlags.h\"\n@@ -63,9 +64,13 @@ limitations under the License.\n #include \"llvm/InitializePasses.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n+#include \"llvm/Object/ELF.h\"\n+#include \"llvm/Object/ELFObjectFile.h\"\n+#include \"llvm/Object/ObjectFile.h\"\n #include \"llvm/PassRegistry.h\"\n #include \"llvm/Passes/PassBuilder.h\"\n #include \"llvm/Passes/StandardInstrumentations.h\"\n+#include \"llvm/Support/AMDGPUMetadata.h\"\n #include \"llvm/Support/Alignment.h\"\n #include \"llvm/Support/CodeGen.h\"\n #include \"llvm/Support/FileSystem.h\"\n@@ -151,6 +156,232 @@ struct HsacoCache {\n \n static HsacoCache g_hsacoCache;  // NOLINT: static/global vars forbidden\n \n+// Structure to hold register spilling and stack information from HSACO metadata\n+struct RegisterSpillInfo {\n+  uint64_t sgpr_spill_count = 0;\n+  uint64_t vgpr_spill_count = 0;\n+  uint64_t private_segment_size = 0;\n+  bool uses_dynamic_stack = false;\n+\n+  bool HasSpilling() const {\n+    return sgpr_spill_count > 0 || vgpr_spill_count > 0;\n+  }\n+\n+  bool HasStackUsage() const {\n+    return private_segment_size > 0 || uses_dynamic_stack;\n+  }\n+};\n+\n+// Parse NT_AMDGPU_METADATA note contents and extract register spill counts.\n+// The metadata is in MessagePack format containing kernel information.\n+RegisterSpillInfo ParseAMDGPUMetadataForSpills(llvm::StringRef metadata) {\n+  RegisterSpillInfo spill_info;\n+\n+  // Parse the MsgPack metadata\n+  llvm::msgpack::Document doc;\n+  if (!doc.readFromBlob(metadata, /*Multi=*/false)) {\n+    VLOG(2) << \"Could not parse MsgPack metadata from NT_AMDGPU_METADATA note\";\n+    return spill_info;\n+  }\n+\n+  llvm::msgpack::DocNode root = doc.getRoot();\n+  if (!root.isMap()) {\n+    VLOG(2) << \"AMDGPU metadata root is not a map (unexpected format)\";\n+    return spill_info;\n+  }\n+\n+  // Look for \"amdhsa.kernels\" array\n+  llvm::msgpack::MapDocNode root_map = root.getMap();\n+  auto kernels_it = root_map.find(\"amdhsa.kernels\");\n+\n+  if (kernels_it == root_map.end() || !kernels_it->second.isArray()) {\n+    VLOG(2) << \"NT_AMDGPU_METADATA found but missing 'amdhsa.kernels' array\";\n+    return spill_info;\n+  }\n+\n+  llvm::msgpack::ArrayDocNode kernels_array = kernels_it->second.getArray();\n+\n+  // Iterate through each kernel\n+  for (auto& kernel_node : kernels_array) {\n+    uint64_t kernel_sgpr_spill = 0;\n+    uint64_t kernel_vgpr_spill = 0;\n+    uint64_t kernel_sgpr_count = 0;\n+    uint64_t kernel_vgpr_count = 0;\n+    uint64_t kernel_private_size = 0;\n+    bool kernel_uses_dynamic = false;\n+\n+    if (!kernel_node.isMap()) continue;\n+\n+    llvm::msgpack::MapDocNode kernel_map = kernel_node.getMap();\n+\n+    // Look for \".sgpr_spill_count\"\n+    auto sgpr_it = kernel_map.find(\".sgpr_spill_count\");\n+    if (sgpr_it != kernel_map.end() &&\n+        sgpr_it->second.getKind() == llvm::msgpack::Type::UInt) {\n+      kernel_sgpr_spill = sgpr_it->second.getUInt();\n+      spill_info.sgpr_spill_count =\n+          std::max(spill_info.sgpr_spill_count, kernel_sgpr_spill);\n+    }\n+\n+    // Look for \".vgpr_spill_count\"\n+    auto vgpr_it = kernel_map.find(\".vgpr_spill_count\");\n+    if (vgpr_it != kernel_map.end() &&\n+        vgpr_it->second.getKind() == llvm::msgpack::Type::UInt) {\n+      kernel_vgpr_spill = vgpr_it->second.getUInt();\n+      spill_info.vgpr_spill_count =\n+          std::max(spill_info.vgpr_spill_count, kernel_vgpr_spill);\n+    }\n+\n+    // Look for \".private_segment_fixed_size\"\n+    auto priv_it = kernel_map.find(\".private_segment_fixed_size\");\n+    if (priv_it != kernel_map.end() &&\n+        priv_it->second.getKind() == llvm::msgpack::Type::UInt) {\n+      kernel_private_size = priv_it->second.getUInt();\n+      spill_info.private_segment_size =\n+          std::max(spill_info.private_segment_size, kernel_private_size);\n+    }\n+\n+    // Look for \".uses_dynamic_stack\"\n+    auto dyn_it = kernel_map.find(\".uses_dynamic_stack\");\n+    if (dyn_it != kernel_map.end() &&\n+        dyn_it->second.getKind() == llvm::msgpack::Type::Boolean) {\n+      kernel_uses_dynamic = dyn_it->second.getBool();\n+      spill_info.uses_dynamic_stack =\n+          spill_info.uses_dynamic_stack || kernel_uses_dynamic;\n+    }\n+\n+    // Helper to get kernel name for logging (only when needed)\n+    auto get_kernel_name = [&kernel_map]() -> std::string {\n+      auto name_it = kernel_map.find(\".name\");\n+      if (name_it != kernel_map.end() &&\n+          name_it->second.getKind() == llvm::msgpack::Type::String) {\n+        return name_it->second.getString().str();\n+      }\n+      return \"unknown\";\n+    };\n+\n+    // Log per-kernel spill information with register usage\n+    if (kernel_sgpr_spill > 0 || kernel_vgpr_spill > 0) {\n+      // Look for \".sgpr_count\" (total SGPRs used)\n+      auto sgpr_count_it = kernel_map.find(\".sgpr_count\");\n+      if (sgpr_count_it != kernel_map.end() &&\n+          sgpr_count_it->second.getKind() == llvm::msgpack::Type::UInt) {\n+        kernel_sgpr_count = sgpr_count_it->second.getUInt();\n+      }\n+\n+      // Look for \".vgpr_count\" (total VGPRs used)\n+      auto vgpr_count_it = kernel_map.find(\".vgpr_count\");\n+      if (vgpr_count_it != kernel_map.end() &&\n+          vgpr_count_it->second.getKind() == llvm::msgpack::Type::UInt) {\n+        kernel_vgpr_count = vgpr_count_it->second.getUInt();\n+      }\n+\n+      VLOG(2) << \"Kernel '\" << get_kernel_name() << \"' has register spilling: \"\n+              << \"SGPR=\" << kernel_sgpr_spill << \", VGPR=\" << kernel_vgpr_spill\n+              << \". Register count: SGPR=\" << kernel_sgpr_count\n+              << \", VGPR=\" << kernel_vgpr_count;\n+    }\n+\n+    // Log per-kernel stack usage\n+    if (kernel_private_size > 0 || kernel_uses_dynamic) {\n+      VLOG(2) << \"Kernel '\" << get_kernel_name() << \"' stack usage: \"\n+              << \"private=\" << kernel_private_size\n+              << \", dynamic=\" << (kernel_uses_dynamic ? \"true\" : \"false\");\n+    }\n+  }\n+\n+  return spill_info;\n+}\n+\n+// ELF note descriptor alignment per ELF specification\n+constexpr int kElfNoteDescAlignment = 4;\n+\n+// Returns spill counts by parsing AMDGPU metadata from note sections of HSACO\n+// ELF binary.\n+//\n+// HSACO file (ELF binary)\n+//   -- .note section(s)\n+//       -- ELF Note with type=NT_AMDGPU_METADATA\n+//           -- MessagePack data\n+//               -- Root map\n+//                   -- \"amdhsa.kernels\" array\n+//                       -- Each kernel object\n+//                           - \".sgpr_spill_count\"\n+//                           - \".vgpr_spill_count\"\n+//                           - ... (other kernel properties)\n+RegisterSpillInfo ExtractRegisterSpillingFromHsaco(\n+    const std::vector<uint8_t>& hsaco) {\n+  RegisterSpillInfo spill_info;\n+\n+  // Create memory buffer from HSACO data\n+  std::unique_ptr<llvm::MemoryBuffer> mem_buffer =\n+      llvm::MemoryBuffer::getMemBuffer(\n+          llvm::StringRef(reinterpret_cast<const char*>(hsaco.data()),\n+                          hsaco.size()),\n+          \"\", /*RequiresNullTerminator=*/false);\n+\n+  // Parse as ELF object file\n+  llvm::Expected<std::unique_ptr<llvm::object::ObjectFile>> obj_or_err =\n+      llvm::object::ObjectFile::createObjectFile(mem_buffer->getMemBufferRef());\n+\n+  if (!obj_or_err) {\n+    VLOG(2) << \"Could not parse HSACO as ELF object file: \"\n+            << llvm::toString(obj_or_err.takeError());\n+    return spill_info;\n+  }\n+\n+  llvm::object::ObjectFile* obj = obj_or_err->get();\n+\n+  // Cast to ELF64LE object file (AMDGPU uses 64-bit little-endian ELF)\n+  auto* elf_obj = llvm::dyn_cast<llvm::object::ELF64LEObjectFile>(obj);\n+  if (!elf_obj) {\n+    VLOG(2) << \"HSACO is not a 64-bit little-endian ELF file\";\n+    return spill_info;\n+  }\n+\n+  // Get the underlying ELFFile to access the notes() API\n+  const auto& elf_file = elf_obj->getELFFile();\n+\n+  for (const auto& section : elf_obj->sections()) {\n+    llvm::Expected<const typename llvm::object::ELF64LEObjectFile::Elf_Shdr*>\n+        shdr_or_err = elf_obj->getSection(section.getRawDataRefImpl());\n+\n+    if (!shdr_or_err) {\n+      continue;  // Skip sections we can't access\n+    }\n+\n+    const auto* shdr = *shdr_or_err;\n+\n+    if (shdr->sh_type != llvm::ELF::SHT_NOTE) {\n+      continue;\n+    }\n+\n+    llvm::Error err = llvm::Error::success();\n+    for (const auto& note : elf_file.notes(*shdr, err)) {\n+      if (note.getType() == llvm::ELF::NT_AMDGPU_METADATA) {\n+        llvm::StringRef metadata =\n+            note.getDescAsStringRef(kElfNoteDescAlignment);\n+\n+        if (metadata.empty()) {\n+          VLOG(2) << \"Found NT_AMDGPU_METADATA note but it contains no data\";\n+          continue;\n+        }\n+\n+        // Parse the metadata and extract spill counts, return immediately\n+        return ParseAMDGPUMetadataForSpills(metadata);\n+      }\n+    }\n+\n+    if (err) {\n+      VLOG(2) << \"Error parsing notes: \" << llvm::toString(std::move(err));\n+    }\n+  }\n+\n+  // If we reach here, no metadata was found\n+  VLOG(2) << \"No AMDGPU metadata found in HSACO\";\n+  return spill_info;\n+}\n+\n bool HsacoCache::Find(const std::string& ir, uint64_t& hash,\n                       const std::string& gfx, std::vector<uint8_t>& hsaco) {\n   absl::MutexLock lock(g_hsacoCache.mutex);\n@@ -332,136 +563,42 @@ absl::StatusOr<std::vector<uint8_t>> EmitModuleToHsaco(\n   hsaco_file.close();\n \n   // Check for register spilling using HSACO metadata\n-  // Use amd_comgr library for fast in-process metadata extraction\n   VLOG(2) << \"Checking for register spilling in: \"\n           << module->getModuleIdentifier();\n \n-  bool has_spilling = false;\n-  int sgpr_spill_count = 0;\n-  int vgpr_spill_count = 0;\n-  int private_segment_size = 0;\n+  RegisterSpillInfo spill_info = ExtractRegisterSpillingFromHsaco(hsaco);\n \n-  // Use already-loaded HSACO data for amd_comgr parsing\n-  {\n-    // Create amd_comgr data object from HSACO\n-    amd_comgr_data_t comgr_data;\n-    amd_comgr_status_t status =\n-        amd_comgr_create_data(AMD_COMGR_DATA_KIND_EXECUTABLE, &comgr_data);\n-\n-    if (status == AMD_COMGR_STATUS_SUCCESS) {\n-      status = amd_comgr_set_data(comgr_data, hsaco.size(),\n-                                  reinterpret_cast<const char*>(hsaco.data()));\n-\n-      if (status == AMD_COMGR_STATUS_SUCCESS) {\n-        // Get metadata from the executable\n-        amd_comgr_metadata_node_t metadata;\n-        status = amd_comgr_get_data_metadata(comgr_data, &metadata);\n-\n-        if (status == AMD_COMGR_STATUS_SUCCESS) {\n-          // Helper lambda to lookup integer value from metadata map\n-          auto lookup_int_value = [](amd_comgr_metadata_node_t root,\n-                                     const char* key) -> int {\n-            amd_comgr_metadata_node_t value_node;\n-            amd_comgr_status_t s =\n-                amd_comgr_metadata_lookup(root, key, &value_node);\n-            if (s != AMD_COMGR_STATUS_SUCCESS) {\n-              return 0;\n-            }\n-\n-            size_t size = 0;\n-            s = amd_comgr_get_metadata_string(value_node, &size, nullptr);\n-            if (s != AMD_COMGR_STATUS_SUCCESS || size == 0) {\n-              amd_comgr_destroy_metadata(value_node);\n-              return 0;\n-            }\n-\n-            std::string str_value(size, '\\0');\n-            s = amd_comgr_get_metadata_string(value_node, &size,\n-                                              str_value.data());\n-            amd_comgr_destroy_metadata(value_node);\n-\n-            if (s != AMD_COMGR_STATUS_SUCCESS) {\n-              return 0;\n-            }\n-\n-            // Parse the integer value\n-            try {\n-              return std::stoi(str_value);\n-            } catch (...) {\n-              return 0;\n-            }\n-          };\n-\n-          // Navigate to amdhsa.kernels array and check each kernel\n-          amd_comgr_metadata_node_t kernels_node;\n-          if (amd_comgr_metadata_lookup(metadata, \"amdhsa.kernels\",\n-                                        &kernels_node) ==\n-              AMD_COMGR_STATUS_SUCCESS) {\n-            size_t kernel_count = 0;\n-            amd_comgr_get_metadata_list_size(kernels_node, &kernel_count);\n-\n-            for (size_t i = 0; i < kernel_count; ++i) {\n-              amd_comgr_metadata_node_t kernel_node;\n-              if (amd_comgr_index_list_metadata(kernels_node, i,\n-                                                &kernel_node) ==\n-                  AMD_COMGR_STATUS_SUCCESS) {\n-                // Get spill counts for this kernel\n-                int kernel_sgpr_spill =\n-                    lookup_int_value(kernel_node, \".sgpr_spill_count\");\n-                int kernel_vgpr_spill =\n-                    lookup_int_value(kernel_node, \".vgpr_spill_count\");\n-                int kernel_private_size = lookup_int_value(\n-                    kernel_node, \".private_segment_fixed_size\");\n-\n-                // Aggregate max values across all kernels\n-                sgpr_spill_count =\n-                    std::max(sgpr_spill_count, kernel_sgpr_spill);\n-                vgpr_spill_count =\n-                    std::max(vgpr_spill_count, kernel_vgpr_spill);\n-                private_segment_size =\n-                    std::max(private_segment_size, kernel_private_size);\n-\n-                amd_comgr_destroy_metadata(kernel_node);\n-              }\n-            }\n-            amd_comgr_destroy_metadata(kernels_node);\n-          }\n-\n-          amd_comgr_destroy_metadata(metadata);\n-        } else {\n-          VLOG(2) << \"Could not get HSACO metadata via amd_comgr\";\n-        }\n-      }\n-      amd_comgr_release_data(comgr_data);\n-    } else {\n-      VLOG(2) << \"Could not create amd_comgr data object\";\n-    }\n-\n-    if (sgpr_spill_count > 0 || vgpr_spill_count > 0 ||\n-        private_segment_size > 0) {\n-      has_spilling = true;\n-    }\n+  if (spill_info.HasSpilling()) {\n+    // We can have SGPR spills without stack being used. They are saved to\n+    // VGPRs. In that case, we don't want to discard such kernel, so just\n+    // report such cases.\n+    VLOG(1) << \"Register spilling (SGPR: \" << spill_info.sgpr_spill_count\n+            << \", VGPR: \" << spill_info.vgpr_spill_count << \") detected in \"\n+            << module->getModuleIdentifier();\n+  } else {\n+    VLOG(2) << \"No register spilling detected in \"\n+            << module->getModuleIdentifier();\n   }\n \n-  if (has_spilling) {\n-    VLOG(0) << \"====== REGISTER SPILLING DETECTED ======\";\n-    VLOG(0) << \"Module: \" << module->getModuleIdentifier();\n-    VLOG(0) << \"SGPR spill count: \" << sgpr_spill_count;\n-    VLOG(0) << \"VGPR spill count: \" << vgpr_spill_count;\n-    VLOG(0) << \"Private segment size: \" << private_segment_size << \" bytes\";\n-    VLOG(0) << \"Performance may be degraded due to register pressure\";\n-    VLOG(0) << \"========================================\";\n+  if (spill_info.HasStackUsage()) {\n+    VLOG(1) << \"Stack usage (private: \" << spill_info.private_segment_size\n+            << \", dynamic: \"\n+            << (spill_info.uses_dynamic_stack ? \"true\" : \"false\")\n+            << \") detected in \" << module->getModuleIdentifier();\n \n     // Filter out kernels with register spilling during autotuning\n     // This matches NVIDIA's behavior in ptx_compiler_impl.cc\n     // TODO: remove ptx from xla_gpu_fail_ptx_compilation_on_register_spilling\n     // to make the flag more general\n     if (debug_options.xla_gpu_fail_ptx_compilation_on_register_spilling()) {\n+      VLOG(0) << \"Discard module \" << module->getModuleIdentifier()\n+              << \" due register spilling or stack usage\";\n       return xla::Cancelled(\n-          \"Compilation result discarded due to register spilling\");\n+          \"Compilation result discarded due to register spilling or stack \"\n+          \"usage\");\n     }\n   } else {\n-    VLOG(2) << \"No register spilling detected\";\n+    VLOG(2) << \"No stack usage detected in \" << module->getModuleIdentifier();\n   }\n \n   // Clean up temp files"
        },
        {
            "sha": "74b1c94feffa47ade0acd82fb94eb06bc0810cde",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/amdgpu_register_spilling_test.cc",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_register_spilling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_register_spilling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_register_spilling_test.cc?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -0,0 +1,127 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <string>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/amdgpu_backend.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/load_ir_module.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"tsl/platform/path.h\"\n+#include \"tsl/platform/test.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+namespace se = ::stream_executor;\n+\n+static std::string RemoveLLExtension(const std::string& filename) {\n+  return filename.substr(0, filename.find(\".ll\"));\n+}\n+\n+// Test parameter structure\n+struct SpillingTestParam {\n+  std::string ir_filename;         // IR file to compile\n+  bool fail_on_spilling;           // Flag value\n+  absl::StatusCode expected_code;  // Expected status code\n+  std::string expected_substring;  // Expected substring in error (if any)\n+};\n+\n+class AMDGPURegisterSpillingTest\n+    : public ::testing::TestWithParam<SpillingTestParam> {\n+ protected:\n+  // Helper to load IR module from test data\n+  std::unique_ptr<llvm::Module> LoadTestModule(llvm::LLVMContext* context,\n+                                               const std::string& filename) {\n+    return LoadIRModule(\n+        tsl::io::JoinPath(tsl::testing::XlaSrcRoot(), \"service\", \"gpu\",\n+                          \"llvm_gpu_backend\", \"tests_data\", filename),\n+        context);\n+  }\n+\n+  // Helper to compile with given debug options\n+  absl::StatusOr<std::vector<uint8_t>> CompileModule(\n+      llvm::Module* module, const std::string& module_id,\n+      bool fail_on_spilling) {\n+    DebugOptions debug_options;\n+    debug_options.set_xla_gpu_fail_ptx_compilation_on_register_spilling(\n+        fail_on_spilling);\n+\n+    module->setModuleIdentifier(module_id);\n+\n+    return amdgpu::CompileToHsaco(\n+        module, se::GpuComputeCapability{se::RocmComputeCapability{\"gfx1100\"}},\n+        debug_options, module_id);\n+  }\n+};\n+\n+TEST_P(AMDGPURegisterSpillingTest, CompileTest) {\n+  const SpillingTestParam& param = GetParam();\n+  llvm::LLVMContext context;\n+\n+  auto module = LoadTestModule(&context, param.ir_filename);\n+  ASSERT_NE(module, nullptr);\n+\n+  // Generate module ID from filename and flag state\n+  std::string module_id =\n+      RemoveLLExtension(param.ir_filename) +\n+      (param.fail_on_spilling ? \"_fail_on_spilling\" : \"_allow_spilling\");\n+\n+  auto result = CompileModule(module.get(), module_id, param.fail_on_spilling);\n+\n+  EXPECT_EQ(result.status().code(), param.expected_code)\n+      << \"IR: \" << param.ir_filename\n+      << \", Flag: \" << (param.fail_on_spilling ? \"enabled\" : \"disabled\")\n+      << \", Status: \" << result.status().message();\n+\n+  if (!param.expected_substring.empty()) {\n+    EXPECT_THAT(result.status().message(),\n+                ::testing::HasSubstr(param.expected_substring))\n+        << \"IR: \" << param.ir_filename;\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    RegisterSpillingTests, AMDGPURegisterSpillingTest,\n+    ::testing::Values(\n+        SpillingTestParam{\"amdgpu_no_spills.ll\",\n+                          /*fail_on_spilling=*/true, absl::StatusCode::kOk, \"\"},\n+        SpillingTestParam{\"amdgpu_vgpr_spills.ll\",\n+                          /*fail_on_spilling=*/false, absl::StatusCode::kOk,\n+                          \"\"},\n+        SpillingTestParam{\"amdgpu_vgpr_spills.ll\",\n+                          /*fail_on_spilling=*/true,\n+                          absl::StatusCode::kCancelled, \"register spilling\"},\n+        SpillingTestParam{\"amdgpu_sgpr_spills.ll\",\n+                          /*fail_on_spilling=*/false, absl::StatusCode::kOk,\n+                          \"\"},\n+        SpillingTestParam{\"amdgpu_sgpr_spills.ll\",\n+                          /*fail_on_spilling=*/true, absl::StatusCode::kOk, \"\"},\n+        SpillingTestParam{\"amdgpu_dynamic_stack.ll\",\n+                          /*fail_on_spilling=*/true,\n+                          absl::StatusCode::kCancelled, \"stack usage\"}),\n+    [](const ::testing::TestParamInfo<SpillingTestParam>& info) {\n+      return RemoveLLExtension(info.param.ir_filename) +\n+             (info.param.fail_on_spilling ? \"_fail_on_spilling\"\n+                                          : \"_allow_spilling\");\n+    });\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "5a8b76446c3e55e58c1a339d1ff0579901288853",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/tests_data/amdgpu_dynamic_stack.ll",
            "status": "added",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_dynamic_stack.ll",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_dynamic_stack.ll",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_dynamic_stack.ll?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -0,0 +1,26 @@\n+; AMDGPU kernel with dynamic stack usage (indirect function call)\n+; Based on real HIP code that uses function pointers\n+target datalayout = \"e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-p7:160:256:256:32-p8:128:128:128:48-p9:192:256:256:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7:8:9\"\n+target triple = \"amdgcn-amd-amdhsa\"\n+\n+@__hip_cuid_40fa47637d275275 = addrspace(1) global i8 0\n+\n+@llvm.compiler.used = appending addrspace(1) global [1 x ptr] [ptr addrspacecast (ptr addrspace(1) @__hip_cuid_40fa47637d275275 to ptr)], section \"llvm.metadata\"\n+\n+; Kernel that uses indirect function call requiring dynamic stack\n+define protected amdgpu_kernel void @_Z4TestPDF16bS_S_(ptr addrspace(1) noundef %dst.coerce, ptr addrspace(1) noundef %ptr1.coerce, ptr addrspace(1) noundef %ptr2.coerce) local_unnamed_addr {\n+entry:\n+  %0 = ptrtoint ptr addrspace(1) %dst.coerce to i64\n+  %1 = inttoptr i64 %0 to ptr\n+  %2 = ptrtoint ptr addrspace(1) %ptr1.coerce to i64\n+  %3 = inttoptr i64 %2 to ptr\n+  %4 = ptrtoint ptr addrspace(1) %ptr2.coerce to i64\n+  %5 = inttoptr i64 %4 to ptr\n+  %6 = tail call ptr asm \"\", \"=s\"() #1\n+  tail call void %6(ptr noundef %1, ptr noundef %3, ptr noundef %5) #2\n+  ret void\n+}\n+\n+attributes #1 = { nounwind }\n+attributes #2 = { nounwind }\n+"
        },
        {
            "sha": "4ab9829a36f90df6b1e9ee5785a36b938bb31597",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/tests_data/amdgpu_no_spills.ll",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_no_spills.ll",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_no_spills.ll",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_no_spills.ll?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -0,0 +1,29 @@\n+; Simple AMDGPU kernel for testing register spilling detection\n+; This module has no external dependencies and minimal module flags\n+target datalayout = \"e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-p7:160:256:256:32-p8:128:128:128:48-p9:192:256:256:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7:8:9\"\n+target triple = \"amdgcn-amd-amdhsa\"\n+\n+; Simple kernel that adds two arrays\n+define amdgpu_kernel void @simple_add(ptr addrspace(1) %a, ptr addrspace(1) %b, ptr addrspace(1) %c) {\n+entry:\n+  %tid = call i32 @llvm.amdgcn.workitem.id.x()\n+  %tidx = zext i32 %tid to i64\n+\n+  %a_ptr = getelementptr float, ptr addrspace(1) %a, i64 %tidx\n+  %b_ptr = getelementptr float, ptr addrspace(1) %b, i64 %tidx\n+  %c_ptr = getelementptr float, ptr addrspace(1) %c, i64 %tidx\n+\n+  %a_val = load float, ptr addrspace(1) %a_ptr, align 4\n+  %b_val = load float, ptr addrspace(1) %b_ptr, align 4\n+\n+  %sum = fadd float %a_val, %b_val\n+\n+  store float %sum, ptr addrspace(1) %c_ptr, align 4\n+  ret void\n+}\n+\n+; Intrinsic declaration\n+declare i32 @llvm.amdgcn.workitem.id.x() #0\n+\n+attributes #0 = { nounwind readnone speculatable }\n+"
        },
        {
            "sha": "51dbc634d680c9b8d5a833ddb7159711ffa6bd2b",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/tests_data/amdgpu_sgpr_spills.ll",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_sgpr_spills.ll",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_sgpr_spills.ll",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_sgpr_spills.ll?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -0,0 +1,166 @@\n+; AMDGPU kernel with high SGPR pressure to force scalar register spilling\n+target datalayout = \"e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-p7:160:256:256:32-p8:128:128:128:48-p9:192:256:256:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7:8:9\"\n+target triple = \"amdgcn-amd-amdhsa\"\n+\n+; Kernel using many scalar operations with limited SGPRs\n+; We use readfirstlane to force values into SGPRs\n+define amdgpu_kernel void @sgpr_pressure(ptr addrspace(1) %in, ptr addrspace(1) %out) #0 {\n+entry:\n+  %tid = call i32 @llvm.amdgcn.workitem.id.x()\n+  %tidx = zext i32 %tid to i64\n+\n+  ; Load many scalar values from memory\n+  ; Using readfirstlane forces values into SGPRs (uniform across wavefront)\n+  %ptr0 = getelementptr i32, ptr addrspace(1) %in, i64 0\n+  %v0_vec = load i32, ptr addrspace(1) %ptr0, align 4\n+  %v0 = call i32 @llvm.amdgcn.readfirstlane(i32 %v0_vec)\n+\n+  %ptr1 = getelementptr i32, ptr addrspace(1) %in, i64 1\n+  %v1_vec = load i32, ptr addrspace(1) %ptr1, align 4\n+  %v1 = call i32 @llvm.amdgcn.readfirstlane(i32 %v1_vec)\n+\n+  %ptr2 = getelementptr i32, ptr addrspace(1) %in, i64 2\n+  %v2_vec = load i32, ptr addrspace(1) %ptr2, align 4\n+  %v2 = call i32 @llvm.amdgcn.readfirstlane(i32 %v2_vec)\n+\n+  %ptr3 = getelementptr i32, ptr addrspace(1) %in, i64 3\n+  %v3_vec = load i32, ptr addrspace(1) %ptr3, align 4\n+  %v3 = call i32 @llvm.amdgcn.readfirstlane(i32 %v3_vec)\n+\n+  %ptr4 = getelementptr i32, ptr addrspace(1) %in, i64 4\n+  %v4_vec = load i32, ptr addrspace(1) %ptr4, align 4\n+  %v4 = call i32 @llvm.amdgcn.readfirstlane(i32 %v4_vec)\n+\n+  %ptr5 = getelementptr i32, ptr addrspace(1) %in, i64 5\n+  %v5_vec = load i32, ptr addrspace(1) %ptr5, align 4\n+  %v5 = call i32 @llvm.amdgcn.readfirstlane(i32 %v5_vec)\n+\n+  %ptr6 = getelementptr i32, ptr addrspace(1) %in, i64 6\n+  %v6_vec = load i32, ptr addrspace(1) %ptr6, align 4\n+  %v6 = call i32 @llvm.amdgcn.readfirstlane(i32 %v6_vec)\n+\n+  %ptr7 = getelementptr i32, ptr addrspace(1) %in, i64 7\n+  %v7_vec = load i32, ptr addrspace(1) %ptr7, align 4\n+  %v7 = call i32 @llvm.amdgcn.readfirstlane(i32 %v7_vec)\n+\n+  %ptr8 = getelementptr i32, ptr addrspace(1) %in, i64 8\n+  %v8_vec = load i32, ptr addrspace(1) %ptr8, align 4\n+  %v8 = call i32 @llvm.amdgcn.readfirstlane(i32 %v8_vec)\n+\n+  %ptr9 = getelementptr i32, ptr addrspace(1) %in, i64 9\n+  %v9_vec = load i32, ptr addrspace(1) %ptr9, align 4\n+  %v9 = call i32 @llvm.amdgcn.readfirstlane(i32 %v9_vec)\n+\n+  %ptr10 = getelementptr i32, ptr addrspace(1) %in, i64 10\n+  %v10_vec = load i32, ptr addrspace(1) %ptr10, align 4\n+  %v10 = call i32 @llvm.amdgcn.readfirstlane(i32 %v10_vec)\n+\n+  %ptr11 = getelementptr i32, ptr addrspace(1) %in, i64 11\n+  %v11_vec = load i32, ptr addrspace(1) %ptr11, align 4\n+  %v11 = call i32 @llvm.amdgcn.readfirstlane(i32 %v11_vec)\n+\n+  %ptr12 = getelementptr i32, ptr addrspace(1) %in, i64 12\n+  %v12_vec = load i32, ptr addrspace(1) %ptr12, align 4\n+  %v12 = call i32 @llvm.amdgcn.readfirstlane(i32 %v12_vec)\n+\n+  %ptr13 = getelementptr i32, ptr addrspace(1) %in, i64 13\n+  %v13_vec = load i32, ptr addrspace(1) %ptr13, align 4\n+  %v13 = call i32 @llvm.amdgcn.readfirstlane(i32 %v13_vec)\n+\n+  %ptr14 = getelementptr i32, ptr addrspace(1) %in, i64 14\n+  %v14_vec = load i32, ptr addrspace(1) %ptr14, align 4\n+  %v14 = call i32 @llvm.amdgcn.readfirstlane(i32 %v14_vec)\n+\n+  %ptr15 = getelementptr i32, ptr addrspace(1) %in, i64 15\n+  %v15_vec = load i32, ptr addrspace(1) %ptr15, align 4\n+  %v15 = call i32 @llvm.amdgcn.readfirstlane(i32 %v15_vec)\n+\n+  ; Create many scalar computations - chain A\n+  %a0 = add i32 %v0, %v1\n+  %a1 = mul i32 %a0, %v2\n+  %a2 = add i32 %a1, %v3\n+  %a3 = mul i32 %a2, %v4\n+  %a4 = add i32 %a3, %v5\n+  %a5 = mul i32 %a4, %v6\n+  %a6 = add i32 %a5, %v7\n+  %a7 = mul i32 %a6, %v8\n+  %a8 = add i32 %a7, %v9\n+  %a9 = mul i32 %a8, %v10\n+  %a10 = add i32 %a9, %v11\n+  %a11 = mul i32 %a10, %v12\n+  %a12 = add i32 %a11, %v13\n+  %a13 = mul i32 %a12, %v14\n+  %a14 = add i32 %a13, %v15\n+\n+  ; Chain B - reverse\n+  %b0 = mul i32 %v15, %v14\n+  %b1 = add i32 %b0, %v13\n+  %b2 = mul i32 %b1, %v12\n+  %b3 = add i32 %b2, %v11\n+  %b4 = mul i32 %b3, %v10\n+  %b5 = add i32 %b4, %v9\n+  %b6 = mul i32 %b5, %v8\n+  %b7 = add i32 %b6, %v7\n+  %b8 = mul i32 %b7, %v6\n+  %b9 = add i32 %b8, %v5\n+  %b10 = mul i32 %b9, %v4\n+  %b11 = add i32 %b10, %v3\n+  %b12 = mul i32 %b11, %v2\n+  %b13 = add i32 %b12, %v1\n+  %b14 = mul i32 %b13, %v0\n+\n+  ; Chain C - subtraction\n+  %c0 = sub i32 %v0, %v1\n+  %c1 = mul i32 %c0, %v2\n+  %c2 = sub i32 %c1, %v3\n+  %c3 = mul i32 %c2, %v4\n+  %c4 = sub i32 %c3, %v5\n+  %c5 = mul i32 %c4, %v6\n+  %c6 = sub i32 %c5, %v7\n+  %c7 = mul i32 %c6, %v8\n+  %c8 = sub i32 %c7, %v9\n+  %c9 = mul i32 %c8, %v10\n+  %c10 = sub i32 %c9, %v11\n+  %c11 = mul i32 %c10, %v12\n+  %c12 = sub i32 %c11, %v13\n+  %c13 = mul i32 %c12, %v14\n+  %c14 = sub i32 %c13, %v15\n+\n+  ; Chain D - cross dependencies\n+  %d0 = add i32 %a0, %b0\n+  %d1 = mul i32 %d0, %c0\n+  %d2 = add i32 %a1, %b1\n+  %d3 = mul i32 %d2, %c1\n+  %d4 = add i32 %a2, %b2\n+  %d5 = mul i32 %d4, %c2\n+  %d6 = add i32 %a3, %b3\n+  %d7 = mul i32 %d6, %c3\n+  %d8 = add i32 %a4, %b4\n+  %d9 = mul i32 %d8, %c4\n+  %d10 = add i32 %a5, %b5\n+  %d11 = mul i32 %d10, %c5\n+  %d12 = add i32 %a6, %b6\n+  %d13 = mul i32 %d12, %c6\n+\n+  ; Combine all chains\n+  %r0 = add i32 %a14, %b14\n+  %r1 = add i32 %r0, %c14\n+  %r2 = add i32 %r1, %d1\n+  %r3 = add i32 %r2, %d3\n+  %r4 = add i32 %r3, %d5\n+  %r5 = add i32 %r4, %d7\n+  %r6 = add i32 %r5, %d9\n+  %r7 = add i32 %r6, %d11\n+  %result = add i32 %r7, %d13\n+\n+  %out_ptr = getelementptr i32, ptr addrspace(1) %out, i64 %tidx\n+  store i32 %result, ptr addrspace(1) %out_ptr, align 4\n+  ret void\n+}\n+\n+declare i32 @llvm.amdgcn.workitem.id.x() #1\n+declare i32 @llvm.amdgcn.readfirstlane(i32) #1\n+\n+; Limit SGPRs to 32, this should force SGPR spilling\n+attributes #0 = { \"amdgpu-num-sgpr\"=\"32\" \"amdgpu-flat-work-group-size\"=\"1,256\" }\n+attributes #1 = { nounwind readnone speculatable }"
        },
        {
            "sha": "5634790c8e6ebab4167541c9f4b552ebbe8dd7ed",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/tests_data/amdgpu_vgpr_spills.ll",
            "status": "added",
            "additions": 145,
            "deletions": 0,
            "changes": 145,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_vgpr_spills.ll",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a72e016e020f84ebc1857ec96c8101ee7f4ccd06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_vgpr_spills.ll",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu_vgpr_spills.ll?ref=a72e016e020f84ebc1857ec96c8101ee7f4ccd06",
            "patch": "@@ -0,0 +1,145 @@\n+; AMDGPU kernel with high register pressure to force spilling\n+; This uses many vector operations to exhaust available VGPRs\n+target datalayout = \"e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-p7:160:256:256:32-p8:128:128:128:48-p9:192:256:256:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7:8:9\"\n+target triple = \"amdgcn-amd-amdhsa\"\n+\n+; Kernel with many live values to force register spilling\n+define amdgpu_kernel void @high_register_pressure(ptr addrspace(1) %in, ptr addrspace(1) %out) #0 {\n+entry:\n+  %tid = call i32 @llvm.amdgcn.workitem.id.x()\n+  %tidx = zext i32 %tid to i64\n+\n+  ; Load many vectors from memory - using volatile to prevent optimization\n+  %ptr0 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 %tidx\n+  %v0 = load volatile <4 x float>, ptr addrspace(1) %ptr0, align 16\n+\n+  %ptr1 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 1\n+  %v1 = load volatile <4 x float>, ptr addrspace(1) %ptr1, align 16\n+\n+  %ptr2 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 2\n+  %v2 = load volatile <4 x float>, ptr addrspace(1) %ptr2, align 16\n+\n+  %ptr3 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 3\n+  %v3 = load volatile <4 x float>, ptr addrspace(1) %ptr3, align 16\n+\n+  %ptr4 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 4\n+  %v4 = load volatile <4 x float>, ptr addrspace(1) %ptr4, align 16\n+\n+  %ptr5 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 5\n+  %v5 = load volatile <4 x float>, ptr addrspace(1) %ptr5, align 16\n+\n+  %ptr6 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 6\n+  %v6 = load volatile <4 x float>, ptr addrspace(1) %ptr6, align 16\n+\n+  %ptr7 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 7\n+  %v7 = load volatile <4 x float>, ptr addrspace(1) %ptr7, align 16\n+\n+  %ptr8 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 8\n+  %v8 = load volatile <4 x float>, ptr addrspace(1) %ptr8, align 16\n+\n+  %ptr9 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 9\n+  %v9 = load volatile <4 x float>, ptr addrspace(1) %ptr9, align 16\n+\n+  %ptr10 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 10\n+  %v10 = load volatile <4 x float>, ptr addrspace(1) %ptr10, align 16\n+\n+  %ptr11 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 11\n+  %v11 = load volatile <4 x float>, ptr addrspace(1) %ptr11, align 16\n+\n+  %ptr12 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 12\n+  %v12 = load volatile <4 x float>, ptr addrspace(1) %ptr12, align 16\n+\n+  %ptr13 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 13\n+  %v13 = load volatile <4 x float>, ptr addrspace(1) %ptr13, align 16\n+\n+  %ptr14 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 14\n+  %v14 = load volatile <4 x float>, ptr addrspace(1) %ptr14, align 16\n+\n+  %ptr15 = getelementptr <4 x float>, ptr addrspace(1) %in, i64 15\n+  %v15 = load volatile <4 x float>, ptr addrspace(1) %ptr15, align 16\n+\n+  ; Create many dependent calculations - chain A\n+  %a0 = fadd <4 x float> %v0, %v1\n+  %a1 = fmul <4 x float> %a0, %v2\n+  %a2 = fadd <4 x float> %a1, %v3\n+  %a3 = fmul <4 x float> %a2, %v4\n+  %a4 = fadd <4 x float> %a3, %v5\n+  %a5 = fmul <4 x float> %a4, %v6\n+  %a6 = fadd <4 x float> %a5, %v7\n+  %a7 = fmul <4 x float> %a6, %v8\n+  %a8 = fadd <4 x float> %a7, %v9\n+  %a9 = fmul <4 x float> %a8, %v10\n+  %a10 = fadd <4 x float> %a9, %v11\n+  %a11 = fmul <4 x float> %a10, %v12\n+  %a12 = fadd <4 x float> %a11, %v13\n+  %a13 = fmul <4 x float> %a12, %v14\n+  %a14 = fadd <4 x float> %a13, %v15\n+\n+  ; Chain B - reverse direction\n+  %b0 = fmul <4 x float> %v15, %v14\n+  %b1 = fadd <4 x float> %b0, %v13\n+  %b2 = fmul <4 x float> %b1, %v12\n+  %b3 = fadd <4 x float> %b2, %v11\n+  %b4 = fmul <4 x float> %b3, %v10\n+  %b5 = fadd <4 x float> %b4, %v9\n+  %b6 = fmul <4 x float> %b5, %v8\n+  %b7 = fadd <4 x float> %b6, %v7\n+  %b8 = fmul <4 x float> %b7, %v6\n+  %b9 = fadd <4 x float> %b8, %v5\n+  %b10 = fmul <4 x float> %b9, %v4\n+  %b11 = fadd <4 x float> %b10, %v3\n+  %b12 = fmul <4 x float> %b11, %v2\n+  %b13 = fadd <4 x float> %b12, %v1\n+  %b14 = fmul <4 x float> %b13, %v0\n+\n+  ; Chain C - subtraction chain\n+  %c0 = fsub <4 x float> %v0, %v1\n+  %c1 = fmul <4 x float> %c0, %v2\n+  %c2 = fsub <4 x float> %c1, %v3\n+  %c3 = fmul <4 x float> %c2, %v4\n+  %c4 = fsub <4 x float> %c3, %v5\n+  %c5 = fmul <4 x float> %c4, %v6\n+  %c6 = fsub <4 x float> %c5, %v7\n+  %c7 = fmul <4 x float> %c6, %v8\n+  %c8 = fsub <4 x float> %c7, %v9\n+  %c9 = fmul <4 x float> %c8, %v10\n+  %c10 = fsub <4 x float> %c9, %v11\n+  %c11 = fmul <4 x float> %c10, %v12\n+  %c12 = fsub <4 x float> %c11, %v13\n+  %c13 = fmul <4 x float> %c12, %v14\n+  %c14 = fsub <4 x float> %c13, %v15\n+\n+  ; Chain D - cross dependencies\n+  %d0 = fadd <4 x float> %a0, %b0\n+  %d1 = fmul <4 x float> %d0, %c0\n+  %d2 = fadd <4 x float> %a1, %b1\n+  %d3 = fmul <4 x float> %d2, %c1\n+  %d4 = fadd <4 x float> %a2, %b2\n+  %d5 = fmul <4 x float> %d4, %c2\n+  %d6 = fadd <4 x float> %a3, %b3\n+  %d7 = fmul <4 x float> %d6, %c3\n+  %d8 = fadd <4 x float> %a4, %b4\n+  %d9 = fmul <4 x float> %d8, %c4\n+  %d10 = fadd <4 x float> %a5, %b5\n+  %d11 = fmul <4 x float> %d10, %c5\n+\n+  ; Final combination to keep all values live\n+  %result0 = fadd <4 x float> %a14, %b14\n+  %result1 = fadd <4 x float> %result0, %c14\n+  %result2 = fadd <4 x float> %result1, %d1\n+  %result3 = fadd <4 x float> %result2, %d3\n+  %result4 = fadd <4 x float> %result3, %d5\n+  %result5 = fadd <4 x float> %result4, %d7\n+  %result6 = fadd <4 x float> %result5, %d9\n+  %result = fadd <4 x float> %result6, %d11\n+\n+  %out_ptr = getelementptr <4 x float>, ptr addrspace(1) %out, i64 %tidx\n+  store <4 x float> %result, ptr addrspace(1) %out_ptr, align 16\n+  ret void\n+}\n+\n+declare i32 @llvm.amdgcn.workitem.id.x() #1\n+\n+; Limit VGPRs to 64 to force spilling\n+attributes #0 = { \"amdgpu-num-vgpr\"=\"64\" \"amdgpu-flat-work-group-size\"=\"1,256\" }\n+attributes #1 = { nounwind readnone speculatable }"
        }
    ],
    "stats": {
        "total": 895,
        "additions": 776,
        "deletions": 119
    }
}