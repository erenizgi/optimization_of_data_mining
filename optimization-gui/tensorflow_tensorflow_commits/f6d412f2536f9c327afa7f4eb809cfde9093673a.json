{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 829239502",
    "sha": "f6d412f2536f9c327afa7f4eb809cfde9093673a",
    "files": [
        {
            "sha": "4006fcbaf94dea94540d4aa1b16fcf4a35e63ca0",
            "filename": "tensorflow/core/kernels/batching_util/adaptive_shared_batch_scheduler.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fadaptive_shared_batch_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fadaptive_shared_batch_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fadaptive_shared_batch_scheduler.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -89,7 +89,7 @@ class AdaptiveSharedBatchScheduler\n \n   struct Options {\n     // The name to use for the pool of batch threads.\n-    string thread_pool_name = {\"batch_threads\"};\n+    std::string thread_pool_name = {\"batch_threads\"};\n     // Number of batch processing threads - the maximum value of\n     // in_flight_batches_limit_.  It is recommended that this value be set by\n     // running the system under load, observing the learned value for\n@@ -329,7 +329,7 @@ class ASBSQueue : public BatchScheduler<TaskType> {\n \n   // Returns uint64 one greater than was returned by the previous call.\n   // Context id is reused after std::numeric_limits<uint64>::max is exhausted.\n-  static uint64 NewTraceMeContextIdForBatch();\n+  static uint64_t NewTraceMeContextIdForBatch();\n \n   std::shared_ptr<AdaptiveSharedBatchScheduler<TaskType>> scheduler_;\n   const QueueOptions options_;\n@@ -347,7 +347,7 @@ template <typename TaskType>\n class ASBSBatch : public Batch<TaskType> {\n  public:\n   ASBSBatch(ASBSQueue<TaskType>* queue, int64_t creation_time_micros,\n-            int64_t batch_timeout_micros, uint64 traceme_context_id)\n+            int64_t batch_timeout_micros, uint64_t traceme_context_id)\n       : queue_(queue),\n         creation_time_micros_(creation_time_micros),\n         schedulable_time_micros_(creation_time_micros + batch_timeout_micros),\n@@ -361,13 +361,13 @@ class ASBSBatch : public Batch<TaskType> {\n \n   int64_t schedulable_time_micros() const { return schedulable_time_micros_; }\n \n-  uint64 traceme_context_id() const { return traceme_context_id_; }\n+  uint64_t traceme_context_id() const { return traceme_context_id_; }\n \n  private:\n   ASBSQueue<TaskType>* queue_;\n   const int64_t creation_time_micros_;\n   const int64_t schedulable_time_micros_;\n-  const uint64 traceme_context_id_;\n+  const uint64_t traceme_context_id_;\n   ASBSBatch(const ASBSBatch&) = delete;\n   void operator=(const ASBSBatch&) = delete;\n };\n@@ -860,8 +860,8 @@ size_t ASBSQueue<TaskType>::SchedulingCapacityLocked() const {\n \n template <typename TaskType>\n // static\n-uint64 ASBSQueue<TaskType>::NewTraceMeContextIdForBatch() {\n-  static std::atomic<uint64> traceme_context_id(0);\n+uint64_t ASBSQueue<TaskType>::NewTraceMeContextIdForBatch() {\n+  static std::atomic<uint64_t> traceme_context_id(0);\n   return traceme_context_id.fetch_add(1, std::memory_order_relaxed);\n }\n }  // namespace internal"
        },
        {
            "sha": "c915deead27a85b814ca00c656d6ad5b1ddf4b3d",
            "filename": "tensorflow/core/kernels/batching_util/basic_batch_scheduler.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbasic_batch_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbasic_batch_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbasic_batch_scheduler.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -161,7 +161,7 @@ class BasicBatchScheduler : public BatchScheduler<TaskType> {\n     // To share a thread pool (2) create a scheduler and pass it in.\n \n     // The name to use for the pool of batch threads.\n-    string thread_pool_name = {\"batch_threads\"};\n+    std::string thread_pool_name = {\"batch_threads\"};\n \n     // The number of threads to use to process batches.\n     // Must be >= 1, and should be tuned carefully."
        },
        {
            "sha": "6301f0e1bf111b12d3faa577d979e74adba49118",
            "filename": "tensorflow/core/kernels/batching_util/basic_batch_scheduler_benchmark_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbasic_batch_scheduler_benchmark_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbasic_batch_scheduler_benchmark_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbasic_batch_scheduler_benchmark_test.cc?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -99,11 +99,11 @@ class BenchmarkBatchTask : public BatchTask {\n \n   size_t size() const override { return 1; }\n \n-  uint64 start_time_micros() const { return start_time_micros_; }\n+  uint64_t start_time_micros() const { return start_time_micros_; }\n \n  private:\n   // The time at which the task was created, in microseconds.\n-  const uint64 start_time_micros_;\n+  const uint64_t start_time_micros_;\n };\n \n BenchmarkBatchTask::BenchmarkBatchTask()\n@@ -164,7 +164,7 @@ class LatencyBenchmark {\n   void InjectLoad();\n \n   // Return latency and batch size stat.\n-  string ReportLatencyBatchSz();\n+  std::string ReportLatencyBatchSz();\n \n   // Reset scheduler. This has a side-effect of waiting for all work to be\n   // completed prior to reset.\n@@ -255,15 +255,15 @@ void LatencyBenchmark::InjectLoad() {\n void LatencyBenchmark::ProcessBatch(\n     std::unique_ptr<Batch<BenchmarkBatchTask>> batch) {\n   PerformBatchCpuWork();\n-  const uint64 batch_completion_time = Env::Default()->NowMicros();\n+  const uint64_t batch_completion_time = Env::Default()->NowMicros();\n \n   {\n     mutex_lock l(mu_);\n     batch_size_histogram_.Add(batch->num_tasks());\n   }\n \n   for (int i = 0; i < batch->num_tasks(); ++i) {\n-    const uint64 task_latency_micros =\n+    const uint64_t task_latency_micros =\n         batch_completion_time - batch->task(i).start_time_micros();\n     {\n       mutex_lock l(mu_);\n@@ -280,7 +280,7 @@ void LatencyBenchmark::PerformBatchCpuWork() const {\n   CHECK_NE(dummy, 0);\n }\n \n-string LatencyBenchmark::ReportLatencyBatchSz() {\n+std::string LatencyBenchmark::ReportLatencyBatchSz() {\n   mutex_lock l(mu_);\n   return absl::StrCat(\n       \"lat_p99.9=\", task_latency_millis_histogram_.Percentile(99.9),\n@@ -347,9 +347,9 @@ void LatencyBM(::testing::benchmark::State& state) {\n     scheduler_options.num_batch_threads = state.range(1);\n     scheduler_options.max_enqueued_batches = INT_MAX;  // Unbounded queue.\n     const int kBatchCpuCost = 10 * 1000 * 1000;\n-    const int64 kQps = state.range(2);\n-    const int64 kInjectionIntervalMicros = 1000000 / (kQps / state.threads());\n-    const int64 kNumTasks = latency_benchmark_duration_secs * kQps;\n+    const int64_t kQps = state.range(2);\n+    const int64_t kInjectionIntervalMicros = 1000000 / (kQps / state.threads());\n+    const int64_t kNumTasks = latency_benchmark_duration_secs * kQps;\n     if (kNumTasks <= 10000) {\n       LOG(WARNING) << \"Not enough tasks (\" << kNumTasks << \")\"\n                    << \" to report meaningful 99.9% latency!\""
        },
        {
            "sha": "b00b468cf79ac7d6bff2200b6fe468ce6fca1959",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 46,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -82,8 +82,9 @@ namespace serving {\n namespace {\n \n // TODO(b/181883417): Replace with RecordPaddingSizeV2.\n-void RecordPaddingSize(int32_t padding_size, const string& model_name,\n-                       int32_t execution_batch_size, const string& op_name) {\n+void RecordPaddingSize(int32_t padding_size, const std::string& model_name,\n+                       int32_t execution_batch_size,\n+                       const std::string& op_name) {\n   static auto* cell = tensorflow::monitoring::PercentileSampler<3>::New(\n       {\"/tensorflow/serving/batching/padding_size\",\n        \"Tracks the padding size distribution on batches by model_name (if \"\n@@ -121,16 +122,17 @@ static auto* padding_size_v2_sampler = tensorflow::monitoring::Sampler<3>::New(\n      \"model_name\", \"execution_batch_size\", \"op_name\"},\n     monitoring::Buckets::Explicit(GetBucketLimitsForPaddingSizeV2()));\n \n-void RecordPaddingSizeV2(int32_t padding_size, const string& model_name,\n-                         int32_t execution_batch_size, const string& op_name) {\n+void RecordPaddingSizeV2(int32_t padding_size, const std::string& model_name,\n+                         int32_t execution_batch_size,\n+                         const std::string& op_name) {\n   padding_size_v2_sampler\n       ->GetCell(model_name, absl::StrCat(execution_batch_size), op_name)\n       ->Add(static_cast<double>(padding_size));\n }\n \n // TODO(b/181883417): Replace with RecordInputBatchSizeV2.\n-void RecordInputBatchSize(int32_t batch_size, const string& model_name,\n-                          const string& op_name) {\n+void RecordInputBatchSize(int32_t batch_size, const std::string& model_name,\n+                          const std::string& op_name) {\n   static auto* cell = tensorflow::monitoring::PercentileSampler<2>::New(\n       {\"/tensorflow/serving/batching/input_batch_size\",\n        \"Tracks the batch size distribution on the inputs by model_name (if \"\n@@ -141,8 +143,8 @@ void RecordInputBatchSize(int32_t batch_size, const string& model_name,\n   cell->GetCell(model_name, op_name)->Add(static_cast<double>(batch_size));\n }\n \n-void RecordInputStatsV2(int32_t batch_size, const string& model_name,\n-                        const string& op_name,\n+void RecordInputStatsV2(int32_t batch_size, const std::string& model_name,\n+                        const std::string& op_name,\n                         const tsl::criticality::Criticality& criticality) {\n   static auto* cell = tensorflow::monitoring::Sampler<3>::New(\n       {\"/tensorflow/serving/batching/input_batch_size_v2\",\n@@ -166,8 +168,8 @@ void RecordInputStatsV2(int32_t batch_size, const string& model_name,\n }\n \n // Record the actual batch size without padding.\n-void RecordBatchSize(int32_t batch_size, const string& model_name,\n-                     const string& op_name) {\n+void RecordBatchSize(int32_t batch_size, const std::string& model_name,\n+                     const std::string& op_name) {\n   static auto* cell = tensorflow::monitoring::Sampler<2>::New(\n       {\"/tensorflow/serving/batching/batch_size\",\n        \"Tracks the batch size distribution on the batch result by model_name \"\n@@ -177,8 +179,8 @@ void RecordBatchSize(int32_t batch_size, const string& model_name,\n   cell->GetCell(model_name, op_name)->Add(static_cast<double>(batch_size));\n }\n \n-void RecordProcessedBatchSize(int32_t batch_size, const string& model_name,\n-                              const string& op_name) {\n+void RecordProcessedBatchSize(int32_t batch_size, const std::string& model_name,\n+                              const std::string& op_name) {\n   static auto* cell = tensorflow::monitoring::PercentileSampler<2>::New(\n       {\"/tensorflow/serving/batching/processed_batch_size\",\n        \"Tracks the batch size distribution on processing by model_name (if \"\n@@ -196,16 +198,17 @@ static auto* processed_batch_size_v2_counter = monitoring::Counter<3>::New(\n     \"model_name\", \"op_name\", \"batch_size\");\n \n // Export the exact number instead of the distribution of processed batch size.\n-void RecordProcessedBatchSizeV2(int32_t batch_size, const string& model_name,\n-                                const string& op_name) {\n+void RecordProcessedBatchSizeV2(int32_t batch_size,\n+                                const std::string& model_name,\n+                                const std::string& op_name) {\n   processed_batch_size_v2_counter\n       ->GetCell(model_name, op_name, std::to_string(batch_size))\n       ->IncrementBy(1);\n }\n \n // TODO(b/181883417): Replace with RecordBatchDelayUsV2.\n-void RecordBatchDelayUs(int64_t batch_delay_us, const string& model_name,\n-                        const string& op_name, int32_t batch_size) {\n+void RecordBatchDelayUs(int64_t batch_delay_us, const std::string& model_name,\n+                        const std::string& op_name, int32_t batch_size) {\n   static auto* cell = monitoring::PercentileSampler<3>::New(\n       {\"/tensorflow/serving/batching/batch_delay_us\",\n        \"Tracks the batching delay (in microseconds) for inputs by model_name \"\n@@ -217,8 +220,8 @@ void RecordBatchDelayUs(int64_t batch_delay_us, const string& model_name,\n       ->Add(static_cast<double>(batch_delay_us));\n }\n \n-void RecordBatchDelayUsV2(int64_t batch_delay_us, const string& model_name,\n-                          const string& op_name, int32_t batch_size) {\n+void RecordBatchDelayUsV2(int64_t batch_delay_us, const std::string& model_name,\n+                          const std::string& op_name, int32_t batch_size) {\n   static auto* cell = tensorflow::monitoring::Sampler<3>::New(\n       {\"/tensorflow/serving/batching/batch_delay_us_v2\",\n        \"Tracks the batching delay (in microseconds) for inputs by model_name \"\n@@ -233,7 +236,8 @@ void RecordBatchDelayUsV2(int64_t batch_delay_us, const string& model_name,\n \n void RecordBatchTaskSizeSum(int32_t batch_task_size,\n                             int32_t unbatched_task_size,\n-                            const string& model_name, const string& op_name) {\n+                            const std::string& model_name,\n+                            const std::string& op_name) {\n   static auto* cell = tensorflow::monitoring::Counter<3>::New(\n       \"/tensorflow/serving/batching/batch_task_size_sum\",\n       \"Tracks the sum of the task sizes in a batch.\", \"model_name\", \"op_name\",\n@@ -243,8 +247,8 @@ void RecordBatchTaskSizeSum(int32_t batch_task_size,\n }\n \n void RecordBatchParamBatchTimeoutMicros(int64_t batch_timeout_micros,\n-                                        const string& model_name,\n-                                        const string& op_name) {\n+                                        const std::string& model_name,\n+                                        const std::string& op_name) {\n   static auto* cell = monitoring::Gauge<int64_t, 2>::New(\n       \"/tensorflow/serving/batching/batch_timeout_micros\",\n       \"Tracks how long a request can wait before being processed by a batch.\",\n@@ -253,38 +257,38 @@ void RecordBatchParamBatchTimeoutMicros(int64_t batch_timeout_micros,\n }\n \n void RecordBatchParamMaxBatchSize(int64_t max_batch_size,\n-                                  const string& model_name,\n-                                  const string& op_name) {\n+                                  const std::string& model_name,\n+                                  const std::string& op_name) {\n   static auto* cell = monitoring::Gauge<int64_t, 2>::New(\n       \"/tensorflow/serving/batching/max_batch_size\",\n       \"Tracks the maximum size of a batch.\", \"model_name\", \"op_name\");\n   cell->GetCell(model_name, op_name)->Set(max_batch_size);\n }\n \n-void RecordBatchParamPaddingPolicy(const string& batch_padding_policy,\n-                                   const string& model_name,\n-                                   const string& op_name) {\n-  static auto* cell = monitoring::Gauge<string, 2>::New(\n+void RecordBatchParamPaddingPolicy(const std::string& batch_padding_policy,\n+                                   const std::string& model_name,\n+                                   const std::string& op_name) {\n+  static auto* cell = monitoring::Gauge<std::string, 2>::New(\n       \"/tensorflow/serving/batching/configured_batch_padding_policy\",\n       \"The value of BatchFunction.batch_padding_policy attribute.\",\n       \"model_name\", \"op_name\");\n   cell->GetCell(model_name, op_name)->Set(batch_padding_policy);\n }\n \n void RecordBatchParamMaxEnqueuedBatches(int64_t max_enqueued_batches,\n-                                        const string& model_name,\n-                                        const string& op_name) {\n+                                        const std::string& model_name,\n+                                        const std::string& op_name) {\n   static auto* cell = monitoring::Gauge<int64_t, 2>::New(\n       \"/tensorflow/serving/batching/max_enqueued_batches\",\n       \"Tracks the maximum number of enqueued batches.\", \"model_name\",\n       \"op_name\");\n   cell->GetCell(model_name, op_name)->Set(max_enqueued_batches);\n }\n \n-void RecordBatchParamAllowedBatchSizes(const string& allowed_batch_sizes,\n-                                       const string& model_name,\n-                                       const string& op_name) {\n-  static auto* cell = monitoring::Gauge<string, 2>::New(\n+void RecordBatchParamAllowedBatchSizes(const std::string& allowed_batch_sizes,\n+                                       const std::string& model_name,\n+                                       const std::string& op_name) {\n+  static auto* cell = monitoring::Gauge<std::string, 2>::New(\n       \"/tensorflow/serving/batching/allowed_batch_sizes\",\n       \"Tracks the sizes that are allowed to form a batch.\", \"model_name\",\n       \"op_name\");\n@@ -308,8 +312,8 @@ void RecordBatchCosts(const std::string& model_name,\n       ->Add(absl::ToDoubleMicroseconds(total_cost));\n }\n \n-const string& GetModelName(OpKernelContext* ctx) {\n-  static string* kModelNameUnset = new string(\"model_name_unset\");\n+const std::string& GetModelName(OpKernelContext* ctx) {\n+  static std::string* kModelNameUnset = new std::string(\"model_name_unset\");\n   if (!ctx->session_metadata()) return *kModelNameUnset;\n   if (ctx->session_metadata()->name().empty()) return *kModelNameUnset;\n   return ctx->session_metadata()->name();\n@@ -354,8 +358,8 @@ using ::tensorflow::concat_split_util::Concat;\n using ::tensorflow::concat_split_util::Split;\n using TensorMatrix = std::vector<std::vector<Tensor>>;\n \n-string GetTensorNamesAndShapesString(const OpKernelContext* context,\n-                                     const OpInputList& tensors) {\n+std::string GetTensorNamesAndShapesString(const OpKernelContext* context,\n+                                          const OpInputList& tensors) {\n   std::stringstream out;\n   int i = 0;\n   for (const Tensor& tensor : tensors) {\n@@ -366,7 +370,8 @@ string GetTensorNamesAndShapesString(const OpKernelContext* context,\n }\n \n absl::Status BatchResourceBase::RegisterWarmupInputs(\n-    int64_t guid, OpKernelContext* context, const string& batcher_queue_name,\n+    int64_t guid, OpKernelContext* context,\n+    const std::string& batcher_queue_name,\n     const CreateBatchTaskFn& create_batch_task_fn,\n     AsyncOpKernel::DoneCallback done) {\n   auto shared_status = std::make_shared<ThreadSafeStatus>();\n@@ -402,7 +407,8 @@ absl::Status BatchResourceBase::RegisterWarmupInputs(\n }\n \n absl::Status BatchResourceBase::RegisterInput(\n-    int64_t guid, OpKernelContext* context, const string& batcher_queue_name,\n+    int64_t guid, OpKernelContext* context,\n+    const std::string& batcher_queue_name,\n     const CreateBatchTaskFn& create_batch_task_fn,\n     AsyncOpKernel::DoneCallback done_callback, int forced_warmup_batch_size) {\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<BatchTask> batch_components,\n@@ -539,7 +545,7 @@ absl::Status BatchResourceBase::RegisterInput(\n BatchResourceBase::GetBatcherQueueOptions(\n     int32_t num_batch_threads, int32_t max_batch_size,\n     int32_t batch_timeout_micros, int32_t max_enqueued_batches,\n-    const std::vector<int32>& allowed_batch_sizes,\n+    const std::vector<int32_t>& allowed_batch_sizes,\n     bool enable_large_batch_splitting, bool disable_padding) {\n   return GetBatcherQueueOptions(\n       num_batch_threads, max_batch_size, batch_timeout_micros,\n@@ -558,12 +564,12 @@ BatchResourceBase::GetBatcherQueueOptions(\n BatchResourceBase::GetBatcherQueueOptions(\n     int32_t num_batch_threads, int32_t max_batch_size,\n     int32_t batch_timeout_micros, int32_t max_enqueued_batches,\n-    const std::vector<int32>& allowed_batch_sizes,\n+    const std::vector<int32_t>& allowed_batch_sizes,\n     bool enable_large_batch_splitting, bool disable_padding,\n     absl::string_view batch_padding_policy, int32_t low_priority_max_batch_size,\n     int32_t low_priority_batch_timeout_micros,\n     int32_t low_priority_max_enqueued_batches,\n-    const std::vector<int32>& low_priority_allowed_batch_sizes,\n+    const std::vector<int32_t>& low_priority_allowed_batch_sizes,\n     MixedPriorityBatchingPolicy mixed_priority_batching_policy) {\n   BatcherT::QueueOptions batcher_queue_options;\n   batcher_queue_options.input_batch_size_limit = max_batch_size;\n@@ -629,7 +635,7 @@ BatchResourceBase::GetBatcherQueueOptions(\n BatchResourceBase::GetAdaptiveBatcherQueueOptions(\n     int32_t max_batch_size, int32_t batch_timeout_micros,\n     int32_t max_enqueued_batches, bool enable_large_batch_splitting,\n-    const std::vector<int32>& allowed_batch_sizes, bool disable_padding) {\n+    const std::vector<int32_t>& allowed_batch_sizes, bool disable_padding) {\n   AdaptiveBatcherT::QueueOptions batcher_queue_options;\n   batcher_queue_options.max_input_task_size =\n       std::make_optional(max_batch_size);\n@@ -686,7 +692,7 @@ bool BatchResourceBase::IsLowPriorityBatch(const BatchT& batch) const {\n // returns 'batch_size'.\n int BatchResourceBase::RoundToLowestAllowedBatchSize(\n     int batch_size, bool is_low_priority_batch) const {\n-  const std::vector<int32>& allowed_batch_sizes =\n+  const std::vector<int32_t>& allowed_batch_sizes =\n       is_low_priority_batch ? batcher_queue_options_.low_priority_queue_options\n                                   .allowed_batch_sizes\n                             : allowed_batch_sizes_;\n@@ -1227,8 +1233,8 @@ void BatchResourceBase::ProcessBatchCallBack(\n }\n \n absl::Status BatchResourceBase::LookupOrCreateBatcherQueue(\n-    const string& queue_name, const string& model_name, const string& op_name,\n-    BatcherQueueT** queue) {\n+    const std::string& queue_name, const std::string& model_name,\n+    const std::string& op_name, BatcherQueueT** queue) {\n   mutex_lock l(batcher_queues_mu_);\n \n   auto it = batcher_queues_.find(queue_name);"
        },
        {
            "sha": "936473a1884dc958a5bd2f421f54527692dcade1",
            "filename": "tensorflow/core/kernels/batching_util/batch_scheduler.h",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -109,17 +109,17 @@ class TaskQueue {\n \n   struct TaskWrapper {\n     std::unique_ptr<TaskType> task;\n-    uint64 start_time_micros;\n+    uint64_t start_time_micros;\n \n-    TaskWrapper(std::unique_ptr<TaskType> task, uint64 start_time_micros)\n+    TaskWrapper(std::unique_ptr<TaskType> task, uint64_t start_time_micros)\n         : task(std::move(task)), start_time_micros(start_time_micros) {}\n   };\n \n   // Appends a task to the end of the queue with the given start time.\n-  void AddTask(std::unique_ptr<TaskType> task, uint64 start_time_micros);\n+  void AddTask(std::unique_ptr<TaskType> task, uint64_t start_time_micros);\n \n   // Adds a task to the front of the queue with the given start time.\n-  void PrependTask(std::unique_ptr<TaskType> task, uint64 start_time_micros);\n+  void PrependTask(std::unique_ptr<TaskType> task, uint64_t start_time_micros);\n \n   // Removes a task from the front of the queue, i.e., the oldest task in the\n   // queue.\n@@ -132,7 +132,7 @@ class TaskQueue {\n \n   // Returns the start time of the earliest task in the queue. If the queue is\n   // empty, return the null value.\n-  std::optional<uint64> EarliestTaskStartTime() const;\n+  std::optional<uint64_t> EarliestTaskStartTime() const;\n \n   // Returns true iff the queue contains 0 tasks.\n   bool empty() const;\n@@ -162,7 +162,7 @@ class TaskQueue {\n \n template <typename TaskType>\n void TaskQueue<TaskType>::AddTask(std::unique_ptr<TaskType> task,\n-                                  uint64 start_time_micros) {\n+                                  uint64_t start_time_micros) {\n   {\n     mutex_lock l(mu_);\n     size_ += task->size();\n@@ -173,7 +173,7 @@ void TaskQueue<TaskType>::AddTask(std::unique_ptr<TaskType> task,\n \n template <typename TaskType>\n void TaskQueue<TaskType>::PrependTask(std::unique_ptr<TaskType> task,\n-                                      uint64 start_time_micros) {\n+                                      uint64_t start_time_micros) {\n   {\n     mutex_lock l(mu_);\n     size_ += task->size();\n@@ -233,7 +233,7 @@ bool TaskQueue<TaskType>::empty() const {\n }\n \n template <typename TaskType>\n-std::optional<uint64> TaskQueue<TaskType>::EarliestTaskStartTime() const {\n+std::optional<uint64_t> TaskQueue<TaskType>::EarliestTaskStartTime() const {\n   {\n     mutex_lock l(mu_);\n \n@@ -275,13 +275,13 @@ template <typename TaskType>\n class Batch {\n  public:\n   Batch();\n-  explicit Batch(uint64 traceme_context_id);\n+  explicit Batch(uint64_t traceme_context_id);\n   virtual ~Batch();  // Blocks until the batch is closed.\n \n   // Appends 'task' to the batch. After calling AddTask(), the newly-added task\n   // can be accessed via task(num_tasks()-1) or mutable_task(num_tasks()-1).\n   // Dies if the batch is closed.\n-  void AddTask(std::unique_ptr<TaskType> task, uint64 start_time_micros = 0);\n+  void AddTask(std::unique_ptr<TaskType> task, uint64_t start_time_micros = 0);\n \n   // Removes the most recently added task. Returns nullptr if the batch is\n   // empty.\n@@ -318,7 +318,7 @@ class Batch {\n   void Close();\n \n   // Returns the TraceMe context id of this batch.\n-  uint64 traceme_context_id() const;\n+  uint64_t traceme_context_id() const;\n \n   // Attempts to trim this batch to a new, smaller size (not to be confused with\n   // the number of tasks in the batch). On success, the trimmed tasks go into\n@@ -331,7 +331,7 @@ class Batch {\n \n   // Returns the start time of the earliest task in the queue. If the queue is\n   // empty, return the null value.\n-  std::optional<uint64> EarliestTaskStartTime() const;\n+  std::optional<uint64_t> EarliestTaskStartTime() const;\n \n  private:\n   mutable mutex mu_;\n@@ -348,11 +348,11 @@ class Batch {\n   absl::Notification closed_;\n \n   // The TracMe context id.\n-  const uint64 traceme_context_id_;\n+  const uint64_t traceme_context_id_;\n \n   // The minimum start time of all tasks in the batch.\n   // If the batch is empty, the value is undefined.\n-  uint64 earliest_task_start_time_micros_ TF_GUARDED_BY(mu_);\n+  uint64_t earliest_task_start_time_micros_ TF_GUARDED_BY(mu_);\n \n   Batch(const Batch&) = delete;\n   void operator=(const Batch&) = delete;\n@@ -421,7 +421,7 @@ template <typename TaskType>\n Batch<TaskType>::Batch() : Batch(0) {}\n \n template <typename TaskType>\n-Batch<TaskType>::Batch(uint64 traceme_context_id)\n+Batch<TaskType>::Batch(uint64_t traceme_context_id)\n     : traceme_context_id_(traceme_context_id) {}\n \n template <typename TaskType>\n@@ -431,7 +431,7 @@ Batch<TaskType>::~Batch() {\n \n template <typename TaskType>\n void Batch<TaskType>::AddTask(std::unique_ptr<TaskType> task,\n-                              uint64 start_time_micros) {\n+                              uint64_t start_time_micros) {\n   DCHECK(!IsClosed());\n   {\n     mutex_lock l(mu_);\n@@ -448,7 +448,7 @@ void Batch<TaskType>::AddTask(std::unique_ptr<TaskType> task,\n }\n \n template <typename TaskType>\n-std::optional<uint64> Batch<TaskType>::EarliestTaskStartTime() const {\n+std::optional<uint64_t> Batch<TaskType>::EarliestTaskStartTime() const {\n   {\n     mutex_lock l(mu_);\n     if (tasks_.empty()) {\n@@ -552,7 +552,7 @@ void Batch<TaskType>::Close() {\n }\n \n template <typename TaskType>\n-uint64 Batch<TaskType>::traceme_context_id() const {\n+uint64_t Batch<TaskType>::traceme_context_id() const {\n   return traceme_context_id_;\n }\n \n@@ -567,9 +567,9 @@ void Batch<TaskType>::TryTrimToNewSize(\n   // Index of the first task to trim away. It is possible that it is the index\n   // of a task of size larger than 1 that will have to be split in order to get\n   // to the target new_size.\n-  int32 first_task_to_move = 0;\n+  int32_t first_task_to_move = 0;\n   // The sum of sizes of tasks i, where i < first_task_to_move.\n-  int32 size_of_previous_tasks = 0;\n+  int32_t size_of_previous_tasks = 0;\n   while (size_of_previous_tasks + tasks_[first_task_to_move]->size() <=\n          new_size) {\n     size_of_previous_tasks += tasks_[first_task_to_move]->size();"
        },
        {
            "sha": "c309110f3afa67dfa2ec6db50fa329b826459b9e",
            "filename": "tensorflow/core/kernels/batching_util/batch_scheduler_utils.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.cc?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -26,7 +26,7 @@ namespace tensorflow {\n namespace serving {\n \n int GetNextAllowedBatchSize(int batch_size,\n-                            const std::vector<int32>& allowed_batch_sizes,\n+                            const std::vector<int32_t>& allowed_batch_sizes,\n                             bool disable_padding) {\n   if (disable_padding || allowed_batch_sizes.empty()) {\n     return batch_size;\n@@ -44,9 +44,9 @@ int GetNextAllowedBatchSize(int batch_size,\n   return batch_size;\n }\n \n-int32 GetPrevAllowedBatchSize(int batch_size,\n-                              const std::vector<int32>& allowed_batch_sizes,\n-                              bool disable_padding) {\n+int32_t GetPrevAllowedBatchSize(int batch_size,\n+                                const std::vector<int32_t>& allowed_batch_sizes,\n+                                bool disable_padding) {\n   if (disable_padding || allowed_batch_sizes.empty()) {\n     return batch_size;\n   }"
        },
        {
            "sha": "d9ce8070a168952b705d09ff6f159b4c0a20d364",
            "filename": "tensorflow/core/kernels/batching_util/batch_scheduler_utils.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_scheduler_utils.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -34,13 +34,13 @@ namespace serving {\n // greater than or equal to the given batch size. If allowed_batch_sizes,\n // returns batch_size as is.\n int GetNextAllowedBatchSize(int batch_size,\n-                            const std::vector<int32>& allowed_batch_sizes,\n+                            const std::vector<int32_t>& allowed_batch_sizes,\n                             bool disable_padding);\n \n // Returns the largest allowed batch size that is smaller than or equal to\n // batch_size. Returns batch_size if no such size exists.\n int GetPrevAllowedBatchSize(int batch_size,\n-                            const std::vector<int32>& allowed_batch_sizes,\n+                            const std::vector<int32_t>& allowed_batch_sizes,\n                             bool disable_padding);\n \n // Constants containing possible values for the batch_padding_policy argument\n@@ -69,7 +69,7 @@ inline constexpr absl::string_view kMinimizeTpuCostPerRequestPolicy =\n // out_trimmed_tasks vector in the same order as they were in the batch.\n template <typename TaskType>\n void MaybeBatchDown(Batch<TaskType>& batch,\n-                    const std::vector<int32>& allowed_batch_sizes,\n+                    const std::vector<int32_t>& allowed_batch_sizes,\n                     bool disable_padding,\n                     absl::string_view batch_padding_policy,\n                     ModelBatchStats* model_batch_stats,\n@@ -103,15 +103,15 @@ void MaybeBatchDown(Batch<TaskType>& batch,\n     return;\n   }\n \n-  int32 batch_size = batch.size();\n+  int32_t batch_size = batch.size();\n \n-  int32 pad_up_size =\n+  int32_t pad_up_size =\n       GetNextAllowedBatchSize(batch_size, allowed_batch_sizes, disable_padding);\n   if (pad_up_size == batch_size) {\n     return;  // Good, no padding is necessary.\n   }\n \n-  int32 batch_down_size =\n+  int32_t batch_down_size =\n       GetPrevAllowedBatchSize(batch_size, allowed_batch_sizes, disable_padding);\n   if (batch_down_size == batch_size) {\n     return;  // Can't batch down (e.g. no smaller batch size available)."
        },
        {
            "sha": "22729032d5d6e26d9859b1bde8d4aa92520403ec",
            "filename": "tensorflow/core/kernels/batching_util/batch_stats.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_stats.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_stats.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_stats.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -140,7 +140,7 @@ class ModelBatchStats {\n   // size.\n   //\n   // The returned reference persist for as long as 'this' is alive.\n-  BatchSizeStats& batch_size(int32 batch_size) {\n+  BatchSizeStats& batch_size(int32_t batch_size) {\n     mutex_lock l(mu_);\n     return batch_size_stats_by_batch_size_[batch_size];\n   }\n@@ -161,8 +161,8 @@ class ModelBatchStats {\n   // Returns the list of batch sizes for which this model has statistics.\n   //\n   // The returned list is not guaranteed to be sorted.\n-  std::vector<int32> BatchSizes() const {\n-    std::vector<int32> result;\n+  std::vector<int32_t> BatchSizes() const {\n+    std::vector<int32_t> result;\n     mutex_lock l(mu_);\n     result.reserve(batch_size_stats_by_batch_size_.size());\n     for (const auto& [key, value] : batch_size_stats_by_batch_size_) {\n@@ -198,7 +198,7 @@ class ModelBatchStats {\n   // element deletion is possible because we return references to items in this\n   // map and don't track their lifetime. We are using the node hash map so that\n   // elements, once created, are fixed in memory.\n-  absl::node_hash_map<int32, BatchSizeStats> batch_size_stats_by_batch_size_\n+  absl::node_hash_map<int32_t, BatchSizeStats> batch_size_stats_by_batch_size_\n       TF_GUARDED_BY(mu_);\n \n   // The total count of individual unit-sized queries processed by this model."
        },
        {
            "sha": "d2fc86ac0e826dad8694bcf5acf144df57e61ad5",
            "filename": "tensorflow/core/kernels/batching_util/fake_clock_env.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Ffake_clock_env.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Ffake_clock_env.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Ffake_clock_env.cc?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -40,7 +40,7 @@ void FakeClockEnv::AdvanceByMicroseconds(int micros) {\n   }\n }\n \n-void FakeClockEnv::BlockUntilSleepingThread(uint64 wake_time) {\n+void FakeClockEnv::BlockUntilSleepingThread(uint64_t wake_time) {\n   for (;;) {\n     {\n       mutex_lock l(mu_);\n@@ -67,7 +67,7 @@ void FakeClockEnv::BlockUntilThreadsAsleep(int num_threads) {\n   }\n }\n \n-uint64 FakeClockEnv::NowMicros() const {\n+uint64_t FakeClockEnv::NowMicros() const {\n   {\n     mutex_lock l(mu_);\n     return current_time_;"
        },
        {
            "sha": "739324b2a7cdac4a7cd5adfd520e098f3ff3e69c",
            "filename": "tensorflow/core/kernels/batching_util/fake_clock_env.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Ffake_clock_env.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Ffake_clock_env.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Ffake_clock_env.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -46,22 +46,22 @@ class FakeClockEnv : public EnvWrapper {\n \n   // Blocks until there is a sleeping thread that is scheduled to wake up at\n   // the given (absolute) time.\n-  void BlockUntilSleepingThread(uint64 wake_time);\n+  void BlockUntilSleepingThread(uint64_t wake_time);\n \n   // Blocks until there are at least num_threads sleeping.\n   void BlockUntilThreadsAsleep(int num_threads);\n \n   // Methods that this class implements.\n-  uint64 NowMicros() const override;\n+  uint64_t NowMicros() const override;\n   void SleepForMicroseconds(int64_t micros) override;\n \n  private:\n   mutable mutex mu_;\n \n-  uint64 current_time_ TF_GUARDED_BY(mu_) = 0;\n+  uint64_t current_time_ TF_GUARDED_BY(mu_) = 0;\n \n   struct SleepingThread {\n-    uint64 wake_time;\n+    uint64_t wake_time;\n     absl::Notification* wake_notification;\n   };\n   std::vector<SleepingThread> sleeping_threads_ TF_GUARDED_BY(mu_);"
        },
        {
            "sha": "b72ac33bdbc3ef446d5f4b343f2e2e84d5162cfe",
            "filename": "tensorflow/core/kernels/batching_util/periodic_function.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function.cc?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -29,9 +29,9 @@ PeriodicFunction::PeriodicFunction(absl::AnyInvocable<void()> function,\n                                    const int64_t interval_micros,\n                                    const Options& options)\n     : function_(std::move(function)),\n-      interval_micros_([interval_micros]() -> int64 {\n+      interval_micros_([interval_micros]() -> int64_t {\n         if (interval_micros < 0) {\n-          const string error =\n+          const std::string error =\n               absl::StrCat(\" The value of 'interval_micros' should be >= 0: \",\n                            interval_micros, \". \");\n           DCHECK(false) << error;"
        },
        {
            "sha": "0cd0504ab88bfed1a3bd344a166194af60d7b979",
            "filename": "tensorflow/core/kernels/batching_util/periodic_function.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -82,7 +82,7 @@ class PeriodicFunction {\n \n     // Specifies the thread name prefix (see the description in class\n     // Thread).\n-    string thread_name_prefix = \"periodic_function\";\n+    std::string thread_name_prefix = \"periodic_function\";\n \n     // The environment to use. Does not take ownership, but must remain alive\n     // for as long as the PeriodicFunction exists."
        },
        {
            "sha": "bc908b500ad7d3994cb2652c686ed2a9ad12070d",
            "filename": "tensorflow/core/kernels/batching_util/periodic_function_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fperiodic_function_test.cc?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -46,7 +46,7 @@ using test_util::FakeClockEnv;\n \n void StopPeriodicFunction(PeriodicFunction* periodic_function,\n                           FakeClockEnv* fake_clock_env,\n-                          const uint64 pf_interval_micros) {\n+                          const uint64_t pf_interval_micros) {\n   fake_clock_env->BlockUntilThreadsAsleep(1);\n   internal::PeriodicFunctionTestAccess(periodic_function).NotifyStop();\n   fake_clock_env->AdvanceByMicroseconds(pf_interval_micros);"
        },
        {
            "sha": "64a63c5e6786536f7f2d22f51bf13cb84d9b2c19",
            "filename": "tensorflow/core/kernels/batching_util/serial_device_batch_scheduler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fserial_device_batch_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fserial_device_batch_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fserial_device_batch_scheduler.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -71,7 +71,7 @@ class SerialDeviceBatchScheduler : public std::enable_shared_from_this<\n \n   struct Options {\n     // The name to use for the pool of batch threads.\n-    string thread_pool_name = {\"batch_threads\"};\n+    std::string thread_pool_name = {\"batch_threads\"};\n     // Maximum number of batch processing threads.\n     int64_t num_batch_threads = port::NumSchedulableCPUs();\n     // Although batch selection is primarily based on age, this parameter\n@@ -87,7 +87,7 @@ class SerialDeviceBatchScheduler : public std::enable_shared_from_this<\n     int64_t initial_in_flight_batches_limit = 3;\n     // Returns the current number of batches directly waiting to be processed\n     // by the serial device (i.e. GPU, TPU).\n-    std::function<int64()> get_pending_on_serial_device;\n+    std::function<int64_t()> get_pending_on_serial_device;\n     // Desired average number of batches directly waiting to be processed by the\n     // serial device. Small numbers of O(1) should deliver the best latency.\n     double target_pending = 2;"
        },
        {
            "sha": "d568707754bd894aaa0c2bb8a0d13db24fcaf507",
            "filename": "tensorflow/core/kernels/batching_util/shared_batch_scheduler.h",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler.h?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -133,7 +133,7 @@ class SharedBatchScheduler\n   // TODO(b/25089730): Tune defaults based on best practices as they develop.\n   struct Options {\n     // The name to use for the pool of batch threads.\n-    string thread_pool_name = {\"batch_threads\"};\n+    std::string thread_pool_name = {\"batch_threads\"};\n \n     // The number of threads to use to process batches.\n     // Must be >= 1, and should be tuned carefully.\n@@ -233,15 +233,15 @@ class SharedBatchScheduler\n     size_t max_execution_batch_size = 1000;\n \n     // If non-empty, contains configured batch sizes.\n-    std::vector<int32> allowed_batch_sizes;\n+    std::vector<int32_t> allowed_batch_sizes;\n \n     // If true, the padding will not be appended.\n     bool disable_padding = false;\n \n     // The padding policy to use.\n     //\n     // See the documentation for kPadUpPolicy for details.\n-    string batch_padding_policy = string(kPadUpPolicy);\n+    std::string batch_padding_policy = std::string(kPadUpPolicy);\n \n     // A pointer to a ModelBatchStats instance for this model. To be used for\n     // cost-based padding policy selection.\n@@ -266,7 +266,7 @@ class SharedBatchScheduler\n       // See QueueOptions.max_enqueued_batches\n       size_t max_enqueued_batches = 0;\n       // See QueueOptions.allowed_batch_sizes\n-      std::vector<int32> allowed_batch_sizes;\n+      std::vector<int32_t> allowed_batch_sizes;\n     };\n     // A subset of queue options for high priority input. These options are\n     // currently not being used in favor of the equivalents options at the\n@@ -516,7 +516,7 @@ class Queue {\n   size_t tail_batch_task_size() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n   // Returns the number of enqueued batches.\n-  int64 num_enqueued_batches() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n+  int64_t num_enqueued_batches() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n   // Gets the appropriate batches.\n   std::deque<std::unique_ptr<Batch<TaskType>>>& GetBatches()\n@@ -573,15 +573,15 @@ class Queue {\n       TF_GUARDED_BY(mu_);\n \n   // The counter of the TraceMe context ids.\n-  uint64 traceme_context_id_counter_ TF_GUARDED_BY(mu_) = 0;\n+  uint64_t traceme_context_id_counter_ TF_GUARDED_BY(mu_) = 0;\n \n   // The time at which the first task was added to the open (back-most) batch\n   // in 'high_priority_batches_'. Valid iff that batch contains at least one\n   // task.\n   //\n   // Note that when using a batch padding policy other than PAD_UP, this field\n   // might contain an approximate value.\n-  uint64 open_batch_start_time_micros_ TF_GUARDED_BY(mu_);\n+  uint64_t open_batch_start_time_micros_ TF_GUARDED_BY(mu_);\n \n   // Whether this queue contains a batch that is eligible to be scheduled.\n   // Used to keep track of when to call 'schedulable_batch_callback_'.\n@@ -980,7 +980,7 @@ void Queue<TaskType>::PadOpenBatchWithLowPriorityTasks() {\n       return;\n     }\n \n-    uint64 task_time = low_priority_tasks_.EarliestTaskStartTime().value();\n+    uint64_t task_time = low_priority_tasks_.EarliestTaskStartTime().value();\n     std::unique_ptr<TaskType> task = low_priority_tasks_.RemoveTask();\n \n     const int64_t input_task_size = task->size();\n@@ -1089,11 +1089,11 @@ size_t Queue<TaskType>::SchedulingCapacity() const {\n \n template <typename TaskType>\n size_t Queue<TaskType>::SchedulingCapacityInternal() const {\n-  const int64 num_new_batches_schedulable =\n+  const int64_t num_new_batches_schedulable =\n       static_cast<int64_t>(options_.max_enqueued_batches) -\n       this->num_enqueued_batches();\n-  const int64 execution_batch_size_limit = max_execution_batch_size();\n-  const int64 open_batch_capacity =\n+  const int64_t execution_batch_size_limit = max_execution_batch_size();\n+  const int64_t open_batch_capacity =\n       execution_batch_size_limit - this->tail_batch_task_size();\n   // Note the returned value is guaranteed to be not negative, since\n   // enqueue operation could only happen if queue has enough capacity.\n@@ -1201,7 +1201,7 @@ Queue<TaskType>::ScheduleBatch() {\n       // batch, making it read-only.\n       Batch<TaskType>& old_batch = *batches[0];\n       if (!old_batch.empty()) {\n-        uint64 old_batch_time = old_batch.EarliestTaskStartTime().value();\n+        uint64_t old_batch_time = old_batch.EarliestTaskStartTime().value();\n         std::vector<std::unique_ptr<TaskType>> trimmed_tasks;\n         MaybeBatchDown(\n             /* batch= */ old_batch,\n@@ -1415,7 +1415,7 @@ Queue<TaskType>::PeekBatchPriorityImpl() const {\n   Batch<TaskType>* open_batch = batches.back().get();\n \n   size_t effective_batch_size = open_batch->size();\n-  uint64 effective_start_time_micros = open_batch_start_time_micros_;\n+  uint64_t effective_start_time_micros = open_batch_start_time_micros_;\n   int64_t effective_batch_timeout_micros = options_.batch_timeout_micros;\n   if (effective_batch_size == 0) {\n     // open_batch_start_time_micros_ is not valid for an empty batch.\n@@ -1498,7 +1498,7 @@ size_t Queue<TaskType>::tail_batch_task_size() const {\n }\n \n template <typename TaskType>\n-int64 Queue<TaskType>::num_enqueued_batches() const {\n+int64_t Queue<TaskType>::num_enqueued_batches() const {\n   return GetBatches().size();\n }\n "
        },
        {
            "sha": "6ae76b0d54c842dd5f5b943443791a517422d26d",
            "filename": "tensorflow/core/kernels/batching_util/shared_batch_scheduler_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6d412f2536f9c327afa7f4eb809cfde9093673a/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fshared_batch_scheduler_test.cc?ref=f6d412f2536f9c327afa7f4eb809cfde9093673a",
            "patch": "@@ -2442,7 +2442,7 @@ void BM_QueueSchedule(::testing::benchmark::State& state) {\n   const int queue_index = state.range(1);\n   Queue* queue = (*queues)[queue_index].get();\n \n-  const string label =\n+  const std::string label =\n       absl::StrCat(state.threads(), \"-Threads\", (*queue_labels)[queue_index]);\n   state.SetLabel(label);\n   for (auto s : state) {"
        }
    ],
    "stats": {
        "total": 254,
        "additions": 130,
        "deletions": 124
    }
}