{
    "author": "ZixuanJiang",
    "message": "The compatible factor shardings should not have overlap between axes across different tensors.\n\nPiperOrigin-RevId: 824815687",
    "sha": "402ead44b22580e26de1af5296c2cf0429c1edd4",
    "files": [
        {
            "sha": "170056aaead49d7cfa68e3155677a8dd5335117e",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/mhlo_extensions_test.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/402ead44b22580e26de1af5296c2cf0429c1edd4/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fmhlo_extensions_test.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/402ead44b22580e26de1af5296c2cf0429c1edd4/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fmhlo_extensions_test.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fmhlo_extensions_test.mlir?ref=402ead44b22580e26de1af5296c2cf0429c1edd4",
            "patch": "@@ -68,7 +68,8 @@ func.func @ragged_dot_mode_batch(\n     %arg0: tensor<16x32x64xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"c\"}]>},\n     %arg1: tensor<16x64x8xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}, {\"c\"}, {\"d\"}]>},\n     %arg2: tensor<4xi32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}]>}) -> (tensor<16x32x8xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"d\"}]>}) {\n-  // CHECK: %[[RAGGED_DOT:.*]] = \"mhlo.ragged_dot\"(%arg0, %arg1, %arg2) <{\n+  // CHECK: %[[RESHARD:.*]] = sdy.reshard %arg2 <@mesh_abcd, [{}]> : tensor<4xi32>\n+  // CHECK: %[[RAGGED_DOT:.*]] = \"mhlo.ragged_dot\"(%arg0, %arg1, %[[RESHARD]]) <{\n   // CHECK: }>\n   // CHECK-SAME: sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"d\"}]>]>\n   // CHECK-SAME: sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k], [m])->([i, j, k]) {i=16, j=32, k=8, l=64, m=1} reduction={l}>"
        },
        {
            "sha": "ad893647423a635c33cef7e7930b8e1c152e3631",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/ragged_dot_insert_explicit_reshards.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/402ead44b22580e26de1af5296c2cf0429c1edd4/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fragged_dot_insert_explicit_reshards.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/402ead44b22580e26de1af5296c2cf0429c1edd4/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fragged_dot_insert_explicit_reshards.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fragged_dot_insert_explicit_reshards.mlir?ref=402ead44b22580e26de1af5296c2cf0429c1edd4",
            "patch": "@@ -62,7 +62,8 @@ func.func @ragged_dot_mode_batch(\n     %arg0: tensor<16x32x64xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"c\"}]>},\n     %arg1: tensor<16x64x8xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}, {\"c\"}, {\"d\"}]>},\n     %arg2: tensor<4xi32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}]>}) -> tensor<16x32x8xf32> {\n-  // CHECK: %[[RAGGED_DOT:.*]] = \"mhlo.ragged_dot\"(%arg0, %arg1, %arg2) <{\n+  // CHECK: %[[RESHARD:.*]] = sdy.reshard %arg2 <@mesh_abcd, [{}]> : tensor<4xi32>\n+  // CHECK: %[[RAGGED_DOT:.*]] = \"mhlo.ragged_dot\"(%arg0, %arg1, %[[RESHARD]]) <{\n   // CHECK: }>\n   // CHECK-SAME: {sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"d\"}]>]>\n   // CHECK: %[[ALL_REDUCE:.*]] = sdy.all_reduce {\"c\"} %[[RAGGED_DOT]] out_sharding=<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"d\"}]> : tensor<16x32x8xf32>"
        },
        {
            "sha": "df81b84f627ac13666e82f30b5bf2bb66b820bcc",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/ragged_dot_insert_explicit_reshards_enable_full_version_true.mlir",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/402ead44b22580e26de1af5296c2cf0429c1edd4/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fragged_dot_insert_explicit_reshards_enable_full_version_true.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/402ead44b22580e26de1af5296c2cf0429c1edd4/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fragged_dot_insert_explicit_reshards_enable_full_version_true.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fragged_dot_insert_explicit_reshards_enable_full_version_true.mlir?ref=402ead44b22580e26de1af5296c2cf0429c1edd4",
            "patch": "@@ -64,12 +64,13 @@ func.func @ragged_dot_mode_batch(\n     %arg0: tensor<16x32x64xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"c\"}]>},\n     %arg1: tensor<16x64x8xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}, {\"c\"}, {\"d\"}]>},\n     %arg2: tensor<4xi32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{\"a\"}]>}) -> tensor<16x32x8xf32> {\n-  // CHECK: %[[RAGGED_DOT:.*]] = \"mhlo.ragged_dot\"(%arg0, %arg1, %arg2) <{\n+  // CHECK: %[[RESHARD0:.*]] = sdy.reshard %arg2 <@mesh_abcd, [{}]> : tensor<4xi32>\n+  // CHECK: %[[RAGGED_DOT:.*]] = \"mhlo.ragged_dot\"(%arg0, %arg1, %[[RESHARD0]]) <{\n   // CHECK: }>\n   // CHECK-SAME: {sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"d\"}]>]>\n   // CHECK: %[[ALL_REDUCE:.*]] = sdy.all_reduce {\"c\"} %[[RAGGED_DOT]] out_sharding=<@mesh_abcd, [{\"a\"}, {\"b\"}, {\"d\"}]> : tensor<16x32x8xf32>\n-  // CHECK-NEXT: %[[RESHARD:.*]] = sdy.reshard %[[ALL_REDUCE]] <@mesh_abcd, [{}, {}, {}]> : tensor<16x32x8xf32>\n-  // CHECK: return %[[RESHARD]] : tensor<16x32x8xf32>\n+  // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %[[ALL_REDUCE]] <@mesh_abcd, [{}, {}, {}]> : tensor<16x32x8xf32>\n+  // CHECK: return %[[RESHARD1]] : tensor<16x32x8xf32>\n   %0 = \"mhlo.ragged_dot\"(%arg0, %arg1, %arg2) <{ragged_dot_dimension_numbers =\n     #mhlo.ragged_dot<dot_dimension_numbers = #mhlo.dot<\n     lhs_batching_dimensions = [0], rhs_batching_dimensions = [0],"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 8,
        "deletions": 5
    }
}