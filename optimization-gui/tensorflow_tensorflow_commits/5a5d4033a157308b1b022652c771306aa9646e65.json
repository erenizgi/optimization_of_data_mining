{
    "author": "ezhulenev",
    "message": "[xla:cpu] Extract sort_lib out of sort_thunk\n\nIn preparation for removing legacy runtime sorting function, extract standalone sort_lib out of sort_thunk.\n\n```\nname                                                                                cpu/op         cpu/op      vs base\nBM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:1/process_time    11.65µ ± 0%    11.63µ ± 1%       ~ (p=0.529 n=20)\nBM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:1/process_time    98.66µ ± 1%   100.03µ ± 1%  +1.39% (p=0.000 n=20)\nBM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:1/process_time    125.9µ ± 1%    125.0µ ± 1%  -0.69% (p=0.018 n=20)\nBM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:1/process_time    197.6µ ± 0%    198.6µ ± 1%       ~ (p=0.289 n=20)\nBM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:1/process_time   347.5µ ± 1%    345.1µ ± 1%  -0.69% (p=0.008 n=20)\nBM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:1/process_time   825.2µ ± 0%    824.1µ ± 1%       ~ (p=0.862 n=20)\nBM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:0/process_time    86.00µ ± 0%    86.18µ ± 1%       ~ (p=0.512 n=20)\nBM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:0/process_time    98.08µ ± 1%    99.54µ ± 1%  +1.49% (p=0.000 n=20)\nBM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:0/process_time    125.6µ ± 0%    124.7µ ± 1%  -0.71% (p=0.033 n=20)\nBM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:0/process_time    197.6µ ± 1%    197.1µ ± 1%       ~ (p=0.398 n=20)\nBM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:0/process_time   347.0µ ± 1%    345.4µ ± 1%  -0.47% (p=0.040 n=20)\nBM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:0/process_time   823.9µ ± 0%    824.6µ ± 1%       ~ (p=0.968 n=20)\ngeomean                                                                             167.6µ         167.7µ       +0.05%\n```\n\nPiperOrigin-RevId: 832395116",
    "sha": "5a5d4033a157308b1b022652c771306aa9646e65",
    "files": [
        {
            "sha": "8fb07437cea969745508442f748a9f98e10bc747",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=5a5d4033a157308b1b022652c771306aa9646e65",
            "patch": "@@ -1016,12 +1016,26 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"sort_lib\",\n+    srcs = [\"sort_lib.cc\"],\n+    hdrs = [\"sort_lib.h\"],\n+    deps = [\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n cc_library(\n     name = \"sort_thunk\",\n     srcs = [\"sort_thunk.cc\"],\n     hdrs = [\"sort_thunk.h\"],\n     deps = [\n         \":function_library\",\n+        \":sort_lib\",\n         \":thunk\",\n         \"//xla:shape_util\",\n         \"//xla:util\","
        },
        {
            "sha": "38290c14a361490eaae86db04e6b18410cce6adc",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_lib.cc",
            "status": "added",
            "additions": 701,
            "deletions": 0,
            "changes": 701,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.cc?ref=5a5d4033a157308b1b022652c771306aa9646e65",
            "patch": "@@ -0,0 +1,701 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/runtime/sort_lib.h\"\n+\n+#include <algorithm>\n+#include <array>\n+#include <cstddef>\n+#include <cstdint>\n+#include <cstring>\n+#include <functional>\n+#include <iterator>\n+#include <type_traits>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/base/attributes.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/types/span.h\"\n+\n+namespace xla::cpu::internal {\n+\n+namespace {\n+\n+// We use a lot of template metaprogramming below to be able to construct\n+// iterators with statically known number of compared elements. We support a\n+// limited set of template instantiations that we need in practice.\n+\n+// The size of the largest element we support (std::complex<double>).\n+static constexpr size_t kMaxElementSize = 16;\n+\n+// Type erased storage suitable for storing any primitive type.\n+using ValueStorage = std::array<std::byte, kMaxElementSize>;\n+\n+// Pointers to the input arrays together with their primitive sizes.\n+template <size_t n>\n+class Inputs {\n+ public:\n+  Inputs(absl::Span<std::byte* const> ptrs,\n+         absl::Span<const size_t> primitive_sizes) {\n+    DCHECK_EQ(n, ptrs.size());\n+    DCHECK_EQ(n, primitive_sizes.size());\n+    for (size_t i = 0; i < n; ++i) {\n+      ptrs_and_primitive_sizes_[i] = {ptrs[i], primitive_sizes[i]};\n+    }\n+  }\n+\n+  // Accessing arrays with `operator[]` has zero overheads, so we don't need to\n+  // use pointers to data in contrast to `DInputs` below.\n+\n+  std::byte* ptr(size_t i, size_t offset) const {\n+    DCHECK_LT(i, n) << \"Input index out of bounds\";\n+    auto& [ptr, primitive_size] = ptrs_and_primitive_sizes_[i];\n+    return ptr + offset * primitive_size;\n+  }\n+\n+  size_t primitive_size(size_t i) const {\n+    return ptrs_and_primitive_sizes_[i].second;\n+  }\n+\n+ private:\n+  // Pointers into the input buffers and each input's primitive size. Keep\n+  // pointers and primitives sizes next to each other to avoid cache misses\n+  // on a hot path.\n+  std::array<std::pair<std::byte*, size_t>, n> ptrs_and_primitive_sizes_;\n+};\n+\n+class DInputs {\n+ public:\n+  DInputs(absl::Span<std::byte* const> ptrs,\n+          absl::Span<const size_t> primitive_sizes)\n+      : n_(ptrs.size()), ptrs_and_primitive_sizes_(ptrs.size()) {\n+    DCHECK_EQ(ptrs.size(), primitive_sizes.size());\n+    for (size_t i = 0; i < ptrs.size(); ++i) {\n+      ptrs_and_primitive_sizes_[i] = {ptrs[i], primitive_sizes[i]};\n+    }\n+  }\n+\n+  size_t n() const { return n_; }\n+\n+  // Accessing vectors with `operator[]` is significantly slower than using a\n+  // pointer to data because of libc++ hardening which checks for OOB access on\n+  // every call. We know that we are not going to access out of bounds, so we\n+  // use a pointer to data instead.\n+\n+  std::byte* ptr(size_t i, size_t offset) const {\n+    DCHECK_LT(i, n_) << \"Input index out of bounds\";\n+    auto& [ptr, primitive_size] = ptrs_and_primitive_sizes_.data()[i];\n+    return ptr + offset * primitive_size;\n+  }\n+\n+  size_t primitive_size(size_t i) const {\n+    return ptrs_and_primitive_sizes_.data()[i].second;\n+  }\n+\n+ private:\n+  size_t n_;  // number of sorted inputs\n+\n+  // Pointers into the input buffers and each input's primitive size. Keep\n+  // pointers and primitives sizes next to each other to avoid cache misses\n+  // on a hot path.\n+  std::vector<std::pair<std::byte*, size_t>> ptrs_and_primitive_sizes_;\n+};\n+\n+// Forward declare reference type defined below.\n+template <size_t n>\n+struct Ref;\n+struct DRef;\n+\n+// Value type to store values loaded from the input buffers.\n+template <size_t n>\n+struct Value {\n+  Value(const Ref<n>& ref);  // NOLINT\n+\n+  void FillComparedValues(const void** __restrict compared_values) const;\n+\n+  std::array<ValueStorage, n> values;\n+};\n+\n+struct DValue {\n+  DValue(const DRef& ref);  // NOLINT\n+\n+  void FillComparedValues(const void** __restrict compared_values) const;\n+\n+  std::vector<ValueStorage> values;\n+};\n+\n+// Reference to values stored in the input buffers.\n+template <size_t n>\n+struct Ref {\n+  Ref(const Inputs<n>* inputs, size_t offset)\n+      : inputs(inputs), offset(offset) {}\n+\n+  Ref& operator=(const Value<n>& value);\n+  Ref& operator=(const Ref<n>& other);\n+\n+  void FillComparedValues(const void** __restrict compared_values) const;\n+\n+  std::byte* ptr(size_t i) const { return inputs->ptr(i, offset); }\n+  size_t primitive_size(size_t i) const { return inputs->primitive_size(i); }\n+\n+  const Inputs<n>* inputs;\n+  size_t offset;\n+};\n+\n+struct DRef {\n+  DRef(const DInputs* inputs, size_t offset) : inputs(inputs), offset(offset) {}\n+\n+  DRef& operator=(const DValue& value);\n+  DRef& operator=(const DRef& other);\n+\n+  void FillComparedValues(const void** __restrict compared_values) const;\n+\n+  size_t n() const { return inputs->n(); }\n+  std::byte* ptr(size_t i) const { return inputs->ptr(i, offset); }\n+  size_t primitive_size(size_t i) const { return inputs->primitive_size(i); }\n+\n+  const DInputs* inputs;\n+  size_t offset;\n+};\n+\n+// We know that we can only copy up to 16 bytes for the largest element type\n+// and can specialize `std::memcpy` to allow LLVM to inline it with statically\n+// known sizes.\n+static ABSL_ATTRIBUTE_ALWAYS_INLINE void Memcpy(void* __restrict dest,\n+                                                const void* __restrict src,\n+                                                size_t n) {\n+  switch (n) {\n+    case 1:\n+      std::memcpy(dest, src, 1);\n+      break;\n+    case 2:\n+      std::memcpy(dest, src, 2);\n+      break;\n+    case 4:\n+      std::memcpy(dest, src, 4);\n+      break;\n+    case 8:\n+      std::memcpy(dest, src, 8);\n+      break;\n+    case 16:\n+      std::memcpy(dest, src, 16);\n+      break;\n+    default:\n+      LOG(FATAL) << \"Unsupported memcpy size: \" << n;\n+  }\n+}\n+\n+// Specialize swap for statically known sizes to avoid going through the same\n+// switch statement multiple times.\n+static ABSL_ATTRIBUTE_ALWAYS_INLINE void Swap(void* __restrict a,\n+                                              void* __restrict b, size_t n) {\n+  std::array<std::byte, kMaxElementSize> tmp;\n+  switch (n) {\n+    case 1:\n+      std::memcpy(tmp.data(), a, 1);\n+      std::memcpy(a, b, 1);\n+      std::memcpy(b, tmp.data(), 1);\n+      break;\n+    case 2:\n+      std::memcpy(tmp.data(), a, 2);\n+      std::memcpy(a, b, 2);\n+      std::memcpy(b, tmp.data(), 2);\n+      break;\n+    case 4:\n+      std::memcpy(tmp.data(), a, 4);\n+      std::memcpy(a, b, 4);\n+      std::memcpy(b, tmp.data(), 4);\n+      break;\n+    case 8:\n+      std::memcpy(tmp.data(), a, 8);\n+      std::memcpy(a, b, 8);\n+      std::memcpy(b, tmp.data(), 8);\n+      break;\n+    case 16:\n+      std::memcpy(tmp.data(), a, 16);\n+      std::memcpy(a, b, 16);\n+      std::memcpy(b, tmp.data(), 16);\n+      break;\n+    default:\n+      LOG(FATAL) << \"Unsupported swap size: \" << n;\n+  }\n+}\n+\n+template <size_t n>\n+ABSL_ATTRIBUTE_ALWAYS_INLINE Value<n>::Value(const Ref<n>& ref) {\n+  for (size_t i = 0; i < n; ++i) {\n+    Memcpy(values[i].data(), ref.ptr(i), ref.primitive_size(i));\n+  }\n+}\n+\n+template <size_t n>\n+ABSL_ATTRIBUTE_ALWAYS_INLINE void Value<n>::FillComparedValues(\n+    const void** __restrict compared_values) const {\n+  for (const ValueStorage& value : values) {\n+    *compared_values = value.data();\n+    compared_values += 2;\n+  }\n+}\n+\n+ABSL_ATTRIBUTE_ALWAYS_INLINE DValue::DValue(const DRef& ref) : values(ref.n()) {\n+  for (size_t i = 0, end = ref.n(); i < end; ++i) {\n+    Memcpy(values.data()[i].data(), ref.ptr(i), ref.primitive_size(i));\n+  }\n+}\n+\n+ABSL_ATTRIBUTE_ALWAYS_INLINE void DValue::FillComparedValues(\n+    const void** __restrict compared_values) const {\n+#pragma unroll 8\n+  for (const ValueStorage& value : values) {\n+    *compared_values = value.data();\n+    compared_values += 2;\n+  }\n+}\n+\n+template <size_t n>\n+ABSL_ATTRIBUTE_ALWAYS_INLINE Ref<n>& Ref<n>::operator=(const Value<n>& value) {\n+  for (size_t i = 0; i < n; ++i) {\n+    Memcpy(ptr(i), value.values.data()[i].data(), primitive_size(i));\n+  }\n+  return *this;\n+}\n+\n+template <size_t n>\n+ABSL_ATTRIBUTE_ALWAYS_INLINE Ref<n>& Ref<n>::operator=(const Ref<n>& other) {\n+  for (size_t i = 0; i < n; ++i) {\n+    DCHECK_EQ(primitive_size(i), other.primitive_size(i));\n+    Memcpy(ptr(i), other.ptr(i), primitive_size(i));\n+  }\n+  return *this;\n+}\n+\n+template <size_t n>\n+ABSL_ATTRIBUTE_ALWAYS_INLINE void Ref<n>::FillComparedValues(\n+    const void** __restrict compared_values) const {\n+  for (size_t i = 0; i < n; ++i) {\n+    *compared_values = ptr(i);\n+    compared_values += 2;\n+  }\n+}\n+\n+ABSL_ATTRIBUTE_ALWAYS_INLINE DRef& DRef::operator=(const DValue& value) {\n+  for (size_t i = 0, end = n(); i < end; ++i) {\n+    Memcpy(ptr(i), value.values.data()[i].data(), primitive_size(i));\n+  }\n+  return *this;\n+}\n+\n+ABSL_ATTRIBUTE_ALWAYS_INLINE DRef& DRef::operator=(const DRef& other) {\n+  for (size_t i = 0, end = n(); i < end; ++i) {\n+    DCHECK_EQ(primitive_size(i), other.primitive_size(i));\n+    Memcpy(ptr(i), other.ptr(i), primitive_size(i));\n+  }\n+  return *this;\n+}\n+\n+ABSL_ATTRIBUTE_ALWAYS_INLINE void DRef::FillComparedValues(\n+    const void** __restrict compared_values) const {\n+#pragma unroll 8\n+  for (size_t i = 0, end = n(); i < end; ++i) {\n+    *compared_values = ptr(i);\n+    compared_values += 2;\n+  }\n+}\n+\n+// Swap function required by `std::sort` and `std::stable_sort` implementations.\n+template <size_t n>\n+ABSL_ATTRIBUTE_ALWAYS_INLINE void swap(const Ref<n>& lhs, const Ref<n>& rhs) {\n+  for (size_t i = 0; i < n; ++i) {\n+    DCHECK_EQ(lhs.primitive_size(i), rhs.primitive_size(i));\n+    size_t primitive_size = lhs.primitive_size(i);\n+    Swap(lhs.ptr(i), rhs.ptr(i), primitive_size);\n+  }\n+}\n+\n+ABSL_ATTRIBUTE_ALWAYS_INLINE void swap(const DRef& lhs, const DRef& rhs) {\n+  for (size_t i = 0, end = lhs.n(); i < end; ++i) {\n+    DCHECK_EQ(lhs.primitive_size(i), rhs.primitive_size(i));\n+    size_t primitive_size = lhs.primitive_size(i);\n+    Swap(lhs.ptr(i), rhs.ptr(i), primitive_size);\n+  }\n+}\n+\n+// An array of pointers to the input data.\n+template <size_t n>\n+struct Ptr {\n+  using difference_type = std::ptrdiff_t;\n+\n+  Ptr() = default;\n+\n+  explicit Ptr(const Inputs<n>* inputs, size_t offset = 0)\n+      : inputs(inputs), offset(offset) {}\n+\n+  Ref<n> operator*() const { return Ref<n>{inputs, offset}; }\n+\n+  Ptr& operator+=(difference_type diff) {\n+    offset += diff;\n+    return *this;\n+  }\n+\n+  Ptr& operator-=(difference_type diff) {\n+    offset -= diff;\n+    return *this;\n+  }\n+\n+  Ptr operator+(difference_type diff) const {\n+    return Ptr(inputs, offset + diff);\n+  }\n+\n+  Ptr operator-(difference_type diff) const {\n+    return Ptr(inputs, offset - diff);\n+  }\n+\n+  difference_type operator-(const Ptr& rhs) const {\n+    return offset - rhs.offset;\n+  }\n+\n+  bool operator==(const Ptr& rhs) const { return offset == rhs.offset; }\n+  bool operator!=(const Ptr& rhs) const { return offset != rhs.offset; }\n+  bool operator>(const Ptr& rhs) const { return offset > rhs.offset; }\n+  bool operator<(const Ptr& rhs) const { return offset < rhs.offset; }\n+  bool operator>=(const Ptr& rhs) const { return offset >= rhs.offset; }\n+  bool operator<=(const Ptr& rhs) const { return offset <= rhs.offset; }\n+\n+  const Inputs<n>* inputs;  // pointer to the input arrays\n+  size_t offset;            // offset into the inputs arrays\n+};\n+\n+struct DPtr {\n+  using difference_type = std::ptrdiff_t;\n+\n+  DPtr() = default;\n+\n+  explicit DPtr(const DInputs* inputs, size_t offset = 0)\n+      : inputs(inputs), offset(offset) {}\n+\n+  DRef operator*() const { return DRef{inputs, offset}; }\n+\n+  DPtr& operator+=(difference_type diff) {\n+    offset += diff;\n+    return *this;\n+  }\n+\n+  DPtr& operator-=(difference_type diff) {\n+    offset -= diff;\n+    return *this;\n+  }\n+\n+  DPtr operator+(difference_type diff) const {\n+    return DPtr(inputs, offset + diff);\n+  }\n+\n+  DPtr operator-(difference_type diff) const {\n+    return DPtr(inputs, offset - diff);\n+  }\n+\n+  difference_type operator-(const DPtr& rhs) const {\n+    return offset - rhs.offset;\n+  }\n+\n+  bool operator==(const DPtr& rhs) const { return offset == rhs.offset; }\n+  bool operator!=(const DPtr& rhs) const { return offset != rhs.offset; }\n+  bool operator>(const DPtr& rhs) const { return offset > rhs.offset; }\n+  bool operator<(const DPtr& rhs) const { return offset < rhs.offset; }\n+  bool operator>=(const DPtr& rhs) const { return offset >= rhs.offset; }\n+  bool operator<=(const DPtr& rhs) const { return offset <= rhs.offset; }\n+\n+  const DInputs* inputs;  // pointer to the input arrays\n+  size_t offset;          // offset into the inputs arrays\n+};\n+\n+// We rely on `std::sort` and `std::stable_sort` to sort the raw data. We sort\n+// multiple input buffers together using the same comparator function, so we\n+// need to provide a custom iterator that can access the data of all input\n+// buffers at the same time and swap elements in them.\n+template <class Value, class Ref, class Ptr>\n+class SortIterator {\n+ public:\n+  using iterator_category = std::random_access_iterator_tag;\n+  using difference_type = std::ptrdiff_t;\n+\n+  using value_type = Value;\n+  using reference = Ref;\n+  using pointer = Ptr;\n+\n+  SortIterator() = default;\n+  SortIterator(pointer ptr, difference_type stride)\n+      : ptr_(std::move(ptr)), stride_(stride) {}\n+\n+  SortIterator(const SortIterator& other) = default;\n+  SortIterator& operator=(const SortIterator& other) = default;\n+  SortIterator(SortIterator&& other) = default;\n+  SortIterator& operator=(SortIterator&& other) = default;\n+\n+  reference operator*() const { return *ptr_; }\n+  reference operator[](difference_type diff) const { return *(*this + diff); }\n+\n+  difference_type operator-(const SortIterator& rhs) const {\n+    return (ptr_ - rhs.ptr_) / stride_;\n+  }\n+\n+  SortIterator& operator+=(difference_type diff) {\n+    ptr_ += diff * stride_;\n+    return *this;\n+  }\n+\n+  SortIterator& operator-=(difference_type diff) {\n+    ptr_ -= diff * stride_;\n+    return *this;\n+  }\n+\n+  SortIterator& operator++() {\n+    ptr_ += stride_;\n+    return *this;\n+  }\n+\n+  SortIterator& operator--() {\n+    ptr_ -= stride_;\n+    return *this;\n+  }\n+\n+  SortIterator operator+(difference_type diff) const {\n+    return SortIterator(ptr_ + diff * stride_, stride_);\n+  }\n+\n+  SortIterator operator-(difference_type diff) const {\n+    return SortIterator(ptr_ - diff * stride_, stride_);\n+  }\n+\n+  bool operator==(const SortIterator& rhs) const { return ptr_ == rhs.ptr_; }\n+  bool operator!=(const SortIterator& rhs) const { return ptr_ != rhs.ptr_; }\n+  bool operator>(const SortIterator& rhs) const { return ptr_ > rhs.ptr_; }\n+  bool operator<(const SortIterator& rhs) const { return ptr_ < rhs.ptr_; }\n+  bool operator>=(const SortIterator& rhs) const { return ptr_ >= rhs.ptr_; }\n+  bool operator<=(const SortIterator& rhs) const { return ptr_ <= rhs.ptr_; }\n+\n+ private:\n+  pointer ptr_;\n+  difference_type stride_ = 1;\n+};\n+\n+}  // namespace\n+\n+template <size_t n>\n+static void Sort1DInplace(const SortDims& sort_dims, int64_t offset,\n+                          absl::Span<std::byte* const> data,\n+                          absl::Span<const size_t> primitive_sizes,\n+                          bool is_stable, LessThan* less_than) {\n+  DCHECK_EQ(n, data.size());\n+  DCHECK_EQ(n, primitive_sizes.size());\n+\n+  std::array<std::byte*, n> ptrs;\n+  for (size_t i = 0; i < n; ++i) {\n+    ptrs[i] = data[i] + offset * primitive_sizes[i];\n+  }\n+\n+  Inputs<n> inputs(ptrs, primitive_sizes);\n+\n+  auto compare = [&](const auto& a, const auto& b) {\n+    std::array<const void*, 2 * n> values;\n+    a.FillComparedValues(&values[0]);\n+    b.FillComparedValues(&values[1]);\n+    return (*less_than)(values.data());\n+  };\n+\n+  SortIterator<Value<n>, Ref<n>, Ptr<n>> begin(\n+      Ptr<n>(&inputs), /*stride=*/sort_dims.inner_dim_size);\n+  if (is_stable) {\n+    std::stable_sort(begin, begin + sort_dims.sort_dim_size, compare);\n+  } else {\n+    std::sort(begin, begin + sort_dims.sort_dim_size, compare);\n+  }\n+}\n+\n+static void DSort1DInplace(const SortDims& sort_dims, int64_t offset,\n+                           absl::Span<std::byte* const> data,\n+                           absl::Span<const size_t> primitive_sizes,\n+                           bool is_stable, LessThan* less_than) {\n+  DCHECK_EQ(data.size(), primitive_sizes.size());\n+\n+  std::vector<std::byte*> ptrs(data.size());\n+  for (size_t i = 0; i < data.size(); ++i) {\n+    ptrs[i] = data[i] + offset * primitive_sizes[i];\n+  }\n+\n+  DInputs inputs(std::move(ptrs), primitive_sizes);\n+\n+  // Allocate scratch space for sorted values outside of the lambda to avoid\n+  // allocating it on every call to `compare`.\n+  std::vector<const void*> values(2 * data.size());\n+\n+  auto compare = [&, values = values.data()](const auto& a, const auto& b) {\n+    a.FillComparedValues(&values[0]);\n+    b.FillComparedValues(&values[1]);\n+    return (*less_than)(values);\n+  };\n+\n+  SortIterator<DValue, DRef, DPtr> begin(DPtr(&inputs),\n+                                         /*stride=*/sort_dims.inner_dim_size);\n+  if (is_stable) {\n+    std::stable_sort(begin, begin + sort_dims.sort_dim_size, compare);\n+  } else {\n+    std::sort(begin, begin + sort_dims.sort_dim_size, compare);\n+  }\n+}\n+\n+// Sorts `data` using `less_than` comparator function.\n+void SortInplace(const SortDims& sort_dims, absl::Span<std::byte* const> data,\n+                 absl::Span<const size_t> primitive_sizes, bool is_stable,\n+                 LessThan* less_than) {\n+  // Iterate over all the 1-dimensional slices of the buffers and sort them.\n+  int64_t num_iterations = sort_dims.outer_dim_size * sort_dims.inner_dim_size;\n+\n+  for (int64_t i = 0; i < num_iterations; ++i) {\n+    int64_t inner_idx = i % sort_dims.inner_dim_size;\n+    int64_t offset = inner_idx + (i - inner_idx) * sort_dims.sort_dim_size;\n+\n+    // Use \"sort\" for statically known number of sorted inputs (expected to be\n+    // faster) and \"dsort\" for dynamically known number of sorted inputs.\n+    auto sort = [&](auto num_inputs) {\n+      Sort1DInplace<decltype(num_inputs)::value>(\n+          sort_dims, offset, data, primitive_sizes, is_stable, less_than);\n+    };\n+\n+    switch (data.size()) {\n+      case 1:\n+        sort(std::integral_constant<size_t, 1>{});\n+        break;\n+      case 2:\n+        sort(std::integral_constant<size_t, 2>{});\n+        break;\n+      case 3:\n+        sort(std::integral_constant<size_t, 3>{});\n+        break;\n+      case 4:\n+        sort(std::integral_constant<size_t, 4>{});\n+        break;\n+      case 5:\n+        sort(std::integral_constant<size_t, 5>{});\n+        break;\n+      case 6:\n+        sort(std::integral_constant<size_t, 6>{});\n+        break;\n+      case 7:\n+        sort(std::integral_constant<size_t, 7>{});\n+        break;\n+      case 8:\n+        sort(std::integral_constant<size_t, 8>{});\n+        break;\n+      case 9:\n+        sort(std::integral_constant<size_t, 9>{});\n+        break;\n+      case 10:\n+        sort(std::integral_constant<size_t, 10>{});\n+        break;\n+      case 11:\n+        sort(std::integral_constant<size_t, 11>{});\n+        break;\n+      case 12:\n+        sort(std::integral_constant<size_t, 12>{});\n+        break;\n+      case 13:\n+        sort(std::integral_constant<size_t, 13>{});\n+        break;\n+      case 14:\n+        sort(std::integral_constant<size_t, 14>{});\n+        break;\n+      case 15:\n+        sort(std::integral_constant<size_t, 15>{});\n+        break;\n+      case 16:\n+        sort(std::integral_constant<size_t, 16>{});\n+        break;\n+      default:\n+        DSort1DInplace(sort_dims, offset, data, primitive_sizes, is_stable,\n+                       less_than);\n+        break;\n+    }\n+  }\n+}\n+\n+template <class Iterator, class T>\n+static void Sort1DInplace(Iterator begin, Iterator end, bool is_stable,\n+                          SortDirection direction) {\n+  if (direction == SortDirection::kAscending) {\n+    if (is_stable) {\n+      std::stable_sort(begin, end, std::less<T>());\n+    } else {\n+      std::sort(begin, end, std::less<T>());\n+    }\n+  } else {\n+    if (is_stable) {\n+      std::stable_sort(begin, end, std::greater<T>());\n+    } else {\n+      std::sort(begin, end, std::greater<T>());\n+    }\n+  };\n+}\n+\n+template <typename T>\n+static void Sort1DInplace(const SortDims& sort_dims, int64_t offset, T* data,\n+                          bool is_stable, SortDirection direction) {\n+  T* begin = data + offset;\n+  T* end = begin + sort_dims.sort_dim_size;\n+\n+  if (sort_dims.inner_dim_size == 1) {\n+    Sort1DInplace<T*, T>(begin, end, is_stable, direction);\n+  } else {\n+    using Iterator = internal::SortIterator<T, T&, T*>;\n+    Iterator begin_it(begin, /*stride=*/sort_dims.inner_dim_size);\n+    Iterator end_it = begin_it + sort_dims.sort_dim_size;\n+    Sort1DInplace<Iterator, T>(begin_it, end_it, is_stable, direction);\n+  }\n+}\n+\n+template <typename T>\n+void SortInplace(const SortDims& sort_dims, T* data, bool is_stable,\n+                 SortDirection direction) {\n+  // Iterate over all the 1-dimensional slices of the buffers and sort them.\n+  int64_t num_iterations = sort_dims.outer_dim_size * sort_dims.inner_dim_size;\n+\n+  for (int64_t i = 0; i < num_iterations; ++i) {\n+    int64_t inner_idx = i % sort_dims.inner_dim_size;\n+    int64_t offset = inner_idx + (i - inner_idx) * sort_dims.sort_dim_size;\n+\n+    Sort1DInplace<T>(sort_dims, offset, data, is_stable, direction);\n+  }\n+}\n+\n+// Declare Sort1DInplace for all supported types. Template is instantiated in\n+// the .cc file.\n+#define DEFINE_SORT_INPLACE(T) \\\n+  template void SortInplace<T>(const SortDims&, T*, bool, SortDirection)\n+\n+DEFINE_SORT_INPLACE(float);\n+DEFINE_SORT_INPLACE(double);\n+DEFINE_SORT_INPLACE(int8_t);\n+DEFINE_SORT_INPLACE(int16_t);\n+DEFINE_SORT_INPLACE(int32_t);\n+DEFINE_SORT_INPLACE(int64_t);\n+DEFINE_SORT_INPLACE(uint8_t);\n+DEFINE_SORT_INPLACE(uint16_t);\n+DEFINE_SORT_INPLACE(uint32_t);\n+DEFINE_SORT_INPLACE(uint64_t);\n+\n+#undef DEFINE_SORT_INPLACE\n+\n+}  // namespace xla::cpu::internal"
        },
        {
            "sha": "7b271b21b221ebdb42f26ea7421fd2e40ae0da6c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_lib.h",
            "status": "added",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_lib.h?ref=5a5d4033a157308b1b022652c771306aa9646e65",
            "patch": "@@ -0,0 +1,81 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_SORT_LIB_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_SORT_LIB_H_\n+\n+#include <cstddef>\n+#include <cstdint>\n+\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/types/span.h\"\n+\n+namespace xla::cpu::internal {\n+\n+// Conceptually we have a 3-dimensional shape:\n+//\n+//   [outer_dim_size, sort_dim_size, inner_dim_size]\n+//\n+// We sort `outer_dim_size * inner_dim_size` vectors of length `sort_dim_size`,\n+// by iterating over `data` memory and calling `std::sort` (or\n+// `std::stable_sort`) on each (strided) slice of the buffer.\n+struct SortDims {\n+  int64_t outer_dim_size;\n+  int64_t sort_dim_size;\n+  int64_t inner_dim_size;\n+};\n+\n+// For trivial sort functors (computation with two parameters that are\n+// compared using `LT` or `GT` direction) we can define sort as a enum. We use\n+// it for performance optimization to be able to inline the sort function.\n+enum class SortDirection {\n+  kAscending,\n+  kDescending,\n+};\n+\n+// Sorts `data` using `less_than` comparator function. Data is sorted in place,\n+// and sort dimensions are specified in `sort_dims`.\n+using LessThan = absl::AnyInvocable<bool(const void** data)>;\n+void SortInplace(const SortDims& sort_dims, absl::Span<std::byte* const> data,\n+                 absl::Span<const size_t> primitive_sizes, bool is_stable,\n+                 LessThan* less_than);\n+\n+// Sorts `data` using the sort `direction` with builtin comparator functions.\n+// This is more efficient, as the comparator can be inlined.\n+template <typename T>\n+void SortInplace(const SortDims& sort_dims, T* data, bool is_stable,\n+                 SortDirection direction);\n+\n+// Declare SortInplace for all supported types. Template is instantiated in\n+// the .cc file.\n+#define DECLARE_SORT_INPLACE(T) \\\n+  extern template void SortInplace<T>(const SortDims&, T*, bool, SortDirection)\n+\n+DECLARE_SORT_INPLACE(float);\n+DECLARE_SORT_INPLACE(double);\n+DECLARE_SORT_INPLACE(int8_t);\n+DECLARE_SORT_INPLACE(int16_t);\n+DECLARE_SORT_INPLACE(int32_t);\n+DECLARE_SORT_INPLACE(int64_t);\n+DECLARE_SORT_INPLACE(uint8_t);\n+DECLARE_SORT_INPLACE(uint16_t);\n+DECLARE_SORT_INPLACE(uint32_t);\n+DECLARE_SORT_INPLACE(uint64_t);\n+\n+#undef DECLARE_SORT_INPLACE\n+\n+}  // namespace xla::cpu::internal\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_SORT_LIB_H_"
        },
        {
            "sha": "972bfbed1eb61dc924e3aacb4a0dd70d6e9561df",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_thunk.cc",
            "status": "modified",
            "additions": 83,
            "deletions": 725,
            "changes": 808,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc?ref=5a5d4033a157308b1b022652c771306aa9646e65",
            "patch": "@@ -15,8 +15,6 @@ limitations under the License.\n \n #include \"xla/backends/cpu/runtime/sort_thunk.h\"\n \n-#include <algorithm>\n-#include <array>\n #include <cassert>\n #include <cstddef>\n #include <cstdint>\n@@ -26,23 +24,21 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n-#include <type_traits>\n #include <utility>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n-#include \"absl/base/attributes.h\"\n #include \"absl/base/call_once.h\"\n #include \"absl/base/dynamic_annotations.h\"\n #include \"absl/base/optimization.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/cpu/runtime/function_library.h\"\n+#include \"xla/backends/cpu/runtime/sort_lib.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/primitive_util.h\"\n@@ -60,8 +56,42 @@ limitations under the License.\n \n namespace xla::cpu {\n \n-static absl::Status VerifySortInputs(absl::Span<const SortThunk::Input> inputs,\n-                                     int64_t dimension) {\n+// Conceptually we have a 3-dimensional shape:\n+//\n+//   [outer_dim_size, sort_dim_size, inner_dim_size]\n+//\n+// We sort `outer_dim_size * inner_dim_size` vectors of length\n+// `sort_dim_size`, by iterating over `data` memory and calling `std::sort`\n+// (or `std::stable_sort`) on each (strided) slice of the buffer.\n+static SortThunk::SortDims GetSortDims(const Shape& shape, int64_t dimension) {\n+  int64_t sort_dimension =\n+      dimension >= 0 ? dimension : shape.dimensions().size() + dimension;\n+\n+  // We need to normalize shape + layout into a descending layout, so that we\n+  // can compute access strides according to the physical layout.\n+  Shape physical_shape =\n+      ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(shape);\n+\n+  // Map `sort_dimension` from logical to physical.\n+  auto logical_to_physical = LayoutUtil::MakeLogicalToPhysical(shape.layout());\n+  sort_dimension = logical_to_physical[sort_dimension];\n+\n+  auto product = [](absl::Span<const int64_t> dims) {\n+    return absl::c_accumulate(dims, int64_t{1}, std::multiplies<>());\n+  };\n+\n+  // Use physical dimensions to compute access strides.\n+  absl::Span<const int64_t> dimensions = physical_shape.dimensions();\n+\n+  int64_t outer_dim_size = product(dimensions.subspan(0, sort_dimension));\n+  int64_t sort_dim_size = dimensions[sort_dimension];\n+  int64_t inner_dim_size = product(dimensions.subspan(sort_dimension + 1));\n+\n+  return SortThunk::SortDims{outer_dim_size, sort_dim_size, inner_dim_size};\n+}\n+\n+static absl::StatusOr<SortThunk::SortDims> VerifySortInputs(\n+    absl::Span<const SortThunk::Input> inputs, int64_t dimension) {\n   // We should have at least one input buffer.\n   if (inputs.empty()) {\n     return Internal(\"Inputs must not be empty\");\n@@ -86,768 +116,96 @@ static absl::Status VerifySortInputs(absl::Span<const SortThunk::Input> inputs,\n         absl::StrJoin(shape.dimensions(), \",\"), dimension);\n   }\n \n-  return absl::OkStatus();\n+  return GetSortDims(inputs[0].shape, dimension);\n }\n \n absl::StatusOr<std::unique_ptr<SortThunk>> SortThunk::Create(\n     Info info, absl::Span<const Input> inputs, int64_t dimension,\n     bool is_stable, LessThan less_than,\n     std::optional<SortDirection> direction) {\n-  TF_RETURN_IF_ERROR(VerifySortInputs(inputs, dimension));\n+  TF_ASSIGN_OR_RETURN(auto sort_dims, VerifySortInputs(inputs, dimension));\n   return absl::WrapUnique(new SortThunk(std::move(info), inputs, dimension,\n                                         is_stable, std::move(less_than),\n-                                        direction));\n+                                        sort_dims, direction));\n }\n \n absl::StatusOr<std::unique_ptr<SortThunk>> SortThunk::Create(\n     Info info, absl::Span<const Input> inputs, int64_t dimension,\n     bool is_stable, std::string comparator_name,\n     std::optional<SortDirection> direction) {\n-  TF_RETURN_IF_ERROR(VerifySortInputs(inputs, dimension));\n+  TF_ASSIGN_OR_RETURN(auto sort_dims, VerifySortInputs(inputs, dimension));\n   return absl::WrapUnique(new SortThunk(std::move(info), inputs, dimension,\n                                         is_stable, std::move(comparator_name),\n-                                        direction));\n+                                        sort_dims, direction));\n }\n \n SortThunk::SortThunk(Info info, absl::Span<const Input> inputs,\n                      int64_t dimension, bool is_stable, LessThan less_than,\n-                     std::optional<SortDirection> direction)\n+                     SortDims sort_dims, std::optional<SortDirection> direction)\n     : Thunk(Kind::kSort, std::move(info)),\n       inputs_(inputs.begin(), inputs.end()),\n       dimension_(dimension),\n       is_stable_(is_stable),\n+      sort_dims_(sort_dims),\n       direction_(direction),\n       less_than_(std::move(less_than)) {}\n \n SortThunk::SortThunk(Info info, absl::Span<const Input> inputs,\n                      int64_t dimension, bool is_stable,\n-                     std::string comparator_name,\n+                     std::string comparator_name, SortDims sort_dims,\n                      std::optional<SortDirection> direction)\n     : Thunk(Kind::kSort, std::move(info)),\n       inputs_(inputs.begin(), inputs.end()),\n       dimension_(dimension),\n       is_stable_(is_stable),\n+      sort_dims_(sort_dims),\n       direction_(direction),\n       comparator_name_(std::move(comparator_name)) {}\n \n-namespace {\n-\n-// We use a lot of template metaprogramming below to be able to construct\n-// iterators with statically known number of compared elements. We support a\n-// limited set of template instantiations that we need in practice.\n-\n-// The size of the largest element we support (std::complex<double>).\n-static constexpr size_t kMaxElementSize = 16;\n-\n-// Type erased storage suitable for storing any primitive type.\n-using ValueStorage = std::array<std::byte, kMaxElementSize>;\n-\n-// Pointers to the input arrays together with their primitive sizes.\n-template <size_t n>\n-class Inputs {\n- public:\n-  Inputs(std::array<std::byte*, n> ptrs,\n-         std::array<size_t, n> primitive_sizes) {\n-    for (size_t i = 0; i < n; ++i) {\n-      ptrs_and_primitive_sizes_[i] = {ptrs[i], primitive_sizes[i]};\n-    }\n-  }\n-\n-  // Accessing arrays with `operator[]` has zero overheads, so we don't need to\n-  // use pointers to data in contrast to `DInputs` below.\n-\n-  std::byte* ptr(size_t i, size_t offset) const {\n-    DCHECK_LT(i, n) << \"Input index out of bounds\";\n-    auto& [ptr, primitive_size] = ptrs_and_primitive_sizes_[i];\n-    return ptr + offset * primitive_size;\n-  }\n-\n-  size_t primitive_size(size_t i) const {\n-    return ptrs_and_primitive_sizes_[i].second;\n-  }\n-\n- private:\n-  // Pointers into the input buffers and each input's primitive size. Keep\n-  // pointers and primitives sizes next to each other to avoid cache misses\n-  // on a hot path.\n-  std::array<std::pair<std::byte*, size_t>, n> ptrs_and_primitive_sizes_;\n-};\n-\n-class DInputs {\n- public:\n-  DInputs(std::vector<std::byte*> ptrs, std::vector<size_t> primitive_sizes)\n-      : n_(ptrs.size()), ptrs_and_primitive_sizes_(ptrs.size()) {\n-    DCHECK_EQ(ptrs.size(), primitive_sizes.size());\n-    for (size_t i = 0; i < ptrs.size(); ++i) {\n-      ptrs_and_primitive_sizes_[i] = {ptrs[i], primitive_sizes[i]};\n-    }\n-  }\n-\n-  size_t n() const { return n_; }\n-\n-  // Accessing vectors with `operator[]` is significantly slower than using a\n-  // pointer to data because of libc++ hardening which checks for OOB access on\n-  // every call. We know that we are not going to access out of bounds, so we\n-  // use a pointer to data instead.\n-\n-  std::byte* ptr(size_t i, size_t offset) const {\n-    DCHECK_LT(i, n_) << \"Input index out of bounds\";\n-    auto& [ptr, primitive_size] = ptrs_and_primitive_sizes_.data()[i];\n-    return ptr + offset * primitive_size;\n-  }\n-\n-  size_t primitive_size(size_t i) const {\n-    return ptrs_and_primitive_sizes_.data()[i].second;\n-  }\n-\n- private:\n-  size_t n_;  // number of sorted inputs\n-\n-  // Pointers into the input buffers and each input's primitive size. Keep\n-  // pointers and primitives sizes next to each other to avoid cache misses\n-  // on a hot path.\n-  std::vector<std::pair<std::byte*, size_t>> ptrs_and_primitive_sizes_;\n-};\n-\n-// Forward declare reference type defined below.\n-template <size_t n>\n-struct Ref;\n-struct DRef;\n-\n-// Value type to store values loaded from the input buffers.\n-template <size_t n>\n-struct Value {\n-  Value(const Ref<n>& ref);  // NOLINT\n-\n-  void FillComparedValues(const void** __restrict compared_values) const;\n-\n-  std::array<ValueStorage, n> values;\n-};\n-\n-struct DValue {\n-  DValue(const DRef& ref);  // NOLINT\n-\n-  void FillComparedValues(const void** __restrict compared_values) const;\n-\n-  std::vector<ValueStorage> values;\n-};\n-\n-// Reference to values stored in the input buffers.\n-template <size_t n>\n-struct Ref {\n-  Ref(const Inputs<n>* inputs, size_t offset)\n-      : inputs(inputs), offset(offset) {}\n-\n-  Ref& operator=(const Value<n>& value);\n-  Ref& operator=(const Ref<n>& other);\n-\n-  void FillComparedValues(const void** __restrict compared_values) const;\n-\n-  std::byte* ptr(size_t i) const { return inputs->ptr(i, offset); }\n-  size_t primitive_size(size_t i) const { return inputs->primitive_size(i); }\n-\n-  const Inputs<n>* inputs;\n-  size_t offset;\n-};\n-\n-struct DRef {\n-  DRef(const DInputs* inputs, size_t offset) : inputs(inputs), offset(offset) {}\n-\n-  DRef& operator=(const DValue& value);\n-  DRef& operator=(const DRef& other);\n-\n-  void FillComparedValues(const void** __restrict compared_values) const;\n-\n-  size_t n() const { return inputs->n(); }\n-  std::byte* ptr(size_t i) const { return inputs->ptr(i, offset); }\n-  size_t primitive_size(size_t i) const { return inputs->primitive_size(i); }\n-\n-  const DInputs* inputs;\n-  size_t offset;\n-};\n-\n-// We know that we can only copy up to 16 bytes for the largest element type\n-// and can specialize `std::memcpy` to allow LLVM to inline it with statically\n-// known sizes.\n-static ABSL_ATTRIBUTE_ALWAYS_INLINE void Memcpy(void* __restrict dest,\n-                                                const void* __restrict src,\n-                                                size_t n) {\n-  switch (n) {\n-    case 1:\n-      std::memcpy(dest, src, 1);\n-      break;\n-    case 2:\n-      std::memcpy(dest, src, 2);\n-      break;\n-    case 4:\n-      std::memcpy(dest, src, 4);\n-      break;\n-    case 8:\n-      std::memcpy(dest, src, 8);\n-      break;\n-    case 16:\n-      std::memcpy(dest, src, 16);\n-      break;\n-    default:\n-      LOG(FATAL) << \"Unsupported memcpy size: \" << n;\n-  }\n-}\n-\n-// Specialize swap for statically known sizes to avoid going through the same\n-// switch statement multiple times.\n-static ABSL_ATTRIBUTE_ALWAYS_INLINE void Swap(void* __restrict a,\n-                                              void* __restrict b, size_t n) {\n-  std::array<std::byte, kMaxElementSize> tmp;\n-  switch (n) {\n-    case 1:\n-      std::memcpy(tmp.data(), a, 1);\n-      std::memcpy(a, b, 1);\n-      std::memcpy(b, tmp.data(), 1);\n-      break;\n-    case 2:\n-      std::memcpy(tmp.data(), a, 2);\n-      std::memcpy(a, b, 2);\n-      std::memcpy(b, tmp.data(), 2);\n-      break;\n-    case 4:\n-      std::memcpy(tmp.data(), a, 4);\n-      std::memcpy(a, b, 4);\n-      std::memcpy(b, tmp.data(), 4);\n-      break;\n-    case 8:\n-      std::memcpy(tmp.data(), a, 8);\n-      std::memcpy(a, b, 8);\n-      std::memcpy(b, tmp.data(), 8);\n-      break;\n-    case 16:\n-      std::memcpy(tmp.data(), a, 16);\n-      std::memcpy(a, b, 16);\n-      std::memcpy(b, tmp.data(), 16);\n-      break;\n-    default:\n-      LOG(FATAL) << \"Unsupported swap size: \" << n;\n-  }\n-}\n-\n-template <size_t n>\n-ABSL_ATTRIBUTE_ALWAYS_INLINE Value<n>::Value(const Ref<n>& ref) {\n-  for (size_t i = 0; i < n; ++i) {\n-    Memcpy(values[i].data(), ref.ptr(i), ref.primitive_size(i));\n-  }\n-}\n-\n-template <size_t n>\n-ABSL_ATTRIBUTE_ALWAYS_INLINE void Value<n>::FillComparedValues(\n-    const void** __restrict compared_values) const {\n-  for (const ValueStorage& value : values) {\n-    *compared_values = value.data();\n-    compared_values += 2;\n-  }\n-}\n-\n-ABSL_ATTRIBUTE_ALWAYS_INLINE DValue::DValue(const DRef& ref) : values(ref.n()) {\n-  for (size_t i = 0, end = ref.n(); i < end; ++i) {\n-    Memcpy(values.data()[i].data(), ref.ptr(i), ref.primitive_size(i));\n-  }\n-}\n-\n-ABSL_ATTRIBUTE_ALWAYS_INLINE void DValue::FillComparedValues(\n-    const void** __restrict compared_values) const {\n-#pragma unroll 8\n-  for (const ValueStorage& value : values) {\n-    *compared_values = value.data();\n-    compared_values += 2;\n-  }\n-}\n-\n-template <size_t n>\n-ABSL_ATTRIBUTE_ALWAYS_INLINE Ref<n>& Ref<n>::operator=(const Value<n>& value) {\n-  for (size_t i = 0; i < n; ++i) {\n-    Memcpy(ptr(i), value.values.data()[i].data(), primitive_size(i));\n-  }\n-  return *this;\n-}\n-\n-template <size_t n>\n-ABSL_ATTRIBUTE_ALWAYS_INLINE Ref<n>& Ref<n>::operator=(const Ref<n>& other) {\n-  for (size_t i = 0; i < n; ++i) {\n-    DCHECK_EQ(primitive_size(i), other.primitive_size(i));\n-    Memcpy(ptr(i), other.ptr(i), primitive_size(i));\n-  }\n-  return *this;\n-}\n-\n-template <size_t n>\n-ABSL_ATTRIBUTE_ALWAYS_INLINE void Ref<n>::FillComparedValues(\n-    const void** __restrict compared_values) const {\n-  for (size_t i = 0; i < n; ++i) {\n-    *compared_values = ptr(i);\n-    compared_values += 2;\n-  }\n-}\n-\n-ABSL_ATTRIBUTE_ALWAYS_INLINE DRef& DRef::operator=(const DValue& value) {\n-  for (size_t i = 0, end = n(); i < end; ++i) {\n-    Memcpy(ptr(i), value.values.data()[i].data(), primitive_size(i));\n-  }\n-  return *this;\n-}\n-\n-ABSL_ATTRIBUTE_ALWAYS_INLINE DRef& DRef::operator=(const DRef& other) {\n-  for (size_t i = 0, end = n(); i < end; ++i) {\n-    DCHECK_EQ(primitive_size(i), other.primitive_size(i));\n-    Memcpy(ptr(i), other.ptr(i), primitive_size(i));\n-  }\n-  return *this;\n-}\n-\n-ABSL_ATTRIBUTE_ALWAYS_INLINE void DRef::FillComparedValues(\n-    const void** __restrict compared_values) const {\n-#pragma unroll 8\n-  for (size_t i = 0, end = n(); i < end; ++i) {\n-    *compared_values = ptr(i);\n-    compared_values += 2;\n-  }\n-}\n-\n-// Swap function required by `std::sort` and `std::stable_sort` implementations.\n-template <size_t n>\n-ABSL_ATTRIBUTE_ALWAYS_INLINE void swap(const Ref<n>& lhs, const Ref<n>& rhs) {\n-  for (size_t i = 0; i < n; ++i) {\n-    DCHECK_EQ(lhs.primitive_size(i), rhs.primitive_size(i));\n-    size_t primitive_size = lhs.primitive_size(i);\n-    Swap(lhs.ptr(i), rhs.ptr(i), primitive_size);\n-  }\n-}\n-\n-ABSL_ATTRIBUTE_ALWAYS_INLINE void swap(const DRef& lhs, const DRef& rhs) {\n-  for (size_t i = 0, end = lhs.n(); i < end; ++i) {\n-    DCHECK_EQ(lhs.primitive_size(i), rhs.primitive_size(i));\n-    size_t primitive_size = lhs.primitive_size(i);\n-    Swap(lhs.ptr(i), rhs.ptr(i), primitive_size);\n-  }\n-}\n-\n-// An array of pointers to the input data.\n-template <size_t n>\n-struct Ptr {\n-  using difference_type = std::ptrdiff_t;\n-\n-  Ptr() = default;\n-\n-  explicit Ptr(const Inputs<n>* inputs, size_t offset = 0)\n-      : inputs(inputs), offset(offset) {}\n-\n-  Ref<n> operator*() const { return Ref<n>{inputs, offset}; }\n-\n-  Ptr& operator+=(difference_type diff) {\n-    offset += diff;\n-    return *this;\n-  }\n-\n-  Ptr& operator-=(difference_type diff) {\n-    offset -= diff;\n-    return *this;\n-  }\n-\n-  Ptr operator+(difference_type diff) const {\n-    return Ptr(inputs, offset + diff);\n-  }\n-\n-  Ptr operator-(difference_type diff) const {\n-    return Ptr(inputs, offset - diff);\n-  }\n-\n-  difference_type operator-(const Ptr& rhs) const {\n-    return offset - rhs.offset;\n-  }\n-\n-  bool operator==(const Ptr& rhs) const { return offset == rhs.offset; }\n-  bool operator!=(const Ptr& rhs) const { return offset != rhs.offset; }\n-  bool operator>(const Ptr& rhs) const { return offset > rhs.offset; }\n-  bool operator<(const Ptr& rhs) const { return offset < rhs.offset; }\n-  bool operator>=(const Ptr& rhs) const { return offset >= rhs.offset; }\n-  bool operator<=(const Ptr& rhs) const { return offset <= rhs.offset; }\n-\n-  const Inputs<n>* inputs;  // pointer to the input arrays\n-  size_t offset;            // offset into the inputs arrays\n-};\n-\n-struct DPtr {\n-  using difference_type = std::ptrdiff_t;\n-\n-  DPtr() = default;\n-\n-  explicit DPtr(const DInputs* inputs, size_t offset = 0)\n-      : inputs(inputs), offset(offset) {}\n-\n-  DRef operator*() const { return DRef{inputs, offset}; }\n-\n-  DPtr& operator+=(difference_type diff) {\n-    offset += diff;\n-    return *this;\n-  }\n-\n-  DPtr& operator-=(difference_type diff) {\n-    offset -= diff;\n-    return *this;\n-  }\n-\n-  DPtr operator+(difference_type diff) const {\n-    return DPtr(inputs, offset + diff);\n-  }\n-\n-  DPtr operator-(difference_type diff) const {\n-    return DPtr(inputs, offset - diff);\n-  }\n-\n-  difference_type operator-(const DPtr& rhs) const {\n-    return offset - rhs.offset;\n-  }\n-\n-  bool operator==(const DPtr& rhs) const { return offset == rhs.offset; }\n-  bool operator!=(const DPtr& rhs) const { return offset != rhs.offset; }\n-  bool operator>(const DPtr& rhs) const { return offset > rhs.offset; }\n-  bool operator<(const DPtr& rhs) const { return offset < rhs.offset; }\n-  bool operator>=(const DPtr& rhs) const { return offset >= rhs.offset; }\n-  bool operator<=(const DPtr& rhs) const { return offset <= rhs.offset; }\n-\n-  const DInputs* inputs;  // pointer to the input arrays\n-  size_t offset;          // offset into the inputs arrays\n-};\n-\n-// We rely on `std::sort` and `std::stable_sort` to sort the raw data. We sort\n-// multiple input buffers together using the same comparator function, so we\n-// need to provide a custom iterator that can access the data of all input\n-// buffers at the same time and swap elements in them.\n-template <class Value, class Ref, class Ptr>\n-class SortIterator {\n- public:\n-  using iterator_category = std::random_access_iterator_tag;\n-  using difference_type = std::ptrdiff_t;\n-\n-  using value_type = Value;\n-  using reference = Ref;\n-  using pointer = Ptr;\n-\n-  SortIterator() = default;\n-  SortIterator(pointer ptr, difference_type stride)\n-      : ptr_(std::move(ptr)), stride_(stride) {}\n-\n-  SortIterator(const SortIterator& other) = default;\n-  SortIterator& operator=(const SortIterator& other) = default;\n-  SortIterator(SortIterator&& other) = default;\n-  SortIterator& operator=(SortIterator&& other) = default;\n-\n-  reference operator*() const { return *ptr_; }\n-  reference operator[](difference_type diff) const { return *(*this + diff); }\n-\n-  difference_type operator-(const SortIterator& rhs) const {\n-    return (ptr_ - rhs.ptr_) / stride_;\n-  }\n-\n-  SortIterator& operator+=(difference_type diff) {\n-    ptr_ += diff * stride_;\n-    return *this;\n-  }\n-\n-  SortIterator& operator-=(difference_type diff) {\n-    ptr_ -= diff * stride_;\n-    return *this;\n-  }\n-\n-  SortIterator& operator++() {\n-    ptr_ += stride_;\n-    return *this;\n-  }\n-\n-  SortIterator& operator--() {\n-    ptr_ -= stride_;\n-    return *this;\n-  }\n-\n-  SortIterator operator+(difference_type diff) const {\n-    return SortIterator(ptr_ + diff * stride_, stride_);\n-  }\n-\n-  SortIterator operator-(difference_type diff) const {\n-    return SortIterator(ptr_ - diff * stride_, stride_);\n-  }\n-\n-  bool operator==(const SortIterator& rhs) const { return ptr_ == rhs.ptr_; }\n-  bool operator!=(const SortIterator& rhs) const { return ptr_ != rhs.ptr_; }\n-  bool operator>(const SortIterator& rhs) const { return ptr_ > rhs.ptr_; }\n-  bool operator<(const SortIterator& rhs) const { return ptr_ < rhs.ptr_; }\n-  bool operator>=(const SortIterator& rhs) const { return ptr_ >= rhs.ptr_; }\n-  bool operator<=(const SortIterator& rhs) const { return ptr_ <= rhs.ptr_; }\n-\n- private:\n-  pointer ptr_;\n-  difference_type stride_ = 1;\n-};\n-\n-struct SortDims {\n-  int64_t outer_dim_size;\n-  int64_t sort_dim_size;\n-  int64_t inner_dim_size;\n-  int64_t num_iterations;\n-};\n-\n-}  // namespace\n-\n-// Conceptually we have a 3-dimensional shape:\n-//\n-//   [outer_dim_size, sort_dim_size, inner_dim_size]\n-//\n-// We sort `outer_dim_size * inner_dim_size` vectors of length\n-// `sort_dim_size`, by iterating over `data` memory and calling `std::sort`\n-// (or `std::stable_sort`) on each (strided) slice of the buffer.\n-static SortDims GetSortDims(const Shape& shape, int64_t dimension) {\n-  int64_t sort_dimension =\n-      dimension >= 0 ? dimension : shape.dimensions().size() + dimension;\n-\n-  // We need to normalize shape + layout into a descending layout, so that we\n-  // can compute access strides according to the physical layout.\n-  Shape physical_shape =\n-      ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(shape);\n-\n-  // Map `sort_dimension` from logical to physical.\n-  auto logical_to_physical = LayoutUtil::MakeLogicalToPhysical(shape.layout());\n-  sort_dimension = logical_to_physical[sort_dimension];\n-\n-  auto product = [](absl::Span<const int64_t> dims) {\n-    return absl::c_accumulate(dims, int64_t{1}, std::multiplies<>());\n-  };\n-\n-  // Use physical dimensions to compute access strides.\n-  absl::Span<const int64_t> dimensions = physical_shape.dimensions();\n-\n-  int64_t outer_dim_size = product(dimensions.subspan(0, sort_dimension));\n-  int64_t sort_dim_size = dimensions[sort_dimension];\n-  int64_t inner_dim_size = product(dimensions.subspan(sort_dimension + 1));\n-  int64_t num_iterations = outer_dim_size * inner_dim_size;\n-\n-  return SortDims{outer_dim_size, sort_dim_size, inner_dim_size,\n-                  num_iterations};\n-}\n-\n-template <class Iterator, class NativeT>\n-static void Sort1DArrInplace(int64_t sort_dims_size, int64_t offset,\n-                             Iterator begin, bool is_stable,\n-                             SortThunk::SortDirection direction) {\n-  if (direction == SortThunk::SortDirection::kAscending) {\n-    if (is_stable) {\n-      std::stable_sort(begin, begin + sort_dims_size, std::less<NativeT>());\n-    } else {\n-      std::sort(begin, begin + sort_dims_size, std::less<NativeT>());\n-    }\n-  } else {\n-    if (is_stable) {\n-      std::stable_sort(begin, begin + sort_dims_size, std::greater<NativeT>());\n-    } else {\n-      std::sort(begin, begin + sort_dims_size, std::greater<NativeT>());\n-    }\n-  };\n-}\n-\n-// The most efficient way to sort a single buffer is to use the builtin\n-// comparator functions.\n-template <PrimitiveType Type>\n-static void Sort1DArrInplace(const SortDims& sort_dims, int64_t offset,\n-                             absl::Span<se::DeviceMemoryBase> data,\n-                             bool is_stable,\n-                             SortThunk::SortDirection direction) {\n-  using NativeT = typename primitive_util::PrimitiveTypeToNative<Type>::type;\n-  DCHECK_EQ(data.size(), 1);\n-  NativeT* begin = reinterpret_cast<NativeT*>(data[0].opaque()) + offset;\n-\n-  if (sort_dims.inner_dim_size == 1) {\n-    Sort1DArrInplace<NativeT*, NativeT>(sort_dims.sort_dim_size, offset, begin,\n-                                        is_stable, direction);\n-  } else {\n-    using Iterator = SortIterator<NativeT, NativeT&, NativeT*>;\n-    Iterator begin_iter(begin, /*stride=*/sort_dims.inner_dim_size);\n-    Sort1DArrInplace<Iterator, NativeT>(sort_dims.sort_dim_size, offset,\n-                                        begin_iter, is_stable, direction);\n-  }\n-}\n-\n-// Sorts `n` buffers in place.\n-template <size_t n>\n-static void SortInplace(const SortDims& sort_dims, int64_t offset,\n+// Sorts `data` of the given `shape` along the `dimension` inplace.\n+static void SortInplace(const SortThunk::SortDims& sort_dims,\n                         absl::Span<se::DeviceMemoryBase> data,\n                         absl::Span<const Shape> shapes, bool is_stable,\n-                        SortThunk::LessThan* less_than) {\n-  std::array<std::byte*, n> ptrs;\n-  std::array<size_t, n> primitive_sizes;\n-\n-  for (size_t i = 0; i < n; ++i) {\n-    std::byte* base = reinterpret_cast<std::byte*>(data[i].opaque());\n-    primitive_sizes[i] = primitive_util::ByteWidth(shapes[i].element_type());\n-    ptrs[i] = base + offset * primitive_sizes[i];\n-  }\n-\n-  Inputs<n> inputs(ptrs, primitive_sizes);\n-\n-  auto compare = [&](const auto& a, const auto& b) {\n-    std::array<const void*, 2 * n> values;\n-    a.FillComparedValues(&values[0]);\n-    b.FillComparedValues(&values[1]);\n-    return (*less_than)(values.data());\n-  };\n-\n-  SortIterator<Value<n>, Ref<n>, Ptr<n>> begin(\n-      Ptr<n>(&inputs), /*stride=*/sort_dims.inner_dim_size);\n-  if (is_stable) {\n-    std::stable_sort(begin, begin + sort_dims.sort_dim_size, compare);\n-  } else {\n-    std::sort(begin, begin + sort_dims.sort_dim_size, compare);\n-  }\n-}\n+                        SortThunk::LessThan* less_than,\n+                        std::optional<SortThunk::SortDirection> direction) {\n+  absl::InlinedVector<std::byte*, 16> raw_data;\n+  absl::c_transform(data, std::back_inserter(raw_data),\n+                    [](const se::DeviceMemoryBase& mem) {\n+                      return reinterpret_cast<std::byte*>(mem.opaque());\n+                    });\n+\n+  absl::InlinedVector<size_t, 16> primitive_sizes;\n+  absl::c_transform(shapes, std::back_inserter(primitive_sizes),\n+                    [](const Shape& shape) {\n+                      return primitive_util::ByteWidth(shape.element_type());\n+                    });\n+\n+  if (raw_data.size() == 1 && direction.has_value()) {\n+    primitive_util::ArrayTypeSwitch(\n+        [&](auto type) {\n+          if constexpr ((primitive_util::IsFloatingPointType(type) &&\n+                         primitive_util::BitWidth(type) >= 32) ||\n+                        (primitive_util::IsIntegralType(type) &&\n+                         primitive_util::BitWidth(type) >= 8)) {\n+            using T = primitive_util::NativeTypeOf<type>;\n+            internal::SortInplace<T>(sort_dims,\n+                                     reinterpret_cast<T*>(raw_data[0]),\n+                                     is_stable, *direction);\n+          } else {\n+            internal::SortInplace(sort_dims, raw_data, primitive_sizes,\n+                                  is_stable, less_than);\n+          }\n+        },\n+        shapes[0].element_type());\n \n-static void DSortInplace(const SortDims& sort_dims, int64_t offset,\n-                         absl::Span<se::DeviceMemoryBase> data,\n-                         absl::Span<const Shape> shapes, bool is_stable,\n-                         SortThunk::LessThan* less_than, size_t n) {\n-  std::vector<std::byte*> ptrs(n);\n-  std::vector<size_t> primitive_sizes(n);\n-\n-  for (size_t i = 0; i < n; ++i) {\n-    std::byte* base = reinterpret_cast<std::byte*>(data[i].opaque());\n-    primitive_sizes[i] = primitive_util::ByteWidth(shapes[i].element_type());\n-    ptrs[i] = base + offset * primitive_sizes[i];\n-  }\n-\n-  DInputs inputs(std::move(ptrs), std::move(primitive_sizes));\n-\n-  // Allocate scratch space for sorted values outside of the lambda to avoid\n-  // allocating it on every call to `compare`.\n-  std::vector<const void*> values(2 * n);\n-\n-  auto compare = [&, values = values.data()](const auto& a, const auto& b) {\n-    a.FillComparedValues(&values[0]);\n-    b.FillComparedValues(&values[1]);\n-    return (*less_than)(values);\n-  };\n-\n-  SortIterator<DValue, DRef, DPtr> begin(DPtr(&inputs),\n-                                         /*stride=*/sort_dims.inner_dim_size);\n-  if (is_stable) {\n-    std::stable_sort(begin, begin + sort_dims.sort_dim_size, compare);\n   } else {\n-    std::sort(begin, begin + sort_dims.sort_dim_size, compare);\n+    internal::SortInplace(sort_dims, raw_data, primitive_sizes, is_stable,\n+                          less_than);\n   }\n }\n \n-// Sorts `data` of the given `shape` along the `dimension` inplace.\n-static absl::Status SortInplace(\n-    absl::Span<se::DeviceMemoryBase> data, absl::Span<const Shape> shapes,\n-    int64_t dimension, bool is_stable, SortThunk::LessThan* less_than,\n-    std::optional<SortThunk::SortDirection> direction) {\n-  // All inputs have the same dimensions and layout, so we can use the first\n-  // shape to get the sort dimensions.\n-  SortDims sort_dims = GetSortDims(shapes[0], dimension);\n-\n-  // Iterate over all the 1-dimensional slices of the buffers and sort them.\n-  for (int64_t i = 0; i < sort_dims.num_iterations; ++i) {\n-    int64_t inner_idx = i % sort_dims.inner_dim_size;\n-    int64_t offset = inner_idx + (i - inner_idx) * sort_dims.sort_dim_size;\n-\n-    auto sort = [&](auto num_inputs) {\n-      SortInplace<decltype(num_inputs)::value>(sort_dims, offset, data, shapes,\n-                                               is_stable, less_than);\n-    };\n-\n-    auto dsort = [&](size_t num_inputs) {\n-      DSortInplace(sort_dims, offset, data, shapes, is_stable, less_than,\n-                   num_inputs);\n-    };\n-\n-    // Sorts array using builtin comparator functor\n-    auto builtin_sort = [&](PrimitiveType type,\n-                            SortThunk::SortDirection direction) {\n-      primitive_util::ArrayTypeSwitch(\n-          [&](auto cst_type) {\n-            if constexpr ((primitive_util::IsFloatingPointType(cst_type) ||\n-                           primitive_util::IsIntegralType(cst_type)) &&\n-                          primitive_util::BitWidth(cst_type) >= 8) {\n-              Sort1DArrInplace<cst_type>(sort_dims, offset, data, is_stable,\n-                                         direction);\n-            } else {\n-              sort(std::integral_constant<size_t, 1>{});\n-            }\n-          },\n-          type);\n-    };\n-\n-    // Use \"sort\" for statically known number of sorted inputs (expected to be\n-    // faster) and \"dsort\" for dynamically known number of sorted inputs.\n-    switch (data.size()) {\n-      case 1:\n-        DCHECK_EQ(shapes.size(), 1);\n-        if (direction.has_value()) {\n-          builtin_sort(shapes[0].element_type(), *direction);\n-        } else {\n-          sort(std::integral_constant<size_t, 1>{});\n-        }\n-        break;\n-      case 2:\n-        sort(std::integral_constant<size_t, 2>{});\n-        break;\n-      case 3:\n-        sort(std::integral_constant<size_t, 3>{});\n-        break;\n-      case 4:\n-        sort(std::integral_constant<size_t, 4>{});\n-        break;\n-      case 5:\n-        sort(std::integral_constant<size_t, 5>{});\n-        break;\n-      case 6:\n-        sort(std::integral_constant<size_t, 6>{});\n-        break;\n-      case 7:\n-        sort(std::integral_constant<size_t, 7>{});\n-        break;\n-      case 8:\n-        sort(std::integral_constant<size_t, 8>{});\n-        break;\n-      case 9:\n-        sort(std::integral_constant<size_t, 9>{});\n-        break;\n-      case 10:\n-        sort(std::integral_constant<size_t, 10>{});\n-        break;\n-      case 11:\n-        sort(std::integral_constant<size_t, 11>{});\n-        break;\n-      case 12:\n-        sort(std::integral_constant<size_t, 12>{});\n-        break;\n-      case 13:\n-        sort(std::integral_constant<size_t, 13>{});\n-        break;\n-      case 14:\n-        sort(std::integral_constant<size_t, 14>{});\n-        break;\n-      case 15:\n-        sort(std::integral_constant<size_t, 15>{});\n-        break;\n-      case 16:\n-        sort(std::integral_constant<size_t, 16>{});\n-        break;\n-      default:\n-        dsort(data.size());\n-        break;\n-    }\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n tsl::AsyncValueRef<SortThunk::ExecuteEvent> SortThunk::Execute(\n     const ExecuteParams& params) {\n-\n   VLOG(3) << absl::StreamFormat(\n       \"Sort %d inputs along dimension %d (is_stable=%v)\", inputs_.size(),\n       dimension_, is_stable_);\n@@ -901,8 +259,8 @@ tsl::AsyncValueRef<SortThunk::ExecuteEvent> SortThunk::Execute(\n   TF_RETURN_IF_ERROR(less_than_.status());\n   LessThan* less_than = &less_than_.value();\n \n-  TF_RETURN_IF_ERROR(SortInplace(absl::MakeSpan(data), shapes, dimension_,\n-                                 is_stable_, less_than, direction_));\n+  SortInplace(sort_dims_, absl::MakeSpan(data), shapes, is_stable_, less_than,\n+              direction_);\n \n   return OkExecuteEvent();\n }"
        },
        {
            "sha": "4f5544834fb0573186165ec018409558f478980f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_thunk.h",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.h?ref=5a5d4033a157308b1b022652c771306aa9646e65",
            "patch": "@@ -25,8 +25,11 @@ limitations under the License.\n #include \"absl/base/call_once.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/cpu/runtime/sort_lib.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n \n@@ -36,12 +39,9 @@ namespace xla::cpu {\n // less-than comparator function.\n class SortThunk final : public Thunk {\n  public:\n-  using LessThan = absl::AnyInvocable<bool(const void** data)>;\n-\n-  enum class SortDirection {\n-    kAscending,\n-    kDescending,\n-  };\n+  using LessThan = internal::LessThan;\n+  using SortDims = internal::SortDims;\n+  using SortDirection = internal::SortDirection;\n \n   struct Input {\n     BufferAllocation::Slice slice;\n@@ -62,27 +62,31 @@ class SortThunk final : public Thunk {\n \n   BufferUses buffer_uses() const final;\n \n-  std::optional<SortDirection> direction() const { return direction_; }\n   int64_t dimension() const { return dimension_; }\n   bool is_stable() const { return is_stable_; }\n-  const std::vector<Input>& inputs() const { return inputs_; }\n \n-  const std::string& comparator_name() const { return comparator_name_; }\n+  absl::Span<const Input> inputs() const { return inputs_; }\n \n+  absl::string_view comparator_name() const { return comparator_name_; }\n   bool has_less_than() const { return less_than_.ok(); }\n \n+  const SortDims& sort_dims() const { return sort_dims_; }\n+  std::optional<SortDirection> direction() const { return direction_; }\n+\n  private:\n   SortThunk(Info info, absl::Span<const Input> inputs, int64_t dimension,\n-            bool is_stable, LessThan less_than,\n+            bool is_stable, LessThan less_than, SortDims sort_dims,\n             std::optional<SortDirection> direction);\n \n   SortThunk(Info info, absl::Span<const Input> inputs, int64_t dimension,\n-            bool is_stable, std::string comparator_name,\n+            bool is_stable, std::string comparator_name, SortDims sort_dims,\n             std::optional<SortDirection> direction);\n \n   std::vector<Input> inputs_;\n   int64_t dimension_;\n   bool is_stable_;\n+\n+  SortDims sort_dims_;\n   std::optional<SortDirection> direction_;\n \n   // Name of the comparator function, lazily resolved to a comparator function"
        },
        {
            "sha": "9f9928ad99c3afd940b5709bec0b52c6c11e13c0",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a5d4033a157308b1b022652c771306aa9646e65/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk_test.cc?ref=5a5d4033a157308b1b022652c771306aa9646e65",
            "patch": "@@ -327,7 +327,9 @@ void BM_Sort1D(benchmark::State& state) {\n   // Use sort direction to activate the most efficient sorting function, or fall\n   // back on the comparator functor.\n   std::optional<SortThunk::SortDirection> direction;\n-  if (sort_ascending) direction = SortThunk::SortDirection::kAscending;\n+  if (sort_ascending) {\n+    direction = SortThunk::SortDirection::kAscending;\n+  }\n \n   auto [alloc, dummy_alloc] = CreateBufferAllocation(*data, *dummy_data);\n   auto [slice, dummy_slice] = CreateBufferAllocationSlice(alloc, dummy_alloc);"
        }
    ],
    "stats": {
        "total": 1634,
        "additions": 897,
        "deletions": 737
    }
}