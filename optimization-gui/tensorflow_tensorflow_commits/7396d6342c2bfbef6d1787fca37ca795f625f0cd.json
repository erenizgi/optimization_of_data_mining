{
    "author": "penpornk",
    "message": "[xla:cpu:onednn] Guard all XLA-oneDNN-related passes with its runtime flag\n\nPiperOrigin-RevId: 816691723",
    "sha": "7396d6342c2bfbef6d1787fca37ca795f625f0cd",
    "files": [
        {
            "sha": "c054e6597343e4f2899f5caa9f295f457603cd70",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=7396d6342c2bfbef6d1787fca37ca795f625f0cd",
            "patch": "@@ -5632,6 +5632,7 @@ cc_library(\n     copts = tsl_copts(),\n     deps = [\n         \":hlo_creation_utils\",\n+        \"//xla:xla_proto_cc\",\n         \"//xla/hlo/pass:hlo_pass\",\n     ] + if_onednn([\n         \"//xla/service/cpu:onednn_contraction_rewriter\","
        },
        {
            "sha": "2f004ec2b3d625cb7c2f5bd1c1eb44a0b5f883bd",
            "filename": "third_party/xla/xla/service/change_op_data_type.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc?ref=7396d6342c2bfbef6d1787fca37ca795f625f0cd",
            "patch": "@@ -18,8 +18,10 @@ limitations under the License.\n #include <optional>\n \n #include \"xla/service/hlo_creation_utils.h\"\n+\n #ifdef XLA_ONEDNN\n #include \"xla/service/cpu/onednn_contraction_rewriter.h\"\n+#include \"xla/xla.pb.h\"\n #endif  // XLA_ONEDNN\n \n namespace xla {\n@@ -62,11 +64,17 @@ absl::StatusOr<bool> ChangeOpDataType::Run(\n       if (it == to_type_map_.end()) {\n         continue;\n       }\n+\n #ifdef XLA_ONEDNN\n-      if (cpu::OneDnnContractionRewriter::ShouldRewriteInstr(instr, true)) {\n+      // TODO(penporn): Move this logic outside of this pass.\n+      const DebugOptions& debug_options = module->config().debug_options();\n+      if (debug_options.xla_cpu_use_onednn() &&\n+          !debug_options.xla_cpu_experimental_onednn_custom_call() &&\n+          cpu::OneDnnContractionRewriter::ShouldRewriteInstr(instr, true)) {\n         continue;\n       }\n #endif  // XLA_ONEDNN\n+\n       const PrimitiveType to_type = it->second;\n       absl::InlinedVector<HloInstruction*, 8> new_operands;\n       for (HloInstruction* operand : instr->mutable_operands()) {"
        },
        {
            "sha": "dfa8a446eaa45c9535b7af35cf0cd6d00e344163",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=7396d6342c2bfbef6d1787fca37ca795f625f0cd",
            "patch": "@@ -191,6 +191,7 @@ cc_library(\n         \":buffer_info_util\",\n         \":conv_canonicalization\",\n         \":cpu_aot_compilation_result\",\n+        \":cpu_aot_loader\",\n         \":cpu_executable\",\n         \":cpu_float_support\",\n         \":cpu_instruction_fusion\",\n@@ -334,7 +335,6 @@ cc_library(\n         \"//xla/service:while_loop_constant_sinking\",\n         \"//xla/service:while_loop_invariant_code_motion\",\n         \"//xla/service:while_loop_simplifier\",\n-        \"//xla/service/cpu:cpu_aot_loader\",\n         \"//xla/service/llvm_ir:llvm_command_line_options\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/service/spmd:stateful_rng_spmd_partitioner\","
        },
        {
            "sha": "893d75c2e57b6257915dc795084ce68915c0f55f",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 18,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7396d6342c2bfbef6d1787fca37ca795f625f0cd/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=7396d6342c2bfbef6d1787fca37ca795f625f0cd",
            "patch": "@@ -337,7 +337,7 @@ ModuleComputationsTransitivelyContainCustomCall(const HloModule& module) {\n namespace cpu {\n \n inline bool IsOneDnnCompatible(bool is_aot_compile) {\n-#ifdef ENABLE_ONEDNN_ASYNC\n+#if defined(XLA_ONEDNN) && defined(ENABLE_ONEDNN_ASYNC)\n   return !is_aot_compile;\n #endif\n   return false;\n@@ -471,7 +471,7 @@ void AddHloVerifier(HloPassPipeline* pipeline, HloVerifierOpts&& opts = {},\n \n std::unique_ptr<HloPassFix<HloPassPipeline>> CreateSimplificationPipeline(\n     absl::string_view name, HloModule* module, bool is_fusion_emitters,\n-    bool is_onednn_compatible) {\n+    bool use_onednn_custom_call) {\n   // Run the following passes to a fixed point.\n   auto pipeline =\n       std::make_unique<HloPassFix<HloPassPipeline>>(std::string(name));\n@@ -485,7 +485,7 @@ std::unique_ptr<HloPassFix<HloPassPipeline>> CreateSimplificationPipeline(\n       !module->config().debug_options().xla_cpu_enable_fast_min_max());\n   options.set_supports_non_canonical_dots(false);\n   options.set_executing_on_cpu(true);\n-  options.set_enable_onednn_support(is_onednn_compatible);\n+  options.set_enable_onednn_support(use_onednn_custom_call);\n   options.set_rewrite_no_op_bitcast_convert_to_bitcast(true);\n   pipeline->AddPass<AlgebraicSimplifier>(options);\n   pipeline->AddPass<SortSimplifier>();\n@@ -540,7 +540,6 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   const bool is_fusion_emitters =\n       module->config().debug_options().xla_cpu_use_fusion_emitters();\n   bool use_shardy_partitioner = module->config().use_shardy_partitioner();\n-  bool is_onednn_compatible = IsOneDnnCompatible(is_aot_compile);\n   bool flatten_before_fusion = !options::FlattenAfterFusion(module->config());\n \n   if (num_partitions > 1) {\n@@ -680,12 +679,12 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   pipeline.AddPass<DotDecomposer>();\n \n   // Rewrite to custom calls with target as oneDNN library calls.\n-#ifdef XLA_ONEDNN\n   bool use_onednn_custom_call =\n       module->config()\n           .debug_options()\n           .xla_cpu_experimental_onednn_custom_call() &&\n-      is_onednn_compatible;\n+      IsOneDnnCompatible(is_aot_compile);\n+#ifdef XLA_ONEDNN\n   if (use_onednn_custom_call) {\n     // Placing OneDnnOpsRewriter here to match the flax patterns\n     // TODO: Decide where would be the appropriate place for this pass to make\n@@ -707,7 +706,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n                                target_machine_features);\n #ifdef XLA_ONEDNN\n   OneDnnFloatSupport onednn_bf16_support(BF16);\n-  if (is_onednn_compatible) {\n+  if (use_onednn_custom_call) {\n     pipeline.AddPass<FloatNormalization>(&onednn_bf16_support);\n   } else {\n     pipeline.AddPass<FloatNormalization>(&bf16_support);\n@@ -804,7 +803,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   }\n \n   pipeline.AddPass(CreateSimplificationPipeline(\n-      \"simplification\", module, is_fusion_emitters, is_onednn_compatible));\n+      \"simplification\", module, is_fusion_emitters, use_onednn_custom_call));\n \n   // Scatter expander is sandwiched between two simplification pipelines to\n   // enable constant folding with the original scatter instructions (which is\n@@ -822,7 +821,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n \n   pipeline.AddPass(CreateSimplificationPipeline(\n       \"post_scatter_expansion_simplification\", module, is_fusion_emitters,\n-      is_onednn_compatible));\n+      use_onednn_custom_call));\n \n   pipeline.AddPass<BitcastDtypesExpander>();\n \n@@ -878,7 +877,6 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n     const CompileOptions& compile_options) {\n   const auto& debug_options = module->config().debug_options();\n   const bool is_fusion_emitters = debug_options.xla_cpu_use_fusion_emitters();\n-  bool is_onednn_compatible = IsOneDnnCompatible(is_aot_compile);\n   bool flatten_after_fusion = options::FlattenAfterFusion(module->config());\n   HloPassPipeline pipeline(\"HLO passes after layout assignment\");\n \n@@ -902,21 +900,21 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n           ? module->config().intra_op_parallelism_threads()\n           : tsl::port::NumSchedulableCPUs();\n \n-#ifdef XLA_ONEDNN\n-  // AOT compiled code runs in single thread.\n   bool use_onednn_custom_call =\n       debug_options.xla_cpu_experimental_onednn_custom_call() &&\n-      is_onednn_compatible;\n-  bool use_onednn_graph =\n-      debug_options.xla_cpu_use_onednn() &&\n-      (!debug_options.xla_cpu_experimental_onednn_fusion_type().empty());\n+      IsOneDnnCompatible(is_aot_compile);\n+\n+#ifdef XLA_ONEDNN\n   if (use_onednn_custom_call) {\n     // Run SimplifyFPConversions pass to simplify the BF16 pattern and make it\n     // easier to match.\n     // Remove `f32 -> bf16 -> f32` casts inserted by bf16 normalization.\n     if (debug_options.xla_allow_excess_precision()) {\n       pipeline.AddPass<SimplifyFPConversions>();\n     }\n+    bool use_onednn_graph =\n+        debug_options.xla_cpu_use_onednn() &&\n+        (!debug_options.xla_cpu_experimental_onednn_fusion_type().empty());\n     pipeline.AddPass<OneDnnContractionRewriter>(\n         max_parallelism, compile_options.thread_pool, use_onednn_graph);\n     // Run SimplifyFPConversions pass again to remove redundant Convert ops\n@@ -986,7 +984,7 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n   // Run this to a fixed point.\n   [&pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n        \"simplification after layout assignment\"),\n-   &module, is_onednn_compatible] {\n+   &module, use_onednn_custom_call] {\n     AddHloVerifier(\n         &pipeline,\n         HloVerifierOpts{}.MakeLayoutSensitive().WithInstructionCanChangeLayout(\n@@ -1001,7 +999,7 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n         !module->config().debug_options().xla_cpu_enable_fast_min_max());\n     options.set_executing_on_cpu(true);\n     // oneDNN support is currently enabled only when thunk runtime is turned off\n-    options.set_enable_onednn_support(is_onednn_compatible);\n+    options.set_enable_onednn_support(use_onednn_custom_call);\n     options.set_rewrite_no_op_bitcast_convert_to_bitcast(true);\n     pipeline.AddPass<AlgebraicSimplifier>(options);\n     pipeline.AddPass<HloDCE>();"
        }
    ],
    "stats": {
        "total": 47,
        "additions": 27,
        "deletions": 20
    }
}