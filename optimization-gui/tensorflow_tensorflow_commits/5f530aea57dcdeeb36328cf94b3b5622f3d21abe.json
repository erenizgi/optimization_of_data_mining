{
    "author": "basioli-k",
    "message": "[XLA][codegen] Add xtile scaled dot op, emit it from the fusion emitter and implement lowering for the triton backend\n\nPiperOrigin-RevId: 831773027",
    "sha": "5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
    "files": [
        {
            "sha": "929681fcdc18af8000d4030409382deea6d5d52d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -422,6 +422,7 @@ cc_library(\n         \":emitter_helpers\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/translate/hlo_to_mhlo:attribute_importer\",\n         \"//xla/service:algorithm_util\","
        },
        {
            "sha": "45312046f1ed042366830e580ff9dee8c363a1e7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 8,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n \n-#include <cstdint>\n #include <limits>\n #include <optional>\n #include <string>\n@@ -39,6 +38,7 @@ limitations under the License.\n #include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/attribute_importer.h\"\n@@ -78,6 +78,10 @@ mlir::stablehlo::Precision XlaPrecisionToStableHloPrecision(\n   }\n }\n \n+}  // namespace\n+\n+namespace internal {\n+\n absl::StatusOr<ttir::ScaleDotElemType> GetScaleDotElemType(Type value) {\n   auto type = getElementTypeOrSelf(value);\n   if (type == mlir::Float8E4M3FNType::get(value.getContext())) {\n@@ -96,12 +100,16 @@ absl::StatusOr<ttir::ScaleDotElemType> GetScaleDotElemType(Type value) {\n       absl::StrCat(\"Unsupported type: \", llvm_ir::DumpToString(type)));\n }\n \n+}  // namespace internal\n+\n+namespace {\n+\n absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n                                 ScaledDotOperands& operands) {\n   TF_ASSIGN_OR_RETURN(auto lhs_dot_elem_type,\n-                      GetScaleDotElemType(operands.lhs.getType()));\n+                      internal::GetScaleDotElemType(operands.lhs.getType()));\n   TF_ASSIGN_OR_RETURN(auto rhs_dot_elem_type,\n-                      GetScaleDotElemType(operands.rhs.getType()));\n+                      internal::GetScaleDotElemType(operands.rhs.getType()));\n \n   Value lhs_scale;\n   if (lhs_dot_elem_type != ttir::ScaleDotElemType::BF16) {\n@@ -114,11 +122,16 @@ absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n         rhs_scale, b.getDenseI64ArrayAttr({1, 0}));\n   }\n \n-  // make type with the same shape as the scale but with i8 type\n-  return b.create<ttir::DotScaledOp>(\n-      operands.accumulator.getType(), operands.lhs, operands.rhs,\n-      operands.accumulator, lhs_scale, rhs_scale, lhs_dot_elem_type,\n-      rhs_dot_elem_type, true);\n+  auto dot_scaled_op =\n+      b.create<xtile::DotScaledOp>(operands.accumulator.getType(), operands.lhs,\n+                                   operands.rhs, lhs_scale, rhs_scale, true);\n+\n+  auto add_result =\n+      mlir::isa<mlir::IntegerType>(\n+          dot_scaled_op.getResult().getType().getElementType())\n+          ? b.create<mlir::arith::AddIOp>(operands.accumulator, dot_scaled_op)\n+          : b.create<mlir::arith::AddFOp>(operands.accumulator, dot_scaled_op);\n+  return add_result->getResult(0);\n }\n \n namespace {"
        },
        {
            "sha": "70d95da0a1ef54f131b7e24c44872ca53a24574f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -2058,6 +2058,7 @@ absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n     }\n     pm.addPass(mlir::triton::xla::CreateTensorLowerToTritonPass());\n     pm.addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n+    pm.addPass(mlir::triton::xla::CreateXTileLowerToTritonPass());\n \n     std::string libdevice_path =\n         GetLibdevicePath(fusion.GetModule()->config(), device_info);"
        },
        {
            "sha": "324641547f2e9af6f639b8b34ed806ca045b6bf4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -309,6 +309,128 @@ CHECK: %[[ADD_RES:.*]] = arith.addf %[[ARG2:.*]], %[[RES]] : tensor<32x8xf32>\n )\"));\n }\n \n+TEST_F(XTileDialectTest, HloScaledDotIsLoweredToXTileDotScaled) {\n+  constexpr absl::string_view kHloText = R\"(\n+HloModule m\n+flhs (p0: f8e5m2[128,128]) -> f8e5m2[128,128] {\n+  ROOT p0 = f8e5m2[128,128]{1,0} parameter(0)\n+}\n+frhs (p0: f8e5m2[128,256]) -> f8e5m2[128,256] {\n+  ROOT p0 = f8e5m2[128,256]{1,0} parameter(0)\n+}\n+flhs_scale (p0: f8e8m0fnu[128,4]) -> f8e8m0fnu[128,4] {\n+  ROOT p0 = f8e8m0fnu[128,4]{1,0} parameter(0)\n+}\n+frhs_scale (p0: f8e8m0fnu[4,256]) -> f8e8m0fnu[4,256] {\n+  ROOT p0 = f8e8m0fnu[4,256]{1,0} parameter(0)\n+}\n+\n+triton_dot {\n+  lhs = f8e5m2[128,128] parameter(0)\n+  lhs1 = f8e5m2[128,128]{1,0} fusion(lhs),\n+    kind=kCustom,\n+    calls=flhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs = f8e5m2[128,256] parameter(1)\n+  rhs1 = f8e5m2[128,256]{1,0} fusion(rhs),\n+    kind=kCustom,\n+    calls=frhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"256\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  lhs_scale = f8e8m0fnu[128,4] parameter(2)\n+  lhs_scale1 = f8e8m0fnu[128,4]{1,0} fusion(lhs_scale),\n+    kind=kCustom,\n+    calls=flhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs_scale = f8e8m0fnu[4,256] parameter(3)\n+  rhs_scale1 = f8e8m0fnu[4,256]{1,0} fusion(rhs_scale),\n+    kind=kCustom,\n+    calls=frhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  ROOT _ = bf16[128,256]{1,0} scaled-dot(lhs1, rhs1, lhs_scale1, rhs_scale1),\n+    lhs_contracting_dims={1},\n+    rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  lhs = f8e5m2[128,128]{1,0} parameter(0)\n+  rhs = f8e5m2[128,256]{1,0} parameter(1)\n+  lhs_scale = f8e8m0fnu[128,4]{1,0} parameter(2)\n+  rhs_scale = f8e8m0fnu[4,256]{1,0} parameter(3)\n+  ROOT _ = bf16[128,256]{1,0} fusion(lhs, rhs, lhs_scale, rhs_scale),\n+    kind=kCustom,\n+    calls=triton_dot,\n+    backend_config={\n+      \"fusion_backend_config\": {\n+        kind: \"__triton_scaled_dot_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\"\n+        }\n+      }\n+    }\n+}\n+\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloText));\n+\n+  auto& debug_options = module->mutable_config().mutable_debug_options();\n+  debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n+\n+  BlockLevelParameters block_level_parameters;\n+  block_level_parameters.output_tile_sizes = {{128, 256}};\n+\n+  TF_EXPECT_OK(CreateXTileIrAndFileCheck(\n+      this, *module->GetComputationWithName(\"triton_dot\"),\n+      block_level_parameters,\n+      R\"(\n+      CHECK: %[[DOT:.*]] = xtile.dot_scaled %[[LHS:.*]] scale %[[LHS_SCALE:.*]], %[[RHS:.*]] scale %[[RHS_SCALE:.*]] {fastMath = true} : tensor<128x128xf8E5M2>, tensor<128x4xi8> * tensor<128x256xf8E5M2>, tensor<256x4xi8> -> tensor<128x256xf32>\n+      CHECK: %[[RES:.*]] = arith.addf %{{.*}}, %[[DOT]] : tensor<128x256xf32>\n+      )\"));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "d8957845ad63ff26d52c09c18accece3669b0d3b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -48,6 +48,7 @@ cc_library(\n         \"triton_xla_math_to_libdevice.cc\",\n         \"triton_xla_squeeze_dims_pass.cc\",\n         \"triton_xla_unswitch_loops_pass.cc\",\n+        \"xtile_lower_to_triton.cc\",\n     ],\n     hdrs = [\"passes.h\"],\n     deps = ["
        },
        {
            "sha": "0e2bcb76852919046d36bec1f7b11313b0002c69",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -52,6 +52,7 @@ std::unique_ptr<mlir::Pass> CreateStableHLOLowerToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTensorLowerToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAMathToLibdevicePass(\n     absl::string_view libdevice_path, absl::string_view triple);\n+std::unique_ptr<mlir::Pass> CreateXTileLowerToTritonPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "2dbec5bf0fc95ecd12db4491079ddc96c5683322",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -246,4 +246,15 @@ def TritonXLAMathToLibdevicePass\n   ];\n }\n \n+\n+def XTileLowerToTritonPass\n+    : Pass<\"xtile-lower-to-triton\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lowers XTile operations to their Triton equivalent.\";\n+  let dependentDialects = [\n+    \"::mlir::triton::TritonDialect\",\n+    \"::xla::xtile::XTileDialect\",\n+  ];\n+  let constructor = \"CreateXTileLowerToTritonPass()\";\n+}\n+\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "aa095137acb3f2cde189a751ed38a3f8ea6b86e6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/xtile_to_triton_lowering.mlir",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fxtile_to_triton_lowering.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fxtile_to_triton_lowering.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fxtile_to_triton_lowering.mlir?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -0,0 +1,32 @@\n+// RUN: xla-opt %s -split-input-file \\\n+// RUN: -xtile-lower-to-triton \\\n+// RUN: | FileCheck %s\n+\n+\n+// CHECK: func @lower_dot_scaled_add_to_triton(%[[LHS:.*]]: tensor<128x128xf8E5M2>, %[[LHS_SCALE:.*]]: tensor<128x4xi8>, %[[RHS:.*]]: tensor<128x256xf8E5M2>, %[[RHS_SCALE:.*]]: tensor<256x4xi8>, %[[ACC:.*]]: tensor<128x256xf32>) -> tensor<128x256xf32> {\n+func.func @lower_dot_scaled_add_to_triton(\n+  %lhs: tensor<128x128xf8E5M2>, %lhs_scale: tensor<128x4xi8>,\n+  %rhs: tensor<128x256xf8E5M2>, %rhs_scale: tensor<256x4xi8>,\n+  %acc: tensor<128x256xf32>) -> tensor<128x256xf32> {\n+  // CHECK: %[[RES:.*]] = tt.dot_scaled %[[LHS]] scale %[[LHS_SCALE]], %[[RHS]] scale %[[RHS_SCALE]], %[[ACC]] lhs = e5m2 rhs = e5m2 {fastMath = true} : tensor<128x128xf8E5M2>, tensor<128x4xi8> * tensor<128x256xf8E5M2>, tensor<256x4xi8> -> tensor<128x256xf32>\n+  // CHECK-NOT: arith.addf\n+  %0 = xtile.dot_scaled %lhs scale %lhs_scale, %rhs scale %rhs_scale\n+    {fastMath = true} : tensor<128x128xf8E5M2>,\n+    tensor<128x4xi8> * tensor<128x256xf8E5M2>, tensor<256x4xi8> -> tensor<128x256xf32>\n+  %1 = arith.addf %acc, %0 : tensor<128x256xf32>\n+  // CHECK: return %[[RES]] : tensor<128x256xf32>\n+  return %1 : tensor<128x256xf32>\n+}\n+\n+// CHECK: func @lower_dot_scaled_without_add_falls_back_to_xtile(%[[LHS:.*]]: tensor<128x128xf8E5M2>, %[[LHS_SCALE:.*]]: tensor<128x4xi8>, %[[RHS:.*]]: tensor<128x256xf8E5M2>, %[[RHS_SCALE:.*]]: tensor<256x4xi8>) -> tensor<128x256xf32> {\n+func.func @lower_dot_scaled_without_add_falls_back_to_xtile(\n+  %lhs: tensor<128x128xf8E5M2>, %lhs_scale: tensor<128x4xi8>,\n+  %rhs: tensor<128x256xf8E5M2>, %rhs_scale: tensor<256x4xi8>)\n+  -> tensor<128x256xf32> {\n+  // CHECK: %[[RES:.*]] = xtile.dot_scaled %[[LHS]] scale %[[LHS_SCALE]], %[[RHS]] scale %[[RHS_SCALE]] {fastMath = true} : tensor<128x128xf8E5M2>, tensor<128x4xi8> * tensor<128x256xf8E5M2>, tensor<256x4xi8> -> tensor<128x256xf32>\n+  %0 = xtile.dot_scaled %lhs scale %lhs_scale, %rhs scale %rhs_scale\n+    {fastMath = true} : tensor<128x128xf8E5M2>,\n+    tensor<128x4xi8> * tensor<128x256xf8E5M2>, tensor<256x4xi8> -> tensor<128x256xf32>\n+  // CHECK: return %[[RES]] : tensor<128x256xf32>\n+  return %0 : tensor<128x256xf32>\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "def9cb2421f05f769f1bd257cb9ead1022310cc0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/xtile_lower_to_triton.cc",
            "status": "added",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fxtile_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fxtile_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fxtile_lower_to_triton.cc?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -0,0 +1,129 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <iterator>\n+#include <memory>\n+#include <utility>\n+\n+#include \"absl/strings/str_cat.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir::triton::xla {\n+\n+namespace ttir = ::mlir::triton;\n+\n+#define GEN_PASS_DEF_XTILELOWERTOTRITONPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+class LowerDotScaled\n+    : public mlir::OpRewritePattern<::xla::xtile::DotScaledOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+ private:\n+  mlir::LogicalResult matchAndRewrite(\n+      ::xla::xtile::DotScaledOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    if (std::distance(op->getUsers().begin(), op->getUsers().end()) != 1) {\n+      return rewriter.notifyMatchFailure(\n+          op->getLoc(),\n+          \"Dot op must have exactly one user in order to be lowered to \"\n+          \"triton.\");\n+    }\n+\n+    mlir::Operation* add_op = dyn_cast<arith::AddFOp>(*op->getUsers().begin());\n+    if (!add_op) {\n+      add_op = dyn_cast<arith::AddIOp>(*op->getUsers().begin());\n+    }\n+\n+    if (!add_op) {\n+      return rewriter.notifyMatchFailure(\n+          op->getLoc(),\n+          \"Dot op must be consumed by an AddOp in order to be convertible to \"\n+          \"triton dot.\");\n+    }\n+\n+    // Accumulator is the operand of add that is not the dot operation.\n+    auto accumulator = add_op->getOperand(1) == op ? add_op->getOperand(0)\n+                                                   : add_op->getOperand(1);\n+\n+    auto lhs_dot_elem_type_or_status =\n+        ::xla::gpu::triton::internal::GetScaleDotElemType(\n+            op.getLhs().getType());\n+    auto rhs_dot_elem_type_or_status =\n+        ::xla::gpu::triton::internal::GetScaleDotElemType(\n+            op.getRhs().getType());\n+\n+    if (!lhs_dot_elem_type_or_status.ok() ||\n+        !rhs_dot_elem_type_or_status.ok()) {\n+      return rewriter.notifyMatchFailure(\n+          op->getLoc(),\n+          absl::StrCat(\n+              \"Failed to get dot element type for lhs or rhs.\\nLhs status: \",\n+              lhs_dot_elem_type_or_status.status().message(), \"\\nRhs status: \",\n+              rhs_dot_elem_type_or_status.status().message()));\n+    }\n+\n+    auto lhs_dot_elem_type = lhs_dot_elem_type_or_status.value();\n+    auto rhs_dot_elem_type = rhs_dot_elem_type_or_status.value();\n+\n+    auto triton_dot_scaled_op = ttir::DotScaledOp::create(\n+        rewriter, op.getLoc(), accumulator.getType(), op.getLhs(), op.getRhs(),\n+        accumulator, op.getLhsScale(), op.getRhsScale(), lhs_dot_elem_type,\n+        rhs_dot_elem_type, op.getFastMath(), op.getLhsKPack(),\n+        op.getRhsKPack());\n+\n+    rewriter.replaceAllOpUsesWith(add_op, op.getResult());\n+    rewriter.replaceOp(op, triton_dot_scaled_op);\n+    return mlir::success();\n+  }\n+};\n+\n+class XTileLowerToTritonPass\n+    : public impl::XTileLowerToTritonPassBase<XTileLowerToTritonPass> {\n+ public:\n+  void runOnOperation() override {\n+    mlir::MLIRContext* mlir_context = &getContext();\n+    mlir::RewritePatternSet patterns(mlir_context);\n+    patterns.add<LowerDotScaled>(mlir_context);\n+\n+    if (mlir::failed(\n+            mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<Pass> CreateXTileLowerToTritonPass() {\n+  return std::make_unique<XTileLowerToTritonPass>();\n+}\n+\n+}  // namespace mlir::triton::xla"
        },
        {
            "sha": "81d11906c4033fd001d0ac416fa76b3471038155",
            "filename": "third_party/xla/xla/codegen/xtile/ir/tests/ops.mlir",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -116,3 +116,12 @@ func.func @type_mismatch_insert(%src: tensor<24xf64>, %dst: memref<1024xf32>) {\n   xtile.insert %src into %dst[%offset][24][1] : tensor<24xf64> -> memref<1024xf32>\n   return\n }\n+\n+\n+// -----\n+\n+func.func @dot_scaled(%lhs: tensor<128x128xf32>, %lhs_scale: tensor<128x4xi8>, %rhs: tensor<128x256xf32>, %rhs_scale: tensor<256x4xi8>, %acc: tensor<128x256xf32>) -> tensor<128x256xf32> {\n+  %0 = xtile.dot_scaled %lhs scale %lhs_scale, %rhs scale %rhs_scale {fastMath = true} : tensor<128x128xf32>, tensor<128x4xi8> * tensor<128x256xf32>, tensor<256x4xi8> -> tensor<128x256xf32>\n+  return %0 : tensor<128x256xf32>\n+}\n+"
        },
        {
            "sha": "4188b213caef0588140b2f08ffda24dda47bba66",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_ops.td",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f530aea57dcdeeb36328cf94b3b5622f3d21abe/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td?ref=5f530aea57dcdeeb36328cf94b3b5622f3d21abe",
            "patch": "@@ -213,5 +213,40 @@ def InsertTileOp : XTile_Op<\"insert\", [TiledBufferInterface]> {\n }\n \n \n+//\n+// DotScaled Op\n+//\n+// TODO(basioli): This op was copied from the triton dialect. If we want to use \n+// it more we should probably consider documenting it properly, and including \n+// more checks (e.x. similar to the Triton DotOpInterface).\n+def DotScaledOp : XTile_Op<\"dot_scaled\", [Pure, AttrSizedOperandSegments]> {\n+    let summary = \"dot_scaled\";\n+\n+    let description = [{\n+        $result = matrix_multiply(scale($lhs, $lhs_scale), scale($rhs, $rhs_scale)).\n+        Where scale(x, s) is a function that applies the scale per block following microscaling spec.\n+    }];\n+\n+    let arguments = (\n+      ins\n+      // inputs are floats if we have a type for them, otherwise (fp4),\n+      // they are packed in pairs in an I8Tensor\n+      RankedTensorOf<[AnyFloat,I8]>:$lhs,\n+      RankedTensorOf<[AnyFloat,I8]>:$rhs,\n+      Optional<RankedTensorOf<[AnyFloat,I8]>>:$lhs_scale,\n+      Optional<RankedTensorOf<[AnyFloat,I8]>>:$rhs_scale,\n+      BoolAttr:$fastMath,\n+      DefaultValuedAttr<BoolAttr, \"true\">:$lhs_k_pack,\n+      DefaultValuedAttr<BoolAttr, \"true\">:$rhs_k_pack\n+    );\n+\n+    let results = (outs RankedTensorOf<[AnyFloat]>:$result);\n+\n+    let assemblyFormat = [{\n+      $lhs (`scale` $lhs_scale^)? `,` $rhs (`scale` $rhs_scale^)? attr-dict\n+      `:` type($lhs) (`,` type($lhs_scale)^)? `*` type($rhs) (`,` type($rhs_scale)^)? `->` type($result)\n+    }];\n+}\n+\n #endif // XLA_CODEGEN_XTILE_IR_XTILE_OPS\n "
        }
    ],
    "stats": {
        "total": 371,
        "additions": 363,
        "deletions": 8
    }
}