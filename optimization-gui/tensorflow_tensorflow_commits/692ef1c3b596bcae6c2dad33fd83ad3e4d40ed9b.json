{
    "author": "derdrdirk",
    "message": "[Autotuner] Add use_default_config option to AutotunerConfig.\n\nPiperOrigin-RevId: 813720129",
    "sha": "692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b",
    "files": [
        {
            "sha": "ba9f629ca0b1893d03692275d8a36b01078a8a40",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 15,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b",
            "patch": "@@ -75,6 +75,25 @@ std::string UnpackedAnyShortDebugString(const google::protobuf::Any& any) {\n \n }  // namespace\n \n+absl::StatusOr<Autotuner::Config> Autotuner::GetDefaultConfig(\n+    const HloInstruction& instr) {\n+  // TODO(b/446870267): Improve default backend selection. Currently we just\n+  // return the first backend that supports the instruction.\n+  for (auto& backend : codegen_backends_) {\n+    auto config = backend->GetDefaultConfig(instr);\n+    if (absl::IsUnimplemented(config.status())) {\n+      LOG(FATAL) << \"GetDefaultConfig is not implemented for \"\n+                 << backend->name();\n+    }\n+    if (config.ok()) {\n+      return Config{backend.get(), std::move(*config)};\n+    }\n+  }\n+  return absl::NotFoundError(\n+      absl::StrCat(\"No backend with default config found for instruction: \",\n+                   instr.ToString()));\n+}\n+\n absl::StatusOr<std::unique_ptr<Autotuner>> Autotuner::Create(\n     std::vector<std::unique_ptr<CodegenBackend>> codegen_backends,\n     std::unique_ptr<Profiler> profiler, AutotuneConfig autotune_config,\n@@ -96,14 +115,12 @@ absl::Status Autotuner::Autotune(HloModule* module,\n     VLOG(1) << \"No instructions to autotune.\";\n     return absl::OkStatus();\n   }\n-\n   VLOG(1) << \"Autotuning \" << instrunctions_by_fingerprint.size()\n           << \" unique instructions.\";\n   for (auto& [_, instructions] : instrunctions_by_fingerprint) {\n     CHECK(!instructions.empty());\n     VLOG(1) << \"Autotuning instruction:\" << instructions[0]->ToString();\n-    TF_ASSIGN_OR_RETURN(Config best_config,\n-                        GetCachedOrTuneBestConfig(instructions[0]));\n+    TF_ASSIGN_OR_RETURN(Config best_config, GetConfig(instructions[0]));\n     CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n     for (auto* instr : instructions) {\n       TF_RETURN_IF_ERROR(best_codegen_backend->ApplyConfig(\n@@ -115,27 +132,31 @@ absl::Status Autotuner::Autotune(HloModule* module,\n \n absl::Status Autotuner::Autotune(HloInstruction* instr) {\n   VLOG(1) << \"Autotuning HLO: \" << instr->ToString();\n-  TF_ASSIGN_OR_RETURN(Config best_config, GetCachedOrTuneBestConfig(instr));\n+  TF_ASSIGN_OR_RETURN(Config best_config, GetConfig(instr));\n   CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n   TF_RETURN_IF_ERROR(\n       best_codegen_backend->ApplyConfig(*instr, *best_config.backend_config));\n   return DumpLogsToFile();\n }\n \n-absl::StatusOr<Autotuner::Config> Autotuner::GetCachedOrTuneBestConfig(\n-    HloInstruction* instr) {\n+absl::StatusOr<Autotuner::Config> Autotuner::GetConfig(HloInstruction* instr) {\n   std::optional<Config> cached_config = LookUp(instr);\n-  Config best_config;\n   if (cached_config.has_value()) {\n-    best_config = std::move(*cached_config);\n-  } else {\n-    if (autotune_config_.expect_all_instructions_in_cache) {\n-      return absl::NotFoundError(\"No cached config found for HLO instr: \" +\n-                                 instr->ToString());\n-    }\n-    TF_ASSIGN_OR_RETURN(best_config, TuneBestConfig(instr));\n-    Insert(instr, best_config);\n+    return std::move(cached_config.value());\n+  }\n+\n+  if (autotune_config_.expect_all_instructions_in_cache) {\n+    return absl::NotFoundError(\"No cached config found for HLO instr: \" +\n+                               instr->ToString());\n   }\n+\n+  if (autotune_config_.use_default_config) {\n+    return GetDefaultConfig(*instr);\n+  }\n+\n+  Config best_config;\n+  TF_ASSIGN_OR_RETURN(best_config, TuneBestConfig(instr));\n+  Insert(instr, best_config);\n   return best_config;\n }\n "
        },
        {
            "sha": "02abbfa37a7b2ee88e89e38dbf368171af30b135",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.h",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h?ref=692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b",
            "patch": "@@ -75,6 +75,12 @@ struct AutotuneConfig {\n   // If true, the autotuner selects the first valid config instead of the best\n   // performing one. This is to guarantee run-to-run determinism.\n   bool select_first_config = false;\n+  // If true, use hardcoded default backend configs instead of autotuning.\n+  // Default configs depend on the backend order. Currently the first backend\n+  // that supports the instruction will be used (see b/446870267).\n+  // Note: If cache is provided, the cached config will be used instead of the\n+  // default config.\n+  bool use_default_config = false;\n };\n \n class Autotuner {\n@@ -151,9 +157,14 @@ class Autotuner {\n   InstructionsByFingerprint GetAutotuningCandidates(\n       const HloModule* module, const InstructionFilterFn& should_autotune);\n \n-  // Gets the best config for the given instruction either from cache or by\n-  // tuning all supported configs if the instruction is not in the cache.\n-  absl::StatusOr<Config> GetCachedOrTuneBestConfig(HloInstruction* instr);\n+  // Gets the default config for the given instruction.\n+  absl::StatusOr<Config> GetDefaultConfig(const HloInstruction& instr);\n+\n+  // Gets the config for the given instruction. If instruction is in cache,\n+  // cached config is returned. If not in cache and use_default_config is\n+  // true, default config is returned. Otherwise, tunes all supported configs\n+  // to find the best config, inserts it into cache and returns it.\n+  absl::StatusOr<Config> GetConfig(HloInstruction* instr);\n   // Gets the best config for the given instruction by compiling and profiling\n   // all supported configs.\n   absl::StatusOr<Config> TuneBestConfig(HloInstruction* instr);"
        },
        {
            "sha": "d3252231999638cd8135e9fef39e9bf712a74e82",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_test.cc",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc?ref=692ef1c3b596bcae6c2dad33fd83ad3e4d40ed9b",
            "patch": "@@ -127,6 +127,8 @@ class MockAutotunerCache : public AutotunerCacheInterface {\n using absl_testing::IsOk;\n using absl_testing::StatusIs;\n using ::testing::_;\n+using ::testing::AtMost;\n+using ::testing::ByMove;\n using ::testing::Return;\n using tsl::proto_utils::ToDurationProto;\n \n@@ -651,5 +653,50 @@ TEST_F(AutotunerTest, SelectFirstConfig) {\n   EXPECT_THAT(autotuner->Autotune(dummy_instr), absl_testing::IsOk());\n }\n \n+TEST_F(AutotunerTest, UseDefaultConfig) {\n+  config_.use_default_config = true;\n+\n+  auto backend = std::make_unique<MockCodegenBackend>();\n+  EXPECT_CALL(*backend, GetSupportedConfigs(_)).Times(0);\n+  EXPECT_CALL(*backend, GetDefaultConfig(_))\n+      .WillOnce(Return(ByMove(GetTestConfig(\"default\"))));\n+  EXPECT_CALL(*backend, ApplyConfig(_, ConfigMatcher(\"default\")))\n+      .Times(1)\n+      .WillRepeatedly(Return(absl::OkStatus()));\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::move(backend));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto autotuner,\n+      Autotuner::Create(std::move(backends), /*profiler=*/nullptr, config_,\n+                        /*cache=*/nullptr));\n+  auto module = ParseAndReturnVerifiedModule(kHlo).value();\n+  auto dummy_instr = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(autotuner->Autotune(dummy_instr), absl_testing::IsOk());\n+}\n+\n+TEST_F(AutotunerTest, UseDefaultConfigUnimplemented) {\n+  config_.use_default_config = true;\n+\n+  auto backend = std::make_unique<MockCodegenBackend>();\n+  EXPECT_CALL(*backend, name()).WillRepeatedly(Return(\"mock_backend\"));\n+  EXPECT_CALL(*backend, GetSupportedConfigs(_)).Times(0);\n+  EXPECT_CALL(*backend, GetDefaultConfig(_))\n+      .Times(AtMost(1))\n+      .WillRepeatedly(\n+          [] { return absl::UnimplementedError(\"not implemented\"); });\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::move(backend));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto autotuner,\n+      Autotuner::Create(std::move(backends), /*profiler=*/nullptr, config_,\n+                        /*cache=*/nullptr));\n+  auto module = ParseAndReturnVerifiedModule(kHlo).value();\n+  auto dummy_instr = module->entry_computation()->root_instruction();\n+  EXPECT_DEATH(autotuner->Autotune(dummy_instr).IgnoreError(),\n+               \"GetDefaultConfig is not implemented for mock_backend\");\n+}\n+\n }  // namespace\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 115,
        "additions": 97,
        "deletions": 18
    }
}