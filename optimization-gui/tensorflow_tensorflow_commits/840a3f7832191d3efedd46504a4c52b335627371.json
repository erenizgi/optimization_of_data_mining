{
    "author": "vwbaker",
    "message": "[xla:gpu] Fix triton pipeline discrepancies\n\nIt seems that in a few previous triton integrations, we have failed to copy over some of the pipeline changes. I went through all of them & think they should be aligned now:\n\n* triton's version: triton/third_party/nvidia/backend/compiler.py\n* xla's version: xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc\n\nTest change: I updated the verifier test to change the tiling as it is now causing a resource exhausted error.\n\nPiperOrigin-RevId: 845075604",
    "sha": "840a3f7832191d3efedd46504a4c52b335627371",
    "files": [
        {
            "sha": "81cdd32773c6e871492c860d7d57455e577b45df",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/840a3f7832191d3efedd46504a4c52b335627371/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/840a3f7832191d3efedd46504a4c52b335627371/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=840a3f7832191d3efedd46504a4c52b335627371",
            "patch": "@@ -211,6 +211,7 @@ cc_library(\n         \"@llvm-project//mlir:Pass\",\n         \"@llvm-project//mlir:SCFToControlFlow\",\n         \"@llvm-project//mlir:Transforms\",\n+        \"@triton//:GluonTransforms\",\n         \"@triton//:TritonDialects\",\n         \"@triton//:TritonGPUToLLVM\",\n         \"@triton//:TritonGPUTransforms\","
        },
        {
            "sha": "5bf494417e65c2fda26069374766901bdc5609be",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/840a3f7832191d3efedd46504a4c52b335627371/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/840a3f7832191d3efedd46504a4c52b335627371/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc?ref=840a3f7832191d3efedd46504a4c52b335627371",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n+#include \"triton/Dialect/Gluon/Transforms/Passes.h\"\n #include \"triton/Dialect/Triton/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n@@ -138,10 +139,12 @@ static void MakeLLIR(mlir::OpPassManager* pm,\n   pm->addPass(mt::gpu::createTritonGPUCombineTensorSelectAndIf());\n   pm->addPass(mt::gpu::createTritonGPUAllocateWarpGroups());\n   pm->addPass(mlir::createSCFToControlFlowPass());\n+  pm->addPass(mlir::triton::gluon::createGluonInline());\n   pm->addPass(mt::createAllocateSharedMemoryNvPass(\n       cuda_cc_as_int,\n       mlir::triton::AllocateSharedMemoryNvOptions{}.ptxVersion));\n   pm->addPass(ttng::createTritonTensorMemoryAllocationPass());\n+  pm->addPass(ttng::createTritonNvidiaGPUCheckMatmulTwoCTAPass());\n   // We could add a flag to XLA to optionally enable the following pass:\n   // pm->addPass(mt::instrument::createTritonInstrumentConcurrencySanitizer());\n   pm->addPass(mt::gpu::createTritonGPUGlobalScratchAllocationPass());\n@@ -153,7 +156,6 @@ static void MakeLLIR(mlir::OpPassManager* pm,\n   pm->addPass(mlir::createCSEPass());\n   pm->addPass(mt::createConvertNVGPUToLLVM());\n   pm->addPass(mt::createConvertWarpSpecializeToLLVM());\n-  pm->addPass(mlir::createArithToLLVMConversionPass());\n   pm->addPass(mlir::createCanonicalizerPass());\n   pm->addPass(mlir::createCSEPass());\n   pm->addPass(mlir::createSymbolDCEPass());"
        },
        {
            "sha": "ccb54900e2ea2f4a08d1adba5dd41d735c12e339",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/840a3f7832191d3efedd46504a4c52b335627371/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/840a3f7832191d3efedd46504a4c52b335627371/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc?ref=840a3f7832191d3efedd46504a4c52b335627371",
            "patch": "@@ -268,7 +268,7 @@ gemm_computation (p0: bf16[128,512], p1: bf16[256,512], p2: bf16[512,512]) -> bf\n       \"kind\":\"__triton_nested_gemm_fusion\",\n       \"block_level_fusion_config\":{\n         \"num_warps\":\"8\",\n-        \"output_tiles\":[{\"sizes\":[\"128\",\"64\"]}],\n+        \"output_tiles\":[{\"sizes\":[\"128\",\"32\"]}],\n         \"num_ctas\":1,\n         \"num_stages\":4,\n         \"is_tma_allowed\":false}}}\n@@ -281,7 +281,7 @@ gemm_computation (p0: bf16[128,512], p1: bf16[256,512], p2: bf16[512,512]) -> bf\n       \"kind\":\"__triton_nested_gemm_fusion\",\n       \"block_level_fusion_config\":{\n         \"num_warps\":\"8\",\n-        \"output_tiles\":[{\"sizes\":[\"64\",\"256\"]}],\n+        \"output_tiles\":[{\"sizes\":[\"32\",\"256\"]}],\n         \"num_ctas\":1,\n         \"num_stages\":4,\n         \"is_tma_allowed\":false}}}"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 6,
        "deletions": 3
    }
}