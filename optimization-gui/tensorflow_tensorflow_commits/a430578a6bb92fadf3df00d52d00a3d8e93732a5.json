{
    "author": "nvgrw",
    "message": "Clean up device assignment logic in HloRunnerPjRt.\n\nThis change reduces the number of places where we generate device assignments,\nand attempts to use static device assignments whenever they are available. As a\nfallback, the client's default device assigments are used.\n\nPiperOrigin-RevId: 846413387",
    "sha": "a430578a6bb92fadf3df00d52d00a3d8e93732a5",
    "files": [
        {
            "sha": "8e30e14818293780d2c14232592fb68fc0a78035",
            "filename": "third_party/xla/xla/service/hlo_runner_pjrt.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 33,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a430578a6bb92fadf3df00d52d00a3d8e93732a5/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a430578a6bb92fadf3df00d52d00a3d8e93732a5/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc?ref=a430578a6bb92fadf3df00d52d00a3d8e93732a5",
            "patch": "@@ -248,6 +248,26 @@ class HloRunnerPjRtExecutable : public OpaqueExecutable {\n   std::unique_ptr<PjRtLoadedExecutable> loaded_executable_;\n };\n \n+// Obtains the best device assignment for the given executable.\n+// If the executable was compiled with a device assignment, that assignment is\n+// returned. Otherwise, the static device assignment is pulled from the module\n+// and returned instead. If that does not exist either, the default device\n+// assignment is computed and returned.\n+absl::StatusOr<DeviceAssignment> GetBestDeviceAssignment(\n+    HloRunnerPjRtExecutable* const executable, PjRtClient& client) {\n+  ASSIGN_OR_RETURN(CompileOptions compile_options,\n+                   executable->executable()->GetCompileOptions());\n+  if (compile_options.executable_build_options.has_device_assignment()) {\n+    return compile_options.executable_build_options.device_assignment();\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(std::vector<std::shared_ptr<HloModule>> hlo_modules,\n+                      executable->executable()->GetHloModules());\n+  TF_RET_CHECK(hlo_modules.size() == 1);\n+  return GetStaticDeviceAssignmentOrComputeDefault(*hlo_modules.front(),\n+                                                   client);\n+}\n+\n }  // namespace\n \n HloRunnerPjRt::HloRunnerPjRt(std::unique_ptr<PjRtClient> pjrt_client)\n@@ -539,10 +559,8 @@ HloRunnerPjRt::DeserializeExecutable(const absl::string_view serialized) const {\n absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n     std::unique_ptr<HloModule> module,\n     const HloRunnerInterface::ReplicatedExecuteOptions& options) {\n-  TF_ASSIGN_OR_RETURN(\n-      DeviceAssignment device_assignment,\n-      GetStaticDeviceAssignmentOrComputeDefault(*module, *pjrt_client_));\n-  return ExecuteReplicated(std::move(module), options, &device_assignment);\n+  return ExecuteReplicated(std::move(module), options,\n+                           /*device_assignment=*/nullptr);\n }\n \n absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n@@ -560,35 +578,35 @@ absl::StatusOr<std::vector<Literal>>\n HloRunnerPjRt::ExecuteReplicatedWithExecutable(\n     OpaqueExecutable* const absl_nonnull executable,\n     const ReplicatedExecuteOptions& options) {\n-  ASSIGN_OR_RETURN(const HloModule* const module,\n-                   HloModuleFromWrapped(executable));\n-  ASSIGN_OR_RETURN(\n-      DeviceAssignment device_assignment,\n-      GetStaticDeviceAssignmentOrComputeDefault(*module, *pjrt_client_));\n   return ExecuteReplicatedWithExecutable(executable, options,\n-                                         &device_assignment);\n+                                         /*device_assignment=*/nullptr);\n }\n \n absl::StatusOr<std::vector<Literal>>\n HloRunnerPjRt::ExecuteReplicatedWithExecutable(\n     OpaqueExecutable* const absl_nonnull executable,\n     const HloRunnerInterface::ReplicatedExecuteOptions& options,\n     DeviceAssignment* device_assignment) {\n-  std::optional<DeviceAssignment> default_device_assignment = std::nullopt;\n+  ASSIGN_OR_RETURN(HloRunnerPjRtExecutable* const wrapped_executable,\n+                   HloRunnerPjRtExecutable::TryUnwrap(*this, executable));\n+\n+  // If a device assignment is provided, use it. Otherwise, use the one from the\n+  // executable, or if that is not available, generate a default one.\n+  std::optional<DeviceAssignment> device_assignment_storage = std::nullopt;\n   if (device_assignment == nullptr) {\n-    ASSIGN_OR_RETURN(default_device_assignment,\n-                     GetDefaultDeviceAssignment(options.num_devices, 1));\n-    device_assignment = &*default_device_assignment;\n+    ASSIGN_OR_RETURN(\n+        device_assignment_storage,\n+        GetBestDeviceAssignment(wrapped_executable, *pjrt_client_));\n+    device_assignment = &*device_assignment_storage;\n   }\n   CHECK_NE(device_assignment, nullptr);\n-  TF_ASSIGN_OR_RETURN(HloRunnerPjRtExecutable* const wrapped_executable,\n-                      HloRunnerPjRtExecutable::TryUnwrap(*this, executable));\n \n   xla::ExecuteOptions execute_options;\n   return ExecuteReplicatedImpl(\n       [&](absl::Span<const std::vector<PjRtBuffer*>> argument_buffer_slices,\n           absl::AnyInvocable<OpaqueExecutable*(int64_t)>\n-              executable_provider_arg)\n+              executable_provider_arg,\n+          absl::Span<PjRtDevice* const>)\n           -> absl::StatusOr<\n               std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>> {\n         TF_ASSIGN_OR_RETURN(\n@@ -609,17 +627,29 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n     absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n     const HloRunnerInterface::ReplicatedExecuteOptions& options,\n     DeviceAssignment* device_assignment) {\n-  std::optional<DeviceAssignment> default_device_assignment = std::nullopt;\n+  CHECK_GT(options.num_devices, 0);\n+  ASSIGN_OR_RETURN(\n+      HloRunnerPjRtExecutable* const wrapped_executable_device0,\n+      HloRunnerPjRtExecutable::TryUnwrap(*this, executable_provider(0)));\n+\n+  // NB: we assume all executables have the same device assignments.  If a\n+  // device assignment is provided, use it. Otherwise, use the one from the\n+  // first device's executable, or if that is not available, generate a default\n+  // one.\n+  std::optional<DeviceAssignment> device_assignment_storage = std::nullopt;\n   if (device_assignment == nullptr) {\n-    TF_ASSIGN_OR_RETURN(default_device_assignment,\n-                        GetDefaultDeviceAssignment(options.num_devices, 1));\n-    device_assignment = &*default_device_assignment;\n+    ASSIGN_OR_RETURN(\n+        device_assignment_storage,\n+        GetBestDeviceAssignment(wrapped_executable_device0, *pjrt_client_));\n+    device_assignment = &*device_assignment_storage;\n   }\n   CHECK_NE(device_assignment, nullptr);\n+\n   return ExecuteReplicatedImpl(\n       [&](absl::Span<const std::vector<PjRtBuffer*>> argument_buffer_slices,\n           absl::AnyInvocable<OpaqueExecutable*(int64_t)>\n-              executable_provider_arg)\n+              executable_provider_arg,\n+          absl::Span<PjRtDevice* const> id_to_device_ptr)\n           -> absl::StatusOr<\n               std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>> {\n         TF_RET_CHECK(options.use_threads);\n@@ -645,12 +675,9 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n             TF_ASSIGN_OR_RETURN(\n                 PjRtLoadedExecutable * pjrt_executable,\n                 executable->GetOrLoadExecutable(pjrt_client_.get()));\n-            TF_ASSIGN_OR_RETURN(\n-                PjRtDevice * device_ptr,\n-                pjrt_client_->LookupDevice(\n-                    DeviceIdForInvocation(*device_assignment, i)));\n             pool.Schedule([&per_replica_results, i, pjrt_executable,\n-                           args = argument_buffer_slices[i], device_ptr]() {\n+                           args = argument_buffer_slices[i],\n+                           device_ptr = id_to_device_ptr[i]]() {\n               std::optional<Future<>> returned_future = {};\n               xla::ExecuteOptions options;\n               per_replica_results[i] = pjrt_executable->ExecuteSharded(\n@@ -686,7 +713,8 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n     absl::AnyInvocable<\n         absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>>(\n             absl::Span<const std::vector<PjRtBuffer*>>,\n-            absl::AnyInvocable<OpaqueExecutable*(int64_t)>)>\n+            absl::AnyInvocable<OpaqueExecutable*(int64_t)>,\n+            absl::Span<PjRtDevice* const>)>\n         execution_helper,\n     absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n     absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n@@ -695,8 +723,9 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n     DeviceAssignment* device_assignment) {\n   TF_RET_CHECK(options.infeed_values.empty() ||\n                options.infeed_values.size() == options.num_devices);\n+  TF_RET_CHECK(device_assignment != nullptr);\n \n-  std::vector<PjRtDevice*> replica_devices(options.num_devices, nullptr);\n+  std::vector<PjRtDevice*> id_to_device_ptr(options.num_devices, nullptr);\n   std::vector<std::vector<std::unique_ptr<PjRtBuffer>>> argument_buffer_slices;\n   argument_buffer_slices.reserve(options.num_devices);\n   std::vector<bool> is_tuple_result(options.num_devices, false);\n@@ -705,7 +734,7 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n     TF_ASSIGN_OR_RETURN(PjRtDevice* const device_ptr,\n                         pjrt_client_->LookupDevice(\n                             DeviceIdForInvocation(*device_assignment, i)));\n-    replica_devices[i] = device_ptr;\n+    id_to_device_ptr[i] = device_ptr;\n \n     // Get the entry layout.\n     OpaqueExecutable* const wrapped_executable = executable_provider(i);\n@@ -750,7 +779,7 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n   if (has_infeed) {\n     for (int64_t i = 0; i < options.num_devices; ++i) {\n       pool->Schedule(\n-          [device = replica_devices[i],\n+          [device = id_to_device_ptr[i],\n            &infeed_literal = *ABSL_DIE_IF_NULL(options.infeed_values[i]),\n            infeed_steps = options.infeed_steps, &infeed_outfeed_status_mu,\n            &infeed_outfeed_status]() {\n@@ -773,7 +802,7 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n       options.outfeed_values->resize(options.num_devices);\n     }\n     for (int64_t i = 0; i < options.num_devices; ++i) {\n-      pool->Schedule([i, device = replica_devices[i],\n+      pool->Schedule([i, device = id_to_device_ptr[i],\n                       outfeed_values = options.outfeed_values,\n                       outfeed_shape = options.outfeed_shape,\n                       infeed_steps = options.infeed_steps,\n@@ -802,7 +831,7 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n       const std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>\n           result_buffers,\n       execution_helper(BufferMatToPointerMat(argument_buffer_slices),\n-                       std::move(executable_provider)));\n+                       std::move(executable_provider), id_to_device_ptr));\n   VLOG(1) << \"Replicated execution terminated\";\n \n   // Get the result from execution."
        },
        {
            "sha": "e5e1da94988b27de037693e2a8b852b3a447288c",
            "filename": "third_party/xla/xla/service/hlo_runner_pjrt.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a430578a6bb92fadf3df00d52d00a3d8e93732a5/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a430578a6bb92fadf3df00d52d00a3d8e93732a5/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.h?ref=a430578a6bb92fadf3df00d52d00a3d8e93732a5",
            "patch": "@@ -145,7 +145,8 @@ class HloRunnerPjRt : public HloRunnerInterface {\n       absl::AnyInvocable<\n           absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>>(\n               absl::Span<const std::vector<PjRtBuffer*>>,\n-              absl::AnyInvocable<OpaqueExecutable*(int64_t)>)>\n+              absl::AnyInvocable<OpaqueExecutable*(int64_t)>,\n+              absl::Span<PjRtDevice* const>)>\n           execution_helper,\n       absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n       absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 64,
        "deletions": 34
    }
}