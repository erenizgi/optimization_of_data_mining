{
    "author": "Moerafaat",
    "message": "[XLA:GPU/TMA] Enable TMA by default. This brings more performance to workloads on Hopper+ devices.\n\nPiperOrigin-RevId: 836689097",
    "sha": "e9e9372eed67751ce544500b30748a53b9f5449c",
    "files": [
        {
            "sha": "2713b91b862dc2203710517999674413710051ef",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e9e9372eed67751ce544500b30748a53b9f5449c/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e9e9372eed67751ce544500b30748a53b9f5449c/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=e9e9372eed67751ce544500b30748a53b9f5449c",
            "patch": "@@ -457,7 +457,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_disable_automatic_host_compute_offload(false);\n   opts.set_xla_unsupported_crash_on_hlo_pass_noop_change(false);\n   opts.set_xla_gpu_experimental_enable_split_k_rewrite(false);\n-  opts.set_xla_gpu_experimental_enable_triton_tma(false);\n+  opts.set_xla_gpu_experimental_enable_triton_tma(true);\n   opts.set_xla_gpu_experimental_enable_triton_warp_specialization(false);\n   opts.set_xla_gpu_experimental_enable_command_buffer_on_thunks(true);\n   opts.set_xla_detect_unstable_reductions(DebugOptions::DETECTION_MODE_NONE);"
        },
        {
            "sha": "fbec0f39ab5e7c4e042623c38a2da82f8405b1ce",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 22,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e9e9372eed67751ce544500b30748a53b9f5449c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e9e9372eed67751ce544500b30748a53b9f5449c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=e9e9372eed67751ce544500b30748a53b9f5449c",
            "patch": "@@ -1848,19 +1848,7 @@ TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsHaveCuBlasFallback) {\n          \"scaled-dot.\";\n }\n \n-// TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n-// default.\n-class GemmFusionAutotunerEnableTma : public GemmFusionAutotunerTest {\n- public:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options =\n-        GemmFusionAutotunerTest::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_experimental_enable_triton_tma(true);\n-    return debug_options;\n-  }\n-};\n-\n-TEST_F(GemmFusionAutotunerEnableTma,\n+TEST_F(GemmFusionAutotunerTest,\n        TmaConfigsAreGeneratedOnlyForHopperAndWorkCorrectly) {\n   if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n@@ -1913,7 +1901,9 @@ TEST_F(GemmFusionAutotunerEnableTma,\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n }\n \n-TEST_F(GemmFusionAutotunerEnableTma, TmaRunCorrectlyForDotsOfBroadcasts) {\n+// Context in b/421858850. This test ensures that we work around the issue\n+// correctly.\n+TEST_F(GemmFusionAutotunerTest, TmaRunCorrectlyForDotsOfBroadcasts) {\n   if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n@@ -1928,14 +1918,6 @@ TEST_F(GemmFusionAutotunerEnableTma, TmaRunCorrectlyForDotsOfBroadcasts) {\n     })\")\n                                                   .value();\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      const std::vector<TritonGemmConfig> hopper_configs,\n-      GetPossibleMatmulAutotuneTritonConfigs(\n-          *Cast<HloDotInstruction>(\n-              module->entry_computation()->root_instruction()),\n-          se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n-\n   EXPECT_TRUE(RunAndCompare(std::move(module),\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n }"
        }
    ],
    "stats": {
        "total": 28,
        "additions": 5,
        "deletions": 23
    }
}