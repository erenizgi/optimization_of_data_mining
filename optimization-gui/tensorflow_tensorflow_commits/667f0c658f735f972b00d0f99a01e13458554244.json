{
    "author": "pavithraes",
    "message": "PR #30414: [DOC] Add docs for HLO Dumps\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30414\n\nThis PR adds a new documentation page around:\n* How to get HLO Dumps in various formats, on different environments\n* Filtering, transforming, and replaying the dumps\n\nI'm also creating a new \"Debugging\" section in the left sidebar. We have some more debugging guides in the works that can be added to this section.\nCopybara import of the project:\n\n--\n32d2f261ba207e07aa5d8a881112bf12fe465b5b by Pavithra Eswaramoorthy <pavithraes@outlook.com>:\n\nAdd docs on getting HLO dumps\n\n--\n014e2acdd1a57cdd727af7e200189369fc179626 by Pavithra Eswaramoorthy <pavithraes@outlook.com>:\n\nAdd section: More with HLO dumps\n\n--\n961852f2256e4800cc5b38f5a8dc5e7d7ad3bf48 by Pavithra Eswaramoorthy <pavithraes@outlook.com>:\n\nUpdate based on code review comments\n\nMerging this change closes #30414\n\nPiperOrigin-RevId: 799670605",
    "sha": "667f0c658f735f972b00d0f99a01e13458554244",
    "files": [
        {
            "sha": "3f18028cf6c9ea709946892693405801886e7f04",
            "filename": "third_party/xla/docs/_toc.yaml",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/667f0c658f735f972b00d0f99a01e13458554244/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/667f0c658f735f972b00d0f99a01e13458554244/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2F_toc.yaml?ref=667f0c658f735f972b00d0f99a01e13458554244",
            "patch": "@@ -51,6 +51,11 @@ toc:\n     path: /xla/flags_guidance\n   - title: XLA Tooling\n     path: /xla/tools\n+- title: Debugging\n+  section:\n+  # This is the default tab for the Debugging section.\n+  - title: Dump HLO Computations\n+    path: /xla/hlo_dumps\n - title: Contributing\n   # These should be in alphabetical order unless otherwise noted.\n   section:"
        },
        {
            "sha": "d834b437b4ae78feac29b6baaa24d05f6438ed8b",
            "filename": "third_party/xla/docs/hlo_dumps.md",
            "status": "added",
            "additions": 260,
            "deletions": 0,
            "changes": 260,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/667f0c658f735f972b00d0f99a01e13458554244/third_party%2Fxla%2Fdocs%2Fhlo_dumps.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/667f0c658f735f972b00d0f99a01e13458554244/third_party%2Fxla%2Fdocs%2Fhlo_dumps.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fhlo_dumps.md?ref=667f0c658f735f972b00d0f99a01e13458554244",
            "patch": "@@ -0,0 +1,260 @@\n+# Dump HLO Computations\n+\n+An HLO dump is a textual representation of the HLO modules at different stages\n+of the computation. It is useful for debugging, and you often need to include it\n+in bug reports. This is typically a human-readable **text file** that lists the\n+HLO instructions and their properties. Sometimes, HLO modules are dumped as:\n+\n+-   **HloProto:** Protocol buffer files, which are a more structured,\n+    machine-readable format.\n+-   **HloSnapshot**: HLO module plus its inputs. For replaying HLOs, you\n+    sometimes require the actual inputs fed to a given computation rather than\n+    random data.\n+\n+You can use XLA flags to specify and get dumps. In most cases, you can set it\n+with an environment variable. JAX also offers a programmatic way to print the\n+HLO dump.\n+\n+## Local Execution\n+\n+### Using Environment Variables\n+\n+You can set the `XLA_FLAGS` environment variable with the necessary flags to get\n+dumps. This works for JAX, TensorFlow, and PyTorch/XLA.\n+\n+To dump HLO modules and other debugging information to a specific directory, run\n+your program with the `--xla_dump_to` flag:\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+For example, you can use `/tmp` or `/tmp/xladump` as the paths.\n+\n+By default, this dumps HLO modules as text, at the very beginning and end of the\n+optimization pipeline.\n+\n+You can also explicitly specify the format:\n+\n+1.  Text dumps\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_text --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  HLO protos\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_proto --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  HLO Snapshots\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_snapshots --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  Graph render with graphviz server (only works well for small graphs)\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_url --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  Graph render to HTML file (only works well for small graphs)\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_html --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+For larger graphs, you can use `interactive_graphviz` to visualize parts of the\n+graph.\n+\n+**Note:** If `--xla_dump_to` is not specified but another dumping flag is\n+specified, it will dump to stdout. But the dump will not include binary data,\n+e.g., proto files, to stdout.\n+\n+## Dump Specific Intermediate Passes\n+\n+In addition to the standard pre-optimized / final-optimized HLOs, you can also\n+dump the state of HLOs after a particular compiler pass.\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_pass_re=regex --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+HLO modules will be dumped for the passes whose names match the regular\n+expression (regex). For example, you can observe the HLOs resulting from passes\n+related to SPMD partitioning with:\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_to=DIRECTORY_PATH --xla_dump_hlo_pass_re=spmd|propagation\"\n+```\n+\n+To dump the result after every XLA pass (this will result in a lot of files),\n+you can set:\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_to=DIRECTORY_PATH --xla_dump_hlo_pass_re=.*\"\n+```\n+\n+### JAX-specific Options\n+\n+#### Programmatically in JAX\n+\n+Instead of passing flags or environment variables, you can also programmatically\n+dump HLO using JAX’s `lower` and `compile` APIs.\n+\n+Locally fetch the unoptimized original lowered HLO with:\n+\n+```python\n+jax.jit(f).lower(*args).as_text('hlo')\n+```\n+\n+For dumping to files during HLO compilation passes, specify:\n+\n+```python\n+compilation_args = {\n+    'xla_dump_to': DIRECTORY_PATH,\n+    'xla_dump_hlo_pass_re': 'spmd|propagation', # or some other pass filter\n+    ...\n+    }\n+\n+jax.jit(f).lower(*args).compile(compilation_args)\n+```\n+\n+#### Dump jaxprs\n+\n+[`jaxpr`s](https://docs.jax.dev/en/latest/jaxpr.html) are JAX's intermediate\n+representation for program traces. To dump this, set the environment variables:\n+\n+```shell\n+JAX_DUMP_IR_TO=\"DIRECTORY_PATH\" JAX_DUMP_IR_MODES=jaxpr\n+```\n+\n+Learn more in JAX documentation on\n+[Exporting and serializing staged-out computations: Debugging](https://docs.jax.dev/en/latest/export/export.html#debugging).\n+\n+## Google Colab\n+\n+### Environment variables\n+\n+In the first executed cell of your notebook (because environment variables and\n+command-line flags are usually only processed once, e.g., at module-import time\n+or XLA backend initialization time), add the `XLA_FLAGS` detailed above with\n+`os.environ`, for example:\n+\n+```python\n+import os\n+os.environ['XLA_FLAGS'] = \"--xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+This will dump the computation to `DIRECTORY_PATH`, for example `/tmp`. On\n+Colab, navigate to the \"Files\" browser in the left sidebar, to view and access\n+this directory.\n+\n+You can use all the flags mentioned in the Local Execution section.\n+\n+### JAX-specific options\n+\n+Similar to local execution; for live, interactive introspection you can directly\n+print a computation’s pre-optimized HLO:\n+\n+```python\n+def f(x):\n+    return jax.numpy.sin(jax.numpy.cos(x))\n+\n+c = jax.jit(f).lower(3.).compiler_ir('hlo')\n+\n+print(c.as_hlo_text())\n+```\n+\n+You can also directly print a computation’s optimized HLO:\n+\n+```python\n+def optimized_HLO(f, *args, platform=None):\n+    print(jax.jit(f).lower(*args).compile().as_text())\n+\n+def f(x):\n+    return jax.numpy.sin(jax.numpy.cos(x))\n+\n+optimized_HLO(f, 1.0)\n+```\n+\n+#### Dumping All/Small Computations\n+\n+If you want to see everything in a dump including all small compilations, set\n+the JAX environment variable:\n+\n+```shell\n+JAX_COMPILER_DETAILED_LOGGING_MIN_OPS=0\n+```\n+\n+#### Mosaic\n+\n+Mosaic is a compiler for the Pallas TPU backend, and the experimental Pallas GPU\n+backend. To dump mosaic computation, set the following flag:\n+\n+```shell\n+--xla_mosaic_dump_to=/tmp/mosaic_dumps\n+```\n+\n+Or, set TPU init arguments as an environment variable:\n+\n+```shell\n+export LIBTPU_INIT_ARGS=\"--xla_mosaic_dump_to=/tmp/mosaic_dumps\"\n+```\n+\n+Check out the\n+[JAX documentation on Pallas and Mosaic](https://docs.jax.dev/en/latest/pallas/index.html)\n+to learn more.\n+\n+## More with HLO Dumps\n+\n+### Finding the right computation\n+\n+Usually, many computations get dumped. The dumped files are explicitly named\n+with the JAX, Tensorflow, or PyTorch/XLA \"computation name” that are called out\n+in the logs, making it easy to identify the relevant HLO files. For example:\n+\n+```\n+1624325116260738.module_0065.pmap__unnamed_wrapped_function_.186875.before_optimizations.txt\n+```\n+\n+Otherwise, you can use `ripgrep` to quickly identify which module holds\n+particular symbols or computations.\n+\n+**Tip:** Include the 3 dumped before/after/buffer-assignment files of interest\n+in your bug reports.\n+\n+### HLO Conversion\n+\n+A tool called `hlo-opt` that can translate between HLOProto and text formats.\n+It's useful in cases where you have one format, but need the other for\n+debugging.\n+\n+Learn to use it:\n+[XLA Tooling documentation: hlo-opt](tools.md#hlo-opt-convert-hlo-module-formats).\n+\n+### Replay\n+\n+You can run (replay) the dumped computations on a specified XLA backend with\n+fake data or input snapshots. This is a convenient way to reproduce, iterate,\n+and debug issues in XLA.\n+\n+The following commands use fake data. If you have saved HLO Snapshots, you can\n+pass those in instead, and the data from the snapshot will be used. To still use\n+fake data while running the snapshot, pass the flag `--force_fake_data`.\n+\n+CPU backend:\n+\n+```shell\n+bazel run -c opt //xla/hlo/tools:run_hlo_module -- --platform=cpu\n+ /tmp/xladump/module_4561.before_optimizations.txt\n+```\n+\n+GPU backend:\n+\n+```shell\n+bazel run -c opt //xla/hlo/tools:run_hlo_module -- --platform=CUDA\n+ /tmp/xladump/module_4561.before_optimizations.txt\n+```"
        }
    ],
    "stats": {
        "total": 265,
        "additions": 265,
        "deletions": 0
    }
}