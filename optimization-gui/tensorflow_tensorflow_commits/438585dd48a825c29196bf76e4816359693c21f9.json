{
    "author": "tensorflower-gardener",
    "message": "Also temporarily disable bf16 in YNNPACK\n\nThis way, we can avoid all numerical non-determinism for now.\n\nPiperOrigin-RevId: 827768849",
    "sha": "438585dd48a825c29196bf76e4816359693c21f9",
    "files": [
        {
            "sha": "6ef551eda82dd0626e37d875f1fa542abd01661a",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/438585dd48a825c29196bf76e4816359693c21f9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/438585dd48a825c29196bf76e4816359693c21f9/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc?ref=438585dd48a825c29196bf76e4816359693c21f9",
            "patch": "@@ -154,11 +154,13 @@ absl::StatusOr<bool> IsDotSupportedByYnn(\n       std::tuple<PrimitiveType, PrimitiveType, PrimitiveType>>>\n       kAllowedTypes({\n           // TODO(b/452693819): We plan to enable this in stages, starting with\n-          // bf16 and int8, and enable f32 later.\n+          // int8, and enable f32 later.\n           // {F32, F32, F32},\n           // TODO(b/449998002): We don't have fast fp16 kernels yet.\n           // {F16, F16, F32},\n-          {BF16, BF16, F32},\n+          // TODO(b/452693819): We plan to enable this in stages, starting with\n+          // int8, and enable bf16 later.\n+          // {BF16, BF16, F32},\n           {S8, S8, S32},\n           {U8, S8, S32},\n           // TODO(b/441600372): We don't have fast int4 kernels yet. Even the"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}