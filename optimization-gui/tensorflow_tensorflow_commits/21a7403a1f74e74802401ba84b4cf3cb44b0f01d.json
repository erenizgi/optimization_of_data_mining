{
    "author": "vwbaker",
    "message": "Reduce and simplify APIs going into Ptx Compiler\n\nThis change removes BundleGpuAsm and CompileGpuAsmOrGetCached, which are no longer used anywhere. The cancel_if_reg_spill is removed as we intend to deprecate it in b/443207721.\n\nPiperOrigin-RevId: 840136835",
    "sha": "21a7403a1f74e74802401ba84b4cf3cb44b0f01d",
    "files": [
        {
            "sha": "f2844a2c1a298bf7b59e312e297dd7975e67afb2",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=21a7403a1f74e74802401ba84b4cf3cb44b0f01d",
            "patch": "@@ -1095,17 +1095,12 @@ cc_library(\n         \":ptx_compiler\",\n         \":ptx_compiler_support\",\n         \":subprocess_compilation\",\n-        \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/gpu:gpu_asm_opts\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/base:core_headers\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/synchronization\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@local_tsl//tsl/platform:path\",\n     ],\n )"
        },
        {
            "sha": "29e219e135213f0e00a7427baf2f484a8d13442d",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_asm_compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 48,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.cc?ref=21a7403a1f74e74802401ba84b4cf3cb44b0f01d",
            "patch": "@@ -18,19 +18,12 @@ limitations under the License.\n #include <cassert>\n #include <cstdint>\n #include <string>\n-#include <tuple>\n #include <utility>\n #include <vector>\n \n-#include \"absl/base/const_init.h\"\n-#include \"absl/base/optimization.h\"\n-#include \"absl/base/thread_annotations.h\"\n-#include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/synchronization/mutex.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/stream_executor/cuda/cubin_or_ptx_image.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler.h\"\n@@ -48,56 +41,19 @@ absl::StatusOr<std::vector<uint8_t>> BundleGpuAsm(\n }\n \n absl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(\n-    const CudaComputeCapability& cc, const std::string& ptx, GpuAsmOpts options,\n-    bool cancel_if_reg_spill) {\n+    const CudaComputeCapability& cc, const std::string& ptx,\n+    GpuAsmOpts options) {\n   if (IsLibNvPtxCompilerSupported()) {\n     VLOG(3) << \"Compiling GPU ASM with libnvptxcompiler\";\n     TF_ASSIGN_OR_RETURN(auto assembly,\n-                        CompileGpuAsmUsingLibNvPtxCompiler(\n-                            cc, ptx, options, cancel_if_reg_spill,\n-                            /*dump_compilation_log=*/false));\n+                        CompileGpuAsmUsingLibNvPtxCompiler(cc, ptx, options));\n     return std::move(assembly.cubin);\n   }\n \n   VLOG(3) << \"Compiling GPU ASM with PTXAS. Libnvptxcompiler compilation \"\n              \"not supported.\";\n-  TF_ASSIGN_OR_RETURN(auto assembly, CompileGpuAsmUsingPtxAs(\n-                                         cc, ptx, options, cancel_if_reg_spill,\n-                                         /*dump_compilation_log=*/false));\n+  TF_ASSIGN_OR_RETURN(auto assembly, CompileGpuAsmUsingPtxAs(cc, ptx, options));\n   return std::move(assembly.cubin);\n }\n \n-absl::StatusOr<absl::Span<const uint8_t>> CompileGpuAsmOrGetCached(\n-    const CudaComputeCapability& cc, const std::string& ptx,\n-    GpuAsmOpts compilation_options) {\n-  using PtxCacheKey = std::tuple<CudaComputeCapability, std::string,\n-                                 GpuAsmOpts::PtxOptionsTuple>;\n-  using PtxCompilerResult = absl::StatusOr<std::vector<uint8_t>>;\n-  static absl::Mutex ptx_cache_mutex(absl::kConstInit);\n-  static auto& ptx_cache ABSL_GUARDED_BY(ptx_cache_mutex) =\n-      *new absl::flat_hash_map<PtxCacheKey, PtxCompilerResult>();\n-\n-  absl::MutexLock lock(ptx_cache_mutex);\n-  PtxCacheKey cache_key{cc, ptx, compilation_options.ToTuple()};\n-  auto it = ptx_cache.find(cache_key);\n-  if (it == ptx_cache.end()) {\n-    PtxCompilerResult compiled = CompileGpuAsm(cc, ptx, compilation_options);\n-    it = ptx_cache.emplace(cache_key, std::move(compiled)).first;\n-  }\n-\n-  CHECK(it != ptx_cache.end());\n-\n-  // Failed compilation attempts are cached.\n-  // Use separate status check and ValueOrDie invocation on ptx_cache\n-  // entry to avoid value moving introduced by TF_ASSIGN_OR_RETURN.\n-\n-  if (ABSL_PREDICT_FALSE(!it->second.ok())) {\n-    return it->second.status();\n-  }\n-\n-  const std::vector<uint8_t>& compiled = it->second.value();\n-  return absl::MakeSpan(compiled);\n-}\n-\n-\n }  // namespace stream_executor"
        },
        {
            "sha": "14305a3d713deb97d096df23629fe67cf5828a3b",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_asm_compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.h?ref=21a7403a1f74e74802401ba84b4cf3cb44b0f01d",
            "patch": "@@ -20,11 +20,9 @@ limitations under the License.\n #include <string>\n #include <vector>\n \n-#include \"absl/base/macros.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/stream_executor/cuda/cubin_or_ptx_image.h\"\n-#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/gpu/gpu_asm_opts.h\"\n \n namespace stream_executor {\n@@ -36,23 +34,7 @@ namespace stream_executor {\n // customized in a passed flag, and for controlling ptxas optimizations.\n absl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(\n     const CudaComputeCapability& cc, const std::string& ptx_contents,\n-    GpuAsmOpts options, bool cancel_if_reg_spill = false);\n-\n-// Temporary overload for users outside of XLA that still use the old API.\n-inline absl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(\n-    int cc_major, int cc_minor, const char* ptx_contents, GpuAsmOpts options,\n-    bool cancel_if_reg_spill = false) {\n-  return CompileGpuAsm(CudaComputeCapability(cc_major, cc_minor),\n-                       std::string(ptx_contents), options, cancel_if_reg_spill);\n-}\n-\n-// Same as CompileGpuAsm, but caches the result, and returns unowned view of\n-// the compiled binary.\n-//\n-// A copy of the string provided in ptx will be made.\n-absl::StatusOr<absl::Span<const uint8_t>> CompileGpuAsmOrGetCached(\n-    const CudaComputeCapability& cc, const std::string& ptx_contents,\n-    GpuAsmOpts compilation_options);\n+    GpuAsmOpts options);\n \n // Bundles the GPU machine code (cubins) and PTX if requested and returns the\n // resulting binary (i.e. a fatbin) as a byte array."
        },
        {
            "sha": "509e6c1ab392c06bb1a8666dcabcde9737b34aed",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21a7403a1f74e74802401ba84b4cf3cb44b0f01d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler.h?ref=21a7403a1f74e74802401ba84b4cf3cb44b0f01d",
            "patch": "@@ -29,7 +29,7 @@ namespace stream_executor {\n // targeting the sm_<cc_major>.<cc_minor> NVIDIA GPU architecture.\n absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingLibNvPtxCompiler(\n     const CudaComputeCapability& cc, const std::string& ptx, GpuAsmOpts options,\n-    bool cancel_if_reg_spill, bool dump_compilation_log);\n+    bool cancel_if_reg_spill = false, bool dump_compilation_log = false);\n \n absl::StatusOr<SemanticVersion> GetLibNvPtxCompilerVersion();\n "
        }
    ],
    "stats": {
        "total": 81,
        "additions": 7,
        "deletions": 74
    }
}