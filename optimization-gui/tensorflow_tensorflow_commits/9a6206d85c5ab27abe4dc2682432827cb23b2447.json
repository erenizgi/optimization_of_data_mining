{
    "author": "golechwierowicz",
    "message": "[XLA:GPU] Do not use the SoL cost model for host offloading case.\n\nPiperOrigin-RevId: 804777547",
    "sha": "9a6206d85c5ab27abe4dc2682432827cb23b2447",
    "files": [
        {
            "sha": "ed6b396e77f877593b4251067473c144c8ece460",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a6206d85c5ab27abe4dc2682432827cb23b2447/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a6206d85c5ab27abe4dc2682432827cb23b2447/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=9a6206d85c5ab27abe4dc2682432827cb23b2447",
            "patch": "@@ -85,7 +85,6 @@ xla_cc_test(\n     srcs = [\"sol_latency_estimator_test.cc\"],\n     deps = [\n         \":collective_interpolator\",\n-        \":gpu_performance_model_base\",\n         \":sol_gpu_cost_model\",\n         \":sol_latency_estimator\",\n         \"//xla:literal_util\",\n@@ -101,6 +100,7 @@ xla_cc_test(\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tests:xla_internal_test_main\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "0ffcc9751b8a50079433a4560f723c18d3b09e03",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a6206d85c5ab27abe4dc2682432827cb23b2447/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a6206d85c5ab27abe4dc2682432827cb23b2447/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=9a6206d85c5ab27abe4dc2682432827cb23b2447",
            "patch": "@@ -63,9 +63,18 @@ bool IsSupportedCollectiveOp(const HloInstruction& instr) {\n                           HloOpcode::kAllGather>(&instr);\n }\n \n+bool IsHostOffloaded(const HloInstruction& instr) {\n+  auto backend_config = instr.backend_config<GpuBackendConfig>();\n+  return backend_config.ok() &&\n+         backend_config->device_type() == DEVICE_TYPE_HOST;\n+}\n+\n bool HasOnlySupportedCollectives(const HloModule& module) {\n   for (const HloComputation* comp : module.computations()) {\n     for (const HloInstruction* instr : comp->instructions()) {\n+      if (IsHostOffloaded(*instr)) {\n+        return false;\n+      }\n       if (hlo_query::IsCollectiveCommunicationOp(instr->opcode()) &&\n           !IsSupportedCollectiveOp(*instr)) {\n         return false;"
        },
        {
            "sha": "c0934b562f77d1ea4a816fa98c35bf16ea12943f",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 1,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a6206d85c5ab27abe4dc2682432827cb23b2447/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a6206d85c5ab27abe4dc2682432827cb23b2447/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=9a6206d85c5ab27abe4dc2682432827cb23b2447",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/literal_util.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/collective_interpolator.h\"\n-#include \"xla/service/gpu/model/gpu_performance_model_base.h\"\n #include \"xla/service/gpu/model/sol_gpu_cost_model.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -42,6 +41,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n@@ -424,6 +424,19 @@ class IsSolLatencyEstimatorEnabledTest : public HloTestBase {\n         shape, dummy_operand, /*source_target_pairs=*/{}, std::nullopt));\n   }\n \n+  absl::Status AddHostOffloaded(HloModule* module) {\n+    HloComputation* entry = module->entry_computation();\n+    Shape shape = ShapeUtil::MakeShape(F32, {2});\n+    auto dummy_operand = entry->AddInstruction(\n+        HloInstruction::CreateConstant(LiteralUtil::CreateR1<float>({2})));\n+    HloInstruction* call =\n+        entry->AddInstruction(HloInstruction::CreateCall(shape, dummy_operand));\n+    TF_ASSIGN_OR_RETURN(GpuBackendConfig new_backend_config,\n+                        call->backend_config<GpuBackendConfig>());\n+    new_backend_config.set_device_type(DEVICE_TYPE_HOST);\n+    return call->set_backend_config(new_backend_config);\n+  }\n+\n   se::DeviceDescription gpu_device_info_;\n };\n \n@@ -445,6 +458,9 @@ TEST_F(IsSolLatencyEstimatorEnabledTest, DisabledIfFlagIsOffOnHopper) {\n   gpu_device_info_.set_cuda_compute_capability(\n       stream_executor::CudaComputeCapability::Hopper());\n \n+  config.mutable_debug_options()\n+      .set_xla_gpu_enable_analytical_sol_latency_estimator(false);\n+\n   auto module = CreateTestModule(config);\n \n   EXPECT_FALSE(\n@@ -499,5 +515,20 @@ TEST_F(IsSolLatencyEstimatorEnabledTest, DisabledIfNotHopper) {\n       SolLatencyEstimator::IsSupportedForModule(*module, gpu_device_info_));\n }\n \n+TEST_F(IsSolLatencyEstimatorEnabledTest, DisabledForHopperWithHostOffloaded) {\n+  HloModuleConfig config;\n+  config.mutable_debug_options()\n+      .set_xla_gpu_enable_analytical_sol_latency_estimator(true);\n+\n+  gpu_device_info_.set_cuda_compute_capability(\n+      stream_executor::CudaComputeCapability::Hopper());\n+\n+  auto module = CreateTestModule(config);\n+  TF_ASSERT_OK(AddHostOffloaded(module.get()));\n+\n+  EXPECT_FALSE(\n+      SolLatencyEstimator::IsSupportedForModule(*module, gpu_device_info_));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 42,
        "deletions": 2
    }
}