{
    "author": "ezhulenev",
    "message": "[xla:ffi] Add correct error handling to internal FFI APIs\n\nPreviously we could reinterpret cast XLA_FFI_Error* as a return type and it would lead to UB and crashes.\n\nPiperOrigin-RevId: 836832350",
    "sha": "36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489",
    "files": [
        {
            "sha": "f414a759c477b1478bfbf91cfedec86d751b35f7",
            "filename": "third_party/xla/xla/ffi/api/api.h",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fapi.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fapi.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fapi.h?ref=36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489",
            "patch": "@@ -470,6 +470,32 @@ inline XLA_FFI_Error* Ffi::StructSizeIsGreaterOrEqual(\n   return nullptr;\n }\n \n+//===----------------------------------------------------------------------===//\n+// XLA_FFI_Error helpers\n+//===----------------------------------------------------------------------===//\n+\n+namespace internal {\n+\n+inline void DestroyError(const XLA_FFI_Api* api, XLA_FFI_Error* error) {\n+  XLA_FFI_Error_Destroy_Args args;\n+  args.struct_size = XLA_FFI_Error_Destroy_Args_STRUCT_SIZE;\n+  args.extension_start = nullptr;\n+  args.error = error;\n+  api->XLA_FFI_Error_Destroy(&args);\n+}\n+\n+inline const char* GetErrorMessage(const XLA_FFI_Api* api,\n+                                   XLA_FFI_Error* error) {\n+  XLA_FFI_Error_GetMessage_Args args;\n+  args.struct_size = XLA_FFI_Error_GetMessage_Args_STRUCT_SIZE;\n+  args.extension_start = nullptr;\n+  args.error = error;\n+  api->XLA_FFI_Error_GetMessage(&args);\n+  return args.message;\n+}\n+\n+}  // namespace internal\n+\n //===----------------------------------------------------------------------===//\n // Type tags for distinguishing handler argument types\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "d620936e2f881f9a1d49e71b00a1ce16468432b1",
            "filename": "third_party/xla/xla/ffi/api/c_api_internal.h",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api_internal.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api_internal.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api_internal.h?ref=36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489",
            "patch": "@@ -79,8 +79,8 @@ typedef void* XLA_FFI_INTERNAL_ExecutionState_Get(\n \n // Returns a pointer to the `Eigen::ThreadPoolDevice` passed via run options,\n // which allows FFI handlers to execute tasks in the same thread pool as XLA.\n-typedef void* XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n-    XLA_FFI_ExecutionContext* ctx);\n+typedef XLA_FFI_Error* XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** thread_pool);\n \n //===----------------------------------------------------------------------===//\n // XLA:GPU specific internal APIs.\n@@ -90,13 +90,14 @@ typedef void* XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n // contrast to public C API which returns a pointer to underlying platform\n // stream (i.e. cudaStream_t for CUDA backend), this API returns a pointer to\n // StreamExecutor stream which is unsafe to use across dynamic library boundary.\n-typedef void* XLA_FFI_INTERNAL_Stream_Get(XLA_FFI_ExecutionContext* ctx);\n+typedef XLA_FFI_Error* XLA_FFI_INTERNAL_Stream_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** stream);\n \n // Returns a pointer to device memory allocator (`se::DeviceMemoryAllocator`\n // pointer) which allows to allocate memory inside a custom call from the same\n // allocator as XLA (i.e. it allows to construct scratch memory allocator).\n-typedef void* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n-    XLA_FFI_ExecutionContext* ctx);\n+typedef XLA_FFI_Error* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** allocator);\n \n //===----------------------------------------------------------------------===//\n // API access"
        },
        {
            "sha": "2a7939dbcabbf68a672deedd93c26feb3004d29c",
            "filename": "third_party/xla/xla/ffi/api/ffi.h",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fffi.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fffi.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fffi.h?ref=36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489",
            "patch": "@@ -1172,24 +1172,6 @@ inline XLA_FFI_Error* CreateError(const XLA_FFI_Api* api, const Error& error) {\n   return api->XLA_FFI_Error_Create(&args);\n }\n \n-inline void DestroyError(const XLA_FFI_Api* api, XLA_FFI_Error* error) {\n-  XLA_FFI_Error_Destroy_Args args;\n-  args.struct_size = XLA_FFI_Error_Destroy_Args_STRUCT_SIZE;\n-  args.extension_start = nullptr;\n-  args.error = error;\n-  api->XLA_FFI_Error_Destroy(&args);\n-}\n-\n-inline const char* GetErrorMessage(const XLA_FFI_Api* api,\n-                                   XLA_FFI_Error* error) {\n-  XLA_FFI_Error_GetMessage_Args args;\n-  args.struct_size = XLA_FFI_Error_GetMessage_Args_STRUCT_SIZE;\n-  args.extension_start = nullptr;\n-  args.error = error;\n-  api->XLA_FFI_Error_GetMessage(&args);\n-  return args.message;\n-}\n-\n }  // namespace internal\n \n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "e8adf8d812e3f87c809db977b677d28e73abad52",
            "filename": "third_party/xla/xla/ffi/ffi.h",
            "status": "modified",
            "additions": 41,
            "deletions": 17,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fffi.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fffi.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi.h?ref=36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489",
            "patch": "@@ -559,11 +559,16 @@ struct CtxDecoding<Stream> {\n   static std::optional<Type> Decode(const XLA_FFI_Api* api,\n                                     XLA_FFI_ExecutionContext* ctx,\n                                     DiagnosticEngine& diagnostic) {\n-    void* ptr = api->internal_api->XLA_FFI_INTERNAL_Stream_Get(ctx);\n-    if (ABSL_PREDICT_FALSE(ptr == nullptr)) {\n-      return diagnostic.Emit(\"Failed to decode stream\");\n+    void* stream = nullptr;\n+    if (XLA_FFI_Error* error =\n+            api->internal_api->XLA_FFI_INTERNAL_Stream_Get(ctx, &stream);\n+        ABSL_PREDICT_FALSE(error)) {\n+      diagnostic.Emit(\"Failed to get stream: \")\n+          << internal::GetErrorMessage(api, error);\n+      internal::DestroyError(api, error);\n+      return std::nullopt;\n     }\n-    return reinterpret_cast<Type>(ptr);\n+    return reinterpret_cast<Type>(stream);\n   }\n };\n \n@@ -584,9 +589,17 @@ struct CtxDecoding<Allocator> {\n \n   static std::optional<Type> Decode(const XLA_FFI_Api* api,\n                                     XLA_FFI_ExecutionContext* ctx,\n-                                    DiagnosticEngine&) {\n-    void* device_allocator =\n-        api->internal_api->XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(ctx);\n+                                    DiagnosticEngine& diagnostic) {\n+    void* device_allocator = nullptr;\n+    if (XLA_FFI_Error* error =\n+            api->internal_api->XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n+                ctx, &device_allocator);\n+        ABSL_PREDICT_FALSE(error)) {\n+      diagnostic.Emit(\"Failed to get device memory allocator: \")\n+          << internal::GetErrorMessage(api, error);\n+      internal::DestroyError(api, error);\n+      return std::nullopt;\n+    }\n     return reinterpret_cast<Type>(device_allocator);\n   }\n };\n@@ -597,15 +610,17 @@ struct CtxDecoding<ScratchAllocator> {\n \n   static std::optional<Type> Decode(const XLA_FFI_Api* api,\n                                     XLA_FFI_ExecutionContext* ctx,\n-                                    DiagnosticEngine&) {\n+                                    DiagnosticEngine& diagnostic) {\n     int32_t device_ordinal =\n         api->internal_api->XLA_FFI_INTERNAL_DeviceOrdinal_Get(ctx);\n-    void* device_allocator =\n-        api->internal_api->XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(ctx);\n \n-    return se::OwningScratchAllocator<>(\n-        device_ordinal,\n-        reinterpret_cast<se::DeviceMemoryAllocator*>(device_allocator));\n+    auto device_allocator =\n+        CtxDecoding<Allocator>::Decode(api, ctx, diagnostic);\n+    if (ABSL_PREDICT_FALSE(!device_allocator)) {\n+      return std::nullopt;\n+    }\n+\n+    return se::OwningScratchAllocator<>(device_ordinal, *device_allocator);\n   }\n };\n \n@@ -627,10 +642,19 @@ struct CtxDecoding<IntraOpThreadPool> {\n \n   static std::optional<Type> Decode(const XLA_FFI_Api* api,\n                                     XLA_FFI_ExecutionContext* ctx,\n-                                    DiagnosticEngine&) {\n-    void* intra_op_thread_pool =\n-        api->internal_api->XLA_FFI_INTERNAL_IntraOpThreadPool_Get(ctx);\n-    return reinterpret_cast<Type>(intra_op_thread_pool);\n+                                    DiagnosticEngine& diagnostic) {\n+    void* thread_pool = nullptr;\n+    if (XLA_FFI_Error* error =\n+            api->internal_api->XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n+                ctx, &thread_pool);\n+        ABSL_PREDICT_FALSE(error)) {\n+      diagnostic.Emit(\"Failed to get intra op thread pool: \")\n+          << internal::GetErrorMessage(api, error);\n+      internal::DestroyError(api, error);\n+      return std::nullopt;\n+    }\n+\n+    return reinterpret_cast<Type>(thread_pool);\n   }\n };\n "
        },
        {
            "sha": "05655c0f2d5f4a92415808798c1cb077f3d05b9b",
            "filename": "third_party/xla/xla/ffi/ffi_api.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 10,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc?ref=36cde4be9c1c7ab8a6bed00f1b0ef5abdb994489",
            "patch": "@@ -936,37 +936,47 @@ static void* XLA_FFI_INTERNAL_ExecutionState_Get(\n // XLA:CPU specific internal APIs.\n //===----------------------------------------------------------------------===//\n \n-static void* XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n-    XLA_FFI_ExecutionContext* ctx) {\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** thread_pool) {\n   if (auto* cpu = std::get_if<XLA_FFI_ExecutionContext::CpuContext>(\n           &ctx->backend_context)) {\n-    return const_cast<Eigen::ThreadPoolDevice*>(  // NOLINT\n+    *thread_pool = const_cast<Eigen::ThreadPoolDevice*>(  // NOLINT\n         cpu->intra_op_thread_pool);\n+    return nullptr;\n   }\n \n-  return new XLA_FFI_Error{\n-      InvalidArgument(\"XLA FFI CPU context is not available\")};\n+  // For GPU backend we don't have intra-op thread pool, but we didn't promise\n+  // to return one, so instead of an error we return a nullptr thread pool.\n+  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n+          &ctx->backend_context)) {\n+    return nullptr;\n+  }\n+\n+  return new XLA_FFI_Error{InvalidArgument(\"XLA FFI context is not available\")};\n }\n \n //===----------------------------------------------------------------------===//\n // XLA:GPU specific internal APIs.\n //===----------------------------------------------------------------------===//\n \n-static void* XLA_FFI_INTERNAL_Stream_Get(XLA_FFI_ExecutionContext* ctx) {\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_Stream_Get(XLA_FFI_ExecutionContext* ctx,\n+                                                  void** stream) {\n   if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n           &ctx->backend_context)) {\n-    return gpu->stream;\n+    *stream = gpu->stream;\n+    return nullptr;\n   }\n \n   return new XLA_FFI_Error{\n       InvalidArgument(\"XLA FFI GPU context is not available\")};\n }\n \n-static void* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n-    XLA_FFI_ExecutionContext* ctx) {\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** allocator) {\n   if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n           &ctx->backend_context)) {\n-    return gpu->allocator;\n+    *allocator = gpu->allocator;\n+    return nullptr;\n   }\n \n   return new XLA_FFI_Error{"
        }
    ],
    "stats": {
        "total": 143,
        "additions": 93,
        "deletions": 50
    }
}