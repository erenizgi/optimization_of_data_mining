{
    "author": "chsigg",
    "message": "[xla:gpu] Move `squeeze-dim` and `fold-transpose` passes to the beginning of the pipeline.\n\nAllow `triton_xla.extract/insert` to be rank-reducing (just like `tensor.extract/insert_slice`), and enforce `sizes` and `strides` to be static.\n\nThis simplifies the patterns because we don't have to deal with `reshape(join(inline_asm))` generated by the `int4-rewrite` pass.\n\nWe also won't need to implement special handling for TMA because it now happens before emitting TMA-specific code in the `extract/insert-rewrite` pass.\n\nIn the `extract/insert-rewrite` pass, emit pointer arithmetic directly instead of `tt.make_tensor_ptr`.\n\nPiperOrigin-RevId: 802476703",
    "sha": "0162a1c45a2f3a13e2035c4686a16edf183a251a",
    "files": [
        {
            "sha": "9344a819a14bdd41c540bfd95d47f52f05e5e397",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -236,7 +236,7 @@ class TileInfo {\n   ValueRange offsets() const { return offsets_; }\n \n   // Tile strides. Its size is equal to the rank of the output shape.\n-  ValueRange tile_strides() const { return tile_strides_; }\n+  ArrayRef<int64_t> tile_strides() const { return tile_strides_; }\n \n   // The original shape of the tensor.\n   ArrayRef<int64_t> original_shape() const { return original_shape_; }\n@@ -255,13 +255,14 @@ class TileInfo {\n \n  private:\n   SmallVector<Value> offsets_;\n-  SmallVector<Value> tile_strides_;\n+  SmallVector<int64_t> tile_strides_;\n   SmallVector<int64_t> original_shape_;\n   SmallVector<int64_t> padded_tile_sizes_;\n   SmallVector<int64_t> minor_to_major_layout_;\n   Type storage_type_;\n \n-  explicit TileInfo(SmallVector<Value> offsets, SmallVector<Value> tile_strides,\n+  explicit TileInfo(SmallVector<Value> offsets,\n+                    SmallVector<int64_t> tile_strides,\n                     SmallVector<int64_t> original_shape,\n                     SmallVector<int64_t> padded_tile_sizes,\n                     SmallVector<int64_t> minor_to_major_layout,\n@@ -291,7 +292,7 @@ absl::StatusOr<TileInfo> TileInfo::Construct(\n                       TritonType(b, shape.element_type()));\n   auto storage_type = StorageType(expected_element_type);\n \n-  auto tile_strides = CreateIndexValues(b, tiled_hlo.tile_strides());\n+  auto tile_strides = tiled_hlo.tile_strides();\n   auto minor_to_major_layout = llvm::to_vector(LayoutUtil::MinorToMajor(shape));\n \n   return TileInfo(offsets, tile_strides, original_shape, padded_tile_sizes,\n@@ -352,11 +353,13 @@ ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder b,\n         ttir::EvictionPolicy::NORMAL, /*isVolatile=*/false));\n   }\n \n-  return ScalarOrTensor(b.create<mtx::ExtractOp>(\n+  return ScalarOrTensor(mtx::ExtractOp::create(\n+      b,\n       mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n                                   tile_info.storage_type()),\n-      parent_base_ptr, tile_info.offsets(), tile_info.tile_strides(),\n-      tile_info.original_shape(), tile_info.minor_to_major_layout()));\n+      parent_base_ptr, tile_info.offsets(), tile_info.padded_tile_sizes(),\n+      tile_info.tile_strides(), tile_info.original_shape(),\n+      tile_info.minor_to_major_layout()));\n }\n \n absl::StatusOr<ScalarOrTensor> EmitScope(\n@@ -1659,8 +1662,8 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n            \"non-empty.\";\n \n     mtx::InsertOp::create(b, result.UnwrapTensor(), parent_base_ptr,\n-                          tile_info.offsets(), tile_info.tile_strides(),\n-                          tile_info.original_shape(),\n+                          tile_info.offsets(), tile_info.padded_tile_sizes(),\n+                          tile_info.tile_strides(), tile_info.original_shape(),\n                           tile_info.minor_to_major_layout());\n   }\n \n@@ -2017,6 +2020,9 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     }\n   }\n \n+  pm.addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n+  pm.addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n+\n   if (is_xla_fusion) {\n     pm.addPass(\n         mlir::triton::xla::CreateInt4ToPackedInt4RewritePass(device_info));\n@@ -2025,9 +2031,6 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   pm.addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n       device_info, block_level_parameters.is_tma_allowed));\n \n-  pm.addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n-  pm.addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n-\n   // Lower affine expressions into arithmetic ops.\n   pm.addPass(mlir::createLowerAffinePass());\n "
        },
        {
            "sha": "f67b4eaba79170a585b41d0e6ee3490df909276d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2FBUILD?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -96,7 +96,6 @@ cc_library(\n         \"@llvm-project//mlir:InferTypeOpInterface\",\n         \"@llvm-project//mlir:SideEffectInterfaces\",\n         \"@llvm-project//mlir:Support\",\n-        \"@llvm-project//mlir:ViewLikeInterface\",\n         \"@triton//:TritonDialects\",\n     ],\n )"
        },
        {
            "sha": "2964fae0a79c218068df3d07d9d0a9e4c9538d75",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/tests/canonicalize.mlir",
            "status": "modified",
            "additions": 15,
            "deletions": 28,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fcanonicalize.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fcanonicalize.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fcanonicalize.mlir?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -1,32 +1,19 @@\n-// RUN: xla-opt %s --split-input-file --canonicalize | FileCheck %s\n+// RUN: xla-opt %s --canonicalize | FileCheck %s\n \n-tt.func @xla_triton_extract(%arg0: !tt.ptr<bf16>, %i : index)\n-    -> tensor<16x64xbf16> {\n+// CHECK-LABEL: xla_triton_extract_insert\n+tt.func @xla_triton_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: index) {\n   %c0 = arith.constant 0 : index\n-  %c1 = arith.constant 1 : index\n-  %c128 = arith.constant 128 : index\n-  %extracted_tensor = triton_xla.extract from %arg0 as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n-      [%c0, %i] [16, 64] [%c128, %c1] {noinline = false} : tensor<16x64xbf16>\n-  tt.return %extracted_tensor : tensor<16x64xbf16>\n-}\n-// CHECK-LABEL: xla_triton_extract\n-\n-// CHECK:       triton_xla.extract\n-// CHECK-SAME:    [0, %{{.*}}] [16, 64] [128, 1]\n-// CHECK-SAME:    {noinline = false}\n-\n-// -----\n-\n-tt.func @xla_triton_insert(%src: tensor<16x64xbf16>, %dst: !tt.ptr<bf16>,\n-    %j: index) {\n-  %c0 = arith.constant 0 : index\n-  %c1 = arith.constant 1 : index\n-  %c64 = arith.constant 64 : index\n-  triton_xla.insert %src into %dst as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n-    [%c0, %c0][16, 64][%j, %c1] {noinline = false} : tensor<16x64xbf16>\n+  // CHECK:       triton_xla.extract\n+  // CHECK-SAME:    [%arg1, 0] [16, 64] [128, 1]\n+  // CHECK-SAME:    {noinline = false}\n+  %tile = triton_xla.extract from %arg0\n+      as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+      [%arg1, %c0] [16, 64] [128, 1] {noinline = false} : tensor<16x64xbf16>\n+  // CHECK:       triton_xla.insert\n+  // CHECK-SAME:    [0, %arg1] [16, 64] [1, 1]\n+  // CHECK-SAME:    {noinline = false}\n+  triton_xla.insert %tile into %arg0\n+      as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+      [%c0, %arg1][16, 64][1, 1] {noinline = false} : tensor<16x64xbf16>\n   tt.return\n }\n-// CHECK-LABEL: xla_triton_insert\n-// CHECK:       triton_xla.insert\n-// CHECK-SAME:    [0, 0] [16, 64] [%{{.*}}, 1]\n-// CHECK-SAME:    {noinline = false}"
        },
        {
            "sha": "06d645febdf17ae3f87955f1098b5ce789732ac3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/tests/invalid.mlir",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -1,63 +1,63 @@\n // RUN: xla-opt --split-input-file --verify-diagnostics %s\n \n func.func @extract_0d(%arg0: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{cannot extract a 0-d tensor}}\n+  // expected-error @+1 {{unsupported 0-d tensor}}\n   %0 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [][][] : tensor<bf16>\n   return\n }\n \n // -----\n \n func.func @insert_0d(%arg0: tensor<bf16>, %arg1: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{cannot insert a 0-d tensor}}\n+  // expected-error @+1 {{unsupported 0-d tensor}}\n   triton_xla.insert %arg0 into %arg1 as memref<bf16, #triton_xla.layout<[]>> [][][] : tensor<bf16>\n   return\n }\n \n // -----\n \n-func.func @extract_wrong_rank(%arg0: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{shape attribute has a wrong size}}\n-  %0 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+func.func @extract_wrong_layout(%arg0: !tt.ptr<bf16>) {\n+  // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n+  %0 = triton_xla.extract from %arg0 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n-func.func @extract_wrong_shape(%arg0: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{shape size must match operand size}}\n-  %0 = triton_xla.extract from %arg0 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n+func.func @insert_wrong_layout(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n+  // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n+  triton_xla.insert %arg0 into %arg1 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n-func.func @extract_wrong_layout(%arg0: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n-  %0 = triton_xla.extract from %arg0 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+func.func @extract_wrong_rank(%arg0: !tt.ptr<bf16>) {\n+  // expected-error @+1 {{expected 0 offset values, got 1}}\n+  %0 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n func.func @insert_wrong_rank(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{shape attribute has a wrong size}}\n+  // expected-error @+1 {{expected 0 offset values, got 1}}\n   triton_xla.insert %arg0 into %arg1 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n-func.func @insert_wrong_shape(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{shape size must match operand size}}\n-  triton_xla.insert %arg0 into %arg1 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n+func.func @extract_wrong_shape(%arg0: !tt.ptr<bf16>) {\n+  // expected-error @+1 {{expected type to be 'tensor<16xbf16>'}}\n+  %0 = triton_xla.extract from %arg0 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n-func.func @insert_wrong_layout(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n-  // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n-  triton_xla.insert %arg0 into %arg1 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+func.func @insert_wrong_shape(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n+  // expected-error @+1 {{expected type to be 'tensor<16xbf16>'}}\n+  triton_xla.insert %arg0 into %arg1 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n   return\n }"
        },
        {
            "sha": "d8c7e1318f95234c71f3cab2d611be7d5bb46753",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/tests/ops.mlir",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fops.mlir?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -10,8 +10,8 @@\n tt.func @xla_triton_extract(%src: !tt.ptr<bf16>, %i : index) -> tensor<16x64xbf16> {\n   // CHECK: triton_xla.extract\n   %extracted_tensor = triton_xla.extract from %src\n-    as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n-    [0, %i] [16, 64] [128, 1] : tensor<16x64xbf16>\n+    as memref<512x1x128xbf16, #triton_xla.layout<[2, 1, 0]>>\n+    [0, 0, %i] [16, 1, 64] [128, 1, 1] : tensor<16x64xbf16>\n   tt.return %extracted_tensor : tensor<16x64xbf16>\n }\n \n@@ -20,6 +20,6 @@ tt.func @xla_triton_insert(%src: tensor<16x64xbf16>, %dst: !tt.ptr<bf16>, %j: in\n   // CHECK: triton_xla.insert\n   triton_xla.insert %src into %dst\n     as memref<512x128xbf16, #triton_xla.layout<[0, 1]>>\n-    [0, 0][16, 64][%j, 1] : tensor<16x64xbf16>\n+    [%j, 0][16, 64][1, 1] : tensor<16x64xbf16>\n   tt.return\n }"
        },
        {
            "sha": "1f1a00741bf05d23e7354e134e0c778741afdf9d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_attrs.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include <cstdint>\n \n #include \"llvm/ADT/STLExtras.h\"\n+#include \"mlir/Dialect/Utils/IndexingUtils.h\"\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/Diagnostics.h\"\n@@ -109,6 +110,10 @@ LogicalResult LayoutAttr::verifyLayout(\n                  << \" dimensions, but shape has \" << shape.size();\n     return failure();\n   }\n+  if (!isPermutationVector(getMinorToMajor().asArrayRef())) {\n+    emit_error() << \"layout is not a permutation\";\n+    return failure();\n+  }\n   return success();\n }\n "
        },
        {
            "sha": "322c84686817c6ed94b13d105e7c27c5d7ce0b6a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.cc",
            "status": "modified",
            "additions": 89,
            "deletions": 86,
            "changes": 175,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -18,14 +18,15 @@ limitations under the License.\n #include <cassert>\n #include <cstdint>\n \n-#include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/TypeSwitch.h\"  // IWYU pragma: keep\n+#include \"llvm/Support/ErrorHandling.h\"\n #include \"llvm/Support/LogicalResult.h\"\n #include \"mlir/Dialect/Utils/StaticValueUtils.h\"\n #include \"mlir/IR/Builders.h\"  // IWYU pragma: keep\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/DialectImplementation.h\"  // IWYU pragma: keep\n #include \"mlir/IR/MLIRContext.h\"  // IWYU pragma: keep\n #include \"mlir/IR/OperationSupport.h\"\n@@ -80,18 +81,53 @@ void printAsMemRefType(OpAsmPrinter& printer, Operation* op, PointerType type,\n                              memory_space);\n }\n \n-LogicalResult verifyShapeMatchesSizes(Operation* op,\n-                                      ArrayRef<int64_t> shape_sizes,\n-                                      ArrayRef<OpFoldResult> operand_sizes) {\n-  for (auto [shape_size, operand_size] :\n-       llvm::zip_equal(shape_sizes, operand_sizes)) {\n-    auto attr =\n-        dyn_cast_if_present<IntegerAttr>(dyn_cast<Attribute>(operand_size));\n-    if (attr && shape_size != ShapedType::kDynamic &&\n-        shape_size != attr.getValue()) {\n-      return op->emitError(\"shape size must match operand size\");\n-    }\n+static LogicalResult produceSliceErrorMsg(SliceVerificationResult result,\n+                                          Operation* op,\n+                                          RankedTensorType expected_type) {\n+  switch (result) {\n+    case SliceVerificationResult::Success:\n+      return success();\n+    case SliceVerificationResult::RankTooLarge:\n+      return op->emitError(\"expected rank to be smaller or equal to \")\n+             << \"the other rank. \";\n+    case SliceVerificationResult::SizeMismatch:\n+      return op->emitError(\"expected type to be \")\n+             << expected_type << \" or a rank-reduced version. (size mismatch) \";\n+    case SliceVerificationResult::ElemTypeMismatch:\n+      return op->emitError(\"expected element type to be \")\n+             << expected_type.getElementType();\n+    default:\n+      llvm_unreachable(\"unexpected extract_slice op verification result\");\n+  }\n+}\n+\n+static LogicalResult verifyExtractInsert(\n+    Operation* op, RankedTensorType tensor_type, PointerType pointer_type,\n+    DenseI64ArrayAttr layout, ArrayRef<int64_t> shape, ArrayRef<int64_t> sizes,\n+    ArrayRef<int64_t> strides) {\n+  if (tensor_type.getRank() == 0) {\n+    return op->emitError(\"unsupported 0-d tensor\");\n+  }\n+  if (ShapedType::isDynamicShape(sizes)) {\n+    return op->emitError(\"dynamic sizes are not supported\");\n+  }\n+  if (ShapedType::isDynamicShape(strides)) {\n+    return op->emitError(\"dynamic strides are not supported\");\n   }\n+  if (failed(LayoutAttr::get(op->getContext(), layout).verifyLayout(shape, [&] {\n+        return op->emitError();\n+      }))) {\n+    return failure();\n+  }\n+  auto expected_type =\n+      RankedTensorType::get(sizes, pointer_type.getPointeeType());\n+  SliceVerificationResult result =\n+      isRankReducedType(expected_type, tensor_type);\n+  if (result != SliceVerificationResult::Success) {\n+    return produceSliceErrorMsg(result, op, expected_type);\n+  }\n+  // Note: other than tensor.extract/insert, offsets, sizes, strides may run\n+  // out-of-bounds with respect to the source/destination.\n   return success();\n }\n \n@@ -105,45 +141,32 @@ void ExtractOp::getAsmResultNames(\n }\n \n LogicalResult ExtractOp::verify() {\n-  int64_t rank = getType().getRank();\n-  if (rank == 0) {\n-    return emitError(\"cannot extract a 0-d tensor\");\n-  }\n-  if (rank != getSrcShape().size()) {\n-    return emitError(\"shape attribute has a wrong size\");\n-  }\n-  if (rank != getSrcLayout().size()) {\n-    return emitError(\"layout attribute has a wrong size\");\n-  }\n-  if (getType().getElementType() != getSrc().getType().getPointeeType()) {\n-    return emitError(\"src pointee type must match result element type\");\n-  }\n-  return verifyShapeMatchesSizes(getOperation(), getType().getShape(),\n-                                 getMixedSizes());\n+  return verifyExtractInsert(getOperation(), getType(), getSrc().getType(),\n+                             getSrcLayoutAttr(), getSrcShape(),\n+                             getStaticSizes(), getStaticStrides());\n }\n \n void ExtractOp::build(OpBuilder& b, OperationState& result,\n                       RankedTensorType result_type, Value src,\n-                      ArrayRef<OpFoldResult> offsets,\n-                      ArrayRef<OpFoldResult> strides, ArrayRef<int64_t> shape,\n-                      ArrayRef<int64_t> layout) {\n-  SmallVector<int64_t> static_offsets, static_strides;\n-  SmallVector<Value> dynamic_offsets, dynamic_strides;\n+                      ArrayRef<OpFoldResult> offsets, ArrayRef<int64_t> sizes,\n+                      ArrayRef<int64_t> strides, ArrayRef<int64_t> src_shape,\n+                      ArrayRef<int64_t> src_layout) {\n+  SmallVector<int64_t> static_offsets;\n+  SmallVector<Value> dynamic_offsets;\n   dispatchIndexOpFoldResults(offsets, dynamic_offsets, static_offsets);\n-  dispatchIndexOpFoldResults(strides, dynamic_strides, static_strides);\n-  build(b, result, result_type, src, dynamic_offsets, {}, dynamic_strides,\n-        b.getDenseI64ArrayAttr(static_offsets),\n-        b.getDenseI64ArrayAttr(result_type.getShape()),\n-        b.getDenseI64ArrayAttr(static_strides), b.getDenseI64ArrayAttr(shape),\n-        b.getDenseI64ArrayAttr(layout));\n+  build(b, result, result_type, src, dynamic_offsets, /*sizes=*/{},\n+        /*strides=*/{}, b.getDenseI64ArrayAttr(static_offsets),\n+        b.getDenseI64ArrayAttr(sizes), b.getDenseI64ArrayAttr(strides),\n+        b.getDenseI64ArrayAttr(src_shape), b.getDenseI64ArrayAttr(src_layout));\n }\n \n void ExtractOp::build(OpBuilder& b, OperationState& result,\n                       RankedTensorType result_type, Value src,\n-                      ValueRange offsets, ValueRange strides,\n-                      ArrayRef<int64_t> shape, ArrayRef<int64_t> layout) {\n-  build(b, result, result_type, src, getAsOpFoldResult(offsets),\n-        getAsOpFoldResult(strides), shape, layout);\n+                      ValueRange offsets, ArrayRef<int64_t> sizes,\n+                      ArrayRef<int64_t> strides, ArrayRef<int64_t> shape,\n+                      ArrayRef<int64_t> layout) {\n+  build(b, result, result_type, src, getAsOpFoldResult(offsets), sizes, strides,\n+        shape, layout);\n }\n \n class ExtractOpOffsetsSizesStridesFolder final\n@@ -154,18 +177,15 @@ class ExtractOpOffsetsSizesStridesFolder final\n   LogicalResult matchAndRewrite(ExtractOp op,\n                                 PatternRewriter &rewriter) const override {\n     SmallVector<OpFoldResult> mixed_offsets(op.getMixedOffsets());\n-    SmallVector<OpFoldResult> mixed_strides(op.getMixedStrides());\n-\n-    // No constant operands were folded, just return;\n-    if (failed(foldDynamicIndexList(mixed_offsets, /*onlyNonNegative=*/true)) &&\n-        failed(foldDynamicIndexList(mixed_strides))) {\n+    if (failed(foldDynamicIndexList(mixed_offsets, /*onlyNonNegative=*/true))) {\n+      // No constant operands were folded, just return;\n       return failure();\n     }\n     // Create the new op in canonical form.\n     auto disable_attrs = to_vector(op->getDiscardableAttrs());\n     auto new_op = rewriter.replaceOpWithNewOp<ExtractOp>(\n-        op, op.getType(), op.getSrc(), mixed_offsets, mixed_strides,\n-        op.getSrcShape(), op.getSrcLayout());\n+        op, op.getType(), op.getSrc(), mixed_offsets, op.getStaticSizes(),\n+        op.getStaticStrides(), op.getSrcShape(), op.getSrcLayout());\n     new_op->setDiscardableAttrs(disable_attrs);\n     return success();\n   }\n@@ -181,45 +201,30 @@ void ExtractOp::getCanonicalizationPatterns(RewritePatternSet &results,\n //===----------------------------------------------------------------------===//\n \n LogicalResult InsertOp::verify() {\n-  int64_t rank = getSrc().getType().getRank();\n-  if (rank == 0) {\n-    return emitError(\"cannot insert a 0-d tensor\");\n-  }\n-  if (rank != getDstShape().size()) {\n-    return emitError(\"shape attribute has a wrong size\");\n-  }\n-  if (rank != getDstLayout().size()) {\n-    return emitError(\"layout attribute has a wrong size\");\n-  }\n-  if (getSrc().getType().getElementType() !=\n-      getDst().getType().getPointeeType()) {\n-    return emitError(\"dst pointee type must match src element type\");\n-  }\n-  return verifyShapeMatchesSizes(getOperation(), getSrc().getType().getShape(),\n-                                 getMixedSizes());\n+  return verifyExtractInsert(\n+      getOperation(), getSrc().getType(), getDst().getType(),\n+      getDstLayoutAttr(), getDstShape(), getStaticSizes(), getStaticStrides());\n }\n \n void InsertOp::build(OpBuilder& b, OperationState& result, Value src, Value dst,\n-                     ArrayRef<OpFoldResult> offsets,\n-                     ArrayRef<OpFoldResult> strides, ArrayRef<int64_t> shape,\n-                     ArrayRef<int64_t> layout) {\n-  RankedTensorType src_type = mlir::cast<RankedTensorType>(src.getType());\n-  SmallVector<int64_t> static_offsets, static_strides;\n-  SmallVector<Value> dynamic_offsets, dynamic_strides;\n+                     ArrayRef<OpFoldResult> offsets, ArrayRef<int64_t> sizes,\n+                     ArrayRef<int64_t> strides, ArrayRef<int64_t> dst_shape,\n+                     ArrayRef<int64_t> dst_layout) {\n+  SmallVector<int64_t> static_offsets;\n+  SmallVector<Value> dynamic_offsets;\n   dispatchIndexOpFoldResults(offsets, dynamic_offsets, static_offsets);\n-  dispatchIndexOpFoldResults(strides, dynamic_strides, static_strides);\n-  build(b, result, {}, src, dst, dynamic_offsets, {}, dynamic_strides,\n-        b.getDenseI64ArrayAttr(static_offsets),\n-        b.getDenseI64ArrayAttr(src_type.getShape()),\n-        b.getDenseI64ArrayAttr(static_strides), b.getDenseI64ArrayAttr(shape),\n-        b.getDenseI64ArrayAttr(layout));\n+  build(b, result, /*resultTypes=*/{}, src, dst, dynamic_offsets, /*sizes=*/{},\n+        /*strides=*/{}, b.getDenseI64ArrayAttr(static_offsets),\n+        b.getDenseI64ArrayAttr(sizes), b.getDenseI64ArrayAttr(strides),\n+        b.getDenseI64ArrayAttr(dst_shape), b.getDenseI64ArrayAttr(dst_layout));\n }\n \n void InsertOp::build(OpBuilder& b, OperationState& result, Value src, Value dst,\n-                     ValueRange offsets, ValueRange strides,\n-                     ArrayRef<int64_t> shape, ArrayRef<int64_t> layout) {\n-  build(b, result, src, dst, getAsOpFoldResult(offsets),\n-        getAsOpFoldResult(strides), shape, layout);\n+                     ValueRange offsets, ArrayRef<int64_t> sizes,\n+                     ArrayRef<int64_t> strides, ArrayRef<int64_t> shape,\n+                     ArrayRef<int64_t> layout) {\n+  build(b, result, src, dst, getAsOpFoldResult(offsets), sizes, strides, shape,\n+        layout);\n }\n \n class InsertOpOffsetsSizesStridesFolder final\n@@ -230,17 +235,15 @@ class InsertOpOffsetsSizesStridesFolder final\n   LogicalResult matchAndRewrite(InsertOp op,\n                                 PatternRewriter &rewriter) const override {\n     SmallVector<OpFoldResult> mixed_offsets(op.getMixedOffsets());\n-    SmallVector<OpFoldResult> mixed_strides(op.getMixedStrides());\n     // No constant operands were folded, just return;\n-    if (failed(foldDynamicIndexList(mixed_offsets, /*onlyNonNegative=*/true)) &&\n-        failed(foldDynamicIndexList(mixed_strides))) {\n+    if (failed(foldDynamicIndexList(mixed_offsets, /*onlyNonNegative=*/true))) {\n       return failure();\n     }\n     // Create the new op in canonical form.\n     auto disable_attrs = to_vector(op->getDiscardableAttrs());\n     auto new_op = rewriter.replaceOpWithNewOp<InsertOp>(\n-        op, op.getSrc(), op.getDst(), mixed_offsets, mixed_strides,\n-        op.getDstShape(), op.getDstLayout());\n+        op, op.getSrc(), op.getDst(), mixed_offsets, op.getStaticSizes(),\n+        op.getStaticStrides(), op.getDstShape(), op.getDstLayout());\n     new_op->setDiscardableAttrs(disable_attrs);\n     return success();\n   }"
        },
        {
            "sha": "095135031706632786b4a70f16c14365f174b923",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.td",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -84,13 +84,13 @@ def TTXLA_ExtractOp : TTXLA_OpWithOffsetSizesAndStrides<\"extract\", [\n   }];\n   let builders = [\n     OpBuilder<(ins \"RankedTensorType\":$result_type, \"Value\":$src,\n-      \"ValueRange\":$offsets,  \"ValueRange\":$strides,\n-      CArg<\"ArrayRef<int64_t>\">:$src_shape,\n-      CArg<\"ArrayRef<int64_t>\">:$src_layout)>,\n+      \"ValueRange\":$offsets, \"ArrayRef<int64_t>\":$sizes,\n+      \"ArrayRef<int64_t>\":$strides, \"ArrayRef<int64_t>\":$src_shape,\n+      \"ArrayRef<int64_t>\":$src_layout)>,\n     OpBuilder<(ins \"RankedTensorType\":$result_type, \"Value\":$src,\n-      \"ArrayRef<OpFoldResult>\":$offsets,  \"ArrayRef<OpFoldResult>\":$strides,\n-      CArg<\"ArrayRef<int64_t>\">:$src_shape,\n-      CArg<\"ArrayRef<int64_t>\">:$src_layout)>\n+      \"ArrayRef<OpFoldResult>\":$offsets, \"ArrayRef<int64_t>\":$sizes,\n+      \"ArrayRef<int64_t>\":$strides, \"ArrayRef<int64_t>\":$src_shape,\n+      \"ArrayRef<int64_t>\":$src_layout)>\n   ];\n \n   let arguments = (ins\n@@ -117,7 +117,7 @@ def TTXLA_ExtractOp : TTXLA_OpWithOffsetSizesAndStrides<\"extract\", [\n     /// Return the expected rank of each of the `static_offsets`, `static_sizes`\n     /// and `static_strides` attributes.\n     std::array<unsigned, 3> getArrayAttrMaxRanks() {\n-      unsigned rank = getType().getRank();\n+      unsigned rank = getSrcShape().size();\n       return {rank, rank, rank};\n     }\n     /// Return the number of leading operands before the `offsets`, `sizes` and\n@@ -145,13 +145,13 @@ def TTXLA_InsertOp : TTXLA_OpWithOffsetSizesAndStrides<\"insert\"> {\n   }];\n   let builders = [\n     OpBuilder<(ins \"Value\":$src, \"Value\":$dst,\n-      \"ValueRange\":$offsets,  \"ValueRange\":$strides,\n-      CArg<\"ArrayRef<int64_t>\">:$dst_shape,\n-      CArg<\"ArrayRef<int64_t>\">:$dst_layout)>,\n+      \"ValueRange\":$offsets, \"ArrayRef<int64_t>\":$sizes,\n+      \"ArrayRef<int64_t>\":$strides, \"ArrayRef<int64_t>\":$dst_shape,\n+      \"ArrayRef<int64_t>\":$dst_layout)>,\n     OpBuilder<(ins \"Value\":$src, \"Value\":$dst,\n-      \"ArrayRef<OpFoldResult>\":$offsets,  \"ArrayRef<OpFoldResult>\":$strides,\n-      CArg<\"ArrayRef<int64_t>\">:$dst_shape,\n-      CArg<\"ArrayRef<int64_t>\">:$dst_layout)>\n+      \"ArrayRef<OpFoldResult>\":$offsets, \"ArrayRef<int64_t>\":$sizes,\n+      \"ArrayRef<int64_t>\":$strides, \"ArrayRef<int64_t>\":$dst_shape,\n+      \"ArrayRef<int64_t>\":$dst_layout)>\n   ];\n \n   let arguments = (ins\n@@ -180,7 +180,7 @@ def TTXLA_InsertOp : TTXLA_OpWithOffsetSizesAndStrides<\"insert\"> {\n     /// Return the expected rank of each of the `static_offsets`, `static_sizes`\n     /// and `static_strides` attributes.\n     std::array<unsigned, 3> getArrayAttrMaxRanks() {\n-      unsigned rank = getSrc().getType().getRank();\n+      unsigned rank = getDstShape().size();\n       return {rank, rank, rank};\n     }\n     /// Return the number of leading operands before the `offsets`, `sizes` and"
        },
        {
            "sha": "e99e570c4a702ef8184d0430edb130a071fbaece",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -64,6 +64,7 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:Analysis\",\n         \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:DialectUtils\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FunctionInterfaces\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "de677c3c804dafd02914fc81234e708478010664",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/int4_passes.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 29,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"llvm/ADT/DenseSet.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -39,7 +40,6 @@ limitations under the License.\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n-#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n #include \"mlir/IR/Location.h\"\n #include \"mlir/IR/Operation.h\"\n #include \"mlir/IR/OperationSupport.h\"\n@@ -217,30 +217,45 @@ class TritonXlaExtractOpConversionPattern\n         getTypeConverter()->convertType(op.getType()));\n \n     ImplicitLocOpBuilder builder(op.getLoc(), r);\n-    // We can safely assume these are static because they were checked in\n-    // GetPackedDimension.\n-    SmallVector<int64_t, 2> tile_strides(adaptor.getStaticStrides());\n \n-    // The stride of the i8 tensor is half of the i4 tensor but at least 1.\n-    SmallVector<Value, 2> tile_strides_values;\n-    for (auto stride : tile_strides) {\n-      tile_strides_values.push_back(builder.create<ma::ConstantOp>(\n-          builder.getIndexAttr(ceil(stride / 2.0))));\n+    std::optional<llvm::SmallDenseSet<unsigned>> optional_mask =\n+        computeRankReductionMask(op.getStaticSizes(), op.getType().getShape());\n+    if (!optional_mask) {\n+      return r.notifyMatchFailure(op, \"Unsupported rank reduction.\");\n+    }\n+    // Convert the packed dimension to the rank-expanded src type.\n+    int packed_dimension = converter_.packed_dimension();\n+    for (auto dim : *optional_mask) {\n+      if (dim > packed_dimension) {\n+        break;\n+      }\n+      ++packed_dimension;\n     }\n \n-    // We update the offset of the packed dimension to be half of the original\n-    // offset.\n-    SmallVector<Value, 2> tile_offsets_values = op.getOffsetsAsValues(builder);\n-    tile_offsets_values[converter_.packed_dimension()] =\n-        div(r, tile_offsets_values[converter_.packed_dimension()], 2);\n+    // We update values of the packed dimension to be half of the original.\n+    SmallVector<Value> offsets = op.getOffsetsAsValues(builder);\n+    offsets[packed_dimension] = div(r, offsets[packed_dimension], 2);\n+\n+    // We checked in GetPackedDimension that the sizes are static and\n+    // the packed dimension is even.\n+    SmallVector<int64_t> sizes(op.getStaticSizes());\n+    sizes[packed_dimension] = sizes[packed_dimension] / 2;\n+\n+    // We checked in GetPackedDimension that the strides are static and\n+    // the packed dimension is one.\n+    SmallVector<int64_t> strides(op.getStaticStrides());\n \n-    SmallVector<int64_t> shape = llvm::to_vector(adaptor.getSrcShape());\n-    shape[converter_.packed_dimension()] =\n-        (shape[converter_.packed_dimension()] + 1) / 2;\n+    SmallVector<int64_t> src_shape(adaptor.getSrcShape());\n+    src_shape[packed_dimension] = (src_shape[packed_dimension] + 1) / 2;\n \n-    r.replaceOpWithNewOp<mtx::ExtractOp>(\n-        op, new_result_type, adaptor.getSrc(), tile_offsets_values,\n-        tile_strides_values, shape, adaptor.getSrcLayout());\n+    // Note: above, we assume that offsets are even, which we check only if it's\n+    // static. We also assume that the residual size is even, which we don't\n+    // check at all. TODO(csigg): see IsOffsetDivisibilityGuaranteed() for how\n+    // we could cover more cases. For the others, maybe emit a cf.assert.\n+\n+    r.replaceOpWithNewOp<mtx::ExtractOp>(op, new_result_type, adaptor.getSrc(),\n+                                         offsets, sizes, op.getStaticStrides(),\n+                                         src_shape, adaptor.getSrcLayout());\n     return success();\n   }\n \n@@ -614,22 +629,40 @@ absl::StatusOr<int> GetPackedDimension(MLIRContext *ctx,\n \n     if (extract_op) {\n       // Make sure the packed dimension is not dynamic and has a stride of 1.\n-      auto tile_strides = extract_op.getStaticStrides();\n-      auto tile_sizes = extract_op.getStaticSizes();\n-      auto original_shape = extract_op.getSrcShape();\n+      auto offsets = extract_op.getStaticOffsets();\n+      auto sizes = extract_op.getStaticSizes();\n+      auto strides = extract_op.getStaticStrides();\n \n-      if (mlir::ShapedType::isDynamicShape(tile_strides) ||\n-          mlir::ShapedType::isDynamicShape(tile_sizes) ||\n-          mlir::ShapedType::isDynamicShape(original_shape)) {\n+      if (ShapedType::isDynamicShape(strides) ||\n+          ShapedType::isDynamicShape(sizes)) {\n         return absl::InvalidArgumentError(\n             \"dynamic shapes, tile strides, and tile sizes not supported\");\n       }\n \n       for (auto dim : extract_op.getSrcLayout()) {\n-        if (tile_strides[dim] == 1 && tile_sizes[dim] > 1 &&\n-            original_shape[dim] > 1) {\n-          return dim;\n+        if (extract_op.getSrcShape()[dim] == 1) {\n+          continue;\n+        }\n+        if (strides[dim] != 1) {\n+          return absl::InvalidArgumentError(\n+              \"Minor-most non-unit dimension has non-unit stride.\");\n+        }\n+        if (sizes[dim] % 2 != 0) {\n+          return absl::InvalidArgumentError(\n+              \"Minor-most non-unit dimension has odd size.\");\n+        }\n+        if (!ShapedType::isDynamic(offsets[dim]) && offsets[dim] % 2 != 0) {\n+          return absl::InvalidArgumentError(\n+              \"Minor-most non-unit dimension has odd offset.\");\n+        }\n+        std::optional<llvm::SmallDenseSet<unsigned>> optional_mask =\n+            computeRankReductionMask(sizes, extract_op.getType().getShape());\n+        if (!optional_mask) {\n+          return absl::InvalidArgumentError(\"Unsupported rank reduction.\");\n         }\n+        auto mask = llvm::to_vector(*optional_mask);\n+        // Convert the packed dimension to the rank-reduced dst type.\n+        return dim - (absl::c_upper_bound(mask, dim) - mask.begin());\n       }\n \n       return absl::InvalidArgumentError(\"Failed to find a packed dimension.\");"
        },
        {
            "sha": "44cf1a3039a2b33994b909f0d6cced4e87252b80",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -38,7 +38,8 @@ def TritonXLASqueezeDimsPass : Pass<\"triton-xla-squeeze-dims\", \"mlir::ModuleOp\">\n     This pass tries to remove size-1 dimensions from tensors.\n   }];\n   let dependentDialects = [\n-    \"::mlir::triton::xla::XlaTritonDialect\"\n+    \"::mlir::triton::xla::XlaTritonDialect\",\n+    \"triton::TritonDialect\"\n   ];\n   let options = [\n     Option<\"finalize_\", \"finalize\", \"bool\", \"true\","
        },
        {
            "sha": "153d844599b98fcbfdf96f9952b12073a6d2bceb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/int4_packed_dim.mlir",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -116,12 +116,12 @@ tt.func @major_3d(%arg0: !tt.ptr<i4>) -> (tensor<8x8x8xi8>) {\n // CHECK-LABEL: @triton_xla_extract_2d\n func.func @triton_xla_extract_2d(%arg0: !tt.ptr<i4>) -> (tensor<16x16xi8>) {\n   // CHECK: %[[EXTRACT:.*]] = triton_xla.extract from %arg0\n-  // CHECK-SAME: as memref<128x64xi8, #triton_xla.layout<[1, 0]>>\n-  // CHECK-SAME: [0, 0] [16, 8] [1, 1] : tensor<16x8xi8>\n+  // CHECK-SAME: as memref<128x8x64xi8, #triton_xla.layout<[2, 1, 0]>>\n+  // CHECK-SAME: [0, 0, 0] [16, 1, 8] [1, 1, 1] : tensor<16x8xi8>\n   %c0 = arith.constant 0 : index\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<128x128xi4, #triton_xla.layout<[1, 0]>>\n-      [0, %c0] [16, 16] [1, 1] : tensor<16x16xi4>\n+      as memref<128x8x128xi4, #triton_xla.layout<[2, 1, 0]>>\n+      [0, 0, %c0] [16, 1, 16] [1, 1, 1] : tensor<16x16xi4>\n   %ext = arith.extsi %extracted_tensor : tensor<16x16xi4> to tensor<16x16xi8>\n   // CHECK: %[[SHLI:.*]] = arith.shli %[[EXTRACT]]\n   // CHECK: %[[SHRI_LO:.*]] = arith.shrsi %[[SHLI]]"
        },
        {
            "sha": "fc20b19831ea5b02330ea44389a80bc5f87da72e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_extract_insert_to_triton.mlir",
            "status": "modified",
            "additions": 47,
            "deletions": 83,
            "changes": 130,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -8,30 +8,27 @@\n \n func.func @lower_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n-      [0, 0] [16, 64] [1, 1] : tensor<16x64xbf16>\n+      as memref<512x8x128xbf16, #triton_xla.layout<[2, 1, 0]>>\n+      [0, 3, 0] [16, 1, 64] [1, 1, 1] : tensor<16x64xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<256x256xbf16, #triton_xla.layout<[1, 0]>>\n-      [0, 0] [16, 64] [1, 1] : tensor<16x64xbf16>\n+      as memref<256x16x256xbf16, #triton_xla.layout<[2, 1, 0]>>\n+      [0, 5, 0] [16, 1, 64] [1, 1, 1] : tensor<16x64xbf16>\n   func.return\n }\n \n-// CHECK-LABEL: tt.func @lower_extract_insert\n-// CHECK-SAME:  %[[ARG_0:.*]]: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %[[ARG_1:.*]]: !tt.ptr<bf16> {tt.divisibility = 16 : i32}\n-// CHECK:         %[[ADDPTR_0:.*]] = tt.addptr %[[ARG_0]]\n-// CHECK:         %[[PTR_0:.*]] = tt.make_tensor_ptr %[[ADDPTR_0]]\n-// CHECK:         %[[LOAD:.*]] = tt.load %[[PTR_0]]\n-// CHECK:         %[[ADDPTR_1:.*]] = tt.addptr %[[ARG_1]]\n-// CHECK:         %[[PTR_1:.*]] = tt.make_tensor_ptr %[[ADDPTR_1]]\n-// CHECK:         tt.store %[[PTR_1]], %[[LOAD]]\n-// CHECK:       tt.return\n+// CHECK-LABEL: tt.func @lower_extract_insert(\n+// CHECK-SAME:      %arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32},\n+// CHECK-SAME:      %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) {\n+// CHECK:         %[[LOAD:.*]] = tt.load\n+// CHECK:         tt.store {{.*}}, %[[LOAD]]\n+// CHECK:         tt.return\n \n // CHECK-TMA-LABEL: tt.func @lower_extract_insert\n-// CHECK-TMA-SAME:  %[[ARG_0:.*]]: !tt.tensordesc<tensor<16x64xbf16>> {tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [512, 128], tile_shape = [16, 64], tile_strides = [1, 1], layout = [1, 0], element_byte_size = 2>},\n-// CHECK-TMA-SAME:  %[[ARG_1:.*]]: !tt.tensordesc<tensor<16x64xbf16>> {tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [256, 256], tile_shape = [16, 64], tile_strides = [1, 1], layout = [1, 0], element_byte_size = 2>}\n-// CHECK-TMA:    %[[LOAD:.*]] = tt.descriptor_load %[[ARG_0]]\n-// CHECK-TMA:    tt.descriptor_store %[[ARG_1]][{{.*}}], %[[LOAD]]\n-// CHECK-TMA:    tt.return\n+// CHECK-TMA-SAME:      %arg0: !tt.tensordesc<tensor<16x1x64xbf16>>\n+// CHECK-TMA-SAME:      %arg1: !tt.tensordesc<tensor<16x1x64xbf16>>\n+// CHECK-TMA:         %[[LOAD:.*]] = tt.descriptor_load %arg0\n+// CHECK-TMA:         tt.descriptor_store %arg1[{{.*}}],\n+// CHECK-TMA:         tt.return\n \n // -----\n \n@@ -46,10 +43,8 @@ func.func @non_perfect_tile_shape(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n }\n \n // CHECK-LABEL: tt.func @non_perfect_tile_shape\n-// CHECK:        tt.load {{.*}} {\n-// CHECK-SAME:     boundaryCheck = array<i32: 0, 1>, padding = 1 : i32\n-// CHECK:        tt.store {{.*}}, {{.*}} {\n-// CHECK-SAME:     boundaryCheck = array<i32: 0, 1>\n+// CHECK:         %[[LOAD:.*]] = tt.load {{.*}}, %{{.*}}, %{{.*}} :\n+// CHECK:         tt.store {{.*}}, %[[LOAD]], %{{.*}} :\n \n // -----\n \n@@ -64,9 +59,7 @@ func.func @incompatible_tma_global_strides(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<\n }\n \n // CHECK-TMA-LABEL: tt.func @incompatible_tma_global_strides\n-// CHECK-TMA:         tt.make_tensor_ptr\n // CHECK-TMA:         tt.load\n-// CHECK-TMA:         tt.make_tensor_ptr\n // CHECK-TMA:         tt.store\n \n // -----\n@@ -95,10 +88,8 @@ module {\n \n // CHECK-LABEL:   tt.func @slice_with_tiling_that_needs_padding_has_boundary_checks\n // CHECK-COUNT-1: tt.load\n-// CHECK:         tt.store\n-// CHECK-SAME:    boundaryCheck = array<i32: 0>\n-// CHECK:         tt.store\n-// CHECK-SAME:    boundaryCheck = array<i32: 0>\n+// CHECK:         tt.store {{.*}}, %{{.*}}, %{{.*}}\n+// CHECK:         tt.store {{.*}}, %{{.*}}, %{{.*}}\n \n // -----\n \n@@ -126,10 +117,8 @@ module {\n \n // CHECK-LABEL:   tt.func @slice_with_extra_output_that_can_reuse_tile_due_to_padding\n // CHECK-COUNT-1: tt.load\n-// CHECK:         tt.store\n-// CHECK-SAME:    boundaryCheck = array<i32: 0>\n-// CHECK:         tt.store\n-// CHECK-NOT:     boundaryCheck = array<i32: 0>\n+// CHECK:         tt.store {{.*}}, %{{.*}}, %{{.*}}\n+// CHECK:         tt.store {{.*}}, %{{.*}} :\n \n // -----\n \n@@ -145,27 +134,6 @@ func.func @extract_with_non_unit_minor_dim_stride(%arg0: !tt.ptr<bf16>,\n }\n \n // CHECK-LABEL: tt.func @extract_with_non_unit_minor_dim_stride\n-// CHECK-TMA:   tt.make_tensor_ptr\n-// CHECK-TMA:   tt.load\n-// CHECK-TMA:   tt.descriptor_store\n-\n-// -----\n-\n-func.func @extract_with_non_static_strides(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n-  %0 = tt.get_program_id x : i32\n-  %1 = arith.extsi %0 : i32 to i64\n-  %2 = arith.index_cast %1 : i64 to index\n-  %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<1024x1024xbf16, #triton_xla.layout<[1, 0]>>\n-      [0, 0] [16, 64] [%2, 1] : tensor<16x64xbf16>\n-  triton_xla.insert %extracted_tensor into %arg1\n-      as memref<256x256xbf16, #triton_xla.layout<[1, 0]>>\n-      [0, 0] [16, 64] [1, 1] : tensor<16x64xbf16>\n-  func.return\n-}\n-\n-// CHECK-LABEL: tt.func @extract_with_non_static_strides\n-// CHECK-TMA:   tt.make_tensor_ptr\n // CHECK-TMA:   tt.load\n // CHECK-TMA:   tt.descriptor_store\n \n@@ -182,19 +150,18 @@ func.func @lower_extract_insert_1d(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n }\n \n // CHECK-LABEL: tt.func @lower_extract_insert_1d\n-// CHECK-SAME:  %[[ARG_0:.*]]: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %[[ARG_1:.*]]: !tt.ptr<bf16> {tt.divisibility = 16 : i32}\n-// CHECK:         %[[PTR_0:.*]] = tt.make_tensor_ptr %[[ARG_0]]\n-// CHECK:         %[[LOAD:.*]] = tt.load %[[PTR_0]]\n-// CHECK:         %[[PTR_1:.*]] = tt.make_tensor_ptr %[[ARG_1]]\n-// CHECK:         tt.store %[[PTR_1]], %[[LOAD]]\n-// CHECK:       tt.return\n+// CHECK-SAME:      %arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32},\n+// CHECK-SAME:      %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}\n+// CHECK:         %[[LOAD:.*]] = tt.load\n+// CHECK:         tt.store {{.*}}, %[[LOAD]]\n+// CHECK:         tt.return\n \n // CHECK-TMA-LABEL: tt.func @lower_extract_insert_1d\n-// CHECK-TMA-SAME:  %[[ARG_0:.*]]: !tt.tensordesc<tensor<16xbf16>> {tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [128], tile_shape = [16], tile_strides = [1], layout = [0], element_byte_size = 2>},\n-// CHECK-TMA-SAME:  %[[ARG_1:.*]]: !tt.tensordesc<tensor<16xbf16>> {tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [256], tile_shape = [16], tile_strides = [1], layout = [0], element_byte_size = 2>}\n-// CHECK-TMA:    %[[LOAD:.*]] = tt.descriptor_load %[[ARG_0]]\n-// CHECK-TMA:    tt.descriptor_store %[[ARG_1]][{{.*}}], %[[LOAD]]\n-// CHECK-TMA:    tt.return\n+// CHECK-TMA-SAME:      %arg0: !tt.tensordesc<tensor<16xbf16>>\n+// CHECK-TMA-SAME:      %arg1: !tt.tensordesc<tensor<16xbf16>>\n+// CHECK-TMA:         %[[LOAD:.*]] = tt.descriptor_load %arg0\n+// CHECK-TMA:         tt.descriptor_store %arg1[{{.*}}], %[[LOAD]]\n+// CHECK-TMA:         tt.return\n \n // -----\n \n@@ -209,21 +176,18 @@ func.func @lower_extract_insert_5d(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n }\n \n // CHECK-LABEL: tt.func @lower_extract_insert_5d\n-// CHECK-SAME:  %[[ARG_0:.*]]: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %[[ARG_1:.*]]: !tt.ptr<bf16> {tt.divisibility = 16 : i32}\n-// CHECK:         %[[ADDPTR_0:.*]] = tt.addptr %[[ARG_0]]\n-// CHECK:         %[[PTR_0:.*]] = tt.make_tensor_ptr %[[ADDPTR_0]]\n-// CHECK:         %[[LOAD:.*]] = tt.load %[[PTR_0]]\n-// CHECK:         %[[ADDPTR_1:.*]] = tt.addptr %[[ARG_1]]\n-// CHECK:         %[[PTR_1:.*]] = tt.make_tensor_ptr %[[ADDPTR_1]]\n-// CHECK:         tt.store %[[PTR_1]], %[[LOAD]]\n-// CHECK:       tt.return\n+// CHECK-SAME:      %arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32},\n+// CHECK-SAME:      %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}\n+// CHECK:         %[[LOAD:.*]] = tt.load\n+// CHECK:         tt.store {{.*}}, %[[LOAD]]\n+// CHECK:         tt.return\n \n // CHECK-TMA-LABEL: tt.func @lower_extract_insert_5d\n-// CHECK-TMA-SAME:  %[[ARG_0:.*]]: !tt.tensordesc<tensor<8x8x8x8x8xbf16>> {tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [16, 16, 16, 16, 16], tile_shape = [8, 8, 8, 8, 8], tile_strides = [1, 1, 1, 1, 1], layout = [4, 3, 2, 1, 0], element_byte_size = 2>},\n-// CHECK-TMA-SAME:  %[[ARG_1:.*]]: !tt.tensordesc<tensor<8x8x8x8x8xbf16>> {tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [32, 32, 32, 32, 32], tile_shape = [8, 8, 8, 8, 8], tile_strides = [1, 1, 1, 1, 1], layout = [4, 3, 2, 1, 0], element_byte_size = 2>}\n-// CHECK-TMA:    %[[LOAD:.*]] = tt.descriptor_load %[[ARG_0]]\n-// CHECK-TMA:    tt.descriptor_store %[[ARG_1]][{{.*}}], %[[LOAD]]\n-// CHECK-TMA:    tt.return\n+// CHECK-TMA-SAME:      %arg0: !tt.tensordesc<tensor<8x8x8x8x8xbf16>>\n+// CHECK-TMA-SAME:      %arg1: !tt.tensordesc<tensor<8x8x8x8x8xbf16>>\n+// CHECK-TMA:         %[[LOAD:.*]] = tt.descriptor_load %arg0\n+// CHECK-TMA:         tt.descriptor_store %arg1[{{.*}}], %[[LOAD]]\n+// CHECK-TMA:         tt.return\n \n // -----\n \n@@ -238,7 +202,8 @@ func.func @extract_insert_with_zero_stride(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<\n }\n \n // CHECK-TMA-LABEL: tt.func @extract_insert_with_zero_stride\n-// CHECK-TMA-SAME:  %[[ARG_0:.*]]: !tt.tensordesc{{.*}} tile_strides = [1, 1], {{.*}} %[[ARG_1:.*]]: !tt.tensordesc{{.*}} tile_strides = [1, 1]\n+// CHECK-TMA-SAME:      %arg0: !tt.tensordesc<tensor<1x64xbf16>>\n+// CHECK-TMA-SAME:      %arg1: !tt.tensordesc<tensor<1x64xbf16>>\n \n // -----\n \n@@ -254,9 +219,8 @@ func.func @incompatible_tma_const_offset_not_divisible_by_16_bytes(\n }\n \n // CHECK-TMA-LABEL: tt.func @incompatible_tma_const_offset_not_divisible_by_16_bytes\n-// CHECK-TMA:   tt.make_tensor_ptr\n-// CHECK-TMA:   tt.load\n-// CHECK-TMA:   tt.descriptor_store\n+// CHECK-TMA:         tt.load\n+// CHECK-TMA:         tt.descriptor_store\n \n // -----\n \n@@ -284,5 +248,5 @@ module {\n }\n \n // CHECK-TMA-LABEL: tt.func @incompatible_tma_dynamic_offset_not_divisible_by_16_bytes\n-// CHECK-TMA:   tt.make_tensor_ptr\n-// CHECK-TMA:   tt.load\n+// CHECK-TMA:         tt.load\n+// CHECK-TMA:         tt.descriptor_store"
        },
        {
            "sha": "9dada12395378e9cb59232a0f31e16630a167124",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_fold_transpose.mlir",
            "status": "modified",
            "additions": 15,
            "deletions": 56,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -1,71 +1,30 @@\n // RUN: xla-opt %s --triton-xla-fold-transpose | FileCheck %s\n \n-// CHECK-LABEL: func @fold_transpose_of_load_ptr\n-tt.func @fold_transpose_of_load_ptr(%arg0: !tt.ptr<f32>, %arg1: i32) -> tensor<8x4xf32> {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %c4_i64 = arith.constant 4 : i64\n-  %c8_i64 = arith.constant 8 : i64\n-  // CHECK:      %[[PTR:.*]] = tt.make_tensor_ptr %arg0,\n-  // CHECK-SAME:     [%c8_i64, %c4_i64], [%c1_i64, %c8_i64], [%c0_i32, %c0_i32]\n-  // CHECK-SAME:     {order = array<i32: 1, 0>} : <tensor<8x4xf32>>\n-  %0 = tt.make_tensor_ptr %arg0, [%c4_i64, %c8_i64], [%c8_i64, %c1_i64], [%c0_i32, %c0_i32] {order = array<i32: 1, 0>} : <tensor<4x8xf32>>\n-  // CHECK:      %[[LOAD:.*]] = tt.load %[[PTR]]\n-  // CHECK-SAME:     {boundaryCheck = array<i32: 0>, padding = 1 : i32} : !tt.ptr<tensor<8x4xf32>>\n-  %1 = tt.load %0 {boundaryCheck = array<i32: 1>, padding = 1 : i32} : !tt.ptr<tensor<4x8xf32>>\n-  // CHECK-NOT:  tt.trans\n-  %2 = tt.trans %1 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n-  tt.return %2 : tensor<8x4xf32>\n-}\n-\n-// CHECK-LABEL: func @fold_transpose_of_load_ptr_with_mask\n-tt.func @fold_transpose_of_load_ptr_with_mask(%arg0: !tt.ptr<f32>, %arg1: tensor<4x8xi1>) -> tensor<8x4xf32> {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %c4_i64 = arith.constant 4 : i64\n-  %c8_i64 = arith.constant 8 : i64\n-  %0 = tt.make_tensor_ptr %arg0, [%c4_i64, %c8_i64], [%c8_i64, %c1_i64], [%c0_i32, %c0_i32] {order = array<i32: 2, 1, 0>} : <tensor<4x8xf32>>\n-  // CHECK: tt.load {{.*}}, %arg1 : !tt.ptr<tensor<4x8xf32>>\n-  %1 = tt.load %0, %arg1 : !tt.ptr<tensor<4x8xf32>>\n-  // CHECK: tt.trans\n-  %2 = tt.trans %1 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n-  tt.return %2 : tensor<8x4xf32>\n+// CHECK-LABEL: func @fold_transpose_of_extract\n+func.func @fold_transpose_of_extract(%arg0: !tt.ptr<f32>, %arg1: i32) -> tensor<8x4xf32> {\n+  // CHECK: %[[EXTRACT:.*]] = triton_xla.extract from %arg0\n+  // CHECK-SAME: as memref<16x8x4xf32, #triton_xla.layout<[0, 2, 1]>>\n+  // CHECK-SAME: [0, 0, 0] [8, 1, 4] [1, 1, 1] : tensor<8x4xf32>\n+  %0 = triton_xla.extract from %arg0\n+    as memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>>\n+    [0, 0, 0] [4, 1, 8] [1, 1, 1] : tensor<4x8xf32>\n+  %1 = tt.trans %0 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n+  // CHECK: return %[[EXTRACT]] : tensor<8x4xf32>\n+  return %1 : tensor<8x4xf32>\n }\n \n // CHECK-LABEL: func @push_transpose_up_through_elementwise\n-tt.func @push_transpose_up_through_elementwise(%arg0: tensor<4x8xf32>) -> tensor<8x4xf32> {\n+func.func @push_transpose_up_through_elementwise(%arg0: tensor<4x8xf32>) -> tensor<8x4xf32> {\n   // CHECK: arith.negf {{.*}} : tensor<8x4xf32>\n   %0 = arith.negf %arg0 : tensor<4x8xf32>\n   %1 = tt.trans %0 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n-  tt.return %1 : tensor<8x4xf32>\n+  return %1 : tensor<8x4xf32>\n }\n \n // CHECK-LABEL: func @push_transpose_up_through_reshape\n-tt.func @push_transpose_up_through_reshape(%arg0: tensor<4x8x2xf32>) -> tensor<16x4xf32> {\n+func.func @push_transpose_up_through_reshape(%arg0: tensor<4x8x2xf32>) -> tensor<16x4xf32> {\n   // CHECK: tt.reshape {{.*}} : tensor<8x2x4xf32> -> tensor<16x4xf32>\n   %0 = tt.reshape %arg0 : tensor<4x8x2xf32> -> tensor<4x16xf32>\n   %1 = tt.trans %0 {order = array<i32: 1, 0>} : tensor<4x16xf32> -> tensor<16x4xf32>\n-  tt.return %1 : tensor<16x4xf32>\n-}\n-\n-// CHECK-LABEL: func @push_transpose_up_through_join_of_inline_asm\n-tt.func @push_transpose_up_through_join_of_inline_asm(%arg0: tensor<4x8xf32>) -> tensor<8x4x2xf32> {\n-  // CHECK: tt.elementwise_inline_asm {{.*}} : tensor<8x4xf32> -> tensor<8x4xf32>, tensor<8x4xf32>\n-  %0:2 = tt.elementwise_inline_asm \"\" {constraints = \"\", packed_element = 1 : i32, pure = true} %arg0 : tensor<4x8xf32> -> tensor<4x8xf32>, tensor<4x8xf32>\n-  // CHECK: tt.join {{.*}} : tensor<8x4xf32> -> tensor<8x4x2xf32>\n-  %1 = tt.join %0#0, %0#1 : tensor<4x8xf32> -> tensor<4x8x2xf32>\n-  %2 = tt.trans %1 {order = array<i32: 1, 0, 2>} : tensor<4x8x2xf32> -> tensor<8x4x2xf32>\n-  tt.return %2 : tensor<8x4x2xf32>\n-}\n-\n-// CHECK-LABEL: func @push_transpose_up_through_int4_unpack\n-tt.func @push_transpose_up_through_int4_unpack(%arg0: tensor<8x4xi8>) -> tensor<4x16xbf16> {\n-  // CHECK: tt.trans {{.*}} : tensor<8x4xi8> -> tensor<4x8xi8>\n-  // CHECK-NOT: tt.trans\n-  %0:2 = tt.elementwise_inline_asm \"\" {constraints = \"\", packed_element = 1 : i32, pure = true} %arg0 : tensor<8x4xi8> -> tensor<8x4xbf16>, tensor<8x4xbf16>\n-  %1 = tt.join %0#0, %0#1 : tensor<8x4xbf16> -> tensor<8x4x2xbf16>\n-  %2 = tt.trans %1 {order = array<i32: 0, 2, 1>} : tensor<8x4x2xbf16> -> tensor<8x2x4xbf16>\n-  %3 = tt.reshape %2 : tensor<8x2x4xbf16> -> tensor<16x4xbf16>\n-  %4 = tt.trans %3 {order = array<i32: 1, 0>} : tensor<16x4xbf16> -> tensor<4x16xbf16>\n-  tt.return %4 : tensor<4x16xbf16>\n+  return %1 : tensor<16x4xf32>\n }"
        },
        {
            "sha": "6359143e9f7e6f7d5d1c050f3f306705120f1327",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_squeeze_dims.mlir",
            "status": "modified",
            "additions": 74,
            "deletions": 135,
            "changes": 209,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -5,57 +5,57 @@\n // RUN: | FileCheck %s --check-prefix=FINALIZE\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_elementwise\n-tt.func @push_squeeze_dims_up_through_elementwise(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n+func.func @push_squeeze_dims_up_through_elementwise(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   // CHECK: arith.negf {{.*}} : tensor<4x8xf32>\n   %0 = arith.negf %arg0 : tensor<4x1x8xf32>\n   %1 = triton_xla.squeeze_dims %0 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %1 : tensor<4x8xf32>\n+  return %1 : tensor<4x8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_multiple_results\n-tt.func @push_squeeze_dims_up_through_multiple_results(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n+func.func @push_squeeze_dims_up_through_multiple_results(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   // CHECK: tt.elementwise_inline_asm {{.*}} : tensor<4x8xf32> -> tensor<4x8xf32>, tensor<4x8xf32>\n   %0:2 = tt.elementwise_inline_asm \"\" {constraints = \"\", packed_element = 1 : i32, pure = true} %arg0 : tensor<4x1x8xf32> -> tensor<4x1x8xf32>, tensor<4x1x8xf32>\n   %1 = triton_xla.squeeze_dims %0#0 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %1 : tensor<4x8xf32>\n+  return %1 : tensor<4x8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_broadcast\n-tt.func @push_squeeze_dims_up_through_broadcast(%arg0: tensor<1x4x1x8xf32>) -> tensor<4x16x8xf32> {\n+func.func @push_squeeze_dims_up_through_broadcast(%arg0: tensor<1x4x1x8xf32>) -> tensor<4x16x8xf32> {\n   // CHECK: tt.broadcast {{.*}} : tensor<4x1x8xf32> -> tensor<4x16x8xf32>\n   %0 = tt.broadcast %arg0 : tensor<1x4x1x8xf32> -> tensor<1x4x16x8xf32>\n   %1 = triton_xla.squeeze_dims %0 {axis = 0 : i32} : tensor<1x4x16x8xf32> -> tensor<4x16x8xf32>\n-  tt.return %1 : tensor<4x16x8xf32>\n+  return %1 : tensor<4x16x8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_trans\n-tt.func @push_squeeze_dims_up_through_trans(%arg0: tensor<4x1x8xf32>) -> tensor<8x4xf32> {\n+func.func @push_squeeze_dims_up_through_trans(%arg0: tensor<4x1x8xf32>) -> tensor<8x4xf32> {\n   // CHECK: tt.trans {{.*}} {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n   %0 = tt.trans %arg0 {order = array<i32: 2, 0, 1>} : tensor<4x1x8xf32> -> tensor<8x4x1xf32>\n   %1 = triton_xla.squeeze_dims %0 {axis = 2 : i32} : tensor<8x4x1xf32> -> tensor<8x4xf32>\n-  tt.return %1 : tensor<8x4xf32>\n+  return %1 : tensor<8x4xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_join\n-tt.func @push_squeeze_dims_up_through_join(%arg0: tensor<1x4xf32>, %arg1: tensor<1x4xf32>) -> tensor<4x2xf32> {\n+func.func @push_squeeze_dims_up_through_join(%arg0: tensor<1x4xf32>, %arg1: tensor<1x4xf32>) -> tensor<4x2xf32> {\n   // CHECK-DAG: tt.join {{.*}} : tensor<4xf32> -> tensor<4x2xf32>\n   %0 = tt.join %arg0, %arg1 : tensor<1x4xf32> -> tensor<1x4x2xf32>\n   %1 = triton_xla.squeeze_dims %0 {axis = 0 : i32} : tensor<1x4x2xf32> -> tensor<4x2xf32>\n-  tt.return %1 : tensor<4x2xf32>\n+  return %1 : tensor<4x2xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_reduce\n-tt.func @push_squeeze_dims_up_through_reduce(%arg0: tensor<8x4x1xf32>) -> tensor<8xf32> {\n+func.func @push_squeeze_dims_up_through_reduce(%arg0: tensor<8x4x1xf32>) -> tensor<8xf32> {\n   // CHECK: \"tt.reduce\"({{.*}}) <{axis = 1 : i32}> ({\n   %0 = \"tt.reduce\"(%arg0) <{axis = 1 : i32}> ({\n   ^bb0(%arg1: f32, %arg2: f32):\n@@ -64,59 +64,59 @@ tt.func @push_squeeze_dims_up_through_reduce(%arg0: tensor<8x4x1xf32>) -> tensor\n   // CHECK: }) : (tensor<8x4xf32>) -> tensor<8xf32>\n   }) : (tensor<8x4x1xf32>) -> tensor<8x1xf32>\n   %2 = triton_xla.squeeze_dims %0 {axis = 1 : i32} : tensor<8x1xf32> -> tensor<8xf32>\n-  tt.return %2 : tensor<8xf32>\n+  return %2 : tensor<8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @fold_squeeze_of_expand_cancelling\n-tt.func @fold_squeeze_of_expand_cancelling(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {\n+func.func @fold_squeeze_of_expand_cancelling(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {\n   // CHECK-NOT: tt.expand_dims\n   // CHECK-NOT: triton_xla.squeeze_dims\n   %0 = tt.expand_dims %arg0 {axis = 1 : i32} : tensor<4x8xf32> -> tensor<4x1x8xf32>\n   %1 = triton_xla.squeeze_dims %0 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %1 : tensor<4x8xf32>\n+  return %1 : tensor<4x8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @fold_squeeze_of_expand_swapping\n-tt.func @fold_squeeze_of_expand_swapping(%arg0: tensor<4x1x8xf32>) -> tensor<1x4x8xf32> {\n+func.func @fold_squeeze_of_expand_swapping(%arg0: tensor<4x1x8xf32>) -> tensor<1x4x8xf32> {\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n   // CHECK: tt.expand_dims {{.*}} {axis = 0 : i32} : tensor<4x8xf32> -> tensor<1x4x8xf32>\n   %0 = tt.expand_dims %arg0 {axis = 0 : i32} : tensor<4x1x8xf32> -> tensor<1x4x1x8xf32>\n   %1 = triton_xla.squeeze_dims %0 {axis = 2 : i32} : tensor<1x4x1x8xf32> -> tensor<1x4x8xf32>\n-  tt.return %1 : tensor<1x4x8xf32>\n+  return %1 : tensor<1x4x8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @squeeze_reshape\n-tt.func @squeeze_reshape(%arg0: tensor<4x1x1xf32>) -> tensor<4xf32> {\n+func.func @squeeze_reshape(%arg0: tensor<4x1x1xf32>) -> tensor<4xf32> {\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1x1xf32> -> tensor<4x1xf32>\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1xf32> -> tensor<4xf32>\n   %0 = tt.reshape %arg0 : tensor<4x1x1xf32> -> tensor<4xf32>\n-  tt.return %0 : tensor<4xf32>\n+  return %0 : tensor<4xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @expand_reshape\n-tt.func @expand_reshape(%arg0: tensor<4xf32>) -> tensor<4x1x1xf32> {\n+func.func @expand_reshape(%arg0: tensor<4xf32>) -> tensor<4x1x1xf32> {\n   %0 = tt.reshape %arg0 : tensor<4xf32> -> tensor<4x1x1xf32>\n   // CHECK: tt.expand_dims {{.*}} {axis = 1 : i32} : tensor<4xf32> -> tensor<4x1xf32>\n   // CHECK: tt.expand_dims {{.*}} {axis = 1 : i32} : tensor<4x1xf32> -> tensor<4x1x1xf32>\n-  tt.return %0 : tensor<4x1x1xf32>\n+  return %0 : tensor<4x1x1xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @skip_reshape_with_attr\n-tt.func @skip_reshape_with_attr(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n+func.func @skip_reshape_with_attr(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n   // CHECK-NOT: triton_xla.squeeze_dims\n   // CHECK: tt.reshape {{.*}} allow_reorder : tensor<4x1xf32> -> tensor<4xf32>\n   %0 = tt.reshape %arg0 allow_reorder : tensor<4x1xf32> -> tensor<4xf32>\n-  tt.return %0 : tensor<4xf32>\n+  return %0 : tensor<4xf32>\n }\n \n // -----\n@@ -126,140 +126,81 @@ tt.func @skip_reshape_with_attr(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n module attributes {\"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 1 : i32} {\n // CHECK-LABEL: func @reshape_with_encoding\n // CHECK-SAME:    tensor<1x32xf32, #[[ARG_ENC:.+]]>) -> tensor<32xf32, #[[RES_ENC:.+]]>\n-tt.func @reshape_with_encoding(%arg0: tensor<1x32xf32, #arg_enc>) -> tensor<32xf32, #res_enc> {\n+func.func @reshape_with_encoding(%arg0: tensor<1x32xf32, #arg_enc>) -> tensor<32xf32, #res_enc> {\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 0 : i32} :\n   // CHECK-SAME: tensor<1x32xf32, #[[ARG_ENC]]> -> tensor<32xf32, #[[RES_ENC]]>\n   %0 = tt.reshape %arg0 : tensor<1x32xf32, #arg_enc> -> tensor<32xf32, #res_enc>\n-  tt.return %0 : tensor<32xf32, #res_enc>\n+  return %0 : tensor<32xf32, #res_enc>\n }\n }\n \n // -----\n \n-// CHECK-LABEL: func @fold_squeeze_dims_of_load_ptr\n-tt.func @fold_squeeze_dims_of_load_ptr(%arg0: !tt.ptr<f32>, %arg1: i32) -> tensor<4x8xf32> {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c3_i32 = arith.constant 3 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %c4_i64 = arith.constant 4 : i64\n-  %c8_i64 = arith.constant 8 : i64\n-  %c16_i64 = arith.constant 16 : i64\n-  %c128_i64 = arith.constant 128 : i64\n-  // CHECK: %[[ADDPTR:.*]] = tt.addptr %arg0, %c24_i64\n-  // CHECK: tt.make_tensor_ptr %[[ADDPTR]], {{.*}} {order = array<i32: 1, 0>} : <tensor<4x8xf32>>\n-  %0 = tt.make_tensor_ptr %arg0, [%c4_i64, %c16_i64, %c8_i64], [%c128_i64, %c8_i64, %c1_i64], [%c0_i32, %c3_i32, %c0_i32] {order = array<i32: 2, 1, 0>} : <tensor<4x1x8xf32>>\n-  // CHECK: tt.load {{.*}} {boundaryCheck = array<i32: 1>, padding = 1 : i32} : !tt.ptr<tensor<4x8xf32>>\n-  %1 = tt.load %0 {boundaryCheck = array<i32: 2>, padding = 1 : i32} : !tt.ptr<tensor<4x1x8xf32>>\n-  // CHECK-NOT: triton_xla.squeeze_dims\n-  %2 = triton_xla.squeeze_dims %1 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %2 : tensor<4x8xf32>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: func @squeeze_dims_of_load_ptr_with_boundary_check\n-tt.func @squeeze_dims_of_load_ptr_with_boundary_check(%arg0: !tt.ptr<f32>) -> tensor<4x8xf32> {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %c4_i64 = arith.constant 4 : i64\n-  %c8_i64 = arith.constant 8 : i64\n-  %0 = tt.make_tensor_ptr %arg0, [%c4_i64, %c1_i64, %c8_i64], [%c8_i64, %c8_i64, %c1_i64], [%c0_i32, %c0_i32, %c0_i32] {order = array<i32: 2, 1, 0>} : <tensor<4x1x8xf32>>\n-  // CHECK: tt.load {{.*}} {boundaryCheck = array<i32: 1>} : !tt.ptr<tensor<4x1x8xf32>>\n-  %1 = tt.load %0 {boundaryCheck = array<i32: 1>} : !tt.ptr<tensor<4x1x8xf32>>\n-  // CHECK: triton_xla.squeeze_dims\n-  %2 = triton_xla.squeeze_dims %1 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %2 : tensor<4x8xf32>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: func @squeeze_dims_of_load_ptr_with_mask\n-tt.func @squeeze_dims_of_load_ptr_with_mask(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1x8xi1>) -> tensor<4x8xf32> {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %c4_i64 = arith.constant 4 : i64\n-  %c8_i64 = arith.constant 8 : i64\n-  %0 = tt.make_tensor_ptr %arg0, [%c4_i64, %c1_i64, %c8_i64], [%c8_i64, %c8_i64, %c1_i64], [%c0_i32, %c0_i32, %c0_i32] {order = array<i32: 2, 1, 0>} : <tensor<4x1x8xf32>>\n-  // CHECK: tt.load {{.*}}, %arg1 : !tt.ptr<tensor<4x1x8xf32>>\n-  %1 = tt.load %0, %arg1 : !tt.ptr<tensor<4x1x8xf32>>\n-  // CHECK: triton_xla.squeeze_dims\n-  %2 = triton_xla.squeeze_dims %1 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %2 : tensor<4x8xf32>\n+// CHECK-LABEL: func @fold_squeeze_dims_of_extract\n+func.func @fold_squeeze_dims_of_extract(%arg0: !tt.ptr<f32>, %arg1: i32) -> tensor<4x8xf32> {\n+  // CHECK: %[[EXTRACT:.*]] = triton_xla.extract from %arg0\n+  // CHECK-SAME: as memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>\n+  // CHECK-SAME: [0, 3, 0] [4, 1, 8] [1, 1, 1] : tensor<4x8xf32>\n+  %0 = triton_xla.extract from %arg0\n+    as memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>\n+    [0, 3, 0] [4, 1, 8] [1, 1, 1] : tensor<4x1x8xf32>\n+  %1 = triton_xla.squeeze_dims %0 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n+  // CHECK: return %[[EXTRACT]]\n+  return %1 : tensor<4x8xf32>\n }\n \n // -----\n \n-// CHECK-LABEL: func @squeeze_store\n-tt.func @squeeze_store(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1x8xf32>) {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c3_i32 = arith.constant 3 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %c4_i64 = arith.constant 4 : i64\n-  %c8_i64 = arith.constant 8 : i64\n-  %c16_i64 = arith.constant 16 : i64\n-  %c128_i64 = arith.constant 128 : i64\n-  // CHECK-DAG: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  // CHECK-DAG: %[[ADDPTR:.*]] = tt.addptr %arg0, %c24_i64\n-  // CHECK-DAG: tt.make_tensor_ptr %[[ADDPTR]], {{.*}} {order = array<i32: 1, 0>} : <tensor<4x8xf32>>\n-  %0 = tt.make_tensor_ptr %arg0, [%c4_i64, %c16_i64, %c8_i64], [%c128_i64, %c8_i64, %c1_i64], [%c0_i32, %c3_i32, %c0_i32] {order = array<i32: 2, 1, 0>} : <tensor<4x1x8xf32>>\n-  // CHECK: tt.store {{.*}} {boundaryCheck = array<i32: 1>} : !tt.ptr<tensor<4x8xf32>>\n-  tt.store %0, %arg1 {boundaryCheck = array<i32: 2>} : !tt.ptr<tensor<4x1x8xf32>>\n-  tt.return\n+// CHECK-LABEL: func @squeeze_insert\n+func.func @squeeze_insert(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1x8xf32>) {\n+  // CHECK: %[[SRC:.*]] = triton_xla.squeeze_dims %arg1 {axis = 1 : i32}\n+  // CHECK: triton_xla.insert %[[SRC]] into %arg0\n+  // CHECK-SAME: as memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>\n+  // CHECK-SAME: [0, 3, 0] [4, 1, 8] [1, 1, 1] : tensor<4x8xf32>\n+  triton_xla.insert %arg1 into %arg0\n+    as memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>\n+    [0, 3, 0] [4, 1, 8] [1, 1, 1] : tensor<4x1x8xf32>\n+  return\n }\n \n // -----\n \n-// CHECK-LABEL: func @squeeze_store_unit_tensor\n-tt.func @squeeze_store_unit_tensor(%arg0: !tt.ptr<f32>, %arg1: tensor<1x1xf32>) {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %0 = tt.make_tensor_ptr %arg0, [%c1_i64, %c1_i64], [%c1_i64, %c1_i64], [%c0_i32, %c0_i32] {order = array<i32: 0>} : <tensor<1x1xf32>>\n+// CHECK-LABEL: func @squeeze_insert_unit_tensor\n+func.func @squeeze_insert_unit_tensor(%arg0: !tt.ptr<f32>, %arg1: tensor<1x1xf32>) {\n   // CHECK: triton_xla.squeeze_dims\n-  // CHECK: tt.store {{.*}} : !tt.ptr<tensor<1xf32>>\n-  tt.store %0, %arg1 : !tt.ptr<tensor<1x1xf32>>\n-  tt.return\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: func @squeeze_store_with_mask\n-tt.func @squeeze_store_with_mask(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1xf32>, %arg2: tensor<4x1xi1>) {\n-  %c0_i32 = arith.constant 0 : i32\n-  %c1_i64 = arith.constant 1 : i64\n-  %c4_i64 = arith.constant 4 : i64\n-  %0 = tt.make_tensor_ptr %arg0, [%c4_i64, %c1_i64], [%c1_i64, %c1_i64], [%c0_i32, %c0_i32] {order = array<i32: 0>} : <tensor<4x1xf32>>\n-  // CHECK-NOT: triton_xla.squeeze_dims\n-  // CHECK: tt.store {{.*}} : !tt.ptr<tensor<4x1xf32>>\n-  tt.store %0, %arg1, %arg2 : !tt.ptr<tensor<4x1xf32>>\n-  tt.return\n+  // CHECK: triton_xla.insert {{.*}} : tensor<1xf32>\n+  triton_xla.insert %arg1 into %arg0\n+    as memref<1x1xf32,#triton_xla.layout<[0, 1]>> \n+    [0, 0] [1, 1] [1, 1] : tensor<1x1xf32>\n+  return\n }\n \n // -----\n \n // CHECK-LABEL: func @reorder_squeeze_dims\n-tt.func @reorder_squeeze_dims(%arg0: tensor<4x1x8x1xf32>) -> tensor<4x8xf32> {\n+func.func @reorder_squeeze_dims(%arg0: tensor<4x1x8x1xf32>) -> tensor<4x8xf32> {\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1x8x1xf32> -> tensor<4x8x1xf32>\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 2 : i32} : tensor<4x8x1xf32> -> tensor<4x8xf32>\n   %0 = triton_xla.squeeze_dims %arg0 {axis = 3 : i32} : tensor<4x1x8x1xf32> -> tensor<4x1x8xf32>\n   %1 = triton_xla.squeeze_dims %0 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %1 : tensor<4x8xf32>\n+  return %1 : tensor<4x8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @diamond\n-tt.func @diamond(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n+func.func @diamond(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   // CHECK-NOT: arith.negf {{.*}} : tensor<4x1x8xf32>\n   %0 = arith.negf %arg0 : tensor<4x1x8xf32>\n   %1 = arith.addf %0, %0 : tensor<4x1x8xf32>\n   %2 = triton_xla.squeeze_dims %1 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %2 : tensor<4x8xf32>\n+  return %2 : tensor<4x8xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @insert_expand_dims\n-tt.func @insert_expand_dims(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n+func.func @insert_expand_dims(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n   // CHECK: %[[NEGF:.*]] = arith.negf {{.*}} : tensor<4xf32>\n   // CHECK-NOT: arith.negf\n   // CHECK: tt.expand_dims %[[NEGF]] {axis = 1 : i32} : tensor<4xf32> -> tensor<4x1xf32>\n@@ -272,39 +213,37 @@ tt.func @insert_expand_dims(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n   %3 = tt.expand_dims %1 {axis = 1 : i32} : tensor<4xf32> -> tensor<4x1xf32>\n   %4 = arith.addf %0, %3 : tensor<4x1xf32>\n   %5 = triton_xla.squeeze_dims %4 {axis = 1 : i32} : tensor<4x1xf32> -> tensor<4xf32>\n-  tt.return %5 : tensor<4xf32>\n+  return %5 : tensor<4xf32>\n }\n \n // -----\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_if\n-tt.func @push_squeeze_dims_up_through_if(%arg0: tensor<16xf32>,\n-    %arg1: tensor<16xf32>, %arg2: tensor<4x1xf32>, %arg3: tensor<4x1xf32>,\n-    %cond: i1) -> (tensor<16xf32>, tensor<4xf32>) {\n+func.func @push_squeeze_dims_up_through_if(\n+    %arg0: tensor<16xf32>, %arg1: tensor<4x1xf32>, %cond: i1\n+) -> (tensor<16xf32>, tensor<4xf32>) {\n+  // CHECK: scf.if %{{.*}} -> (tensor<16xf32>, tensor<4xf32>) {\n   %if:2 = scf.if %cond -> (tensor<16xf32>, tensor<4x1xf32>) {\n-    scf.yield %arg0, %arg2 : tensor<16xf32>, tensor<4x1xf32>\n+    // CHECK: %[[SQUEEZE:.*]] = triton_xla.squeeze_dims\n+    // CHECK: scf.yield %arg0, %[[SQUEEZE]] : tensor<16xf32>, tensor<4xf32>\n+    scf.yield %arg0, %arg1 : tensor<16xf32>, tensor<4x1xf32>\n   } else {\n-    scf.yield %arg1, %arg3 : tensor<16xf32>, tensor<4x1xf32>\n+    // CHECK: %[[SQUEEZE:.*]] = triton_xla.squeeze_dims\n+    // CHECK: scf.yield %arg0, %[[SQUEEZE]] : tensor<16xf32>, tensor<4xf32>\n+    scf.yield %arg0, %arg1 : tensor<16xf32>, tensor<4x1xf32>\n   }\n+  // CHECK-NOT: triton_xla.squeeze_dims\n   %squeeze = triton_xla.squeeze_dims %if#1 {axis = 1 : i32}\n-    : tensor<4x1xf32> -> tensor<4xf32>\n-  tt.return %if#0, %squeeze : tensor<16xf32>, tensor<4xf32>\n+      : tensor<4x1xf32> -> tensor<4xf32>\n+  // CHECK: return\n+  return %if#0, %squeeze : tensor<16xf32>, tensor<4xf32>\n }\n-// CHECK:        scf.if %{{.*}} -> (tensor<16xf32>, tensor<4xf32>) {\n-// CHECK-NEXT:    %[[SQUEEZE:.*]] = triton_xla.squeeze_dims\n-// CHECK-NEXT:    scf.yield %arg0, %[[SQUEEZE]] : tensor<16xf32>, tensor<4xf32>\n-// CHECK-NEXT:   } else {\n-// CHECK-NEXT:    %[[SQUEEZE:.*]] = triton_xla.squeeze_dims\n-// CHECK-NEXT:    scf.yield %arg1, %[[SQUEEZE]] : tensor<16xf32>, tensor<4xf32>\n-// CHECK-NEXT:  }\n-// CHECK-NOT:   triton_xla.squeeze_dims\n-// CHECK:       tt.return\n \n // -----\n \n // FINALIZE-LABEL: func @squeeze_dims_to_reshape\n-tt.func @squeeze_dims_to_reshape(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n+func.func @squeeze_dims_to_reshape(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   // FINALIZE: tt.reshape {{.*}} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n   %0 = triton_xla.squeeze_dims %arg0 {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n-  tt.return %0 : tensor<4x8xf32>\n+  return %0 : tensor<4x8xf32>\n }"
        },
        {
            "sha": "7c2dac722ba345ed5a62559161ce625076a945f0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 262,
            "deletions": 259,
            "changes": 521,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -14,13 +14,18 @@ limitations under the License.\n ==============================================================================*/\n \n #include <algorithm>\n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <numeric>\n #include <optional>\n #include <utility>\n #include <vector>\n \n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+// Above header needs to be included first to avoid 'major' macro collision.\n+\n+#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -53,10 +58,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n-#include \"xla/hlo/analysis/indexing_analysis.h\"\n #include \"xla/permutation_util.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n \n@@ -75,12 +77,6 @@ PointerType GetTensorPtrType(Type type) {\n                           mlir::NVVM::kGlobalMemorySpace);\n }\n \n-bool AreRankedTensors(ArrayRef<Type> types) {\n-  return llvm::all_of(types, [](mlir::Type type) {\n-    return mlir::isa<mlir::RankedTensorType>(type);\n-  });\n-}\n-\n SmallVector<Value> IndexCast(::xla::EmitterLocOpBuilder& builder, Type type,\n                              ValueRange values) {\n   SmallVector<Value> result;\n@@ -186,6 +182,7 @@ bool CanUseTma(bool tma_enabled,\n   // used or not.\n   SmallVector<int64_t> canonical_tile_strides(tile_strides.begin(),\n                                               tile_strides.end());\n+  // TODO(csigg): canonicalize_status is ignored.\n   auto canonicalize_status = CanonicalizeTileStrides(canonical_tile_strides,\n                                                      tile_shape, original_shape,\n                                                      /*validate=*/false);\n@@ -215,98 +212,6 @@ bool CanUseTma(bool tma_enabled,\n   return true;\n }\n \n-SmallVector<int32_t> ComputeBoundaryChecks(\n-    const ArrayRef<int64_t>& original_shape,\n-    const ArrayRef<int64_t>& tile_shape) {\n-  SmallVector<int32_t> boundary_checks;\n-  for (auto [dim_idx, sizes] :\n-       llvm::enumerate(llvm::zip(original_shape, tile_shape))) {\n-    auto [dim_size, tile_size] = sizes;\n-    if (dim_size % tile_size) {\n-      boundary_checks.push_back(dim_idx);\n-    }\n-  }\n-  return boundary_checks;\n-}\n-\n-// TensorPtr is intended to wrap the base pointer of the TiledHloInstruction and\n-// the necessary offsets so that Triton can compute the pointer to the\n-// block specific to the given pid. This option would yield simpler code,\n-// but cannot handle all combinations of strides and offsets, because Triton\n-// always multiplies the offset by the stride. E.g., it's not possible to\n-// slice [10] with [1:5:2] because the offset is misaligned with regards to the\n-// stride.\n-//\n-// Instead, we output a TensorPtr that points directly to the tile specific\n-// to the pid. All offset computation is done in advance. MakeTensorPtrOp\n-// sees 0 offsets. This allows Triton to read any block regardless of\n-// strides size or offsets. To make sure that masking is correct, we compute\n-// a \"residual shape\" which is the original parent shape minus the offsets.\n-SmallVector<Value> ComputeResidualShape(::xla::EmitterLocOpBuilder& builder,\n-                                        ArrayRef<int64_t> original_shape,\n-                                        ValueRange tile_offsets) {\n-  SmallVector<Value> residual_shape;\n-  for (auto [dim_idx, shape_and_tile_offset] :\n-       llvm::enumerate(llvm::zip(original_shape, tile_offsets))) {\n-    auto [shape, tile_offset] = shape_and_tile_offset;\n-    Value size =\n-        ::xla::gpu::triton::CreateConst(builder, builder.getI64Type(), shape)\n-            .UnwrapScalar();\n-    // Offsets are necessarily positive since they represent a distance\n-    // between 0 and the size of the tensor on the given axis. Therefore, it\n-    // is safe to use 'IndexCast' here. This allows index canonicalizations\n-    // later on.\n-    Value offset =\n-        builder.create<arith::IndexCastOp>(builder.getI64Type(), tile_offset);\n-    residual_shape.push_back(builder.create<arith::SubIOp>(size, offset));\n-  }\n-\n-  return residual_shape;\n-}\n-\n-// Compute physical strides of the tile. `tile_strides` contains strides for\n-// individual dimensions. We need to convert them to strides in the buffer\n-// taking into account physical layout. Note that we should pass in the\n-// minor-to-major layout for this to work correctly.\n-SmallVector<Value> ComputeStrides(::xla::EmitterLocOpBuilder& builder,\n-                                  ArrayRef<int64_t> original_shape,\n-                                  ValueRange tile_strides,\n-                                  ArrayRef<int64_t> minor_to_major_layout) {\n-  SmallVector<Value> strides(tile_strides.size());\n-  int64_t current_stride = 1;\n-  for (int64_t cur_dim : minor_to_major_layout) {\n-    strides[cur_dim] = builder.create<arith::MulIOp>(\n-        builder.create<arith::IndexCastOp>(builder.getI64Type(),\n-                                           tile_strides[cur_dim]),\n-        ::xla::gpu::triton::CreateConst(builder, builder.getI64Type(),\n-                                        current_stride)\n-            .UnwrapScalar());\n-    current_stride *= original_shape[cur_dim];\n-  }\n-  return strides;\n-}\n-\n-// Based on the multi-dimensional offsets and layout of the shape, we compute\n-// a linear offset. We do this because we move the pointer to the correct\n-// position via tt.addptr prior to calling tt.make_tensor_ptr.\n-Value ComputeLinearOffset(::xla::EmitterLocOpBuilder& builder,\n-                          Type element_type, ValueRange offsets,\n-                          llvm::ArrayRef<int64_t> shape,\n-                          llvm::ArrayRef<int64_t> layout) {\n-  ::xla::Shape xla_shape = ::xla::ShapeUtil::MakeShapeWithDenseLayout(\n-      xgt::GetPrimitiveType(element_type).value(), shape, layout);\n-\n-  ::xla::Shape linear_shape = ::xla::ShapeUtil::MakeShape(\n-      xla_shape.element_type(), {::xla::ShapeUtil::ElementsIn(xla_shape)});\n-  auto bitcast_map =\n-      ::xla::GetBitcastMap(xla_shape, linear_shape, builder.getContext());\n-\n-  return builder.create<arith::IndexCastOp>(\n-      builder.getI64Type(),\n-      builder.create<::xla::ApplyIndexingOp>(offsets, bitcast_map)\n-          .getResult(0));\n-}\n-\n // Add TMA attributes to the corresponding argument in the function.\n void AddTmaAttributes(::xla::EmitterLocOpBuilder& builder,\n                       const TypedValue<PointerType>& pointer,\n@@ -329,42 +234,39 @@ void AddTmaAttributes(::xla::EmitterLocOpBuilder& builder,\n           pointer.getType().getPointeeType().getIntOrFloatBitWidth() / 8));\n }\n \n-// Normalized layout is in the form of [N-1, N-2, ... 1, 0]. It is identical\n-// to HLO's layout.\n-bool IsNormalizedLayout(ArrayRef<int64_t> layout) {\n-  for (auto&& [idx, layout_entry] : llvm::enumerate(layout)) {\n-    if (layout_entry != layout.size() - 1 - idx) {\n+// Checks whether 'layout' is the default HLO layout in major-to-minor order,\n+// i.e. iff it's [N-1, N-2, ... 1, 0].\n+bool IsMajorToMinorLayout(ArrayRef<int64_t> layout) {\n+  for (auto [i, value] : llvm::enumerate(layout)) {\n+    if (value != layout.size() - 1 - i) {\n       return false;\n     }\n   }\n   return true;\n }\n \n-// Permutes the given array based on the given layout.\n+// Returns 'values' in major-to-minor order given minor-to-major 'layout'.\n template <typename T>\n-SmallVector<T> NormalizeImpl(ArrayRef<T> values, ArrayRef<int64_t> layout) {\n-  if (IsNormalizedLayout(layout)) {\n+SmallVector<T> GetMajorToMinorOrder(ArrayRef<T> values,\n+                                    ArrayRef<int64_t> layout) {\n+  if (IsMajorToMinorLayout(layout)) {\n     return llvm::to_vector(values);\n   }\n \n   auto reversed_layout = llvm::to_vector(layout);\n   std::reverse(reversed_layout.begin(), reversed_layout.end());\n-  std::vector<T> normalized_values = ::xla::Permute(values, reversed_layout);\n-  return SmallVector<T>(normalized_values.begin(), normalized_values.end());\n-}\n-\n-SmallVector<Value> Normalize(ValueRange values, ArrayRef<int64_t> layout) {\n-  SmallVector<Value> values_vec = llvm::to_vector(values);\n-  return NormalizeImpl<Value>(values_vec, layout);\n+  std::vector<T> vector = ::xla::Permute(values, reversed_layout);\n+  return SmallVector<T>(vector.begin(), vector.end());\n }\n \n-SmallVector<int64_t> Normalize(ArrayRef<int64_t> values,\n-                               ArrayRef<int64_t> layout) {\n-  return NormalizeImpl<int64_t>(values, layout);\n+// Returns 'values' in major-to-minor order given minor-to-major 'layout'.\n+SmallVector<Value> GetMajorToMinorOrder(ValueRange values,\n+                                        ArrayRef<int64_t> layout) {\n+  return GetMajorToMinorOrder(ArrayRef<Value>(llvm::to_vector(values)), layout);\n }\n \n // Given the layout of a tensor, return the inverse permutation required to\n-// transpose an already normalized tensor to the original tensor.\n+// transpose an already major-to-minor tensor to the original tensor.\n SmallVector<int32_t> GetInverseLayoutPermutation(ArrayRef<int64_t> layout) {\n   auto reversed_layout = llvm::to_vector(layout);\n   std::reverse(reversed_layout.begin(), reversed_layout.end());\n@@ -373,45 +275,6 @@ SmallVector<int32_t> GetInverseLayoutPermutation(ArrayRef<int64_t> layout) {\n   return SmallVector<int32_t>(permutation.begin(), permutation.end());\n }\n \n-Value CreateAddPtrOp(::xla::EmitterLocOpBuilder& builder,\n-                     const TypedValue<PointerType>& pointer, ValueRange offsets,\n-                     llvm::ArrayRef<int64_t> shape,\n-                     llvm::ArrayRef<int64_t> layout) {\n-  auto linear_offset = ComputeLinearOffset(\n-      builder, pointer.getType().getPointeeType(), offsets, shape, layout);\n-  return builder.create<AddPtrOp>(pointer.getType(), pointer, linear_offset);\n-}\n-\n-Value CreateMakeTensorPtrOp(::xla::EmitterLocOpBuilder& builder, Value ptr,\n-                            ArrayRef<int64_t> original_shape,\n-                            ArrayRef<int64_t> tile_shape,\n-                            SmallVector<Value> offsets,\n-                            SmallVector<Value> tile_strides,\n-                            ArrayRef<int64_t> layout) {\n-  SmallVector<Value> residual_shape =\n-      ComputeResidualShape(builder, original_shape, offsets);\n-\n-  // Offsets are always passed as 0 since we are using \"residual shape\".\n-  SmallVector<Value> zero_offsets(\n-      tile_shape.size(),\n-      ::xla::gpu::triton::CreateConst(builder, builder.getI32Type(), 0)\n-          .UnwrapScalar());\n-\n-  SmallVector<Value> strides =\n-      ComputeStrides(builder, original_shape, tile_strides, layout);\n-\n-  // Strides already encode the layout, so we can use the default order.\n-  // Note that the order attribute is ignored in the Triton lowering.\n-  SmallVector<int32_t> dim_order(layout.size());\n-  std::iota(dim_order.rbegin(), dim_order.rend(), 0);\n-\n-  return builder\n-      .create<MakeTensorPtrOp>(ptr, residual_shape, strides, zero_offsets,\n-                               llvm::to_vector_of<int32_t>(tile_shape),\n-                               dim_order)\n-      .getResult();\n-}\n-\n // Rewrite func.func to tt.func.\n class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n  public:\n@@ -435,12 +298,12 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n         auto tma_descriptor = mlir::cast<TmaDescriptorAttr>(attr);\n         auto layout = tma_descriptor.getLayout();\n         auto block_shape = tma_descriptor.getTileShape();\n-        SmallVector<int64_t> normalized_block_shape =\n-            Normalize(block_shape, layout);\n+        SmallVector<int64_t> ordered_block_shape =\n+            GetMajorToMinorOrder(block_shape, layout);\n \n         operand_type = TensorDescType::get(\n             builder.getContext(),\n-            RankedTensorType::get(normalized_block_shape, element_type));\n+            RankedTensorType::get(ordered_block_shape, element_type));\n         // !tt.tensordesc<tensor<block_shape x element_type>> -> !tt.ptr<>\n         cast_to_orig_type = builder.create<mlir::UnrealizedConversionCastOp>(\n             operand_type, func_arg);\n@@ -501,6 +364,140 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n   }\n };\n \n+// Compute the strides of a dense tensor given its shape and layout.\n+SmallVector<int64_t> ComputeStrides(ArrayRef<int64_t> shape,\n+                                    ArrayRef<int64_t> layout) {\n+  CHECK_EQ(shape.size(), layout.size());\n+  SmallVector<int64_t> result(shape.size());\n+  int64_t stride = 1;\n+  for (int64_t dim : layout) {\n+    result[dim] = stride;\n+    stride *= shape[dim];\n+  }\n+  return result;\n+}\n+\n+// Returns the set of not-reduced dimensions.\n+SmallVector<unsigned> GetRetainedDims(ArrayRef<unsigned> reduced_dims,\n+                                      size_t rank) {\n+  SmallVector<unsigned> result;\n+  result.reserve(rank);\n+  for (auto [i, dim] : llvm::enumerate(reduced_dims)) {\n+    for (unsigned j = result.size() + i; j < dim; ++j) {\n+      result.push_back(j);\n+    }\n+  }\n+  while (result.size() < rank) {\n+    result.push_back(result.size() + reduced_dims.size());\n+  }\n+  return result;\n+}\n+\n+// Expands the value in all dimensions except `dim` and broadcasts the result\n+// to the provided tile shape.\n+Value ExpandAndBroadcastValue(::xla::EmitterLocOpBuilder& builder, Value value,\n+                              int dim, RankedTensorType tile_type) {\n+  for (int i = 0; i < tile_type.getRank(); ++i) {\n+    if (i != dim) {\n+      value = builder.create<ExpandDimsOp>(value, i);\n+    }\n+  }\n+  return BroadcastOp::create(builder, tile_type, value);\n+}\n+\n+// Returns a pair of tensors:\n+// - The first tensor is a tensor of pointers to load/store.\n+// - The second tensor is a tensor of in-bounds predicates.\n+static std::pair<Value, Value> CreateTensorOfPointersAndMask(\n+    ::xla::EmitterLocOpBuilder& builder, Value base_ptr,\n+    ArrayRef<int64_t> original_shape, ArrayRef<int64_t> layout,\n+    ValueRange offsets, ArrayRef<int64_t> sizes, ArrayRef<int64_t> strides,\n+    ArrayRef<unsigned> reduced_dims, ArrayRef<int64_t> tile_shape) {\n+  CHECK_EQ(original_shape.size(), layout.size());\n+  CHECK_EQ(original_shape.size(), offsets.size());\n+  CHECK_EQ(original_shape.size(), sizes.size());\n+  CHECK_EQ(original_shape.size(), strides.size());\n+  CHECK_EQ(original_shape.size(), reduced_dims.size() + tile_shape.size());\n+\n+  SmallVector<int64_t> shape_strides = ComputeStrides(original_shape, layout);\n+  SmallVector<unsigned> retained_dims =\n+      GetRetainedDims(reduced_dims, tile_shape.size());\n+\n+  Type i64_type = builder.getI64Type();\n+  auto i64_tile_type = RankedTensorType::get(tile_shape, i64_type);\n+\n+  // Combines the values using op, if rhs is present. Otherwise returns lhs.\n+  auto add_if = [&](auto op, Value lhs, Value rhs) -> Value {\n+    if (rhs) {\n+      return decltype(op)::create(builder, lhs.getType(), lhs, rhs);\n+    }\n+    return lhs;\n+  };\n+\n+  SmallVector<Value> cast_offsets = IndexCast(builder, i64_type, offsets);\n+\n+  Value range_tile, mask_tile;\n+  for (auto [i, dim] : llvm::enumerate(retained_dims)) {\n+    auto i64_row_type = RankedTensorType::get({sizes[dim]}, i64_type);\n+\n+    // Create iota range row tensor.\n+    Value range = MakeRangeOp::create(\n+        builder, i64_row_type.clone(builder.getI32Type()), 0, sizes[dim]);\n+    range = arith::ExtSIOp::create(builder, i64_row_type, range);\n+\n+    // Multiply range by tile stride.\n+    Value stride = arith::ConstantOp::create(\n+        builder, DenseIntElementsAttr::get(i64_row_type, strides[dim]));\n+    range = arith::MulIOp::create(builder, range, stride);\n+\n+    // Expand and broadcast range to tile shape.\n+    range = ExpandAndBroadcastValue(builder, range, i, i64_tile_type);\n+\n+    Value mask;\n+    if (original_shape[dim] % sizes[dim] != 0) {\n+      // Imperfect tiling, create a mask for values that are inside bounds.\n+      Value upper_bound =\n+          arith::ConstantIntOp::create(builder, i64_type, original_shape[dim]);\n+      upper_bound =\n+          arith::SubIOp::create(builder, upper_bound, cast_offsets[dim]);\n+      upper_bound = SplatOp::create(builder, i64_tile_type, upper_bound);\n+      mask = arith::CmpIOp::create(builder, arith::CmpIPredicate::slt, range,\n+                                   upper_bound);\n+\n+      // Combine mask with previous iteration.\n+      mask_tile = add_if(arith::AndIOp(), mask, mask_tile);\n+    }\n+\n+    // Multiply range by shape strides.\n+    Value shape_stride = arith::ConstantOp::create(\n+        builder, DenseIntElementsAttr::get(i64_tile_type, shape_strides[dim]));\n+    range = arith::MulIOp::create(builder, range, shape_stride);\n+\n+    // Combine range with previous iteration.\n+    range_tile = add_if(arith::AddIOp(), range, range_tile);\n+  }\n+\n+  // Sum up block-uniform offsets multiplied by strides.\n+  Value block_offset;\n+  for (auto [cast_offset, shape_stride] :\n+       llvm::zip_equal(cast_offsets, shape_strides)) {\n+    Value offset = arith::MulIOp::create(\n+        builder, cast_offset,\n+        arith::ConstantIntOp::create(builder, i64_type, shape_stride));\n+    // Combine offset with previous iteration.\n+    block_offset = add_if(arith::AddIOp(), offset, block_offset);\n+  }\n+  // Add the accumulated offsets to the base pointer.\n+  Value block_ptr = add_if(AddPtrOp(), base_ptr, block_offset);\n+\n+  // Splat block-uniform pointer and add range offsets.\n+  auto ptr_tile_type = RankedTensorType::get(tile_shape, base_ptr.getType());\n+  Value ptr_tile = SplatOp::create(builder, ptr_tile_type, block_ptr);\n+  ptr_tile = add_if(AddPtrOp(), ptr_tile, range_tile);\n+\n+  return std::make_pair(ptr_tile, mask_tile);\n+}\n+\n class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n  public:\n   RewriteExtract(mlir::MLIRContext* context,\n@@ -520,7 +517,7 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n   // With TMA:\n   // tt.descriptor_load.\n   // Offsets are resolved in tt.descriptor_load.\n-  // If the layout is not normalized, we insert a transpose to ensure that\n+  // If the layout is not major-to-minor, we insert a transpose to ensure that\n   // the tile loaded in both TMA and non-TMA cases is the same:\n   // tt.descriptor_load + tt.transpose.\n   mlir::LogicalResult matchAndRewrite(\n@@ -532,64 +529,67 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n     ArrayRef<int64_t> src_layout = op.getSrcLayout();\n \n     auto offsets = op.getOffsetsAsValues(builder);\n-    if (CanUseTma(tma_enabled_, *device_description_, src_shape, tile_shape,\n-                  op.getStaticStrides(), offsets, op.getSrc(), src_layout)) {\n-      SmallVector<int64_t> strides = llvm::to_vector(op.getStaticStrides());\n-      if (auto result = CanonicalizeTileStrides(strides, tile_shape, src_shape);\n+    auto sizes = op.getStaticSizes();\n+    auto strides = to_vector(op.getStaticStrides());\n+\n+    if (CanUseTma(tma_enabled_, *device_description_, src_shape, sizes, strides,\n+                  offsets, op.getSrc(), src_layout)) {\n+      if (auto result = CanonicalizeTileStrides(strides, sizes, src_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n       }\n \n-      AddTmaAttributes(builder, op.getSrc(), src_shape, src_layout, tile_shape,\n+      AddTmaAttributes(builder, op.getSrc(), src_shape, src_layout, sizes,\n                        strides);\n \n-      SmallVector<int64_t> normalized_tile_shape =\n-          Normalize(tile_shape, src_layout);\n-      auto normalized_tile_type = RankedTensorType::get(\n-          normalized_tile_shape, tile_type.getElementType());\n-      auto normalized_offsets = Normalize(offsets, src_layout);\n-\n-      // tensor -> !tt.tensordesc<tile_type>\n-      auto cast_to_tensor_desc =\n-          builder\n-              .create<mlir::UnrealizedConversionCastOp>(\n-                  TensorDescType::get(builder.getContext(),\n-                                      normalized_tile_type),\n-                  op.getSrc())\n-              .getResult(0);\n-\n-      auto descriptor_load = builder.create<DescriptorLoadOp>(\n-          normalized_tile_type, cast_to_tensor_desc,\n-          IndexCast(builder, builder.getI32Type(), normalized_offsets));\n-\n-      // Insert a transpose if the layout is not normalized.\n-      if (!IsNormalizedLayout(src_layout)) {\n-        // Transpose an already normalized tensor back to the original layout.\n-        auto transpose =\n-            builder.create<TransOp>(op.getType(), descriptor_load,\n-                                    GetInverseLayoutPermutation(src_layout));\n-        rewriter.replaceOp(op, transpose);\n-        return mlir::success();\n+      auto ordered_offsets = GetMajorToMinorOrder(offsets, src_layout);\n+      auto ordered_sizes = GetMajorToMinorOrder(sizes, src_layout);\n+      auto ordered_type =\n+          tile_type.clone(GetMajorToMinorOrder(sizes, src_layout));\n+\n+      // ptr -> !tt.tensordesc<tile_type>\n+      auto desc_type = TensorDescType::get(builder.getContext(), ordered_type);\n+      auto cast_to_tensor_desc = mlir::UnrealizedConversionCastOp::create(\n+          builder, desc_type, op.getSrc());\n+\n+      Value result = DescriptorLoadOp::create(\n+          builder, ordered_type, cast_to_tensor_desc.getResult(0),\n+          IndexCast(builder, builder.getI32Type(), ordered_offsets));\n+\n+      // Insert a transpose if the layout is not major-to-minor.\n+      if (!IsMajorToMinorLayout(src_layout)) {\n+        result = TransOp::create(builder, result,\n+                                 GetInverseLayoutPermutation(src_layout));\n+      }\n+      // Insert a reshape if the result is rank-reduced.\n+      if (sizes.size() != tile_shape.size()) {\n+        result = ReshapeOp::create(builder, tile_shape, result,\n+                                   /*allowReorder=*/false);\n       }\n \n-      rewriter.replaceOp(op, descriptor_load);\n+      rewriter.replaceOp(op, result);\n       return mlir::success();\n     }\n \n-    auto ptr =\n-        CreateAddPtrOp(builder, op.getSrc(), offsets, src_shape, src_layout);\n-    auto strides = op.getStridesAsValues(builder);\n-    ptr = CreateMakeTensorPtrOp(builder, ptr, src_shape, tile_shape, offsets,\n-                                strides, src_layout);\n-    auto boundary_checks = ComputeBoundaryChecks(src_shape, tile_shape);\n-    std::optional<PaddingOption> padding;\n-    if (!boundary_checks.empty()) {\n-      padding = PaddingOption::PAD_ZERO;\n+    // Compute the set of reduced dimensions.\n+    auto reduction_mask = mlir::computeRankReductionMask(sizes, tile_shape);\n+    if (!reduction_mask) {\n+      return rewriter.notifyMatchFailure(op, \"Unsupported rank reduction.\");\n+    }\n+    SmallVector<unsigned> reduced_dims = to_vector(*reduction_mask);\n+    absl::c_sort(reduced_dims);\n+\n+    auto [ptr, mask] = CreateTensorOfPointersAndMask(\n+        builder, op.getSrc(), src_shape, src_layout, offsets, sizes, strides,\n+        reduced_dims, tile_shape);\n+    Value other;\n+    if (mask) {\n+      other = builder.create<arith::ConstantOp>(builder.getZeroAttr(\n+          RankedTensorType::get(tile_shape, tile_type.getElementType())));\n     }\n-    auto load =\n-        builder.create<LoadOp>(ptr, boundary_checks, padding,\n-                               CacheModifier::NONE, EvictionPolicy::NORMAL,\n-                               /*isVolatile=*/false);\n+    auto load = builder.create<LoadOp>(ptr, mask, other, CacheModifier::NONE,\n+                                       EvictionPolicy::NORMAL,\n+                                       /*isVolatile=*/false);\n     rewriter.replaceOp(op, load);\n     return mlir::success();\n   }\n@@ -617,9 +617,9 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n   // With TMA:\n   // tt.descriptor_store.\n   // Offsets are resolved in tt.descriptor_store.\n-  // If the layout is not normalized, we insert a transpose to to be compatible\n-  // with TMA's physical restrictions.\n-  // tt.transpose + tt.descriptor_store.\n+  // If the layout is not major-to-minor, we insert a transpose to to be\n+  // compatible with TMA's physical restrictions. tt.transpose +\n+  // tt.descriptor_store.\n   mlir::LogicalResult matchAndRewrite(\n       InsertOp op, mlir::PatternRewriter& rewriter) const override {\n     ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n@@ -629,53 +629,57 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n     ArrayRef<int64_t> dst_layout = op.getDstLayout();\n \n     auto offsets = op.getOffsetsAsValues(builder);\n-    if (CanUseTma(tma_enabled_, *device_description_, dst_shape, tile_shape,\n-                  op.getStaticStrides(), offsets, op.getDst(), dst_layout)) {\n-      SmallVector<int64_t> strides = llvm::to_vector(op.getStaticStrides());\n-      if (auto result = CanonicalizeTileStrides(strides, tile_shape, dst_shape);\n+    auto sizes = op.getStaticSizes();\n+    auto strides = to_vector(op.getStaticStrides());\n+\n+    // Compute the set of reduced dimensions.\n+    auto reduction_mask = mlir::computeRankReductionMask(sizes, tile_shape);\n+    if (!reduction_mask) {\n+      return rewriter.notifyMatchFailure(op, \"Unsupported rank reduction.\");\n+    }\n+    SmallVector<unsigned> reduced_dims = to_vector(*reduction_mask);\n+    absl::c_sort(reduced_dims);\n+\n+    if (CanUseTma(tma_enabled_, *device_description_, dst_shape, sizes, strides,\n+                  offsets, op.getDst(), dst_layout)) {\n+      if (auto result = CanonicalizeTileStrides(strides, sizes, dst_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n       }\n \n-      AddTmaAttributes(builder, op.getDst(), dst_shape, dst_layout, tile_shape,\n+      AddTmaAttributes(builder, op.getDst(), dst_shape, dst_layout, sizes,\n                        strides);\n \n-      SmallVector<int64_t> normalized_tile_shape =\n-          Normalize(tile_shape, dst_layout);\n-      auto normalized_tile_type = RankedTensorType::get(\n-          normalized_tile_shape, tile_type.getElementType());\n-      auto normalized_offsets = Normalize(offsets, dst_layout);\n-\n-      // tensor -> !tt.tensordesc<tile_type>\n-      auto cast_to_tensor_desc =\n-          builder\n-              .create<mlir::UnrealizedConversionCastOp>(\n-                  TensorDescType::get(builder.getContext(),\n-                                      normalized_tile_type),\n-                  op.getDst())\n-              .getResult(0);\n-\n-      // Insert a transpose if the layout is not normalized.\n-      auto src = op.getSrc();\n-      if (!IsNormalizedLayout(dst_layout)) {\n-        // Transpose to a normalized tensor by simply reversing the layout.\n+      // ptr -> !tt.tensordesc<tile_type>\n+      auto desc_type = TensorDescType::get(\n+          builder.getContext(),\n+          tile_type.clone(GetMajorToMinorOrder(sizes, dst_layout)));\n+      auto cast_to_tensor_desc = mlir::UnrealizedConversionCastOp::create(\n+          builder, desc_type, op.getDst());\n+\n+      Value src = op.getSrc();\n+      // Insert a expand_dims if the source is rank-reduced.\n+      for (auto dim : reduced_dims) {\n+        src = ExpandDimsOp::create(builder, src, dim);\n+      }\n+      // Insert a transpose if the layout is not major-to-minor.\n+      if (!IsMajorToMinorLayout(dst_layout)) {\n+        // Transpose to a major-to-minor tensor by simply reversing the layout.\n         auto transpose_order = llvm::to_vector_of<int32_t>(dst_layout);\n         std::reverse(transpose_order.begin(), transpose_order.end());\n-        src = builder.create<TransOp>(normalized_tile_type, op.getSrc(),\n-                                      transpose_order);\n+        src = builder.create<TransOp>(src, transpose_order);\n       }\n-      builder.create<DescriptorStoreOp>(\n-          cast_to_tensor_desc, src,\n-          IndexCast(builder, builder.getI32Type(), normalized_offsets));\n+\n+      auto ordered_offsets = GetMajorToMinorOrder(offsets, dst_layout);\n+      DescriptorStoreOp::create(\n+          builder, cast_to_tensor_desc.getResult(0), src,\n+          IndexCast(builder, builder.getI32Type(), ordered_offsets));\n     } else {\n-      auto ptr =\n-          CreateAddPtrOp(builder, op.getDst(), offsets, dst_shape, dst_layout);\n-      auto strides = op.getStridesAsValues(builder);\n-      ptr = CreateMakeTensorPtrOp(builder, ptr, dst_shape, tile_shape, offsets,\n-                                  strides, dst_layout);\n-      builder.create<StoreOp>(ptr, op.getSrc(),\n-                              ComputeBoundaryChecks(dst_shape, tile_shape),\n-                              CacheModifier::NONE, EvictionPolicy::NORMAL);\n+      auto [ptr, mask] = CreateTensorOfPointersAndMask(\n+          builder, op.getDst(), dst_shape, dst_layout, offsets, sizes, strides,\n+          reduced_dims, tile_shape);\n+      StoreOp::create(builder, ptr, op.getSrc(), mask, CacheModifier::NONE,\n+                      EvictionPolicy::NORMAL);\n     }\n     rewriter.eraseOp(op);\n     return mlir::success();\n@@ -701,9 +705,9 @@ class RewriteScalarInsert : public mlir::OpRewritePattern<tensor::InsertOp> {\n     auto cast_dst_to_tensor_ptr_type =\n         builder.create<mlir::UnrealizedConversionCastOp>(ptr_type, op.getDest())\n             .getResult(0);\n-    builder.create<StoreOp>(cast_dst_to_tensor_ptr_type, op.getScalar(),\n-                            /*boundary_checks=*/std::vector<int32_t>{},\n-                            CacheModifier::NONE, EvictionPolicy::NORMAL);\n+    StoreOp::create(builder, cast_dst_to_tensor_ptr_type, op.getScalar(),\n+                    /*boundary_checks=*/std::vector<int32_t>{},\n+                    CacheModifier::NONE, EvictionPolicy::NORMAL);\n     rewriter.replaceOp(op, op.getDest());\n     return mlir::success();\n   }\n@@ -722,13 +726,12 @@ class RewriteScalarExtract : public mlir::OpRewritePattern<tensor::ExtractOp> {\n     }\n     ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n     auto ptr_type = GetTensorPtrType(op.getType());\n-    auto cast_src_to_tensor_ptr_type =\n-        builder\n-            .create<mlir::UnrealizedConversionCastOp>(ptr_type, op.getTensor())\n-            .getResult(0);\n-    auto scalar =\n-        builder.create<LoadOp>(cast_src_to_tensor_ptr_type, CacheModifier::NONE,\n-                               EvictionPolicy::NORMAL, /*isVolatile=*/false);\n+    auto cast_src_to_tensor_ptr_type = mlir::UnrealizedConversionCastOp::create(\n+                                           builder, ptr_type, op.getTensor())\n+                                           .getResult(0);\n+    auto scalar = LoadOp::create(builder, cast_src_to_tensor_ptr_type,\n+                                 CacheModifier::NONE, EvictionPolicy::NORMAL,\n+                                 /*isVolatile=*/false);\n     rewriter.replaceOp(op, scalar.getResult());\n     return mlir::success();\n   }"
        },
        {
            "sha": "a62104f679ac8371250f8207c4b84e412b02b6f1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_fold_transpose_pass.cc",
            "status": "modified",
            "additions": 68,
            "deletions": 81,
            "changes": 149,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -13,16 +13,20 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n+#include <optional>\n #include <type_traits>\n #include <utility>\n \n #include \"absl/algorithm/container.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/DenseSet.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/OperationSupport.h\"\n #include \"mlir/IR/PatternMatch.h\"\n@@ -32,6 +36,7 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/util.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n@@ -44,56 +49,77 @@ namespace mlir::triton::xla {\n \n namespace {\n \n-template <typename T>\n-auto ApplyPermutation(T input, ArrayRef<int32_t> perm) {\n-  SmallVector<std::decay_t<decltype(*input.begin())>> result;\n-  result.reserve(perm.size());\n-  for (int32_t p : perm) {\n-    result.push_back(input[p]);\n+LogicalResult FoldTransposeOfExtract(TransOp op, PatternRewriter& rewriter) {\n+  auto extract = op.getSrc().getDefiningOp<ExtractOp>();\n+  if (!extract) {\n+    return rewriter.notifyMatchFailure(op, \"Transpose source is not extract.\");\n+  }\n+\n+  // Compute the dimensions dropped from the source.\n+  std::optional<llvm::SmallDenseSet<unsigned>> reduction_mask =\n+      computeRankReductionMask(extract.getStaticSizes(),\n+                               extract.getType().getShape());\n+  if (!reduction_mask) {\n+    return rewriter.notifyMatchFailure(op, \"Unsupported rank reduction.\");\n+  }\n+  SmallVector<unsigned> reduced_dims = to_vector(*reduction_mask);\n+  absl::c_sort(reduced_dims);\n+\n+  // Compute the set of not-reduced dimensions.\n+  size_t dst_rank = extract.getType().getRank();\n+  SmallVector<unsigned> retained_dims;\n+  retained_dims.reserve(dst_rank);\n+  for (auto [i, dim] : llvm::enumerate(reduced_dims)) {\n+    for (unsigned j = retained_dims.size() + i; j < dim; ++j) {\n+      retained_dims.push_back(j);\n+    }\n   }\n-  return result;\n-}\n-\n-LogicalResult FoldTransposeOfLoad(TransOp op, PatternRewriter& rewriter) {\n-  auto load = op.getSrc().getDefiningOp<LoadOp>();\n-  if (!load) {\n-    return rewriter.notifyMatchFailure(op, \"Transpose source is not a load.\");\n+  while (retained_dims.size() < dst_rank) {\n+    retained_dims.push_back(retained_dims.size() + reduced_dims.size());\n   }\n-  auto make_ptr = load.getPtr().getDefiningOp<MakeTensorPtrOp>();\n-  if (!make_ptr) {\n-    return rewriter.notifyMatchFailure(op, \"Expected load of make_tensor_ptr.\");\n+\n+  // Compute the permutation of source dimensions.\n+  size_t src_rank = extract.getSrcShape().size();\n+  SmallVector<int32_t> permutation;\n+  permutation.reserve(src_rank);\n+  for (auto [src_dim, dst_dim] :\n+       llvm::zip_equal(retained_dims, op.getOrder())) {\n+    while (permutation.size() < src_dim) {\n+      permutation.push_back(permutation.size());\n+    }\n+    permutation.push_back(retained_dims[dst_dim]);\n   }\n-  if (load.getMask() || load.getOther()) {\n-    return rewriter.notifyMatchFailure(op, \"Unsupported load.\");\n+  while (permutation.size() < src_rank) {\n+    permutation.push_back(permutation.size());\n   }\n \n-  auto apply_order = [&](auto range) {\n-    return ApplyPermutation(range, op.getOrder());\n+  auto permute = [&](auto range) {\n+    SmallVector<std::decay_t<decltype(*range.begin())>> result;\n+    result.reserve(range.size());\n+    for (int32_t dim : permutation) {\n+      result.push_back(range[dim]);\n+    }\n+    return result;\n   };\n \n-  auto ptr_type =\n-      PointerType::get(op.getType(), make_ptr.getType().getAddressSpace());\n-  auto new_make_ptr = rewriter.create<MakeTensorPtrOp>(\n-      make_ptr.getLoc(), ptr_type, make_ptr.getBase(),\n-      apply_order(make_ptr.getShape()), apply_order(make_ptr.getStrides()),\n-      // Leave original order, it's unused but checked to be default elsewhere.\n-      apply_order(make_ptr.getOffsets()), make_ptr.getOrderAttr());\n-\n-  SmallVector<bool> boundary_check_bits(op.getType().getRank());\n-  for (auto dim : load.getBoundaryCheck()) {\n-    boundary_check_bits[dim] = true;\n-  }\n-  SmallVector<int32_t> new_boundary_check;\n-  for (auto [dim, value] : llvm::enumerate(apply_order(boundary_check_bits))) {\n-    if (value) {\n-      new_boundary_check.push_back(dim);\n-    }\n+  SmallVector<int32_t> inv_permutation(permutation.size());\n+  for (auto [i, dim] : llvm::enumerate(permutation)) {\n+    inv_permutation[dim] = i;\n+  }\n+\n+  SmallVector<int64_t> layout;\n+  layout.reserve(extract.getSrcLayout().size());\n+  for (auto dim : extract.getSrcLayout()) {\n+    layout.push_back(inv_permutation[dim]);\n   }\n-  auto new_load = rewriter.create<LoadOp>(\n-      load.getLoc(), new_make_ptr, new_boundary_check, load.getPadding(),\n-      load.getCache(), load.getEvict(), load.getIsVolatile());\n \n-  rewriter.replaceOp(op, new_load.getResult());\n+  rewriter.replaceOpWithNewOp<ExtractOp>(\n+      op, op.getType(), extract.getSrc(), permute(extract.getMixedOffsets()),\n+      permute(extract.getStaticSizes()), permute(extract.getStaticStrides()),\n+      permute(extract.getSrcShape()), layout);\n+  if (extract->use_empty()) {\n+    rewriter.eraseOp(extract);\n+  }\n   return success();\n }\n \n@@ -181,44 +207,6 @@ LogicalResult PushTransposeUpThroughReshape(TransOp op,\n   return success();\n }\n \n-LogicalResult PushTransposeUpThroughJoinOfInlineAsm(TransOp op,\n-                                                    PatternRewriter& rewriter) {\n-  auto join = op.getSrc().getDefiningOp<JoinOp>();\n-  if (!join) {\n-    return rewriter.notifyMatchFailure(op, \"Transpose source is not a join.\");\n-  }\n-  if (op.getOrder().back() + 1 != op.getOrder().size()) {\n-    return rewriter.notifyMatchFailure(op, \"Transposes last dimension.\");\n-  }\n-  auto inline_asm = join.getLhs().getDefiningOp<ElementwiseInlineAsmOp>();\n-  if (!inline_asm || join.getRhs().getDefiningOp() != inline_asm) {\n-    return rewriter.notifyMatchFailure(op, \"Join source is not an inline asm.\");\n-  }\n-\n-  SmallVector<Value> new_operands;\n-  new_operands.reserve(inline_asm->getNumOperands());\n-  auto order = op.getOrder().drop_back();\n-  for (Value operand : inline_asm->getOperands()) {\n-    if (auto tensor_type = dyn_cast<RankedTensorType>(operand.getType())) {\n-      operand = rewriter.create<TransOp>(inline_asm->getLoc(), operand, order);\n-    }\n-    new_operands.push_back(operand);\n-  }\n-\n-  Operation* new_inline_asm = rewriter.clone(*inline_asm.getOperation());\n-  new_inline_asm->setOperands(new_operands);\n-  for (Value result : new_inline_asm->getResults()) {\n-    if (auto tensor_type = dyn_cast<RankedTensorType>(result.getType())) {\n-      auto shape = ApplyPermutation(tensor_type.getShape(), order);\n-      result.setType(tensor_type.clone(shape));\n-    }\n-  }\n-  rewriter.replaceOpWithNewOp<JoinOp>(op, op.getType(),\n-                                      new_inline_asm->getResults());\n-\n-  return success();\n-}\n-\n class TritonXLAFoldTransposePass\n     : public impl::TritonXLAFoldTransposePassBase<TritonXLAFoldTransposePass> {\n  public:\n@@ -227,10 +215,9 @@ class TritonXLAFoldTransposePass\n  private:\n   void runOnOperation() override {\n     RewritePatternSet patterns(&getContext());\n-    patterns.add(FoldTransposeOfLoad);\n+    patterns.add(FoldTransposeOfExtract);\n     patterns.add(PushTransposeUpThroughElementwise);\n     patterns.add(PushTransposeUpThroughReshape);\n-    patterns.add(PushTransposeUpThroughJoinOfInlineAsm);\n     if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       return signalPassFailure();\n     }"
        },
        {
            "sha": "01c2bed678eb630a4ca7d5cacf2e9641f0246e4c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_squeeze_dims_pass.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 101,
            "changes": 124,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0162a1c45a2f3a13e2035c4686a16edf183a251a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc?ref=0162a1c45a2f3a13e2035c4686a16edf183a251a",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #include <cstdint>\n #include <iterator>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <utility>\n \n@@ -46,7 +45,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace mlir::triton::xla {\n \n@@ -73,7 +71,7 @@ SmallVector<uint32_t> GetDimsToSqueeze(RankedTensorType type) {\n // Returns the axis of first squeeze_dims user.\n std::optional<uint32_t> GetSqueezeDimsUserAxis(Operation* op) {\n   for (Operation* user : op->getUsers()) {\n-    if (auto op = dyn_cast<SqueezeDimsOp>(user); op) {\n+    if (auto op = dyn_cast<SqueezeDimsOp>(user)) {\n       return op.getAxis();\n     }\n   }\n@@ -109,7 +107,7 @@ void ReplaceOpWithExpandDimsOf(PatternRewriter& rewriter, Operation* op,\n \n // Returns a new container with the given dimensions removed.\n template <typename ContainerT>\n-auto SqueezeElements(ContainerT elements, ArrayRef<uint32_t> squeeze_dims) {\n+auto SqueezeElements(ContainerT&& elements, ArrayRef<uint32_t> squeeze_dims) {\n   CHECK(absl::c_is_sorted(squeeze_dims));\n   auto it = elements.begin();\n   SmallVector<typename std::iterator_traits<decltype(it)>::value_type> result;\n@@ -123,23 +121,6 @@ auto SqueezeElements(ContainerT elements, ArrayRef<uint32_t> squeeze_dims) {\n   return result;\n }\n \n-// Returns a new boundary check with the given dimensions removed.\n-SmallVector<int32_t> SqueezeBoundaryCheck(ArrayRef<int32_t> boundary_check,\n-                                          ArrayRef<uint32_t> squeeze_dims) {\n-  CHECK(absl::c_is_sorted(boundary_check));\n-  CHECK(absl::c_is_sorted(squeeze_dims));\n-  SmallVector<int32_t> result;\n-  auto it = squeeze_dims.begin();\n-  for (int32_t dim : boundary_check) {\n-    it = std::lower_bound(it, squeeze_dims.end(), dim);\n-    if (it != squeeze_dims.end() && *it == dim) {\n-      continue;\n-    }\n-    result.push_back(dim - (it - squeeze_dims.begin()));\n-  }\n-  return result;\n-}\n-\n // Returns a new tensor type with the given dimensions removed.\n RankedTensorType SqueezeTensorType(RankedTensorType type,\n                                    ArrayRef<uint32_t> squeeze_dims) {\n@@ -169,97 +150,38 @@ Value SqueezeTensorValue(PatternRewriter& rewriter, Value value,\n   return value;\n }\n \n-// Returns a new pointer by applying the given offsets and strides for the\n-// given dimensions.\n-Value SqueezePointer(PatternRewriter& rewriter, Location loc, Value base,\n-                     ValueRange offsets, ValueRange strides,\n-                     ArrayRef<uint32_t> squeeze_dims) {\n-  for (auto dim : squeeze_dims) {\n-    Value extsi = rewriter.create<arith::ExtSIOp>(loc, rewriter.getI64Type(),\n-                                                  offsets[dim]);\n-    Value muli = rewriter.create<arith::MulIOp>(loc, extsi, strides[dim]);\n-    base = rewriter.create<AddPtrOp>(loc, base.getType(), base, muli);\n-  }\n-  return base;\n-}\n-\n-// Rewrites tt.make_tensor_ptr with unit dimensions. Returns the\n-// new MakeTensorPtrOp result and the dimensions that were removed.\n-Value SqueezeMakeTensorPtr(PatternRewriter& rewriter, MakeTensorPtrOp op,\n-                           ArrayRef<uint32_t> squeeze_dims) {\n-  auto tensor_type = cast<RankedTensorType>(op.getType().getPointeeType());\n-  auto squeeze_type = SqueezeTensorType(tensor_type, squeeze_dims);\n-  auto ptr_type =\n-      PointerType::get(squeeze_type, op.getType().getAddressSpace());\n-\n-  // Strides already encode the layout, so we can use the default order.\n-  // Note that the order attribute is ignored in the Triton lowering.\n-  SmallVector<int32_t> order(squeeze_type.getShape().size());\n-  std::iota(order.rbegin(), order.rend(), 0);\n-\n-  OpBuilder::InsertionGuard guard = SetInsertionPoint(rewriter, op);\n-  // Add the offsets along the dimensions to squeeze to the base pointer.\n-  Value base = SqueezePointer(rewriter, op.getLoc(), op.getBase(),\n-                              op.getOffsets(), op.getStrides(), squeeze_dims);\n-  return rewriter.create<MakeTensorPtrOp>(\n-      op.getLoc(), ptr_type, base, SqueezeElements(op.getShape(), squeeze_dims),\n-      SqueezeElements(op.getStrides(), squeeze_dims),\n-      SqueezeElements(op.getOffsets(), squeeze_dims), order);\n-}\n-\n-// Folds squeeze_dims into tt.load(tt.make_tensor_ptr).\n-// TODO(csigg): Add support for tt.load(tt.make_tensor_descriptor).\n-LogicalResult FoldSqueezeDimsOfLoad(LoadOp op, PatternRewriter& rewriter) {\n-  if (op.getMask() || op.getOther()) {\n-    return rewriter.notifyMatchFailure(op, \"Unsupported load.\");\n-  }\n+// Folds squeeze_dims into extract.\n+LogicalResult FoldSqueezeDimsOfExtract(ExtractOp op,\n+                                       PatternRewriter& rewriter) {\n   std::optional<uint32_t> axis = GetSqueezeDimsUserAxis(op);\n   if (!axis) {\n     return rewriter.notifyMatchFailure(op, \"No squeeze_dims users.\");\n   }\n-  if (absl::c_contains(op.getBoundaryCheck(), *axis)) {\n-    return rewriter.notifyMatchFailure(op, \"Boundary check contains axis.\");\n-  }\n-  auto make_tensor_ptr = op.getPtr().getDefiningOp<MakeTensorPtrOp>();\n-  if (!make_tensor_ptr) {\n-    return rewriter.notifyMatchFailure(\n-        op, \"Expected ptr to be defined by make_tensor_ptr.\");\n-  }\n \n-  Value pointer = SqueezeMakeTensorPtr(rewriter, make_tensor_ptr, *axis);\n-  Value new_load = rewriter.create<LoadOp>(\n-      op.getLoc(), pointer, SqueezeBoundaryCheck(op.getBoundaryCheck(), *axis),\n-      op.getPadding(), op.getCache(), op.getEvict(), op.getIsVolatile());\n-  ReplaceOpWithExpandDimsOf(rewriter, op, new_load, *axis);\n+  Value new_op = rewriter.create<ExtractOp>(\n+      op.getLoc(), SqueezeTensorType(op.getType(), *axis), op.getSrc(),\n+      op.getMixedOffsets(), op.getStaticSizes(), op.getStaticStrides(),\n+      op.getSrcShape(), op.getSrcLayout());\n+  ReplaceOpWithExpandDimsOf(rewriter, op, new_op, *axis);\n+  rewriter.eraseOp(op);\n   return success();\n }\n \n-// Extracts unit dimensions from tt.store and prepends them as squeeze_dims.\n-LogicalResult SqueezeStore(StoreOp op, PatternRewriter& rewriter) {\n-  if (op.getMask()) {\n-    return rewriter.notifyMatchFailure(op, \"Unsupported store.\");\n-  }\n-  auto make_tensor_ptr = op.getPtr().getDefiningOp<MakeTensorPtrOp>();\n-  if (!make_tensor_ptr) {\n-    return rewriter.notifyMatchFailure(\n-        op, \"Expected ptr to be defined by make_tensor_ptr.\");\n-  }\n-  auto tensor_type = dyn_cast<RankedTensorType>(op.getValue().getType());\n-  if (!tensor_type || tensor_type.getRank() == 0) {\n-    return rewriter.notifyMatchFailure(op, \"Expected tensor type.\");\n+// Extracts unit dimensions from insert and prepends them as squeeze_dims.\n+LogicalResult SqueezeInsert(InsertOp op, PatternRewriter& rewriter) {\n+  if (op.getSrc().getType().getRank() == 0) {\n+    return rewriter.notifyMatchFailure(op, \"Expected non-scalar source.\");\n   }\n \n-  auto squeeze_dims = GetDimsToSqueeze(tensor_type);\n+  auto squeeze_dims = GetDimsToSqueeze(op.getSrc().getType());\n   if (squeeze_dims.empty()) {\n-    return rewriter.notifyMatchFailure(op, \"No unit dimensions.\");\n+    return rewriter.notifyMatchFailure(op, \"No dimensions to squeeze.\");\n   }\n \n-  Value pointer = SqueezeMakeTensorPtr(rewriter, make_tensor_ptr, squeeze_dims);\n-  Value value = SqueezeTensorValue(rewriter, op.getValue(), squeeze_dims);\n-  rewriter.replaceOpWithNewOp<StoreOp>(\n-      op, pointer, value,\n-      SqueezeBoundaryCheck(op.getBoundaryCheck(), squeeze_dims), op.getCache(),\n-      op.getEvict());\n+  Value src = SqueezeTensorValue(rewriter, op.getSrc(), squeeze_dims);\n+  rewriter.replaceOpWithNewOp<InsertOp>(\n+      op, src, op.getDst(), op.getMixedOffsets(), op.getStaticSizes(),\n+      op.getStaticStrides(), op.getDstShape(), op.getDstLayout());\n   return success();\n }\n \n@@ -589,8 +511,8 @@ class TritonXLASqueezeDimsPass\n  private:\n   void runOnOperation() override {\n     RewritePatternSet patterns(&getContext());\n-    patterns.add(FoldSqueezeDimsOfLoad);\n-    patterns.add(SqueezeStore);\n+    patterns.add(FoldSqueezeDimsOfExtract);\n+    patterns.add(SqueezeInsert);\n     patterns.add(SqueezeReshapeOperand);\n     patterns.add(ExpandReshapeResult);\n     patterns.add<PushSqueezeDimsUpThroughElementwise>(&getContext());"
        }
    ],
    "stats": {
        "total": 1628,
        "additions": 717,
        "deletions": 911
    }
}