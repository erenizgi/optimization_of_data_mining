{
    "author": "tensorflower-gardener",
    "message": "Remove unused `se::StreamExecutor` arguments from GPU compiler methods.\n\n- The logic for determining `can_use_link_modules` shouldn't rely on executor being non-null.\n- It will help us to move towards compiler-runtime split.\n\nPiperOrigin-RevId: 811345745",
    "sha": "037786c0aafa9738cfa56821b7ec2c6125c142fe",
    "files": [
        {
            "sha": "493a73da3c584e9f9638240e406493289a02ca8c",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 23,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=037786c0aafa9738cfa56821b7ec2c6125c142fe",
            "patch": "@@ -2240,8 +2240,7 @@ absl::StatusOr<GpuCompiler::BackendCompileResult> GpuCompiler::CompileAndLink(\n     const HloModuleConfig& module_config,\n     CompileModuleResults& compile_module_results,\n     const se::DeviceDescription& device_description,\n-    se::StreamExecutor* stream_exec, const CompileOptions& options,\n-    const HloModule* debug_module) {\n+    const CompileOptions& options, const HloModule* debug_module) {\n   tsl::profiler::TraceMe traceme(\"CompileAndLink\");\n   llvm::Module* llvm_module = &*compile_module_results.llvm_module;\n \n@@ -2450,7 +2449,7 @@ absl::StatusOr<GpuCompiler::BackendCompileResult> GpuCompiler::CompileAndLink(\n   }\n \n   auto maybe_backend_result =\n-      LinkModules(device_description, stream_exec, std::move(binaries_to_link),\n+      LinkModules(device_description, std::move(binaries_to_link),\n                   module_config.debug_options());\n   if (!maybe_backend_result.ok()) {\n     LOG(ERROR) << \"The CUDA linking API did not work. Please use XLA_FLAGS=\"\n@@ -2468,12 +2467,12 @@ absl::StatusOr<GpuCompiler::BackendCompileResult> GpuCompiler::CompileAndLink(\n absl::StatusOr<GpuCompiler::CompileResultWithMetadata>\n GpuCompiler::CompileToBackendResult(\n     HloModule* module, llvm::LLVMContext* llvm_context,\n-    se::StreamExecutor* executor, const CompileOptions& options,\n+    const CompileOptions& options,\n     const se::DeviceDescription& gpu_device_info) {\n   tsl::profiler::TraceMe traceme(\"CompileToBackendResult\");\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n-  TF_RETURN_IF_ERROR(RunPreSchedulingPasses(module, executor, gpu_device_info,\n-                                            alias_info.get()));\n+  TF_RETURN_IF_ERROR(\n+      RunPreSchedulingPasses(module, gpu_device_info, alias_info.get()));\n   TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n                       ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n                                         &mlir_context_, alias_info.get()));\n@@ -2494,12 +2493,8 @@ GpuCompiler::CompileToBackendResult(\n             \". Are you missing gpu_plugin or stream_executor dependency?\"));\n   }\n \n-  // Test whether LinkModules is supported.\n-  bool can_use_link_modules = (executor != nullptr);\n-  if (can_use_link_modules) {\n-    TF_ASSIGN_OR_RETURN(can_use_link_modules,\n-                        CanUseLinkModules(module->config(), gpu_device_info));\n-  }\n+  TF_ASSIGN_OR_RETURN(bool can_use_link_modules,\n+                      CanUseLinkModules(module->config(), gpu_device_info));\n   const bool split_modules =\n       can_use_link_modules &&\n       module->config()\n@@ -2544,10 +2539,9 @@ GpuCompiler::CompileToBackendResult(\n   // TODO(anlunx): Enable multi-threading once deviceless AOT compilation is\n   // enabled.\n   if (split_modules) {\n-    TF_ASSIGN_OR_RETURN(\n-        backend_result,\n-        CompileAndLink(module->config(), compile_module_results,\n-                       gpu_device_info, executor, options, module));\n+    TF_ASSIGN_OR_RETURN(backend_result,\n+                        CompileAndLink(module->config(), compile_module_results,\n+                                       gpu_device_info, options, module));\n   } else {\n     CHECK(compile_module_results.llvm_module_constants == nullptr);\n     TF_ASSIGN_OR_RETURN(\n@@ -2633,10 +2627,9 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n     }\n   }\n \n-  TF_ASSIGN_OR_RETURN(\n-      CompileResultWithMetadata res,\n-      CompileToBackendResult(module.get(), &llvm_context, stream_exec, options,\n-                             gpu_device_info));\n+  TF_ASSIGN_OR_RETURN(CompileResultWithMetadata res,\n+                      CompileToBackendResult(module.get(), &llvm_context,\n+                                             options, gpu_device_info));\n \n   if (DumpingEnabledForHloModule(*module)) {\n     DumpToFileInDirOrStdout(\n@@ -2755,7 +2748,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,\n     llvm::LLVMContext llvm_context;\n     TF_ASSIGN_OR_RETURN(\n         CompileResultWithMetadata res,\n-        CompileToBackendResult(module.get(), &llvm_context, options.executor(),\n+        CompileToBackendResult(module.get(), &llvm_context,\n                                {options.device_allocator()}, gpu_device_info));\n \n     // Create GpuThunkAotCompilationResult if thunk runtime is enabled.\n@@ -2787,8 +2780,7 @@ absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(\n }\n \n absl::Status GpuCompiler::RunPreSchedulingPasses(\n-    HloModule* module, se::StreamExecutor* stream_exec,\n-    const se::DeviceDescription& gpu_device_info,\n+    HloModule* module, const se::DeviceDescription& gpu_device_info,\n     const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"RunPreSchedulingPasses\");\n   HloPassPipeline pipeline(\"pre-scheduling-passes\");"
        },
        {
            "sha": "021aa99a3bbc1abda539707a7e041599d28fa2b4",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=037786c0aafa9738cfa56821b7ec2c6125c142fe",
            "patch": "@@ -212,15 +212,14 @@ class GpuCompiler : public LLVMCompiler {\n   // Schedule and compile the module.\n   absl::StatusOr<CompileResultWithMetadata> CompileToBackendResult(\n       HloModule* module, llvm::LLVMContext* llvm_context,\n-      se::StreamExecutor* executor, const CompileOptions& options,\n+      const CompileOptions& options,\n       const se::DeviceDescription& gpu_device_info);\n \n   absl::StatusOr<BackendCompileResult> CompileAndLink(\n       const HloModuleConfig& module_config,\n       CompileModuleResults& compile_module_results,\n       const stream_executor::DeviceDescription& device_description,\n-      se::StreamExecutor* stream_exec, const CompileOptions& options,\n-      const HloModule* debug_module);\n+      const CompileOptions& options, const HloModule* debug_module);\n \n   absl::StatusOr<BackendCompileResult> CompileSingleModule(\n       const HloModuleConfig& module_config,\n@@ -234,8 +233,7 @@ class GpuCompiler : public LLVMCompiler {\n       const DebugOptions& debug_options);\n \n   absl::Status RunPreSchedulingPasses(\n-      HloModule* module, se::StreamExecutor* stream_exec,\n-      const se::DeviceDescription& gpu_device_info,\n+      HloModule* module, const se::DeviceDescription& gpu_device_info,\n       const GpuAliasInfo* alias_info);\n   absl::Status RunCollectiveScheduleLinearizerPasses(\n       HloModule* hlo_module, se::StreamExecutor* stream_exec);\n@@ -269,7 +267,6 @@ class GpuCompiler : public LLVMCompiler {\n \n   virtual absl::StatusOr<std::vector<uint8_t>> LinkModules(\n       const stream_executor::DeviceDescription& device_description,\n-      se::StreamExecutor* stream_exec,\n       std::vector<std::vector<uint8_t>> modules,\n       const DebugOptions& debug_options) {\n     return Unimplemented(\"LinkModules is not implemented.\");"
        },
        {
            "sha": "c970f7cd3a8b7440d426583c93b01c8211d663b0",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=037786c0aafa9738cfa56821b7ec2c6125c142fe",
            "patch": "@@ -735,7 +735,7 @@ absl::StatusOr<bool> NVPTXCompiler::CanUseLinkModules(\n \n absl::StatusOr<std::vector<uint8_t>> NVPTXCompiler::LinkModules(\n     const stream_executor::DeviceDescription& device_description,\n-    se::StreamExecutor* stream_exec, std::vector<std::vector<uint8_t>> modules,\n+    std::vector<std::vector<uint8_t>> modules,\n     const DebugOptions& debug_options) {\n   if (modules.empty()) {\n     return std::vector<uint8_t>{};"
        },
        {
            "sha": "19d1f977307a3a1037e1969f33321475c70b6269",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/037786c0aafa9738cfa56821b7ec2c6125c142fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h?ref=037786c0aafa9738cfa56821b7ec2c6125c142fe",
            "patch": "@@ -112,7 +112,6 @@ class NVPTXCompiler : public GpuCompiler {\n  private:\n   absl::StatusOr<std::vector<uint8_t>> LinkModules(\n       const stream_executor::DeviceDescription& device_description,\n-      se::StreamExecutor* stream_exec,\n       std::vector<std::vector<uint8_t>> modules,\n       const DebugOptions& debug_options) override;\n "
        }
    ],
    "stats": {
        "total": 50,
        "additions": 19,
        "deletions": 31
    }
}