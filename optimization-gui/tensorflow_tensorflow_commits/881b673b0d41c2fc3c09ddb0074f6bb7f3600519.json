{
    "author": "ezhulenev",
    "message": "[stream_executor] Rename DeviceMemoryAllocator to DeviceAddressAllocator and explain the difference between memory and address allocation\n\nPiperOrigin-RevId: 840599732",
    "sha": "881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
    "files": [
        {
            "sha": "51a41499cedbd97b33dfe0afba43ddacec3d5708",
            "filename": "third_party/xla/xla/client/executable_build_options.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fclient%2Fexecutable_build_options.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fclient%2Fexecutable_build_options.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fclient%2Fexecutable_build_options.cc?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -39,12 +39,12 @@ limitations under the License.\n namespace xla {\n \n ExecutableBuildOptions& ExecutableBuildOptions::set_device_allocator(\n-    se::DeviceMemoryAllocator* allocator) {\n+    se::DeviceAddressAllocator* allocator) {\n   device_allocator_ = allocator;\n   return *this;\n }\n \n-se::DeviceMemoryAllocator* ExecutableBuildOptions::device_allocator() const {\n+se::DeviceAddressAllocator* ExecutableBuildOptions::device_allocator() const {\n   return device_allocator_;\n }\n "
        },
        {
            "sha": "82d87829cc31fa18436d7c9c3cd7c24deb8c44a8",
            "filename": "third_party/xla/xla/client/executable_build_options.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fclient%2Fexecutable_build_options.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fclient%2Fexecutable_build_options.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fclient%2Fexecutable_build_options.h?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n namespace stream_executor {\n \n // Forward-declared to avoid StreamExecutor dependency.\n-class DeviceMemoryAllocator;\n+class DeviceAddressAllocator;\n \n }  // namespace stream_executor\n \n@@ -87,11 +87,11 @@ class ExecutableBuildOptions {\n   // want to run various algorithms on the device and pick the fastest one -- it\n   // might allocate buffers for use by these algorithms using this allocator.\n   //\n-  // This does not need to be the same as the se::DeviceMemoryAllocator passed\n+  // This does not need to be the same as the se::DeviceAddressAllocator passed\n   // when running the executable.\n   ExecutableBuildOptions& set_device_allocator(\n-      se::DeviceMemoryAllocator* allocator);\n-  se::DeviceMemoryAllocator* device_allocator() const;\n+      se::DeviceAddressAllocator* allocator);\n+  se::DeviceAddressAllocator* device_allocator() const;\n \n   // The number of replicas of this computation that are to be executed.\n   // Defaults to 1.\n@@ -310,7 +310,7 @@ class ExecutableBuildOptions {\n   bool result_layout_set_ = false;\n   std::optional<CompilationEnvironments> comp_envs_;\n   std::optional<DebugOptions> debug_options_;\n-  se::DeviceMemoryAllocator* device_allocator_ = nullptr;\n+  se::DeviceAddressAllocator* device_allocator_ = nullptr;\n   int num_replicas_ = 1;\n   int num_partitions_ = 1;\n   bool use_spmd_partitioning_ = false;"
        },
        {
            "sha": "7532cea00e3345137a1709bff6db47d1eb9bfeab",
            "filename": "third_party/xla/xla/executable_run_options.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fexecutable_run_options.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fexecutable_run_options.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fexecutable_run_options.cc?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -55,12 +55,12 @@ int ExecutableRunOptions::physical_device_ordinal() const {\n }\n \n ExecutableRunOptions& ExecutableRunOptions::set_allocator(\n-    stream_executor::DeviceMemoryAllocator* allocator) {\n+    stream_executor::DeviceAddressAllocator* allocator) {\n   allocator_ = allocator;\n   return *this;\n }\n \n-stream_executor::DeviceMemoryAllocator* ExecutableRunOptions::allocator()\n+stream_executor::DeviceAddressAllocator* ExecutableRunOptions::allocator()\n     const {\n   return allocator_;\n }"
        },
        {
            "sha": "930ba4140d0bbfaee76449ce91263c7bd1d7272e",
            "filename": "third_party/xla/xla/executable_run_options.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fexecutable_run_options.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fexecutable_run_options.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fexecutable_run_options.h?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -33,7 +33,7 @@ namespace stream_executor {\n class Stream;\n class Event;\n class Platform;\n-class DeviceMemoryAllocator;\n+class DeviceAddressAllocator;\n class DeviceAddressBase;\n }  // namespace stream_executor\n \n@@ -126,8 +126,8 @@ class ExecutableRunOptions {\n  public:\n   // Specifies the allocator to use during execution.\n   ExecutableRunOptions& set_allocator(\n-      stream_executor::DeviceMemoryAllocator* allocator);\n-  stream_executor::DeviceMemoryAllocator* allocator() const;\n+      stream_executor::DeviceAddressAllocator* allocator);\n+  stream_executor::DeviceAddressAllocator* allocator() const;\n \n   // If set, this is the device to run the computation on. Valid device_ordinal\n   // values are: 0 to # of devices - 1. These are the logical device ordinals,\n@@ -261,7 +261,7 @@ class ExecutableRunOptions {\n   std::vector<std::unique_ptr<CliqueKey>>* clique_keys() const;\n \n  private:\n-  stream_executor::DeviceMemoryAllocator* allocator_ = nullptr;\n+  stream_executor::DeviceAddressAllocator* allocator_ = nullptr;\n   int device_ordinal_ = -1;\n   int local_device_count_ = 0;\n   int physical_device_ordinal_ = -1;"
        },
        {
            "sha": "62c1d3ee167451550dc9e4f9f2a2603e48085e60",
            "filename": "third_party/xla/xla/ffi/ffi_api.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.h?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -54,7 +54,7 @@ struct ThreadPoolDevice;\n \n namespace stream_executor {\n class Stream;\n-class DeviceMemoryAllocator;\n+class DeviceAddressAllocator;\n }  // namespace stream_executor\n \n namespace xla::gpu {\n@@ -79,7 +79,7 @@ struct CallOptions {\n \n   struct GpuOptions {\n     se::Stream* stream = nullptr;\n-    se::DeviceMemoryAllocator* allocator = nullptr;\n+    se::DeviceAddressAllocator* allocator = nullptr;\n     const xla::gpu::CollectiveParams* collective_params = nullptr;\n     xla::gpu::CollectiveCliqueRequests* collective_clique_requests = nullptr;\n     const xla::gpu::CollectiveCliques* collective_cliques = nullptr;"
        },
        {
            "sha": "26433337cd21cab77919fa8650f5aa2dceb3d3e5",
            "filename": "third_party/xla/xla/ffi/ffi_structs.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -37,7 +37,7 @@ struct ThreadPoolDevice;\n \n namespace stream_executor {\n class Stream;\n-class DeviceMemoryAllocator;\n+class DeviceAddressAllocator;\n }  // namespace stream_executor\n \n namespace xla::gpu {\n@@ -65,7 +65,7 @@ struct XLA_FFI_ExecutionContext {\n \n   struct GpuContext {\n     stream_executor::Stream* stream = nullptr;\n-    stream_executor::DeviceMemoryAllocator* allocator = nullptr;\n+    stream_executor::DeviceAddressAllocator* allocator = nullptr;\n     const xla::gpu::CollectiveParams* collective_params = nullptr;\n     xla::gpu::CollectiveCliqueRequests* collective_clique_requests = nullptr;\n     const xla::gpu::CollectiveCliques* collective_cliques = nullptr;"
        },
        {
            "sha": "7cf34a542b9673025c79ef335bdda2905ccf7852",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -318,8 +318,8 @@ xla_cc_test(\n         \"//xla/pjrt:pjrt_common\",\n         \"//xla/service:gpu_plugin\",\n         \"//xla/service:shaped_buffer\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/tsl/concurrency:async_value\",\n         # copybara:uncomment \"//xla/tsl/framework:allocator\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "4c0020c87b23291c789add1ff5e139d7c89f74d6",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer_test.cc?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -39,8 +39,8 @@ limitations under the License.\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/tsl/concurrency/async_value.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/env.h\"\n@@ -57,16 +57,16 @@ using ::tsl::MakeConstructedAsyncValueRef;\n \n void* kOpaque = reinterpret_cast<void*>(1234567890);\n \n-class TestAllocator : public se::DeviceMemoryAllocator {\n+class TestAllocator : public se::DeviceAddressAllocator {\n  public:\n-  TestAllocator() : DeviceMemoryAllocator(nullptr) {}\n+  TestAllocator() : DeviceAddressAllocator(nullptr) {}\n \n-  using se::DeviceMemoryAllocator::Allocate;\n-  absl::StatusOr<stream_executor::OwningDeviceMemory> Allocate(\n+  using se::DeviceAddressAllocator::Allocate;\n+  absl::StatusOr<stream_executor::ScopedDeviceAddress<uint8_t>> Allocate(\n       int device_ordinal, uint64_t size, bool retry_on_failure,\n       int64_t memory_space) override {\n     const se::DeviceMemoryBase base(kOpaque, size);\n-    return stream_executor::OwningDeviceMemory(base, 0, this);\n+    return stream_executor::ScopedDeviceAddress<uint8_t>(base, 0, this);\n   }\n   absl::Status Deallocate(int device_ordinal,\n                           se::DeviceMemoryBase mem) override {"
        },
        {
            "sha": "7ba5e7db937015ae1ffb7264cae527644cfdd07d",
            "filename": "third_party/xla/xla/stream_executor/BUILD",
            "status": "modified",
            "additions": 23,
            "deletions": 13,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -106,6 +106,20 @@ xla_cc_test(\n     ],\n )\n \n+cc_library(\n+    name = \"device_address_allocator\",\n+    hdrs = [\"device_address_allocator.h\"],\n+    deps = [\n+        \":device_address\",\n+        \":platform\",\n+        \"//xla/tsl/platform:errors\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n cc_library(\n     name = \"device_memory\",\n     hdrs = [\"device_memory.h\"],\n@@ -125,6 +139,15 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"device_memory_allocator\",\n+    hdrs = [\"device_memory_allocator.h\"],\n+    deps = [\n+        \":device_address_allocator\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+    ],\n+)\n+\n cc_library(\n     name = \"module_spec\",\n     hdrs = [\"module_spec.h\"],\n@@ -246,19 +269,6 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"device_memory_allocator\",\n-    hdrs = [\"device_memory_allocator.h\"],\n-    deps = [\n-        \":device_memory\",\n-        \":platform\",\n-        \"//xla/tsl/platform:errors\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-    ],\n-)\n-\n cc_library(\n     name = \"stream_executor_memory_allocator\",\n     srcs = [\"stream_executor_memory_allocator.cc\"],"
        },
        {
            "sha": "883d312f2d4cdf047416bf0d44a8e43ec35c64a4",
            "filename": "third_party/xla/xla/stream_executor/device_address_allocator.h",
            "status": "added",
            "additions": 256,
            "deletions": 0,
            "changes": 256,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_address_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_address_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_address_allocator.h?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -0,0 +1,256 @@\n+/* Copyright 2017 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_DEVICE_ADDRESS_ALLOCATOR_H_\n+#define XLA_STREAM_EXECUTOR_DEVICE_ADDRESS_ALLOCATOR_H_\n+\n+#include <cstddef>\n+#include <cstdint>\n+\n+#include \"absl/base/macros.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+\n+namespace stream_executor {\n+\n+class Stream;\n+\n+// Forward declaration for owned device address. See implementation below.\n+template <typename T>\n+class ScopedDeviceAddress;\n+\n+// A type-erased owning device address.\n+using OwningDeviceAddress ABSL_DEPRECATE_AND_INLINE() =\n+    ScopedDeviceAddress<uint8_t>;\n+\n+// An allocator that allocates physical memory on the device, and maps it into\n+// the adressable range and returns it as `DeviceAddress` object to the caller.\n+//\n+// This allocator hides the physical memory allocation details from the user,\n+// and gives back a device address that can be used immediately to access data\n+// on the device. This is what most users want to use.\n+class DeviceAddressAllocator {\n+ public:\n+  // Parameter platform indicates which platform the allocator allocates\n+  // addresses on. Must be non-null.\n+  explicit DeviceAddressAllocator(const Platform* platform)\n+      : platform_(platform) {}\n+  virtual ~DeviceAddressAllocator() = default;\n+\n+  // Allocates addressable memory on the device.\n+  //\n+  // If size > 0 and the returned absl::StatusOr is OK, the wrapped\n+  // ScopedDeviceAddress must not be null.  If size == 0, must return a null\n+  // ScopedDeviceAddress.\n+  //\n+  // 'retry_on_failure': If false, and the first attempt to allocate the address\n+  // fails, the allocation should return immediately without retrying.  An\n+  // example use case is optional scratch spaces where a failure has only\n+  // performance impact.\n+  virtual absl::StatusOr<ScopedDeviceAddress<uint8_t>> Allocate(\n+      int device_ordinal, uint64_t size, bool retry_on_failure,\n+      int64_t memory_space) = 0;\n+\n+  // Two-arg version of Allocate(), which sets retry-on-failure to true and\n+  // memory_space to default (0).\n+  //\n+  // (We don't simply use a default argument on the virtual Allocate function\n+  // because default args on virtual functions are disallowed by the Google\n+  // style guide.)\n+  absl::StatusOr<ScopedDeviceAddress<uint8_t>> Allocate(int device_ordinal,\n+                                                        uint64_t size);\n+\n+  // Three-arg version of Allocate(), which sets memory_space to default (0).\n+  absl::StatusOr<ScopedDeviceAddress<uint8_t>> Allocate(int device_ordinal,\n+                                                        uint64_t size,\n+                                                        bool retry_on_failure);\n+\n+  // Typed version of the allocation, returning typed address.\n+  template <typename T>\n+  absl::StatusOr<ScopedDeviceAddress<T>> Allocate(int device_ordinal,\n+                                                  uint64_t size,\n+                                                  bool retry_on_failure = true,\n+                                                  int64_t memory_space = 0);\n+\n+  // Return the platform that the allocator allocates addresses on.\n+  const Platform* platform() const { return platform_; }\n+\n+  // Can we call Deallocate() as soon as a computation has been scheduled on\n+  // a stream, or do we have to wait for the computation to complete first?\n+  virtual bool AllowsAsynchronousDeallocation() const { return false; }\n+\n+  // Returns a stream pointer on which it is always safe to access address\n+  // allocated by this allocator. It is not necessary to use the returned stream\n+  // though, as clients may have additional information letting them safely use\n+  // a different stream.\n+  virtual absl::StatusOr<Stream*> GetStream(int device_ordinal) = 0;\n+\n+  // TODO(ezhulenev): Make this method private.\n+  virtual absl::Status Deallocate(int device_ordinal,\n+                                  DeviceAddressBase mem) = 0;\n+\n+ private:\n+  template <typename T>\n+  friend class ScopedDeviceAddress;\n+\n+  const Platform* platform_;\n+};\n+\n+// An owning container for device address allocated via DeviceAddressAllocator.\n+//\n+// ScopedDeviceAddress is an owning std::unique_ptr-like object, but it can\n+// point to address that resides on a \"device\" (e.g. a GPU). When a\n+// ScopedDeviceAddress goes out of scope, it frees the address it owns.\n+//\n+// We say that an instance of ScopedDeviceAddress is \"active\" if it currently\n+// owns a (possibly empty) address range on the device. Moving,\n+// Release()'ing, Free()'ing, and other actions can deactivate an active object.\n+template <typename T>\n+class ScopedDeviceAddress {\n+ public:\n+  // Default construction initializes the internal state to nullptr.  This\n+  // mirrors the std::unique_ptr<> functionality, where default construction\n+  // produces a nullptr unique_ptr, which can be assigned later.\n+  ScopedDeviceAddress() : device_ordinal_(-1), allocator_(nullptr) {}\n+\n+  // Construct a ScopedDeviceAddress from a custom allocator.\n+  //\n+  // Parameters:\n+  //  mem: Already-allocated device address value for this scoped mechanism to\n+  //       deallocate. This address must have been allocated by parent.\n+  //  device_ordinal: Device on which the address was allocated.\n+  //  allocator: Allocator used to deallocate the address when this instance\n+  //             goes out of scope.\n+  ScopedDeviceAddress(DeviceAddressBase mem, int device_ordinal,\n+                      DeviceAddressAllocator* allocator)\n+      : wrapped_(mem), device_ordinal_(device_ordinal), allocator_(allocator) {\n+    DCHECK_GE(device_ordinal_, 0);\n+  }\n+\n+  // Moves ownership of the device address from other to the constructed\n+  // object.\n+  //\n+  // Postcondition: other == nullptr.\n+  ScopedDeviceAddress(ScopedDeviceAddress&& other) noexcept\n+      : wrapped_(other.Release()),\n+        device_ordinal_(other.device_ordinal_),\n+        allocator_(other.allocator_) {}\n+\n+  // Releases the device address that was provided in the constructor.\n+  ~ScopedDeviceAddress() { CHECK_OK(Free()); }\n+\n+  // Moves ownership of the device address from other to this object.\n+  //\n+  // Postcondition: other == nullptr.\n+  ScopedDeviceAddress& operator=(ScopedDeviceAddress&& other) noexcept {\n+    CHECK_OK(Free());\n+    wrapped_ = other.Release();\n+    allocator_ = other.allocator_;\n+    device_ordinal_ = other.device_ordinal_;\n+    return *this;\n+  }\n+\n+  // Returns the device address that backs this scoped allocation converted to\n+  // DeviceAddress<T> apparent type. This is useful for cases where the\n+  // DeviceAddress must be passed by const-ref, as the ScopedDeviceAddress\n+  // doesn't allow copying, for scoped-object-lifetime reasons.\n+  const DeviceAddress<T>& cref() const { return wrapped_; }\n+\n+  // Returns a pointer to the DeviceAddress<T> apparent type for use in mutable\n+  // operations. The value returned should not be used outside the scope of this\n+  // ScopedDeviceAddress object's lifetime.\n+  DeviceAddress<T>* ptr() { return &wrapped_; }\n+  const DeviceAddress<T>* ptr() const { return &wrapped_; }\n+\n+  // Smart-pointer-like operators for the wrapped DeviceAddress.\n+  // This reference must not be used outside the lifetime of this\n+  // ScopedDeviceAddress.\n+  const DeviceAddress<T>& operator*() const { return cref(); }\n+  DeviceAddress<T>* operator->() { return ptr(); }\n+  const DeviceAddress<T>* operator->() const { return ptr(); }\n+\n+  bool is_null() const { return wrapped_.is_null(); }\n+  bool operator==(std::nullptr_t other) const { return is_null(); }\n+  bool operator!=(std::nullptr_t other) const { return !is_null(); }\n+\n+  // Analogous to std::unique_ptr::release, releases ownership of the held\n+  // device address and transfers it to the caller.\n+  //\n+  // Postcondition: *this == nullptr\n+  DeviceAddress<T> Release() {\n+    DeviceAddress<T> tmp = wrapped_;\n+    wrapped_ = DeviceAddress<T>{};\n+    return tmp;\n+  }\n+\n+  // The returned allocator is nonnull iff this object is active.\n+  DeviceAddressAllocator* allocator() const { return allocator_; }\n+\n+  int device_ordinal() const { return device_ordinal_; }\n+\n+  // Frees the existing device address, resets the wrapped address to null.\n+  absl::Status Free();\n+\n+ private:\n+  DeviceAddress<T> wrapped_;           // Value we wrap with scoped-release.\n+  int device_ordinal_;                 // Negative one for inactive object.\n+  DeviceAddressAllocator* allocator_;  // Null if this object is inactive.\n+\n+  ScopedDeviceAddress(const ScopedDeviceAddress&) = delete;\n+  void operator=(const ScopedDeviceAddress&) = delete;\n+};\n+\n+//===-----------------------------------------------------------------------===/\n+// Implementation details.\n+//===-----------------------------------------------------------------------===/\n+\n+inline absl::StatusOr<ScopedDeviceAddress<uint8_t>>\n+DeviceAddressAllocator::Allocate(int device_ordinal, uint64_t size) {\n+  return Allocate(device_ordinal, size, /*retry_on_failure=*/true,\n+                  /*memory_space=*/0);\n+}\n+\n+// Three-arg version of Allocate(), which sets memory_space to default (0).\n+inline absl::StatusOr<ScopedDeviceAddress<uint8_t>>\n+DeviceAddressAllocator::Allocate(int device_ordinal, uint64_t size,\n+                                 bool retry_on_failure) {\n+  return Allocate(device_ordinal, size, retry_on_failure,\n+                  /*memory_space=*/0);\n+}\n+\n+template <typename T>\n+absl::StatusOr<ScopedDeviceAddress<T>> DeviceAddressAllocator::Allocate(\n+    int device_ordinal, uint64_t size, bool retry_on_failure,\n+    int64_t memory_space) {\n+  return Allocate(device_ordinal, size, retry_on_failure, memory_space);\n+}\n+\n+template <typename T>\n+absl::Status ScopedDeviceAddress<T>::Free() {\n+  if (!wrapped_.is_null()) {\n+    CHECK(allocator_ != nullptr) << \"Owning pointer in inconsistent state\";\n+    TF_RETURN_IF_ERROR(allocator_->Deallocate(device_ordinal_, wrapped_));\n+  }\n+  wrapped_ = DeviceAddress<T>{};\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace stream_executor\n+\n+#endif  // XLA_STREAM_EXECUTOR_DEVICE_ADDRESS_ALLOCATOR_H_"
        },
        {
            "sha": "682a36d89cfb6a7b8c22f77a684091ad5511ac8d",
            "filename": "third_party/xla/xla/stream_executor/device_memory_allocator.h",
            "status": "modified",
            "additions": 10,
            "deletions": 203,
            "changes": 213,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_memory_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/881b673b0d41c2fc3c09ddb0074f6bb7f3600519/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_memory_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_memory_allocator.h?ref=881b673b0d41c2fc3c09ddb0074f6bb7f3600519",
            "patch": "@@ -1,4 +1,4 @@\n-/* Copyright 2017 The OpenXLA Authors.\n+/* Copyright 2025 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n@@ -16,213 +16,20 @@ limitations under the License.\n #ifndef XLA_STREAM_EXECUTOR_DEVICE_MEMORY_ALLOCATOR_H_\n #define XLA_STREAM_EXECUTOR_DEVICE_MEMORY_ALLOCATOR_H_\n \n-#include <cstddef>\n-#include <cstdint>\n-\n-#include \"absl/log/check.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/platform.h\"\n-#include \"xla/tsl/platform/errors.h\"\n+#include \"absl/base/macros.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n \n namespace stream_executor {\n \n-class Stream;\n-class DeviceMemoryAllocator;\n-\n-// Owning pointer for memory on a device.\n-//\n-// ScopedDeviceMemory is an owning pointer like std::unique_ptr, but it can\n-// point to memory that resides on a \"device\" (e.g. a GPU).  When a\n-// ScopedDeviceMemory goes out of scope, it frees the memory it owns.\n-//\n-// We say that an instance of ScopedDeviceMemory is \"active\" if it currently\n-// owns a (possibly empty) slice of memory on the device.  Moving,\n-// Release()'ing, Free()'ing, and other actions can deactivate an active object.\n-template <typename ElemT>\n-class ScopedDeviceMemory {\n- public:\n-  // Default construction initializes the internal state to nullptr.  This\n-  // mirrors the std::unique_ptr<> functionality, where default construction\n-  // produces a nullptr unique_ptr, which can be assigned later.\n-  ScopedDeviceMemory() : device_ordinal_(-1), allocator_(nullptr) {}\n-\n-  // Construct a ScopedDeviceMemory from a custom allocator.\n-  //\n-  // Parameters:\n-  //  mem: Already-allocated device memory value for this scoped mechanism to\n-  //       deallocate. This memory must have been allocated by parent.\n-  //  device_ordinal: Device on which the memory was allocated.\n-  //  allocator: Allocator used to deallocate memory when this instance goes\n-  //             out of scope.\n-  ScopedDeviceMemory(DeviceMemoryBase mem, int device_ordinal,\n-                     DeviceMemoryAllocator *allocator)\n-      : wrapped_(mem), device_ordinal_(device_ordinal), allocator_(allocator) {\n-    DCHECK_GE(device_ordinal_, 0);\n-  }\n-\n-  // Moves ownership of the memory from other to the constructed\n-  // object.\n-  //\n-  // Postcondition: other == nullptr.\n-  ScopedDeviceMemory(ScopedDeviceMemory &&other) noexcept\n-      : wrapped_(other.Release()),\n-        device_ordinal_(other.device_ordinal_),\n-        allocator_(other.allocator_) {}\n-\n-  // Releases the memory that was provided in the constructor.\n-  ~ScopedDeviceMemory() { CHECK_OK(Free()); }\n-\n-  // Moves ownership of the memory from other to this object.\n-  //\n-  // Postcondition: other == nullptr.\n-  ScopedDeviceMemory &operator=(ScopedDeviceMemory &&other) noexcept {\n-    CHECK_OK(Free());\n-    wrapped_ = other.Release();\n-    allocator_ = other.allocator_;\n-    device_ordinal_ = other.device_ordinal_;\n-    return *this;\n-  }\n-\n-  // Returns the memory that backs this scoped allocation converted to\n-  // DeviceMemory<T> apparent type. This is useful for cases where the\n-  // DeviceMemory must be passed by const-ref, as the ScopedDeviceMemory doesn't\n-  // allow copying, for scoped-object-lifetime reasons.\n-  const DeviceMemory<ElemT> &cref() const { return wrapped_; }\n-\n-  // Returns a pointer to the DeviceMemory<T> apparent type for use in mutable\n-  // operations. The value returned should not be used outside the scope of this\n-  // ScopedDeviceMemory object's lifetime.\n-  DeviceMemory<ElemT> *ptr() { return &wrapped_; }\n-  const DeviceMemory<ElemT> *ptr() const { return &wrapped_; }\n-\n-  // Smart-pointer-like operators for the wrapped DeviceMemory.\n-  // This reference must not be used outside the lifetime of this\n-  // ScopedDeviceMemory.\n-  const DeviceMemory<ElemT> &operator*() const { return cref(); }\n-  DeviceMemory<ElemT> *operator->() { return ptr(); }\n-  const DeviceMemory<ElemT> *operator->() const { return ptr(); }\n-\n-  bool is_null() const { return wrapped_.is_null(); }\n-  bool operator==(std::nullptr_t other) const { return is_null(); }\n-  bool operator!=(std::nullptr_t other) const { return !is_null(); }\n-\n-  // Analogous to std::unique_ptr::release, releases ownership of the held\n-  // memory and transfers it to the caller.\n-  //\n-  // Postcondition: *this == nullptr\n-  DeviceMemory<ElemT> Release() {\n-    DeviceMemory<ElemT> tmp = wrapped_;\n-    wrapped_ = DeviceMemory<ElemT>{};\n-    return tmp;\n-  }\n-\n-  // The returned allocator is nonnull iff this object is active.\n-  DeviceMemoryAllocator *allocator() const { return allocator_; }\n-\n-  int device_ordinal() const { return device_ordinal_; }\n-\n-  // Frees the existing memory, resets the wrapped memory to null.\n-  absl::Status Free();\n-\n- private:\n-  DeviceMemory<ElemT> wrapped_;       // Value we wrap with scoped-release.\n-  int device_ordinal_;                // Negative one for inactive object.\n-  DeviceMemoryAllocator *allocator_;  // Null if this object is inactive.\n-\n-  ScopedDeviceMemory(const ScopedDeviceMemory &) = delete;\n-  void operator=(const ScopedDeviceMemory &) = delete;\n-};\n-\n-// Type alias for compatibility with the previous managed memory implementation.\n-using OwningDeviceMemory = ScopedDeviceMemory<uint8_t>;\n-\n-// Memory allocator interface for the device.\n-//\n-// Intended usage is through Allocate() functions which return an owning smart\n-// pointer.\n-class DeviceMemoryAllocator {\n- public:\n-  // Parameter platform indicates which platform the allocator allocates memory\n-  // on. Must be non-null.\n-  explicit DeviceMemoryAllocator(const Platform *platform)\n-      : platform_(platform) {}\n-  virtual ~DeviceMemoryAllocator() {}\n-\n-  // Allocates memory on the device.\n-  //\n-  // If size > 0 and the returned absl::StatusOr is OK, the wrapped\n-  // OwningDeviceMemory must not be null.  If size == 0, must return a null\n-  // OwningDeviceMemory.\n-  //\n-  // 'retry_on_failure': If false, and the first attempt to allocate the memory\n-  // fails, the allocation should return immediately without retrying.  An\n-  // example use case is optional scratch spaces where a failure has only\n-  // performance impact.\n-  virtual absl::StatusOr<OwningDeviceMemory> Allocate(int device_ordinal,\n-                                                      uint64_t size,\n-                                                      bool retry_on_failure,\n-                                                      int64_t memory_space) = 0;\n-\n-  // Two-arg version of Allocate(), which sets retry-on-failure to true and\n-  // memory_space to default (0).\n-  //\n-  // (We don't simply use a default argument on the virtual Allocate function\n-  // because default args on virtual functions are disallowed by the Google\n-  // style guide.)\n-  absl::StatusOr<OwningDeviceMemory> Allocate(int device_ordinal,\n-                                              uint64_t size) {\n-    return Allocate(device_ordinal, size, /*retry_on_failure=*/true,\n-                    /*memory_space=*/0);\n-  }\n-\n-  // Three-arg version of Allocate(), which sets memory_space to default (0).\n-  absl::StatusOr<OwningDeviceMemory> Allocate(int device_ordinal, uint64_t size,\n-                                              bool retry_on_failure) {\n-    return Allocate(device_ordinal, size, retry_on_failure,\n-                    /*memory_space=*/0);\n-  }\n-\n-  // Typed version of the allocation, returning typed memory.\n-  template <typename ElemT>\n-  absl::StatusOr<ScopedDeviceMemory<ElemT>> Allocate(\n-      int device_ordinal, uint64_t size, bool retry_on_failure = true,\n-      int64_t memory_space = 0) {\n-    return Allocate(device_ordinal, size, retry_on_failure, memory_space);\n-  }\n-\n-  // Must be a nop for null pointers. Should not be used.\n-  //\n-  // TODO(cheshire): Add deprecation notice.\n-  virtual absl::Status Deallocate(int device_ordinal, DeviceMemoryBase mem) = 0;\n-\n-  // Return the platform that the allocator allocates memory on.\n-  const Platform *platform() const { return platform_; }\n-\n-  // Can we call Deallocate() as soon as a computation has been scheduled on\n-  // a stream, or do we have to wait for the computation to complete first?\n-  virtual bool AllowsAsynchronousDeallocation() const { return false; }\n-\n-  // Returns a stream pointer on which it is always safe to access memory\n-  // allocated by this allocator. It is not necessary to use the returned stream\n-  // though, as clients may have additional information letting them safely use\n-  // a different stream.\n-  virtual absl::StatusOr<Stream *> GetStream(int device_ordinal) = 0;\n+template <typename T>\n+using ScopedDeviceMemory ABSL_DEPRECATE_AND_INLINE() =\n+    ::stream_executor::ScopedDeviceAddress<T>;\n \n- protected:\n-  const Platform *platform_;\n-};\n+using OwningDeviceMemory ABSL_DEPRECATE_AND_INLINE() =\n+    ::stream_executor::ScopedDeviceAddress<uint8_t>;\n \n-template <typename ElemT>\n-absl::Status ScopedDeviceMemory<ElemT>::Free() {\n-  if (!wrapped_.is_null()) {\n-    CHECK(allocator_ != nullptr) << \"Owning pointer in inconsistent state\";\n-    TF_RETURN_IF_ERROR(allocator_->Deallocate(device_ordinal_, wrapped_));\n-  }\n-  wrapped_ = DeviceMemory<ElemT>{};\n-  return absl::OkStatus();\n-}\n+using DeviceMemoryAllocator ABSL_DEPRECATE_AND_INLINE() =\n+    ::stream_executor::DeviceAddressAllocator;\n \n }  // namespace stream_executor\n "
        }
    ],
    "stats": {
        "total": 553,
        "additions": 313,
        "deletions": 240
    }
}