{
    "author": "tensorflower-gardener",
    "message": "Adds new optional hints to the IFRT RemapPlan description.\n\nIn addition to constructing the mappings, the caller can now supply, for any given output array, an explicit list of which inputs it is constructed from, along with a device list per input indicating which devices that input populates.\n\nAdding these hints can improve performance for some implementations when the remap takes a subset of shards from each of multiple inputs, by precomputing the device lists those subsets reside on.\n\nPiperOrigin-RevId: 805557433",
    "sha": "1f5a77d9eee0aa87571688ca48e8790f4b589d50",
    "files": [
        {
            "sha": "ffac708e88781ba9567ceafaca06646712a22f60",
            "filename": "third_party/xla/xla/python/ifrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD?ref=1f5a77d9eee0aa87571688ca48e8790f4b589d50",
            "patch": "@@ -893,6 +893,7 @@ tf_proto_library(\n         \":internal\",\n         \":users\",\n     ]),\n+    deps = [\":device_proto\"],\n )\n \n xla_cc_test("
        },
        {
            "sha": "9f6367b6f31ae468796ce99ce8b7cafd1dbf1e08",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan.cc",
            "status": "modified",
            "additions": 127,
            "deletions": 1,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.cc?ref=1f5a77d9eee0aa87571688ca48e8790f4b589d50",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -31,6 +32,7 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/array_spec.h\"\n+#include \"xla/python/ifrt/client.h\"\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/remap_plan.pb.h\"\n@@ -103,6 +105,28 @@ absl::StatusOr<RemapPlanProto::MappingProto> MappingToProto(\n   return proto;\n }\n \n+absl::StatusOr<RemapPlan::InputDeviceRange> InputDeviceRangeFromProto(\n+    Client* client, const RemapPlanProto::InputDevices& proto) {\n+  RemapPlan::InputDeviceRange range;\n+  range.in_array = proto.in_array();\n+  TF_ASSIGN_OR_RETURN(range.input_devices,\n+                      DeviceList::FromProto(client, proto.device_list()));\n+  return range;\n+}\n+\n+RemapPlanProto::InputDevicesForOutput InputDeviceToOutputToProto(\n+    SerDesVersion version, int out_array,\n+    absl::Span<const RemapPlan::InputDeviceRange> input_devices) {\n+  RemapPlanProto::InputDevicesForOutput proto;\n+  proto.set_out_array(out_array);\n+  for (const RemapPlan::InputDeviceRange& input : input_devices) {\n+    RemapPlanProto::InputDevices* input_proto = proto.add_input_devices();\n+    input_proto->set_in_array(input.in_array);\n+    *input_proto->mutable_device_list() = input.input_devices->ToProto(version);\n+  }\n+  return proto;\n+}\n+\n // Checks if `interval` is in a valid range for the given number of shards.\n absl::Status CheckRange(int64_t num_shards,\n                         const RemapPlan::Interval& interval) {\n@@ -192,8 +216,16 @@ absl::Status RemapPlan::Validate() const {\n     return InvalidArgument(\"Must have at least one mapping\");\n   }\n \n+  absl::flat_hash_map<int,\n+                      absl::flat_hash_map<int, absl::flat_hash_set<Device*>>>\n+      out_buffer_to_in_buffer_and_devices;\n   for (int64_t i = 0; i < mappings->size(); ++i) {\n     const RemapPlan::Mapping& mapping = (*mappings)[i];\n+    absl::flat_hash_set<Device*>* in_device_set =\n+        input_devices_for_output_map.contains(mapping.out_array)\n+            ? &out_buffer_to_in_buffer_and_devices[mapping.out_array]\n+                                                  [mapping.in_array]\n+            : nullptr;\n     if (mapping.in_array < 0 || mapping.in_array >= num_inputs) {\n       return InvalidArgument(\n           \"mappings[%d].in_array must be in [0, %d], but is %d\", i,\n@@ -266,6 +298,15 @@ absl::Status RemapPlan::Validate() const {\n                                  mapping.in_array, in_shard);\n         }\n         in_used_buffers[in_shard] = true;\n+        if (in_device_set) {\n+          if (!in_device_set->insert(in_devices[in_shard]).second) {\n+            return InvalidArgument(\n+                \"Input device %s used more than once in mappings from input \"\n+                \"array %d to output array %d\",\n+                in_devices[in_shard]->DebugString(), mapping.in_array,\n+                mapping.out_array);\n+          }\n+        }\n         if (out_assigned_devices[out_shard] != nullptr) {\n           return InvalidArgument(\"Output array %d shard %d is already assigned\",\n                                  mapping.out_array, out_shard);\n@@ -277,6 +318,48 @@ absl::Status RemapPlan::Validate() const {\n     }\n   }\n \n+  for (const auto& [out_array, inputs] : input_devices_for_output_map) {\n+    const auto out_it = out_buffer_to_in_buffer_and_devices.find(out_array);\n+    if (out_it == out_buffer_to_in_buffer_and_devices.end()) {\n+      return InvalidArgument(\n+          \"Output buffer index %d in `input_devices_for_output_map` but not in \"\n+          \"`mappings`\",\n+          out_array);\n+    }\n+    if (inputs.size() != out_it->second.size()) {\n+      return InvalidArgument(\n+          \"Output buffer index %d in `input_devices_for_output_map` has %d \"\n+          \"inputs, but `mappings` reference %d inputs\",\n+          out_array, inputs.size(), out_it->second.size());\n+    }\n+    for (const InputDeviceRange& range : inputs) {\n+      const auto in_it = out_it->second.find(range.in_array);\n+      if (in_it == out_it->second.end()) {\n+        return InvalidArgument(\n+            \"Output buffer index %d in `input_devices_for_output_map` \"\n+            \"references input array %d that is not present in `mappings`\",\n+            out_array, range.in_array);\n+      }\n+      if (in_it->second.size() != range.input_devices->size()) {\n+        return InvalidArgument(\n+            \"Output buffer index %d in `input_devices_for_output_map` \"\n+            \"uses %d devices from input array %d, but `mappings` contains %d \"\n+            \"devices\",\n+            out_array, range.input_devices->size(), range.in_array,\n+            in_it->second.size());\n+      }\n+      for (const Device* const device : range.input_devices->devices()) {\n+        if (!in_it->second.contains(device)) {\n+          return InvalidArgument(\n+              \"Output buffer index %d in `input_devices_for_output_map` \"\n+              \"references device %s from input array %d, but `mappings` does \"\n+              \"not reference that device\",\n+              out_array, device->DebugString(), range.in_array);\n+        }\n+      }\n+    }\n+  }\n+\n   for (int i = 0; i < num_outputs; ++i) {\n     for (int out_shard = 0;\n          out_shard < output_specs[i].sharding->devices()->size(); ++out_shard) {\n@@ -331,6 +414,19 @@ absl::StatusOr<RemapPlan> RemapPlan::FromProto(Client* client,\n     plan.mappings->push_back(std::move(mapping));\n   }\n \n+  plan.input_devices_for_output_map.reserve(\n+      proto.input_devices_for_output_size());\n+  for (const auto& inputs_for_output_proto : proto.input_devices_for_output()) {\n+    std::vector<InputDeviceRange>& input_ranges =\n+        plan.input_devices_for_output_map[inputs_for_output_proto.out_array()];\n+    for (const auto& inputs_range_proto :\n+         inputs_for_output_proto.input_devices()) {\n+      TF_ASSIGN_OR_RETURN(\n+          auto devices, InputDeviceRangeFromProto(client, inputs_range_proto));\n+      input_ranges.push_back(std::move(devices));\n+    }\n+  }\n+\n   return plan;\n }\n \n@@ -359,6 +455,13 @@ absl::StatusOr<RemapPlanProto> RemapPlan::ToProto(SerDesVersion version) const {\n     TF_ASSIGN_OR_RETURN(*proto.add_mappings(), MappingToProto(mapping));\n   }\n \n+  proto.mutable_input_devices_for_output()->Reserve(\n+      input_devices_for_output_map.size());\n+  for (const auto& [out_array, input_devices] : input_devices_for_output_map) {\n+    *proto.add_input_devices_for_output() =\n+        InputDeviceToOutputToProto(version, out_array, input_devices);\n+  }\n+\n   return proto;\n }\n \n@@ -381,9 +484,32 @@ std::string RemapPlan::DebugString() const {\n                       }),\n         \"]\");\n   };\n+  auto format_output_to_inputs =\n+      [](const absl::flat_hash_map<int, std::vector<InputDeviceRange>>&\n+             output_to_inputs) {\n+        return absl::StrCat(\n+            \"[\",\n+            absl::StrJoin(\n+                output_to_inputs, \",\",\n+                [](std::string* out, const auto& output_to_inputs) {\n+                  const auto& [out_array, input_devices] = output_to_inputs;\n+                  absl::StrAppend(\n+                      out, \"o\", out_array, \":{\",\n+                      absl::StrJoin(\n+                          input_devices, \",\",\n+                          [](std::string* out, const InputDeviceRange& range) {\n+                            absl::StrAppend(out, \"i\", range.in_array, \":#\",\n+                                            range.input_devices->size());\n+                          }),\n+                      \"}\");\n+                }),\n+            \"]\");\n+      };\n   return absl::StrCat(\"RemapPlan(input_specs=\", format_array_specs(input_specs),\n                       \",output_specs=\", format_array_specs(output_specs), \",\",\n-                      \"mappings=\", format_mappings(*mappings), \")\");\n+                      \"mappings=\", format_mappings(*mappings), \",output_map=\",\n+                      format_output_to_inputs(input_devices_for_output_map),\n+                      \")\");\n }\n \n absl::Status RemapPlan::CheckArrayCopySemantics("
        },
        {
            "sha": "8ba709f27a4eccd75389e6f7aabac6bad259054f",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan.h",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.h?ref=1f5a77d9eee0aa87571688ca48e8790f4b589d50",
            "patch": "@@ -21,10 +21,12 @@ limitations under the License.\n #include <string>\n #include <vector>\n \n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/array_spec.h\"\n+#include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/remap_plan.pb.h\"\n #include \"xla/python/ifrt/serdes_default_version_accessor.h\"\n #include \"xla/python/ifrt/serdes_version.h\"\n@@ -80,6 +82,13 @@ struct RemapPlan {\n     std::string DebugString() const;\n   };\n \n+  // List of devices that are used as the source shards for a given input array\n+  // contributing to a given output array.\n+  struct InputDeviceRange {\n+    int in_array;\n+    DeviceListRef input_devices;\n+  };\n+\n   // Specification of inputs.\n   std::vector<ArraySpec> input_specs;\n \n@@ -89,6 +98,23 @@ struct RemapPlan {\n   // Mappings.\n   std::shared_ptr<std::vector<Mapping>> mappings;\n \n+  // If a key K is present in `input_devices_for_output_map` then it describes\n+  // all the inputs that contribute to the output with index K.\n+  //\n+  // The value lists all the input array indices that contribute to output K,\n+  // and for each input array I a device list containing all of the devices that\n+  // hold shards coming from I.\n+  //\n+  // Information must be consistent with the information in `mappings`, i.e.,\n+  // `input_devices_for_output_map` must duplicate, not replace, information in\n+  // `mappings`.\n+  //\n+  // Entries in `input_devices_for_output_map` are strictly optional, but their\n+  // presence may allow some implementations to be more efficient since the\n+  // implementation need not construct the device lists at execution time.\n+  absl::flat_hash_map<int, std::vector<InputDeviceRange>>\n+      input_devices_for_output_map;\n+\n   // Validates this plan against the requirements (see `RemapPlan` comment).\n   // This is a slow operation. It should not be performed repeatedly.\n   // Implementations of `Client::RemapArrays()` may bypass runtime checks on a"
        },
        {
            "sha": "8ff39411c241b9fc420dab35dc50cd6201142809",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan.proto",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.proto?ref=1f5a77d9eee0aa87571688ca48e8790f4b589d50",
            "patch": "@@ -18,6 +18,7 @@ syntax = \"proto3\";\n package xla.ifrt;\n \n import \"xla/python/ifrt/array_spec.proto\";\n+import \"xla/python/ifrt/device.proto\";\n \n // Wire format for `RemapPlan`. See `RemapPlan` for the semantics of the proto\n // fields.\n@@ -36,7 +37,16 @@ message RemapPlanProto {\n     repeated int64 to_end = 7;\n     repeated int64 to_step = 8;\n   }\n+  message InputDevices {\n+    int32 in_array = 1;\n+    DeviceListProto device_list = 2;\n+  }\n+  message InputDevicesForOutput {\n+    int32 out_array = 1;\n+    repeated InputDevices input_devices = 2;\n+  }\n   repeated ArraySpecProto input_specs = 1;\n   repeated ArraySpecProto output_specs = 2;\n   repeated MappingProto mappings = 3;\n+  repeated InputDevicesForOutput input_devices_for_output = 5;\n }"
        },
        {
            "sha": "1ab426812b1abd7daa61ea4b48994de5c4f04dc3",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan_test.cc",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f5a77d9eee0aa87571688ca48e8790f4b589d50/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc?ref=1f5a77d9eee0aa87571688ca48e8790f4b589d50",
            "patch": "@@ -592,6 +592,58 @@ TEST_P(RemapPlanTest, CheckMultipleInputsToOneOutput) {\n       xla::ifrt::ArrayCopySemantics::kDonateInput));\n }\n \n+TEST_P(RemapPlanTest, InvalidInputDevicesForOutputMap) {\n+  ArraySpec dummy_spec = GetDummySpec();\n+\n+  RemapPlan plan;\n+  plan.input_specs = {dummy_spec, dummy_spec};\n+  plan.output_specs = {dummy_spec};\n+\n+  plan.mappings = std::make_shared<std::vector<RemapPlan::Mapping>>();\n+  *plan.mappings = {RemapPlan::Mapping{/*in_array=*/0,\n+                                       /*out_array=*/0,\n+                                       /*from=*/{RemapPlan::Interval{0, 1, 1}},\n+                                       /*to=*/{RemapPlan::Interval{0, 1, 1}}},\n+                    RemapPlan::Mapping{/*in_array=*/1,\n+                                       /*out_array=*/0,\n+                                       /*from=*/{RemapPlan::Interval{1, 2, 1}},\n+                                       /*to=*/{RemapPlan::Interval{1, 2, 1}}}};\n+\n+  plan.input_devices_for_output_map.insert({1, {}});\n+  EXPECT_THAT(plan.Validate(),\n+              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument,\n+                                     HasSubstr(\"not in `mappings`\")));\n+\n+  plan.input_devices_for_output_map.clear();\n+  plan.input_devices_for_output_map.insert(\n+      {0, {{1, dummy_spec.sharding->devices()}}});\n+  EXPECT_THAT(plan.Validate(),\n+              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument,\n+                                     HasSubstr(\"has 1 inputs\")));\n+\n+  plan.input_devices_for_output_map.clear();\n+  plan.input_devices_for_output_map.insert(\n+      {0,\n+       {{3, dummy_spec.sharding->devices()},\n+        {4, dummy_spec.sharding->devices()}}});\n+  EXPECT_THAT(plan.Validate(),\n+              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument,\n+                                     HasSubstr(\"references input array 3\")));\n+\n+  plan.input_devices_for_output_map.clear();\n+  plan.input_devices_for_output_map.insert(\n+      {0, {{0, GetDevices({1})}, {4, dummy_spec.sharding->devices()}}});\n+  EXPECT_THAT(\n+      plan.Validate(),\n+      absl_testing::StatusIs(absl::StatusCode::kInvalidArgument,\n+                             HasSubstr(\"does not reference that device\")));\n+\n+  plan.input_devices_for_output_map.clear();\n+  plan.input_devices_for_output_map.insert(\n+      {0, {{0, GetDevices({0})}, {1, GetDevices({1})}}});\n+  TF_EXPECT_OK(plan.Validate());\n+}\n+\n INSTANTIATE_TEST_SUITE_P(NumDevices, RemapPlanTest,\n                          testing::Values(test_util::DeviceTestParam{\n                              /*num_devices=*/4,\n@@ -624,6 +676,7 @@ TEST_P(RemapPlanSerDesTest, ToFromProto) {\n   Shape shape({20, 20});\n   Shape shard_shape({5, 20});\n   DeviceListRef devices = GetDevices({0, 1, 2, 3});\n+  DeviceListRef devices_2 = GetDevices({1, 2});\n   ShardingRef sharding =\n       ConcreteEvenSharding::Create(devices, MemoryKind(), /*shape=*/shape,\n                                    /*shard_shape=*/shard_shape);\n@@ -651,11 +704,28 @@ TEST_P(RemapPlanSerDesTest, ToFromProto) {\n       /*from=*/{RemapPlan::Interval{0, 4, 2}, RemapPlan::Interval{1, 4, 2}},\n       /*to=*/{RemapPlan::Interval{0, 2, 1}, RemapPlan::Interval{2, 4, 1}}});\n \n+  plan.input_devices_for_output_map.insert({0, {{1, devices}}});\n+  plan.input_devices_for_output_map.insert({1, {{2, devices}, {3, devices_2}}});\n+\n   TF_ASSERT_OK_AND_ASSIGN(RemapPlanProto plan_proto, plan.ToProto(version()));\n   TF_ASSERT_OK_AND_ASSIGN(RemapPlan plan_copy,\n                           RemapPlan::FromProto(client(), plan_proto));\n \n   EXPECT_THAT(*plan_copy.mappings, ElementsAreArray(*plan.mappings));\n+  ASSERT_EQ(plan.input_devices_for_output_map.size(),\n+            plan_copy.input_devices_for_output_map.size());\n+  for (const auto& [out_array, input_devices] :\n+       plan.input_devices_for_output_map) {\n+    ASSERT_TRUE(plan_copy.input_devices_for_output_map.contains(out_array));\n+    const auto& copy_input_devices =\n+        plan_copy.input_devices_for_output_map[out_array];\n+    ASSERT_EQ(copy_input_devices.size(), input_devices.size());\n+    for (int i = 0; i < input_devices.size(); ++i) {\n+      EXPECT_EQ(copy_input_devices[i].in_array, input_devices[i].in_array);\n+      EXPECT_EQ(copy_input_devices[i].input_devices,\n+                input_devices[i].input_devices);\n+    }\n+  }\n \n   EXPECT_THAT(plan_copy.output_specs, SizeIs(2));\n   for (const auto& spec : plan_copy.input_specs) {"
        }
    ],
    "stats": {
        "total": 235,
        "additions": 234,
        "deletions": 1
    }
}