{
    "author": "loislo",
    "message": "[XLA:GPU] Enable fusion of broadcast and reshape into Triton scaled-dot.\n\nThis change allows broadcast and reshape operations on the scale operands of `scaled-dot` to be fused into the Triton kernel. It generalizes the operand fusion logic to handle all four operands of `scaled-dot` and adds support for `BroadcastOp` and `ExpandDimsOp` in the Triton MLIR conversion. A new test case is added to verify this fusion.\n\nPiperOrigin-RevId: 826012089",
    "sha": "398fefb5206dabe2cd6bf96833b08f52225e52a0",
    "files": [
        {
            "sha": "aa281c1b83e39a08d2e89d775f7c2623a7c869fb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/398fefb5206dabe2cd6bf96833b08f52225e52a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/398fefb5206dabe2cd6bf96833b08f52225e52a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=398fefb5206dabe2cd6bf96833b08f52225e52a0",
            "patch": "@@ -4914,6 +4914,80 @@ ENTRY e {\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n+\n+TEST_F(TritonScaledDotTest, BroadcastAndReshapeGetFused) {\n+  if (!GetCudaComputeCapability().IsAtLeastHopper()) {\n+    GTEST_SKIP() << \"Skipping test for pre-Hopper GPUs.\";\n+  }\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule ScaledDotWithBatchGetFusedAndExecutedCorrectly\n+\n+ENTRY e {\n+  lhs = f8e4m3fn[3,128,128] parameter(0)\n+  rhs = f8e4m3fn[3,128,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[3,128,1] parameter(2)\n+  lhs_scale_broadcasted = f8e8m0fnu[3,128,1,4] broadcast(lhs_scale),\n+      dimensions={0,1,2}\n+  lhs_scale_reshaped = f8e8m0fnu[3,128,4] reshape(lhs_scale_broadcasted)\n+  rhs_scale = f8e8m0fnu[3,128,1] parameter(3)\n+  rhs_scale_broadcasted = f8e8m0fnu[3,128,1,4] broadcast(rhs_scale),\n+      dimensions={0,1,2}\n+  rhs_scale_reshaped = f8e8m0fnu[3,128,4] reshape(rhs_scale_broadcasted)\n+  ROOT _ = bf16[3,128,128] scaled-dot(\n+      lhs,\n+      rhs,\n+      lhs_scale_reshaped,\n+      rhs_scale_reshaped),\n+    lhs_batch_dims={0},\n+    rhs_batch_dims={0},\n+    lhs_contracting_dims={2},\n+    rhs_contracting_dims={2}\n+}\n+  )hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloTextTemplate));\n+  TF_ASSERT_OK_AND_ASSIGN(auto optimized_module,\n+                          GetOptimizedModule(std::move(module)));\n+  constexpr absl::string_view kExpectedOptimizedHLO = R\"(\n+    CHECK: ROOT %{{.*}} = f8e8m0fnu[3,128,4]{2,1,0} broadcast(%{{.*}}), dimensions={0,1}\n+    CHECK: ROOT %{{.*}} = f8e8m0fnu[3,128,4]{2,1,0} broadcast(%{{.*}}), dimensions={0,1}\n+    CHECK: %fusion\n+    CHECK: %[[parameter_2:.*]] = f8e8m0fnu[3,128]{1,0} parameter(2)\n+    CHECK: %{{.*}} = f8e8m0fnu[3,128,4]{2,1,0} fusion(%[[parameter_2]])\n+    CHECK: %[[parameter_3:.*]] = f8e8m0fnu[3,128]{1,0} parameter(3)\n+    CHECK: %{{.*}} = f8e8m0fnu[3,128,4]{2,1,0} fusion(%[[parameter_3]])\n+    CHECK: ROOT {{.*}} scaled-dot\n+    CHECK: ENTRY\n+    CHECK: __triton_nested_gemm_fusion\n+  )\";\n+  EXPECT_THAT(RunFileCheck(optimized_module->ToString(), kExpectedOptimizedHLO),\n+              true);\n+\n+  HloComputation* scaled_dot_computation = GetFirstComputationWithInstruction(\n+      *optimized_module, HloOpcode::kScaledDot);\n+  constexpr absl::string_view kExpectedTritonIr = R\"(\n+      CHECK: tt.dot_scaled\n+      CHECK: tensor<16x128xf8E4M3FN>, tensor<16x4xi8>\n+      CHECK: tensor<128x16xf8E4M3FN>, tensor<16x4xi8>\n+      CHECK: -> tensor<16x16xf32>\n+  )\";\n+  EXPECT_THAT(CreateTritonIrAndFileCheck(*scaled_dot_computation,\n+                                         /*block_level_parameters=*/\n+                                         {\n+                                             {{1, 16, 16}},\n+                                             4,\n+                                             1,\n+                                             1,\n+                                             false,\n+                                         },\n+                                         kExpectedTritonIr),\n+              absl_testing::IsOk());\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "c72caaa635600230a87fd77fa0b915d76d62ab41",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_convert_unsupported_types.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/398fefb5206dabe2cd6bf96833b08f52225e52a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/398fefb5206dabe2cd6bf96833b08f52225e52a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_convert_unsupported_types.cc?ref=398fefb5206dabe2cd6bf96833b08f52225e52a0",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n #include <utility>\n \n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/SCF/Transforms/Patterns.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n@@ -26,7 +25,6 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n-#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n@@ -101,6 +99,8 @@ class TritonXLAConvertUnsupportedTypesPass\n                  GenericOpConversionPattern<::xla::xtile::InsertTileOp>,\n                  GenericOpConversionPattern<ReshapeOp>,\n                  GenericOpConversionPattern<TransOp>,\n+                 GenericOpConversionPattern<ExpandDimsOp>,\n+                 GenericOpConversionPattern<BroadcastOp>,\n                  GenericOpConversionPattern<arith::BitcastOp>>(converter, ctx);\n     scf::populateSCFStructuralTypeConversions(converter, patterns);\n     populateFunctionOpInterfaceTypeConversionPattern<::xla::xtile::EntryFuncOp>("
        },
        {
            "sha": "a6e1d2a336e6dab6e6849c4eff3073f87822c151",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 26,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/398fefb5206dabe2cd6bf96833b08f52225e52a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/398fefb5206dabe2cd6bf96833b08f52225e52a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc?ref=398fefb5206dabe2cd6bf96833b08f52225e52a0",
            "patch": "@@ -159,16 +159,18 @@ struct HlosAndRequirements {\n };\n \n // Clones the hero kDot operation into the fusion.\n-HloInstruction& FuseDot(const HloDotInstruction& dot,\n-                        const HloInstruction& fused_lhs,\n-                        const HloInstruction& fused_rhs,\n+HloInstruction& FuseDot(const HloInstruction& dot,\n+                        const std::vector<HlosAndRequirements>& hlos_and_reqs,\n                         HloComputation::Builder& builder  // append\n ) {\n   VLOG(3) << \"Fusing \" << dot.ToString();\n \n-  std::vector<HloInstruction*> hlo_new_operands = {\n-      const_cast<HloInstruction*>(&fused_lhs),\n-      const_cast<HloInstruction*>(&fused_rhs)};\n+  std::vector<HloInstruction*> hlo_new_operands;\n+  hlo_new_operands.reserve(dot.operand_count());\n+  for (int i = 0; i < hlos_and_reqs.size(); ++i) {\n+    hlo_new_operands.push_back(\n+        const_cast<HloInstruction*>(hlos_and_reqs[i].fused_hlo));\n+  }\n   return *builder.AddInstruction(\n       dot.CloneWithNewOperands(dot.shape(), hlo_new_operands));\n }\n@@ -672,14 +674,17 @@ absl::StatusOr<Decision> CreateDotFusion(\n     return Decision::Deny(is_supported.Explain());\n   }\n \n+  std::vector<HlosAndRequirements> hlos_and_reqs;\n+  hlos_and_reqs.reserve(dot.operand_count());\n   TF_ASSIGN_OR_RETURN(HlosAndRequirements lhs_hlos_and_reqs,\n                       FuseDotOperand(dot, /*operand_index=*/0, gpu_version,\n                                      builder, fusion_inputs));\n+  hlos_and_reqs.push_back(lhs_hlos_and_reqs);\n   TF_ASSIGN_OR_RETURN(HlosAndRequirements rhs_hlos_and_reqs,\n                       FuseDotOperand(dot, /*operand_index=*/1, gpu_version,\n                                      builder, fusion_inputs));\n-  HloInstruction& fused_dot = FuseDot(dot, *lhs_hlos_and_reqs.fused_hlo,\n-                                      *rhs_hlos_and_reqs.fused_hlo, builder);\n+  hlos_and_reqs.push_back(rhs_hlos_and_reqs);\n+  HloInstruction& fused_dot = FuseDot(dot, hlos_and_reqs, builder);\n   // For now the RHS doesn't support splits, so it also doesn't impose any\n   // requirements.\n   HlosAndRequirements fused_output_and_reqs =\n@@ -860,29 +865,38 @@ class GemmFusionVisitor : public DfsHloRewriteVisitor {\n     HloComputation::Builder builder(\n         absl::StrCat(\"fusion_\", scaled_dot->name()));\n \n-    auto create_parameter = [&](int64_t parameter_number,\n-                                absl::string_view name) {\n-      return builder.AddInstruction(HloInstruction::CreateParameter(\n-          parameter_number, scaled_dot->operand(parameter_number)->shape(),\n-          name));\n-    };\n-    std::vector<HloInstruction*> new_operands{\n-        create_parameter(0, \"lhs\"),\n-        create_parameter(1, \"rhs\"),\n-        create_parameter(2, \"lhs_scale\"),\n-        create_parameter(3, \"rhs_scale\"),\n-    };\n-    builder.AddInstruction(\n-        scaled_dot->CloneWithNewOperands(scaled_dot->shape(), new_operands));\n+    std::vector<HloInstruction*> fusion_inputs;\n+\n+    std::vector<HlosAndRequirements> hlos_and_reqs;\n+    hlos_and_reqs.reserve(scaled_dot->operand_count());\n+    TF_ASSIGN_OR_RETURN(HlosAndRequirements lhs_hlos_and_reqs,\n+                        FuseDotOperand(*scaled_dot, /*operand_index=*/0,\n+                                       gpu_version_, builder, fusion_inputs));\n+    hlos_and_reqs.push_back(lhs_hlos_and_reqs);\n+    TF_ASSIGN_OR_RETURN(HlosAndRequirements rhs_hlos_and_reqs,\n+                        FuseDotOperand(*scaled_dot, /*operand_index=*/1,\n+                                       gpu_version_, builder, fusion_inputs));\n+    hlos_and_reqs.push_back(rhs_hlos_and_reqs);\n+    TF_ASSIGN_OR_RETURN(HlosAndRequirements lhs_scale_hlos_and_reqs,\n+                        FuseDotOperand(*scaled_dot, /*operand_index=*/2,\n+                                       gpu_version_, builder, fusion_inputs));\n+    hlos_and_reqs.push_back(lhs_scale_hlos_and_reqs);\n+    TF_ASSIGN_OR_RETURN(HlosAndRequirements rhs_scale_hlos_and_reqs,\n+                        FuseDotOperand(*scaled_dot, /*operand_index=*/3,\n+                                       gpu_version_, builder, fusion_inputs));\n+    hlos_and_reqs.push_back(rhs_scale_hlos_and_reqs);\n+\n+    FuseDot(*scaled_dot, hlos_and_reqs, builder);\n \n     HloComputation* computation =\n         scaled_dot->GetModule()->AddComputationAndUnifyNamesAndIds(\n             builder.Build(),\n             /*is_entry=*/false);\n-    HloInstruction* fusion = scaled_dot->parent()->AddInstruction(\n-        HloInstruction::CreateFusion(computation->root_instruction()->shape(),\n-                                     HloInstruction::FusionKind::kCustom,\n-                                     scaled_dot->operands(), computation));\n+\n+    HloInstruction* fusion =\n+        scaled_dot->parent()->AddInstruction(HloInstruction::CreateFusion(\n+            computation->root_instruction()->shape(),\n+            HloInstruction::FusionKind::kCustom, fusion_inputs, computation));\n \n     TF_ASSIGN_OR_RETURN(auto gpu_config,\n                         fusion->backend_config<GpuBackendConfig>());"
        }
    ],
    "stats": {
        "total": 144,
        "additions": 116,
        "deletions": 28
    }
}