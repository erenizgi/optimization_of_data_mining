{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Remove combined fission backend as we have per codegen fission now.\n\nPiperOrigin-RevId: 840186626",
    "sha": "3d4217a49d34e3939267fe0e23ff050bf8b0d8b2",
    "files": [
        {
            "sha": "312ec4a5cff0341dbd39ac6c287f1afc2ac06d1b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 79,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d4217a49d34e3939267fe0e23ff050bf8b0d8b2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d4217a49d34e3939267fe0e23ff050bf8b0d8b2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=3d4217a49d34e3939267fe0e23ff050bf8b0d8b2",
            "patch": "@@ -414,85 +414,6 @@ xla_test(\n     ],\n )\n \n-cc_library(\n-    name = \"fission\",\n-    srcs = [\"fission.cc\"],\n-    hdrs = [\"fission.h\"],\n-    tags = [\"gpu\"],\n-    deps = [\n-        \":cublas\",\n-        \":cublaslt\",\n-        \":custom_kernel\",\n-        \":gpu_codegen_backend\",\n-        \"//xla:autotuning_proto_cc\",\n-        \"//xla:util\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/utils:hlo_query\",\n-        \"//xla/service:call_inliner\",\n-        \"//xla/service:compiler\",\n-        \"//xla/service:hlo_cost_analysis\",\n-        \"//xla/service:hlo_module_config\",\n-        \"//xla/service:hlo_proto_cc\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu:cublas_cudnn\",\n-        \"//xla/service/gpu:hlo_fusion_analysis\",\n-        \"//xla/service/gpu:ir_emission_utils\",\n-        \"//xla/service/gpu/transforms:custom_kernel_fusion_rewriter\",\n-        \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n-        \"//xla/service/gpu/transforms:gemm_rewriter\",\n-        \"//xla/service/gpu/transforms:priority_fusion\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/tools:hlo_decomposer_lib\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//mlir:IR\",\n-    ],\n-)\n-\n-xla_test(\n-    name = \"fission_test\",\n-    srcs = [\"fission_test.cc\"],\n-    backends = [\n-        \"a100\",\n-        \"h100\",\n-        \"b200\",\n-    ],\n-    tags = [\n-        \"cuda-only\",\n-        \"no_mac\",\n-    ],\n-    deps = [\n-        \":fission\",\n-        \"//xla:autotuning_proto_cc\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/testlib:filecheck\",\n-        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/service:compiler\",\n-        \"//xla/service:platform_util\",\n-        \"//xla/service/gpu:nvptx_compiler_impl\",\n-        \"//xla/stream_executor:device_description_proto_cc\",\n-        \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:status_matchers\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@llvm-project//mlir:IR\",\n-    ],\n-)\n-\n cc_library(\n     name = \"triton\",\n     srcs = [\"triton.cc\"],"
        },
        {
            "sha": "8953ad72ec566ee12fcdf719f44ce8e42fa13694",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 369,
            "changes": 369,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -1,369 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/gpu/autotuner/fission.h\"\n-\n-#include <iterator>\n-#include <memory>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/autotuning.pb.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/backends/gpu/autotuner/cublas.h\"\n-#include \"xla/backends/gpu/autotuner/cublaslt.h\"\n-#include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/utils/hlo_query.h\"\n-#include \"xla/service/call_inliner.h\"\n-#include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/cublas_cudnn.h\"\n-#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/service/gpu/transforms/custom_kernel_fusion_rewriter.h\"\n-#include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n-#include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n-#include \"xla/service/gpu/transforms/priority_fusion.h\"\n-#include \"xla/service/hlo.pb.h\"\n-#include \"xla/service/hlo_cost_analysis.h\"\n-#include \"xla/service/hlo_module_config.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tools/hlo_decomposer.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-namespace se = ::stream_executor;\n-\n-using ::mlir::MLIRContext;\n-\n-using CublasOrCublasLtBackendConfig = AutotuneResult::GemmKey;\n-using CustomKernelBackendConfig = AutotuneResult::CustomKernelFusionKey;\n-\n-namespace {\n-HloCostAnalysis::Options PriorityFusionOptions() {\n-  // The real pointer size is set in GpuCompiler. In HloCostAnalysis, the\n-  // pointer size is used only to determine the size of tuple types. We\n-  // shouldn't have any tuples in the autotuned module, so it's safe to use\n-  // the default value here, instead of piping the real value.\n-  return {.count_multiple_input_accesses = true};\n-}\n-}  // namespace\n-\n-// Unfuses a fusion instruction and rewrites it to using a cublas or cublasLt\n-// custom call for the dot operation.\n-// If rewrite_to_cublaslt is true, we will try to rewrite the dot to a cublasLt\n-// custom call, otherwise we will try to rewrite it to a cublas custom call.\n-absl::Status FissionToCublas(HloModule* hlo_module,\n-                             const se::DeviceDescription& device_description,\n-                             bool rewrite_to_cublaslt,\n-                             MLIRContext* mlir_context) {\n-  hlo_module->mutable_config()\n-      .mutable_debug_options()\n-      .set_xla_gpu_enable_cublaslt(rewrite_to_cublaslt);\n-\n-  HloInstruction* dot = nullptr;\n-  bool has_dot = false;\n-  for (HloComputation* computation : hlo_module->computations()) {\n-    dot =\n-        hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n-    if (dot != nullptr) {\n-      // Substitute algorithms, which are not supported by cuBLAS for the check,\n-      // but don't use cuBlas in the end. This assumes that the substituting\n-      // algorithm has result which are close enough for the check in this file.\n-      if (dot->precision_config().algorithm() ==\n-          PrecisionConfig::ALG_DOT_TF32_TF32_F32_X3) {\n-        dot->mutable_precision_config()->set_algorithm(\n-            PrecisionConfig::ALG_DOT_F32_F32_F32);\n-      }\n-      has_dot = true;\n-    }\n-  }\n-\n-  if (!has_dot) {\n-    return absl::InvalidArgumentError(\n-        \"Fission to cuBLAS failed because no dot instruction found.\");\n-  }\n-\n-  bool is_rewritten_to_cublas_custom_call = false;\n-  for (GemmRewriterOptions::DType dtype :\n-       {GemmRewriterOptions::DType::kFp8Only,\n-        GemmRewriterOptions::DType::kNonFp8Only}) {\n-    DotAlgorithmRewriter dot_algorithm_rewriter;\n-\n-    TF_RETURN_IF_ERROR(dot_algorithm_rewriter.Run(hlo_module).status());\n-\n-    GemmRewriter gemm_rewriter(device_description.gpu_compute_capability(),\n-                               device_description.runtime_version(),\n-                               GemmRewriterOptions{dtype});\n-    TF_ASSIGN_OR_RETURN(bool changed, gemm_rewriter.Run(hlo_module));\n-    is_rewritten_to_cublas_custom_call |= changed;\n-\n-    PriorityFusion fusion_pass(\n-        /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n-        mlir_context);\n-    TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n-  }\n-\n-  if (is_rewritten_to_cublas_custom_call) {\n-    return absl::OkStatus();\n-  }\n-\n-  return absl::InvalidArgumentError(\"Failed to rewrite fusion to cuBLAS.\");\n-}\n-\n-absl::Status FissionToCustomKernel(\n-    HloModule* hlo_module, const se::DeviceDescription& device_description,\n-    MLIRContext* mlir_context) {\n-  CustomKernelFusionRewriter custom_kernel_fusion_rewriter(&device_description);\n-  PriorityFusion fusion_pass(\n-      /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n-      mlir_context);\n-  TF_ASSIGN_OR_RETURN(bool is_rewritten_to_custom_kernel,\n-                      custom_kernel_fusion_rewriter.Run(hlo_module));\n-  TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n-\n-  if (is_rewritten_to_custom_kernel) {\n-    return absl::OkStatus();\n-  }\n-\n-  return absl::InvalidArgumentError(\n-      \"Failed to rewrite fusion to custom kernel.\");\n-}\n-\n-absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> GetCublasConfigs(\n-    CublasBackend& cublas_backend, std::unique_ptr<HloModule> module,\n-    se::StreamExecutor* stream_executor) {\n-  std::vector<std::unique_ptr<BackendConfig>> configs;\n-\n-  for (HloComputation* computation : module->MakeNonfusionComputations()) {\n-    for (HloInstruction* instruction : computation->instructions()) {\n-      if (IsLegacyCublasMatmul(*instruction)) {\n-        TF_ASSIGN_OR_RETURN(configs,\n-                            cublas_backend.GetSupportedConfigs(*instruction));\n-        return configs;\n-      }\n-    }\n-  }\n-\n-  return configs;\n-}\n-\n-absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> GetCublasLtConfigs(\n-    CublasLtBackend& cublaslt_backend, std::unique_ptr<HloModule> module,\n-    se::StreamExecutor* stream_executor) {\n-  std::vector<std::unique_ptr<BackendConfig>> configs;\n-\n-  for (HloComputation* computation : module->MakeNonfusionComputations()) {\n-    for (HloInstruction* instruction : computation->instructions()) {\n-      if (IsCublasLtMatmul(*instruction) || IsCublasLtMatmulF8(*instruction)) {\n-        TF_ASSIGN_OR_RETURN(configs,\n-                            cublaslt_backend.GetSupportedConfigs(*instruction));\n-        return configs;\n-      }\n-    }\n-  }\n-\n-  return configs;\n-}\n-\n-bool IsCustomKernel(const HloComputation* computation) {\n-  if (!computation->IsFusionComputation()) {\n-    return false;\n-  }\n-\n-  HloInstruction* instruction = computation->FusionInstruction();\n-  absl::StatusOr<GpuBackendConfig> gpu_backend_config =\n-      instruction->backend_config<GpuBackendConfig>();\n-  if (!gpu_backend_config.ok()) {\n-    return false;\n-  }\n-\n-  if (instruction->fusion_kind() != HloInstruction::FusionKind::kCustom) {\n-    return false;\n-  }\n-\n-  return IsGpuFusionKind(*instruction, kCustomFusionKind);\n-}\n-\n-absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n-GetCustomKernelConfigs(CustomKernelBackend& custom_kernel_backend,\n-                       std::unique_ptr<HloModule> hlo_module,\n-                       se::StreamExecutor* stream_executor) {\n-  std::vector<std::unique_ptr<BackendConfig>> configs;\n-\n-  for (HloComputation* computation : hlo_module->computations()) {\n-    if (IsCustomKernel(computation)) {\n-      TF_ASSIGN_OR_RETURN(configs, custom_kernel_backend.GetSupportedConfigs(\n-                                       *computation->FusionInstruction()));\n-    }\n-  }\n-\n-  return configs;\n-}\n-\n-absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n-FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n-  if (instr.opcode() != HloOpcode::kFusion) {\n-    return absl::InvalidArgumentError(\"Not a fusion instruction.\");\n-  }\n-\n-  const HloFusionInstruction* fusion = DynCast<HloFusionInstruction>(&instr);\n-  const HloComputation* fusion_computation = fusion->called_computation();\n-  std::unique_ptr<HloModule> hlo_module =\n-      ExtractComputationIntoNewModule(*fusion_computation);\n-\n-  std::vector<std::unique_ptr<BackendConfig>> configs;\n-  std::unique_ptr<HloModule> cublas_hlo_module = hlo_module->Clone();\n-  if (FissionToCublas(cublas_hlo_module.get(),\n-                      target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/false, mlir_context_)\n-          .ok()) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::vector<std::unique_ptr<BackendConfig>> cublas_configs,\n-        GetCublasConfigs(cublas_backend_, std::move(cublas_hlo_module),\n-                         stream_executor()));\n-    VLOG(2) << \"Found \" << cublas_configs.size() << \" cublas configs.\";\n-    configs.insert(configs.end(),\n-                   std::make_move_iterator(cublas_configs.begin()),\n-                   std::make_move_iterator(cublas_configs.end()));\n-  }\n-\n-  std::unique_ptr<HloModule> cublaslt_hlo_module = hlo_module->Clone();\n-  if (FissionToCublas(cublaslt_hlo_module.get(),\n-                      target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/true, mlir_context_)\n-          .ok()) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::vector<std::unique_ptr<BackendConfig>> cublaslt_configs,\n-        GetCublasLtConfigs(cublaslt_backend_, std::move(cublaslt_hlo_module),\n-                           stream_executor()));\n-    VLOG(2) << \"Found \" << cublaslt_configs.size() << \" cublasLt configs.\";\n-    configs.insert(configs.end(),\n-                   std::make_move_iterator(cublaslt_configs.begin()),\n-                   std::make_move_iterator(cublaslt_configs.end()));\n-  }\n-\n-  std::unique_ptr<HloModule> custom_kernel_hlo_module = hlo_module->Clone();\n-  if (FissionToCustomKernel(custom_kernel_hlo_module.get(),\n-                            target_config().device_description, mlir_context_)\n-          .ok()) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::vector<std::unique_ptr<BackendConfig>> custom_kernel_configs,\n-        GetCustomKernelConfigs(custom_kernel_backend_,\n-                               std::move(custom_kernel_hlo_module),\n-                               stream_executor()));\n-    VLOG(2) << \"Found \" << custom_kernel_configs.size()\n-            << \" custom kernel configs. \";\n-    configs.insert(configs.end(),\n-                   std::make_move_iterator(custom_kernel_configs.begin()),\n-                   std::make_move_iterator(custom_kernel_configs.end()));\n-  }\n-\n-  return configs;\n-}\n-\n-absl::StatusOr<std::unique_ptr<BackendConfig>> FissionBackend::GetDefaultConfig(\n-    const HloInstruction& instr) {\n-  return absl::InvalidArgumentError(\n-      \"FissionBackend doesn't support getting a default config.\");\n-}\n-\n-absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n-                                         const BackendConfig& config) {\n-  if (instr.opcode() != HloOpcode::kFusion) {\n-    return absl::InvalidArgumentError(\"Not a fusion instruction.\");\n-  }\n-\n-  // Inline the fusion into a new module.\n-  HloComputation* computation = instr.parent();\n-  HloInstruction* call = computation->AddInstruction(HloInstruction::CreateCall(\n-      instr.shape(), instr.operands(), instr.fused_instructions_computation()));\n-  TF_RETURN_IF_ERROR(computation->ReplaceInstruction(&instr, call));\n-  HloModule* hlo_module = call->GetModule();\n-\n-  CallInliner call_inliner(\n-      /*single_call_site=*/false, /*update_domain=*/false,\n-      /*composites_to_preserve=*/absl::flat_hash_set<std::string>(),\n-      /*uniquify_channel_ids=*/true);\n-  TF_RETURN_IF_ERROR(call_inliner.Run(hlo_module).status());\n-\n-  bool use_cublaslt =\n-      computation->parent()->config().debug_options().xla_gpu_enable_cublaslt();\n-\n-  if (!use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(\n-        FissionToCublas(hlo_module, target_config().device_description,\n-                        /*rewrite_to_cublaslt=*/false, mlir_context_));\n-    for (HloComputation* computation :\n-         hlo_module->MakeNonfusionComputations()) {\n-      for (HloInstruction* instruction : computation->instructions()) {\n-        if (IsLegacyCublasMatmul(*instruction)) {\n-          TF_RETURN_IF_ERROR(cublas_backend_.ApplyConfig(*instruction, config));\n-        }\n-      }\n-    }\n-\n-    return absl::OkStatus();\n-  }\n-\n-  if (use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(\n-        FissionToCublas(hlo_module, target_config().device_description,\n-                        /*rewrite_to_cublaslt=*/true, mlir_context_));\n-    for (HloComputation* computation :\n-         hlo_module->MakeNonfusionComputations()) {\n-      for (HloInstruction* instruction : computation->instructions()) {\n-        if (IsCublasLtMatmul(*instruction) ||\n-            IsCublasLtMatmulF8(*instruction)) {\n-          TF_RETURN_IF_ERROR(\n-              cublaslt_backend_.ApplyConfig(*instruction, config));\n-        }\n-      }\n-    }\n-\n-    return absl::OkStatus();\n-  }\n-\n-  if (config.Is<CustomKernelBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(FissionToCustomKernel(\n-        hlo_module, target_config().device_description, mlir_context_));\n-    for (HloComputation* computation : hlo_module->computations()) {\n-      if (IsCustomKernel(computation)) {\n-        TF_RETURN_IF_ERROR(custom_kernel_backend_.ApplyConfig(\n-            *computation->FusionInstruction(), config));\n-      }\n-    }\n-\n-    return absl::OkStatus();\n-  }\n-  return absl::UnimplementedError(\"Not implemented.\");\n-}\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "1bb54ed13ab35054dfc443d65d0e1192c4a88986",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "removed",
            "additions": 0,
            "deletions": 82,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -1,82 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_GPU_AUTOTUNER_FISSION_H_\n-#define XLA_BACKENDS_GPU_AUTOTUNER_FISSION_H_\n-\n-#include <memory>\n-#include <vector>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/backends/gpu/autotuner/cublas.h\"\n-#include \"xla/backends/gpu/autotuner/cublaslt.h\"\n-#include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n-#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/service/compiler.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// The FissionBackend tries to unfuse a fusion instruction.\n-// The resulting 'configurations\" (HloModules) are equivalent to the original\n-// hlo graph but try to use a different backend for the dot operation: cublas,\n-// cublasLt, custom calls. If the CustomKernel registry matches a hlo\n-// subgraph, it will generate a config using the CustomKernel.\n-class FissionBackend : public GpuCodegenBackend {\n- public:\n-  explicit FissionBackend(stream_executor::StreamExecutor* stream_executor,\n-                          const DebugOptions* debug_options, Compiler* compiler,\n-                          const Compiler::GpuTargetConfig* target_config,\n-                          mlir::MLIRContext* mlir_context)\n-      : GpuCodegenBackend(\"Fission\", debug_options, compiler, target_config),\n-        cublas_backend_(stream_executor, debug_options, compiler,\n-                        target_config),\n-        cublaslt_backend_(stream_executor, debug_options, compiler,\n-                          target_config),\n-        custom_kernel_backend_(stream_executor, debug_options, compiler,\n-                               target_config),\n-        mlir_context_(mlir_context) {}\n-\n-  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n-  GetSupportedConfigs(const HloInstruction& instr) override;\n-\n-  absl::StatusOr<std::unique_ptr<BackendConfig>> GetDefaultConfig(\n-      const HloInstruction& instr) override;\n-\n-  absl::Status ApplyConfig(HloInstruction& instr,\n-                           const BackendConfig& config) override;\n-\n- private:\n-  bool IsSupported(const HloInstruction& instr) override {\n-    return instr.opcode() == HloOpcode::kFusion;\n-  }\n-\n-  CublasBackend cublas_backend_;\n-  CublasLtBackend cublaslt_backend_;\n-  CustomKernelBackend custom_kernel_backend_;\n-  mlir::MLIRContext* mlir_context_;\n-};\n-\n-}  // namespace gpu\n-}  // namespace xla\n-\n-#endif  // XLA_BACKENDS_GPU_AUTOTUNER_FISSION_H_"
        },
        {
            "sha": "56b8907c42c13bfeb46d13a202d09bff31e76fe8",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 263,
            "changes": 263,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -1,263 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/gpu/autotuner/fission.h\"\n-\n-#include <memory>\n-#include <string>\n-#include <vector>\n-\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-#include \"absl/status/status.h\"\n-#include \"absl/status/status_matchers.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"xla/autotuning.pb.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/testlib/filecheck.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/service/compiler.h\"\n-#include \"xla/service/gpu/nvptx_compiler.h\"\n-#include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_description.pb.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-namespace {\n-\n-using ::absl_testing::IsOkAndHolds;\n-using ::absl_testing::StatusIs;\n-using ::testing::SizeIs;\n-\n-const char kTritonFusionHlo[] = R\"(\n-  HloModule module\n-\n-  computation {\n-    p0 = bf16[1024,1024]{1,0} parameter(0)\n-    convert0 = f32[1024,1024]{1,0} convert(p0)\n-    p1 = s8[1024,1024]{1,0} parameter(1)\n-    convert1 = f32[1024,1024]{1,0} convert(p1)\n-    ROOT dot = f32[1024,1024]{1,0} dot(convert0, convert1),\n-        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-  }\n-\n-  ENTRY main {\n-    p0 = bf16[1024,1024]{1,0} parameter(0)\n-    p1 = s8[1024,1024]{1,0} parameter(1)\n-    ROOT fusion = f32[1024,1024]{1,0} fusion(p0, p1),\n-      kind=kCustom, calls=computation,\n-      backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n-  })\";\n-\n-class FissionBackendTest : public HloHardwareIndependentTestBase {\n- protected:\n-  DebugOptions debug_options_;\n-  NVPTXCompiler compiler_;\n-  se::StreamExecutor* stream_executor_;\n-  Compiler::GpuTargetConfig target_config_;\n-  FissionBackend backend_;\n-  mlir::MLIRContext mlir_context_;\n-\n-  FissionBackendTest()\n-      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n-                             .value()\n-                             ->ExecutorForDevice(0)\n-                             .value()),\n-        target_config_(stream_executor_),\n-        backend_(stream_executor_, &debug_options_, &compiler_, &target_config_,\n-                 &mlir_context_) {}\n-};\n-\n-TEST_F(FissionBackendTest, CanCreateCublasBackend) {\n-  ASSERT_NE(nullptr, &backend_);\n-}\n-\n-TEST_F(FissionBackendTest, GetSupportedConfigsFromCublasCustomCall) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n-  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n-      backend_.GetSupportedConfigs(\n-          (*module->entry_computation()->root_instruction()));\n-  EXPECT_THAT(configs, absl_testing::IsOkAndHolds(SizeIs(testing::Ge(2))));\n-  // The first config is the cublas config.\n-  AutotuneResult::GemmKey cublas_config;\n-  EXPECT_TRUE(configs.value().front()->UnpackTo(&cublas_config));\n-  EXPECT_EQ(cublas_config.algorithm(), -1);\n-  // The last config is the custom kernel config.\n-  AutotuneResult::CustomKernelFusionKey custom_kernel_config;\n-  EXPECT_TRUE(configs.value().back()->UnpackTo(&custom_kernel_config));\n-  EXPECT_EQ(custom_kernel_config.kernel_index(), 0);\n-}\n-\n-TEST_F(FissionBackendTest, GetSupportedConfigsForUnsupportedInstructionFails) {\n-  std::string hlo = R\"(\n-    HloModule module\n-\n-    ENTRY main {\n-      p0 = f32[1024,1024]{1,0} parameter(0)\n-      p1 = f32[1024,1024]{1,0} parameter(1)\n-      ROOT dot = f32[1024,1024]{1,0} dot(p0, p1),\n-          lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-    })\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n-      backend_.GetSupportedConfigs(\n-          (*module->entry_computation()->root_instruction()));\n-  EXPECT_THAT(configs.status(), StatusIs(absl::StatusCode::kInvalidArgument));\n-}\n-\n-TEST_F(FissionBackendTest, GetDefaultConfigFails) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n-\n-  absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n-      backend_.GetDefaultConfig(\n-          (*module->entry_computation()->root_instruction()));\n-  EXPECT_THAT(config.status(), StatusIs(absl::StatusCode::kInvalidArgument));\n-}\n-\n-TEST_F(FissionBackendTest, ApplyCublasConfigToFusionInstruction) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n-  hlo_module->mutable_config()\n-      .mutable_debug_options()\n-      .set_xla_gpu_enable_cublaslt(false);\n-  AutotuneResult::GemmKey config;\n-  config.set_algorithm(3);\n-  google::protobuf::Any any;\n-  any.PackFrom(config);\n-  TF_EXPECT_OK(backend_.ApplyConfig(\n-      *hlo_module->entry_computation()->root_instruction(), any));\n-  EXPECT_THAT(RunFileCheck(hlo_module->ToString(),\n-                           \"CHECK: \\\"__cublas$gemm\\\"\\n\"\n-                           \"CHECK: \\\"selected_algorithm\\\":\\\"3\\\"\"),\n-              IsOkAndHolds(true));\n-}\n-\n-TEST_F(FissionBackendTest, ApplyCublasLtConfigToFusionInstruction) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n-  hlo_module->mutable_config()\n-      .mutable_debug_options()\n-      .set_xla_gpu_enable_cublaslt(true);\n-  AutotuneResult::GemmKey config;\n-  config.set_algorithm(3);\n-  google::protobuf::Any any;\n-  any.PackFrom(config);\n-  TF_EXPECT_OK(backend_.ApplyConfig(\n-      *hlo_module->entry_computation()->root_instruction(), any));\n-  EXPECT_THAT(\n-      RunFileCheck(hlo_module->ToString(), \"CHECK: \\\"__cublas$lt$matmul\\\"\"),\n-      IsOkAndHolds(true));\n-}\n-\n-TEST_F(FissionBackendTest, ApplyCustomKernelConfigToFusionInstruction) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n-                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n-  AutotuneResult::CustomKernelFusionKey config;\n-  config.set_kernel_index(3);\n-  google::protobuf::Any any;\n-  any.PackFrom(config);\n-  TF_EXPECT_OK(backend_.ApplyConfig(\n-      *hlo_module->entry_computation()->root_instruction(), any));\n-  EXPECT_THAT(RunFileCheck(hlo_module->ToString(), \"CHECK: \\\"kernel_index\\\":3\"),\n-              IsOkAndHolds(true));\n-}\n-\n-TEST_F(FissionBackendTest, ApplyCublasConfigToFusionInWhileBody) {\n-  const char kWhileHlo[] = R\"(\n-HloModule module\n-\n-fusion_computation {\n-  fp0 = bf16[1024,1024]{1,0} parameter(0)\n-  convert0 = f32[1024,1024]{1,0} convert(fp0)\n-  fp1 = s8[1024,1024]{1,0} parameter(1)\n-  convert1 = f32[1024,1024]{1,0} convert(fp1)\n-  ROOT dot = f32[1024,1024]{1,0} dot(convert0, convert1),\n-      lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-}\n-\n-while_cond {\n-  cond_param = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) parameter(0)\n-  count = s32[] get-tuple-element(cond_param), index=0\n-  limit = s32[] constant(1)\n-  ROOT result = pred[] compare(count, limit), direction=LT\n-}\n-\n-while_body {\n-  body_param = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) parameter(0)\n-  count = s32[] get-tuple-element(body_param), index=0\n-  p0_body = bf16[1024,1024]{1,0} get-tuple-element(body_param), index=2\n-  p1_body = s8[1024,1024]{1,0} get-tuple-element(body_param), index=3\n-  fusion = f32[1024,1024]{1,0} fusion(p0_body, p1_body),\n-    kind=kCustom, calls=fusion_computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n-  one = s32[] constant(1)\n-  new_count = s32[] add(count, one)\n-  ROOT result = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) tuple(new_count, fusion, p0_body, p1_body)\n-}\n-\n-ENTRY main {\n-  p0 = bf16[1024,1024]{1,0} parameter(0)\n-  p1 = s8[1024,1024]{1,0} parameter(1)\n-  c0 = s32[] constant(0)\n-  init_f32 = f32[1024,1024]{1,0} broadcast(f32[] constant(0.0)), dimensions={}\n-  while_init = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) tuple(c0, init_f32, p0, p1)\n-  while_result = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) while(while_init),\n-    body=while_body, condition=while_cond\n-  ROOT result = f32[1024,1024]{1,0} get-tuple-element(while_result), index=1\n-}\n-  )\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n-                          ParseAndReturnVerifiedModule(kWhileHlo));\n-  hlo_module->mutable_config()\n-      .mutable_debug_options()\n-      .set_xla_gpu_enable_cublaslt(false);\n-\n-  HloInstruction* while_instr =\n-      hlo_module->entry_computation()->root_instruction()->mutable_operand(0);\n-  ASSERT_EQ(while_instr->opcode(), HloOpcode::kWhile);\n-  HloComputation* body_computation = while_instr->while_body();\n-  HloInstruction* fusion_instr =\n-      body_computation->root_instruction()->mutable_operand(1);\n-  ASSERT_EQ(fusion_instr->opcode(), HloOpcode::kFusion);\n-\n-  AutotuneResult::GemmKey config;\n-  config.set_algorithm(3);\n-  google::protobuf::Any any;\n-  any.PackFrom(config);\n-  TF_EXPECT_OK(backend_.ApplyConfig(*fusion_instr, any));\n-  EXPECT_THAT(\n-      RunFileCheck(\n-          hlo_module->ToString(),\n-          \"CHECK: while_body\"\n-          \"\\nCHECK: custom-call({{.*}}), custom_call_target=\\\"__cublas$gemm\\\"\"\n-          \"\\nCHECK: \\\"selected_algorithm\\\":\\\"3\\\"\"),\n-      IsOkAndHolds(true));\n-}\n-\n-}  // namespace\n-}  // namespace gpu\n-}  // namespace xla"
        }
    ],
    "stats": {
        "total": 793,
        "additions": 0,
        "deletions": 793
    }
}