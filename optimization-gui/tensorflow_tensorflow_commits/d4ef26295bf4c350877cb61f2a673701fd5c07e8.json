{
    "author": "junwhanahn",
    "message": "Replace IFRT futures/promises with TSL futures/promises\n\nIFRT futures/promises have been deprecated in favor of TSL futures/promises.\n\nPiperOrigin-RevId: 812879965",
    "sha": "d4ef26295bf4c350877cb61f2a673701fd5c07e8",
    "files": [
        {
            "sha": "e98453ff7358ba73cb0687cdbe5fd1527485d9e0",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -190,6 +190,7 @@ cc_library(\n         \"//xla/python/ifrt:test_util\",\n         \"//xla/python/ifrt:user_context\",\n         \"//xla/python/ifrt/hlo:hlo_program\",\n+        \"//xla/tsl/concurrency:future\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n@@ -248,15 +249,16 @@ cc_library(\n         \":internal\",\n     ]),\n     deps = [\n+        \":pjrt_dtype\",\n+        \":xla_ifrt\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo_sharding\",\n         \"//xla/pjrt:pjrt_layout\",\n         \"//xla/python/ifrt\",\n         \"//xla/python/ifrt:test_util\",\n-        \"//xla/python/pjrt_ifrt:pjrt_dtype\",\n-        \"//xla/python/pjrt_ifrt:xla_ifrt\",\n+        \"//xla/tsl/concurrency:future\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n@@ -347,6 +349,7 @@ cc_library(\n         \"//xla/service:computation_placer_hdr\",\n         \"//xla/service:global_device_id\",\n         \"//xla/service:hlo_proto_cc\",\n+        \"//xla/tsl/concurrency:future\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/distributed_runtime:call_options\",\n         \"//xla/tsl/distributed_runtime/coordination:coordination_service_agent\",\n@@ -551,6 +554,7 @@ cc_library(\n         \"//xla/pjrt:pjrt_layout\",\n         \"//xla/python/ifrt\",\n         \"//xla/python/ifrt:user_context\",\n+        \"//xla/tsl/concurrency:future\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -576,6 +580,7 @@ xla_cc_test(\n         \"//xla/pjrt:pjrt_future\",\n         \"//xla/python/ifrt\",\n         \"//xla/python/ifrt:test_util\",\n+        \"//xla/tsl/concurrency:future\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "8c909fe775f4c0fb31cd1495fd2f84a30db181ca",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -34,11 +34,11 @@ limitations under the License.\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/user_context.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -62,14 +62,14 @@ namespace ifrt {\n char BasicStringArray::ID = 0;\n \n absl::StatusOr<tsl::RCReference<BasicStringArray>> BasicStringArray::Create(\n-    Client* client, Shape shape, ShardingRef sharding, Future<Buffers> buffers,\n-    OnDoneWithBuffer on_done_with_buffer) {\n+    Client* client, Shape shape, ShardingRef sharding,\n+    tsl::Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer) {\n   if (!buffers.IsValid()) {\n     return absl::InvalidArgumentError(\"Got buffers_ future is invalid\");\n   }\n \n-  auto [buffers_promise, buffers_future] = Future<Buffers>::MakePromise();\n-  auto [ready_promise, ready_future] = Future<>::MakePromise();\n+  auto [buffers_promise, buffers_future] = tsl::Future<Buffers>::MakePromise();\n+  auto [ready_promise, ready_future] = tsl::Future<>::MakePromise();\n \n   // Buffers when the become ready must be consistent with the sharding. For\n   // instance, Buffers.size() (the number of per-shard spans of absl::Cords)\n@@ -112,8 +112,8 @@ absl::StatusOr<tsl::RCReference<BasicStringArray>> BasicStringArray::Create(\n \n BasicStringArray::BasicStringArray(Client* client, Shape shape,\n                                    ShardingRef sharding,\n-                                   Future<Buffers> buffers,\n-                                   Future<> ready_future,\n+                                   tsl::Future<Buffers> buffers,\n+                                   tsl::Future<> ready_future,\n                                    OnDoneWithBuffer on_done_with_buffer)\n     : client_(client),\n       shape_(std::move(shape)),\n@@ -125,9 +125,9 @@ BasicStringArray::BasicStringArray(Client* client, Shape shape,\n \n BasicStringArray::~BasicStringArray() { DeleteInternal(); }\n \n-Future<> BasicStringArray::Delete() {\n+tsl::Future<> BasicStringArray::Delete() {\n   DeleteInternal();\n-  return Future<>(absl::OkStatus());\n+  return tsl::Future<>(absl::OkStatus());\n }\n \n bool BasicStringArray::IsDeleted() const {\n@@ -146,11 +146,11 @@ void BasicStringArray::DeleteInternal() {\n   is_deleted_ = true;\n }\n \n-Future<> BasicStringArray::GetReadyFuture() const {\n+tsl::Future<> BasicStringArray::GetReadyFuture() const {\n   DCHECK(this);\n   absl::MutexLock lock(mu_);\n   if (is_deleted_) {\n-    return Future<>(\n+    return tsl::Future<>(\n         absl::FailedPreconditionError(\"Array has already been deleted\"));\n   }\n   return ready_future_;\n@@ -191,9 +191,9 @@ BasicStringArray::DisassembleIntoSingleDeviceArrays(\n   // are used to make the arrays. The promises and the per-shard stores\n   // are passed onto the OnReady callback that populates them when the buffers\n   // of the source array become ready.\n-  std::vector<Promise<Buffers>> buffer_promises;\n+  std::vector<tsl::Promise<Buffers>> buffer_promises;\n   buffer_promises.reserve(num_shards);\n-  std::vector<Future<Buffers>> buffer_futures;\n+  std::vector<tsl::Future<Buffers>> buffer_futures;\n   buffer_futures.reserve(num_shards);\n \n   struct PerShardStringStore {  // Data (strings) for a single shard.\n@@ -213,7 +213,7 @@ BasicStringArray::DisassembleIntoSingleDeviceArrays(\n \n   for (int i = 0; i < num_shards; ++i) {\n     std::tie(buffer_promises.emplace_back(), buffer_futures.emplace_back()) =\n-        Future<Buffers>::MakePromise();\n+        tsl::Future<Buffers>::MakePromise();\n \n     auto current_shard_strings = std::make_shared<PerShardStringStore>();\n     per_shard_strings.push_back(current_shard_strings);\n@@ -259,25 +259,25 @@ BasicStringArray::DisassembleIntoSingleDeviceArrays(\n   return arrays;\n }\n \n-Future<> BasicStringArray::CopyToHostBuffer(\n+tsl::Future<> BasicStringArray::CopyToHostBuffer(\n     void* data, std::optional<absl::Span<const int64_t>> byte_strides,\n     ArrayCopySemantics semantics) {\n   DCHECK(this);\n   absl::MutexLock lock(mu_);\n   if (is_deleted_) {\n-    return Future<>(\n+    return tsl::Future<>(\n         absl::FailedPreconditionError(\"Array has already been deleted\"));\n   }\n \n   if (sharding_->devices()->size() != 1) {\n-    return Future<>(absl::InvalidArgumentError(absl::StrFormat(\n+    return tsl::Future<>(absl::InvalidArgumentError(absl::StrFormat(\n         \"CopyToHostBuffer only supports single device string arrays. This \"\n         \"array has been sharded over %d devices.\",\n         sharding_->devices()->size())));\n   }\n \n   auto [copy_completion_promise, copy_completion_future] =\n-      Future<>::MakePromise();\n+      tsl::Future<>::MakePromise();\n \n   buffers_.OnReady(\n       [copy_completion_promise = std::move(copy_completion_promise),\n@@ -329,7 +329,7 @@ absl::StatusOr<ArrayRef> BasicStringArray::Copy(\n \n   auto string_store = std::make_shared<StringStore>();\n   auto on_done_with_buffer = [string_store]() {};\n-  auto [buffers_promise, buffers_future] = Future<Buffers>::MakePromise();\n+  auto [buffers_promise, buffers_future] = tsl::Future<Buffers>::MakePromise();\n \n   auto copier = [string_store = std::move(string_store),\n                  buffers_promise = std::move(buffers_promise)](\n@@ -378,7 +378,7 @@ absl::StatusOr<ArrayRef> BasicStringArray::FullyReplicatedShard(\n \n   auto string_store = std::make_shared<StringStore>();\n   auto on_done_with_buffer = [string_store]() {};\n-  auto [buffers_promise, buffers_future] = Future<Buffers>::MakePromise();\n+  auto [buffers_promise, buffers_future] = tsl::Future<Buffers>::MakePromise();\n \n   auto copier = [string_store = std::move(string_store),\n                  buffers_promise = std::move(buffers_promise)]("
        },
        {
            "sha": "4afc97604df3ac4812c1f34eb9eaad0843fe59e7",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array.h?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -36,11 +36,11 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/user_context.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n \n namespace xla {\n@@ -71,7 +71,7 @@ class BasicStringArray final\n   // in `sharding`.\n   static absl::StatusOr<tsl::RCReference<BasicStringArray>> Create(\n       Client* client, Shape shape, ShardingRef sharding,\n-      Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer);\n+      tsl::Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer);\n \n   ~BasicStringArray() override;\n \n@@ -115,7 +115,7 @@ class BasicStringArray final\n       SingleDeviceShardSemantics single_device_shard_semantics) override;\n \n   ABSL_MUST_USE_RESULT\n-  Future<> CopyToHostBuffer(\n+  tsl::Future<> CopyToHostBuffer(\n       void* data, std::optional<absl::Span<const int64_t>> byte_strides,\n       ArrayCopySemantics semantics) override;\n \n@@ -124,9 +124,9 @@ class BasicStringArray final\n       std::optional<xla::ifrt::MemoryKind> memory_kind,\n       ArrayCopySemantics semantics);\n \n-  Future<> GetReadyFuture() const override;\n+  tsl::Future<> GetReadyFuture() const override;\n \n-  Future<> Delete() override;\n+  tsl::Future<> Delete() override;\n   bool IsDeleted() const override;\n \n   std::string DebugString() const override;\n@@ -135,7 +135,7 @@ class BasicStringArray final\n \n   // Returns a future holding the string buffers underlying this array. Valid\n   // only while this Array object is alive.\n-  Future<Buffers> buffers() const {\n+  tsl::Future<Buffers> buffers() const {\n     return buffers_;  // Future copying is not considered expensive.\n   }\n \n@@ -146,7 +146,7 @@ class BasicStringArray final\n   friend tsl::RCReference<T> tsl::MakeRef(Args&&... args);\n \n   BasicStringArray(Client* client, Shape shape, ShardingRef sharding,\n-                   Future<Buffers> buffers, Future<> ready_future,\n+                   tsl::Future<Buffers> buffers, tsl::Future<> ready_future,\n                    OnDoneWithBuffer on_done_with_buffer);\n \n   // Internal implementation of delete.\n@@ -156,8 +156,8 @@ class BasicStringArray final\n   Shape shape_;\n   ShardingRef sharding_;\n   const UserContextRef user_context_;\n-  Future<Buffers> buffers_;\n-  Future<> ready_future_;\n+  tsl::Future<Buffers> buffers_;\n+  tsl::Future<> ready_future_;\n \n   mutable absl::Mutex mu_;\n   OnDoneWithBuffer on_done_with_buffer_ ABSL_GUARDED_BY(mu_);"
        },
        {
            "sha": "8fe3f79ca54047620edb62245da1e36f85e02700",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 29,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array_test.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -41,11 +41,11 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/test_util.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n@@ -70,7 +70,7 @@ using ::testing::HasSubstr;\n // factory method: `BasicStringArray::Create`. Uses the first device from the\n // `client->addressable_devices()`.\n absl::StatusOr<ArrayRef> CreateTestArray(\n-    Client* client, Future<BasicStringArray::Buffers> buffers,\n+    Client* client, tsl::Future<BasicStringArray::Buffers> buffers,\n     BasicStringArray::OnDoneWithBuffer on_done_with_buffer) {\n   Shape shape({1});\n   Device* device = client->addressable_devices().at(0);\n@@ -105,12 +105,12 @@ MakeBuffersAndOnDoneWithBuffer(\n // then they must ensure that the underlying strings live until the\n // `on-host-buffer-done` callback they provided is run.\n absl::StatusOr<std::pair<tsl::RCReference<BasicStringArray>,\n-                         Promise<BasicStringArray::Buffers>>>\n+                         tsl::Promise<BasicStringArray::Buffers>>>\n CreateNonReadyTestArray(\n     Client* client, Device* const device,\n     BasicStringArray::OnDoneWithBuffer on_done_with_buffer) {\n   auto [buffers_promise, buffers_future] =\n-      Future<BasicStringArray::Buffers>::MakePromise();\n+      tsl::Future<BasicStringArray::Buffers>::MakePromise();\n   Shape shape({1});\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -136,16 +136,17 @@ TEST(BasicStringArrayTest, CreateSuccess) {\n   // and that the destruction of the BasicStringArray object completes\n   // successfully (even when the callback is a nullptr).\n   TF_EXPECT_OK(CreateTestArray(client.get(),\n-                               Future<BasicStringArray::Buffers>(buffers),\n+                               tsl::Future<BasicStringArray::Buffers>(buffers),\n                                /*on_done_with_buffer=*/nullptr));\n }\n \n TEST(BasicStringArrayTest, CreateFailureWithInvalidFuture) {\n   TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n   // Create fails if with invalid future.\n-  EXPECT_THAT(CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(),\n-                              /*on_done_with_buffer=*/nullptr),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+  EXPECT_THAT(\n+      CreateTestArray(client.get(), tsl::Future<BasicStringArray::Buffers>(),\n+                      /*on_done_with_buffer=*/nullptr),\n+      absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n TEST(BasicStringArrayTest, Destruction) {\n@@ -159,13 +160,13 @@ TEST(BasicStringArrayTest, Destruction) {\n       [&on_done_with_buffer_called]() { on_done_with_buffer_called.Notify(); };\n \n   auto [array_creation_promise, array_creation_future] =\n-      Future<>::MakePromise();\n+      tsl::Future<>::MakePromise();\n \n   tsl::Env::Default()->SchedClosure(\n       ([&, promise = std::move(array_creation_promise)]() mutable {\n-        auto array = CreateTestArray(client.get(),\n-                                     Future<BasicStringArray::Buffers>(buffers),\n-                                     std::move(on_done_with_buffer));\n+        auto array = CreateTestArray(\n+            client.get(), tsl::Future<BasicStringArray::Buffers>(buffers),\n+            std::move(on_done_with_buffer));\n         promise.Set(array.status());\n         // `array` goes out of scope and gets destroyed.\n       }));\n@@ -223,7 +224,8 @@ TEST(BasicStringArrayTest, Delete) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto array,\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   tsl::Env::Default()->SchedClosure([&]() { array->Delete(); });\n@@ -239,7 +241,7 @@ TEST(GetReadyFutureTest, SuccessCase) {\n   TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n   // Make a BasicStringArray with a future that is not ready.\n   auto [promise, buffers_future] =\n-      Future<BasicStringArray::Buffers>::MakePromise();\n+      tsl::Future<BasicStringArray::Buffers>::MakePromise();\n   TF_ASSERT_OK_AND_ASSIGN(auto array,\n                           CreateTestArray(client.get(), buffers_future,\n                                           /*on_done_with_buffer=*/nullptr));\n@@ -260,7 +262,7 @@ TEST(GetReadyFutureTest, FailureCases) {\n   TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n   // Make a BasicStringArray with a future that is not ready.\n   auto [promise, buffers_future] =\n-      Future<BasicStringArray::Buffers>::MakePromise();\n+      tsl::Future<BasicStringArray::Buffers>::MakePromise();\n   TF_ASSERT_OK_AND_ASSIGN(auto array,\n                           CreateTestArray(client.get(), buffers_future,\n                                           /*on_done_with_buffer=*/nullptr));\n@@ -517,7 +519,7 @@ TEST(AssembleArrayFromSingleDeviceArraysTest,\n \n   // Make two non-ready single device sharded arrays.\n   std::vector<ArrayRef> arrays;\n-  std::vector<Promise<BasicStringArray::Buffers>> promises;\n+  std::vector<tsl::Promise<BasicStringArray::Buffers>> promises;\n   arrays.reserve(2);\n   auto buf_and_on_done_with_buffer = MakeBuffersAndOnDoneWithBuffer({\"abc\"});\n   auto buffers0 = buf_and_on_done_with_buffer.first;\n@@ -569,7 +571,7 @@ TEST(AssembleArrayFromSingleDeviceArraysTest,\n \n   // Make two non-ready single device sharded arrays.\n   std::vector<ArrayRef> arrays;\n-  std::vector<Promise<BasicStringArray::Buffers>> promises;\n+  std::vector<tsl::Promise<BasicStringArray::Buffers>> promises;\n   arrays.reserve(2);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -620,7 +622,8 @@ TEST(DisassembleArrayIntoSingleDeviceArrays,\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto array,\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto disassembled_arrays,\n@@ -670,7 +673,8 @@ TEST(DisassembleArrayIntoSingleDeviceArrays, FailsIfTheArrayHasBeenDeleted) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto array,\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   array->Delete();\n@@ -690,7 +694,8 @@ TEST(CopyTest, SuccessSingleDeviceShardedArray) {\n   std::vector<ArrayRef> arrays;\n   TF_ASSERT_OK_AND_ASSIGN(\n       arrays.emplace_back(),\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   // CreateTestArray above would place the array on the first device. Use the\n@@ -759,7 +764,8 @@ TEST(CopyTest, FailsAfterDeletion) {\n   std::vector<ArrayRef> arrays;\n   TF_ASSERT_OK_AND_ASSIGN(\n       arrays.emplace_back(),\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   arrays[0]->Delete();\n@@ -780,7 +786,8 @@ TEST(CopyTest, FailsWithDifferentNumbersDevices) {\n   std::vector<ArrayRef> arrays;\n   TF_ASSERT_OK_AND_ASSIGN(\n       arrays.emplace_back(),\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   TF_ASSERT_OK_AND_ASSIGN(DeviceListRef device_list,\n@@ -879,7 +886,8 @@ TEST(FullyReplicatedShardTest, SuccessSingleDeviceShardedArray) {\n       MakeBuffersAndOnDoneWithBuffer({kContents});\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto array,\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -924,7 +932,8 @@ TEST(FullyReplicatedShardTest, FailsAfterDeletion) {\n       MakeBuffersAndOnDoneWithBuffer({kContents});\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto array,\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   array->Delete();\n@@ -940,10 +949,10 @@ TEST(LayoutTest, Success) {\n   auto [buffers, on_done_with_buffer] =\n       MakeBuffersAndOnDoneWithBuffer({kContents});\n   TF_ASSERT_OK_AND_ASSIGN(\n-      auto array,\n-      CreateTestArray(client.get(),\n-                      Future<BasicStringArray::Buffers>(std::move(buffers)),\n-                      std::move(on_done_with_buffer)));\n+      auto array, CreateTestArray(client.get(),\n+                                  tsl::Future<BasicStringArray::Buffers>(\n+                                      std::move(buffers)),\n+                                  std::move(on_done_with_buffer)));\n }\n \n TEST(LayoutTest, FailsAfterDeletion) {\n@@ -954,7 +963,8 @@ TEST(LayoutTest, FailsAfterDeletion) {\n       MakeBuffersAndOnDoneWithBuffer({kContents});\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto array,\n-      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+      CreateTestArray(client.get(),\n+                      tsl::Future<BasicStringArray::Buffers>(buffers),\n                       std::move(on_done_with_buffer)));\n \n   array->Delete();"
        },
        {
            "sha": "baa0a1d3b71db6c4d42034d1e110c252f0ef56e6",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -42,7 +42,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n@@ -53,6 +52,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_memory.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -344,26 +344,26 @@ PjRtArray::DisassembleIntoSingleDeviceArrays(\n   return result;\n }\n \n-Future<> PjRtArray::CopyToHostBuffer(\n+tsl::Future<> PjRtArray::CopyToHostBuffer(\n     void* data, std::optional<absl::Span<const int64_t>> byte_strides,\n     ArrayCopySemantics semantics) {\n   DCHECK(this);\n   if (sharding_->devices()->size() != 1) {\n     if (sharding_->IsFullyReplicated()) {\n       absl::StatusOr<ArrayRef> replicated = FullyReplicatedShard(semantics);\n       if (!replicated.ok()) {\n-        return Future<>(std::move(replicated).status());\n+        return tsl::Future<>(std::move(replicated).status());\n       }\n       return (*replicated)->CopyToHostBuffer(data, byte_strides, semantics);\n     }\n-    return Future<>(\n+    return tsl::Future<>(\n         InvalidArgument(\"Only single-shard is implemented, but got %d\",\n                         sharding_->devices()->size()));\n   }\n \n   auto dtype = ToPrimitiveType(dtype_);\n   if (!dtype.ok()) {\n-    return Future<>(std::move(dtype).status());\n+    return tsl::Future<>(std::move(dtype).status());\n   }\n \n   PjRtBuffer* pjrt_buffer = pjrt_buffers_.front().get();\n@@ -380,7 +380,7 @@ Future<> PjRtArray::CopyToHostBuffer(\n     // TODO(b/314805296): Use the new dynamic shape here.\n     logical_dims = pjrt_buffer->logical_dimensions();\n     if (!logical_dims.ok()) {\n-      return Future<>(std::move(logical_dims).status());\n+      return tsl::Future<>(std::move(logical_dims).status());\n     }\n     dims = *logical_dims;\n   }\n@@ -390,7 +390,7 @@ Future<> PjRtArray::CopyToHostBuffer(\n     auto xla_shape =\n         MakeShapeWithTrivialByteStrides(*dtype, dims, *byte_strides);\n     if (!xla_shape.ok()) {\n-      return Future<>(std::move(xla_shape).status());\n+      return tsl::Future<>(std::move(xla_shape).status());\n     }\n     literal = std::make_unique<xla::MutableBorrowingLiteral>(\n         static_cast<char*>(data), *xla_shape);\n@@ -400,7 +400,7 @@ Future<> PjRtArray::CopyToHostBuffer(\n         static_cast<char*>(data), xla_shape);\n   }\n   auto* literal_ptr = literal.get();\n-  auto [promise, future] = Future<>::MakePromise();\n+  auto [promise, future] = tsl::Future<>::MakePromise();\n   // TODO(hyeontaek): Handle semantics == kDonateInput.\n   pjrt_buffer->ToLiteral(literal_ptr)\n       .OnReady([literal = std::move(literal),\n@@ -545,27 +545,27 @@ absl::StatusOr<ArrayRef> PjRtArray::Copy(\n       shape_);\n }\n \n-Future<> PjRtArray::GetReadyFuture() const {\n+tsl::Future<> PjRtArray::GetReadyFuture() const {\n   DCHECK(this);\n   if (pjrt_buffers_.size() == 1) {\n     return pjrt_buffers_.front()->GetReadyFuture();\n   }\n-  std::vector<Future<>> futures;\n+  std::vector<tsl::Future<>> futures;\n   futures.reserve(pjrt_buffers_.size());\n   for (auto& buf : pjrt_buffers_) {\n     futures.push_back(buf->GetReadyFuture());\n   }\n   return JoinFutures(absl::MakeSpan(futures));\n }\n \n-Future<> PjRtArray::Delete() {\n+tsl::Future<> PjRtArray::Delete() {\n   DCHECK(this);\n   for (auto& buffer : pjrt_buffers_) {\n     buffer->Delete();\n   }\n   is_deleted_ = true;\n   // TODO(hyeontaek): Return a correct future.\n-  return Future<>(absl::OkStatus());\n+  return tsl::Future<>(absl::OkStatus());\n }\n \n bool PjRtArray::IsDeleted() const {"
        },
        {
            "sha": "6145f1b39741279a4fd559281fb8429cfdb590fc",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.h?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -34,12 +34,12 @@ limitations under the License.\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/user_context.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_client.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n \n namespace xla {\n@@ -165,7 +165,7 @@ class PjRtArray final\n       SingleDeviceShardSemantics single_device_shard_semantics) override;\n \n   ABSL_MUST_USE_RESULT\n-  Future<> CopyToHostBuffer(\n+  tsl::Future<> CopyToHostBuffer(\n       void* data, std::optional<absl::Span<const int64_t>> byte_strides,\n       ArrayCopySemantics semantics) override;\n \n@@ -174,12 +174,12 @@ class PjRtArray final\n       std::optional<xla::ifrt::MemoryKind> memory_kind,\n       ArrayCopySemantics semantics);\n \n-  Future<> GetReadyFuture() const override;\n+  tsl::Future<> GetReadyFuture() const override;\n \n   std::shared_ptr<PjRtBuffer> GetPjRtBuffer(ArrayCopySemantics semantics,\n                                             int index) const;\n \n-  Future<> Delete() override;\n+  tsl::Future<> Delete() override;\n   bool IsDeleted() const override;\n \n   std::string DebugString() const override;"
        },
        {
            "sha": "830a5d5e5fb0dc64c9466955bb463ec08f22e675",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -65,7 +65,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/remap_plan.h\"\n #include \"xla/python/ifrt/shape.h\"\n@@ -88,6 +87,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/distributed_runtime/call_options.h\"\n #include \"xla/tsl/distributed_runtime/coordination/coordination_service_agent.h\"\n@@ -605,7 +605,7 @@ absl::StatusOr<ArrayRef> MakeStringArrayFromHostBuffer(\n \n   return BasicStringArray::Create(\n       client, std::move(shape), std::move(sharding),\n-      Future<BasicStringArray::Buffers>(std::move(buffers)),\n+      tsl::Future<BasicStringArray::Buffers>(std::move(buffers)),\n       std::move(buffer_releaser));\n }\n \n@@ -658,7 +658,7 @@ absl::StatusOr<ArrayRef> AssembleStringArrayFromSingleDeviceStringArrays(\n       arrays.size(), std::move(buffer_backing_store));\n \n   auto [buffers_promise, buffers_future] =\n-      Future<BasicStringArray::Buffers>::MakePromise();\n+      tsl::Future<BasicStringArray::Buffers>::MakePromise();\n \n   auto buffer_copier = [state = buffer_copying_state,\n                         promise = std::move(buffers_promise).ToShared()](\n@@ -1077,11 +1077,11 @@ absl::StatusOr<std::vector<ArrayRef>> PjRtClient::MakeErrorArrays(\n   arrays.reserve(array_specs.size());\n   for (const auto& array_spec : array_specs) {\n     if (array_spec.dtype.kind() == DType::kString) {\n-      TF_ASSIGN_OR_RETURN(\n-          arrays.emplace_back(),\n-          BasicStringArray::Create(this, array_spec.shape, array_spec.sharding,\n-                                   Future<BasicStringArray::Buffers>(error),\n-                                   /*on_done_with_buffer=*/[]() {}));\n+      TF_ASSIGN_OR_RETURN(arrays.emplace_back(),\n+                          BasicStringArray::Create(\n+                              this, array_spec.shape, array_spec.sharding,\n+                              tsl::Future<BasicStringArray::Buffers>(error),\n+                              /*on_done_with_buffer=*/[]() {}));\n       continue;\n     }\n \n@@ -1507,7 +1507,7 @@ absl::Status PjRtClient::CrossHostSendBuffers(\n   // TODO(emilyaf): Use an async version of KeyValueStore::Get or query batched\n   // keys together to reduce the number of threads used.\n   for (int i = 0; i < keys.size(); ++i) {\n-    auto [promise, descriptor_future] = Future<std::string>::MakePromise();\n+    auto [promise, descriptor_future] = tsl::Future<std::string>::MakePromise();\n     work_queue_->Schedule(\n         [this, k = keys[i], promise = std::move(promise).ToShared()]() mutable {\n           std::string key = absl::StrCat(kKeyPrefix, k);\n@@ -1596,8 +1596,8 @@ absl::StatusOr<std::vector<xla::ifrt::ArrayRef>> PjRtClient::ReshardArrays(\n   return Unimplemented(\"ReshardArrays not available with pjrt-ifrt client.\");\n }\n \n-Future<> PjRtClient::GetReadyFuture(absl::Span<const ValueRef> values) {\n-  absl::InlinedVector<Future<>, 1> futures;\n+tsl::Future<> PjRtClient::GetReadyFuture(absl::Span<const ValueRef> values) {\n+  absl::InlinedVector<tsl::Future<>, 1> futures;\n   futures.reserve(values.size());\n   for (const auto& value : values) {\n     futures.push_back(value->GetReadyFuture());"
        },
        {
            "sha": "5116e1e0921af30a3e71005fc8beec2354358add",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.h?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -51,7 +51,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/remap_plan.h\"\n #include \"xla/python/ifrt/shape.h\"\n@@ -63,6 +62,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_compiler.h\"\n #include \"xla/python/pjrt_ifrt/transfer_server_interface.h\"\n #include \"xla/shape.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -224,7 +224,7 @@ class PjRtClient final\n       absl::Span<ArrayRef> arrays, absl::Span<const ArraySpec> specs,\n       ArrayCopySemantics semantics) override;\n \n-  Future<> GetReadyFuture(absl::Span<const ValueRef> values) override;\n+  tsl::Future<> GetReadyFuture(absl::Span<const ValueRef> values) override;\n \n   absl::StatusOr<tsl::RCReference<Tuple>> MakeTuple(\n       absl::Span<ValueRef> values) override;"
        },
        {
            "sha": "17d56898a0a9df8e594ef2ba1883256b8705bc2b",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -51,7 +51,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n #include \"xla/python/ifrt/executable.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/host_callback.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/shape.h\"\n@@ -69,6 +68,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -657,9 +657,9 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n \n   // Execute the computation.\n   std::vector<std::vector<std::unique_ptr<PjRtBuffer>>> pjrt_outputs;\n-  xla::ifrt::Future<> status;\n+  tsl::Future<> status;\n   if (portable_execution) {\n-    std::optional<Future<>> returned_pjrt_future;\n+    std::optional<tsl::Future<>> returned_pjrt_future;\n     TF_RET_CHECK(portable_execution_device->IsAddressable());\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<PjRtBuffer>> single_device_pjrt_results,\n@@ -671,7 +671,7 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n     pjrt_outputs.push_back(std::move(single_device_pjrt_results));\n     status = *std::move(returned_pjrt_future);\n   } else {\n-    std::optional<std::vector<Future<>>> returned_pjrt_futures;\n+    std::optional<std::vector<tsl::Future<>>> returned_pjrt_futures;\n     returned_pjrt_futures.emplace();\n \n     TF_ASSIGN_OR_RETURN("
        },
        {
            "sha": "3e68e21dff1959f5ae3820b47c812b7c5f10d188",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -39,14 +39,14 @@ limitations under the License.\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n #include \"xla/python/ifrt/executable.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/host_callback.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/user_context.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_attribute_map_util.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_client.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_host_callback.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -230,10 +230,10 @@ class PjRtLoadedExecutable final\n \n   UserContextRef user_context() const override { return user_context_; }\n \n-  Future<> GetReadyFuture() const override {\n+  tsl::Future<> GetReadyFuture() const override {\n     // PjRtCompiler blocks until compilation finishes and returns only the\n     // executables that are ready.\n-    return Future<>(absl::OkStatus());\n+    return tsl::Future<>(absl::OkStatus());\n   }\n \n   std::optional<std::vector<OpSharding>> GetParameterShardings()"
        },
        {
            "sha": "bbbbed349c9c6d83dd4e389dae4bee76b4a0ecc2",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_tuple.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_tuple.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_tuple.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_tuple.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"llvm/Support/ExtensibleRTTI.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/client.h\"\n-#include \"xla/python/ifrt/future.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n \n namespace xla {\n@@ -40,23 +40,23 @@ namespace ifrt {\n   return tsl::MakeRef<PjRtTuple>(client, values);\n }\n \n-Future<> PjRtTuple::GetReadyFuture() const {\n-  std::vector<Future<>> futures;\n+tsl::Future<> PjRtTuple::GetReadyFuture() const {\n+  std::vector<tsl::Future<>> futures;\n   futures.reserve(values_.size());\n   for (const auto& value : values_) {\n     futures.push_back(value->GetReadyFuture());\n   }\n   return JoinFutures(absl::MakeSpan(futures));\n }\n \n-Future<> PjRtTuple::Delete() {\n+tsl::Future<> PjRtTuple::Delete() {\n   {\n     absl::MutexLock lock(mu_);\n     if (!is_deleted_.HasBeenNotified()) {\n       is_deleted_.Notify();\n     }\n   }\n-  std::vector<Future<>> futures;\n+  std::vector<tsl::Future<>> futures;\n   futures.reserve(values_.size());\n   for (const auto& value : values_) {\n     futures.push_back(value->Delete());"
        },
        {
            "sha": "5ac0093ffd57340c71ff76c8f6b613cbc41990a7",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_tuple.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_tuple.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_tuple.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_tuple.h?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -55,9 +55,9 @@ class PjRtTuple final : public llvm::RTTIExtends<PjRtTuple, Tuple> {\n     return {};\n   }\n \n-  Future<> GetReadyFuture() const override;\n+  tsl::Future<> GetReadyFuture() const override;\n \n-  Future<> Delete() override;\n+  tsl::Future<> Delete() override;\n \n   bool IsDeleted() const override;\n "
        },
        {
            "sha": "51693ddd6b9c28af106088752af89d1c39403296",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/reshard_impl_test_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -45,7 +45,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/index.h\"\n #include \"xla/python/ifrt/index_domain.h\"\n #include \"xla/python/ifrt/memory.h\"\n@@ -55,6 +54,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_dtype.h\"\n #include \"xla/python/pjrt_ifrt/xla_sharding.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -139,7 +139,7 @@ absl::StatusOr<xla::Literal> CopyArrayToLiteral(ArrayRef array) {\n     TF_ASSIGN_OR_RETURN(xla::Literal slice,\n                         xla::Literal::Make(xla::ShapeUtil::MakeShape(\n                             element_type, shard_shape.dims())));\n-    Future<> future = shards[i]->CopyToHostBuffer(\n+    tsl::Future<> future = shards[i]->CopyToHostBuffer(\n         slice.untyped_data(), std::nullopt, ArrayCopySemantics::kAlwaysCopy);\n     TF_RETURN_IF_ERROR(future.Await());\n     VLOG(2) << \"Slice #\" << i << \" (\" << index_domains[i]"
        },
        {
            "sha": "ce5a898e9a313fc444c843feead776eed5befea2",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_executable_impl_test_lib.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_executable_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4ef26295bf4c350877cb61f2a673701fd5c07e8/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_executable_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_executable_impl_test_lib.cc?ref=d4ef26295bf4c350877cb61f2a673701fd5c07e8",
            "patch": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n #include \"xla/python/ifrt/executable.h\"\n-#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/hlo/hlo_program.h\"\n #include \"xla/python/ifrt/layout.h\"\n #include \"xla/python/ifrt/memory.h\"\n@@ -53,6 +52,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_layout.h\"\n #include \"xla/python/pjrt_ifrt/xla_compiler.h\"\n #include \"xla/python/pjrt_ifrt/xla_executable_version.h\"\n+#include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n@@ -548,7 +548,7 @@ module @add_sub {\n   // Enqueue a read operation just before donation. The scheduler must not\n   // reorder read and donation.\n   std::vector<int32_t> data(6);\n-  Future<> copy_future =\n+  tsl::Future<> copy_future =\n       arrays[0]->CopyToHostBuffer(data.data(), /*byte_strides=*/std::nullopt,\n                                   ArrayCopySemantics::kAlwaysCopy);\n \n@@ -838,7 +838,7 @@ TEST(ExecutableTest, ExecutableSerialization) {\n \n   {\n     std::vector<int32_t> out_data(6);\n-    xla::ifrt::Future<> future = result.outputs[0]->CopyToHostBuffer(\n+    tsl::Future<> future = result.outputs[0]->CopyToHostBuffer(\n         out_data.data(), /*byte_strides=*/std::nullopt,\n         xla::ifrt::ArrayCopySemantics::kAlwaysCopy);\n     TF_ASSERT_OK(future.Await());"
        }
    ],
    "stats": {
        "total": 231,
        "additions": 123,
        "deletions": 108
    }
}