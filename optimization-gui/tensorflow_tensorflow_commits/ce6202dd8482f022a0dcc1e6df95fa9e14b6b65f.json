{
    "author": "GleasonK",
    "message": "[Kernelgen] Remove uses of mhlo.reshape / dynamic_reshape on unranked tensors from kernel templates.\n\nThis allows for deletion of duplicate MHLO/StableHLO passes (linalg being the immediate target). Since StableHLO doesn't support unranked types, flip kernel_gen kernels to use `tensor.reshape` and `tensor.dynamic_reshape` for unranked reshapes.\n\nStill uses mhlo reshape ops for bufferization / deeper lowering, that will be changed in future cleanup.\n\nPiperOrigin-RevId: 811006356",
    "sha": "ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
    "files": [
        {
            "sha": "8eee1df064a27360eaeba85aa643401aefbd8c11",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/kernel_creator.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Fkernel_creator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Fkernel_creator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Fkernel_creator.cc?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -183,6 +183,13 @@ absl::Status LowerHlotoLoops(mlir::ModuleOp module,\n   pm.addNestedPass<FuncOp>(::mlir::mhlo::createLegalizeHloToLinalgPass());\n   pm.addPass(::mlir::mhlo::createLegalizeToArithmeticPass());\n \n+  // Convert tensor.reshape to MHLO before running any bufferization passes.\n+  // We'll need to teach this pipeline how to properly handle tensor.reshape\n+  // in order to fully deprecate MHLO but in the meantime this ublocks lots of\n+  // cleanup.\n+  pm.addNestedPass<FuncOp>(\n+      ::mlir::kernel_gen::createLegalizeTensorReshapePass());\n+\n   // Remove the remaining references to unsigned types after all HLO compute\n   // operations were converted.\n   pm.addPass(mlir::stablehlo::createStablehloConvertToSignlessPass());"
        },
        {
            "sha": "26d7d9b029d59e60b83e266daada60f596f48ef6",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/tests/hlo_to_kernel/add_v2.mlir",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fadd_v2.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fadd_v2.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fadd_v2.mlir?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -15,11 +15,12 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n   %6 = shape.shape_of %arg1 : tensor<*xf32> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xf32>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xf32>) -> tensor<f32>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xf32>, tensor<0xindex>) -> tensor<f32>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n     %17 = chlo.broadcast_add %15, %16 : (tensor<f32>, tensor<?xf32>) -> tensor<?xf32>\n     %cast = tensor.cast %17 : tensor<?xf32> to tensor<*xf32>\n     scf.yield %cast : tensor<*xf32>\n@@ -29,8 +30,8 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n     %16 = scf.if %15 -> (tensor<*xf32>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xf32>) -> tensor<f32>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xf32>, tensor<0xindex>) -> tensor<f32>\n       %20 = chlo.broadcast_add %18, %19 : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\n       %cast = tensor.cast %20 : tensor<?xf32> to tensor<*xf32>\n       scf.yield %cast : tensor<*xf32>\n@@ -40,8 +41,8 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n         %23 = chlo.broadcast_add %21, %22 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\n         %cast = tensor.cast %23 : tensor<?xf32> to tensor<*xf32>\n         scf.yield %cast : tensor<*xf32>\n@@ -55,10 +56,10 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n         %25 = scf.if %24 -> (tensor<*xf32>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n           %30 = chlo.broadcast_add %27, %29 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\n           %cast_1 = tensor.cast %30 : tensor<?xf32> to tensor<*xf32>\n           scf.yield %cast_1 : tensor<*xf32>\n@@ -67,10 +68,10 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n           %27 = scf.if %26 -> (tensor<*xf32>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xf32>, tensor<2xindex>) -> tensor<?x?xf32>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xf32>, tensor<2xindex>) -> tensor<?x?xf32>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xf32>, tensor<2xindex>) -> tensor<?x?xf32>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xf32>, tensor<2xindex>) -> tensor<?x?xf32>\n             %32 = chlo.broadcast_add %29, %31 : (tensor<?x?xf32>, tensor<?x?xf32>) -> tensor<?x?xf32>\n             %cast_1 = tensor.cast %32 : tensor<?x?xf32> to tensor<*xf32>\n             scf.yield %cast_1 : tensor<*xf32>\n@@ -79,10 +80,10 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n             %29 = scf.if %28 -> (tensor<*xf32>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>\n               %34 = chlo.broadcast_add %31, %33 : (tensor<?x?x?xf32>, tensor<?x?x?xf32>) -> tensor<?x?x?xf32>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xf32> to tensor<*xf32>\n               scf.yield %cast_1 : tensor<*xf32>\n@@ -91,10 +92,10 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n               %31 = scf.if %30 -> (tensor<*xf32>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xf32>, tensor<4xindex>) -> tensor<?x?x?x?xf32>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xf32>, tensor<4xindex>) -> tensor<?x?x?x?xf32>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xf32>, tensor<4xindex>) -> tensor<?x?x?x?xf32>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xf32>, tensor<4xindex>) -> tensor<?x?x?x?xf32>\n                 %36 = chlo.broadcast_add %33, %35 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xf32> to tensor<*xf32>\n                 scf.yield %cast_1 : tensor<*xf32>\n@@ -103,10 +104,10 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xf32>, tensor<5xindex>) -> tensor<?x?x?x?x?xf32>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xf32>, tensor<5xindex>) -> tensor<?x?x?x?x?xf32>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xf32>, tensor<5xindex>) -> tensor<?x?x?x?x?xf32>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xf32>, tensor<5xindex>) -> tensor<?x?x?x?x?xf32>\n                 %37 = chlo.broadcast_add %34, %36 : (tensor<?x?x?x?x?xf32>, tensor<?x?x?x?x?xf32>) -> tensor<?x?x?x?x?xf32>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xf32> to tensor<*xf32>\n                 scf.yield %cast_1 : tensor<*xf32>\n@@ -126,6 +127,6 @@ func.func @AddV2(%arg0: tensor<*xf32>, %arg1: tensor<*xf32>) -> tensor<*xf32> at\n   %10 = shape.shape_of %arg0 : tensor<*xf32> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xf32> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xf32>, tensor<?xindex>) -> tensor<*xf32>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xf32>, tensor<?xindex>) -> tensor<*xf32>\n   return %13 : tensor<*xf32>\n }"
        },
        {
            "sha": "e4c711863917fe0e0b6d5daf74830ed92ea2c20b",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/tests/hlo_to_kernel/add_v2_unsigned.mlir",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fadd_v2_unsigned.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fadd_v2_unsigned.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fadd_v2_unsigned.mlir?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -15,11 +15,12 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n   %6 = shape.shape_of %arg1 : tensor<*xui32> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xui32>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xui32>) -> tensor<ui32>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xui32>, tensor<0xindex>) -> tensor<ui32>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n     %17 = chlo.broadcast_add %15, %16 : (tensor<ui32>, tensor<?xui32>) -> tensor<?xui32>\n     %cast = tensor.cast %17 : tensor<?xui32> to tensor<*xui32>\n     scf.yield %cast : tensor<*xui32>\n@@ -29,8 +30,8 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n     %16 = scf.if %15 -> (tensor<*xui32>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xui32>) -> tensor<ui32>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xui32>, tensor<0xindex>) -> tensor<ui32>\n       %20 = chlo.broadcast_add %18, %19 : (tensor<?xui32>, tensor<ui32>) -> tensor<?xui32>\n       %cast = tensor.cast %20 : tensor<?xui32> to tensor<*xui32>\n       scf.yield %cast : tensor<*xui32>\n@@ -40,8 +41,8 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n         %23 = chlo.broadcast_add %21, %22 : (tensor<?xui32>, tensor<?xui32>) -> tensor<?xui32>\n         %cast = tensor.cast %23 : tensor<?xui32> to tensor<*xui32>\n         scf.yield %cast : tensor<*xui32>\n@@ -55,10 +56,10 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n         %25 = scf.if %24 -> (tensor<*xui32>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n           %30 = chlo.broadcast_add %27, %29 : (tensor<?xui32>, tensor<?xui32>) -> tensor<?xui32>\n           %cast_1 = tensor.cast %30 : tensor<?xui32> to tensor<*xui32>\n           scf.yield %cast_1 : tensor<*xui32>\n@@ -67,10 +68,10 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n           %27 = scf.if %26 -> (tensor<*xui32>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n             %32 = chlo.broadcast_add %29, %31 : (tensor<?x?xui32>, tensor<?x?xui32>) -> tensor<?x?xui32>\n             %cast_1 = tensor.cast %32 : tensor<?x?xui32> to tensor<*xui32>\n             scf.yield %cast_1 : tensor<*xui32>\n@@ -79,10 +80,10 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n             %29 = scf.if %28 -> (tensor<*xui32>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n               %34 = chlo.broadcast_add %31, %33 : (tensor<?x?x?xui32>, tensor<?x?x?xui32>) -> tensor<?x?x?xui32>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xui32> to tensor<*xui32>\n               scf.yield %cast_1 : tensor<*xui32>\n@@ -91,10 +92,10 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n               %31 = scf.if %30 -> (tensor<*xui32>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n                 %36 = chlo.broadcast_add %33, %35 : (tensor<?x?x?x?xui32>, tensor<?x?x?x?xui32>) -> tensor<?x?x?x?xui32>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xui32> to tensor<*xui32>\n                 scf.yield %cast_1 : tensor<*xui32>\n@@ -103,10 +104,10 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n                 %37 = chlo.broadcast_add %34, %36 : (tensor<?x?x?x?x?xui32>, tensor<?x?x?x?x?xui32>) -> tensor<?x?x?x?x?xui32>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xui32> to tensor<*xui32>\n                 scf.yield %cast_1 : tensor<*xui32>\n@@ -126,6 +127,6 @@ func.func @AddV2(%arg0: tensor<*xui32>, %arg1: tensor<*xui32>) -> tensor<*xui32>\n   %10 = shape.shape_of %arg0 : tensor<*xui32> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xui32> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xui32>, tensor<?xindex>) -> tensor<*xui32>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xui32>, tensor<?xindex>) -> tensor<*xui32>\n   return %13 : tensor<*xui32>\n }"
        },
        {
            "sha": "6dd4e5afc2366c427f70a5ccd3f05ce248dcdf05",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/tests/hlo_to_kernel/minimum.mlir",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fminimum.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fminimum.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Fminimum.mlir?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -15,11 +15,12 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n   %6 = shape.shape_of %arg1 : tensor<*xui32> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xui32>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xui32>) -> tensor<ui32>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xui32>, tensor<0xindex>) -> tensor<ui32>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n     %17 = chlo.broadcast_minimum %15, %16 : (tensor<ui32>, tensor<?xui32>) -> tensor<?xui32>\n     %cast = tensor.cast %17 : tensor<?xui32> to tensor<*xui32>\n     scf.yield %cast : tensor<*xui32>\n@@ -29,8 +30,8 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n     %16 = scf.if %15 -> (tensor<*xui32>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xui32>) -> tensor<ui32>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xui32>, tensor<0xindex>) -> tensor<ui32>\n       %20 = chlo.broadcast_minimum %18, %19 : (tensor<?xui32>, tensor<ui32>) -> tensor<?xui32>\n       %cast = tensor.cast %20 : tensor<?xui32> to tensor<*xui32>\n       scf.yield %cast : tensor<*xui32>\n@@ -40,8 +41,8 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n         %23 = chlo.broadcast_minimum %21, %22 : (tensor<?xui32>, tensor<?xui32>) -> tensor<?xui32>\n         %cast = tensor.cast %23 : tensor<?xui32> to tensor<*xui32>\n         scf.yield %cast : tensor<*xui32>\n@@ -55,10 +56,10 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n         %25 = scf.if %24 -> (tensor<*xui32>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<1xindex>) -> tensor<?xui32>\n           %30 = chlo.broadcast_minimum %27, %29 : (tensor<?xui32>, tensor<?xui32>) -> tensor<?xui32>\n           %cast_1 = tensor.cast %30 : tensor<?xui32> to tensor<*xui32>\n           scf.yield %cast_1 : tensor<*xui32>\n@@ -67,10 +68,10 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n           %27 = scf.if %26 -> (tensor<*xui32>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<2xindex>) -> tensor<?x?xui32>\n             %32 = chlo.broadcast_minimum %29, %31 : (tensor<?x?xui32>, tensor<?x?xui32>) -> tensor<?x?xui32>\n             %cast_1 = tensor.cast %32 : tensor<?x?xui32> to tensor<*xui32>\n             scf.yield %cast_1 : tensor<*xui32>\n@@ -79,10 +80,10 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n             %29 = scf.if %28 -> (tensor<*xui32>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<3xindex>) -> tensor<?x?x?xui32>\n               %34 = chlo.broadcast_minimum %31, %33 : (tensor<?x?x?xui32>, tensor<?x?x?xui32>) -> tensor<?x?x?xui32>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xui32> to tensor<*xui32>\n               scf.yield %cast_1 : tensor<*xui32>\n@@ -91,10 +92,10 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n               %31 = scf.if %30 -> (tensor<*xui32>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<4xindex>) -> tensor<?x?x?x?xui32>\n                 %36 = chlo.broadcast_minimum %33, %35 : (tensor<?x?x?x?xui32>, tensor<?x?x?x?xui32>) -> tensor<?x?x?x?xui32>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xui32> to tensor<*xui32>\n                 scf.yield %cast_1 : tensor<*xui32>\n@@ -103,10 +104,10 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xui32>, tensor<5xindex>) -> tensor<?x?x?x?x?xui32>\n                 %37 = chlo.broadcast_minimum %34, %36 : (tensor<?x?x?x?x?xui32>, tensor<?x?x?x?x?xui32>) -> tensor<?x?x?x?x?xui32>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xui32> to tensor<*xui32>\n                 scf.yield %cast_1 : tensor<*xui32>\n@@ -126,6 +127,6 @@ func.func @Minimum_GPU_DT_UINT32_DT_UINT32(%arg0: tensor<*xui32>, %arg1: tensor<\n   %10 = shape.shape_of %arg0 : tensor<*xui32> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xui32> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xui32>, tensor<?xindex>) -> tensor<*xui32>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xui32>, tensor<?xindex>) -> tensor<*xui32>\n   return %13 : tensor<*xui32>\n }"
        },
        {
            "sha": "09f0573823e591edc6b2ad97ad18c96293ae588b",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/tests/hlo_to_kernel/tanh.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Ftanh.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Ftanh.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftests%2Fhlo_to_kernel%2Ftanh.mlir?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -4,8 +4,8 @@ func.func @tanh(%arg0: tensor<*xf32>) -> tensor<*xf32> attributes {tf_entry} {\n   %0 = shape.shape_of %arg0 : tensor<*xf32> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xf32>, tensor<1xindex>) -> tensor<?xf32>\n   %3 = mhlo.tanh %2 : tensor<?xf32>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xf32>, tensor<?xindex>) -> tensor<*xf32>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xf32>, tensor<?xindex>) -> tensor<*xf32>\n   return %4 : tensor<*xf32>\n }"
        },
        {
            "sha": "f50df07657c11cb452bdb3bb73dc0a8d30801c6b",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2FBUILD?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -186,6 +186,7 @@ cc_library(\n         \"embed_tf_framework_pass.cc\",\n         \"func_to_jit_invocations.cc\",\n         \"fuse_inner_parallel_loops_pass.cc\",\n+        \"legalize_tensor_reshape_pass.cc\",\n         \"merge_assuming_ops_pass.cc\",\n         \"parallel_loops_to_sequential.cc\",\n         \"rewrite_tf_framework_assert.cc\","
        },
        {
            "sha": "7e19f967230e9dce315859c7b4796987422ad52a",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/legalize_tensor_reshape_pass.cc",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Flegalize_tensor_reshape_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Flegalize_tensor_reshape_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Flegalize_tensor_reshape_pass.cc?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -0,0 +1,73 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <utility>\n+\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"  // from @llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n+#include \"mlir/IR/OperationSupport.h\"  // from @llvm-project\n+#include \"mlir/IR/PatternMatch.h\"  // from @llvm-project\n+#include \"mlir/Pass/Pass.h\"  // from @llvm-project  // IWYU pragma: keep\n+#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"  // from @llvm-project\n+#include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+\n+namespace mlir {\n+namespace kernel_gen {\n+\n+#define GEN_PASS_DEF_LEGALIZETENSORRESHAPEPASS\n+#include \"tensorflow/compiler/mlir/tools/kernel_gen/transforms/kernel_gen_passes.h.inc\"\n+\n+namespace {\n+\n+struct LegalizeTensorReshapePattern\n+    : public OpRewritePattern<tensor::ReshapeOp> {\n+  explicit LegalizeTensorReshapePattern(MLIRContext* context)\n+      : OpRewritePattern<tensor::ReshapeOp>(context) {}\n+\n+  LogicalResult matchAndRewrite(tensor::ReshapeOp op,\n+                                PatternRewriter& rewriter) const override {\n+    rewriter.replaceOpWithNewOp<mhlo::DynamicReshapeOp>(op, op.getResultType(),\n+                                                        op.getOperands());\n+    return success();\n+  }\n+};\n+\n+}  // namespace\n+\n+struct LegalizeTensorReshapePass\n+    : public impl::LegalizeTensorReshapePassBase<LegalizeTensorReshapePass> {\n+  void getDependentDialects(DialectRegistry& registry) const override {\n+    registry.insert<mhlo::MhloDialect>();\n+  }\n+\n+  void runOnOperation() override {\n+    MLIRContext* ctx = &getContext();\n+    RewritePatternSet patterns(ctx);\n+    patterns.add<LegalizeTensorReshapePattern>(ctx);\n+    GreedyRewriteConfig config;\n+    config.setMaxIterations(GreedyRewriteConfig::kNoLimit);\n+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns),\n+                                     config))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace kernel_gen\n+}  // namespace mlir"
        },
        {
            "sha": "3855c4b27c8c93c6958480b9f310df6efff0461c",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fpasses.h?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #define GEN_PASS_DECL_SHAPESIMPLIFICATIONPASS\n #define GEN_PASS_DECL_MERGEASSUMINGOPSPASS\n #define GEN_PASS_DECL_BROADCASTPROPAGATIONPASS\n+#define GEN_PASS_DECL_LEGALIZETENSORRESHAPEPASS\n \n namespace mlir {\n namespace kernel_gen {"
        },
        {
            "sha": "473fe7d21f55b561be9c208af1d7a86e08f5b907",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/passes.td",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fpasses.td?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -154,5 +154,11 @@ def BroadcastPropagationPass : Pass<\"mhlo-broadcast-propagation\", \"func::FuncOp\"\n     \"for larger fusions.\";\n }\n \n+def LegalizeTensorReshapePass\n+    : Pass<\"legalize-tensor-reshape\", \"mlir::func::FuncOp\"> {\n+  let summary = \"Legalize tensor.reshape to mhlo.dynamic_reshape\";\n+}\n+\n+\n \n #endif // TF_KERNEL_GEN_PASSES"
        },
        {
            "sha": "97367a601feaeadd1124c69008d5301a29944eef",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/abs.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fabs.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fabs.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fabs.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Abs_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.abs %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "fb27cac61bf0929a4475ca7fdc6be66f91ba8619",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/acos.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Facos.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Facos.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Facos.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Acos_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.acos %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "196fd20a8f41827b5cd565def0ff39b462fc0cba",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/acosh.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Facosh.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Facosh.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Facosh.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Acosh_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.acosh %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "233dd7e60c775e5940739460dd8343b1b6679b54",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/add_v2.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fadd_v2.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fadd_v2.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fadd_v2.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_add %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_add %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_add %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_add %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_add %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_add %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_add %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_add %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @AddV2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "0735907903358229e64491390d03147788e3fbb3",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/angle.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fangle.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fangle.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fangle.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,10 +2,10 @@ func.func @Angle_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.imag %2 : (tensor<?xelem_type>) -> tensor<?xoutput_type>\n   %4 = mhlo.real %2 : (tensor<?xelem_type>) -> tensor<?xoutput_type>\n   %5 = mhlo.atan2 %3, %4 : tensor<?xoutput_type>\n-  %6 = mhlo.dynamic_reshape %5, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %6 = tensor.reshape %5(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %6 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "4cf0774211409dfda696caa990bc770ae1bb9031",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/asin.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fasin.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fasin.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fasin.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Asin_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.asin %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "facba2eecd731c6c8f5ccf63c97bbd329204cadb",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/asinh.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fasinh.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fasinh.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fasinh.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Asinh_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.asinh %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "9f100eaedeccd3553bbacf08124b5369e71a4ca2",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/atan.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatan.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatan.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatan.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Atan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.atan %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "440bb28816b3ea5367fb27a976572335ae56f978",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/atan2.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatan2.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatan2.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatan2.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_atan2 %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_atan2 %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_atan2 %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_atan2 %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_atan2 %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_atan2 %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_atan2 %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_atan2 %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Atan2_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "948367805f61d612c272dfc99a13fc7523b7fded",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/atanh.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatanh.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatanh.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fatanh.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Atanh_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.atanh %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "1233e3d034780f80314f210d323d65b8b8c9c466",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/bitwise_and.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_and.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_and.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_and.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_and %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_and %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_and %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_and %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_and %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_and %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_and %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_and %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @BitwiseAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "adddc48aa90c66002c8daed8aec25c1ad79d59bf",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/bitwise_or.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_or.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_or.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_or.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_or %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_or %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_or %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_or %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_or %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_or %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_or %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_or %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @BitwiseOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "6242dee1599822b1c786f7e11b5446f43788c79e",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/bitwise_xor.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_xor.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_xor.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fbitwise_xor.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_xor %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_xor %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_xor %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_xor %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_xor %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_xor %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_xor %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_xor %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @BitwiseXor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "a7ebb285b86b4fffefde19a0b994a68129f56794",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/cast.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcast.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcast.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcast.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Cast_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.convert %2 : (tensor<?xelem_type>) -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "c6a60e309daec0bbf41b5e26d60f38450dc99f06",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/ceil.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fceil.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fceil.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fceil.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Ceil_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.ceil %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "bc897d0c33f6422b3d86cff46e27dd9a93fdf71e",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/complex.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcomplex.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcomplex.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcomplex.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_complex %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_complex %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_complex %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_complex %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_complex %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_complex %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_complex %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_complex %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Complex_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "7ad8f82d3922970b4b17ff26b7049e66be76648c",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/complex_abs.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcomplex_abs.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcomplex_abs.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcomplex_abs.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @ComplexAbs_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.abs %2 : (tensor<?xelem_type>) -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "8e63be83fa105b8cf644d3df7d12aea01003c95e",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/conj.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fconj.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fconj.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fconj.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Conj_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.conj %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "1f7a59e1b8802747a4bf634d8e9f173452a57e26",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/cos.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcos.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcos.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcos.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Cos_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.cosine %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "f431fff754f6a0e13f6765f2d30fd56b623d7314",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/cosh.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcosh.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcosh.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fcosh.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Cosh_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.cosh %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "3ed4ea2ae829c44f67d645475331f25acb61a40b",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/digamma.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdigamma.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdigamma.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdigamma.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Digamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) ->\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.digamma %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "b51816f57a3ef4773528f15cbc3cb63a3b38feac",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/div.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_divide %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_divide %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_divide %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_divide %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_divide %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_divide %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_divide %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_divide %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Div_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "584c245aa20fdfd5f241f3e6771320f70df4df75",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/div_no_nan.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv_no_nan.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv_no_nan.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv_no_nan.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xoutput_type>) {\n     %17 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-    %18 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %19 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %18 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %19 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %20 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n     %21 = chlo.broadcast_divide %18, %19 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %22 = chlo.broadcast_select %20, %5, %21 : (tensor<?xi1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -30,8 +31,8 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %19 = scf.if %18 -> (tensor<*xoutput_type>) {\n       %20 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-      %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %22 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %22 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %23 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<output_type>) -> tensor<i1>\n       %24 = chlo.broadcast_divide %21, %22 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %25 = chlo.broadcast_select %23, %5, %24 : (tensor<i1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -43,8 +44,8 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %22 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %23 = shape.num_elements %22 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %23 : tensor<1xindex>\n-        %24 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %25 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %24 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %26 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n         %27 = chlo.broadcast_divide %24, %25 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %28 = chlo.broadcast_select %26, %5, %27 : (tensor<?xi1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -60,10 +61,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %28 = scf.if %27 -> (tensor<*xoutput_type>) {\n           %29 = shape.broadcast %22#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %29 : tensor<?xindex> to tensor<1xindex>\n-          %30 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %30 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %31 = shape.broadcast %22#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %31 : tensor<?xindex> to tensor<1xindex>\n-          %32 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %32 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %33 = chlo.broadcast_compare %32, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n           %34 = chlo.broadcast_divide %30, %32 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %35 = chlo.broadcast_select %33, %5, %34 : (tensor<?xi1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -74,10 +75,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %30 = scf.if %29 -> (tensor<*xoutput_type>) {\n             %31 = shape.broadcast %22#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %31 : tensor<?xindex> to tensor<2xindex>\n-            %32 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %32 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %33 = shape.broadcast %22#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %33 : tensor<?xindex> to tensor<2xindex>\n-            %34 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %34 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %35 = chlo.broadcast_compare %34, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<output_type>) -> tensor<?x?xi1>\n             %36 = chlo.broadcast_divide %32, %34 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %37 = chlo.broadcast_select %35, %5, %36 : (tensor<?x?xi1>, tensor<output_type>, tensor<?x?xoutput_type>) -> tensor<?x?xoutput_type>\n@@ -88,10 +89,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %32 = scf.if %31 -> (tensor<*xoutput_type>) {\n               %33 = shape.broadcast %22#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %33 : tensor<?xindex> to tensor<3xindex>\n-              %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %35 = shape.broadcast %22#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<3xindex>\n-              %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %37 = chlo.broadcast_compare %36, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?xi1>\n               %38 = chlo.broadcast_divide %34, %36 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %39 = chlo.broadcast_select %37, %5, %38 : (tensor<?x?x?xi1>, tensor<output_type>, tensor<?x?x?xoutput_type>) -> tensor<?x?x?xoutput_type>\n@@ -102,10 +103,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %34 = scf.if %33 -> (tensor<*xoutput_type>) {\n                 %35 = shape.broadcast %22#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %35 : tensor<?xindex> to tensor<4xindex>\n-                %36 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %37 = shape.broadcast %22#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %37 : tensor<?xindex> to tensor<4xindex>\n-                %38 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?x?xi1>\n                 %40 = chlo.broadcast_divide %36, %38 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %41 = chlo.broadcast_select %39, %5, %40 : (tensor<?x?x?x?xi1>, tensor<output_type>, tensor<?x?x?x?xoutput_type>) -> tensor<?x?x?x?xoutput_type>\n@@ -116,10 +117,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %35, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %36 = shape.broadcast %22#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<5xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %38 = shape.broadcast %22#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<5xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %39, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?x?x?xi1>\n                 %41 = chlo.broadcast_divide %37, %39 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %42 = chlo.broadcast_select %40, %5, %41 : (tensor<?x?x?x?x?xi1>, tensor<output_type>, tensor<?x?x?x?x?xoutput_type>) -> tensor<?x?x?x?x?xoutput_type>\n@@ -143,6 +144,6 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %13 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %14 = shape.broadcast %12, %13 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %15 = shape.broadcast %11, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %16 = mhlo.dynamic_reshape %10, %15 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %16 = tensor.reshape %10(%15) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %16 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "8772431e79a8984253a1f9f413829a17abfce344",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/div_no_nan_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv_no_nan_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv_no_nan_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fdiv_no_nan_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xoutput_type>) {\n     %17 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-    %18 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %19 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %18 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %19 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %20 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n     %21 = chlo.broadcast_divide %18, %19 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %22 = chlo.broadcast_select %20, %5, %21 : (tensor<?xi1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -30,8 +31,8 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %19 = scf.if %18 -> (tensor<*xoutput_type>) {\n       %20 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-      %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %22 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %22 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %23 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<output_type>) -> tensor<i1>\n       %24 = chlo.broadcast_divide %21, %22 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %25 = chlo.broadcast_select %23, %5, %24 : (tensor<i1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -43,8 +44,8 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %22 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %23 = shape.num_elements %22 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %23 : tensor<1xindex>\n-        %24 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %25 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %24 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %26 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n         %27 = chlo.broadcast_divide %24, %25 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %28 = chlo.broadcast_select %26, %5, %27 : (tensor<?xi1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -60,10 +61,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %28 = scf.if %27 -> (tensor<*xoutput_type>) {\n           %29 = shape.broadcast %22#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %29 : tensor<?xindex> to tensor<1xindex>\n-          %30 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %30 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %31 = shape.broadcast %22#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %31 : tensor<?xindex> to tensor<1xindex>\n-          %32 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %32 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %33 = chlo.broadcast_compare %32, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n           %34 = chlo.broadcast_divide %30, %32 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %35 = chlo.broadcast_select %33, %5, %34 : (tensor<?xi1>, tensor<output_type>, tensor<?xoutput_type>) -> tensor<?xoutput_type>\n@@ -74,10 +75,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %30 = scf.if %29 -> (tensor<*xoutput_type>) {\n             %31 = shape.broadcast %22#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %31 : tensor<?xindex> to tensor<2xindex>\n-            %32 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %32 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %33 = shape.broadcast %22#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %33 : tensor<?xindex> to tensor<2xindex>\n-            %34 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %34 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %35 = chlo.broadcast_compare %34, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<output_type>) -> tensor<?x?xi1>\n             %36 = chlo.broadcast_divide %32, %34 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %37 = chlo.broadcast_select %35, %5, %36 : (tensor<?x?xi1>, tensor<output_type>, tensor<?x?xoutput_type>) -> tensor<?x?xoutput_type>\n@@ -88,10 +89,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %32 = scf.if %31 -> (tensor<*xoutput_type>) {\n               %33 = shape.broadcast %22#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %33 : tensor<?xindex> to tensor<3xindex>\n-              %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %35 = shape.broadcast %22#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<3xindex>\n-              %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %37 = chlo.broadcast_compare %36, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?xi1>\n               %38 = chlo.broadcast_divide %34, %36 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %39 = chlo.broadcast_select %37, %5, %38 : (tensor<?x?x?xi1>, tensor<output_type>, tensor<?x?x?xoutput_type>) -> tensor<?x?x?xoutput_type>\n@@ -102,10 +103,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %34 = scf.if %33 -> (tensor<*xoutput_type>) {\n                 %35 = shape.broadcast %22#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %35 : tensor<?xindex> to tensor<4xindex>\n-                %36 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %37 = shape.broadcast %22#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %37 : tensor<?xindex> to tensor<4xindex>\n-                %38 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?x?xi1>\n                 %40 = chlo.broadcast_divide %36, %38 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %41 = chlo.broadcast_select %39, %5, %40 : (tensor<?x?x?x?xi1>, tensor<output_type>, tensor<?x?x?x?xoutput_type>) -> tensor<?x?x?x?xoutput_type>\n@@ -116,10 +117,10 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %35, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %36 = shape.broadcast %22#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<5xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %38 = shape.broadcast %22#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<5xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %39, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?x?x?xi1>\n                 %41 = chlo.broadcast_divide %37, %39 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %42 = chlo.broadcast_select %40, %5, %41 : (tensor<?x?x?x?x?xi1>, tensor<output_type>, tensor<?x?x?x?x?xoutput_type>) -> tensor<?x?x?x?x?xoutput_type>\n@@ -143,6 +144,6 @@ func.func @DivNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %13 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %14 = shape.broadcast %12, %13 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %15 = shape.broadcast %11, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %16 = mhlo.dynamic_reshape %10, %15 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %16 = tensor.reshape %10(%15) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %16 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "21fee052bd3a096b5de626862f5b5b75722f864c",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/elu.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Felu.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Felu.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Felu.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,11 +2,11 @@ func.func @Elu_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = 0.000000e+00 : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n   %4 = mhlo.compare  GT, %2, %3 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xi1>\n   %5 = mhlo.exponential_minus_one %2 : tensor<?xelem_type>\n   %6 = mhlo.select %4, %2, %5 : tensor<?xi1>, tensor<?xelem_type>\n-  %7 = mhlo.dynamic_reshape %6, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %7 = tensor.reshape %6(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %7 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "b61313484e1134af89871fae14bb815b4781f126",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/equal.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fequal.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fequal.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fequal.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_compare %15, %16 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_compare %18, %19 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_compare %21, %22 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_compare %27, %29 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_compare %29, %31 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_compare %31, %33 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_compare %33, %35 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_compare %34, %36 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Equal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "e9ec451832a7ecde00ceba7bdb2ae8038969ceb6",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/erf.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ferf.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ferf.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ferf.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Erf_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.erf %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "efaea371ec78ee1d3e832ca256b838c6f38d70af",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/erfc.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ferfc.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ferfc.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ferfc.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Erfc_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.erfc %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "9deadb3db1d9646ba7f9c3fdd0cae340746a4d6a",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/exp.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fexp.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fexp.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fexp.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Exp_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.exponential %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "fd046412fa6e797dd83f64c55b532692e7030d2e",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/expm1.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fexpm1.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fexpm1.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fexpm1.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Expm1_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.exponential_minus_one %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "08fe6acf085ebfed238603792fd94a83c99bd32e",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/floor.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Floor_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.floor %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "ff439f4948f952dd7229cbe25fdc5e3819c1c004",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/floor_div.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_div.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_div.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_div.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -15,11 +15,12 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %8 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %9 = shape.num_elements %7 : tensor<?xindex> -> index\n   %10 = arith.cmpi eq, %9, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %11 = scf.if %10 -> (tensor<*xelem_type>) {\n     %16 = shape.num_elements %8 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %16 : tensor<1xindex>\n-    %17 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %17 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %19 = chlo.broadcast_divide %18, %17 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n     %20 = chlo.broadcast_multiply %19, %17 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %20, %18 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xi1>\n@@ -37,8 +38,8 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %18 = scf.if %17 -> (tensor<*xelem_type>) {\n       %19 = shape.num_elements %7 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %19 : tensor<1xindex>\n-      %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %21 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %21 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %22 = chlo.broadcast_divide %21, %20 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n       %23 = chlo.broadcast_multiply %22, %20 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n       %24 = chlo.broadcast_compare %23, %21 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -56,8 +57,8 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %21 = shape.any %7, %8 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %22 = shape.num_elements %21 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %22 : tensor<1xindex>\n-        %23 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %24 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %23 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %24 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %25 = chlo.broadcast_divide %24, %23 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %26 = chlo.broadcast_multiply %25, %23 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %26, %24 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xi1>\n@@ -79,10 +80,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %27 = scf.if %26 -> (tensor<*xelem_type>) {\n           %28 = shape.broadcast %21#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = shape.broadcast %21#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = chlo.broadcast_divide %31, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %33 = chlo.broadcast_multiply %32, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %33, %31 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xi1>\n@@ -99,10 +100,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %29 = scf.if %28 -> (tensor<*xelem_type>) {\n             %30 = shape.broadcast %21#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = shape.broadcast %21#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = chlo.broadcast_divide %33, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %35 = chlo.broadcast_multiply %34, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %35, %33 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xi1>\n@@ -119,10 +120,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %31 = scf.if %30 -> (tensor<*xelem_type>) {\n               %32 = shape.broadcast %21#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = shape.broadcast %21#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = chlo.broadcast_divide %35, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %37 = chlo.broadcast_multiply %36, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %37, %35 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xi1>\n@@ -139,10 +140,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %33 = scf.if %32 -> (tensor<*xelem_type>) {\n                 %34 = shape.broadcast %21#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = shape.broadcast %21#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = chlo.broadcast_divide %37, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_multiply %38, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %39, %37 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xi1>\n@@ -159,10 +160,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %34, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %35 = shape.broadcast %21#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = shape.broadcast %21#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_divide %38, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_multiply %39, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %40, %38 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xi1>\n@@ -190,6 +191,6 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %12 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %13 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %14 = shape.broadcast %12, %13 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %15 = mhlo.dynamic_reshape %11, %14 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %15 = tensor.reshape %11(%14) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %15 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "8cfe62bcf5fb050f418170bd035ac2637e97d1bc",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/floor_div_float.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_div_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_div_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_div_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_divide %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %18 = mhlo.floor %17 : tensor<?xoutput_type>\n     %cast = tensor.cast %18 : tensor<?xoutput_type> to tensor<*xoutput_type>\n@@ -28,8 +29,8 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_divide %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %21 = mhlo.floor %20 : tensor<?xoutput_type>\n       %cast = tensor.cast %21 : tensor<?xoutput_type> to tensor<*xoutput_type>\n@@ -40,8 +41,8 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_divide %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %24 = mhlo.floor %23 : tensor<?xoutput_type>\n         %cast = tensor.cast %24 : tensor<?xoutput_type> to tensor<*xoutput_type>\n@@ -56,10 +57,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_divide %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %31 = mhlo.floor %30 : tensor<?xoutput_type>\n           %cast_1 = tensor.cast %31 : tensor<?xoutput_type> to tensor<*xoutput_type>\n@@ -69,10 +70,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_divide %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %33 = mhlo.floor %32 : tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %33 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n@@ -82,10 +83,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_divide %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %35 = mhlo.floor %34 : tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %35 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n@@ -95,10 +96,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_divide %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %37 = mhlo.floor %36 : tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n@@ -108,10 +109,10 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_divide %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %38 = mhlo.floor %37 : tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %38 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n@@ -132,6 +133,6 @@ func.func @FloorDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "5a7b90e113fb2da1268335a4a42175615955b750",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/floor_mod.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %7 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %17 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-    %18 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %19 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %18 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %19 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %20 = chlo.broadcast_remainder %19, %18 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %20, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n     %22 = chlo.broadcast_compare %18, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n@@ -35,8 +36,8 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %19 = scf.if %18 -> (tensor<*xelem_type>) {\n       %20 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-      %21 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %22 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %21 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %22 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %23 = chlo.broadcast_remainder %22, %21 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n       %24 = chlo.broadcast_compare %23, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = chlo.broadcast_compare %21, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -53,8 +54,8 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %22 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %23 = shape.num_elements %22 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %23 : tensor<1xindex>\n-        %24 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %24 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %26 = chlo.broadcast_remainder %25, %24 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %26, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = chlo.broadcast_compare %24, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -75,10 +76,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %28 = scf.if %27 -> (tensor<*xelem_type>) {\n           %29 = shape.broadcast %22#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %29 : tensor<?xindex> to tensor<1xindex>\n-          %30 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %30 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %31 = shape.broadcast %22#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %31 : tensor<?xindex> to tensor<1xindex>\n-          %32 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %32 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %33 = chlo.broadcast_remainder %32, %30 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = chlo.broadcast_compare %30, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -94,10 +95,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %30 = scf.if %29 -> (tensor<*xelem_type>) {\n             %31 = shape.broadcast %22#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %31 : tensor<?xindex> to tensor<2xindex>\n-            %32 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %32 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %33 = shape.broadcast %22#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %33 : tensor<?xindex> to tensor<2xindex>\n-            %34 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %34 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %35 = chlo.broadcast_remainder %34, %32 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = chlo.broadcast_compare %32, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n@@ -113,10 +114,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %32 = scf.if %31 -> (tensor<*xelem_type>) {\n               %33 = shape.broadcast %22#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %33 : tensor<?xindex> to tensor<3xindex>\n-              %34 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %34 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %35 = shape.broadcast %22#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<3xindex>\n-              %36 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %36 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %37 = chlo.broadcast_remainder %36, %34 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = chlo.broadcast_compare %34, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n@@ -132,10 +133,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %34 = scf.if %33 -> (tensor<*xelem_type>) {\n                 %35 = shape.broadcast %22#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %35 : tensor<?xindex> to tensor<4xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %37 = shape.broadcast %22#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %37 : tensor<?xindex> to tensor<4xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_remainder %38, %36 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %39, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = chlo.broadcast_compare %36, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n@@ -151,10 +152,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %35, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %36 = shape.broadcast %22#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<5xindex>\n-                %37 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %38 = shape.broadcast %22#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<5xindex>\n-                %39 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_remainder %39, %37 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %40, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n@@ -183,6 +184,6 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %13 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %14 = shape.broadcast %12, %13 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %15 = shape.broadcast %11, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %16 = mhlo.dynamic_reshape %10, %15 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %16 = tensor.reshape %10(%15) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %16 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "91b0f983b3e48cd2117c56518a28f86f10384766",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/floor_mod_float.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %7 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %17 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-    %18 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %19 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %18 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %19 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %20 = chlo.broadcast_remainder %19, %18 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %20, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n     %22 = chlo.broadcast_compare %18, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n@@ -35,8 +36,8 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %19 = scf.if %18 -> (tensor<*xelem_type>) {\n       %20 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-      %21 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %22 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %21 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %22 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %23 = chlo.broadcast_remainder %22, %21 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n       %24 = chlo.broadcast_compare %23, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = chlo.broadcast_compare %21, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -53,8 +54,8 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %22 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %23 = shape.num_elements %22 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %23 : tensor<1xindex>\n-        %24 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %24 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %26 = chlo.broadcast_remainder %25, %24 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %26, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = chlo.broadcast_compare %24, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -75,10 +76,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %28 = scf.if %27 -> (tensor<*xelem_type>) {\n           %29 = shape.broadcast %22#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %29 : tensor<?xindex> to tensor<1xindex>\n-          %30 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %30 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %31 = shape.broadcast %22#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %31 : tensor<?xindex> to tensor<1xindex>\n-          %32 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %32 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %33 = chlo.broadcast_remainder %32, %30 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = chlo.broadcast_compare %30, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -94,10 +95,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %30 = scf.if %29 -> (tensor<*xelem_type>) {\n             %31 = shape.broadcast %22#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %31 : tensor<?xindex> to tensor<2xindex>\n-            %32 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %32 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %33 = shape.broadcast %22#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %33 : tensor<?xindex> to tensor<2xindex>\n-            %34 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %34 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %35 = chlo.broadcast_remainder %34, %32 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = chlo.broadcast_compare %32, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n@@ -113,10 +114,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %32 = scf.if %31 -> (tensor<*xelem_type>) {\n               %33 = shape.broadcast %22#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %33 : tensor<?xindex> to tensor<3xindex>\n-              %34 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %34 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %35 = shape.broadcast %22#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<3xindex>\n-              %36 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %36 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %37 = chlo.broadcast_remainder %36, %34 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = chlo.broadcast_compare %34, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n@@ -132,10 +133,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %34 = scf.if %33 -> (tensor<*xelem_type>) {\n                 %35 = shape.broadcast %22#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %35 : tensor<?xindex> to tensor<4xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %37 = shape.broadcast %22#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %37 : tensor<?xindex> to tensor<4xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_remainder %38, %36 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %39, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = chlo.broadcast_compare %36, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n@@ -151,10 +152,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %35, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %36 = shape.broadcast %22#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<5xindex>\n-                %37 = mhlo.dynamic_reshape %arg1, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg1(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %38 = shape.broadcast %22#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<5xindex>\n-                %39 = mhlo.dynamic_reshape %arg0, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg0(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_remainder %39, %37 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %40, %5 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n@@ -183,6 +184,6 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %13 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %14 = shape.broadcast %12, %13 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %15 = shape.broadcast %11, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %16 = mhlo.dynamic_reshape %10, %15 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %16 = tensor.reshape %10(%15) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %16 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "5cce20390a94050704ca857b8600a35a3454726d",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/floor_mod_unsigned.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod_unsigned.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod_unsigned.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ffloor_mod_unsigned.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_remainder %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_remainder %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_remainder %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_remainder %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_remainder %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_remainder %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_remainder %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_remainder %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @FloorMod_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "85d5873a15529a9d2620630de7993e566e381576",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/greater.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fgreater.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fgreater.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fgreater.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_compare %15, %16 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_compare %18, %19 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_compare %21, %22 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_compare %27, %29 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_compare %29, %31 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_compare %31, %33 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_compare %33, %35 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_compare %34, %36 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Greater_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "f3683eb5304c4631c3fe3204f668045ecf1221f5",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/greater_equal.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fgreater_equal.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fgreater_equal.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fgreater_equal.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_compare %15, %16 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_compare %18, %19 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_compare %21, %22 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_compare %27, %29 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_compare %29, %31 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_compare %31, %33 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_compare %33, %35 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_compare %34, %36 {comparison_direction = #chlo<comparison_direction GE>} : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @GreaterEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "f6410f07f8450de54ac7b0db3175255739980f41",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/imag.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fimag.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fimag.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fimag.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Imag_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.imag %2 : (tensor<?xelem_type>) -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "902e7a1b47a7f3b719d9b3a6a91ddedef42c9e74",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/invert.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Finvert.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Finvert.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Finvert.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Invert_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) ->\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.not %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "77573854d09abd53462efe5e2710e8ac3f9d2b9a",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/is_finite.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_finite.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_finite.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_finite.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @IsFinite_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.is_finite %2 : (tensor<?xelem_type>) -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "86fef155dc45bb6c8c94ba5e288fe98fccc2adf0",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/is_inf.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_inf.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_inf.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_inf.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @IsInf_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.is_inf %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "7699f9f9a324a9889b51cff7eeb44b9b5dead503",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/is_nan.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_nan.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_nan.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fis_nan.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @IsNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.broadcast_compare %2, %2 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "0f35dead95bb4e2047294499ad13af59a09244bc",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/left_shift.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fleft_shift.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fleft_shift.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fleft_shift.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_shift_left %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_shift_left %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_shift_left %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_shift_left %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_shift_left %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_shift_left %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_shift_left %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_shift_left %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @LeftShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "99f10addfc6ee2d501089b157e893403d56c7d89",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/less.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fless.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fless.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fless.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_compare %15, %16 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_compare %18, %19 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_compare %21, %22 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_compare %27, %29 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_compare %29, %31 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_compare %31, %33 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_compare %33, %35 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_compare %34, %36 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Less_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "da2fdc74e84fb0b31cb6fb42fc2d63844064a6db",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/less_equal.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fless_equal.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fless_equal.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fless_equal.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_compare %15, %16 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_compare %18, %19 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_compare %21, %22 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_compare %27, %29 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_compare %29, %31 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_compare %31, %33 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_compare %33, %35 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_compare %34, %36 {comparison_direction = #chlo<comparison_direction LE>} : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @LessEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "5b25b206ad7cbda16ceb7b0ebf70e9a124fe2083",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/lgamma.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flgamma.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flgamma.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flgamma.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Lgamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) ->\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.lgamma %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "b3a880842fe0bc53184692827b00c5fb080e90f2",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/log.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flog.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flog.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flog.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Log_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.log %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "8dd770e4c6df3a115e91ea8ba486b42386f1d009",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/log1p.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flog1p.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flog1p.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flog1p.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Log1p_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.log_plus_one %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "b8331f7dab756ecd58a74d43561a3eabc6bb6d79",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/logical_and.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_and.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_and.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_and.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_and %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_and %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_and %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_and %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_and %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_and %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_and %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_and %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @LogicalAnd_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "bcae90ef3679535bdc0b5bbe5c9c9acbd0eca15b",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/logical_not.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_not.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_not.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_not.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @LogicalNot_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.not %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "9e8e38f764b4c75dd20fb2fef210261e005f02de",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/logical_or.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_or.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_or.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Flogical_or.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_or %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_or %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_or %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_or %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_or %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_or %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_or %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_or %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @LogicalOr_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "96569ededa9c5c3f53acfa6e23593d6cba85bf30",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/maximum.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmaximum.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmaximum.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmaximum.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_maximum %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_maximum %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_maximum %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_maximum %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_maximum %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_maximum %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_maximum %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_maximum %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Maximum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "2834bbb474b6ab6a9524901eb4f6bd7ad4de83fa",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/minimum.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fminimum.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fminimum.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fminimum.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_minimum %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_minimum %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_minimum %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_minimum %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_minimum %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_minimum %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_minimum %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_minimum %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @Minimum_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "2da425fd06379719a408d9aeacbb64ef624bdb7c",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/mul.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_multiply %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_multiply %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_multiply %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_multiply %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_multiply %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_multiply %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_multiply %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_multiply %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Mul_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "2abd9bbfc1c2d78f1ed81637edbf6bdbf8ed1b3a",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/mul_no_nan.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul_no_nan.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul_no_nan.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul_no_nan.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %17 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-    %18 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %19 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %18 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %19 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %20 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n     %21 = chlo.broadcast_multiply %18, %19 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %22 = chlo.broadcast_select %20, %5, %21 : (tensor<?xi1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -30,8 +31,8 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %19 = scf.if %18 -> (tensor<*xelem_type>) {\n       %20 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-      %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %22 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %22 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %23 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n       %24 = chlo.broadcast_multiply %21, %22 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %25 = chlo.broadcast_select %23, %5, %24 : (tensor<i1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -43,8 +44,8 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %22 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %23 = shape.num_elements %22 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %23 : tensor<1xindex>\n-        %24 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %25 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %24 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %26 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %27 = chlo.broadcast_multiply %24, %25 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %28 = chlo.broadcast_select %26, %5, %27 : (tensor<?xi1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -60,10 +61,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %28 = scf.if %27 -> (tensor<*xelem_type>) {\n           %29 = shape.broadcast %22#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %29 : tensor<?xindex> to tensor<1xindex>\n-          %30 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %30 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %31 = shape.broadcast %22#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %31 : tensor<?xindex> to tensor<1xindex>\n-          %32 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %32 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %33 = chlo.broadcast_compare %32, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %34 = chlo.broadcast_multiply %30, %32 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %35 = chlo.broadcast_select %33, %5, %34 : (tensor<?xi1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -74,10 +75,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %30 = scf.if %29 -> (tensor<*xelem_type>) {\n             %31 = shape.broadcast %22#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %31 : tensor<?xindex> to tensor<2xindex>\n-            %32 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %32 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %33 = shape.broadcast %22#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %33 : tensor<?xindex> to tensor<2xindex>\n-            %34 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %34 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %35 = chlo.broadcast_compare %34, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %36 = chlo.broadcast_multiply %32, %34 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %37 = chlo.broadcast_select %35, %5, %36 : (tensor<?x?xi1>, tensor<elem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -88,10 +89,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %32 = scf.if %31 -> (tensor<*xelem_type>) {\n               %33 = shape.broadcast %22#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %33 : tensor<?xindex> to tensor<3xindex>\n-              %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %35 = shape.broadcast %22#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<3xindex>\n-              %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %37 = chlo.broadcast_compare %36, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %38 = chlo.broadcast_multiply %34, %36 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %39 = chlo.broadcast_select %37, %5, %38 : (tensor<?x?x?xi1>, tensor<elem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -102,10 +103,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %34 = scf.if %33 -> (tensor<*xelem_type>) {\n                 %35 = shape.broadcast %22#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %35 : tensor<?xindex> to tensor<4xindex>\n-                %36 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %37 = shape.broadcast %22#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %37 : tensor<?xindex> to tensor<4xindex>\n-                %38 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %40 = chlo.broadcast_multiply %36, %38 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_select %39, %5, %40 : (tensor<?x?x?x?xi1>, tensor<elem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -116,10 +117,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %35, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %36 = shape.broadcast %22#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<5xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %38 = shape.broadcast %22#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<5xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %39, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %41 = chlo.broadcast_multiply %37, %39 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_select %40, %5, %41 : (tensor<?x?x?x?x?xi1>, tensor<elem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -143,6 +144,6 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %13 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %14 = shape.broadcast %12, %13 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %15 = shape.broadcast %11, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %16 = mhlo.dynamic_reshape %10, %15 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %16 = tensor.reshape %10(%15) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %16 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "1b72aa32efd3664b83f21843130f81720f43ebbb",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/mul_no_nan_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul_no_nan_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul_no_nan_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fmul_no_nan_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %17 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-    %18 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %19 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %18 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %19 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %20 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n     %21 = chlo.broadcast_multiply %18, %19 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %22 = chlo.broadcast_select %20, %5, %21 : (tensor<?xi1>, tensor<output_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -30,8 +31,8 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %19 = scf.if %18 -> (tensor<*xelem_type>) {\n       %20 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-      %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %22 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %22 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %23 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<output_type>) -> tensor<i1>\n       %24 = chlo.broadcast_multiply %21, %22 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %25 = chlo.broadcast_select %23, %5, %24 : (tensor<i1>, tensor<output_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -43,8 +44,8 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %22 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %23 = shape.num_elements %22 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %23 : tensor<1xindex>\n-        %24 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %25 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %24 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %26 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n         %27 = chlo.broadcast_multiply %24, %25 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %28 = chlo.broadcast_select %26, %5, %27 : (tensor<?xi1>, tensor<output_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -60,10 +61,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %28 = scf.if %27 -> (tensor<*xelem_type>) {\n           %29 = shape.broadcast %22#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %29 : tensor<?xindex> to tensor<1xindex>\n-          %30 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %30 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %31 = shape.broadcast %22#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %31 : tensor<?xindex> to tensor<1xindex>\n-          %32 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %32 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %33 = chlo.broadcast_compare %32, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<output_type>) -> tensor<?xi1>\n           %34 = chlo.broadcast_multiply %30, %32 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %35 = chlo.broadcast_select %33, %5, %34 : (tensor<?xi1>, tensor<output_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -74,10 +75,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %30 = scf.if %29 -> (tensor<*xelem_type>) {\n             %31 = shape.broadcast %22#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %31 : tensor<?xindex> to tensor<2xindex>\n-            %32 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %32 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %33 = shape.broadcast %22#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %33 : tensor<?xindex> to tensor<2xindex>\n-            %34 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %34 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %35 = chlo.broadcast_compare %34, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<output_type>) -> tensor<?x?xi1>\n             %36 = chlo.broadcast_multiply %32, %34 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %37 = chlo.broadcast_select %35, %5, %36 : (tensor<?x?xi1>, tensor<output_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -88,10 +89,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %32 = scf.if %31 -> (tensor<*xelem_type>) {\n               %33 = shape.broadcast %22#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %33 : tensor<?xindex> to tensor<3xindex>\n-              %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %35 = shape.broadcast %22#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<3xindex>\n-              %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %37 = chlo.broadcast_compare %36, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?xi1>\n               %38 = chlo.broadcast_multiply %34, %36 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %39 = chlo.broadcast_select %37, %5, %38 : (tensor<?x?x?xi1>, tensor<output_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -102,10 +103,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %34 = scf.if %33 -> (tensor<*xelem_type>) {\n                 %35 = shape.broadcast %22#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %35 : tensor<?xindex> to tensor<4xindex>\n-                %36 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %37 = shape.broadcast %22#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %37 : tensor<?xindex> to tensor<4xindex>\n-                %38 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %39 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?x?xi1>\n                 %40 = chlo.broadcast_multiply %36, %38 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_select %39, %5, %40 : (tensor<?x?x?x?xi1>, tensor<output_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -116,10 +117,10 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %35, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %36 = shape.broadcast %22#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<5xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %38 = shape.broadcast %22#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<5xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %39, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<output_type>) -> tensor<?x?x?x?x?xi1>\n                 %41 = chlo.broadcast_multiply %37, %39 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_select %40, %5, %41 : (tensor<?x?x?x?x?xi1>, tensor<output_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -143,6 +144,6 @@ func.func @MulNoNan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %13 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %14 = shape.broadcast %12, %13 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %15 = shape.broadcast %11, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %16 = mhlo.dynamic_reshape %10, %15 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %16 = tensor.reshape %10(%15) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %16 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "f3ee592bb4ae5f865d12a05000da1c71c4395856",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/neg.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fneg.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fneg.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fneg.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Neg_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.negate %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "017936b067f6ee3c49ea0bb4dc18b5144781e7ba",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/next_after.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fnext_after.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fnext_after.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fnext_after.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_next_after %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_next_after %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_next_after %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_next_after %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_next_after %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_next_after %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_next_after %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_next_after %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @NextAfter_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "b70a5c770afb14961f54aaf3e6ab17a0e965341e",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/not_equal.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fnot_equal.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fnot_equal.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fnot_equal.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_compare %15, %16 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_compare %18, %19 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_compare %21, %22 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_compare %27, %29 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_compare %29, %31 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_compare %31, %33 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_compare %33, %35 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_compare %34, %36 {comparison_direction = #chlo<comparison_direction NE>} : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @NotEqual_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "0e087e2fcdb9926a90c9dbc959627d934c81f2a5",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/ones_like.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @OnesLike_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = 1 : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "f80dde47652bcb6167618821cf34d3cdb516dbf2",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/ones_like_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @OnesLike_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = #complex.number<:scalar_type 1.000000e+00, 0.000000e+00> : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "7b8d21a7309cbd1b2445a59da723509f8a3a8d7a",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/ones_like_float.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fones_like_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @OnesLike_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = 1.000000e+00 : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "bd8e475ba750ee4e9ea7b485765b61fd40d51382",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/polygamma.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fpolygamma.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fpolygamma.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fpolygamma.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_polygamma %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_polygamma %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_polygamma %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_polygamma %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_polygamma %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_polygamma %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_polygamma %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_polygamma %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Polygamma_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "e96889c5b40223e9d3ff4712d2b4f5b0a92632f6",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/pow.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fpow.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fpow.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fpow.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_power %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_power %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_power %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_power %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_power %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_power %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_power %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_power %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Pow_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "c2de6e55ba75c6e8d5b32e0e2c037c2c8b724354",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/real.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freal.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freal.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freal.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Real_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.real %2 : (tensor<?xelem_type>) -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "46ce08c3d69a2e8a96575eb33a8846d4b12bcde3",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/reciprocal.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -3,8 +3,8 @@ func.func @Reciprocal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %1 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %2 = shape.num_elements %1 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %2 : tensor<1xindex>\n-  %3 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %3 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %4 = chlo.broadcast_divide %0, %3 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %5 = mhlo.dynamic_reshape %4, %1 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %5 = tensor.reshape %4(%1) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %5 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "155b8349297adcdff8a21cb1655be1a33aacc0d0",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/reciprocal_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -3,8 +3,8 @@ func.func @Reciprocal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %1 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %2 = shape.num_elements %1 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %2 : tensor<1xindex>\n-  %3 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %3 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %4 = chlo.broadcast_divide %0, %3 : (tensor<output_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %5 = mhlo.dynamic_reshape %4, %1 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %5 = tensor.reshape %4(%1) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %5 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "8786983280f66e17fdc28fdb6d601eb4c03fa954",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/reciprocal_float.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Freciprocal_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -3,8 +3,8 @@ func.func @Reciprocal_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %1 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %2 = shape.num_elements %1 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %2 : tensor<1xindex>\n-  %3 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %3 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %4 = chlo.broadcast_divide %0, %3 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %5 = mhlo.dynamic_reshape %4, %1 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %5 = tensor.reshape %4(%1) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %5 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "f843f375e45fcb07133a48e077ba9603893f9c12",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/relu.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -3,8 +3,8 @@ func.func @Relu_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %1 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %2 = shape.num_elements %1 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %2 : tensor<1xindex>\n-  %3 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %3 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %4 = chlo.broadcast_maximum %3, %0 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n-  %5 = mhlo.dynamic_reshape %4, %1 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %5 = tensor.reshape %4(%1) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %5 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "62a610876c5dde118748ac77982c473750a9c323",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/relu_float.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -3,8 +3,8 @@ func.func @Relu_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %1 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %2 = shape.num_elements %1 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %2 : tensor<1xindex>\n-  %3 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %3 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %4 = chlo.broadcast_maximum %3, %0 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n-  %5 = mhlo.dynamic_reshape %4, %1 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %5 = tensor.reshape %4(%1) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %5 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "e7138ee5175c738fa86ac52115eddd3c8784935b",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/relu_grad.mlir.tmpl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu_grad.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu_grad.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frelu_grad.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -4,11 +4,11 @@ func.func @ReluGrad_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %\n   %2 = shape.any %0, %1 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %3 = shape.num_elements %2 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %3 : tensor<1xindex>\n-  %4 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-  %5 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %4 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %5 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %6 = \"chlo.constant_like\"(%5) {value = 0.000000e+00 : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n   %7 = mhlo.compare  GT, %5, %6 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xi1>\n   %8 = mhlo.select %7, %4, %6 : tensor<?xi1>, tensor<?xelem_type>\n-  %9 = mhlo.dynamic_reshape %8, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %9 = tensor.reshape %8(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %9 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "0dcf117b895a9c1ee32eac99c41481693acf83a5",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/right_shift.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fright_shift.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fright_shift.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fright_shift.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_shift_right_arithmetic %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_shift_right_arithmetic %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_shift_right_arithmetic %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_shift_right_arithmetic %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_shift_right_arithmetic %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_shift_right_arithmetic %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_shift_right_arithmetic %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_shift_right_arithmetic %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "7a348d36e88132068a82f50fe990180ef96e9956",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/right_shift_unsigned.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fright_shift_unsigned.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fright_shift_unsigned.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fright_shift_unsigned.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_shift_right_logical %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_shift_right_logical %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_shift_right_logical %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_shift_right_logical %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_shift_right_logical %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_shift_right_logical %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_shift_right_logical %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_shift_right_logical %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @RightShift_platform_elem_type_output_type(%arg0: tensor<*xelem_type>,\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "e50dc93b1bad04936f8828e180e4af14cd49b898",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/rint.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frint.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frint.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frint.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -6,7 +6,7 @@ func.func @Rint_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %4 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %5 = shape.num_elements %4 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %5 : tensor<1xindex>\n-  %6 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %6 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %7 = mhlo.floor %6 : tensor<?xelem_type>\n   %8 = chlo.broadcast_subtract %6, %7 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n   %9 = chlo.broadcast_compare %8, %2 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -22,6 +22,6 @@ func.func @Rint_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %19 = chlo.broadcast_select %17, %18, %7 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n   %20 = chlo.broadcast_compare %19, %3 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n   %21 = chlo.broadcast_select %20, %3, %19 : (tensor<?xi1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %22 = mhlo.dynamic_reshape %21, %4 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %22 = tensor.reshape %21(%4) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %22 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "8c08d88a56ffa8a187786f7836a3c20dbcc513d1",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/round_float.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fround_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fround_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fround_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -6,7 +6,7 @@ func.func @Round_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %4 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %5 = shape.num_elements %4 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %5 : tensor<1xindex>\n-  %6 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %6 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %7 = mhlo.floor %6 : tensor<?xelem_type>\n   %8 = chlo.broadcast_subtract %6, %7 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n   %9 = chlo.broadcast_compare %8, %2 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n@@ -22,6 +22,6 @@ func.func @Round_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %19 = chlo.broadcast_select %17, %18, %7 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n   %20 = chlo.broadcast_compare %19, %3 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n   %21 = chlo.broadcast_select %20, %3, %19 : (tensor<?xi1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %22 = mhlo.dynamic_reshape %21, %4 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %22 = tensor.reshape %21(%4) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %22 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "082a438d2c5fcd918ea01cebd144b3d97100e22a",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/rsqrt.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frsqrt.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frsqrt.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Frsqrt.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Rsqrt_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> t\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.rsqrt %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "02a1e6ef1a77d16abea6feb2cd65d0130456db78",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/select_v2.mlir.tmpl",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fselect_v2.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fselect_v2.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fselect_v2.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -25,9 +25,9 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n     %20 = shape.any %8, %9, %10 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n     %21 = shape.num_elements %20 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-    %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xi1>, tensor<1xindex>) -> tensor<?xi1>\n-    %23 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-    %24 = mhlo.dynamic_reshape %arg2, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xi1>, tensor<1xindex>) -> tensor<?xi1>\n+    %23 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %24 = tensor.reshape %arg2(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %25 = chlo.broadcast_select %22, %23, %24 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %25 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -44,13 +44,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n     %29 = scf.if %28 -> (tensor<*xelem_type>) {\n       %30 = shape.broadcast %20#0, %7 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n       %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-      %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<1xindex>) -> tensor<?xi1>\n+      %31 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<1xindex>) -> tensor<?xi1>\n       %32 = shape.broadcast %20#1, %7 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n       %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-      %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n       %34 = shape.broadcast %20#2, %7 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n       %cast_1 = tensor.cast %34 : tensor<?xindex> to tensor<1xindex>\n-      %35 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %35 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n       %36 = chlo.broadcast_select %31, %33, %35 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n       %cast_2 = tensor.cast %36 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast_2 : tensor<*xelem_type>\n@@ -59,13 +59,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n       %31 = scf.if %30 -> (tensor<*xelem_type>) {\n         %32 = shape.broadcast %20#0, %6 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n         %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-        %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<2xindex>) -> tensor<?x?xi1>\n+        %33 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<2xindex>) -> tensor<?x?xi1>\n         %34 = shape.broadcast %20#1, %6 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n         %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-        %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+        %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n         %36 = shape.broadcast %20#2, %6 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n         %cast_1 = tensor.cast %36 : tensor<?xindex> to tensor<2xindex>\n-        %37 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+        %37 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n         %38 = chlo.broadcast_select %33, %35, %37 : (tensor<?x?xi1>, tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n         %cast_2 = tensor.cast %38 : tensor<?x?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast_2 : tensor<*xelem_type>\n@@ -74,13 +74,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n         %33 = scf.if %32 -> (tensor<*xelem_type>) {\n           %34 = shape.broadcast %20#0, %5 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n           %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-          %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<3xindex>) -> tensor<?x?x?xi1>\n+          %35 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<3xindex>) -> tensor<?x?x?xi1>\n           %36 = shape.broadcast %20#1, %5 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-          %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+          %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n           %38 = shape.broadcast %20#2, %5 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n           %cast_1 = tensor.cast %38 : tensor<?xindex> to tensor<3xindex>\n-          %39 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+          %39 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n           %40 = chlo.broadcast_select %35, %37, %39 : (tensor<?x?x?xi1>, tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n           %cast_2 = tensor.cast %40 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_2 : tensor<*xelem_type>\n@@ -89,13 +89,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n           %35 = scf.if %34 -> (tensor<*xelem_type>) {\n             %36 = shape.broadcast %20#0, %4 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n             %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-            %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<4xindex>) -> tensor<?x?x?x?xi1>\n+            %37 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<4xindex>) -> tensor<?x?x?x?xi1>\n             %38 = shape.broadcast %20#1, %4 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-            %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+            %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n             %40 = shape.broadcast %20#2, %4 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n             %cast_1 = tensor.cast %40 : tensor<?xindex> to tensor<4xindex>\n-            %41 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+            %41 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n             %42 = chlo.broadcast_select %37, %39, %41 : (tensor<?x?x?x?xi1>, tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n             %cast_2 = tensor.cast %42 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_2 : tensor<*xelem_type>\n@@ -104,13 +104,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n             %37 = scf.if %36 -> (tensor<*xelem_type>) {\n               %38 = shape.broadcast %20#0, %3 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n               %cast = tensor.cast %38 : tensor<?xindex> to tensor<5xindex>\n-              %39 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<5xindex>) -> tensor<?x?x?x?x?xi1>\n+              %39 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<5xindex>) -> tensor<?x?x?x?x?xi1>\n               %40 = shape.broadcast %20#1, %3 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %40 : tensor<?xindex> to tensor<5xindex>\n-              %41 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+              %41 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n               %42 = shape.broadcast %20#2, %3 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n               %cast_1 = tensor.cast %42 : tensor<?xindex> to tensor<5xindex>\n-              %43 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+              %43 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n               %44 = chlo.broadcast_select %39, %41, %43 : (tensor<?x?x?x?x?xi1>, tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n               %cast_2 = tensor.cast %44 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_2 : tensor<*xelem_type>\n@@ -119,13 +119,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n               %39 = scf.if %38 -> (tensor<*xelem_type>) {\n                 %40 = shape.broadcast %20#0, %2 : tensor<?xindex>, tensor<6xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %40 : tensor<?xindex> to tensor<6xindex>\n-                %41 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<6xindex>) -> tensor<?x?x?x?x?x?xi1>\n+                %41 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<6xindex>) -> tensor<?x?x?x?x?x?xi1>\n                 %42 = shape.broadcast %20#1, %2 : tensor<?xindex>, tensor<6xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %42 : tensor<?xindex> to tensor<6xindex>\n-                %43 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<6xindex>) -> tensor<?x?x?x?x?x?xelem_type>\n+                %43 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<6xindex>) -> tensor<?x?x?x?x?x?xelem_type>\n                 %44 = shape.broadcast %20#2, %2 : tensor<?xindex>, tensor<6xindex> -> tensor<?xindex>\n                 %cast_1 = tensor.cast %44 : tensor<?xindex> to tensor<6xindex>\n-                %45 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<6xindex>) -> tensor<?x?x?x?x?x?xelem_type>\n+                %45 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<6xindex>) -> tensor<?x?x?x?x?x?xelem_type>\n                 %46 = chlo.broadcast_select %41, %43, %45 : (tensor<?x?x?x?x?x?xi1>, tensor<?x?x?x?x?x?xelem_type>, tensor<?x?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?x?xelem_type>\n                 %cast_2 = tensor.cast %46 : tensor<?x?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_2 : tensor<*xelem_type>\n@@ -134,13 +134,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n                 %41 = scf.if %40 -> (tensor<*xelem_type>) {\n                   %42 = shape.broadcast %20#0, %1 : tensor<?xindex>, tensor<7xindex> -> tensor<?xindex>\n                   %cast = tensor.cast %42 : tensor<?xindex> to tensor<7xindex>\n-                  %43 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<7xindex>) -> tensor<?x?x?x?x?x?x?xi1>\n+                  %43 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<7xindex>) -> tensor<?x?x?x?x?x?x?xi1>\n                   %44 = shape.broadcast %20#1, %1 : tensor<?xindex>, tensor<7xindex> -> tensor<?xindex>\n                   %cast_0 = tensor.cast %44 : tensor<?xindex> to tensor<7xindex>\n-                  %45 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<7xindex>) -> tensor<?x?x?x?x?x?x?xelem_type>\n+                  %45 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<7xindex>) -> tensor<?x?x?x?x?x?x?xelem_type>\n                   %46 = shape.broadcast %20#2, %1 : tensor<?xindex>, tensor<7xindex> -> tensor<?xindex>\n                   %cast_1 = tensor.cast %46 : tensor<?xindex> to tensor<7xindex>\n-                  %47 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<7xindex>) -> tensor<?x?x?x?x?x?x?xelem_type>\n+                  %47 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<7xindex>) -> tensor<?x?x?x?x?x?x?xelem_type>\n                   %48 = chlo.broadcast_select %43, %45, %47 : (tensor<?x?x?x?x?x?x?xi1>, tensor<?x?x?x?x?x?x?xelem_type>, tensor<?x?x?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?x?x?xelem_type>\n                   %cast_2 = tensor.cast %48 : tensor<?x?x?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                   scf.yield %cast_2 : tensor<*xelem_type>\n@@ -149,13 +149,13 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n                   cf.assert %42, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 8\"\n                   %43 = shape.broadcast %20#0, %0 : tensor<?xindex>, tensor<8xindex> -> tensor<?xindex>\n                   %cast = tensor.cast %43 : tensor<?xindex> to tensor<8xindex>\n-                  %44 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xi1>, tensor<8xindex>) -> tensor<?x?x?x?x?x?x?x?xi1>\n+                  %44 = tensor.reshape %arg0(%cast) : (tensor<*xi1>, tensor<8xindex>) -> tensor<?x?x?x?x?x?x?x?xi1>\n                   %45 = shape.broadcast %20#1, %0 : tensor<?xindex>, tensor<8xindex> -> tensor<?xindex>\n                   %cast_0 = tensor.cast %45 : tensor<?xindex> to tensor<8xindex>\n-                  %46 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<8xindex>) -> tensor<?x?x?x?x?x?x?x?xelem_type>\n+                  %46 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<8xindex>) -> tensor<?x?x?x?x?x?x?x?xelem_type>\n                   %47 = shape.broadcast %20#2, %0 : tensor<?xindex>, tensor<8xindex> -> tensor<?xindex>\n                   %cast_1 = tensor.cast %47 : tensor<?xindex> to tensor<8xindex>\n-                  %48 = mhlo.dynamic_reshape %arg2, %cast_1 : (tensor<*xelem_type>, tensor<8xindex>) -> tensor<?x?x?x?x?x?x?x?xelem_type>\n+                  %48 = tensor.reshape %arg2(%cast_1) : (tensor<*xelem_type>, tensor<8xindex>) -> tensor<?x?x?x?x?x?x?x?xelem_type>\n                   %49 = chlo.broadcast_select %44, %46, %48 : (tensor<?x?x?x?x?x?x?x?xi1>, tensor<?x?x?x?x?x?x?x?xelem_type>, tensor<?x?x?x?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?x?x?x?xelem_type>\n                   %cast_2 = tensor.cast %49 : tensor<?x?x?x?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                   scf.yield %cast_2 : tensor<*xelem_type>\n@@ -178,6 +178,6 @@ func.func @SelectV2_platform_elem_type_output_type(%arg0: tensor<*xi1>, %arg1: t\n   %16 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %17 = shape.shape_of %arg2 : tensor<*xelem_type> -> tensor<?xindex>\n   %18 = shape.broadcast %15, %16, %17 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %19 = mhlo.dynamic_reshape %14, %18 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %19 = tensor.reshape %14(%18) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %19 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "142e60f9d1197aae1b0b851d7ac7f80414fbebde",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/selu.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fselu.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fselu.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fselu.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -5,12 +5,12 @@ func.func @Selu_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %3 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %4 = shape.num_elements %3 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %4 : tensor<1xindex>\n-  %5 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %5 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %6 = chlo.broadcast_compare %5, %2 {comparison_direction = #chlo<comparison_direction GT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n   %7 = chlo.broadcast_multiply %5, %1 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n   %8 = mhlo.exponential_minus_one %5 : tensor<?xelem_type>\n   %9 = chlo.broadcast_multiply %8, %0 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n   %10 = chlo.broadcast_select %6, %7, %9 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %11 = mhlo.dynamic_reshape %10, %3 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %11 = tensor.reshape %10(%3) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %11 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "984ca6d7e64af194a4d85aca7ea44aa79420891a",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/sigmoid.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsigmoid.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsigmoid.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsigmoid.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Sigmoid_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) ->\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.logistic %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "95f7ec34336e5d53c8dbd42e995194af5a1bd60d",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/sign.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsign.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsign.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsign.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Sign_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.sign %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "1085cec03a707ed2e41e3325c1152a5e82bc201c",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/sin.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsin.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsin.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsin.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Sin_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.sine %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "025e779ae77f5f7d8e6e0d62702a46cf6bf87261",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/sinh.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsinh.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsinh.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsinh.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Sinh_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.sinh %2 : tensor<?xelem_type> -> tensor<?xoutput_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %4 : tensor<*xoutput_type>\n }"
        },
        {
            "sha": "2edd856020ed11874c5ae253ae4a00531826ee1f",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/softplus_f16.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f16.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f16.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f16.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -4,7 +4,7 @@ func.func @Softplus_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %2 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %3 = shape.num_elements %2 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %3 : tensor<1xindex>\n-  %4 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %4 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %5 = mhlo.exponential %4 : tensor<?xelem_type>\n   %6 = chlo.broadcast_add %0, %1 : (tensor<elem_type>, tensor<elem_type>) -> tensor<elem_type>\n   %7 = mhlo.negate %6 : tensor<elem_type>\n@@ -13,6 +13,6 @@ func.func @Softplus_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %10 = mhlo.log_plus_one %5 : tensor<?xelem_type>\n   %11 = mhlo.select %9, %5, %10 : tensor<?xi1>, tensor<?xelem_type>\n   %12 = mhlo.select %8, %4, %11 : tensor<?xi1>, tensor<?xelem_type>\n-  %13 = mhlo.dynamic_reshape %12, %2 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %12(%2) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "6560de16ef3bafd20cc205b972efea94514b2582",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/softplus_f32.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f32.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f32.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f32.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -4,7 +4,7 @@ func.func @Softplus_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %2 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %3 = shape.num_elements %2 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %3 : tensor<1xindex>\n-  %4 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %4 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %5 = mhlo.exponential %4 : tensor<?xelem_type>\n   %6 = chlo.broadcast_add %0, %1 : (tensor<elem_type>, tensor<elem_type>) -> tensor<elem_type>\n   %7 = mhlo.negate %6 : tensor<elem_type>\n@@ -13,6 +13,6 @@ func.func @Softplus_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %10 = mhlo.log_plus_one %5 : tensor<?xelem_type>\n   %11 = mhlo.select %9, %5, %10 : tensor<?xi1>, tensor<?xelem_type>\n   %12 = mhlo.select %8, %4, %11 : tensor<?xi1>, tensor<?xelem_type>\n-  %13 = mhlo.dynamic_reshape %12, %2 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %12(%2) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "6e2e7ae040df505ea2efbaa67d572b1a389ceaf7",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/softplus_f64.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f64.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f64.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftplus_f64.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -4,7 +4,7 @@ func.func @Softplus_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %2 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %3 = shape.num_elements %2 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %3 : tensor<1xindex>\n-  %4 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %4 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %5 = mhlo.exponential %4 : tensor<?xelem_type>\n   %6 = chlo.broadcast_add %0, %1 : (tensor<elem_type>, tensor<elem_type>) -> tensor<elem_type>\n   %7 = mhlo.negate %6 : tensor<elem_type>\n@@ -13,6 +13,6 @@ func.func @Softplus_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %10 = mhlo.log_plus_one %5 : tensor<?xelem_type>\n   %11 = mhlo.select %9, %5, %10 : tensor<?xi1>, tensor<?xelem_type>\n   %12 = mhlo.select %8, %4, %11 : tensor<?xi1>, tensor<?xelem_type>\n-  %13 = mhlo.dynamic_reshape %12, %2 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %12(%2) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "0bd77eb479b2a2c8ca1c654aa18539d7df61c5b2",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/softsign.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftsign.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftsign.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsoftsign.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,11 +2,11 @@ func.func @Softsign_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = 1.000000e+00 : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n   %4 = mhlo.abs %2 : tensor<?xelem_type>\n   %5 = mhlo.add %3, %4 : tensor<?xelem_type>\n   %6 = mhlo.divide %2, %5 : tensor<?xelem_type>\n-  %7 = mhlo.dynamic_reshape %6, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %7 = tensor.reshape %6(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %7 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "592c058651275f74f01e7ede1155b3f9a283ee11",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/sqrt.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsqrt.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsqrt.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsqrt.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Sqrt_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.sqrt %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "d151cc8799b989c33e5147985ff079b48540db5f",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/square.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsquare.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsquare.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsquare.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Square_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) ->\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = chlo.broadcast_multiply %2, %2 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "f1fc02e32ad90754767c358ecff187d5d83f42ac",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/squared_difference.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsquared_difference.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsquared_difference.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsquared_difference.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %18 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %18 : tensor<1xindex>\n-    %19 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %19 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_subtract %19, %20 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %22 = chlo.broadcast_multiply %21, %21 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %22 : tensor<?xelem_type> to tensor<*xelem_type>\n@@ -28,8 +29,8 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n     %20 = scf.if %19 -> (tensor<*xelem_type>) {\n       %21 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-      %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %23 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %23 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %24 = chlo.broadcast_subtract %22, %23 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %25 = chlo.broadcast_multiply %24, %24 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %25 : tensor<?xelem_type> to tensor<*xelem_type>\n@@ -40,8 +41,8 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n         %23 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %24 = shape.num_elements %23 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %24 : tensor<1xindex>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %26 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %26 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_subtract %25, %26 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %28 = chlo.broadcast_multiply %27, %27 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %28 : tensor<?xelem_type> to tensor<*xelem_type>\n@@ -56,10 +57,10 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n         %29 = scf.if %28 -> (tensor<*xelem_type>) {\n           %30 = shape.broadcast %23#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = shape.broadcast %23#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-          %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_subtract %31, %33 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %35 = chlo.broadcast_multiply %34, %34 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %35 : tensor<?xelem_type> to tensor<*xelem_type>\n@@ -69,10 +70,10 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n           %31 = scf.if %30 -> (tensor<*xelem_type>) {\n             %32 = shape.broadcast %23#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = shape.broadcast %23#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-            %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_subtract %33, %35 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %37 = chlo.broadcast_multiply %36, %36 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %37 : tensor<?x?xelem_type> to tensor<*xelem_type>\n@@ -82,10 +83,10 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n             %33 = scf.if %32 -> (tensor<*xelem_type>) {\n               %34 = shape.broadcast %23#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = shape.broadcast %23#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-              %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_subtract %35, %37 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %39 = chlo.broadcast_multiply %38, %38 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %39 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n@@ -95,10 +96,10 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n               %35 = scf.if %34 -> (tensor<*xelem_type>) {\n                 %36 = shape.broadcast %23#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = shape.broadcast %23#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_subtract %37, %39 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_multiply %40, %40 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %41 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n@@ -108,10 +109,10 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n                 cf.assert %36, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %37 = shape.broadcast %23#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = shape.broadcast %23#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %39 : tensor<?xindex> to tensor<5xindex>\n-                %40 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %40 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_subtract %38, %40 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_multiply %41, %41 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %42 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n@@ -136,6 +137,6 @@ func.func @SquaredDifference_platform_elem_type_output_type(%arg0: tensor<*xelem\n   %14 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %15 = shape.broadcast %13, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %16 = shape.broadcast %12, %15 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %17 = mhlo.dynamic_reshape %9, %16 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %17 = tensor.reshape %9(%16) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %17 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "0dc45dd6b2c6aae4f8e3c2fab9757dd6a4894690",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/sub.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsub.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsub.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fsub.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_subtract %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_subtract %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_subtract %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_subtract %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_subtract %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_subtract %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_subtract %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_subtract %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @Sub_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1:\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "5d05120862093bab8081cb94ed2ec11244ead1a5",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/tan.mlir.tmpl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftan.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftan.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftan.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Tan_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> ten\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-  %3 = mhlo.tan %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %3 = chlo.tan %2 : tensor<?xelem_type> -> tensor<?xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "3b85cebe9d8b1d4cba37b957ac3a59f69ec8b642",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/tanh.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftanh.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftanh.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftanh.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @Tanh_platform_elem_type_output_type(%arg0: tensor<*xelem_type>) -> te\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = mhlo.tanh %2 : tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "0fee6e0558b183c29547d1b3d54c68a2cabec9ce",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/truncate_div.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftruncate_div.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftruncate_div.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftruncate_div.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xelem_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_divide %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %cast = tensor.cast %17 : tensor<?xelem_type> to tensor<*xelem_type>\n     scf.yield %cast : tensor<*xelem_type>\n@@ -27,8 +28,8 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n     %16 = scf.if %15 -> (tensor<*xelem_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_divide %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %cast = tensor.cast %20 : tensor<?xelem_type> to tensor<*xelem_type>\n       scf.yield %cast : tensor<*xelem_type>\n@@ -38,8 +39,8 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_divide %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %cast = tensor.cast %23 : tensor<?xelem_type> to tensor<*xelem_type>\n         scf.yield %cast : tensor<*xelem_type>\n@@ -53,10 +54,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n         %25 = scf.if %24 -> (tensor<*xelem_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_divide %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %cast_1 = tensor.cast %30 : tensor<?xelem_type> to tensor<*xelem_type>\n           scf.yield %cast_1 : tensor<*xelem_type>\n@@ -65,10 +66,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n           %27 = scf.if %26 -> (tensor<*xelem_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_divide %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xelem_type> to tensor<*xelem_type>\n             scf.yield %cast_1 : tensor<*xelem_type>\n@@ -77,10 +78,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n             %29 = scf.if %28 -> (tensor<*xelem_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_divide %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xelem_type> to tensor<*xelem_type>\n               scf.yield %cast_1 : tensor<*xelem_type>\n@@ -89,10 +90,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n               %31 = scf.if %30 -> (tensor<*xelem_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_divide %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -101,10 +102,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_divide %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xelem_type> to tensor<*xelem_type>\n                 scf.yield %cast_1 : tensor<*xelem_type>\n@@ -124,6 +125,6 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %13 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "a583b296c7b07830c5026e8e12555926ce66c726",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/truncate_div_float.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftruncate_div_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftruncate_div_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Ftruncate_div_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %22 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %22 : tensor<1xindex>\n-    %23 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %24 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %23 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %24 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %25 = chlo.broadcast_divide %23, %24 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %26 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n     %27 = mhlo.ceil %25 : tensor<?xelem_type>\n@@ -32,8 +33,8 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n     %24 = scf.if %23 -> (tensor<*xelem_type>) {\n       %25 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %25 : tensor<1xindex>\n-      %26 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %27 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %26 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %27 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %28 = chlo.broadcast_divide %26, %27 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %29 = chlo.broadcast_compare %28, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %30 = mhlo.ceil %28 : tensor<?xelem_type>\n@@ -47,8 +48,8 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n         %27 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %28 = shape.num_elements %27 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %28 : tensor<1xindex>\n-        %29 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %30 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %29 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %30 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %31 = chlo.broadcast_divide %29, %30 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %32 = chlo.broadcast_compare %31, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %33 = mhlo.ceil %31 : tensor<?xelem_type>\n@@ -66,10 +67,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n         %33 = scf.if %32 -> (tensor<*xelem_type>) {\n           %34 = shape.broadcast %27#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %34 : tensor<?xindex> to tensor<1xindex>\n-          %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %36 = shape.broadcast %27#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<1xindex>\n-          %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %38 = chlo.broadcast_divide %35, %37 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %39 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %40 = mhlo.ceil %38 : tensor<?xelem_type>\n@@ -82,10 +83,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n           %35 = scf.if %34 -> (tensor<*xelem_type>) {\n             %36 = shape.broadcast %27#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %36 : tensor<?xindex> to tensor<2xindex>\n-            %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %38 = shape.broadcast %27#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<2xindex>\n-            %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %40 = chlo.broadcast_divide %37, %39 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %41 = chlo.broadcast_compare %40, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %42 = mhlo.ceil %40 : tensor<?x?xelem_type>\n@@ -98,10 +99,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n             %37 = scf.if %36 -> (tensor<*xelem_type>) {\n               %38 = shape.broadcast %27#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %38 : tensor<?xindex> to tensor<3xindex>\n-              %39 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %39 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %40 = shape.broadcast %27#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %40 : tensor<?xindex> to tensor<3xindex>\n-              %41 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %41 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %42 = chlo.broadcast_divide %39, %41 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %43 = chlo.broadcast_compare %42, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %44 = mhlo.ceil %42 : tensor<?x?x?xelem_type>\n@@ -114,10 +115,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n               %39 = scf.if %38 -> (tensor<*xelem_type>) {\n                 %40 = shape.broadcast %27#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %40 : tensor<?xindex> to tensor<4xindex>\n-                %41 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %41 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %42 = shape.broadcast %27#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %42 : tensor<?xindex> to tensor<4xindex>\n-                %43 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %43 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %44 = chlo.broadcast_divide %41, %43 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %45 = chlo.broadcast_compare %44, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %46 = mhlo.ceil %44 : tensor<?x?x?x?xelem_type>\n@@ -130,10 +131,10 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n                 cf.assert %40, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %41 = shape.broadcast %27#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %41 : tensor<?xindex> to tensor<5xindex>\n-                %42 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %42 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %43 = shape.broadcast %27#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %43 : tensor<?xindex> to tensor<5xindex>\n-                %44 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %44 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %45 = chlo.broadcast_divide %42, %44 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %46 = chlo.broadcast_compare %45, %5 {comparison_direction = #chlo<comparison_direction LT>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %47 = mhlo.ceil %45 : tensor<?x?x?x?x?xelem_type>\n@@ -164,6 +165,6 @@ func.func @TruncateDiv_platform_elem_type_output_type(%arg0: tensor<*xelem_type>\n   %18 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %19 = shape.broadcast %17, %18 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %20 = shape.broadcast %13, %16, %19 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %21 = mhlo.dynamic_reshape %10, %20 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %21 = tensor.reshape %10(%20) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %21 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "429b017f986c022e4130269cb705e671dd27bbad",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/xdivy.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxdivy.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxdivy.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxdivy.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %18 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %18 : tensor<1xindex>\n-    %19 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %19 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n     %22 = chlo.broadcast_divide %19, %20 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %23 = chlo.broadcast_select %21, %19, %22 : (tensor<i1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -30,8 +31,8 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n     %20 = scf.if %19 -> (tensor<*xelem_type>) {\n       %21 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-      %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %23 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %23 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %24 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = chlo.broadcast_divide %22, %23 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %26 = chlo.broadcast_select %24, %22, %25 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -43,8 +44,8 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %23 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %24 = shape.num_elements %23 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %24 : tensor<1xindex>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %26 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %26 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = chlo.broadcast_divide %25, %26 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %29 = chlo.broadcast_select %27, %25, %28 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -60,10 +61,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %29 = scf.if %28 -> (tensor<*xelem_type>) {\n           %30 = shape.broadcast %23#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = shape.broadcast %23#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-          %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %31, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = chlo.broadcast_divide %31, %33 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %36 = chlo.broadcast_select %34, %31, %35 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -74,10 +75,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n           %31 = scf.if %30 -> (tensor<*xelem_type>) {\n             %32 = shape.broadcast %23#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = shape.broadcast %23#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-            %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = chlo.broadcast_divide %33, %35 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %38 = chlo.broadcast_select %36, %33, %37 : (tensor<?x?xi1>, tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -88,10 +89,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n             %33 = scf.if %32 -> (tensor<*xelem_type>) {\n               %34 = shape.broadcast %23#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = shape.broadcast %23#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-              %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = chlo.broadcast_divide %35, %37 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %40 = chlo.broadcast_select %38, %35, %39 : (tensor<?x?x?xi1>, tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -102,10 +103,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n               %35 = scf.if %34 -> (tensor<*xelem_type>) {\n                 %36 = shape.broadcast %23#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = shape.broadcast %23#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = chlo.broadcast_divide %37, %39 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_select %40, %37, %41 : (tensor<?x?x?x?xi1>, tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -116,10 +117,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n                 cf.assert %36, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %37 = shape.broadcast %23#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = shape.broadcast %23#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %39 : tensor<?xindex> to tensor<5xindex>\n-                %40 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %40 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = chlo.broadcast_divide %38, %40 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %43 = chlo.broadcast_select %41, %38, %42 : (tensor<?x?x?x?x?xi1>, tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -144,6 +145,6 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %14 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %15 = shape.broadcast %13, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %16 = shape.broadcast %11, %12, %15 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %17 = mhlo.dynamic_reshape %10, %16 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %17 = tensor.reshape %10(%16) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %17 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "ae2ce94f00e092ec6fc35c653cf9836c42a77169",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/xdivy_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxdivy_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxdivy_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxdivy_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %18 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %18 : tensor<1xindex>\n-    %19 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %19 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n     %22 = chlo.broadcast_divide %19, %20 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n     %23 = chlo.broadcast_select %21, %19, %22 : (tensor<i1>, tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -30,8 +31,8 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n     %20 = scf.if %19 -> (tensor<*xelem_type>) {\n       %21 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-      %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %23 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %23 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %24 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = chlo.broadcast_divide %22, %23 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n       %26 = chlo.broadcast_select %24, %22, %25 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -43,8 +44,8 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %23 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %24 = shape.num_elements %23 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %24 : tensor<1xindex>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %26 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %26 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = chlo.broadcast_divide %25, %26 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n         %29 = chlo.broadcast_select %27, %25, %28 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -60,10 +61,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %29 = scf.if %28 -> (tensor<*xelem_type>) {\n           %30 = shape.broadcast %23#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = shape.broadcast %23#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-          %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %31, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = chlo.broadcast_divide %31, %33 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n           %36 = chlo.broadcast_select %34, %31, %35 : (tensor<?xi1>, tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -74,10 +75,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n           %31 = scf.if %30 -> (tensor<*xelem_type>) {\n             %32 = shape.broadcast %23#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = shape.broadcast %23#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-            %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = chlo.broadcast_divide %33, %35 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n             %38 = chlo.broadcast_select %36, %33, %37 : (tensor<?x?xi1>, tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -88,10 +89,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n             %33 = scf.if %32 -> (tensor<*xelem_type>) {\n               %34 = shape.broadcast %23#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = shape.broadcast %23#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-              %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = chlo.broadcast_divide %35, %37 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n               %40 = chlo.broadcast_select %38, %35, %39 : (tensor<?x?x?xi1>, tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -102,10 +103,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n               %35 = scf.if %34 -> (tensor<*xelem_type>) {\n                 %36 = shape.broadcast %23#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = shape.broadcast %23#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = chlo.broadcast_divide %37, %39 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_select %40, %37, %41 : (tensor<?x?x?x?xi1>, tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -116,10 +117,10 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n                 cf.assert %36, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %37 = shape.broadcast %23#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = shape.broadcast %23#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %39 : tensor<?xindex> to tensor<5xindex>\n-                %40 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %40 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = chlo.broadcast_divide %38, %40 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n                 %43 = chlo.broadcast_select %41, %38, %42 : (tensor<?x?x?x?x?xi1>, tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -144,6 +145,6 @@ func.func @Xdivy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %14 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %15 = shape.broadcast %13, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %16 = shape.broadcast %11, %12, %15 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %17 = mhlo.dynamic_reshape %10, %16 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %17 = tensor.reshape %10(%16) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %17 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "97d76e45a604efa69297108a2b4b741662452166",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/xlog1py.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlog1py.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlog1py.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlog1py.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %18 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %18 : tensor<1xindex>\n-    %19 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %19 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n     %22 = mhlo.log_plus_one %20 : tensor<?xelem_type>\n     %23 = chlo.broadcast_multiply %19, %22 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -31,8 +32,8 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n     %20 = scf.if %19 -> (tensor<*xelem_type>) {\n       %21 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-      %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %23 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %23 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %24 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = mhlo.log_plus_one %23 : tensor<elem_type>\n       %26 = chlo.broadcast_multiply %22, %25 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n@@ -45,8 +46,8 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %23 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %24 = shape.num_elements %23 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %24 : tensor<1xindex>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %26 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %26 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = mhlo.log_plus_one %26 : tensor<?xelem_type>\n         %29 = chlo.broadcast_multiply %25, %28 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -63,10 +64,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %29 = scf.if %28 -> (tensor<*xelem_type>) {\n           %30 = shape.broadcast %23#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = shape.broadcast %23#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-          %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %31, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = mhlo.log_plus_one %33 : tensor<?xelem_type>\n           %36 = chlo.broadcast_multiply %31, %35 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -78,10 +79,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n           %31 = scf.if %30 -> (tensor<*xelem_type>) {\n             %32 = shape.broadcast %23#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = shape.broadcast %23#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-            %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = mhlo.log_plus_one %35 : tensor<?x?xelem_type>\n             %38 = chlo.broadcast_multiply %33, %37 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -93,10 +94,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n             %33 = scf.if %32 -> (tensor<*xelem_type>) {\n               %34 = shape.broadcast %23#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = shape.broadcast %23#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-              %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = mhlo.log_plus_one %37 : tensor<?x?x?xelem_type>\n               %40 = chlo.broadcast_multiply %35, %39 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -108,10 +109,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n               %35 = scf.if %34 -> (tensor<*xelem_type>) {\n                 %36 = shape.broadcast %23#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = shape.broadcast %23#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = mhlo.log_plus_one %39 : tensor<?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_multiply %37, %41 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -123,10 +124,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n                 cf.assert %36, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %37 = shape.broadcast %23#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = shape.broadcast %23#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %39 : tensor<?xindex> to tensor<5xindex>\n-                %40 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %40 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = mhlo.log_plus_one %40 : tensor<?x?x?x?x?xelem_type>\n                 %43 = chlo.broadcast_multiply %38, %42 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -152,6 +153,6 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %14 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %15 = shape.broadcast %13, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %16 = shape.broadcast %11, %12, %15 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %17 = mhlo.dynamic_reshape %10, %16 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %17 = tensor.reshape %10(%16) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %17 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "8c884d20688e15a09a0d5fbc3d9f1e091388084a",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/xlog1py_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlog1py_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlog1py_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlog1py_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %18 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %18 : tensor<1xindex>\n-    %19 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %19 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n     %22 = mhlo.log_plus_one %20 : tensor<?xelem_type>\n     %23 = chlo.broadcast_multiply %19, %22 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -31,8 +32,8 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n     %20 = scf.if %19 -> (tensor<*xelem_type>) {\n       %21 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-      %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %23 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %23 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %24 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = mhlo.log_plus_one %23 : tensor<elem_type>\n       %26 = chlo.broadcast_multiply %22, %25 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n@@ -45,8 +46,8 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %23 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %24 = shape.num_elements %23 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %24 : tensor<1xindex>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %26 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %26 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = mhlo.log_plus_one %26 : tensor<?xelem_type>\n         %29 = chlo.broadcast_multiply %25, %28 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -63,10 +64,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n         %29 = scf.if %28 -> (tensor<*xelem_type>) {\n           %30 = shape.broadcast %23#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = shape.broadcast %23#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-          %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %31, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = mhlo.log_plus_one %33 : tensor<?xelem_type>\n           %36 = chlo.broadcast_multiply %31, %35 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -78,10 +79,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n           %31 = scf.if %30 -> (tensor<*xelem_type>) {\n             %32 = shape.broadcast %23#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = shape.broadcast %23#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-            %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = mhlo.log_plus_one %35 : tensor<?x?xelem_type>\n             %38 = chlo.broadcast_multiply %33, %37 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -93,10 +94,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n             %33 = scf.if %32 -> (tensor<*xelem_type>) {\n               %34 = shape.broadcast %23#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = shape.broadcast %23#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-              %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = mhlo.log_plus_one %37 : tensor<?x?x?xelem_type>\n               %40 = chlo.broadcast_multiply %35, %39 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -108,10 +109,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n               %35 = scf.if %34 -> (tensor<*xelem_type>) {\n                 %36 = shape.broadcast %23#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = shape.broadcast %23#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = mhlo.log_plus_one %39 : tensor<?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_multiply %37, %41 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -123,10 +124,10 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n                 cf.assert %36, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %37 = shape.broadcast %23#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = shape.broadcast %23#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %39 : tensor<?xindex> to tensor<5xindex>\n-                %40 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %40 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = mhlo.log_plus_one %40 : tensor<?x?x?x?x?xelem_type>\n                 %43 = chlo.broadcast_multiply %38, %42 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -152,6 +153,6 @@ func.func @Xlog1py_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %a\n   %14 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %15 = shape.broadcast %13, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %16 = shape.broadcast %11, %12, %15 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %17 = mhlo.dynamic_reshape %10, %16 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %17 = tensor.reshape %10(%16) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %17 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "06c20a8226e00d0e8a92aee121d70678da1960e0",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/xlogy.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlogy.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlogy.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlogy.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %18 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %18 : tensor<1xindex>\n-    %19 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %19 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n     %22 = mhlo.log %20 : tensor<?xelem_type>\n     %23 = chlo.broadcast_multiply %19, %22 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -31,8 +32,8 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n     %20 = scf.if %19 -> (tensor<*xelem_type>) {\n       %21 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-      %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %23 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %23 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %24 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = mhlo.log %23 : tensor<elem_type>\n       %26 = chlo.broadcast_multiply %22, %25 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n@@ -45,8 +46,8 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %23 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %24 = shape.num_elements %23 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %24 : tensor<1xindex>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %26 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %26 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = mhlo.log %26 : tensor<?xelem_type>\n         %29 = chlo.broadcast_multiply %25, %28 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -63,10 +64,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %29 = scf.if %28 -> (tensor<*xelem_type>) {\n           %30 = shape.broadcast %23#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = shape.broadcast %23#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-          %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %31, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = mhlo.log %33 : tensor<?xelem_type>\n           %36 = chlo.broadcast_multiply %31, %35 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -78,10 +79,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n           %31 = scf.if %30 -> (tensor<*xelem_type>) {\n             %32 = shape.broadcast %23#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = shape.broadcast %23#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-            %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = mhlo.log %35 : tensor<?x?xelem_type>\n             %38 = chlo.broadcast_multiply %33, %37 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -93,10 +94,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n             %33 = scf.if %32 -> (tensor<*xelem_type>) {\n               %34 = shape.broadcast %23#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = shape.broadcast %23#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-              %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = mhlo.log %37 : tensor<?x?x?xelem_type>\n               %40 = chlo.broadcast_multiply %35, %39 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -108,10 +109,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n               %35 = scf.if %34 -> (tensor<*xelem_type>) {\n                 %36 = shape.broadcast %23#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = shape.broadcast %23#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = mhlo.log %39 : tensor<?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_multiply %37, %41 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -123,10 +124,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n                 cf.assert %36, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %37 = shape.broadcast %23#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = shape.broadcast %23#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %39 : tensor<?xindex> to tensor<5xindex>\n-                %40 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %40 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = mhlo.log %40 : tensor<?x?x?x?x?xelem_type>\n                 %43 = chlo.broadcast_multiply %38, %42 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -152,6 +153,6 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %14 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %15 = shape.broadcast %13, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %16 = shape.broadcast %11, %12, %15 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %17 = mhlo.dynamic_reshape %10, %16 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %17 = tensor.reshape %10(%16) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %17 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "172ba62d10c28ca70185003bcd211d479b7be3da",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/xlogy_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlogy_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlogy_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fxlogy_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -14,11 +14,12 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %7 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %8 = shape.num_elements %6 : tensor<?xindex> -> index\n   %9 = arith.cmpi eq, %8, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %10 = scf.if %9 -> (tensor<*xelem_type>) {\n     %18 = shape.num_elements %7 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %18 : tensor<1xindex>\n-    %19 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %20 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %19 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %20 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %21 = chlo.broadcast_compare %19, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<elem_type>, tensor<elem_type>) -> tensor<i1>\n     %22 = mhlo.log %20 : tensor<?xelem_type>\n     %23 = chlo.broadcast_multiply %19, %22 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -31,8 +32,8 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n     %20 = scf.if %19 -> (tensor<*xelem_type>) {\n       %21 = shape.num_elements %6 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %21 : tensor<1xindex>\n-      %22 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %23 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %22 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %23 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %24 = chlo.broadcast_compare %22, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n       %25 = mhlo.log %23 : tensor<elem_type>\n       %26 = chlo.broadcast_multiply %22, %25 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xelem_type>\n@@ -45,8 +46,8 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %23 = shape.any %6, %7 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %24 = shape.num_elements %23 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %24 : tensor<1xindex>\n-        %25 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %26 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %25 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %26 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %27 = chlo.broadcast_compare %25, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n         %28 = mhlo.log %26 : tensor<?xelem_type>\n         %29 = chlo.broadcast_multiply %25, %28 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -63,10 +64,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n         %29 = scf.if %28 -> (tensor<*xelem_type>) {\n           %30 = shape.broadcast %23#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %30 : tensor<?xindex> to tensor<1xindex>\n-          %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %32 = shape.broadcast %23#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<1xindex>\n-          %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %34 = chlo.broadcast_compare %31, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xi1>\n           %35 = mhlo.log %33 : tensor<?xelem_type>\n           %36 = chlo.broadcast_multiply %31, %35 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xelem_type>\n@@ -78,10 +79,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n           %31 = scf.if %30 -> (tensor<*xelem_type>) {\n             %32 = shape.broadcast %23#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %32 : tensor<?xindex> to tensor<2xindex>\n-            %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %34 = shape.broadcast %23#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<2xindex>\n-            %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %36 = chlo.broadcast_compare %33, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?xelem_type>, tensor<elem_type>) -> tensor<?x?xi1>\n             %37 = mhlo.log %35 : tensor<?x?xelem_type>\n             %38 = chlo.broadcast_multiply %33, %37 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xelem_type>\n@@ -93,10 +94,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n             %33 = scf.if %32 -> (tensor<*xelem_type>) {\n               %34 = shape.broadcast %23#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %34 : tensor<?xindex> to tensor<3xindex>\n-              %35 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %35 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %36 = shape.broadcast %23#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %36 : tensor<?xindex> to tensor<3xindex>\n-              %37 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %37 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %38 = chlo.broadcast_compare %35, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?xi1>\n               %39 = mhlo.log %37 : tensor<?x?x?xelem_type>\n               %40 = chlo.broadcast_multiply %35, %39 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xelem_type>\n@@ -108,10 +109,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n               %35 = scf.if %34 -> (tensor<*xelem_type>) {\n                 %36 = shape.broadcast %23#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %36 : tensor<?xindex> to tensor<4xindex>\n-                %37 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %37 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %38 = shape.broadcast %23#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %38 : tensor<?xindex> to tensor<4xindex>\n-                %39 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %39 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %40 = chlo.broadcast_compare %37, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?xi1>\n                 %41 = mhlo.log %39 : tensor<?x?x?x?xelem_type>\n                 %42 = chlo.broadcast_multiply %37, %41 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xelem_type>\n@@ -123,10 +124,10 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n                 cf.assert %36, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %37 = shape.broadcast %23#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %37 : tensor<?xindex> to tensor<5xindex>\n-                %38 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %38 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %39 = shape.broadcast %23#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %39 : tensor<?xindex> to tensor<5xindex>\n-                %40 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %40 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %41 = chlo.broadcast_compare %38, %5 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<?x?x?x?x?xelem_type>, tensor<elem_type>) -> tensor<?x?x?x?x?xi1>\n                 %42 = mhlo.log %40 : tensor<?x?x?x?x?xelem_type>\n                 %43 = chlo.broadcast_multiply %38, %42 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xelem_type>\n@@ -152,6 +153,6 @@ func.func @Xlogy_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg\n   %14 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %15 = shape.broadcast %13, %14 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n   %16 = shape.broadcast %11, %12, %15 : tensor<?xindex>, tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %17 = mhlo.dynamic_reshape %10, %16 : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %17 = tensor.reshape %10(%16) : (tensor<*xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %17 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "2e356ee029d61288f61ed50f6adcb7f865410abf",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/zeros_like.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @ZerosLike_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = 0 : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "fba3c56744dd896fb7458c8aba3768b4bb49f412",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/zeros_like_cmplx.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like_cmplx.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like_cmplx.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like_cmplx.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @ZerosLike_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = #complex.number<:scalar_type 0.000000e+00, 0.000000e+00> : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "a198ad24516f48993b8e7d4f15af960f6417ac24",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/zeros_like_float.mlir.tmpl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like_float.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like_float.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeros_like_float.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -2,8 +2,8 @@ func.func @ZerosLike_platform_elem_type_output_type(%arg0: tensor<*xelem_type>)\n   %0 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %1 = shape.num_elements %0 : tensor<?xindex> -> index\n   %from_elements = tensor.from_elements %1 : tensor<1xindex>\n-  %2 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+  %2 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n   %3 = \"chlo.constant_like\"(%2) {value = 0.000000e+00 : elem_type} : (tensor<?xelem_type>) -> tensor<?xelem_type>\n-  %4 = mhlo.dynamic_reshape %3, %0 : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n+  %4 = tensor.reshape %3(%0) : (tensor<?xelem_type>, tensor<?xindex>) -> tensor<*xelem_type>\n   return %4 : tensor<*xelem_type>\n }"
        },
        {
            "sha": "2fd61121fc0f995a24ba3867d1c1d913fdedd64b",
            "filename": "tensorflow/core/kernels/mlir_generated/op_definitions/zeta.mlir.tmpl",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeta.mlir.tmpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeta.mlir.tmpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fop_definitions%2Fzeta.mlir.tmpl?ref=ce6202dd8482f022a0dcc1e6df95fa9e14b6b65f",
            "patch": "@@ -13,11 +13,12 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n   %6 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %7 = shape.num_elements %5 : tensor<?xindex> -> index\n   %8 = arith.cmpi eq, %7, %c1 : index\n+  %c_empty = arith.constant dense<> : tensor<0xindex>\n   %9 = scf.if %8 -> (tensor<*xoutput_type>) {\n     %14 = shape.num_elements %6 : tensor<?xindex> -> index\n     %from_elements = tensor.from_elements %14 : tensor<1xindex>\n-    %15 = mhlo.reshape %arg0 : (tensor<*xelem_type>) -> tensor<elem_type>\n-    %16 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+    %15 = tensor.reshape %arg0(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n+    %16 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n     %17 = chlo.broadcast_zeta %15, %16 : (tensor<elem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n     %cast = tensor.cast %17 : tensor<?xoutput_type> to tensor<*xoutput_type>\n     scf.yield %cast : tensor<*xoutput_type>\n@@ -27,8 +28,8 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n     %16 = scf.if %15 -> (tensor<*xoutput_type>) {\n       %17 = shape.num_elements %5 : tensor<?xindex> -> index\n       %from_elements = tensor.from_elements %17 : tensor<1xindex>\n-      %18 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-      %19 = mhlo.reshape %arg1 : (tensor<*xelem_type>) -> tensor<elem_type>\n+      %18 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+      %19 = tensor.reshape %arg1(%c_empty) : (tensor<*xelem_type>, tensor<0xindex>) -> tensor<elem_type>\n       %20 = chlo.broadcast_zeta %18, %19 : (tensor<?xelem_type>, tensor<elem_type>) -> tensor<?xoutput_type>\n       %cast = tensor.cast %20 : tensor<?xoutput_type> to tensor<*xoutput_type>\n       scf.yield %cast : tensor<*xoutput_type>\n@@ -38,8 +39,8 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n         %19 = shape.any %5, %6 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n         %20 = shape.num_elements %19 : tensor<?xindex> -> index\n         %from_elements = tensor.from_elements %20 : tensor<1xindex>\n-        %21 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n-        %22 = mhlo.dynamic_reshape %arg1, %from_elements : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %21 = tensor.reshape %arg0(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+        %22 = tensor.reshape %arg1(%from_elements) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n         %23 = chlo.broadcast_zeta %21, %22 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n         %cast = tensor.cast %23 : tensor<?xoutput_type> to tensor<*xoutput_type>\n         scf.yield %cast : tensor<*xoutput_type>\n@@ -53,10 +54,10 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n         %25 = scf.if %24 -> (tensor<*xoutput_type>) {\n           %26 = shape.broadcast %19#0, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast = tensor.cast %26 : tensor<?xindex> to tensor<1xindex>\n-          %27 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %27 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %28 = shape.broadcast %19#1, %4 : tensor<?xindex>, tensor<1xindex> -> tensor<?xindex>\n           %cast_0 = tensor.cast %28 : tensor<?xindex> to tensor<1xindex>\n-          %29 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n+          %29 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<1xindex>) -> tensor<?xelem_type>\n           %30 = chlo.broadcast_zeta %27, %29 : (tensor<?xelem_type>, tensor<?xelem_type>) -> tensor<?xoutput_type>\n           %cast_1 = tensor.cast %30 : tensor<?xoutput_type> to tensor<*xoutput_type>\n           scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -65,10 +66,10 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n           %27 = scf.if %26 -> (tensor<*xoutput_type>) {\n             %28 = shape.broadcast %19#0, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast = tensor.cast %28 : tensor<?xindex> to tensor<2xindex>\n-            %29 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %29 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %30 = shape.broadcast %19#1, %3 : tensor<?xindex>, tensor<2xindex> -> tensor<?xindex>\n             %cast_0 = tensor.cast %30 : tensor<?xindex> to tensor<2xindex>\n-            %31 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n+            %31 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<2xindex>) -> tensor<?x?xelem_type>\n             %32 = chlo.broadcast_zeta %29, %31 : (tensor<?x?xelem_type>, tensor<?x?xelem_type>) -> tensor<?x?xoutput_type>\n             %cast_1 = tensor.cast %32 : tensor<?x?xoutput_type> to tensor<*xoutput_type>\n             scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -77,10 +78,10 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n             %29 = scf.if %28 -> (tensor<*xoutput_type>) {\n               %30 = shape.broadcast %19#0, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast = tensor.cast %30 : tensor<?xindex> to tensor<3xindex>\n-              %31 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %31 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %32 = shape.broadcast %19#1, %2 : tensor<?xindex>, tensor<3xindex> -> tensor<?xindex>\n               %cast_0 = tensor.cast %32 : tensor<?xindex> to tensor<3xindex>\n-              %33 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n+              %33 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<3xindex>) -> tensor<?x?x?xelem_type>\n               %34 = chlo.broadcast_zeta %31, %33 : (tensor<?x?x?xelem_type>, tensor<?x?x?xelem_type>) -> tensor<?x?x?xoutput_type>\n               %cast_1 = tensor.cast %34 : tensor<?x?x?xoutput_type> to tensor<*xoutput_type>\n               scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -89,10 +90,10 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n               %31 = scf.if %30 -> (tensor<*xoutput_type>) {\n                 %32 = shape.broadcast %19#0, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %32 : tensor<?xindex> to tensor<4xindex>\n-                %33 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %33 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %34 = shape.broadcast %19#1, %1 : tensor<?xindex>, tensor<4xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %34 : tensor<?xindex> to tensor<4xindex>\n-                %35 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n+                %35 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<4xindex>) -> tensor<?x?x?x?xelem_type>\n                 %36 = chlo.broadcast_zeta %33, %35 : (tensor<?x?x?x?xelem_type>, tensor<?x?x?x?xelem_type>) -> tensor<?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %36 : tensor<?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -101,10 +102,10 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n                 cf.assert %32, \"Input for dynamic binary or n-ary op lowering was of a rank greater than 5\"\n                 %33 = shape.broadcast %19#0, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast = tensor.cast %33 : tensor<?xindex> to tensor<5xindex>\n-                %34 = mhlo.dynamic_reshape %arg0, %cast : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %34 = tensor.reshape %arg0(%cast) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %35 = shape.broadcast %19#1, %0 : tensor<?xindex>, tensor<5xindex> -> tensor<?xindex>\n                 %cast_0 = tensor.cast %35 : tensor<?xindex> to tensor<5xindex>\n-                %36 = mhlo.dynamic_reshape %arg1, %cast_0 : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n+                %36 = tensor.reshape %arg1(%cast_0) : (tensor<*xelem_type>, tensor<5xindex>) -> tensor<?x?x?x?x?xelem_type>\n                 %37 = chlo.broadcast_zeta %34, %36 : (tensor<?x?x?x?x?xelem_type>, tensor<?x?x?x?x?xelem_type>) -> tensor<?x?x?x?x?xoutput_type>\n                 %cast_1 = tensor.cast %37 : tensor<?x?x?x?x?xoutput_type> to tensor<*xoutput_type>\n                 scf.yield %cast_1 : tensor<*xoutput_type>\n@@ -124,6 +125,6 @@ func.func @Zeta_platform_elem_type_output_type(%arg0: tensor<*xelem_type>, %arg1\n   %10 = shape.shape_of %arg0 : tensor<*xelem_type> -> tensor<?xindex>\n   %11 = shape.shape_of %arg1 : tensor<*xelem_type> -> tensor<?xindex>\n   %12 = shape.broadcast %10, %11 : tensor<?xindex>, tensor<?xindex> -> tensor<?xindex>\n-  %13 = mhlo.dynamic_reshape %9, %12 : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n+  %13 = tensor.reshape %9(%12) : (tensor<*xoutput_type>, tensor<?xindex>) -> tensor<*xoutput_type>\n   return %13 : tensor<*xoutput_type>\n }"
        }
    ],
    "stats": {
        "total": 2037,
        "additions": 1086,
        "deletions": 951
    }
}