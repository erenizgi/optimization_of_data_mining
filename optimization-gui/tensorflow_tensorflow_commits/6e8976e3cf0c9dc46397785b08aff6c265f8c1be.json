{
    "author": "ermilovmaxim",
    "message": "Remove no longer needed one liner methods\n\nPiperOrigin-RevId: 824631750",
    "sha": "6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
    "files": [
        {
            "sha": "f181669f97ceea095e0b0b44789c03bb105c6830",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 30,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -59,39 +59,37 @@ namespace gpu {\n namespace {\n std::vector<TritonGemmConfig> GetDefaultTritonConfigs(\n     se::GpuComputeCapability compute_capability, bool autotune_tma) {\n-  if (compute_capability.IsCuda()) {\n-    auto* cuda_compute_capability =\n-        compute_capability.cuda_compute_capability();\n-    std::vector<TritonGemmConfig> configs;\n-\n-    if (cuda_compute_capability->IsAtLeastBlackwell()) {\n-      configs = *kBlackwellConfigs;\n-    } else if (cuda_compute_capability->IsHopper() ||\n-               cuda_compute_capability->IsAmpere()) {\n-      configs = *kHopperAmpereConfigs;\n-    } else {\n-      configs = *kDefaultCudaConfigs;\n-    }\n-\n-    if (!autotune_tma) {\n-      return configs;\n-    }\n-\n-    // Hopper+ devices support TMA. Add TMA parameterized configs.\n-    std::vector<TritonGemmConfig> tma_parameterized_configs;\n-    for (auto& config : configs) {\n-      config.is_tma_allowed = false;\n-      tma_parameterized_configs.push_back(config);\n-\n-      config.is_tma_allowed = true;\n-      tma_parameterized_configs.push_back(config);\n-    }\n-    return tma_parameterized_configs;\n-  }\n   if (compute_capability.IsRocm()) {\n     return *kDefaultRocmConfigs;\n   }\n-  return {};\n+\n+  CHECK(compute_capability.IsCuda());\n+  auto* cuda_compute_capability = compute_capability.cuda_compute_capability();\n+  std::vector<TritonGemmConfig> configs;\n+\n+  if (cuda_compute_capability->IsAtLeastBlackwell()) {\n+    configs = *kBlackwellConfigs;\n+  } else if (cuda_compute_capability->IsHopper() ||\n+             cuda_compute_capability->IsAmpere()) {\n+    configs = *kHopperAmpereConfigs;\n+  } else {\n+    configs = *kDefaultCudaConfigs;\n+  }\n+\n+  if (!autotune_tma) {\n+    return configs;\n+  }\n+\n+  // Hopper+ devices support TMA. Add TMA parameterized configs.\n+  std::vector<TritonGemmConfig> tma_parameterized_configs;\n+  for (auto& config : configs) {\n+    config.is_tma_allowed = false;\n+    tma_parameterized_configs.push_back(config);\n+\n+    config.is_tma_allowed = true;\n+    tma_parameterized_configs.push_back(config);\n+  }\n+  return tma_parameterized_configs;\n }\n \n }  // namespace"
        },
        {
            "sha": "ce9b86ca2a41a0d5a8011ef8101c9e730fdf85c4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -112,15 +112,6 @@ class TritonTest : public GpuCodegenTest {\n     return device_desc().gpu_compute_capability();\n   }\n \n-  stream_executor::GpuComputeCapability CudaAmpereOrRocm() {\n-    if (GpuComputeCapability().IsRocm()) {\n-      return stream_executor::GpuComputeCapability{\n-          device_desc().rocm_compute_capability()};\n-    }\n-    return stream_executor::GpuComputeCapability{\n-        se::CudaComputeCapability::Ampere()};\n-  }\n-\n   // Returns the module, its fusion computation and associated block level\n   // parameters from an HLO module text whose entry computation contains a\n   // single GEMM fusion."
        },
        {
            "sha": "34682660e4e0ee5cdc728b6f7057b0be26cf4087",
            "filename": "third_party/xla/xla/service/gpu/conv_layout_normalization_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fconv_layout_normalization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fconv_layout_normalization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fconv_layout_normalization_test.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -118,8 +118,8 @@ ENTRY TestComputation {\n }\n \n TEST_F(ConvolutionLayoutNormalizationTest, GraphConvF8) {\n-  if (!GetCudaComputeCapability().IsAtLeast(\n-          se::CudaComputeCapability::kHopper)) {\n+  if (IsRocm() || !GetCudaComputeCapability().IsAtLeast(\n+                      se::CudaComputeCapability::kHopper)) {\n     GTEST_SKIP() << \"FP8 convolutions require Hopper or newer architecture.\";\n   }\n   const char* hlo = R\"("
        },
        {
            "sha": "71deb452111e3cd4cf1b349a13239ecec422e500",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -100,14 +100,6 @@ class IrEmitterContext {\n   const se::GpuComputeCapability& gpu_compute_capability() const {\n     return gpu_device_info_.gpu_compute_capability();\n   }\n-  se::CudaComputeCapability cuda_compute_capability() const {\n-    auto* cc = gpu_compute_capability().cuda_compute_capability();\n-    return cc != nullptr ? *cc : se::CudaComputeCapability();\n-  }\n-  se::RocmComputeCapability rocm_compute_capability() const {\n-    auto* cc = gpu_compute_capability().rocm_compute_capability();\n-    return cc != nullptr ? *cc : se::RocmComputeCapability();\n-  }\n \n   // TODO: b/451959933 - Add nullability annotation to be explicit about this\n   // pointer: go/totw/230. Alternatively, return by reference instead of pointer"
        },
        {
            "sha": "bfdf9d925c53eaf87ae4bc4809b84a75247560c0",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fused_conv_rewriter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -97,8 +97,6 @@ bool IsNonDepthwiseConvCustomCall(const HloInstruction* instr) {\n   return IsConvCustomCall(instr) && !IsConvDepthwise(instr);\n }\n \n-bool IsROCm(se::GpuComputeCapability cc) { return cc.IsRocm(); }\n-\n // elu, relu6, and leaky-relu activations are supported in cudnn via the\n // \"runtime fusion\" engine, which JIT compiles C++ code.  This can be slow to\n // compile, so we guard it with a debug option.\n@@ -1452,7 +1450,7 @@ absl::StatusOr<bool> FuseConvertToF16(HloComputation* comp) {\n \n absl::StatusOr<bool> FuseConvertToS8(HloComputation* comp,\n                                      se::GpuComputeCapability cc) {\n-  if (IsROCm(cc)) return false;\n+  if (cc.IsRocm()) return false;\n   bool changed = false;\n   for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {\n     HloInstruction* gte = nullptr;\n@@ -1694,7 +1692,7 @@ absl::StatusOr<bool> CudnnFusedConvRewriter::Run(\n     bool changed = false;\n     // Rewrite FP8 convolutions and supported adjacent pointwise ops into a\n     // ForwardGraph Custom Call.\n-    if (!IsROCm(compute_capability_)) {\n+    if (!compute_capability_.IsRocm()) {\n       auto* cc = compute_capability_.cuda_compute_capability();\n       TF_ASSIGN_OR_RETURN(\n           changed, F8GraphConv(comp, *cc, dnn_version_, toolkit_version_));"
        },
        {
            "sha": "d3d24ccf1428983ba2fcb191b0f0d4ac8156db8b",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 20,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -653,7 +653,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n                  const_cast<HloInstruction*>(instr->operand(0)))) &&\n             (b = MatchFp8Param(\n                  const_cast<HloInstruction*>(instr->operand(1))))) {\n-          if (IsRocm(gpu_version_) &&\n+          if (gpu_version_.IsRocm() &&\n               toolkit_version_ < stream_executor::SemanticVersion{6, 2, 0} &&\n               instr->shape().element_type() != F16 &&\n               instr->shape().element_type() != F32) {\n@@ -1048,10 +1048,6 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return absl::OkStatus();\n   }\n \n-  static bool IsCuda(const se::GpuComputeCapability& gpu_version) {\n-    return gpu_version.IsCuda();\n-  }\n-\n   static absl::StatusOr<se::CudaComputeCapability> GetCudaComputeCapability(\n       const se::GpuComputeCapability& gpu_version) {\n     auto* cuda_cc = gpu_version.cuda_compute_capability();\n@@ -1061,10 +1057,6 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return *cuda_cc;\n   }\n \n-  static bool IsRocm(const se::GpuComputeCapability& gpu_version) {\n-    return gpu_version.IsRocm();\n-  }\n-\n   static absl::StatusOr<se::RocmComputeCapability> GetRocmComputeCapability(\n       const se::GpuComputeCapability& gpu_version) {\n     auto rocm_cc = gpu_version.rocm_compute_capability();\n@@ -1081,7 +1073,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     GemmBackendConfig& gemm_backend_config =\n         *gpu_backend_config.mutable_gemm_backend_config();\n     se::CudaComputeCapability cuda_compute_capability;\n-    if (IsCuda(gpu_version_)) {\n+    if (gpu_version_.IsCuda()) {\n       TF_ASSIGN_OR_RETURN(cuda_compute_capability,\n                           GetCudaComputeCapability(gpu_version_));\n       // FP8 GEMM kernels are only available on Ada, Hopper, and later\n@@ -1100,7 +1092,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       }\n     }\n \n-    if (IsRocm(gpu_version_)) {\n+    if (gpu_version_.IsRocm()) {\n       TF_ASSIGN_OR_RETURN(auto rocm_compute_capability,\n                           GetRocmComputeCapability(gpu_version_));\n       if (!rocm_compute_capability.has_fp8_support()) {\n@@ -1119,7 +1111,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     // cuBLASLt FP8 GEMM kernels require one of the two operands to be in\n     // F8E4M3FN format.\n-    if (IsCuda(gpu_version_)) {\n+    if (gpu_version_.IsCuda()) {\n       if (a_type == F8E5M2 && b_type == F8E5M2) {\n         VLOG(1)\n             << \"Failed to rewrite \" << instr->ToShortString()\n@@ -1138,7 +1130,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       }\n     }\n \n-    if (IsRocm(gpu_version_)) {\n+    if (gpu_version_.IsRocm()) {\n       TF_ASSIGN_OR_RETURN(auto rocm_compute_capability,\n                           GetRocmComputeCapability(gpu_version_));\n       if (rocm_compute_capability.has_ocp_fp8_support()) {\n@@ -1232,7 +1224,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     PrimitiveType d_type = instr->shape().element_type();\n     std::unordered_set<PrimitiveType> supported_d_types = {BF16, F16, F32};\n-    if (IsCuda(gpu_version_)) {\n+    if (gpu_version_.IsCuda()) {\n       supported_d_types.insert(F8E4M3FN);\n       supported_d_types.insert(F8E5M2);\n       if (supported_d_types.find(d_type) == supported_d_types.end()) {\n@@ -1243,7 +1235,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         return false;\n       }\n     }\n-    if (IsRocm(gpu_version_)) {\n+    if (gpu_version_.IsRocm()) {\n       if (toolkit_version_ < stream_executor::SemanticVersion{6, 2, 0}) {\n         if (supported_d_types.find(d_type) == supported_d_types.end()) {\n           VLOG(1) << \"Failed to rewrite \" << instr->ToShortString()\n@@ -1476,7 +1468,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n                           HloInstruction* clamp_upper,\n                           bool mult_scale = false) {\n     // TODO: add ROCm support to this fusion pattern\n-    if (IsRocm(gpu_version_)) {\n+    if (gpu_version_.IsRocm()) {\n       return absl::OkStatus();\n     }\n     // Verify the data types and the operands of clamp.\n@@ -1946,7 +1938,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     // CUBLAS_STATUS_NOT_SUPPORTED in some cases when fusing gelu into an FP8\n     // matmul. We cannot check the patch version, so disable this fusion with\n     // CUDA versions less than 12.4.\n-    if (IsCuda(gpu_version_) &&\n+    if (gpu_version_.IsCuda() &&\n         toolkit_version_ < stream_executor::SemanticVersion{12, 4, 0} &&\n         IsCublasLtMatmulF8(*gemm)) {\n       return absl::OkStatus();\n@@ -2002,7 +1994,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     // CUBLAS_STATUS_NOT_SUPPORTED in some cases when fusing gelu into an FP8\n     // matmul. We cannot check the patch version, so disable this fusion with\n     // CUDA versions less than 12.4.\n-    if (IsCuda(gpu_version_) &&\n+    if (gpu_version_.IsCuda() &&\n         toolkit_version_ < stream_executor::SemanticVersion{12, 4, 0} &&\n         IsCublasLtMatmulF8(*gemm)) {\n       return absl::OkStatus();\n@@ -2278,7 +2270,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         {ComputationType::kF64, DataType::kComplexDouble, PrimitiveType::C128,\n          PrimitiveType::C128, DataType::kComplexDouble},\n     };\n-    if (IsCuda(gpu_version_) &&\n+    if (gpu_version_.IsCuda() &&\n         absl::c_linear_search(supported_cublas_type_combinations,\n                               std::tuple{compute_type, scale_type, a_dtype,\n                                          b_dtype, output_dtype})) {\n@@ -2349,7 +2341,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         {ComputationType::kF32, DataType::kFloat, PrimitiveType::F8E5M2FNUZ,\n          PrimitiveType::F8E4M3FNUZ, DataType::kFloat},\n     };\n-    if (IsRocm(gpu_version_) &&\n+    if (gpu_version_.IsRocm() &&\n         absl::c_linear_search(supported_hipblas_type_combinations,\n                               std::tuple{compute_type, scale_type, a_dtype,\n                                          b_dtype, output_dtype})) {"
        },
        {
            "sha": "43619779c5e1e2c97fd6f786ec739e4ac63d0f8c",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -146,13 +146,11 @@ class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n         reference_platform, /*intra_op_parallelism_threads=*/0);\n \n     replacements_[kF8E4M3DatatypePlaceholder] =\n-        IsCuda() ? \"f8e4m3fn\" : \"f8e4m3fnuz\";\n+        Capability().IsCuda() ? \"f8e4m3fn\" : \"f8e4m3fnuz\";\n     replacements_[kF8E5M2DatatypePlaceholder] =\n-        IsCuda() ? \"f8e5m2\" : \"f8e5m2fnuz\";\n+        Capability().IsCuda() ? \"f8e5m2\" : \"f8e5m2fnuz\";\n   }\n \n-  bool IsCuda() { return Capability().IsCuda(); }\n-\n   const se::GpuComputeCapability& Capability() {\n     return hlo_runner_->backend()\n         .default_stream_executor()\n@@ -161,7 +159,7 @@ class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n   }\n \n   bool HasFp8Support() {\n-    if (IsCuda()) {\n+    if (Capability().IsCuda()) {\n       return Capability().cuda_compute_capability()->IsAtLeast(8, 9);\n     }\n     return Capability().rocm_compute_capability()->has_fp8_support() &&"
        },
        {
            "sha": "26dab88fea8a8b80011cdea83aedf494a6fb2169",
            "filename": "third_party/xla/xla/tests/collective_ops_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -2665,15 +2665,13 @@ class Fp8CollectiveOpsTest : public CollectiveOpsTest {\n  public:\n   Fp8CollectiveOpsTest() {\n     replacements_[kF8E4M3DatatypePlaceholder] =\n-        IsCuda() ? \"f8e4m3fn\" : \"f8e4m3fnuz\";\n+        Capability().IsCuda() ? \"f8e4m3fn\" : \"f8e4m3fnuz\";\n     replacements_[kF8E5M2DatatypePlaceholder] =\n-        IsCuda() ? \"f8e5m2\" : \"f8e5m2fnuz\";\n+        Capability().IsCuda() ? \"f8e5m2\" : \"f8e5m2fnuz\";\n     replacements_[kF8E8M0DatatypePlaceholder] = \"f8e8m0fnu\";\n   }\n \n  protected:\n-  bool IsCuda() { return Capability().IsCuda(); }\n-\n   const se::GpuComputeCapability& Capability() {\n     return backend()\n         .default_stream_executor()"
        },
        {
            "sha": "ea57e1d2231bb073461abe135b58f0fac98d7b7e",
            "filename": "third_party/xla/xla/tools/collective_perf_table_gen_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftools%2Fcollective_perf_table_gen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftools%2Fcollective_perf_table_gen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fcollective_perf_table_gen_test.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -33,21 +33,17 @@ using ::testing::Property;\n \n class CollectivePerfTableGenTest : public HloTestBase {\n   void SetUp() override {\n-    if (!IsCuda()) {\n+    if (!backend()\n+             .default_stream_executor()\n+             ->GetDeviceDescription()\n+             .gpu_compute_capability()\n+             .IsCuda()) {\n       GTEST_SKIP() << \"Not built with --config=cuda\";\n     }\n     cfg_.dry_run = true;\n   }\n \n  protected:\n-  bool IsCuda() {\n-    return backend()\n-        .default_stream_executor()\n-        ->GetDeviceDescription()\n-        .gpu_compute_capability()\n-        .IsCuda();\n-  }\n-\n   CollectivePerfTableGen::Config cfg_;\n };\n "
        },
        {
            "sha": "7aa3d6863d752e8357209cb1dfd9ee7dc8d10ef9",
            "filename": "third_party/xla/xla/tools/matmul_perf_table_gen_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftools%2Fmatmul_perf_table_gen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e8976e3cf0c9dc46397785b08aff6c265f8c1be/third_party%2Fxla%2Fxla%2Ftools%2Fmatmul_perf_table_gen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmatmul_perf_table_gen_test.cc?ref=6e8976e3cf0c9dc46397785b08aff6c265f8c1be",
            "patch": "@@ -35,19 +35,14 @@ namespace {\n \n class MatmulPerfTableGenTest : public HloTestBase {\n   void SetUp() override {\n-    if (!IsCuda()) {\n+    if (!backend()\n+             .default_stream_executor()\n+             ->GetDeviceDescription()\n+             .gpu_compute_capability()\n+             .IsCuda()) {\n       GTEST_SKIP() << \"Not built with --config=cuda\";\n     }\n   }\n-\n- protected:\n-  bool IsCuda() {\n-    return backend()\n-        .default_stream_executor()\n-        ->GetDeviceDescription()\n-        .gpu_compute_capability()\n-        .IsCuda();\n-  }\n };\n \n TEST_F(MatmulPerfTableGenTest, DryRunsSpecifiedSweepSpace) {"
        }
    ],
    "stats": {
        "total": 160,
        "additions": 59,
        "deletions": 101
    }
}