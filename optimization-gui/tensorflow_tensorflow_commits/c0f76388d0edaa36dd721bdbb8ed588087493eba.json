{
    "author": "terryysun",
    "message": "PR #34143: [GPU] Add all-to-all support to S-curve model\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34143\n\nüìù Summary of Changes\nAdded all-to-all support to S-curve model.\n\nüéØ Justification\nS-curve model doesn't support all-to-all, fallback may lead to bad performance, benchmarking justified that the added all-to-all model can improve performance for models with cross-NVL domain all-to-all.\n\nüöÄ Kind of Contribution\n‚ö°Ô∏è Performance Improvement/‚ú® New Feature\n\nüìä Benchmark (for Performance Improvements)\n| Branch | End-to-end execution time mean on mixtral_8x7b_bf16_2x8 |\n| :------- | :------: |\n| main     | 1128328 us   |\n| terryysun/a2a_s_curve (this branch)   | 1009397 us |\n\nSpeedup over main: 11.78%.\n\nüß™ Unit Tests:\nAdded exact-matching unit tests to guard the estimation value.\n\nüß™ Execution Tests:\nAdded execution tests to guard the comm-compute overlapping behavior.\n\nCopybara import of the project:\n\n--\n794ef568fe9fcc0f6b4571f19e2a6ce6e06d0099 by Terry Sun <tesun@nvidia.com>:\n\ns-curve a2a support\n\n--\n4f85dae4e688af0e6b1f0f5ff1aa0bfef052f15f by Terry Sun <tesun@nvidia.com>:\n\nfix buffer size calculation\n\n--\n1dc94f78cd73ec3f0784b6b2db795a608468cdc7 by Terry Sun <tesun@nvidia.com>:\n\nadd LHS test\n\n--\n56aef84b36c2bc99bf39562fa868398240ae79c3 by Terry Sun <tesun@nvidia.com>:\n\nadd model dispatching test\n\n--\nd20ed933cbd97d56f1664bbea1b8d35f9092146e by Terry Sun <tesun@nvidia.com>:\n\nfix merge issue\n\n--\nf09474bc513804097956e15a7684f1299bef4173 by Terry Sun <tesun@nvidia.com>:\n\nrephase doc string\n\nMerging this change closes #34143\n\nPiperOrigin-RevId: 843090023",
    "sha": "c0f76388d0edaa36dd721bdbb8ed588087493eba",
    "files": [
        {
            "sha": "397e1c85737bf2c7a7c9f608ee00be3b34759b4b",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc?ref=c0f76388d0edaa36dd721bdbb8ed588087493eba",
            "patch": "@@ -408,6 +408,52 @@ TEST_F(GpuLatencyHidingSchedulerBaseTest,\n                   GetIndexByName(instruction_sequence, \"rs_1\"));\n }\n \n+TEST_F(GpuLatencyHidingSchedulerBaseTest,\n+       AllToAllAndGemmOverlapWithSolCostModel) {\n+  // Verify SoL cost model successfully enables all-to-all overlap with compute.\n+  absl::string_view kHloModule = R\"(\n+    HloModule m, replica_count=16\n+\n+    async_a2a {\n+      param = f32[2048,2048] parameter(0)\n+      ROOT a2a_inner = f32[2048,2048] all-to-all(param), dimensions={0},\n+        replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}}\n+    }\n+\n+    ENTRY main {\n+      lhs = f32[8192,8192] parameter(0)\n+      rhs = f32[8192,8192] parameter(1)\n+      comm = f32[2048,2048] parameter(2)\n+      compute = f32[8192,8192] dot(lhs, rhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+      a2a = ((f32[2048,2048]), f32[2048,2048]) async-start(comm), calls=async_a2a\n+      a2a_done = f32[2048,2048] async-done(a2a)\n+      ROOT tuple = (f32[2048,2048], f32[8192,8192]) tuple(a2a_done, compute)\n+    }\n+  )\";\n+\n+  auto config = GetModuleConfig(\"\");\n+  DebugOptions& debug_options = config.mutable_debug_options();\n+  debug_options.set_xla_gpu_enable_latency_hiding_scheduler(true);\n+  debug_options.set_xla_gpu_enable_analytical_sol_latency_estimator(true);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule, config));\n+  auto scheduled = ScheduleModule(module.get(), /*num_parallel_resources=*/1);\n+  TF_ASSERT_OK(scheduled.status());\n+\n+  const auto& sequence = scheduled.value()\n+                             ->schedule()\n+                             .sequence(module->entry_computation())\n+                             .instructions();\n+  int64_t a2a_idx = GetIndexByName(sequence, \"a2a\");\n+  int64_t compute_idx = GetIndexByName(sequence, \"compute\");\n+  int64_t a2a_done_idx = GetIndexByName(sequence, \"a2a_done\");\n+\n+  // Check that overlap occurs: a2a < compute < a2a_done\n+  EXPECT_LT(a2a_idx, compute_idx);\n+  EXPECT_LT(compute_idx, a2a_done_idx);\n+}\n+\n TEST_F(GpuLatencyHidingSchedulerBaseTest,\n        OverlappingRanksPreventOverlappingCollectives) {\n   // TODO TJ re-enable this test when the multi-streamed"
        },
        {
            "sha": "01f7df1228dd534b9a93ae1c224ccc9d42926672",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.cc?ref=c0f76388d0edaa36dd721bdbb8ed588087493eba",
            "patch": "@@ -224,6 +224,27 @@ absl::StatusOr<absl::Duration> SolGPUCostModel::RingLatency(\n   return ret + xla_flag_config_.nccl_op_launch_time;\n }\n \n+absl::StatusOr<absl::Duration> SolGPUCostModel::AllToAllLatency(\n+    const int64_t buff_size_bytes, const int num_nodes,\n+    const int num_communicators) const {\n+  TF_ASSIGN_OR_RETURN(\n+      int num_gpus,\n+      NumGpusPerComm(num_nodes, SolGPUCostModel::CollectiveType::kAllToAll,\n+                     num_communicators));\n+\n+  const int num_gpus_per_node = num_gpus / num_nodes;\n+  // Each GPU sends to (num_gpus_per_node * (num_nodes-1)) peers off-node.\n+  const int inter_node_peers_per_gpu = num_gpus_per_node * (num_nodes - 1);\n+  // Sending buff_size_bytes / (num_gpus - 1) bytes to each peer off-node.\n+  const int64_t per_peer_bytes = buff_size_bytes / (num_gpus - 1);\n+  absl::Duration per_peer_duration = TransferDuration(per_peer_bytes) +\n+                                     ChunkPrepLatency(per_peer_bytes) +\n+                                     xla_flag_config_.rtt;\n+  absl::Duration total = inter_node_peers_per_gpu * per_peer_duration;\n+\n+  return total + xla_flag_config_.nccl_op_launch_time;\n+}\n+\n // Helper functions\n absl::StatusOr<int> SolGPUCostModel::NumGpusPerComm(\n     int num_nodes, const CollectiveType& coll_type,"
        },
        {
            "sha": "21cf4bd32f9cc7d4d41dad5ff6a3b18e190ffcd6",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model.h",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.h?ref=c0f76388d0edaa36dd721bdbb8ed588087493eba",
            "patch": "@@ -50,8 +50,9 @@ class SolGPUCostModel {\n   };\n \n   enum class CollectiveType {\n-    kAllReduce,\n     kAllGather,\n+    kAllReduce,\n+    kAllToAll,\n     kReduceScatter,\n     kSendRecv,\n   };\n@@ -73,6 +74,16 @@ class SolGPUCostModel {\n                                              const CollectiveType& coll_type,\n                                              int num_communicators) const;\n \n+  // Returns the latency of an AllToAll collective across multiple nodes.\n+  //\n+  // `buff_size_bytes`: the size of the message to be transferred.\n+  // `num_nodes`: the number of nodes participating in the all-to-all.\n+  // `num_communicators`: the number of communicators participating in the\n+  // all-to-all.\n+  absl::StatusOr<absl::Duration> AllToAllLatency(int64_t buff_size_bytes,\n+                                                 int num_nodes,\n+                                                 int num_communicators) const;\n+\n  private:\n   // Helper functions to estimate the latency subcomponents\n   absl::Duration ChunkPrepLatency(int64_t per_gpu_msg_size_bytes) const;"
        },
        {
            "sha": "7b778e0f682b61dd602a8c56ca60ecc0f446c338",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_test.cc",
            "status": "modified",
            "additions": 51,
            "deletions": 19,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_test.cc?ref=c0f76388d0edaa36dd721bdbb8ed588087493eba",
            "patch": "@@ -27,48 +27,80 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n-constexpr int64_t kTenMB = 10 * 1024 * 1024;  // 10MB\n+constexpr int64_t kEightMB = 8 * 1024 * 1024;  // 8MB\n \n using ::testing::TestWithParam;\n using ::testing::ValuesIn;\n \n-struct RingLatencyTestCase {\n+struct LatencyTestCase {\n   SolGPUCostModel::CollectiveType collective_type;\n+  int num_nodes;\n   absl::Duration expected_latency;\n };\n \n-class SolGPUCostModelTest : public TestWithParam<RingLatencyTestCase> {\n+class SolGPUCostModelTest : public TestWithParam<LatencyTestCase> {\n  protected:\n   SolGPUCostModelTest()\n       : model_({\n             /*nccl_op_launch_time=*/absl::Microseconds(100),\n             /*nic_speed_gbps=*/100,\n             /*chunk_prep_time=*/absl::Microseconds(100),\n             /*rtt=*/absl::Microseconds(100),\n-            /*gpus_per_node=*/100,\n+            /*gpus_per_node=*/8,\n             /*chunk_size_bytes=*/4 * 1024 * 1024,\n         }) {}\n   SolGPUCostModel model_;\n };\n \n-TEST_P(SolGPUCostModelTest, TestRingLatency) {\n-  const RingLatencyTestCase& test_case = GetParam();\n-  absl::Duration actual_latency =\n-      absl::Trunc(*model_.RingLatency(kTenMB, 1, test_case.collective_type,\n-                                      /*num_communicators=*/1),\n-                  absl::Microseconds(1));\n+TEST_P(SolGPUCostModelTest, TestLatency) {\n+  const LatencyTestCase& test_case = GetParam();\n+  absl::Duration actual_latency;\n+  if (test_case.collective_type == SolGPUCostModel::CollectiveType::kAllToAll) {\n+    actual_latency =\n+        absl::Trunc(*model_.AllToAllLatency(kEightMB, test_case.num_nodes,\n+                                            /*num_communicators=*/1),\n+                    absl::Microseconds(1));\n+  } else {\n+    actual_latency =\n+        absl::Trunc(*model_.RingLatency(kEightMB, test_case.num_nodes,\n+                                        test_case.collective_type,\n+                                        /*num_communicators=*/1),\n+                    absl::Microseconds(1));\n+  }\n   EXPECT_EQ(actual_latency, test_case.expected_latency);\n }\n \n-INSTANTIATE_TEST_SUITE_P(\n-    SolGPUCostModelTests, SolGPUCostModelTest,\n-    ValuesIn<RingLatencyTestCase>({\n-        {SolGPUCostModel::CollectiveType::kAllGather, absl::Microseconds(299)},\n-        {SolGPUCostModel::CollectiveType::kAllReduce, absl::Microseconds(498)},\n-        {SolGPUCostModel::CollectiveType::kReduceScatter,\n-         absl::Microseconds(299)},\n-        {SolGPUCostModel::CollectiveType::kSendRecv, absl::Microseconds(353)},\n-    }));\n+INSTANTIATE_TEST_SUITE_P(SolGPUCostModelTests, SolGPUCostModelTest,\n+                         ValuesIn<LatencyTestCase>({\n+                             {SolGPUCostModel::CollectiveType::kAllGather,\n+                              /*num_nodes=*/1, absl::Microseconds(284)},\n+                             {SolGPUCostModel::CollectiveType::kAllGather,\n+                              /*num_nodes=*/2, absl::Microseconds(485)},\n+                             {SolGPUCostModel::CollectiveType::kAllGather,\n+                              /*num_nodes=*/4, absl::Microseconds(885)},\n+                             {SolGPUCostModel::CollectiveType::kAllReduce,\n+                              /*num_nodes=*/1, absl::Microseconds(468)},\n+                             {SolGPUCostModel::CollectiveType::kAllReduce,\n+                              /*num_nodes=*/2, absl::Microseconds(870)},\n+                             {SolGPUCostModel::CollectiveType::kAllReduce,\n+                              /*num_nodes=*/4, absl::Microseconds(1670)},\n+                             {SolGPUCostModel::CollectiveType::kReduceScatter,\n+                              /*num_nodes=*/1, absl::Microseconds(284)},\n+                             {SolGPUCostModel::CollectiveType::kReduceScatter,\n+                              /*num_nodes=*/2, absl::Microseconds(485)},\n+                             {SolGPUCostModel::CollectiveType::kReduceScatter,\n+                              /*num_nodes=*/4, absl::Microseconds(885)},\n+                             {SolGPUCostModel::CollectiveType::kSendRecv,\n+                              /*num_nodes=*/1, absl::Microseconds(292)},\n+                             {SolGPUCostModel::CollectiveType::kSendRecv,\n+                              /*num_nodes=*/2, absl::Microseconds(485)},\n+                             {SolGPUCostModel::CollectiveType::kAllToAll,\n+                              /*num_nodes=*/1, absl::Microseconds(100)},\n+                             {SolGPUCostModel::CollectiveType::kAllToAll,\n+                              /*num_nodes=*/2, absl::Microseconds(1745)},\n+                             {SolGPUCostModel::CollectiveType::kAllToAll,\n+                              /*num_nodes=*/4, absl::Microseconds(4966)},\n+                         }));\n \n TEST(SolGPUCostModelGetConfigTest, ConfigForHopper) {\n   constexpr absl::string_view kDummyModule = R\"("
        },
        {
            "sha": "8fdcdb8232159d1f0652cdf797efa46d337cfec9",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=c0f76388d0edaa36dd721bdbb8ed588087493eba",
            "patch": "@@ -61,7 +61,7 @@ using ::mlir::MLIRContext;\n bool IsSupportedCollectiveOp(const HloInstruction& instr) {\n   return HloPredicateIsOp<HloOpcode::kAllReduceStart, HloOpcode::kAllReduce,\n                           HloOpcode::kReduceScatter, HloOpcode::kAllGatherStart,\n-                          HloOpcode::kAllGather>(&instr);\n+                          HloOpcode::kAllGather, HloOpcode::kAllToAll>(&instr);\n }\n \n bool IsHostOffloaded(const HloInstruction& instr) {\n@@ -127,6 +127,14 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n       result += runtime;\n       break;\n     }\n+    case HloOpcode::kAllToAll: {\n+      TF_ASSIGN_OR_RETURN(\n+          absl::Duration runtime,\n+          sol_model.AllToAllLatency(msg_size, num_participating_hosts,\n+                                    num_communicators));\n+      result += runtime;\n+      break;\n+    }\n     case HloOpcode::kAllReduce:\n     case HloOpcode::kAllReduceStart: {\n       result += gpu_performance_model.Get()\n@@ -165,6 +173,13 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n                                 num_communicators));\n         result += runtime;\n       }\n+      if (instr.async_wrapped_opcode() == HloOpcode::kAllToAll) {\n+        TF_ASSIGN_OR_RETURN(\n+            absl::Duration runtime,\n+            sol_model.AllToAllLatency(msg_size, num_participating_hosts,\n+                                      num_communicators));\n+        result += runtime;\n+      }\n       break;\n     }\n     case HloOpcode::kRecv:"
        },
        {
            "sha": "088099eb468ee27a6c6646b185239a0468610d22",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 49,
            "deletions": 2,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0f76388d0edaa36dd721bdbb8ed588087493eba/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=c0f76388d0edaa36dd721bdbb8ed588087493eba",
            "patch": "@@ -31,7 +31,6 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/literal_util.h\"\n-#include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/collective_interpolator.h\"\n #include \"xla/service/gpu/model/sol_gpu_cost_model.h\"\n@@ -45,7 +44,6 @@ limitations under the License.\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/xla_data.pb.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -553,6 +551,55 @@ INSTANTIATE_TEST_SUITE_P(SolLatencyEstimatorTests, SolLatencyEstimatorTest,\n                            return info.param.test_name;\n                          });\n \n+TEST_F(HloHardwareIndependentTestBase, CollectiveCostModelDispatching) {\n+  const auto shape_size_fn = HloCostAnalysis::DefaultShapeSize;\n+  const auto gpu_info = TestGpuDeviceInfo::RTXH100SXMDeviceInfo();\n+  const SolGPUCostModel::Config sol_flags = {\n+      absl::Microseconds(100), 100, absl::Microseconds(100),\n+      absl::Microseconds(100), 8,   4 * 1024 * 1024};\n+  mlir::MLIRContext mlir_ctx;\n+  auto interpolator =\n+      *CollectiveInterpolator::Create(sol_flags.gpus_per_node, gpu_info,\n+                                      /*analysis=*/nullptr);\n+\n+  // NVLink domain collective should use CollectiveInterpolator.\n+  TF_ASSERT_OK_AND_ASSIGN(auto nvl_module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m, num_partitions=16\n+ENTRY main {\n+  p = bf16[8,16000,1000] parameter(0)\n+  ROOT a2a = bf16[8,16000,1000] all-to-all(p),\n+    replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}},\n+    channel_id=1, dimensions={0}\n+})\"));\n+  HloInstruction* nvl_instr = hlo_query::FindInstruction(\n+      nvl_module->entry_computation(), HloOpcode::kAllToAll);\n+  EXPECT_FALSE(SolLatencyEstimator::ComputeCollectiveTime(\n+                   *nvl_instr, gpu_info, shape_size_fn, sol_flags, &mlir_ctx,\n+                   /*collective_interpolator=*/nullptr)\n+                   .ok());\n+  EXPECT_TRUE(SolLatencyEstimator::ComputeCollectiveTime(\n+                  *nvl_instr, gpu_info, shape_size_fn, sol_flags, &mlir_ctx,\n+                  interpolator.get())\n+                  .ok());\n+\n+  // Cross-partition collective should use S-curve model (world-level across 2\n+  // hosts).\n+  TF_ASSERT_OK_AND_ASSIGN(auto ib_module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m, num_partitions=16\n+ENTRY main {\n+  p = bf16[16,16000,1000] parameter(0)\n+  ROOT a2a = bf16[16,16000,1000] all-to-all(p),\n+    replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}},\n+    channel_id=1, dimensions={0}\n+})\"));\n+  HloInstruction* ib_instr = hlo_query::FindInstruction(\n+      ib_module->entry_computation(), HloOpcode::kAllToAll);\n+  EXPECT_TRUE(SolLatencyEstimator::ComputeCollectiveTime(\n+                  *ib_instr, gpu_info, shape_size_fn, sol_flags, &mlir_ctx,\n+                  /*collective_interpolator=*/nullptr)\n+                  .ok());\n+}\n+\n class IsSolLatencyEstimatorEnabledTest : public HloTestBase {\n  protected:\n   IsSolLatencyEstimatorEnabledTest()"
        }
    ],
    "stats": {
        "total": 218,
        "additions": 195,
        "deletions": 23
    }
}