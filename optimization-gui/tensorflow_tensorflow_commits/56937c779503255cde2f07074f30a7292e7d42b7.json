{
    "author": "basioli-k",
    "message": "[XLA][codegen] Remove ir_emission_util dependency by moving checks for supported fusion types outside of the emitting logic\n\nThis logic is triton specific and the emitter should be generic.\n\nPiperOrigin-RevId: 840793754",
    "sha": "56937c779503255cde2f07074f30a7292e7d42b7",
    "files": [
        {
            "sha": "8e365aba6a0b59414f94de02300e0b6dba5d6f06",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=56937c779503255cde2f07074f30a7292e7d42b7",
            "patch": "@@ -277,7 +277,6 @@ cc_library(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:instruction_fusion\",\n         \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tools:hlo_decomposer_lib\","
        },
        {
            "sha": "17168e1b7fb2512746b789c861657fe8614146cc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 38,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=56937c779503255cde2f07074f30a7292e7d42b7",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #include <cstdint>\n #include <memory>\n #include <optional>\n-#include <string>\n #include <utility>\n #include <variant>\n #include <vector>\n@@ -95,7 +94,6 @@ limitations under the License.\n #include \"xla/permutation_util.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/instruction_fusion.h\"\n@@ -1486,7 +1484,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    MLIRContext& mlir_context) {\n+    MLIRContext& mlir_context, absl::Span<mlir::Type> opaque_args_types) {\n   const auto debug_options = fusion->GetModule()->config().debug_options();\n \n   const HloComputation* hlo_computation =\n@@ -1500,32 +1498,6 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n       llvm_ir::CreateMlirModuleOp(loc);\n   b.setInsertionPointToEnd(triton_module->getBody());\n \n-  std::string fusion_kind(kTritonFusionKind);\n-  if (fusion->has_backend_config()) {\n-    auto backend_config = fusion->backend_config<GpuBackendConfig>();\n-    if (backend_config.ok()) {\n-      fusion_kind = backend_config->fusion_backend_config().kind();\n-    }\n-  }\n-\n-  if (fusion_kind == kTritonGemmFusionKind) {\n-    return Internal(\n-        \"Attempted to emit a GEMM fusion through the legacy Triton \"\n-        \"emitter, but it has been deleted. This is a bug.\");\n-  }\n-\n-  // TODO(bchetioui,pifon): this list should be consolidated; why do we need so\n-  // many different fusion kinds?\n-  const std::vector<absl::string_view> kSupportedFusionKinds = {\n-      kTritonFusionKind,\n-      kTritonNestedGemmFusionKind,\n-      kTritonCollectiveFusionKind,\n-  };\n-\n-  if (!absl::c_linear_search(kSupportedFusionKinds, fusion_kind)) {\n-    return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n-  }\n-\n   // Build Triton kernel.\n   SmallVector<Type> fn_arg_types;\n   for (HloInstruction* p : hlo_computation->parameter_instructions()) {\n@@ -1547,18 +1519,16 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     fn_arg_types.push_back(GetMemRefType(shape, triton_ty));\n   }\n \n-  // Add metadata arguments for collectives.\n-  // This is done after the input and output arguments but before the tile\n-  // index.\n-  int32_t num_metadata_arguments = 0;\n-  if (fusion_kind == kTritonCollectiveFusionKind) {\n-    TF_ASSIGN_OR_RETURN(\n-        num_metadata_arguments,\n-        AddCollectiveMetadataArguments(fn_arg_types, b, hlo_computation));\n+  // Add opaque arguments.\n+  fn_arg_types.reserve(fn_arg_types.size() + opaque_args_types.size());\n+\n+  for (const auto& type : opaque_args_types) {\n+    fn_arg_types.push_back(type);\n   }\n+\n   // Metadata arguments are opaque to the tiling infra.\n   llvm::SmallVector<mlir::NamedAttribute> named_attributes{b.getNamedAttr(\n-      \"num_opaque_args\", b.getI32IntegerAttr(num_metadata_arguments))};\n+      \"num_opaque_args\", b.getI32IntegerAttr(opaque_args_types.size()))};\n \n   auto fn = xtile::EntryFuncOp::create(b, fn_name, fn_arg_types,\n                                        named_attributes, {});"
        },
        {
            "sha": "d62052920c7e8a0656064a960d38df8b1c084796",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=56937c779503255cde2f07074f30a7292e7d42b7",
            "patch": "@@ -18,9 +18,11 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n+#include \"mlir/IR/Types.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n@@ -29,17 +31,22 @@ limitations under the License.\n \n namespace xla::gpu {\n \n-// This function (or its future equivalent) should emit the MLIR module in the\n-// shared dialect between XLA:CPU and XLA:GPU. At the moment it is still\n-// emitting GPU specific modules. It is currently exposed only for testing\n-// purposes and will only be used to make sure we are properly emitting the\n-// shared dialect.\n+// Emits an xtile module.\n+// fn_name: The name of the function to emit.\n+// emitter_specific_constraints_builder: A builder for the emitter specific\n+//   constraints.\n+// fusion: The fusion instruction to emit.\n+// block_level_parameters: The block level parameters.\n+// mlir_context: The MLIR context to use.\n+// opaque_args_types: The types of the opaque arguments to the function, e.x.\n+// arguments for collectives in XLA:GPU.\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     absl::string_view fn_name,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::MLIRContext& mlir_context);\n+    mlir::MLIRContext& mlir_context,\n+    absl::Span<mlir::Type> opaque_args_types = {});\n \n }  // namespace xla::gpu\n "
        },
        {
            "sha": "8d2b17c25097dd5e1d5677b6c6e3a789a4ba6778",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 4,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/56937c779503255cde2f07074f30a7292e7d42b7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc?ref=56937c779503255cde2f07074f30a7292e7d42b7",
            "patch": "@@ -15,13 +15,15 @@ limitations under the License.\n \n #include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <optional>\n #include <string>\n #include <system_error>  // NOLINT\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -62,6 +64,7 @@ limitations under the License.\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/Diagnostics.h\"\n #include \"mlir/IR/DialectRegistry.h\"\n+#include \"mlir/IR/Location.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Value.h\"\n@@ -78,6 +81,7 @@ limitations under the License.\n #include \"mlir/Transforms/Passes.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/emitters/ir/xla_gpu_ops.h\"\n+#include \"xla/backends/gpu/codegen/triton/collective_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n@@ -234,16 +238,58 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n \n   LoadMlirDialectsForTriton(mlir_context);\n \n+  const HloComputation* hlo_computation =\n+      fusion->fused_instructions_computation();\n+\n+  std::string fusion_kind(kTritonFusionKind);\n+  if (fusion->has_backend_config()) {\n+    auto backend_config = fusion->backend_config<GpuBackendConfig>();\n+    if (backend_config.ok()) {\n+      fusion_kind = backend_config->fusion_backend_config().kind();\n+    }\n+  }\n+\n+  if (fusion_kind == kTritonGemmFusionKind) {\n+    return Internal(\n+        \"Attempted to emit a GEMM fusion through the legacy Triton \"\n+        \"emitter, but it has been deleted. This is a bug.\");\n+  }\n+\n+  // TODO(bchetioui,pifon): this list should be consolidated; why do we need so\n+  // many different fusion kinds?\n+  const std::vector<absl::string_view> kSupportedFusionKinds = {\n+      kTritonFusionKind,\n+      kTritonNestedGemmFusionKind,\n+      kTritonCollectiveFusionKind,\n+  };\n+\n+  if (!absl::c_linear_search(kSupportedFusionKinds, fusion_kind)) {\n+    return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n+  }\n+\n+  llvm::SmallVector<mlir::Type> opaque_args_types;\n+  // Add metadata arguments for collectives.\n+  // This is done after the input and output arguments but before the tile\n+  // index.\n+  int32_t num_metadata_arguments = 0;\n+  if (fusion_kind == kTritonCollectiveFusionKind) {\n+    auto loc = mlir::NameLoc::get(\n+        mlir::StringAttr::get(&mlir_context, hlo_computation->name()));\n+    mlir::ImplicitLocOpBuilder b(loc, &mlir_context);\n+\n+    TF_ASSIGN_OR_RETURN(\n+        num_metadata_arguments,\n+        AddCollectiveMetadataArguments(opaque_args_types, b, hlo_computation));\n+  }\n+\n   // TODO: b/451959933 - Use reference or check pointer.\n \n   TF_ASSIGN_OR_RETURN(\n       auto triton_module,\n       EmitXTileModule(fn_name,\n                       TritonEmitterConstraints::GetBuilder(device_info), fusion,\n-                      block_level_parameters, mlir_context));\n-\n-  const HloComputation* hlo_computation =\n-      fusion->fused_instructions_computation();\n+                      block_level_parameters, mlir_context,\n+                      absl::MakeSpan(opaque_args_types)));\n \n   const auto debug_options = fusion->GetModule()->config().debug_options();\n "
        }
    ],
    "stats": {
        "total": 120,
        "additions": 71,
        "deletions": 49
    }
}