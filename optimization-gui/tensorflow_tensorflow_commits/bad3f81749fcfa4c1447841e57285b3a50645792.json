{
    "author": "ermilovmaxim",
    "message": "Port more BufferUse users to provide shape\n\nPiperOrigin-RevId: 834386352",
    "sha": "bad3f81749fcfa4c1447841e57285b3a50645792",
    "files": [
        {
            "sha": "281451c65e2823a17eb8161eda4ef5659e03ea39",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_dims.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_dims.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_dims.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_dims.cc?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -33,9 +33,9 @@ namespace xla::cpu {\n \n absl::InlinedVector<BufferUse, 4> ConvolutionBufferUses(\n     const ConvolutionSlices& slices) {\n-  return {BufferUse::Read(slices.input_buffer),\n-          BufferUse::Read(slices.kernel_buffer),\n-          BufferUse::Write(slices.output_buffer)};\n+  return {BufferUse::Read(slices.input_buffer, slices.input_shape),\n+          BufferUse::Read(slices.kernel_buffer, slices.kernel_shape),\n+          BufferUse::Write(slices.output_buffer, slices.output_shape)};\n }\n \n ConvolutionCanonicalDims::Dims::Dims(absl::Span<const int64_t> dims)"
        },
        {
            "sha": "418c3a718835a18b9e7b0b665a9ef4b0536e7e30",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_dims.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_dims.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_dims.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_dims.cc?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -40,9 +40,9 @@ limitations under the License.\n namespace xla::cpu {\n \n absl::InlinedVector<BufferUse, 4> DotBufferUses(const DotSlices& slices) {\n-  return {BufferUse::Read(slices.lhs_buffer),\n-          BufferUse::Read(slices.rhs_buffer),\n-          BufferUse::Write(slices.out_buffer)};\n+  return {BufferUse::Read(slices.lhs_buffer, slices.lhs_shape),\n+          BufferUse::Read(slices.rhs_buffer, slices.rhs_shape),\n+          BufferUse::Write(slices.out_buffer, slices.out_shape)};\n }\n \n std::string MakeVectorString(absl::Span<const int64_t> values) {"
        },
        {
            "sha": "c2c147c694eb96137228a6fb297235a9442a98d3",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_executor_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -242,10 +242,14 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> AddI32Thunk::Execute(\n AddI32Thunk::BufferUses AddI32Thunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const auto& src : srcs_) {\n-    buffer_uses.push_back(BufferUse::Read(src));\n+    buffer_uses.push_back(BufferUse::Read(\n+        src, ShapeUtil::MakeShape(\n+                 S32, {src.size() / ShapeUtil::ByteSizeOfPrimitiveType(S32)})));\n   }\n   for (const auto& dst : dsts_) {\n-    buffer_uses.push_back(BufferUse::Write(dst));\n+    buffer_uses.push_back(BufferUse::Write(\n+        dst, ShapeUtil::MakeShape(\n+                 S32, {dst.size() / ShapeUtil::ByteSizeOfPrimitiveType(S32)})));\n   }\n   return buffer_uses;\n }"
        },
        {
            "sha": "a23e6bd11b0a1af3d7c879c8a85631605ecca991",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -3068,6 +3068,7 @@ xla_cc_test(\n         \":thunk_pass_pipeline\",\n         \":while_thunk\",\n         \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\","
        },
        {
            "sha": "3e4f4cd74c005bfdbb022177ce738a34630a5682",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/hlo_module_config.h\"\n+#include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -193,6 +194,8 @@ TEST_F(ThunkBufferDebugPassTest, InsertsBuffersDebugChecksumThunks) {\n   FakeThunkPassBufferAllocator allocator;\n   // Create a fake thunk with a few different buffer uses.\n   BufferAllocation alloc(0, 1024, 0);\n+\n+  Shape arg_shape = ShapeUtil::MakeShape(U8, {1});\n   BufferAllocation::Slice slice_i(&alloc, 0, 1);\n   BufferAllocation::Slice slice_o(&alloc, 1, 1);\n   BufferAllocation::Slice slice_io(&alloc, 2, 1);\n@@ -204,14 +207,14 @@ TEST_F(ThunkBufferDebugPassTest, InsertsBuffersDebugChecksumThunks) {\n       Thunk::BufferUses{\n           // Consume means the thunk can reuse the buffer for scratch space, so\n           // only check it on input.\n-          BufferUse::Consume(slice_i),\n+          BufferUse::Consume(slice_i, arg_shape),\n           // Write is undefined on input, but defined on output.\n-          BufferUse::Write(slice_o),\n+          BufferUse::Write(slice_o, arg_shape),\n           // Unlike Consume, Read is supposed to preserve the contents of the\n           // buffer, so we check it on input *and* output.\n-          BufferUse::Read(slice_io),\n+          BufferUse::Read(slice_io, arg_shape),\n           // Scratch buffers are not checked at all.\n-          BufferUse::Scratch(slice_scratch),\n+          BufferUse::Scratch(slice_scratch, arg_shape),\n       });\n   Thunk* fake_thunk_ptr = fake_thunk.get();\n   std::vector<std::unique_ptr<Thunk>> thunks;\n@@ -457,25 +460,26 @@ TEST_F(ThunkBufferDebugPassTest, InsertsBuffersDebugFloatCheckThunks) {\n   hlo_module.AddEntryComputation(std::move(entry_computation));\n   // Create a fake thunk with a few different buffer uses.\n   BufferAllocation alloc(0, 1024, 0);\n-  BufferAllocation::Slice slice_i(&alloc, 0, 1, PrimitiveType::F32);\n-  BufferAllocation::Slice slice_o(&alloc, 1, 1, PrimitiveType::F32);\n-  BufferAllocation::Slice slice_io(&alloc, 2, 1, PrimitiveType::F32);\n-  BufferAllocation::Slice slice_scratch(&alloc, 3, 1, PrimitiveType::F32);\n+  Shape arg_shape = ShapeUtil::MakeShape(F32, {1});\n+  BufferAllocation::Slice slice_i(&alloc, 0, 4, PrimitiveType::F32);\n+  BufferAllocation::Slice slice_o(&alloc, 4, 4, PrimitiveType::F32);\n+  BufferAllocation::Slice slice_io(&alloc, 8, 4, PrimitiveType::F32);\n+  BufferAllocation::Slice slice_scratch(&alloc, 12, 4, PrimitiveType::F32);\n   Thunk::ThunkInfo fake_thunk_info;\n   fake_thunk_info.thunk_id = ThunkId(kTestThunkId);\n   auto fake_thunk = std::make_unique<FakeThunk>(\n       fake_thunk_info,\n       Thunk::BufferUses{\n           // Consume means the thunk can reuse the buffer for scratch space, so\n           // only check it on input.\n-          BufferUse::Consume(slice_i),\n+          BufferUse::Consume(slice_i, arg_shape),\n           // Write is undefined on input, but defined on output.\n-          BufferUse::Write(slice_o),\n+          BufferUse::Write(slice_o, arg_shape),\n           // Unlike Consume, Read is supposed to preserve the contents of the\n           // buffer, so we check it on input *and* output.\n-          BufferUse::Read(slice_io),\n+          BufferUse::Read(slice_io, arg_shape),\n           // Scratch buffers are not checked at all.\n-          BufferUse::Scratch(slice_scratch),\n+          BufferUse::Scratch(slice_scratch, arg_shape),\n       });\n   Thunk* fake_thunk_ptr = fake_thunk.get();\n   std::vector<std::unique_ptr<Thunk>> thunks;\n@@ -536,6 +540,7 @@ TEST_F(ThunkBufferDebugPassTest, BufferSaverInserter) {\n \n   // Create a fake thunk with a few different buffer uses.\n   BufferAllocation alloc(0, 1024, 0);\n+  Shape arg_shape = ShapeUtil::MakeShape(U8, {1});\n   BufferAllocation::Slice slice_o(&alloc, 1, 1, PrimitiveType::F32);\n   BufferAllocation::Slice slice_io(&alloc, 2, 1, PrimitiveType::F32);\n   Thunk::ThunkInfo fake_thunk_info;\n@@ -546,10 +551,10 @@ TEST_F(ThunkBufferDebugPassTest, BufferSaverInserter) {\n       fake_thunk_info,\n       Thunk::BufferUses{\n           // Write is undefined on input, but defined on output.\n-          BufferUse::Write(slice_o),\n+          BufferUse::Write(slice_o, arg_shape),\n           // Unlike Consume, Read is supposed to preserve the contents of the\n           // buffer, so we check it on input *and* output.\n-          BufferUse::Read(slice_io),\n+          BufferUse::Read(slice_io, arg_shape),\n       }));\n   auto root_thunk =\n       std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));"
        },
        {
            "sha": "11838b6966143d5e1d6aac4a753e9ffbfc88f923",
            "filename": "third_party/xla/xla/runtime/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2FBUILD?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -33,6 +33,7 @@ xla_cc_test(\n     srcs = [\"buffer_use_test.cc\"],\n     deps = [\n         \":buffer_use\",\n+        \"//xla:shape_util\",\n         \"//xla/service:buffer_assignment\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n@@ -93,6 +94,7 @@ xla_cc_test(\n         \":buffer_use\",\n         \":execution_graph\",\n         \":resource_use\",\n+        \"//xla:shape_util\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\","
        },
        {
            "sha": "4d0fa1cc40ee1a8b512a5395c972d552b0d1ac4e",
            "filename": "third_party/xla/xla/runtime/buffer_use.h",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.h?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -84,17 +84,6 @@ class BufferUse {\n                      ContentValidity::kDefinedOnOutput);\n   }\n \n-  ABSL_DEPRECATED(\"Please provide shape as well.\")\n-  static BufferUse Scratch(BufferAllocation::Slice slice) {\n-    return BufferUse(slice, MemoryAccess::kWrite, ContentValidity::kUndefined);\n-  }\n-\n-  ABSL_DEPRECATED(\"Please provide shape as well.\")\n-  static BufferUse Consume(BufferAllocation::Slice slice) {\n-    return BufferUse(slice, MemoryAccess::kWrite,\n-                     ContentValidity::kDefinedOnInput);\n-  }\n-\n   static BufferUse Read(BufferAllocation::Slice slice, Shape shape) {\n     return BufferUse(slice, MemoryAccess::kRead,\n                      ContentValidity::kDefinedOnInputAndOutput, shape);"
        },
        {
            "sha": "2110753016c5ce78ea580aee54d8cccb3a36dab5",
            "filename": "third_party/xla/xla/runtime/buffer_use_test.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 27,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use_test.cc?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -18,19 +18,22 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/strings/str_cat.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n \n namespace xla {\n namespace {\n \n TEST(BufferUseTest, Equality) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  Shape slice0_shape = ShapeUtil::MakeShape(F32, {2});\n+  BufferAllocation::Slice slice0(&alloc, 0, 8);\n \n-  BufferUse use_read0 = BufferUse::Read(slice0);\n-  BufferUse use_read1 = BufferUse::Read(slice0);\n-  BufferUse use_write = BufferUse::Write(slice0);\n-  BufferUse use_scratch = BufferUse::Scratch(slice0);\n-  BufferUse use_consume = BufferUse::Consume(slice0);\n+  BufferUse use_read0 = BufferUse::Read(slice0, slice0_shape);\n+  BufferUse use_read1 = BufferUse::Read(slice0, slice0_shape);\n+  BufferUse use_write = BufferUse::Write(slice0, slice0_shape);\n+  BufferUse use_scratch = BufferUse::Scratch(slice0, slice0_shape);\n+  BufferUse use_consume = BufferUse::Consume(slice0, slice0_shape);\n \n   EXPECT_EQ(use_read0, use_read1);\n   EXPECT_NE(use_read0, use_write);\n@@ -45,59 +48,62 @@ TEST(BufferUseTest, Equality) {\n \n TEST(BufferUseTest, HasDefinedContents) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice(&alloc, 0, 10);\n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {2});\n+  BufferAllocation::Slice slice(&alloc, 0, 8);\n \n-  BufferUse read = BufferUse::Read(slice);\n+  BufferUse read = BufferUse::Read(slice, slice_shape);\n   EXPECT_TRUE(read.HasDefinedContentsOnInput());\n   EXPECT_TRUE(read.HasDefinedContentsOnOutput());\n \n-  BufferUse write = BufferUse::Write(slice);\n+  BufferUse write = BufferUse::Write(slice, slice_shape);\n   EXPECT_FALSE(write.HasDefinedContentsOnInput());\n   EXPECT_TRUE(write.HasDefinedContentsOnOutput());\n \n-  BufferUse scratch = BufferUse::Scratch(slice);\n+  BufferUse scratch = BufferUse::Scratch(slice, slice_shape);\n   EXPECT_FALSE(scratch.HasDefinedContentsOnInput());\n   EXPECT_FALSE(scratch.HasDefinedContentsOnOutput());\n \n-  BufferUse consume = BufferUse::Consume(slice);\n+  BufferUse consume = BufferUse::Consume(slice, slice_shape);\n   EXPECT_TRUE(consume.HasDefinedContentsOnInput());\n   EXPECT_FALSE(consume.HasDefinedContentsOnOutput());\n }\n \n TEST(BufferUseTest, AbslStringify) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice(&alloc, 0, 10);\n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {2});\n+  BufferAllocation::Slice slice(&alloc, 0, 8);\n \n   EXPECT_EQ(\n-      absl::StrCat(BufferUse::Read(slice)),\n-      \"{slice: {index:0, offset:0, size:10}, access: R, content_validity: IO}\");\n+      absl::StrCat(BufferUse::Read(slice, slice_shape)),\n+      \"{slice: {index:0, offset:0, size:8}, access: R, content_validity: IO}\");\n   EXPECT_EQ(\n-      absl::StrCat(BufferUse::Write(slice)),\n-      \"{slice: {index:0, offset:0, size:10}, access: W, content_validity: O}\");\n+      absl::StrCat(BufferUse::Write(slice, slice_shape)),\n+      \"{slice: {index:0, offset:0, size:8}, access: W, content_validity: O}\");\n   EXPECT_EQ(\n-      absl::StrCat(BufferUse::Scratch(slice)),\n-      \"{slice: {index:0, offset:0, size:10}, access: W, content_validity: }\");\n+      absl::StrCat(BufferUse::Scratch(slice, slice_shape)),\n+      \"{slice: {index:0, offset:0, size:8}, access: W, content_validity: }\");\n   EXPECT_EQ(\n-      absl::StrCat(BufferUse::Consume(slice)),\n-      \"{slice: {index:0, offset:0, size:10}, access: W, content_validity: I}\");\n+      absl::StrCat(BufferUse::Consume(slice, slice_shape)),\n+      \"{slice: {index:0, offset:0, size:8}, access: W, content_validity: I}\");\n }\n \n TEST(BufferUseTest, ReadWriteSet) {\n   BufferUse::ReadWriteSet rwset;\n \n   BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n \n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  BufferAllocation::Slice slice1(&alloc, 5, 10);\n-  BufferAllocation::Slice slice2(&alloc, 10, 10);\n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {2});\n+  BufferAllocation::Slice slice0(&alloc, 0, 8);\n+  BufferAllocation::Slice slice1(&alloc, 4, 8);\n+  BufferAllocation::Slice slice2(&alloc, 8, 8);\n \n   rwset.Add(BufferUse::Read(slice0));\n-  EXPECT_FALSE(rwset.HasConflicts({BufferUse::Read(slice1)}));\n-  EXPECT_TRUE(rwset.HasConflicts({BufferUse::Write(slice1)}));\n-  EXPECT_FALSE(rwset.HasConflicts({BufferUse::Write(slice2)}));\n+  EXPECT_FALSE(rwset.HasConflicts({BufferUse::Read(slice1, slice_shape)}));\n+  EXPECT_TRUE(rwset.HasConflicts({BufferUse::Write(slice1, slice_shape)}));\n+  EXPECT_FALSE(rwset.HasConflicts({BufferUse::Write(slice2, slice_shape)}));\n \n   rwset.Add(BufferUse::Read(slice1));\n-  EXPECT_TRUE(rwset.HasConflicts({BufferUse::Write(slice2)}));\n+  EXPECT_TRUE(rwset.HasConflicts({BufferUse::Write(slice2, slice_shape)}));\n }\n \n }  // namespace"
        },
        {
            "sha": "77dd0ecf7bda9e5c89fd9732f0c6f510888b1080",
            "filename": "third_party/xla/xla/runtime/execution_graph_test.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 34,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2Fexecution_graph_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bad3f81749fcfa4c1447841e57285b3a50645792/third_party%2Fxla%2Fxla%2Fruntime%2Fexecution_graph_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fexecution_graph_test.cc?ref=bad3f81749fcfa4c1447841e57285b3a50645792",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n \n@@ -66,17 +67,18 @@ TEST(ExecutionGraphTest, EdgePriority) {\n TEST(ExecutionGraphTest, DependencyOrdering) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/80, /*color=*/0);\n \n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {10});\n   BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/40);\n   BufferAllocation::Slice slice1(&alloc, /*offset=*/40, /*size=*/40);\n   BufferAllocation::Slice slice2(&alloc, /*offset=*/20, /*size=*/40);\n \n   std::vector<Operation> operations;\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice0), BufferUse::Write(slice0)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice1), BufferUse::Write(slice1)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice2), BufferUse::Write(slice2)}));\n+  operations.push_back(Operation({BufferUse::Read(slice0, slice_shape),\n+                                  BufferUse::Write(slice0, slice_shape)}));\n+  operations.push_back(Operation({BufferUse::Read(slice1, slice_shape),\n+                                  BufferUse::Write(slice1, slice_shape)}));\n+  operations.push_back(Operation({BufferUse::Read(slice2, slice_shape),\n+                                  BufferUse::Write(slice2, slice_shape)}));\n \n   TF_ASSERT_OK_AND_ASSIGN(ExecutionGraph execution_graph,\n                           ExecutionGraph::Create<Operation>(operations));\n@@ -99,15 +101,16 @@ TEST(ExecutionGraphTest, DependencyOrdering) {\n \n TEST(ExecutionGraphTest, SequentialOrdering) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/80, /*color=*/0);\n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {10});\n   BufferAllocation::Slice slice(&alloc, /*offset=*/0, /*size=*/40);\n \n   std::vector<Operation> operations;\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice), BufferUse::Write(slice)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice), BufferUse::Write(slice)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice), BufferUse::Write(slice)}));\n+  operations.push_back(Operation({BufferUse::Read(slice, slice_shape),\n+                                  BufferUse::Write(slice, slice_shape)}));\n+  operations.push_back(Operation({BufferUse::Read(slice, slice_shape),\n+                                  BufferUse::Write(slice, slice_shape)}));\n+  operations.push_back(Operation({BufferUse::Read(slice, slice_shape),\n+                                  BufferUse::Write(slice, slice_shape)}));\n \n   TF_ASSERT_OK_AND_ASSIGN(ExecutionGraph execution_graph,\n                           ExecutionGraph::Create<Operation>(operations));\n@@ -133,18 +136,19 @@ TEST(ExecutionGraphTest, SequentialOrdering) {\n TEST(ExecutionGraphTest, TokenResourceOrdering) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/80, /*color=*/0);\n \n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {10});\n   BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/40);\n   BufferAllocation::Slice slice1(&alloc, /*offset=*/40, /*size=*/40);\n \n   auto resource = Resource::Create(Resource::Kind::kToken);\n \n   std::vector<Operation> operations;\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice0), BufferUse::Write(slice0)},\n-                {ResourceUse::Write(resource)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice1), BufferUse::Write(slice1)},\n-                {ResourceUse::Write(resource)}));\n+  operations.push_back(Operation({BufferUse::Read(slice0, slice_shape),\n+                                  BufferUse::Write(slice0, slice_shape)},\n+                                 {ResourceUse::Write(resource)}));\n+  operations.push_back(Operation({BufferUse::Read(slice1, slice_shape),\n+                                  BufferUse::Write(slice1, slice_shape)},\n+                                 {ResourceUse::Write(resource)}));\n \n   TF_ASSERT_OK_AND_ASSIGN(ExecutionGraph execution_graph,\n                           ExecutionGraph::Create<Operation>(operations));\n@@ -165,18 +169,19 @@ TEST(ExecutionGraphTest, TokenResourceOrdering) {\n TEST(ExecutionGraphTest, CollectivesResourceOrdering) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/80, /*color=*/0);\n \n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {10});\n   BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/40);\n   BufferAllocation::Slice slice1(&alloc, /*offset=*/40, /*size=*/40);\n \n   auto resource = Resource::Create(Resource::Kind::kCollectiveCommunicator);\n \n   std::vector<Operation> operations;\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice0), BufferUse::Write(slice0)},\n-                {ResourceUse::Write(resource)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice1), BufferUse::Write(slice1)},\n-                {ResourceUse::Write(resource)}));\n+  operations.push_back(Operation({BufferUse::Read(slice0, slice_shape),\n+                                  BufferUse::Write(slice0, slice_shape)},\n+                                 {ResourceUse::Write(resource)}));\n+  operations.push_back(Operation({BufferUse::Read(slice1, slice_shape),\n+                                  BufferUse::Write(slice1, slice_shape)},\n+                                 {ResourceUse::Write(resource)}));\n   operations.push_back(\n       Operation({BufferUse::Read(slice1), BufferUse::Write(slice1)},\n                 {ResourceUse::Write(resource)}));\n@@ -208,15 +213,16 @@ TEST(ExecutionGraphTest, CollectivesResourceOrdering) {\n \n TEST(ExecutionGraphTest, TransitiveReduction) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/80, /*color=*/0);\n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {10});\n   BufferAllocation::Slice slice(&alloc, /*offset=*/0, /*size=*/40);\n \n   std::vector<Operation> operations;\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice), BufferUse::Write(slice)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice), BufferUse::Write(slice)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice), BufferUse::Write(slice)}));\n+  operations.push_back(Operation({BufferUse::Read(slice, slice_shape),\n+                                  BufferUse::Write(slice, slice_shape)}));\n+  operations.push_back(Operation({BufferUse::Read(slice, slice_shape),\n+                                  BufferUse::Write(slice, slice_shape)}));\n+  operations.push_back(Operation({BufferUse::Read(slice, slice_shape),\n+                                  BufferUse::Write(slice, slice_shape)}));\n \n   TF_ASSERT_OK_AND_ASSIGN(ExecutionGraph execution_graph,\n                           ExecutionGraph::Create<Operation>(operations));\n@@ -240,6 +246,7 @@ TEST(ExecutionGraphTest, TransitiveReduction) {\n \n TEST(ExecutionGraphTest, TransitiveReductionKeepsExecutionEdge) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/80, /*color=*/0);\n+  Shape slice_shape = ShapeUtil::MakeShape(F32, {10});\n   BufferAllocation::Slice slice(&alloc, /*offset=*/0, /*size=*/40);\n \n   auto resource = Resource::Create(Resource::Kind::kCollectiveCommunicator);\n@@ -249,12 +256,12 @@ TEST(ExecutionGraphTest, TransitiveReductionKeepsExecutionEdge) {\n   // All three operations connected with scheduling edges, but because execution\n   // edge provides stronger ordering guarantee, we must keep an 0-2 execution\n   // edge, or we might get a data race.\n-  operations.push_back(\n-      Operation({BufferUse::Write(slice)}, {ResourceUse::Write(resource)}));\n+  operations.push_back(Operation({BufferUse::Write(slice, slice_shape)},\n+                                 {ResourceUse::Write(resource)}));\n   operations.push_back(\n       Operation(/*buffers=*/{}, {ResourceUse::Write(resource)}));\n-  operations.push_back(\n-      Operation({BufferUse::Write(slice)}, {ResourceUse::Write(resource)}));\n+  operations.push_back(Operation({BufferUse::Write(slice, slice_shape)},\n+                                 {ResourceUse::Write(resource)}));\n \n   TF_ASSERT_OK_AND_ASSIGN(ExecutionGraph execution_graph,\n                           ExecutionGraph::Create<Operation>(operations));"
        }
    ],
    "stats": {
        "total": 202,
        "additions": 108,
        "deletions": 94
    }
}