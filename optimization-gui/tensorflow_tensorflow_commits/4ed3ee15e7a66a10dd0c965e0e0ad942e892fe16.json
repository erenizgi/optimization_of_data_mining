{
    "author": "bchetioui",
    "message": "[XLA:GPU] Allow simplifying some `dot` point dimensions in `SymbolicTileAnalysis`.\n\nPreviously, we would never allow simplification when encountering a `dot`\ninstruction. But this constraint was overly conservative; the only dimensions\nthat we shouldn't simplify are those along which we intend to perform\nnon-standard padding to fit to hardware restrictions, i.e. the non-contracting\nand contracting dimensions.\n\nRestricting this pattern further works around a bug whereby expanding a\nnon-standardly padded dimension into a `1` dim can result in propagating a\ntile with the wrong size.\n\nThe underlying reason for this is a bug in the `kPreserve` behaviour of\n`IndexingMap` simplification, which will need to be fixed separately (the new\ntiling should avoid this issue, since it shouldn't rely on the correctness of\n`IndexingMap` simplification at this level).\n\nPiperOrigin-RevId: 823258725",
    "sha": "4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16",
    "files": [
        {
            "sha": "3f91528102bbf3e4b60f6a0769361bfbe3302ab3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16",
            "patch": "@@ -169,6 +169,82 @@ ENTRY entry {\n                       \"(num_warps, num_ctas, num_stages) must be positive\")));\n }\n \n+TEST_F(TritonEmitterDevicelessTest,\n+       BitcastReshapeDifferentTotalSizeRegressionTest) {\n+  // This is a regression test for a bug where indexing analysis would fail to\n+  // correctly preserve trivial dimensions, causing symbolic tile analysis to\n+  // produce an incorrect reshape.\n+  const std::string kHloText = R\"(\n+parameter0 {\n+  p0 = bf16[1,5,4] parameter(0)\n+  convert = f32[1,5,4] convert(p0)\n+  slice = f32[1,5,2] slice(convert), slice={[0:1], [0:5], [2:4]}\n+  ROOT bitcast = f32[5,2] bitcast(slice)\n+}\n+\n+parameter1 {\n+  ROOT p0 = f32[5,20]{0,1} parameter(0)\n+}\n+\n+fusion {\n+  p0 = bf16[1,5,4] parameter(0)\n+  p1 = f32[5,20]{0,1} parameter(1)\n+  fusion0 = f32[5,2] fusion(p0), kind=kCustom, calls=parameter0,\n+    backend_config={\"fusion_backend_config\":{\n+      \"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\n+        \"output_tiles\":[{\"sizes\":[\"16\",\"16\"]}],\n+        \"num_ctas\":1,\n+        \"num_stages\":1,\n+        \"is_tma_allowed\":false}}}\n+  fusion1 = f32[5,20]{0,1} fusion(p1), kind=kCustom, calls=parameter1,\n+    backend_config={\"fusion_backend_config\":{\n+      \"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\n+        \"output_tiles\":[{\"sizes\":[\"16\",\"16\"]}],\n+        \"num_ctas\":1,\n+        \"num_stages\":1,\n+        \"is_tma_allowed\":false}}}\n+  ROOT dot = f32[2,20] dot(fusion0, fusion1),\n+    lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY entry {\n+  p0 = bf16[1,5,4] parameter(0)\n+  p1 = f32[5,20]{0,1} parameter(1)\n+  ROOT root = f32[2,20] fusion(p0, p1), kind=kCustom, calls=fusion,\n+    backend_config={\"fusion_backend_config\":{\n+      \"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\n+        \"num_warps\":\"2\",\n+        \"output_tiles\":[{\"sizes\":[\"16\",\"16\"]}],\n+        \"num_ctas\":1,\n+        \"num_stages\":1,\n+        \"is_tma_allowed\":false}}}\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(kHloText));\n+  const HloFusionInstruction* triton_fusion = Cast<HloFusionInstruction>(\n+      hlo_module->entry_computation()->root_instruction());\n+  const se::DeviceDescription dev_info =\n+      TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+  llvm::LLVMContext llvm_ctx;\n+  llvm::Module llvm_module(\"module\", llvm_ctx);\n+  mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n+\n+  EXPECT_OK(\n+      CreateTritonModule(\"test_fn\", triton_fusion, dev_info,\n+                         BlockLevelParameters::FromBlockLevelFusionConfig(\n+                             triton_fusion->backend_config<GpuBackendConfig>()\n+                                 ->fusion_backend_config()\n+                                 .block_level_fusion_config()),\n+                         symbolic_expr_context));\n+}\n+\n TEST_F(WarpSpecializationTritonEmitterTest,\n        ExtraWarpsAreRequestedForWarpSpecialization) {\n   const std::string hlo_text = R\"("
        },
        {
            "sha": "d1c5e0fdfbcc6385ca294ae2c4bcba3168a325d4",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 1,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc?ref=4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16",
            "patch": "@@ -541,6 +541,41 @@ void SortTiledHloInstructionsInPostOrder(\n   }\n }\n \n+// Returns `true` if the given dot instruction has a non-batch point dimension.\n+//\n+// This function will perform these checks for all `dot`-like instructions for\n+// which `IsSomeDot` returns `true`.\n+bool IsDotWithNonBatchPointDimension(const HloInstruction* instr) {\n+  if (!IsSomeDot(instr)) {\n+    return false;\n+  }\n+\n+  auto has_any_trivial_dimension = [](const Shape& shape,\n+                                      absl::Span<const int64_t> dimensions) {\n+    return absl::c_any_of(\n+        dimensions, [&](int64_t dim) { return shape.dimensions(dim) == 1; });\n+  };\n+\n+  absl::Span<const int64_t> lhs_contracting_dimensions =\n+      instr->dot_dimension_numbers().lhs_contracting_dimensions();\n+  auto lhs_non_contracting_dimensions = GetNonContractingDims(\n+      instr->operand(0)->shape().dimensions().size(),\n+      lhs_contracting_dimensions,\n+      instr->dot_dimension_numbers().lhs_batch_dimensions());\n+\n+  auto rhs_non_contracting_dimensions = GetNonContractingDims(\n+      instr->operand(0)->shape().dimensions().size(),\n+      instr->dot_dimension_numbers().rhs_contracting_dimensions(),\n+      instr->dot_dimension_numbers().rhs_batch_dimensions());\n+\n+  return has_any_trivial_dimension(instr->operand(0)->shape(),\n+                                   lhs_non_contracting_dimensions) ||\n+         has_any_trivial_dimension(instr->operand(0)->shape(),\n+                                   lhs_contracting_dimensions) ||\n+         has_any_trivial_dimension(instr->operand(1)->shape(),\n+                                   rhs_non_contracting_dimensions);\n+}\n+\n // Returns `true` if `SymbolicTileAnalysis` should simplify point dimensions\n // away when deriving indexing maps.\n //\n@@ -571,7 +606,11 @@ bool ShouldDerivationSimplifyPointDimensions(const HloFusionAdaptor& fusion) {\n       continue;\n     }\n \n-    if (IsSomeDot(&instruction_adaptor.instruction())) {\n+    // We're OK with simplifying point dimensions if they occur only in the\n+    // batch dimensions of a dot, but not if they occur in the contracting or\n+    // or non-contracting dimensions. That's because batch dimensions are\n+    // unconstrained by the hardware, unlike the others.\n+    if (IsDotWithNonBatchPointDimension(&instruction_adaptor.instruction())) {\n       return false;\n     }\n "
        },
        {
            "sha": "e3c639360471b6cc42950a6a09bc4d304059ceec",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis_test.cc",
            "status": "modified",
            "additions": 75,
            "deletions": 3,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc?ref=4ed3ee15e7a66a10dd0c965e0e0ad942e892fe16",
            "patch": "@@ -64,6 +64,7 @@ namespace {\n \n using absl_testing::IsOkAndHolds;\n using detail::GetFlatTilingsForInputSpace;\n+using ::testing::ElementsAre;\n using ::testing::ElementsAreArray;\n using ::testing::ExplainMatchResult;\n using ::testing::IsEmpty;\n@@ -1645,8 +1646,8 @@ ENTRY e {\n   EXPECT_EQ(dynamic_slice->hlo()->opcode(), HloOpcode::kDynamicSlice);\n   const TiledHloInstruction* p0 = dynamic_slice->operand(0);\n   EXPECT_THAT(*p0, MatchTiledHloInstruction(\n-                       /*tile_sizes=*/{2, 8, 2},\n-                       /*tile_strides=*/{1, 1, 1},\n+                       /*tile_sizes=*/{1, 8, 2},\n+                       /*tile_strides=*/{0, 1, 1},\n                        /*tile_offsets_indexing=*/R\"(\n     (pid_0){rt0} -> (rt0, 0, 0), domain: pid_0 in [0, 0], rt0 in [0, 3]\n   )\"));\n@@ -2292,7 +2293,8 @@ ENTRY main {\n                   ::testing::HasSubstr(\"not divisible by tile size\")));\n }\n \n-TEST_F(SymbolicTileAnalysisTest, TrivialDimensionParametersArePreserved) {\n+TEST_F(SymbolicTileAnalysisTest,\n+       TrivialNonBatchDotDimensionParametersArePreserved) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n lhs {\n@@ -2369,6 +2371,76 @@ ENTRY main {\n                   \"pid_0 in [0, 35]\"));\n }\n \n+TEST_F(SymbolicTileAnalysisTest,\n+       TrivialBatchDotDimensionParametersAreEliminated) {\n+  // Note: the batch dot dimension parameters are only eliminated if contracting\n+  // and non-contracting dimensions do not contain trivial dimensions.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+lhs {\n+  ROOT p0 = f32[1,137,115] parameter(0)\n+}\n+\n+rhs {\n+  p0 = f32[1,2,115] parameter(0)\n+  ROOT root = f32[1,2,115] convert(p0)\n+}\n+\n+dot {\n+  p0 = f32[1,137,115] parameter(0)\n+  p1 = f32[1,2,115] parameter(1)\n+\n+  lhs = f32[1,137,115] fusion(p0),\n+    kind=kCustom, calls=lhs, backend_config={\n+      \"fusion_backend_config\":{\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"1\",\"16\",\"32\"]}]}}}\n+  rhs = f32[1,2,115] fusion(p1),\n+    kind=kCustom, calls=rhs, backend_config={\n+      \"fusion_backend_config\":{\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"1\",\"16\",\"32\"]}]}}}\n+\n+  ROOT dot = f32[1,137,2] dot(lhs, rhs),\n+    lhs_batch_dims={0}, rhs_batch_dims={0},\n+    lhs_contracting_dims={2}, rhs_contracting_dims={2}\n+}\n+\n+ENTRY main {\n+  p0 = f32[1,137,115] parameter(0)\n+  p1 = f32[1,2,115] parameter(1)\n+  ROOT fusion = f32[1,137,2] fusion(p0, p1),\n+    kind=kCustom, calls=dot\n+})\"));\n+  std::optional<SymbolicTileAnalysis> analysis = TryAnalyzeModule(module.get());\n+  ASSERT_TRUE(analysis.has_value());\n+  const HloInstruction* dot_hlo =\n+      module->entry_computation()->root_instruction()->fused_expression_root();\n+  Tiling tiling(Tiling::TileMapping{{dot_hlo, {32, 1, 16, 16}}});\n+  TF_ASSERT_OK_AND_ASSIGN(TiledHloComputation tiled_hlo_computation,\n+                          analysis->ComputeTiledHloInstructions(\n+                              tiling, default_schedule_builder_,\n+                              /*constraints_are_known_satisfied=*/false,\n+                              /*compute_all_tile_offset_indexing_maps=*/true));\n+\n+  const TiledHloInstruction* dot = tiled_hlo_computation.GetRoots().front();\n+  ASSERT_EQ(dot->hlo()->opcode(), HloOpcode::kDot);\n+\n+  const TiledHloFusionInstruction* lhs_fusion =\n+      static_cast<const TiledHloFusionInstruction*>(dot->operand(0));\n+  const TiledHloFusionInstruction* rhs_fusion =\n+      static_cast<const TiledHloFusionInstruction*>(dot->operand(1));\n+\n+  // We recognize that the batch dimension has been simplified away by the fact\n+  // that the stride in the relevant dimension is 0.\n+  EXPECT_THAT(\n+      lhs_fusion->called_computation()->GetRoots().front()->tile_strides(),\n+      ElementsAre(0, 1, 1));\n+  EXPECT_THAT(\n+      rhs_fusion->called_computation()->GetRoots().front()->tile_strides(),\n+      ElementsAre(0, 1, 1));\n+}\n+\n TEST_F(SymbolicTileAnalysisTest,\n        SymbolicTilesAlwaysDependOnAllTheHiddenParameters) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,"
        }
    ],
    "stats": {
        "total": 195,
        "additions": 191,
        "deletions": 4
    }
}