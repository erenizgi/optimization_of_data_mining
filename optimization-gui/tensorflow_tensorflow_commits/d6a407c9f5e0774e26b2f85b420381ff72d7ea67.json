{
    "author": "tensorflower-gardener",
    "message": "Integrate LLVM at llvm/llvm-project@7d381f2a5634\n\nUpdates LLVM usage to match\n[7d381f2a5634](https://github.com/llvm/llvm-project/commit/7d381f2a5634)\n\nPiperOrigin-RevId: 846858892",
    "sha": "d6a407c9f5e0774e26b2f85b420381ff72d7ea67",
    "files": [
        {
            "sha": "f82404ca1cbe14f2e2dda487bea23f3978f899da",
            "filename": "third_party/xla/third_party/llvm/generated.patch",
            "status": "modified",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch?ref=d6a407c9f5e0774e26b2f85b420381ff72d7ea67",
            "patch": "@@ -1 +1,152 @@\n Auto generated patch. Do not edit or delete it, even if empty.\n+diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTReaderDecl.cpp b/clang/lib/Serialization/ASTReaderDecl.cpp\n+--- a/clang/lib/Serialization/ASTReaderDecl.cpp\n++++ b/clang/lib/Serialization/ASTReaderDecl.cpp\n+@@ -2107,8 +2107,9 @@\n+     auto *Def = DD.Definition;\n+     DD = std::move(MergeDD);\n+     DD.Definition = Def;\n+-    for (auto *D : Def->redecls())\n+-      cast<CXXRecordDecl>(D)->DefinitionData = &DD;\n++    for (auto *R = Reader.getMostRecentExistingDecl(Def); R;\n++         R = R->getPreviousDecl())\n++      cast<CXXRecordDecl>(R)->DefinitionData = &DD;\n+     return;\n+   }\n+ \n+diff -ruN --strip-trailing-cr a/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h b/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h\n+--- a/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h\n++++ b/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h\n+@@ -61,14 +61,14 @@\n+ LIBC_INLINE static void write_mxcsr(uint32_t w) { _mm_setcsr(w); }\n+ \n+ LIBC_INLINE static void clear_except(uint16_t excepts) {\n+-  uint32_t mxcsr = _MM_GET_EXCEPTION_STATE();\n++  uint32_t mxcsr = get_mxcsr();\n+   mxcsr &= ~static_cast<uint32_t>(excepts);\n+-  _MM_SET_EXCEPTION_STATE(mxcsr);\n++  write_mxcsr(mxcsr);\n+ }\n+ \n+ LIBC_INLINE static uint16_t test_except(uint16_t excepts) {\n+   uint32_t mxcsr = get_mxcsr();\n+-  return static_cast<uint16_t>(excepts & mxcsr);\n++  return static_cast<uint16_t>(excepts & ExceptionFlags::ALL_F & mxcsr);\n+ }\n+ \n+ LIBC_INLINE static uint16_t get_except() {\n+@@ -83,9 +83,9 @@\n+ }\n+ \n+ LIBC_INLINE static void raise_except(uint16_t excepts) {\n+-  uint32_t mxcsr = _MM_GET_EXCEPTION_STATE();\n+-  mxcsr |= excepts;\n+-  _MM_SET_EXCEPTION_STATE(mxcsr);\n++  uint32_t mxcsr = get_mxcsr();\n++  mxcsr |= excepts & ExceptionFlags::ALL_F;\n++  write_mxcsr(mxcsr);\n+ #ifdef LIBC_TRAP_ON_RAISE_FP_EXCEPT\n+   // We will try to trigger the SIGFPE if floating point exceptions are not\n+   // masked.  Since we already set all the floating point exception flags, we\n+diff -ruN --strip-trailing-cr a/libcxx/include/__flat_map/flat_map.h b/libcxx/include/__flat_map/flat_map.h\n+--- a/libcxx/include/__flat_map/flat_map.h\n++++ b/libcxx/include/__flat_map/flat_map.h\n+@@ -465,13 +465,13 @@\n+   }\n+ \n+   // [flat.map.access], element access\n+-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](const key_type& __x)\n++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](const key_type& __x)\n+     requires is_constructible_v<mapped_type>\n+   {\n+     return try_emplace(__x).first->second;\n+   }\n+ \n+-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](key_type&& __x)\n++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](key_type&& __x)\n+     requires is_constructible_v<mapped_type>\n+   {\n+     return try_emplace(std::move(__x)).first->second;\n+@@ -480,7 +480,7 @@\n+   template <class _Kp>\n+     requires(__is_compare_transparent && is_constructible_v<key_type, _Kp> && is_constructible_v<mapped_type> &&\n+              !is_convertible_v<_Kp &&, const_iterator> && !is_convertible_v<_Kp &&, iterator>)\n+-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](_Kp&& __x) {\n++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](_Kp&& __x) {\n+     return try_emplace(std::forward<_Kp>(__x)).first->second;\n+   }\n+ \n+diff -ruN --strip-trailing-cr a/libcxx/include/map b/libcxx/include/map\n+--- a/libcxx/include/map\n++++ b/libcxx/include/map\n+@@ -1092,9 +1092,9 @@\n+   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI size_type size() const _NOEXCEPT { return __tree_.size(); }\n+   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI size_type max_size() const _NOEXCEPT { return __tree_.max_size(); }\n+ \n+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n+ #  ifndef _LIBCPP_CXX03_LANG\n+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n+ #  endif\n+ \n+   template <class _Arg,\n+diff -ruN --strip-trailing-cr a/libcxx/include/unordered_map b/libcxx/include/unordered_map\n+--- a/libcxx/include/unordered_map\n++++ b/libcxx/include/unordered_map\n+@@ -1262,9 +1262,9 @@\n+   }\n+ #  endif // _LIBCPP_STD_VER >= 20\n+ \n+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n+ #  ifndef _LIBCPP_CXX03_LANG\n+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n+ #  endif\n+ \n+   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& at(const key_type& __k);\n+diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp\n+--- a/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp\n++++ b/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp\n+@@ -66,9 +66,9 @@\n+   TransparentKey<int> tkey;\n+ \n+   std::flat_map<int, int> nfm;\n+-  nfm[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+-  fm[std::move(key)];  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+-  fm[std::move(tkey)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++  nfm[key];            // no-warning\n++  fm[std::move(key)];  // no-warning\n++  fm[std::move(tkey)]; // no-warning\n+ \n+   fm.at(key);   // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+   cfm.at(key);  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp\n+--- a/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp\n++++ b/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp\n+@@ -55,8 +55,8 @@\n+ \n+   int key = 0;\n+ \n+-  m[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+-  m[std::move(key)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++  m[key];            // no-warning\n++  m[std::move(key)]; // no-warning\n+ \n+ #if TEST_STD_VER >= 14\n+   std::map<std::string, int, std::less<>> strMap;\n+diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp\n+--- a/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp\n++++ b/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp\n+@@ -81,8 +81,8 @@\n+   ctm.equal_range(tkey); // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+ #endif\n+ \n+-  m[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+-  m[std::move(key)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++  m[key];            // no-warning\n++  m[std::move(key)]; // no-warning\n+ \n+   m.at(key);  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+   cm.at(key); // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}"
        },
        {
            "sha": "29af0ffbd8c12c7a0f62e07307111582fac4d76d",
            "filename": "third_party/xla/third_party/llvm/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl?ref=d6a407c9f5e0774e26b2f85b420381ff72d7ea67",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n \n def repo(name):\n     \"\"\"Imports LLVM.\"\"\"\n-    LLVM_COMMIT = \"8f264586d7521b0e305ca7bb78825aa3382ffef7\"\n-    LLVM_SHA256 = \"5784c4af94caba66bc8c460e07e222f751e4f4c9db9c45b3a68ff55379cf587d\"\n+    LLVM_COMMIT = \"7d381f2a5634d1e41b61299839d652cc4a021898\"\n+    LLVM_SHA256 = \"f1641918fd3f5e1667d39afb9c261da39ed9f74e30f1c2f98031d6d609a8de15\"\n \n     tf_http_archive(\n         name = name,"
        },
        {
            "sha": "3e0e0520e60482a43b5bc7809832b70143fdc43f",
            "filename": "third_party/xla/third_party/shardy/temporary.patch",
            "status": "modified",
            "additions": 471,
            "deletions": 0,
            "changes": 471,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch?ref=d6a407c9f5e0774e26b2f85b420381ff72d7ea67",
            "patch": "@@ -0,0 +1,471 @@\n+diff --git a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc\n+index aceb4d7..8752484 100644\n+--- a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc\n++++ b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc\n+@@ -908,8 +908,8 @@ void insertAllReducesForReductionFactors(\n+   }\n+ }\n+ \n+-bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,\n+-                                          const SymbolTable& symbolTable) {\n++bool convertReshardToShardedToUnreduced(Operation* op, IRRewriter& rewriter,\n++                                        const SymbolTable& symbolTable) {\n+   ReshardOp reshardOp = dyn_cast<ReshardOp>(op);\n+   if (!reshardOp) {\n+     return false;\n+@@ -934,12 +934,7 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,\n+       << \"Reshard op has different meshes for input and output. The result has \"\n+          \"non-empty unreduced axes.\";\n+ \n+-  // The relationship of the unreduced axes is \"out = in + r2u + s2u\", where\n+-  // \"r2u\" is the replicated-to-unreduced axes and \"s2u\" is the\n+-  // sharded-to-unreduced axes.\n+-  SmallVector<AxisRefAttr> r2uAnds2uAxes =\n+-      getAxisSetDiff(outUnreducedAxes, inUnreducedAxes, inMesh);\n+-  if (r2uAnds2uAxes.empty()) {\n++  if (getAxisSetDiff(outUnreducedAxes, inUnreducedAxes, inMesh).empty()) {\n+     return false;\n+   }\n+ \n+@@ -950,7 +945,7 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,\n+       << \"Input of sharded-to-unreduced reshard must be a block argument or a \"\n+          \"reshard op.\";\n+ \n+-  SmallVector<AxisRefAttr> s2uAxes;\n++  SmallVector<AxisRefAttr> newUnreducedAxes = llvm::to_vector(inUnreducedAxes);\n+   SmallVector<AxisRefListAttr> axesPerDim(inSharding.getRank());\n+   for (auto [inDimSharding, outDimSharding, axes] :\n+        llvm::zip_equal(inSharding.getDimShardings(),\n+@@ -971,7 +966,7 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,\n+       }\n+       diff.append(inAxes.begin() + outAxes.size(), inAxes.end());\n+       axes = AxisRefListAttr::get(rewriter.getContext(), diff);\n+-      s2uAxes.append(diff);\n++      newUnreducedAxes.append(diff);\n+     } else {\n+       SDY_LOG(FATAL)\n+           << \"The reshard op needs to be decomposed to a sharded-to-unreduced \"\n+@@ -979,27 +974,17 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,\n+     }\n+   }\n+ \n++  sortAndMergeAxes(newUnreducedAxes, inMesh);\n++\n+   rewriter.setInsertionPoint(reshardOp);\n+-  Value result = input;\n+-\n+-  SmallVector<AxisRefAttr> r2uAxes =\n+-      getAxisSetDiff(r2uAnds2uAxes, s2uAxes, inMesh);\n+-  if (!r2uAxes.empty()) {\n+-    SmallVector<AxisRefAttr> inPlusR2uAxes = llvm::to_vector(inUnreducedAxes);\n+-    inPlusR2uAxes.append(r2uAxes.begin(), r2uAxes.end());\n+-    sortAndMergeAxes(inPlusR2uAxes, inMesh);\n+-    TensorShardingAttr r2uSharding =\n+-        TensorShardingAttr::get(rewriter.getContext(), inSharding.getMeshName(),\n+-                                inSharding.getDimShardings(),\n+-                                outSharding.getReplicatedAxes(), inPlusR2uAxes);\n+-    result = ReplicatedToUnreducedOp::create(rewriter, reshardOp.getLoc(),\n+-                                             result, r2uAxes, r2uSharding);\n+-  }\n+-  if (!s2uAxes.empty()) {\n+-    result = ShardedToUnreducedOp::create(rewriter, reshardOp.getLoc(), result,\n+-                                          axesPerDim, outSharding);\n++  Operation* result = ShardedToUnreducedOp::create(\n++      rewriter, reshardOp.getLoc(), input, axesPerDim,\n++      outSharding.replaceUnreducedAxes(newUnreducedAxes));\n++  if (newUnreducedAxes != outUnreducedAxes) {\n++    SDY_LOG(WARNING) << \"need repliaced-to-unreduced\";\n++    result = ReshardOp::create(rewriter, reshardOp.getLoc(),\n++                               result->getResult(0), outSharding);\n+   }\n+-\n+   rewriter.replaceOp(reshardOp, result);\n+   return true;\n+ }\n+diff --git a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h\n+index 0a5563f..c183216 100644\n+--- a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h\n++++ b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h\n+@@ -164,19 +164,15 @@ AxesPerFactor findCommonAxes(const ShardingProjection& shardingProjection,\n+                              OpShardingRuleAttr shardingRule,\n+                              ArrayRef<int64_t> tensorSizes, const Mesh& mesh);\n+ \n+-// Converts a `sdy.reshard` op to an `sdy.replicated-to-unreduced` op and/or an\n+-// `sdy.sharded-to-unreduced` op. Returns true if the conversion is successful.\n+-//\n+-// `r2u` keeps the sharded size, while `s2u` increases the sharded size. Hence,\n+-// we do `r2u` first and then `s2u`.\n++// Converts a `sdy.reshard` op to an `sdy.sharded-to-unreduced` op. Returns true\n++// if the conversion is successful.\n+ //\n+ // The requirements are:\n+ // 1. `op` is a `sdy.reshard` op.\n+-// 2. The input and output shardings have the same mesh.\n+-// 3. The input of `op` is another `sdy.reshard` op or a block argument.\n+-// 4. The input unreduced axes is a strict subset of the output unreduced axes.\n+-bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,\n+-                                          const SymbolTable& symbolTable);\n++// 2. The input of `op` is another `sdy.reshard` op or a block argument.\n++// 3. The `op` can be converted to a single `sdy.sharded-to-unreduced` op.\n++bool convertReshardToShardedToUnreduced(Operation* op, IRRewriter& rewriter,\n++                                        const SymbolTable& symbolTable);\n+ \n+ }  // namespace sdy\n+ }  // namespace mlir\n+diff --git a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc\n+index 85d048e..7f96c9b 100644\n+--- a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc\n++++ b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc\n+@@ -486,7 +486,7 @@ struct InsertExplicitReshardsPass\n+         return;\n+       }\n+ \n+-      if (convertReshardToUnreducedCollectives(op, rewriter, symbolTable)) {\n++      if (convertReshardToShardedToUnreduced(op, rewriter, symbolTable)) {\n+         return;\n+       }\n+ \n+diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir\n+index f30109e..f3868a9 100644\n+--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir\n++++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir\n+@@ -2,7 +2,6 @@\n+ \n+ sdy.mesh @mesh = <[\"x\"=2, \"y\"=2, \"z\"=4]>\n+ sdy.mesh @other_mesh = <[\"x\"=2, \"y\"=2]>\n+-sdy.mesh @mesh_x16 = <[\"x\"=16]>\n+ sdy.mesh @mesh_abcd = <[\"a\"=2, \"b\"=2, \"c\"=2, \"d\"=2]>\n+ \n+ //===----------------------------------------------------------------------===//\n+@@ -521,17 +520,17 @@ func.func @different_arguments_to_multiple_named_computations_with_same_input_ou\n+ }\n+ \n+ //===----------------------------------------------------------------------===//\n+-// Replicated and sharded to unreduced tests\n++// Sharded to unreduced tests\n+ //===----------------------------------------------------------------------===//\n+ \n+-// CHECK-LABEL: func @sharded_to_unreduced\n+-func.func @sharded_to_unreduced(\n+-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>})\n+-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\"}>}) {\n++// CHECK-LABEL: func @sharded_to_unreduced_1\n++func.func @sharded_to_unreduced_1(\n++    %arg0 : tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>})\n++    -> (tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\"}>}) {\n+   // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{\"x\"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\"}>\n+   // CHECK-NEXT: return %0\n+-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\"}> : tensor<16x8xf32>\n+-  return %0 : tensor<16x8xf32>\n++  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\"}> : tensor<24x8xf32>\n++  return %0 : tensor<24x8xf32>\n+ }\n+ \n+ // CHECK-LABEL: func @sharded_to_unreduced_single_axis\n+@@ -574,44 +573,13 @@ func.func @sharded_to_unreduced_with_subaxis(\n+  return %0 : tensor<16x8xf32>\n+ }\n+ \n+-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_full_axis\n+-func.func @implicitly_and_explicitly_replicated_to_unreduced_full_axis(\n+-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], replicated={\"z\"}, unreduced={\"y\"}>})\n+-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"x\", \"z\"} %arg0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>\n+-  // CHECK-NEXT: return %0\n+-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}> : tensor<16x8xf32>\n+-  return %0 : tensor<16x8xf32>\n+-}\n+-\n+-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis\n+-func.func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis(\n+-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{\"x\":(1)2}, {}], replicated={\"x\":(8)2}, unreduced={\"x\":(4)2}>})\n+-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{\"x\":(1)2}, {}], unreduced={\"x\":(2)8}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"x\":(2)2, \"x\":(8)2} %arg0 out_sharding=<@mesh_x16, [{\"x\":(1)2}, {}], unreduced={\"x\":(2)8}>\n+-  // CHECK-NEXT: return %0\n+-  %0 = sdy.reshard %arg0 <@mesh_x16, [{\"x\":(1)2}, {}], unreduced={\"x\":(2)8}> : tensor<16x8xf32>\n+-  return %0 : tensor<16x8xf32>\n+-}\n+-\n+-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_full_axis\n+-func.func @replicated_and_sharded_to_unreduced_full_axis(\n++// CHECK-LABEL: func @sharded_to_unreduced_and_replicated_to_unreduced\n++func.func @sharded_to_unreduced_and_replicated_to_unreduced(\n+     %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}], unreduced={\"y\"}>})\n+     -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"z\"} %arg0 out_sharding=<@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}> : tensor<16x8xf32>\n+-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{\"x\"}, {}] %0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}> : tensor<16x8xf32>\n++  // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{\"x\"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\", \"y\"}>\n++  // CHECK-NEXT: %1 = sdy.reshard %0 <@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>\n+   // CHECK-NEXT: return %1\n+  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}> :  tensor<16x8xf32>\n+  return %0 : tensor<16x8xf32>\n+ }\n+-\n+-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_sub_axis\n+-func.func @replicated_and_sharded_to_unreduced_sub_axis(\n+-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"z\":(1)2}], unreduced={\"y\"}>})\n+-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"z\":(2)2} %arg0 out_sharding=<@mesh, [{\"x\"}, {\"z\":(1)2}], unreduced={\"y\", \"z\":(2)2}> : tensor<16x8xf32>\n+-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{}, {\"z\":(1)2}] %0 out_sharding=<@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}> : tensor<16x8xf32>\n+-  // CHECK-NEXT: return %1\n+- %0 = sdy.reshard %arg0 <@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}> :  tensor<16x8xf32>\n+- return %0 : tensor<16x8xf32>\n+-}\n+diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir\n+index 5b1973a..5dea360 100644\n+--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir\n++++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir\n+@@ -1,7 +1,6 @@\n+ // RUN: sdy_opt %s -sdy-insert-explicit-reshards='enable-full-version=true' | FileCheck %s\n+ \n+ sdy.mesh @mesh = <[\"x\"=4, \"y\"=2, \"z\"=4]>\n+-sdy.mesh @mesh_x16 = <[\"x\"=16]>\n+ \n+ // CHECK-LABEL: func @all_reduce_on_func_input\n+ func.func @all_reduce_on_func_input(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"y\"}>}, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32> {\n+@@ -307,17 +306,17 @@ func.func @all_reduce_source_and_target_fully_replicated_shardings_and_different\n+ }\n+ \n+ //===----------------------------------------------------------------------===//\n+-// Replicated and sharded to unreduced tests\n++// Sharded to unreduced tests\n+ //===----------------------------------------------------------------------===//\n+ \n+-// CHECK-LABEL: func @sharded_to_unreduced\n+-func.func @sharded_to_unreduced(\n+-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>})\n+-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\"}>}) {\n++// CHECK-LABEL: func @sharded_to_unreduced_1\n++func.func @sharded_to_unreduced_1(\n++    %arg0 : tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>})\n++    -> (tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\"}>}) {\n+   // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{\"x\"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\"}>\n+   // CHECK-NEXT: return %0\n+-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\"}> : tensor<32x32xf32>\n+-  return %0 : tensor<32x32xf32>\n++  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\"}> : tensor<24x8xf32>\n++  return %0 : tensor<24x8xf32>\n+ }\n+ \n+ // CHECK-LABEL: func @sharded_to_unreduced_single_axis\n+@@ -360,44 +359,13 @@ func.func @sharded_to_unreduced_with_subaxis(\n+  return %0 : tensor<32x32xf32>\n+ }\n+ \n+-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_full_axis\n+-func.func @implicitly_and_explicitly_replicated_to_unreduced_full_axis(\n+-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], replicated={\"z\"}, unreduced={\"y\"}>})\n+-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"x\", \"z\"} %arg0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>\n+-  // CHECK-NEXT: return %0\n+-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}> : tensor<32x32xf32>\n+-  return %0 : tensor<32x32xf32>\n+-}\n+-\n+-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis\n+-func.func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis(\n+-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{\"x\":(1)2}, {}], replicated={\"x\":(8)2}, unreduced={\"x\":(4)2}>})\n+-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{\"x\":(1)2}, {}], unreduced={\"x\":(2)8}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"x\":(2)2, \"x\":(8)2} %arg0 out_sharding=<@mesh_x16, [{\"x\":(1)2}, {}], unreduced={\"x\":(2)8}>\n+-  // CHECK-NEXT: return %0\n+-  %0 = sdy.reshard %arg0 <@mesh_x16, [{\"x\":(1)2}, {}], unreduced={\"x\":(2)8}> : tensor<32x32xf32>\n+-  return %0 : tensor<32x32xf32>\n+-}\n+-\n+-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_full_axis\n+-func.func @replicated_and_sharded_to_unreduced_full_axis(\n++// CHECK-LABEL: func @sharded_to_unreduced_and_replicated_to_unreduced\n++func.func @sharded_to_unreduced_and_replicated_to_unreduced(\n+     %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}], unreduced={\"y\"}>})\n+     -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"z\"} %arg0 out_sharding=<@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}> : tensor<32x32xf32>\n+-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{\"x\"}, {}] %0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}> : tensor<32x32xf32>\n++  // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{\"x\"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\", \"y\"}>\n++  // CHECK-NEXT: %1 = sdy.reshard %0 <@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>\n+   // CHECK-NEXT: return %1\n+  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}> :  tensor<32x32xf32>\n+  return %0 : tensor<32x32xf32>\n+ }\n+-\n+-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_sub_axis\n+-func.func @replicated_and_sharded_to_unreduced_sub_axis(\n+-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"z\":(1)2}], unreduced={\"y\"}>})\n+-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}>}) {\n+-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {\"z\":(2)2} %arg0 out_sharding=<@mesh, [{\"x\"}, {\"z\":(1)2}], unreduced={\"y\", \"z\":(2)2}> : tensor<32x32xf32>\n+-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{}, {\"z\":(1)2}] %0 out_sharding=<@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}> : tensor<32x32xf32>\n+-  // CHECK-NEXT: return %1\n+- %0 = sdy.reshard %arg0 <@mesh, [{\"x\"}, {}], unreduced={\"y\", \"z\"}> :  tensor<32x32xf32>\n+- return %0 : tensor<32x32xf32>\n+-}\n+diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch\n+index 509398d..f82404c 100644\n+--- a/third_party/llvm/generated.patch\n++++ b/third_party/llvm/generated.patch\n+@@ -1 +1,152 @@\n+ Auto generated patch. Do not edit or delete it, even if empty.\n++diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTReaderDecl.cpp b/clang/lib/Serialization/ASTReaderDecl.cpp\n++--- a/clang/lib/Serialization/ASTReaderDecl.cpp\n+++++ b/clang/lib/Serialization/ASTReaderDecl.cpp\n++@@ -2107,8 +2107,9 @@\n++     auto *Def = DD.Definition;\n++     DD = std::move(MergeDD);\n++     DD.Definition = Def;\n++-    for (auto *D : Def->redecls())\n++-      cast<CXXRecordDecl>(D)->DefinitionData = &DD;\n+++    for (auto *R = Reader.getMostRecentExistingDecl(Def); R;\n+++         R = R->getPreviousDecl())\n+++      cast<CXXRecordDecl>(R)->DefinitionData = &DD;\n++     return;\n++   }\n++ \n++diff -ruN --strip-trailing-cr a/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h b/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h\n++--- a/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h\n+++++ b/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h\n++@@ -61,14 +61,14 @@\n++ LIBC_INLINE static void write_mxcsr(uint32_t w) { _mm_setcsr(w); }\n++ \n++ LIBC_INLINE static void clear_except(uint16_t excepts) {\n++-  uint32_t mxcsr = _MM_GET_EXCEPTION_STATE();\n+++  uint32_t mxcsr = get_mxcsr();\n++   mxcsr &= ~static_cast<uint32_t>(excepts);\n++-  _MM_SET_EXCEPTION_STATE(mxcsr);\n+++  write_mxcsr(mxcsr);\n++ }\n++ \n++ LIBC_INLINE static uint16_t test_except(uint16_t excepts) {\n++   uint32_t mxcsr = get_mxcsr();\n++-  return static_cast<uint16_t>(excepts & mxcsr);\n+++  return static_cast<uint16_t>(excepts & ExceptionFlags::ALL_F & mxcsr);\n++ }\n++ \n++ LIBC_INLINE static uint16_t get_except() {\n++@@ -83,9 +83,9 @@\n++ }\n++ \n++ LIBC_INLINE static void raise_except(uint16_t excepts) {\n++-  uint32_t mxcsr = _MM_GET_EXCEPTION_STATE();\n++-  mxcsr |= excepts;\n++-  _MM_SET_EXCEPTION_STATE(mxcsr);\n+++  uint32_t mxcsr = get_mxcsr();\n+++  mxcsr |= excepts & ExceptionFlags::ALL_F;\n+++  write_mxcsr(mxcsr);\n++ #ifdef LIBC_TRAP_ON_RAISE_FP_EXCEPT\n++   // We will try to trigger the SIGFPE if floating point exceptions are not\n++   // masked.  Since we already set all the floating point exception flags, we\n++diff -ruN --strip-trailing-cr a/libcxx/include/__flat_map/flat_map.h b/libcxx/include/__flat_map/flat_map.h\n++--- a/libcxx/include/__flat_map/flat_map.h\n+++++ b/libcxx/include/__flat_map/flat_map.h\n++@@ -465,13 +465,13 @@\n++   }\n++ \n++   // [flat.map.access], element access\n++-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](const key_type& __x)\n+++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](const key_type& __x)\n++     requires is_constructible_v<mapped_type>\n++   {\n++     return try_emplace(__x).first->second;\n++   }\n++ \n++-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](key_type&& __x)\n+++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](key_type&& __x)\n++     requires is_constructible_v<mapped_type>\n++   {\n++     return try_emplace(std::move(__x)).first->second;\n++@@ -480,7 +480,7 @@\n++   template <class _Kp>\n++     requires(__is_compare_transparent && is_constructible_v<key_type, _Kp> && is_constructible_v<mapped_type> &&\n++              !is_convertible_v<_Kp &&, const_iterator> && !is_convertible_v<_Kp &&, iterator>)\n++-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](_Kp&& __x) {\n+++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](_Kp&& __x) {\n++     return try_emplace(std::forward<_Kp>(__x)).first->second;\n++   }\n++ \n++diff -ruN --strip-trailing-cr a/libcxx/include/map b/libcxx/include/map\n++--- a/libcxx/include/map\n+++++ b/libcxx/include/map\n++@@ -1092,9 +1092,9 @@\n++   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI size_type size() const _NOEXCEPT { return __tree_.size(); }\n++   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI size_type max_size() const _NOEXCEPT { return __tree_.max_size(); }\n++ \n++-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n+++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n++ #  ifndef _LIBCPP_CXX03_LANG\n++-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n+++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n++ #  endif\n++ \n++   template <class _Arg,\n++diff -ruN --strip-trailing-cr a/libcxx/include/unordered_map b/libcxx/include/unordered_map\n++--- a/libcxx/include/unordered_map\n+++++ b/libcxx/include/unordered_map\n++@@ -1262,9 +1262,9 @@\n++   }\n++ #  endif // _LIBCPP_STD_VER >= 20\n++ \n++-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n+++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);\n++ #  ifndef _LIBCPP_CXX03_LANG\n++-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n+++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);\n++ #  endif\n++ \n++   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& at(const key_type& __k);\n++diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp\n++--- a/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp\n+++++ b/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp\n++@@ -66,9 +66,9 @@\n++   TransparentKey<int> tkey;\n++ \n++   std::flat_map<int, int> nfm;\n++-  nfm[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++-  fm[std::move(key)];  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++-  fm[std::move(tkey)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+++  nfm[key];            // no-warning\n+++  fm[std::move(key)];  // no-warning\n+++  fm[std::move(tkey)]; // no-warning\n++ \n++   fm.at(key);   // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++   cfm.at(key);  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp\n++--- a/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp\n+++++ b/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp\n++@@ -55,8 +55,8 @@\n++ \n++   int key = 0;\n++ \n++-  m[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++-  m[std::move(key)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+++  m[key];            // no-warning\n+++  m[std::move(key)]; // no-warning\n++ \n++ #if TEST_STD_VER >= 14\n++   std::map<std::string, int, std::less<>> strMap;\n++diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp\n++--- a/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp\n+++++ b/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp\n++@@ -81,8 +81,8 @@\n++   ctm.equal_range(tkey); // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++ #endif\n++ \n++-  m[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++-  m[std::move(key)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+++  m[key];            // no-warning\n+++  m[std::move(key)]; // no-warning\n++ \n++   m.at(key);  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n++   cm.at(key); // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}\n+diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl\n+index f2c3289..29af0ff 100644\n+--- a/third_party/llvm/workspace.bzl\n++++ b/third_party/llvm/workspace.bzl\n+@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n+ \n+ def repo(name):\n+     \"\"\"Imports LLVM.\"\"\"\n+-    LLVM_COMMIT = \"8f264586d7521b0e305ca7bb78825aa3382ffef7\"\n+-    LLVM_SHA256 = \"5784c4af94caba66bc8c460e07e222f751e4f4c9db9c45b3a68ff55379cf587d\"\n++    LLVM_COMMIT = \"7d381f2a5634d1e41b61299839d652cc4a021898\"\n++    LLVM_SHA256 = \"f1641918fd3f5e1667d39afb9c261da39ed9f74e30f1c2f98031d6d609a8de15\"\n+ \n+     tf_http_archive(\n+         name = name,"
        },
        {
            "sha": "03bd1efd1ba5778884c7431a97ca306fd27e82cf",
            "filename": "third_party/xla/third_party/shardy/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d6a407c9f5e0774e26b2f85b420381ff72d7ea67/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl?ref=d6a407c9f5e0774e26b2f85b420381ff72d7ea67",
            "patch": "@@ -3,8 +3,8 @@\n load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n-    SHARDY_COMMIT = \"e74939f4948986b2b5fe0e04cefb0afc2300672b\"\n-    SHARDY_SHA256 = \"04243cb1d585b5d43cf0d8bd8e611bc732090859a0ab1370bc93dcec0efe8e9e\"\n+    SHARDY_COMMIT = \"05276b9c4469f2331e326f614d712da7b907f7df\"\n+    SHARDY_SHA256 = \"f76bef82a597c4d72505dc1c5f8559cf77e720bdeacf976845578970e03265ea\"\n \n     tf_http_archive(\n         name = \"shardy\","
        }
    ],
    "stats": {
        "total": 630,
        "additions": 626,
        "deletions": 4
    }
}