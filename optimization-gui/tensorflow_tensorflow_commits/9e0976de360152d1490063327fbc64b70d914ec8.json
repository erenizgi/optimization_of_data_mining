{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 847189521",
    "sha": "9e0976de360152d1490063327fbc64b70d914ec8",
    "files": [
        {
            "sha": "1f4943889cc06b15c89b05123c79e3bf5467b200",
            "filename": "tensorflow/core/grappler/optimizers/auto_mixed_precision_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e0976de360152d1490063327fbc64b70d914ec8/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fauto_mixed_precision_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e0976de360152d1490063327fbc64b70d914ec8/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fauto_mixed_precision_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fauto_mixed_precision_test.cc?ref=9e0976de360152d1490063327fbc64b70d914ec8",
            "patch": "@@ -72,7 +72,7 @@ Tensor GenerateRandomTensorInRange(const TensorShape& shape, double minval,\n \n void VerifyGraphsEquivalent(const GraphDef& original_graph,\n                             const GraphDef& optimized_graph,\n-                            const string& func) {\n+                            const std::string& func) {\n   EXPECT_EQ(original_graph.node_size(), optimized_graph.node_size()) << func;\n   GraphView optimized_view(&optimized_graph);\n   for (int i = 0; i < original_graph.node_size(); ++i) {\n@@ -146,10 +146,10 @@ class AutoMixedPrecisionTest : public GrapplerTest {\n \n   void TearDown() override { TF_CHECK_OK(virtual_cluster_->Shutdown()); }\n \n-  NodeDef* AddSimpleNode(const string& name, const string& op,\n-                         const std::vector<string>& inputs,\n+  NodeDef* AddSimpleNode(const std::string& name, const std::string& op,\n+                         const std::vector<std::string>& inputs,\n                          GraphDef* graph) const {\n-    std::vector<std::pair<string, AttrValue>> attributes;\n+    std::vector<std::pair<std::string, AttrValue>> attributes;\n     if (op == \"AddN\" || op == \"ShapeN\") {\n       AttrValue num_inputs;\n       num_inputs.set_i(inputs.size());\n@@ -203,7 +203,8 @@ class AutoMixedPrecisionTest : public GrapplerTest {\n     TF_CHECK_OK(s.ToGraphDef(&item.graph));\n     auto input_tensor = GenerateRandomTensorInRange<DT_FLOAT>(\n         TensorShape({size, size}), input_min, input_max);\n-    std::vector<std::pair<string, Tensor>> feed = {{\"input\", input_tensor}};\n+    std::vector<std::pair<std::string, Tensor>> feed = {\n+        {\"input\", input_tensor}};\n     auto tensors_expected = EvaluateNodes(item.graph, item.fetch, feed);\n \n     AutoMixedPrecision optimizer(mode_);\n@@ -564,7 +565,7 @@ TEST_P(AutoMixedPrecisionParamTest, PreserveIdentityAfterVariable) {\n   TF_CHECK_OK(s.ToGraphDef(&item.graph));\n   auto var1_tensor =\n       GenerateConstantTensor<DT_FLOAT>(TensorShape({32, 32}), 3.141593f);\n-  std::vector<std::pair<string, Tensor>> feed = {{\"var1\", var1_tensor}};\n+  std::vector<std::pair<std::string, Tensor>> feed = {{\"var1\", var1_tensor}};\n   auto tensors_expected = EvaluateNodes(item.graph, item.fetch, feed);\n \n   AutoMixedPrecision optimizer(mode_);\n@@ -1035,7 +1036,7 @@ TEST_P(AutoMixedPrecisionParamTest, TensorListThroughFunction) {\n   // A separate Tensor List cluster is added to test that it is still changed to\n   // DT_HALF.\n   FunctionDefLibrary function_lib;\n-  const Tensor kShape = test::AsTensor<int32>({32, 32});\n+  const Tensor kShape = test::AsTensor<int32_t>({32, 32});\n   FunctionDef func1 = FunctionDefHelper::Define(\n       \"Func1\", {\"ihandle: variant\", \"x: float\"},\n       {\"ohandle: variant\", \"y: float\"}, {},\n@@ -1120,7 +1121,7 @@ int GetCudaVersion(const Cluster& cluster) {\n       const auto& device_env = device_properties.environment();\n       auto it = device_env.find(\"cuda\");\n       if (it != device_env.end()) {\n-        string cuda_version_str = it->second;\n+        std::string cuda_version_str = it->second;\n         return std::stoi(cuda_version_str);\n       }\n     }\n@@ -1407,7 +1408,7 @@ TEST_F(AutoMixedPrecisionCpuTest, MixedFanout) {\n class AutoMixedPrecisionSimulateGpuTest : public GrapplerTest {\n  protected:\n   void SetUp() override {\n-    std::unordered_map<string, DeviceProperties> devices;\n+    std::unordered_map<std::string, DeviceProperties> devices;\n     DeviceProperties cpu_device;\n     cpu_device.set_type(\"CPU\");\n     cpu_device.set_frequency(1000);"
        },
        {
            "sha": "8f3603829ffb46511b1428cc8b31a1914a93f13d",
            "filename": "tensorflow/core/grappler/optimizers/constant_folding_test.cc",
            "status": "modified",
            "additions": 93,
            "deletions": 94,
            "changes": 187,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e0976de360152d1490063327fbc64b70d914ec8/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e0976de360152d1490063327fbc64b70d914ec8/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding_test.cc?ref=9e0976de360152d1490063327fbc64b70d914ec8",
            "patch": "@@ -95,11 +95,11 @@ class ConstantFoldingTest : public GrapplerTest {\n       TF_EXPECT_OK(status);\n \n       EXPECT_EQ(7, output.node_size());\n-      const string snapshot_or_identity =\n+      const std::string snapshot_or_identity =\n           use_snapshot ? \"Snapshot\" : \"Identity\";\n       for (int i = 0; i < output.node_size(); ++i) {\n         const NodeDef& node = output.node(i);\n-        const string& name = node.name();\n+        const std::string& name = node.name();\n         if (name == \"mul1\") {\n           EXPECT_EQ(\"Const\", node.op());\n           EXPECT_EQ(\"^x\", node.input(0));\n@@ -220,7 +220,7 @@ class ConstantFoldingTest : public GrapplerTest {\n     EXPECT_EQ(2, found);\n \n     // Check that const folded multiplication node has the expected value.\n-    std::vector<string> fetch = {\"mul\"};\n+    std::vector<std::string> fetch = {\"mul\"};\n     Tensor value(DT_FLOAT, input_shape);\n     for (int i = 0; i < value.NumElements(); ++i) {\n       value.flat<float>()(i) = i;\n@@ -309,7 +309,7 @@ TEST_F(ConstantFoldingTest, SimpleFolding) {\n   EXPECT_EQ(\"d\", node_d.name());\n   EXPECT_EQ(\"Const\", node_d.op());\n \n-  std::vector<string> fetch = {\"d\"};\n+  std::vector<std::string> fetch = {\"d\"};\n   auto tensors_expected = EvaluateNodes(item.graph, fetch);\n   auto tensors = EvaluateNodes(output, fetch);\n   EXPECT_EQ(1, tensors_expected.size());\n@@ -397,7 +397,7 @@ TEST_F(ConstantFoldingTest, AddTree) {\n   auto x_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n   auto y_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n \n-  std::vector<string> fetch = {\"add_parent\", \"mul_parent\"};\n+  std::vector<std::string> fetch = {\"add_parent\", \"mul_parent\"};\n   auto tensor_expected =\n       EvaluateNodes(item.graph, fetch, {{\"x\", x_t}, {\"y\", y_t}});\n   ASSERT_EQ(fetch.size(), tensor_expected.size());\n@@ -453,7 +453,7 @@ TEST_F(ConstantFoldingTest, AddSubtactTree) {\n   // Check that the result nodes have the expected value.\n   auto x_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n \n-  std::vector<string> fetch = {\"add_parent\"};\n+  std::vector<std::string> fetch = {\"add_parent\"};\n   auto tensor_expected = EvaluateNodes(item.graph, fetch, {{\"x\", x_t}});\n   ASSERT_EQ(fetch.size(), tensor_expected.size());\n   fetch = {\"add_parent\"};\n@@ -478,7 +478,7 @@ TEST_F(ConstantFoldingTest, ConstantPushDown) {\n                                  ops::Placeholder::Shape(TensorShape({2, 2})));\n \n             auto get_op = [&](bool is_commutative, bool is_left_arg_const,\n-                              const string& name, const Output& const_arg,\n+                              const std::string& name, const Output& const_arg,\n                               const Output non_const_arg) -> Output {\n               if (is_add) {\n                 if (is_commutative) {\n@@ -523,7 +523,7 @@ TEST_F(ConstantFoldingTest, ConstantPushDown) {\n \n             // Check that the result nodes have the expected value.\n             auto x_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n-            std::vector<string> fetch = {\"parent\"};\n+            std::vector<std::string> fetch = {\"parent\"};\n             auto tensor_expected =\n                 EvaluateNodes(item.graph, fetch, {{\"x\", x_t}});\n             ASSERT_EQ(fetch.size(), tensor_expected.size());\n@@ -600,7 +600,7 @@ TEST_F(ConstantFoldingTest, ConstantPushDownBiasAdd) {\n   // Check that the result nodes have the expected value.\n   auto x_mat_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n   auto x_vec_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2}));\n-  std::vector<string> fetch = item.fetch;\n+  std::vector<std::string> fetch = item.fetch;\n   auto tensor_expected = EvaluateNodes(\n       item.graph, fetch, {{\"x_vec\", x_vec_t}, {\"x_mat\", x_mat_t}});\n   ASSERT_EQ(fetch.size(), tensor_expected.size());\n@@ -615,10 +615,9 @@ TEST_F(ConstantFoldingTest, ConstantPushDownBiasAdd) {\n // This test fails on ROCm platform (see commit message for details)\n #ifndef TENSORFLOW_USE_ROCM\n TEST_F(ConstantFoldingTest, MulConvPushDownTest_Conv2D_ScalarConst) {\n-  for (string data_format : {\n-         \"NHWC\",\n+  for (std::string data_format : {\"NHWC\",\n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n-             \"NCHW\"\n+                                  \"NCHW\"\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n        }) {\n     MulConvPushDownTest(\n@@ -636,10 +635,9 @@ TEST_F(ConstantFoldingTest, MulConvPushDownTest_Conv2D_ScalarConst) {\n // This test fails on ROCm platform (see commit message for details)\n #ifndef TENSORFLOW_USE_ROCM\n TEST_F(ConstantFoldingTest, MulConvPushDownTest_Conv2D_SingletonConst) {\n-  for (string data_format : {\n-         \"NHWC\",\n+  for (std::string data_format : {\"NHWC\",\n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n-             \"NCHW\"\n+                                  \"NCHW\"\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n        }) {\n     for (auto mul_const_input_shape :\n@@ -658,10 +656,9 @@ TEST_F(ConstantFoldingTest, MulConvPushDownTest_Conv2D_SingletonConst) {\n \n TEST_F(ConstantFoldingTest,\n        MulConvPushDownTest_Conv2D_SingletonConst_ShapeMismatch) {\n-  for (string data_format : {\n-         \"NHWC\",\n+  for (std::string data_format : {\"NHWC\",\n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n-             \"NCHW\"\n+                                  \"NCHW\"\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n        }) {\n     MulConvPushDownTest(\n@@ -841,18 +838,18 @@ TEST_F(ConstantFoldingTest, NeutralElement) {\n         optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n     TF_EXPECT_OK(status);\n \n-    const string suffix =\n+    const std::string suffix =\n         (const_type == kConst ? \"_const\"\n                               : (const_type == kLike ? \"_like\" : \"_fill\"));\n-    const string zeros_name = strings::StrCat(\"zeros\", suffix);\n-    const string ones_name = strings::StrCat(\"ones\", suffix);\n-    const string ctrl_zeros_name = strings::StrCat(\"^zeros\", suffix);\n-    const string ctrl_ones_name = strings::StrCat(\"^ones\", suffix);\n+    const std::string zeros_name = absl::StrCat(\"zeros\", suffix);\n+    const std::string ones_name = absl::StrCat(\"ones\", suffix);\n+    const std::string ctrl_zeros_name = absl::StrCat(\"^zeros\", suffix);\n+    const std::string ctrl_ones_name = absl::StrCat(\"^ones\", suffix);\n \n     EXPECT_EQ(const_type == kFill ? 43 : 39, output.node_size());\n     for (int i = 0; i < output.node_size(); ++i) {\n       const NodeDef& node = output.node(i);\n-      const string& name = node.name();\n+      const std::string& name = node.name();\n       if (name == \"mul1\") {\n         EXPECT_EQ(\"Const\", node.op());\n         EXPECT_EQ(\"^x\", node.input(0));\n@@ -968,8 +965,8 @@ TEST_F(ConstantFoldingTest, NeutralElement) {\n         EXPECT_EQ(\"y\", node.input(0));\n         EXPECT_EQ(ctrl_zeros_name, node.input(1));\n       }\n-      const std::set<string> square_zero_const{\"mul1\", \"mul2\",    \"mul5\",\n-                                               \"mul6\", \"matmul1\", \"matmul2\"};\n+      const std::set<std::string> square_zero_const{\n+          \"mul1\", \"mul2\", \"mul5\", \"mul6\", \"matmul1\", \"matmul2\"};\n       if (square_zero_const.count(name) > 0) {\n         TensorProto t = node.attr().at(\"value\").tensor();\n         EXPECT_EQ(1, t.float_val_size());\n@@ -1029,7 +1026,7 @@ TEST_F(ConstantFoldingTest, StrengthReduce_Reciprocal) {\n   EXPECT_EQ(8, output.node_size());\n   for (int i = 0; i < output.node_size(); ++i) {\n     const NodeDef& node = output.node(i);\n-    const string& name = node.name();\n+    const std::string& name = node.name();\n     if (name == \"div_i\") {\n       // Integer division is unchanged.\n       EXPECT_EQ(\"Div\", node.op());\n@@ -1061,7 +1058,7 @@ TEST_F(ConstantFoldingTest, StrengthReduce_Reciprocal) {\n   }\n \n   // Check that the reciprocals have the expected value.\n-  std::vector<string> fetch = {\"cf_half\"};\n+  std::vector<std::string> fetch = {\"cf_half\"};\n   auto tensor_expected = EvaluateNodes(item.graph, fetch);\n   EXPECT_EQ(fetch.size(), tensor_expected.size());\n   fetch = {\"ConstantFolding/div_f_recip\", \"ConstantFolding/realdiv_recip\"};\n@@ -1090,13 +1087,13 @@ TEST_F(ConstantFoldingTest, NeutralElement_PartialShape_UnknownOutputShape) {\n   // Multiplies without any additional ops to supply the output shape.\n   int count = 0;\n   std::vector<Output> muls;\n-  std::unordered_set<string> not_converted;\n-  std::unordered_set<string> to_const;\n-  std::unordered_set<string> to_identity;\n+  std::unordered_set<std::string> not_converted;\n+  std::unordered_set<std::string> to_const;\n+  std::unordered_set<std::string> to_identity;\n   for (const auto* x : {&x_known, &x_partially_known, &x_unknown}) {\n     for (const auto* zeros :\n          {&zeros_known, &zeros_partially_known, &zeros_unknown}) {\n-      const string name = strings::StrCat(\"mul_\", count++);\n+      const std::string name = absl::StrCat(\"mul_\", count++);\n       muls.push_back(ops::Mul(s.WithOpName(name), *x, *zeros));\n       if (x == &x_partially_known && zeros == &zeros_partially_known) {\n         to_identity.insert(name);\n@@ -1120,7 +1117,7 @@ TEST_F(ConstantFoldingTest, NeutralElement_PartialShape_UnknownOutputShape) {\n   EXPECT_EQ(15, output.node_size());\n   for (int i = 0; i < output.node_size(); ++i) {\n     const NodeDef& node = output.node(i);\n-    const string& name = node.name();\n+    const std::string& name = node.name();\n     if (to_const.count(name) > 0) {\n       EXPECT_EQ(\"Const\", node.op()) << node.name();\n     } else if (to_identity.count(name) > 0) {\n@@ -1130,7 +1127,7 @@ TEST_F(ConstantFoldingTest, NeutralElement_PartialShape_UnknownOutputShape) {\n     }\n   }\n \n-  const std::vector<string> fetch = {\"mul_0\", \"mul_4\", \"mul_8\"};\n+  const std::vector<std::string> fetch = {\"mul_0\", \"mul_4\", \"mul_8\"};\n   auto x_known_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n   auto x_partially_unknown_t =\n       GenerateRandomTensor<DT_FLOAT>(TensorShape({3, 4}));\n@@ -1166,11 +1163,11 @@ TEST_F(ConstantFoldingTest, NeutralElement_PartialShape_KnownOutputShape) {\n   // will propagate the shape back to the inputs of AddN, making the\n   // output shapes of all its inputs known\n   std::vector<Output> muls_deduced_output_shape;\n-  std::unordered_set<string> to_const;\n+  std::unordered_set<std::string> to_const;\n   int count = 0;\n   for (const auto& x : {x_partially_known, x_unknown}) {\n     for (const auto& zeros : {zeros_partially_known, zeros_unknown}) {\n-      const string name = strings::StrCat(\"mul_\", count++);\n+      const std::string name = absl::StrCat(\"mul_\", count++);\n       muls_deduced_output_shape.push_back(\n           ops::Mul(s.WithOpName(name), x, zeros));\n       to_const.insert(name);\n@@ -1193,15 +1190,15 @@ TEST_F(ConstantFoldingTest, NeutralElement_PartialShape_KnownOutputShape) {\n   EXPECT_EQ(10, output.node_size());\n   for (int i = 0; i < output.node_size(); ++i) {\n     const NodeDef& node = output.node(i);\n-    const string& name = node.name();\n+    const std::string& name = node.name();\n     if (to_const.count(name) > 0) {\n       EXPECT_EQ(\"Const\", node.op()) << node.name();\n       EXPECT_EQ(2, node.input_size());\n       EXPECT_TRUE(IsControlInput(node.input(0)));\n       EXPECT_TRUE(IsControlInput(node.input(1)));\n     }\n   }\n-  const std::vector<string> fetch = {\"addn1\"};\n+  const std::vector<std::string> fetch = {\"addn1\"};\n   auto x_partially_unknown_t =\n       GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n   auto x_unknown_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n@@ -1230,10 +1227,10 @@ TEST_F(ConstantFoldingTest, CreateConstNodes) {\n   MAKE_TEST_GRAPH(float);\n   MAKE_TEST_GRAPH(double);\n   MAKE_TEST_GRAPH(int64_t);\n-  MAKE_TEST_GRAPH(int32);\n-  MAKE_TEST_GRAPH(int16);\n-  MAKE_TEST_GRAPH(int8);\n-  MAKE_TEST_GRAPH(uint8);\n+  MAKE_TEST_GRAPH(int32_t);\n+  MAKE_TEST_GRAPH(int16_t);\n+  MAKE_TEST_GRAPH(int8_t);\n+  MAKE_TEST_GRAPH(uint8_t);\n #undef MAKE_TEST_GRAPH\n \n   Output bool_const = ops::Const(s.WithOpName(\"bool_const\"), true, {5});\n@@ -1307,7 +1304,7 @@ TEST_F(ConstantFoldingTest, FoldingNodeWithTwoOutputs) {\n   EXPECT_EQ(\"f\", new_d.name());\n   EXPECT_EQ(\"Const\", new_d.op());\n \n-  std::vector<string> fetch = {\"e\", \"f\"};\n+  std::vector<std::string> fetch = {\"e\", \"f\"};\n   auto tensors_expected = EvaluateNodes(item.graph, fetch);\n   auto tensors = EvaluateNodes(output, fetch);\n   EXPECT_EQ(fetch.size(), tensors_expected.size());\n@@ -1338,7 +1335,7 @@ TEST_F(ConstantFoldingTest, ControlDependencies) {\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n \n-  std::vector<string> expected_nodes = {\"dflt\", \"p1\", \"p2\", \"i3\"};\n+  std::vector<std::string> expected_nodes = {\"dflt\", \"p1\", \"p2\", \"i3\"};\n   EXPECT_EQ(output.node_size(), expected_nodes.size());\n   int i = 0;\n   int found = 0;\n@@ -1381,8 +1378,8 @@ TEST_F(ConstantFoldingTest, ControlDependenciesEmptyFetch) {\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n \n-  std::vector<string> expected_nodes = {\"dflt\", \"p1\", \"p2\", \"c\",\n-                                        \"i1\",   \"i2\", \"e\"};\n+  std::vector<std::string> expected_nodes = {\"dflt\", \"p1\", \"p2\", \"c\",\n+                                             \"i1\",   \"i2\", \"e\"};\n   EXPECT_EQ(output.node_size(), expected_nodes.size());\n   int i = 0;\n   int found = 0;\n@@ -1439,7 +1436,7 @@ TEST_F(ConstantFoldingTest, ControlDependenciesDeduplicate) {\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n \n-  std::vector<string> expected_nodes = {\"dflt\", \"p1\", \"p2\", \"i2\"};\n+  std::vector<std::string> expected_nodes = {\"dflt\", \"p1\", \"p2\", \"i2\"};\n   EXPECT_EQ(output.node_size(), expected_nodes.size());\n   int i = 0;\n   for (const auto& node : output.node()) {\n@@ -1466,9 +1463,9 @@ TEST_F(ConstantFoldingTest, VariableNumberOfOutputs) {\n   ops::DynamicPartition part(scope.WithOpName(\"partition\"), input, indices,\n                              num_partitions);\n \n-  std::vector<string> outputs;\n+  std::vector<std::string> outputs;\n   for (int i = 0; i < num_partitions; ++i) {\n-    string part_out_name = strings::StrCat(\"part_out\", i);\n+    std::string part_out_name = absl::StrCat(\"part_out\", i);\n     ops::Identity partition_out(scope.WithOpName(part_out_name),\n                                 {part.outputs[i]});\n     outputs.push_back(part_out_name);\n@@ -1481,7 +1478,7 @@ TEST_F(ConstantFoldingTest, VariableNumberOfOutputs) {\n   Tensor initial_val(DT_INT32, TensorShape({3}));\n   test::FillIota<int>(&initial_val, 7);\n   for (int i = 1; i < 5; ++i) {\n-    TF_CHECK_OK(NodeDefBuilder(strings::StrCat(\"in\", i), \"Const\")\n+    TF_CHECK_OK(NodeDefBuilder(absl::StrCat(\"in\", i), \"Const\")\n                     .Attr(\"dtype\", DT_INT32)\n                     .Attr(\"value\", initial_val)\n                     .Finalize(item.graph.add_node()));\n@@ -1502,7 +1499,7 @@ TEST_F(ConstantFoldingTest, VariableNumberOfOutputs) {\n                   .Finalize(item.graph.add_node()));\n \n   for (int i = 0; i < 4; ++i) {\n-    string concat_offset_out_name = strings::StrCat(\"concat_offset_out\", i);\n+    std::string concat_offset_out_name = absl::StrCat(\"concat_offset_out\", i);\n     TF_CHECK_OK(NodeDefBuilder(concat_offset_out_name, \"Identity\")\n                     .Attr(\"T\", DT_INT32)\n                     .Input(\"concat_offsets\", i, DT_INT32)\n@@ -1518,8 +1515,8 @@ TEST_F(ConstantFoldingTest, VariableNumberOfOutputs) {\n \n   int constant_folded = 0;\n   for (const auto& node : output.node()) {\n-    if (node.name().find(\"part_out\") != string::npos ||\n-        node.name().find(\"concat_offset_out\") != string::npos) {\n+    if (node.name().find(\"part_out\") != std::string::npos ||\n+        node.name().find(\"concat_offset_out\") != std::string::npos) {\n       ++constant_folded;\n       EXPECT_EQ(\"Const\", node.op());\n     }\n@@ -1638,7 +1635,7 @@ TEST_F(ConstantFoldingTest, ShapeMaterializationEmptyFetch) {\n   auto v1_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({3}));\n   auto v2_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({5, 7}));\n   auto v3_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({11, 13}));\n-  std::vector<string> fetch_nodes = {\"p2\"};\n+  std::vector<std::string> fetch_nodes = {\"p2\"};\n   auto tensors_expected = EvaluateNodes(\n       item.graph, fetch_nodes, {{\"v1\", v1_t}, {\"v2\", v2_t}, {\"v3\", v3_t}});\n   EXPECT_EQ(1, tensors_expected.size());\n@@ -1711,8 +1708,8 @@ TEST_F(ConstantFoldingTest, ShapeMaterializationShapeN) {\n   auto v1_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({3, 4}));\n   auto v2_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({5, 6}));\n   auto v3_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({4, 6}));\n-  const std::vector<string> fetch_nodes = {\"i1a\", \"i1b\", \"i2a\", \"i2b\",\n-                                           \"i2c\", \"i3a\", \"i3b\"};\n+  const std::vector<std::string> fetch_nodes = {\"i1a\", \"i1b\", \"i2a\", \"i2b\",\n+                                                \"i2c\", \"i3a\", \"i3b\"};\n   auto tensors_expected = EvaluateNodes(\n       item.graph, fetch_nodes, {{\"v1\", v1_t}, {\"v2\", v2_t}, {\"v3\", v3_t}});\n   EXPECT_EQ(fetch_nodes.size(), tensors_expected.size());\n@@ -1814,15 +1811,16 @@ TEST_F(ConstantFoldingTest, SwitchNodesEmptyFetch) {\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n \n-  std::set<string> present_nodes = {\"v_in\",     \"v_ctrl\",\n-                                    \"switch\",   \"i\",\n-                                    \"p1\",       \"p2\",\n-                                    \"m\",        \"false\",\n-                                    \"constant\", \"switch2\",\n-                                    \"i2\",       \"i3\",\n-                                    \"m2\",       \"ConstantFoldingCtrl/switch_0\",\n-                                    \"rank\",     \"size\"};\n-  std::set<string> not_present_nodes = {\"ConstantFolding/switch2-0\"};\n+  std::set<std::string> present_nodes = {\n+      \"v_in\",     \"v_ctrl\",\n+      \"switch\",   \"i\",\n+      \"p1\",       \"p2\",\n+      \"m\",        \"false\",\n+      \"constant\", \"switch2\",\n+      \"i2\",       \"i3\",\n+      \"m2\",       \"ConstantFoldingCtrl/switch_0\",\n+      \"rank\",     \"size\"};\n+  std::set<std::string> not_present_nodes = {\"ConstantFolding/switch2-0\"};\n   EXPECT_EQ(present_nodes.size(), output.node_size());\n   int found = 0;\n   for (const auto& node : output.node()) {\n@@ -1862,7 +1860,7 @@ TEST_F(ConstantFoldingTest, SwitchNodesEmptyFetch) {\n   Tensor v_ctrl_t(DT_BOOL, TensorShape({}));\n \n   v_ctrl_t.flat<bool>()(0) = true;\n-  std::vector<string> fetch_nodes = {\"m\", \"m2\"};\n+  std::vector<std::string> fetch_nodes = {\"m\", \"m2\"};\n   auto tensors_expected = EvaluateNodes(\n       item.graph, fetch_nodes, {{\"v_in\", v_in_t}, {\"v_ctrl\", v_ctrl_t}});\n   EXPECT_EQ(2, tensors_expected.size());\n@@ -1915,15 +1913,16 @@ TEST_F(ConstantFoldingTest, SwitchNodes) {\n   GraphDef output;\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n-  std::set<string> present_nodes = {\"v_in\",     \"v_ctrl\",\n-                                    \"switch\",   \"i\",\n-                                    \"p1\",       \"p2\",\n-                                    \"m\",        \"false\",\n-                                    \"constant\", \"switch2\",\n-                                    \"i2\",       \"i3\",\n-                                    \"m2\",       \"ConstantFoldingCtrl/switch_0\"};\n-  std::set<string> not_present_nodes = {\"rank\", \"size\",\n-                                        \"ConstantFolding/switch2-0\"};\n+  std::set<std::string> present_nodes = {\n+      \"v_in\",     \"v_ctrl\",\n+      \"switch\",   \"i\",\n+      \"p1\",       \"p2\",\n+      \"m\",        \"false\",\n+      \"constant\", \"switch2\",\n+      \"i2\",       \"i3\",\n+      \"m2\",       \"ConstantFoldingCtrl/switch_0\"};\n+  std::set<std::string> not_present_nodes = {\"rank\", \"size\",\n+                                             \"ConstantFolding/switch2-0\"};\n   EXPECT_EQ(present_nodes.size(), output.node_size());\n \n   int found = 0;\n@@ -2584,7 +2583,7 @@ TEST_F(ConstantFoldingTest, MergeConcat_PartialFolding) {\n }\n \n TEST_F(ConstantFoldingTest, PaddingWithZeroSize) {\n-  PaddingWithZeroSize<int32>();\n+  PaddingWithZeroSize<int32_t>();\n   PaddingWithZeroSize<int64_t>();\n }\n \n@@ -2770,7 +2769,7 @@ TEST_F(ConstantFoldingTest, SingleElementEmptyAxisReduction) {\n       GenerateRandomTensor<DT_FLOAT>(TensorShape({1, 1, 1}));\n   auto input_var_one_dim_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({1}));\n   Tensor input_var_axis_t(DT_INT32, TensorShape({1}));\n-  input_var_axis_t.flat<int32>()(0) = 0;\n+  input_var_axis_t.flat<int32_t>()(0) = 0;\n   auto tensors_expected =\n       EvaluateNodes(item.graph, item.fetch,\n                     {{\"input_var_three_dim\", input_var_three_dim_t},\n@@ -2895,7 +2894,7 @@ TEST_F(ConstantFoldingTest, Packing) {\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n \n-  const std::vector<string> fetch_nodes = {\"i1\", \"i2\"};\n+  const std::vector<std::string> fetch_nodes = {\"i1\", \"i2\"};\n   auto tensors_expected = EvaluateNodes(item.graph, fetch_nodes);\n   EXPECT_EQ(fetch_nodes.size(), tensors_expected.size());\n   auto tensors = EvaluateNodes(output, fetch_nodes);\n@@ -2971,7 +2970,7 @@ TEST_F(ConstantFoldingTest, MaterializeBroadcastGradientArgs) {\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n \n-  std::vector<string> fetch_nodes = {\"o1\", \"o2\", \"p1\", \"p2\"};\n+  std::vector<std::string> fetch_nodes = {\"o1\", \"o2\", \"p1\", \"p2\"};\n   auto a_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({1, 5}));\n   auto g_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({1}));\n   auto tensors_expected =\n@@ -3042,7 +3041,7 @@ TEST_F(ConstantFoldingTest, MaterializeBroadcastGradientArgs_InfiniteLoop) {\n   GrapplerItem item;\n   TF_CHECK_OK(s.ToGraphDef(&item.graph));\n \n-  std::vector<string> fetch_nodes = {\"o1\", \"o2\"};\n+  std::vector<std::string> fetch_nodes = {\"o1\", \"o2\"};\n   auto a_t = GenerateRandomTensor<DT_FLOAT>(TensorShape({2, 2}));\n   auto tensors_expected = EvaluateNodes(item.graph, fetch_nodes, {{\"a\", a_t}});\n   EXPECT_EQ(fetch_nodes.size(), tensors_expected.size());\n@@ -3331,7 +3330,7 @@ TEST_F(ConstantFoldingTest, PartialFolding_AssociativeAndCommutative) {\n       };\n   for (bool use_add_n : {true, false}) {\n     auto fun = use_add_n ? addn_fun : accumulate_fun;\n-    const string op_name = use_add_n ? \"AddN\" : \"AccumulateNV2\";\n+    const std::string op_name = use_add_n ? \"AddN\" : \"AccumulateNV2\";\n     Scope s = Scope::NewRootScope();\n     Output x = ops::Placeholder(s.WithOpName(\"x\"), DT_FLOAT,\n                                 ops::Placeholder::Shape(TensorShape({2, 2})));\n@@ -3411,7 +3410,7 @@ TEST_F(ConstantFoldingTest, PartialFolding_AssociativeAndCommutative) {\n       }\n     }\n \n-    std::vector<string> fetch = {\"acc0\"};\n+    std::vector<std::string> fetch = {\"acc0\"};\n     auto tensors_expected = EvaluateNodes(item.graph, fetch);\n     auto tensors = EvaluateNodes(output, fetch);\n     EXPECT_EQ(1, tensors_expected.size());\n@@ -3613,7 +3612,7 @@ TEST_F(ConstantFoldingTest, TrivialPack) {\n   }\n   EXPECT_EQ(found, 3);\n \n-  std::vector<string> fetch = {\"stack\", \"stack_no_axis\"};\n+  std::vector<std::string> fetch = {\"stack\", \"stack_no_axis\"};\n   auto tensors_expected = EvaluateNodes(item.graph, fetch);\n   auto tensors = EvaluateNodes(output, fetch);\n   EXPECT_EQ(2, tensors_expected.size());\n@@ -3741,8 +3740,8 @@ TEST_F(ConstantFoldingTest, TensorArraySize) {\n   auto tensors_actual = EvaluateNodes(output, {\"dynamic_sz\", \"static_sz\"});\n   EXPECT_EQ(2, tensors_expected.size());\n   EXPECT_EQ(2, tensors_actual.size());\n-  test::ExpectTensorEqual<int32>(tensors_expected[0], tensors_actual[0]);\n-  test::ExpectTensorEqual<int32>(tensors_expected[1], tensors_actual[1]);\n+  test::ExpectTensorEqual<int32_t>(tensors_expected[0], tensors_actual[0]);\n+  test::ExpectTensorEqual<int32_t>(tensors_expected[1], tensors_actual[1]);\n }\n \n TEST_F(ConstantFoldingTest, FoldingPreservesDenormalFlushing) {\n@@ -3770,7 +3769,7 @@ TEST_F(ConstantFoldingTest, FoldingPreservesDenormalFlushing) {\n   EXPECT_EQ(\"c\", node_d.name());\n   EXPECT_EQ(\"Const\", node_d.op());\n \n-  std::vector<string> fetch = {\"c\"};\n+  std::vector<std::string> fetch = {\"c\"};\n   auto tensors_expected = EvaluateNodes(item.graph, fetch);\n   auto tensors = EvaluateNodes(output, fetch);\n   EXPECT_EQ(1, tensors_expected.size());\n@@ -3800,7 +3799,7 @@ TEST_F(ConstantFoldingTest, EvaluatingLargeConstantNoFoldingMergingLoop) {\n   absl::Status status = optimizer.Optimize(/*cluster=*/nullptr, item, &output);\n   TF_EXPECT_OK(status);\n \n-  std::vector<string> fetch = {\"result\"};\n+  std::vector<std::string> fetch = {\"result\"};\n   auto tensors_expected = EvaluateNodes(item.graph, fetch);\n   auto tensors = EvaluateNodes(output, fetch);\n   EXPECT_EQ(1, tensors_expected.size());\n@@ -3869,9 +3868,9 @@ class ConstantFoldingCastConstTest : public GrapplerTest {\n     return output;\n   }\n \n-  void EvaluateAndCompareUnoptimized(const GraphDef& unoptimized_graph,\n-                                     const GraphDef& optimized_graph,\n-                                     const std::vector<string>& fetch_nodes) {\n+  void EvaluateAndCompareUnoptimized(\n+      const GraphDef& unoptimized_graph, const GraphDef& optimized_graph,\n+      const std::vector<std::string>& fetch_nodes) {\n     auto tensors_expected = EvaluateNodes(unoptimized_graph, fetch_nodes);\n     auto tensors = EvaluateNodes(optimized_graph, fetch_nodes);\n     ASSERT_EQ(fetch_nodes.size(), tensors_expected.size());\n@@ -4093,8 +4092,8 @@ TEST_F(ConstantFoldingTest, SimplifyCase) {\n     TensorShapeProto* g_shape = output_shapes.mutable_list()->add_shape();\n     g_shape->set_unknown_rank(true);\n \n-    const Tensor kZero = test::AsScalar<int32>(0);\n-    const Tensor kOne = test::AsScalar<int32>(1);\n+    const Tensor kZero = test::AsScalar<int32_t>(0);\n+    const Tensor kOne = test::AsScalar<int32_t>(1);\n     item.graph = test::function::GDef(\n         {NDef(\"one\", \"Const\", {},\n               {{\"value\", index == 0 ? kZero : kOne}, {\"dtype\", DT_INT32}},\n@@ -4265,8 +4264,8 @@ TEST_F(ConstantFoldingTest, SimplifySelect_BroadcastTo) {\n           ASSERT_EQ(node.input_size(), 4);\n           EXPECT_EQ(node.input(0), pred_val ? \"then\" : \"else\");\n           EXPECT_EQ(node.input(1),\n-                    strings::StrCat(\"ConstantFolding/select-broadcastto_shape-\",\n-                                    pred_val ? 1 : 2));\n+                    absl::StrCat(\"ConstantFolding/select-broadcastto_shape-\",\n+                                 pred_val ? 1 : 2));\n           EXPECT_EQ(node.input(2), pred_val ? \"^else\" : \"^if\");\n           EXPECT_EQ(node.input(3), pred_val ? \"^if\" : \"^then\");\n         }"
        },
        {
            "sha": "aef15c4fdf1b2e627eaceadb47c1669593d2cdbf",
            "filename": "tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 53,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e0976de360152d1490063327fbc64b70d914ec8/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fgeneric_layout_optimizer_transposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e0976de360152d1490063327fbc64b70d914ec8/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fgeneric_layout_optimizer_transposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fgeneric_layout_optimizer_transposer.cc?ref=9e0976de360152d1490063327fbc64b70d914ec8",
            "patch": "@@ -252,7 +252,7 @@ absl::Status TransposeContext::InitializeTransposeContext(\n   TF_RETURN_IF_ERROR(status);\n   context->num_nodes = context->graph.node_size();\n   const auto& nodes_to_preserve = item.NodesToPreserve();\n-  context->nodes_to_preserve = absl::flat_hash_set<string>(\n+  context->nodes_to_preserve = absl::flat_hash_set<std::string>(\n       nodes_to_preserve.begin(), nodes_to_preserve.end());\n   TF_RETURN_IF_ERROR(context->frames.InferFromGraph(context->graph));\n   return absl::OkStatus();\n@@ -262,9 +262,9 @@ absl::Status TransposeContext::InitializeTransposeContext(\n void TransposeContext::AssignDeviceAndDataFormats(\n     absl::string_view target_device, absl::string_view src_format,\n     absl::string_view dst_format) {\n-  this->target_device = string(target_device);\n-  this->src_format = string(src_format);\n-  this->dst_format = string(dst_format);\n+  this->target_device = std::string(target_device);\n+  this->src_format = std::string(src_format);\n+  this->dst_format = std::string(dst_format);\n   this->src_dim_indices = GetDimensionIndices(src_format);\n   this->dst_dim_indices = GetDimensionIndices(dst_format);\n   this->src_to_dst = GetPermutation(this->src_dim_indices, dst_format);\n@@ -276,9 +276,9 @@ void TransposeContext::AssignDeviceAndDataFormats(\n bool Transposer::ShouldProcess(const TransposeContext& context,\n                                const utils::MutableNodeView& node) const {\n   const auto* node_def = node.node();\n-  const string& device_name = GetDeviceName(*node_def);\n-  string device;\n-  string task;\n+  const std::string& device_name = GetDeviceName(*node_def);\n+  std::string device;\n+  std::string task;\n   const bool is_on_target_device =\n       DeviceNameUtils::SplitDeviceName(device_name, &task, &device) &&\n       absl::StrContains(absl::AsciiStrToLower(device),\n@@ -306,12 +306,12 @@ absl::Status Transposer::CreateConstPermNode(\n   DCHECK(!graph_view->HasNode(node_name));\n \n   NodeDef node;\n-  node.set_name(string(node_name));\n+  node.set_name(node_name);\n   node.set_op(kOpConst);\n-  node.set_device(string(device));\n+  node.set_device(device);\n \n   if (!control_node_name.empty()) {\n-    node.add_input(string(control_node_name));\n+    node.add_input(std::string(control_node_name));\n   }\n \n   AttrValue attr_data_type;\n@@ -337,16 +337,16 @@ absl::Status Transposer::CreateTransposeNode(\n     const DataType& data_type, absl::string_view device,\n     TensorShapeProto fanin_shape, absl::Span<const int> permutation,\n     absl::string_view control_node_name, utils::MutationNewNode* added_node,\n-    string* transpose_node_name) {\n-  const string node_name = absl::Substitute(name_format, kOpTranspose);\n+    std::string* transpose_node_name) {\n+  const std::string node_name = absl::Substitute(name_format, kOpTranspose);\n   auto* graph_view = context->graph_view.get();\n   DCHECK(!graph_view->HasNode(node_name));\n   *transpose_node_name = node_name;\n \n   NodeDef node;\n   node.set_name(node_name);\n   node.set_op(kOpTranspose);\n-  node.set_device(string(device));\n+  node.set_device(device);\n \n   AttrValue attr_data_type;\n   attr_data_type.set_type(data_type);\n@@ -367,7 +367,7 @@ absl::Status Transposer::CreateTransposeNode(\n \n   // Create Const Node\n   utils::MutationNewNode const_perm_added_node;\n-  const string const_perm_node_name =\n+  const std::string const_perm_node_name =\n       absl::Substitute(name_format, \"PermConst\");\n   TF_RETURN_IF_ERROR(CreateConstPermNode(context, const_perm_node_name, device,\n                                          permutation, control_node_name,\n@@ -457,11 +457,11 @@ absl::Status Transposer::CreateDataFormatNode(\n \n   // Create the node\n   NodeDef node;\n-  node.set_name(string(node_name));\n+  node.set_name(node_name);\n \n   // Set up parameters of node.\n-  node.set_op(string(op));\n-  node.set_device(string(device));\n+  node.set_op(op);\n+  node.set_device(device);\n   AttrValue attr_data_type;\n   attr_data_type.set_type(data_type);\n   node.mutable_attr()->insert({\"T\", attr_data_type});\n@@ -503,7 +503,7 @@ absl::Status Transposer::UpdateEdge(\n   auto* dst_node_def = dst_node->node();\n \n   // TODO(lyandy): Minimize device parsing/fetching.\n-  const string device = GetDeviceName(\n+  const std::string device = GetDeviceName(\n       is_src_format_to_dst_format ? *dst_node_def : *src_node_def);\n   DataType data_type =\n       is_src_format_to_dst_format\n@@ -515,7 +515,7 @@ absl::Status Transposer::UpdateEdge(\n                 .dtype();\n \n   utils::MutationNewNode added_node;\n-  string added_node_name;\n+  std::string added_node_name;\n   if (op == kOpTranspose) {\n     TensorShapeProto input_shape_proto;\n     input_shape_proto.set_unknown_rank(true);\n@@ -527,7 +527,7 @@ absl::Status Transposer::UpdateEdge(\n         input_shape_proto = src_node_shape_attr->list().shape(src_port);\n       }\n     }\n-    const string control_node_name =\n+    const std::string control_node_name =\n         is_in_frame ? AsControlDependency(src_node_def->name()) : \"\";\n     const std::vector<int>& permutation =\n         is_src_format_to_dst_format ? context->src_to_dst : context->dst_to_src;\n@@ -540,7 +540,7 @@ absl::Status Transposer::UpdateEdge(\n                                 GetDeviceName(*src_node_def), &parsed_name) &&\n                             parsed_name.type != \"CPU\" &&\n                             IsHostMemory(*src_node_def, src_port);\n-    const string node_name = absl::Substitute(name_format, op);\n+    const std::string node_name = absl::Substitute(name_format, op);\n     TF_RETURN_IF_ERROR(CreateDataFormatNode(\n         context, node_name, op, device, data_type, is_fanin_on_host,\n         is_src_format_to_dst_format, &added_node));\n@@ -655,40 +655,42 @@ bool Transposer::CanProcessNode(const TransposeContext& context,\n          !(node.NumRegularFanouts() == 0 && node.NumControlledFanouts() == 0);\n }\n \n-string Transposer::GetFaninNameFormat(absl::string_view node_name, int port,\n-                                      absl::string_view src_format,\n-                                      absl::string_view dst_format) {\n+std::string Transposer::GetFaninNameFormat(absl::string_view node_name,\n+                                           int port,\n+                                           absl::string_view src_format,\n+                                           absl::string_view dst_format) {\n   return absl::StrCat(node_name, \"-\", port, \"-$0\", src_format, \"To\", dst_format,\n                       \"-\", kOptimizedSuffix);\n }\n \n-string Transposer::GetFanoutNameFormat(absl::string_view node_name, int port,\n-                                       int index, absl::string_view src_format,\n-                                       absl::string_view dst_format) {\n+std::string Transposer::GetFanoutNameFormat(absl::string_view node_name,\n+                                            int port, int index,\n+                                            absl::string_view src_format,\n+                                            absl::string_view dst_format) {\n   return absl::StrCat(node_name, \"-\", port, \"-\", index, \"-$0\", dst_format, \"To\",\n                       src_format, \"-\", kOptimizedSuffix);\n }\n \n-string Transposer::LayoutOptimizerNode(absl::string_view node_name) {\n+std::string Transposer::LayoutOptimizerNode(absl::string_view node_name) {\n   return absl::StrCat(node_name, \"-\", kOptimizedSuffix);\n }\n \n-string Transposer::GetReshapeNodeNameFormat(absl::string_view node_name,\n-                                            int index,\n-                                            absl::string_view src_format,\n-                                            absl::string_view dst_format) {\n+std::string Transposer::GetReshapeNodeNameFormat(absl::string_view node_name,\n+                                                 int index,\n+                                                 absl::string_view src_format,\n+                                                 absl::string_view dst_format) {\n   return absl::StrCat(node_name, \"-\", index, \"-\", kReshape, src_format, \"To\",\n                       dst_format);\n }\n \n-string Transposer::GetShapeConstNodeNameFormat(absl::string_view node_name,\n-                                               int index) {\n+std::string Transposer::GetShapeConstNodeNameFormat(absl::string_view node_name,\n+                                                    int index) {\n   return absl::StrCat(node_name, \"-\", index, \"-\", kReshapeConst);\n }\n \n // Layout sensitive transposer.\n \n-inline string GetLayoutSensitiveNodeDataFormat(\n+inline std::string GetLayoutSensitiveNodeDataFormat(\n     const utils::MutableNodeView& node) {\n   const auto* attr = node.GetAttr(kAttrDataFormat);\n   if (attr != nullptr) {\n@@ -1086,7 +1088,7 @@ inline bool IsValidConstPermTransposeNode(const utils::MutableNodeView& node,\n     return false;\n   }\n \n-  const auto& tensor_data = tensor.unaligned_flat<int32>();\n+  const auto& tensor_data = tensor.unaligned_flat<int32_t>();\n   for (int i = 0; i < permutation_size; i++) {\n     if (permutation[i] != tensor_data(i)) {\n       return false;\n@@ -1252,11 +1254,11 @@ absl::Status BinaryOpTransposer::AddNodeReshape(\n     absl::string_view node_device, absl::string_view input_name,\n     absl::string_view shape_const_node_name, const DataType& data_type) {\n   NodeDef new_node;\n-  new_node.set_name(string(node_name));\n-  new_node.add_input(string(input_name));\n-  new_node.add_input(string(shape_const_node_name));\n+  new_node.set_name(node_name);\n+  new_node.add_input(std::string(input_name));\n+  new_node.add_input(std::string(shape_const_node_name));\n   new_node.set_op(kReshape);\n-  new_node.set_device(string(node_device));\n+  new_node.set_device(node_device);\n \n   AttrValue attr_type_indices;\n   attr_type_indices.set_type(DT_INT32);\n@@ -1276,9 +1278,9 @@ absl::Status BinaryOpTransposer::AddNodeShapeConst(\n     absl::string_view node_device, bool node_in_frame, int num_channels,\n     absl::string_view depended_node, int rank) {\n   NodeDef new_node;\n-  new_node.set_name(string(node_name));\n+  new_node.set_name(node_name);\n   new_node.set_op(kOpConst);\n-  new_node.set_device(string(node_device));\n+  new_node.set_device(node_device);\n   AttrValue attr_data_type;\n   attr_data_type.set_type(DT_INT32);\n   new_node.mutable_attr()->insert({\"dtype\", attr_data_type});\n@@ -1296,7 +1298,7 @@ absl::Status BinaryOpTransposer::AddNodeShapeConst(\n     // This is to ensure the transpose node and the const node are in the same\n     // frame.\n     // TODO(halehri): Add Test that exercises this condition.\n-    new_node.add_input(AsControlDependency(string(depended_node)));\n+    new_node.add_input(AsControlDependency(std::string(depended_node)));\n   }\n \n   absl::Status status;\n@@ -1313,11 +1315,12 @@ absl::Status BinaryOpTransposer::MaybeReshapeVectorFanin(\n     vector_index = 0;\n   }\n   if (vector_index != -1) {\n-    const string& node_name = node->GetName();\n-    const string& node_device = node->GetDevice();\n-    string reshape_node_name = LayoutOptimizerNode(GetReshapeNodeNameFormat(\n-        node_name, vector_index, context->src_format, context->dst_format));\n-    string shape_const_node_name = LayoutOptimizerNode(\n+    const std::string& node_name = node->GetName();\n+    const std::string& node_device = node->GetDevice();\n+    std::string reshape_node_name =\n+        LayoutOptimizerNode(GetReshapeNodeNameFormat(\n+            node_name, vector_index, context->src_format, context->dst_format));\n+    std::string shape_const_node_name = LayoutOptimizerNode(\n         GetShapeConstNodeNameFormat(node_name, vector_index));\n     const auto& fanin = node->GetRegularFanin(vector_index);\n     auto* fanin_node = fanin.node_view();\n@@ -1513,7 +1516,7 @@ bool ReduceTransposer::IsAlongAxis(const Tensor& tensor,\n   for (int i = 0; i < axis_size; ++i) {\n     int local_axis = 0;\n     if (tensor.dtype() == DT_INT32) {\n-      local_axis = tensor.flat<int32>()(i);\n+      local_axis = tensor.flat<int32_t>()(i);\n     } else {\n       local_axis = tensor.flat<int64_t>()(i);\n     }\n@@ -2023,10 +2026,10 @@ absl::Status UnaryGradTransposer::TransposeNode(TransposeContext* context,\n \n // Utils.\n \n-string GetDeviceName(const NodeDef& node) { return node.device(); }\n+std::string GetDeviceName(const NodeDef& node) { return node.device(); }\n \n bool IsDefaultLayoutSensitiveOp(const NodeDef& node) {\n-  static absl::flat_hash_set<string>* default_layout_sensitive_ops =\n+  static absl::flat_hash_set<std::string>* default_layout_sensitive_ops =\n       new absl::flat_hash_set<std::string>(\n           {\"AvgPool\", \"Conv2D\", \"DepthwiseConv2dNative\", \"DepthToSpace\",\n            \"FusedBatchNorm\", \"FusedBatchNormV2\", \"FusedBatchNormV3\",\n@@ -2049,7 +2052,7 @@ bool IsLayoutSensitiveOp(const NodeDef& node) {\n }\n \n bool IsDefaultLayoutAgnosticOp(const NodeDef& node) {\n-  static absl::flat_hash_set<string>* agnostic_nodes =\n+  static absl::flat_hash_set<std::string>* agnostic_nodes =\n       new absl::flat_hash_set<std::string>({\"Abs\",\n                                             \"Acos\",\n                                             \"Acosh\",\n@@ -2253,7 +2256,7 @@ bool GetValueAttrFromConstInputNode(\n }\n \n bool IsDataFormatOp(const utils::MutableNodeView& node) {\n-  const string& op = node.GetOp();\n+  const std::string& op = node.GetOp();\n   return op == kOpDataFormatDimMap || op == kOpDataFormatVecPermute;\n }\n "
        }
    ],
    "stats": {
        "total": 315,
        "additions": 159,
        "deletions": 156
    }
}