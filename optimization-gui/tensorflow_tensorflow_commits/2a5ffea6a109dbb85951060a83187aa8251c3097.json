{
    "author": "amd-songpiao",
    "message": "PR #34806: [ROCm] fix the calling convention for AMD GPU\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34806\n\nBugfix: PR #34230 (\"argument removal without building prototype\") removed the call to **BuildKernelPrototypeFromUniqueName** which internally called **AnnotateFunctionAsGpuKernel** to set the correct calling convention based on the target GPU. Without this, Triton's **PTX_Kernel** calling convention was copied directly, which doesn't work on AMD GPUs and lead to \"LLVM ERROR: unsupported calling convention\".\n\nFix: Added a call to **AnnotateFunctionAsGpuKernel** in **RemoveUnusedTritonAbiArguments** to properly set:\n\nPTX_Kernel (71) for NVIDIA\nAMDGPU_KERNEL (91) for AMD\nSPIR_KERNEL (76) for SPIR\n\n@xla-rotation could you review my PR, please?\nCopybara import of the project:\n\n--\nebd6e1fa03033bc9f6913351323fce26e1a8e4d2 by Songlin Piao <Songlin.Piao@amd.com>:\n\nreplace the manual calling convention fix with AnnotateFunctionAsGpuKernel\n\n--\n4f16d9579b11c2984c8ebe58041b0d2b9ea5ba3f by Songlin Piao <Songlin.Piao@amd.com>:\n\nadded a filecheck test\n\nMerging this change closes #34806\n\nPiperOrigin-RevId: 842146580",
    "sha": "2a5ffea6a109dbb85951060a83187aa8251c3097",
    "files": [
        {
            "sha": "d051a6daf3e778fb03d349383edc35cdbdf8e1db",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a5ffea6a109dbb85951060a83187aa8251c3097/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a5ffea6a109dbb85951060a83187aa8251c3097/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=2a5ffea6a109dbb85951060a83187aa8251c3097",
            "patch": "@@ -266,10 +266,15 @@ absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n           .getCallee();\n   llvm::Function* new_function = static_cast<llvm::Function*>(inserted);\n \n-  new_function->setCallingConv(impl_fn->getCallingConv());\n   new_function->copyMetadata(impl_fn, 0);\n   new_function->setAttributes(impl_fn->getAttributes());\n \n+  // Set the correct calling convention for the target GPU.\n+  // Triton generates PTX_Kernel CC even for AMD, so we need to use\n+  // AnnotateFunctionAsGpuKernel to set the correct CC based on target triple.\n+  llvm::IRBuilder<> builder(llvm_module->getContext());\n+  AnnotateFunctionAsGpuKernel(llvm_module, new_function, &builder);\n+\n   new_function->splice(new_function->begin(), impl_fn);\n \n   for (const auto& [impl_fn_arg, kernel_arg] :"
        },
        {
            "sha": "284f4090f3fe0a76ceb7a5d5f68076e743c6cf3d",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a5ffea6a109dbb85951060a83187aa8251c3097/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a5ffea6a109dbb85951060a83187aa8251c3097/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=2a5ffea6a109dbb85951060a83187aa8251c3097",
            "patch": "@@ -661,6 +661,7 @@ lit_test_suite_for_gpus(\n             \"slice_to_dynamic.hlo\",\n             \"sorting.hlo\",\n             \"sub_byte_collectives.hlo\",\n+            \"triton_calling_convention.hlo\",\n             \"triton_naming.hlo\",\n             \"zero_clamp_abs_index.hlo\",\n         ],\n@@ -673,10 +674,12 @@ lit_test_suite_for_gpus(\n     disabled_on_gpus = {\n         \"v100\": [\n             \"kernel_reuse.hlo\",\n+            \"triton_calling_convention.hlo\",\n             \"triton_naming.hlo\",\n         ],\n         \"p100\": [\n             \"kernel_reuse.hlo\",\n+            \"triton_calling_convention.hlo\",\n             \"triton_naming.hlo\",\n         ],\n         \"mi200\": ["
        },
        {
            "sha": "6a83c444793d47e13cfe19cbac0d282f609453ad",
            "filename": "third_party/xla/xla/service/gpu/tests/triton_calling_convention.hlo",
            "status": "added",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a5ffea6a109dbb85951060a83187aa8251c3097/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Ftriton_calling_convention.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a5ffea6a109dbb85951060a83187aa8251c3097/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Ftriton_calling_convention.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Ftriton_calling_convention.hlo?ref=2a5ffea6a109dbb85951060a83187aa8251c3097",
            "patch": "@@ -0,0 +1,26 @@\n+// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb | FileCheck --check-prefixes=CHECK-%{PTX} %s\n+\n+// Verify that Triton kernels have the correct calling convention:\n+// - PTX_KERNEL (71) for NVIDIA targets\n+// - AMDGPU_KERNEL (91) for AMD targets\n+// CHECK-PTX: define ptx_kernel void @triton_\n+// CHECK-GCN: define amdgpu_kernel void @triton_\n+\n+HloModule TritonCallingConvention, is_scheduled=true\n+\n+triton_softmax {\n+  param_0 = f32[4,4]{1,0} parameter(0)\n+  ROOT exp = f32[4,4]{1,0} exponential(param_0)\n+}\n+\n+ENTRY main {\n+  param_0 = f32[4,4]{1,0} parameter(0)\n+  ROOT triton_softmax = f32[4,4]{1,0} fusion(param_0), kind=kCustom,\n+    calls=triton_softmax,\n+    backend_config={\"fusion_backend_config\":{\n+      \"kind\":\"__triton\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"4\",\"4\"]}],\n+                                   \"num_warps\":\"1\",\n+                                   \"num_ctas\":\"1\",\n+                                   \"num_stages\":\"1\"}}}\n+}"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 35,
        "deletions": 1
    }
}