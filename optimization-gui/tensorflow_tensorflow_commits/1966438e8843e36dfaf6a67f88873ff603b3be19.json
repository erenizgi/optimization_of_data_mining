{
    "author": "vwbaker",
    "message": "Add is_autotuning_compilation to autotuner\n\nThis is necessary to bailout of autotuning when the autotuning pass is added to RunBackend (we should be able to pull this pass out in the future though).\n\nPiperOrigin-RevId: 801702171",
    "sha": "1966438e8843e36dfaf6a67f88873ff603b3be19",
    "files": [
        {
            "sha": "b68c2376a40c37abd366ec64d8ba55918f61c6a0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1966438e8843e36dfaf6a67f88873ff603b3be19/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1966438e8843e36dfaf6a67f88873ff603b3be19/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=1966438e8843e36dfaf6a67f88873ff603b3be19",
            "patch": "@@ -611,14 +611,21 @@ xla_test(\n         \":native_emitter\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:hlo_module_group\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:executable\",\n+        \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:nvptx_compiler\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "3b947c735aaa697daeffecc050c14aacf4e7b264",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1966438e8843e36dfaf6a67f88873ff603b3be19/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1966438e8843e36dfaf6a67f88873ff603b3be19/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=1966438e8843e36dfaf6a67f88873ff603b3be19",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_AUTOTUNER_GPU_CODEGEN_BACKEND_H_\n \n #include <memory>\n-#include <optional>\n #include <string>\n #include <utility>\n \n@@ -73,6 +72,7 @@ class GpuCodegenBackend : public CodegenBackend {\n         false);\n \n     Compiler::CompileOptions options;\n+    options.is_autotuning_compilation = true;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));\n     return compiler_->RunBackend(std::move(optimized_module), stream_executor_,"
        },
        {
            "sha": "46dbc301dcd6d199467133fe8685c8a9d77ff2e3",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1966438e8843e36dfaf6a67f88873ff603b3be19/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1966438e8843e36dfaf6a67f88873ff603b3be19/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=1966438e8843e36dfaf6a67f88873ff603b3be19",
            "patch": "@@ -22,12 +22,19 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/executable.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n+#include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -183,6 +190,54 @@ TEST_F(NativeEmitterBackendTest, CompileForDefaultConfig) {\n   EXPECT_THAT(maybe_executable, absl_testing::IsOk());\n }\n \n+class MockCompiler : public Compiler {\n+ public:\n+  MOCK_METHOD(absl::StatusOr<std::unique_ptr<Executable>>, RunBackend,\n+              (std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n+               const CompileOptions& options),\n+              (override));\n+  MOCK_METHOD(se::Platform::Id, PlatformId, (), (const, override));\n+  MOCK_METHOD(absl::StatusOr<std::unique_ptr<HloModule>>, RunHloPasses,\n+              (std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n+               const CompileOptions& options),\n+              (override));\n+  MOCK_METHOD(absl::StatusOr<std::vector<std::unique_ptr<Executable>>>, Compile,\n+              (std::unique_ptr<HloModuleGroup> module_group,\n+               std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n+               const CompileOptions& options),\n+              (override));\n+  MOCK_METHOD(\n+      absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>,\n+      CompileAheadOfTime,\n+      (std::unique_ptr<HloModuleGroup> module_group,\n+       const AotCompilationOptions& options),\n+      (override));\n+  MOCK_METHOD(HloCostAnalysis::ShapeSizeFunction, ShapeSizeBytesFunction, (),\n+              (const, override));\n+};\n+\n+TEST_F(NativeEmitterBackendTest, CompileSetsIsAutotuningCompilationOption) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n+                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n+  auto fusion = reduction_module->entry_computation()->root_instruction();\n+  MockCompiler mock_compiler;\n+  NativeEmitterBackend backend(\n+      PlatformUtil::GetDefaultPlatform().value()->ExecutorForDevice(0).value(),\n+      &debug_options_, &mock_compiler);\n+  // Call GetDefaultConfig on the fusion instruction.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n+                          backend.GetDefaultConfig(*(fusion)));\n+  EXPECT_CALL(\n+      mock_compiler,\n+      RunBackend(\n+          testing::_, testing::_,\n+          testing::Field(&Compiler::CompileOptions::is_autotuning_compilation,\n+                         true)))\n+      .WillOnce(testing::Return(std::unique_ptr<Executable>()));\n+  // Attempt to compile the fusion using the retrieved backend config.\n+  EXPECT_THAT(backend.Compile(*fusion, *config), absl_testing::IsOk());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 63,
        "deletions": 1
    }
}