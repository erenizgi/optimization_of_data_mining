{
    "author": "Moerafaat",
    "message": "[XLA:GPU/TMA] Pruning TMA configurations in the new autotuner infrastructure. This is done in a similar way to the old one.\n\nPiperOrigin-RevId: 837230629",
    "sha": "150206144900ce3e5f45cfe1506ef28eac20c2f7",
    "files": [
        {
            "sha": "6f8544fa7f97660898a5c6e17a25d80fbcd20a12",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=150206144900ce3e5f45cfe1506ef28eac20c2f7",
            "patch": "@@ -57,7 +57,7 @@ cc_library(\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/backends/gpu/codegen/triton:support\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n+        \"//xla/backends/gpu/codegen/triton:tma_utils\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:compiler\",\n@@ -113,6 +113,7 @@ xla_test(\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status:status_matchers\",\n@@ -504,6 +505,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/backends/gpu/codegen/triton:tma_utils\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/transforms/simplifiers:float_normalization\",\n         \"//xla/hlo/utils:hlo_query\","
        },
        {
            "sha": "bfda4e0c804bcc251c29afd41e764db921e2e607",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc?ref=150206144900ce3e5f45cfe1506ef28eac20c2f7",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n+#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -201,11 +202,13 @@ void ExtendConfigsWithTma(\n       LOG(ERROR) << \"Failed to unpack BlockLevelFusionConfig\";\n       continue;\n     }\n-    BlockLevelFusionConfig new_config = original_config;\n-    new_config.set_is_tma_allowed(true);\n-    auto any = std::make_unique<google::protobuf::Any>();\n-    any->PackFrom(new_config);\n-    configs.push_back(std::move(any));\n+    if (IsTmaRecommended(original_config)) {\n+      BlockLevelFusionConfig new_config = original_config;\n+      new_config.set_is_tma_allowed(true);\n+      auto any = std::make_unique<google::protobuf::Any>();\n+      any->PackFrom(new_config);\n+      configs.push_back(std::move(any));\n+    }\n   }\n }\n }  // namespace"
        },
        {
            "sha": "096e9080256933f58eededdaddcf5de02b0b6fe0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 36,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=150206144900ce3e5f45cfe1506ef28eac20c2f7",
            "patch": "@@ -35,6 +35,7 @@ limitations under the License.\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n@@ -45,10 +46,9 @@ namespace gpu {\n \n using ::tsl::proto_testing::EqualsProto;\n \n-// Counts the number of configs with is_tma_allowed set to true.\n-int CountTmaAllowed(\n-    const std::vector<std::unique_ptr<BackendConfig>>& configs) {\n-  return std::count_if(configs.begin(), configs.end(), [](auto& config) {\n+// Checks if any config has is_tma_allowed set to true.\n+bool AnyTmaAllowed(const std::vector<std::unique_ptr<BackendConfig>>& configs) {\n+  return std::any_of(configs.begin(), configs.end(), [](auto& config) {\n     BlockLevelFusionConfig actual_config;\n     if (!config->UnpackTo(&actual_config)) {\n       return false;\n@@ -73,8 +73,6 @@ class TritonBlockLevelFusionEmitterBackendTest\n         target_config_(stream_executor_),\n         backend_(&debug_options_, &compiler_,\n                  compiler_.ShapeSizeBytesFunction(), &target_config_) {\n-    // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n-    // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n   }\n \n@@ -204,23 +202,15 @@ ENTRY %main {\n       backend_.GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction())));\n \n-  // If device supports TMA, the backend should generate 70 combinations:\n-  // (7 x 5) x 2.\n-  // Expect 70 total configurations:\n+  // Expect 35 configurations without TMA:\n   // - 7 choices for d0 (output dim 0 = 64): 1, 2, 4, 8, 16, 32, 64\n   // - 5 choices for d2 (output dim 2 = 16): 1, 2, 4, 8, 16\n-  // - 2 choices for is_tma_allowed: true, false\n   // The middle dimension (d1 = 1) must always have tile size 1.\n-  //\n-  // If device doesn't support TMA, we currently expect half the number (35).\n-  bool is_tma_supported = backend_.target_config()\n-                              .device_description.cuda_compute_capability()\n-                              .IsAtLeastHopper();\n-  if (is_tma_supported) {\n-    ASSERT_EQ(configs.size(), 70);\n-    // The current TMA autotuning duplicates the given configurations with\n-    // is_tma_allowed set to true.\n-    EXPECT_EQ(CountTmaAllowed(configs), configs.size() / 2);\n+  if (stream_executor::gpu::IsTmaAvailableForDevice(\n+          backend_.target_config().device_description)) {\n+    ASSERT_GT(configs.size(), 35);\n+    // Check that TMA configurations are generated.\n+    EXPECT_TRUE(AnyTmaAllowed(configs));\n   } else {\n     ASSERT_EQ(configs.size(), 35);\n   }\n@@ -229,8 +219,6 @@ ENTRY %main {\n \n   // Iterate over all expected tile size combinations for d0 and d2.\n   // (d1 is fixed at 1 as per the input shape [16,1,64]).\n-  // TMA configurations repeat in the 2nd half of the configs. We already\n-  // checked them, so we don't inspect them here.\n   for (int d0 : {1, 2, 4, 8, 16, 32, 64}) {\n     for (int d2 : {1, 2, 4, 8, 16}) {\n       BlockLevelFusionConfig block_level_fusion_config;\n@@ -288,21 +276,15 @@ backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n       backend_.GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction())));\n \n-  // If device supports TMA, expect 40 total configurations:\n+  // Expect 20 configurations without TMA:\n   // - 5 choices for d0 (output dim 0 = 10): 1, 2, 4, 8, 16\n   // - 4 choices for d2 (output dim 2 = 8): 1, 2, 4, 8\n-  // - 2 choices for is_tma_allowed: true, false\n   // The middle dimension (d1 = 0) must always have tile size 0.\n-  //\n-  // If device doesn't support TMA, we currently expect half the number (20).\n-  bool is_tma_supported = backend_.target_config()\n-                              .device_description.cuda_compute_capability()\n-                              .IsAtLeastHopper();\n-  if (is_tma_supported) {\n-    ASSERT_EQ(configs.size(), 40);\n-    // The current TMA autotuning duplicates the given configurations with\n-    // is_tma_allowed set to true.\n-    EXPECT_EQ(CountTmaAllowed(configs), configs.size() / 2);\n+  if (stream_executor::gpu::IsTmaAvailableForDevice(\n+          backend_.target_config().device_description)) {\n+    ASSERT_GT(configs.size(), 20);\n+    // Check that TMA configurations are generated.\n+    EXPECT_TRUE(AnyTmaAllowed(configs));\n   } else {\n     ASSERT_EQ(configs.size(), 20);\n   }\n@@ -311,8 +293,6 @@ backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n \n   // Iterate over tile size combinations for dimensions 0 and 2.\n   // Dimension 1 (middle) is zero-sized, so its tile size is fixed to 0.\n-  // TMA configurations repeat in the 2nd half of the configs. We already\n-  // checked them, so we don't inspect them here.\n   for (int d0 : {1, 2, 4, 8, 16}) {\n     for (int d2 : {1, 2, 4, 8}) {\n       BlockLevelFusionConfig block_level_fusion_config;"
        },
        {
            "sha": "3ea59acbd0c445df8b1d0a6a6f514fe4e22f171d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=150206144900ce3e5f45cfe1506ef28eac20c2f7",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -88,8 +89,10 @@ std::vector<TritonGemmConfig> GetDefaultTritonConfigs(\n     config.is_tma_allowed = false;\n     tma_parameterized_configs.push_back(config);\n \n-    config.is_tma_allowed = true;\n-    tma_parameterized_configs.push_back(config);\n+    if (IsTmaRecommended(config)) {\n+      config.is_tma_allowed = true;\n+      tma_parameterized_configs.push_back(config);\n+    }\n   }\n   return tma_parameterized_configs;\n }"
        },
        {
            "sha": "f617f6261cfa50a46bf33ea9f6c6bb72419af1b3",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 16,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=150206144900ce3e5f45cfe1506ef28eac20c2f7",
            "patch": "@@ -79,8 +79,6 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n                              .value()),\n         target_config_(stream_executor_),\n         backend_(&debug_options_, &compiler_, &target_config_, &mlir_context_) {\n-    // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n-    // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n   }\n \n@@ -105,20 +103,15 @@ TEST_F(TritonBackendTest, GetSupportedConfigs) {\n   if (backend_.target_config()\n           .device_description.cuda_compute_capability()\n           .IsAtLeastHopper()) {\n-    auto count_tma_allowed =\n-        [](const std::vector<std::unique_ptr<BackendConfig>>& configs) {\n-          return std::count_if(configs.begin(), configs.end(),\n-                               [](auto& config) {\n-                                 TritonBackendConfig actual_config;\n-                                 if (!config->UnpackTo(&actual_config)) {\n-                                   return false;\n-                                 }\n-                                 return actual_config.is_tma_allowed();\n-                               });\n-        };\n-    // The current TMA autotuning duplicates the given configurations with\n-    // is_tma_allowed set to true.\n-    EXPECT_EQ(count_tma_allowed(configs.value()), configs.value().size() / 2);\n+    // Check that TMA configurations are generated.\n+    EXPECT_TRUE(std::any_of(configs.value().begin(), configs.value().end(),\n+                            [](auto& config) {\n+                              TritonBackendConfig actual_config;\n+                              if (!config->UnpackTo(&actual_config)) {\n+                                return false;\n+                              }\n+                              return actual_config.is_tma_allowed();\n+                            }));\n   }\n }\n "
        },
        {
            "sha": "b7a6eb1b6ce3975f2e0ed10b9518a07eb762dfbc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/tma_utils.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc?ref=150206144900ce3e5f45cfe1506ef28eac20c2f7",
            "patch": "@@ -151,4 +151,21 @@ bool IsTmaRecommended(const TritonGemmConfig& config) {\n          config.block_k <= 256;\n }\n \n+// Equivalent to the recommendation constructed for TritonGemmConfig.\n+bool IsTmaRecommended(const BlockLevelFusionConfig& config) {\n+  if (!(config.num_warps() <= 8 &&\n+        (config.num_stages() == 1 || config.num_stages() == 3 ||\n+         config.num_stages() == 4))) {\n+    return false;\n+  }\n+  for (const auto& tile : config.output_tiles()) {\n+    for (const auto& dim : tile.sizes()) {\n+      if (dim > 256) {\n+        return false;\n+      }\n+    }\n+  }\n+  return true;\n+}\n+\n }  // namespace xla::gpu"
        },
        {
            "sha": "19985f1fa863f16ec27933294fd571119cd9764d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/tma_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150206144900ce3e5f45cfe1506ef28eac20c2f7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.h?ref=150206144900ce3e5f45cfe1506ef28eac20c2f7",
            "patch": "@@ -35,7 +35,11 @@ absl::StatusOr<stream_executor::gpu::TmaDescriptor> CreateTmaDescriptor(\n // Recommends whether to attempt using TMA for a given configuration. This helps\n // prune the search space and avoid compile-time regressions from trying out all\n // configurations.\n+//\n+// // The new autotuner uses BlockLevelFusionConfig instead of TritonGemmConfig,\n+// hence the two functions.\n bool IsTmaRecommended(const TritonGemmConfig& config);\n+bool IsTmaRecommended(const BlockLevelFusionConfig& config);\n }  // namespace xla::gpu\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_TMA_UTILS_H_"
        }
    ],
    "stats": {
        "total": 122,
        "additions": 62,
        "deletions": 60
    }
}