{
    "author": "sergachev",
    "message": "PR #31525: [GPU] Optimize collectives on sub-byte types.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31525\n\nThe new pass replaces suitable collectives on packed sub-byte types with equivalent ones on the same data casted to S8, which eliminates unnecessary conversions and reduces the volume of moved data.\nCopybara import of the project:\n\n--\n79fbbe9db92fc96bf407107b6b85307ff15a23b2 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Optimize collectives on sub-byte types.\n\n--\n86843e5ee656f40f8694b05ed2766925cdb06574 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review feedback.\n\n--\nb5d92c504aec3687930681bed5ba4307cc3f4bd4 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review feedback.\n\n--\ne35144cbc5dac746dd495249f21e41a5dfdf9278 by Ilia Sergachev <isergachev@nvidia.com>:\n\nFix skipping of variadic all-to-all.\n\n--\nc80c73deba3bbc3f18e8f52c8ea4d61bb7d0b1a3 by Ilia Sergachev <isergachev@nvidia.com>:\n\nFix more all-to-all skipping.\n\n--\nf7f00fc390ecbb37ff61381c366bf76e7e12e81a by Ilia Sergachev <isergachev@nvidia.com>:\n\nDeduplicate checks.\n\n--\nd48cc359f45264b5fb50fbe349e41292d506e229 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review feedback.\n\nMerging this change closes #31525\n\nPiperOrigin-RevId: 816123777",
    "sha": "1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
    "files": [
        {
            "sha": "a9a1517345c96c34f20975d34e036e59dc6d11b4",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/BUILD",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -1602,6 +1602,40 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"sub_byte_collective_normalization\",\n+    srcs = [\"sub_byte_collective_normalization.cc\"],\n+    hdrs = [\"sub_byte_collective_normalization.h\"],\n+    deps = [\n+        \":hlo_dce\",\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/service:collective_ops_utils\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:statusor\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"sub_byte_collective_normalization_test\",\n+    srcs = [\"sub_byte_collective_normalization_test.cc\"],\n+    deps = [\n+        \":sub_byte_collective_normalization\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"all_gather_permuted_ds_simplifier\",\n     srcs = [\"all_gather_permuted_ds_simplifier.cc\"],"
        },
        {
            "sha": "e58bae332fb7810bf1190d845d21da2b3185b4e0",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/sub_byte_collective_normalization.cc",
            "status": "added",
            "additions": 208,
            "deletions": 0,
            "changes": 208,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization.cc?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -0,0 +1,208 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/transforms/simplifiers/sub_byte_collective_normalization.h\"\n+\n+#include <cstdint>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n+#include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace {\n+\n+HloInstruction* ReshapeAndCastToWiderType(HloInstruction* input,\n+                                          const PrimitiveType type) {\n+  const Shape& input_shape = input->shape();\n+  const int64_t ratio = primitive_util::BitWidth(type) /\n+                        primitive_util::BitWidth(input_shape.element_type());\n+\n+  std::vector<int64_t> bitcast_dimensions(input_shape.dimensions().begin(),\n+                                          input_shape.dimensions().end());\n+  bitcast_dimensions.back() /= ratio;\n+  bitcast_dimensions.push_back(ratio);\n+  Shape bitcast_shape =\n+      ShapeUtil::MakeShape(input_shape.element_type(), bitcast_dimensions);\n+  if (input_shape.has_layout()) {\n+    *bitcast_shape.mutable_layout() = LayoutUtil::MoveDimToMinor(\n+        input_shape.layout(), bitcast_dimensions.size() - 1);\n+  }\n+  HloInstruction* bitcast = input->parent()->AddInstruction(\n+      HloInstruction::CreateBitcast(bitcast_shape, input));\n+  HloInstruction* result =\n+      input->parent()->AddInstruction(HloInstruction::CreateBitcastConvert(\n+          ShapeUtil::MakeShape(\n+              type, std::vector<int64_t>(bitcast_dimensions.begin(),\n+                                         bitcast_dimensions.end() - 1)),\n+          bitcast));\n+  if (input_shape.has_layout()) {\n+    *result->mutable_shape()->mutable_layout() = input_shape.layout();\n+    result->mutable_shape()->mutable_layout()->set_element_size_in_bits(0);\n+  }\n+  return result;\n+}\n+\n+HloInstruction* CastToNarrowerTypeAndReshape(HloInstruction* input,\n+                                             const Shape& shape) {\n+  const Shape& input_shape = input->shape();\n+  const int64_t ratio = primitive_util::BitWidth(input_shape.element_type()) /\n+                        primitive_util::BitWidth(shape.element_type());\n+\n+  std::vector<int64_t> convert_dimensions(input_shape.dimensions().begin(),\n+                                          input_shape.dimensions().end());\n+  convert_dimensions.push_back(ratio);\n+  Shape convert_shape =\n+      ShapeUtil::MakeShape(shape.element_type(), convert_dimensions);\n+  if (shape.has_layout()) {\n+    *convert_shape.mutable_layout() = LayoutUtil::MoveDimToMinor(\n+        input_shape.layout(), convert_dimensions.size() - 1);\n+    convert_shape.mutable_layout()->set_element_size_in_bits(\n+        shape.layout().element_size_in_bits());\n+  }\n+  HloInstruction* convert = input->parent()->AddInstruction(\n+      HloInstruction::CreateBitcastConvert(convert_shape, input));\n+  return input->parent()->AddInstruction(\n+      HloInstruction::CreateBitcast(shape, convert));\n+}\n+\n+bool CanBeRepresentedAs(const Shape& shape, const PrimitiveType casted_type) {\n+  const int64_t ratio = primitive_util::BitWidth(casted_type) /\n+                        primitive_util::BitWidth(shape.element_type());\n+  return primitive_util::IsSubByteNonPredType(shape.element_type()) &&\n+         ShapeUtil::LastDimIsMinorMost(shape) &&\n+         shape.layout().element_size_in_bits() ==\n+             primitive_util::BitWidth(shape.element_type()) &&\n+         shape.dimensions().back() % ratio == 0;\n+}\n+\n+class SubByteCollectiveNormalizationVisitor : public DfsHloRewriteVisitor {\n+ public:\n+  SubByteCollectiveNormalizationVisitor() = default;\n+  absl::Status HandleAllGather(HloInstruction* hlo) override;\n+  absl::Status HandleAllToAll(HloInstruction* hlo) override;\n+  absl::Status HandleCollectiveBroadcast(HloInstruction* hlo) override;\n+  absl::Status HandleCollectivePermute(HloInstruction* hlo) override;\n+\n+ private:\n+  bool ShouldProcessInstruction(const HloInstruction& hlo) const;\n+  absl::Status ProcessCollectiveInstruction(HloInstruction& hlo);\n+  static constexpr PrimitiveType casted_type_ = S8;\n+};\n+\n+bool SubByteCollectiveNormalizationVisitor::ShouldProcessInstruction(\n+    const HloInstruction& hlo) const {\n+  return hlo.operand_count() == 1 &&\n+         CanBeRepresentedAs(hlo.operand(0)->shape(), casted_type_);\n+}\n+\n+absl::Status SubByteCollectiveNormalizationVisitor::HandleAllGather(\n+    HloInstruction* hlo) {\n+  return ProcessCollectiveInstruction(*hlo);\n+}\n+\n+absl::Status SubByteCollectiveNormalizationVisitor::HandleAllToAll(\n+    HloInstruction* hlo) {\n+  if (!ShouldProcessInstruction(*hlo)) {\n+    return absl::OkStatus();\n+  }\n+\n+  const int64_t ratio = primitive_util::BitWidth(casted_type_) /\n+                        primitive_util::BitWidth(hlo->shape().element_type());\n+  const auto* all_to_all = Cast<HloAllToAllInstruction>(hlo);\n+  if (all_to_all->split_dimension()) {\n+    TF_ASSIGN_OR_RETURN(const CollectiveOpGroupMode group_mode,\n+                        GetCollectiveOpGroupMode(all_to_all));\n+    const int64_t split_dimension_size =\n+        hlo->shape().dimensions(*all_to_all->split_dimension());\n+    if (split_dimension_size %\n+            (GetSubgroupSize(all_to_all, group_mode) * ratio) !=\n+        0) {\n+      return absl::OkStatus();\n+    }\n+  }\n+  return ProcessCollectiveInstruction(*hlo);\n+}\n+\n+absl::Status SubByteCollectiveNormalizationVisitor::HandleCollectiveBroadcast(\n+    HloInstruction* hlo) {\n+  return ProcessCollectiveInstruction(*hlo);\n+}\n+\n+absl::Status SubByteCollectiveNormalizationVisitor::HandleCollectivePermute(\n+    HloInstruction* hlo) {\n+  return ProcessCollectiveInstruction(*hlo);\n+}\n+\n+absl::Status\n+SubByteCollectiveNormalizationVisitor::ProcessCollectiveInstruction(\n+    HloInstruction& hlo) {\n+  if (!ShouldProcessInstruction(hlo)) {\n+    return absl::OkStatus();\n+  }\n+\n+  const int64_t ratio = primitive_util::BitWidth(casted_type_) /\n+                        primitive_util::BitWidth(hlo.shape().element_type());\n+\n+  std::vector<int64_t> new_collective_dimensions(\n+      hlo.shape().dimensions().begin(), hlo.shape().dimensions().end());\n+  new_collective_dimensions.back() /= ratio;\n+  Shape new_collective_shape =\n+      ShapeUtil::MakeShape(casted_type_, new_collective_dimensions);\n+  if (hlo.shape().has_layout()) {\n+    *new_collective_shape.mutable_layout() = hlo.shape().layout();\n+    new_collective_shape.mutable_layout()->set_element_size_in_bits(0);\n+  }\n+  HloInstruction* new_collective =\n+      hlo.parent()->AddInstruction(hlo.CloneWithNewOperands(\n+          new_collective_shape,\n+          {ReshapeAndCastToWiderType(hlo.mutable_operand(0), casted_type_)}));\n+  TF_RETURN_IF_ERROR(hlo.parent()->ReplaceInstructionWithDifferentShape(\n+      &hlo, CastToNarrowerTypeAndReshape(new_collective, hlo.shape())));\n+\n+  MarkAsChanged();\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace\n+\n+absl::StatusOr<bool> SubByteCollectiveNormalization::Run(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  SubByteCollectiveNormalizationVisitor visitor;\n+  for (HloComputation* computation :\n+       module->MakeComputationPostOrder(execution_threads)) {\n+    TF_RETURN_IF_ERROR(computation->Accept(&visitor));\n+  }\n+\n+  return visitor.changed();\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "6e7dd5b31465d07f88ade94ba5526ea0c8aa8632",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/sub_byte_collective_normalization.h",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization.h?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -0,0 +1,46 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_TRANSFORMS_SIMPLIFIERS_SUB_BYTE_COLLECTIVE_NORMALIZATION_H_\n+#define XLA_HLO_TRANSFORMS_SIMPLIFIERS_SUB_BYTE_COLLECTIVE_NORMALIZATION_H_\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+\n+namespace xla {\n+\n+// Replaces pure data movement collectives on sub-byte data types\n+// with equivalent ones on whole bytes.\n+class SubByteCollectiveNormalization : public HloModulePass {\n+ public:\n+  SubByteCollectiveNormalization() = default;\n+\n+  ~SubByteCollectiveNormalization() override = default;\n+  absl::string_view name() const override {\n+    return \"sub-byte-collective-normalization\";\n+  }\n+\n+  using HloPassInterface::Run;\n+  absl::StatusOr<bool> Run(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_TRANSFORMS_SIMPLIFIERS_SUB_BYTE_COLLECTIVE_NORMALIZATION_H_"
        },
        {
            "sha": "262b2dec03cb440259ca2308eef023d8fd9d817a",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/sub_byte_collective_normalization_test.cc",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fsub_byte_collective_normalization_test.cc?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -0,0 +1,136 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/transforms/simplifiers/sub_byte_collective_normalization.h\"\n+\n+#include <gtest/gtest.h>\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+\n+namespace xla {\n+\n+class SubByteCollectiveNormalizationTest\n+    : public HloHardwareIndependentTestBase {};\n+\n+TEST_F(SubByteCollectiveNormalizationTest, SkipNonSubByteTypes) {\n+  TF_ASSERT_OK(RunAndCheckHloRewrite(R\"(\n+e {\n+ a = s8[4,8]{1,0} parameter(0)\n+ b = s8[8,8]{1,0} all-gather(a), dimensions={0}\n+})\",\n+                                     SubByteCollectiveNormalization(),\n+                                     /*expect_change=*/false));\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, SkipNonPacked) {\n+  TF_ASSERT_OK(RunAndCheckHloRewrite(R\"(\n+e {\n+ a = s2[16,32]{1,0} parameter(0)\n+ b = s2[32,32]{1,0} all-gather(a), dimensions={0}\n+})\",\n+                                     SubByteCollectiveNormalization(),\n+                                     /*expect_change=*/false));\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, SkipNonMinorMost) {\n+  TF_ASSERT_OK(RunAndCheckHloRewrite(R\"(\n+e {\n+ a = s4[32,16]{0,1:E(4)} parameter(0)\n+ b = s4[32,16]{0,1:E(4)} all-gather(a), dimensions={0}\n+})\",\n+                                     SubByteCollectiveNormalization(),\n+                                     /*expect_change=*/false));\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, SkipOddElementCount) {\n+  TF_ASSERT_OK(RunAndCheckHloRewrite(R\"(\n+e {\n+ a = s4[4,9]{1,0:E(4)} parameter(0)\n+ b = s4[8,9]{1,0:E(4)} all-gather(a), dimensions={0}\n+})\",\n+                                     SubByteCollectiveNormalization(),\n+                                     /*expect_change=*/false));\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, SkipVariadic) {\n+  TF_ASSERT_OK(RunAndCheckHloRewrite(R\"(\n+e {\n+ a = s4[2]{0:E(4)} parameter(0)\n+ b = s4[2]{0:E(4)} parameter(1)\n+ c = (s4[2]{0:E(4)}, s4[2]{0:E(4)}) all-to-all(a, b), replica_groups={{2,1},{3,0}}\n+})\",\n+                                     SubByteCollectiveNormalization(),\n+                                     /*expect_change=*/false));\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, TransformS4AllGather) {\n+  RunAndFilecheckHloRewrite(R\"(\n+e {\n+ a = s4[4,8]{1,0:E(4)} parameter(0)\n+ b = s4[8,8]{1,0:E(4)} all-gather(a), dimensions={0}\n+})\",\n+                            SubByteCollectiveNormalization(), R\"(\n+CHECK: s4[4,8]{1,0:E(4)} parameter\n+CHECK-NEXT: s4[4,4,2]{2,1,0:E(4)} bitcast\n+CHECK-NEXT: s8[4,4]{1,0} bitcast-convert\n+CHECK-NEXT: s8[8,4]{1,0} all-gather\n+CHECK-NEXT: s4[8,4,2]{2,1,0:E(4)} bitcast-convert\n+CHECK-NEXT: s4[8,8]{1,0:E(4)} bitcast\n+)\");\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, SkipTinyAllToAll) {\n+  TF_ASSERT_OK(RunAndCheckHloRewrite(R\"(\n+HloModule m, replica_count=2\n+e {\n+  a = u4[2]{0:E(4)} parameter(0)\n+  b = u4[2]{0:E(4)} all-to-all(a), dimensions={0}\n+})\",\n+                                     SubByteCollectiveNormalization(),\n+                                     /*expect_change=*/false));\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, TransformF4AllToAll) {\n+  RunAndFilecheckHloRewrite(R\"(\n+e {\n+ a = f4e2m1fn[3,6,10]{2,1,0:E(4)} parameter(0)\n+ b = f4e2m1fn[3,6,10]{2,1,0:E(4)} all-to-all(a), dimensions={1}\n+})\",\n+                            SubByteCollectiveNormalization(), R\"(\n+CHECK: f4e2m1fn[3,6,10]{2,1,0:E(4)} parameter\n+CHECK-NEXT: f4e2m1fn[3,6,5,2]{3,2,1,0:E(4)} bitcast\n+CHECK-NEXT: s8[3,6,5]{2,1,0} bitcast-convert\n+CHECK-NEXT: s8[3,6,5]{2,1,0} all-to-all\n+CHECK-NEXT: f4e2m1fn[3,6,5,2]{3,2,1,0:E(4)} bitcast-convert\n+CHECK-NEXT: f4e2m1fn[3,6,10]{2,1,0:E(4)} bitcast\n+)\");\n+}\n+\n+TEST_F(SubByteCollectiveNormalizationTest, TransformU2CollectiveBroadcast) {\n+  RunAndFilecheckHloRewrite(R\"(\n+e {\n+ a = u2[5,9,8]{2,0,1:E(2)} parameter(0)\n+ b = u2[5,9,8]{2,0,1:E(2)} collective-broadcast(a), replica_groups={}\n+})\",\n+                            SubByteCollectiveNormalization(), R\"(\n+CHECK: u2[5,9,8]{2,0,1:E(2)} parameter\n+CHECK-NEXT: u2[5,9,2,4]{3,2,0,1:E(2)} bitcast\n+CHECK-NEXT: s8[5,9,2]{2,0,1} bitcast-convert\n+CHECK-NEXT: s8[5,9,2]{2,0,1} collective-broadcast\n+CHECK-NEXT: u2[5,9,2,4]{3,2,0,1:E(2)} bitcast-convert\n+CHECK-NEXT: u2[5,9,8]{2,0,1:E(2)} bitcast\n+)\");\n+}\n+}  // namespace xla"
        },
        {
            "sha": "5f0135b5c3706026743a4e2197418656199efbc1",
            "filename": "third_party/xla/xla/service/collective_ops_utils.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 2,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n-#include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -816,7 +815,6 @@ HloInstruction* IsOrHasCollectiveWithChannelId(HloInstruction* instruction) {\n   return nullptr;\n }\n \n-\n bool IsExclusivelyCrossModule(absl::Span<const ReplicaGroup> replica_groups,\n                               bool use_global_ids, bool has_channel_id,\n                               const DeviceAssignment& device_assignment) {\n@@ -893,4 +891,31 @@ bool HasDuplicateSourcesOrTargets(const SourceTargetPairs& pairs) {\n   }\n   return false;\n }\n+\n+int64_t GetSubgroupSize(const HloCollectiveInstruction* hlo,\n+                        CollectiveOpGroupMode group_mode) {\n+  const HloModuleConfig& config = hlo->GetModule()->config();\n+  switch (group_mode) {\n+    case CollectiveOpGroupMode::kCrossReplica:\n+    case CollectiveOpGroupMode::kCrossReplicaAndPartition: {\n+      int64_t replica_subgroup_size =\n+          hlo->replica_groups().empty()\n+              ? config.replica_count()\n+              : hlo->replica_groups()[0].replica_ids_size();\n+      if (group_mode == CollectiveOpGroupMode::kCrossReplicaAndPartition) {\n+        // Replicas from all partitions participate.\n+        replica_subgroup_size *= config.num_partitions();\n+      }\n+      return replica_subgroup_size;\n+    }\n+    case CollectiveOpGroupMode::kFlattenedID:\n+      // Empty replica groups not allowed in this mode.\n+      return hlo->replica_groups()[0].replica_ids_size();\n+    case CollectiveOpGroupMode::kCrossPartition:\n+      return hlo->replica_groups().empty()\n+                 ? config.num_partitions()\n+                 : hlo->replica_groups()[0].replica_ids_size();\n+  }\n+}\n+\n }  // end namespace xla"
        },
        {
            "sha": "c1af0b93e4493a86610936e56f340a9dc58b6ba7",
            "filename": "third_party/xla/xla/service/collective_ops_utils.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -225,7 +225,6 @@ absl::StatusOr<bool> IsAsyncCollective(const HloInstruction* instruction);\n // collective fusion) with channel_id.\n HloInstruction* IsOrHasCollectiveWithChannelId(HloInstruction* instruction);\n \n-\n // Key that identifies a particular Rendezvous object in our global hashtable.\n // This determines which calls to ExecuteOnStream communicate with each other.\n // The rules are as follows.\n@@ -344,6 +343,9 @@ inline constexpr absl::string_view kCollectiveStreamAttrName =\n     \"_xla_gpu_collective_stream\";\n inline constexpr absl::string_view kCollectiveStreamP2P = \"p2p\";\n \n+int64_t GetSubgroupSize(const HloCollectiveInstruction* hlo,\n+                        CollectiveOpGroupMode group_mode);\n+\n }  // end namespace xla\n \n #endif  // XLA_SERVICE_COLLECTIVE_OPS_UTILS_H_"
        },
        {
            "sha": "0dcbe366d2e231834f87f9dbe47e4829fed16581",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -1655,6 +1655,7 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:simplify_fp_conversions\",\n         \"//xla/hlo/transforms/simplifiers:slice_sinker\",\n         \"//xla/hlo/transforms/simplifiers:sort_simplifier\",\n+        \"//xla/hlo/transforms/simplifiers:sub_byte_collective_normalization\",\n         \"//xla/hlo/transforms/simplifiers:sub_byte_normalization\",\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n         \"//xla/hlo/transforms/simplifiers:zero_sized_hlo_elimination\","
        },
        {
            "sha": "9dac0945113a76149a634d18329cf2e60352cef1",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -135,6 +135,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/simplify_fp_conversions.h\"\n #include \"xla/hlo/transforms/simplifiers/slice_sinker.h\"\n #include \"xla/hlo/transforms/simplifiers/sort_simplifier.h\"\n+#include \"xla/hlo/transforms/simplifiers/sub_byte_collective_normalization.h\"\n #include \"xla/hlo/transforms/simplifiers/sub_byte_normalization.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/zero_sized_hlo_elimination.h\"\n@@ -1854,6 +1855,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n                      .VerifyReshapeIsBitcast(),\n                  /*debug_only=*/true);\n \n+  pipeline.AddPass<SubByteCollectiveNormalization>();\n+\n   // Triton compilation needs normalized operations on bf16 (i.e. converted to\n   // f32).\n   add_float_normalization(pipeline);"
        },
        {
            "sha": "5f470d0529aa0df3d8fa1e89f50f9aaffde4e3d5",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -630,6 +630,7 @@ lit_test_suite(\n             \"single_instruction.hlo\",\n             \"slice_to_dynamic.hlo\",\n             \"sorting.hlo\",\n+            \"sub_byte_collectives.hlo\",\n             \"triton_naming.hlo\",\n             \"zero_clamp_abs_index.hlo\",\n         ],"
        },
        {
            "sha": "bb680f4db76f0f338bd37f3a3e140e3127b3d9d8",
            "filename": "third_party/xla/xla/service/gpu/tests/sub_byte_collectives.hlo",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsub_byte_collectives.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsub_byte_collectives.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsub_byte_collectives.hlo?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -0,0 +1,46 @@\n+// RUN: hlo-opt %s --platform=gpu --split-input-file --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb | FileCheck  %s\n+\n+e {\n+  a = s4[4,16]{1,0:E(4)} parameter(0)\n+  b = s4[32,16]{1,0:E(4)} all-gather(a), dimensions={0}\n+}\n+\n+// CHECK-NOT: convert\n+// CHECK: s8[4,8]{1,0} bitcast\n+// CHECK: s8[32,8]{1,0} all-gather-done\n+// CHECK: s4[32,16]{1,0:E(4)} bitcast\n+\n+// -----\n+\n+e {\n+  a = f4e2m1fn[80,20]{1,0:E(4)} parameter(0)\n+  b = f4e2m1fn[80,20]{1,0:E(4)} all-to-all(a), dimensions={0}\n+}\n+\n+// CHECK-NOT: convert\n+// CHECK: s8[80,10]{1,0} bitcast\n+// CHECK: s8[80,10]{1,0} async-done\n+// CHECK: f4e2m1fn[80,20]{1,0:E(4)} bitcast\n+\n+// -----\n+\n+e {\n+  a = u4[5,14,6]{2,0,1:E(4)} parameter(0)\n+  b = u4[5,14,6]{2,0,1:E(4)} collective-broadcast(a), replica_groups={}\n+}\n+\n+// CHECK-NOT: convert\n+// CHECK: s8[5,14,3]{2,0,1} async-done\n+// CHECK: u4[5,14,6]{2,0,1:E(4)} bitcast\n+\n+// -----\n+\n+e {\n+  a = u4[18]{0:E(4)} parameter(0)\n+  b = u4[18]{0:E(4)} collective-permute(a), source_target_pairs={}\n+}\n+\n+// CHECK-NOT: convert\n+// CHECK: s8[9]{0} bitcast\n+// CHECK: s8[9]{0} collective-permute-done\n+// CHECK: u4[18]{0:E(4)} bitcast"
        },
        {
            "sha": "534636a1b1692797aeb18d41584369ecc125a5c4",
            "filename": "third_party/xla/xla/service/hlo_verifier.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -88,32 +88,6 @@ absl::Status CheckOperandCount(const HloInstruction* hlo, int expected) {\n   return absl::OkStatus();\n }\n \n-int64_t GetSubgroupSize(HloCollectiveInstruction* hlo,\n-                        CollectiveOpGroupMode group_mode) {\n-  const HloModuleConfig& config = hlo->GetModule()->config();\n-  switch (group_mode) {\n-    case CollectiveOpGroupMode::kCrossReplica:\n-    case CollectiveOpGroupMode::kCrossReplicaAndPartition: {\n-      int64_t replica_subgroup_size =\n-          hlo->replica_groups().empty()\n-              ? config.replica_count()\n-              : hlo->replica_groups()[0].replica_ids_size();\n-      if (group_mode == CollectiveOpGroupMode::kCrossReplicaAndPartition) {\n-        // Replicas from all partitions participate.\n-        replica_subgroup_size *= config.num_partitions();\n-      }\n-      return replica_subgroup_size;\n-    }\n-    case CollectiveOpGroupMode::kFlattenedID:\n-      // Empty replica groups not allowed in this mode.\n-      return hlo->replica_groups()[0].replica_ids_size();\n-    case CollectiveOpGroupMode::kCrossPartition:\n-      return hlo->replica_groups().empty()\n-                 ? config.num_partitions()\n-                 : hlo->replica_groups()[0].replica_ids_size();\n-  }\n-}\n-\n absl::Status CheckUnaryOpWithResultAccuracy(HloInstruction* unary) {\n   HloOpcode opcode = unary->opcode();\n   if (unary->has_result_accuracy()) {"
        },
        {
            "sha": "94d3b017a1821ff7ec981dfbe3b6c33d551b7bae",
            "filename": "third_party/xla/xla/service/pattern_matcher.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fpattern_matcher.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Fservice%2Fpattern_matcher.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fpattern_matcher.h?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -2740,6 +2740,8 @@ XLA_UNOP_PATTERN(Ceil)\n XLA_UNOP_PATTERN(Convert)\n XLA_UNOP_PATTERN(Copy)\n XLA_UNOP_PATTERN(Cos)\n+XLA_UNOP_PATTERN(AllGatherStart)\n+XLA_UNOP_PATTERN(AllGatherDone)\n XLA_UNOP_PATTERN(AllReduceStart)\n XLA_UNOP_PATTERN(AllReduceDone)\n XLA_UNOP_PATTERN(AllToAll)"
        },
        {
            "sha": "c36ae8a525b3fc5b9722d6eea160db704fef6ae0",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -2790,6 +2790,7 @@ xla_test(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/service:backend\","
        },
        {
            "sha": "68b766b6180461cd4761c48043f4f25b8aace260",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=1f8d2ef01fff991749af9d7ba7a80ed6bb4e59ca",
            "patch": "@@ -46,6 +46,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/hlo/utils/hlo_matchers.h\"\n #include \"xla/literal.h\"\n@@ -80,6 +81,7 @@ namespace xla {\n namespace {\n \n namespace op = ::xla::testing::opcode_matchers;\n+namespace m = ::xla::match;\n using ::testing::NotNull;\n \n // Makes a DeviceAssignment device#i to replica_id #i.\n@@ -4454,6 +4456,44 @@ TEST_P(AllReduceTest, AsyncAllReduce_8GPUs_2ReplicasPerGroup) {\n   }\n }\n \n+TEST_F(CollectiveOpsTestE2E, OptimizedSubByteAllGatherOutputIsCorrect) {\n+  constexpr int kNumReplicas = 2;\n+  if (test_runner().device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"The test requires at least \" << kNumReplicas\n+                 << \" devices.\";\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(\n+                                           R\"(\n+HloModule m, replica_count=2\n+\n+e {\n+  a = s4[2,4]{1,0:E(4)} constant({{0,1,2,3},{4,5,5,4}})\n+  b = s4[4,4]{1,0:E(4)} all-gather(a), dimensions={0}\n+})\"));\n+\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::Bitcast(m::AllGatherDone().WithShape(S8, {4, 2}))));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> result,\n+      HloTestBase::ExecuteReplicated(std::move(module),\n+                                     absl::Span<const Literal* const>{},\n+                                     kNumReplicas, /*use_threads=*/true));\n+\n+  const Literal expected_result =\n+      LiteralUtil::CreateR2<s4>({{s4(0), s4(1), s4(2), s4(3)},\n+                                 {s4(4), s4(5), s4(5), s4(4)},\n+                                 {s4(0), s4(1), s4(2), s4(3)},\n+                                 {s4(4), s4(5), s4(5), s4(4)}});\n+\n+  ASSERT_EQ(result.size(), kNumReplicas);\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_result, result[i]))\n+        << \"Results differ at replica \" << i;\n+  }\n+}\n+\n INSTANTIATE_TEST_SUITE_P(\n     AllReduceTest, AllReduceTest,\n     ::testing::Combine(::testing::Bool(), ::testing::Bool()),"
        }
    ],
    "stats": {
        "total": 577,
        "additions": 548,
        "deletions": 29
    }
}