{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Refactor CollectiveThunk buffer creation in ThunkEmitter.\n\nMove most of the repeated parts inside of the add_buffer lambda.\n\nPiperOrigin-RevId: 838725607",
    "sha": "1a19efb9118fdd02612fa905032a366a58cf70a2",
    "files": [
        {
            "sha": "51e20f4c859508ec323294eef3d6ef8114c5506a",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 48,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a19efb9118fdd02612fa905032a366a58cf70a2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a19efb9118fdd02612fa905032a366a58cf70a2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=1a19efb9118fdd02612fa905032a366a58cf70a2",
            "patch": "@@ -1696,28 +1696,34 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveThunk(\n   const auto& hlo_config = ir_emitter_context_->hlo_module().config();\n   int64_t replica_count = hlo_config.replica_count();\n   int64_t partition_count = hlo_config.num_partitions();\n+  int64_t operand_count = inst->operand_count();\n   VLOG(2) << CollectiveThunkType::GetHloOpName()\n           << \"; replica count: \" << replica_count\n           << \"; partition count: \" << partition_count\n-          << \"; operand count: \" << inst->operand_count();\n+          << \"; operand count: \" << operand_count;\n \n   // Stash relevant information in CollectiveThunk::Buffer even if\n   // we may not generate an CollectiveThunk.\n   std::vector<CollectiveThunk::Buffer> buffers;\n-\n-  int64_t operand_count = inst->operand_count();\n   buffers.reserve(operand_count);\n \n   // Adds a source and destination buffers pair to `buffers`.\n-  auto add_buffer = [&](int64_t element_count, BufferAllocation::Slice src,\n-                        int64_t src_memory_space, BufferAllocation::Slice dst,\n-                        int64_t dst_memory_space) {\n-    buffers.push_back(\n-        CollectiveThunk::Buffer{/*element_count=*/element_count,\n-                                /*source_buffer=*/src,\n-                                /*destination_buffer=*/dst,\n-                                /*source_memory_space=*/src_memory_space,\n-                                /*destination_memory_space=*/dst_memory_space});\n+  auto add_buffer = [&](const HloInstruction* src, const HloInstruction* dst,\n+                        const ShapeIndex& dst_shape_index) -> absl::Status {\n+    const Shape& src_shape = src->shape();\n+    const Shape& dst_shape =\n+        ShapeUtil::GetSubshape(dst->shape(), dst_shape_index);\n+    TF_ASSIGN_OR_RETURN(auto src_slice, GetAllocationSliceForHlo(src));\n+    TF_ASSIGN_OR_RETURN(auto dst_slice,\n+                        GetAllocationSliceForHlo(dst, dst_shape_index));\n+\n+    buffers.push_back(CollectiveThunk::Buffer{\n+        /*element_count=*/ShapeUtil::ElementsIn(src_shape),\n+        /*source_buffer=*/src_slice,\n+        /*destination_buffer=*/dst_slice,\n+        /*source_memory_space=*/src_shape.layout().memory_space(),\n+        /*destination_memory_space=*/dst_shape.layout().memory_space()});\n+    return absl::OkStatus();\n   };\n \n   if (kind == Thunk::Kind::kAllGatherStart) {\n@@ -1726,56 +1732,28 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveThunk(\n     // multiple operands).\n     for (int64_t i = 0; i < operand_count; i++) {\n       ShapeIndex idx = operand_count > 1 ? ShapeIndex({1, i}) : ShapeIndex({1});\n-      const Shape& src_shape = inst->operand(i)->shape();\n-      const Shape& dst_shape = ShapeUtil::GetSubshape(inst->shape(), idx);\n-      TF_ASSIGN_OR_RETURN(auto src, GetAllocationSliceForHlo(inst->operand(i)));\n-      TF_ASSIGN_OR_RETURN(auto dst, GetAllocationSliceForHlo(inst, idx));\n-      add_buffer(ShapeUtil::ElementsIn(src_shape), src,\n-                 src_shape.layout().memory_space(), dst,\n-                 dst_shape.layout().memory_space());\n+      TF_RETURN_IF_ERROR(add_buffer(inst->operand(i), inst, idx));\n     }\n   } else if (kind == Thunk::Kind::kRaggedAllToAll) {\n     // RaggedAllToAll operation has 6 operands: input, output,\n     // input_offset, send_size, output_offset, recv_size. `output`\n     // operand is aliased with the instruction result. All other\n     // operands are not aliased.\n-    const Shape& input_shape = inst->operand(0)->shape();\n-    TF_ASSIGN_OR_RETURN(auto input_buffer,\n-                        GetAllocationSliceForHlo(inst->operand(0)));\n-    add_buffer(ShapeUtil::ElementsIn(input_shape), input_buffer,\n-               input_shape.layout().memory_space(), input_buffer,\n-               input_shape.layout().memory_space());\n-\n-    const Shape& output_shape = inst->operand(1)->shape();\n-    const Shape& result_shape = inst->shape();\n-    TF_ASSIGN_OR_RETURN(auto output_buffer,\n-                        GetAllocationSliceForHlo(inst->operand(1)));\n-    TF_ASSIGN_OR_RETURN(auto result_buffer, GetAllocationSliceForHlo(inst));\n-\n-    add_buffer(ShapeUtil::ElementsIn(result_shape), output_buffer,\n-               output_shape.layout().memory_space(), result_buffer,\n-               result_shape.layout().memory_space());\n+    TF_RETURN_IF_ERROR(\n+        add_buffer(inst->operand(0), inst->operand(0), ShapeIndex({})));\n+    TF_RETURN_IF_ERROR(add_buffer(inst->operand(1), inst, ShapeIndex({})));\n \n     for (int64_t i = 2; i < operand_count; i++) {\n-      const Shape& shape = inst->operand(i)->shape();\n-      TF_ASSIGN_OR_RETURN(auto slice,\n-                          GetAllocationSliceForHlo(inst->operand(i)));\n-      add_buffer(ShapeUtil::ElementsIn(shape), slice,\n-                 shape.layout().memory_space(), slice,\n-                 shape.layout().memory_space());\n+      TF_RETURN_IF_ERROR(\n+          add_buffer(inst->operand(i), inst->operand(i), ShapeIndex({})));\n     }\n   } else {\n     // For other operations simply zip operands with results.\n     for (int64_t i = 0; i < operand_count; i++) {\n       ShapeIndex idx =\n           inst->shape().IsTuple() ? ShapeIndex({i}) : ShapeIndex({});\n-      const Shape& src_shape = inst->operand(i)->shape();\n-      const Shape& dst_shape = ShapeUtil::GetSubshape(inst->shape(), idx);\n-      TF_ASSIGN_OR_RETURN(auto src, GetAllocationSliceForHlo(inst->operand(i)));\n-      TF_ASSIGN_OR_RETURN(auto dst, GetAllocationSliceForHlo(inst, idx));\n-      add_buffer(ShapeUtil::ElementsIn(src_shape), src,\n-                 src_shape.layout().memory_space(), dst,\n-                 dst_shape.layout().memory_space());\n+\n+      TF_RETURN_IF_ERROR(add_buffer(inst->operand(i), inst, idx));\n     }\n   }\n "
        }
    ],
    "stats": {
        "total": 74,
        "additions": 26,
        "deletions": 48
    }
}