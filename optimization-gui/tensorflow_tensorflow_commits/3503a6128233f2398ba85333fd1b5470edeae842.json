{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Combine metadata AllToAlls in RaggedAllToAllMultiHostDecomposer.\n\nInstead of performing four separate AllToAll operations, the metadata tensors are reshaped, concatenated, and then a single AllToAll is executed. The result is then sliced back into the individual metadata tensors. This reduces latency required to initiate separate collective operations.\n\nPiperOrigin-RevId: 822674605",
    "sha": "3503a6128233f2398ba85333fd1b5470edeae842",
    "files": [
        {
            "sha": "60dfba5464edcc646b94ad65eb0eea433bcab74d",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 38,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3503a6128233f2398ba85333fd1b5470edeae842/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3503a6128233f2398ba85333fd1b5470edeae842/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc?ref=3503a6128233f2398ba85333fd1b5470edeae842",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n+\n using hlo_query::NextChannelId;\n \n // Corrects the offsets in the local metadata to account for the number of input\n@@ -81,60 +82,90 @@ HloInstruction* CorrectOffsets(HloRaggedAllToAllInstruction* ragged_all_to_all,\n       /*rhs=*/input_offsets_offset));\n }\n \n-// Exchanges the metadata between the hosts and computes the intra-host\n+// Exchanges the metadata operands between the hosts and computes the intra-host\n // metadata.\n-//\n-// If `correct_offsets` is true, the offsets are corrected to account for the\n-// number of input rows in the combined ragged tensor. It's needed for\n-// `input_offsets`.\n-HloInstruction* GetIntraHostMetadata(\n+absl::InlinedVector<HloInstruction*, 4> GetIntraHostMetadata(\n     HloRaggedAllToAllInstruction* ragged_all_to_all,\n-    HloInstruction* metadata_operand, HloComputation* computation,\n-    absl::Span<ReplicaGroup const> replica_groups, int64_t num_hosts,\n-    int64_t num_devices_in_replica, bool correct_offsets) {\n+    HloComputation* computation, absl::Span<ReplicaGroup const> replica_groups,\n+    int64_t num_hosts, int64_t num_devices_in_replica) {\n   int64_t num_devices_in_replica_per_host = num_devices_in_replica / num_hosts;\n \n+  absl::InlinedVector<HloInstruction*, 4> metadata_operands;\n+  metadata_operands.reserve(4);\n+  for (int i = 2; i < 6; ++i) {\n+    metadata_operands.push_back(ragged_all_to_all->mutable_operand(i));\n+  }\n+\n+  Shape metadata_operand_shape = metadata_operands[0]->shape();\n+\n   int64_t num_updates_per_replica =\n-      metadata_operand->shape().dimensions(0) / num_devices_in_replica;\n+      metadata_operand_shape.dimensions(0) / num_devices_in_replica;\n \n   Shape new_metadata_shape = ShapeUtil::MakeShape(\n-      metadata_operand->shape().element_type(),\n+      metadata_operand_shape.element_type(),\n       {num_hosts, num_devices_in_replica_per_host, num_updates_per_replica});\n \n   Shape new_metadata_transposed_shape = ShapeUtil::MakeShape(\n-      metadata_operand->shape().element_type(),\n+      metadata_operand_shape.element_type(),\n       {num_devices_in_replica_per_host, num_hosts, num_updates_per_replica});\n \n-  HloInstruction* new_input_offsets = computation->AddInstruction(\n-      HloInstruction::CreateReshape(new_metadata_shape, metadata_operand));\n+  for (int64_t i = 0; i < metadata_operands.size(); ++i) {\n+    metadata_operands[i] =\n+        computation->AddInstruction(HloInstruction::CreateReshape(\n+            new_metadata_shape, metadata_operands[i]));\n+  }\n+\n+  Shape all_to_all_shape =\n+      ShapeUtil::MakeShape(metadata_operand_shape.element_type(),\n+                           {num_hosts, num_devices_in_replica_per_host,\n+                            4 * num_updates_per_replica});\n \n-  HloInstruction* new_local_metadata =\n+  HloInstruction* all_to_all_input =\n+      computation->AddInstruction(HloInstruction::CreateConcatenate(\n+          /*shape=*/all_to_all_shape,\n+          /*operands=*/metadata_operands,\n+          /*dimension=*/2));\n+\n+  HloInstruction* all_to_all =\n       computation->AddInstruction(HloInstruction::CreateAllToAll(\n-          /*shape=*/new_metadata_shape,\n-          /*operands=*/{new_input_offsets},\n+          /*shape=*/all_to_all_shape,\n+          /*operands=*/{all_to_all_input},\n           /*device_list=*/CollectiveDeviceList(replica_groups),\n           /*constrain_layout=*/false,\n           /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n               ? std::make_optional(NextChannelId(*computation->parent()))\n               : std::nullopt,\n           /*split_dimension=*/0));\n \n-  if (correct_offsets) {\n-    new_local_metadata =\n-        CorrectOffsets(ragged_all_to_all, new_local_metadata, computation);\n+  for (int i = 0; i < metadata_operands.size(); ++i) {\n+    metadata_operands[i] =\n+        computation->AddInstruction(HloInstruction::CreateSlice(\n+            /*shape=*/new_metadata_shape,\n+            /*operand=*/all_to_all,\n+            /*start_indices=*/{0, 0, i * num_updates_per_replica},\n+            /*limit_indices=*/\n+            {num_hosts, num_devices_in_replica_per_host,\n+             (i + 1) * num_updates_per_replica},\n+            /*strides=*/{1, 1, 1}));\n   }\n \n-  HloInstruction* new_local_metadata_transposed =\n-      computation->AddInstruction(HloInstruction::CreateTranspose(\n-          /*shape=*/new_metadata_transposed_shape,\n-          /*operand=*/new_local_metadata,\n-          /*dimensions=*/{1, 0, 2}));\n-\n-  HloInstruction* intra_host_metadata =\n-      computation->AddInstruction(HloInstruction::CreateReshape(\n-          metadata_operand->shape(), new_local_metadata_transposed));\n+  // Correct input offsets that need to be adjusted for the number of input\n+  // rows.\n+  metadata_operands[0] =\n+      CorrectOffsets(ragged_all_to_all, metadata_operands[0], computation);\n+\n+  for (int i = 0; i < metadata_operands.size(); ++i) {\n+    metadata_operands[i] =\n+        computation->AddInstruction(HloInstruction::CreateTranspose(\n+            /*shape=*/new_metadata_transposed_shape,\n+            /*operand=*/metadata_operands[i],\n+            /*dimensions=*/{1, 0, 2}));\n+    metadata_operands[i] =\n+        computation->AddInstruction(HloInstruction::CreateReshape(\n+            metadata_operand_shape, metadata_operands[i]));\n+  }\n \n-  return intra_host_metadata;\n+  return metadata_operands;\n }\n \n absl::StatusOr<bool> DecomposeRaggedAllToAll(\n@@ -219,8 +250,6 @@ absl::StatusOr<bool> DecomposeRaggedAllToAll(\n     }\n   }\n \n-  std::vector<HloInstruction*> intra_host_metadata;\n-\n   HloInstruction* input_operand = ragged_all_to_all->mutable_operand(0);\n \n   Shape new_input_shape = input_operand->shape();\n@@ -247,12 +276,10 @@ absl::StatusOr<bool> DecomposeRaggedAllToAll(\n           /*use_global_device_ids=*/\n           ragged_all_to_all->channel_id().has_value()));\n \n-  for (int i = 2; i < 6; ++i) {\n-    intra_host_metadata.push_back(GetIntraHostMetadata(\n-        ragged_all_to_all, ragged_all_to_all->mutable_operand(i), computation,\n-        inter_host_replica_groups, num_hosts, num_devices_in_replica,\n-        /*correct_offsets=*/i == 2));\n-  }\n+  absl::InlinedVector<HloInstruction*, 4> intra_host_metadata =\n+      GetIntraHostMetadata(ragged_all_to_all, computation,\n+                           inter_host_replica_groups, num_hosts,\n+                           num_devices_in_replica);\n \n   HloInstruction* new_ragged_all_to_all =\n       computation->AddInstruction(HloInstruction::CreateRaggedAllToAll("
        },
        {
            "sha": "e67d15fec95ddade01b103b68b210d05b0ac21c8",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3503a6128233f2398ba85333fd1b5470edeae842/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3503a6128233f2398ba85333fd1b5470edeae842/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc?ref=3503a6128233f2398ba85333fd1b5470edeae842",
            "patch": "@@ -62,7 +62,7 @@ ENTRY main {\n \n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n     // CHECK: all-gather{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n-    // CHECK-COUNT-4: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n+    // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n     // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}{{[}]}}\n   )\"));\n }\n@@ -96,7 +96,7 @@ ENTRY main {\n \n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n     // CHECK: all-gather{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n-    // CHECK-COUNT-4: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n+    // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n     // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}{{[}]}}\n   )\"));\n }\n@@ -152,7 +152,7 @@ ENTRY main {\n \n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n     // CHECK: all-gather{{.*}}, replica_groups={{[{]}}{0,8},{2,10},{4,12},{6,14},{1,9},{3,11},{5,13},{7,15}{{[}]}}\n-    // CHECK-COUNT-4: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{2,10},{4,12},{6,14},{1,9},{3,11},{5,13},{7,15}{{[}]}}\n+    // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{2,10},{4,12},{6,14},{1,9},{3,11},{5,13},{7,15}{{[}]}}\n     // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,2,4,6},{8,10,12,14},{1,3,5,7},{9,11,13,15}{{[}]}}\n   )\"));\n }"
        }
    ],
    "stats": {
        "total": 109,
        "additions": 68,
        "deletions": 41
    }
}