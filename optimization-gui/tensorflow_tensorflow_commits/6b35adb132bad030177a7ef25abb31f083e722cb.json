{
    "author": "basioli-k",
    "message": "[XLA:CPU] Remove reduce/reduce-window llvm options overrides.\n\nThe new emitters don't need this.\n\nPiperOrigin-RevId: 810630948",
    "sha": "6b35adb132bad030177a7ef25abb31f083e722cb",
    "files": [
        {
            "sha": "8a7e2cea4f1484c0a7f53516a5785b51006d019f",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b35adb132bad030177a7ef25abb31f083e722cb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b35adb132bad030177a7ef25abb31f083e722cb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=6b35adb132bad030177a7ef25abb31f083e722cb",
            "patch": "@@ -130,91 +130,6 @@ namespace xla::cpu {\n \n namespace {\n \n-bool ShouldDisableLoopUnrollingForReduce(const HloInstruction* instruction) {\n-  bool disable_loop_unrolling = true;\n-  auto* reduce = Cast<HloReduceInstruction>(instruction);\n-  auto reduce_dimensions = reduce->dimensions();\n-  // All inputs have the same shape.\n-  auto reduce_input_shape = reduce->inputs()[0]->shape();\n-  auto reduce_input_rank = reduce_input_shape.dimensions().size();\n-\n-  // If reduce happens over outer dimensions we turn on loop unrolling.\n-  for (auto it = reduce_dimensions.rbegin(); it != reduce_dimensions.rend();\n-       ++it) {\n-    if (*it != --reduce_input_rank) {\n-      disable_loop_unrolling = false;\n-      break;\n-    }\n-  }\n-\n-  return disable_loop_unrolling;\n-}\n-\n-bool ShouldDisableLoopUnrollingForReduceWindow(\n-    const HloInstruction* instruction,\n-    const TargetMachineFeatures& target_machine_features) {\n-  bool disable_loop_unrolling = true;\n-  auto* reduce_window = Cast<HloReduceWindowInstruction>(instruction);\n-\n-  auto max_simd_width_bytes = [&]() -> std::optional<int> {\n-    auto features = target_machine_features.get_target_feature_string();\n-    constexpr int kAvx512 = 512;\n-    constexpr int kAvx = 256;\n-    constexpr int kSse = 128;\n-    constexpr int kBitsInByte = 8;\n-    if (absl::StrContains(features, \"+avx512\")) {\n-      return kAvx512 / kBitsInByte;\n-    }\n-    if (absl::StrContains(features, \"+avx\")) {\n-      return kAvx / kBitsInByte;\n-    }\n-    if (absl::StrContains(features, \"+sse\")) {\n-      return kSse / kBitsInByte;\n-    }\n-    return std::nullopt;\n-  }();\n-\n-  std::vector<int64_t> strides;\n-  strides.reserve(reduce_window->window().dimensions_size());\n-\n-  for (const auto& dim : reduce_window->window().dimensions()) {\n-    strides.push_back(dim.stride());\n-  }\n-\n-  auto input_type = reduce_window->inputs()[0]->shape().element_type();\n-  // If the innermost stride is lesser than the size of the vectorization\n-  // for the given platform we turn on loop unrolling.\n-  if (max_simd_width_bytes.has_value() &&\n-      *max_simd_width_bytes >\n-          strides.back() * ShapeUtil::ByteSizeOfPrimitiveType(input_type)) {\n-    disable_loop_unrolling = false;\n-  }\n-\n-  return disable_loop_unrolling;\n-}\n-\n-absl::Status HandleReduceAndReduceWindowElementalKernelCompilationOptions(\n-    const HloInstruction* instruction, llvm::Module& llvm_module,\n-    const TargetMachineFeatures& target_machine_features) {\n-  bool disable_loop_unrolling = true;\n-\n-  if (instruction->opcode() == HloOpcode::kReduce) {\n-    disable_loop_unrolling = ShouldDisableLoopUnrollingForReduce(instruction);\n-  } else if (instruction->opcode() == HloOpcode::kReduceWindow) {\n-    disable_loop_unrolling = ShouldDisableLoopUnrollingForReduceWindow(\n-        instruction, target_machine_features);\n-  } else {\n-    return absl::InvalidArgumentError(\n-        absl::StrCat(\"Unsupported HLO instruction: \", instruction->ToString()));\n-  }\n-\n-  LlvmKernelOptions llvm_kernel_options;\n-  llvm_kernel_options.set_disable_loop_unrolling(disable_loop_unrolling);\n-  SetXlaCpuBackendOptions(llvm_module, llvm_kernel_options);\n-\n-  return absl::OkStatus();\n-}\n-\n absl::StatusOr<std::string> GetFusionFingerprint(\n     const HloFusionInstruction& fusion,\n     const BufferAssignment& buffer_assignment,\n@@ -895,17 +810,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitElementalKernelThunk(\n   kernels_.push_back(\n       {kernel_spec.name(), std::move(kernel_source).thread_safe_module()});\n \n-  // AOT compiled kernels get linked together, so we aren't allowed to change\n-  // module flags as that will break linking.\n-  if (!options_.is_aot_compilation &&\n-      (instruction->opcode() == HloOpcode::kReduce ||\n-       instruction->opcode() == HloOpcode::kReduceWindow)) {\n-    TF_RETURN_IF_ERROR(\n-        HandleReduceAndReduceWindowElementalKernelCompilationOptions(\n-            instruction, *kernels_.back().module.getModuleUnlocked(),\n-            target_machine_features_));\n-  }\n-\n   return MakeKernelThunkSequence(instruction, std::move(kernel_spec),\n                                  /*min_alignment=*/MinAlign());\n }"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 0,
        "deletions": 96
    }
}