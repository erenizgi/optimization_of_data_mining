{
    "author": "pschuh",
    "message": "Implement StreamExecutorGpuClient::ScheduleRemoteSend. This allows migrating\nCopyToRemoteDevice to CommonPjRtBuffer APIs.\n\nPiperOrigin-RevId: 819949965",
    "sha": "a6a11a6036917d7ddf85b702648ad93f881d165e",
    "files": [
        {
            "sha": "f1b765487b0a15612b05a55deee7b5c44aec03aa",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=a6a11a6036917d7ddf85b702648ad93f881d165e",
            "patch": "@@ -90,6 +90,7 @@ cc_library(\n         \"//xla/pjrt:pjrt_device_description\",\n         \"//xla/pjrt:pjrt_executable\",\n         \"//xla/pjrt:pjrt_stream_executor_client\",\n+        \"//xla/pjrt:raw_buffer\",\n         \"//xla/pjrt:stream_executor_executable\",\n         \"//xla/pjrt:stream_executor_executable_proto_cc\",\n         \"//xla/pjrt:utils\","
        },
        {
            "sha": "0102f23b3812294030d5499bf504549b27e7273a",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 100,
            "deletions": 74,
            "changes": 174,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=a6a11a6036917d7ddf85b702648ad93f881d165e",
            "patch": "@@ -80,6 +80,7 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_stream_executor_client.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_allocator_config.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n+#include \"xla/pjrt/raw_buffer.h\"\n #include \"xla/pjrt/se_raw_buffer.h\"\n #include \"xla/pjrt/tracked_device_buffer.h\"\n #include \"xla/pjrt/worker_thread.h\"\n@@ -100,6 +101,7 @@ limitations under the License.\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/concurrency/async_value.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/distributed_runtime/coordination/coordination_service_agent.h\"\n@@ -890,8 +892,12 @@ absl::Status StreamExecutorGpuClient::UpdateCompileOptionsInternal(\n   return absl::OkStatus();\n }\n \n-void StreamExecutorGpuClient::CopyToRemoteDevice(\n-    PjRtBuffer* buffer, absl::string_view serialized_descriptor,\n+void StreamExecutorGpuClient::ScheduleRemoteSend(\n+    PjRtMemorySpace* memory_space,\n+    tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,\n+    std::vector<tsl::RCReference<tsl::AsyncValue>> definition_events,\n+    tsl::RCReference<PjRtDeviceEventPromise> usage_event_promise,\n+    Future<std::string> serialized_descriptor,\n     PjRtBuffer::RemoteSendCallback on_done) {\n   // Get the default GpuCollectives instance.\n   absl::StatusOr<Collectives*> collectives =\n@@ -902,79 +908,99 @@ void StreamExecutorGpuClient::CopyToRemoteDevice(\n   gpu::GpuCollectives* gpu_collectives =\n       tsl::down_cast<gpu::GpuCollectives*>(*collectives);\n   if (gpu_collectives == nullptr) {\n-    on_done(absl::InternalError(\"Failed to get GPU collectives\"),\n-            /*sends_were_enqueued=*/false);\n-  }\n-\n-  // Parse the CliqueId;\n-  CliqueId clique_id(serialized_descriptor);\n-\n-  // Get the local device.\n-  absl::StatusOr<LocalDeviceState*> local_device =\n-      tensorflow::down_cast<PjRtStreamExecutorDevice*>(buffer->device())\n-          ->GetLocalDeviceState();\n-  if (!local_device.ok()) {\n-    on_done(local_device.status(), /*sends_were_enqueued=*/false);\n-  }\n-\n-  // Get the buffer's shape.\n-  absl::StatusOr<Shape> shape = buffer->HostShape();\n-  if (!shape.ok()) {\n-    on_done(shape.status(), /*sends_were_enqueued=*/false);\n+    auto error = absl::InternalError(\"Failed to get GPU collectives\");\n+    on_done(error, /*sends_were_enqueued=*/false);\n+    usage_event_promise->SetError(error);\n+    return;\n   }\n \n-  // Acquire a hold on the buffer.\n-  auto* handle = tensorflow::down_cast<PjRtStreamExecutorBuffer*>(buffer);\n-  PjRtStreamExecutorBuffer::ScopedHold hold = handle->GetBufferWithUsageHold();\n-\n-  auto send = [gpu_collectives, clique_id, on_done, mem = hold->device_memory(),\n-               local_device = *local_device, shape = *shape,\n-               dtype = buffer->element_type(),\n-               stream = (*local_device)->GetDeviceToDeviceStream()]() mutable {\n-    auto f = [&]() -> absl::Status {\n-      // Create a communicator.\n-      //\n-      // TODO(mwhittaker): The way we are constructing GpuCliqueKeys is a big\n-      // hack. This code doesn't know the GlobalDeviceId of the sending process.\n-      // Instead, we use two arbitrary GlobalDeviceIds. This works because\n-      // NcclCommunicators don't actually use the GlobalDeviceIds.  Instead,\n-      // they just need to the know the number of devices (2 in this case).\n-      gpu::GpuCliqueKey clique_key(\n-          /*devices=*/{GlobalDeviceId(0), GlobalDeviceId(1)},\n-          /*num_local_participants=*/1);\n-      CliqueIds clique_ids(clique_id);\n-      gpu::GpuCollectives::Device collectives_device(local_device->executor());\n-      std::vector<Collectives::DeviceRank> ranks = {\n-          Collectives::DeviceRank(&collectives_device, RankId(1))};\n-      gpu::GpuCollectives::Config config;\n-      TF_ASSIGN_OR_RETURN(\n-          std::vector<std::unique_ptr<Communicator>> communicators,\n-          gpu_collectives->CreateCommunicators(clique_key, clique_ids, ranks,\n-                                               config));\n-      CHECK_EQ(communicators.size(), 1);\n-      std::unique_ptr<Communicator> communicator = std::move(communicators[0]);\n-\n-      // Send data to the receiver.\n-      Future<> send_future = communicator->Send(\n-          mem->mem(), shape.element_type(), ShapeUtil::ElementsIn(shape),\n-          RankId(0), gpu::GpuCollectives::On(*stream));\n-      TF_RETURN_IF_ERROR(send_future.Await());\n-\n-      // Keep mem alive until the Send has finished executing. Note that\n-      // send_event is fulfilled when the send is enqueued, but not necessarily\n-      // executed.\n-      TF_RETURN_IF_ERROR(local_device->ThenRelease(stream, mem));\n-\n-      return absl::OkStatus();\n-    };\n+  BufferSequencingEventRef usage_event =\n+      BufferSequencingEvent::Create(this->thread_pool());\n \n-    if (absl::Status s = f(); !s.ok()) {\n-      on_done(s, /*sends_were_enqueued=*/false);\n-    } else {\n-      on_done(absl::OkStatus(), /*sends_were_enqueued=*/true);\n-    }\n-  };\n-  thread_pool()->Schedule(send);\n+  // Keep memory alive until the event is done.\n+  usage_event.AndThen([raw_buffer]() {});\n+\n+  serialized_descriptor.OnReady(\n+      [this, gpu_collectives = std::move(gpu_collectives),\n+       on_done = std::move(on_done),\n+       definition_events = std::move(definition_events),\n+       memory_space = memory_space, raw_buffer = std::move(raw_buffer),\n+       usage_event = usage_event](\n+          absl::StatusOr<std::string> serialized_descriptor) mutable {\n+        if (!serialized_descriptor.ok()) {\n+          on_done(serialized_descriptor.status(),\n+                  /*sends_were_enqueued=*/false);\n+          SetEventAsError(usage_event, serialized_descriptor.status());\n+        }\n+        auto events = absl::MakeSpan(definition_events);\n+        async_work_runner()->ScheduleWhenReady(\n+            events,\n+            [this, on_done = std::move(on_done),\n+             gpu_collectives = std::move(gpu_collectives),\n+             definition_events = std::move(definition_events),\n+             raw_buffer = std::move(raw_buffer), usage_event = usage_event,\n+             serialized_descriptor =\n+                 *std::move(serialized_descriptor)]() mutable {\n+              auto status = [&]() {\n+                for (const auto& event : definition_events) {\n+                  if (auto* status = event->GetErrorIfPresent()) {\n+                    return *status;\n+                  }\n+                }\n+                auto* local_device =\n+                    tensorflow::down_cast<PjRtStreamExecutorRawBuffer*>(\n+                        raw_buffer.get())\n+                        ->local_device();\n+                auto* stream = local_device->GetDeviceToDeviceStream();\n+                auto mem = tensorflow::down_cast<PjRtStreamExecutorRawBuffer*>(\n+                               raw_buffer.get())\n+                               ->device_buffer();\n+                CliqueId clique_id(serialized_descriptor);\n+\n+                // Create a communicator.\n+                //\n+                // TODO(mwhittaker): The way we are constructing GpuCliqueKeys\n+                // is a big hack. This code doesn't know the GlobalDeviceId of\n+                // the sending process. Instead, we use two arbitrary\n+                // GlobalDeviceIds. This works because NcclCommunicators don't\n+                // actually use the GlobalDeviceIds.  Instead, they just need to\n+                // the know the number of devices (2 in this case).\n+                gpu::GpuCliqueKey clique_key(\n+                    /*devices=*/{GlobalDeviceId(0), GlobalDeviceId(1)},\n+                    /*num_local_participants=*/1);\n+                CliqueIds clique_ids(clique_id);\n+                gpu::GpuCollectives::Device collectives_device(\n+                    local_device->executor());\n+                std::vector<Collectives::DeviceRank> ranks = {\n+                    Collectives::DeviceRank(&collectives_device, RankId(1))};\n+                gpu::GpuCollectives::Config config;\n+                TF_ASSIGN_OR_RETURN(\n+                    std::vector<std::unique_ptr<Communicator>> communicators,\n+                    gpu_collectives->CreateCommunicators(clique_key, clique_ids,\n+                                                         ranks, config));\n+                CHECK_EQ(communicators.size(), 1);\n+                std::unique_ptr<Communicator> communicator =\n+                    std::move(communicators[0]);\n+\n+                // Send data to the receiver.\n+                Future<> send_future = communicator->Send(\n+                    mem->mem(), xla::PrimitiveType::U8, mem->mem().size(),\n+                    RankId(0), gpu::GpuCollectives::On(*stream));\n+                TF_RETURN_IF_ERROR(send_future.Await());\n+\n+                TF_RETURN_IF_ERROR(\n+                    AllocateAndRecordEvent(usage_event, local_device, stream));\n+\n+                return absl::OkStatus();\n+              }();\n+              std::move(on_done)(status, /*sends_were_enqueued=*/status.ok());\n+              if (!status.ok()) {\n+                SetEventAsError(usage_event, status);\n+              }\n+            });\n+      });\n+  usage_event_promise->Set(\n+      tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(std::move(usage_event)));\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n@@ -1064,8 +1090,8 @@ StreamExecutorGpuClient::MakeCrossHostReceiveBuffers(\n \n       // Receive data from the sender.\n       Future<> recv_future = communicator->Recv(\n-          mem->mem(), shape.element_type(), ShapeUtil::ElementsIn(shape),\n-          RankId(1), gpu::GpuCollectives::On(*stream));\n+          mem->mem(), xla::PrimitiveType::U8, mem->mem().size(), RankId(1),\n+          gpu::GpuCollectives::On(*stream));\n       TF_RETURN_IF_ERROR(recv_future.Await());\n \n       // Keep mem alive until the Recv has finished executing. Note that"
        },
        {
            "sha": "4cb94b11835e28172033d5c561974afee3f505db",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.h",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h?ref=a6a11a6036917d7ddf85b702648ad93f881d165e",
            "patch": "@@ -144,9 +144,13 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n                                   int64_t offset,\n                                   int64_t transfer_size) override;\n \n-  void CopyToRemoteDevice(PjRtBuffer* buffer,\n-                          absl::string_view serialized_descriptor,\n-                          PjRtBuffer::RemoteSendCallback on_done) override;\n+  void ScheduleRemoteSend(\n+      PjRtMemorySpace* memory_space,\n+      tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,\n+      std::vector<tsl::RCReference<tsl::AsyncValue>> definition_events,\n+      tsl::RCReference<PjRtDeviceEventPromise> usage_event_promise,\n+      Future<std::string> serialized_descriptor,\n+      PjRtBuffer::RemoteSendCallback on_done) override;\n \n   absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n   MakeCrossHostReceiveBuffers(absl::Span<const Shape> shapes,"
        },
        {
            "sha": "0121661696cf904487b34dfe7dea722df27fb634",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=a6a11a6036917d7ddf85b702648ad93f881d165e",
            "patch": "@@ -1834,19 +1834,6 @@ PjRtStreamExecutorBuffer::CopyToMemorySpace(PjRtMemorySpace* dst_memory_space) {\n   return Unimplemented(\"CopyToMemorySpace is not supported\");\n }\n \n-void PjRtStreamExecutorBuffer::CopyToRemoteDevice(\n-    Future<std::string> serialized_descriptor, RemoteSendCallback on_done) {\n-  VLOG(3) << \"PjRtStreamExecutorBuffer::CopyToRemoteDevice\";\n-  auto desc = serialized_descriptor.Await();\n-  if (desc.ok()) {\n-    auto* se_client =\n-        tensorflow::down_cast<PjRtStreamExecutorClient*>(client());\n-    se_client->CopyToRemoteDevice(this, *desc, std::move(on_done));\n-  } else {\n-    on_done(desc.status(), /*sends_enqueued=*/false);\n-  }\n-}\n-\n Future<> PjRtStreamExecutorBuffer::GetReadyFuture() {\n   absl::InlinedVector<BufferSequencingEventRef, 2> definition_events;\n   Promise<> definition_promise;"
        },
        {
            "sha": "df0b50c6e99ac08920cf946f7c0569c2bcff0974",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.h",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6a11a6036917d7ddf85b702648ad93f881d165e/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h?ref=a6a11a6036917d7ddf85b702648ad93f881d165e",
            "patch": "@@ -637,9 +637,6 @@ class PjRtStreamExecutorBuffer : public CommonPjRtBufferImpl {\n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> CopyToMemorySpace(\n       PjRtMemorySpace* dst_memory_space) override;\n \n-  void CopyToRemoteDevice(Future<std::string> serialized_descriptor,\n-                          RemoteSendCallback on_done) override;\n-\n   Future<> GetReadyFuture() override;\n \n   bool IsOnCpu() const override;"
        }
    ],
    "stats": {
        "total": 201,
        "additions": 108,
        "deletions": 93
    }
}