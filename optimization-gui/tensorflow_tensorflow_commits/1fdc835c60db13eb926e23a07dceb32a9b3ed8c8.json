{
    "author": "ermilovmaxim",
    "message": "raise nccl channel limit and add blackwell nvlink bandwidth\n\nPiperOrigin-RevId: 831023473",
    "sha": "1fdc835c60db13eb926e23a07dceb32a9b3ed8c8",
    "files": [
        {
            "sha": "12f031b53d39a7f03468540bdf72c3ff7038fea4",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 39,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1fdc835c60db13eb926e23a07dceb32a9b3ed8c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1fdc835c60db13eb926e23a07dceb32a9b3ed8c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc?ref=1fdc835c60db13eb926e23a07dceb32a9b3ed8c8",
            "patch": "@@ -35,15 +35,8 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n-\n namespace {\n \n-// Different algorithms that can be used to perform the collective.\n-enum class CollectiveAlgo {\n-  RING = 0,\n-  TREE,\n-};\n-\n struct CudaBandwidthSettings {\n   // Table for max system bandwidths GB/s for using NCCL's low latency\n   // algorithm. This is used for intra-node estimate.\n@@ -82,15 +75,22 @@ struct CudaBandwidthSettings {\n \n   // Returns NVLink bw in GB/s\n   float GetNvlinkBw() const {\n-    return compute_capability.IsAtLeast(se::CudaComputeCapability::kHopper)\n-               ? kSm90NvlinkBandwidth\n-           : compute_capability.IsAtLeast(se::CudaComputeCapability::kAmpere)\n-               ? kSm80NvlinkBandwidth\n-           : compute_capability.IsAtLeast(se::CudaComputeCapability::kVolta)\n-               ? kSm70NvlinkBandwidth\n-           : compute_capability.IsAtLeast(se::CudaComputeCapability::kPascal)\n-               ? kSm60NvlinkBandwidth\n-               : kSm80NvlinkBandwidth;\n+    switch (compute_capability.major) {\n+      case se::CudaComputeCapability::kBlackwell:\n+        return kSm100NvlinkBandwidth;\n+      case se::CudaComputeCapability::kHopper:\n+        return kSm90NvlinkBandwidth;\n+      case se::CudaComputeCapability::kAmpere:\n+        return kSm80NvlinkBandwidth;\n+      case se::CudaComputeCapability::kVolta:\n+        return kSm70NvlinkBandwidth;\n+      case se::CudaComputeCapability::kPascal:\n+        return kSm60NvlinkBandwidth;\n+      default:\n+        LOG(WARNING) << \"NVLink bandwidth for \" << compute_capability.ToString()\n+                     << \"unknown. Assumes Blackwell.\";\n+        return kSm100NvlinkBandwidth;\n+    }\n   }\n \n   // Max bandwidth in GB/s for ring low latency 128 algorithm per channel on a\n@@ -106,12 +106,13 @@ struct CudaBandwidthSettings {\n   static constexpr double kSm70NvlinkBandwidth = 20.0;\n   static constexpr double kSm80NvlinkBandwidth = 20.0;\n   static constexpr double kSm90NvlinkBandwidth = 20.0;\n+  static constexpr double kSm100NvlinkBandwidth = 40.0;\n \n   // Discount factor for ring algorithm\n   static constexpr double kRingAlgorithmDiscountFactor = 0.92;\n \n   // Maximum number of channels allowed by NCCL\n-  static constexpr int64_t kMaxNumChannelsRing = 16;\n+  static constexpr int64_t kMaxNumChannelsRing = 32;\n \n   // ll128 is by default enabled for Volta, Ampere and Hopper, ll128 by default\n   // launches 640 threads.\n@@ -234,15 +235,9 @@ float GetMaxLowLatencyBandwidth(const BandwidthSettings& bandwidth_settings) {\n static constexpr absl::Duration kNcclKernelLaunchOverhead =\n     absl::Microseconds(5);\n \n-int64_t GetNcclMaxNumChannels(CollectiveAlgo algorithm) {\n-  int64_t max_nchannels = 0;\n-  switch (algorithm) {\n-      // Tree and Ring algos share the same max channel number.\n-    case CollectiveAlgo::RING:\n-    case CollectiveAlgo::TREE:\n-      max_nchannels = CudaBandwidthSettings::kMaxNumChannelsRing;\n-      break;\n-  }\n+int64_t GetNcclMaxNumChannels() {\n+  int64_t max_nchannels = CudaBandwidthSettings::kMaxNumChannelsRing;\n+\n   const char* env = std::getenv(\"NCCL_MAX_NCHANNELS\");\n   if (env != nullptr) {\n     int64_t max_nchannels_from_env;\n@@ -253,15 +248,8 @@ int64_t GetNcclMaxNumChannels(CollectiveAlgo algorithm) {\n   return max_nchannels;\n }\n \n-int64_t GetMinNumberOfChannels(CollectiveAlgo algorithm) {\n-  int64_t min_nchannels = 0;\n-  switch (algorithm) {\n-      // Tree and Ring algos share the same min channel number.\n-    case CollectiveAlgo::RING:\n-    case CollectiveAlgo::TREE:\n-      min_nchannels = 1;\n-      break;\n-  }\n+int64_t GetMinNumberOfChannels() {\n+  int64_t min_nchannels = 1;\n   const char* env = std::getenv(\"NCCL_MIN_NCHANNELS\");\n   if (env != nullptr) {\n     int64_t min_nchannels_from_env;\n@@ -306,10 +294,8 @@ absl::Duration ComputeAllreduceTimeImpl(\n   float bw_intra_node = GetMaxLowLatencyBandwidth(bandwidth_settings);\n   int64_t num_devices = cost_analysis->NumOfDevices(instr);\n \n-  int64_t min_nchannels =\n-      std::max(num_devices, GetMinNumberOfChannels(CollectiveAlgo::RING));\n-  int64_t num_channels =\n-      std::max(min_nchannels, GetNcclMaxNumChannels(CollectiveAlgo::RING));\n+  int64_t min_nchannels = std::max(num_devices, GetMinNumberOfChannels());\n+  int64_t num_channels = std::max(min_nchannels, GetNcclMaxNumChannels());\n   int64_t pcie_bandwidth_gbps =\n       gpu_device_info.pcie_bandwidth() / 1024 / 1024 / 1024;\n   int default_threads = (bw_intra_node * num_channels <= pcie_bandwidth_gbps)\n@@ -333,6 +319,10 @@ absl::Duration ComputeAllreduceTimeImpl(\n     VLOG(8) << \"Nvlink supports p2p communication, setting intra node \"\n                \"bandwidth to nvlink bw.\";\n     bw_intra_node = bandwidth_settings.GetNvlinkBw();\n+    num_channels =\n+        std::min(static_cast<int64_t>(\n+                     gpu_device_info.device_interconnect_info().active_links),\n+                 num_channels);\n   } else {\n     VLOG(8) << \"Nvlink doesn't support p2p communication. Model will \"\n                \"continue using default system bandwidth.\";"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 29,
        "deletions": 39
    }
}