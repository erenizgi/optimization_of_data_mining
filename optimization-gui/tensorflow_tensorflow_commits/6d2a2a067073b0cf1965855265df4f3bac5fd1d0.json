{
    "author": "Tixxx",
    "message": "PR #34196: [NVIDIA GPU] Fix a deadlock when doing comm split\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34196\n\nüìù Summary of Changes\nIf participant group of a collective is empty, doing split can sometimes cause deadlocks since we dont know what are the participating ranks. Instead it should proceed to a regular comm init.\n\nüéØ Justification\nA target case would be if there are multiple communicators cached from different modules that use different device groups.\nWe rely one a rendezvous to return early if a communicator has been cached. Since participant groups are part of the key, there will be inconsistent cache hits among ranks leading to some ranks go to split but other won't.  ncclCommSplit requires all ranks to be call the api otherwise it will hang.\nüöÄ Kind of Contribution\nPlease remove what does not apply: üêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nNA\nüß™ Unit Tests:\ncan only be tested with execution test\n\nüß™ Execution Tests:\nadded\n\nCopybara import of the project:\n\n--\n4e61773e4bc99ef8671d8acada954917643897cb by TJ Xu <tjx@nvidia.com>:\n\nFix a deadlock when doing comm split\n\n--\n11c103570c8f1126e3e57208b2e091458f62fc8f by TJ Xu <tjx@nvidia.com>:\n\nadded execution test\n\n--\n13848529b145c13e061a97e4e5cda910a50640c4 by TJ Xu <tjx@nvidia.com>:\n\nchange test to take input param's element count\n\n--\neef95c1c4b94577050a252baa7e361ce0c14da4d by TJ Xu <tjx@nvidia.com>:\n\nadded vlogs and tracing for device sync before split\n\n--\nadf545721d0d36308e9e39bc86655d92de1613d7 by TJ Xu <tjx@nvidia.com>:\n\nskip the test if less than 4 gpus\n\n--\n9421fbb5159e90d4e245dcfa2427d6bbb9e18b87 by TJ Xu <tjx@nvidia.com>:\n\nreduce input size of test\n\nMerging this change closes #34196\n\nPiperOrigin-RevId: 844802220",
    "sha": "6d2a2a067073b0cf1965855265df4f3bac5fd1d0",
    "files": [
        {
            "sha": "3eca048ba751756adfbb7080948daddc8d1bbf29",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_key.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc?ref=6d2a2a067073b0cf1965855265df4f3bac5fd1d0",
            "patch": "@@ -86,6 +86,11 @@ bool GpuCliqueKey::is_p2p() const { return is_p2p_; }\n \n GlobalDeviceId GpuCliqueKey::root_device() const { return root_device_; }\n \n+std::vector<std::vector<GlobalDeviceId>> GpuCliqueKey::ParticipantGroups()\n+    const {\n+  return participant_groups_;\n+};\n+\n bool GpuCliqueKey::IsSubsetOf(const CliqueKey& other) const {\n   auto* other_gpu = tsl::down_cast<const GpuCliqueKey*>(&other);\n   if (other_gpu == nullptr) {"
        },
        {
            "sha": "9099fd9befd897e7bb1c95e924d20c3fd36c4a63",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_key.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.h?ref=6d2a2a067073b0cf1965855265df4f3bac5fd1d0",
            "patch": "@@ -62,6 +62,8 @@ class GpuCliqueKey : public CliqueKey {\n \n   CollectiveStreamId stream_id() const;\n \n+  std::vector<std::vector<GlobalDeviceId>> ParticipantGroups() const;\n+\n   // Device generating the unique id for this key\n   GlobalDeviceId root_device() const;\n "
        },
        {
            "sha": "696c8925e8ac821e31f52b7d26b9717843096f0b",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_cliques.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc?ref=6d2a2a067073b0cf1965855265df4f3bac5fd1d0",
            "patch": "@@ -492,6 +492,17 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n   GpuCollectives::DeviceRank device_rank = {&gpu_device, rank};\n   RankPair rank_pair = {parent_rank, device_rank};\n \n+  // Synchronize the device to make sure no other collectives are\n+  // running before we do the split.\n+  {\n+    tsl::profiler::TraceMe trace(\"SynchronizeAllActivityBeforeSplit\");\n+    if (!device->SynchronizeAllActivity()) {\n+      return Internal(\n+          \"Failed to synchronize GPU before splitting communicators.\");\n+    }\n+    VLOG(3) << \"Synchronized device before splitting\";\n+  }\n+\n   // Current approach for communicator splitting works because of XLAs SPMD\n   // programming model where all collective operations have replica groups that\n   // include all ranks. This property guarantees that we'll split each\n@@ -718,10 +729,16 @@ absl::StatusOr<std::shared_ptr<LockableGpuClique::Lock>> AcquireGpuClique(\n \n   if (enable_nccl_comm_splitting) {\n     for (auto& [acquired_clique_key, acquired_clique] : acquired_cliques) {\n-      if (clique_key.IsSubsetOf(acquired_clique_key)) {\n+      // If the participant group is empty, we won't know if there are other\n+      // ranks involved in the split. Proceed to normal initialization.\n+      if (clique_key.IsSubsetOf(acquired_clique_key) &&\n+          !clique_key.ParticipantGroups().empty()) {\n         return InitializeGpuClique(collectives, device, run_id, clique_key,\n                                    acquired_clique, num_local_participants,\n                                    rank, config);\n+      } else if (clique_key.ParticipantGroups().empty()) {\n+        LOG(WARNING) << \"Found empty participant groups.\"\n+                     << \" Skip splitting communicators.\";\n       }\n     }\n   }"
        },
        {
            "sha": "0d68205cd6e6eb9a986cdfb7c52b75bc7ef7c397",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d2a2a067073b0cf1965855265df4f3bac5fd1d0/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=6d2a2a067073b0cf1965855265df4f3bac5fd1d0",
            "patch": "@@ -2889,5 +2889,96 @@ INSTANTIATE_TEST_SUITE_P(\n       return absl::StrCat(GetAsyncTestName(std::get<0>(info.param)), \"_\",\n                           std::get<1>(info.param) ? \"one_shot\" : \"nccl\");\n     });\n+\n+TEST_F(CollectiveOpsTestE2E, MultipleModuleDifferentDeviceGroupsShouldRun) {\n+  const absl::string_view kModuleStr_1 = R\"(\n+  HloModule test\n+\n+  apply_op {\n+    x = f32[] parameter(0)\n+    y = f32[] parameter(1)\n+    ROOT apply_op = f32[] add(x, y)\n+  }\n+\n+  ENTRY test_computation {\n+    param_0 = f32[8] parameter(0)\n+    ROOT all-reduce = f32[8] all-reduce(param_0), to_apply=apply_op, replica_groups={{0,1}}\n+  }\n+  )\";\n+  const absl::string_view kModuleStr_2 = R\"(\n+  HloModule test\n+\n+  apply_op {\n+    x = f32[] parameter(0)\n+    y = f32[] parameter(1)\n+    ROOT apply_op = f32[] add(x, y)\n+  }\n+\n+  ENTRY test_computation {\n+    param_0 = f32[8] parameter(0)\n+    all-reduce.1 = f32[8] all-reduce(param_0), to_apply=apply_op, replica_groups={{0,1}, {2,3}}\n+    all-reduce.2 = f32[8] all-reduce(all-reduce.1), to_apply=apply_op, replica_groups={{0,1}, {2,3}}\n+    all-reduce.3 = f32[8] all-reduce(all-reduce.2), to_apply=apply_op, replica_groups={{0,1}, {2,3}}\n+    ROOT all-reduce.4 = f32[8] all-reduce(all-reduce.3), to_apply=apply_op, replica_groups={{0,1,2,3}}\n+  }\n+  )\";\n+\n+  const int64_t kNumReplicas_1 = 2;\n+  const int64_t kNumReplicas_2 = 4;\n+  if (hlo_runner_->device_count() < kNumReplicas_2) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas_2 << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n+  }\n+\n+  HloModuleConfig config_1 =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas_1);\n+  HloModuleConfig config_2 =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas_2);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module_1,\n+                          ParseAndReturnVerifiedModule(kModuleStr_1, config_1));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module_2,\n+                          ParseAndReturnVerifiedModule(kModuleStr_2, config_2));\n+\n+  int64_t num_elements_1 = ShapeUtil::ElementsIn(\n+      module_1->entry_computation()->parameter_instructions()[0]->shape());\n+\n+  int64_t num_elements_2 = ShapeUtil::ElementsIn(\n+      module_2->entry_computation()->parameter_instructions()[0]->shape());\n+\n+  Array<float> input1_1({num_elements_1}), input1_2({num_elements_1});\n+  input1_1.FillRandom(1.0f, 10.0f, /*seed=*/0);\n+  input1_2.FillRandom(1.0f, 10.0f, /*seed=*/1);\n+\n+  Literal input_literal1_1 = LiteralUtil::CreateFromArray(input1_1);\n+  Literal input_literal1_2 = LiteralUtil::CreateFromArray(input1_2);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ExecutionResult execution_result_1,\n+      ExecuteReplicated(std::move(module_1),\n+                        std::vector<std::vector<Literal*>>{\n+                            {&input_literal1_1}, {&input_literal1_2}}));\n+\n+  Array<float> input2_1({num_elements_2}), input2_2({num_elements_2}),\n+      input2_3({num_elements_2}), input2_4({num_elements_2});\n+  input2_1.FillRandom(1.0f, 10.0f, /*seed=*/0);\n+  input2_2.FillRandom(1.0f, 10.0f, /*seed=*/1);\n+  input2_3.FillRandom(1.0f, 10.0f, /*seed=*/2);\n+  input2_4.FillRandom(1.0f, 10.0f, /*seed=*/3);\n+\n+  Literal input_literal2_1 = LiteralUtil::CreateFromArray(input2_1);\n+  Literal input_literal2_2 = LiteralUtil::CreateFromArray(input2_2);\n+  Literal input_literal2_3 = LiteralUtil::CreateFromArray(input2_3);\n+  Literal input_literal2_4 = LiteralUtil::CreateFromArray(input2_4);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ExecutionResult execution_result_2,\n+      ExecuteReplicated(std::move(module_2), std::vector<std::vector<Literal*>>{\n+                                                 {&input_literal2_1},\n+                                                 {&input_literal2_2},\n+                                                 {&input_literal2_3},\n+                                                 {&input_literal2_4}}));\n+}\n }  // namespace\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 117,
        "additions": 116,
        "deletions": 1
    }
}