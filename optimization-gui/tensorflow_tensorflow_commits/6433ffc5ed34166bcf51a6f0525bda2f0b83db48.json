{
    "author": "ermilovmaxim",
    "message": "[XLA] port __xla_gpu_assert to ffi and add tests\n\nPiperOrigin-RevId: 808828632",
    "sha": "6433ffc5ed34166bcf51a6f0525bda2f0b83db48",
    "files": [
        {
            "sha": "91f4a9f85e1d0efe56869501efd0722ba95ead63",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=6433ffc5ed34166bcf51a6f0525bda2f0b83db48",
            "patch": "@@ -2781,21 +2781,18 @@ tsl_gpu_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/ffi\",\n+        \"//xla/ffi:ffi_api\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:custom_call_status\",\n         \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:platform\",\n-        \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n-        \"//xla/stream_executor:stream_finder\",\n+        \"//xla/tsl/platform:errors\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n     alwayslink = 1,\n )"
        },
        {
            "sha": "ded15dc1a383f910528ee8f49d5f3f414f83e666",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=6433ffc5ed34166bcf51a6f0525bda2f0b83db48",
            "patch": "@@ -854,8 +854,8 @@ absl::Status RunOptimizationPasses(\n         auto created = Cast<HloCustomCallInstruction>(\n             inst->parent()->AddInstruction(HloInstruction::CreateCustomCall(\n                 ShapeUtil::MakeTokenShape(), {inst}, kXlaGpuAssertCustomCallTag,\n-                \"Buffers have different size at runtime\",\n-                API_VERSION_STATUS_RETURNING)));\n+                \"{error_msg = \\\"Buffers have different size at runtime\\\"}\",\n+                API_VERSION_TYPED_FFI)));\n         created->set_custom_call_has_side_effect(true);\n       };\n       break;"
        },
        {
            "sha": "a95681f5c2d1e9cca31368dcc2afcf949c1226cb",
            "filename": "third_party/xla/xla/service/gpu/runtime_intrinsics.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 32,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fruntime_intrinsics.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fruntime_intrinsics.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fruntime_intrinsics.cc?ref=6433ffc5ed34166bcf51a6f0525bda2f0b83db48",
            "patch": "@@ -23,20 +23,17 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/strings/ascii.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/ffi/ffi.h\"\n+#include \"xla/ffi/ffi_api.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/platform.h\"\n-#include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/stream_executor/stream_finder.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n \n@@ -47,23 +44,18 @@ std::string GetGpuPlatformName() {\n       PlatformUtil::CanonicalPlatformName(\"gpu\").value());\n }\n \n-absl::Status AssertOnGpu(void* stream_handle, void* buffer,\n-                         absl::string_view error_msg) {\n-  TF_ASSIGN_OR_RETURN(\n-      se::Platform * platform,\n-      se::PlatformManager::PlatformWithName(GetGpuPlatformName()));\n-  TF_ASSIGN_OR_RETURN(se::Stream * stream,\n-                      stream_executor::FindStream(platform, stream_handle));\n+absl::Status AssertionCustomCall(\n+    se::Stream* stream, ffi::Buffer<PRED> buffer, absl::string_view error_msg,\n+    xla::ffi::Result<xla::ffi::Buffer<xla::TOKEN>> res) {\n   if (!stream) {\n-    return Internal(\"Stream not found for: %p\", stream_handle);\n+    return Internal(\"Stream is nullptr.\");\n   }\n \n   int8_t expected = false;\n   int64_t byte_size = sizeof(int8_t);\n   CHECK_EQ(byte_size, ShapeUtil::ByteSizeOfPrimitiveType(PrimitiveType::PRED));\n-  TF_RETURN_IF_ERROR(stream->Memcpy(\n-      &expected, se::DeviceMemoryBase{buffer, static_cast<uint64_t>(byte_size)},\n-      byte_size));\n+  TF_RETURN_IF_ERROR(\n+      stream->Memcpy(&expected, buffer.device_memory(), byte_size));\n   TF_RETURN_IF_ERROR(stream->BlockHostUntilDone());\n   if (!static_cast<bool>(expected)) {\n     return Internal(\"%s\", error_msg);\n@@ -72,18 +64,6 @@ absl::Status AssertOnGpu(void* stream_handle, void* buffer,\n   return absl::OkStatus();\n }\n \n-void AssertionCustomCall(void* stream_handle, void** buffers,\n-                         const char* opaque, int opaque_len,\n-                         XlaCustomCallStatus* status) {\n-  absl::Status s =\n-      AssertOnGpu(stream_handle, buffers[0],\n-                  absl::string_view{opaque, static_cast<uint64_t>(opaque_len)});\n-  if (!s.ok()) {\n-    auto msg = s.message();\n-    XlaCustomCallStatusSetFailure(status, msg.data(), msg.size());\n-  }\n-}\n-\n void NopReturnTokenCustomCall(void* stream_handle, void** buffers,\n                               const char* opaque, int opaque_len,\n                               XlaCustomCallStatus* status) {\n@@ -92,9 +72,15 @@ void NopReturnTokenCustomCall(void* stream_handle, void** buffers,\n \n }  // namespace\n \n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\n-    std::string(kXlaGpuAssertCustomCallTag), AssertionCustomCall,\n-    GetGpuPlatformName());\n+XLA_FFI_DEFINE_HANDLER(kXlaGpuAssertCustomCall, AssertionCustomCall,\n+                       ffi::Ffi::Bind()\n+                           .Ctx<ffi::Stream>()\n+                           .Arg<ffi::Buffer<xla::PRED>>()\n+                           .Attr<absl::string_view>(\"error_msg\")\n+                           .Ret<xla::ffi::Buffer<xla::TOKEN>>());\n+\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), kXlaGpuAssertCustomCallTag,\n+                         GetGpuPlatformName(), kXlaGpuAssertCustomCall);\n \n // This allows measuring exported HLOs where kOutfeed and kSendDone has been\n // replaced with NopReturnToken. In that case the runtime of the original"
        },
        {
            "sha": "b5d8bd54a65c8be0b1a17f6b3a7ef49dfa55cfeb",
            "filename": "third_party/xla/xla/service/gpu/runtime_intrinsics_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fruntime_intrinsics_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6433ffc5ed34166bcf51a6f0525bda2f0b83db48/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fruntime_intrinsics_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fruntime_intrinsics_test.cc?ref=6433ffc5ed34166bcf51a6f0525bda2f0b83db48",
            "patch": "@@ -46,6 +46,42 @@ ENTRY e {\n   EXPECT_TRUE(Run(std::move(module), /*run_hlo_passes=*/false));\n }\n \n+TEST_F(RuntimeIntrinsicsTest, AssertionCustomCall) {\n+  constexpr absl::string_view kHloText = R\"(\n+HloModule m\n+\n+ENTRY e {\n+  constant = pred[] constant(true)\n+  ROOT nop_return_token = token[] custom-call(constant), backend_config=\"{error_msg = \\\"1\\\"}\", custom_call_target=\"__xla_gpu_assert\", custom_call_has_side_effect=true, api_version=API_VERSION_TYPED_FFI\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(kHloText));\n+\n+  // The parameter of the NopReturnToken is not removed.\n+  EXPECT_EQ(module->entry_computation()->instruction_count(), 2);\n+  // Can run.\n+  EXPECT_TRUE(Run(std::move(module), /*run_hlo_passes=*/false));\n+}\n+\n+TEST_F(RuntimeIntrinsicsTest, AssertionCustomCallFalse) {\n+  constexpr absl::string_view kHloText = R\"(\n+HloModule m\n+\n+ENTRY e {\n+  constant = pred[] constant(false)\n+  ROOT nop_return_token = token[] custom-call(constant), backend_config=\"{error_msg = \\\"1\\\"}\", custom_call_target=\"__xla_gpu_assert\", custom_call_has_side_effect=true, api_version=API_VERSION_TYPED_FFI\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(kHloText));\n+\n+  // The parameter of the NopReturnToken is not removed.\n+  EXPECT_EQ(module->entry_computation()->instruction_count(), 2);\n+  // Can run.\n+  EXPECT_FALSE(Run(std::move(module), /*run_hlo_passes=*/false));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 99,
        "additions": 59,
        "deletions": 40
    }
}