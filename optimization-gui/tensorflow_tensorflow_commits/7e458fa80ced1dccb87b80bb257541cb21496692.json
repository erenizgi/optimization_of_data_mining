{
    "author": "chsigg",
    "message": "[xla:gpu] Add shape size verification to `triton_xla.extract` and `triton_xla.insert` ops.\n\nThe verification now ensures that static dimensions in the result/source tensor type match the corresponding values provided in the `getMixedSizes()` operands. New test cases are added to `invalid.mlir` to cover these checks. Additionally, `tt.func` is replaced with `func.func` in the test file.\n\nPiperOrigin-RevId: 797203419",
    "sha": "7e458fa80ced1dccb87b80bb257541cb21496692",
    "files": [
        {
            "sha": "db6c1ef7ca58def28a8869d7a7f424d845c626c3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/tests/invalid.mlir",
            "status": "modified",
            "additions": 29,
            "deletions": 13,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7e458fa80ced1dccb87b80bb257541cb21496692/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7e458fa80ced1dccb87b80bb257541cb21496692/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir?ref=7e458fa80ced1dccb87b80bb257541cb21496692",
            "patch": "@@ -1,47 +1,63 @@\n // RUN: xla-opt --split-input-file --verify-diagnostics %s\n \n-tt.func @extract_0d(%arg0: !tt.ptr<bf16>) {\n+func.func @extract_0d(%arg0: !tt.ptr<bf16>) {\n   // expected-error @+1 {{cannot extract a 0-d tensor}}\n   %0 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [][][] : tensor<bf16>\n-  tt.return\n+  return\n }\n \n // -----\n \n-tt.func @insert_0d(%arg0: tensor<bf16>, %arg1: !tt.ptr<bf16>) {\n+func.func @insert_0d(%arg0: tensor<bf16>, %arg1: !tt.ptr<bf16>) {\n   // expected-error @+1 {{cannot insert a 0-d tensor}}\n   triton_xla.insert %arg0 into %arg1 as memref<bf16, #triton_xla.layout<[]>> [][][] : tensor<bf16>\n-  tt.return\n+  return\n }\n \n // -----\n \n-tt.func @extract_wrong_shape(%arg0: !tt.ptr<bf16>) {\n+func.func @extract_wrong_rank(%arg0: !tt.ptr<bf16>) {\n   // expected-error @+1 {{shape attribute has a wrong size}}\n-  %1 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n-  tt.return\n+  %0 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+  return\n }\n \n // -----\n \n-tt.func @extract_wrong_layout(%arg0: !tt.ptr<bf16>) {\n+func.func @extract_wrong_shape(%arg0: !tt.ptr<bf16>) {\n+  // expected-error @+1 {{shape size must match operand size}}\n+  %0 = triton_xla.extract from %arg0 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n+  return\n+}\n+\n+// -----\n+\n+func.func @extract_wrong_layout(%arg0: !tt.ptr<bf16>) {\n   // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n   %0 = triton_xla.extract from %arg0 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n-  tt.return\n+  return\n }\n \n // -----\n \n-tt.func @insert_wrong_shape(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n+func.func @insert_wrong_rank(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n   // expected-error @+1 {{shape attribute has a wrong size}}\n   triton_xla.insert %arg0 into %arg1 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n-  tt.return\n+  return\n+}\n+\n+// -----\n+\n+func.func @insert_wrong_shape(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n+  // expected-error @+1 {{shape size must match operand size}}\n+  triton_xla.insert %arg0 into %arg1 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n+  return\n }\n \n // -----\n \n-tt.func @insert_wrong_layout(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n+func.func @insert_wrong_layout(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n   // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n   triton_xla.insert %arg0 into %arg1 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n-  tt.return\n+  return\n }"
        },
        {
            "sha": "243c5f944a89c2b77a35c1462ad8ac10f73fabef",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 2,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7e458fa80ced1dccb87b80bb257541cb21496692/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7e458fa80ced1dccb87b80bb257541cb21496692/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc?ref=7e458fa80ced1dccb87b80bb257541cb21496692",
            "patch": "@@ -18,12 +18,14 @@ limitations under the License.\n #include <cassert>\n #include <cstdint>\n \n+#include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/TypeSwitch.h\"  // IWYU pragma: keep\n #include \"llvm/Support/LogicalResult.h\"\n #include \"mlir/Dialect/Utils/StaticValueUtils.h\"\n #include \"mlir/IR/Builders.h\"  // IWYU pragma: keep\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/DialectImplementation.h\"  // IWYU pragma: keep\n #include \"mlir/IR/MLIRContext.h\"  // IWYU pragma: keep\n #include \"mlir/IR/OperationSupport.h\"\n@@ -78,6 +80,21 @@ void printAsMemRefType(OpAsmPrinter& printer, Operation* op, PointerType type,\n                              memory_space);\n }\n \n+LogicalResult verifyShapeMatchesSizes(Operation* op,\n+                                      ArrayRef<int64_t> shape_sizes,\n+                                      ArrayRef<OpFoldResult> operand_sizes) {\n+  for (auto [shape_size, operand_size] :\n+       llvm::zip_equal(shape_sizes, operand_sizes)) {\n+    auto attr =\n+        dyn_cast_if_present<IntegerAttr>(dyn_cast<Attribute>(operand_size));\n+    if (attr && shape_size != ShapedType::kDynamic &&\n+        shape_size != attr.getValue()) {\n+      return op->emitError(\"shape size must match operand size\");\n+    }\n+  }\n+  return success();\n+}\n+\n //===----------------------------------------------------------------------===//\n // ExtractOp\n //===----------------------------------------------------------------------===//\n@@ -101,7 +118,8 @@ LogicalResult ExtractOp::verify() {\n   if (getType().getElementType() != getSrc().getType().getPointeeType()) {\n     return emitError(\"src pointee type must match result element type\");\n   }\n-  return success();\n+  return verifyShapeMatchesSizes(getOperation(), getType().getShape(),\n+                                 getMixedSizes());\n }\n \n void ExtractOp::build(OpBuilder& b, OperationState& result,\n@@ -177,7 +195,8 @@ LogicalResult InsertOp::verify() {\n       getDst().getType().getPointeeType()) {\n     return emitError(\"dst pointee type must match src element type\");\n   }\n-  return success();\n+  return verifyShapeMatchesSizes(getOperation(), getSrc().getType().getShape(),\n+                                 getMixedSizes());\n }\n \n void InsertOp::build(OpBuilder& b, OperationState& result, Value src, Value dst,"
        }
    ],
    "stats": {
        "total": 65,
        "additions": 50,
        "deletions": 15
    }
}