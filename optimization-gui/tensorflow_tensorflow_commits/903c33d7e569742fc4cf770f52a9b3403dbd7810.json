{
    "author": "sergachev",
    "message": "PR #30818: [GPU] Layout assignment: optimize bitcast-converts on different type widths.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30818\n\nCopybara import of the project:\n\n--\nff6db1f9404af200c8fb887ae496651bdde4a83f by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Layout assignment: optimize bitcast-converts on different type widths.\n\nMerging this change closes #30818\n\nPiperOrigin-RevId: 802034015",
    "sha": "903c33d7e569742fc4cf770f52a9b3403dbd7810",
    "files": [
        {
            "sha": "223ebfa1758d566ceb03e25ee1db3a17e1c8173e",
            "filename": "third_party/xla/xla/layout_util.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Flayout_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Flayout_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Flayout_util.cc?ref=903c33d7e569742fc4cf770f52a9b3403dbd7810",
            "patch": "@@ -506,6 +506,21 @@ absl::Status LayoutUtil::CopyLayoutBetweenShapes(const Shape& src, Shape* dst) {\n   return ret;\n }\n \n+Layout LayoutUtil::MoveDimToMinor(const Layout& layout, const int64_t dim) {\n+  if (dim == MinorToMajor(layout).front()) {\n+    return layout;\n+  }\n+  Layout result = layout;\n+  result.clear_minor_to_major();\n+  result.add_minor_to_major(dim);\n+  for (int64_t current_dim : MinorToMajor(layout)) {\n+    if (current_dim != dim) {\n+      result.add_minor_to_major(current_dim);\n+    }\n+  }\n+  return result;\n+}\n+\n /*static*/ int64_t LayoutUtil::LinearIndex(const Shape& shape,\n                                            absl::Span<const int64_t> indices) {\n   CHECK(shape.IsArray());"
        },
        {
            "sha": "e5f807c0286eb34853f9587543ba91eba94e34de",
            "filename": "third_party/xla/xla/layout_util.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Flayout_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Flayout_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Flayout_util.h?ref=903c33d7e569742fc4cf770f52a9b3403dbd7810",
            "patch": "@@ -235,6 +235,10 @@ class LayoutUtil {\n   // layout `layout` as the most major dimension.\n   static Layout MoveDimToMajor(const Layout& layout, int64_t dim);\n \n+  // Constructs a new layout by making the given dimension in the given\n+  // layout the minor most.\n+  static Layout MoveDimToMinor(const Layout& layout, int64_t dim);\n+\n   // Returns the linearized index of the cell at the given indices. The unit\n   // of the offset is in elements of the shape.\n   //"
        },
        {
            "sha": "3107b88330824c0bc9bd9bcf0ad50cc20a41935c",
            "filename": "third_party/xla/xla/layout_util_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Flayout_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Flayout_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Flayout_util_test.cc?ref=903c33d7e569742fc4cf770f52a9b3403dbd7810",
            "patch": "@@ -438,6 +438,13 @@ TEST_F(LayoutUtilTest, MoveDimToMajor) {\n   EXPECT_EQ(new_layout, LayoutUtil::MakeLayout({2, 0, 1}));\n }\n \n+TEST_F(LayoutUtilTest, MoveDimToMinor) {\n+  const Layout layout = LayoutUtil::MakeLayout({2, 0, 3, 1});\n+  EXPECT_EQ(LayoutUtil::MoveDimToMinor(layout, 2), layout);\n+  EXPECT_EQ(LayoutUtil::MoveDimToMinor(layout, 3),\n+            LayoutUtil::MakeLayout({3, 2, 0, 1}));\n+}\n+\n TEST_F(LayoutUtilTest, StridesIsMajorToMinor) {\n   std::vector<int64_t> byte_strides = {3960, 440, 44, 4};\n   EXPECT_TRUE(LayoutUtil::ByteStridesIsMajorToMinor("
        },
        {
            "sha": "b24f5470c118132d08b1f06aab2cdb07970e76b9",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc?ref=903c33d7e569742fc4cf770f52a9b3403dbd7810",
            "patch": "@@ -563,6 +563,22 @@ absl::Status GpuLayoutAssignment::AddBackendConstraints(\n           auto indices_buffer,\n           points_to_analysis_->GetBufferDefinedAt(instruction, {1}));\n       TF_RETURN_IF_ERROR(SetBufferLayout(default_layout, *indices_buffer));\n+    } else if (HloPredicateIsOp<HloOpcode::kBitcastConvert>(instruction)) {\n+      Shape operand_shape = instruction->operand(0)->shape();\n+      Shape output_shape = instruction->shape();\n+      // Make the added or removed dimension the minor most to give the\n+      // operation a chance to become a no-op (bitcast).\n+      if (operand_shape.dimensions().size() >\n+          output_shape.dimensions().size()) {\n+        *operand_shape.mutable_layout() = LayoutUtil::MoveDimToMinor(\n+            operand_shape.layout(), operand_shape.dimensions().size() - 1);\n+        TF_RETURN_IF_ERROR(SetOperandLayout(operand_shape, instruction, 0));\n+      } else if (operand_shape.dimensions().size() <\n+                 output_shape.dimensions().size()) {\n+        *output_shape.mutable_layout() = LayoutUtil::MoveDimToMinor(\n+            output_shape.layout(), output_shape.dimensions().size() - 1);\n+        TF_RETURN_IF_ERROR(SetInstructionLayout(output_shape, instruction));\n+      }\n     } else if (HloPredicateIsOp<HloOpcode::kTriangularSolve>(instruction)) {\n       // TODO(phawkins): Ideally we would relax this constraint. What we\n       // actually want is that:"
        },
        {
            "sha": "f23e6bcb322572ff40cf22e0bda51a3d27f8e319",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_test.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 1,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/903c33d7e569742fc4cf770f52a9b3403dbd7810/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc?ref=903c33d7e569742fc4cf770f52a9b3403dbd7810",
            "patch": "@@ -53,7 +53,6 @@ namespace {\n \n namespace m = ::xla::match;\n using ::testing::NotNull;\n-using ::tsl::testing::IsOkAndHolds;\n \n class LayoutAssignmentTest : public HloTestBase {\n  public:\n@@ -458,6 +457,51 @@ TEST_F(LayoutAssignmentTest, TopKLayout) {\n                       .WithShape(F32, {6, 2048}, {1, 0}))));\n }\n \n+TEST_F(LayoutAssignmentTest,\n+       BitcastConvertFromNarrowerTypeGetsOptimalInputLayout) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+e {\n+  a = s4[3,5,2]{0,1,2:E(4)} parameter(0)\n+  b = s8[3,5]{0,1} bitcast-convert(a)\n+})\"));\n+\n+  ComputationLayout computation_layout(\n+      module->entry_computation()->ComputeProgramShape(),\n+      /*ignore_layouts=*/false);\n+  GpuLayoutAssignment layout_assignment(\n+      &computation_layout, GetGpuComputeCapability(), GetDnnVersion(),\n+      GetDeviceDescription());\n+  EXPECT_THAT(layout_assignment.Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(m::BitcastConvert(\n+          m::Copy(m::Parameter()).WithShape(S4, {3, 5, 2}, {2, 0, 1}))));\n+}\n+\n+TEST_F(LayoutAssignmentTest,\n+       BitcastConvertToNarrowerTypeGetsOptimalOutputLayout) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+e {\n+  a = s8[3,5] parameter(0)\n+  b = s4[3,5,2]{0,1,2} bitcast-convert(a)\n+})\"));\n+\n+  ComputationLayout computation_layout(\n+      module->entry_computation()->ComputeProgramShape(),\n+      /*ignore_layouts=*/false);\n+  GpuLayoutAssignment layout_assignment(\n+      &computation_layout, GetGpuComputeCapability(), GetDnnVersion(),\n+      GetDeviceDescription());\n+  EXPECT_THAT(layout_assignment.Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::Copy(m::BitcastConvert(m::Parameter())\n+                                     .WithShape(S4, {3, 5, 2}, {2, 0, 1}))));\n+}\n+\n TEST_F(LayoutAssignmentTest, FftLayout) {\n   const char* hlo_text = R\"(\n   HloModule Fft_module"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 87,
        "deletions": 1
    }
}