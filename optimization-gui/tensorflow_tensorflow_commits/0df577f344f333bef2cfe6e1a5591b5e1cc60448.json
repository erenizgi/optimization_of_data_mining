{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Remove unused ExecuteReplicated that require OpaqueExecutable.\n\nPiperOrigin-RevId: 839297753",
    "sha": "0df577f344f333bef2cfe6e1a5591b5e1cc60448",
    "files": [
        {
            "sha": "b950513875436ed9cf376abecfdfb84db1ecb3ef",
            "filename": "third_party/xla/xla/hlo/testlib/hlo_hardware_independent_test_base.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Fhlo%2Ftestlib%2Fhlo_hardware_independent_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Fhlo%2Ftestlib%2Fhlo_hardware_independent_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftestlib%2Fhlo_hardware_independent_test_base.cc?ref=0df577f344f333bef2cfe6e1a5591b5e1cc60448",
            "patch": "@@ -37,8 +37,8 @@ limitations under the License.\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/hlo/testlib/filecheck.h\""
        },
        {
            "sha": "0e6696977cdf0463c69ad51610481ee102689fcf",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 20,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=0df577f344f333bef2cfe6e1a5591b5e1cc60448",
            "patch": "@@ -2792,27 +2792,23 @@ TEST_F(CollectiveOpsTestE2E, OptimizedSubByteAllGatherOnDim0OutputIsCorrect) {\n     e {\n       a = s4[2,4]{1,0:E(4)} constant({{0,1,2,3},{4,5,5,4}})\n       b = s4[4,4]{1,0:E(4)} all-gather(a), dimensions={0}\n-    })\"));\n+    })\",\n+                                                       kNumReplicas));\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto executable, hlo_runner_->CreateExecutable(\n-                                               std::move(unoptimized_module),\n-                                               /*run_hlo_passes=*/true));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(const HloModule* const module,\n-                          hlo_runner_->HloModuleFromWrapped(executable.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(unoptimized_module)));\n \n+  const HloModule* module = execution_result.optimized_module;\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n               GmockMatch(m::Bitcast(m::AllGatherDone().WithShape(S8, {4, 2}))));\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::vector<Literal> result,\n-                          ExecuteReplicated(executable.get(), kNumReplicas));\n-\n   const Literal expected_result =\n       LiteralUtil::CreateR2<s4>({{s4(0), s4(1), s4(2), s4(3)},\n                                  {s4(4), s4(5), s4(5), s4(4)},\n                                  {s4(0), s4(1), s4(2), s4(3)},\n                                  {s4(4), s4(5), s4(5), s4(4)}});\n \n+  const std::vector<Literal>& result = execution_result.results;\n   ASSERT_EQ(result.size(), kNumReplicas);\n   for (int i = 0; i < kNumReplicas; ++i) {\n     EXPECT_TRUE(LiteralTestUtil::Equal(expected_result, result[i]))\n@@ -2833,30 +2829,26 @@ TEST_F(CollectiveOpsTestE2E, OptimizedSubByteAllGatherOnDim1OutputIsCorrect) {\n     e {\n       a = s4[4,2]{1,0:E(4)} constant({{0,1},{2,3},{4,5},{5,4}})\n       b = s4[4,4]{1,0:E(4)} all-gather(a), dimensions={1}\n-    })\"));\n+    })\",\n+                                                       kNumReplicas));\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto executable, hlo_runner_->CreateExecutable(\n-                                               std::move(unoptimized_module),\n-                                               /*run_hlo_passes=*/true));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(const HloModule* const module,\n-                          hlo_runner_->HloModuleFromWrapped(executable.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(unoptimized_module)));\n \n+  const HloModule* module = execution_result.optimized_module;\n   const HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, GmockMatch(m::Fusion(\n                         m::Bitcast(m::AllGatherDone().WithShape(S8, {2, 4})))));\n   EXPECT_THAT(root->fused_expression_root(),\n               GmockMatch(m::Transpose(m::Parameter())));\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::vector<Literal> result,\n-                          ExecuteReplicated(executable.get(), kNumReplicas));\n-\n   const Literal expected_result =\n       LiteralUtil::CreateR2<s4>({{s4(0), s4(1), s4(0), s4(1)},\n                                  {s4(2), s4(3), s4(2), s4(3)},\n                                  {s4(4), s4(5), s4(4), s4(5)},\n                                  {s4(5), s4(4), s4(5), s4(4)}});\n \n+  const std::vector<Literal>& result = execution_result.results;\n   ASSERT_EQ(result.size(), kNumReplicas);\n   for (int i = 0; i < kNumReplicas; ++i) {\n     EXPECT_TRUE(LiteralTestUtil::Equal(expected_result, result[i]))"
        },
        {
            "sha": "a729cd3abe5aea4fc3f277269debe547f39891b1",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 75,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc?ref=0df577f344f333bef2cfe6e1a5591b5e1cc60448",
            "patch": "@@ -116,67 +116,6 @@ CollectiveOpsE2ETestBase::CollectiveOpsE2ETestBase() {\n       reference_platform, /*intra_op_parallelism_threads=*/0);\n }\n \n-absl::StatusOr<std::vector<Literal>>\n-CollectiveOpsE2ETestBase::ExecuteReplicated(\n-    absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n-    absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n-    absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n-    const int64_t num_replicas, const bool run_hlo_passes,\n-    DeviceAssignment* const device_assignment) {\n-  // TODO(b/441865120): Use designated initializers this once XLA moves to\n-  // C++20.\n-  HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_replicas = num_replicas;\n-  options.run_hlo_passes = run_hlo_passes;\n-  options.use_threads = true;\n-\n-  return hlo_runner_->ExecuteReplicated(\n-      std::move(executable_provider), std::move(argument_count_provider),\n-      std::move(argument_provider), std::move(options), device_assignment);\n-}\n-\n-absl::StatusOr<std::vector<Literal>>\n-CollectiveOpsE2ETestBase::ExecuteReplicated(\n-    std::unique_ptr<HloModule> module,\n-    const absl::Span<const Literal* const> arguments,\n-    const int64_t num_replicas, DeviceAssignment* const device_assignment,\n-    const bool run_hlo_passes, const bool use_threads) {\n-  // TODO(b/441865120): Use designated initializers this once XLA moves to\n-  // C++20.\n-  HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_replicas = num_replicas;\n-  options.arguments = {arguments.begin(), arguments.end()};\n-  options.run_hlo_passes = run_hlo_passes;\n-  options.use_threads = use_threads;\n-\n-  return hlo_runner_->ExecuteReplicated(std::move(module), std::move(options),\n-                                        device_assignment);\n-}\n-\n-absl::StatusOr<std::vector<Literal>>\n-CollectiveOpsE2ETestBase::ExecuteReplicated(\n-    std::unique_ptr<HloModule> module,\n-    const std::vector<std::vector<Literal*>> arguments,\n-    DeviceAssignment* const device_assignment, const int64_t num_replicas,\n-    const bool run_hlo_passes) {\n-  CHECK(num_replicas > 0 && \"expect at least one replica\");\n-  CHECK(num_replicas == arguments.size() &&\n-        \"expect arguments for each replica\");\n-  int64_t argument_count = arguments.front().size();\n-  TF_ASSIGN_OR_RETURN(\n-      const std::unique_ptr<OpaqueExecutable> executable,\n-      hlo_runner_->CreateExecutable(std::move(module), run_hlo_passes));\n-  return ExecuteReplicated(\n-      /*executable_provider=*/[&](int64_t) { return executable.get(); },\n-      /*argument_count_provider=*/[&](int64_t) { return argument_count; },\n-      /*argument_provider=*/\n-      [&](int64_t replica_idx, int64_t argument_idx) -> const Literal* {\n-        return arguments[replica_idx][argument_idx];\n-      },\n-      num_replicas, /*run_hlo_passes=*/run_hlo_passes,\n-      /*device_assignment=*/device_assignment);\n-}\n-\n absl::StatusOr<CollectiveOpsE2ETestBase::ExecutionResult>\n CollectiveOpsE2ETestBase::ExecuteReplicated(std::unique_ptr<HloModule> module) {\n   return ExecuteReplicated(std::move(module),\n@@ -224,9 +163,16 @@ CollectiveOpsE2ETestBase::ExecuteReplicated(\n       execution_result.optimized_module,\n       hlo_runner_->HloModuleFromWrapped(execution_result.executable.get()));\n \n+  // TODO(b/441865120): Use designated initializers this once XLA moves to\n+  // C++20.\n+  HloRunnerInterface::ReplicatedExecuteOptions options;\n+  options.num_replicas = num_devices;\n+  options.run_hlo_passes = run_hlo_passes;\n+  options.use_threads = true;\n+\n   TF_ASSIGN_OR_RETURN(\n       execution_result.results,\n-      ExecuteReplicated(\n+      hlo_runner_->ExecuteReplicated(\n           /*executable_provider=*/\n           [&](int64_t) { return execution_result.executable.get(); },\n           /*argument_count_provider=*/\n@@ -235,21 +181,10 @@ CollectiveOpsE2ETestBase::ExecuteReplicated(\n           [&](int64_t replica_idx, int64_t argument_idx) -> const Literal* {\n             return arguments[replica_idx][argument_idx];\n           },\n-          /*num_replicas=*/num_devices,\n-          /*run_hlo_passes=*/run_hlo_passes,\n+          std::move(options),\n           /*device_assignment=*/&device_assignment));\n-  return execution_result;\n-}\n \n-absl::StatusOr<std::vector<Literal>>\n-CollectiveOpsE2ETestBase::ExecuteReplicated(OpaqueExecutable* executable,\n-                                            int64_t num_replicas) {\n-  DeviceAssignment device_assignment = MakeDeviceAssignment(num_replicas);\n-  return ExecuteReplicated(\n-      /*executable_provider*/ [&](int64_t) { return executable; },\n-      /*argument_count_provider*/ [](int64_t) { return 0; },\n-      /*argument_provider*/ [](int64_t, int64_t) { return nullptr; },\n-      num_replicas, /*run_hlo_passes=*/false, &device_assignment);\n+  return execution_result;\n }\n \n DebugOptions CollectiveOpsWithFlagsBase::GetDebugOptionsForTest() const {"
        },
        {
            "sha": "d6d0f9dc141cd49ab25c2a1d6cc611389781ab91",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.h",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h?ref=0df577f344f333bef2cfe6e1a5591b5e1cc60448",
            "patch": "@@ -54,27 +54,6 @@ class CollectiveOpsE2ETestBase : public HloHardwareIndependentTestBase {\n     const HloModule* optimized_module;\n   };\n \n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n-      absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n-      absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n-      int64_t num_replicas, bool run_hlo_passes,\n-      DeviceAssignment* device_assignment);\n-\n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      std::unique_ptr<HloModule> module,\n-      absl::Span<const Literal* const> arguments, int64_t num_replicas,\n-      DeviceAssignment* vice_assignment, bool run_hlo_passes, bool use_threads);\n-\n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      std::unique_ptr<HloModule> module,\n-      std::vector<std::vector<Literal*>> arguments,\n-      DeviceAssignment* device_assignment, int64_t num_replicas,\n-      bool run_hlo_passes);\n-\n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      OpaqueExecutable* executable, int64_t num_replicas);\n-\n   absl::StatusOr<ExecutionResult> ExecuteReplicated(\n       std::unique_ptr<HloModule> module);\n "
        },
        {
            "sha": "caa071646ca01457a9797890c147ccce5114a8b4",
            "filename": "third_party/xla/xla/tests/collective_ops_sharded_unsharded_e2e_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 22,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_sharded_unsharded_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0df577f344f333bef2cfe6e1a5591b5e1cc60448/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_sharded_unsharded_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_sharded_unsharded_e2e_test.cc?ref=0df577f344f333bef2cfe6e1a5591b5e1cc60448",
            "patch": "@@ -61,13 +61,15 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsE2ETestBase {\n                    << \" available)\";\n     }\n \n-    TF_ASSERT_OK_AND_ASSIGN(std::vector<Literal> ref_results,\n+    TF_ASSERT_OK_AND_ASSIGN(ExecutionResult ref_execution_result,\n                             ExecuteUnsharded(hlo_text));\n+    const std::vector<Literal>& ref_results = ref_execution_result.results;\n     ASSERT_EQ(ref_results.size(), 1);\n \n     TF_ASSERT_OK_AND_ASSIGN(\n-        std::vector<Literal> results,\n+        ExecutionResult execution_result,\n         ExecuteSharded(hlo_text, num_partitions, enable_enzyme_comms_opt));\n+    const std::vector<Literal>& results = execution_result.results;\n     ASSERT_EQ(results.size(), num_partitions);\n \n     ErrorSpec error_spec{1e-4, 1e-4};\n@@ -77,7 +79,7 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsE2ETestBase {\n \n  private:\n   // Execute the unsharded case.\n-  absl::StatusOr<std::vector<Literal>> ExecuteUnsharded(\n+  absl::StatusOr<ExecutionResult> ExecuteUnsharded(\n       const std::string& hlo_text) {\n     // Create the unsharded reference case by removing the sharding metadata\n     // from the HLO string.\n@@ -90,9 +92,12 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsE2ETestBase {\n     DebugOptions ref_opts = GetDebugOptionsForTest();\n     ref_opts.set_xla_gpu_enable_triton_gemm(false);\n     ref_config.set_debug_options(ref_opts);\n-    ref_config.set_num_partitions(1);\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<VerifiedHloModule> ref_module,\n                         ParseAndReturnVerifiedModule(hlo_text_ref, ref_config));\n+\n+    ref_module->mutable_config().set_replica_count(1);\n+    ref_module->mutable_config().set_num_partitions(1);\n+\n     const int64_t num_params =\n         ref_module->entry_computation()->num_parameters();\n \n@@ -102,17 +107,11 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsE2ETestBase {\n       ref_fake_ptrs[i] = &fake_args[i];\n     }\n \n-    DeviceAssignment ref_assn(/*replica_count=*/1,\n-                              /*computation_count=*/1);\n-    ref_assn(0, 0) = 0;\n-    return ExecuteReplicated(std::move(ref_module), ref_fake_ptrs,\n-                             /*num_replicas=*/1, &ref_assn,\n-                             /*run_hlo_passes=*/true,\n-                             /*use_threads=*/true);\n+    return ExecuteReplicated(std::move(ref_module), ref_fake_ptrs);\n   }\n \n   // Execute the sharded case.\n-  absl::StatusOr<std::vector<Literal>> ExecuteSharded(\n+  absl::StatusOr<ExecutionResult> ExecuteSharded(\n       const std::string& hlo_text, int64_t num_partitions,\n       bool enable_enzyme_comms_opt = false) {\n     HloModuleConfig config = GetModuleConfigForTest();\n@@ -182,21 +181,14 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsE2ETestBase {\n       }\n     }\n \n-    DeviceAssignment assn(/*replica_count=*/1,\n-                          /*computation_count=*/num_partitions);\n-    for (int64_t i = 0; i < num_partitions; ++i) {\n-      assn(0, i) = i;\n-    }\n-    return ExecuteReplicated(std::move(module), fake_ptrs, &assn,\n-                             num_partitions,\n-                             /*run_hlo_passes=*/true);\n+    return ExecuteReplicated(std::move(module), fake_ptrs);\n   }\n \n   // Slice the unsharded reference results and compare to the sharded case.\n   void CompareShardedUnsharded(const std::string& hlo_text,\n                                int64_t num_partitions,\n-                               std::vector<Literal>& ref_results,\n-                               std::vector<Literal>& results,\n+                               const std::vector<Literal>& ref_results,\n+                               const std::vector<Literal>& results,\n                                ErrorSpec& error_spec) {\n     HloModuleConfig config = GetModuleConfigForTest();\n     DebugOptions opts = GetDebugOptionsForTest();"
        }
    ],
    "stats": {
        "total": 176,
        "additions": 37,
        "deletions": 139
    }
}