{
    "author": "Moerafaat",
    "message": "[XLA:GPU/WS] Adding `xla_gpu_experimental_enable_triton_warp_specialization` flag. This is currently only used to decorate the contracting dimension loop for dot fusions going through Triton with `tt.warp_specialize`, enabling the feature in Triton.\n\nPiperOrigin-RevId: 819765526",
    "sha": "6969cce01e53661bbbc66eed74b0666b45bfad79",
    "files": [
        {
            "sha": "b91661d35515d13d2cee4378956949520cd356eb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6969cce01e53661bbbc66eed74b0666b45bfad79/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6969cce01e53661bbbc66eed74b0666b45bfad79/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=6969cce01e53661bbbc66eed74b0666b45bfad79",
            "patch": "@@ -1008,6 +1008,14 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n       /*lowerBound=*/MakeIndex(b, 0),\n       /*upperBound=*/MakeIndex(b, loop_iteration_count),\n       /*step=*/MakeIndex(b, 1), accumulator);\n+\n+  if (fusion->GetModule()\n+          ->config()\n+          .debug_options()\n+          .xla_gpu_experimental_enable_triton_warp_specialization()) {\n+    for_op->setAttr(\"tt.warp_specialize\", b.getBoolAttr(true));\n+  }\n+\n   {  // Loop body.\n     mlir::OpBuilder::InsertionGuard g(b);\n     b.setInsertionPointToStart(for_op.getBody());"
        },
        {
            "sha": "9c114d3980621758f5eca974047fab35ee8d9aff",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6969cce01e53661bbbc66eed74b0666b45bfad79/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6969cce01e53661bbbc66eed74b0666b45bfad79/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=6969cce01e53661bbbc66eed74b0666b45bfad79",
            "patch": "@@ -447,6 +447,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_unsupported_crash_on_hlo_pass_noop_change(false);\n   opts.set_xla_gpu_experimental_enable_split_k_rewrite(false);\n   opts.set_xla_gpu_experimental_enable_triton_tma(false);\n+  opts.set_xla_gpu_experimental_enable_triton_warp_specialization(false);\n   opts.set_xla_gpu_experimental_enable_command_buffer_on_thunks(true);\n   opts.set_xla_detect_unstable_reductions(\n       DebugOptions::UNSTABLE_REDUCTION_DETECTION_MODE_NONE);\n@@ -2525,6 +2526,13 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n           &DebugOptions::set_xla_gpu_experimental_enable_triton_tma),\n       debug_options->xla_gpu_experimental_enable_triton_tma(),\n       \"Enable Triton's TMA loads/stores for arguments where applicable.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_experimental_enable_triton_warp_specialization\",\n+      bool_setter_for(\n+          &DebugOptions::\n+              set_xla_gpu_experimental_enable_triton_warp_specialization),\n+      debug_options->xla_gpu_experimental_enable_triton_warp_specialization(),\n+      \"Enable Triton's auto warp specialization feature where applicable.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_experimental_enable_command_buffer_on_thunks\",\n       bool_setter_for("
        },
        {
            "sha": "4a2692b2b903af8ed10baa6c3edfad40abb4d1b1",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6969cce01e53661bbbc66eed74b0666b45bfad79/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6969cce01e53661bbbc66eed74b0666b45bfad79/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=6969cce01e53661bbbc66eed74b0666b45bfad79",
            "patch": "@@ -657,9 +657,12 @@ message DebugOptions {\n   optional bool xla_gpu_experimental_enable_triton_heroless_priority_fusion =\n       340;\n \n-  // When possible, XLA will use Triton's experimental TMA feature.\n+  // When possible, XLA will use Triton's TMA loads/stores.\n   optional bool xla_gpu_experimental_enable_triton_tma = 355;\n \n+  // When possible, XLA will use Triton's auto warp specialization feature.\n+  optional bool xla_gpu_experimental_enable_triton_warp_specialization = 421;\n+\n   // For sub-byte dot operands, layout them along contracting dimensions.\n   optional bool xla_gpu_experimental_pack_dot_operands_along_k_dimension = 362;\n \n@@ -1358,7 +1361,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 421\n+  // Next id: 422\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 23,
        "additions": 21,
        "deletions": 2
    }
}