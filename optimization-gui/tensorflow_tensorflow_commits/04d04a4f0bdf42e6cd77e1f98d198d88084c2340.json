{
    "author": "pifon2a",
    "message": "[XLA:GPU] Add documentation that describes the path from HLO to Thunks.\n\nPiperOrigin-RevId: 811313117",
    "sha": "04d04a4f0bdf42e6cd77e1f98d198d88084c2340",
    "files": [
        {
            "sha": "e5b747ba5d2fc137b62e9c5e4847ef38afb3d269",
            "filename": "third_party/xla/docs/_toc.yaml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04d04a4f0bdf42e6cd77e1f98d198d88084c2340/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04d04a4f0bdf42e6cd77e1f98d198d88084c2340/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2F_toc.yaml?ref=04d04a4f0bdf42e6cd77e1f98d198d88084c2340",
            "patch": "@@ -12,6 +12,8 @@ toc:\n     path: /xla/architecture\n   - title: XLA terminology\n     path: /xla/terminology\n+  - title: HLO to Thunks\n+    path: /xla/hlo_to_thunks\n - title: Developer details\n   # These should be in alphabetical order unless otherwise noted.\n   section:"
        },
        {
            "sha": "c7dccc4de4fc73089dca70c13d12728e8005f5d7",
            "filename": "third_party/xla/docs/hlo_to_thunks.md",
            "status": "added",
            "additions": 160,
            "deletions": 0,
            "changes": 160,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04d04a4f0bdf42e6cd77e1f98d198d88084c2340/third_party%2Fxla%2Fdocs%2Fhlo_to_thunks.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04d04a4f0bdf42e6cd77e1f98d198d88084c2340/third_party%2Fxla%2Fdocs%2Fhlo_to_thunks.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fhlo_to_thunks.md?ref=04d04a4f0bdf42e6cd77e1f98d198d88084c2340",
            "patch": "@@ -0,0 +1,160 @@\n+# From HLO to Thunks\n+\n+## Pre-optimization HLO\n+We start with pre-optimization HLO. Pre-optimization HLO does not contain ops\n+that are considered internal to XLA, e.g. `fusion` or `bitcast`. Ops don't have\n+a layout at this stage, or if they have, it will be ignored. Pre-optimization\n+HLO is usually produced by higher level frameworks like Tensorflow and JAX.\n+When using the XLA flag `-xla_dump_to`, the pre-optimization HLO is dumped to a\n+file with file name suffix “before_optimizations.txt”.\n+\n+## Optimize HLO Module\n+\n+The XLA:GPU pipeline will turn the pre-optimization HLO into optimized HLO by\n+running a sequence of passes. The passes can be grouped together semantically\n+and run in the following order:\n+\n+### Sharding related passes\n+\n+[Shardy Partitioner](https://openxla.org/shardy/overview) or SPMD sharding.\n+\n+### Optimization passes.\n+\n+This can include both legalization passes and simplification passes.\n+\n+### Collective optimization passes.\n+\n+Similar to **Optimization passes**, but focuses on collective ops.\n+\n+### Layout assignment passes\n+\n+Each HLO op is assigned a layout which is a part of the instruction shape. The\n+layout controls how the tensor is laid out physically in memory.\n+\n+Example of a shape with layout:\n+\n+```\n+f32[10,20,30]{2,0,1}\n+```\n+\n+After the element type, there are the logical dimensions of the shape, followed\n+by the layout permutation in minor to major order. In this example, the most\n+minor dimension is 30, the second most minor dimension is 10, and the major\n+dimension is 20.\n+\n+The goal of the layout assignment is to minimize the number of physical\n+transpositions that are required using a greedy strategy. It starts off with\n+certain layout constraints (e.g. CuDNN/cuBLAS libraries expect consecutive\n+dimensions) and propagates layout “down”, and then “up” the HLO graph. At the\n+end of layout propagation, some instructions may have conflicting layouts, one\n+propagated from an operand, one propagated from a user. To resolve this\n+conflict, a `copy` HLO instruction is inserted that changes the layout from the\n+operand layout to the instruction layout.\n+\n+### Layout normalization passes\n+\n+Given that it is somewhat difficult to figure out the physical shape, layout\n+normalization attempts to rewrite the shape such that it uses the default layout\n+`{rank-1, rank-2, …, 0}`. In the example above, the normalized shape would be\n+`f32[20,10,30]{2,1,0}`. Copy ops that change layouts are rewritten to a\n+combination of `transpose` + `bitcast`. Given that currently we cannot normalize\n+all ops, there are still some ops that may have non-default layouts, most\n+notably `gather` and `dot`. At the boundaries between normalized ops and\n+non-normalized ops there will be `bitcast` ops that represent a transpose, i.e.\n+a transpose with a layout assigned that makes it a no-op physically.\n+\n+Layout normalization also makes some implicit transposes explicit which is\n+important because codegen can handle explicit transposes with a dedicated\n+emitter. For example, a reshape is technically allowed to have a different\n+physical layout between operand and result (e.g. due to different rank). The\n+`ReshapeDecomposer` pass that runs as part of the layout normalization passes\n+turns a reshape into a sequence of `transpose`, reshape `bitcast` and\n+`transpose`.\n+\n+### Post layout assignment optimization passes\n+\n+The most important passes here are Triton fusions (GEMM fusions +\n+Softmax/Layernorm fusions) or rewrites to library calls. But also Autotuning\n+runs in this step, where we pick the best algorithm for convolutions or dots, or\n+the best tiling for dots handled by the legacy Triton GEMM emitter, or whether\n+we should use Triton or Cublas for a certain dot fusion.\n+\n+### Fusion passes\n+\n+The two main passes are `PriorityFusion` and `Multi-Output` fusion.\n+\n+In `PriorityFusion`, we form fusions guided by the cost model. When fusing we\n+would allow duplicating ops with several users if the op can be fused into all\n+users. We would also allow extending existing Triton Softmax fusions if\n+possible.\n+\n+`Multi-Output` fusion is a separate pass that allows to fuse ops/fusions\n+together that share an operand, or fuse operands/operand fusions into users\n+without duplication but instead adding extra output(s) so other users of the op\n+to be fused can be redirected to this output. This pass needs to be careful not\n+to introduce cycles into the HLO graph.\n+\n+ After Multi-Output fusion, we run common subexpression elimination (`HloCSE`\n+ pass) which will potentially merge previously duplicated ops back together if\n+ they ended up in the same fusion.\n+\n+### Several post-fusion passes\n+\n+Several passes related to collectives (like turning them to async, or enforcing\n+a certain relative order of collectives).\n+\n+Finally we run `CopyInsertion` where copies are added to ensure that in-place\n+operations don't overwrite data that is still needed elsewhere.\n+\n+At the end of optimization, the optimized HLO is dumped if using the flag\n+`-xla_dump_to` to a file that has the file name suffix\n+\"after_optimizations.txt\". If you want to dump the HLO after intermediate\n+passes that actually change the HloModule, you can use the flag\n+`-xla_dump_hlo_pass_re=.*` (or a specific regular expression to restrict it to\n+certain passes).\n+\n+## Scheduling\n+An HloModule without schedule still has some degree of freedom in which order\n+the ops are processed. Basically any topological sort according to\n+operand/result relationship and control dependencies is ok. The scheduling\n+enforces a certain order. This influences the amount of memory that is required,\n+because we cannot reuse a buffer as long as not all readers of that buffer have\n+been processed. In an initial step, we try different scheduler algorithms and\n+pick the schedule that minimizes peak memory consumption.\n+\n+As a follow-up, we run the `LatencyHidingScheduler` pass that tries to maximize\n+compute-communication overlap but may increase memory usage again.\n+\n+After scheduling, we run `HloRematerialization` which attempts to reduce memory\n+usage in case peak memory consumption is higher than the amount of memory we\n+have available. This is at the cost of performance, as e.g. some fusions might\n+be split and some ops might be duplicated to have shorter buffer lifetimes. If\n+rematerialization is happening, it would potentially make sense to look if there\n+are ways at model side to reduce the amount of memory required (e.g. smaller\n+batch sizes).\n+\n+## Thunks and CommandBuffers\n+\n+TBD\n+\n+## BufferAssignment\n+\n+Immediately before we lower to LLVM IR, we run the buffer assignment passes that\n+will assign buffer slices to each instruction in the HLO graph. The buffer\n+assignment runs in several steps.\n+\n+1. `HloDataflowAnalysis` assigns `HloValues` (essentially logical buffers) to\n+instructions. For in-place ops, the `HloValue` of an operand can be reused. An\n+op may define more than one `HloValue` (e.g. with a tuple result shape).\n+\n+2. `HloAliasAnalysis` attempts to combine buffers for aliasing operations, and\n+computes a mapping from `HloValue` to `HloBuffer`.\n+\n+3. `BufferAssignment` computes a mapping of `HloBuffers` to buffer slices inside\n+a big buffer in such a way that the same buffer slice is not used for different\n+`HloBuffers` with overlapping life times. For ops that may alias, it is ok that\n+there is a slight overlap (the end time of the one `HloBuffer` may coincide with\n+the start time of the other `HloBuffer`). When using the flag `-xla_dump_to`,\n+some information about buffer assignment is dumped to a file with the name\n+suffix \"after_optimizations-buffer-assignment.txt\".\n+"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 162,
        "deletions": 0
    }
}