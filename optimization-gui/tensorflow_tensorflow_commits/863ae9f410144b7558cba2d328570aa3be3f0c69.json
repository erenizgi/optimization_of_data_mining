{
    "author": "metaflow",
    "message": "[XLA:GPU] nest gemm fusion pass: dry run on extracted module first\n\nPreviously we handled fusions that nest gemm fusion pass cannot rewrite by running on a cloned module.\nThe issue with the that approach: module is rejected if *any* fusion is rejected.\nNow we will rewrite fusions we support and skip others by dry-running on an extracted module.\n\nPiperOrigin-RevId: 797680902",
    "sha": "863ae9f410144b7558cba2d328570aa3be3f0c69",
    "files": [
        {
            "sha": "1915f75f521ca91c3cec3422c7a1cfaed83056ec",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=863ae9f410144b7558cba2d328570aa3be3f0c69",
            "patch": "@@ -1953,7 +1953,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n       log_stream.emplace(path, err, llvm::sys::fs::OF_None);\n       if (err) {\n         log_stream.reset();\n-        LOG(ERROR) << err.message();\n+        LOG(ERROR) << \"Failed to dump triton passes to \" << path << \": \"\n+                   << err.message();\n       } else {\n         pm.getContext()->disableMultithreading();\n         auto print_always = [](mlir::Pass*, mlir::Operation*) { return true; };"
        },
        {
            "sha": "6abc5ab0f8ac1b2a0a500380dcb50db61ab9d9be",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc?ref=863ae9f410144b7558cba2d328570aa3be3f0c69",
            "patch": "@@ -1913,7 +1913,12 @@ absl::Status EmitMatMul(EmitterLocOpBuilder& b,\n                       TritonFusionAnalysis::Execute(\n                           *fusion->called_computation(), config.split_k));\n \n-  TF_RETURN_IF_ERROR(CheckGemmTilingComplexityHeuristic(config));\n+  absl::Status status = CheckGemmTilingComplexityHeuristic(config);\n+  if (!status.ok()) {\n+    VLOG(1) << \"EmitMatMul heuristic check failed: \"\n+            << fusion->called_computation()->ToString() << status.message();\n+    return status;\n+  }\n \n   const HloComputation* computation = fusion->fused_instructions_computation();\n   const HloInstruction* instr ="
        },
        {
            "sha": "2d0f44333dce8c0e03c4f4b08d468e8119969dbc",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=863ae9f410144b7558cba2d328570aa3be3f0c69",
            "patch": "@@ -2020,6 +2020,7 @@ cc_library(\n         \"//xla/service/gpu/model:symbolic_tiled_hlo_instruction\",\n         \"//xla/service/gpu/model:tiled_hlo_instruction_or_computation\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tools:hlo_extractor\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "25ba6df50ad77662eede55b8ecff80065e93cb5f",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 66,
            "deletions": 42,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=863ae9f410144b7558cba2d328570aa3be3f0c69",
            "patch": "@@ -65,6 +65,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tools/hlo_extractor.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -1149,36 +1150,29 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n   // remove this check completely as all computations will be supported by the\n   // generic emitter and performance regressions will be addressed.\n   absl::Status AcceptResultingFusion(const HloFusionInstruction* fusion) {\n-    VLOG(3) << absl::StrCat(\"CheckResultingFusion \", fusion->ToString());\n     const HloComputation* computation = fusion->called_computation();\n     for (const HloInstruction* instruction : computation->instructions()) {\n       TF_RETURN_IF_ERROR(AcceptNestedInstruction(instruction));\n     }\n     return absl::OkStatus();\n   }\n \n-  absl::Status HandleFusion(HloInstruction* instruction) override {\n-    HloFusionInstruction* fusion = Cast<HloFusionInstruction>(instruction);\n-\n-    absl::StatusOr<TritonGemmConfig> config = GetTritonGemmConfig(*fusion);\n-    if (!config.ok()) {\n-      VLOG(2) << \"Skipping fusion as it does not have a TritonGemmConfig\";\n-      return absl::OkStatus();\n-    }\n-\n+  absl::Status RewriteFusion(HloFusionInstruction* fusion,\n+                             CallGraph* call_graph) {\n     HloComputation* computation = fusion->called_computation();\n+    TF_ASSIGN_OR_RETURN(TritonGemmConfig config, GetTritonGemmConfig(*fusion));\n     HloInstruction* instr =\n         hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n     if (instr == nullptr) {\n-      VLOG(2) << \"Skipping fusion as it has no dot instruction\";\n-      return absl::OkStatus();\n+      return absl::InternalError(absl::StrCat(\"Computation of fusion \",\n+                                              fusion->ToString(),\n+                                              \" has no dot instruction\"));\n     }\n-    DCHECK_EQ(GetDotCount(computation), 1) << \"Fusion has more than one dot.\";\n-    HloDotInstruction* dot = Cast<HloDotInstruction>(instr);\n     TF_RETURN_IF_ERROR(\n-        TryHoistBitcastsInComputationToCallers(instr, call_graph_));\n+        TryHoistBitcastsInComputationToCallers(instr, call_graph));\n+    HloDotInstruction* dot = Cast<HloDotInstruction>(instr);\n     TF_RETURN_IF_ERROR(\n-        MakeNestedFusionFromGemmFusion(fusion, config.value(), dot, ctx_));\n+        MakeNestedFusionFromGemmFusion(fusion, config, dot, ctx_));\n \n     MarkAsChanged();\n \n@@ -1193,6 +1187,61 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n     return AcceptResultingFusion(fusion);\n   }\n \n+  absl::Status HandleFusion(HloInstruction* instruction) override {\n+    HloFusionInstruction* fusion = Cast<HloFusionInstruction>(instruction);\n+\n+    // Check if we target this fusion.\n+    absl::StatusOr<TritonGemmConfig> config = GetTritonGemmConfig(*fusion);\n+    if (!config.ok()) {\n+      VLOG(2) << \"Skipping fusion as it does not have a TritonGemmConfig\";\n+      return absl::OkStatus();\n+    }\n+    HloComputation* computation = fusion->called_computation();\n+    HloInstruction* instr =\n+        hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n+    if (instr == nullptr) {\n+      VLOG(2) << \"Skipping fusion as it has no dot instruction\";\n+      return absl::OkStatus();\n+    }\n+\n+    {\n+      // Symbolic tile analysis and nesting do not support all HLOs yet and\n+      // might leave the module in an invalid state. To avoid that we first dry\n+      // run the rewrite on an extracted module.\n+      // TODO(b/393299275): remove dry-run once we can handle all HLOs.\n+      std::unique_ptr<HloModule> extracted_module =\n+          ExtractInstructionIntoNewModule(*fusion);\n+      extracted_module->mutable_config().set_debug_options(\n+          fusion->GetModule()->config().debug_options());\n+      HloComputation* entry = extracted_module->entry_computation();\n+      HloFusionInstruction* extracted_fusion =\n+          Cast<HloFusionInstruction>(entry->root_instruction());\n+      if (extracted_fusion == nullptr) {\n+        return absl::InternalError(absl::StrCat(\n+            \"Failed to create a cloned module for fusion \", fusion->name()));\n+      }\n+      std::unique_ptr<CallGraph> cloned_call_graph =\n+          CallGraph::Build(extracted_module.get(), {});\n+      absl::Status status =\n+          RewriteFusion(extracted_fusion, cloned_call_graph.get());\n+      if (!status.ok()) {\n+        VLOG(2) << \"Failed to rewrite the fusion \" << fusion->ToString()\n+                << \" in a cloned module: \" << status;\n+        if (IsFeatureEnabled(\n+                fusion->GetModule(),\n+                DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n+          // As legacy emitter is disabled we are doomed to fail now, returning\n+          // the dry run result failure as it is a better diagnostic.\n+          return status;\n+        }\n+        return absl::OkStatus();\n+      }\n+    }\n+    absl::Status status = RewriteFusion(fusion, call_graph_);\n+    VLOG(2) << \"RewriteFusion \" << fusion->name() << \": \" << status;\n+    return status;\n+  }\n+\n  private:\n   mlir::MLIRContext* ctx_;\n   CallGraph* call_graph_;\n@@ -1230,32 +1279,7 @@ absl::StatusOr<bool> NestGemmFusion::Run(\n     VLOG(1) << \"Generic Triton emitter for gemms is disabled, exiting\";\n     return false;\n   }\n-  // Symbolic tile analysis and nesting does not support all HLOs yet, but for\n-  // the supported cases we use the generic emitter. To avoid corrupting the\n-  // module with rewrite we first run the pass on a clone of the module and do\n-  // nothing on error, allowing the legacy emitter to handle the module.\n-  // TODO(b/393299275): remove once we can handle all HLOs.\n-  VLOG(2) << \"dry run on cloned module\";\n-  auto module_clone = module->Clone();\n-  absl::StatusOr<bool> dryrun_result =\n-      RunOnModule(module_clone.get(), execution_threads);\n-\n-  if (!dryrun_result.ok()) {\n-    if (IsFeatureEnabled(\n-            module, DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n-      // As legacy emitter is disabled we are doomed to fail now, returning the\n-      // dry run result failure as it is a better diagnostic.\n-      return dryrun_result;\n-    }\n-    VLOG(1) << \"Failed to nest GEMM fusion: \" << dryrun_result.status()\n-            << \". No changes to the module were made.\";\n-    return false;\n-  }\n-  if (!*dryrun_result) {\n-    VLOG(1) << \"no changes were made during dryrun, exiting\";\n-    return false;\n-  }\n-  VLOG(2) << \"updating module\";\n+\n   TF_ASSIGN_OR_RETURN(bool result, RunOnModule(module, execution_threads));\n   return result;\n }"
        },
        {
            "sha": "b5006405956ff228c931a7573507160dcac26551",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h?ref=863ae9f410144b7558cba2d328570aa3be3f0c69",
            "patch": "@@ -29,8 +29,7 @@ limitations under the License.\n \n namespace xla::gpu {\n \n-// Rewrites Triton GEMM fusions to generic Triton fusions. Any other fusions are\n-// left unchanged.\n+// Rewrites supported Triton GEMM fusions to generic Triton fusions.\n //\n // Fusions with kind kCustom and fusion_backend_config.kind \"__triton_gemm\" are\n // rewritten to fusion_backend_config.kind"
        },
        {
            "sha": "72193b84b57c02ed7b1853bda341f9d81d93d932",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 9,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/863ae9f410144b7558cba2d328570aa3be3f0c69/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc?ref=863ae9f410144b7558cba2d328570aa3be3f0c69",
            "patch": "@@ -40,7 +40,6 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n \n using ::testing::ElementsAre;\n-using ::tsl::testing::IsOkAndHolds;\n \n namespace xla {\n \n@@ -190,36 +189,68 @@ ENTRY e {\n \n TEST_F(NestGemmFusionTest, UnsupportedComputationsAreNotChanged) {\n   // Fusions other than kTritonNestedGemmFusionKind are not supported.\n-  // In this case pass should not make any changes to the module.\n+  // In this case pass should only change the supported fusions.\n   absl::string_view hlo = R\"(\n identity {\n   ROOT result = f32[128,128]{1,0} parameter(0)\n }\n \n-triton_dot {\n+unsupported_fusion {\n   p0 = f32[128,128]{1,0} parameter(0)\n+  // This fusion is not supported by nest_gemm_fusion pass.\n   cp0 = f32[128,128]{1,0} fusion(p0), kind=kCustom, calls=identity\n   p1 = f32[128,128]{1,0} parameter(1)\n   ROOT result = f32[128,128]{1,0} dot(cp0, p1),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n+supported_fusion {\n+  lhs = f32[8192,512] parameter(0)\n+  rhs = f32[512,512] parameter(1)\n+  ROOT dot = f32[8192,512] dot(lhs, rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n ENTRY e {\n   p0 = f32[128,128]{1,0} parameter(0)\n   p1 = f32[128,128]{1,0} parameter(1)\n-  ROOT result = f32[128,128] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+  r1 = f32[128,128] fusion(p0, p1), kind=kCustom, calls=unsupported_fusion,\n     backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n     \"triton_gemm_config\": {\n       \"block_m\":32,\"block_n\":16,\"block_k\":128,\n-      \"split_k\":1,\"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}}\n+      \"split_k\":1,\"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}\n+  p2 = f32[8192,512] parameter(2)\n+  p3 = f32[512,512] parameter(3)\n+  r2 = f32[8192,512] fusion(p2, p3), kind=kCustom, calls=supported_fusion,\n+    backend_config={\n+      \"fusion_backend_config\": {\n+        \"kind\":\"__triton_gemm\",  \"triton_gemm_config\": {\n+          \"block_m\":\"64\", \"block_n\":\"256\", \"block_k\":\"32\",\n+          \"split_k\":\"1\", \"num_stages\":\"5\", \"num_warps\":\"4\", \"num_ctas\":\"3\"\n+        }\n+      }\n+    }\n+  ROOT result = (f32[128,128], f32[8192,512]) tuple(r1, r2)\n+}\n )\";\n-\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n-  size_t hash_before = absl::HashOf(module.get());\n   TF_ASSERT_OK_AND_ASSIGN(\n       bool updated, NestGemmFusion(compute_capability_).Run(module.get()));\n-  EXPECT_FALSE(updated);\n-  EXPECT_EQ(absl::HashOf(module.get()), hash_before);\n+  EXPECT_TRUE(updated);\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_EQ(root->opcode(), HloOpcode::kTuple);\n+  EXPECT_EQ(root->operand(0)->opcode(), HloOpcode::kFusion);\n+  EXPECT_EQ(root->operand(0)\n+                ->backend_config<GpuBackendConfig>()\n+                ->fusion_backend_config()\n+                .kind(),\n+            \"__triton_gemm\");\n+  EXPECT_EQ(root->operand(1)->opcode(), HloOpcode::kFusion);\n+  EXPECT_EQ(root->operand(1)\n+                ->backend_config<GpuBackendConfig>()\n+                ->fusion_backend_config()\n+                .kind(),\n+            \"__triton_nested_gemm_fusion\");\n }\n \n class NestGemmFusionReshapeTest"
        }
    ],
    "stats": {
        "total": 171,
        "additions": 116,
        "deletions": 55
    }
}