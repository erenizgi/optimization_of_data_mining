{
    "author": "ZixuanJiang",
    "message": "Update `xla/service/spmd/shardy/test/stablehlo_export_manual_reduction_collectives.mlir`.\n\nFor sdy.all_reduce, the order of reduction axes does not matter. For all-reduce HLO instruction, the order of device lists in one replica group does not matter. Use the ordered one, as a preparation for cl/810587599.\n\nPiperOrigin-RevId: 810968741",
    "sha": "a52312a766fb589637abf0b32caa1096827bd2ee",
    "files": [
        {
            "sha": "ef1e4ce2438dcaaff287131da3aa0a5e16693e7b",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/stablehlo_export_manual_reduction_collectives.mlir",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a52312a766fb589637abf0b32caa1096827bd2ee/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a52312a766fb589637abf0b32caa1096827bd2ee/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir?ref=a52312a766fb589637abf0b32caa1096827bd2ee",
            "patch": "@@ -108,8 +108,8 @@ func.func @al_reduce_multiple_axes(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.s\n \n // CHECK-LABEL: func @al_reduce_multiple_axes_2\n func.func @al_reduce_multiple_axes_2(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_x_2_y_2, [{}, {}], unreduced={\"x\", \"y\"}>}) -> tensor<8x8xf32> {\n-  // CHECK{LITERAL}: replica_groups = dense<[[0, 2, 1, 3]]>\n-  %0 = sdy.all_reduce {\"y\", \"x\"} %arg0 out_sharding=<@mesh_x_2_y_2, [{}, {}]> : tensor<8x8xf32>\n+  // CHECK{LITERAL}: replica_groups = dense<[[0, 1, 2, 3]]>\n+  %0 = sdy.all_reduce {\"x\", \"y\"} %arg0 out_sharding=<@mesh_x_2_y_2, [{}, {}]> : tensor<8x8xf32>\n   return %0 : tensor<8x8xf32>\n }\n \n@@ -122,8 +122,8 @@ func.func @all_reduce_non_iota_device_order(%arg0: tensor<8x8xf32> {sdy.sharding\n \n // CHECK-LABEL: func @all_reduce_sub_axis\n func.func @all_reduce_sub_axis(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_x_4_y_6, [{}, {}], unreduced={\"x\", \"y\"}>}) -> tensor<8x8xf32> {\n-  // CHECK{LITERAL}: replica_groups = dense<[[0, 6, 12, 18, 1, 7, 13, 19, 2, 8, 14, 20], [3, 9, 15, 21, 4, 10, 16, 22, 5, 11, 17, 23]]>\n-  %0 = sdy.all_reduce {\"y\":(2)3, \"x\"} %arg0 out_sharding=<@mesh_x_4_y_6, [{}, {}]> : tensor<8x8xf32>\n+  // CHECK{LITERAL}: replica_groups = dense<[[0, 1, 2, 6, 7, 8, 12, 13, 14, 18, 19, 20], [3, 4, 5, 9, 10, 11, 15, 16, 17, 21, 22, 23]]>\n+  %0 = sdy.all_reduce {\"x\", \"y\":(2)3} %arg0 out_sharding=<@mesh_x_4_y_6, [{}, {}]> : tensor<8x8xf32>\n   return %0 : tensor<8x8xf32>\n }\n "
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}