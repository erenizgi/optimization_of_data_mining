{
    "author": "unknown",
    "message": "PR #30998: [DOC] Update to operation_semantics\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30998\n\n## Summary of Changes\n\n- **`AllReduce`**\n   - Fixed Spelling Error\n\n- **`AllToAll`**\n   - Fixed declaration\n   - Fixed Table\n   - Adding link to XLA Shapes\n   - Updated example code formatting\n   - Add AllToAll - Example 2 - StableHLO header 3\n          - Adding new image file for example\n          - Added brief description of image\n          - Added alt text\n\n- **`Dot`**\n   - Fixed Spelling Error\n\n- **`DotGeneral`**\n   - Fixed Spelling Error\n\n- **`Recv`**\n   - Added mention of `Recv`, `RecvWithTokens`, `RecvToHost` limitation\n\n- **`ReduceScatter `**\n   - Added information on `use_global_device_ids` and `layout`\n          - Updated declaration\n          - Updated Table with formatting\n          - Added information to bulleted list\n          - Added links\n   - Added ReduceScatter - Example 1 - StableHLO header 3\n          - Add new image file for example\n          - Linked image\n          - Added image descriptions\n          - Added alt text\n\n- **`ReduceWindow `**\n   - Updated ReduceWindow - Example 1 to header 3\n   - Added ReduceWindow - Example 2 - StableHLO header 3\n          - Add new image file for example\n          - Linked image\n          - Added image descriptions\n          - Added alt text\n\n- **`Scatter `**\n   - Added information on `use_global_device_ids` and `layout`\n          - Updated declaration\n          - Updated Table with formatting\n          - Added information to bulleted list\n          - Added links\n   - Added Scatter - Example 1 - StableHLO  header 3\n          - Add new image file for example\n          - Linked image\n          - Added image description\n          - Added alt text\n\n- **`Send`**\n   - Added mention of `Send`, `SendWithTokens`, `SendToHost` limitation\n\n- **`Slice`**\n   - Fixed Typo\n\n- **`Convolution`**\n   - New image file (not yet linked)\n\nðŸŽ¯ Justification\nDocumentation update\n\nðŸš€ Kind of Contribution\nðŸ“š Documentation\nCopybara import of the project:\n\n--\n9f3748e61f79eed8d51c48bbc1ea0326218610cb by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nupdates of send,rec,alltoall\n\n--\n5a448097ee4af73bc7bd0b3a79409cf39e75a449 by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nupdates of operation_semantics\n\n--\n316dcf3f510cc0a25d5fa846364875a3ece5f01e by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nupdates of operation_semantics\n\n--\n706e50b54059eded91a4ee5d7f50cea6ec9b69e3 by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nupdates of operation_semantics\n\n--\n6e4c796b70053b5356e019104c9288c2f8a057d0 by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nupdates of operation_semantics\n\n--\n77d346af259996add37c2033cc7b13f580732eb6 by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nupdated images to include alt text\n\nMerging this change closes #30998\n\nPiperOrigin-RevId: 808044525",
    "sha": "83823801c11473e1ba380ded0ee505aa6b1c553a",
    "files": [
        {
            "sha": "fdba81fc5561eebf1e290d45260f4042efb37156",
            "filename": "third_party/xla/docs/images/ops_alltoall_2.svg",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_alltoall_2.svg",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_alltoall_2.svg",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fimages%2Fops_alltoall_2.svg?ref=83823801c11473e1ba380ded0ee505aa6b1c553a"
        },
        {
            "sha": "cb6da4c6f2e0b8ab3f1a622c5ce34ee78d9cfdbf",
            "filename": "third_party/xla/docs/images/ops_convolution_1.svg",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_convolution_1.svg",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_convolution_1.svg",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fimages%2Fops_convolution_1.svg?ref=83823801c11473e1ba380ded0ee505aa6b1c553a"
        },
        {
            "sha": "fc47324c5322f9ae315bda9f5987220bfbda1b46",
            "filename": "third_party/xla/docs/images/ops_reduce_scatter_1.svg",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_reduce_scatter_1.svg",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_reduce_scatter_1.svg",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fimages%2Fops_reduce_scatter_1.svg?ref=83823801c11473e1ba380ded0ee505aa6b1c553a"
        },
        {
            "sha": "e05f52c4a055cb4c253470bd72cc05c10a52ddd6",
            "filename": "third_party/xla/docs/images/ops_reduce_window_2.svg",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_reduce_window_2.svg",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_reduce_window_2.svg",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fimages%2Fops_reduce_window_2.svg?ref=83823801c11473e1ba380ded0ee505aa6b1c553a"
        },
        {
            "sha": "3a0c49be6b5d76ddd8afb632249d631ead1c0d81",
            "filename": "third_party/xla/docs/images/ops_scatter_1.svg",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_scatter_1.svg",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Fimages%2Fops_scatter_1.svg",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fimages%2Fops_scatter_1.svg?ref=83823801c11473e1ba380ded0ee505aa6b1c553a"
        },
        {
            "sha": "754d8e6cb580f14da983f6a782f9e70628a2d519",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 204,
            "deletions": 65,
            "changes": 269,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83823801c11473e1ba380ded0ee505aa6b1c553a/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=83823801c11473e1ba380ded0ee505aa6b1c553a",
            "patch": "@@ -136,7 +136,7 @@ Computing the result of `AllReduce` requires having one input from each replica,\n so if one replica executes an `AllReduce` node more times than another, then the\n former replica will wait forever. Since the replicas are all running the same\n program, there are not a lot of ways for that to happen, but it is possible when\n-a while loop's condition depends on data from infeed and the data that is infed\n+a while loop's condition depends on data from infeed and the data that is infeed\n causes the while loop to iterate more times on one replica than another.\n \n ## AllToAll\n@@ -173,44 +173,67 @@ Prerequisites:\n -   The operand's shape is not tuple.\n \n **`AllToAll(operand, split_dimension, concat_dimension, split_count,\n-replica_groups)`**\n-\n-| Arguments          | Type                  | Semantics                       |\n-| ------------------ | --------------------- | ------------------------------- |\n-| `operand`          | `XlaOp`               | n dimensional input array       |\n-| `split_dimension`  | `int64`               | A value in the interval `[0,    |\n-:                    :                       : n)` that names the dimension    :\n-:                    :                       : along which the operand is      :\n-:                    :                       : split                           :\n-| `concat_dimension` | `int64`               | A value in the interval `[0,    |\n-:                    :                       : n)` that names the dimension    :\n-:                    :                       : along which the split blocks    :\n-:                    :                       : are concatenated                :\n-| `split_count`      | `int64`               | The number of cores that        |\n-:                    :                       : participate this operation. If  :\n-:                    :                       : `replica_groups` is empty, this :\n-:                    :                       : should be the number of         :\n-:                    :                       : replicas; otherwise, this       :\n-:                    :                       : should be equal to the number   :\n-:                    :                       : of replicas in each group.      :\n-| `replica_groups`   | `ReplicaGroup` vector | Each group contains a list of   |\n-:                    :                       : replica ids.                    :\n-\n-Below shows an example of Alltoall.\n+replica_groups, layout, channel_id)`**\n+\n+| Arguments          | Type                     | Semantics                    |\n+| ------------------ | ------------------------ | ---------------------------- |\n+| `operand`          | `XlaOp`                  | n dimensional input array    |\n+| `split_dimension`  | `int64`                  | A value in the interval `[0, |\n+:                    :                          : n)` that names the dimension :\n+:                    :                          : along which the operand is   :\n+:                    :                          : split                        :\n+| `concat_dimension` | `int64`                  | A value in the interval `[0, |\n+:                    :                          : n)` that names the dimension :\n+:                    :                          : along which the split blocks :\n+:                    :                          : are concatenated             :\n+| `split_count`      | `int64`                  | The number of cores that     |\n+:                    :                          : participate this operation.  :\n+:                    :                          : If `replica_groups` is       :\n+:                    :                          : empty, this should be the    :\n+:                    :                          : number of replicas;          :\n+:                    :                          : otherwise, this should be    :\n+:                    :                          : equal to the number of       :\n+:                    :                          : replicas in each group.      :\n+| `replica_groups`   | `ReplicaGroup` vector    | Each group contains a list   |\n+:                    :                          : of replica ids.              :\n+| `layout`           | optional `Layout`        | user-specified memory layout |\n+| `ChannelHandle`    | optional `ChannelHandle` | unique identifier for each   |\n+:                    :                          : send/recv pair               :\n+\n+See\n+[xla::shapes for more information on shapes and layouts.](https://openxla.org/xla/shapes)\n+\n+### AllToAll - Example 1.\n \n ```cpp\n XlaBuilder b(\"alltoall\");\n auto x = Parameter(&b, 0, ShapeUtil::MakeShape(F32, {4, 16}), \"x\");\n-AllToAll(x, /*split_dimension=*/1, /*concat_dimension=*/0, /*split_count=*/4);\n+AllToAll(\n+    x,\n+    /*split_dimension=*/ 1,\n+    /*concat_dimension=*/ 0,\n+    /*split_count=*/ 4);\n ```\n \n ![](images/ops_alltoall.png)\n \n-In this example, there are 4 cores participating in the Alltoall. On each core,\n-the operand is split into 4 parts along dimension 1, so each part has shape\n-f32[4,4]. The 4 parts are scattered to all cores. Then each core concatenates\n-the received parts along dimension 0, in the order of core 0-4. So the output on\n-each core has shape f32[16,4].\n+In the above example, there are 4 cores participating in the Alltoall. On each\n+core, the operand is split into 4 parts along dimension 1, so each part has\n+shape f32[4,4]. The 4 parts are scattered to all cores. Then each core\n+concatenates the received parts along dimension 0, in the order of core 0-4. So\n+the output on each core has shape f32[16,4].\n+\n+### AllToAll - Example 2 - StableHLO\n+\n+![An example of AllToAll dataflow for StableHLO](images/ops_alltoall_2.svg)\n+\n+In the above example, there are 2 replicas participating in the AllToAll. On\n+each replica, the operand has shape f32[2,4]. The operand is split into 2 parts\n+along dimension 1, so each part has shape f32[2,2]. The 2 parts are then\n+exchanged across the replicas according to their position in the replica group.\n+Each replica collects its corresponding part from both operands and concatenates\n+them along dimension 0. As a result, the output on each replica has shape\n+f32[4,2].\n \n ## BatchNormGrad\n \n@@ -1227,7 +1250,7 @@ emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n \n `preferred_element_type` is a scalar element of higher/lower precision output\n types used for accumulation. `preferred_element_type` recommends the\n-accumulation type for the given operaiton, however it is not guaranteed. This\n+accumulation type for the given operation, however it is not guaranteed. This\n allows for some hardware backends to instead accumulate in a different type and\n convert to the preferred output type.\n \n@@ -1333,7 +1356,7 @@ emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n \n `preferred_element_type` is a scalar element of higher/lower precision output\n types used for accumulation. `preferred_element_type` recommends the\n-accumulation type for the given operaiton, however it is not guaranteed. This\n+accumulation type for the given operation, however it is not guaranteed. This\n allows for some hardware backends to instead accumulate in a different type and\n convert to the preferred output type.\n \n@@ -2151,6 +2174,11 @@ interior padding values are all 0. The figure below shows examples of different\n See also\n [`XlaBuilder::Recv`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n+`Recv`, `RecvWithTokens`, `RecvToHost` are operations that serve as\n+communication primitives in HLO. These ops typically appear in HLO dumps as part\n+of low-level input/output or cross-device transfer, but they are not intended to\n+be constructed manually by end users.\n+\n **`Recv(shape, channel_handle)`**\n \n | Arguments        | Type            | Semantics                            |\n@@ -2385,17 +2413,25 @@ then scatters the result by splitting it into `shard_count` blocks along the\n `scatter_dimension` and replica `i` in the replica group receives the `ith`\n shard.\n \n-**`ReduceScatter(operand, computation, scatter_dim, shard_count,\n-replica_group_ids, channel_id)`**\n+**`ReduceScatter(operand, computation, scatter_dimension, shard_count,\n+replica_groups, channel_id, layout, use_global_device_ids)`**\n \n-| Arguments           | Type                 | Semantics                     |\n-| ------------------- | -------------------- | ----------------------------- |\n-| `operand`           | `XlaOp`              | Array or a non-empty tuple of arrays to reduce across replicas. |\n-| `computation`       | `XlaComputation`     | Reduction computation         |\n-| `scatter_dimension` | `int64`              | Dimension to scatter.         |\n-| `shard_count`       | `int64`              | Number of blocks to split `scatter_dimension`  |\n-| `replica_groups`    | vector of vectors of  `int64` | Groups between which the reductions are performed |\n-| `channel_id`        | optional `int64`     | Optional channel ID for cross-module communication |\n+| Arguments               | Type                 | Semantics                  |\n+| ----------------------- | -------------------- | -------------------------- |\n+| `operand`               | `XlaOp`              | Array or a non-empty tuple |\n+:                         :                      : of arrays to reduce across :\n+:                         :                      : replicas.                  :\n+| `computation`           | `XlaComputation`     | Reduction computation      |\n+| `scatter_dimension`     | `int64`              | Dimension to scatter.      |\n+| `shard_count`           | `int64`              | Number of blocks to split  |\n+:                         :                      : `scatter_dimension`        :\n+| `replica_groups`        | vector of vectors of | Groups between which the   |\n+:                         : `int64`              : reductions are performed   :\n+| `channel_id`            | optional `int64`     | Optional channel ID for    |\n+:                         :                      : cross-module communication :\n+| `layout`                | optional `Layout`    | user-specified memory      |\n+:                         :                      : layout                     :\n+| `use_global_device_ids` | optional `bool`      | user-specified flag        |\n \n -   When `operand` is a tuple of arrays, the reduce-scatter is performed on each\n     element of the tuple.\n@@ -2413,13 +2449,36 @@ replica_group_ids, channel_id)`**\n     must be equal to the size of each replica group.\n -   `channel_id` is used for cross-module communication: only `reduce-scatter`\n     operations with the same `channel_id` can communicate with each other.\n+-   `layout` See\n+    [xla::shapes for more information on layouts.](https://openxla.org/xla/shapes)\n+-   `use_global_device_ids` is a user-specified flag. When `false`(default) the\n+    numbers in `replica_groups` are [`ReplicaId`](#replicaid) when `true` the\n+    `replica_groups` represent a global id of (`ReplicaID`*`partition_count` +\n+    `partition_id`). For example:\n+    -   With 2 replicas and 4 partitions,\n+    -   replica_groups={{0,1,4,5},{2,3,6,7}} and use_global_device_ids=true\n+    -   group[0] = (0,0), (0,1), (1,0), (1,1)\n+    -   group[1] = (0,2), (0,3), (1,2), (1,3)\n+    -   where each pair is (replica_id, partition_id).\n \n The output shape is the input shape with the `scatter_dimension` made\n `shard_count` times smaller. For example, if there are two replicas and the\n operand has the value `[1.0, 2.25]` and `[3.0, 5.25]` respectively on the two\n replicas, then the output value from this op where `scatter_dim` is `0` will be\n `[4.0]` for the first replica and `[7.5]` for the second replica.\n \n+### ReduceScatter - Example 1 - StableHLO\n+\n+![An example of ReduceScatter dataflow for StableHLO](images/ops_reduce_scatter_1.svg)\n+\n+In the above example, there are 2 replicas participating in the ReduceScatter.\n+On each replica, the operand has shape f32[2,4]. An all-reduce (sum) is\n+performed across the replicas, producing a reduced value of shape f32[2,4] on\n+each replica. This reduced value is then split into 2 parts along dimension 1,\n+so each part has shape f32[2,2]. Each replica within the process group receives\n+the part corresponding to its position in the group. As a result, the output on\n+each replica has shape f32[2,2].\n+\n ## ReduceWindow\n \n See also\n@@ -2454,9 +2513,10 @@ Where:\n *   If `N > 1`, `Collate(T_0, ..., T_{N-1})` is a tuple of `N` elements of type\n     `(T0,...T{N-1})`.\n \n-Below code and figure shows an example of using `ReduceWindow`. Input is a\n-matrix of size [4x6] and both window_dimensions and window_stride_dimensions are\n-[2x3].\n+### ReduceWindow - Example 1\n+\n+Input is a matrix of size [4x6] and both window_dimensions and\n+window_stride_dimensions are [2x3].\n \n ```cpp\n // Create a computation for the reduction (maximum).\n@@ -2510,6 +2570,37 @@ non-deterministic. Therefore, the reduction function should not be overly\n sensitive to reassociation. See the discussion about associativity in the\n context of [`Reduce`](#reduce) for more details.\n \n+### ReduceWindow - Example 2 - StableHLO\n+\n+![A example of ReduceWindow dataflow for StableHLO](images/ops_reduce_window_2.svg)\n+\n+In the above example:\n+\n+Input) The operand has a input shape of S32[3,2]. With a values of\n+`[[1,2],[3,4],[5,6]]`\n+\n+Step 1) Base dilation with factor 2 along the row dimension inserts holes\n+between each row of the operand. Padding of 2 rows at the top and 1 row at the\n+bottom is applied after dilation. As a result, the tensor becomes taller.\n+\n+Step 2) A window of shape [2,1] is defined, with window dilation [3,1]. This\n+means each window selects two elements from the same column, but the second\n+element is taken three rows below the first rather than directly beneath it.\n+\n+Step 3) The windows are then slid across the operand with stride [4,1]. This\n+causes the window to move down four rows at a time, while shifting one column at\n+a time horizontally. Padding cells are filled with the `init_value` (in this\n+case `init_value = 0`). Values 'falling into' dilation cells are ignored.\n+Because of the stride and padding, some windows overlap only zeros and holes,\n+while others overlap real input values.\n+\n+Step 4) Within each window, the elements are combined using the reduction\n+function (a, b) â†’ a + b, starting from an initial value of 0. The top two\n+windows see only padding and holes, so their results are 0. The bottom windows\n+capture the values 3 and 4 from the input and return those as results.\n+\n+Results) The final output has shape S32[2,2], with values: `[[0,0],[3,4]]`\n+\n ## ReplicaId\n \n See also\n@@ -2697,22 +2788,22 @@ of the input array `operands`, with several slices (at indices specified by\n See also\n [`XlaBuilder::Scatter`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-**`scatter(operands..., scatter_indices, updates..., update_computation,\n-index_vector_dim, update_window_dims, inserted_window_dims,\n-scatter_dims_to_operand_dims)`**\n-\n-Arguments                      | Type                  | Semantics\n------------------------------- | --------------------- | ---------\n-`operands`                     | Sequence of N `XlaOp` | N arrays of types `T_0, ..., T_N` to be scattered into.\n-`scatter_indices`              | `XlaOp`               | Array containing the starting indices of the slices that must be scattered to.\n-`updates`                      | Sequence of N `XlaOp` | N arrays of types `T_0, ..., T_N`. `updates[i]` contains the values that must be used for scattering `operands[i]`.\n-`update_computation`           | `XlaComputation`      | Computation to be used for combining the existing values in the input array and the updates during scatter. This computation should be of type `T_0, ..., T_N, T_0, ..., T_N -> Collate(T_0, ..., T_N)`.\n-`index_vector_dim`             | `int64`               | The dimension in `scatter_indices` that contains the starting indices.\n-`update_window_dims`           | `ArraySlice<int64>`   | The set of dimensions in `updates` shape that are *window dimensions*.\n-`inserted_window_dims`         | `ArraySlice<int64>`   | The set of *window dimensions* that must be inserted into `updates` shape.\n-`scatter_dims_to_operand_dims` | `ArraySlice<int64>`   | A dimensions map from the scatter indices to the operand index space. This array is interpreted as mapping `i` to `scatter_dims_to_operand_dims[i]` . It has to be one-to-one and total.\n-`indices_are_sorted`           | `bool`                | Whether the indices are guaranteed to be sorted by the caller.\n-`unique_indices`               | `bool`                | Whether the indices are guaranteed to be unique by the caller.\n+**`Scatter(operands..., scatter_indices, updates..., update_computation,\n+dimension_numbers, indices_are_sorted, unique_indices)`**\n+\n+Arguments                      | Type                      | Semantics\n+------------------------------ | ------------------------- | ---------\n+`operands`                     | Sequence of N `XlaOp`     | N arrays of types `T_0, ..., T_N` to be scattered into.\n+`scatter_indices`              | `XlaOp`                   | Array containing the starting indices of the slices that must be scattered to.\n+`updates`                      | Sequence of N `XlaOp`     | N arrays of types `T_0, ..., T_N`. `updates[i]` contains the values that must be used for scattering `operands[i]`.\n+`update_computation`           | `XlaComputation`          | Computation to be used for combining the existing values in the input array and the updates during scatter. This computation should be of type `T_0, ..., T_N, T_0, ..., T_N -> Collate(T_0, ..., T_N)`.\n+`index_vector_dim`             | `int64`                   | The dimension in `scatter_indices` that contains the starting indices.\n+`update_window_dims`           | `ArraySlice<int64>`       | The set of dimensions in `updates` shape that are *window dimensions*.\n+`inserted_window_dims`         | `ArraySlice<int64>`       | The set of *window dimensions* that must be inserted into `updates` shape.\n+`scatter_dims_to_operand_dims` | `ArraySlice<int64>`       | A dimensions map from the scatter indices to the operand index space. This array is interpreted as mapping `i` to `scatter_dims_to_operand_dims[i]` . It has to be one-to-one and total.\n+`dimension_number`             | `ScatterDimensionNumbers` | Dimension numbers for scatter operation\n+`indices_are_sorted`           | `bool`                    | Whether the indices are guaranteed to be sorted by the caller.\n+`unique_indices`               | `bool`                    | Whether the indices are guaranteed to be unique by the caller.\n \n Where:\n \n@@ -2828,6 +2919,45 @@ corresponding gather op.\n For a detailed informal description and examples, refer to the\n \"Informal Description\" section under `Gather`.\n \n+### Scatter - Example 1 - StableHLO\n+\n+![An example of Scatter dataflow for StableHLO](images/ops_scatter_1.svg)\n+\n+In the above image, each row of the table is an example of one update index\n+example. Let's review stepwise from left(Update Index) to right(Result Index):\n+\n+Input) `input` has shape S32[2,3,4,2]. `scatter_indices` have shape\n+S64[2,2,3,2]. `updates` have shape S32[2,2,3,1,2].\n+\n+Update Index) As part of the input we are given `update_window_dims:[3,4]`. This\n+tell us that `updates`'s dim 3 and dim 4 are window dimensions, highlighted in\n+yellow. This allows us to derive that `update_scatter_dims` = [0,1,2].\n+\n+Update Scatter Index) Shows us the extracted `updated_scatter_dims` for each.\n+(The non-yellow of column Update Index)\n+\n+Start Index) Looking at the `scatter_indices` tensor image we can see that our\n+values from the previous step (Update scatter Index), give us the location of\n+the start index. From `index_vector_dim` we are also told the dimension of the\n+`starting_indices` that contains the starting indices, which for\n+`scatter_indices` is dim 3 with a size 2.\n+\n+Full Start Index) `scatter_dims_to_operand_dims` = [2,1] tells us the first\n+element of the index vector goes to operand dim 2. The second element of the\n+index vector goes to operand dim 1. The remaining operand dimensions are filled\n+with 0.\n+\n+Full Batching Index) We can see the purple highlighted area is shown in this\n+column(full batching index), the update scatter index column, and update index\n+column.\n+\n+Full Window Index) Computed from the `update_window_dimensions` [3,4].\n+\n+Result Index) The addition of Full Start Index, Full Batching Index, and Full\n+Window Index in the `operand` tensor. Notice the green highlighted regions\n+correspond to the `operand` figure as well. The last row is skipped because it\n+falls outside of `operand` tensor.\n+\n ## Select\n \n See also\n@@ -2945,6 +3075,11 @@ context of [`Reduce`](#reduce) for more details.\n See also\n [`XlaBuilder::Send`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n+`Send`, `SendWithTokens`, `SendToHost` are operations that serve as\n+communication primitives in HLO. These ops typically appear in HLO dumps as part\n+of low-level input/output or cross-device transfer, but they are not intended to\n+be constructed manually by end users.\n+\n **`Send(operand, channel_handle)`**\n \n Arguments        | Type            | Semantics\n@@ -3209,9 +3344,13 @@ See also\n \n | Arguments   | Type             | Semantics                                |\n | ----------- | ---------------- | ---------------------------------------- |\n-| `condition` | `XlaComputation` | XlaComputation of type `T -> PRED` which defines the termination condition of theloop. |\n-| `body`      | `XlaComputation` | XlaComputation of type `T -> T` which defines the body of the loop. |\n-| `init`      | `T`              | Initial value for the parameter of `condition` and `body`. |\n+| `condition` | `XlaComputation` | XlaComputation of type `T -> PRED` which |\n+:             :                  : defines the termination condition of the :\n+:             :                  : loop.                                    :\n+| `body`      | `XlaComputation` | XlaComputation of type `T -> T` which    |\n+:             :                  : defines the body of the loop.            :\n+| `init`      | `T`              | Initial value for the parameter of       |\n+:             :                  : `condition` and `body`.                  :\n \n Sequentially executes the `body` until the `condition` fails. This is similar to\n a typical while loop in many other languages except for the differences and"
        }
    ],
    "stats": {
        "total": 274,
        "additions": 209,
        "deletions": 65
    }
}