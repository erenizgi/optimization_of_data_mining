{
    "author": "basioli-k",
    "message": "[XLA:CPU][nanort] Enable nanort client to compile an HLO module without running HLO passes\n\nPiperOrigin-RevId: 830848849",
    "sha": "e539f514a7c7284b04d1b08d4387127e41abb793",
    "files": [
        {
            "sha": "ee6537863a524d3530c0f2feb5dec620b03ab779",
            "filename": "third_party/xla/xla/backends/cpu/nanort/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e539f514a7c7284b04d1b08d4387127e41abb793/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e539f514a7c7284b04d1b08d4387127e41abb793/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2FBUILD?ref=e539f514a7c7284b04d1b08d4387127e41abb793",
            "patch": "@@ -24,6 +24,7 @@ cc_library(\n         \"//xla:debug_options_flags\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n+        \"//xla/client:executable_build_options\",\n         \"//xla/hlo/builder:xla_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/pjrt:utils\","
        },
        {
            "sha": "bec4d24483521b5b7b2e86a5df71861e381ad51c",
            "filename": "third_party/xla/xla/backends/cpu/nanort/nanort_client.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e539f514a7c7284b04d1b08d4387127e41abb793/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e539f514a7c7284b04d1b08d4387127e41abb793/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_client.cc?ref=e539f514a7c7284b04d1b08d4387127e41abb793",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"xla/backends/cpu/nanort/nanort_executable.h\"\n+#include \"xla/client/executable_build_options.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -42,7 +43,8 @@ using ::tsl::profiler::TraceMe;\n using ::tsl::profiler::TraceMeEncode;\n \n absl::StatusOr<std::unique_ptr<NanoRtExecutable>> NanoRtClient::Compile(\n-    const XlaComputation& computation) {\n+    const XlaComputation& computation,\n+    const ExecutableBuildOptions& executable_build_options) {\n   TraceMe trace([&] {\n     return TraceMeEncode(\"NanoRtClient::Compile\",\n                          {{\"computation\", computation.name()}});\n@@ -61,14 +63,16 @@ absl::StatusOr<std::unique_ptr<NanoRtExecutable>> NanoRtClient::Compile(\n   static constexpr char kBeforeOptimizationsDumpName[] = \"before_optimizations\";\n   DumpHloModuleIfEnabled(*hlo_module, kBeforeOptimizationsDumpName);\n \n-  // Use default XLA compiler options.\n   Compiler::CompileOptions compile_options;\n \n   // Run high-level XLA CPU compiler passes.\n   cpu::CpuCompiler compiler;\n-  TF_ASSIGN_OR_RETURN(hlo_module, compiler.RunHloPasses(std::move(hlo_module),\n-                                                        /*stream_exec=*/nullptr,\n-                                                        compile_options));\n+  if (!executable_build_options.run_backend_only()) {\n+    TF_ASSIGN_OR_RETURN(\n+        hlo_module,\n+        compiler.RunHloPasses(std::move(hlo_module),\n+                              /*stream_exec=*/nullptr, compile_options));\n+  }\n \n   auto optimized_hlo_program_shape =\n       hlo_module->entry_computation_layout().ComputeProgramShape();"
        },
        {
            "sha": "cad9c584a0fba47387a7df3aeb140e943f132a0a",
            "filename": "third_party/xla/xla/backends/cpu/nanort/nanort_client.h",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e539f514a7c7284b04d1b08d4387127e41abb793/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e539f514a7c7284b04d1b08d4387127e41abb793/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_client.h?ref=e539f514a7c7284b04d1b08d4387127e41abb793",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"xla/backends/cpu/nanort/nanort_executable.h\"\n+#include \"xla/client/executable_build_options.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/service/compiler.h\"\n \n@@ -33,7 +34,9 @@ class NanoRtClient {\n   // Compiles the given XLA computation to a NanoRtExecutable using the XLA:CPU\n   // backend.\n   absl::StatusOr<std::unique_ptr<NanoRtExecutable>> Compile(\n-      const XlaComputation& computation);\n+      const XlaComputation& computation,\n+      const ExecutableBuildOptions& executable_build_options =\n+          ExecutableBuildOptions());\n \n   // Exports the given NanoRtExecutable to an AotCompilationResult.\n   absl::StatusOr<std::unique_ptr<AotCompilationResult>> Export("
        }
    ],
    "stats": {
        "total": 20,
        "additions": 14,
        "deletions": 6
    }
}