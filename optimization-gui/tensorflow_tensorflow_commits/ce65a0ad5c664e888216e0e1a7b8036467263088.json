{
    "author": "alexander-shaposhnikov",
    "message": "[XLA:CPU] Add initial bits for YNNPACK support.\n\n+ Do not build XLA with YNNPACK on Windows.\n\nCo-authored-by: Penporn Koanantakool <penporn@google.com>\nPiperOrigin-RevId: 820896434",
    "sha": "ce65a0ad5c664e888216e0e1a7b8036467263088",
    "files": [
        {
            "sha": "a687883ad900544f1b80b303af68c82ffc194bcc",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -153,6 +153,30 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"ynn_emitter\",\n+    srcs = [\"ynn_emitter.cc\"],\n+    hdrs = [\"ynn_emitter.h\"],\n+    deps = [\n+        \":ynn_support\",\n+        \"//xla:literal\",\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/cpu/runtime/xnnpack:xnn_interop\",\n+        \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@XNNPACK//ynnpack\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n cc_library(\n     name = \"xnn_gemm_config\",\n     srcs = [\"xnn_gemm_config.cc\"],\n@@ -234,6 +258,33 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"ynn_support\",\n+    srcs = [\"ynn_support.cc\"],\n+    hdrs = [\"ynn_support.h\"],\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/cpu/codegen:target_machine_features\",\n+        \"//xla/backends/cpu/runtime:dot_lib\",\n+        \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@XNNPACK//ynnpack\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n cc_library(\n     name = \"constant_allocation\",\n     srcs = [\"constant_allocation.cc\"],"
        },
        {
            "sha": "2a891e0fe78dabc88e35f0d044741ee6d639983f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -4,6 +4,7 @@ load(\"//xla/tsl:tsl.bzl\", \"if_windows\", \"internal_visibility\")\n load(\"//xla/tsl:tsl.default.bzl\", \"filegroup\")\n load(\"//xla/tsl/platform:build_config.bzl\", \"tf_proto_library\")\n load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+load(\"//xla/tsl/xnnpack:build_defs.bzl\", \"if_ynnpack\")\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -160,6 +161,7 @@ cc_library(\n     name = \"thunk\",\n     srcs = [\"thunk.cc\"],\n     hdrs = [\"thunk.h\"],\n+    defines = if_ynnpack([\"XLA_YNNPACK\"]),\n     deps = [\n         \":buffer_allocations\",\n         \":function_library\",\n@@ -188,7 +190,10 @@ cc_library(\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n         \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n-    ],\n+    ] + if_ynnpack([\n+        \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n+        \"//xla/backends/cpu/runtime/ynnpack:ynn_threadpool\",\n+    ]),\n )\n \n cc_library(\n@@ -1246,6 +1251,7 @@ xla_cc_test(\n xla_cc_test(\n     name = \"thunk_sequence_serdes_test\",\n     srcs = [\"thunk_sequence_serdes_test.cc\"],\n+    local_defines = if_ynnpack([\"XLA_YNNPACK\"]),\n     deps = [\n         \":all_gather_thunk\",\n         \":all_reduce_thunk\",\n@@ -1261,7 +1267,6 @@ xla_cc_test(\n         \":dot_thunk\",\n         \":fft_thunk\",\n         \":infeed_thunk\",\n-        \":kernel\",\n         \":kernel_thunk\",\n         \":logical_id_thunk\",\n         \":outfeed_thunk\",\n@@ -1293,7 +1298,6 @@ xla_cc_test(\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -1305,7 +1309,9 @@ xla_cc_test(\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:casts\",\n-    ],\n+    ] + if_ynnpack([\n+        \"//xla/backends/cpu/runtime/ynnpack:ynn_fusion_thunk\",\n+    ]),\n )\n \n cc_library("
        },
        {
            "sha": "219fb4f69d052729fad9538d48bcb2d75b83a97a",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -43,6 +43,11 @@ limitations under the License.\n #include \"tsl/profiler/lib/traceme.h\"\n #include \"tsl/profiler/lib/traceme_encode.h\"\n \n+#ifdef XLA_YNNPACK\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n+#endif  // XLA_YNNPACK\n+\n namespace xla::cpu {\n \n // Ok execute event allocated with the static storage duration.\n@@ -88,6 +93,8 @@ absl::string_view Thunk::KindToString(Kind kind) {\n       return \"while\";\n     case Kind::kXnnFusion:\n       return \"xnn-fusion\";\n+    case Kind::kYnnFusion:\n+      return \"ynn-fusion\";\n     case Kind::kOneDnnFusion:\n       return \"onednn-fusion\";\n   }\n@@ -168,6 +175,18 @@ absl::StatusOr<Thunk::XnnParams> Thunk::XnnParams::Create(\n Thunk::XnnParams::XnnParams(XnnThreadpool threadpool)\n     : threadpool(std::move(threadpool)) {}\n \n+#ifdef XLA_YNNPACK\n+absl::StatusOr<Thunk::YnnParams> Thunk::YnnParams::Create(\n+    const ExecutableRunOptions* run_options) {\n+  TF_ASSIGN_OR_RETURN(YnnThreadpool threadpool,\n+                      CreateYnnThreadpool(run_options->intra_op_thread_pool()));\n+  return YnnParams(std::move(threadpool));\n+}\n+\n+Thunk::YnnParams::YnnParams(YnnThreadpool threadpool)\n+    : threadpool(std::move(threadpool)) {}\n+#endif  // XLA_YNNPACK\n+\n Thunk::ExecuteSession::ExecuteSession(int64_t max_workers,\n                                       int64_t split_threshold)\n     : lock_(std::make_shared<std::nullopt_t>(std::nullopt)),"
        },
        {
            "sha": "4d98447b3c462881de2f9620a60e95532ef8896a",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk.h",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -47,6 +47,11 @@ limitations under the License.\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n+#ifdef XLA_YNNPACK\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n+#endif  // XLA_YNNPACK\n+\n namespace Eigen {\n struct ThreadPoolDevice;\n }  // namespace Eigen\n@@ -87,6 +92,7 @@ class Thunk {\n     kTopK,\n     kWhile,\n     kXnnFusion,\n+    kYnnFusion,\n     kOneDnnFusion,\n   };\n \n@@ -262,6 +268,25 @@ class Thunk {\n     explicit XnnParams(XnnThreadpool threadpool);\n   };\n \n+  //===--------------------------------------------------------------------===//\n+  // YnnParams\n+  //===--------------------------------------------------------------------===//\n+\n+#ifdef XLA_YNNPACK\n+  // Parameters capturing all the details required for running XNNPACK fusions.\n+  struct YnnParams {\n+    static absl::StatusOr<YnnParams> Create(\n+        const ExecutableRunOptions* run_options);\n+\n+    YnnThreadpool threadpool = nullptr;\n+\n+    explicit YnnParams(YnnThreadpool threadpool);\n+  };\n+#else\n+  // Use XnnParams for placeholder. The parameter won't be used anyway.\n+  using YnnParams = XnnParams;\n+#endif  // XLA_YNNPACK\n+\n   //===--------------------------------------------------------------------===//\n   // ExecuteParams\n   //===--------------------------------------------------------------------===//\n@@ -277,6 +302,7 @@ class Thunk {\n     CollectiveExecuteParams* collective_params = nullptr;\n     CustomCallExecuteParams* custom_call_params = nullptr;\n     XnnParams* xnn_params = nullptr;\n+    YnnParams* ynn_params = nullptr;\n     int64_t run_id = -1;          // -1 means no run id is set.\n     int64_t device_ordinal = -1;  // -1 means no device ordinal is set.\n     ExecuteSession session = ExecuteSession(ExecuteSession::kMaxWorkers,"
        },
        {
            "sha": "8daa2cb544d363aeb1a9e9d78e7b04596c895738",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_sequence_serdes_test.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -80,6 +80,10 @@ limitations under the License.\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n \n+#ifdef XLA_YNNPACK\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.h\"\n+#endif  // XLA_YNNPACK\n+\n namespace xla::cpu {\n namespace {\n \n@@ -1103,6 +1107,15 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n     return false;\n   }\n \n+#ifdef XLA_YNNPACK\n+  bool VerifyYnnFusionThunkEquality(const YnnFusionThunk& thunk_1,\n+                                    const YnnFusionThunk& thunk_2) {\n+    // TODO(ashaposhnikov) assume this is always false until we implement\n+    // serialization of YnnFusionThunk.\n+    return false;\n+  }\n+#endif  // XLA_YNNPACK\n+\n   bool VerifyXnnDotThunkEquality(const XnnDotThunk& thunk_1,\n                                  const XnnDotThunk& thunk_2) {\n     const bool are_dot_dimensions_equal =\n@@ -1412,6 +1425,24 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n                 tsl::down_cast<const XnnConvolutionThunk&>(thunk_2));\n         }\n       }\n+      case Thunk::Kind::kYnnFusion: {\n+#ifdef XLA_YNNPACK\n+        const YnnFusionThunk& ynn_fusion_thunk_1 =\n+            tsl::down_cast<const YnnFusionThunk&>(thunk_1);\n+        const YnnFusionThunk& ynn_fusion_thunk_2 =\n+            tsl::down_cast<const YnnFusionThunk&>(thunk_2);\n+        if (ynn_fusion_thunk_1.ynn_fusion_kind() !=\n+            ynn_fusion_thunk_2.ynn_fusion_kind()) {\n+          return false;\n+        }\n+        return VerifyYnnFusionThunkEquality(\n+            tsl::down_cast<const YnnFusionThunk&>(thunk_1),\n+            tsl::down_cast<const YnnFusionThunk&>(thunk_2));\n+#else\n+        CHECK(false) << \"Unsupported YNN fusion thunk type\";\n+        return false;\n+#endif  // XLA_YNNPACK\n+      }\n       case Thunk::Kind::kKernel:\n         return VerifyKernelThunkEquality(\n             tsl::down_cast<const KernelThunkBase&>(thunk_1),"
        },
        {
            "sha": "6e03bc5b4db070774c32ad7ee7ce66c3459228e9",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/BUILD",
            "status": "added",
            "additions": 103,
            "deletions": 0,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,103 @@\n+load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+load(\"//xla/tsl/xnnpack:build_defs.bzl\", \"ynn_cc_test\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = [\":friends\"],\n+    licenses = [\"notice\"],\n+)\n+\n+package_group(\n+    name = \"friends\",\n+    includes = [\n+        \"//xla:friends\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"ynn_interop\",\n+    srcs = [\"ynn_interop.cc\"],\n+    hdrs = [\"ynn_interop.h\"],\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla/tsl/platform:logging\",\n+        \"@XNNPACK//ynnpack\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/functional:function_ref\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"ynn_threadpool\",\n+    srcs = [\"ynn_threadpool.cc\"],\n+    hdrs = [\"ynn_threadpool.h\"],\n+    deps = [\n+        \":ynn_interop\",\n+        \"@XNNPACK//ynnpack:ynnpack_h\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@eigen_archive//:eigen3\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"ynn_fusion_thunk\",\n+    srcs = [\"ynn_fusion_thunk.cc\"],\n+    hdrs = [\"ynn_fusion_thunk.h\"],\n+    deps = [\n+        \":ynn_interop\",\n+        \"//xla:shape_util\",\n+        \"//xla/backends/cpu/runtime:thunk\",\n+        \"//xla/runtime:buffer_use\",\n+        \"//xla/runtime:object_pool\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/tsl/concurrency:async_value\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@XNNPACK//ynnpack:ynnpack_h\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/functional:bind_front\",\n+        \"@com_google_absl//absl/functional:function_ref\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+ynn_cc_test(\n+    name = \"ynn_fusion_thunk_test\",\n+    srcs = [\"ynn_fusion_thunk_test.cc\"],\n+    deps = [\n+        \":ynn_fusion_thunk\",\n+        \":ynn_interop\",\n+        \":ynn_threadpool\",\n+        \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/cpu/runtime:buffer_allocations\",\n+        \"//xla/backends/cpu/runtime:thunk\",\n+        \"//xla/backends/cpu/runtime:thunk_testlib\",\n+        \"//xla/tsl/concurrency:async_value\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test\",\n+        \"@XNNPACK//ynnpack:ynnpack_h\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@eigen_archive//:eigen3\",\n+    ],\n+)"
        },
        {
            "sha": "614a156dfbfcbabaf6966967898d631b0fed43be",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.cc",
            "status": "added",
            "additions": 371,
            "deletions": 0,
            "changes": 371,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,371 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <ostream>\n+#include <utility>\n+#include <vector>\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/base/no_destructor.h\"\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/functional/bind_front.h\"\n+#include \"absl/functional/function_ref.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/memory/memory.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/runtime/buffer_use.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/logging.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::cpu {\n+\n+absl::string_view YnnFusionThunk::YnnFusionKindToString(YnnFusionKind kind) {\n+  switch (kind) {\n+    case YnnFusionKind::kFusion:\n+      return \"ynn-fusion\";\n+  }\n+}\n+\n+std::ostream& operator<<(std::ostream& os, YnnFusionThunk::YnnFusionKind kind) {\n+  return os << YnnFusionThunk::YnnFusionKindToString(kind);\n+}\n+\n+// YNNPACK executable instantiated for the fusion operation.\n+struct YnnFusionThunk::YnnExecutable {\n+  tsl::AsyncValueRef<YnnFusionThunk::ExecuteEvent> Invoke(\n+      const YnnThreadpool& threadpool,\n+      absl::Span<se::DeviceMemoryBase> arguments,\n+      absl::Span<se::DeviceMemoryBase> results,\n+      absl::FunctionRef<bool(size_t)> is_captured_argument);\n+\n+  // Resets YNNPACK runtime and subgraph.\n+  absl::Status Reset();\n+\n+  YnnSubgraph subgraph = nullptr;\n+  YnnRuntime runtime = nullptr;\n+\n+  // TODO(ezhulenev): Today we rely on device memory as an identity of the\n+  // captured argument, and this is not correct as we can have multiple\n+  // arguments allocated to the heap address. This is work in progress, and will\n+  // be migrated to a buffer identity passed to XLA by the client (PjRt).\n+  std::vector<se::DeviceMemoryBase> captured_arguments;\n+};\n+\n+namespace {\n+struct YnnExternalValue {\n+  uint32_t id;\n+  void* data;\n+};\n+}  // namespace\n+\n+static enum ynn_status set_external_values(\n+    ynn_runtime_t runtime, absl::Span<const YnnExternalValue> external_values) {\n+  for (const auto& [id, data] : external_values) {\n+    enum ynn_status status = ynn_set_external_value_data(runtime, id, data);\n+    if (status != ynn_status_success) {\n+      return status;\n+    }\n+  }\n+  return ynn_status_success;\n+}\n+\n+tsl::AsyncValueRef<YnnFusionThunk::ExecuteEvent>\n+YnnFusionThunk::YnnExecutable::Invoke(\n+    const YnnThreadpool& threadpool, absl::Span<se::DeviceMemoryBase> arguments,\n+    absl::Span<se::DeviceMemoryBase> results,\n+    absl::FunctionRef<bool(size_t)> is_captured_argument) {\n+  // Create external values for all arguments and results.\n+  absl::InlinedVector<YnnExternalValue, 8> external_values;\n+  external_values.reserve(arguments.size() + results.size());\n+\n+  // External tensor id for arguments and results.\n+  uint32_t id = 0;\n+\n+  for (const se::DeviceMemoryBase& argument : arguments) {\n+    YnnExternalValue value{id++, argument.opaque()};\n+    if (!is_captured_argument(value.id)) {\n+      external_values.push_back(value);\n+    }\n+  }\n+\n+  for (const se::DeviceMemoryBase& result : results) {\n+    YnnExternalValue value{id++, result.opaque()};\n+    external_values.push_back(value);\n+  }\n+\n+  DCHECK_NE(runtime.get(), nullptr) << \"YNNPACK runtime is not initialized\";\n+\n+  YNN_RETURN_IF_ERROR(set_external_values(runtime.get(), external_values));\n+\n+  // Update threadpool used by the YNNPACK runtime.\n+  YNN_RETURN_IF_ERROR(ynn_update_runtime_with_threadpool(\n+      runtime.get(), reinterpret_cast<ynn_threadpool_t>(threadpool.get())));\n+\n+  // Execute YNNPACK runtime in the caller thread.\n+  YNN_RETURN_IF_ERROR(ynn_invoke_runtime(runtime.get()));\n+  return OkExecuteEvent();\n+}\n+\n+absl::Status YnnFusionThunk::YnnExecutable::Reset() {\n+  runtime.reset();\n+  subgraph.reset();\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<YnnFusionThunk::YnnExecutable>\n+YnnFusionThunk::CreateYnnExecutable(\n+    const YnnThreadpool& threadpool,\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+  bool capturing = !captured_arguments_ids_.empty();\n+  VLOG(3) << absl::StreamFormat(\n+      \"Create %s YNN executable for `%s` operation: num_created=%d\",\n+      capturing ? \"capturing\" : \"pooled\", info().op_name,\n+      capturing ? num_capturing_created_.fetch_add(1)\n+                : ynn_executable_pool_.num_created());\n+\n+  YnnExecutable executable;\n+\n+  // Keep track of the arguments captured by value.\n+  executable.captured_arguments = CaptureArguments(arguments_buffers);\n+\n+  if (builder_) {\n+    TF_ASSIGN_OR_RETURN(executable.subgraph, builder_(arguments_, results_));\n+  } else {\n+    TF_ASSIGN_OR_RETURN(\n+        executable.subgraph,\n+        capturing_builder_(arguments_, results_, arguments_buffers));\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      executable.runtime, CreateYnnRuntime([&](ynn_runtime_t* runtime) {\n+        uint32_t ynn_flags = 0;\n+        return ynn_create_runtime(\n+            executable.subgraph.get(),\n+            reinterpret_cast<ynn_threadpool_t>(threadpool.get()), ynn_flags,\n+            runtime);\n+      }));\n+  YNN_RETURN_IF_ERROR(ynn_reshape_runtime(executable.runtime.get()));\n+\n+  return {std::move(executable)};\n+}\n+\n+absl::Status YnnFusionThunk::UpdateYnnExecutable(\n+    const YnnThreadpool& threadpool, YnnExecutable& executable,\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+  DCHECK(capturing_builder_) << \"YNN executable is not capturing arguments\";\n+  DCHECK_EQ(executable.captured_arguments.size(),\n+            captured_arguments_ids_.size())\n+      << \"Unexpected number of captured arguments\";\n+\n+  // If all arguments captured by value are the same as the last execution,\n+  // we can reuse the YNN executable.\n+  auto capture_arguments = CaptureArguments(arguments_buffers);\n+  if (executable.captured_arguments == capture_arguments) {\n+    VLOG(3) << absl::StreamFormat(\"Reuse YNN executable for `%s` operation\",\n+                                  info().op_name);\n+    return absl::OkStatus();\n+  }\n+\n+  VLOG(3) << absl::StreamFormat(\"Update YNN executable for `%s` operation\",\n+                                info().op_name);\n+\n+  TF_RETURN_IF_ERROR(executable.Reset());\n+\n+  // Keep track of the updated arguments captured by value.\n+  executable.captured_arguments = std::move(capture_arguments);\n+\n+  TF_ASSIGN_OR_RETURN(\n+      executable.subgraph,\n+      capturing_builder_(arguments_, results_, arguments_buffers));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      executable.runtime, CreateYnnRuntime([&](ynn_runtime_t* runtime) {\n+        uint32_t ynn_flags = 0;\n+        return ynn_create_runtime(\n+            executable.subgraph.get(),\n+            reinterpret_cast<ynn_threadpool_t>(threadpool.get()), ynn_flags,\n+            runtime);\n+      }));\n+  YNN_RETURN_IF_ERROR(ynn_reshape_runtime(executable.runtime.get()));\n+\n+  return absl::OkStatus();\n+}\n+\n+std::vector<se::DeviceMemoryBase> YnnFusionThunk::CaptureArguments(\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+  std::vector<se::DeviceMemoryBase> captured_arguments_ids;\n+  captured_arguments_ids.reserve(captured_arguments_ids_.size());\n+  for (int64_t i = 0; i < captured_arguments_ids_.size(); ++i) {\n+    int32_t arg_index = captured_arguments_ids_[i];\n+    captured_arguments_ids.push_back(arguments_buffers[arg_index]);\n+  }\n+  return captured_arguments_ids;\n+}\n+\n+absl::StatusOr<std::unique_ptr<YnnFusionThunk>> YnnFusionThunk::Create(\n+    Options options, Info info, std::vector<Argument> arguments,\n+    std::vector<Result> results, Builder builder) {\n+  return absl::WrapUnique(new YnnFusionThunk(\n+      YnnFusionKind::kFusion, std::move(options), std::move(info),\n+      std::move(arguments), std::move(results), std::move(builder)));\n+}\n+\n+absl::StatusOr<std::unique_ptr<YnnFusionThunk>> YnnFusionThunk::Create(\n+    Options options, Info info, std::vector<Argument> arguments,\n+    std::vector<Result> results, CapturingBuilder capturing_builder,\n+    absl::Span<const int64_t> captured_arguments_ids) {\n+  return absl::WrapUnique(new YnnFusionThunk(\n+      YnnFusionKind::kFusion, std::move(options), std::move(info),\n+      std::move(arguments), std::move(results), std::move(capturing_builder),\n+      captured_arguments_ids));\n+}\n+\n+YnnFusionThunk::YnnFusionThunk(YnnFusionKind kind, Options options, Info info,\n+                               std::vector<Argument> arguments,\n+                               std::vector<Result> results, Builder builder)\n+    : Thunk(Kind::kYnnFusion, std::move(info)),\n+      ynn_fusion_kind_(kind),\n+      options_(std::move(options)),\n+      arguments_(std::move(arguments)),\n+      results_(std::move(results)),\n+      builder_(std::move(builder)),\n+      ynn_executable_pool_(\n+          absl::bind_front(&YnnFusionThunk::CreateYnnExecutable, this)) {}\n+\n+YnnFusionThunk::YnnFusionThunk(YnnFusionKind kind, Options options, Info info,\n+                               std::vector<Argument> arguments,\n+                               std::vector<Result> results,\n+                               CapturingBuilder capturing_builder,\n+                               absl::Span<const int64_t> captured_arguments_ids)\n+    : Thunk(Kind::kYnnFusion, std::move(info)),\n+      ynn_fusion_kind_(kind),\n+      options_(std::move(options)),\n+      arguments_(std::move(arguments)),\n+      results_(std::move(results)),\n+      capturing_builder_(std::move(capturing_builder)),\n+      captured_arguments_ids_(captured_arguments_ids.begin(),\n+                              captured_arguments_ids.end()),\n+      ynn_executable_pool_(\n+          absl::bind_front(&YnnFusionThunk::CreateYnnExecutable, this)) {}\n+\n+YnnFusionThunk::~YnnFusionThunk() = default;\n+\n+YnnFusionThunk::BufferUses YnnFusionThunk::buffer_uses() const {\n+  BufferUses buffer_uses;\n+  for (const Argument& argument : arguments_) {\n+    buffer_uses.push_back(BufferUse::Read(argument.slice));\n+  }\n+  for (const Result& result : results_) {\n+    buffer_uses.push_back(BufferUse::Write(result.slice));\n+  }\n+\n+  return buffer_uses;\n+}\n+\n+const YnnThreadpool& GetYnnThreadpool(const Thunk::ExecuteParams& params) {\n+  static absl::NoDestructor<YnnThreadpool> no_threadpool(nullptr);\n+  return params.ynn_params ? params.ynn_params->threadpool : *no_threadpool;\n+}\n+\n+tsl::AsyncValueRef<YnnFusionThunk::ExecuteEvent> YnnFusionThunk::Execute(\n+    const ExecuteParams& params) {\n+  VLOG(3) << absl::StreamFormat(\"YNN %s `%s`: %s\", fusion_kind(),\n+                                info().op_name, fusion_description());\n+\n+  if (VLOG_IS_ON(3) && has_fusion_details()) {\n+    for (auto& detail : fusion_details()) {\n+      VLOG(3) << detail;\n+    }\n+  }\n+\n+  // Resolve device memory for arguments.\n+  absl::InlinedVector<se::DeviceMemoryBase, 8> arguments_buffers;\n+  arguments_buffers.resize(arguments_.size());\n+  for (size_t i = 0; i < arguments_.size(); ++i) {\n+    Argument& argument = arguments_[i];\n+\n+    TF_ASSIGN_OR_RETURN(\n+        arguments_buffers[i],\n+        params.buffer_allocations->GetDeviceAddress(argument.slice));\n+\n+    VLOG(3) << absl::StreamFormat(\"  %s: %s in slice %s (%p)\", argument_name(i),\n+                                  argument.shape.ToString(true),\n+                                  argument.slice.ToString(),\n+                                  arguments_buffers[i].opaque());\n+  }\n+\n+  // Resolve device memory for results.\n+  absl::InlinedVector<se::DeviceMemoryBase, 4> results_buffers;\n+  results_buffers.resize(results_.size());\n+  for (size_t i = 0; i < results_.size(); ++i) {\n+    Result& result = results_[i];\n+\n+    TF_ASSIGN_OR_RETURN(\n+        results_buffers[i],\n+        params.buffer_allocations->GetDeviceAddress(results_[i].slice));\n+\n+    VLOG(3) << absl::StreamFormat(\"  %s: %s in slice %s (%p)\", result_name(i),\n+                                  result.shape.ToString(true),\n+                                  result.slice.ToString(),\n+                                  results_buffers[i].opaque());\n+  }\n+\n+  DCHECK(builder_ || capturing_builder_) << \"One of the builders must be set.\";\n+\n+  auto invoke = [&](typename YnnExecutablePool::BorrowedObject executable) {\n+    auto executed = executable->Invoke(\n+        GetYnnThreadpool(params), absl::MakeSpan(arguments_buffers),\n+        absl::MakeSpan(results_buffers), [&](size_t id) {\n+          return absl::c_linear_search(captured_arguments_ids_, id);\n+        });\n+\n+    // Do not return executable to the pool until the execution is done.\n+    executed.AndThen([executable = std::move(executable)] {});\n+    return executed;\n+  };\n+\n+  // Borrow YnnExecutable from the pool.\n+  TF_ASSIGN_OR_RETURN(auto executable,\n+                      ynn_executable_pool_.GetOrCreate(GetYnnThreadpool(params),\n+                                                       arguments_buffers));\n+\n+  // If YNN graph doesn't capture any of the arguments by value, we can execute\n+  // XnnExecutable immediately.\n+  if (captured_arguments_ids_.empty()) {\n+    return invoke(std::move(executable));\n+  }\n+\n+  // Otherwise reset YnnExecutable to capture new arguments buffers.\n+  TF_RETURN_IF_ERROR(UpdateYnnExecutable(GetYnnThreadpool(params), *executable,\n+                                         arguments_buffers));\n+  return invoke(std::move(executable));\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "2afd9e133c904a1790abcccc88beb7b633baa3d7",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.h",
            "status": "added",
            "additions": 182,
            "deletions": 0,
            "changes": 182,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,182 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_FUSION_THUNK_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_FUSION_THUNK_H_\n+\n+#include <stdbool.h>\n+\n+#include <atomic>\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <ostream>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/runtime/object_pool.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+\n+namespace xla::cpu {\n+\n+// YNN fusion thunk encapsulates YNNPACK subgraph constructed from an XLA fusion\n+// operation.\n+class YnnFusionThunk : public Thunk {\n+ public:\n+  enum class YnnFusionKind {\n+    kFusion,\n+  };\n+\n+  static absl::string_view YnnFusionKindToString(YnnFusionKind kind);\n+\n+  ~YnnFusionThunk() override;\n+\n+  struct Options {\n+    // Pass YnnThreadpool constructed from the intra-op threadpool to the\n+    // YNNPACK runtime to allow YNNPACK to parallelize the execution.\n+    bool use_threadpool = true;\n+  };\n+\n+  struct Argument {\n+    BufferAllocation::Slice slice;\n+    Shape shape;\n+  };\n+\n+  struct Result {\n+    BufferAllocation::Slice slice;\n+    Shape shape;\n+  };\n+\n+  // Builder function constructs YNNPACK subgraph for the fusion operation.\n+  using Builder = absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n+      absl::Span<const Argument> arguments, absl::Span<const Result> results)>;\n+\n+  // Builder function that constructs YNNPACK subgraph for the fusion operation\n+  // and captures some of the arguments buffers by value. Such YNNPACK subgraphs\n+  // can't be reused if captured arguments are not the same, and can lead to\n+  // crashes and undefined behavior if captured arguments are destroyed.\n+  // Capturing arguments by value allows YNNPACK to do packing at graph compile\n+  // time, and avoid re-packing costs at run time (at inference weights stay\n+  // constant, i.e. convolution filters and one of the dot arguments).\n+  using CapturingBuilder = absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n+      absl::Span<const Argument> arguments, absl::Span<const Result> results,\n+      absl::Span<const se::DeviceMemoryBase> arguments_buffers)>;\n+\n+  static absl::StatusOr<std::unique_ptr<YnnFusionThunk>> Create(\n+      Options options, Info info, std::vector<Argument> arguments,\n+      std::vector<Result> results, Builder builder);\n+\n+  static absl::StatusOr<std::unique_ptr<YnnFusionThunk>> Create(\n+      Options options, Info info, std::vector<Argument> arguments,\n+      std::vector<Result> results, CapturingBuilder capturing_builder,\n+      absl::Span<const int64_t> captured_arguments_ids);\n+\n+  tsl::AsyncValueRef<ExecuteEvent> Execute(const ExecuteParams& params) final;\n+\n+  bool ExecuteMayBlock() const final { return true; }\n+\n+  BufferUses buffer_uses() const final;\n+\n+  Options options() const { return options_; }\n+\n+  YnnFusionKind ynn_fusion_kind() const { return ynn_fusion_kind_; }\n+\n+ protected:\n+  YnnFusionThunk(YnnFusionKind kind, Options options, Info info,\n+                 std::vector<Argument> arguments, std::vector<Result> results,\n+                 Builder builder);\n+\n+  YnnFusionThunk(YnnFusionKind kind, Options options, Info info,\n+                 std::vector<Argument> arguments, std::vector<Result> results,\n+                 CapturingBuilder capturing_builder,\n+                 absl::Span<const int64_t> captured_arguments_ids);\n+\n+  // Extension points for subclasses to customize the logging behavior.\n+  virtual std::string fusion_kind() const { return \"fusion\"; }\n+  virtual std::string fusion_description() const { return \"\"; }\n+\n+  virtual bool has_fusion_details() const { return false; }\n+  virtual std::vector<std::string> fusion_details() const { return {}; }\n+\n+  virtual std::string argument_name(size_t index) const {\n+    return absl::StrCat(\"arg #\", index);\n+  }\n+\n+  virtual std::string result_name(size_t index) const {\n+    return absl::StrCat(\"res #\", index);\n+  }\n+\n+ private:\n+  // YNNPACK subgraph + runtime instantiated and ready for execution.\n+  struct YnnExecutable;\n+\n+  // Creates YnnExecutable for the fusion operation using one of the builders.\n+  absl::StatusOr<YnnExecutable> CreateYnnExecutable(\n+      const YnnThreadpool& threadpool,\n+      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+\n+  // Updates YnnExecutable to the YNN subgraph constructed with the given\n+  // arguments buffers.\n+  absl::Status UpdateYnnExecutable(\n+      const YnnThreadpool& threadpool, YnnExecutable& executable,\n+      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+\n+  // Returns the list of captured arguments buffers.\n+  std::vector<se::DeviceMemoryBase> CaptureArguments(\n+      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+\n+  YnnFusionKind ynn_fusion_kind_;\n+  Options options_;\n+\n+  std::vector<Argument> arguments_;\n+  std::vector<Result> results_;\n+\n+  // Builder that constructs YNNPACK subgraph for the fusion operation.\n+  Builder builder_;\n+\n+  // Builder that constructs YNNPACK subgraph for the fusion operation and\n+  // captures some of the arguments buffers by value. Such subgraphs can't be\n+  // reused if captured arguments changed since the last execution.\n+  CapturingBuilder capturing_builder_;\n+\n+  // Indices of arguments that are captured by YNNPACK subgraph by value.\n+  std::vector<int64_t> captured_arguments_ids_;\n+\n+  // XLA:CPU executable can be called concurrently from multiple threads,\n+  // and we need to keep a pool of YNNPACK executables to avoid data races.\n+  using YnnExecutablePool = ObjectPool<YnnExecutable, const YnnThreadpool&,\n+                                       absl::Span<const se::DeviceMemoryBase>>;\n+  YnnExecutablePool ynn_executable_pool_;\n+\n+  // The number of YNNPACK executables created for capturing graphs.\n+  std::atomic<int64_t> num_capturing_created_{0};\n+};\n+\n+std::ostream& operator<<(std::ostream& os, YnnFusionThunk::YnnFusionKind kind);\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_FUSION_THUNK_H_"
        },
        {
            "sha": "9e10fee93e2b8f335e10b0cfc9859460e754d941",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk_test.cc",
            "status": "added",
            "additions": 161,
            "deletions": 0,
            "changes": 161,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk_test.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,161 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/cpu/runtime/buffer_allocations.h\"\n+#include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/backends/cpu/runtime/thunk_testlib.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+#define EIGEN_USE_THREADS\n+#include \"unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace xla::cpu {\n+namespace {\n+\n+static absl::StatusOr<YnnSubgraph> BuildBinaryAddSubgraph(\n+    absl::Span<const YnnFusionThunk::Argument> arguments,\n+    absl::Span<const YnnFusionThunk::Result> results) {\n+  TF_ASSIGN_OR_RETURN(YnnSubgraph subgraph,\n+                      CreateYnnSubgraph([&](ynn_subgraph_t* subgraph) {\n+                        return ynn_create_subgraph(\n+                            /*external_value_ids=*/3,\n+                            /*flags=*/0, subgraph);\n+                      }));\n+\n+  auto dims = [](absl::Span<const int64_t> dims) -> std::vector<size_t> {\n+    return {dims.begin(), dims.end()};\n+  };\n+\n+  uint32_t lhs_id = 0;\n+  uint32_t rhs_id = 1;\n+  uint32_t out_id = 2;\n+\n+  std::vector<size_t> lhs_dims = dims(arguments[0].shape.dimensions());\n+  std::vector<size_t> rhs_dims = dims(arguments[1].shape.dimensions());\n+  std::vector<size_t> out_dims = dims(results[0].shape.dimensions());\n+\n+  YNN_RETURN_IF_ERROR(\n+      ynn_define_tensor_value(subgraph.get(), ynn_type_fp32, lhs_dims.size(),\n+                              lhs_dims.data(), /*data=*/nullptr,\n+                              /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+                              /*scale_id=*/YNN_INVALID_VALUE_ID,\n+                              YNN_VALUE_FLAG_EXTERNAL_INPUT, &lhs_id));\n+\n+  YNN_RETURN_IF_ERROR(\n+      ynn_define_tensor_value(subgraph.get(), ynn_type_fp32, rhs_dims.size(),\n+                              rhs_dims.data(), /*data=*/nullptr,\n+                              /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+                              /*scale_id=*/YNN_INVALID_VALUE_ID,\n+                              YNN_VALUE_FLAG_EXTERNAL_INPUT, &rhs_id));\n+\n+  YNN_RETURN_IF_ERROR(\n+      ynn_define_tensor_value(subgraph.get(), ynn_type_fp32, rhs_dims.size(),\n+                              rhs_dims.data(), /*data=*/nullptr,\n+                              /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+                              /*scale_id=*/YNN_INVALID_VALUE_ID,\n+                              YNN_VALUE_FLAG_EXTERNAL_OUTPUT, &out_id));\n+\n+  YNN_RETURN_IF_ERROR(ynn_define_binary(subgraph.get(), ynn_binary_add, lhs_id,\n+                                        rhs_id, &out_id, /*flags=*/0));\n+\n+  return subgraph;\n+}\n+\n+class YnnFusionThunkTest : public testing::TestWithParam<bool> {\n+ public:\n+  static std::string Name(const ::testing::TestParamInfo<bool>& info) {\n+    return absl::StrCat(info.param ? \"threadpool\" : \"single_threaded\");\n+  }\n+\n+ protected:\n+  bool use_threadpool() const { return GetParam(); }\n+};\n+\n+TEST_P(YnnFusionThunkTest, ElementwiseAdd) {\n+  if (use_threadpool()) {\n+    GTEST_SKIP() << \"Threadpool is not yet supported. Needs more clean-up.\";\n+  }\n+\n+  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 8);\n+  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n+                                 threads.NumThreads());\n+\n+  auto lhs = LiteralUtil::CreateR1<float>({1.0, 2.0, 3.0, 4.0});\n+  auto rhs = LiteralUtil::CreateR1<float>({4.0, 3.0, 2.0, 1.0});\n+  auto out = LiteralUtil::CreateR1<float>({0.0, 0.0, 0.0, 0.0});\n+\n+  BufferAllocations allocations = CreateBufferAllocations(lhs, rhs, out);\n+\n+  auto [lhs_alloc, rhs_alloc, out_alloc] =\n+      CreateBufferAllocation(lhs, rhs, out);\n+  auto [lhs_slice, rhs_slice, out_slice] =\n+      CreateBufferAllocationSlice(lhs_alloc, rhs_alloc, out_alloc);\n+\n+  Shape shape = ShapeUtil::MakeShape(F32, {2, 2});\n+\n+  YnnFusionThunk::Argument lhs_arg = {lhs_slice, shape};\n+  YnnFusionThunk::Argument rhs_arg = {rhs_slice, shape};\n+  YnnFusionThunk::Result out_res = {out_slice, shape};\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto thunk, YnnFusionThunk::Create(\n+                      YnnFusionThunk::Options{use_threadpool()}, {\"fusion\"},\n+                      {lhs_arg, rhs_arg}, {out_res}, &BuildBinaryAddSubgraph));\n+\n+  YnnThreadpool threadpool;\n+  if (use_threadpool()) {\n+    TF_ASSERT_OK_AND_ASSIGN(threadpool, CreateYnnThreadpool(&device));\n+  }\n+  Thunk::YnnParams ynn_params(std::move(threadpool));\n+\n+  Thunk::ExecuteParams params;\n+  params.buffer_allocations = &allocations;\n+  params.intra_op_threadpool = use_threadpool() ? &device : nullptr;\n+  params.ynn_params = &ynn_params;\n+\n+  auto execute_event = thunk->Execute(params);\n+  tsl::BlockUntilReady(execute_event);\n+  ASSERT_FALSE(execute_event.IsError()) << execute_event.GetError();\n+\n+  EXPECT_EQ(out, LiteralUtil::CreateR1<float>({5.0, 5.0, 5.0, 5.0}));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(YnnFusion, YnnFusionThunkTest, ::testing::Bool(),\n+                         YnnFusionThunkTest::Name);\n+\n+}  // namespace\n+}  // namespace xla::cpu"
        },
        {
            "sha": "29342a02af6518b2ba06389d69a5b31d3418d02f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_interop.cc",
            "status": "added",
            "additions": 61,
            "deletions": 0,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,61 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/functional/function_ref.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::cpu {\n+\n+absl::StatusOr<YnnSubgraph> CreateYnnSubgraph(\n+    absl::FunctionRef<ynn_status(ynn_subgraph_t*)> builder) {\n+  ynn_subgraph_t subgraph = nullptr;\n+  YNN_RETURN_IF_ERROR(builder(&subgraph));\n+  return YnnSubgraph(subgraph);\n+}\n+\n+absl::StatusOr<YnnRuntime> CreateYnnRuntime(\n+    absl::FunctionRef<ynn_status(ynn_runtime_t*)> builder) {\n+  ynn_runtime_t runtime = nullptr;\n+  YNN_RETURN_IF_ERROR(builder(&runtime));\n+  return YnnRuntime(runtime);\n+}\n+\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    absl::FunctionRef<ynn_status(ynn_threadpool_t*)> builder) {\n+  ynn_threadpool_t threadpool = nullptr;\n+  YNN_RETURN_IF_ERROR(builder(&threadpool));\n+  return YnnThreadpool(threadpool);\n+}\n+\n+absl::StatusOr<ynn_type> YnnType(const PrimitiveType& type) {\n+  switch (type) {\n+    case BF16:\n+      return ynn_type_bf16;\n+    case F16:\n+      return ynn_type_fp16;\n+    case F32:\n+      return ynn_type_fp32;\n+    default:\n+      return InvalidArgument(\"Unsupported YNNPACK type: %s\",\n+                             primitive_util::LowercasePrimitiveTypeName(type));\n+  }\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "75189abf52f7f12b0d2c57b883fe617f079bc69a",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_interop.h",
            "status": "added",
            "additions": 111,
            "deletions": 0,
            "changes": 111,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,111 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_INTEROP_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_INTEROP_H_\n+\n+#include <memory>\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/base/optimization.h\"\n+#include \"absl/functional/function_ref.h\"\n+#include \"absl/status/status.h\"\n+#include \"xla/tsl/platform/logging.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::cpu {\n+\n+//===----------------------------------------------------------------------===//\n+// YNNPACK status to ABSL status conversion macros.\n+//===----------------------------------------------------------------------===//\n+\n+#define YNN_RETURN_IF_ERROR(expr)             \\\n+  do {                                        \\\n+    absl::Status s = YnnStatusToStatus(expr); \\\n+    if (!s.ok()) {                            \\\n+      return s;                               \\\n+    }                                         \\\n+  } while (0)\n+\n+#define YNN_LOG_IF_ERROR(expr)                         \\\n+  do {                                                 \\\n+    absl::Status s = YnnStatusToStatus(expr);          \\\n+    if (!s.ok()) {                                     \\\n+      LOG(ERROR) << \"YNNPACK operation failed: \" << s; \\\n+    }                                                  \\\n+  } while (0)\n+\n+// Converts YNNPACK status to absl::Status.\n+inline absl::Status YnnStatusToStatus(ynn_status status) {\n+  if (ABSL_PREDICT_TRUE(status == ynn_status_success)) {\n+    return absl::OkStatus();\n+  }\n+\n+  auto error_message = [](ynn_status status) {\n+    switch (status) {\n+      case ynn_status_success:\n+        return \"\";\n+      case ynn_status_deprecated:\n+        return \"deprecated\";\n+      case ynn_status_error:\n+        return \"error\";\n+      case ynn_status_invalid_parameter:\n+        return \"invalid parameter\";\n+      case ynn_status_unsupported_parameter:\n+        return \"unsupported parameter\";\n+    }\n+  };\n+\n+  return Internal(\"YNNPACK operation failed: %s\", error_message(status));\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// XLA to YNNPACK type conversions.\n+//===----------------------------------------------------------------------===//\n+\n+absl::StatusOr<ynn_type> YnnType(const PrimitiveType& type);\n+\n+//===----------------------------------------------------------------------===//\n+// RAII wrappers for YNNPACK types.\n+//===----------------------------------------------------------------------===//\n+\n+namespace internal {\n+\n+struct YnnDeleter {\n+  void operator()(ynn_subgraph* subgraph) { ynn_delete_subgraph(subgraph); }\n+  void operator()(ynn_runtime* runtime) { ynn_delete_runtime(runtime); }\n+  void operator()(ynn_threadpool* threadpool) {\n+    ynn_delete_threadpool(threadpool);\n+  }\n+};\n+\n+}  // namespace internal\n+\n+using YnnSubgraph = std::unique_ptr<ynn_subgraph, internal::YnnDeleter>;\n+using YnnRuntime = std::unique_ptr<ynn_runtime, internal::YnnDeleter>;\n+using YnnThreadpool = std::unique_ptr<ynn_threadpool, internal::YnnDeleter>;\n+\n+absl::StatusOr<YnnSubgraph> CreateYnnSubgraph(\n+    absl::FunctionRef<ynn_status(ynn_subgraph_t*)> builder);\n+\n+absl::StatusOr<YnnRuntime> CreateYnnRuntime(\n+    absl::FunctionRef<ynn_status(ynn_runtime_t*)> builder);\n+\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    absl::FunctionRef<ynn_status(ynn_threadpool_t*)> builder);\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_INTEROP_H_"
        },
        {
            "sha": "1736bbbde2c3165e9e6ee68e6148905d8580f82f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_threadpool.cc",
            "status": "added",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,62 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n+\n+#include <cstdint>\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/base/optimization.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+\n+#define EIGEN_USE_THREADS\n+#include \"Eigen/ThreadPool\"\n+#include \"unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace xla::cpu {\n+\n+static int32_t NumThreads(void* pool) {\n+  if (ABSL_PREDICT_FALSE(pool == nullptr)) {\n+    return 0;\n+  }\n+  return reinterpret_cast<Eigen::ThreadPoolInterface*>(pool)->NumThreads();\n+}\n+\n+static void Schedule(void* pool, void* context, void (*task)(void* context)) {\n+  if (ABSL_PREDICT_FALSE(pool == nullptr)) {\n+    (*task)(context);\n+  }\n+  reinterpret_cast<Eigen::ThreadPoolInterface*>(pool)->Schedule(\n+      [task, context]() { (*task)(context); });\n+}\n+\n+// An adaptor from Eigen::ThreadPoolInterface to xnn_threadpool_t.\n+static constexpr ynn_scheduler kYnnScheduler = {&NumThreads, &Schedule};\n+\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    Eigen::ThreadPoolInterface* threadpool) {\n+  return CreateYnnThreadpool([&](ynn_threadpool_t* ynn_threadpool) {\n+    return ynn_create_threadpool(&kYnnScheduler, threadpool, /*flags=*/1,\n+                                 ynn_threadpool);\n+  });\n+}\n+\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    const Eigen::ThreadPoolDevice* device) {\n+  return CreateYnnThreadpool(device->getPool());\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "9fe8186c34a64cbefd66cc01ae4280dde07ac90e",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h",
            "status": "added",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,39 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_THREADPOOL_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_THREADPOOL_H_\n+\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+\n+namespace Eigen {\n+struct ThreadPoolDevice;\n+class ThreadPoolInterface;\n+}  // namespace Eigen\n+\n+namespace xla::cpu {\n+\n+// Creates an YNNPACK threadpool from an Eigen threadpool.\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    Eigen::ThreadPoolInterface* threadpool);\n+\n+// Creates an YNNPACK threadpool from an Eigen ThreadPoolDevice.\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    const Eigen::ThreadPoolDevice* device);\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_YNNPACK_YNN_THREADPOOL_H_"
        },
        {
            "sha": "d865d5c56e17d3811c06c58ce906fbbc54c5629f",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.cc",
            "status": "added",
            "additions": 306,
            "deletions": 0,
            "changes": 306,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,306 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/ynn_emitter.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/backends/cpu/ynn_support.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/tsl/platform/logging.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla::cpu {\n+\n+// A mapping from HloInstruction to YNNPACK subgraph tensor id.\n+using TensorIdMap = absl::flat_hash_map<const HloInstruction*, uint32_t>;\n+\n+//===----------------------------------------------------------------------===//\n+// XLA <-> YNNPACK type conversion library.\n+//===----------------------------------------------------------------------===//\n+\n+static std::vector<size_t> YnnDimensions(const Shape& shape) {\n+  std::vector<size_t> dims;\n+  for (auto& dim : shape.dimensions()) {\n+    dims.push_back(dim);\n+  }\n+  return dims;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// XLA <-> YNNPACK emitters.\n+//===----------------------------------------------------------------------===//\n+\n+static absl::StatusOr<uint32_t> FindTensorValue(const TensorIdMap& tensor_ids,\n+                                                const HloInstruction* instr) {\n+  if (auto it = tensor_ids.find(instr); it != tensor_ids.end()) {\n+    return it->second;\n+  }\n+  return Internal(\"Can't fine YNNPACK tensor value for instruction %s\",\n+                  instr->ToString());\n+}\n+\n+static absl::StatusOr<uint32_t> DefineTensorValue(ynn_subgraph_t subgraph,\n+                                                  const HloInstruction* instr) {\n+  // We do not support instructions with multiple results (tuples).\n+  if (!instr->shape().IsArray()) {\n+    return Internal(\"Unsupported YNNPACK instruction shape: %s\",\n+                    instr->ToString());\n+  }\n+\n+  auto dims = YnnDimensions(instr->shape());\n+  TF_ASSIGN_OR_RETURN(auto type, YnnType(instr->shape().element_type()));\n+\n+  uint32_t tensor_id = YNN_INVALID_VALUE_ID;\n+  uint32_t tensor_flags = 0;\n+\n+  // If instruction is a root instruction of the parent computation we assign it\n+  // an external tensor id corresponding to the result index.\n+  const HloComputation* computation = instr->parent();\n+  if (computation->root_instruction() == instr) {\n+    tensor_id = computation->num_parameters();\n+    tensor_flags = YNN_VALUE_FLAG_EXTERNAL_OUTPUT;\n+  }\n+\n+  YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n+      subgraph, type, dims.size(), dims.data(), /*data=*/nullptr,\n+      /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+      /*scale_id=*/YNN_INVALID_VALUE_ID, tensor_flags, &tensor_id));\n+  return tensor_id;\n+}\n+\n+static absl::StatusOr<uint32_t> DefineConstant(\n+    ynn_subgraph_t subgraph, std::vector<std::unique_ptr<Literal>>& literals,\n+    const HloInstruction* instr) {\n+  // We do not support instructions with multiple results (tuples).\n+  if (!instr->shape().IsArray()) {\n+    return Internal(\"Unsupported YNNPACK instruction shape: %s\",\n+                    instr->ToString());\n+  }\n+\n+  auto dims = YnnDimensions(instr->shape());\n+  TF_ASSIGN_OR_RETURN(auto type, YnnType(instr->shape().element_type()));\n+\n+  uint32_t tensor_id = YNN_INVALID_VALUE_ID;\n+\n+  literals.push_back(instr->literal().CloneToUnique());\n+  const void* value = literals.back()->untyped_data();\n+\n+  YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n+      subgraph, type, dims.size(), dims.data(), /*data=*/value,\n+      /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+      /*scale_id=*/YNN_INVALID_VALUE_ID,\n+      /*flags=*/0, &tensor_id));\n+\n+  return tensor_id;\n+}\n+\n+static absl::StatusOr<uint32_t> DefineParameter(ynn_subgraph_t subgraph,\n+                                                const HloInstruction* param) {\n+  VLOG(3) << absl::StreamFormat(\"Define tensor value for parameter: %s\",\n+                                param->ToString());\n+\n+  auto dims = YnnDimensions(param->shape());\n+  TF_ASSIGN_OR_RETURN(auto type, YnnType(param->shape().element_type()));\n+\n+  uint32_t tensor_id = param->parameter_number();\n+  YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n+      subgraph, type, dims.size(), dims.data(), /*data=*/nullptr,\n+      /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+      /*scale_id=*/YNN_INVALID_VALUE_ID, YNN_VALUE_FLAG_EXTERNAL_INPUT,\n+      &tensor_id));\n+\n+  return tensor_id;\n+}\n+\n+static absl::StatusOr<uint32_t> DefineBitcastOp(ynn_subgraph_t subgraph,\n+                                                TensorIdMap& tensor_ids,\n+                                                const HloInstruction* instr) {\n+  VLOG(3) << absl::StreamFormat(\"Define tensor value for bitcast op: %s\",\n+                                instr->ToString());\n+  CHECK_EQ(instr->opcode(), HloOpcode::kBitcast);\n+  const HloInstruction* input = instr->operand(0);\n+  CHECK_EQ(input->shape().element_type(), instr->shape().element_type());\n+  TF_ASSIGN_OR_RETURN(auto in, FindTensorValue(tensor_ids, input));\n+  TF_ASSIGN_OR_RETURN(auto out, DefineTensorValue(subgraph, instr));\n+\n+  auto dims = YnnDimensions(instr->shape());\n+  YNN_RETURN_IF_ERROR(ynn_define_static_reshape(subgraph, dims.size(),\n+                                                dims.data(), in, &out,\n+                                                /*flags=*/0));\n+  return out;\n+}\n+\n+static absl::StatusOr<uint32_t> DefineUnaryOp(ynn_subgraph_t subgraph,\n+                                              TensorIdMap& tensor_ids,\n+                                              const HloInstruction* instr) {\n+  VLOG(3) << absl::StreamFormat(\"Define tensor value for unary op: %s\",\n+                                instr->ToString());\n+  TF_ASSIGN_OR_RETURN(auto unary_op, YnnUnaryOperator(instr->opcode()));\n+\n+  TF_ASSIGN_OR_RETURN(auto in, FindTensorValue(tensor_ids, instr->operand(0)));\n+  TF_ASSIGN_OR_RETURN(auto out, DefineTensorValue(subgraph, instr));\n+\n+  VLOG(3) << absl::StreamFormat(\"  tensors: in=%d, out=%d\", in, out);\n+\n+  YNN_RETURN_IF_ERROR(\n+      ynn_define_unary(subgraph, unary_op, in, &out, /*flags=*/0));\n+\n+  return out;\n+}\n+\n+static absl::StatusOr<uint32_t> DefineBinaryOp(ynn_subgraph_t subgraph,\n+                                               TensorIdMap& tensor_ids,\n+                                               const HloInstruction* instr) {\n+  VLOG(3) << absl::StreamFormat(\"Define tensor value for binary op: %s\",\n+                                instr->ToString());\n+\n+  TF_ASSIGN_OR_RETURN(auto binary_op, YnnBinaryOperator(instr->opcode()));\n+\n+  TF_ASSIGN_OR_RETURN(auto lhs, FindTensorValue(tensor_ids, instr->operand(0)));\n+  TF_ASSIGN_OR_RETURN(auto rhs, FindTensorValue(tensor_ids, instr->operand(1)));\n+  TF_ASSIGN_OR_RETURN(auto out, DefineTensorValue(subgraph, instr));\n+\n+  VLOG(3) << absl::StreamFormat(\"  tensors: lhs=%d, rhs=%d, out=%d\", lhs, rhs,\n+                                out);\n+\n+  YNN_RETURN_IF_ERROR(\n+      ynn_define_binary(subgraph, binary_op, lhs, rhs, &out, /*flags=*/0));\n+\n+  return out;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Emit YNNPACK subgraph for the given HLO computation.\n+//===----------------------------------------------------------------------===//\n+\n+static absl::StatusOr<YnnSubgraph> EmitYnnSubgraph(\n+    const HloComputation* computation,\n+    std::vector<std::unique_ptr<Literal>>& literals) {\n+  VLOG(3) << \"Emit YNNPACK subgraph for computation: \" << computation->name();\n+\n+  TF_ASSIGN_OR_RETURN(\n+      YnnSubgraph subgraph, CreateYnnSubgraph([&](ynn_subgraph_t* subgraph) {\n+        return ynn_create_subgraph(\n+            /*external_value_ids=*/computation->num_parameters() + 1,\n+            /*flags=*/0, subgraph);\n+      }));\n+\n+  // Traverse fused computation in post-order and define YNNPACK operations\n+  // corresponding to each HLO instruction.\n+  TensorIdMap tensor_ids;\n+  auto instructions = computation->MakeInstructionPostOrder();\n+\n+  for (const HloInstruction* instr : instructions) {\n+    if (!IsLayoutSupportedByYnn(instr->shape())) {\n+      return InvalidArgument(\n+          \"Instruction with unsupported layout in YNN fusion: %s\",\n+          instr->ToString());\n+    }\n+\n+    if (instr->IsConstant()) {\n+      if (!IsConstantSupportedByYnn(instr)) {\n+        return InvalidArgument(\n+            \"Unsupported constant instruction in YNN fusion: %s\",\n+            instr->ToString());\n+      }\n+      TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n+                          DefineConstant(subgraph.get(), literals, instr));\n+      continue;\n+    }\n+\n+    if (instr->IsElementwise()) {\n+      if (!IsElementwiseOpSupportedByYnn(instr)) {\n+        return InvalidArgument(\n+            \"Unsupported elementwise instruction in YNN fusion: %s\",\n+            instr->ToString());\n+      }\n+      if (instr->operand_count() == 1) {\n+        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n+                            DefineUnaryOp(subgraph.get(), tensor_ids, instr));\n+      } else if (instr->operand_count() == 2) {\n+        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n+                            DefineBinaryOp(subgraph.get(), tensor_ids, instr));\n+      } else {\n+        LOG(FATAL) << \"Unexpected operand count \" << instr->operand_count();\n+      }\n+      continue;\n+    }\n+\n+    switch (instr->opcode()) {\n+      case HloOpcode::kParameter: {\n+        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n+                            DefineParameter(subgraph.get(), instr));\n+      } break;\n+\n+      case HloOpcode::kBitcast: {\n+        if (!IsBitcastOpSupportedByYnn(instr)) {\n+          return InvalidArgument(\n+              \"Unsupported bitcast instruction in YNN fusion: %s\",\n+              instr->ToString());\n+        }\n+        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n+                            DefineBitcastOp(subgraph.get(), tensor_ids, instr));\n+      } break;\n+\n+      default: {\n+        return InvalidArgument(\"Unsupported fusion instruction: %s\",\n+                               instr->ToString());\n+      }\n+    }\n+  }\n+\n+  return subgraph;\n+}\n+\n+absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>()>>\n+EmitYnnFusionBuilder(const HloComputation* computation) {\n+  // We do not support non-array parameters for YNNPACK operations.\n+  for (auto& param : computation->parameter_instructions()) {\n+    if (!param->shape().IsArray()) {\n+      return InvalidArgument(\n+          \"YNNPACK fusion parameters must have array shapes, got %s\",\n+          param->shape().ToString());\n+    }\n+  }\n+\n+  // Result also must be a single array.\n+  if (!computation->root_instruction()->shape().IsArray()) {\n+    return InvalidArgument(\"YNNPACK fusion result must be an array, got %s\",\n+                           computation->root_instruction()->shape().ToString());\n+  }\n+\n+  return [computation,\n+          literals = std::vector<std::unique_ptr<Literal>>()]() mutable {\n+    return EmitYnnSubgraph(computation, literals);\n+  };\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "86461960da46a4a4a0e5803748243a70d68a611e",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.h",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,31 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_YNN_EMITTER_H_\n+#define XLA_BACKENDS_CPU_YNN_EMITTER_H_\n+\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+\n+namespace xla::cpu {\n+\n+absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>()>>\n+EmitYnnFusionBuilder(const HloComputation* computation);\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_YNN_EMITTER_H_"
        },
        {
            "sha": "f9821859e2b2a1d66ae0906981606a5f09485fa2",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.cc",
            "status": "added",
            "additions": 141,
            "deletions": 0,
            "changes": 141,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,141 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/ynn_support.h\"\n+\n+#include <algorithm>\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/base/no_destructor.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::cpu {\n+\n+const absl::flat_hash_map<HloOpcode, ynn_unary_operator>& GetYnnUnaryOpMap() {\n+  static absl::NoDestructor<absl::flat_hash_map<HloOpcode, ynn_unary_operator>>\n+      unary_op_map({\n+          {HloOpcode::kAbs, ynn_unary_abs},\n+          {HloOpcode::kCeil, ynn_unary_ceil},\n+          {HloOpcode::kConvert, ynn_unary_convert},\n+          {HloOpcode::kCos, ynn_unary_cosine},\n+          {HloOpcode::kExp, ynn_unary_exp},\n+          {HloOpcode::kCbrt, ynn_unary_cube_root},\n+          {HloOpcode::kFloor, ynn_unary_floor},\n+          {HloOpcode::kLog, ynn_unary_log},\n+          {HloOpcode::kLogistic, ynn_unary_sigmoid},\n+          {HloOpcode::kNegate, ynn_unary_negate},\n+          {HloOpcode::kRoundNearestEven, ynn_unary_round},\n+          {HloOpcode::kRsqrt, ynn_unary_reciprocal_square_root},\n+          {HloOpcode::kSign, ynn_unary_sign},\n+          {HloOpcode::kSin, ynn_unary_sine},\n+          {HloOpcode::kSqrt, ynn_unary_square_root},\n+          {HloOpcode::kTanh, ynn_unary_tanh},\n+      });\n+  return *unary_op_map;\n+}\n+\n+absl::StatusOr<ynn_unary_operator> YnnUnaryOperator(const HloOpcode& opcode) {\n+  const auto& unary_op_map = GetYnnUnaryOpMap();\n+  auto result = unary_op_map.find(opcode);\n+  if (result == unary_op_map.end()) {\n+    return InvalidArgument(\"Unsupported YNNPACK unary operator: %s\",\n+                           HloOpcodeString(opcode));\n+  }\n+  return result->second;\n+}\n+\n+const absl::flat_hash_map<HloOpcode, ynn_binary_operator>& GetYnnBinaryOpMap() {\n+  static absl::NoDestructor<absl::flat_hash_map<HloOpcode, ynn_binary_operator>>\n+      binary_op_map({\n+          {HloOpcode::kAdd, ynn_binary_add},\n+          {HloOpcode::kDivide, ynn_binary_divide},\n+          {HloOpcode::kMaximum, ynn_binary_max},\n+          {HloOpcode::kMinimum, ynn_binary_min},\n+          {HloOpcode::kMultiply, ynn_binary_multiply},\n+          {HloOpcode::kPower, ynn_binary_pow},\n+          {HloOpcode::kSubtract, ynn_binary_subtract},\n+      });\n+  return *binary_op_map;\n+}\n+\n+absl::StatusOr<ynn_binary_operator> YnnBinaryOperator(const HloOpcode& opcode) {\n+  const auto& binary_op_map = GetYnnBinaryOpMap();\n+  auto result = binary_op_map.find(opcode);\n+  if (result == binary_op_map.end()) {\n+    return InvalidArgument(\"Unsupported YNNPACK binary operator: %s\",\n+                           HloOpcodeString(opcode));\n+  }\n+  return result->second;\n+}\n+\n+bool IsLayoutSupportedByYnn(const Shape& shape) {\n+  return !shape.has_layout() || LayoutUtil::HasDescendingLayout(shape.layout());\n+}\n+\n+bool IsBitcastOpSupportedByYnn(const HloInstruction* hlo) {\n+  CHECK_EQ(hlo->opcode(), HloOpcode::kBitcast);\n+  if (!YnnType(hlo->shape().element_type()).ok()) {\n+    return false;\n+  }\n+  const HloInstruction* input = hlo->operand(0);\n+  return hlo->shape().element_type() == input->shape().element_type();\n+}\n+\n+bool IsConstantSupportedByYnn(const HloInstruction* hlo) {\n+  CHECK(hlo->IsConstant());\n+\n+  if (!YnnType(hlo->shape().element_type()).ok()) {\n+    return false;\n+  }\n+\n+  return hlo->shape().IsArray();\n+}\n+\n+bool IsElementwiseOpSupportedByYnn(const HloInstruction* hlo) {\n+  CHECK(hlo->IsElementwise());\n+  // In XLA IsElementwise is true for constants.\n+  CHECK(!hlo->IsConstant());\n+\n+  if (!YnnType(hlo->shape().element_type()).ok()) {\n+    return false;\n+  }\n+\n+  if (!std::all_of(hlo->operands().begin(), hlo->operands().end(),\n+                   [](const HloInstruction* op) {\n+                     return YnnType(op->shape().element_type()).ok();\n+                   })) {\n+    return false;\n+  }\n+\n+  switch (hlo->operand_count()) {\n+    case 1:\n+      return YnnUnaryOperator(hlo->opcode()).ok();\n+    case 2:\n+      return YnnBinaryOperator(hlo->opcode()).ok();\n+    default:\n+      return false;\n+  }\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "74205d2dfc64f403d0b9187c283709a343670f14",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.h",
            "status": "added",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -0,0 +1,60 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_YNN_SUPPORT_H_\n+#define XLA_BACKENDS_CPU_YNN_SUPPORT_H_\n+\n+#include \"ynnpack/include/ynnpack.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+\n+namespace xla::cpu {\n+\n+inline constexpr absl::string_view kYnnFusionKind = \"__ynn_fusion\";\n+\n+// Returns the mappings from HLO opcodes to YNNPACK unary operators.\n+const absl::flat_hash_map<HloOpcode, ynn_unary_operator>& GetYnnUnaryOpMap();\n+\n+// Returns the YNNPACK unary operator corresponding to the given HLO opcode.\n+// Returns `InvalidArgument` if the opcode is not supported.\n+absl::StatusOr<ynn_unary_operator> YnnUnaryOperator(const HloOpcode& opcode);\n+\n+// Returns the mappings from HLO opcodes to YNNPACK binary operators.\n+const absl::flat_hash_map<HloOpcode, ynn_binary_operator>& GetYnnBinaryOpMap();\n+\n+// Returns the YNNPACK binary operator corresponding to the given HLO opcode.\n+// Returns `InvalidArgument` if the opcode is not supported.\n+absl::StatusOr<ynn_binary_operator> YnnBinaryOperator(const HloOpcode& opcode);\n+\n+// Returns true if the shape either doesn't have a layout or the layout is\n+// descending. Shapes without layout are accepted to make HLO tests less\n+// verbose.\n+bool IsLayoutSupportedByYnn(const Shape& shape);\n+\n+// Returns true if the bitcast op is supported by YNNPACK.\n+bool IsBitcastOpSupportedByYnn(const HloInstruction* hlo);\n+\n+// Returns true if the constant is supported by YNNPACK.\n+bool IsConstantSupportedByYnn(const HloInstruction* hlo);\n+\n+// Returns true if the nonconstant elementwise op is supported by YNNPACK.\n+bool IsElementwiseOpSupportedByYnn(const HloInstruction* hlo);\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_YNN_SUPPORT_H_"
        },
        {
            "sha": "b0ec4d20591fc11f9d8f79f1c933c0717a53c410",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -1676,6 +1676,12 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n                             cpu::Thunk::XnnParams::Create(&run_options));\n       }\n \n+      std::optional<cpu::Thunk::YnnParams> ynn_params;\n+      if (cpu_executable->has_ynn_fusions()) {\n+        TF_ASSIGN_OR_RETURN(ynn_params,\n+                            cpu::Thunk::YnnParams::Create(&run_options));\n+      }\n+\n       cpu::ThreadPoolTaskRunner task_runner(\n           run_options.intra_op_thread_pool()->getPool());\n \n@@ -1688,6 +1694,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n           &collective_params,\n           &custom_call_execute_params,\n           xnn_params ? &*xnn_params : nullptr,\n+          ynn_params ? &*ynn_params : nullptr,\n           run_options.run_id().ToInt(),\n           run_options.device_ordinal(),\n       };\n@@ -1814,6 +1821,12 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n               xnn_params = cpu::Thunk::XnnParams::Create(&run_options);\n             }\n \n+            absl::StatusOr<std::optional<cpu::Thunk::YnnParams>> ynn_params(\n+                std::nullopt);\n+            if (cpu_executable->has_ynn_fusions()) {\n+              ynn_params = cpu::Thunk::YnnParams::Create(&run_options);\n+            }\n+\n             cpu::ThreadPoolTaskRunner task_runner(\n                 run_options.intra_op_thread_pool()->getPool());\n \n@@ -1827,6 +1840,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n                   &*collective_params,\n                   &*custom_call_params,\n                   *xnn_params ? &**xnn_params : nullptr,\n+                  *ynn_params ? &**ynn_params : nullptr,\n                   run_options.run_id().ToInt(),\n                   run_options.device_ordinal(),\n               };"
        },
        {
            "sha": "47d09b42e5259a5a3c3ea090880602fb87e19754",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -24,6 +24,7 @@ load(\n     \"if_llvm_x86_available\",\n )\n load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+load(\"//xla/tsl/xnnpack:build_defs.bzl\", \"if_ynnpack\")\n load(\":build_defs.bzl\", \"runtime_copts\")\n \n package(\n@@ -931,7 +932,7 @@ cc_library(\n     srcs = [\"thunk_emitter.cc\"],\n     hdrs = [\"thunk_emitter.h\"],\n     copts = tsl_copts(),\n-    local_defines = if_graph_api([\"XLA_ONEDNN_USE_GRAPH_API=1\"]),\n+    local_defines = if_graph_api([\"XLA_ONEDNN_USE_GRAPH_API=1\"]) + if_ynnpack([\"XLA_YNNPACK\"]),\n     deps = [\n         \":backend_config_proto_cc\",\n         \":cpu_options\",\n@@ -1023,6 +1024,10 @@ cc_library(\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n     ] + if_onednn([\n         \"//xla/backends/cpu/runtime/onednn:onednn_op_thunk\",\n+    ]) + if_ynnpack([\n+        \"//xla/backends/cpu:ynn_emitter\",\n+        \"//xla/backends/cpu:ynn_support\",\n+        \"//xla/backends/cpu/runtime/ynnpack:ynn_fusion_thunk\",\n     ]),\n )\n "
        },
        {
            "sha": "40dacd9e10bf2c021a4e73307eb1dc0165168c27",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -110,6 +110,12 @@ absl::StatusOr<std::unique_ptr<CpuExecutable>> CpuExecutable::Create(\n     executable->has_xnn_fusions_ |= thunk.kind() == Thunk::Kind::kXnnFusion;\n   });\n \n+  // Find if the thunk sequence contains any YNN fusion thunks. If we do have\n+  // any, we will prepare the YNNPACK thread pool for them at run time.\n+  executable->thunks_->thunk_sequence().ForEach([&](const Thunk& thunk) {\n+    executable->has_ynn_fusions_ |= thunk.kind() == Thunk::Kind::kYnnFusion;\n+  });\n+\n   // Re-index constants by their allocation index to allow efficient lookup.\n   for (auto& constant : constants) {\n     if (executable->constants_.size() <= constant.index) {"
        },
        {
            "sha": "9704d8488450f5161cea6e859c6f31f5875dc1f2",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -128,6 +128,7 @@ class CpuExecutable : public Executable {\n   ThunkExecutor& thunks() { return *thunks_; }\n \n   bool has_xnn_fusions() const { return has_xnn_fusions_; }\n+  bool has_ynn_fusions() const { return has_ynn_fusions_; }\n \n   const BufferAssignment& buffer_assignment() const { return *assignment_; }\n   absl::Span<const ConstantAllocation> constants() const { return constants_; }\n@@ -230,6 +231,9 @@ class CpuExecutable : public Executable {\n   // Whether the thunk executor contains any XNN fusion thunks.\n   bool has_xnn_fusions_ = false;\n \n+  // Whether the thunk executor contains any YNN fusion thunks.\n+  bool has_ynn_fusions_ = false;\n+\n   // Entry function name for the computation.\n   std::string entry_function_name_;\n "
        },
        {
            "sha": "5d1195a947994ccc27fa716b2929f2a10c66320a",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -126,6 +126,12 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/onednn/onednn_fusion_thunk.h\"\n #endif  // XLA_ONEDNN_USE_GRAPH_API\n \n+#ifdef XLA_YNNPACK\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.h\"\n+#include \"xla/backends/cpu/ynn_emitter.h\"\n+#include \"xla/backends/cpu/ynn_support.h\"\n+#endif  // XLA_YNNPACK\n+\n namespace xla::cpu {\n \n namespace {\n@@ -440,6 +446,12 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloInstruction(\n           return EmitXnnFusionThunk(instruction);\n         }\n \n+#ifdef XLA_YNNPACK\n+        if (backend_config.fusion_config().kind() == kYnnFusionKind) {\n+          return EmitYnnFusionThunk(instruction);\n+        }\n+#endif  // XLA_YNNPACK\n+\n         return Internal(\"Unsupported custom fusion kind: %s\",\n                         backend_config.DebugString());\n       }\n@@ -1494,6 +1506,45 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitXnnFusionThunk(\n       [b = std::move(builder)](auto, auto) mutable { return b(); });\n }\n \n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitYnnFusionThunk(\n+    const HloInstruction* instruction) {\n+#ifdef XLA_YNNPACK\n+  auto* fusion = Cast<HloFusionInstruction>(instruction);\n+\n+  // Collect YNNPACK fusion arguments.\n+  std::vector<YnnFusionThunk::Argument> arguments;\n+  for (HloInstruction* operand : instruction->operands()) {\n+    for (auto& indexed : ShapeUtil::GetLeafShapes(operand->shape())) {\n+      TF_ASSIGN_OR_RETURN(\n+          BufferAllocation::Slice slice,\n+          buffer_assignment_.GetUniqueSlice(operand, indexed.index));\n+      arguments.push_back(YnnFusionThunk::Argument{slice, indexed.shape});\n+    }\n+  }\n+\n+  // Collect YNNPACK fusion results.\n+  std::vector<YnnFusionThunk::Result> results;\n+  for (auto& indexed : ShapeUtil::GetLeafShapes(instruction->shape())) {\n+    TF_ASSIGN_OR_RETURN(\n+        BufferAllocation::Slice slice,\n+        buffer_assignment_.GetUniqueSlice(instruction, indexed.index));\n+    results.push_back(YnnFusionThunk::Result{slice, indexed.shape});\n+  }\n+\n+  const HloComputation* computation = fusion->fused_instructions_computation();\n+\n+  // Construct YNNPACK subgraph builder from the fusion computation.\n+  TF_ASSIGN_OR_RETURN(auto builder, EmitYnnFusionBuilder(computation));\n+\n+  return ThunkSequence::Of<YnnFusionThunk>(\n+      YnnFusionThunk::Options{}, ThunkInfo(instruction), std::move(arguments),\n+      std::move(results),\n+      [b = std::move(builder)](auto, auto) mutable { return b(); });\n+#else\n+  return Unimplemented(\"XLA is not built with YNNPACK.\");\n+#endif  // XLA_YNNPACK\n+}\n+\n absl::StatusOr<ThunkEmitter::HostKernelAllocationSlices>\n ThunkEmitter::GetHostKernelAllocationSlices(const HloInstruction* instruction) {\n   HostKernelAllocationSlices slices;"
        },
        {
            "sha": "8b052b416d8c612bf5a355f17eef1791d0db0b2e",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce65a0ad5c664e888216e0e1a7b8036467263088/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h?ref=ce65a0ad5c664e888216e0e1a7b8036467263088",
            "patch": "@@ -217,6 +217,9 @@ class ThunkEmitter {\n   absl::StatusOr<ThunkSequence> EmitXnnFusionThunk(\n       const HloInstruction* instruction);\n \n+  absl::StatusOr<ThunkSequence> EmitYnnFusionThunk(\n+      const HloInstruction* instruction);\n+\n   absl::StatusOr<ThunkSequence> EmitOneDnnFusionThunk(\n       const HloInstruction* instruction);\n "
        }
    ],
    "stats": {
        "total": 1854,
        "additions": 1849,
        "deletions": 5
    }
}