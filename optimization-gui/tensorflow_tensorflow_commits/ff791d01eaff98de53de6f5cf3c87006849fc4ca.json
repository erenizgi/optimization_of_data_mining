{
    "author": "tensorflower-gardener",
    "message": "Add support for saving benchmark results to a binary proto file.\n\nPiperOrigin-RevId: 821870348",
    "sha": "ff791d01eaff98de53de6f5cf3c87006849fc4ca",
    "files": [
        {
            "sha": "4577c2afb2c867b4e5ab302e81efbbeb9d7f5380",
            "filename": "tensorflow/lite/tools/benchmark/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff791d01eaff98de53de6f5cf3c87006849fc4ca/tensorflow%2Flite%2Ftools%2Fbenchmark%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff791d01eaff98de53de6f5cf3c87006849fc4ca/tensorflow%2Flite%2Ftools%2Fbenchmark%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftools%2Fbenchmark%2FBUILD?ref=ff791d01eaff98de53de6f5cf3c87006849fc4ca",
            "patch": "@@ -126,10 +126,10 @@ cc_test(\n         \"//tensorflow/lite/testing:util\",\n         \"//tensorflow/lite/tools:command_line_flags\",\n         \"//tensorflow/lite/tools:logging\",\n+        \"//tensorflow/lite/tools/benchmark/proto:benchmark_result_cc\",\n         \"//tensorflow/lite/tools/delegates:delegate_provider_hdr\",\n         \"@com_google_absl//absl/algorithm\",\n         \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\",\n@@ -183,6 +183,7 @@ cc_library(\n         \"//tensorflow/lite/tools:logging\",\n         \"//tensorflow/lite/tools:model_loader\",\n         \"//tensorflow/lite/tools:utils\",\n+        \"//tensorflow/lite/tools/benchmark/proto:benchmark_result_cc\",\n         \"//tensorflow/lite/tools/delegates:delegate_provider_hdr\",\n         \"//tensorflow/lite/tools/delegates:tflite_execution_providers\",\n         \"@com_google_absl//absl/base:core_headers\","
        },
        {
            "sha": "1cbef60a257136ea4bd7e33b22ecc488de246580",
            "filename": "tensorflow/lite/tools/benchmark/benchmark_test.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 0,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff791d01eaff98de53de6f5cf3c87006849fc4ca/tensorflow%2Flite%2Ftools%2Fbenchmark%2Fbenchmark_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff791d01eaff98de53de6f5cf3c87006849fc4ca/tensorflow%2Flite%2Ftools%2Fbenchmark%2Fbenchmark_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftools%2Fbenchmark%2Fbenchmark_test.cc?ref=ff791d01eaff98de53de6f5cf3c87006849fc4ca",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"tensorflow/lite/testing/util.h\"\n #include \"tensorflow/lite/tools/benchmark/benchmark_performance_options.h\"\n #include \"tensorflow/lite/tools/benchmark/benchmark_tflite_model.h\"\n+#include \"tensorflow/lite/tools/benchmark/proto/benchmark_result.pb.h\"\n #include \"tensorflow/lite/tools/command_line_flags.h\"\n #include \"tensorflow/lite/tools/delegates/delegate_provider.h\"\n #include \"tensorflow/lite/tools/logging.h\"\n@@ -614,6 +615,83 @@ TEST(BenchmarkTest, InitializationFailedWhenInvalidGraphFdIsProvided) {\n   EXPECT_EQ(benchmark.Init(), kTfLiteError);\n }\n \n+class TestBenchmarkListener : public BenchmarkListener {\n+ public:\n+  void OnBenchmarkEnd(const BenchmarkResults& results) override {\n+    results_ = results;\n+  }\n+\n+  const BenchmarkResults& results() const { return results_; }\n+\n+ private:\n+  BenchmarkResults results_;\n+};\n+\n+TEST(BenchmarkTest, BenchmarkResultFileIsWritten) {\n+  ASSERT_THAT(g_fp32_model_path, testing::NotNull());\n+  BenchmarkParams params = BenchmarkTfLiteModel::DefaultParams();\n+\n+  std::string result_file_path = \"/tmp/result.txtproto\";\n+#if defined(__ANDROID__)\n+  result_file_path = \"/data/local/tmp/result.txtproto\";\n+#endif\n+  params.Set<std::string>(\"result_file_path\", result_file_path);\n+  params.Set<bool>(\"report_peak_memory_footprint\", true);\n+  params.Set<std::string>(\"graph\", *g_fp32_model_path);\n+\n+  TestBenchmark benchmark(std::move(params));\n+  TestBenchmarkListener listener;\n+  benchmark.AddListener(&listener);\n+  benchmark.Run();\n+\n+  std::ifstream in_file(result_file_path, std::ios::binary | std::ios::in);\n+  tflite::tools::benchmark::BenchmarkResult result;\n+  result.ParseFromIstream(&in_file);\n+\n+  // Verify latency metrics.\n+  EXPECT_FLOAT_EQ(result.latency_metrics().init_ms(),\n+                  listener.results().startup_latency_us() / 1000.0);\n+  EXPECT_FLOAT_EQ(result.latency_metrics().first_inference_ms(),\n+                  listener.results().warmup_time_us().first() / 1000.0);\n+  EXPECT_FLOAT_EQ(result.latency_metrics().average_warm_up_ms(),\n+                  listener.results().warmup_time_us().avg() / 1000.0);\n+  EXPECT_FLOAT_EQ(result.latency_metrics().avg_ms(),\n+                  listener.results().inference_time_us().avg() / 1000.0);\n+  EXPECT_FLOAT_EQ(result.latency_metrics().min_ms(),\n+                  listener.results().inference_time_us().min() / 1000.0);\n+  EXPECT_FLOAT_EQ(result.latency_metrics().max_ms(),\n+                  listener.results().inference_time_us().max() / 1000.0);\n+  EXPECT_FLOAT_EQ(\n+      result.latency_metrics().stddev_ms(),\n+      listener.results().inference_time_us().std_deviation() / 1000.0);\n+  EXPECT_FLOAT_EQ(\n+      result.latency_metrics().median_ms(),\n+      listener.results().inference_time_us().percentile(50) / 1000.0);\n+  EXPECT_FLOAT_EQ(\n+      result.latency_metrics().p95_ms(),\n+      listener.results().inference_time_us().percentile(95) / 1000.0);\n+  EXPECT_FLOAT_EQ(\n+      result.latency_metrics().p5_ms(),\n+      listener.results().inference_time_us().percentile(5) / 1000.0);\n+\n+  // Verify memory metrics.\n+  EXPECT_EQ(result.memory_metrics().init_footprint_kb(),\n+            listener.results().init_mem_usage().mem_footprint_kb);\n+  EXPECT_EQ(result.memory_metrics().overall_footprint_kb(),\n+            listener.results().overall_mem_usage().mem_footprint_kb);\n+  EXPECT_EQ(result.memory_metrics().has_peak_mem_mb(), true);\n+\n+  // Verify misc metrics.\n+  EXPECT_FLOAT_EQ(result.misc_metrics().model_size_mb(),\n+                  listener.results().model_size_mb());\n+  EXPECT_EQ(result.misc_metrics().num_runs(),\n+            listener.results().inference_time_us().count());\n+  EXPECT_EQ(result.misc_metrics().num_warmup_runs(),\n+            listener.results().warmup_time_us().count());\n+  EXPECT_FLOAT_EQ(result.misc_metrics().model_throughput_in_mb_per_sec(),\n+                  listener.results().throughput_MB_per_second());\n+}\n+\n }  // namespace\n }  // namespace benchmark\n }  // namespace tflite"
        },
        {
            "sha": "91ad3c4582d2f822e1c5dd0d809678412827e85e",
            "filename": "tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc",
            "status": "modified",
            "additions": 87,
            "deletions": 1,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff791d01eaff98de53de6f5cf3c87006849fc4ca/tensorflow%2Flite%2Ftools%2Fbenchmark%2Fbenchmark_tflite_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff791d01eaff98de53de6f5cf3c87006849fc4ca/tensorflow%2Flite%2Ftools%2Fbenchmark%2Fbenchmark_tflite_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftools%2Fbenchmark%2Fbenchmark_tflite_model.cc?ref=ff791d01eaff98de53de6f5cf3c87006849fc4ca",
            "patch": "@@ -58,6 +58,7 @@ limitations under the License.\n #include \"tensorflow/lite/tools/benchmark/benchmark_params.h\"\n #include \"tensorflow/lite/tools/benchmark/benchmark_utils.h\"\n #include \"tensorflow/lite/tools/benchmark/profiling_listener.h\"\n+#include \"tensorflow/lite/tools/benchmark/proto/benchmark_result.pb.h\"\n #include \"tensorflow/lite/tools/delegates/delegate_provider.h\"\n #include \"tensorflow/lite/tools/logging.h\"\n #include \"tensorflow/lite/tools/model_loader.h\"\n@@ -74,6 +75,7 @@ RegisterSelectedOps(::tflite::MutableOpResolver* resolver) {}\n namespace tflite {\n namespace benchmark {\n namespace {\n+using ::tflite::tools::benchmark::BenchmarkResult;\n using utils::InputTensorData;\n using utils::VoidUniquePtr;\n \n@@ -264,6 +266,78 @@ class ModelRuntimeInfoListener : public BenchmarkListener {\n   Interpreter* const interpreter_ = nullptr;  // not own the memory.\n };\n \n+// Dumps the benchmark result to a file in proto format if result_file_path is\n+// set.\n+class ProtoBenchmarkReporter : public BenchmarkListener {\n+ public:\n+  void OnBenchmarkStart(\n+      const ::tflite::benchmark::BenchmarkParams& params) override {\n+    if (!params.Get<std::string>(\"result_file_path\").empty()) {\n+      result_file_path_ =\n+          std::string(params.Get<std::string>(\"result_file_path\"));\n+    }\n+  }\n+\n+  void OnBenchmarkEnd(const BenchmarkResults& results) override {\n+    if (!result_file_path_.empty()) {\n+      auto inference_us = results.inference_time_us();\n+      auto init_us = results.startup_latency_us();\n+      auto warmup_us = results.warmup_time_us();\n+      auto init_mem_usage = results.init_mem_usage();\n+      auto overall_mem_usage = results.overall_mem_usage();\n+\n+      BenchmarkResult result;\n+      result.mutable_latency_metrics()->set_init_ms(init_us / 1000.0);\n+      result.mutable_latency_metrics()->set_first_inference_ms(\n+          warmup_us.first() / 1000.0);\n+      result.mutable_latency_metrics()->set_average_warm_up_ms(warmup_us.avg() /\n+                                                               1000.0);\n+      result.mutable_latency_metrics()->set_min_ms(inference_us.min() / 1000.0);\n+      result.mutable_latency_metrics()->set_max_ms(inference_us.max() / 1000.0);\n+      result.mutable_latency_metrics()->set_stddev_ms(\n+          inference_us.std_deviation() / 1000.0);\n+      result.mutable_latency_metrics()->set_avg_ms(inference_us.avg() / 1000.0);\n+      result.mutable_latency_metrics()->set_median_ms(\n+          inference_us.percentile(50) / 1000.0);\n+      result.mutable_latency_metrics()->set_p5_ms(inference_us.percentile(5) /\n+                                                  1000.0);\n+      result.mutable_latency_metrics()->set_p95_ms(inference_us.percentile(95) /\n+                                                   1000.0);\n+      if (init_mem_usage.IsSupported()) {\n+        result.mutable_memory_metrics()->set_init_footprint_kb(\n+            init_mem_usage.mem_footprint_kb);\n+        result.mutable_memory_metrics()->set_overall_footprint_kb(\n+            overall_mem_usage.mem_footprint_kb);\n+        if (results.peak_mem_mb() > 0) {\n+          result.mutable_memory_metrics()->set_peak_mem_mb(\n+              results.peak_mem_mb());\n+        }\n+      }\n+\n+      result.mutable_misc_metrics()->set_model_size_mb(results.model_size_mb());\n+      result.mutable_misc_metrics()->set_num_runs(inference_us.count());\n+      result.mutable_misc_metrics()->set_num_warmup_runs(warmup_us.count());\n+      result.mutable_misc_metrics()->set_model_throughput_in_mb_per_sec(\n+          results.throughput_MB_per_second());\n+\n+      std::ofstream out_file(result_file_path_,\n+                             std::ios::binary | std::ios::out);\n+      if (out_file.good()) {\n+        TFLITE_LOG(INFO) << \"Saving benchmark result to: \" << result_file_path_;\n+        result.SerializeToOstream(&out_file);\n+        out_file.close();\n+        TFLITE_LOG(INFO) << \"Saved benchmark result to: \" << result_file_path_;\n+      } else {\n+        TFLITE_LOG(ERROR) << \"Failed to save benchmark result to: \"\n+                          << result_file_path_;\n+      }\n+    }\n+  }\n+\n+ private:\n+  std::string result_file_path_;\n+};\n+\n std::vector<std::string> Split(const std::string& str, const char delim) {\n   if (str.empty()) {\n     return {};\n@@ -615,6 +689,8 @@ BenchmarkParams BenchmarkTfLiteModel::DefaultParams() {\n                           BenchmarkParam::Create<std::string>(\"\"));\n   default_params.AddParam(\"output_proto_filepath\",\n                           BenchmarkParam::Create<std::string>(\"\"));\n+  default_params.AddParam(\"result_file_path\",\n+                          BenchmarkParam::Create<std::string>(\"\"));\n \n   default_params.AddParam(\"tensor_name_display_length\",\n                           BenchmarkParam::Create<int32_t>(25));\n@@ -747,7 +823,10 @@ std::vector<Flag> BenchmarkTfLiteModel::GetFlags() {\n           \"default signature will be used.\"),\n       CreateFlag<bool>(\"list_signatures\", &params_,\n                        \"Displays all signatures present in the model and then \"\n-                       \"terminates the program.\")};\n+                       \"terminates the program.\"),\n+      CreateFlag<std::string>(\n+          \"result_file_path\", &params_,\n+          \"Path to save the benchmark result in binary proto format.\")};\n \n   flags.insert(flags.end(), specific_flags.begin(), specific_flags.end());\n \n@@ -811,6 +890,10 @@ void BenchmarkTfLiteModel::LogParams() {\n   LOG_BENCHMARK_PARAM(std::string, \"output_proto_filepath\",\n                       \"File path to export outputs layer as tf example to\",\n                       verbose);\n+  LOG_BENCHMARK_PARAM(std::string, \"result_file_path\",\n+                      \"File path to save the benchmark result in binary proto \"\n+                      \"format\",\n+                      verbose);\n   LOG_BENCHMARK_PARAM(int32_t, \"tensor_name_display_length\",\n                       \"Tensor name display length\", verbose);\n   LOG_BENCHMARK_PARAM(int32_t, \"tensor_type_display_length\",\n@@ -1242,6 +1325,9 @@ TfLiteStatus BenchmarkTfLiteModel::Init() {\n   AddOwnedListener(\n       std::unique_ptr<BenchmarkListener>(new RuyProfileListener()));\n \n+  AddOwnedListener(\n+      std::unique_ptr<BenchmarkListener>(new ProtoBenchmarkReporter()));\n+\n   AddOwnedListener(std::unique_ptr<BenchmarkListener>(\n       new OutputSaver(interpreter_runner_.get())));\n "
        }
    ],
    "stats": {
        "total": 169,
        "additions": 167,
        "deletions": 2
    }
}