{
    "author": "penpornk",
    "message": "[xla:cpu:onednn] Remove INTEL_MKL ifdef guards around `onednn_ops_rewriter`.\n\n+ Also remove the guards in oneDNN LayerNorm and Softmax tests.\n+ Delete unused file onednn_rewriter.h.\n+ Minor ClangTidy/Linter errors/warnings fixes.\n\nPiperOrigin-RevId: 812562429",
    "sha": "688136f1dbd8946045203e3bcb74af07a3df8f59",
    "files": [
        {
            "sha": "c48fc08fa60881f8216b4fed1e85be264508fb2e",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=688136f1dbd8946045203e3bcb74af07a3df8f59",
            "patch": "@@ -1911,20 +1911,22 @@ cc_library(\n     deps = [\n         \":backend_config_proto_cc\",\n         \":onednn_config_proto_cc\",\n-        \":onednn_memory_util\",\n         \":onednn_pattern_utils\",\n         \":onednn_util\",\n         \"//xla:literal_comparison\",\n         \"//xla:literal_util\",\n-        \"//xla:status_macros\",\n-        \"//xla:xla_data_proto_cc\",\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n-        \"//xla/service:hlo_creation_utils\",\n         \"//xla/service:pattern_matcher\",\n-        \"//xla/tsl/mkl:onednn\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@local_tsl//tsl/platform:platform_port\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n "
        },
        {
            "sha": "bea079e3d74de69a88b87c2bb1416daa43163f1d",
            "filename": "third_party/xla/xla/service/cpu/onednn_ops_rewriter.cc",
            "status": "modified",
            "additions": 75,
            "deletions": 31,
            "changes": 106,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc?ref=688136f1dbd8946045203e3bcb74af07a3df8f59",
            "patch": "@@ -11,21 +11,32 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n-#if defined(INTEL_MKL)\n \n #include \"xla/service/cpu/onednn_ops_rewriter.h\"\n \n+#include <cstdint>\n+#include <optional>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/literal_comparison.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n #include \"xla/service/cpu/onednn_config.pb.h\"\n-#include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/onednn_pattern_utils.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/service/pattern_matcher.h\"\n-#include \"xla/status_macros.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n \n namespace xla {\n namespace cpu {\n@@ -255,13 +266,19 @@ std::optional<bool> MatchTFKerasLayerNorm(HloInstruction* instr,\n     return std::nullopt;\n   }\n \n-  if (scaled_norm_a != scaled_norm_b) return std::nullopt;\n+  if (scaled_norm_a != scaled_norm_b) {\n+    return std::nullopt;\n+  }\n \n   const Shape& src_shape = (*src)->shape();\n-  if (!IsSupportedType(src_shape.element_type())) return std::nullopt;\n+  if (!IsSupportedType(src_shape.element_type())) {\n+    return std::nullopt;\n+  }\n \n   // Get bias\n-  if (!Match(bias_node, m::Broadcast(m::Op(bias)))) return std::nullopt;\n+  if (!Match(bias_node, m::Broadcast(m::Op(bias)))) {\n+    return std::nullopt;\n+  }\n \n   // Match Z = scale / sqrt(variance + eps)\n   if (!Match(scaled_norm_a,\n@@ -291,12 +308,18 @@ std::optional<bool> MatchTFKerasLayerNorm(HloInstruction* instr,\n     return std::nullopt;\n   }\n \n-  if (src_a != src_b && src_a != *src) return std::nullopt;\n+  if (src_a != src_b && src_a != *src) {\n+    return std::nullopt;\n+  }\n \n   // Match mean from Bias - Mean(X)*Z\n-  if (!Match(mean0_a, MeanPattern(&src_c))) return std::nullopt;\n+  if (!Match(mean0_a, MeanPattern(&src_c))) {\n+    return std::nullopt;\n+  }\n \n-  if (src_c != *src) return std::nullopt;\n+  if (src_c != *src) {\n+    return std::nullopt;\n+  }\n \n   return true;\n }\n@@ -334,10 +357,14 @@ bool MatchFlaxLayerNorm(HloInstruction* instr, HloInstruction** src,\n                   .WithOneUser())\n           .WithOneUser());\n \n-  if (!Match(instr, spine)) return false;\n+  if (!Match(instr, spine)) {\n+    return false;\n+  }\n \n   const Shape& prod_shape = prod_s->shape();\n-  if (!IsSupportedType(prod_shape.element_type())) return false;\n+  if (!IsSupportedType(prod_shape.element_type())) {\n+    return false;\n+  }\n \n   HloInstruction* shift = FindLayerNormShift(instr);\n   shiftFound = (shift != nullptr);\n@@ -347,8 +374,8 @@ bool MatchFlaxLayerNorm(HloInstruction* instr, HloInstruction** src,\n \n   // Currently patterns without scale and shift are not supported.\n   // OneDNN only supports 2 <= rank <= 5\n-  if (!(prod_shape.dimensions_size() >= 2 &&\n-        prod_shape.dimensions_size() <= 5) ||\n+  if (!(prod_shape.dimensions().size() >= 2 &&\n+        prod_shape.dimensions().size() <= 5) ||\n       !shiftFound || !scaleFound) {\n     return false;\n   }\n@@ -386,9 +413,13 @@ bool MatchFlaxLayerNorm(HloInstruction* instr, HloInstruction** src,\n           .WithOneUser());\n   // NOLINTEND\n \n-  if (!Match(hinge, main_pipeline)) return false;\n+  if (!Match(hinge, main_pipeline)) {\n+    return false;\n+  }\n \n-  if ((div_red != div1) || (main_pipe_mul_in0 != div1)) return false;\n+  if ((div_red != div1) || (main_pipe_mul_in0 != div1)) {\n+    return false;\n+  }\n \n   auto div_red_mul_src =\n       m::Divide()\n@@ -398,18 +429,23 @@ bool MatchFlaxLayerNorm(HloInstruction* instr, HloInstruction** src,\n                                     m::Constant())\n                               .WithPredicate([](const HloInstruction* reduce) {\n                                 HloComputation* reducer = reduce->to_apply();\n-                                return (reducer->root_instruction()->opcode() ==\n-                                            HloOpcode::kAdd &&\n-                                        reduce->dimensions().size() == 1 &&\n-                                        reduce->dimensions()[0] ==\n-                                            reduce->shape().dimensions_size());\n+                                return (\n+                                    reducer->root_instruction()->opcode() ==\n+                                        HloOpcode::kAdd &&\n+                                    reduce->dimensions().size() == 1 &&\n+                                    reduce->dimensions()[0] ==\n+                                        reduce->shape().dimensions().size());\n                               }))\n           .WithOperand(1, m::Op(&broadcast0).WithOpcode(HloOpcode::kBroadcast))\n           .WithOneUser();\n \n-  if (!Match(div0, div_red_mul_src)) return false;\n+  if (!Match(div0, div_red_mul_src)) {\n+    return false;\n+  }\n \n-  if (mul_in0 != mul_in1) return false;\n+  if (mul_in0 != mul_in1) {\n+    return false;\n+  }\n \n   auto div_red_subgraph =\n       m::Divide()\n@@ -422,11 +458,13 @@ bool MatchFlaxLayerNorm(HloInstruction* instr, HloInstruction** src,\n                                 HloOpcode::kAdd &&\n                             reduce->dimensions().size() == 1 &&\n                             reduce->dimensions()[0] ==\n-                                reduce->shape().dimensions_size());\n+                                reduce->shape().dimensions().size());\n                   }))\n           .WithOperand(1, m::Op(&broadcast1).WithOpcode(HloOpcode::kBroadcast));\n \n-  if (!Match(div1, div_red_subgraph)) return false;\n+  if (!Match(div1, div_red_subgraph)) {\n+    return false;\n+  }\n \n   if (broadcast1 != broadcast0 || reduce_in0 != mul_in0 || mul_in0 != prod_s) {\n     return false;\n@@ -474,7 +512,9 @@ class OneDnnOpsRewriterVisitor : public DfsHloRewriteVisitor {\n                                     &is_producer_bf16orfp16, &convert_instr);\n     }\n \n-    if (!found_ln) return absl::OkStatus();\n+    if (!found_ln) {\n+      return absl::OkStatus();\n+    }\n \n     const Shape& src_shape = src->shape();\n     auto scale_type = scale->shape().element_type();\n@@ -528,13 +568,16 @@ class OneDnnOpsRewriterVisitor : public DfsHloRewriteVisitor {\n                                 .WithOneUser()\n                                 .WithElementType(PrimitiveType::F32));\n \n-    if (!IsSupportedType(instr->shape().element_type()))\n+    if (!IsSupportedType(instr->shape().element_type())) {\n       return absl::OkStatus();\n+    }\n     if (Match(instr, pattern)) {\n       bool is_bf16orfp16_convert =\n           (convert_instr->shape().element_type() == PrimitiveType::BF16) ||\n           (convert_instr->shape().element_type() == PrimitiveType::F16);\n-      if (!is_bf16orfp16_convert) return absl::OkStatus();\n+      if (!is_bf16orfp16_convert) {\n+        return absl::OkStatus();\n+      }\n       HloInstruction* producer = instr->mutable_operand(0)->mutable_operand(0);\n       HloInstruction* newinp =\n           producer->AddInstruction(HloInstruction::CreateConvert(\n@@ -554,11 +597,14 @@ class OneDnnOpsRewriterVisitor : public DfsHloRewriteVisitor {\n \n   absl::Status HandleDivide(HloInstruction* divide_instr) override {\n     if (divide_instr->HasControlDependencies()) return absl::OkStatus();\n-    if (!IsSupportedType(divide_instr->shape().element_type()))\n+    if (!IsSupportedType(divide_instr->shape().element_type())) {\n       return absl::OkStatus();\n+    }\n     int axis = -1;\n     std::optional<HloInstruction*> producer = MatchSoftmax(divide_instr, &axis);\n-    if (producer == std::nullopt) return absl::OkStatus();\n+    if (producer == std::nullopt) {\n+      return absl::OkStatus();\n+    }\n \n     const Shape& output_shape = divide_instr->shape();\n     HloInstruction* softmax_call =\n@@ -588,5 +634,3 @@ absl::StatusOr<bool> OneDnnOpsRewriter::Run(\n \n }  // namespace cpu\n }  // namespace xla\n-\n-#endif  // INTEL_MKL"
        },
        {
            "sha": "fec2a4e4cc11d08a30ec5953e4fc2628f3db8788",
            "filename": "third_party/xla/xla/service/cpu/onednn_ops_rewriter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.h?ref=688136f1dbd8946045203e3bcb74af07a3df8f59",
            "patch": "@@ -13,12 +13,10 @@ limitations under the License.\n ==============================================================================*/\n #ifndef XLA_SERVICE_CPU_ONEDNN_OPS_REWRITER_H_\n #define XLA_SERVICE_CPU_ONEDNN_OPS_REWRITER_H_\n-#if defined(INTEL_MKL)\n \n-#include <optional>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n \n@@ -40,5 +38,4 @@ class OneDnnOpsRewriter : public HloModulePass {\n }  // namespace cpu\n }  // namespace xla\n \n-#endif  // INTEL_MKL\n #endif  // XLA_SERVICE_CPU_ONEDNN_OPS_REWRITER_H_"
        },
        {
            "sha": "febecf177e9ec63ed148f639d2bce3df17957a0e",
            "filename": "third_party/xla/xla/service/cpu/onednn_rewriter.h",
            "status": "removed",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1e7d167d9a341dbea0938b04ee120839ebbf902e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1e7d167d9a341dbea0938b04ee120839ebbf902e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_rewriter.h?ref=1e7d167d9a341dbea0938b04ee120839ebbf902e",
            "patch": "@@ -1,45 +0,0 @@\n-/* Copyright 2023 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_SERVICE_CPU_ONEDNN_REWRITER_H_\n-#define XLA_SERVICE_CPU_ONEDNN_REWRITER_H_\n-#if defined(INTEL_MKL)\n-\n-#include <optional>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/pass/hlo_pass_interface.h\"\n-\n-namespace xla {\n-namespace cpu {\n-\n-// This pass pattern-matches hlo instructions and rewrites into custom calls.\n-class OneDnnRewriter : public HloModulePass {\n- public:\n-  absl::string_view name() const override { return \"onednn-rewriter\"; }\n-\n-  using HloPassInterface::Run;\n-  absl::StatusOr<bool> Run(\n-      HloModule* module,\n-      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n-};\n-\n-}  // namespace cpu\n-}  // namespace xla\n-\n-#endif  // INTEL_MKL\n-#endif  // XLA_SERVICE_CPU_ONEDNN_REWRITER_H_"
        },
        {
            "sha": "529106d33da8c4bddb718fb3421a41084f358e88",
            "filename": "third_party/xla/xla/service/cpu/tests/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD?ref=688136f1dbd8946045203e3bcb74af07a3df8f59",
            "patch": "@@ -374,6 +374,7 @@ xla_cc_test(\n         \"notap\",\n     ],\n     deps = [\n+        \"//xla:error_spec\",\n         \"//xla/hlo/testlib:test\",\n         \"//xla/service:cpu_plugin\",\n         \"//xla/service/cpu:onednn_util\","
        },
        {
            "sha": "217cb652a191fb7613d53ac14da3a7698d2d3d88",
            "filename": "third_party/xla/xla/service/cpu/tests/onednn_layer_norm_test.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 27,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_layer_norm_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_layer_norm_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_layer_norm_test.cc?ref=688136f1dbd8946045203e3bcb74af07a3df8f59",
            "patch": "@@ -13,22 +13,18 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <string>\n+\n+#include \"xla/error_spec.h\"\n #include \"xla/hlo/testlib/test.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/tests/hlo_test_base.h\"\n \n namespace xla {\n namespace {\n \n-#if defined(INTEL_MKL)\n-\n class LayerNormTest : public HloTestBase {\n  protected:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options = HloTestBase::GetDebugOptionsForTest();\n-    return debug_options;\n-  }\n-\n   const char* onednn_layer_norm_ =\n       R\"(\n   ; CHECK:     custom_call_target=\"__onednn$layernorm\",\n@@ -56,21 +52,24 @@ class LayerNormTest : public HloTestBase {\n     convert.290 = f32[84,197,768]{2,1,0} convert(Arg_0.1)\n     constant.291 = f32[] constant(0)\n     convert.292 = f32[] convert(constant.291)\n-    reduce.297 = f32[84,197]{1,0} reduce(convert.290, convert.292), dimensions={2}, to_apply=region_add\n+    reduce.297 = f32[84,197]{1,0} reduce(convert.290, convert.292),\n+                 dimensions={2}, to_apply=region_add\n     constant.298 = s32[] constant(768)\n     convert.299 = f32[] convert(constant.298)\n     broadcast.300 = f32[84,197]{1,0} broadcast(convert.299), dimensions={}\n     divide.301 = f32[84,197]{1,0} divide(reduce.297, broadcast.300)\n     convert.302 = f32[84,197]{1,0} convert(divide.301)\n     reshape.303 = f32[84,197,1]{2,1,0} reshape(convert.302)\n     reshape.304 = f32[84,197]{1,0} reshape(reshape.303)\n-    broadcast.305 = f32[84,197,768]{2,1,0} broadcast(reshape.304), dimensions={0,1}\n+    broadcast.305 = f32[84,197,768]{2,1,0} broadcast(reshape.304),\n+                    dimensions={0,1}\n     subtract.306 = f32[84,197,768]{2,1,0} subtract(Arg_0.1, broadcast.305)\n     multiply.307 = f32[84,197,768]{2,1,0} multiply(subtract.306, subtract.306)\n     convert.308 = f32[84,197,768]{2,1,0} convert(multiply.307)\n     constant.309 = f32[] constant(0)\n     convert.310 = f32[] convert(constant.309)\n-    reduce.315 = f32[84,197]{1,0} reduce(convert.308, convert.310), dimensions={2}, to_apply=region_add\n+    reduce.315 = f32[84,197]{1,0} reduce(convert.308, convert.310),\n+                 dimensions={2}, to_apply=region_add\n     constant.316 = s32[] constant(768)\n     convert.317 = f32[] convert(constant.316)\n     broadcast.318 = f32[84,197]{1,0} broadcast(convert.317), dimensions={}\n@@ -82,27 +81,29 @@ class LayerNormTest : public HloTestBase {\n     add.324 = f32[84,197,1]{2,1,0} add(reshape.321, broadcast.323)\n     rsqrt.325 = f32[84,197,1]{2,1,0} rsqrt(add.324)\n     reshape.328 = f32[84,197]{1,0} reshape(rsqrt.325)\n-    broadcast.329 = f32[84,197,768]{2,1,0} broadcast(reshape.328), dimensions={0,1}\n+    broadcast.329 = f32[84,197,768]{2,1,0} broadcast(reshape.328),\n+                    dimensions={0,1}\n     broadcast.327 = f32[84,197,768]{2,1,0} broadcast(Arg_0.2), dimensions={2}\n     multiply.330 = f32[84,197,768]{2,1,0} multiply(broadcast.329, broadcast.327)\n     multiply.331 = f32[84,197,768]{2,1,0} multiply(Arg_0.1, multiply.330)\n     broadcast.336 = f32[84,197,768]{2,1,0} broadcast(Arg_0.3), dimensions={2}\n     reshape.332 = f32[84,197]{1,0} reshape(reshape.303)\n-    broadcast.333 = f32[84,197,768]{2,1,0} broadcast(reshape.332), dimensions={0,1}\n+    broadcast.333 = f32[84,197,768]{2,1,0} broadcast(reshape.332),\n+                    dimensions={0,1}\n     multiply.334 = f32[84,197,768]{2,1,0} multiply(multiply.330, broadcast.333)\n     subtract.337 = f32[84,197,768]{2,1,0} subtract(broadcast.336, multiply.334)\n )\";\n };\n \n TEST_F(LayerNormTest, LayerNormTest0_FP32) {\n-  std::string layer_norm_module_str =\n-      R\"(HloModule layer_norm.test, entry_computation_layout={(f32[84,197,768]{2,1,0}, f32[768]{0}, f32[768]{0})->f32[84,197,768]{2,1,0}})\" +\n-      common_hlo_region_ + R\"(\n+  std::string layer_norm_module_str = R\"(\n+      HloModule layer_norm.test, entry_computation_layout={(f32[84,197,768]{2,1,0}, f32[768]{0}, f32[768]{0})->f32[84,197,768]{2,1,0}})\" +\n+                                      common_hlo_region_ + R\"(\n   ENTRY main {\n     Arg_0.1 = f32[84,197,768]{2,1,0} parameter(0), sharding={replicated}\n \n   )\" + common_hlo_entry_computation_block_ +\n-      R\"(\n+                                      R\"(\n     ROOT add.338 = f32[84,197,768]{2,1,0} add(multiply.331, subtract.337)\n   }\n   )\";\n@@ -169,7 +170,8 @@ TEST_F(LayerNormTest, LayerNormTest1_F16) {\n     convert_0 = f32[2,4,8] convert(Arg_2)\n     constant_0 = f32[] constant(0)\n     convert_1 = f32[] convert(constant_0)\n-    reduce_0 = f32[2,4] reduce(convert_0, convert_1), dimensions={2}, to_apply=region_add\n+    reduce_0 = f32[2,4] reduce(convert_0, convert_1), dimensions={2},\n+               to_apply=region_add\n     constant_1 = s32[] constant(8)\n     convert_2 = f32[] convert(constant_1)\n     broadcast_0 = f32[2,4] broadcast(convert_2), dimensions={}\n@@ -183,7 +185,8 @@ TEST_F(LayerNormTest, LayerNormTest1_F16) {\n     convert_4 = f32[2,4,8] convert(multiply_0)\n     constant_2 = f32[] constant(0)\n     convert_5 = f32[] convert(constant_2)\n-    reduce_2 = f32[2,4] reduce(convert_4, convert_5), dimensions={2}, to_apply=region_add\n+    reduce_2 = f32[2,4] reduce(convert_4, convert_5), dimensions={2},\n+               to_apply=region_add\n     constant_3 = s32[] constant(8)\n     convert_6 = f32[] convert(constant_3)\n     broadcast_2 = f32[2,4] broadcast(convert_6), dimensions={}\n@@ -228,7 +231,8 @@ TEST_F(LayerNormTest, LayerNormTest2_F16) {\n     convert_0 = f32[2,4,8] convert(Arg_2)\n     constant_0 = f32[] constant(0)\n     convert_1 = f32[] convert(constant_0)\n-    reduce_0 = f32[2,4] reduce(convert_0, convert_1), dimensions={2}, to_apply=region_add\n+    reduce_0 = f32[2,4] reduce(convert_0, convert_1), dimensions={2},\n+               to_apply=region_add\n     constant_1 = s32[] constant(8)\n     convert_2 = f32[] convert(constant_1)\n     broadcast_0 = f32[2,4] broadcast(convert_2), dimensions={}\n@@ -242,7 +246,8 @@ TEST_F(LayerNormTest, LayerNormTest2_F16) {\n     convert_4 = f32[2,4,8] convert(multiply_0)\n     constant_2 = f32[] constant(0)\n     convert_5 = f32[] convert(constant_2)\n-    reduce_1 = f32[2,4] reduce(convert_4, convert_5), dimensions={2}, to_apply=region_add\n+    reduce_1 = f32[2,4] reduce(convert_4, convert_5), dimensions={2},\n+               to_apply=region_add\n     constant_3 = s32[] constant(8)\n     convert_6 = f32[] convert(constant_3)\n     broadcast_2 = f32[2,4] broadcast(convert_6), dimensions={}\n@@ -291,7 +296,8 @@ TEST_F(LayerNormTest, LayerNormTest1_BF16) {\n     convert.80 = f32[160,197,768] convert(Arg_0.1)\n     constant.81 = f32[] constant(0)\n     convert.82 = f32[] convert(constant.81)\n-    reduce.87 = f32[160,197] reduce(convert.80, convert.82), dimensions={2}, to_apply=region_add\n+    reduce.87 = f32[160,197] reduce(convert.80, convert.82), dimensions={2},\n+                to_apply=region_add\n     constant.88 = s32[] constant(768)\n     convert.89 = f32[] convert(constant.88)\n     broadcast.90 = f32[160,197] broadcast(convert.89), dimensions={}\n@@ -305,7 +311,8 @@ TEST_F(LayerNormTest, LayerNormTest1_BF16) {\n     convert.98 = f32[160,197,768] convert(multiply.97)\n     constant.99 = f32[] constant(0)\n     convert.100 = f32[] convert(constant.99)\n-    reduce.105 = f32[160,197] reduce(convert.98, convert.100), dimensions={2}, to_apply=region_add\n+    reduce.105 = f32[160,197] reduce(convert.98, convert.100), dimensions={2},\n+                 to_apply=region_add\n     constant.106 = s32[] constant(768)\n     convert.107 = f32[] convert(constant.106)\n     broadcast.108 = f32[160,197] broadcast(convert.107), dimensions={}\n@@ -334,11 +341,6 @@ TEST_F(LayerNormTest, LayerNormTest1_BF16) {\n   MatchOptimizedHlo(layer_norm_module_str, onednn_layer_norm_);\n }\n \n-#endif  // INTEL_MKL\n-\n-// Ensure at least one test case is linked to avoid test failures.\n-TEST(Dummy, Test) {}\n-\n }  // namespace\n }  // namespace xla\n "
        },
        {
            "sha": "3414c8c7d6b2f67b866b65890b56ba1d8bb41384",
            "filename": "third_party/xla/xla/service/cpu/tests/onednn_softmax_test.cc",
            "status": "modified",
            "additions": 119,
            "deletions": 107,
            "changes": 226,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_softmax_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/688136f1dbd8946045203e3bcb74af07a3df8f59/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_softmax_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_softmax_test.cc?ref=688136f1dbd8946045203e3bcb74af07a3df8f59",
            "patch": "@@ -13,27 +13,28 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include <utility>\n+#include <memory>\n+#include <string>\n+#include <tuple>\n \n-#include \"absl/strings/str_replace.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/substitute.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/hlo/testlib/test.h\"\n-#include \"xla/hlo/testlib/test_helpers.h\"\n-#include \"xla/literal.h\"\n+#include \"xla/primitive_util.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n #include \"xla/service/cpu/onednn_config.pb.h\"\n #include \"xla/service/cpu/onednn_ops_rewriter.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/service/pattern_matcher.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/tests/hlo_test_base.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace cpu {\n \n-#if defined(INTEL_MKL)\n-\n std::string TestParamsToString(\n     const ::testing::TestParamInfo<std::tuple<PrimitiveType, int>>& data) {\n   PrimitiveType data_type;\n@@ -62,35 +63,41 @@ class OneDnnSoftmaxTest\n   const std::string GetGenericSoftmaxHLORawText(PrimitiveType data_type,\n                                                 int batch_size) {\n     const std::string softmax_hlo_template_string = R\"(\n-        HloModule softmax_module\n-        region_max {\n-            Arg_0 = $0[] parameter(0)\n-            Arg_1 = $0[] parameter(1)\n-            ROOT maximum = $0[] maximum(Arg_0, Arg_1)\n-        }\n-        region_add {\n-            Arg_0 = $0[] parameter(0)\n-            Arg_1 = $0[] parameter(1)\n-            ROOT add = $0[] add(Arg_0, Arg_1)\n-        }\n-        ENTRY main {\n-            Arg_0 = $0[$1,128,30522]{2,1,0} parameter(0)\n-            neg_inf = $0[] constant(-inf)\n-            reduce_max = $0[$1,128]{1,0} reduce(Arg_0, neg_inf), dimensions={2}, to_apply=region_max\n-            reshape.0 = $0[$1,128,1]{2,1,0} reshape(reduce_max)\n-            broadcast.0 = $0[$1,128,1]{2,1,0} broadcast(reshape.0), dimensions={0,1,2}\n-            reshape.1 = $0[$1,128]{1,0} reshape(broadcast.0)\n-            broadcast.1 = $0[$1,128,30522]{2,1,0} broadcast(reshape.1), dimensions={0,1}\n-            subtract.0 = $0[$1,128,30522]{2,1,0} subtract(Arg_0, broadcast.1)\n-            exponential = $0[$1,128,30522]{2,1,0} exponential(subtract.0)\n-            const_zero = $0[] constant(0)\n-            reduce_add = $0[$1,128]{1,0} reduce(exponential, const_zero), dimensions={2}, to_apply=region_add\n-            reshape.2 = $0[$1,128,1]{2,1,0} reshape(reduce_add)\n-            broadcast.2 = $0[$1,128,1]{2,1,0} broadcast(reshape.2), dimensions={0,1,2}\n-            reshape.3 = $0[$1,128]{1,0} reshape(broadcast.2)\n-            broadcast.3 = $0[$1,128,30522]{2,1,0} broadcast(reshape.3), dimensions={0,1}\n-            ROOT divide = $0[$1,128,30522]{2,1,0} divide(exponential, broadcast.3)\n-        }\n+      HloModule softmax_module\n+      region_max {\n+        Arg_0 = $0[] parameter(0)\n+        Arg_1 = $0[] parameter(1)\n+        ROOT maximum = $0[] maximum(Arg_0, Arg_1)\n+      }\n+      region_add {\n+        Arg_0 = $0[] parameter(0)\n+        Arg_1 = $0[] parameter(1)\n+        ROOT add = $0[] add(Arg_0, Arg_1)\n+      }\n+      ENTRY main {\n+        Arg_0 = $0[$1,128,30522]{2,1,0} parameter(0)\n+        neg_inf = $0[] constant(-inf)\n+        reduce_max = $0[$1,128]{1,0} reduce(Arg_0, neg_inf), dimensions={2},\n+                     to_apply=region_max\n+        reshape.0 = $0[$1,128,1]{2,1,0} reshape(reduce_max)\n+        broadcast.0 = $0[$1,128,1]{2,1,0} broadcast(reshape.0),\n+                      dimensions={0,1,2}\n+        reshape.1 = $0[$1,128]{1,0} reshape(broadcast.0)\n+        broadcast.1 = $0[$1,128,30522]{2,1,0} broadcast(reshape.1),\n+                      dimensions={0,1}\n+        subtract.0 = $0[$1,128,30522]{2,1,0} subtract(Arg_0, broadcast.1)\n+        exponential = $0[$1,128,30522]{2,1,0} exponential(subtract.0)\n+        const_zero = $0[] constant(0)\n+        reduce_add = $0[$1,128]{1,0} reduce(exponential, const_zero),\n+                     dimensions={2}, to_apply=region_add\n+        reshape.2 = $0[$1,128,1]{2,1,0} reshape(reduce_add)\n+        broadcast.2 = $0[$1,128,1]{2,1,0} broadcast(reshape.2),\n+                      dimensions={0,1,2}\n+        reshape.3 = $0[$1,128]{1,0} reshape(broadcast.2)\n+        broadcast.3 = $0[$1,128,30522]{2,1,0} broadcast(reshape.3),\n+                      dimensions={0,1}\n+        ROOT divide = $0[$1,128,30522]{2,1,0} divide(exponential, broadcast.3)\n+      }\n     )\";\n \n     const std::string softmax_hlo_string = absl::Substitute(\n@@ -150,11 +157,13 @@ TEST_P(OneDnnSoftmaxTest, SoftmaxGenericNumericalCorrectnessTest) {\n   }\n \n   const std::string onednn_softmax_hlo_template_string = R\"(\n-        HloModule softmax_module\n-        ENTRY main {\n-            Arg_0 = $0[$1,128,30522]{2,1,0} parameter(0)\n-            ROOT custom-call = $0[$1,128,30522]{2,1,0} custom-call(Arg_0), custom_call_target=\"$2\", backend_config={\"onednn_softmax_config\":{\"softmax_axis\":2}}\n-        }\n+    HloModule softmax_module\n+    ENTRY main {\n+      Arg_0 = $0[$1,128,30522]{2,1,0} parameter(0)\n+      ROOT custom-call = $0[$1,128,30522]{2,1,0} custom-call(Arg_0),\n+          custom_call_target=\"$2\",\n+          backend_config={\"onednn_softmax_config\":{\"softmax_axis\":2}}\n+    }\n     )\";\n \n   auto onednn_softmax_hlo_string =\n@@ -180,37 +189,39 @@ INSTANTIATE_TEST_SUITE_P(OneDnnSoftmaxTestSuite, OneDnnSoftmaxTest,\n \n TEST_F(OneDnnSoftmaxTest, SoftmaxFP32OnAxisZero) {\n   const std::string softmax_hlo_string = R\"(\n-        HloModule softmax_module\n-        region_max {\n-          Arg_0 = f32[] parameter(0)\n-          Arg_1 = f32[] parameter(1)\n-          ROOT maximum = f32[] maximum(Arg_0, Arg_1)\n-        }\n-        region_add {\n-          Arg_0 = f32[] parameter(0)\n-          Arg_1 = f32[] parameter(1)\n-          ROOT add = f32[] add(Arg_0, Arg_1)\n-        }\n-        ENTRY main {\n-          Arg_0 = f32[3,1,1]{2,1,0} parameter(0)\n-          neg_inf = f32[] constant(-inf)\n-          reduce_max = f32[1,1]{1,0} reduce(Arg_0, neg_inf), dimensions={0}, to_apply=region_max\n-          neg_inf.1 = f32[1,1]{1,0} constant({ {-inf} })\n-          maximum = f32[1,1]{1,0} maximum(reduce_max, neg_inf.1)\n-          reshape.0 = f32[1,1,1]{2,1,0} reshape(maximum)\n-          broadcast.0 = f32[1,1,1]{2,1,0} broadcast(reshape.0), dimensions={0,1,2}\n-          reshape.1 = f32[1,1]{1,0} reshape(broadcast.0)\n-          broadcast.1 = f32[3,1,1]{2,1,0} broadcast(reshape.1), dimensions={1,2}\n-          subtract = f32[3,1,1]{2,1,0} subtract(Arg_0, broadcast.1)\n-          exponential = f32[3,1,1]{2,1,0} exponential(subtract)\n-          const_zero = f32[] constant(0)\n-          reduce_add = f32[1,1]{1,0} reduce(exponential, const_zero), dimensions={0}, to_apply=region_add\n-          reshape.2 = f32[1,1,1]{2,1,0} reshape(reduce_add)\n-          broadcast.2 = f32[1,1,1]{2,1,0} broadcast(reshape.2), dimensions={0,1,2}\n-          reshape.3 = f32[1,1]{1,0} reshape(broadcast.2)\n-          broadcast.3 = f32[3,1,1]{2,1,0} broadcast(reshape.3), dimensions={1,2}\n-          ROOT divide = f32[3,1,1]{2,1,0} divide(exponential, broadcast.3)\n-        }\n+    HloModule softmax_module\n+    region_max {\n+      Arg_0 = f32[] parameter(0)\n+      Arg_1 = f32[] parameter(1)\n+      ROOT maximum = f32[] maximum(Arg_0, Arg_1)\n+    }\n+    region_add {\n+      Arg_0 = f32[] parameter(0)\n+      Arg_1 = f32[] parameter(1)\n+      ROOT add = f32[] add(Arg_0, Arg_1)\n+    }\n+    ENTRY main {\n+      Arg_0 = f32[3,1,1]{2,1,0} parameter(0)\n+      neg_inf = f32[] constant(-inf)\n+      reduce_max = f32[1,1]{1,0} reduce(Arg_0, neg_inf), dimensions={0},\n+                   to_apply=region_max\n+      neg_inf.1 = f32[1,1]{1,0} constant({ {-inf} })\n+      maximum = f32[1,1]{1,0} maximum(reduce_max, neg_inf.1)\n+      reshape.0 = f32[1,1,1]{2,1,0} reshape(maximum)\n+      broadcast.0 = f32[1,1,1]{2,1,0} broadcast(reshape.0), dimensions={0,1,2}\n+      reshape.1 = f32[1,1]{1,0} reshape(broadcast.0)\n+      broadcast.1 = f32[3,1,1]{2,1,0} broadcast(reshape.1), dimensions={1,2}\n+      subtract = f32[3,1,1]{2,1,0} subtract(Arg_0, broadcast.1)\n+      exponential = f32[3,1,1]{2,1,0} exponential(subtract)\n+      const_zero = f32[] constant(0)\n+      reduce_add = f32[1,1]{1,0} reduce(exponential, const_zero),\n+                   dimensions={0}, to_apply=region_add\n+      reshape.2 = f32[1,1,1]{2,1,0} reshape(reduce_add)\n+      broadcast.2 = f32[1,1,1]{2,1,0} broadcast(reshape.2), dimensions={0,1,2}\n+      reshape.3 = f32[1,1]{1,0} reshape(broadcast.2)\n+      broadcast.3 = f32[3,1,1]{2,1,0} broadcast(reshape.3), dimensions={1,2}\n+      ROOT divide = f32[3,1,1]{2,1,0} divide(exponential, broadcast.3)\n+    }\n     )\";\n \n   TestSoftmaxPatternMatching(softmax_hlo_string, /*expected_softmax_axis*/ 0);\n@@ -222,45 +233,46 @@ TEST_F(OneDnnSoftmaxTest, SoftmaxWithBF16ConvertOutputFP32Pattern) {\n   }\n \n   const std::string softmax_hlo_string = R\"(\n-        HloModule softmax_module\n-        region_max {\n-            Arg_0 = f32[] parameter(0)\n-            Arg_1 = f32[] parameter(1)\n-            ROOT maximum = f32[] maximum(Arg_0, Arg_1)\n-        }\n-        region_add {\n-            Arg_0 = f32[] parameter(0)\n-            Arg_1 = f32[] parameter(1)\n-            ROOT add = f32[] add(Arg_0, Arg_1)\n-        }\n-        ENTRY main {\n-            Arg_0 = f32[16,128,30522]{2,1,0} parameter(0)\n-            neg_inf = f32[] constant(-inf)\n-            reduce_max = f32[16,128]{1,0} reduce(Arg_0, neg_inf), dimensions={2}, to_apply=region_max\n-            reshape.0 = f32[16,128,1]{2,1,0} reshape(reduce_max)\n-            broadcast.0 = f32[16,128,1]{2,1,0} broadcast(reshape.0), dimensions={0,1,2}\n-            reshape.1 = f32[16,128]{1,0} reshape(broadcast.0)\n-            broadcast.1 = f32[16,128,30522]{2,1,0} broadcast(reshape.1), dimensions={0,1}\n-            subtract = f32[16,128,30522]{2,1,0} subtract(Arg_0, broadcast.1)\n-            exponential = f32[16,128,30522]{2,1,0} exponential(subtract)\n-            const_zero = f32[] constant(0)\n-            reduce_add = f32[16,128]{1,0} reduce(exponential, const_zero), dimensions={2}, to_apply=region_add\n-            reshape.2 = f32[16,128,1]{2,1,0} reshape(reduce_add)\n-            broadcast.2 = f32[16,128,1]{2,1,0} broadcast(reshape.2), dimensions={0,1,2}\n-            reshape.3 = f32[16,128]{1,0} reshape(broadcast.2)\n-            broadcast.3 = f32[16,128,30522]{2,1,0} broadcast(reshape.3), dimensions={0,1}\n-            divide = f32[16,128,30522]{2,1,0} divide(exponential, broadcast.3)\n-            ROOT convert = bf16[16,128,30522]{2,1,0} convert(divide)\n-        }\n+    HloModule softmax_module\n+    region_max {\n+      Arg_0 = f32[] parameter(0)\n+      Arg_1 = f32[] parameter(1)\n+      ROOT maximum = f32[] maximum(Arg_0, Arg_1)\n+    }\n+    region_add {\n+      Arg_0 = f32[] parameter(0)\n+      Arg_1 = f32[] parameter(1)\n+      ROOT add = f32[] add(Arg_0, Arg_1)\n+    }\n+    ENTRY main {\n+      Arg_0 = f32[16,128,30522]{2,1,0} parameter(0)\n+      neg_inf = f32[] constant(-inf)\n+      reduce_max = f32[16,128]{1,0} reduce(Arg_0, neg_inf), dimensions={2},\n+                   to_apply=region_max\n+      reshape.0 = f32[16,128,1]{2,1,0} reshape(reduce_max)\n+      broadcast.0 = f32[16,128,1]{2,1,0} broadcast(reshape.0),\n+                    dimensions={0,1,2}\n+      reshape.1 = f32[16,128]{1,0} reshape(broadcast.0)\n+      broadcast.1 = f32[16,128,30522]{2,1,0} broadcast(reshape.1),\n+                    dimensions={0,1}\n+      subtract = f32[16,128,30522]{2,1,0} subtract(Arg_0, broadcast.1)\n+      exponential = f32[16,128,30522]{2,1,0} exponential(subtract)\n+      const_zero = f32[] constant(0)\n+      reduce_add = f32[16,128]{1,0} reduce(exponential, const_zero),\n+                   dimensions={2}, to_apply=region_add\n+      reshape.2 = f32[16,128,1]{2,1,0} reshape(reduce_add)\n+      broadcast.2 = f32[16,128,1]{2,1,0} broadcast(reshape.2),\n+                    dimensions={0,1,2}\n+      reshape.3 = f32[16,128]{1,0} reshape(broadcast.2)\n+      broadcast.3 = f32[16,128,30522]{2,1,0} broadcast(reshape.3),\n+                    dimensions={0,1}\n+      divide = f32[16,128,30522]{2,1,0} divide(exponential, broadcast.3)\n+      ROOT convert = bf16[16,128,30522]{2,1,0} convert(divide)\n+    }\n     )\";\n \n   TestSoftmaxPatternMatching(softmax_hlo_string, /*expected_softmax_axis=*/2);\n }\n \n-#endif  // INTEL_MKL\n-\n-// Ensure at least one test case is linked to avoid test failures.\n-TEST(Dummy, Test) {}\n-\n }  // namespace cpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 459,
        "additions": 236,
        "deletions": 223
    }
}