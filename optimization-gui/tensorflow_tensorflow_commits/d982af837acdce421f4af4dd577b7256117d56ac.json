{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 829994596",
    "sha": "d982af837acdce421f4af4dd577b7256117d56ac",
    "files": [
        {
            "sha": "f882ddc4fc0194b20eab5fff8e4621d21323a807",
            "filename": "tensorflow/core/tpu/tpu_compile.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d982af837acdce421f4af4dd577b7256117d56ac/tensorflow%2Fcore%2Ftpu%2Ftpu_compile.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d982af837acdce421f4af4dd577b7256117d56ac/tensorflow%2Fcore%2Ftpu%2Ftpu_compile.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Ftpu_compile.cc?ref=d982af837acdce421f4af4dd577b7256117d56ac",
            "patch": "@@ -148,7 +148,8 @@ absl::Status AssignDevicesToArgsAndRetvals(\n   auto assign = [&](Node* node,\n                     const xla::OpSharding& sharding) -> absl::Status {\n     if (sharding.type() == xla::OpSharding::MAXIMAL) {\n-      const string device = CoreDevice(sharding.tile_assignment_devices(0));\n+      const std::string device =\n+          CoreDevice(sharding.tile_assignment_devices(0));\n       node->set_assigned_device_name(device);\n       node->set_requested_device(device);\n     } else {\n@@ -180,16 +181,17 @@ absl::Status AssignDevicesToArgsAndRetvals(\n \n void ConvertGraphShapeInfoToShapeMap(\n     const Graph& graph, const GraphShapeInfo& graph_shape_info,\n-    std::unordered_map<string, std::vector<PartialTensorShape>>* shape_map) {\n+    std::unordered_map<std::string, std::vector<PartialTensorShape>>*\n+        shape_map) {\n   // Builds a map from node name to Node* for `graph`.\n-  std::unordered_map<string, Node*> index;\n+  std::unordered_map<std::string, Node*> index;\n   for (Node* node : graph.nodes()) {\n     index[node->name()] = node;\n   }\n   // Discards the resource handle shape info while converting to the correct map\n   // form.\n   for (const auto& node_shape_info : graph_shape_info) {\n-    const string& node_name = node_shape_info.first;\n+    const std::string& node_name = node_shape_info.first;\n     const std::vector<InferredShape>& output_shapes = node_shape_info.second;\n     // Gets the vector of partial shapes, first converting node name to Node*\n     // using index. graph is the subgraph of the original graph assigned to a\n@@ -248,7 +250,7 @@ absl::Status OptimizeGraph(const tpu::TPUCompileMetadataProto& metadata,\n         metadata, arg_shapes, graph->get(), flr, &shape_info));\n     // Converts the GraphShapeInfo into the form needed by the constant-folding\n     // pass of the optimizer.\n-    std::unordered_map<string, std::vector<PartialTensorShape>> shape_map;\n+    std::unordered_map<std::string, std::vector<PartialTensorShape>> shape_map;\n     ConvertGraphShapeInfoToShapeMap(**graph, shape_info, &shape_map);\n     optimizer_opts.shape_map = &shape_map;\n     optimizer.Optimize(flr, flr->env(), flr->device(), graph, optimizer_opts);\n@@ -259,7 +261,7 @@ absl::Status OptimizeGraph(const tpu::TPUCompileMetadataProto& metadata,\n     GraphShapeInfo shape_info;\n     TF_RETURN_IF_ERROR(internal::RunShapeInferenceOnComputation(\n         metadata, arg_shapes, graph->get(), flr, &shape_info));\n-    std::unordered_map<string, std::vector<PartialTensorShape>> shape_map;\n+    std::unordered_map<std::string, std::vector<PartialTensorShape>> shape_map;\n     ConvertGraphShapeInfoToShapeMap(**graph, shape_info, &shape_map);\n     GraphOptimizer::Options optimizer_opts;\n     optimizer_opts.shape_map = &shape_map;\n@@ -487,7 +489,7 @@ absl::Status CompileTFFunctionToHlo(\n   TF_RETURN_IF_ERROR(compiler->flib_runtime()->Instantiate(\n       function.name(), AttrSlice(&function.attr()), &handle));\n   const FunctionBody* fbody = compiler->flib_runtime()->GetFunctionBody(handle);\n-  const string function_id =\n+  const std::string function_id =\n       Canonicalize(function.name(), AttrSlice(&function.attr()));\n \n   std::unique_ptr<Graph> graph(new Graph(&flib_definition));"
        },
        {
            "sha": "587d6341527a204f5d6aa895af1698491aa8d52a",
            "filename": "tensorflow/core/tpu/tpu_embedding_optimization_parameters_utils.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d982af837acdce421f4af4dd577b7256117d56ac/tensorflow%2Fcore%2Ftpu%2Ftpu_embedding_optimization_parameters_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d982af837acdce421f4af4dd577b7256117d56ac/tensorflow%2Fcore%2Ftpu%2Ftpu_embedding_optimization_parameters_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Ftpu_embedding_optimization_parameters_utils.cc?ref=d982af837acdce421f4af4dd577b7256117d56ac",
            "patch": "@@ -475,7 +475,7 @@ absl::Status LoadOpShapeFunction::operator()(\n     shape_inference::InferenceContext* c) const {\n   int table_id;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"table_id\", &table_id));\n-  string table_name;\n+  std::string table_name;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"table_name\", &table_name));\n   // Exactly one must be non-default.\n   if ((table_id >= 0) == (!table_name.empty())) {\n@@ -505,7 +505,7 @@ absl::Status RetrieveOpShapeFunction::operator()(\n     shape_inference::InferenceContext* c) const {\n   int table_id;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"table_id\", &table_id));\n-  string table_name;\n+  std::string table_name;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"table_name\", &table_name));\n   // Exactly one must be non-default.\n   if ((table_id >= 0) == (!table_name.empty())) {"
        },
        {
            "sha": "aab534085423a7c0670ff0edb50e049835d39c46",
            "filename": "tensorflow/core/tpu/tpu_execute.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d982af837acdce421f4af4dd577b7256117d56ac/tensorflow%2Fcore%2Ftpu%2Ftpu_execute.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d982af837acdce421f4af4dd577b7256117d56ac/tensorflow%2Fcore%2Ftpu%2Ftpu_execute.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Ftpu_execute.h?ref=d982af837acdce421f4af4dd577b7256117d56ac",
            "patch": "@@ -42,7 +42,7 @@ absl::StatusOr<xla::ExecutionOutput> TPUExecute(\n     const TPUHostTransferInfoProto& host_transfers,\n     const xla::HloProto& hlo_metadata,\n     std::vector<xla::ExecutionInput> arguments,\n-    const std::string& rendezvous_key_base, uint32 rng_seed,\n+    const std::string& rendezvous_key_base, uint32_t rng_seed,\n     tpu::TpuNodeContext* node_context, xla::DeviceAssignment* device_assignment,\n     CancellationManager* cancellation_manager, OpKernelContext* ctx,\n     stream_executor::Stream* stream,"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 12,
        "deletions": 10
    }
}