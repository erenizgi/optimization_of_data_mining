{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 832710361",
    "sha": "89fab6cb5d6bc2a053bc89d81d1b268eb9226f82",
    "files": [
        {
            "sha": "88b379331b32ef2bf368e8bb2ed19ef8d7f17e0e",
            "filename": "tensorflow/compiler/tests/randomized_tests.cc",
            "status": "modified",
            "additions": 190,
            "deletions": 182,
            "changes": 372,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/89fab6cb5d6bc2a053bc89d81d1b268eb9226f82/tensorflow%2Fcompiler%2Ftests%2Frandomized_tests.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/89fab6cb5d6bc2a053bc89d81d1b268eb9226f82/tensorflow%2Fcompiler%2Ftests%2Frandomized_tests.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftests%2Frandomized_tests.cc?ref=89fab6cb5d6bc2a053bc89d81d1b268eb9226f82",
            "patch": "@@ -110,12 +110,12 @@ namespace {\n int64_t tf_xla_random_seed = 0;\n int32_t tf_xla_test_repetitions = 20;\n int64_t tf_xla_max_tensor_size = 10000LL;\n-string* tf_xla_test_device_ptr;       // initial value set in main()\n-string* tf_xla_reference_device_ptr;  // initial value set in main()\n+std::string* tf_xla_test_device_ptr;       // initial value set in main()\n+std::string* tf_xla_reference_device_ptr;  // initial value set in main()\n bool tf_xla_test_use_jit = true;\n bool tf_xla_test_use_mlir = false;\n \n-string LocalDeviceToFullDeviceName(const string& device) {\n+std::string LocalDeviceToFullDeviceName(const std::string& device) {\n   return absl::StrCat(\"/job:localhost/replica:0/task:0/device:\", device);\n }\n \n@@ -129,7 +129,7 @@ constexpr std::array<DataType, 4> kAllNumberTypes = {\n // operator.\n class OpTestBuilder {\n  public:\n-  explicit OpTestBuilder(const string& op_name);\n+  explicit OpTestBuilder(const std::string& op_name);\n \n   // Adds an input 'tensor' as a Placeholder node.\n   OpTestBuilder& Input(const Tensor& tensor);\n@@ -161,10 +161,11 @@ class OpTestBuilder {\n   // sets it to the NodeDef of the operator under test. Fills 'inputs' and\n   // 'outputs' with the names of the input placeholder nodes and the output\n   // identity nodes, respectively.\n-  absl::Status BuildGraph(const string& name_prefix, const string& device,\n-                          bool use_jit, GraphDef* graphdef,\n-                          NodeDef** test_node_def, std::vector<string>* inputs,\n-                          std::vector<string>* outputs) const;\n+  absl::Status BuildGraph(const std::string& name_prefix,\n+                          const std::string& device, bool use_jit,\n+                          GraphDef* graphdef, NodeDef** test_node_def,\n+                          std::vector<std::string>* inputs,\n+                          std::vector<std::string>* outputs) const;\n \n   struct InputDescription {\n     Tensor tensor;\n@@ -182,7 +183,7 @@ class OpTestBuilder {\n   std::vector<InputDescription> inputs_;\n };\n \n-OpTestBuilder::OpTestBuilder(const string& op_name) {\n+OpTestBuilder::OpTestBuilder(const std::string& op_name) {\n   node_def_.set_op(op_name);\n }\n \n@@ -247,12 +248,10 @@ OpTestBuilder& OpTestBuilder::Attr(absl::string_view attr_name,\n   return *this;\n }\n \n-absl::Status OpTestBuilder::BuildGraph(const string& name_prefix,\n-                                       const string& device, bool use_jit,\n-                                       GraphDef* graphdef,\n-                                       NodeDef** test_node_def,\n-                                       std::vector<string>* inputs,\n-                                       std::vector<string>* outputs) const {\n+absl::Status OpTestBuilder::BuildGraph(\n+    const std::string& name_prefix, const std::string& device, bool use_jit,\n+    GraphDef* graphdef, NodeDef** test_node_def,\n+    std::vector<std::string>* inputs, std::vector<std::string>* outputs) const {\n   OpRegistryInterface* op_registry = OpRegistry::Global();\n \n   const OpDef* op_def;\n@@ -275,7 +274,7 @@ absl::Status OpTestBuilder::BuildGraph(const string& name_prefix,\n   // Build feed and fetch nodes.\n   for (int i = 0; i < input_types.size(); ++i) {\n     NodeDef* def = graphdef->add_node();\n-    string name = absl::StrCat(name_prefix, \"_input_\", i);\n+    std::string name = absl::StrCat(name_prefix, \"_input_\", i);\n     TF_RETURN_IF_ERROR(NodeDefBuilder(name, \"Placeholder\")\n                            .Device(device)\n                            .Attr(\"dtype\", input_types[i])\n@@ -286,7 +285,7 @@ absl::Status OpTestBuilder::BuildGraph(const string& name_prefix,\n \n   for (int i = 0; i < output_types.size(); ++i) {\n     NodeDef* def = graphdef->add_node();\n-    string name = absl::StrCat(name_prefix, \"_output_\", i);\n+    std::string name = absl::StrCat(name_prefix, \"_output_\", i);\n     TF_RETURN_IF_ERROR(NodeDefBuilder(name, \"Identity\")\n                            .Device(device)\n                            .Attr(\"T\", output_types[i])\n@@ -494,7 +493,7 @@ class OpTest : public ::testing::Test {\n                                  const std::vector<int64_t>& spatial_dims);\n \n   // Converts an int64 vector to an int32 vector.\n-  std::vector<int32> AsInt32s(const std::vector<int64_t>& int64s);\n+  std::vector<int32_t> AsInt32s(const std::vector<int64_t>& int64s);\n \n   std::mt19937& generator() { return *generator_; }\n \n@@ -664,16 +663,16 @@ class TensorGeneratorComplex64 : public TensorGenerator<complex64> {\n   }\n };\n \n-class TensorGeneratorInt32 : public TensorGenerator<int32> {\n+class TensorGeneratorInt32 : public TensorGenerator<int32_t> {\n  public:\n   explicit TensorGeneratorInt32(OpTest& test) : TensorGenerator(test) {}\n   DataType dtype() override { return DT_INT32; }\n-  void RandomVals(std::optional<int32> lo, std::optional<int32> hi,\n+  void RandomVals(std::optional<int32_t> lo, std::optional<int32_t> hi,\n                   bool needs_unique_values,\n-                  absl::FixedArray<int32>& vals) override {\n-    absl::flat_hash_set<int32> already_generated;\n-    std::uniform_int_distribution<int32> distribution(lo.value_or(-(1 << 20)),\n-                                                      hi.value_or(1 << 20));\n+                  absl::FixedArray<int32_t>& vals) override {\n+    absl::flat_hash_set<int32_t> already_generated;\n+    std::uniform_int_distribution<int32_t> distribution(lo.value_or(-(1 << 20)),\n+                                                        hi.value_or(1 << 20));\n     for (int64_t i = 0; i < vals.size(); ++i) {\n       int32_t generated;\n       do {\n@@ -685,13 +684,13 @@ class TensorGeneratorInt32 : public TensorGenerator<int32> {\n   }\n };\n \n-class TensorGeneratorInt64 : public TensorGenerator<int64> {\n+class TensorGeneratorInt64 : public TensorGenerator<int64_t> {\n  public:\n   explicit TensorGeneratorInt64(OpTest& test) : TensorGenerator(test) {}\n   DataType dtype() override { return DT_INT64; }\n-  void RandomVals(std::optional<int64> lo, std::optional<int64> hi,\n+  void RandomVals(std::optional<int64_t> lo, std::optional<int64_t> hi,\n                   bool needs_unique_values,\n-                  absl::FixedArray<int64>& vals) override {\n+                  absl::FixedArray<int64_t>& vals) override {\n     absl::flat_hash_set<int64_t> already_generated;\n     std::uniform_int_distribution<int64_t> distribution(\n         lo.value_or(-(1LL << 40)), hi.value_or(1LL << 40));\n@@ -928,18 +927,19 @@ Tensor OpTest::RandomBoundedTensor(DataType dtype, Tensor lo, Tensor hi) {\n       break;\n     }\n     case DT_INT32: {\n-      auto lo_flat = lo.flat<int32>();\n-      auto hi_flat = hi.flat<int32>();\n-      test::FillFn<int32>(&tensor, [this, &lo_flat, &hi_flat](int i) -> int32 {\n-        std::uniform_int_distribution<int32> distribution(lo_flat(i),\n-                                                          hi_flat(i));\n-        return distribution(generator());\n-      });\n+      auto lo_flat = lo.flat<int32_t>();\n+      auto hi_flat = hi.flat<int32_t>();\n+      test::FillFn<int32_t>(\n+          &tensor, [this, &lo_flat, &hi_flat](int i) -> int32_t {\n+            std::uniform_int_distribution<int32_t> distribution(lo_flat(i),\n+                                                                hi_flat(i));\n+            return distribution(generator());\n+          });\n       break;\n     }\n     case DT_INT64: {\n-      auto lo_flat = lo.flat<int64>();\n-      auto hi_flat = hi.flat<int64>();\n+      auto lo_flat = lo.flat<int64_t>();\n+      auto hi_flat = hi.flat<int64_t>();\n       test::FillFn<int64_t>(\n           &tensor, [this, &lo_flat, &hi_flat](int i) -> int64_t {\n             std::uniform_int_distribution<int64_t> distribution(lo_flat(i),\n@@ -1021,21 +1021,21 @@ OpTest::BroadcastableDims() {\n \n Tensor OpTest::RandomReductionIndices(int rank) {\n   std::bernoulli_distribution random_bool;\n-  std::vector<int32> indices;\n+  std::vector<int32_t> indices;\n   for (int i = 0; i < rank; ++i) {\n     if (random_bool(generator())) {\n       indices.push_back(i);\n     }\n   }\n-  return test::AsTensor<int32>(indices);\n+  return test::AsTensor<int32_t>(indices);\n }\n \n // Helper that converts 'values' to an int32 or int64 Tensor.\n static Tensor AsIntTensor(DataType dtype, const std::vector<int64_t>& values) {\n   switch (dtype) {\n     case DT_INT32: {\n-      std::vector<int32> values32(values.begin(), values.end());\n-      return test::AsTensor<int32>(values32);\n+      std::vector<int32_t> values32(values.begin(), values.end());\n+      return test::AsTensor<int32_t>(values32);\n     }\n     case DT_INT64:\n       return test::AsTensor<int64_t>(values);\n@@ -1092,9 +1092,9 @@ OpTest::ConcatArguments OpTest::ChooseConcatArguments(bool int64_idx_allowed) {\n   std::vector<int64_t> dims = RandomDims(1, 4, 0, 64);\n \n   int axis =\n-      std::uniform_int_distribution<int32>(0, dims.size() - 1)(generator());\n-  a.axis =\n-      use_int64_idx ? test::AsScalar<int64>(axis) : test::AsScalar<int32>(axis);\n+      std::uniform_int_distribution<int32_t>(0, dims.size() - 1)(generator());\n+  a.axis = use_int64_idx ? test::AsScalar<int64_t>(axis)\n+                         : test::AsScalar<int32_t>(axis);\n \n   for (int i = 0; i < a.n; ++i) {\n     std::vector<int64_t> shape = dims;\n@@ -1113,7 +1113,7 @@ OpTest::EinsumArguments OpTest::ChooseEinsumArguments() {\n   switch (op_kind) {\n     case matmul:\n     case batchmatmul: {\n-      std::vector<int64> dims;\n+      std::vector<int64_t> dims;\n       if (op_kind == matmul) {\n         a.equation = \"ij,jk->ik\";\n         dims = RandomDims(2, 2);\n@@ -1131,7 +1131,7 @@ OpTest::EinsumArguments OpTest::ChooseEinsumArguments() {\n     }\n     case dot: {\n       a.equation = \"i,i->\";\n-      std::vector<int64> dims = RandomDims(1, 1);\n+      std::vector<int64_t> dims = RandomDims(1, 1);\n       a.lhs_dims = dims;\n       a.rhs_dims = dims;\n       break;\n@@ -1166,11 +1166,11 @@ OpTest::GatherArguments OpTest::ChooseGatherArguments(bool axis_0) {\n         a.batch_dims, kDefaultMaxRank - 1);\n     axis = axis_distribution(generator());\n   }\n-  a.axis = test::AsScalar<int32>((int32)axis);\n+  a.axis = test::AsScalar<int32_t>((int32_t)axis);\n   a.params_shape = RandomDims(axis + 1, kDefaultMaxRank, 1, 16);\n   std::vector<int64_t> indices_shape = RandomDims(0, 3, 0, 16);\n-  a.indices = RandomBoundedTensor<int32>(DT_INT32, 0, a.params_shape[axis] - 1,\n-                                         false, indices_shape);\n+  a.indices = RandomBoundedTensor<int32_t>(\n+      DT_INT32, 0, a.params_shape[axis] - 1, false, indices_shape);\n \n   return a;\n }\n@@ -1209,7 +1209,7 @@ OpTest::ScatterArguments OpTest::ChooseScatterArguments() {\n   a.indices_type = DT_INT32;\n   a.shape = RandomDims(1, kDefaultMaxRank, 1);\n   int rank = a.shape.size();\n-  std::uniform_int_distribution<int32> index_len_dist(1, rank);\n+  std::uniform_int_distribution<int32_t> index_len_dist(1, rank);\n   int index_len = index_len_dist(generator());\n   std::vector<int64_t> indices_first = RandomDims(1, kDefaultMaxRank - 1, 1);\n   std::vector<int64_t> indices_shape(indices_first);\n@@ -1219,9 +1219,9 @@ OpTest::ScatterArguments OpTest::ChooseScatterArguments() {\n     updates_shape.push_back(a.shape[index_len + i]);\n   }\n   Tensor indices_lo(a.indices_type, TensorShape(indices_shape));\n-  test::FillFn<int32>(&indices_lo, [](int i) -> int32 { return 0; });\n+  test::FillFn<int32_t>(&indices_lo, [](int i) -> int32_t { return 0; });\n   Tensor indices_hi(a.indices_type, TensorShape(indices_shape));\n-  test::FillFn<int32>(&indices_hi, [index_len, &a](int i) -> int32 {\n+  test::FillFn<int32_t>(&indices_hi, [index_len, &a](int i) -> int32_t {\n     int idx_dim = i % index_len;\n     return a.shape[idx_dim] - 1;\n   });\n@@ -1239,16 +1239,16 @@ OpTest::SliceArguments OpTest::ChooseSliceArguments(bool neg_one_size) {\n   a.shape = RandomDims();\n   int rank = a.shape.size();\n \n-  std::vector<int32> indices(rank);\n+  std::vector<int32_t> indices(rank);\n   a.size.resize(rank);\n   for (int i = 0; i < rank; ++i) {\n     indices[i] =\n-        std::uniform_int_distribution<int32>(0, a.shape[i])(generator());\n+        std::uniform_int_distribution<int32_t>(0, a.shape[i])(generator());\n     int64_t low = neg_one_size ? -1 : 0;\n     a.size[i] = std::uniform_int_distribution<int64_t>(\n         low, a.shape[i] - indices[i])(generator());\n   }\n-  a.indices = test::AsTensor<int32>(indices);\n+  a.indices = test::AsTensor<int32_t>(indices);\n \n   return a;\n }\n@@ -1341,8 +1341,8 @@ std::vector<int64_t> OpTest::ImageDims(\n   return dims;\n }\n \n-std::vector<int32> OpTest::AsInt32s(const std::vector<int64_t>& int64s) {\n-  return std::vector<int32>(int64s.begin(), int64s.end());\n+std::vector<int32_t> OpTest::AsInt32s(const std::vector<int64_t>& int64s) {\n+  return std::vector<int32_t>(int64s.begin(), int64s.end());\n }\n \n // Functions for comparing tensors.\n@@ -1382,11 +1382,11 @@ bool IsClose<complex64>(const complex64& x, const complex64& y, double atol,\n }\n \n template <typename T>\n-string Str(T x) {\n+std::string Str(T x) {\n   return absl::StrCat(x);\n }\n template <>\n-string Str<complex64>(complex64 x) {\n+std::string Str<complex64>(complex64 x) {\n   return absl::StrCat(\"(\", x.real(), \", \", x.imag(), \")\");\n }\n \n@@ -1460,7 +1460,7 @@ absl::Status TensorsAreClose(const Tensor& a, const Tensor& b, double atol,\n     case DT_COMPLEX64:\n       return TensorsAreCloseImpl<complex64>(a, b, atol, rtol);\n     case DT_INT32:\n-      return TensorsAreEqualImpl<int32>(a, b);\n+      return TensorsAreEqualImpl<int32_t>(a, b);\n     case DT_INT64:\n       return TensorsAreEqualImpl<int64_t>(a, b);\n     case DT_BOOL:\n@@ -1499,9 +1499,10 @@ OpTest::TestResult OpTest::ExpectTfAndXlaOutputsAreClose(\n     VLOG(1) << \"Input: \" << input_tensors.back().DebugString();\n   }\n \n-  string reference_device =\n+  std::string reference_device =\n       LocalDeviceToFullDeviceName(*tf_xla_reference_device_ptr);\n-  string test_device = LocalDeviceToFullDeviceName(*tf_xla_test_device_ptr);\n+  std::string test_device =\n+      LocalDeviceToFullDeviceName(*tf_xla_test_device_ptr);\n \n   DeviceNameUtils::ParsedName parsed_name;\n   if (!DeviceNameUtils::ParseLocalName(*tf_xla_test_device_ptr, &parsed_name)) {\n@@ -1512,8 +1513,8 @@ OpTest::TestResult OpTest::ExpectTfAndXlaOutputsAreClose(\n   ++num_tests_;\n \n   GraphDef graph;\n-  std::vector<string> expected_inputs, test_inputs;\n-  std::vector<string> expected_fetches, test_fetches;\n+  std::vector<std::string> expected_inputs, test_inputs;\n+  std::vector<std::string> expected_fetches, test_fetches;\n   absl::Status status = builder.BuildGraph(\n       absl::StrCat(\"test\", num_tests_, \"_expected\"), reference_device,\n       /*use_jit=*/false, &graph, /*test_node_def=*/nullptr, &expected_inputs,\n@@ -1550,8 +1551,9 @@ OpTest::TestResult OpTest::ExpectTfAndXlaOutputsAreClose(\n     return kFatalError;\n   }\n \n-  std::vector<std::pair<string, Tensor>> expected_feeds(expected_inputs.size());\n-  std::vector<std::pair<string, Tensor>> test_feeds(test_inputs.size());\n+  std::vector<std::pair<std::string, Tensor>> expected_feeds(\n+      expected_inputs.size());\n+  std::vector<std::pair<std::string, Tensor>> test_feeds(test_inputs.size());\n   CHECK_EQ(input_tensors.size(), expected_inputs.size());\n   CHECK_EQ(input_tensors.size(), test_inputs.size());\n \n@@ -1707,12 +1709,12 @@ TEST_F(OpTest, ArgMax) {\n     auto type = Choose<DataType>({DT_BOOL, DT_FLOAT});\n     std::vector<int64_t> dims = RandomDims(1, 5, 1);\n     int num_dims = dims.size();\n-    int reduce_dim =\n-        std::uniform_int_distribution<int32>(-num_dims, num_dims)(generator());\n+    int reduce_dim = std::uniform_int_distribution<int32_t>(\n+        -num_dims, num_dims)(generator());\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"ArgMax\")\n             .RandomInput(type, dims)\n-            .Input(test::AsScalar<int32>(reduce_dim))\n+            .Input(test::AsScalar<int32_t>(reduce_dim))\n             .Attr(\"T\", type)\n             .Attr(\"Tidx\", DT_INT32)\n             .Attr(\"output_type\", DT_INT32));\n@@ -1724,12 +1726,12 @@ TEST_F(OpTest, ArgMin) {\n     auto type = Choose<DataType>({DT_BOOL, DT_FLOAT});\n     std::vector<int64_t> dims = RandomDims(1, 5, 1);\n     int num_dims = dims.size();\n-    int reduce_dim =\n-        std::uniform_int_distribution<int32>(-num_dims, num_dims)(generator());\n+    int reduce_dim = std::uniform_int_distribution<int32_t>(\n+        -num_dims, num_dims)(generator());\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"ArgMin\")\n             .RandomInput(type, dims)\n-            .Input(test::AsScalar<int32>(reduce_dim))\n+            .Input(test::AsScalar<int32_t>(reduce_dim))\n             .Attr(\"T\", type)\n             .Attr(\"Tidx\", DT_INT32)\n             .Attr(\"output_type\", DT_INT32));\n@@ -1786,7 +1788,7 @@ TEST_F(OpTest, AvgPool) {\n         std::uniform_int_distribution<int>(1, dims[2])(generator());\n     int stride_rows = random_int(generator()),\n         stride_cols = random_int(generator());\n-    string padding = Choose<string>({\"SAME\", \"VALID\"});\n+    std::string padding = Choose<std::string>({\"SAME\", \"VALID\"});\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"AvgPool\")\n             .RandomInput(DT_FLOAT, dims)\n@@ -1817,7 +1819,7 @@ TEST_F(OpTest, AvgPool3D) {\n     int64_t batch = dims[3];\n     int64_t feature = dims[4];\n \n-    string padding = Choose<string>({\"SAME\", \"VALID\"});\n+    std::string padding = Choose<std::string>({\"SAME\", \"VALID\"});\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"AvgPool3D\")\n             .RandomInput(DT_FLOAT,\n@@ -1837,13 +1839,13 @@ TEST_F(OpTest, AvgPoolGrad) {\n   Repeatedly([this]() {\n     int batch = RandomDim(1), features = RandomDim(1);\n     WindowedSpatialDims d = ChooseWindowedSpatialDims(2);\n-    std::vector<int32> input_dims =\n+    std::vector<int32_t> input_dims =\n         AsInt32s(ImageDims(FORMAT_NHWC, batch, features, d.input_dims));\n     std::vector<int64_t> output_dims =\n         ImageDims(FORMAT_NHWC, batch, features, d.output_dims);\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"AvgPoolGrad\")\n-            .Input(test::AsTensor<int32>(input_dims))\n+            .Input(test::AsTensor<int32_t>(input_dims))\n             .RandomInput(DT_FLOAT, output_dims)\n             .Attr(\"T\", DT_FLOAT)\n             .Attr(\"ksize\", ImageDims(FORMAT_NHWC, 1, 1, d.kernel_dims))\n@@ -1859,13 +1861,13 @@ TEST_F(OpTest, AvgPool3DGrad) {\n   Repeatedly([this]() {\n     int batch = RandomDim(1), features = RandomDim(1);\n     WindowedSpatialDims d = ChooseWindowedSpatialDims(3);\n-    std::vector<int32> input_dims =\n+    std::vector<int32_t> input_dims =\n         AsInt32s(ImageDims(FORMAT_NHWC, batch, features, d.input_dims));\n     std::vector<int64_t> output_dims =\n         ImageDims(FORMAT_NHWC, batch, features, d.output_dims);\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"AvgPool3DGrad\")\n-            .Input(test::AsTensor<int32>(input_dims))\n+            .Input(test::AsTensor<int32_t>(input_dims))\n             .RandomInput(DT_FLOAT, output_dims)\n             .Attr(\"T\", DT_FLOAT)\n             .Attr(\"ksize\", ImageDims(FORMAT_NHWC, 1, 1, d.kernel_dims))\n@@ -1976,8 +1978,8 @@ TEST_F(OpTest, BatchToSpaceND) {\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"BatchToSpaceND\")\n             .RandomInput(type, input_dims)\n-            .Input(test::AsTensor<int32>(\n-                std::vector<int32>(block_dims.begin(), block_dims.end())))\n+            .Input(test::AsTensor<int32_t>(\n+                std::vector<int32_t>(block_dims.begin(), block_dims.end())))\n             .Input(crops)\n             .Attr(\"T\", type));\n   });\n@@ -2198,15 +2200,15 @@ TEST_F(OpTest, ConcatOffset) {\n \n     std::vector<int64_t> dims = RandomDims(1);\n     int concat_dim =\n-        std::uniform_int_distribution<int32>(0, dims.size() - 1)(generator());\n+        std::uniform_int_distribution<int32_t>(0, dims.size() - 1)(generator());\n \n     OpTestBuilder builder(\"ConcatOffset\");\n-    builder.Input(test::AsScalar<int32>(concat_dim));\n+    builder.Input(test::AsScalar<int32_t>(concat_dim));\n     builder.Attr(\"N\", n);\n     for (int i = 0; i < n; ++i) {\n-      std::vector<int32> shape(dims.begin(), dims.end());\n+      std::vector<int32_t> shape(dims.begin(), dims.end());\n       shape[concat_dim] = RandomDim();\n-      builder.Input(test::AsTensor<int32>(shape));\n+      builder.Input(test::AsTensor<int32_t>(shape));\n     }\n     return ExpectTfAndXlaOutputsAreClose(builder);\n   });\n@@ -2280,7 +2282,8 @@ TEST_F(OpTest, IFFT3D) {\n TEST_F(OpTest, RFFT) {\n   Repeatedly([this]() {\n     std::vector<int64_t> dims = RandomDims(1, kDefaultMaxRank, 3);\n-    Tensor fft_shape = test::AsTensor<int32>(AsInt32s({dims[dims.size() - 1]}));\n+    Tensor fft_shape =\n+        test::AsTensor<int32_t>(AsInt32s({dims[dims.size() - 1]}));\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"RFFT\").RandomInput(DT_FLOAT, dims).Input(fft_shape));\n   });\n@@ -2289,7 +2292,7 @@ TEST_F(OpTest, RFFT) {\n TEST_F(OpTest, RFFT2D) {\n   Repeatedly([this]() {\n     std::vector<int64_t> dims = RandomDims(2, kDefaultMaxRank, 3);\n-    Tensor fft_shape = test::AsTensor<int32>(\n+    Tensor fft_shape = test::AsTensor<int32_t>(\n         AsInt32s({dims[dims.size() - 2], dims[dims.size() - 1]}));\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"RFFT2D\").RandomInput(DT_FLOAT, dims).Input(fft_shape));\n@@ -2299,7 +2302,7 @@ TEST_F(OpTest, RFFT2D) {\n TEST_F(OpTest, RFFT3D) {\n   Repeatedly([this]() {\n     std::vector<int64_t> dims = RandomDims(3, kDefaultMaxRank, 3);\n-    Tensor fft_shape = test::AsTensor<int32>(AsInt32s(\n+    Tensor fft_shape = test::AsTensor<int32_t>(AsInt32s(\n         {dims[dims.size() - 3], dims[dims.size() - 2], dims[dims.size() - 1]}));\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"RFFT3D\").RandomInput(DT_FLOAT, dims).Input(fft_shape));\n@@ -2311,7 +2314,7 @@ TEST_F(OpTest, IRFFT) {\n     std::vector<int64_t> dims = RandomDims(1, kDefaultMaxRank, 3);\n     int64_t orig_size = dims[dims.size() - 1];\n     dims[dims.size() - 1] = dims[dims.size() - 1] / 2 + 1;\n-    Tensor fft_shape = test::AsTensor<int32>(AsInt32s({orig_size}));\n+    Tensor fft_shape = test::AsTensor<int32_t>(AsInt32s({orig_size}));\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"IRFFT\")\n                                              .RandomInput(DT_COMPLEX64, dims)\n                                              .Input(fft_shape));\n@@ -2324,7 +2327,7 @@ TEST_F(OpTest, IRFFT2D) {\n     std::vector<int64_t> orig_size = {dims[dims.size() - 2],\n                                       dims[dims.size() - 1]};\n     dims[dims.size() - 1] = dims[dims.size() - 1] / 2 + 1;\n-    Tensor fft_shape = test::AsTensor<int32>(AsInt32s({orig_size}));\n+    Tensor fft_shape = test::AsTensor<int32_t>(AsInt32s({orig_size}));\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"IRFFT2D\")\n                                              .RandomInput(DT_COMPLEX64, dims)\n                                              .Input(fft_shape));\n@@ -2337,7 +2340,7 @@ TEST_F(OpTest, IRFFT3D) {\n     std::vector<int64_t> orig_size = {\n         dims[dims.size() - 3], dims[dims.size() - 2], dims[dims.size() - 1]};\n     dims[dims.size() - 1] = dims[dims.size() - 1] / 2 + 1;\n-    Tensor fft_shape = test::AsTensor<int32>(AsInt32s({orig_size}));\n+    Tensor fft_shape = test::AsTensor<int32_t>(AsInt32s({orig_size}));\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"IRFFT3D\")\n                                              .RandomInput(DT_COMPLEX64, dims)\n                                              .Input(fft_shape));\n@@ -2383,7 +2386,7 @@ TEST_F(OpTest, Conv2DBackpropFilter) {\n         ImageDims(FORMAT_NHWC, batch, features_in, d.input_dims);\n     std::vector<int64_t> backprop =\n         ImageDims(FORMAT_NHWC, batch, features_out, d.output_dims);\n-    Tensor kernel_shape = test::AsTensor<int32>(AsInt32s(\n+    Tensor kernel_shape = test::AsTensor<int32_t>(AsInt32s(\n         {d.kernel_dims[0], d.kernel_dims[1], features_in, features_out}));\n     DataType type = DT_FLOAT;\n     return ExpectTfAndXlaOutputsAreClose(\n@@ -2405,7 +2408,7 @@ TEST_F(OpTest, Conv2DBackpropInput) {\n     int features_in = random_int(generator());\n     int features_out = random_int(generator());\n     int32_t batch = RandomDim();\n-    Tensor in_shape = test::AsTensor<int32>(\n+    Tensor in_shape = test::AsTensor<int32_t>(\n         AsInt32s(ImageDims(FORMAT_NHWC, batch, features_in, d.input_dims)));\n     std::vector<int64_t> backprop =\n         ImageDims(FORMAT_NHWC, batch, features_out, d.output_dims);\n@@ -2461,7 +2464,7 @@ TEST_F(OpTest, Conv3DBackpropFilter) {\n         ImageDims(FORMAT_NHWC, batch, features_in, d.input_dims);\n     std::vector<int64_t> backprop =\n         ImageDims(FORMAT_NHWC, batch, features_out, d.output_dims);\n-    Tensor kernel_shape = test::AsTensor<int32>(\n+    Tensor kernel_shape = test::AsTensor<int32_t>(\n         AsInt32s({d.kernel_dims[0], d.kernel_dims[1], d.kernel_dims[2],\n                   features_in, features_out}));\n     DataType type = DT_FLOAT;\n@@ -2485,7 +2488,7 @@ TEST_F(OpTest, Conv3DBackpropInput) {\n     int features_in = random_int(generator());\n     int features_out = random_int(generator());\n     int32_t batch = RandomDim(1);\n-    Tensor in_shape = test::AsTensor<int32>(\n+    Tensor in_shape = test::AsTensor<int32_t>(\n         AsInt32s(ImageDims(FORMAT_NHWC, batch, features_in, d.input_dims)));\n     std::vector<int64_t> backprop =\n         ImageDims(FORMAT_NHWC, batch, features_out, d.output_dims);\n@@ -2583,7 +2586,7 @@ TEST_F(OpTest, DepthwiseConv2DNativeBackpropFilter) {\n         ImageDims(FORMAT_NHWC, batch, features_in, d.input_dims);\n     std::vector<int64_t> backprop = ImageDims(\n         FORMAT_NHWC, batch, features_in * depth_multiplier, d.output_dims);\n-    Tensor kernel_shape = test::AsTensor<int32>(AsInt32s(\n+    Tensor kernel_shape = test::AsTensor<int32_t>(AsInt32s(\n         {d.kernel_dims[0], d.kernel_dims[1], features_in, depth_multiplier}));\n     std::vector<int64_t> strides = ImageDims(FORMAT_NHWC, 1, 1, d.stride_dims);\n     strides[2] = strides[1];  // Current impl only supports equal strides\n@@ -2608,7 +2611,7 @@ TEST_F(OpTest, DepthwiseConv2DBackpropInput) {\n     int features_in = random_int(generator());\n     int depth_multiplier = random_int(generator());\n     int32_t batch = RandomDim();\n-    Tensor in_shape = test::AsTensor<int32>(\n+    Tensor in_shape = test::AsTensor<int32_t>(\n         AsInt32s(ImageDims(FORMAT_NHWC, batch, features_in, d.input_dims)));\n     std::vector<int64_t> backprop = ImageDims(\n         FORMAT_NHWC, batch, features_in * depth_multiplier, d.output_dims);\n@@ -2713,15 +2716,15 @@ TEST_F(OpTest, DynamicStitch) {\n     // implementation does so require. However, the native TF implementation\n     // leaves undefined values if we don't cover everything, so we can't\n     // really test that case anyway.\n-    std::vector<int32> indices(size);\n+    std::vector<int32_t> indices(size);\n     std::iota(indices.begin(), indices.end(), 0);\n     std::shuffle(indices.begin(), indices.end(), generator());\n \n     int pos = 0;\n     for (int i = 0; i < n; ++i) {\n       TensorShape shape(index_dims[i]);\n-      Tensor t = test::AsTensor<int32>(\n-          absl::Span<const int32>(indices).subspan(pos, shape.num_elements()),\n+      Tensor t = test::AsTensor<int32_t>(\n+          absl::Span<const int32_t>(indices).subspan(pos, shape.num_elements()),\n           shape);\n       builder.Input(t);\n       pos += t.NumElements();\n@@ -2781,8 +2784,8 @@ TEST_F(OpTest, EluGrad) {\n TEST_F(OpTest, ScatterNd) {\n   Repeatedly([this]() {\n     auto a = ChooseScatterArguments();\n-    auto shape = test::AsTensor<int32>(\n-        std::vector<int32>(a.shape.begin(), a.shape.end()));\n+    auto shape = test::AsTensor<int32_t>(\n+        std::vector<int32_t>(a.shape.begin(), a.shape.end()));\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"ScatterNd\")\n                                              .Input(a.indices)\n                                              .Input(a.updates)\n@@ -2855,8 +2858,9 @@ TEST_F(OpTest, ExpandDims) {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     std::vector<int64_t> in_dims = RandomDims();\n     Tensor dim(DT_INT32, TensorShape());\n-    std::uniform_int_distribution<int32> d(-1 - in_dims.size(), in_dims.size());\n-    dim.scalar<int32>()() = d(generator());\n+    std::uniform_int_distribution<int32_t> d(-1 - in_dims.size(),\n+                                             in_dims.size());\n+    dim.scalar<int32_t>()() = d(generator());\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"ExpandDims\")\n                                              .RandomInput(type, in_dims)\n                                              .Input(dim)\n@@ -2868,10 +2872,10 @@ TEST_F(OpTest, Fill) {\n   Repeatedly([this]() {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     std::vector<int64_t> dims = RandomDims();\n-    std::vector<int32> shape(dims.begin(), dims.end());\n+    std::vector<int32_t> shape(dims.begin(), dims.end());\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"Fill\")\n-            .Input(test::AsTensor<int32>(shape))\n+            .Input(test::AsTensor<int32_t>(shape))\n             .RandomInput(type, {})\n             .Attr(\"T\", type));\n   });\n@@ -2949,9 +2953,9 @@ TEST_F(OpTest, GatherNd) {\n     std::vector<int64_t> output_shape(output_outer_shape);\n     output_shape.push_back(index_len);\n     Tensor lo(indices_type, TensorShape(output_shape));\n-    test::FillFn<int32>(&lo, [](int i) -> int32 { return 0; });\n+    test::FillFn<int32_t>(&lo, [](int i) -> int32_t { return 0; });\n     Tensor hi(indices_type, TensorShape(output_shape));\n-    test::FillFn<int32>(&hi, [index_len, &params_shape](int i) -> int32 {\n+    test::FillFn<int32_t>(&hi, [index_len, &params_shape](int i) -> int32_t {\n       int idx_dim = i % index_len;\n       return params_shape[idx_dim] - 1;\n     });\n@@ -3016,7 +3020,7 @@ TEST_F(OpTest, InplaceUpdate) {\n     x_dims.insert(x_dims.end(), common_dims.begin(), common_dims.end());\n     std::vector<int64_t> i_shape{v_dims[0]};\n     Tensor i =\n-        RandomBoundedTensor<int32>(DT_INT32, 0, x_dims[0] - 1, true, i_shape);\n+        RandomBoundedTensor<int32_t>(DT_INT32, 0, x_dims[0] - 1, true, i_shape);\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"InplaceUpdate\")\n                                              .RandomInput(type, x_dims)\n                                              .Input(i)\n@@ -3046,7 +3050,7 @@ TEST_F(OpTest, InvertPermutation) {\n     // TODO(b/211012712): Once needs_unique_values case is linear instead of\n     // quadratic time, use default Dim max instead of 8.\n     int64_t len = RandomDim(0, 8);\n-    Tensor x = RandomBoundedTensor<int32>(DT_INT32, 0, len - 1, true, {len});\n+    Tensor x = RandomBoundedTensor<int32_t>(DT_INT32, 0, len - 1, true, {len});\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"InvertPermutation\").Input(x).Attr(\"T\", DT_INT32));\n   });\n@@ -3151,7 +3155,7 @@ TEST_F(OpTest, Lgamma) {\n TEST_F(OpTest, LinSpace) {\n   Repeatedly([this]() {\n     auto ToScalar = [](DataType type, int x) {\n-      if (type == DT_INT32) return test::AsScalar<int32>(x);\n+      if (type == DT_INT32) return test::AsScalar<int32_t>(x);\n       return test::AsScalar<int64_t>(x);\n     };\n     std::uniform_int_distribution<int> distribution(-50, 50);\n@@ -3290,11 +3294,11 @@ TEST_F(OpTest, MatrixBandPart) {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     auto index_type = Choose<DataType>({DT_INT32, DT_INT64});\n     auto num_lower =\n-        RandomBoundedTensor<int32>(index_type, -2 * kDefaultMaxDimensionSize,\n-                                   2 * kDefaultMaxDimensionSize, false, {});\n+        RandomBoundedTensor<int32_t>(index_type, -2 * kDefaultMaxDimensionSize,\n+                                     2 * kDefaultMaxDimensionSize, false, {});\n     auto num_upper =\n-        RandomBoundedTensor<int32>(index_type, -2 * kDefaultMaxDimensionSize,\n-                                   2 * kDefaultMaxDimensionSize, false, {});\n+        RandomBoundedTensor<int32_t>(index_type, -2 * kDefaultMaxDimensionSize,\n+                                     2 * kDefaultMaxDimensionSize, false, {});\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"MatrixBandPart\")\n                                              .RandomInput(type)\n                                              .Input(num_lower)\n@@ -3330,12 +3334,12 @@ TEST_F(OpTest, MatrixDiagPartV3) {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     auto align = Choose<std::string>(\n         {\"LEFT_RIGHT\", \"RIGHT_LEFT\", \"LEFT_LEFT\", \"RIGHT_RIGHT\"});\n-    auto k0 = std::uniform_int_distribution<int32>(\n+    auto k0 = std::uniform_int_distribution<int32_t>(\n         -2 * kDefaultMaxDimensionSize,\n         2 * kDefaultMaxDimensionSize)(generator());\n-    auto k1 = std::uniform_int_distribution<int32>(\n+    auto k1 = std::uniform_int_distribution<int32_t>(\n         k0, 2 * kDefaultMaxDimensionSize)(generator());\n-    auto k = test::AsTensor<int32>({k0, k1});\n+    auto k = test::AsTensor<int32_t>({k0, k1});\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"MatrixDiagPartV3\")\n                                              .RandomInput(type)\n                                              .Input(k)\n@@ -3369,10 +3373,10 @@ TEST_F(OpTest, MatrixSetDiagV2) {\n     int64_t max_num_diags = shape[rank - 2] + shape[rank - 1] - 1;\n     int64_t num_diags =\n         std::uniform_int_distribution<int64_t>(2, max_num_diags)(generator());\n-    int32 k0 = std::uniform_int_distribution<int32>(\n+    int32_t k0 = std::uniform_int_distribution<int32_t>(\n         -shape[rank - 2] + 1, shape[rank - 1] - num_diags)(generator());\n-    int32 k1 = k0 + num_diags - 1;\n-    Tensor k = test::AsTensor<int32>({k0, k1});\n+    int32_t k1 = k0 + num_diags - 1;\n+    Tensor k = test::AsTensor<int32_t>({k0, k1});\n     int64_t max_diag_len = std::min(shape[rank - 2] + std::min(k1, 0),\n                                     shape[rank - 1] + std::min(-k0, 0));\n     std::vector<int64_t> diagonal_shape(shape);\n@@ -3424,7 +3428,7 @@ TEST_F(OpTest, MaxPool) {\n     int stride_rows = random_int(generator()),\n         stride_cols = random_int(generator());\n \n-    string padding = Choose<string>({\"SAME\", \"VALID\"});\n+    std::string padding = Choose<std::string>({\"SAME\", \"VALID\"});\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"MaxPool\")\n             .RandomInput(DT_FLOAT, dims)\n@@ -3458,7 +3462,7 @@ TEST_F(OpTest, MaxPool3D) {\n     int64_t batch = dims[3];\n     int64_t feature = dims[4];\n \n-    string padding = Choose<string>({\"SAME\", \"VALID\"});\n+    std::string padding = Choose<std::string>({\"SAME\", \"VALID\"});\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"MaxPool3D\")\n             .RandomInput(DT_FLOAT,\n@@ -3585,20 +3589,20 @@ TEST_F(OpTest, OneHot) {\n     int32_t depth = RandomDim();\n \n     Tensor indices(DT_INT32, TensorShape(dims));\n-    std::uniform_int_distribution<int32> distribution(-depth * 2, depth * 2);\n-    test::FillFn<int32>(&indices, [this, &distribution](int i) -> int32 {\n+    std::uniform_int_distribution<int32_t> distribution(-depth * 2, depth * 2);\n+    test::FillFn<int32_t>(&indices, [this, &distribution](int i) -> int32_t {\n       return distribution(generator());\n     });\n \n-    int axis = std::uniform_int_distribution<int32>(-num_dims - 5,\n-                                                    num_dims + 5)(generator());\n+    int axis = std::uniform_int_distribution<int32_t>(\n+        -num_dims - 5, num_dims + 5)(generator());\n \n     OpTestBuilder builder(\"OneHot\");\n     builder.Attr(\"T\", type);\n     builder.Attr(\"TI\", DT_INT32);\n     builder.Attr(\"axis\", axis);\n     builder.Input(indices);\n-    builder.Input(test::AsScalar<int32>(depth));\n+    builder.Input(test::AsScalar<int32_t>(depth));\n     builder.RandomInput(type, {});\n     builder.RandomInput(type, {});\n     return ExpectTfAndXlaOutputsAreClose(builder);\n@@ -3621,8 +3625,8 @@ TEST_F(OpTest, Pack) {\n \n     std::vector<int64_t> dims = RandomDims();\n     int num_dims = dims.size();\n-    int axis = std::uniform_int_distribution<int32>(-num_dims - 1,\n-                                                    num_dims)(generator());\n+    int axis = std::uniform_int_distribution<int32_t>(-num_dims - 1,\n+                                                      num_dims)(generator());\n \n     OpTestBuilder builder(\"Pack\");\n     builder.Attr(\"T\", type);\n@@ -3764,7 +3768,7 @@ TEST_F(OpTest, RandomUniform) {\n TEST_F(OpTest, Range) {\n   Repeatedly([this]() {\n     auto ToScalar = [](DataType type, int x) {\n-      if (type == DT_INT32) return test::AsScalar<int32>(x);\n+      if (type == DT_INT32) return test::AsScalar<int32_t>(x);\n       if (type == DT_INT64) return test::AsScalar<int64_t>(x);\n       if (type == DT_FLOAT) return test::AsScalar<float>(x);\n       if (type == DT_DOUBLE) return test::AsScalar<double>(x);\n@@ -3881,8 +3885,8 @@ TEST_F(OpTest, Reshape) {\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"Reshape\")\n             .RandomInput(type, dims_before)\n-            .Input(test::AsTensor<int32>(\n-                std::vector<int32>(dims_after.begin(), dims_after.end())))\n+            .Input(test::AsTensor<int32_t>(\n+                std::vector<int32_t>(dims_after.begin(), dims_after.end())))\n             .Attr(\"T\", type));\n   });\n }\n@@ -3908,8 +3912,8 @@ TEST_F(OpTest, ResizeBilinear) {\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"ResizeBilinear\")\n             .RandomInput(DT_FLOAT, in_dims)\n-            .Input(test::AsTensor<int32>(\n-                std::vector<int32>(out_dims.begin(), out_dims.end())))\n+            .Input(test::AsTensor<int32_t>(\n+                std::vector<int32_t>(out_dims.begin(), out_dims.end())))\n             .Attr(\"T\", DT_FLOAT)\n             .Attr(\"align_corners\", true));\n   });\n@@ -3961,14 +3965,14 @@ TEST_F(OpTest, ReverseSequence) {\n \n     int batch_size = dims[batch_dim];\n     int max_seq_len = dims[seq_dim];\n-    std::vector<int32> seq_lens(batch_size);\n-    std::uniform_int_distribution<int32> d(0, max_seq_len);\n+    std::vector<int32_t> seq_lens(batch_size);\n+    std::uniform_int_distribution<int32_t> d(0, max_seq_len);\n     absl::c_generate(seq_lens, [&]() { return d(generator()); });\n \n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"ReverseSequence\")\n             .RandomInput(type, dims)\n-            .Input(test::AsTensor<int32>(seq_lens))\n+            .Input(test::AsTensor<int32_t>(seq_lens))\n             .Attr(\"seq_dim\", seq_dim)\n             .Attr(\"batch_dim\", batch_dim)\n             .Attr(\"T\", type)\n@@ -4157,14 +4161,15 @@ TEST_F(OpTest, Size) {\n TEST_F(OpTest, Slice) {\n   Repeatedly([this]() {\n     SliceArguments a = ChooseSliceArguments(true);\n-    std::vector<int32> size;\n+    std::vector<int32_t> size;\n     size.insert(size.end(), a.size.begin(), a.size.end());\n-    return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"Slice\")\n-                                             .RandomInput(a.type, a.shape)\n-                                             .Input(a.indices)\n-                                             .Input(test::AsTensor<int32>(size))\n-                                             .Attr(\"T\", a.type)\n-                                             .Attr(\"Index\", a.indices_type));\n+    return ExpectTfAndXlaOutputsAreClose(\n+        OpTestBuilder(\"Slice\")\n+            .RandomInput(a.type, a.shape)\n+            .Input(a.indices)\n+            .Input(test::AsTensor<int32_t>(size))\n+            .Attr(\"T\", a.type)\n+            .Attr(\"Index\", a.indices_type));\n   });\n }\n \n@@ -4298,8 +4303,8 @@ TEST_F(OpTest, SpaceToBatchND) {\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"SpaceToBatchND\")\n             .RandomInput(type, input_dims)\n-            .Input(test::AsTensor<int32>(\n-                std::vector<int32>(block_dims.begin(), block_dims.end())))\n+            .Input(test::AsTensor<int32_t>(\n+                std::vector<int32_t>(block_dims.begin(), block_dims.end())))\n             .Input(paddings)\n             .Attr(\"T\", type));\n   });\n@@ -4356,16 +4361,16 @@ TEST_F(OpTest, SparseSoftmaxCrossEntropyWithLogits) {\n     int64_t batch_size = dims[0];\n     int64_t num_classes = dims[1];\n \n-    std::vector<int32> indices(batch_size);\n+    std::vector<int32_t> indices(batch_size);\n     for (int64_t i = 0; i < batch_size; ++i) {\n-      indices[i] =\n-          std::uniform_int_distribution<int32>(0, num_classes - 1)(generator());\n+      indices[i] = std::uniform_int_distribution<int32_t>(\n+          0, num_classes - 1)(generator());\n     }\n \n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"SparseSoftmaxCrossEntropyWithLogits\")\n             .RandomInput(DT_FLOAT, dims)\n-            .Input(test::AsTensor<int32>(indices))\n+            .Input(test::AsTensor<int32_t>(indices))\n             .Attr(\"T\", DT_FLOAT)\n             .Attr(\"Tlabels\", DT_INT32));\n   });\n@@ -4379,18 +4384,19 @@ TEST_F(OpTest, Split) {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     std::vector<int64_t> dims = RandomDims(1);\n     std::uniform_int_distribution<int> ud;\n-    int32_t dim = std::uniform_int_distribution<int32>(\n-        -static_cast<int32>(dims.size()),\n-        static_cast<int32>(dims.size()) - 1)(generator());\n+    int32_t dim = std::uniform_int_distribution<int32_t>(\n+        -static_cast<int32_t>(dims.size()),\n+        static_cast<int32_t>(dims.size()) - 1)(generator());\n     int n = std::uniform_int_distribution<int>(1, 5)(generator());\n     // Ensure 'dim' is evenly divisible by 'n'.\n     dims[dim] /= n;\n     dims[dim] *= n;\n-    return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"Split\")\n-                                             .Input(test::AsScalar<int32>(dim))\n-                                             .RandomInput(type, dims)\n-                                             .Attr(\"T\", type)\n-                                             .Attr(\"num_split\", n));\n+    return ExpectTfAndXlaOutputsAreClose(\n+        OpTestBuilder(\"Split\")\n+            .Input(test::AsScalar<int32_t>(dim))\n+            .RandomInput(type, dims)\n+            .Attr(\"T\", type)\n+            .Attr(\"num_split\", n));\n   });\n }\n \n@@ -4401,21 +4407,21 @@ TEST_F(OpTest, SplitV) {\n   Repeatedly([this]() {  // NOLINT: due to GTEST_SKIP\n     auto type = Choose<DataType>(kAllXlaTypes);\n     std::vector<int64_t> dims = RandomDims(1, kDefaultMaxRank, 1);\n-    int32_t dim = std::uniform_int_distribution<int32>(\n-        -static_cast<int32>(dims.size()),\n-        static_cast<int32>(dims.size()) - 1)(generator());\n+    int32_t dim = std::uniform_int_distribution<int32_t>(\n+        -static_cast<int32_t>(dims.size()),\n+        static_cast<int32_t>(dims.size()) - 1)(generator());\n     int n = std::uniform_int_distribution<int>(\n         1, std::min(5, static_cast<int>(dims[dim])))(generator());\n-    std::vector<int32> size_splits(n);\n+    std::vector<int32_t> size_splits(n);\n     for (int i = 0; i < n - 1; ++i) {\n       size_splits.push_back(dims[dim] / n);\n     }\n     size_splits.push_back(dims[dim] - (n - 1) * (dims[dim] / n));\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"SplitV\")\n             .RandomInput(type, dims)\n-            .Input(test::AsTensor<int32>(size_splits))\n-            .Input(test::AsScalar<int32>(dim))\n+            .Input(test::AsTensor<int32_t>(size_splits))\n+            .Input(test::AsScalar<int32_t>(dim))\n             .Attr(\"T\", type)\n             .Attr(\"num_split\", n)\n             .Attr(\"Tlen\", DT_INT32));\n@@ -4515,12 +4521,12 @@ TEST_F(OpTest, StridedSlice) {\n   Repeatedly([this]() {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     std::vector<int64_t> data_dims = RandomDims();\n-    std::vector<int32> begin(data_dims.size()), end(data_dims.size());\n-    std::vector<int32> strides(data_dims.size());\n+    std::vector<int32_t> begin(data_dims.size()), end(data_dims.size());\n+    std::vector<int32_t> strides(data_dims.size());\n     for (int i = 0; i < data_dims.size(); ++i) {\n-      begin[i] = std::uniform_int_distribution<int32>(\n+      begin[i] = std::uniform_int_distribution<int32_t>(\n           -2 * data_dims[i], 2 * data_dims[i])(generator());\n-      end[i] = std::uniform_int_distribution<int32>(\n+      end[i] = std::uniform_int_distribution<int32_t>(\n           -2 * data_dims[i], 2 * data_dims[i])(generator());\n       // TODO(b/31360685): support strides other than 1 or -1\n       strides[i] = std::bernoulli_distribution()(generator()) ? 1 : -1;\n@@ -4543,9 +4549,9 @@ TEST_F(OpTest, StridedSlice) {\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"StridedSlice\")\n             .RandomInput(type, data_dims)\n-            .Input(test::AsTensor<int32>(begin))\n-            .Input(test::AsTensor<int32>(end))\n-            .Input(test::AsTensor<int32>(strides))\n+            .Input(test::AsTensor<int32_t>(begin))\n+            .Input(test::AsTensor<int32_t>(end))\n+            .Input(test::AsTensor<int32_t>(strides))\n             .Attr(\"T\", type)\n             .Attr(\"Index\", DT_INT32)\n             .Attr(\"begin_mask\", begin_mask)\n@@ -4656,14 +4662,14 @@ TEST_F(OpTest, Tile) {\n   Repeatedly([this]() {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     std::vector<int64_t> t_dims = RandomDims(1);\n-    std::vector<int32> multiples(t_dims.size());\n+    std::vector<int32_t> multiples(t_dims.size());\n     for (int i = 0; i < t_dims.size(); ++i) {\n       multiples[i] = std::uniform_int_distribution<int>(1, 3)(generator());\n     }\n     return ExpectTfAndXlaOutputsAreClose(\n         OpTestBuilder(\"Tile\")\n             .RandomInput(type, t_dims)\n-            .Input(test::AsTensor<int32>(multiples))\n+            .Input(test::AsTensor<int32_t>(multiples))\n             .Attr(\"T\", type));\n   });\n }\n@@ -4674,10 +4680,11 @@ TEST_F(OpTest, TopKV2) {\n   Repeatedly([this]() {  // NOLINT: due to GTEST_SKIP\n     auto type = Choose<DataType>({DT_INT32, DT_FLOAT, DT_INT64});\n     auto shape = RandomDims(1);\n-    int32 k = std::uniform_int_distribution<int32>(1, shape[0])(generator());\n+    int32_t k =\n+        std::uniform_int_distribution<int32_t>(1, shape[0])(generator());\n     return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"TopKV2\")\n                                              .RandomInput(type, shape)\n-                                             .Input(test::AsScalar<int32>(k))\n+                                             .Input(test::AsScalar<int32_t>(k))\n                                              .Attr(\"sorted\", RandomBool())\n                                              .Attr(\"T\", type));\n   });\n@@ -4687,13 +4694,14 @@ TEST_F(OpTest, Transpose) {\n   Repeatedly([this]() {\n     auto type = Choose<DataType>(kAllXlaTypes);\n     std::vector<int64_t> data_dims = RandomDims();\n-    std::vector<int32> perm(data_dims.size());\n+    std::vector<int32_t> perm(data_dims.size());\n     std::iota(perm.begin(), perm.end(), 0);\n     std::shuffle(perm.begin(), perm.end(), generator());\n-    return ExpectTfAndXlaOutputsAreClose(OpTestBuilder(\"Transpose\")\n-                                             .RandomInput(type, data_dims)\n-                                             .Input(test::AsTensor<int32>(perm))\n-                                             .Attr(\"T\", type));\n+    return ExpectTfAndXlaOutputsAreClose(\n+        OpTestBuilder(\"Transpose\")\n+            .RandomInput(type, data_dims)\n+            .Input(test::AsTensor<int32_t>(perm))\n+            .Attr(\"T\", type));\n   });\n }\n \n@@ -4883,8 +4891,8 @@ TEST_F(OpTest, FusedBatchNormTraining) {\n }  // namespace tensorflow\n \n int main(int argc, char** argv) {\n-  tensorflow::tf_xla_test_device_ptr = new tensorflow::string(\"GPU:0\");\n-  tensorflow::tf_xla_reference_device_ptr = new tensorflow::string(\"CPU:0\");\n+  tensorflow::tf_xla_test_device_ptr = new std::string(\"GPU:0\");\n+  tensorflow::tf_xla_reference_device_ptr = new std::string(\"CPU:0\");\n   std::vector<tensorflow::Flag> flag_list = {\n       tensorflow::Flag(\n           \"tf_xla_random_seed\", &tensorflow::tf_xla_random_seed,\n@@ -4909,7 +4917,7 @@ int main(int argc, char** argv) {\n           \"tf_xla_test_use_mlir\", &tensorflow::tf_xla_test_use_mlir,\n           \"Use MLIR legalization kernels for the operator under test\"),\n   };\n-  tensorflow::string usage = tensorflow::Flags::Usage(argv[0], flag_list);\n+  std::string usage = tensorflow::Flags::Usage(argv[0], flag_list);\n   const bool parse_result = tensorflow::Flags::Parse(&argc, argv, flag_list);\n   if (!parse_result) {\n     LOG(ERROR) << \"\\n\" << usage;"
        },
        {
            "sha": "c27b8070bbb45019636d4aa0527bd2ce5a5ca640",
            "filename": "tensorflow/compiler/tests/unary_ops_composition_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/89fab6cb5d6bc2a053bc89d81d1b268eb9226f82/tensorflow%2Fcompiler%2Ftests%2Funary_ops_composition_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/89fab6cb5d6bc2a053bc89d81d1b268eb9226f82/tensorflow%2Fcompiler%2Ftests%2Funary_ops_composition_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftests%2Funary_ops_composition_test.cc?ref=89fab6cb5d6bc2a053bc89d81d1b268eb9226f82",
            "patch": "@@ -48,9 +48,9 @@ static bool Initialized = [] {\n class UnaryOpsCompositionTest : public OpsTestBase {\n  protected:\n   template <typename T>\n-  void RunComposedOp(const std::vector<string> op_names, T input_scalar_value,\n-                     T expected_scalar_value) {\n-    string xla_device_name =\n+  void RunComposedOp(const std::vector<std::string> op_names,\n+                     T input_scalar_value, T expected_scalar_value) {\n+    std::string xla_device_name =\n         tensorflow::IsGoogleCudaEnabled() ? DEVICE_XLA_GPU : DEVICE_XLA_CPU;\n     SetDevice(DeviceType(xla_device_name),\n               std::unique_ptr<tensorflow::Device>(DeviceFactory::NewDevice("
        }
    ],
    "stats": {
        "total": 378,
        "additions": 193,
        "deletions": 185
    }
}