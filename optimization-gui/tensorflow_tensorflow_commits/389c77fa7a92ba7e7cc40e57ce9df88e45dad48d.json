{
    "author": "jpienaar",
    "message": "Enabling indexing analysis directly on StableHLO.\n\nThis is adding an additional helper that can operate directly on StableHLO. It tries to reuse/extract indexing utils as much as possible excluding where the cost of reuse negates value.\n\nOnly one direction of support is implemented as size was rather large already. The support for runtime variables ended up being a bit more invasive.\n\nPiperOrigin-RevId: 846390948",
    "sha": "389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
    "files": [
        {
            "sha": "2684e6378f55459dc02f013bb4eb95827702cdcf",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -1147,7 +1147,7 @@ ComposeIndexingResult ComposeInstructionIndexing(\n     IndexingMap rt_map =\n         ComposeIndexingMaps(tiled_hlo_instruction->indexing_map(), rt_var.map);\n     HloInstructionAdaptor hlo_adaptor =\n-        instruction_adaptor.parent().GetInstruction(rt_var.hlo);\n+        instruction_adaptor.parent().GetInstruction(rt_var.hlo());\n     auto tiled_runtime_var = std::make_unique<SymbolicTiledHloInstruction>(\n         &hlo_adaptor.instruction(), rt_map,\n         tiled_hlo_instruction->runtime_variables());"
        },
        {
            "sha": "1494f6de2d76e8d623f21b1430f0185bc2726d15",
            "filename": "third_party/xla/xla/hlo/analysis/BUILD",
            "status": "modified",
            "additions": 29,
            "deletions": 3,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -594,11 +594,13 @@ cc_library(\n     name = \"indexing_analysis\",\n     srcs = [\n         \"indexing_analysis.cc\",\n+        \"indexing_analysis_utils.cc\",\n         \"indexing_map.cc\",\n         \"indexing_map_serialization.cc\",\n     ],\n     hdrs = [\n         \"indexing_analysis.h\",\n+        \"indexing_analysis_utils.h\",\n         \"indexing_map.h\",\n         \"indexing_map_serialization.h\",\n     ],\n@@ -626,6 +628,7 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Support\",\n         \"@local_tsl//tsl/platform:logging\",\n+        \"@stablehlo//:stablehlo_ops\",\n     ],\n )\n \n@@ -644,8 +647,6 @@ xla_cc_test(\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//mlir:IR\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n \n@@ -698,12 +699,18 @@ xla_cc_test(\n     deps = [\n         \":indexing_analysis\",\n         \":indexing_test_utils\",\n+        \":stablehlo_indexing_analysis\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/translate:stablehlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/tests:xla_internal_test_main\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\",\n-        \"@local_tsl//tsl/platform:test\",\n+        \"@llvm-project//mlir:FuncDialect\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@stablehlo//:stablehlo_ops\",\n     ],\n )\n \n@@ -796,3 +803,22 @@ xla_cc_test(\n         \"@llvm-project//mlir:Support\",\n     ],\n )\n+\n+cc_library(\n+    name = \"stablehlo_indexing_analysis\",\n+    srcs = [\"stablehlo_indexing_analysis.cc\"],\n+    hdrs = [\"stablehlo_indexing_analysis.h\"],\n+    deps = [\n+        \":indexing_analysis\",\n+        \":interval\",\n+        \"//xla:permutation_util\",\n+        \"//xla:shape_util\",\n+        \"//xla/mlir_hlo\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@stablehlo//:stablehlo_ops\",\n+    ],\n+)"
        },
        {
            "sha": "7ce3f66bd2c36b08f9afb636d935ea90d28a8e86",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.cc",
            "status": "modified",
            "additions": 234,
            "deletions": 311,
            "changes": 545,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -42,8 +42,13 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/Operation.h\"\n #include \"mlir/Support/LLVM.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n+#include \"xla/hlo/analysis/indexing_analysis_utils.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/analysis/indexing_map_serialization.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -93,7 +98,7 @@ HloInstructionIndexing CreateUnknownIndexing(int64_t count = 1) {\n //   into `idx`.\n struct HLORTVar {\n   Interval feasible_values;\n-  const HloInstruction* hlo;\n+  InstructionRef hlo;\n   mlir::AffineMap map;\n   DimensionVector dim_upper_bounds;\n };\n@@ -107,33 +112,80 @@ inline bool operator!=(const HLORTVar& lhs, const HLORTVar& rhs) {\n   return !(lhs == rhs);\n }\n \n-// Optimizes runtime variable if it's possible to replace it with a constant.\n-//\n-// Note: we had a more complex logic here that handled more instruction types\n-// but was removed due to previous version not updating value ranges\n-// (b/419279949).\n-std::optional<AffineExpr> OptimizeRTVar(HLORTVar rt_var,\n-                                        MLIRContext* mlir_context) {\n-  if (auto constant_expr = DynCast<HloConstantInstruction>(rt_var.hlo)) {\n-    if (rt_var.map.isConstant()) {\n-      const auto idx = rt_var.map.getConstantResults();\n+std::optional<int64_t> GetIntOrSplatIntValue(mlir::Attribute attr) {\n+  if (auto int_attr = mlir::dyn_cast<mlir::IntegerAttr>(attr)) {\n+    return int_attr.getInt();\n+  }\n+  if (auto splat = mlir::dyn_cast<mlir::SplatElementsAttr>(attr)) {\n+    if (auto element_attr = mlir::dyn_cast_or_null<mlir::IntegerAttr>(\n+            splat.getSplatValue<mlir::Attribute>())) {\n+      return element_attr.getInt();\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n+}  // namespace\n+\n+std::optional<AffineExpr> OptimizeHloRTVar(const HloInstruction* hlo,\n+                                           const RuntimeVarIndexing& rt_var,\n+                                           const Interval& feasible_values,\n+                                           MLIRContext* mlir_context) {\n+  if (auto constant_expr = DynCast<HloConstantInstruction>(hlo)) {\n+    if (rt_var.map.GetAffineMap().isConstant()) {\n+      const auto idx = rt_var.map.GetAffineMap().getConstantResults();\n       auto const_value = constant_expr->literal().GetIntegralAsS64(idx).value();\n-      if (!rt_var.feasible_values.Contains(const_value)) {\n-        // Constant is outside of the feasible values, keep the symbol to let\n-        // the runtime to handle that.\n+      if (!feasible_values.Contains(const_value)) {\n         return std::nullopt;\n       }\n       return getAffineConstantExpr(const_value, mlir_context);\n     }\n   }\n-  if (auto iota_expr = DynCast<HloIotaInstruction>(rt_var.hlo)) {\n+  if (auto iota_expr = DynCast<HloIotaInstruction>(hlo)) {\n     auto iota_dimension = iota_expr->iota_dimension();\n-    CHECK(iota_dimension < rt_var.map.getNumResults());\n-    return rt_var.map.getResults()[iota_dimension];\n+    CHECK(iota_dimension < rt_var.map.GetAffineMap().getNumResults());\n+    return rt_var.map.GetAffineMap().getResults()[iota_dimension];\n+  }\n+  return std::nullopt;\n+}\n+\n+std::optional<AffineExpr> OptimizeMlirRTVar(mlir::Operation* op,\n+                                            const RuntimeVarIndexing& rt_var,\n+                                            const Interval& feasible_values,\n+                                            MLIRContext* mlir_context) {\n+  mlir::Attribute attr;\n+  if (mlir::matchPattern(op, mlir::m_Constant(&attr))) {\n+    auto int_val = GetIntOrSplatIntValue(attr);\n+    if (int_val.has_value()) {\n+      if (!feasible_values.Contains(*int_val)) {\n+        return std::nullopt;\n+      }\n+      return getAffineConstantExpr(*int_val, mlir_context);\n+    }\n+  }\n+  if (auto iota_op = llvm::dyn_cast<mlir::stablehlo::IotaOp>(op)) {\n+    int64_t iota_dim = iota_op.getIotaDimension();\n+    if (iota_dim < rt_var.map.GetAffineMap().getNumResults()) {\n+      return rt_var.map.GetAffineMap().getResults()[iota_dim];\n+    }\n   }\n   return std::nullopt;\n }\n \n+std::optional<AffineExpr> OptimizeRTVar(const RuntimeVarIndexing& rt_var,\n+                                        const Interval& feasible_values,\n+                                        MLIRContext* mlir_context) {\n+  if (const HloInstruction* hlo = rt_var.hlo()) {\n+    return OptimizeHloRTVar(hlo, rt_var, feasible_values, mlir_context);\n+  }\n+  if (auto* op = rt_var.mlir_op()) {\n+    return OptimizeMlirRTVar(op, rt_var, feasible_values, mlir_context);\n+  }\n+  return std::nullopt;\n+}\n+\n+namespace {\n+\n std::vector<IndexingMap::Variable> ConvertHLORTVarsToRTVars(\n     const std::vector<HLORTVar>& hlo_rt_vars) {\n   std::vector<IndexingMap::Variable> rt_vars;\n@@ -154,7 +206,11 @@ IndexingMap FoldRTVarsAndConstructIndexingMap(\n   CHECK_EQ(affine_map.getNumSymbols(), hlo_rt_vars.size());\n   for (auto idx = 0; idx < affine_map.getNumSymbols(); ++idx) {\n     auto& rt_var = hlo_rt_vars[idx];\n-    std::optional<AffineExpr> result = OptimizeRTVar(rt_var, mlir_context);\n+    std::optional<AffineExpr> result = OptimizeRTVar(\n+        RuntimeVarIndexing{rt_var.hlo, IndexingMap::FromTensorSizes(\n+                                           rt_var.map, rt_var.dim_upper_bounds,\n+                                           /*symbol_upper_bounds=*/{})},\n+        rt_var.feasible_values, mlir_context);\n     if (!result) {\n       continue;\n     }\n@@ -183,29 +239,24 @@ OperandIndexing CreateOperandIndexingWithRTVars(\n   IndexingMap update_map_ops = FoldRTVarsAndConstructIndexingMap(\n       operand_map, dim_vars, std::move(rt_vars));\n \n-  return OperandIndexing(update_map_ops, rt_indexing);\n+  OperandIndexing indexing(update_map_ops, rt_indexing);\n+  indexing.RemoveUnusedSymbols();\n+  return indexing;\n }\n \n HloInstructionIndexing ComputeOutputToInputCwiseOpIndexing(\n     const HloInstruction* instr, MLIRContext* mlir_context) {\n-  IndexingMap identity_map = CreateIdentityMap(instr->shape(), mlir_context);\n-  IndexingMap unit_map(\n-      mlir::AffineMap::get(identity_map.GetAffineMap().getNumDims(),\n-                           /*symbolCount=*/0, mlir_context),\n-      identity_map.GetDimVars(), /*range_vars=*/{}, /*rt_vars=*/{});\n-\n-  HloInstructionIndexing instr_indexing;\n-  instr_indexing.indexing_maps.resize(instr->operand_count());\n-  int64_t operand_count = instr->operand_count();\n-  for (int64_t operand_id = 0; operand_id < operand_count; ++operand_id) {\n+  HloInstructionIndexing instr_indexing = CreateElementwiseIndexing(\n+      instr->operand_count(), instr->shape(), mlir_context);\n+  for (int64_t operand_id = 0; operand_id < instr->operand_count();\n+       ++operand_id) {\n     // Select allows implicit broadcasting in the predicate. We just handle it\n     // generically here.\n-    auto* operand = instr->operand(operand_id);\n-    if (operand->shape().dimensions().size() == 0 &&\n-        instr->shape().dimensions().size() > 0) {\n-      instr_indexing.indexing_maps[operand_id].emplace(unit_map);\n-    } else {\n-      instr_indexing.indexing_maps[operand_id].emplace(identity_map);\n+    if (instr->operand(operand_id)->shape().dimensions().empty() &&\n+        !instr->shape().dimensions().empty()) {\n+      instr_indexing.indexing_maps[operand_id].clear();\n+      instr_indexing.indexing_maps[operand_id].emplace(\n+          CreateScalarIndexingMap(instr->shape(), mlir_context));\n     }\n   }\n   return instr_indexing;\n@@ -217,19 +268,14 @@ HloInstructionIndexing ComputeInputToOutputCwiseOpIndexing(\n   return HloInstructionIndexing::FromIndexingMaps({identity_map});\n }\n \n+}  // namespace\n+\n+namespace {\n+\n HloInstructionIndexing ComputeOutputToInputBroadcastOpIndexing(\n     const HloBroadcastInstruction* bcast, MLIRContext* mlir_context) {\n-  auto output_dims = bcast->shape().dimensions();\n-\n-  std::vector<AffineExpr> exprs;\n-  exprs.reserve(bcast->dimensions().size());\n-  for (int64_t bcast_dim : bcast->dimensions()) {\n-    exprs.push_back(getAffineDimExpr(bcast_dim, mlir_context));\n-  }\n-  IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_dims.size(), /*symbolCount=*/0, exprs,\n-                     mlir_context),\n-      output_dims, {});\n+  IndexingMap indexing_map = ComputeBroadcastIndexingMap(\n+      bcast->shape().dimensions(), bcast->dimensions(), mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps({indexing_map});\n }\n \n@@ -266,31 +312,15 @@ HloInstructionIndexing ComputeInputToOutputBroadcastOpIndexing(\n \n HloInstructionIndexing ComputeOutputToInputConcatenateOpIndexing(\n     const HloConcatenateInstruction* concat, MLIRContext* mlir_context) {\n-  const auto& operand_0_dims = concat->operand(0)->shape().dimensions();\n-\n-  // Initialize affine map and domain. Only concat_dim elements of both have to\n-  // be adjusted for a particular operand_id.\n-  mlir::MutableAffineMap affine_map =\n-      AffineMap::getMultiDimIdentityMap(operand_0_dims.size(), mlir_context);\n-  std::vector<IndexingMap::Variable> dim_vars =\n-      DimVarsFromTensorSizes(operand_0_dims);\n-\n-  HloInstructionIndexing concat_indexing;\n-  concat_indexing.indexing_maps.resize(concat->operand_count());\n   int64_t concat_dim = concat->concatenate_dimension();\n-  AffineExpr concat_dim_expr = getAffineDimExpr(concat_dim, mlir_context);\n-  int64_t offset = 0;\n-  for (const auto [operand_id, operand] : llvm::enumerate(concat->operands())) {\n-    affine_map.setResult(concat_dim, concat_dim_expr - offset);\n-    int64_t operand_concat_dim = operand->shape().dimensions()[concat_dim];\n-    dim_vars[concat_dim] =\n-        IndexingMap::Variable{{offset, offset + operand_concat_dim - 1}};\n-    concat_indexing.indexing_maps[operand_id].insert(\n-        OperandIndexing(IndexingMap(affine_map.getAffineMap(), dim_vars,\n-                                    /*range_vars=*/{}, /*rt_vars=*/{})));\n-    offset += operand_concat_dim;\n+  std::vector<int64_t> operand_concat_dim_sizes;\n+  operand_concat_dim_sizes.reserve(concat->operand_count());\n+  for (const auto* operand : concat->operands()) {\n+    operand_concat_dim_sizes.push_back(operand->shape().dimensions(concat_dim));\n   }\n-  return concat_indexing;\n+  return ComputeConcatenateIndexing(concat->shape().dimensions().size(),\n+                                    concat_dim, concat->shape().dimensions(),\n+                                    operand_concat_dim_sizes, mlir_context);\n }\n \n HloInstructionIndexing ComputeInputToOutputConcatenateOpIndexing(\n@@ -335,74 +365,11 @@ HloInstructionIndexing ComputeOutputToInputFusionOpIndexing(\n std::pair<IndexingMap, IndexingMap> ComputeDotOperandsIndexingImpl(\n     const Shape& lhs_shape, const Shape& rhs_shape, const Shape& output_shape,\n     const DotDimensionNumbers& dim_numbers, MLIRContext* mlir_context) {\n-  absl::Span<const int64_t> lhs_contracting_dims(\n-      dim_numbers.lhs_contracting_dimensions());\n-  absl::Span<const int64_t> rhs_contracting_dims =\n-      dim_numbers.rhs_contracting_dimensions();\n-\n-  absl::Span<const int64_t> lhs_batch_dims = dim_numbers.lhs_batch_dimensions();\n-  absl::Span<const int64_t> rhs_batch_dims = dim_numbers.rhs_batch_dimensions();\n-\n-  // According to the StableHLO specification, the dimensions of the output\n-  // shape are ordered as follows:\n-  //   lhs_batch_dims | lhs_non_contracting_dims | rhs_non_contracting_dims\n-  SmallVector<AffineExpr> lhs_exprs(lhs_shape.dimensions().size());\n-  SmallVector<AffineExpr> rhs_exprs(rhs_shape.dimensions().size());\n-  int64_t output_dim_id = 0;\n-\n-  // lhs_batch_dims\n-  for (auto [lhs_batch_dim, rhs_batch_dim] :\n-       llvm::zip(lhs_batch_dims, rhs_batch_dims)) {\n-    AffineExpr output_dim_expr = getAffineDimExpr(output_dim_id, mlir_context);\n-    lhs_exprs[lhs_batch_dim] = output_dim_expr;\n-    rhs_exprs[rhs_batch_dim] = output_dim_expr;\n-    ++output_dim_id;\n-  }\n-\n-  // lhs_non_contracting_dims\n-  auto lhs_non_contracting_dims =\n-      GetNonContractingDims(lhs_shape, lhs_batch_dims, lhs_contracting_dims);\n-  assert(lhs_non_contracting_dims.ok());\n-\n-  for (int64_t lhs_non_contracting_dim : lhs_non_contracting_dims.value()) {\n-    lhs_exprs[lhs_non_contracting_dim] =\n-        getAffineDimExpr(output_dim_id++, mlir_context);\n-  }\n-\n-  // rhs_non_contracting_dims\n-  auto rhs_non_contracting_dims =\n-      GetNonContractingDims(rhs_shape, rhs_batch_dims, rhs_contracting_dims);\n-  assert(rhs_non_contracting_dims.ok());\n-  for (int64_t rhs_non_contracting_dim : rhs_non_contracting_dims.value()) {\n-    rhs_exprs[rhs_non_contracting_dim] =\n-        getAffineDimExpr(output_dim_id++, mlir_context);\n-  }\n-\n-  int64_t input_dim_id = 0;\n-  std::vector<int64_t> input_dim_sizes;\n-  input_dim_sizes.reserve(lhs_contracting_dims.size());\n-\n-  for (auto [lhs_contracting_dim, rhs_contracting_dim] :\n-       llvm::zip(lhs_contracting_dims, rhs_contracting_dims)) {\n-    AffineExpr input_dim_expr = getAffineSymbolExpr(input_dim_id, mlir_context);\n-    lhs_exprs[lhs_contracting_dim] = input_dim_expr;\n-    rhs_exprs[rhs_contracting_dim] = input_dim_expr;\n-    ++input_dim_id;\n-\n-    // LHS and RHS contracting dimensions must match pairwise, and we therefore\n-    // need only populate a single input_dim_sizes vector.\n-    input_dim_sizes.push_back(lhs_shape.dimensions(lhs_contracting_dim));\n-  }\n-\n-  int64_t output_rank = output_shape.dimensions().size();\n-  return std::make_pair(IndexingMap::FromTensorSizes(\n-                            AffineMap::get(output_rank, input_dim_sizes.size(),\n-                                           lhs_exprs, mlir_context),\n-                            output_shape.dimensions(), input_dim_sizes),\n-                        IndexingMap::FromTensorSizes(\n-                            AffineMap::get(output_rank, input_dim_sizes.size(),\n-                                           rhs_exprs, mlir_context),\n-                            output_shape.dimensions(), input_dim_sizes));\n+  return ComputeDotOperandsIndexing(\n+      lhs_shape.dimensions(), rhs_shape.dimensions(), output_shape.dimensions(),\n+      dim_numbers.lhs_batch_dimensions(), dim_numbers.rhs_batch_dimensions(),\n+      dim_numbers.lhs_contracting_dimensions(),\n+      dim_numbers.rhs_contracting_dimensions(), mlir_context);\n }\n \n // Returns the new map with the results scaled by (operand_shape / scale_shape).\n@@ -472,29 +439,45 @@ HloInstructionIndexing ComputeOutputToInputDynamicSliceOpIndexing(\n       << \"b/118437727: Old form, not supported.\";\n   // A map from tensor iteration space to (), because index operands are 0d\n   // tensors.\n+  IndexingMap start_indices_map =\n+      CreateScalarIndexingMap(output_shape, mlir_context);\n+\n   AffineMap empty_results_affine_map = AffineMap::get(\n       /*dimCount=*/rank, /*symbolCount=*/0, /*results=*/{}, mlir_context);\n-  IndexingMap start_indices_map = IndexingMap::FromTensorSizes(\n-      empty_results_affine_map, output_shape.dimensions(), {});\n-\n   std::vector<HLORTVar> offsets_rt_vars;\n   offsets_rt_vars.reserve(rank);\n   std::vector<AffineExpr> exprs;\n   exprs.reserve(rank);\n+\n   for (auto [dim, slice_size] :\n        llvm::enumerate(dynamic_slice->dynamic_slice_sizes())) {\n-    exprs.push_back(getAffineDimExpr(dim, mlir_context) +\n-                    getAffineSymbolExpr(dim, mlir_context));\n-    offsets_rt_vars.push_back(HLORTVar{\n-        Interval{0, input_shape.dimensions(dim) - slice_size},\n-        dynamic_slice->operand(dim + first_index_num), empty_results_affine_map,\n-        ShapeUtil::CreateDimensionVectorFromShape(output_shape)});\n+    AffineExpr dim_expr = getAffineDimExpr(dim, mlir_context);\n+    const HloInstruction* offset_op =\n+        dynamic_slice->operand(dim + first_index_num);\n+    int64_t max_index = input_shape.dimensions(dim) - slice_size;\n+\n+    // Construct temp objects for optimization\n+    RuntimeVarIndexing rt_indexing{offset_op, start_indices_map};\n+    Interval feasible_values{0, max_index};\n+\n+    auto simplified_expr =\n+        OptimizeRTVar(rt_indexing, feasible_values, mlir_context);\n+    if (simplified_expr) {\n+      exprs.push_back(dim_expr + *simplified_expr);\n+    } else {\n+      exprs.push_back(\n+          dim_expr + getAffineSymbolExpr(offsets_rt_vars.size(), mlir_context));\n+      offsets_rt_vars.push_back(\n+          HLORTVar{feasible_values, offset_op, empty_results_affine_map,\n+                   ShapeUtil::CreateDimensionVectorFromShape(output_shape)});\n+    }\n   }\n   std::vector<OperandIndexing> indexing_maps(\n       dynamic_slice->operand_count(), OperandIndexing(start_indices_map));\n \n+  int symbol_count = offsets_rt_vars.size();\n   indexing_maps[0] = CreateOperandIndexingWithRTVars(\n-      AffineMap::get(/*dimCount=*/rank, /*symbolCount=*/rank, exprs,\n+      AffineMap::get(/*dimCount=*/rank, /*symbolCount=*/symbol_count, exprs,\n                      mlir_context),\n       start_indices_map.GetDimVars(), std::move(offsets_rt_vars));\n   HloInstructionIndexing result =\n@@ -520,11 +503,11 @@ HloInstructionIndexing ComputeOutputToInputDynamicUpdateSliceOpIndexing(\n       output_shape.dimensions(), {});\n \n   // start_indices: (d0, ... d{N-1}) -> ()\n+  IndexingMap start_indices_map =\n+      CreateScalarIndexingMap(output_shape, mlir_context);\n+\n   AffineMap empty_results_affine_map = AffineMap::get(\n       /*dimCount=*/rank, /*symbolCount=*/0, /*results=*/{}, mlir_context);\n-  IndexingMap start_indices_map = IndexingMap::FromTensorSizes(\n-      empty_results_affine_map, output_shape.dimensions(), {});\n-\n   // update: (d0 - rt0, ..., d{N-1} - rt{N-1})\n   std::vector<AffineExpr> exprs;\n   exprs.reserve(rank);\n@@ -618,40 +601,9 @@ HloInstructionIndexing ComputeOutputToInputGatherOpIndexing(\n       {operand_indexing, OperandIndexing(indices_map)});\n }\n \n-IndexingMap ComputeOutputToInputPadOpIndexingImpl(\n-    absl::Span<const int64_t> output_dims,\n-    absl::Span<const int64_t> padding_low,\n-    absl::Span<const int64_t> padding_high,\n-    absl::Span<const int64_t> padding_interior, MLIRContext* mlir_context) {\n-  int64_t output_rank = output_dims.size();\n+}  // namespace\n \n-  std::vector<AffineExpr> exprs;\n-  std::vector<std::pair<AffineExpr, Interval>> constraints;\n-  std::vector<IndexingMap::Variable> dim_vars;\n-  exprs.reserve(output_rank);\n-  constraints.reserve(output_rank);\n-  int64_t output_dim_id = 0;\n-  for (const auto [output_dim, pad_low, pad_high, pad_interior] :\n-       llvm::zip(output_dims, padding_low, padding_high, padding_interior)) {\n-    AffineExpr dim_expr = getAffineDimExpr(output_dim_id, mlir_context);\n-    dim_vars.push_back({IndexingMap::Variable{\n-        std::max(int64_t{0}, pad_low),\n-        std::min(output_dim - 1, output_dim - 1 - pad_high)}});\n-    if (pad_interior == 0) {\n-      exprs.push_back(dim_expr - pad_low);\n-    } else {\n-      exprs.push_back((dim_expr - pad_low).floorDiv(pad_interior + 1));\n-      constraints.push_back(\n-          {(dim_expr - pad_low) % (pad_interior + 1), Interval{0, 0}});\n-    }\n-    ++output_dim_id;\n-  }\n-  return IndexingMap{\n-      AffineMap::get(output_rank, /*symbolCount=*/0, exprs, mlir_context),\n-      std::move(dim_vars),\n-      /*range_vars = */ {},\n-      /*rt_vars = */ {}, absl::MakeSpan(constraints)};\n-}\n+namespace {\n \n HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n     const HloPadInstruction* pad, MLIRContext* mlir_context) {\n@@ -666,47 +618,25 @@ HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n     padding_high.push_back(dim_config.edge_padding_high());\n     padding_interior.push_back(dim_config.interior_padding());\n   }\n-  IndexingMap input_indexing_map = ComputeOutputToInputPadOpIndexingImpl(\n-      output_shape.dimensions(), padding_low, padding_high, padding_interior,\n-      mlir_context);\n-  IndexingMap padding_value_indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_shape.dimensions().size(), /*symbolCount=*/0, {},\n-                     mlir_context),\n-      output_shape.dimensions(), /*symbol_upper_bounds=*/{});\n+  IndexingMap input_indexing_map =\n+      ComputePadIndexingMap(output_shape.dimensions(), padding_low,\n+                            padding_high, padding_interior, mlir_context);\n+  IndexingMap padding_value_indexing_map =\n+      CreateScalarIndexingMap(output_shape, mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps(\n       {input_indexing_map, padding_value_indexing_map});\n }\n \n HloInstructionIndexing ComputeOutputToInputReduceOpIndexing(\n     const HloReduceInstruction* reduce, MLIRContext* mlir_context) {\n-  absl::flat_hash_set<int64_t> reduce_dims_ids(reduce->dimensions().begin(),\n-                                               reduce->dimensions().end());\n-\n   const Shape& input_shape = reduce->operand(0)->shape();\n   const Shape& output_shape = GetOutputShape(reduce, 0);\n \n-  std::vector<int64_t> parallel_dims_sizes;\n-  int64_t output_dim_id = 0;\n-  std::vector<AffineExpr> exprs;\n-  exprs.reserve(input_shape.dimensions().size());\n-  for (auto [input_dim_id, input_dim] :\n-       llvm::enumerate(input_shape.dimensions())) {\n-    if (reduce_dims_ids.contains(input_dim_id)) {\n-      exprs.push_back(\n-          getAffineSymbolExpr(parallel_dims_sizes.size(), mlir_context));\n-      parallel_dims_sizes.push_back(input_dim);\n-      continue;\n-    }\n-    exprs.push_back(getAffineDimExpr(output_dim_id++, mlir_context));\n-  }\n-  IndexingMap inputs_indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_shape.dimensions().size(), reduce_dims_ids.size(),\n-                     exprs, mlir_context),\n-      output_shape.dimensions(), parallel_dims_sizes);\n-  IndexingMap inits_indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_shape.dimensions().size(), /*symbolCount=*/0, {},\n-                     mlir_context),\n-      output_shape.dimensions(), {});\n+  IndexingMap inputs_indexing_map = ComputeReduceInputIndexingMap(\n+      input_shape.dimensions(), output_shape.dimensions(), reduce->dimensions(),\n+      mlir_context);\n+  IndexingMap inits_indexing_map =\n+      CreateScalarIndexingMap(output_shape, mlir_context);\n \n   HloInstructionIndexing instr_indexing;\n   instr_indexing.indexing_maps.resize(reduce->operand_count());\n@@ -775,56 +705,24 @@ IndexingMap ComposeIndexingMapsForWindow(\n     absl::Span<const int64_t> output_dimensions, const Window& window,\n     MLIRContext* mlir_context) {\n   size_t rank = input_dimensions.size();\n-\n-  // Compute shape of the padded input and the indexing map of pad op required\n-  // to pad the input.\n-  SmallVector<int64_t> padding_low, padding_high, padding_interior,\n-      padded_input_dimensions;\n-  padding_low.reserve(rank);\n-  padding_high.reserve(rank);\n-  padding_interior.reserve(rank);\n-  padded_input_dimensions.reserve(rank);\n-  SmallVector<AffineExpr, 4> exprs;\n-  std::vector<IndexingMap::Variable> dim_vars;\n-  std::vector<IndexingMap::Variable> range_vars;\n-  exprs.reserve(rank);\n-  dim_vars.reserve(rank);\n-  range_vars.reserve(rank);\n-  for (const auto& [dim_id, window_config] :\n-       llvm::enumerate(window.dimensions())) {\n-    padding_low.push_back(window_config.padding_low());\n-    padding_high.push_back(window_config.padding_high());\n-    // For some reason interior_padding in HLO pad is offset from base_dilations\n-    // in HLO reduce-window by 1.\n-    padding_interior.push_back(window_config.base_dilation() - 1);\n-    padded_input_dimensions.push_back(\n-        input_dimensions[dim_id] + window_config.padding_low() +\n-        window_config.padding_high() +\n-        (input_dimensions[dim_id] - 1) * (window_config.base_dilation() - 1));\n-    AffineExpr dim_expr = getAffineDimExpr(dim_id, mlir_context);\n-    AffineExpr symbol_expr = getAffineSymbolExpr(dim_id, mlir_context);\n-\n-    exprs.push_back(symbol_expr * window_config.window_dilation() +\n-                    window_config.stride() * dim_expr);\n-    dim_vars.push_back(\n-        {IndexingMap::Variable{0, output_dimensions[dim_id] - 1}});\n-    range_vars.push_back({IndexingMap::Variable{0, window_config.size() - 1}});\n-  }\n-  // Indexing map for pad op that pads the input.\n-  IndexingMap padded_input_indexing = ComputeOutputToInputPadOpIndexingImpl(\n-      padded_input_dimensions, padding_low, padding_high, padding_interior,\n-      mlir_context);\n-  // Indexing map for reduce-window, that does not do any padding.\n-  IndexingMap input_indexing_no_padding(\n-      AffineMap::get(rank, rank, exprs, mlir_context), dim_vars, range_vars,\n-      /*rt_vars=*/{});\n-\n-  // Composed indexing.\n-  IndexingMap result =\n-      ComposeIndexingMaps(input_indexing_no_padding, padded_input_indexing);\n-  result.Simplify();\n-  result.RemoveUnusedSymbols();\n-  return result;\n+  SmallVector<int64_t> window_dims, window_strides, window_dilations,\n+      base_dilations, padding;\n+  window_dims.reserve(rank);\n+  window_strides.reserve(rank);\n+  window_dilations.reserve(rank);\n+  base_dilations.reserve(rank);\n+  padding.reserve(rank * 2);\n+  for (const auto& dim : window.dimensions()) {\n+    window_dims.push_back(dim.size());\n+    window_strides.push_back(dim.stride());\n+    window_dilations.push_back(dim.window_dilation());\n+    base_dilations.push_back(dim.base_dilation());\n+    padding.push_back(dim.padding_low());\n+    padding.push_back(dim.padding_high());\n+  }\n+  return ComposeWindowIndexingMap(input_dimensions, output_dimensions,\n+                                  window_dims, window_strides, window_dilations,\n+                                  base_dilations, padding, mlir_context);\n }\n \n // Indexing for reduce-window with dilations and non-trivial padding can be\n@@ -842,10 +740,8 @@ HloInstructionIndexing ComputeOutputToInputReduceWindowOpIndexing(\n       reduce_window->window(), mlir_context);\n \n   // Indexing map for the init value.\n-  IndexingMap inits_indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_shape.dimensions().size(), /*symbolCount=*/0, {},\n-                     mlir_context),\n-      output_shape.dimensions(), /*symbol_upper_bounds=*/{});\n+  IndexingMap inits_indexing_map =\n+      CreateScalarIndexingMap(output_shape, mlir_context);\n \n   HloInstructionIndexing instr_indexing;\n   instr_indexing.indexing_maps.resize(reduce_window->operand_count());\n@@ -1167,43 +1063,16 @@ HloInstructionIndexing ComputeInputToOutputReshapeOpIndexing(\n \n HloInstructionIndexing ComputeReverseOpIndexing(\n     const HloReverseInstruction* reverse, MLIRContext* mlir_context) {\n-  absl::flat_hash_set<int64_t> reverse_dims(reverse->dimensions().begin(),\n-                                            reverse->dimensions().end());\n-  auto output_dims = reverse->shape().dimensions();\n-\n-  std::vector<AffineExpr> exprs;\n-  exprs.reserve(output_dims.size());\n-  for (auto [output_dim_id, output_dim] : llvm::enumerate(output_dims)) {\n-    auto dim_expr = getAffineDimExpr(output_dim_id, mlir_context);\n-    if (!reverse_dims.contains(output_dim_id)) {\n-      exprs.push_back(dim_expr);\n-      continue;\n-    }\n-    exprs.push_back(-dim_expr + output_dim - 1);\n-  }\n-\n-  IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_dims.size(), /*symbolCount=*/0, exprs,\n-                     mlir_context),\n-      output_dims, {});\n-\n+  IndexingMap indexing_map = ComputeReverseIndexingMap(\n+      reverse->shape().dimensions(), reverse->dimensions(), mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps({indexing_map});\n }\n \n HloInstructionIndexing ComputeOutputToInputSliceOpIndexing(\n     const HloSliceInstruction* slice, MLIRContext* mlir_context) {\n-  auto output_rank = slice->shape().dimensions().size();\n-\n-  std::vector<AffineExpr> exprs;\n-  exprs.reserve(output_rank);\n-  for (int64_t dim = 0; dim < output_rank; ++dim) {\n-    AffineExpr dim_expr = getAffineDimExpr(dim, mlir_context);\n-    exprs.push_back(dim_expr * slice->slice_strides()[dim] +\n-                    slice->slice_starts()[dim]);\n-  }\n-  IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_rank, /*symbolCount=*/0, exprs, mlir_context),\n-      slice->shape().dimensions(), {});\n+  IndexingMap indexing_map = ComputeSliceIndexingMap(\n+      slice->shape().dimensions(), slice->slice_starts(),\n+      slice->slice_strides(), mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps({indexing_map});\n }\n \n@@ -1236,13 +1105,6 @@ HloInstructionIndexing ComputeInputToOutputSliceOpIndexing(\n   return HloInstructionIndexing::FromIndexingMaps({std::move(indexing_map)});\n }\n \n-AffineMap ComputeTransposeIndexingMap(absl::Span<const int64_t> permutation,\n-                                      MLIRContext* mlir_context) {\n-  return AffineMap::getPermutationMap(\n-      std::vector<unsigned>(permutation.begin(), permutation.end()),\n-      mlir_context);\n-}\n-\n HloInstructionIndexing ComputeOutputToInputTransposeOpIndexing(\n     const HloTransposeInstruction* transpose, MLIRContext* mlir_context) {\n   AffineMap inverse_permutation = ComputeTransposeIndexingMap(\n@@ -1946,7 +1808,7 @@ bool operator==(const OperandIndexing& lhs, const OperandIndexing& rhs) {\n }\n \n bool operator==(const RuntimeVarIndexing& lhs, const RuntimeVarIndexing& rhs) {\n-  return lhs.map == rhs.map && lhs.hlo == rhs.hlo;\n+  return lhs.map == rhs.map && lhs.instruction_ref == rhs.instruction_ref;\n }\n \n OperandIndexing ComposeOperandIndexing(const OperandIndexing& first,\n@@ -1960,7 +1822,8 @@ OperandIndexing ComposeOperandIndexing(const OperandIndexing& first,\n                           first.runtime_variables().end());\n   for (const auto& rt_var : second.runtime_variables()) {\n     IndexingMap combined_map = ComposeIndexingMaps(first.map(), rt_var.map);\n-    combined_runtime.push_back(RuntimeVarIndexing{rt_var.hlo, combined_map});\n+    combined_runtime.push_back(\n+        RuntimeVarIndexing{rt_var.instruction_ref, combined_map});\n   }\n \n   std::optional<IndexingMap> replica_id_map;\n@@ -1981,7 +1844,67 @@ OperandIndexing ComposeOperandIndexing(const OperandIndexing& first,\n }\n \n std::string RuntimeVarIndexing::ToString() const {\n-  return absl::StrCat(hlo->ToString(), \"; \", xla::ToString(map));\n+  // Handle both HLO and MLIR operations producing a unified enough format to\n+  // avoid duplication in tests.\n+  std::string instruction_str;\n+  if (auto* hlo = std::get_if<const HloInstruction*>(&instruction_ref)) {\n+    if (*hlo) {\n+      // For HLO, print simplified format for parameter and constant\n+      if ((*hlo)->opcode() == HloOpcode::kParameter) {\n+        instruction_str =\n+            absl::StrCat(\"parameter(\", (*hlo)->parameter_number(), \")\");\n+      } else if ((*hlo)->opcode() == HloOpcode::kConstant) {\n+        instruction_str = \"constant\";\n+        // Print constant value for scalar constants\n+        const xla::Literal& literal = (*hlo)->literal();\n+        if (xla::ShapeUtil::IsScalar(literal.shape())) {\n+          instruction_str =\n+              absl::StrCat(\"constant(\", literal.ToStringWithoutShape(), \")\");\n+        }\n+      } else {\n+        instruction_str = (*hlo)->name();\n+      }\n+    } else {\n+      instruction_str = \"<null hlo>\";\n+    }\n+  } else if (auto* val = std::get_if<mlir::Value>(&instruction_ref)) {\n+    if (*val) {\n+      if (auto* op = val->getDefiningOp()) {\n+        // Try to extract constant value for stablehlo/mhlo constant ops\n+        llvm::StringRef op_name = op->getName().getStringRef();\n+        if (op_name == \"stablehlo.constant\" || op_name == \"mhlo.constant\") {\n+          instruction_str = \"constant\";\n+          if (auto attr = op->getAttrOfType<mlir::DenseElementsAttr>(\"value\")) {\n+            if (attr.isSplat() && attr.getNumElements() == 1) {\n+              // Scalar constant - print the value\n+              auto elem_type = attr.getElementType();\n+              if (elem_type.isSignlessInteger()) {\n+                instruction_str = absl::StrCat(\n+                    \"constant(\",\n+                    attr.getSplatValue<llvm::APInt>().getSExtValue(), \")\");\n+              } else if (elem_type.isF32()) {\n+                instruction_str =\n+                    absl::StrCat(\"constant(\", attr.getSplatValue<float>(), \")\");\n+              } else if (elem_type.isF64()) {\n+                instruction_str = absl::StrCat(\n+                    \"constant(\", attr.getSplatValue<double>(), \")\");\n+              }\n+            }\n+          }\n+        } else {\n+          instruction_str = op_name.str();\n+        }\n+      } else {\n+        // Block argument is print as \"parameter(N)\" to match HLO format.\n+        auto block_arg = llvm::cast<mlir::BlockArgument>(*val);\n+        instruction_str =\n+            absl::StrCat(\"parameter(\", block_arg.getArgNumber(), \")\");\n+      }\n+    } else {\n+      instruction_str = \"<null value>\";\n+    }\n+  }\n+  return absl::StrCat(instruction_str, \"; \", xla::ToString(map));\n }\n \n std::ostream& operator<<(std::ostream& os, const RuntimeVarIndexing& var) {"
        },
        {
            "sha": "8295d0133f8d89e5660471c872ff87660a5da2a1",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.h",
            "status": "modified",
            "additions": 35,
            "deletions": 4,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -21,6 +21,8 @@ limitations under the License.\n #include <optional>\n #include <ostream>\n #include <string>\n+#include <utility>\n+#include <variant>\n #include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n@@ -31,7 +33,9 @@ limitations under the License.\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/Value.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/analysis/interval.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/shape.h\"\n@@ -108,15 +112,37 @@ IndexingMap ComputeEpilogueInputToOutputIndexing(\n     HloInstructionAdaptor epilogue_parent, HloInstructionAdaptor epilogue_root,\n     mlir::MLIRContext* mlir_context);\n \n-// Indexing of the runtime variable of the HLO instruction.\n+// Type for referencing either an HloInstruction or MLIR Value\n+using InstructionRef = std::variant<const HloInstruction*, mlir::Value>;\n+\n+// Indexing of the runtime variable of the HLO instruction or MLIR operation.\n struct RuntimeVarIndexing {\n-  // Instruction of the runtime variable. Note that while in trivial cases it\n+  // Instruction reference. Can be either HloInstruction* (for XLA HLO) or\n+  // mlir::Value (for StableHLO/MLIR). Note that while in trivial cases it\n   // points to one of the operands of the instruction, with multiple\n   // instructions and fusions it may point to an arbitrary instruction in the\n   // computation.\n-  const HloInstruction* hlo;\n-  // Output-to-input indexing map from the instruction to the output of `hlo`.\n+  InstructionRef instruction_ref;\n+\n+  // Output-to-input indexing map from the instruction to the output.\n   IndexingMap map;\n+\n+  // Accessor for HloInstruction*\n+  const HloInstruction* hlo() const {\n+    if (auto* hlo = std::get_if<const HloInstruction*>(&instruction_ref)) {\n+      return *hlo;\n+    }\n+    return nullptr;\n+  }\n+\n+  // Accessor for MLIR operations\n+  mlir::Operation* mlir_op() const {\n+    if (auto* val = std::get_if<mlir::Value>(&instruction_ref)) {\n+      return val->getDefiningOp();\n+    }\n+    return nullptr;\n+  }\n+\n   std::string ToString() const;\n };\n \n@@ -213,6 +239,11 @@ llvm::SmallVector<IndexingMap, 4> MapLogicalToLinearizedPhysicalShape(\n     absl::Span<const HloInstruction* const> operands,\n     mlir::MLIRContext* mlir_context);\n \n+// Optimizes a runtime variable if it's possible to replace it with a constant.\n+std::optional<mlir::AffineExpr> OptimizeRTVar(const RuntimeVarIndexing& rt_var,\n+                                              const Interval& feasible_values,\n+                                              mlir::MLIRContext* mlir_context);\n+\n // Computes the indexing map from logical to linearized physical shape for each\n // operand and adds them to `result`. `result` may be non-empty when this\n // function is called and can be used to accumulate results from several calls"
        },
        {
            "sha": "8db318e73f6b8b7fcdf7373fdfe98dc0fb61f990",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis_test.cc",
            "status": "modified",
            "additions": 200,
            "deletions": 108,
            "changes": 308,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -15,20 +15,33 @@ limitations under the License.\n \n #include \"xla/hlo/analysis/indexing_analysis.h\"\n \n+#include <utility>\n+#include <vector>\n+\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/OwningOpRef.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/analysis/indexing_map_serialization.h\"\n #include \"xla/hlo/analysis/indexing_test_utils.h\"\n+#include \"xla/hlo/analysis/stablehlo_indexing_analysis.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/translate/stablehlo.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace xla {\n namespace gpu {\n namespace {\n \n+using ::llvm::dyn_cast;\n using ::testing::ElementsAre;\n using ::testing::Eq;\n using ::testing::ExplainMatchResult;\n@@ -42,9 +55,84 @@ MATCHER_P2(MatchInstrIndexing, operand_id, indexing_map_matchers, \"\") {\n                             result_listener);\n }\n \n-using IndexingAnalysisTest = IndexingTestBase;\n+class IndexingAnalysisTest : public IndexingTestBase,\n+                             public ::testing::WithParamInterface<bool> {\n+ public:\n+  using IndexingTestBase::GetInputToOutputIndexing;\n+  using IndexingTestBase::GetOutputToInputIndexing;\n+\n+  HloInstructionIndexing GetOutputToInputIndexing(\n+      const HloInstruction* instr, int output_id,\n+      bool use_physical_layout) override {\n+    if (GetParam()) {\n+      // StableHLO mode\n+      auto module_ref =\n+          xla::ConvertHloToStablehlo(mlir_context_, instr->GetModule());\n+      if (!module_ref.ok()) {\n+        ADD_FAILURE() << \"HLO to StableHLO conversion failed: \"\n+                      << module_ref.status();\n+        return HloInstructionIndexing::FromIndexingMaps({});\n+      }\n+      stablehlo_modules_.push_back(std::move(module_ref.value()));\n+      auto module_op = *stablehlo_modules_.back();\n+      mlir::Operation* op = nullptr;\n+      module_op->walk([&](mlir::Operation* nested_op) {\n+        if (auto name_loc = dyn_cast<mlir::NameLoc>(nested_op->getLoc())) {\n+          if (name_loc.getName() == instr->name()) {\n+            op = nested_op;\n+            return mlir::WalkResult::interrupt();\n+          }\n+        }\n+        return mlir::WalkResult::advance();\n+      });\n+      if (!op) {\n+        ADD_FAILURE() << \"Could not find corresponding StableHLO op for \"\n+                      << instr->name();\n+        return HloInstructionIndexing::FromIndexingMaps({});\n+      }\n+      return ComputeOutputToInputIndexing(op, output_id);\n+    }\n+    return IndexingTestBase::GetOutputToInputIndexing(instr, output_id,\n+                                                      use_physical_layout);\n+  }\n+\n+  void SetUp() override {\n+    IndexingTestBase::SetUp();\n+    mlir_context_.loadDialect<mlir::stablehlo::StablehloDialect,\n+                              mlir::func::FuncDialect>();\n+\n+    static const auto* unsupported_tests =\n+        new absl::flat_hash_set<absl::string_view>{\n+            // StableHLO indexing analysis does not support physical layout /\n+            // permutations yet.\n+            \"PhysicalLayoutTestInputPermutation/1\",\n+            \"PhysicalLayoutTestOutputPermutation/1\",\n+            \"PhysicalLayoutTestInputAndOutputPermutation/1\",\n+\n+            // Custom call / MHLO unknown handling.\n+            \"ScaledDotOp/1\",\n+        };\n+\n+    if (GetParam()) {\n+      const testing::TestInfo* test_info =\n+          testing::UnitTest::GetInstance()->current_test_info();\n+      absl::string_view test_name = test_info->name();\n+      // Here we rely on exact match of \"TestName/1\" which corresponds to\n+      // GetParam() == true.\n+      if (unsupported_tests->contains(test_name)) {\n+        GTEST_SKIP() << \"Skipping unsupported StableHLO test: \" << test_name;\n+      }\n+    }\n+  }\n+\n+ private:\n+  std::vector<mlir::OwningOpRef<mlir::ModuleOp>> stablehlo_modules_;\n+};\n+\n+INSTANTIATE_TEST_SUITE_P(StablehloIndexingAnalysis, IndexingAnalysisTest,\n+                         ::testing::Values(false, true));\n \n-TEST_F(IndexingAnalysisTest, GroupIndexingMapsByProducers) {\n+TEST_P(IndexingAnalysisTest, GroupIndexingMapsByProducers) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -75,7 +163,7 @@ TEST_F(IndexingAnalysisTest, GroupIndexingMapsByProducers) {\n                   )\")))));\n }\n \n-TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing) {\n+TEST_P(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -119,7 +207,7 @@ TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing) {\n                       )\")))));\n }\n \n-TEST_F(IndexingAnalysisTest,\n+TEST_P(IndexingAnalysisTest,\n        ComputeGroupedOutputToInputIndexing_VariadicReduce) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n@@ -180,7 +268,7 @@ TEST_F(IndexingAnalysisTest,\n                   )\")))));\n }\n \n-TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing_SingleOp) {\n+TEST_P(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing_SingleOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -211,7 +299,7 @@ TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing_SingleOp) {\n                                                    )\")))));\n }\n \n-TEST_F(IndexingAnalysisTest,\n+TEST_P(IndexingAnalysisTest,\n        ComputeGroupedOutputToInputIndexing_StartNotAtRoot) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n@@ -263,7 +351,7 @@ TEST_F(IndexingAnalysisTest,\n           )\")))));\n }\n \n-TEST_F(IndexingAnalysisTest, PhysicalLayoutTestOutputPermutation) {\n+TEST_P(IndexingAnalysisTest, PhysicalLayoutTestOutputPermutation) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -294,7 +382,7 @@ TEST_F(IndexingAnalysisTest, PhysicalLayoutTestOutputPermutation) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, CopyNothing) {\n+TEST_P(IndexingAnalysisTest, CopyNothing) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -313,7 +401,7 @@ TEST_F(IndexingAnalysisTest, CopyNothing) {\n               MatchIndexingString(\"operand id = 0 KNOWN EMPTY\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReshapeNothing) {\n+TEST_P(IndexingAnalysisTest, ReshapeNothing) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -340,7 +428,7 @@ TEST_F(IndexingAnalysisTest, ReshapeNothing) {\n             1);\n }\n \n-TEST_F(IndexingAnalysisTest, PhysicalLayoutTestInputPermutation) {\n+TEST_P(IndexingAnalysisTest, PhysicalLayoutTestInputPermutation) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -371,7 +459,7 @@ TEST_F(IndexingAnalysisTest, PhysicalLayoutTestInputPermutation) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, PhysicalLayoutTestInputAndOutputPermutation) {\n+TEST_P(IndexingAnalysisTest, PhysicalLayoutTestInputAndOutputPermutation) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -402,7 +490,7 @@ TEST_F(IndexingAnalysisTest, PhysicalLayoutTestInputAndOutputPermutation) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ElementwiseOp) {\n+TEST_P(IndexingAnalysisTest, ElementwiseOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -444,7 +532,7 @@ TEST_F(IndexingAnalysisTest, ElementwiseOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, Map) {\n+TEST_P(IndexingAnalysisTest, Map) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     mapper {\n@@ -491,7 +579,7 @@ TEST_F(IndexingAnalysisTest, Map) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, BitcastIsReshape) {\n+TEST_P(IndexingAnalysisTest, BitcastIsReshape) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -509,7 +597,7 @@ TEST_F(IndexingAnalysisTest, BitcastIsReshape) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, BitcastIsTranspose) {\n+TEST_P(IndexingAnalysisTest, BitcastIsTranspose) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -528,7 +616,7 @@ TEST_F(IndexingAnalysisTest, BitcastIsTranspose) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, BitcastIsTransposeReshapeTranspose) {\n+TEST_P(IndexingAnalysisTest, BitcastIsTransposeReshapeTranspose) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -555,7 +643,7 @@ TEST_F(IndexingAnalysisTest, BitcastIsTransposeReshapeTranspose) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, BroadcastOp) {\n+TEST_P(IndexingAnalysisTest, BroadcastOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -583,7 +671,7 @@ TEST_F(IndexingAnalysisTest, BroadcastOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConstantOp) {\n+TEST_P(IndexingAnalysisTest, ConstantOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -594,7 +682,7 @@ TEST_F(IndexingAnalysisTest, ConstantOp) {\n   EXPECT_THAT(input_indexing.ToString(), IsEmpty());\n }\n \n-TEST_F(IndexingAnalysisTest, ConcatenateOp) {\n+TEST_P(IndexingAnalysisTest, ConcatenateOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -658,7 +746,7 @@ TEST_F(IndexingAnalysisTest, ConcatenateOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, DynamicSliceOp) {\n+TEST_P(IndexingAnalysisTest, DynamicSliceOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -682,11 +770,11 @@ TEST_F(IndexingAnalysisTest, DynamicSliceOp) {\n         rt1 in [0, 0],\n         rt2 in [0, 226]\n       runtime variables:\n-        rt0: %of1 = s32[] parameter(1); (d0, d1, d2) -> (),\n+        rt0: parameter(1); (d0, d1, d2) -> (),\n           domain: d0 in [0, 0], d1 in [0, 1], d2 in [0, 31]\n-        rt1: %of2 = s32[] parameter(2); (d0, d1, d2) -> (),\n+        rt1: parameter(2); (d0, d1, d2) -> (),\n           domain: d0 in [0, 0], d1 in [0, 1], d2 in [0, 31]\n-        rt2: %of3 = s32[] parameter(3); (d0, d1, d2) -> (),\n+        rt2: parameter(3); (d0, d1, d2) -> (),\n           domain: d0 in [0, 0], d1 in [0, 1], d2 in [0, 31]\n     operand id = 1\n       (d0, d1, d2) -> (),\n@@ -709,7 +797,7 @@ TEST_F(IndexingAnalysisTest, DynamicSliceOp) {\n     )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, DynamicUpdateSliceOp) {\n+TEST_P(IndexingAnalysisTest, DynamicUpdateSliceOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -735,9 +823,9 @@ TEST_F(IndexingAnalysisTest, DynamicUpdateSliceOp) {\n         rt0 in [0, 15],\n         rt1 in [0, 20]\n       runtime variables:\n-        rt0: %of1 = s32[] parameter(2); (d0, d1) -> (),\n+        rt0: parameter(2); (d0, d1) -> (),\n           domain: d0 in [0, 19], d1 in [0, 29]\n-        rt1: %of2 = s32[] parameter(3); (d0, d1) -> (),\n+        rt1: parameter(3); (d0, d1) -> (),\n           domain: d0 in [0, 19], d1 in [0, 29]\n     operand id = 2\n       (d0, d1) -> (),\n@@ -752,7 +840,7 @@ TEST_F(IndexingAnalysisTest, DynamicUpdateSliceOp) {\n       )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithSingleBinaryOp) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithSingleBinaryOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -778,7 +866,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithSingleBinaryOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithDot) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithDot) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     f {\n       p0 = s8[3,12288,6,128]{3,2,1,0} parameter(0)\n@@ -900,7 +988,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithDot) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithSoftmax) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithSoftmax) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     add_computation {\n       p0 = f32[] parameter(0)\n@@ -964,7 +1052,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithSoftmax) {\n                           )\"))));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpTensorPlusTransposedTensor) {\n+TEST_P(IndexingAnalysisTest, FusionOpTensorPlusTransposedTensor) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -992,7 +1080,7 @@ TEST_F(IndexingAnalysisTest, FusionOpTensorPlusTransposedTensor) {\n                           )\"))));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionExponentialDuplication) {\n+TEST_P(IndexingAnalysisTest, FusionExponentialDuplication) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule test_module\n \n@@ -1047,7 +1135,7 @@ TEST_F(IndexingAnalysisTest, FusionExponentialDuplication) {\n                           )\"))));\n }\n \n-TEST_F(IndexingAnalysisTest, GatherOp) {\n+TEST_P(IndexingAnalysisTest, GatherOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY main {\n@@ -1069,9 +1157,9 @@ TEST_F(IndexingAnalysisTest, GatherOp) {\n         rt0 in [0, 26],\n         rt1 in [0, 68]\n       runtime variables:\n-        rt0: %indices = s32[1806,2]{1,0} parameter(1); (d0, d1, d2, d3) -> (d0, 0),\n+        rt0: parameter(1); (d0, d1, d2, d3) -> (d0, 0),\n           domain: d0 in [0, 1805], d1 in [0, 6], d2 in [0, 7], d3 in [0, 3]\n-        rt1: %indices = s32[1806,2]{1,0} parameter(1); (d0, d1, d2, d3) -> (d0, 1),\n+        rt1: parameter(1); (d0, d1, d2, d3) -> (d0, 1),\n           domain: d0 in [0, 1805], d1 in [0, 6], d2 in [0, 7], d3 in [0, 3]\n     operand id = 1\n       (d0, d1, d2, d3)[s0] -> (d0, s0),\n@@ -1084,7 +1172,7 @@ TEST_F(IndexingAnalysisTest, GatherOp) {\n     )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, GatherOpWithShuffledStartIndexMap) {\n+TEST_P(IndexingAnalysisTest, GatherOpWithShuffledStartIndexMap) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY main {\n@@ -1106,9 +1194,9 @@ TEST_F(IndexingAnalysisTest, GatherOpWithShuffledStartIndexMap) {\n         rt0 in [0, 26],\n         rt1 in [0, 68]\n       runtime variables:\n-        rt0: %indices = s32[1806,2]{1,0} parameter(1); (d0, d1, d2, d3) -> (d0, 1),\n+        rt0: parameter(1); (d0, d1, d2, d3) -> (d0, 1),\n           domain: d0 in [0, 1805], d1 in [0, 6], d2 in [0, 7], d3 in [0, 3]\n-        rt1: %indices = s32[1806,2]{1,0} parameter(1); (d0, d1, d2, d3) -> (d0, 0),\n+        rt1: parameter(1); (d0, d1, d2, d3) -> (d0, 0),\n           domain: d0 in [0, 1805], d1 in [0, 6], d2 in [0, 7], d3 in [0, 3]\n     operand id = 1\n       (d0, d1, d2, d3)[s0] -> (d0, s0),\n@@ -1121,7 +1209,7 @@ TEST_F(IndexingAnalysisTest, GatherOpWithShuffledStartIndexMap) {\n     )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithReduceOfReduce) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithReduceOfReduce) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -1158,7 +1246,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithReduceOfReduce) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithReduceOfBroadcast) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithReduceOfBroadcast) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -1195,7 +1283,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithReduceOfBroadcast) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithTransposeOfTranspose) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithTransposeOfTranspose) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1230,7 +1318,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithTransposeOfTranspose) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithReducedSlice) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithReducedSlice) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -1266,7 +1354,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithReducedSlice) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithReshape_CollapseOfExpand) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithReshape_CollapseOfExpand) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1287,7 +1375,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithReshape_CollapseOfExpand) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithReshape_ExpandOfCollapse) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithReshape_ExpandOfCollapse) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1309,7 +1397,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithReshape_ExpandOfCollapse) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithReshape_ChainedGenericReshapes) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithReshape_ChainedGenericReshapes) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1332,7 +1420,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithReshape_ChainedGenericReshapes) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithSliceOfSlice) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithSliceOfSlice) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1357,7 +1445,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithSliceOfSlice) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithDynSliceOfDynSlice) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithDynSliceOfDynSlice) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1394,19 +1482,19 @@ TEST_F(IndexingAnalysisTest, FusionOpWithDynSliceOfDynSlice) {\n         rt2 in [0, 25],\n         rt3 in [0, 16]\n       runtime variables:\n-        rt0: %of21 = s32[] parameter(3); (d0, d1) -> (),\n+        rt0: parameter(3); (d0, d1) -> (),\n           domain: d0 in [0, 24], d1 in [0, 15]\n-        rt1: %of22 = s32[] parameter(4); (d0, d1) -> (),\n+        rt1: parameter(4); (d0, d1) -> (),\n           domain: d0 in [0, 24], d1 in [0, 15]\n-        rt2: %of11 = s32[] parameter(1); (d0, d1){rt0, rt1} -> (),\n+        rt2: parameter(1); (d0, d1){rt0, rt1} -> (),\n           domain:\n             d0 in [0, 24],\n             d1 in [0, 15],\n             rt0 in [0, 25],\n             rt1 in [0, 16],\n             d0 + rt0 in [0, 49],\n             d1 + rt1 in [0, 31]\n-        rt3: %of12 = s32[] parameter(2); (d0, d1){rt0, rt1} -> (),\n+        rt3: parameter(2); (d0, d1){rt0, rt1} -> (),\n           domain:\n             d0 in [0, 24], d1 in [0, 15],\n             rt0 in [0, 25], rt1 in [0, 16],\n@@ -1435,7 +1523,7 @@ TEST_F(IndexingAnalysisTest, FusionOpWithDynSliceOfDynSlice) {\n     )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpSliceOfAllConcatenateOpInputs) {\n+TEST_P(IndexingAnalysisTest, FusionOpSliceOfAllConcatenateOpInputs) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1476,7 +1564,7 @@ TEST_F(IndexingAnalysisTest, FusionOpSliceOfAllConcatenateOpInputs) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpSliceOfOneOfConcatenateOpInputs) {\n+TEST_P(IndexingAnalysisTest, FusionOpSliceOfOneOfConcatenateOpInputs) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1509,7 +1597,7 @@ TEST_F(IndexingAnalysisTest, FusionOpSliceOfOneOfConcatenateOpInputs) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpReshapeOfConcat) {\n+TEST_P(IndexingAnalysisTest, FusionOpReshapeOfConcat) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     f {\n@@ -1540,7 +1628,7 @@ TEST_F(IndexingAnalysisTest, FusionOpReshapeOfConcat) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, IotaOp) {\n+TEST_P(IndexingAnalysisTest, IotaOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1551,7 +1639,7 @@ TEST_F(IndexingAnalysisTest, IotaOp) {\n   EXPECT_THAT(input_indexing.indexing_maps, IsEmpty());\n }\n \n-TEST_F(IndexingAnalysisTest, ReshapeOpCollapseShape) {\n+TEST_P(IndexingAnalysisTest, ReshapeOpCollapseShape) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1567,7 +1655,7 @@ TEST_F(IndexingAnalysisTest, ReshapeOpCollapseShape) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReshapeOpExpandShape) {\n+TEST_P(IndexingAnalysisTest, ReshapeOpExpandShape) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1584,7 +1672,7 @@ TEST_F(IndexingAnalysisTest, ReshapeOpExpandShape) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReshapeOpExpandAndCollapseShape) {\n+TEST_P(IndexingAnalysisTest, ReshapeOpExpandAndCollapseShape) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1613,7 +1701,7 @@ TEST_F(IndexingAnalysisTest, ReshapeOpExpandAndCollapseShape) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReshapeOpExpandSubshapeOnly) {\n+TEST_P(IndexingAnalysisTest, ReshapeOpExpandSubshapeOnly) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1631,7 +1719,7 @@ TEST_F(IndexingAnalysisTest, ReshapeOpExpandSubshapeOnly) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReshapeOpGenericReshape2DTo3D) {\n+TEST_P(IndexingAnalysisTest, ReshapeOpGenericReshape2DTo3D) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1649,7 +1737,7 @@ TEST_F(IndexingAnalysisTest, ReshapeOpGenericReshape2DTo3D) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReshapeOpGenericReshape3DTo2D) {\n+TEST_P(IndexingAnalysisTest, ReshapeOpGenericReshape3DTo2D) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1668,7 +1756,7 @@ TEST_F(IndexingAnalysisTest, ReshapeOpGenericReshape3DTo2D) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, PadOp) {\n+TEST_P(IndexingAnalysisTest, PadOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1692,7 +1780,7 @@ TEST_F(IndexingAnalysisTest, PadOp) {\n                                 )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, PadOpNoInterior) {\n+TEST_P(IndexingAnalysisTest, PadOpNoInterior) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -1715,7 +1803,7 @@ TEST_F(IndexingAnalysisTest, PadOpNoInterior) {\n                                 )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, PadOpNegativePadding) {\n+TEST_P(IndexingAnalysisTest, PadOpNegativePadding) {\n   // The interior padding is applied first (even with negative padding), so we\n   // get a size of 5 (7 + 6 - 8).\n   // in:     0 1 2 3 4 5 6\n@@ -1742,7 +1830,7 @@ TEST_F(IndexingAnalysisTest, PadOpNegativePadding) {\n                                 )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReduceOp) {\n+TEST_P(IndexingAnalysisTest, ReduceOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -1793,7 +1881,7 @@ TEST_F(IndexingAnalysisTest, ReduceOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, VariadicReduceOp) {\n+TEST_P(IndexingAnalysisTest, VariadicReduceOp) {\n   HloInstruction* root = ParseAndGetRoot(R\"(\n     HloModule m\n     min {\n@@ -1895,7 +1983,7 @@ TEST_F(IndexingAnalysisTest, VariadicReduceOp) {\n                   ElementsAre(MatchOperandIndexing(kInitToOutputIndexing))));\n }\n \n-TEST_F(IndexingAnalysisTest, ReduceWindowOp_NoPadding) {\n+TEST_P(IndexingAnalysisTest, ReduceWindowOp_NoPadding) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -1926,7 +2014,7 @@ TEST_F(IndexingAnalysisTest, ReduceWindowOp_NoPadding) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReduceWindowOp_4DWithTrivalDims_NoPadding) {\n+TEST_P(IndexingAnalysisTest, ReduceWindowOp_4DWithTrivalDims_NoPadding) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -1961,7 +2049,7 @@ TEST_F(IndexingAnalysisTest, ReduceWindowOp_4DWithTrivalDims_NoPadding) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReduceWindowOp_PaddingAndWindowStride) {\n+TEST_P(IndexingAnalysisTest, ReduceWindowOp_PaddingAndWindowStride) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -1995,7 +2083,7 @@ TEST_F(IndexingAnalysisTest, ReduceWindowOp_PaddingAndWindowStride) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReduceWindowOp_BaseDilation) {\n+TEST_P(IndexingAnalysisTest, ReduceWindowOp_BaseDilation) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -2027,7 +2115,7 @@ TEST_F(IndexingAnalysisTest, ReduceWindowOp_BaseDilation) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReduceWindowOp_WindowDilation) {\n+TEST_P(IndexingAnalysisTest, ReduceWindowOp_WindowDilation) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     max {\n@@ -2058,7 +2146,7 @@ TEST_F(IndexingAnalysisTest, ReduceWindowOp_WindowDilation) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReduceWindowOp_Variadic) {\n+TEST_P(IndexingAnalysisTest, ReduceWindowOp_Variadic) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     combiner {\n@@ -2136,7 +2224,7 @@ TEST_F(IndexingAnalysisTest, ReduceWindowOp_Variadic) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConvolutionOp_NoPadding) {\n+TEST_P(IndexingAnalysisTest, ConvolutionOp_NoPadding) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2171,7 +2259,7 @@ TEST_F(IndexingAnalysisTest, ConvolutionOp_NoPadding) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConvolutionOp_4DWithTrivialDims_NoPadding) {\n+TEST_P(IndexingAnalysisTest, ConvolutionOp_4DWithTrivialDims_NoPadding) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2208,7 +2296,7 @@ TEST_F(IndexingAnalysisTest, ConvolutionOp_4DWithTrivialDims_NoPadding) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConvolutionOp_PaddingAndWindowStride) {\n+TEST_P(IndexingAnalysisTest, ConvolutionOp_PaddingAndWindowStride) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2245,7 +2333,7 @@ TEST_F(IndexingAnalysisTest, ConvolutionOp_PaddingAndWindowStride) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConvolutionOp_LhsDilation) {\n+TEST_P(IndexingAnalysisTest, ConvolutionOp_LhsDilation) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2282,7 +2370,7 @@ TEST_F(IndexingAnalysisTest, ConvolutionOp_LhsDilation) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConvolutionOp_RhsDilation) {\n+TEST_P(IndexingAnalysisTest, ConvolutionOp_RhsDilation) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2317,7 +2405,7 @@ TEST_F(IndexingAnalysisTest, ConvolutionOp_RhsDilation) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConvolutionOp_FeatureGroups) {\n+TEST_P(IndexingAnalysisTest, ConvolutionOp_FeatureGroups) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2352,7 +2440,7 @@ TEST_F(IndexingAnalysisTest, ConvolutionOp_FeatureGroups) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ConvolutionOp_BatchGroups) {\n+TEST_P(IndexingAnalysisTest, ConvolutionOp_BatchGroups) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2388,7 +2476,7 @@ TEST_F(IndexingAnalysisTest, ConvolutionOp_BatchGroups) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReverseOp) {\n+TEST_P(IndexingAnalysisTest, ReverseOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2419,7 +2507,7 @@ TEST_F(IndexingAnalysisTest, ReverseOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ReverseReshape) {\n+TEST_P(IndexingAnalysisTest, ReverseReshape) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     fused_computation {\n@@ -2444,7 +2532,7 @@ TEST_F(IndexingAnalysisTest, ReverseReshape) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, SliceOp) {\n+TEST_P(IndexingAnalysisTest, SliceOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2479,7 +2567,7 @@ TEST_F(IndexingAnalysisTest, SliceOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, TransposeOp) {\n+TEST_P(IndexingAnalysisTest, TransposeOp) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2508,7 +2596,7 @@ TEST_F(IndexingAnalysisTest, TransposeOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, TransposeOp4D) {\n+TEST_P(IndexingAnalysisTest, TransposeOp4D) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2527,7 +2615,7 @@ TEST_F(IndexingAnalysisTest, TransposeOp4D) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, DotOp) {\n+TEST_P(IndexingAnalysisTest, DotOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2564,7 +2652,7 @@ TEST_F(IndexingAnalysisTest, DotOp) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, ScaledDotOp) {\n+TEST_P(IndexingAnalysisTest, ScaledDotOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2605,7 +2693,7 @@ TEST_F(IndexingAnalysisTest, ScaledDotOp) {\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, UnsupportedOps) {\n+TEST_P(IndexingAnalysisTest, UnsupportedOps) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2633,7 +2721,7 @@ TEST_F(IndexingAnalysisTest, UnsupportedOps) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionWithUnsupportedOp) {\n+TEST_P(IndexingAnalysisTest, FusionWithUnsupportedOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     fused_computation {\n@@ -2670,7 +2758,7 @@ TEST_F(IndexingAnalysisTest, FusionWithUnsupportedOp) {\n                           )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, EpilogueIndexing) {\n+TEST_P(IndexingAnalysisTest, EpilogueIndexing) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     HloModule m\n     fused_computation {\n@@ -2703,7 +2791,7 @@ TEST_F(IndexingAnalysisTest, EpilogueIndexing) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, EpilogueIndexing_NoEpilogue) {\n+TEST_P(IndexingAnalysisTest, EpilogueIndexing_NoEpilogue) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     HloModule m\n     fused_computation {\n@@ -2732,7 +2820,7 @@ TEST_F(IndexingAnalysisTest, EpilogueIndexing_NoEpilogue) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, BroadcastingElementwise) {\n+TEST_P(IndexingAnalysisTest, BroadcastingElementwise) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m\n     ENTRY e {\n@@ -2758,7 +2846,7 @@ TEST_F(IndexingAnalysisTest, BroadcastingElementwise) {\n               )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionWithRTVarsSimplification_ScalarConstant) {\n+TEST_P(IndexingAnalysisTest, FusionWithRTVarsSimplification_ScalarConstant) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"hlo(\n       HloModule m\n       fused_computation {\n@@ -2781,7 +2869,7 @@ TEST_F(IndexingAnalysisTest, FusionWithRTVarsSimplification_ScalarConstant) {\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest,\n+TEST_P(IndexingAnalysisTest,\n        FusionWithRTVarsSimplification_ScalarConstantOutsideOfRangeIsKept) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"hlo(\n       HloModule m\n@@ -2804,12 +2892,12 @@ TEST_F(IndexingAnalysisTest,\n         d0 in [0, 9],\n         rt0 in [0, 90]\n       runtime variables:\n-        rt0: %offset = s64[] constant(99); (d0) -> (),\n+        rt0: constant(99); (d0) -> (),\n           domain: d0 in [0, 9]\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionWithRTVarsSimplification_Iota) {\n+TEST_P(IndexingAnalysisTest, FusionWithRTVarsSimplification_Iota) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"hlo(\n       HloModule m\n       fused_computation {\n@@ -2837,7 +2925,7 @@ TEST_F(IndexingAnalysisTest, FusionWithRTVarsSimplification_Iota) {\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionWithRTVarsSimplification_IotaAsConstant) {\n+TEST_P(IndexingAnalysisTest, FusionWithRTVarsSimplification_IotaAsConstant) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"hlo(\n       HloModule m\n       fused_computation {\n@@ -2865,7 +2953,7 @@ TEST_F(IndexingAnalysisTest, FusionWithRTVarsSimplification_IotaAsConstant) {\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, FusionOpWithPadAndDynamicSlice) {\n+TEST_P(IndexingAnalysisTest, FusionOpWithPadAndDynamicSlice) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"hlo(\n       HloModule m\n       fused_computation {\n@@ -2884,7 +2972,10 @@ TEST_F(IndexingAnalysisTest, FusionOpWithPadAndDynamicSlice) {\n           calls=fused_computation\n       }\n     )hlo\"));\n-  EXPECT_THAT(input_indexing.ToString(), MatchIndexingString(R\"(\n+  // HLO uses instruction name, StableHLO uses op name\n+  EXPECT_THAT(input_indexing.ToString(),\n+              MatchIndexingString(\n+                  absl::StrFormat(R\"(\n     operand id = 0\n       (d0, d1){rt0} -> (0, d1 + rt0 - 4096),\n       domain:\n@@ -2893,17 +2984,18 @@ TEST_F(IndexingAnalysisTest, FusionOpWithPadAndDynamicSlice) {\n         rt0 in [0, 4096],\n         d1 + rt0 in [4096, 8191]\n       runtime variables:\n-        rt0: %bitcast.4 = s32[] bitcast(%slice); (d0, d1) -> (),\n+        rt0: %s; (d0, d1) -> (),\n           domain: d0 in [0, 0], d1 in [0, 4095]\n     operand id = 1\n       (d0, d1) -> (0),\n       domain:\n         d0 in [0, 0],\n         d1 in [0, 4095]\n-  )\"));\n+    )\",\n+                                  GetParam() ? \"mhlo.bitcast\" : \"bitcast.4\")));\n }\n \n-TEST_F(IndexingAnalysisTest, NestedDotFusionWithDynamicUpdateSlice) {\n+TEST_P(IndexingAnalysisTest, NestedDotFusionWithDynamicUpdateSlice) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule t\n \n@@ -2946,7 +3038,7 @@ TEST_F(IndexingAnalysisTest, NestedDotFusionWithDynamicUpdateSlice) {\n     operand id = 1 (d0, d1)[s0]{rt0} -> (rt0, d1, s0),\n       domain: d0 in [0, 3], d1 in [0, 4], s0 in [0, 1], rt0 in [0, 3]\n     runtime variables:\n-      rt0: %p1 = s32[] parameter(1);\n+      rt0: parameter(1);\n         (d0, d1)[s0] -> (), domain: d0 in [0, 3], d1 in [0, 4], s0 in [0, 1]\n     operand id = 2 (d0, d1) -> (),\n       domain: d0 in [0, 3], d1 in [0, 4]\n@@ -2957,7 +3049,7 @@ TEST_F(IndexingAnalysisTest, NestedDotFusionWithDynamicUpdateSlice) {\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, AllGatherOp) {\n+TEST_P(IndexingAnalysisTest, AllGatherOp) {\n   auto input_indexing = GetOutputToInputIndexing(ParseAndGetRoot(R\"(\n     HloModule m, replica_count=4\n     ENTRY e {\n@@ -2980,7 +3072,7 @@ TEST_F(IndexingAnalysisTest, AllGatherOp) {\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, AllGatherFusionWithTranspose) {\n+TEST_P(IndexingAnalysisTest, AllGatherFusionWithTranspose) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n \n@@ -3019,7 +3111,7 @@ TEST_F(IndexingAnalysisTest, AllGatherFusionWithTranspose) {\n   )\"));\n }\n \n-TEST_F(IndexingAnalysisTest, AllGatherFusionWithReshape) {\n+TEST_P(IndexingAnalysisTest, AllGatherFusionWithReshape) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n \n@@ -3063,7 +3155,7 @@ TEST_F(IndexingAnalysisTest, AllGatherFusionWithReshape) {\n   )\")));\n }\n \n-TEST_F(IndexingAnalysisTest, ChainedAllGatherFusion) {\n+TEST_P(IndexingAnalysisTest, ChainedAllGatherFusion) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n \n@@ -3086,7 +3178,7 @@ TEST_F(IndexingAnalysisTest, ChainedAllGatherFusion) {\n               ElementsAre(UndefinedOperandIndexing()));\n }\n \n-TEST_F(IndexingAnalysisTest, AllGatherDotFusion_GatherNonContractingDim) {\n+TEST_P(IndexingAnalysisTest, AllGatherDotFusion_GatherNonContractingDim) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n \n@@ -3121,7 +3213,7 @@ TEST_F(IndexingAnalysisTest, AllGatherDotFusion_GatherNonContractingDim) {\n   )\")));\n }\n \n-TEST_F(IndexingAnalysisTest, AllGatherDotFusion_GatherContractingDim) {\n+TEST_P(IndexingAnalysisTest, AllGatherDotFusion_GatherContractingDim) {\n   auto root = ParseAndGetRoot(R\"(\n     HloModule m\n "
        },
        {
            "sha": "ba37bf74e778e7b3ee7b6e86489cffb475b7d8e4",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis_utils.cc",
            "status": "added",
            "additions": 340,
            "deletions": 0,
            "changes": 340,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_utils.cc?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -0,0 +1,340 @@\n+/* Copyright 2023 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/analysis/indexing_analysis_utils.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/IR/AffineExpr.h\"\n+#include \"mlir/IR/AffineMap.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/hlo/analysis/indexing_analysis.h\"\n+#include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/analysis/interval.h\"\n+#include \"xla/shape.h\"\n+\n+namespace xla {\n+\n+using llvm::SmallVector;\n+using mlir::AffineExpr;\n+using mlir::AffineMap;\n+using mlir::getAffineConstantExpr;\n+using mlir::getAffineDimExpr;\n+using mlir::getAffineSymbolExpr;\n+using mlir::MLIRContext;\n+\n+IndexingMap ComputeBroadcastIndexingMap(\n+    absl::Span<const int64_t> output_dims,\n+    absl::Span<const int64_t> broadcast_dims, MLIRContext* mlir_context) {\n+  std::vector<AffineExpr> exprs;\n+  exprs.reserve(broadcast_dims.size());\n+  for (int64_t bcast_dim : broadcast_dims) {\n+    exprs.push_back(getAffineDimExpr(bcast_dim, mlir_context));\n+  }\n+  return IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_dims.size(), /*symbolCount=*/0, exprs,\n+                     mlir_context),\n+      output_dims, {});\n+}\n+\n+IndexingMap ComputeSliceIndexingMap(absl::Span<const int64_t> output_shape_dims,\n+                                    absl::Span<const int64_t> slice_starts,\n+                                    absl::Span<const int64_t> slice_strides,\n+                                    mlir::MLIRContext* mlir_context) {\n+  auto rank = output_shape_dims.size();\n+  std::vector<AffineExpr> exprs;\n+  exprs.reserve(rank);\n+  for (int64_t dim = 0; dim < rank; ++dim) {\n+    AffineExpr dim_expr = getAffineDimExpr(dim, mlir_context);\n+    exprs.push_back(dim_expr * slice_strides[dim] + slice_starts[dim]);\n+  }\n+  return IndexingMap::FromTensorSizes(\n+      AffineMap::get(rank, /*symbolCount=*/0, exprs, mlir_context),\n+      output_shape_dims, {});\n+}\n+\n+IndexingMap ComputeReverseIndexingMap(\n+    absl::Span<const int64_t> output_shape_dims,\n+    absl::Span<const int64_t> reverse_dims, mlir::MLIRContext* mlir_context) {\n+  absl::flat_hash_set<int64_t> reverse_dims_set(reverse_dims.begin(),\n+                                                reverse_dims.end());\n+  std::vector<AffineExpr> exprs;\n+  exprs.reserve(output_shape_dims.size());\n+  for (auto [output_dim_id, output_dim] : llvm::enumerate(output_shape_dims)) {\n+    auto dim_expr = getAffineDimExpr(output_dim_id, mlir_context);\n+    exprs.push_back(reverse_dims_set.contains(output_dim_id)\n+                        ? -dim_expr + output_dim - 1\n+                        : dim_expr);\n+  }\n+  return IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_shape_dims.size(), /*symbolCount=*/0, exprs,\n+                     mlir_context),\n+      output_shape_dims, {});\n+}\n+\n+HloInstructionIndexing ComputeConcatenateIndexing(\n+    int64_t rank, int64_t concat_dim, absl::Span<const int64_t> output_dims,\n+    const std::vector<int64_t>& operand_concat_dim_sizes,\n+    mlir::MLIRContext* mlir_context) {\n+  mlir::MutableAffineMap affine_map =\n+      AffineMap::getMultiDimIdentityMap(rank, mlir_context);\n+  std::vector<IndexingMap::Variable> dim_vars =\n+      DimVarsFromTensorSizes(output_dims);\n+\n+  HloInstructionIndexing concat_indexing;\n+  concat_indexing.indexing_maps.resize(operand_concat_dim_sizes.size());\n+  AffineExpr concat_dim_expr = getAffineDimExpr(concat_dim, mlir_context);\n+  int64_t offset = 0;\n+  for (const auto [operand_id, operand_concat_dim] :\n+       llvm::enumerate(operand_concat_dim_sizes)) {\n+    affine_map.setResult(concat_dim, concat_dim_expr - offset);\n+    dim_vars[concat_dim] =\n+        IndexingMap::Variable{{offset, offset + operand_concat_dim - 1}};\n+    concat_indexing.indexing_maps[operand_id].insert(\n+        OperandIndexing(IndexingMap(affine_map.getAffineMap(), dim_vars,\n+                                    /*range_vars=*/{}, /*rt_vars=*/{})));\n+    offset += operand_concat_dim;\n+  }\n+  return concat_indexing;\n+}\n+\n+std::pair<IndexingMap, IndexingMap> ComputeDotOperandsIndexing(\n+    absl::Span<const int64_t> lhs_dims, absl::Span<const int64_t> rhs_dims,\n+    absl::Span<const int64_t> output_dims,\n+    absl::Span<const int64_t> lhs_batch_dims,\n+    absl::Span<const int64_t> rhs_batch_dims,\n+    absl::Span<const int64_t> lhs_contracting_dims,\n+    absl::Span<const int64_t> rhs_contracting_dims, MLIRContext* mlir_context) {\n+  SmallVector<AffineExpr> lhs_exprs(lhs_dims.size());\n+  SmallVector<AffineExpr> rhs_exprs(rhs_dims.size());\n+  int64_t output_dim_id = 0;\n+\n+  // Batch dimensions\n+  for (auto [lhs_batch_dim, rhs_batch_dim] :\n+       llvm::zip(lhs_batch_dims, rhs_batch_dims)) {\n+    AffineExpr output_dim_expr = getAffineDimExpr(output_dim_id, mlir_context);\n+    lhs_exprs[lhs_batch_dim] = output_dim_expr;\n+    rhs_exprs[rhs_batch_dim] = output_dim_expr;\n+    ++output_dim_id;\n+  }\n+\n+  // LHS non-contracting dims\n+  absl::flat_hash_set<int64_t> lhs_batch_set(lhs_batch_dims.begin(),\n+                                             lhs_batch_dims.end());\n+  absl::flat_hash_set<int64_t> lhs_contracting_set(lhs_contracting_dims.begin(),\n+                                                   lhs_contracting_dims.end());\n+  for (int64_t i = 0; i < lhs_dims.size(); ++i) {\n+    if (!lhs_batch_set.contains(i) && !lhs_contracting_set.contains(i)) {\n+      lhs_exprs[i] = getAffineDimExpr(output_dim_id++, mlir_context);\n+    }\n+  }\n+\n+  // RHS non-contracting dims\n+  absl::flat_hash_set<int64_t> rhs_batch_set(rhs_batch_dims.begin(),\n+                                             rhs_batch_dims.end());\n+  absl::flat_hash_set<int64_t> rhs_contracting_set(rhs_contracting_dims.begin(),\n+                                                   rhs_contracting_dims.end());\n+  for (int64_t i = 0; i < rhs_dims.size(); ++i) {\n+    if (!rhs_batch_set.contains(i) && !rhs_contracting_set.contains(i)) {\n+      rhs_exprs[i] = getAffineDimExpr(output_dim_id++, mlir_context);\n+    }\n+  }\n+\n+  // Contracting dimensions (as symbols)\n+  int64_t symbol_id = 0;\n+  std::vector<int64_t> symbol_sizes;\n+  symbol_sizes.reserve(lhs_contracting_dims.size());\n+  for (auto [lhs_contract, rhs_contract] :\n+       llvm::zip(lhs_contracting_dims, rhs_contracting_dims)) {\n+    AffineExpr symbol_expr = getAffineSymbolExpr(symbol_id, mlir_context);\n+    lhs_exprs[lhs_contract] = symbol_expr;\n+    rhs_exprs[rhs_contract] = symbol_expr;\n+    symbol_sizes.push_back(lhs_dims[lhs_contract]);\n+    ++symbol_id;\n+  }\n+\n+  int64_t output_rank = output_dims.size();\n+  return std::make_pair(IndexingMap::FromTensorSizes(\n+                            AffineMap::get(output_rank, symbol_sizes.size(),\n+                                           lhs_exprs, mlir_context),\n+                            output_dims, symbol_sizes),\n+                        IndexingMap::FromTensorSizes(\n+                            AffineMap::get(output_rank, symbol_sizes.size(),\n+                                           rhs_exprs, mlir_context),\n+                            output_dims, symbol_sizes));\n+}\n+\n+IndexingMap ComputeReduceInputIndexingMap(absl::Span<const int64_t> input_dims,\n+                                          absl::Span<const int64_t> output_dims,\n+                                          absl::Span<const int64_t> reduce_dims,\n+                                          MLIRContext* mlir_context) {\n+  absl::flat_hash_set<int64_t> reduce_dims_set(reduce_dims.begin(),\n+                                               reduce_dims.end());\n+  std::vector<int64_t> parallel_dims_sizes;\n+  int64_t output_dim_id = 0;\n+  std::vector<AffineExpr> exprs;\n+  exprs.reserve(input_dims.size());\n+\n+  for (auto [input_dim_id, input_dim] : llvm::enumerate(input_dims)) {\n+    if (reduce_dims_set.contains(input_dim_id)) {\n+      exprs.push_back(\n+          getAffineSymbolExpr(parallel_dims_sizes.size(), mlir_context));\n+      parallel_dims_sizes.push_back(input_dim);\n+      continue;\n+    }\n+    exprs.push_back(getAffineDimExpr(output_dim_id++, mlir_context));\n+  }\n+\n+  return IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_dims.size(), reduce_dims_set.size(), exprs,\n+                     mlir_context),\n+      output_dims, parallel_dims_sizes);\n+}\n+\n+IndexingMap ComputePadIndexingMap(absl::Span<const int64_t> output_dims,\n+                                  absl::Span<const int64_t> padding_low,\n+                                  absl::Span<const int64_t> padding_high,\n+                                  absl::Span<const int64_t> padding_interior,\n+                                  MLIRContext* mlir_context) {\n+  int64_t output_rank = output_dims.size();\n+\n+  std::vector<AffineExpr> exprs;\n+  std::vector<std::pair<AffineExpr, Interval>> constraints;\n+  std::vector<IndexingMap::Variable> dim_vars;\n+  exprs.reserve(output_rank);\n+  constraints.reserve(output_rank);\n+  int64_t output_dim_id = 0;\n+  for (const auto [output_dim, pad_low, pad_high, pad_interior] :\n+       llvm::zip(output_dims, padding_low, padding_high, padding_interior)) {\n+    AffineExpr dim_expr = getAffineDimExpr(output_dim_id, mlir_context);\n+    dim_vars.push_back({IndexingMap::Variable{\n+        std::max(int64_t{0}, pad_low),\n+        std::min(output_dim - 1, output_dim - 1 - pad_high)}});\n+    if (pad_interior == 0) {\n+      exprs.push_back(dim_expr - pad_low);\n+    } else {\n+      exprs.push_back((dim_expr - pad_low).floorDiv(pad_interior + 1));\n+      constraints.push_back(\n+          {(dim_expr - pad_low) % (pad_interior + 1), Interval{0, 0}});\n+    }\n+    ++output_dim_id;\n+  }\n+  return IndexingMap{\n+      AffineMap::get(output_rank, /*symbolCount=*/0, exprs, mlir_context),\n+      std::move(dim_vars),\n+      /*range_vars = */ {},\n+      /*rt_vars = */ {}, absl::MakeSpan(constraints)};\n+}\n+\n+IndexingMap ComposeWindowIndexingMap(absl::Span<const int64_t> input_dims,\n+                                     absl::Span<const int64_t> output_dims,\n+                                     absl::Span<const int64_t> window_dims,\n+                                     absl::Span<const int64_t> window_strides,\n+                                     absl::Span<const int64_t> window_dilations,\n+                                     absl::Span<const int64_t> base_dilations,\n+                                     absl::Span<const int64_t> padding,\n+                                     MLIRContext* mlir_context) {\n+  size_t rank = input_dims.size();\n+\n+  // Compute shape of the padded input and the indexing map of pad op required\n+  // to pad the input.\n+  SmallVector<int64_t> padding_low, padding_high, padding_interior,\n+      padded_input_dimensions;\n+  SmallVector<AffineExpr, 4> exprs;\n+  std::vector<IndexingMap::Variable> dim_vars;\n+  std::vector<IndexingMap::Variable> range_vars;\n+  exprs.reserve(rank);\n+  dim_vars.reserve(rank);\n+  range_vars.reserve(rank);\n+\n+  for (size_t dim_id = 0; dim_id < rank; ++dim_id) {\n+    int64_t pad_low = padding[dim_id * 2];\n+    int64_t pad_high = padding[dim_id * 2 + 1];\n+    int64_t base_dilation = base_dilations[dim_id];\n+    int64_t window_dilation = window_dilations[dim_id];\n+    int64_t window_stride = window_strides[dim_id];\n+    int64_t output_dim = output_dims[dim_id];\n+    int64_t window_dim = window_dims[dim_id];\n+    int64_t input_dim_size = input_dims[dim_id];\n+\n+    padding_low.push_back(pad_low);\n+    padding_high.push_back(pad_high);\n+    // For some reason interior_padding in HLO pad is offset from base_dilations\n+    // in HLO reduce-window by 1.\n+    padding_interior.push_back(base_dilation - 1);\n+    padded_input_dimensions.push_back(input_dim_size + pad_low + pad_high +\n+                                      (input_dim_size - 1) *\n+                                          (base_dilation - 1));\n+    AffineExpr dim_expr = getAffineDimExpr(dim_id, mlir_context);\n+    AffineExpr symbol_expr = getAffineSymbolExpr(dim_id, mlir_context);\n+\n+    exprs.push_back(symbol_expr * window_dilation + window_stride * dim_expr);\n+    dim_vars.push_back({IndexingMap::Variable{0, output_dim - 1}});\n+    range_vars.push_back({IndexingMap::Variable{0, window_dim - 1}});\n+  }\n+  // Indexing map for pad op that pads the input.\n+  IndexingMap padded_input_indexing =\n+      ComputePadIndexingMap(padded_input_dimensions, padding_low, padding_high,\n+                            padding_interior, mlir_context);\n+  // Indexing map for reduce-window, that does not do any padding.\n+  IndexingMap input_indexing_no_padding(\n+      AffineMap::get(rank, rank, exprs, mlir_context), dim_vars, range_vars,\n+      /*rt_vars=*/{});\n+\n+  // Composed indexing.\n+  IndexingMap result =\n+      ComposeIndexingMaps(input_indexing_no_padding, padded_input_indexing);\n+  result.Simplify();\n+  result.RemoveUnusedSymbols();\n+  return result;\n+}\n+\n+HloInstructionIndexing CreateElementwiseIndexing(int64_t num_operands,\n+                                                 const Shape& output_shape,\n+                                                 MLIRContext* mlir_context) {\n+  IndexingMap identity_map = IndexingMap::FromTensorSizes(\n+      AffineMap::getMultiDimIdentityMap(output_shape.dimensions().size(),\n+                                        mlir_context),\n+      output_shape.dimensions(), {});\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(num_operands);\n+  for (int64_t i = 0; i < num_operands; ++i) {\n+    indexing.indexing_maps[i].insert(OperandIndexing{identity_map});\n+  }\n+  return indexing;\n+}\n+\n+IndexingMap CreateScalarIndexingMap(const Shape& output_shape,\n+                                    MLIRContext* mlir_context) {\n+  return IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_shape.dimensions().size(), /*symbolCount=*/0, {},\n+                     mlir_context),\n+      output_shape.dimensions(), /*symbol_upper_bounds=*/{});\n+}\n+\n+AffineMap ComputeTransposeIndexingMap(absl::Span<const int64_t> permutation,\n+                                      MLIRContext* mlir_context) {\n+  return AffineMap::getPermutationMap(\n+      std::vector<unsigned>(permutation.begin(), permutation.end()),\n+      mlir_context);\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "d6b84d0d0b82f95af7ad3d1d0920b35f6c789d43",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis_utils.h",
            "status": "added",
            "additions": 102,
            "deletions": 0,
            "changes": 102,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_utils.h?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -0,0 +1,102 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_ANALYSIS_INDEXING_ANALYSIS_UTILS_H_\n+#define XLA_HLO_ANALYSIS_INDEXING_ANALYSIS_UTILS_H_\n+\n+#include <cstdint>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/types/span.h\"\n+#include \"mlir/IR/AffineMap.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/shape.h\"\n+\n+namespace xla {\n+\n+struct HloInstructionIndexing;\n+\n+// Computes the indexing map for a Pad operation.\n+IndexingMap ComputePadIndexingMap(absl::Span<const int64_t> output_dims,\n+                                  absl::Span<const int64_t> padding_low,\n+                                  absl::Span<const int64_t> padding_high,\n+                                  absl::Span<const int64_t> padding_interior,\n+                                  mlir::MLIRContext* mlir_context);\n+\n+// Computes the indexing map for a window-based operation (e.g. ReduceWindow,\n+// Convolution).\n+IndexingMap ComposeWindowIndexingMap(absl::Span<const int64_t> input_dims,\n+                                     absl::Span<const int64_t> output_dims,\n+                                     absl::Span<const int64_t> window_dims,\n+                                     absl::Span<const int64_t> window_strides,\n+                                     absl::Span<const int64_t> window_dilations,\n+                                     absl::Span<const int64_t> base_dilations,\n+                                     absl::Span<const int64_t> padding,\n+                                     mlir::MLIRContext* mlir_context);\n+\n+// Creates an elementwise indexing for num_operands operands with the given\n+// output shape. All operands use an identity mapping.\n+HloInstructionIndexing CreateElementwiseIndexing(\n+    int64_t num_operands, const Shape& output_shape,\n+    mlir::MLIRContext* mlir_context);\n+\n+// Creates a scalar (empty) indexing map for the given output shape.\n+// Used for scalar operands like init values or padding values.\n+IndexingMap CreateScalarIndexingMap(const Shape& output_shape,\n+                                    mlir::MLIRContext* mlir_context);\n+\n+IndexingMap ComputeBroadcastIndexingMap(\n+    absl::Span<const int64_t> output_dims,\n+    absl::Span<const int64_t> broadcast_dims, mlir::MLIRContext* mlir_context);\n+\n+IndexingMap ComputeSliceIndexingMap(absl::Span<const int64_t> output_shape_dims,\n+                                    absl::Span<const int64_t> slice_starts,\n+                                    absl::Span<const int64_t> slice_strides,\n+                                    mlir::MLIRContext* mlir_context);\n+\n+IndexingMap ComputeReverseIndexingMap(\n+    absl::Span<const int64_t> output_shape_dims,\n+    absl::Span<const int64_t> reverse_dims, mlir::MLIRContext* mlir_context);\n+\n+mlir::AffineMap ComputeTransposeIndexingMap(\n+    absl::Span<const int64_t> permutation, mlir::MLIRContext* mlir_context);\n+\n+HloInstructionIndexing ComputeConcatenateIndexing(\n+    int64_t rank, int64_t concat_dim, absl::Span<const int64_t> output_dims,\n+    const std::vector<int64_t>& operand_concat_dim_sizes,\n+    mlir::MLIRContext* mlir_context);\n+\n+// Computes indexing maps for DotGeneral operands.\n+// Returns a pair of (lhs_indexing_map, rhs_indexing_map).\n+std::pair<IndexingMap, IndexingMap> ComputeDotOperandsIndexing(\n+    absl::Span<const int64_t> lhs_dims, absl::Span<const int64_t> rhs_dims,\n+    absl::Span<const int64_t> output_dims,\n+    absl::Span<const int64_t> lhs_batch_dims,\n+    absl::Span<const int64_t> rhs_batch_dims,\n+    absl::Span<const int64_t> lhs_contracting_dims,\n+    absl::Span<const int64_t> rhs_contracting_dims,\n+    mlir::MLIRContext* mlir_context);\n+\n+// Computes indexing map for reduce input operands.\n+IndexingMap ComputeReduceInputIndexingMap(absl::Span<const int64_t> input_dims,\n+                                          absl::Span<const int64_t> output_dims,\n+                                          absl::Span<const int64_t> reduce_dims,\n+                                          mlir::MLIRContext* mlir_context);\n+\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_ANALYSIS_INDEXING_ANALYSIS_UTILS_H_"
        },
        {
            "sha": "23e519deac9c01d1d38b11dac1f7372e11d4a5d9",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -35,8 +35,6 @@ limitations under the License.\n #include \"xla/hlo/analysis/interval.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n-#include \"tsl/platform/statusor.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace xla {\n namespace {\n@@ -1456,8 +1454,8 @@ TEST_F(IndexingMapTest, RangeVarSupportsAbslHashAndEqAndNe) {\n }\n \n TEST_F(IndexingMapTest, RTVarSupportsAbslHashAndEqAndNe) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> hlo_module,\n-                          ParseAndReturnVerifiedModule(R\"(\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> hlo_module,\n+                       ParseAndReturnVerifiedModule(R\"(\n                             HloModule m\n                             ENTRY e {\n                               ROOT %constant = s64[] constant(42)"
        },
        {
            "sha": "d14e6d621ce6e80cedc70fc945eabb999ff3cb35",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_test_utils.h",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.h?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -70,13 +70,19 @@ class IndexingTestBase : public HloHardwareIndependentTestBase {\n  public:\n   HloInstruction* ParseAndGetRoot(absl::string_view hlo_string);\n \n-  HloInstructionIndexing GetOutputToInputIndexing(\n-      const HloInstruction* instr, int output_id = 0,\n-      bool use_physical_layout = false);\n+  virtual HloInstructionIndexing GetOutputToInputIndexing(\n+      const HloInstruction* instr, int output_id, bool use_physical_layout);\n+  HloInstructionIndexing GetOutputToInputIndexing(const HloInstruction* instr,\n+                                                  int output_id = 0) {\n+    return GetOutputToInputIndexing(instr, output_id, false);\n+  }\n \n-  HloInstructionIndexing GetInputToOutputIndexing(\n-      const HloInstruction* instr, int input_id = 0,\n-      bool use_physical_layout = false);\n+  virtual HloInstructionIndexing GetInputToOutputIndexing(\n+      const HloInstruction* instr, int input_id, bool use_physical_layout);\n+  HloInstructionIndexing GetInputToOutputIndexing(const HloInstruction* instr,\n+                                                  int input_id = 0) {\n+    return GetInputToOutputIndexing(instr, input_id, false);\n+  }\n \n   mlir::MLIRContext mlir_context_;\n   std::unique_ptr<VerifiedHloModule> module_;"
        },
        {
            "sha": "ee547bb52b8ea94e6501c2d163458f09b8c9d1da",
            "filename": "third_party/xla/xla/hlo/analysis/stablehlo_indexing_analysis.cc",
            "status": "added",
            "additions": 1009,
            "deletions": 0,
            "changes": 1009,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fstablehlo_indexing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fstablehlo_indexing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fstablehlo_indexing_analysis.cc?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -0,0 +1,1009 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/analysis/stablehlo_indexing_analysis.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/MapVector.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/TypeSwitch.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"mlir/IR/AffineExpr.h\"\n+#include \"mlir/IR/AffineMap.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"  // IWYU pragma: keep\n+#include \"xla/hlo/analysis/indexing_analysis.h\"\n+#include \"xla/hlo/analysis/indexing_analysis_utils.h\"\n+#include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/analysis/interval.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"  // IWYU pragma: keep\n+#include \"xla/permutation_util.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+\n+namespace xla {\n+namespace {\n+\n+using namespace ::mlir::stablehlo;  // NOLINT\n+namespace mhlo = ::mlir::mhlo;\n+\n+using ::llvm::ArrayRef;\n+using ::llvm::enumerate;\n+using ::mlir::AffineExpr;\n+using ::mlir::AffineMap;\n+using ::mlir::BlockArgument;\n+using ::mlir::DenseIntElementsAttr;\n+using ::mlir::dyn_cast;\n+using ::mlir::MLIRContext;\n+using ::mlir::Operation;\n+using ::mlir::RankedTensorType;\n+using ::mlir::SmallVector;\n+using ::mlir::Value;\n+\n+HloInstructionIndexing CreateUnknownIndexing(int64_t count) {\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(count);\n+  for (int64_t i = 0; i < count; ++i) {\n+    indexing.indexing_maps[i].insert(\n+        OperandIndexing{IndexingMap::GetUndefined()});\n+  }\n+  return indexing;\n+}\n+\n+Shape GetShape(Value value) {\n+  auto shaped_type = dyn_cast<RankedTensorType>(value.getType());\n+  if (!shaped_type) {\n+    return Shape();\n+  }\n+  std::vector<int64_t> dimensions(shaped_type.getShape().begin(),\n+                                  shaped_type.getShape().end());\n+  return ShapeUtil::MakeShape(F32, dimensions);\n+}\n+\n+// Operation-specific helper implementations\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    AllGatherOp all_gather, int output_id) {\n+  MLIRContext* context = all_gather.getContext();\n+  int64_t all_gather_dim = all_gather.getAllGatherDim();\n+  auto output_shape = GetShape(all_gather.getResult(0));\n+  int64_t output_rank = output_shape.dimensions().size();\n+\n+  // Input shape for the first operand\n+  auto input_shape = GetShape(all_gather.getOperand(0));\n+  int64_t all_gather_input_dim_size = input_shape.dimensions(all_gather_dim);\n+\n+  std::vector<AffineExpr> exprs;\n+  exprs.reserve(output_rank);\n+\n+  for (int64_t i = 0; i < output_rank; ++i) {\n+    auto dim = mlir::getAffineDimExpr(i, context);\n+    exprs.push_back(i == all_gather_dim ? dim % all_gather_input_dim_size\n+                                        : dim);\n+  }\n+\n+  IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_rank, 0, exprs, context), output_shape.dimensions(),\n+      {});\n+\n+  AffineExpr replica_id_expr = mlir::getAffineDimExpr(all_gather_dim, context)\n+                                   .floorDiv(all_gather_input_dim_size);\n+\n+  IndexingMap replica_id_map = IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_rank, 0, replica_id_expr, context),\n+      output_shape.dimensions(), {});\n+\n+  OperandIndexing operand_indexing(indexing_map, {}, replica_id_map);\n+\n+  HloInstructionIndexing indexing;\n+  // HLO implementation only returns indexing for the first operand.\n+  // We mirror this behavior for consistency, although StableHLO ops might be\n+  // variadic.\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(operand_indexing);\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    BitcastConvertOp bitcast, int output_id) {\n+  MLIRContext* context = bitcast.getContext();\n+  auto input_shape = GetShape(bitcast.getOperand());\n+  auto output_shape = GetShape(bitcast.getResult());\n+  IndexingMap indexing_map = GetBitcastMap(output_shape, input_shape, context);\n+  indexing_map.Simplify();\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    BroadcastInDimOp bcast, int output_id) {\n+  MLIRContext* context = bcast.getContext();\n+  // Check if result has RankedTensorType\n+  if (!dyn_cast<RankedTensorType>(bcast.getResult().getType())) {\n+    return CreateUnknownIndexing(1);\n+  }\n+  auto output_shape = GetShape(bcast.getResult());\n+  IndexingMap indexing_map = ComputeBroadcastIndexingMap(\n+      output_shape.dimensions(), bcast.getBroadcastDimensions(), context);\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    ConcatenateOp concat, int output_id) {\n+  MLIRContext* context = concat.getContext();\n+  int64_t concat_dim = concat.getDimension();\n+  auto output_shape = GetShape(concat.getResult());\n+  std::vector<int64_t> operand_concat_dim_sizes;\n+  operand_concat_dim_sizes.reserve(concat.getInputs().size());\n+  for (Value operand : concat.getInputs()) {\n+    operand_concat_dim_sizes.push_back(\n+        GetShape(operand).dimensions(concat_dim));\n+  }\n+  return ComputeConcatenateIndexing(output_shape.dimensions().size(),\n+                                    concat_dim, output_shape.dimensions(),\n+                                    operand_concat_dim_sizes, context);\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    ConvolutionOp conv, int output_id) {\n+  MLIRContext* context = conv.getContext();\n+  auto input_shape = GetShape(conv.getLhs());\n+  auto kernel_shape = GetShape(conv.getRhs());\n+  auto output_shape = GetShape(conv.getResult());\n+  auto dnums = conv.getDimensionNumbers();\n+  size_t rank = output_shape.dimensions().size();\n+\n+  // Collect sizes for input/output spatial dimensions.\n+  size_t spatial_rank = dnums.getInputSpatialDimensions().size();\n+  std::vector<int64_t> input_spatial_sizes(spatial_rank);\n+  std::vector<int64_t> kernel_spatial_sizes(spatial_rank);\n+  std::vector<int64_t> output_spatial_sizes(spatial_rank);\n+  for (int i = 0; i < spatial_rank; ++i) {\n+    input_spatial_sizes[i] =\n+        input_shape.dimensions(dnums.getInputSpatialDimensions()[i]);\n+    kernel_spatial_sizes[i] =\n+        kernel_shape.dimensions(dnums.getKernelSpatialDimensions()[i]);\n+    output_spatial_sizes[i] =\n+        output_shape.dimensions(dnums.getOutputSpatialDimensions()[i]);\n+  }\n+\n+  SmallVector<int64_t> ones(spatial_rank, 1);\n+  auto strides = conv.getWindowStrides().value_or(ones);\n+  auto lhs_dilation = conv.getLhsDilation().value_or(ones);\n+  auto rhs_dilation = conv.getRhsDilation().value_or(ones);\n+  SmallVector<int64_t> padding_flat;\n+  if (conv.getPadding()) {\n+    for (auto val : conv.getPadding()->getValues<int64_t>()) {\n+      padding_flat.push_back(val);\n+    }\n+  } else {\n+    padding_flat.assign(spatial_rank * 2, 0);\n+  }\n+\n+  // Indexing map for the input value (spatial dimensions only).\n+  // The dimension numbers in the resulting affine expressions have to be\n+  // remapped to correspond to the correct output dimensions.\n+  IndexingMap input_spatial_indexing = ComposeWindowIndexingMap(\n+      input_spatial_sizes, output_spatial_sizes, kernel_spatial_sizes, strides,\n+      rhs_dilation, lhs_dilation, padding_flat, context);\n+  std::vector<AffineExpr> replacement_dims(spatial_rank);\n+  for (int i = 0; i < spatial_rank; ++i) {\n+    replacement_dims[i] =\n+        mlir::getAffineDimExpr(dnums.getOutputSpatialDimensions()[i], context);\n+  }\n+\n+  // Build affine expressions and constraints for input spatial dimensions.\n+  std::vector<AffineExpr> input_exprs(rank);\n+  for (int i = 0; i < spatial_rank; ++i) {\n+    input_exprs[dnums.getInputSpatialDimensions()[i]] =\n+        input_spatial_indexing.GetAffineMap().getResult(i).replaceDims(\n+            replacement_dims);\n+  }\n+  llvm::MapVector<AffineExpr, Interval> input_constraints;\n+  for (const auto& [key, val] : input_spatial_indexing.GetConstraints()) {\n+    input_constraints[key.replaceDims(replacement_dims)] = val;\n+  }\n+\n+  // Build affine expressions for kernel spatial and output dimensions.\n+  std::vector<AffineExpr> kernel_exprs(rank);\n+  for (int i = 0; i < spatial_rank; ++i) {\n+    kernel_exprs[dnums.getKernelSpatialDimensions()[i]] =\n+        mlir::getAffineSymbolExpr(i, context);\n+  }\n+  AffineExpr dim_expr =\n+      mlir::getAffineDimExpr(dnums.getOutputFeatureDimension(), context);\n+  kernel_exprs[dnums.getKernelOutputFeatureDimension()] = dim_expr;\n+\n+  // Build initial symbol ranges.\n+  std::vector<IndexingMap::Variable> input_symbols =\n+      input_spatial_indexing.GetRangeVars();\n+  std::vector<IndexingMap::Variable> kernel_symbols =\n+      RangeVarsFromTensorSizes(kernel_spatial_sizes);\n+\n+  // Add symbol for input feature dimension.\n+  input_exprs[dnums.getInputFeatureDimension()] =\n+      mlir::getAffineSymbolExpr(input_symbols.size(), context);\n+  kernel_exprs[dnums.getKernelInputFeatureDimension()] =\n+      mlir::getAffineSymbolExpr(kernel_symbols.size(), context);\n+\n+  int64_t input_group_size =\n+      kernel_shape.dimensions(dnums.getKernelInputFeatureDimension());\n+  Interval input_feature_range{0, input_group_size - 1};\n+  input_symbols.push_back(IndexingMap::Variable{input_feature_range});\n+  kernel_symbols.push_back(IndexingMap::Variable{input_feature_range});\n+\n+  // With multiple feature groups, the input feature dimension is equally split.\n+  if (conv.getFeatureGroupCount() > 1) {\n+    AffineExpr& input_feature = input_exprs[dnums.getInputFeatureDimension()];\n+    int64_t output_group_size =\n+        output_shape.dimensions(dnums.getOutputFeatureDimension());\n+    int64_t feature_group_size =\n+        output_group_size / conv.getFeatureGroupCount();\n+    input_feature = dim_expr.floorDiv(feature_group_size) * input_group_size +\n+                    input_feature;\n+  }\n+\n+  // With multiple batch groups, the input batch dimension is equally split.\n+  AffineExpr batch_dim_expr =\n+      mlir::getAffineDimExpr(dnums.getOutputBatchDimension(), context);\n+  if (conv.getBatchGroupCount() > 1) {\n+    int64_t batch_group_size =\n+        output_shape.dimensions(dnums.getOutputBatchDimension());\n+    AffineExpr batch_group_expr =\n+        mlir::getAffineSymbolExpr(input_symbols.size(), context);\n+    input_symbols.push_back(IndexingMap::Variable{\n+        {0, static_cast<int64_t>(conv.getBatchGroupCount()) - 1}});\n+    input_exprs[dnums.getInputBatchDimension()] =\n+        batch_group_expr * batch_group_size + batch_dim_expr;\n+  } else {\n+    input_exprs[dnums.getInputBatchDimension()] = batch_dim_expr;\n+  }\n+\n+  // Indexing map for the input value.\n+  IndexingMap inputs_indexing(\n+      AffineMap::get(rank, input_symbols.size(), input_exprs, context),\n+      DimVarsFromTensorSizes(output_shape.dimensions()), input_symbols,\n+      /*rt_vars=*/{}, input_constraints);\n+  // We may need to simplify and remove unused symbols again, as the input\n+  // feature dimension size may be trivial.\n+  inputs_indexing.Simplify();\n+  inputs_indexing.RemoveUnusedSymbols();\n+\n+  // Indexing map for the kernel value.\n+  IndexingMap kernel_indexing(\n+      AffineMap::get(rank, kernel_symbols.size(), kernel_exprs, context),\n+      DimVarsFromTensorSizes(output_shape.dimensions()), kernel_symbols,\n+      /*rt_vars=*/{});\n+  kernel_indexing.Simplify();\n+  kernel_indexing.RemoveUnusedSymbols();\n+\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(2);\n+  indexing.indexing_maps[0].insert(OperandIndexing{inputs_indexing});\n+  indexing.indexing_maps[1].insert(OperandIndexing{kernel_indexing});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    DotGeneralOp dot_general, int output_id) {\n+  MLIRContext* context = dot_general.getContext();\n+  auto lhs_shape = GetShape(dot_general.getLhs());\n+  auto rhs_shape = GetShape(dot_general.getRhs());\n+  auto output_shape = GetShape(dot_general.getResult());\n+  auto dim_numbers = dot_general.getDotDimensionNumbers();\n+\n+  auto lhs_batch_dims = dim_numbers.getLhsBatchingDimensions();\n+  auto rhs_batch_dims = dim_numbers.getRhsBatchingDimensions();\n+  auto lhs_contracting_dims = dim_numbers.getLhsContractingDimensions();\n+  auto rhs_contracting_dims = dim_numbers.getRhsContractingDimensions();\n+\n+  auto [lhs_map, rhs_map] = ComputeDotOperandsIndexing(\n+      lhs_shape.dimensions(), rhs_shape.dimensions(), output_shape.dimensions(),\n+      lhs_batch_dims, rhs_batch_dims, lhs_contracting_dims,\n+      rhs_contracting_dims, context);\n+\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(2);\n+  indexing.indexing_maps[0].insert(OperandIndexing{lhs_map});\n+  indexing.indexing_maps[1].insert(OperandIndexing{rhs_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    DotOp dot, int output_id) {\n+  MLIRContext* context = dot.getContext();\n+  auto lhs_shape = GetShape(dot.getLhs());\n+  auto rhs_shape = GetShape(dot.getRhs());\n+  auto output_shape = GetShape(dot.getResult());\n+\n+  // Following XLA's DotOp pattern:\n+  // For dot product: lhs[..., k] * rhs[k, ...] -> output[..., ...]\n+  // LHS: batch_dims + k (contracting)\n+  // RHS: k (contracting) + non_contracting\n+  int64_t lhs_rank = lhs_shape.dimensions().size();\n+  int64_t rhs_rank = rhs_shape.dimensions().size();\n+  int64_t output_rank = output_shape.dimensions().size();\n+\n+  llvm::SmallVector<AffineExpr> lhs_exprs(lhs_rank);\n+  llvm::SmallVector<AffineExpr> rhs_exprs(rhs_rank);\n+  // LHS non-contracting dimensions map to output dims [0, output_rank-1)\n+  // For vector-matrix or matrix-vector: this is either batch dims or empty\n+  for (int64_t i = 0; i < lhs_rank - 1; ++i) {\n+    lhs_exprs[i] = mlir::getAffineDimExpr(i, context);\n+  }\n+  // RHS non-contracting dimensions map to output dims starting after LHS\n+  // For matrix-vector: output_rank may be < rhs_rank-1 (vector result)\n+  for (int64_t i = 0; i < rhs_rank - 1; ++i) {\n+    int64_t output_dim = (lhs_rank - 1) + i;\n+    if (output_dim < output_rank) {\n+      rhs_exprs[i + 1] = mlir::getAffineDimExpr(output_dim, context);\n+    } else {\n+      // Matrix-vector case: result is vector, extra RHS dims are implicit\n+      rhs_exprs[i + 1] = mlir::getAffineConstantExpr(0, context);\n+    }\n+  }\n+\n+  // Contracting dimension (k): symbol for both LHS and RHS\n+  int64_t k_dim = lhs_shape.dimensions()[lhs_rank - 1];\n+  AffineExpr k_expr = mlir::getAffineSymbolExpr(0, context);\n+  lhs_exprs[lhs_rank - 1] = k_expr;\n+  rhs_exprs[0] = k_expr;\n+  IndexingMap lhs_map = IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_rank, 1, lhs_exprs, context),\n+      std::vector<int64_t>(output_shape.dimensions().begin(),\n+                           output_shape.dimensions().end()),\n+      {k_dim});\n+  IndexingMap rhs_map = IndexingMap::FromTensorSizes(\n+      AffineMap::get(output_rank, 1, rhs_exprs, context),\n+      std::vector<int64_t>(output_shape.dimensions().begin(),\n+                           output_shape.dimensions().end()),\n+      {k_dim});\n+\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(2);\n+  indexing.indexing_maps[0].insert(OperandIndexing{lhs_map});\n+  indexing.indexing_maps[1].insert(OperandIndexing{rhs_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    DynamicSliceOp dynamic_slice, int output_id) {\n+  MLIRContext* context = dynamic_slice.getContext();\n+  auto input_shape = GetShape(dynamic_slice.getOperand());\n+  auto output_shape = GetShape(dynamic_slice.getResult());\n+  int64_t rank = output_shape.dimensions().size();\n+\n+  std::vector<int64_t> dim_sizes(output_shape.dimensions().begin(),\n+                                 output_shape.dimensions().end());\n+  std::vector<IndexingMap::Variable> dim_vars;\n+  dim_vars.reserve(dim_sizes.size());\n+  for (auto size : dim_sizes) {\n+    dim_vars.push_back(IndexingMap::Variable{{0, size - 1}});\n+  }\n+\n+  std::vector<AffineExpr> exprs;\n+  exprs.reserve(rank);\n+  std::vector<IndexingMap::Variable> rt_vars;\n+  std::vector<RuntimeVarIndexing> runtime_vars;\n+\n+  // An empty affine map for scalar runtime variables.\n+  // Needed for indices_map construction below\n+  AffineMap empty_map = AffineMap::get(rank, 0, {}, context);\n+\n+  for (auto [dim, slice_size] :\n+       llvm::enumerate(dynamic_slice.getSliceSizes())) {\n+    AffineExpr dim_expr = getAffineDimExpr(dim, context);\n+    Value rt_var_val = dynamic_slice.getStartIndices()[dim];\n+    int64_t max_index = input_shape.dimensions(dim) - slice_size;\n+\n+    // Construct indexing map for the start index (scalar map keyed by output\n+    // dimensions) We reuse the scalar map logic: (d0...dN) -> ()\n+    IndexingMap rt_index_map = CreateScalarIndexingMap(output_shape, context);\n+\n+    // Attempt constant folding/optimization\n+    RuntimeVarIndexing rt_indexing{rt_var_val, rt_index_map};\n+    Interval feasible_values{0, max_index};\n+\n+    auto simplified_expr = OptimizeRTVar(rt_indexing, feasible_values, context);\n+\n+    if (simplified_expr) {\n+      exprs.push_back(dim_expr + *simplified_expr);\n+    } else {\n+      exprs.push_back(dim_expr + getAffineSymbolExpr(rt_vars.size(), context));\n+      rt_vars.push_back(IndexingMap::Variable{{0, max_index}});\n+      runtime_vars.push_back(RuntimeVarIndexing{rt_var_val, rt_index_map});\n+    }\n+  }\n+\n+  IndexingMap input_map{AffineMap::get(rank, rt_vars.size(), exprs, context),\n+                        dim_vars,\n+                        {},\n+                        rt_vars};\n+\n+  OperandIndexing operand_indexing{input_map, runtime_vars};\n+\n+  IndexingMap indices_map =\n+      IndexingMap::FromTensorSizes(empty_map, dim_sizes, {});\n+\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(dynamic_slice.getNumOperands());\n+  indexing.indexing_maps[0].insert(operand_indexing);\n+  for (size_t i = 1; i < dynamic_slice.getNumOperands(); ++i) {\n+    indexing.indexing_maps[i].insert(OperandIndexing{indices_map});\n+  }\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    DynamicUpdateSliceOp dus, int output_id) {\n+  MLIRContext* context = dus.getContext();\n+  auto operand_shape = GetShape(dus.getOperand());\n+  auto update_shape = GetShape(dus.getUpdate());\n+  auto output_shape = GetShape(dus.getResult());\n+  int64_t rank = output_shape.dimensions().size();\n+\n+  // Operand (input): identity mapping\n+  std::vector<AffineExpr> identity;\n+  identity.reserve(rank);\n+  for (int64_t dim = 0; dim < rank; ++dim) {\n+    identity.push_back(getAffineDimExpr(dim, context));\n+  }\n+  std::vector<int64_t> dim_sizes(output_shape.dimensions().begin(),\n+                                 output_shape.dimensions().end());\n+  IndexingMap operand_map = IndexingMap::FromTensorSizes(\n+      AffineMap::get(rank, 0, identity, context), dim_sizes, {});\n+\n+  // Update: (d0 - rt0, ..., d{N-1} - rt{N-1}) with runtime variables\n+  std::vector<AffineExpr> update_exprs;\n+  std::vector<IndexingMap::Variable> rt_vars;\n+  update_exprs.reserve(rank);\n+  rt_vars.reserve(rank);\n+\n+  for (int64_t dim = 0; dim < rank; ++dim) {\n+    update_exprs.push_back(getAffineDimExpr(dim, context) -\n+                           getAffineSymbolExpr(dim, context));\n+    rt_vars.push_back(IndexingMap::Variable{\n+        {0, operand_shape.dimensions(dim) - update_shape.dimensions(dim)}});\n+  }\n+\n+  std::vector<IndexingMap::Variable> dim_vars;\n+  dim_vars.reserve(dim_sizes.size());\n+  for (auto size : dim_sizes) {\n+    dim_vars.push_back(IndexingMap::Variable{{0, size - 1}});\n+  }\n+\n+  IndexingMap update_map{\n+      AffineMap::get(rank, rank, update_exprs, context), dim_vars, {}, rt_vars};\n+\n+  // Create RuntimeVarIndexing for offset operands\n+  std::vector<RuntimeVarIndexing> runtime_vars;\n+  runtime_vars.reserve(rank);\n+  AffineMap empty_map = AffineMap::get(rank, 0, {}, context);\n+  IndexingMap rt_index_map =\n+      IndexingMap::FromTensorSizes(empty_map, dim_sizes, {});\n+\n+  for (auto offset_value : dus.getStartIndices()) {\n+    runtime_vars.push_back(RuntimeVarIndexing{offset_value, rt_index_map});\n+  }\n+\n+  OperandIndexing update_indexing{update_map, runtime_vars};\n+\n+  // Start indices: empty map\n+  IndexingMap indices_map =\n+      IndexingMap::FromTensorSizes(empty_map, dim_sizes, {});\n+\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(dus.getNumOperands());\n+  indexing.indexing_maps[0].insert(OperandIndexing{operand_map});\n+  indexing.indexing_maps[1].insert(update_indexing);\n+  for (size_t i = 2; i < dus.getNumOperands(); ++i) {\n+    indexing.indexing_maps[i].insert(OperandIndexing{indices_map});\n+  }\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    GatherOp gather, int output_id) {\n+  MLIRContext* context = gather.getContext();\n+  auto operand_shape = GetShape(gather.getOperand());\n+  auto start_indices_shape = GetShape(gather.getStartIndices());\n+  auto output_shape = GetShape(gather.getResult());\n+  int64_t output_rank = output_shape.dimensions().size();\n+\n+  auto dimension_numbers = gather.getDimensionNumbers();\n+  int64_t index_vector_dim = dimension_numbers.getIndexVectorDim();\n+  int64_t index_vector_length =\n+      start_indices_shape.dimensions(index_vector_dim);\n+\n+  // Map for indices operand: (d0, ..., d{rank-1}) -> (d0, s0)\n+  // where s0 ranges over index vector dimension\n+  AffineExpr indices_id_dim = getAffineDimExpr(0, context);\n+  std::vector<int64_t> dim_sizes(output_shape.dimensions().begin(),\n+                                 output_shape.dimensions().end());\n+  std::vector<IndexingMap::Variable> dim_vars;\n+  dim_vars.reserve(dim_sizes.size());\n+  for (auto size : dim_sizes) {\n+    dim_vars.push_back(IndexingMap::Variable{{0, size - 1}});\n+  }\n+\n+  IndexingMap indices_map{\n+      AffineMap::get(output_rank, 1,\n+                     {indices_id_dim, getAffineSymbolExpr(0, context)},\n+                     context),\n+      dim_vars,\n+      {IndexingMap::Variable{{0, index_vector_length - 1}}},\n+      /*rt_vars=*/{}};\n+\n+  // Map for operand with runtime variables\n+  std::vector<AffineExpr> exprs;\n+  std::vector<RuntimeVarIndexing> runtime_vars;\n+  std::vector<IndexingMap::Variable> rt_vars;\n+  auto slice_sizes = gather.getSliceSizes();\n+  auto offset_dims = dimension_numbers.getOffsetDims();\n+  auto start_index_map = dimension_numbers.getStartIndexMap();\n+\n+  exprs.reserve(operand_shape.dimensions().size());\n+\n+  for (auto [operand_dim_id, slice_size] : enumerate(slice_sizes)) {\n+    int64_t output_dim_id = offset_dims[operand_dim_id];\n+    exprs.push_back(mlir::getAffineDimExpr(output_dim_id, context));\n+\n+    // Check if this dimension is indexed by start_indices\n+    auto it = absl::c_find(start_index_map, operand_dim_id);\n+    if (it == start_index_map.end()) {\n+      continue;\n+    }\n+\n+    int64_t start_index_map_idx = it - start_index_map.begin();\n+\n+    // Create runtime variable for this index\n+    AffineMap rt_var_map = AffineMap::get(\n+        output_rank, 0,\n+        {indices_id_dim,\n+         mlir::getAffineConstantExpr(start_index_map_idx, context)},\n+        context);\n+\n+    IndexingMap rt_index_map =\n+        IndexingMap::FromTensorSizes(rt_var_map, dim_sizes, {});\n+\n+    int64_t upper_bound = operand_shape.dimensions(operand_dim_id) - slice_size;\n+\n+    RuntimeVarIndexing rt_indexing{gather.getStartIndices(), rt_index_map};\n+    Interval feasible_values{0, upper_bound};\n+\n+    if (auto simplified =\n+            OptimizeRTVar(rt_indexing, feasible_values, context)) {\n+      exprs.back() = exprs.back() + *simplified;\n+      continue;\n+    }\n+\n+    runtime_vars.push_back(rt_indexing);\n+    rt_vars.push_back(IndexingMap::Variable{{0, upper_bound}});\n+\n+    // Add runtime variable to expression\n+    exprs.back() = exprs.back() +\n+                   mlir::getAffineSymbolExpr(runtime_vars.size() - 1, context);\n+  }\n+\n+  IndexingMap operand_map{\n+      AffineMap::get(output_rank, runtime_vars.size(), exprs, context),\n+      dim_vars,\n+      {},\n+      rt_vars};\n+\n+  OperandIndexing operand_indexing{operand_map, runtime_vars};\n+\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(2);\n+  indexing.indexing_maps[0].insert(operand_indexing);\n+  indexing.indexing_maps[1].insert(OperandIndexing{indices_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    GetTupleElementOp gte, int output_id) {\n+  if (!dyn_cast<RankedTensorType>(gte.getResult().getType())) {\n+    return CreateUnknownIndexing(1);\n+  }\n+  auto output_shape = GetShape(gte.getResult());\n+  IndexingMap identity_map = IndexingMap::FromTensorSizes(\n+      AffineMap::getMultiDimIdentityMap(output_shape.dimensions().size(),\n+                                        gte.getContext()),\n+      std::vector<int64_t>(output_shape.dimensions().begin(),\n+                           output_shape.dimensions().end()),\n+      {});\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{identity_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    PadOp pad, int output_id) {\n+  MLIRContext* context = pad.getContext();\n+  auto output_shape = GetShape(pad.getResult());\n+  auto edge_padding_low = pad.getEdgePaddingLow();\n+  auto edge_padding_high = pad.getEdgePaddingHigh();\n+  auto interior_padding = pad.getInteriorPadding();\n+  IndexingMap input_indexing_map =\n+      ComputePadIndexingMap(output_shape.dimensions(), edge_padding_low,\n+                            edge_padding_high, interior_padding, context);\n+  IndexingMap padding_value_indexing_map =\n+      CreateScalarIndexingMap(output_shape, context);\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(2);\n+  indexing.indexing_maps[0].insert(OperandIndexing{input_indexing_map});\n+  indexing.indexing_maps[1].insert(OperandIndexing{padding_value_indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    ReduceOp reduce, int output_id) {\n+  MLIRContext* context = reduce.getContext();\n+\n+  auto input_shape = GetShape(reduce.getInputs()[0]);\n+  auto output_shape = GetShape(reduce.getResults()[0]);\n+\n+  IndexingMap inputs_indexing_map = ComputeReduceInputIndexingMap(\n+      input_shape.dimensions(), output_shape.dimensions(),\n+      reduce.getDimensions(), context);\n+\n+  IndexingMap inits_indexing_map =\n+      CreateScalarIndexingMap(output_shape, context);\n+\n+  HloInstructionIndexing indexing;\n+  int64_t num_inputs = reduce.getInputs().size();\n+  int64_t num_operands = num_inputs + reduce.getInitValues().size();\n+  indexing.indexing_maps.resize(num_operands);\n+\n+  for (int64_t id = 0; id < num_inputs; ++id) {\n+    indexing.indexing_maps[id].insert(OperandIndexing(inputs_indexing_map));\n+  }\n+  for (int64_t id = num_inputs; id < num_operands; ++id) {\n+    indexing.indexing_maps[id].insert(OperandIndexing(inits_indexing_map));\n+  }\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    ReduceWindowOp reduce_window, int output_id) {\n+  MLIRContext* context = reduce_window.getContext();\n+\n+  // Following XLA's ReduceWindowOp pattern:\n+  // Indexing for reduce-window with dilations and non-trivial padding\n+  // is represented as a composition using ComposeWindowIndexingMap\n+\n+  auto input_shape = GetShape(reduce_window.getInputs()[0]);\n+  auto output_shape = GetShape(reduce_window.getResults()[0]);\n+\n+  SmallVector<int64_t> default_dilations(input_shape.dimensions().size(), 1);\n+  SmallVector<int64_t> default_padding(input_shape.dimensions().size() * 2, 0);\n+\n+  ArrayRef<int64_t> window_dilations =\n+      reduce_window.getWindowDilations()\n+          ? ArrayRef<int64_t>(*reduce_window.getWindowDilations())\n+          : ArrayRef(default_dilations);\n+  ArrayRef<int64_t> base_dilations =\n+      reduce_window.getBaseDilations()\n+          ? ArrayRef<int64_t>(*reduce_window.getBaseDilations())\n+          : ArrayRef(default_dilations);\n+\n+  SmallVector<int64_t> padding_flat;\n+  if (reduce_window.getPadding()) {\n+    auto padding_attr = reduce_window.getPadding().value();\n+    for (auto val : padding_attr.getValues<int64_t>()) {\n+      padding_flat.push_back(val);\n+    }\n+  } else {\n+    padding_flat = default_padding;\n+  }\n+\n+  // Indexing map for the input value\n+  IndexingMap inputs_indexing = ComposeWindowIndexingMap(\n+      input_shape.dimensions(), output_shape.dimensions(),\n+      reduce_window.getWindowDimensions(),\n+      reduce_window.getWindowStrides().value_or(\n+          reduce_window.getWindowDimensions()),\n+      window_dilations, base_dilations, padding_flat, context);\n+\n+  // Indexing map for the init value\n+  IndexingMap inits_indexing_map =\n+      CreateScalarIndexingMap(output_shape, context);\n+\n+  HloInstructionIndexing indexing;\n+  int64_t num_inputs = reduce_window.getInputs().size();\n+  int64_t num_operands = num_inputs + reduce_window.getInitValues().size();\n+  indexing.indexing_maps.resize(num_operands);\n+\n+  for (int64_t id = 0; id < num_inputs; ++id) {\n+    indexing.indexing_maps[id].insert(OperandIndexing(inputs_indexing));\n+  }\n+  for (int64_t id = num_inputs; id < num_operands; ++id) {\n+    indexing.indexing_maps[id].insert(OperandIndexing(inits_indexing_map));\n+  }\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    ReshapeOp reshape, int output_id) {\n+  MLIRContext* context = reshape.getContext();\n+  auto input_shape = GetShape(reshape.getOperand());\n+  auto output_shape = GetShape(reshape.getResult());\n+  IndexingMap indexing_map = GetBitcastMap(output_shape, input_shape, context);\n+  indexing_map.Simplify();\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    ReverseOp reverse, int output_id) {\n+  MLIRContext* context = reverse.getContext();\n+  auto output_shape = GetShape(reverse.getResult());\n+  IndexingMap indexing_map = ComputeReverseIndexingMap(\n+      output_shape.dimensions(), reverse.getDimensions(), context);\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    SliceOp slice, int output_id) {\n+  MLIRContext* context = slice.getContext();\n+  auto output_shape = GetShape(slice.getResult());\n+  IndexingMap indexing_map = ComputeSliceIndexingMap(\n+      output_shape.dimensions(), slice.getStartIndices(), slice.getStrides(),\n+      context);\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    TransposeOp transpose, int output_id) {\n+  MLIRContext* context = transpose.getContext();\n+  auto output_shape = GetShape(transpose.getResult());\n+  auto permutation = std::vector<int64_t>(transpose.getPermutation().begin(),\n+                                          transpose.getPermutation().end());\n+  IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n+      ComputeTransposeIndexingMap(InversePermutation(permutation), context),\n+      output_shape.dimensions(), {});\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    TupleOp tuple_op, int output_id) {\n+  MLIRContext* context = tuple_op.getContext();\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(tuple_op->getNumOperands());\n+  for (auto [i, operand] : enumerate(tuple_op->getOperands())) {\n+    if (!dyn_cast<RankedTensorType>(operand.getType())) {\n+      continue;\n+    }\n+    auto operand_shape = GetShape(operand);\n+    IndexingMap identity_map = IndexingMap::FromTensorSizes(\n+        AffineMap::getMultiDimIdentityMap(operand_shape.dimensions().size(),\n+                                          context),\n+        std::vector<int64_t>(operand_shape.dimensions().begin(),\n+                             operand_shape.dimensions().end()),\n+        {});\n+    indexing.indexing_maps[i].insert(OperandIndexing{identity_map});\n+  }\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    mlir::mhlo::BitcastOp op, int output_id) {\n+  Shape input_shape = GetShape(op.getOperand());\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"source_layout\")) {\n+    std::vector<int64_t> layout;\n+    for (const auto& val : attr.getValues<int64_t>()) {\n+      layout.push_back(val);\n+    }\n+    *input_shape.mutable_layout() = LayoutUtil::MakeLayout(layout);\n+  }\n+\n+  Shape output_shape = GetShape(op.getResult());\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"result_layout\")) {\n+    std::vector<int64_t> layout;\n+    for (const auto& val : attr.getValues<int64_t>()) {\n+      layout.push_back(val);\n+    }\n+    *output_shape.mutable_layout() = LayoutUtil::MakeLayout(layout);\n+  }\n+  IndexingMap indexing_map =\n+      GetBitcastMap(output_shape, input_shape, op.getContext());\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{indexing_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    mhlo::CopyOp op, int output_id) {\n+  auto output_shape = GetShape(op.getResult());\n+  IndexingMap identity_map = IndexingMap::FromTensorSizes(\n+      AffineMap::getMultiDimIdentityMap(output_shape.dimensions().size(),\n+                                        op.getContext()),\n+      std::vector<int64_t>(output_shape.dimensions().begin(),\n+                           output_shape.dimensions().end()),\n+      {});\n+  HloInstructionIndexing indexing;\n+  indexing.indexing_maps.resize(1);\n+  indexing.indexing_maps[0].insert(OperandIndexing{identity_map});\n+  return indexing;\n+}\n+\n+[[maybe_unused]] HloInstructionIndexing ComputeOutputToInputIndexingImpl(\n+    mhlo::FusionOp op, int output_id) {\n+  auto& region = op.getRegion();\n+  if (region.empty()) {\n+    return CreateUnknownIndexing(op.getNumOperands());\n+  }\n+\n+  auto& block = region.front();\n+  auto terminator = block.getTerminator();\n+  if (output_id >= terminator->getNumOperands()) {\n+    return CreateUnknownIndexing(op.getNumOperands());\n+  }\n+\n+  HloInstructionIndexing fusion_indexing;\n+  fusion_indexing.indexing_maps.resize(op.getNumOperands());\n+\n+  struct WorkItem {\n+    Value value;\n+    OperandIndexing indexing;\n+  };\n+  std::vector<WorkItem> worklist;\n+\n+  // Start with the result of the fusion corresponding to output_id\n+  Value root_val = terminator->getOperand(output_id);\n+  Shape root_shape = GetShape(root_val);\n+  int64_t rank = root_shape.dimensions().size();\n+\n+  IndexingMap identity_map = IndexingMap::FromTensorSizes(\n+      AffineMap::getMultiDimIdentityMap(rank, op.getContext()),\n+      std::vector<int64_t>(root_shape.dimensions().begin(),\n+                           root_shape.dimensions().end()),\n+      {});\n+  worklist.push_back({root_val, OperandIndexing{identity_map}});\n+\n+  while (!worklist.empty()) {\n+    auto [val, current_indexing] = worklist.back();\n+    worklist.pop_back();\n+\n+    if (current_indexing.IsUndefined()) {\n+      // Propagate undefined?\n+    }\n+\n+    if (auto block_arg = dyn_cast<BlockArgument>(val)) {\n+      if (block_arg.getOwner() == &block) {\n+        int arg_idx = block_arg.getArgNumber();\n+        if (arg_idx < fusion_indexing.indexing_maps.size()) {\n+          fusion_indexing.indexing_maps[arg_idx].insert(current_indexing);\n+        }\n+      }\n+      continue;\n+    }\n+\n+    Operation* producer = val.getDefiningOp();\n+    if (!producer) {\n+      continue;\n+    }\n+\n+    // Recursive call to handle internal op\n+    int producer_result_idx = llvm::cast<mlir::OpResult>(val).getResultNumber();\n+    auto producer_indexing =\n+        ComputeOutputToInputIndexing(producer, producer_result_idx);\n+\n+    for (size_t i = 0; i < producer->getNumOperands(); ++i) {\n+      Value operand = producer->getOperand(i);\n+      for (const auto& operand_indexing : producer_indexing.indexing_maps[i]) {\n+        if (operand_indexing.IsUndefined() || current_indexing.IsUndefined()) {\n+          worklist.push_back(\n+              {operand, OperandIndexing{IndexingMap::GetUndefined()}});\n+          continue;\n+        }\n+        // Note: ComposeOperandIndexing order is (Inner, Outer) aka (Consumer,\n+        // Producer) to compute Outer(Inner(x)).\n+        OperandIndexing composed =\n+            ComposeOperandIndexing(current_indexing, operand_indexing);\n+        if (!composed.IsUndefined()) {\n+          composed.Simplify();\n+          composed.RemoveUnusedSymbols();\n+        }\n+        worklist.push_back({operand, composed});\n+      }\n+    }\n+  }\n+  return fusion_indexing;\n+}\n+\n+}  // namespace\n+\n+HloInstructionIndexing ComputeOutputToInputIndexing(Operation* op,\n+                                                    int output_id) {\n+  MLIRContext* context = op->getContext();\n+  HloInstructionIndexing indexing =\n+      llvm::TypeSwitch<Operation*, HloInstructionIndexing>(op)\n+          // Operations with extracted helpers.\n+          .Case<AllGatherOp, BitcastConvertOp, BroadcastInDimOp, ConcatenateOp,\n+                ConvolutionOp, DotOp, DotGeneralOp, DynamicSliceOp,\n+                DynamicUpdateSliceOp, GatherOp, GetTupleElementOp, PadOp,\n+                ReduceOp, ReduceWindowOp, ReshapeOp, ReverseOp, SliceOp,\n+                TransposeOp, TupleOp,\n+                // MHLO ops.\n+                mhlo::BitcastOp, mhlo::CopyOp, mhlo::FusionOp>(\n+              [&](auto typed_op) {\n+                return ComputeOutputToInputIndexingImpl(typed_op, output_id);\n+              })\n+\n+          // Elementwise identity operations, all operands use identity mapping.\n+          .Case<AddOp, SubtractOp, MulOp, DivOp, RemOp, MaxOp, MinOp, AndOp,\n+                OrOp, XorOp, AbsOp, NegOp, SignOp, CosineOp, SineOp, TanhOp,\n+                SqrtOp, RsqrtOp, ExpOp, Expm1Op, LogOp, Log1pOp, FloorOp,\n+                CeilOp, ConvertOp, SelectOp, ClampOp, CompareOp,\n+                PopulationCountOp, NotOp, IsFiniteOp, RoundNearestEvenOp,\n+                OptimizationBarrierOp, MapOp, SortOp>([&](Operation* op) {\n+            if (!dyn_cast<RankedTensorType>(op->getResult(0).getType())) {\n+              return CreateUnknownIndexing(op->getNumOperands());\n+            }\n+            auto output_shape = GetShape(op->getResult(0));\n+            HloInstructionIndexing indexing = CreateElementwiseIndexing(\n+                op->getNumOperands(), output_shape, context);\n+            // Handle scalar broadcast for operands with no dimensions\n+            for (auto [i, operand] : llvm::enumerate(op->getOperands())) {\n+              if (GetShape(operand).dimensions().empty()) {\n+                indexing.indexing_maps[i].clear();\n+                indexing.indexing_maps[i].insert(OperandIndexing{\n+                    CreateScalarIndexingMap(output_shape, context)});\n+              }\n+            }\n+            return indexing;\n+          })\n+\n+          // Default:\n+          //  - IotaOp, ConstantOp, CreateTokenOp, AfterAllOp\n+          //  - unknown indexing for unsupported operations\n+          .Default([&](Operation* op) {\n+            return CreateUnknownIndexing(op->getNumOperands());\n+          });\n+  return indexing;\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "b5fca80c1bc70e5ae2eed78aa8abcca8c4d9c95b",
            "filename": "third_party/xla/xla/hlo/analysis/stablehlo_indexing_analysis.h",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fstablehlo_indexing_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/389c77fa7a92ba7e7cc40e57ce9df88e45dad48d/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fstablehlo_indexing_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fstablehlo_indexing_analysis.h?ref=389c77fa7a92ba7e7cc40e57ce9df88e45dad48d",
            "patch": "@@ -0,0 +1,29 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_ANALYSIS_STABLEHLO_INDEXING_ANALYSIS_H_\n+#define XLA_HLO_ANALYSIS_STABLEHLO_INDEXING_ANALYSIS_H_\n+\n+#include \"mlir/IR/Operation.h\"\n+\n+namespace xla {\n+struct HloInstructionIndexing;\n+\n+HloInstructionIndexing ComputeOutputToInputIndexing(mlir::Operation* op,\n+                                                    int output_id);\n+\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_ANALYSIS_STABLEHLO_INDEXING_ANALYSIS_H_"
        }
    ],
    "stats": {
        "total": 2430,
        "additions": 1993,
        "deletions": 437
    }
}