{
    "author": "ZixuanJiang",
    "message": "Refactor CMV1 in spmd/dot_handler.\n\nPiperOrigin-RevId: 817313607",
    "sha": "127937ccb022d16aeb2890c264bccfaf035fe058",
    "files": [
        {
            "sha": "1ae8c89b525937b8505668ac9a7753158238b8e7",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 29,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/127937ccb022d16aeb2890c264bccfaf035fe058/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/127937ccb022d16aeb2890c264bccfaf035fe058/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=127937ccb022d16aeb2890c264bccfaf035fe058",
            "patch": "@@ -37,12 +37,14 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/array.h\"\n #include \"xla/comparison_util.h\"\n #include \"xla/hlo/analysis/hlo_reachability.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/ir/replica_group.h\"\n@@ -136,7 +138,7 @@ std::pair<Shape, Shape> GetPerGroupBaseShape(\n class CreateShardedDotFunctor final\n     : public CreateShardedFunctorBase<PartitionedHlo> {\n  public:\n-  CreateShardedDotFunctor(HloDotInstruction* dot) : dot_(dot) {}\n+  explicit CreateShardedDotFunctor(HloDotInstruction* dot) : dot_(dot) {}\n \n   // Implements the creation of sharded dots.\n   absl::StatusOr<HloInstruction*> CreateSharded(\n@@ -343,19 +345,17 @@ bool RequiresTransposeSharding(\n bool should_enable_windowed_einsum_with_threshold(\n     const SpmdPartitionerOptions& options, const HloInstruction* lhs,\n     const HloInstruction* rhs, int64_t operand_or_output_shape_size) {\n-  if (options.total_bytes_windowed_einsum_threshold != std::nullopt) {\n+  if (options.total_bytes_windowed_einsum_threshold) {\n     if (lhs == nullptr || rhs == nullptr) {\n       return false;\n     }\n     int64_t total_operand_bytes = (ShapeUtil::ByteSizeOf(rhs->shape()) +\n                                    ShapeUtil::ByteSizeOf(lhs->shape()));\n-    int64_t operand_bytes_threshold =\n-        options.total_bytes_windowed_einsum_threshold.value();\n-    return total_operand_bytes >= operand_bytes_threshold;\n-  } else {\n-    return operand_or_output_shape_size >=\n-           options.threshold_for_windowed_einsum_mib * 1024 * 1024;\n+    return total_operand_bytes >=\n+           *options.total_bytes_windowed_einsum_threshold;\n   }\n+  return operand_or_output_shape_size >=\n+         options.threshold_for_windowed_einsum_mib * 1024 * 1024;\n }\n \n template <typename CreateShardedFunctor>\n@@ -1949,9 +1949,9 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n     }\n   }\n \n-  std::optional<WindowedEinsumConfig> e_config = std::nullopt;\n   // Disable windowed einsums for block-scaled dot.\n   if constexpr (std::is_same_v<PartitionedHloMaybeMX, PartitionedHlo>) {\n+    std::optional<WindowedEinsumConfig> e_config = std::nullopt;\n     if (!should_skip_windowed_einsum) {\n       e_config = GetWindowedEinsumConfiguration<CreateShardedFunctor>(\n           num_partitions, output_lhs_non_contracting_partitions,\n@@ -1977,27 +1977,17 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n       if (e_config->windowing_dims.empty()) {\n         loop_partitions = num_partitions;\n       }\n-      if (e_config) {\n-        int64_t loop_partitions = 1;\n-        for (int64_t dim : e_config->windowing_dims) {\n-          loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n-        }\n-        if (e_config->windowing_dims.empty()) {\n-          loop_partitions = num_partitions;\n-        }\n \n-        VLOG(2) << \"Emit windowed dot.\";\n-        return EmitWindowedDotGeneral(\n-            lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n-            num_partitions, loop_partitions, create_sharded_dot, conv_window,\n-            module, original_hlo, options, b, windowed_dot_general_loops,\n-            *e_config, indices_map, lhs_sharding_transposed_to_match_output,\n-            rhs_sharding_transposed_to_match_output,\n-            rhs_sharding_transposed_to_match_lhs,\n-            lhs_sharding_transposed_to_match_rhs,\n-            output_sharding_transposed_to_match_rhs,\n-            output_sharding_transposed_to_match_lhs);\n-      }\n+      return EmitWindowedDotGeneral(\n+          lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n+          num_partitions, loop_partitions, create_sharded_dot, conv_window,\n+          module, original_hlo, options, b, windowed_dot_general_loops,\n+          *e_config, indices_map, lhs_sharding_transposed_to_match_output,\n+          rhs_sharding_transposed_to_match_output,\n+          rhs_sharding_transposed_to_match_lhs,\n+          lhs_sharding_transposed_to_match_rhs,\n+          output_sharding_transposed_to_match_rhs,\n+          output_sharding_transposed_to_match_lhs);\n     }\n   }\n "
        }
    ],
    "stats": {
        "total": 48,
        "additions": 19,
        "deletions": 29
    }
}