{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Fix channel_ids and use_global_device_ids in RaggedAllToAllMultiHostDecomposer.\n\nThe decomposer now uses `NextChannelId` to ensure unique channel IDs for the generated collectives. Additionally, set `use_global_device_ids=true` for `all-gather` to correctly work in cross-partition case.\n\nPiperOrigin-RevId: 817220017",
    "sha": "850cea882a2dc719f9f3edda317436ccf9106a11",
    "files": [
        {
            "sha": "707db5d658fc742e1c680f968f0011f386971a62",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/850cea882a2dc719f9f3edda317436ccf9106a11/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/850cea882a2dc719f9f3edda317436ccf9106a11/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=850cea882a2dc719f9f3edda317436ccf9106a11",
            "patch": "@@ -3214,6 +3214,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/hlo/utils:hlo_query\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "67c07ae03115310aaecf0f9c8c8809666bee25d9",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/850cea882a2dc719f9f3edda317436ccf9106a11/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/850cea882a2dc719f9f3edda317436ccf9106a11/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc?ref=850cea882a2dc719f9f3edda317436ccf9106a11",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/replica_group.h\"\n+#include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -41,6 +42,7 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n+using hlo_query::NextChannelId;\n \n // Exchanges the metadata between the hosts and computes the intra-host\n // metadata.\n@@ -71,7 +73,7 @@ HloInstruction* GetIntraHostMetadata(\n           /*operands=*/{new_input_offsets},\n           /*device_list=*/CollectiveDeviceList(replica_groups),\n           /*constrain_layout=*/false,\n-          /*channel_id=*/ragged_all_to_all->channel_id(),\n+          /*channel_id=*/NextChannelId(*computation->parent()),\n           /*split_dimension=*/0));\n \n   if (correct_offsets) {\n@@ -199,8 +201,8 @@ absl::StatusOr<bool> DecomposeRaggedAllToAll(\n           /*all_gather_dimension=*/0,\n           /*device_list=*/CollectiveDeviceList(inter_host_replica_groups),\n           /*constrain_layout=*/false,\n-          /*channel_id=*/ragged_all_to_all->channel_id(),\n-          /*use_global_device_ids=*/false));\n+          /*channel_id=*/NextChannelId(*computation->parent()),\n+          /*use_global_device_ids=*/true));\n \n   for (int i = 2; i < 6; ++i) {\n     intra_host_metadata.push_back(GetIntraHostMetadata("
        },
        {
            "sha": "7b4b3457875e06b88e8ace14fc6e193f62fb6d14",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 4,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/850cea882a2dc719f9f3edda317436ccf9106a11/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/850cea882a2dc719f9f3edda317436ccf9106a11/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc?ref=850cea882a2dc719f9f3edda317436ccf9106a11",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n #include <memory>\n \n #include <gtest/gtest.h>\n-#include \"absl/log/log.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n@@ -34,9 +33,10 @@ namespace {\n \n using RaggedAllToAllDecomposerTest = HloHardwareIndependentTestBase;\n \n-TEST_F(RaggedAllToAllDecomposerTest, SimpleRaggedAllToAllIsSupported) {\n+TEST_F(RaggedAllToAllDecomposerTest,\n+       SimpleRaggedAllToAllCrossReplicaIsSupported) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n-HloModule module\n+HloModule module, replica_count=16\n \n ENTRY main {\n   input = bf16[128] parameter(0)\n@@ -60,7 +60,39 @@ ENTRY main {\n   TF_EXPECT_OK(HloDCE().Run(module.get()));\n   TF_EXPECT_OK(HloCSE(true).Run(module.get()));\n \n-  LOG(ERROR) << module->ToString();\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    // CHECK: all-gather{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n+    // CHECK-COUNT-4: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n+    // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}{{[}]}}\n+  )\"));\n+}\n+\n+TEST_F(RaggedAllToAllDecomposerTest,\n+       SimpleRaggedAllToAllCrossPartitionIsSupported) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module, num_partitions=16\n+\n+ENTRY main {\n+  input = bf16[128] parameter(0)\n+  output = bf16[256] parameter(1)\n+  input_offsets = s64[16] parameter(2)\n+  send_sizes = s64[16] parameter(3)\n+  output_offsets = s64[16] parameter(4)\n+  recv_sizes = s64[16] parameter(5)\n+  ROOT ra2a = bf16[256] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), \n+    replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}}\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/8);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+\n+  EXPECT_TRUE(changed);\n+  TF_EXPECT_OK(VerifyHloModule(module.get(), true, true));\n+  TF_EXPECT_OK(HloDCE().Run(module.get()));\n+  TF_EXPECT_OK(HloCSE(true).Run(module.get()));\n \n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n     // CHECK: all-gather{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 42,
        "deletions": 7
    }
}