{
    "author": "ZixuanJiang",
    "message": "Add a CHECK and remove unused parameters from SPMD dot partitioning functions.\n\nPiperOrigin-RevId: 817355606",
    "sha": "7a35ca1b32532e97db00c22c6cc5cd0c272489fb",
    "files": [
        {
            "sha": "d8844cb90887da48a4acda1b622da92927271a0f",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a35ca1b32532e97db00c22c6cc5cd0c272489fb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a35ca1b32532e97db00c22c6cc5cd0c272489fb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=7a35ca1b32532e97db00c22c6cc5cd0c272489fb",
            "patch": "@@ -1971,11 +1971,13 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n     }\n     if (e_config) {\n       int64_t loop_partitions = 1;\n-      for (int64_t dim : e_config->windowing_dims) {\n-        loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n-      }\n       if (e_config->windowing_dims.empty()) {\n         loop_partitions = num_partitions;\n+      } else {\n+        CHECK_EQ(e_config->windowed_op, WindowedEinsumOperand::LHS);\n+        for (int64_t dim : e_config->windowing_dims) {\n+          loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n+        }\n       }\n \n       return EmitWindowedDotGeneral(\n@@ -4090,7 +4092,7 @@ absl::StatusOr<HloInstruction*> PartitionDotRemovingOutputPartialReplication(\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n-    bool require_matching_devices_to_group, SpmdPartitioningVisitor* visitor) {\n+    SpmdPartitioningVisitor* visitor) {\n   if (lhs.sharding().IsReplicated() && rhs.sharding().IsReplicated() &&\n       output_sharding.ReplicateOnLastTileDim()) {\n     auto grouped_output = hlo_sharding_util::GroupShardingOnDims(\n@@ -4204,8 +4206,7 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n       PartitionDotRemovingOutputPartialReplication(\n           lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n           num_partitions, create_sharded_dot, conv_window, module, original_hlo,\n-          options, b, windowed_dot_general_loops,\n-          require_matching_devices_to_group, visitor));\n+          options, b, windowed_dot_general_loops, visitor));\n   if (partitioned_dot) {\n     return partitioned_dot;\n   }\n@@ -4572,7 +4573,7 @@ bool CheckOperandsRecursive(\n // Later optimization passes (TpuPadSliceMover) will merge the dynamic slice\n // with the input nodes (broadcast).\n absl::Status MoveUsersIntoWindowedDotGeneralLoopOnNonContractingDimensions(\n-    HloInstruction* loop, const SpmdPartitionerOptions& options) {\n+    HloInstruction* loop) {\n   CHECK_EQ(loop->user_count(), 1);\n   // There should be a single direct user of the while loop, which is the\n   // gte for element 2, i.e., the dot output.\n@@ -5030,8 +5031,7 @@ absl::Status MoveUsersIntoWindowedDotGeneralLoopOnNonContractingDimensions(\n \n }  // namespace\n \n-absl::Status SpmdPartitioningVisitor::DoCodeMotionForWindowedDotGeneralLoops(\n-    HloComputation* computation, const SpmdPartitionerOptions& options) {\n+absl::Status SpmdPartitioningVisitor::DoCodeMotionForWindowedDotGeneralLoops() {\n   for (auto& loop : windowed_dot_general_loops_) {\n     if (loop.windowed_in_contracting_dims || loop.windowed_in_batch_dims ||\n         loop.operands_sharded_at_contracting_dims) {\n@@ -5051,7 +5051,7 @@ absl::Status SpmdPartitioningVisitor::DoCodeMotionForWindowedDotGeneralLoops(\n       // into the loop could help reduce memory.\n       TF_RETURN_IF_ERROR(\n           MoveUsersIntoWindowedDotGeneralLoopOnNonContractingDimensions(\n-              loop.while_loop, options));\n+              loop.while_loop));\n     }\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "d1b01dbd3996a1e45f637cad366f09c94ff2ab5f",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a35ca1b32532e97db00c22c6cc5cd0c272489fb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a35ca1b32532e97db00c22c6cc5cd0c272489fb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=7a35ca1b32532e97db00c22c6cc5cd0c272489fb",
            "patch": "@@ -4837,8 +4837,7 @@ absl::Status SpmdPartitioningVisitor::HandleTuple(HloInstruction* hlo) {\n }\n \n absl::StatusOr<bool> SpmdPartitioningVisitor::DoPartition(\n-    HloComputation* computation, const HloSharding& root_sharding,\n-    const SpmdPartitionerOptions& options) {\n+    HloComputation* computation, const HloSharding& root_sharding) {\n   VLOG(2) << \"Partitioning computation \" << computation->name() << \" for \"\n           << num_replicas_ << \" replicas and \" << num_partitions_\n           << \" partitions\" << \" with root sharding \" << root_sharding;\n@@ -4849,8 +4848,7 @@ absl::StatusOr<bool> SpmdPartitioningVisitor::DoPartition(\n       GetPartitionedHlo(computation->root_instruction()).Reshard(root_sharding);\n   auto new_computation =\n       module->AddEmbeddedComputation(b_.Build(new_root.hlo()));\n-  TF_RETURN_IF_ERROR(\n-      DoCodeMotionForWindowedDotGeneralLoops(new_computation, options));\n+  TF_RETURN_IF_ERROR(DoCodeMotionForWindowedDotGeneralLoops());\n \n   // Replace the original computation with the new SPMD computation.\n   absl::flat_hash_map<HloComputation*, HloComputation*> replacement;\n@@ -5350,7 +5348,7 @@ absl::StatusOr<bool> SpmdPartitioner::PartitionComputation(\n   auto visitor = CreateVisitor(computation, num_partitions_, num_replicas_,\n                                collective_ops_creator_, next_channel_id, logger,\n                                options_, call_graph);\n-  return visitor->DoPartition(computation, root_sharding, options_);\n+  return visitor->DoPartition(computation, root_sharding);\n }\n \n std::unique_ptr<SpmdPartitioningVisitor> SpmdPartitioner::CreateVisitor("
        },
        {
            "sha": "a5df88a74a331cc99e6d3bcc9417c5b9bd2ef19e",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a35ca1b32532e97db00c22c6cc5cd0c272489fb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a35ca1b32532e97db00c22c6cc5cd0c272489fb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=7a35ca1b32532e97db00c22c6cc5cd0c272489fb",
            "patch": "@@ -813,9 +813,8 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n \n   SpmdBuilder* builder() { return &b_; }\n \n-  virtual absl::StatusOr<bool> DoPartition(\n-      HloComputation* computation, const HloSharding& root_sharding,\n-      const SpmdPartitionerOptions& options);\n+  virtual absl::StatusOr<bool> DoPartition(HloComputation* computation,\n+                                           const HloSharding& root_sharding);\n \n   virtual double GetComputationTimeInMilliSec(HloInstruction* hlo) {\n     return 0.0;\n@@ -875,8 +874,7 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n   // Performs code motion for windowed dot-general loops in\n   // windowed_dot_general_loops_. Invoked after the visitor finishes traversing\n   // the graph.\n-  absl::Status DoCodeMotionForWindowedDotGeneralLoops(\n-      HloComputation* computation, const SpmdPartitionerOptions& options);\n+  absl::Status DoCodeMotionForWindowedDotGeneralLoops();\n \n   bool changed_;\n   HloModule* module_;"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 16,
        "deletions": 20
    }
}