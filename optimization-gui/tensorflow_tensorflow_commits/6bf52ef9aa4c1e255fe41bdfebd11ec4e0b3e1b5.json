{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Emit vectorized reduce as a single loop.\n\nWhat it says on the tin, we don't need to materialize the intermediate non-minor result as we can just reduce it directly.\n\nPiperOrigin-RevId: 831745388",
    "sha": "6bf52ef9aa4c1e255fe41bdfebd11ec4e0b3e1b5",
    "files": [
        {
            "sha": "b15888fc2a31f422a0fc95e635097093157b9896",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/shlo_to_vector.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6bf52ef9aa4c1e255fe41bdfebd11ec4e0b3e1b5/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6bf52ef9aa4c1e255fe41bdfebd11ec4e0b3e1b5/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir?ref=6bf52ef9aa4c1e255fe41bdfebd11ec4e0b3e1b5",
            "patch": "@@ -116,21 +116,16 @@ func.func @reduce_outer_and_inner(%input : tensor<1024x32x8xf32>, %init : tensor\n }\n \n // CHECK: func.func @reduce_outer_and_inner\n-// CHECK:   %[[BUFFER0:.*]] = memref.alloca() : memref<32x8xf32>\n+// CHECK:   %[[BUFFER:.*]] = memref.alloca() : memref<32xf32>\n // CHECK:   scf.for\n // CHECK:     vector.transfer_read {{.*}} : tensor<1024x32x8xf32>, vector<8xf32>\n // CHECK:     scf.for\n // CHECK:       vector.transfer_read %{{.*}} : tensor<1024x32x8xf32>, vector<8xf32>\n // CHECK:       arith.addf %{{.*}} : vector<8xf32>\n // CHECK:       scf.yield {{.*}} : vector<8xf32>\n // CHECK:     }\n-// CHECK:     vector.transfer_write {{.*}}, %[[BUFFER0]]{{.*}} : vector<8xf32>, memref<32x8xf32>\n-// CHECK:   }\n-// CHECK:   %[[BUFFER1:.*]] = memref.alloca() : memref<32xf32>\n-// CHECK:   scf.for\n-// CHECK:     vector.transfer_read %[[BUFFER0]]{{.*}} : memref<32x8xf32>, vector<8xf32>\n // CHECK:     vector.reduction <add>, {{.*}} : vector<8xf32> into f32\n-// CHECK:     memref.store {{.*}} %[[BUFFER1]]{{.*}} : memref<32xf32>\n+// CHECK:     memref.store {{.*}}, %[[BUFFER]]{{.*}} : memref<32xf32>\n // CHECK:   }\n // CHECK: }\n "
        },
        {
            "sha": "4f7354d0cc669c065283bfae6311c163cbd2b5a0",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.cc",
            "status": "modified",
            "additions": 79,
            "deletions": 103,
            "changes": 182,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6bf52ef9aa4c1e255fe41bdfebd11ec4e0b3e1b5/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6bf52ef9aa4c1e255fe41bdfebd11ec4e0b3e1b5/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc?ref=6bf52ef9aa4c1e255fe41bdfebd11ec4e0b3e1b5",
            "patch": "@@ -143,32 +143,74 @@ mlir::Value VectorizeBody(mlir::OpBuilder& builder, mlir::Location loc,\n   return mapping.lookup(old_body.getTerminator()->getOperand(0));\n }\n \n-mlir::TypedValue<mlir::MemRefType> EmitNonMinorReduction(\n+// Reduce a 1D vector to a scalar with the given body.\n+mlir::Value EmitMinorReduction(mlir::OpBuilder& builder, mlir::Location loc,\n+                               mlir::RankedTensorType result_type,\n+                               mlir::TypedValue<mlir::VectorType> input,\n+                               mlir::Value init_value, mlir::Block& body) {\n+  absl::StatusOr<mlir::vector::CombiningKind> kind_or = GetCombiningKind(body);\n+  if (!kind_or.ok()) {\n+    body.getParentOp()->emitRemark() << kind_or.status().ToString();\n+  }\n+\n+  auto input_type = input.getType();\n+  int64_t minor_dim_size = input_type.getShape().back();\n+\n+  if (kind_or.ok()) {\n+    // TODO(willfroom): Investigate tree-reduction to split the reduction\n+    // op into natural sizes (2, 4, 8, 16, ...) and then remove the\n+    // reassociation flag.\n+    mlir::Value reduced_scalar = mlir::vector::ReductionOp::create(\n+        builder, loc, *kind_or, input, init_value,\n+        mlir::arith::FastMathFlags::reassoc);\n+\n+    return reduced_scalar;\n+  }\n+\n+  mlir::Value lbs = mlir::arith::ConstantIndexOp::create(builder, loc, 0);\n+  mlir::Value ubs =\n+      mlir::arith::ConstantIndexOp::create(builder, loc, minor_dim_size);\n+  mlir::Value step = mlir::arith::ConstantIndexOp::create(builder, loc, 1);\n+  auto loop = mlir::scf::ForOp::create(\n+      builder, loc, lbs, ubs, step, {init_value},\n+      [&](mlir::OpBuilder& builder, mlir::Location loc, mlir::Value index,\n+          mlir::ValueRange carry_value) {\n+        mlir::TypedValue<mlir::VectorType> element_vector =\n+            ExtractVector(builder, loc, input, index);\n+        mlir::Value element =\n+            mlir::vector::ExtractOp::create(builder, loc, element_vector);\n+\n+        mlir::Value result =\n+            VectorizeBody(builder, loc, body, element, carry_value.front());\n+\n+        mlir::scf::YieldOp::create(builder, loc, result);\n+      });\n+\n+  return loop.getResult(0);\n+}\n+\n+mlir::TypedValue<mlir::MemRefType> EmitReductionLoop(\n     mlir::OpBuilder& builder, mlir::Location loc,\n     mlir::RankedTensorType result_type,\n     mlir::TypedValue<mlir::RankedTensorType> source_tensor,\n     llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body,\n-    bool minor_dim_reduced) {\n+    mlir::Value init_value) {\n   mlir::RankedTensorType source_tensor_type = source_tensor.getType();\n   int64_t rank = source_tensor_type.getRank();\n   int64_t minor_dim = rank - 1;\n-  int64_t minor_dim_size = source_tensor_type.getDimSize(minor_dim);\n-  llvm::SmallVector<int64_t> non_reduced_dims(rank);\n-  absl::c_iota(non_reduced_dims, 0);\n-  non_reduced_dims.erase(\n-      std::remove_if(non_reduced_dims.begin(), non_reduced_dims.end(),\n-                     [&](int64_t dim) {\n-                       return absl::c_find(reduction_dims, dim) !=\n-                              reduction_dims.end();\n-                     }),\n-      non_reduced_dims.end());\n+  bool minor_dim_reduced = reduction_dims.back() == minor_dim;\n \n   // The set of non-reduced dimensions that are not the minor dimension.\n-  llvm::SmallVector<int64_t> non_reduced_non_minor_dims(non_reduced_dims);\n-  if (auto itr = absl::c_find(non_reduced_non_minor_dims, minor_dim);\n-      itr != non_reduced_non_minor_dims.end()) {\n-    non_reduced_non_minor_dims.erase(itr);\n-  }\n+  llvm::SmallVector<int64_t> non_reduced_non_minor_dims(rank);\n+  absl::c_iota(non_reduced_non_minor_dims, 0);\n+  non_reduced_non_minor_dims.erase(\n+      std::remove_if(\n+          non_reduced_non_minor_dims.begin(), non_reduced_non_minor_dims.end(),\n+          [&](int64_t dim) {\n+            return absl::c_find(reduction_dims, dim) != reduction_dims.end() ||\n+                   dim == minor_dim;\n+          }),\n+      non_reduced_non_minor_dims.end());\n \n   // The set of reduced dimensions that are not the minor dimension.\n   llvm::SmallVector<int64_t> non_minor_reduced_dims(reduction_dims);\n@@ -177,15 +219,7 @@ mlir::TypedValue<mlir::MemRefType> EmitNonMinorReduction(\n     non_minor_reduced_dims.erase(itr);\n   }\n \n-  // The shape of the of the non-minor-reduced output.\n-  llvm::SmallVector<int64_t> output_shape(result_type.getShape());\n-  if (minor_dim_reduced) {\n-    output_shape.push_back(minor_dim_size);\n-  }\n-\n-  auto output_buffer_shape =\n-      mlir::MemRefType::get(output_shape, result_type.getElementType());\n-  auto buffer = CreateBufferOfShape(builder, loc, output_buffer_shape);\n+  auto buffer = CreateBufferOfShape(builder, loc, result_type);\n \n   auto get_source_vector_dim_size = [&](llvm::ArrayRef<int64_t> dims) {\n     return llvm::map_to_vector(\n@@ -220,11 +254,12 @@ mlir::TypedValue<mlir::MemRefType> EmitNonMinorReduction(\n                 mlir::ValueRange inner_induction_vars,\n                 mlir::ValueRange minor_accumilator)\n                 -> mlir::SmallVector<mlir::Value> {\n-              llvm::SmallVector<mlir::Value> indices(rank - 1);\n-              for (auto [idx, var] : llvm::zip(non_reduced_non_minor_dims,\n-                                               outer_induction_vars)) {\n-                indices[idx] = var;\n+              // Handle the case when there are no non-minor reduced dimensions.\n+              if (inner_induction_vars.empty()) {\n+                return {minor_accumilator.front()};\n               }\n+\n+              llvm::SmallVector<mlir::Value> indices = zeroth_step_indices;\n               for (auto [idx, var] :\n                    llvm::zip(non_minor_reduced_dims, inner_induction_vars)) {\n                 indices[idx] = var;\n@@ -237,67 +272,21 @@ mlir::TypedValue<mlir::MemRefType> EmitNonMinorReduction(\n                                     minor_accumilator.front())};\n             });\n \n-        InsertValue(builder, loc, loop_nest.results.front(), buffer,\n-                    outer_induction_vars);\n-      });\n+        auto non_minor_reduced_result =\n+            mlir::cast<mlir::TypedValue<mlir::VectorType>>(\n+                loop_nest.results.front());\n \n-  return buffer;\n-}\n-\n-mlir::TypedValue<mlir::MemRefType> EmitMinorReduction(\n-    mlir::OpBuilder& builder, mlir::Location loc,\n-    mlir::RankedTensorType result_type,\n-    mlir::TypedValue<mlir::ShapedType> input, mlir::Value init_value,\n-    mlir::Block& body) {\n-  absl::StatusOr<mlir::vector::CombiningKind> kind_or = GetCombiningKind(body);\n-  if (!kind_or.ok()) {\n-    body.getParentOp()->emitRemark() << kind_or.status().ToString();\n-  }\n-\n-  auto input_type = input.getType();\n-  int64_t minor_dim_size = input_type.getShape().back();\n-\n-  auto [lbs, ubs, step] = GetLoopBounds(builder, loc, result_type.getShape());\n-\n-  auto buffer = CreateBufferOfShape(builder, loc, result_type);\n-\n-  mlir::scf::buildLoopNest(\n-      builder, loc, lbs, ubs, step,\n-      [&](mlir::OpBuilder& builder, mlir::Location loc,\n-          mlir::ValueRange induction_vars) {\n-        if (kind_or.ok()) {\n-          // TODO(willfroom): Investigate tree-reduction to split the reduction\n-          // op into natural sizes (2, 4, 8, 16, ...) and then remove the\n-          // reassociation flag.\n-          mlir::Value vector_slice =\n-              ExtractVector(builder, loc, input, induction_vars);\n+        if (minor_dim_reduced) {\n           mlir::Value reduced_scalar =\n-              builder.create<mlir::vector::ReductionOp>(\n-                  loc, *kind_or, vector_slice, init_value,\n-                  mlir::arith::FastMathFlags::reassoc);\n-\n-          InsertValue(builder, loc, reduced_scalar, buffer, induction_vars);\n-          return;\n+              EmitMinorReduction(builder, loc, result_type,\n+                                 non_minor_reduced_result, init_value, body);\n+\n+          InsertValue(builder, loc, reduced_scalar, buffer,\n+                      outer_induction_vars);\n+        } else {\n+          InsertValue(builder, loc, non_minor_reduced_result, buffer,\n+                      outer_induction_vars);\n         }\n-\n-        auto [lbs, ubs, step] = GetLoopBounds(builder, loc, {minor_dim_size});\n-        mlir::scf::LoopNest minor_reduction_loop = mlir::scf::buildLoopNest(\n-            builder, loc, lbs, ubs, step, {init_value},\n-            [&](mlir::OpBuilder& builder, mlir::Location loc,\n-                mlir::ValueRange index, mlir::ValueRange carry_value)\n-                -> mlir::SmallVector<mlir::Value> {\n-              auto full_index = llvm::to_vector(\n-                  llvm::concat<mlir::Value>(induction_vars, index));\n-              mlir::TypedValue<mlir::VectorType> element_vector =\n-                  ExtractVector(builder, loc, input, full_index);\n-              mlir::Value element =\n-                  mlir::vector::ExtractOp::create(builder, loc, element_vector);\n-              return {VectorizeBody(builder, loc, body, element,\n-                                    carry_value.front())};\n-            });\n-\n-        InsertValue(builder, loc, minor_reduction_loop.results.front(), buffer,\n-                    induction_vars);\n       });\n \n   return buffer;\n@@ -308,22 +297,9 @@ mlir::Value EmitVectorizedReduction(\n     mlir::RankedTensorType result_type,\n     mlir::TypedValue<mlir::RankedTensorType> source, mlir::Value init_value,\n     llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body) {\n-  int64_t rank = source.getType().getRank();\n-  int64_t minor_dim = rank - 1;\n-\n-  bool minor_dim_reduced = reduction_dims.back() == minor_dim;\n-  bool non_minor_dim_reduced = reduction_dims.size() > 1 || !minor_dim_reduced;\n-\n   mlir::TypedValue<mlir::ShapedType> result;\n-  if (non_minor_dim_reduced) {\n-    result = EmitNonMinorReduction(builder, loc, result_type, source,\n-                                   reduction_dims, body, minor_dim_reduced);\n-  }\n-\n-  if (minor_dim_reduced) {\n-    result = EmitMinorReduction(builder, loc, result_type,\n-                                result ? result : source, init_value, body);\n-  }\n+  result = EmitReductionLoop(builder, loc, result_type, source, reduction_dims,\n+                             body, init_value);\n \n   auto to_tensor = mlir::bufferization::ToTensorOp::create(builder, loc,\n                                                            result_type, result);"
        }
    ],
    "stats": {
        "total": 191,
        "additions": 81,
        "deletions": 110
    }
}