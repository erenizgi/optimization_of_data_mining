{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Run PTX though CollectiveKernelThunk\n\n[XLA:GPU]: Run PTX though CollectiveKernelThunk\nAdds ability to run PTX through CollectiveKernelThunk and tests for the same.\nAt the moment the number of arguments to the kernel are fixed to 7. This can be\ngeneralized as needed later when we have more kernels to run through this\ninterface.\n\nPiperOrigin-RevId: 814177186",
    "sha": "a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2",
    "files": [
        {
            "sha": "afff6eca9eff981139a50f9081aee4b7e5176e2d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2",
            "patch": "@@ -1111,8 +1111,10 @@ cc_library(\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu:stream_executor_util\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n@@ -1133,6 +1135,41 @@ cc_library(\n     ],\n )\n \n+xla_test(\n+    name = \"collective_kernel_thunk_test\",\n+    srcs = [\"collective_kernel_thunk_test.cc\"],\n+    backends = [\"h100\"],\n+    deps = [\n+        \":collective_kernel_thunk\",\n+        \":collective_thunk\",\n+        \":thunk\",\n+        \"//xla:array\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n+        \"//xla/runtime:device_id\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service:collective_ops_utils\",\n+        \"//xla/service:computation_placer_hdr\",\n+        \"//xla/service:executable\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/service/gpu:buffer_allocations\",\n+        \"//xla/service/gpu:gpu_constants\",\n+        \"//xla/service/gpu:gpu_executable_run_options\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"all_reduce_thunk\",\n     srcs = [\"all_reduce_thunk.cc\"],"
        },
        {
            "sha": "9983f7f3913478769daede7b733f9b3f64015ca8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 10,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2",
            "patch": "@@ -37,13 +37,15 @@ limitations under the License.*/\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n+#include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -58,6 +60,15 @@ using se::gpu::AllReduceStrategy;\n static constexpr int64_t kMaxOneShotAllReduceSizeBytes = 256 * 1024;  // 256 KB\n static constexpr int64_t kMaxTwoShotAllReduceSizeBytes =\n     2 * 1024 * 1024;  // 2 MB\n+// Number of arguments for the all-reduce kernel.\n+// - Metadata pointer.\n+// - Input buffer pointer.\n+// - Output buffer pointer.\n+// - Num elements.\n+// - Num elements per rank.\n+// - Rank offset.\n+// - Signal value.\n+static constexpr int kAllReduceArgsCount = 7;\n \n // Helper for allocating memory on the device.\n absl::StatusOr<se::DeviceMemoryHandle> AllocateMemory(\n@@ -128,7 +139,8 @@ absl::Status CollectiveKernelThunk::Prepare(\n     const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n-      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_));\n+      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n+                                /*use_nccl=*/false));\n   return resource_requests.AddClique(clique_key);\n }\n \n@@ -215,7 +227,8 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n   TF_ASSIGN_OR_RETURN(\n       const GpuCliqueKey clique_key,\n-      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_));\n+      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n+                                /*use_nccl=*/false));\n   const std::optional<RankId> rank =\n       clique_key.rank(params.collective_params->global_device_id);\n   TF_RET_CHECK(rank.has_value())\n@@ -251,12 +264,22 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n       // initialization.\n       TF_RETURN_IF_ERROR(params.executor->SynchronousMemZero(\n           local_buffer_alloc.memory_ptr(), local_buffer_alloc.memory().size()));\n-\n+      // Create a kernel for execution.\n+      std::unique_ptr<se::Kernel> kernel = nullptr;\n+      // If PTX is provided, we create a kernel from it.\n+      if (!kernel_name_.empty()) {\n+        VLOG(3) << \"Creating kernel from PTX.\" << params.src.text;\n+        TF_ASSIGN_OR_RETURN(kernel,\n+                            CreateKernel(kernel_name_, kAllReduceArgsCount,\n+                                         params.src.text, params.executor, 0));\n+      }\n       // Step3: Emplace into the stream state.\n       per_stream_state_.emplace(\n-          params.executor, std::make_unique<StreamState>(\n-                               params.executor->device_ordinal(), rank.value(),\n-                               std::move(local_buffer_alloc)));\n+          params.executor,\n+          std::make_unique<StreamState>(\n+              params.executor->device_ordinal(), rank.value(),\n+              std::move(local_buffer_alloc), std::move(kernel)));\n+\n       state = per_stream_state_.at(params.executor).get();\n \n       // NB: This is a double buffer allocation. So size of a single buffer is\n@@ -294,8 +317,9 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n \n   TF_ASSIGN_OR_RETURN(\n       const GpuCliqueKey clique_key,\n-      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_));\n-  const int32_t kNumRanks = clique_key.num_devices();\n+      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n+                                /*use_nccl=*/false));\n+  const int32_t num_devices = clique_key.num_devices();\n \n   // TODO(b/407736956): Support variadic all-reduce.\n   if (collective_config_.operand_element_type.size() != 1) {\n@@ -326,7 +350,7 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n   const uint32_t buffer_index = state->invocation_count % kNumBuffers;\n   auto const strategy = GetAllReduceStrategy(GetInputSizeBytes());\n   const LaunchDimensions launch_dimensions =\n-      AllReduceLaunchDimensions(buffer.element_count, kNumRanks, strategy);\n+      AllReduceLaunchDimensions(buffer.element_count, num_devices, strategy);\n   // In case of two-shot we want to increment in multiples of 2.\n   state->invocation_count += 1 + static_cast<uint32_t>(strategy);\n   VLOG(3) << \"[\" << device_ordinal\n@@ -340,6 +364,27 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n   VLOG(3) << \"[\" << device_ordinal\n           << \"] input_buffer_ptr: \" << (uint64_t)input_buffer_ptr.opaque()\n           << \" signal_buffer_ptr: \" << (uint64_t)signal_buffer_ptr.opaque();\n+  VLOG(3) << \"[\" << device_ordinal\n+          << \"] launch dimensions: \" << launch_dimensions.num_blocks() << \"x\"\n+          << launch_dimensions.num_threads_per_block()\n+          << \"(block x threadsPerBlock)\";\n+\n+  if (state->kernel != nullptr) {\n+    // NB: The assumption is one-shot all-reduce (for now).\n+    std::vector<se::KernelArgument> kernel_args = {\n+        state->metadata,\n+        source_buffer,\n+        destination_buffer,\n+        buffer.element_count,\n+        /*num_elements_per_rank=*/buffer.element_count / num_devices,\n+        /*rank_offset=*/0,\n+        state->invocation_count};\n+    TF_RET_CHECK(kernel_args.size() == kAllReduceArgsCount)\n+        << \"Kernel argument size mismatch.\" << kernel_args.size()\n+        << \" != \" << kAllReduceArgsCount;\n+    return ExecuteKernelOnStream(*state->kernel, kernel_args, launch_dimensions,\n+                                 /*cluster_dim=*/std::nullopt, stream);\n+  }\n \n   // TODO(b/407736956): Change this to emitted kernel.\n   return RunAllReduceKernel(\n@@ -352,7 +397,7 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n       /*local_input_buffer=*/source_buffer,\n       /*output_buffer=*/destination_buffer,\n       /*rank=*/rank.value(),\n-      /*num_ranks=*/kNumRanks,\n+      /*num_ranks=*/num_devices,\n       /*num_elements=*/buffer.element_count,\n       /*symmetric_signal_buffer=*/signal_buffer_ptr,\n       /*signal_value=*/state->invocation_count,"
        },
        {
            "sha": "ebcff9784677fcf1823366c3e3291dab9cf7eb2e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 18,
            "deletions": 5,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2",
            "patch": "@@ -18,13 +18,14 @@ limitations under the License.*/\n #include <array>\n #include <cstdint>\n #include <memory>\n+#include <string>\n #include <utility>\n \n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n@@ -35,6 +36,7 @@ limitations under the License.*/\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n+#include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/stream.h\"\n \n namespace xla::gpu {\n@@ -57,12 +59,14 @@ class CollectiveKernelThunk : public Thunk {\n   CollectiveKernelThunk(ThunkInfo info, CollectiveConfig collective_config,\n                         ReductionKind reduction_kind, bool is_async,\n                         absl::Span<const CollectiveThunk::Buffer> buffers,\n-                        bool is_collective_kernel_enabled)\n+                        bool is_collective_kernel_enabled,\n+                        absl::string_view kernel_name = \"\")\n       : Thunk{Thunk::kCollectiveKernel, info},\n         collective_kernel_enabled_(is_collective_kernel_enabled),\n         is_async_(is_async),\n         collective_config_(std::move(collective_config)),\n         reduction_kind_(reduction_kind),\n+        kernel_name_(kernel_name),\n         buffers_(buffers) {\n     per_stream_state_.reserve(kMaxNumExecutors);\n   }\n@@ -77,6 +81,8 @@ class CollectiveKernelThunk : public Thunk {\n                        ResourceRequestsInterface& resource_requests) final;\n \n   // Allocate buffers and events as needed for cross device communication.\n+  // If InitializeParams contains a PTX kernel, it will be used instead of the\n+  // custom cuda kernel.\n   absl::Status Initialize(const InitializeParams& params) final;\n \n   // Execute the kernel on all devices.\n@@ -108,15 +114,19 @@ class CollectiveKernelThunk : public Thunk {\n     // changed.\n     std::array<se::DeviceMemoryBase, kNumBuffers> remote_buffer_ptrs;\n     std::array<se::DeviceMemoryBase, kNumBuffers> signal_buffer_ptrs;\n+    // Kernel entry for the stream executor.\n+    std::unique_ptr<se::Kernel> kernel;\n     uint32_t invocation_count = 0;\n \n     // Constructor to make OSS builds happy.\n     StreamState() = default;\n     StreamState(int device_ordinal_arg, RankId rank_arg,\n-                se::DeviceMemoryHandle local_buffer_arg)\n+                se::DeviceMemoryHandle local_buffer_arg,\n+                std::unique_ptr<se::Kernel> kernel_arg)\n         : device_ordinal(device_ordinal_arg),\n           rank(rank_arg),\n-          local_buffer(std::move(local_buffer_arg)) {}\n+          local_buffer(std::move(local_buffer_arg)),\n+          kernel(std::move(kernel_arg)) {}\n   };\n \n   // Returns the input size in bytes for the collective.\n@@ -135,7 +145,10 @@ class CollectiveKernelThunk : public Thunk {\n   // Collective config being used. Copied over to avoid lifetime issues.\n   const CollectiveConfig collective_config_;\n   // Reduction kind being to use for AllReduce collective.\n-  ReductionKind reduction_kind_;\n+  const ReductionKind reduction_kind_;\n+  // Kernel name to execute. Required when Codegen/PTX kernel is used.\n+  // Must match the kernel name in the generated PTX kernel.\n+  const std::string kernel_name_;\n   // Reference to the buffer related information required for the collective.\n   absl::Span<const CollectiveThunk::Buffer> buffers_;\n "
        },
        {
            "sha": "c30ee8366d7fdb285aae61c48bebc0549ebfd07e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk_test.cc",
            "status": "added",
            "additions": 269,
            "deletions": 0,
            "changes": 269,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc?ref=a6474f8a3d7d99d1bacf609a8293fe9a1d275fb2",
            "patch": "@@ -0,0 +1,269 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_kernel_thunk.h\"\n+\n+#include <cstdint>\n+#include <map>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/array.h\"\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/service/gpu/buffer_allocations.h\"\n+#include \"xla/service/gpu/gpu_constants.h\"\n+#include \"xla/service/gpu/gpu_executable_run_options.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/service/service_executable_run_options.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+static constexpr absl::string_view kProfileName = \"test_kernel_profiler\";\n+static constexpr absl::string_view kKernelName = \"seven_argument_kernel\";\n+\n+// Test kernel was compiled using following CUDA source:\n+// __global__ void seven_argument_kernel(void* metadata,                 // 1\n+//                                       int64_t* input_buffer,          // 2\n+//                                       int64_t* output_buffer,         // 3\n+//                                       int64_t num_elements,           // 4\n+//                                       int64_t num_elements_per_rank,  // 5\n+//                                       int64_t rank_offset,            // 6\n+//                                       int64_t signal_value            // 7\n+// ) {\n+//   (void)num_elements;\n+//   (void)num_elements_per_rank;\n+//   (void)rank_offset;\n+//   (void)signal_value;\n+//   (void)metadata;\n+//   int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n+//   for (int i = idx; i < num_elements; i += gridDim.x * blockDim.x) {\n+//     if (i < num_elements) {\n+//       output_buffer[i] = input_buffer[i] + signal_value;\n+//     }\n+//   }\n+// }\n+static constexpr absl::string_view kKernelSource = R\"(\n+  .version 8.7\n+  .target sm_90\n+  .address_size 64\n+\n+  //\n+  //\n+  .visible .entry seven_argument_kernel(\n+  .param .u64 .ptr .align 1 metadata,\n+  .param .u64 .ptr .align 1 input_buffer,\n+  .param .u64 .ptr .align 1 output_buffer,\n+  .param .u64 num_elements,\n+  .param .u64 num_elements_per_rank,\n+  .param .u64 rank_offset,\n+  .param .u64 signal_value\n+  )\n+  {\n+  .reg .pred %p<3>;\n+  .reg .b32 %r<12>;\n+  .reg .b64 %rd<16>;\n+\n+  //\n+  ld.param.b64 %rd6, [num_elements];\n+  ld.param.b64 %rd8, [input_buffer];\n+  cvta.to.global.u64 %rd1, %rd8;\n+  ld.param.b64 %rd9, [output_buffer];\n+  cvta.to.global.u64 %rd2, %rd9;\n+  mov.u32 %r1, %ctaid.x;\n+  mov.u32 %r2, %ntid.x;\n+  mov.u32 %r3, %tid.x;\n+  mad.lo.s32 %r8, %r1, %r2, %r3;\n+  cvt.s64.s32 %rd15, %r8;\n+  setp.le.s64 %p1, %rd6, %rd15;\n+  @%p1 bra $L__BB0_3;\n+  //\n+  ld.param.b64 %rd7, [signal_value];\n+  mov.u32 %r9, %nctaid.x;\n+  mul.lo.s32 %r4, %r9, %r2;\n+  add.s32 %r10, %r9, %r1;\n+  mad.lo.s32 %r11, %r2, %r10, %r3;\n+  $L__BB0_2: //\n+  shl.b64 %rd10, %rd15, 3;\n+  add.s64 %rd11, %rd1, %rd10;\n+  ld.global.b64 %rd12, [%rd11];\n+  add.s64 %rd13, %rd12, %rd7;\n+  add.s64 %rd14, %rd2, %rd10;\n+  st.global.b64 [%rd14], %rd13;\n+  cvt.s64.s32 %rd15, %r11;\n+  setp.gt.s64 %p2, %rd6, %rd15;\n+  add.s32 %r11, %r11, %r4;\n+  @%p2 bra $L__BB0_2;\n+  $L__BB0_3:\n+  ret;\n+  //\n+  }\n+)\";\n+\n+absl::StatusOr<se::StreamExecutor*> GpuExecutor(int32_t device_ordinal) {\n+  TF_ASSIGN_OR_RETURN(auto name, PlatformUtil::CanonicalPlatformName(\"gpu\"));\n+  TF_ASSIGN_OR_RETURN(auto* platform,\n+                      se::PlatformManager::PlatformWithName(name));\n+  return platform->ExecutorForDevice(device_ordinal);\n+}\n+\n+TEST(CollectiveKernelThunkTest, ExecutesPtxKernel) {\n+  using DataT = int64_t;\n+  static constexpr int64_t kNumElements = 128;\n+  static constexpr int64_t kInputSizeBytes = kNumElements * sizeof(DataT);\n+  static constexpr uint32_t kExpectedSignalValue = 1;\n+\n+  // --------------------\n+  // Arrange\n+  // --------------------\n+  // # Prepare input data and expected output data.\n+  Array<DataT> input_data({/*num_elements=*/kNumElements});\n+  input_data.FillRandom(5, 5, /*seed=*/12345);\n+  Array<DataT> expected_output_data({/*num_elements=*/kNumElements});\n+  expected_output_data.Each([&](absl::Span<const int64_t> indices, DataT* val) {\n+    *val = input_data(indices) + kExpectedSignalValue;\n+  });\n+  // # Prepare Infrastructure.\n+  TF_ASSERT_OK_AND_ASSIGN(se::StreamExecutor * executor0, GpuExecutor(0));\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.profile_annotation = kProfileName;\n+  ReplicaGroup replica_group;\n+  replica_group.add_replica_ids(0);\n+  CollectiveConfig collective_config{\n+      /*operand_count=*/1,\n+      /*operand_element_type=*/{PrimitiveType::F32},\n+      /* replica_groups=*/{replica_group},\n+      /* collective_op_kind=*/RendezvousKey::CollectiveOpKind::kCrossReplica,\n+      /* op_id=*/0,\n+      /* group_mode=*/CollectiveOpGroupMode::kCrossReplica,\n+      /* use_symmetric_buffer=*/false};\n+  const int64_t aligned_input_size_bytes =\n+      xla::RoundUpTo<uint64_t>(kInputSizeBytes, kXlaAllocatedBufferAlignBytes);\n+  // 2x because we have two buffers, one for input and one for output so we can\n+  // test output independently of input.\n+  const int64_t total_buffer_size = aligned_input_size_bytes * 2;\n+  // ## Create physical buffers.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n+                          executor0->CreateStream());\n+  std::vector<se::DeviceMemoryBase> allocated_buffers = {\n+      executor0->AllocateArray<DataT>(total_buffer_size)};\n+  std::vector<se::DeviceMemoryBase> input_buffers = {\n+      allocated_buffers[0].GetByteSlice(0, aligned_input_size_bytes)};\n+  std::vector<se::DeviceMemoryBase> output_buffers = {\n+      allocated_buffers[0].GetByteSlice(aligned_input_size_bytes,\n+                                        aligned_input_size_bytes)};\n+  BufferAllocations buffer_allocations(\n+      /*buffers=*/allocated_buffers,\n+      /*device_ordinal=*/0,\n+      /*memory_allocator=*/nullptr);\n+  TF_ASSERT_OK(\n+      stream->Memcpy(&input_buffers[0], input_data.data(), kInputSizeBytes));\n+\n+  // ## Create Logical Buffers.\n+  BufferAllocation buffer_allocation(\n+      /*index=*/0, /*size=*/total_buffer_size, /*color=*/0);\n+  BufferAllocation::Slice input_slice(&buffer_allocation, /*offset=*/0,\n+                                      /*size=*/aligned_input_size_bytes);\n+  BufferAllocation::Slice output_slice(\n+      &buffer_allocation, aligned_input_size_bytes, aligned_input_size_bytes);\n+  std::vector<CollectiveThunk::Buffer> buffers = {\n+      {/*element_count=*/kNumElements,\n+       /*source_buffer=*/input_slice,\n+       /*destination_buffer=*/output_slice,\n+       /*source_memory_space=*/0,\n+       /*destination_memory_space=*/0,\n+       /*source_value=*/nullptr,\n+       /*destination_value=*/nullptr}};\n+\n+  // ## Setup device mapping.\n+  DeviceAssignment device_assignment(/*replica_count=*/1,\n+                                     /*computation_count=*/1);\n+  device_assignment(0, 0) = 0;\n+  GpuExecutableRunOptions gpu_options;\n+  gpu_options.set_gpu_global_device_ids(\n+      std::map{std::make_pair(0, GlobalDeviceId(0))});\n+  ServiceExecutableRunOptions run_options;\n+  run_options.mutable_run_options()->set_stream(stream.get());\n+  run_options.mutable_run_options()->set_device_assignment(&device_assignment);\n+  run_options.mutable_run_options()->set_gpu_executable_run_options(\n+      &gpu_options);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto collective_params,\n+      Thunk::CollectiveExecuteParams::Create(run_options, /*async_streams=*/{},\n+                                             /*local_device_ordinal=*/0));\n+  // --------------------\n+  // Act\n+  // --------------------\n+  CollectiveKernelThunk thunk(std::move(thunk_info), collective_config,\n+                              ReductionKind::SUM,\n+                              /*is_async=*/false, buffers,\n+                              /*is_collective_kernel_enabled=*/true,\n+                              /*kernel_name=*/kKernelName);\n+\n+  // # Thunk::Initialize\n+  Thunk::InitializeParams initialize_params;\n+  initialize_params.executor = executor0;\n+  initialize_params.stream = stream.get();\n+  initialize_params.buffer_allocations = &buffer_allocations;\n+  initialize_params.collective_params = &collective_params;\n+  initialize_params.src = {kKernelSource};\n+  TF_ASSERT_OK(thunk.Initialize(initialize_params));\n+\n+  // # Thunk::Execute\n+  auto execute_params =\n+      Thunk::ExecuteParams::Create(run_options,                              //\n+                                   buffer_allocations,                       //\n+                                   stream.get(),                             //\n+                                   /*command_buffer_trace_stream=*/nullptr,  //\n+                                   &collective_params,                       //\n+                                   /*collective_cliques=*/nullptr            //\n+      );\n+  TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));\n+\n+  // --------------------\n+  // Assert\n+  // --------------------\n+  Array<DataT> output_data({kNumElements});\n+  TF_ASSERT_OK(\n+      stream->Memcpy(output_data.data(), output_buffers[0], kInputSizeBytes));\n+  for (auto i = 0; i < kNumElements; ++i) {\n+    ASSERT_EQ(expected_output_data(i), output_data(i))\n+        << \"comparison failed at i = \" << i;\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 394,
        "additions": 379,
        "deletions": 15
    }
}