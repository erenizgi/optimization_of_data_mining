{
    "author": "pifon2a",
    "message": "[XLA:GPU] Emit SliceToDynamic and PadToStatic in a separate llvm module.\n\nPiperOrigin-RevId: 836276286",
    "sha": "3a12588ef9d47c8cf89b2470837cc7e61fb9e2c7",
    "files": [
        {
            "sha": "66a7bf9ce8f55470d71f6e1c1150b0828d7f4081",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 15,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a12588ef9d47c8cf89b2470837cc7e61fb9e2c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a12588ef9d47c8cf89b2470837cc7e61fb9e2c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=3a12588ef9d47c8cf89b2470837cc7e61fb9e2c7",
            "patch": "@@ -233,6 +233,16 @@ EmitCollectiveKernelThunk(IrEmitterContext* ir_emitter_context,\n           .xla_gpu_unsupported_use_all_reduce_one_shot_kernel());\n }\n \n+std::unique_ptr<llvm::Module> CreateLocalLLVMModule(\n+    const std::string& module_name, llvm::Module* global_llvm_module) {\n+  auto llvm_module = std::make_unique<llvm::Module>(\n+      module_name, global_llvm_module->getContext());\n+  llvm_module->setTargetTriple(\n+      llvm::Triple(global_llvm_module->getTargetTriple()));\n+  llvm_module->setDataLayout(global_llvm_module->getDataLayout());\n+  return llvm_module;\n+}\n+\n }  // namespace\n \n IrEmitterUnnested::IrEmitterUnnested(IrEmitterContext* ir_emitter_context)\n@@ -352,16 +362,18 @@ void IrEmitterUnnested::CreateStore(llvm::Value* data, llvm::Value* address,\n // end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n absl::Status IrEmitterUnnested::EmitPadToStatic(\n     const HloCustomCallInstruction* instr) {\n-  int unroll_factor = 1;\n   std::string ir_name = std::string(instr->name());\n+  auto local_llvm_module =\n+      CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n+\n+  constexpr int kUnrollFactor = 1;\n   const Shape& input_shape = instr->operand(0)->shape();\n \n   LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n-      input_shape, ir_emitter_context_->gpu_device_info(), {unroll_factor});\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<llvm_ir::IrArray> ir_arrays,\n-      BuildKernelThunkForNonFusionOp(ir_emitter_context_->llvm_module(), instr,\n-                                     launch_dimensions));\n+      input_shape, ir_emitter_context_->gpu_device_info(), {kUnrollFactor});\n+  TF_ASSIGN_OR_RETURN(std::vector<llvm_ir::IrArray> ir_arrays,\n+                      BuildKernelThunkForNonFusionOp(local_llvm_module.get(),\n+                                                     instr, launch_dimensions));\n \n   const llvm_ir::IrArray& source_array = ir_arrays[0];\n   const llvm_ir::IrArray& output_array = ir_arrays[1];\n@@ -473,29 +485,33 @@ absl::Status IrEmitterUnnested::EmitPadToStatic(\n   const Shape& data_shape = instr->shape().tuple_shapes(0);\n   TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n                                          launch_dimensions, &b_,\n-                                         {unroll_factor})\n+                                         {kUnrollFactor})\n                          .EmitLoop(ir_name, index_ty));\n+  CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n+                                   std::move(local_llvm_module),\n+                                   llvm::Linker::Flags::OverrideFromSrc));\n   return absl::OkStatus();\n }\n \n // Input = {dynamic array(with dynamic dimension meta data at the\n // end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n absl::Status IrEmitterUnnested::EmitSliceToDynamic(\n     const HloCustomCallInstruction* instr) {\n-  // TODO(jurahul): Create an op to represent SliceToDynamic.\n-  int unroll_factor = 1;\n   std::string ir_name = std::string(instr->name());\n+  auto local_llvm_module =\n+      CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n \n+  // TODO(jurahul): Create an op to represent SliceToDynamic.\n+  constexpr int kUnrollFactor = 1;\n   const Shape& input_shape = instr->operand(0)->shape();\n \n   LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n-      input_shape, ir_emitter_context_->gpu_device_info(), {unroll_factor});\n+      input_shape, ir_emitter_context_->gpu_device_info(), {kUnrollFactor});\n   llvm::Type* index_ty =\n       GetIndexTypeForKernel(instr, launch_dimensions.launch_bound(), &b_);\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<llvm_ir::IrArray> ir_arrays,\n-      BuildKernelThunkForNonFusionOp(ir_emitter_context_->llvm_module(), instr,\n-                                     launch_dimensions));\n+  TF_ASSIGN_OR_RETURN(std::vector<llvm_ir::IrArray> ir_arrays,\n+                      BuildKernelThunkForNonFusionOp(local_llvm_module.get(),\n+                                                     instr, launch_dimensions));\n \n   const Shape& data_shape = ShapeUtil::MakeStaticShape(instr->shape());\n   TF_RET_CHECK(data_shape.IsArray());\n@@ -596,8 +612,11 @@ absl::Status IrEmitterUnnested::EmitSliceToDynamic(\n \n   TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n                                          launch_dimensions, &b_,\n-                                         {unroll_factor})\n+                                         {kUnrollFactor})\n                          .EmitLoop(ir_name, index_ty));\n+  CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n+                                   std::move(local_llvm_module),\n+                                   llvm::Linker::Flags::OverrideFromSrc));\n   return absl::OkStatus();\n }\n "
        }
    ],
    "stats": {
        "total": 49,
        "additions": 34,
        "deletions": 15
    }
}