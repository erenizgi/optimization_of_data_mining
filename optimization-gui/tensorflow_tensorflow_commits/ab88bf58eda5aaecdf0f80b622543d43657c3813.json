{
    "author": "majiddadashi",
    "message": "Expand TFL FullyConnected constant folding to support batched inputs.\n\nThe constant folding logic for `tfl.fully_connected` is updated to handle inputs with a batch dimension. Previously, only 1D inputs were fully supported.\n\nPiperOrigin-RevId: 804503563",
    "sha": "ab88bf58eda5aaecdf0f80b622543d43657c3813",
    "files": [
        {
            "sha": "342acb49aabe0ee3f97cc5974f06a69b4b2fddc0",
            "filename": "tensorflow/compiler/mlir/lite/ir/tfl_ops.cc",
            "status": "modified",
            "additions": 57,
            "deletions": 46,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ab88bf58eda5aaecdf0f80b622543d43657c3813/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ab88bf58eda5aaecdf0f80b622543d43657c3813/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.cc?ref=ab88bf58eda5aaecdf0f80b622543d43657c3813",
            "patch": "@@ -1723,13 +1723,22 @@ LogicalResult FullyConnectedOp::fold(FoldAdaptor adaptor,\n       !(!getBias() || mlir::isa<NoneType>(getBias().getType()));\n \n   // Get the tensors.\n-  DenseElementsAttr input_tensor, weights_tensor, bias_tensor;\n-  if (!matchPattern(getInput(), m_Constant(&input_tensor)) ||\n-      !matchPattern(getFilter(), m_Constant(&weights_tensor)) ||\n-      (has_bias && !matchPattern(getBias(), m_Constant(&bias_tensor)))) {\n+  auto operands = adaptor.getOperands();\n+  DenseElementsAttr input_tensor =\n+      dyn_cast_or_null<DenseElementsAttr>(operands[0]);\n+  DenseElementsAttr weights_tensor =\n+      dyn_cast_or_null<DenseElementsAttr>(operands[1]);\n+  DenseElementsAttr bias_tensor;\n+\n+  if (!input_tensor || !weights_tensor) {\n     return failure();\n   }\n \n+  if (has_bias) {\n+    bias_tensor = dyn_cast_or_null<DenseElementsAttr>(operands[2]);\n+    if (!bias_tensor) return failure();\n+  }\n+\n   // Get the tensor types.\n   const auto input_type = mlir::cast<ShapedType>(input_tensor.getType());\n   const auto weights_type = mlir::cast<ShapedType>(weights_tensor.getType());\n@@ -1752,58 +1761,60 @@ LogicalResult FullyConnectedOp::fold(FoldAdaptor adaptor,\n     return failure();\n   }\n \n-  auto is_foldable = [](llvm::ArrayRef<int64_t> shape) {\n-    return shape.size() == 1 || (shape.size() == 2 && shape.front() == 1);\n-  };\n+  if (weights_type.getRank() != 2) {\n+    return failure();\n+  }\n \n-  const bool weights_foldable = weights_type.getShape().size() == 2;\n-  const bool bias_foldable = !has_bias || is_foldable(bias_type.getShape());\n+  const int64_t in_dim = weights_type.getDimSize(1);\n+  const int64_t out_dim = weights_type.getDimSize(0);\n \n-  // Folding only implemented for 1D input, 2D weights and 1D bias\n-  if (!is_foldable(input_type.getShape()) || !bias_foldable ||\n-      !weights_foldable) {\n-    return failure();\n+  if (has_bias) {\n+    if (bias_type.getRank() > 2 ||\n+        (bias_type.getRank() == 2 && bias_type.getDimSize(0) != 1) ||\n+        bias_type.getNumElements() != out_dim) {\n+      return failure();\n+    }\n   }\n \n-  // Get the sizes\n-  const auto input_size = input_type.getNumElements();\n-  const auto output_size = output_type.getNumElements();\n+  const int64_t batch_size = input_type.getNumElements() / in_dim;\n+\n+  if (output_type.getNumElements() != batch_size * out_dim) {\n+    return failure();\n+  }\n \n-  // Get iterators to the tensors.\n-  const auto input_values_it = input_tensor.getValues<float>().begin();\n-  const auto weights_values_ptr = weights_tensor.getValues<float>().begin();\n-  auto weights_row_it = weights_values_ptr;\n-  // The 'else' case could be nullptr, but the types don't match.\n-  auto bias_values_it =\n-      has_bias ? bias_tensor.getValues<float>().begin() : input_values_it;\n+  auto input_values_range = input_tensor.getValues<float>();\n+  auto weights_values_range = weights_tensor.getValues<float>();\n+  std::optional<decltype(input_values_range)> bias_values_range;\n+  if (has_bias) bias_values_range = bias_tensor.getValues<float>();\n \n   // Do the actual folding, one output at a time.\n   std::vector<float> result_values;\n-  result_values.reserve(output_size);\n-\n-  for (int i = 0; i < output_size; ++i) {\n-    // Dot product with Kahan/Neumaier summation to minimize numeric errors.\n-    float sum = has_bias ? *bias_values_it : 0.0f;\n-    float compensation = 0.0f;\n-    for (int j = 0; j < input_size; ++j) {\n-      const float addend = input_values_it[j] * weights_row_it[j];\n-      const float new_sum = sum + addend;\n-      // DO NOT enable -funsafe-math-optimizations here.\n-      // There is a test detecting unsafe optimizations.\n-      // Unsafe math optimizations can reorder float formulas, and set the\n-      // compensation to constant 0. The formula must be evaluated as written\n-      // for the algorithm to work.\n-      // (Note: -ffast-math is a superset of -funsafe-math-optimizations.)\n-      if (std::abs(sum) >= std::abs(addend)) {\n-        compensation += (sum - new_sum) + addend;\n-      } else {\n-        compensation += (addend - new_sum) + sum;\n+  result_values.reserve(batch_size * out_dim);\n+\n+  for (int b = 0; b < batch_size; ++b) {\n+    for (int o = 0; o < out_dim; ++o) {\n+      // Dot product with Kahan/Neumaier summation to minimize numeric errors.\n+      float sum = has_bias ? (*bias_values_range)[o] : 0.0f;\n+      float compensation = 0.0f;\n+      for (int i = 0; i < in_dim; ++i) {\n+        const float addend = input_values_range[b * in_dim + i] *\n+                             weights_values_range[o * in_dim + i];\n+        const float new_sum = sum + addend;\n+        // DO NOT enable -funsafe-math-optimizations here.\n+        // There is a test detecting unsafe optimizations.\n+        // Unsafe math optimizations can reorder float formulas, and set the\n+        // compensation to constant 0. The formula must be evaluated as written\n+        // for the algorithm to work.\n+        // (Note: -ffast-math is a superset of -funsafe-math-optimizations.)\n+        if (std::abs(sum) >= std::abs(addend)) {\n+          compensation += (sum - new_sum) + addend;\n+        } else {\n+          compensation += (addend - new_sum) + sum;\n+        }\n+        sum = new_sum;\n       }\n-      sum = new_sum;\n+      result_values.push_back(sum + compensation);\n     }\n-    result_values.push_back(sum + compensation);\n-    weights_row_it += input_size;\n-    bias_values_it++;\n   }\n \n   // Set result tensor"
        },
        {
            "sha": "6043e26cb757d8f55293081d09132e14c220695a",
            "filename": "tensorflow/compiler/mlir/lite/tests/const-fold.mlir",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ab88bf58eda5aaecdf0f80b622543d43657c3813/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fconst-fold.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ab88bf58eda5aaecdf0f80b622543d43657c3813/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fconst-fold.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fconst-fold.mlir?ref=ab88bf58eda5aaecdf0f80b622543d43657c3813",
            "patch": "@@ -1186,20 +1186,18 @@ func.func @NoFoldFullyConnectedNonFloat() -> tensor<1024xf32> {\n   // CHECK: return %[[VAL]] : tensor<1024xf32>\n }\n \n-// CHECK-LABEL: @NoFoldFullyConnectedHighRank\n-func.func @NoFoldFullyConnectedHighRank() -> tensor<2x1024xf32> {\n+// CHECK-LABEL: @ConstantFoldFullyConnectedHighRank\n+func.func @ConstantFoldFullyConnectedHighRank() -> tensor<2x1024xf32> {\n   %cst_input = arith.constant dense<1.0> : tensor<2x512xf32>\n   %cst_weights = arith.constant dense<2.0> : tensor<1024x512xf32>\n   %cst_bias = arith.constant dense<4.0> : tensor<1024xf32>\n \n   %0 = \"tfl.fully_connected\" (%cst_input, %cst_weights, %cst_bias) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<2x512xf32>, tensor<1024x512xf32>, tensor<1024xf32>) -> tensor<2x1024xf32>\n \n   func.return %0 : tensor<2x1024xf32>\n-  // CHECK-DAG: %[[CST:.*]] = arith.constant dense<1.000000e+00> : tensor<2x512xf32>\n-  // CHECK-DAG: %[[CST_0:.*]] = arith.constant dense<2.000000e+00> : tensor<1024x512xf32>\n-  // CHECK-DAG: %[[CST_1:.*]] = arith.constant dense<4.000000e+00> : tensor<1024xf32>\n-  // CHECK: %[[VAL:.*]] = \"tfl.fully_connected\"(%[[CST]], %[[CST_0]], %[[CST_1]]) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<2x512xf32>, tensor<1024x512xf32>, tensor<1024xf32>) -> tensor<2x1024xf32>\n-  // CHECK: return %[[VAL]] : tensor<2x1024xf32>\n+  // 1.0 * 2.0 * 512 + 4.0 = 1028.0\n+  // CHECK: %[[CST:.*]] = arith.constant dense<1.028000e+03> : tensor<2x1024xf32>\n+  // CHECK:  return %[[CST]]\n }\n \n // CHECK-LABEL: @ConstantFoldFullyConnectedCheckPrecision\n@@ -1227,6 +1225,20 @@ func.func @fully_connected_with_unit_dim() -> tensor<1x5xf32> {\n // CHECK:     %cst = arith.constant dense<6.000000e+00> : tensor<1x5xf32>\n // CHECK-NOT: fully_connected\n \n+// CHECK-LABEL: @ConstantFoldFullyConnectedBatched\n+func.func @ConstantFoldFullyConnectedBatched() -> tensor<13x1536xf32> {\n+  %cst_input = arith.constant dense<1.0> : tensor<13x1536xf32>\n+  %cst_weights = arith.constant dense<1.0> : tensor<1536x1536xf32>\n+  %cst_bias = \"tfl.no_value\"() {value = unit} : () -> none\n+\n+  %0 = \"tfl.fully_connected\" (%cst_input, %cst_weights, %cst_bias) {fused_activation_function = \"NONE\", keep_num_dims = true, weights_format = \"DEFAULT\"} : (tensor<13x1536xf32>, tensor<1536x1536xf32>, none) -> tensor<13x1536xf32>\n+  func.return %0 : tensor<13x1536xf32>\n+\n+  // 1.0 * 1.0 * 1536 = 1536.0\n+  // CHECK: %[[CST:.*]] = arith.constant dense<1.536000e+03> : tensor<13x1536xf32>\n+  // CHECK:  return %[[CST]]\n+}\n+\n // CHECK-LABEL: @ShapeOpI32\n func.func @ShapeOpI32(%arg0 : tensor<576x72xf32>) -> tensor<2xi32> {\n   %0 = \"tfl.shape\"(%arg0) : (tensor<576x72xf32>) -> tensor<2xi32>"
        }
    ],
    "stats": {
        "total": 129,
        "additions": 76,
        "deletions": 53
    }
}