{
    "author": "derdrdirk",
    "message": "Add --xla_gpu_experimental_autotune_backends to allow for selecting backends.\nThis change for the new autotuner. The new autotuner with its Triton backend competes with cuDNN fusions leading to flaky tests. Also some tests disable some autotuning paths via --xla_gpu_cudnn_gemm_fusion_level or --xla_gpu_cublas_fallback which are not fully compatible with the new autotuner. Other tests rely on the order of the backends, which would be resolved by adding a backend selection mechanism.\n\nPiperOrigin-RevId: 847750954",
    "sha": "3ea706cab38f3e0cc873e921c1ef11a39bbc7fef",
    "files": [
        {
            "sha": "8b05ff60afbc84e67db72d46be60ea2a169f3171",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ea706cab38f3e0cc873e921c1ef11a39bbc7fef/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ea706cab38f3e0cc873e921c1ef11a39bbc7fef/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=3ea706cab38f3e0cc873e921c1ef11a39bbc7fef",
            "patch": "@@ -474,6 +474,15 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_experimental_scaled_dot_with_triton(false);\n   opts.set_xla_gpu_experimental_use_raft_select_k(false);\n \n+  opts.add_xla_gpu_experimental_autotune_backends(\n+      DebugOptions::AUTOTUNE_BACKEND_CUDNN);\n+  opts.add_xla_gpu_experimental_autotune_backends(\n+      DebugOptions::AUTOTUNE_BACKEND_TRITON);\n+  opts.add_xla_gpu_experimental_autotune_backends(\n+      DebugOptions::AUTOTUNE_BACKEND_CUBLAS);\n+  opts.add_xla_gpu_experimental_autotune_backends(\n+      DebugOptions::AUTOTUNE_BACKEND_CUBLASLT);\n+\n   opts.set_xla_cpu_collective_call_warn_stuck_seconds(20);\n   opts.set_xla_cpu_collective_call_terminate_timeout_seconds(40);\n   opts.set_xla_cpu_collective_timeout_seconds(30 * 60);\n@@ -713,6 +722,16 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n     return absl::StrJoin(command_types, \", \", Formatter());\n   };\n \n+  auto autotune_backends_to_string =\n+      [](google::protobuf::RepeatedField<int> backends) -> std::string {\n+    struct Formatter {\n+      void operator()(std::string* out, int type) const {\n+        absl::StrAppend(out, DebugOptions::AutotuneBackend_Name(type));\n+      }\n+    };\n+    return absl::StrJoin(backends, \", \", Formatter());\n+  };\n+\n   // Custom \"sub-parser\" for xla_fuel.  Note that ConsumeFuel does not do any\n   // locking on the fuel global variables.  This means that it's\n   // illegal/undefined behavior to modify this flag value while the compiler is\n@@ -2360,6 +2379,20 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n           &DebugOptions::set_xla_gpu_experimental_autotuner_cache_dir),\n       debug_options->xla_gpu_experimental_autotuner_cache_dir(),\n       \"Experimental: Specify the directory to read/write autotuner cache to.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_experimental_autotune_backends\",\n+      SetterForRepeatedEnum<DebugOptions::AutotuneBackend>(\n+          \"xla_gpu_experimental_autotune_backends\",\n+          /*enum_prefix=*/\"AUTOTUNE_BACKEND_\",\n+          &DebugOptions::AutotuneBackend_Parse,\n+          debug_options->mutable_xla_gpu_experimental_autotune_backends()),\n+      autotune_backends_to_string(\n+          debug_options->xla_gpu_experimental_autotune_backends()),\n+      \"Backends to enable for autotuning. Comma-separated (no spaces). \"\n+      \"Examples:\\n\"\n+      \"  'cudnn,triton' (overwrites defaults)\\n\"\n+      \"  '+cudnn,-cublas' (adds/removes from defaults)\\n\"\n+      \"Available: cudnn, triton, cublas, cublaslt.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_gemm_autotuner_override_file\",\n       string_setter_for("
        },
        {
            "sha": "e09c83fbcbf57fbbf01ba4ad71563fac83e4ad3a",
            "filename": "third_party/xla/xla/debug_options_parsers_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ea706cab38f3e0cc873e921c1ef11a39bbc7fef/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ea706cab38f3e0cc873e921c1ef11a39bbc7fef/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc?ref=3ea706cab38f3e0cc873e921c1ef11a39bbc7fef",
            "patch": "@@ -507,6 +507,38 @@ TEST(ParseRepeatedEnumFlagsTest, XnnFusionType) {\n   TestLibraryFusionType(\"xnn\");\n }\n \n+TEST(ParseRepeatedEnumFlagsTest, AutotuneBackend) {\n+  DebugOptions debug_options = DefaultDebugOptionsIgnoringFlags();\n+  std::vector<tsl::Flag> flag_objects;\n+  MakeDebugOptionsFlags(&flag_objects, &debug_options);\n+\n+  const auto& enabled_backends =\n+      debug_options.xla_gpu_experimental_autotune_backends();\n+\n+  // Check that the default setting is populated.\n+  ASSERT_THAT(enabled_backends,\n+              ElementsAre(DebugOptions::AUTOTUNE_BACKEND_CUDNN,\n+                          DebugOptions::AUTOTUNE_BACKEND_TRITON,\n+                          DebugOptions::AUTOTUNE_BACKEND_CUBLAS,\n+                          DebugOptions::AUTOTUNE_BACKEND_CUBLASLT));\n+\n+  // Overwriting the default setting.\n+  SetXlaFlagsEnvVar(\"--xla_gpu_experimental_autotune_backends=cudnn,triton\");\n+  ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n+  EXPECT_EQ(enabled_backends.size(), 2);\n+  EXPECT_THAT(enabled_backends,\n+              ElementsAre(DebugOptions::AUTOTUNE_BACKEND_CUDNN,\n+                          DebugOptions::AUTOTUNE_BACKEND_TRITON));\n+\n+  // Adding / removing options from the existing setting.\n+  SetXlaFlagsEnvVar(\"--xla_gpu_experimental_autotune_backends=+cublas,-triton\");\n+  ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n+  EXPECT_EQ(enabled_backends.size(), 2);\n+  EXPECT_THAT(enabled_backends,\n+              ElementsAre(DebugOptions::AUTOTUNE_BACKEND_CUDNN,\n+                          DebugOptions::AUTOTUNE_BACKEND_CUBLAS));\n+}\n+\n TEST(ParseIntRangeInclusiveTest, SingleInteger) {\n   IntRangeInclusive range;\n   EXPECT_TRUE(ParseIntRangeInclusive(\"10\", range));"
        },
        {
            "sha": "bd5234e66d22d23f79eb5d0695ecf3576140c078",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ea706cab38f3e0cc873e921c1ef11a39bbc7fef/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ea706cab38f3e0cc873e921c1ef11a39bbc7fef/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=3ea706cab38f3e0cc873e921c1ef11a39bbc7fef",
            "patch": "@@ -61,6 +61,15 @@ message ThunkBufferDebugFilter {\n // field presence is available to support merging between command-line flags and\n // stored instances. This is enforced via a unit test.\n message DebugOptions {\n+  // Enum to define all backends that can be autotuned.\n+  enum AutotuneBackend {\n+    AUTOTUNE_BACKEND_ALL = 0;\n+    AUTOTUNE_BACKEND_CUDNN = 1;\n+    AUTOTUNE_BACKEND_TRITON = 2;\n+    AUTOTUNE_BACKEND_CUBLAS = 3;\n+    AUTOTUNE_BACKEND_CUBLASLT = 4;\n+  }\n+\n   // Enum to define all collective ops\n   // that xla supports.\n   enum CollectiveOpType {\n@@ -306,7 +315,7 @@ message DebugOptions {\n   // XLA:GPU options.\n   //--------------------------------------------------------------------------//\n   // clang-format off\n-  // go/keep-sorted start newline_separated=yes skip_lines=2 ignore_prefixes=[\"optional AutotuneCacheMode\",\"optional bool\",\"optional float\",\"optional int32\",\"optional int64\",\"optional LibNvJitLinkMode\",\"map<string, string>\",\"optional PGLEStrictnessLevel\",\"optional PipelineParallelismOptLevel\",\"repeated CollectiveOpType\",\"repeated CommandBufferCmdType\",\"repeated string\",\"optional ShapeChecks\",\"optional string\",\"optional WhileLoopUnrolling\",\"repeated GenericTritonEmitterFeature\",\"optional CommandBufferSchedulingMode\"] // NOLINT\n+  // go/keep-sorted start newline_separated=yes skip_lines=2 ignore_prefixes=[\"optional AutotuneCacheMode\",\"optional bool\",\"optional float\",\"optional int32\",\"optional int64\",\"optional LibNvJitLinkMode\",\"map<string, string>\",\"optional PGLEStrictnessLevel\",\"optional PipelineParallelismOptLevel\",\"repeated CollectiveOpType\",\"repeated CommandBufferCmdType\",\"repeated string\",\"optional ShapeChecks\",\"optional string\",\"optional WhileLoopUnrolling\",\"repeated GenericTritonEmitterFeature\",\"optional CommandBufferSchedulingMode\", \"repeated AutotuneBackend\"] // NOLINT\n   // clang-format on\n \n   // Command buffer scheduling mode.\n@@ -630,6 +639,9 @@ message DebugOptions {\n   // up to the HLO optimization stage, before Thunk generation.\n   optional bool xla_gpu_experimental_aot_compiled_thunks = 435;\n \n+  // List of autotuner backends to enable. If empty, all backends are enabled.\n+  repeated AutotuneBackend xla_gpu_experimental_autotune_backends = 442;\n+\n   // Specifies the behavior of per kernel autotuning cache.\n   optional AutotuneCacheMode xla_gpu_experimental_autotune_cache_mode = 324;\n \n@@ -1341,7 +1353,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 442\n+  // Next id: 443\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 81,
        "additions": 79,
        "deletions": 2
    }
}