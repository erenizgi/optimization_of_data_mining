{
    "author": "unknown",
    "message": "PR #30871: Doc: operation_semantics.md update - Dot, DotGeneral\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30871\n\n## Summary of Changes\n\n- Table formatting for consistency in both Dot and DotGeneral\n- Inclusion of `precision_config`, `preferred_element_type` in both Dot & DotGeneral\n    - Table and Declaration both updated\n    - Development of type and semantics in table\n- Small description of `recision_config`, `preferred_element_type` included at bottom of both Dot & DotGeneral\n\nðŸŽ¯ Justification\nDocumentation update\n\nðŸš€ Kind of Contribution\nðŸ“š Documentation\n\nCopybara import of the project:\n\n--\nea38445ed5f1c42d684520a87ff6fdba11a8f27a by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nDot and DotGeneral Update\n\nMerging this change closes #30871\n\nPiperOrigin-RevId: 802433765",
    "sha": "cd88e316f9fcfecc1f4416cfd3552c1c31bc89ab",
    "files": [
        {
            "sha": "45d8687b0ef6226bd61e657dcf232e5c6f0fe2f2",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 54,
            "deletions": 16,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd88e316f9fcfecc1f4416cfd3552c1c31bc89ab/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd88e316f9fcfecc1f4416cfd3552c1c31bc89ab/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=cd88e316f9fcfecc1f4416cfd3552c1c31bc89ab",
            "patch": "@@ -1021,39 +1021,64 @@ idempotent.\n See also\n [`XlaBuilder::Dot`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-**`Dot(lhs, rhs)`**\n+**`Dot(lhs, rhs, precision_config, preferred_element_type)`**\n \n-Arguments | Type    | Semantics\n---------- | ------- | ---------------\n-`lhs`     | `XlaOp` | array of type T\n-`rhs`     | `XlaOp` | array of type T\n+| Arguments                | Type              | Semantics                   |\n+| ------------------------ | ----------------- | --------------------------- |\n+| `lhs`                    | `XlaOp`           | array of type T             |\n+| `rhs`                    | `XlaOp`           | array of type T             |\n+| `precision_config`       | optional          | enum for level of precision |\n+:                          : `PrecisionConfig` :                             :\n+| `preferred_element_type` | optional          | enum of scalar element type |\n+:                          : `PrimitiveType`   :                             :\n \n The exact semantics of this operation depend on the ranks of the operands:\n \n-| Input                               | Output          | Semantics               |\n-| ----------------------------------- | --------------- | ----------------------- |\n-| vector [n] `dot` vector [n]         | scalar          | vector dot product      |\n-| matrix [m x k] `dot` vector [k]     | vector [m]      | matrix-vector multiplication |\n-| matrix [m x k] `dot` matrix [k x n] | matrix [m x n]  | matrix-matrix multiplication |\n+| Input                       | Output         | Semantics                    |\n+| --------------------------- | -------------- | ---------------------------- |\n+| vector [n] `dot` vector [n] | scalar         | vector dot product           |\n+| matrix [m x k] `dot` vector | vector [m]     | matrix-vector multiplication |\n+: [k]                         :                :                              :\n+| matrix [m x k] `dot` matrix | matrix [m x n] | matrix-matrix multiplication |\n+: [k x n]                     :                :                              :\n \n The operation performs sum of products over the second dimension of `lhs` (or\n the first if it has 1 dimension) and the first dimension of `rhs`. These are the\n \"contracted\" dimensions. The contracted dimensions of `lhs` and `rhs` must be of\n the same size. In practice, it can be used to perform dot products between\n vectors, vector/matrix multiplications or matrix/matrix multiplications.\n \n+`precision_config` is used to indicate the precision configuration. The level\n+dictates whether hardware should attempt to generate more machine code\n+instructions to provide more accurate dtype emulation when needed (i.e.\n+emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n+`DEFAULT`, `HIGH`, `HIGHEST`. Additional details\n+[in the MXU sections](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus).\n+\n+`preferred_element_type` is a scalar element of higher/lower precision output\n+types used for accumulation. `preferred_element_type` recommends the\n+accumulation type for the given operaiton, however it is not guaranteed. This\n+allows for some hardware backends to instead accumulate in a different type and\n+convert to the preferred output type.\n+\n ## DotGeneral\n \n See also\n [`XlaBuilder::DotGeneral`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-**`DotGeneral(lhs, rhs, dimension_numbers)`**\n+**`DotGeneral(lhs, rhs, dimension_numbers, precision_config,\n+preferred_element_type)`**\n \n-Arguments           | Type                  | Semantics\n-------------------- | --------------------- | ---------------\n-`lhs`               | `XlaOp`               | array of type T\n-`rhs`               | `XlaOp`               | array of type T\n-`dimension_numbers` | `DotDimensionNumbers` | contracting and batch dimension numbers\n+| Arguments                | Type                  | Semantics              |\n+| ------------------------ | --------------------- | ---------------------- |\n+| `lhs`                    | `XlaOp`               | array of type T        |\n+| `rhs`                    | `XlaOp`               | array of type T        |\n+| `dimension_numbers`      | `DotDimensionNumbers` | contracting and batch  |\n+:                          :                       : dimension numbers      :\n+| `precision_config`       | optional              | enum for level of      |\n+:                          : `PrecisionConfig`     : precision              :\n+| `preferred_element_type` | optional              | enum of scalar element |\n+:                          : `PrimitiveType`       : type                   :\n \n Similar to Dot, but allows contracting and batch dimension numbers to be\n specified for both the `lhs` and `rhs`.\n@@ -1129,6 +1154,19 @@ It follows that the resulting dimension number starts with the batch dimension,\n then the `lhs` non-contracting/non-batch dimension, and finally the `rhs`\n non-contracting/non-batch dimension.\n \n+`precision_config` is used to indicate the precision configuration. The level\n+dictates whether hardware should attempt to generate more machine code\n+instructions to provide more accurate dtype emulation when needed (i.e.\n+emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n+`DEFAULT`, `HIGH`, `HIGHEST`. Additional details\n+[can be found in the MXU sections](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus).\n+\n+`preferred_element_type` is a scalar element of higher/lower precision output\n+types used for accumulation. `preferred_element_type` recommends the\n+accumulation type for the given operaiton, however it is not guaranteed. This\n+allows for some hardware backends to instead accumulate in a different type and\n+convert to the preferred output type.\n+\n ## DynamicSlice\n \n See also"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 54,
        "deletions": 16
    }
}