{
    "author": "tensorflower-gardener",
    "message": "Apply llvm-use-new-mlir-op-builder fixes\n\nThis migrates `builder.create<Op>()` => `Op::create()`\n\nPiperOrigin-RevId: 846865415",
    "sha": "af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
    "files": [
        {
            "sha": "f963d7a9c8dcb14f0a2e04209bcf2e38f6aa9dd9",
            "filename": "tensorflow/compiler/mlir/stablehlo/transforms/utils.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Fstablehlo%2Ftransforms%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Fstablehlo%2Ftransforms%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fstablehlo%2Ftransforms%2Futils.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -27,14 +27,14 @@ namespace odml {\n \n mhlo::ConstantOp GetScalarConstOfType(Type ty, Location loc, int64_t raw_value,\n                                       OpBuilder* builder) {\n-  return builder->create<mhlo::ConstantOp>(loc,\n-                                           hlo::getScalarOfType(ty, raw_value));\n+  return mhlo::ConstantOp::create(*builder, loc,\n+                                  hlo::getScalarOfType(ty, raw_value));\n }\n \n mhlo::ConstantOp GetScalarNegZeroOfType(Type ty, Location loc,\n                                         OpBuilder* builder) {\n-  return builder->create<mhlo::ConstantOp>(loc,\n-                                           hlo::getScalarNegZeroOfType(ty));\n+  return mhlo::ConstantOp::create(*builder, loc,\n+                                  hlo::getScalarNegZeroOfType(ty));\n }\n \n DenseIntElementsAttr GetI64ElementsAttr(ArrayAttr attr) {"
        },
        {
            "sha": "2beec1bcd8794477cadfce1091a04a9a1cb2c0ff",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/cluster_formation.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fcluster_formation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fcluster_formation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fcluster_formation.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -107,17 +107,17 @@ void BuildLaunchForCluster(const TF::Cluster& c, OpBuilder* builder) {\n   // as operand.\n   OpBuilder return_builder(builder->getContext());\n   return_builder.setInsertionPointToEnd(block);\n-  return_builder.create<tf_device::ReturnOp>(return_builder.getUnknownLoc(),\n-                                             live_outs);\n+  tf_device::ReturnOp::create(return_builder, return_builder.getUnknownLoc(),\n+                              live_outs);\n \n   llvm::SmallVector<Type, 4> live_out_types;\n   live_out_types.reserve(live_outs.size());\n   for (Value v : live_outs) {\n     live_out_types.emplace_back(v.getType());\n   }\n \n-  tf_device::LaunchOp launch_op = builder->create<tf_device::LaunchOp>(\n-      builder->getUnknownLoc(), builder->getStringAttr(c.target),\n+  tf_device::LaunchOp launch_op = tf_device::LaunchOp::create(\n+      *builder, builder->getUnknownLoc(), builder->getStringAttr(c.target),\n       live_out_types);\n \n   // Attach the region to launch_op."
        },
        {
            "sha": "9158ecc6f7fcd795472550c2ceec70f650542c5a",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/cluster_ops_by_policy.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fcluster_ops_by_policy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fcluster_ops_by_policy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fcluster_ops_by_policy.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -575,7 +575,7 @@ tf_device::ClusterOp CreateClusterOp(Cluster &cluster, StringAttr policy) {\n   OpBuilder builder(back);\n \n   auto cluster_op =\n-      builder.create<tf_device::ClusterOp>(loc, return_types, policy);\n+      tf_device::ClusterOp::create(builder, loc, return_types, policy);\n \n   // Create block in cluster_op's region and move 'cluster.operations' into\n   // it.\n@@ -585,7 +585,7 @@ tf_device::ClusterOp CreateClusterOp(Cluster &cluster, StringAttr policy) {\n \n   // Add 'tf_device::ReturnOp' at the end of the block.\n   builder.setInsertionPointToEnd(block);\n-  builder.create<tf_device::ReturnOp>(loc, return_values.getArrayRef());\n+  tf_device::ReturnOp::create(builder, loc, return_values.getArrayRef());\n \n   // Set device attribute\n   if (auto device = back->getAttr(kDeviceAttr))"
        },
        {
            "sha": "ea7dce395d84d963fe202f530b75d0043f93b4d9",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/convert_control_to_data_outputs.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fconvert_control_to_data_outputs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fconvert_control_to_data_outputs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fconvert_control_to_data_outputs.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -370,7 +370,7 @@ void AppendFunctionResults(func::FuncOp func, int num_resources,\n   // function.\n   OpBuilder builder(graph_op);\n   auto new_graph_op =\n-      builder.create<GraphOp>(graph_op.getLoc(), new_result_types);\n+      GraphOp::create(builder, graph_op.getLoc(), new_result_types);\n   new_graph_op.getRegion().takeBody(graph_op.getRegion());\n   graph_op->replaceAllUsesWith(\n       new_graph_op->getResults().drop_back(num_resources));\n@@ -388,14 +388,15 @@ IslandOp CreateIsland(Operation* sub_op, ValueRange control_inputs,\n                       OpBuilder builder) {\n   assert(sub_op);\n   auto control_type = ControlType::get(builder.getContext());\n-  auto island = builder.create<IslandOp>(\n-      sub_op->getLoc(), sub_op->getResultTypes(), control_type, control_inputs);\n+  auto island =\n+      IslandOp::create(builder, sub_op->getLoc(), sub_op->getResultTypes(),\n+                       control_type, control_inputs);\n   island.getBody().push_back(new Block);\n   Block* block = &island.getBody().back();\n   builder.setInsertionPointToEnd(block);\n   sub_op->replaceAllUsesWith(island.getOutputs());\n   sub_op->moveBefore(block, block->begin());\n-  builder.create<YieldOp>(sub_op->getLoc(), sub_op->getResults());\n+  YieldOp::create(builder, sub_op->getLoc(), sub_op->getResults());\n   return island;\n }\n \n@@ -429,12 +430,12 @@ void ChainResourceOps(\n     // Create chain source and sink identity islands for current equivalence\n     // class.\n     auto chain_arg = func.getArgument(chain_index++);\n-    auto src_identity = builder_chain_src.create<TF::IdentityOp>(\n-        chain_arg.getLoc(), chain_arg.getType(), chain_arg);\n+    auto src_identity = TF::IdentityOp::create(\n+        builder_chain_src, chain_arg.getLoc(), chain_arg.getType(), chain_arg);\n     auto chain_src_island = CreateIsland(src_identity, {}, builder_chain_src);\n \n-    auto sink_identity = builder_chain_sink.create<TF::IdentityOp>(\n-        chain_arg.getLoc(), chain_arg.getType(), chain_arg);\n+    auto sink_identity = TF::IdentityOp::create(\n+        builder_chain_sink, chain_arg.getLoc(), chain_arg.getType(), chain_arg);\n     auto chain_sink_island =\n         CreateIsland(sink_identity, {}, builder_chain_sink);\n \n@@ -477,7 +478,7 @@ void ChainResourceOps(\n IslandOp GetDummyConstant(OpBuilder builder, ShapedType const_type,\n                           Location loc) {\n   DenseIntElementsAttr val = DenseIntElementsAttr::get(const_type, 1);\n-  auto const_op = builder.create<TF::ConstOp>(loc, val);\n+  auto const_op = TF::ConstOp::create(builder, loc, val);\n   auto const_island = CreateIsland(const_op, {}, builder);\n   return const_island;\n }\n@@ -506,8 +507,9 @@ TF::WhileOp RewriteWhileOp(TF::WhileOp while_op, int num_resource_inputs,\n   }\n \n   // Replace old while op with new while op.\n-  auto new_while_op = builder.create<TF::WhileOp>(\n-      while_op.getLoc(), new_result_types, new_operands, while_op->getAttrs());\n+  auto new_while_op =\n+      TF::WhileOp::create(builder, while_op.getLoc(), new_result_types,\n+                          new_operands, while_op->getAttrs());\n   auto new_while_wrapper =\n       CreateIsland(new_while_op, while_wrapper.getControlInputs(), builder);\n   for (auto result : while_wrapper.getOutputs()) {"
        },
        {
            "sha": "cda422d0d9938ee49b44bc4962e601301e85b878",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/decompose_resource_ops.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fdecompose_resource_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fdecompose_resource_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fdecompose_resource_ops.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -137,12 +137,12 @@ class DecomposeRngReadAndSkipOp : public RewritePattern {\n \n     // Read the state value from the resource.\n     Value state =\n-        rewriter.create<ReadVariableOp>(loc, res_type, rng_op.getResource());\n+        ReadVariableOp::create(rewriter, loc, res_type, rng_op.getResource());\n \n     // Extract the key and counter from the state.\n     RankedTensorType word_type = RankedTensorType::get({}, state_element_type);\n-    auto unpacked = rewriter.create<UnpackOp>(\n-        loc, SmallVector<Type, 4>(state_size, word_type), state, 0);\n+    auto unpacked = UnpackOp::create(\n+        rewriter, loc, SmallVector<Type, 4>(state_size, word_type), state, 0);\n     Value key = unpacked.getResult(counter_size);\n \n     SmallVector<Value, 4> counter;\n@@ -153,39 +153,40 @@ class DecomposeRngReadAndSkipOp : public RewritePattern {\n     // Set the increment to 256 * delta.\n     Type u64 = rewriter.getIntegerType(64, /*isSigned=*/false);\n     RankedTensorType u64_scalar = RankedTensorType::get({}, u64);\n-    Value step_size = rewriter.create<ConstOp>(loc, GetScalarOfType(u64, 256));\n+    Value step_size = ConstOp::create(rewriter, loc, GetScalarOfType(u64, 256));\n     Value increment =\n-        rewriter.create<MulOp>(loc, u64_scalar, step_size, rng_op.getDelta());\n+        MulOp::create(rewriter, loc, u64_scalar, step_size, rng_op.getDelta());\n \n     // Increment the counter.\n     SmallVector<Value, 4> pack_args;\n     RankedTensorType word_u64_type = RankedTensorType::get({}, u64);\n-    Value zero_u64 = rewriter.create<ConstOp>(loc, GetScalarOfType(u64, 0));\n-    Value one_u64 = rewriter.create<ConstOp>(loc, GetScalarOfType(u64, 1));\n+    Value zero_u64 = ConstOp::create(rewriter, loc, GetScalarOfType(u64, 0));\n+    Value one_u64 = ConstOp::create(rewriter, loc, GetScalarOfType(u64, 1));\n     for (int i = 0; i < counter_size; ++i) {\n       Value word = counter[i];\n-      Value word_u64 = rewriter.create<CastOp>(loc, word_u64_type, word);\n-      Value new_word_u64 = rewriter.create<AddV2Op>(loc, word_u64, increment);\n-      Value new_word = rewriter.create<CastOp>(loc, word_type, new_word_u64);\n+      Value word_u64 = CastOp::create(rewriter, loc, word_u64_type, word);\n+      Value new_word_u64 = AddV2Op::create(rewriter, loc, word_u64, increment);\n+      Value new_word = CastOp::create(rewriter, loc, word_type, new_word_u64);\n       pack_args.push_back(new_word);\n \n-      Value overflow = rewriter.create<LessOp>(loc, new_word_u64, word_u64);\n-      increment = rewriter.create<SelectV2Op>(loc, overflow, one_u64, zero_u64);\n+      Value overflow = LessOp::create(rewriter, loc, new_word_u64, word_u64);\n+      increment =\n+          SelectV2Op::create(rewriter, loc, overflow, one_u64, zero_u64);\n     }\n \n     // Save the new state value to the resource.\n     pack_args.push_back(key);\n-    Value new_state = rewriter.create<PackOp>(loc, res_type, pack_args);\n-    rewriter.create<AssignVariableOp>(loc, rng_op.getResource(), new_state);\n+    Value new_state = PackOp::create(rewriter, loc, res_type, pack_args);\n+    AssignVariableOp::create(rewriter, loc, rng_op.getResource(), new_state);\n \n     // Pad the original state as necessary to fill the output shape.\n     int pad = tensorflow::RNG_MAX_COUNTER_SIZE - counter_size;\n     Type i64 = rewriter.getI64Type();\n     RankedTensorType paddings_ty = RankedTensorType::get({1, 2}, i64);\n     std::vector<int64_t> paddings_values = {0, pad};\n-    Value paddings = rewriter.create<ConstOp>(\n-        loc, DenseIntElementsAttr::get(paddings_ty, paddings_values));\n-    Value output = rewriter.create<PadOp>(loc, op_type, state, paddings);\n+    Value paddings = ConstOp::create(\n+        rewriter, loc, DenseIntElementsAttr::get(paddings_ty, paddings_values));\n+    Value output = PadOp::create(rewriter, loc, op_type, state, paddings);\n \n     rewriter.replaceOp(op, output);\n     return success();"
        },
        {
            "sha": "73dc7802c7d56db2d9d7c1caad0db897372494b2",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/einsum.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 31,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Feinsum.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Feinsum.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Feinsum.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -61,7 +61,7 @@ namespace {\n ConstOp createI32ConstOp(int32_t value, Location loc,\n                          PatternRewriter* rewriter) {\n   auto int_attr = IntegerAttr::get(rewriter->getIntegerType(32), value);\n-  return rewriter->create<ConstOp>(loc, int_attr);\n+  return ConstOp::create(*rewriter, loc, int_attr);\n }\n \n // Creates ConstantOp for array of int32_t.\n@@ -70,7 +70,7 @@ arith::ConstantOp createI32ConstantOp(llvm::ArrayRef<int32_t> values,\n   auto values_type = RankedTensorType::get(\n       {static_cast<int32_t>(values.size())}, rewriter->getIntegerType(32));\n   auto constant_attr = rewriter->getI32TensorAttr(values);\n-  return rewriter->create<arith::ConstantOp>(loc, values_type, constant_attr);\n+  return arith::ConstantOp::create(*rewriter, loc, values_type, constant_attr);\n }\n \n // Creates ConstantOp for array of int64_t.\n@@ -79,7 +79,7 @@ arith::ConstantOp createI64ConstantOp(llvm::ArrayRef<int64_t> values,\n   auto values_type = RankedTensorType::get(\n       {static_cast<int64_t>(values.size())}, rewriter->getIntegerType(64));\n   auto constant_attr = rewriter->getI64TensorAttr(values);\n-  return rewriter->create<arith::ConstantOp>(loc, values_type, constant_attr);\n+  return arith::ConstantOp::create(*rewriter, loc, values_type, constant_attr);\n }\n \n // Function to create a tf.SumOp to sum the element in 'value' reduced along the\n@@ -98,8 +98,9 @@ TF::SumOp createSumOp(Value value, Location loc,\n       sum_shape.push_back(shape[i]);\n     }\n   }\n-  return rewriter->create<TF::SumOp>(\n-      loc, RankedTensorType::get(sum_shape, value_type.getElementType()), value,\n+  return TF::SumOp::create(\n+      *rewriter, loc,\n+      RankedTensorType::get(sum_shape, value_type.getElementType()), value,\n       redux_op);\n }\n \n@@ -115,8 +116,8 @@ TF::TransposeOp createTransposeOp(Value value, Location loc,\n   }\n   auto transposed_type =\n       RankedTensorType::get(transposed_shape, value_type.getElementType());\n-  return rewriter->create<TF::TransposeOp>(loc, transposed_type, value,\n-                                           perm_op);\n+  return TF::TransposeOp::create(*rewriter, loc, transposed_type, value,\n+                                 perm_op);\n }\n \n TF::ReshapeOp createReshapeOp(Value value, ArrayRef<int64_t> shape,\n@@ -125,8 +126,8 @@ TF::ReshapeOp createReshapeOp(Value value, ArrayRef<int64_t> shape,\n   auto shape_tensor = createI64ConstantOp(\n       tensorflow::ConvertMlirShapeToTF(shape), loc, rewriter);\n   Type resultType = RankedTensorType::get(shape, element_type);\n-  return rewriter->create<TF::ReshapeOp>(loc, resultType, /*tensor=*/value,\n-                                         /*shape=*/shape_tensor);\n+  return TF::ReshapeOp::create(*rewriter, loc, resultType, /*tensor=*/value,\n+                               /*shape=*/shape_tensor);\n }\n \n // Creates ReshapeOp with runtime calcuation of required shape to support\n@@ -140,24 +141,24 @@ TF::ReshapeOp createReshapeOpForDynamic(Value value, ArrayRef<int64_t> shape,\n                                         PatternRewriter* rewriter) {\n   // Build ShapeOp\n   auto input_shape =\n-      rewriter->create<TF::ShapeOp>(loc, value, rewriter->getBoolAttr(true));\n+      TF::ShapeOp::create(*rewriter, loc, value, rewriter->getBoolAttr(true));\n \n   // Build UnsortedSegmentProdOp\n   Type segProdresultType =\n       RankedTensorType::get(num_reshape_segids, rewriter->getIntegerType(32));\n   auto segids_tensor = createI32ConstantOp(reshape_segids, loc, rewriter);\n   auto num_reshape_segids_tensor =\n       createI32ConstOp(num_reshape_segids, loc, rewriter);\n-  auto segprod = rewriter->create<TF::UnsortedSegmentProdOp>(\n-      loc, segProdresultType, input_shape->getResults()[0], segids_tensor,\n-      num_reshape_segids_tensor);\n+  auto segprod = TF::UnsortedSegmentProdOp::create(\n+      *rewriter, loc, segProdresultType, input_shape->getResults()[0],\n+      segids_tensor, num_reshape_segids_tensor);\n \n   // Build ReshapeOp with the result of UnsortedSegmentProdOp.\n   Type out_tensor_type =\n       RankedTensorType::get(shape, getElementTypeOrSelf(value.getType()));\n-  return rewriter->create<TF::ReshapeOp>(loc, out_tensor_type,\n-                                         /*tensor=*/value,\n-                                         /*shape=*/segprod->getResults()[0]);\n+  return TF::ReshapeOp::create(*rewriter, loc, out_tensor_type,\n+                               /*tensor=*/value,\n+                               /*shape=*/segprod->getResults()[0]);\n }\n \n struct EinsumDimensionNumbers {\n@@ -178,8 +179,8 @@ TF::ReshapeOp createOutputReshapeOpForDynamic(\n     EinsumDimensionNumbers& dnums, Location loc, PatternRewriter* rewriter) {\n   BoolAttr true_attr = rewriter->getBoolAttr(true);\n   // Build ShapeOp\n-  auto shape_lhs = rewriter->create<TF::ShapeOp>(loc, org_lhs, true_attr);\n-  auto shape_rhs = rewriter->create<TF::ShapeOp>(loc, org_rhs, true_attr);\n+  auto shape_lhs = TF::ShapeOp::create(*rewriter, loc, org_lhs, true_attr);\n+  auto shape_rhs = TF::ShapeOp::create(*rewriter, loc, org_rhs, true_attr);\n \n   std::vector<int32_t> bl_index;  // Indexes of B0,...,Bn and L0,...,Ln\n   bl_index.reserve(dnums.lhs_rhs_out.size() + dnums.lhs_out.size());\n@@ -196,20 +197,20 @@ TF::ReshapeOp createOutputReshapeOpForDynamic(\n   }\n \n   auto lhs_index_tensor = createI32ConstantOp(bl_index, loc, rewriter);\n-  auto gather_lhs = rewriter->create<TF::GatherOp>(\n-      loc,\n+  auto gather_lhs = TF::GatherOp::create(\n+      *rewriter, loc,\n       RankedTensorType::get({static_cast<int>(bl_index.size())},\n                             rewriter->getIntegerType(32)),\n       shape_lhs->getResults()[0], lhs_index_tensor->getResults()[0], true_attr);\n   auto rhs_index_tensor = createI32ConstantOp(r_index, loc, rewriter);\n-  auto gather_rhs = rewriter->create<TF::GatherOp>(\n-      loc,\n+  auto gather_rhs = TF::GatherOp::create(\n+      *rewriter, loc,\n       RankedTensorType::get({static_cast<int>(r_index.size())},\n                             rewriter->getIntegerType(32)),\n       shape_rhs->getResults()[0], rhs_index_tensor->getResults()[0], true_attr);\n   Value zero_value = createI32ConstOp(0, loc, rewriter);\n-  auto concat_out_shape = rewriter->create<TF::ConcatOp>(\n-      loc,\n+  auto concat_out_shape = TF::ConcatOp::create(\n+      *rewriter, loc,\n       RankedTensorType::get({static_cast<int>(bl_index.size()) +\n                              static_cast<int>(r_index.size())},\n                             rewriter->getIntegerType(32)),\n@@ -220,10 +221,9 @@ TF::ReshapeOp createOutputReshapeOpForDynamic(\n   // Build ReshapeOp with the calculated output shape.\n   Type out_type =\n       RankedTensorType::get(shape, getElementTypeOrSelf(value.getType()));\n-  return rewriter->create<TF::ReshapeOp>(\n-      loc, out_type,\n-      /*tensor=*/value,\n-      /*shape=*/concat_out_shape->getResults()[0]);\n+  return TF::ReshapeOp::create(*rewriter, loc, out_type,\n+                               /*tensor=*/value,\n+                               /*shape=*/concat_out_shape->getResults()[0]);\n }\n \n std::optional<llvm::SmallDenseMap<char, int64_t>> EquationToMap(\n@@ -793,9 +793,9 @@ LogicalResult rewriteToBatchMatmul(TF::EinsumOp op,\n \n   auto matmul_type =\n       RankedTensorType::get(matmul_shape, original_type.getElementType());\n-  Value out = rewriter.create<TF::BatchMatMulV2Op>(\n-      op.getLoc(), matmul_type, lhs, rhs, rewriter.getBoolAttr(false),\n-      rewriter.getBoolAttr(false));\n+  Value out = TF::BatchMatMulV2Op::create(rewriter, op.getLoc(), matmul_type,\n+                                          lhs, rhs, rewriter.getBoolAttr(false),\n+                                          rewriter.getBoolAttr(false));\n \n   bool out_reshape_need = (reshape_shape.size() != matmul_shape.size() ||\n                            original_type.getRank() != matmul_shape.size());"
        },
        {
            "sha": "883da73f2fb37848ba67a48eafceb8f0e0e1fcfd",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/extract_tpu_copy_with_dynamic_shape_op.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fextract_tpu_copy_with_dynamic_shape_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fextract_tpu_copy_with_dynamic_shape_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fextract_tpu_copy_with_dynamic_shape_op.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -127,8 +127,8 @@ tf_device::LaunchOp CreateNewHostLaunchOpWithNewResult(\n   for (Value result : new_launch_op_results)\n     new_launch_op_results_types.push_back(result.getType());\n \n-  auto new_launch_op = builder.create<tf_device::LaunchOp>(\n-      old_launch_op->getLoc(), old_launch_op->getDeviceAttr(),\n+  auto new_launch_op = tf_device::LaunchOp::create(\n+      builder, old_launch_op->getLoc(), old_launch_op->getDeviceAttr(),\n       /*result_types=*/new_launch_op_results_types);\n \n   new_launch_op.getBody().takeBody(old_launch_op->getBody());\n@@ -154,17 +154,16 @@ LogicalResult CreateNewDeviceLaunchOp(\n     return failure();\n   }\n \n-  new_device_launch_op = builder.create<tf_device::LaunchOp>(\n-      tpu_copy_with_dynamic_shape_op->getLoc(),\n+  new_device_launch_op = tf_device::LaunchOp::create(\n+      builder, tpu_copy_with_dynamic_shape_op->getLoc(),\n       builder.getStringAttr(device_str),\n       /*result_types=*/tpu_copy_with_dynamic_shape_op->getResultTypes());\n \n   new_device_launch_op.getBody().push_back(new Block);\n   builder.setInsertionPointToEnd(&new_device_launch_op.GetBody());\n-  auto* return_op = builder\n-                        .create<tf_device::ReturnOp>(\n-                            tpu_copy_with_dynamic_shape_op->getLoc(),\n-                            tpu_copy_with_dynamic_shape_op->getResults())\n+  auto* return_op = tf_device::ReturnOp::create(\n+                        builder, tpu_copy_with_dynamic_shape_op->getLoc(),\n+                        tpu_copy_with_dynamic_shape_op->getResults())\n                         .getOperation();\n   tpu_copy_with_dynamic_shape_op->moveBefore(return_op);\n   return success();"
        },
        {
            "sha": "b2ab71fa5129cb55f48009a8079ca75b95a14ea8",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/fused_kernel_matcher.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ffused_kernel_matcher.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ffused_kernel_matcher.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ffused_kernel_matcher.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -230,8 +230,8 @@ class FuseContractionWithBiasAdd : public OpRewritePattern<SrcOpT> {\n     auto *bias_add_op = bias_add.getOperation();\n     if (bias_add_op) rewriter.setInsertionPoint(bias_add_op);\n \n-    Value fused_op = rewriter.create<FusedOpT>(fused_loc, result_type,\n-                                               ValueRange(operands), attrs);\n+    Value fused_op = FusedOpT::create(rewriter, fused_loc, result_type,\n+                                      ValueRange(operands), attrs);\n     auto op_to_replace = fuse_activation ? activation : bias_add;\n     rewriter.replaceOp(op_to_replace, ValueRange({fused_op}));\n     return success();"
        },
        {
            "sha": "18fc8fc1cb58ccc50d7cc333c178e2f91c764a8c",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/hoist_replicate_invariant_resource_writes.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhoist_replicate_invariant_resource_writes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhoist_replicate_invariant_resource_writes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhoist_replicate_invariant_resource_writes.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -89,9 +89,9 @@ void MoveTailWritesAfterReplicate(\n \n   OpBuilder builder(replicate_op);\n   // Clone this old replicate op but with new result types.\n-  auto new_replicate_op = builder.create<tf_device::ReplicateOp>(\n-      replicate_op->getLoc(), new_result_types, replicate_op->getOperands(),\n-      replicate_op->getAttrs());\n+  auto new_replicate_op = tf_device::ReplicateOp::create(\n+      builder, replicate_op->getLoc(), new_result_types,\n+      replicate_op->getOperands(), replicate_op->getAttrs());\n \n   // Move region to the new op.\n   new_replicate_op.getRegion().takeBody(replicate_op.getRegion());"
        },
        {
            "sha": "7806967d7dcfe9b2c3d5b76be159d393b373d189",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/host_runtime/tpu_merge_variables_with_execute.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Ftpu_merge_variables_with_execute.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Ftpu_merge_variables_with_execute.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Ftpu_merge_variables_with_execute.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -413,8 +413,8 @@ void ReplaceParallelExecute(\n       &output_types, parallel_execute, region_index + 1, num_regions);\n \n   builder->setInsertionPoint(parallel_execute);\n-  auto new_parallel_execute = builder->create<tf_device::ParallelExecuteOp>(\n-      parallel_execute.getLoc(), num_regions, output_types);\n+  auto new_parallel_execute = tf_device::ParallelExecuteOp::create(\n+      *builder, parallel_execute.getLoc(), num_regions, output_types);\n \n   // Replace the uses of the original parallel_execute before region containing\n   // merged execute.\n@@ -449,8 +449,8 @@ void ReplaceParallelExecute(\n   // execute results.\n   Operation* old_terminator = execute_region->front().getTerminator();\n   builder->setInsertionPointToEnd(&execute_region->front());\n-  builder->create<tf_device::ReturnOp>(old_terminator->getLoc(),\n-                                       merged_execute_launch.getResults());\n+  tf_device::ReturnOp::create(*builder, old_terminator->getLoc(),\n+                              merged_execute_launch.getResults());\n   old_terminator->erase();\n \n   // Remove the original TPUExecute op.\n@@ -532,8 +532,8 @@ LogicalResult MergeForOneTPUExecute(\n   }\n \n   // Create the merged execute and update variables op.\n-  auto merged_execute = builder->create<TF::TPUExecuteAndUpdateVariablesOp>(\n-      execute_launch.getLoc(), new_output_types,\n+  auto merged_execute = TF::TPUExecuteAndUpdateVariablesOp::create(\n+      *builder, execute_launch.getLoc(), new_output_types,\n       var_access_info.new_operand_values,\n       llvm::ArrayRef<NamedAttribute>{\n           builder->getNamedAttr(\n@@ -544,14 +544,14 @@ LogicalResult MergeForOneTPUExecute(\n               builder->getI64ArrayAttr(device_var_updates_indices))});\n \n   // Wrap in launch for device assignment.\n-  auto merged_execute_launch = builder->create<tf_device::LaunchOp>(\n-      merged_execute.getLoc(), execute_launch.getDeviceAttr(),\n+  auto merged_execute_launch = tf_device::LaunchOp::create(\n+      *builder, merged_execute.getLoc(), execute_launch.getDeviceAttr(),\n       merged_execute.getResultTypes());\n   merged_execute_launch.getBody().push_back(new Block);\n \n   builder->setInsertionPointToEnd(&merged_execute_launch.GetBody());\n-  builder->create<tf_device::ReturnOp>(merged_execute.getLoc(),\n-                                       merged_execute.getResults());\n+  tf_device::ReturnOp::create(*builder, merged_execute.getLoc(),\n+                              merged_execute.getResults());\n \n   merged_execute.getOperation()->moveBefore(\n       merged_execute_launch.GetBody().getTerminator());"
        },
        {
            "sha": "0b5976b619ea26666d12cc743005d6de9938f0c5",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/lower_tf.cc",
            "status": "modified",
            "additions": 314,
            "deletions": 299,
            "changes": 613,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Flower_tf.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Flower_tf.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Flower_tf.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -90,7 +90,7 @@ static Value CreateTFCastOpF32(OpBuilder *builder, Location loc, Value x,\n   auto x_type = mlir::dyn_cast_or_null<ShapedType>(x.getType());\n   if (!x_type) llvm_unreachable(\"unsupported type\");\n   Type type = x_type.clone(builder->getF32Type());\n-  return builder->create<CastOp>(loc, type, x, truncate);\n+  return CastOp::create(*builder, loc, type, x, truncate);\n }\n \n // Returns a TF_CastOp to I32. This function is used for CastOps that are\n@@ -103,7 +103,7 @@ static Value CreateTFCastOpI32(OpBuilder *builder, Location loc, Value x,\n   auto x_type = mlir::dyn_cast_or_null<ShapedType>(x.getType());\n   if (!x_type) llvm_unreachable(\"unsupported type\");\n   Type type = x_type.clone(builder->getI32Type());\n-  return builder->create<CastOp>(loc, type, x, truncate);\n+  return CastOp::create(*builder, loc, type, x, truncate);\n }\n \n static APFloat ConvertToAPFloat(double val, Type type) {\n@@ -125,22 +125,24 @@ static Value GetDimensionSize(OpBuilder *builder, Location loc, Value input,\n     }\n     // Return a ConstOp if it's static dimension.\n     if (!ranked_ty.isDynamicDim(idx)) {\n-      return builder->create<TF::ConstOp>(\n-          loc, GetScalarOfType(\n-                   builder->getIntegerType(use_32bit.getValue() ? 32 : 64),\n-                   ranked_ty.getDimSize(idx)));\n+      return TF::ConstOp::create(\n+          *builder, loc,\n+          GetScalarOfType(\n+              builder->getIntegerType(use_32bit.getValue() ? 32 : 64),\n+              ranked_ty.getDimSize(idx)));\n     }\n   }\n \n-  auto shape = builder->create<TF::ShapeOp>(loc, input, use_32bit);\n-  return builder->create<TF::StridedSliceOp>(\n-      loc, mlir::RankedTensorType::get({}, getElementTypeOrSelf(shape)), shape,\n+  auto shape = TF::ShapeOp::create(*builder, loc, input, use_32bit);\n+  return TF::StridedSliceOp::create(\n+      *builder, loc,\n+      mlir::RankedTensorType::get({}, getElementTypeOrSelf(shape)), shape,\n       /*begin=*/\n-      builder->create<TF::ConstOp>(loc, builder->getI32TensorAttr({idx})),\n+      TF::ConstOp::create(*builder, loc, builder->getI32TensorAttr({idx})),\n       /*end=*/\n-      builder->create<TF::ConstOp>(loc, builder->getI32TensorAttr({idx + 1})),\n+      TF::ConstOp::create(*builder, loc, builder->getI32TensorAttr({idx + 1})),\n       /*strides=*/\n-      builder->create<TF::ConstOp>(loc, builder->getI32TensorAttr({1})),\n+      TF::ConstOp::create(*builder, loc, builder->getI32TensorAttr({1})),\n       /*begin_mask=*/0, /*end_mask=*/0, /*ellipsis_mask=*/0,\n       /*new_axis_mask=*/0, /*shrink_axis_mask=*/1);\n }\n@@ -211,9 +213,9 @@ Value ValuesToRank1(PatternRewriter &rewriter, Location loc, Type dtype,\n                     ArrayRef<Value> vals) {\n   int64_t length = vals.size();\n   auto type = tensorflow::GetTypeFromTFTensorShape({length}, dtype);\n-  auto axis = rewriter.create<ConstOp>(\n-      loc, GetScalarOfType(rewriter.getIntegerType(64), 0));\n-  return rewriter.create<ConcatV2Op>(loc, type, ValueRange(vals), axis);\n+  auto axis = ConstOp::create(rewriter, loc,\n+                              GetScalarOfType(rewriter.getIntegerType(64), 0));\n+  return ConcatV2Op::create(rewriter, loc, type, ValueRange(vals), axis);\n }\n \n // Lowers AddN op to a sequence of AddV2 ops to accumulate operands.\n@@ -277,10 +279,10 @@ class LowerAddNOp : public RewritePattern {\n     while (n > 1) {\n       for (int64_t i = 0; i < n; i += 2) {\n         // Add two adjacent operands if applicable.\n-        operands[i / 2] =\n-            (i + 1 < n) ? rewriter.create<AddV2Op>(addn_op.getLoc(),\n-                                                   operands[i], operands[i + 1])\n-                        : operands[i];\n+        operands[i / 2] = (i + 1 < n)\n+                              ? AddV2Op::create(rewriter, addn_op.getLoc(),\n+                                                operands[i], operands[i + 1])\n+                              : operands[i];\n       }\n       n = (n + 1) / 2;\n     }\n@@ -363,8 +365,8 @@ class LowerDynamicStitchOp : public RewritePattern {\n     packed_shape.push_back(-1);\n     packed_shape.append(item_shape.begin(), item_shape.end());\n     Location loc = op.getLoc();\n-    auto packed_shape_val = rewriter.create<ConstOp>(\n-        loc, GetI64ElementsAttr(packed_shape, &rewriter));\n+    auto packed_shape_val = ConstOp::create(\n+        rewriter, loc, GetI64ElementsAttr(packed_shape, &rewriter));\n \n     // Prepare each of the output item by unpacking data and then putting it to\n     // the specified index.\n@@ -374,12 +376,13 @@ class LowerDynamicStitchOp : public RewritePattern {\n       Value data = std::get<1>(it);\n \n       auto reshaped_data =\n-          rewriter.create<ReshapeOp>(loc, data, packed_shape_val);\n+          ReshapeOp::create(rewriter, loc, data, packed_shape_val);\n       auto num_items =\n           mlir::cast<RankedTensorType>(reshaped_data.getType()).getShape()[0];\n-      auto items = rewriter.create<UnpackOp>(\n-          loc, SmallVector<Type, 4>(num_items, item_ty), reshaped_data,\n-          /*axis=*/0);\n+      auto items = UnpackOp::create(rewriter, loc,\n+                                    SmallVector<Type, 4>(num_items, item_ty),\n+                                    reshaped_data,\n+                                    /*axis=*/0);\n       for (auto index_item : llvm::zip(index_attr, items.getResults())) {\n         int64_t output_index = std::get<0>(index_item).getSExtValue();\n         Value item = std::get<1>(index_item);\n@@ -426,80 +429,84 @@ class ConvertFakeQuantWithMinMaxVarsOp : public RewritePattern {\n     auto float_min = op.getMin();\n     auto float_max = op.getMax();\n \n-    auto float_diff = rewriter.create<SubOp>(op.getLoc(), float_max, float_min);\n+    auto float_diff =\n+        SubOp::create(rewriter, op.getLoc(), float_max, float_min);\n \n     // Compute the range when quantized.\n-    auto quant_min = rewriter.create<ConstOp>(\n-        op.getLoc(), DenseElementsAttr::get(\n-                         scalar_ty, ConvertToAPFloat(bits_min, element_ty)));\n-\n-    auto quant_max = rewriter.create<ConstOp>(\n-        op.getLoc(), DenseElementsAttr::get(\n-                         scalar_ty, ConvertToAPFloat(bits_max, element_ty)));\n-\n-    auto quant_diff = rewriter.create<ConstOp>(\n-        op.getLoc(),\n+    auto quant_min =\n+        ConstOp::create(rewriter, op.getLoc(),\n+                        DenseElementsAttr::get(\n+                            scalar_ty, ConvertToAPFloat(bits_min, element_ty)));\n+\n+    auto quant_max =\n+        ConstOp::create(rewriter, op.getLoc(),\n+                        DenseElementsAttr::get(\n+                            scalar_ty, ConvertToAPFloat(bits_max, element_ty)));\n+\n+    auto quant_diff = ConstOp::create(\n+        rewriter, op.getLoc(),\n         DenseElementsAttr::get(\n             scalar_ty, ConvertToAPFloat(bits_max - bits_min, element_ty)));\n \n     auto quant_to_float =\n-        rewriter.create<DivOp>(op.getLoc(), float_diff, quant_diff);\n+        DivOp::create(rewriter, op.getLoc(), float_diff, quant_diff);\n \n     auto float_to_quant =\n-        rewriter.create<DivOp>(op.getLoc(), quant_diff, float_diff);\n+        DivOp::create(rewriter, op.getLoc(), quant_diff, float_diff);\n \n     // During quantization, the quantized min/max values may not line up\n     // perfectly with the specified min/max. Nudge them into the right range.\n     auto min_scaled =\n-        rewriter.create<DivOp>(op.getLoc(), float_min, quant_to_float);\n+        DivOp::create(rewriter, op.getLoc(), float_min, quant_to_float);\n     auto min_scaled_sub =\n-        rewriter.create<SubOp>(op.getLoc(), quant_min, min_scaled);\n+        SubOp::create(rewriter, op.getLoc(), quant_min, min_scaled);\n \n     auto mid_rounded =\n-        rewriter.create<RoundOp>(op.getLoc(), scalar_ty, min_scaled_sub);\n+        RoundOp::create(rewriter, op.getLoc(), scalar_ty, min_scaled_sub);\n \n-    auto nudged_zero_point_val = rewriter.create<ClipByValueOp>(\n-        op.getLoc(), scalar_ty, mid_rounded, quant_min, quant_max);\n+    auto nudged_zero_point_val = ClipByValueOp::create(\n+        rewriter, op.getLoc(), scalar_ty, mid_rounded, quant_min, quant_max);\n \n     auto quant_min_sub =\n-        rewriter.create<SubOp>(op.getLoc(), quant_min, nudged_zero_point_val);\n+        SubOp::create(rewriter, op.getLoc(), quant_min, nudged_zero_point_val);\n     auto quant_max_sub =\n-        rewriter.create<SubOp>(op.getLoc(), quant_max, nudged_zero_point_val);\n+        SubOp::create(rewriter, op.getLoc(), quant_max, nudged_zero_point_val);\n \n     auto nudged_float_min =\n-        rewriter.create<MulOp>(op.getLoc(), quant_min_sub, quant_to_float);\n+        MulOp::create(rewriter, op.getLoc(), quant_min_sub, quant_to_float);\n \n     auto nudged_float_max =\n-        rewriter.create<MulOp>(op.getLoc(), quant_max_sub, quant_to_float);\n+        MulOp::create(rewriter, op.getLoc(), quant_max_sub, quant_to_float);\n \n     // Now quantize the input value with the approximated min/max values.\n \n     // Move the input value into quantized space\n-    Value quantized_input = rewriter.create<ClipByValueOp>(\n-        op.getLoc(), input_ty, input, nudged_float_min, nudged_float_max);\n+    Value quantized_input =\n+        ClipByValueOp::create(rewriter, op.getLoc(), input_ty, input,\n+                              nudged_float_min, nudged_float_max);\n \n-    quantized_input = rewriter.create<SubOp>(op.getLoc(), input_ty,\n-                                             quantized_input, nudged_float_min);\n+    quantized_input = SubOp::create(rewriter, op.getLoc(), input_ty,\n+                                    quantized_input, nudged_float_min);\n \n-    quantized_input = rewriter.create<MulOp>(op.getLoc(), input_ty,\n-                                             quantized_input, float_to_quant);\n+    quantized_input = MulOp::create(rewriter, op.getLoc(), input_ty,\n+                                    quantized_input, float_to_quant);\n \n     // Round the quantized input always to the positive direction.\n-    auto half_val = rewriter.create<ConstOp>(\n-        op.getLoc(),\n+    auto half_val = ConstOp::create(\n+        rewriter, op.getLoc(),\n         DenseElementsAttr::get(scalar_ty, ConvertToAPFloat(0.5, element_ty)));\n \n-    quantized_input = rewriter.create<AddV2Op>(op.getLoc(), input_ty,\n-                                               quantized_input, half_val);\n+    quantized_input = AddV2Op::create(rewriter, op.getLoc(), input_ty,\n+                                      quantized_input, half_val);\n \n-    quantized_input = rewriter.create<FloorOp>(op.getLoc(), quantized_input);\n+    quantized_input = FloorOp::create(rewriter, op.getLoc(), quantized_input);\n \n     // Convert back into floating point spae.\n-    Value output = rewriter.create<MulOp>(op.getLoc(), input_ty,\n-                                          quantized_input, quant_to_float);\n+    Value output = MulOp::create(rewriter, op.getLoc(), input_ty,\n+                                 quantized_input, quant_to_float);\n \n-    output = rewriter.create<AddV2Op>(op.getLoc(), input_ty, output,\n-                                      nudged_float_min);\n+    output = AddV2Op::create(rewriter, op.getLoc(), input_ty, output,\n+                             nudged_float_min);\n \n     rewriter.replaceOp(op, {output});\n     return success();\n@@ -549,20 +556,21 @@ class LowerInvertPermutationOp : public RewritePattern {\n     Type int_type = x_type.getElementType();  // Could be i32 or i64.\n \n     auto result_type = x_type;\n-    auto start = rewriter.create<ConstOp>(loc, GetScalarOfType(int_type, 0));\n-    Value limit = rewriter.create<ConstOp>(\n-        loc, GetScalarOfType(int_type, x_type.getShape()[0]));\n-    auto delta = rewriter.create<ConstOp>(loc, GetScalarOfType(int_type, 1));\n+    auto start = ConstOp::create(rewriter, loc, GetScalarOfType(int_type, 0));\n+    Value limit = ConstOp::create(\n+        rewriter, loc, GetScalarOfType(int_type, x_type.getShape()[0]));\n+    auto delta = ConstOp::create(rewriter, loc, GetScalarOfType(int_type, 1));\n     // Construct a sequence of numbers [0, 1, ... len(x)-1].\n     auto updates =\n-        rewriter.create<RangeOp>(loc, result_type, start, limit, delta);\n+        RangeOp::create(rewriter, loc, result_type, start, limit, delta);\n \n     auto shape_type =\n         tensorflow::GetTypeFromTFTensorShape({2}, rewriter.getIntegerType(32));\n-    auto shape = rewriter.create<ConstOp>(\n-        loc, DenseElementsAttr::get(\n-                 shape_type, {static_cast<int>(x_type.getDimSize(0)), 1}));\n-    auto indices = rewriter.create<ReshapeOp>(loc, op.getX(), shape);\n+    auto shape = ConstOp::create(\n+        rewriter, loc,\n+        DenseElementsAttr::get(shape_type,\n+                               {static_cast<int>(x_type.getDimSize(0)), 1}));\n+    auto indices = ReshapeOp::create(rewriter, loc, op.getX(), shape);\n \n     rewriter.replaceOpWithNewOp<TensorScatterUpdateOp>(\n         op, result_type, op.getX(), indices, updates);\n@@ -641,16 +649,17 @@ class LowerLgammaOp : public RewritePattern {\n       } else {\n         tensor_type = UnrankedTensorType::get(float_type);\n       }\n-      input = rewriter.create<CastOp>(loc, tensor_type, input);\n+      input = CastOp::create(rewriter, loc, tensor_type, input);\n     }\n \n     // Helper lambda function for creating a ConstOp for a tensor filled with\n     // the given constant float value.\n     auto create_const_op = [&rewriter, loc, tensor_type,\n                             float_type](double value) {\n-      return rewriter.create<ConstOp>(\n-          loc, DenseElementsAttr::get(tensor_type,\n-                                      FloatAttr::get(float_type, value)));\n+      return ConstOp::create(\n+          rewriter, loc,\n+          DenseElementsAttr::get(tensor_type,\n+                                 FloatAttr::get(float_type, value)));\n     };\n \n     Value one_half = create_const_op(0.5);\n@@ -664,41 +673,41 @@ class LowerLgammaOp : public RewritePattern {\n         create_const_op(std::log(kLanczosGamma + 0.5));\n     Value base_lanczos_coeff = create_const_op(kBaseLanczosCoeff);\n \n-    Value minus_input = rewriter.create<NegOp>(loc, input);\n-    Value input_minus_one = rewriter.create<SubOp>(loc, input, one);\n+    Value minus_input = NegOp::create(rewriter, loc, input);\n+    Value input_minus_one = SubOp::create(rewriter, loc, input, one);\n \n     // If the input is less than 0.5 use Euler's reflection formula:\n     // gamma(x) = pi / (sin(pi * x) * gamma(1 - x))\n-    Value need_to_reflect = rewriter.create<LessOp>(loc, input, one_half);\n+    Value need_to_reflect = LessOp::create(rewriter, loc, input, one_half);\n     Type tensor_bool_type = need_to_reflect.getType();\n-    Value z = rewriter.create<SelectV2Op>(loc, need_to_reflect, minus_input,\n-                                          input_minus_one);\n+    Value z = SelectV2Op::create(rewriter, loc, need_to_reflect, minus_input,\n+                                 input_minus_one);\n \n     Value x = base_lanczos_coeff;\n     for (int i = 0, end = kLanczosCoefficients.size(); i < end; ++i) {\n       Value lanczos_coefficient = create_const_op(kLanczosCoefficients[i]);\n       Value index = create_const_op(static_cast<double>(i));\n-      Value z_plus_index = rewriter.create<AddV2Op>(loc, z, index);\n+      Value z_plus_index = AddV2Op::create(rewriter, loc, z, index);\n       Value z_plus_index_plus_one =\n-          rewriter.create<AddV2Op>(loc, z_plus_index, one);\n-      Value incr = rewriter.create<DivOp>(loc, lanczos_coefficient,\n-                                          z_plus_index_plus_one);\n-      x = rewriter.create<AddV2Op>(loc, x, incr);\n+          AddV2Op::create(rewriter, loc, z_plus_index, one);\n+      Value incr = DivOp::create(rewriter, loc, lanczos_coefficient,\n+                                 z_plus_index_plus_one);\n+      x = AddV2Op::create(rewriter, loc, x, incr);\n     }\n \n     // To improve accuracy on platforms with less-precise log implementations,\n     // compute log(lanczos_gamma_plus_one_half) at compile time and use log1p on\n     // the device.\n     // log(t) = log(kLanczosGamma + 0.5 + z)\n     //        = log(kLanczosGamma + 0.5) + log1p(z / (kLanczosGamma + 0.5))\n-    Value t = rewriter.create<AddV2Op>(loc, lanczos_gamma_plus_one_half, z);\n+    Value t = AddV2Op::create(rewriter, loc, lanczos_gamma_plus_one_half, z);\n     Value z_div_lanczos_gamma_plus_one_half =\n-        rewriter.create<DivOp>(loc, z, lanczos_gamma_plus_one_half);\n+        DivOp::create(rewriter, loc, z, lanczos_gamma_plus_one_half);\n     Value log1p_z_div_lanczos_gamma_plus_one_half =\n-        rewriter.create<Log1pOp>(loc, z_div_lanczos_gamma_plus_one_half);\n+        Log1pOp::create(rewriter, loc, z_div_lanczos_gamma_plus_one_half);\n     Value log_t =\n-        rewriter.create<AddV2Op>(loc, log_lanczos_gamma_plus_one_half,\n-                                 log1p_z_div_lanczos_gamma_plus_one_half);\n+        AddV2Op::create(rewriter, loc, log_lanczos_gamma_plus_one_half,\n+                        log1p_z_div_lanczos_gamma_plus_one_half);\n \n     // Compute the final result (modulo reflection).  t(z) may be large, and we\n     // need to be careful not to overflow to infinity in the first term of\n@@ -710,17 +719,17 @@ class LowerLgammaOp : public RewritePattern {\n     //   (z + 1/2 - t(z) / log(t(z))) * log(t(z)).\n     //\n     // log_y = log_sqrt_two_pi + (z + one_half - t / log_t) * log_t + Log(x);\n-    Value t_div_log_t = rewriter.create<DivOp>(loc, t, log_t);\n+    Value t_div_log_t = DivOp::create(rewriter, loc, t, log_t);\n     Value one_half_minus_t_div_log_t =\n-        rewriter.create<SubOp>(loc, one_half, t_div_log_t);\n+        SubOp::create(rewriter, loc, one_half, t_div_log_t);\n     Value z_plus_one_half_minus_t_div_log_t =\n-        rewriter.create<AddV2Op>(loc, z, one_half_minus_t_div_log_t);\n+        AddV2Op::create(rewriter, loc, z, one_half_minus_t_div_log_t);\n     Value z_plus_one_half_minus_t_div_log_t_mul_log_t =\n-        rewriter.create<MulOp>(loc, z_plus_one_half_minus_t_div_log_t, log_t);\n-    Value log_x = rewriter.create<LogOp>(loc, x);\n-    Value log_y_rhs = rewriter.create<AddV2Op>(\n-        loc, z_plus_one_half_minus_t_div_log_t_mul_log_t, log_x);\n-    Value log_y = rewriter.create<AddV2Op>(loc, log_sqrt_two_pi, log_y_rhs);\n+        MulOp::create(rewriter, loc, z_plus_one_half_minus_t_div_log_t, log_t);\n+    Value log_x = LogOp::create(rewriter, loc, x);\n+    Value log_y_rhs = AddV2Op::create(\n+        rewriter, loc, z_plus_one_half_minus_t_div_log_t_mul_log_t, log_x);\n+    Value log_y = AddV2Op::create(rewriter, loc, log_sqrt_two_pi, log_y_rhs);\n \n     // Compute the reflected value, used when x < 0.5:\n     //\n@@ -747,48 +756,48 @@ class LowerLgammaOp : public RewritePattern {\n     // Furthermore, pi * abs(frac(x)) loses precision when abs(frac(x)) is close\n     // to 1.  To remedy this, we can use the fact that sin(pi * x) in the domain\n     // [0, 1] is symmetric across the line Y=0.5.\n-    Value abs_input = rewriter.create<AbsOp>(loc, input);\n-    Value abs_input_floor = rewriter.create<FloorOp>(loc, abs_input);\n+    Value abs_input = AbsOp::create(rewriter, loc, input);\n+    Value abs_input_floor = FloorOp::create(rewriter, loc, abs_input);\n     Value abs_frac_input =\n-        rewriter.create<SubOp>(loc, abs_input, abs_input_floor);\n+        SubOp::create(rewriter, loc, abs_input, abs_input_floor);\n \n     // Convert values of abs_frac_input > 0.5 to (1 - frac_input) to improve\n     // precision of pi * abs_frac_input for values of abs_frac_input close to 1.\n     Value one_minus_abs_frac_input =\n-        rewriter.create<SubOp>(loc, one, abs_frac_input);\n+        SubOp::create(rewriter, loc, one, abs_frac_input);\n     Value abs_frac_input_gt_one_half =\n-        rewriter.create<GreaterOp>(loc, abs_frac_input, one_half);\n+        GreaterOp::create(rewriter, loc, abs_frac_input, one_half);\n     Value reduced_frac_input =\n-        rewriter.create<SelectV2Op>(loc, abs_frac_input_gt_one_half,\n-                                    one_minus_abs_frac_input, abs_frac_input);\n+        SelectV2Op::create(rewriter, loc, abs_frac_input_gt_one_half,\n+                           one_minus_abs_frac_input, abs_frac_input);\n     Value pi_mul_reduced_frac_input =\n-        rewriter.create<MulOp>(loc, pi, reduced_frac_input);\n+        MulOp::create(rewriter, loc, pi, reduced_frac_input);\n     Value sin_pi_mul_reduced_frac_input =\n-        rewriter.create<SinOp>(loc, pi_mul_reduced_frac_input);\n+        SinOp::create(rewriter, loc, pi_mul_reduced_frac_input);\n     Value reflection_denom =\n-        rewriter.create<LogOp>(loc, sin_pi_mul_reduced_frac_input);\n+        LogOp::create(rewriter, loc, sin_pi_mul_reduced_frac_input);\n \n     // Avoid computing -inf - inf, which is nan.  If reflection_denom is +/-inf,\n     // then it \"wins\" and the result is +/-inf.\n     Value is_finite =\n-        rewriter.create<IsFiniteOp>(loc, tensor_bool_type, reflection_denom);\n-    Value neg_reflection_denom = rewriter.create<NegOp>(loc, reflection_denom);\n+        IsFiniteOp::create(rewriter, loc, tensor_bool_type, reflection_denom);\n+    Value neg_reflection_denom = NegOp::create(rewriter, loc, reflection_denom);\n     Value log_pi_minus_reflection_denom =\n-        rewriter.create<SubOp>(loc, log_pi, reflection_denom);\n+        SubOp::create(rewriter, loc, log_pi, reflection_denom);\n     Value reflection_if_finite =\n-        rewriter.create<SubOp>(loc, log_pi_minus_reflection_denom, log_y);\n-    Value reflection = rewriter.create<SelectV2Op>(\n-        loc, is_finite, reflection_if_finite, neg_reflection_denom);\n+        SubOp::create(rewriter, loc, log_pi_minus_reflection_denom, log_y);\n+    Value reflection = SelectV2Op::create(\n+        rewriter, loc, is_finite, reflection_if_finite, neg_reflection_denom);\n \n     Value result =\n-        rewriter.create<SelectV2Op>(loc, need_to_reflect, reflection, log_y);\n+        SelectV2Op::create(rewriter, loc, need_to_reflect, reflection, log_y);\n \n     // lgamma(+/-inf) = +inf.\n-    Value is_inf = rewriter.create<IsInfOp>(loc, tensor_bool_type, input);\n-    result = rewriter.create<SelectV2Op>(loc, is_inf, infinity, result);\n+    Value is_inf = IsInfOp::create(rewriter, loc, tensor_bool_type, input);\n+    result = SelectV2Op::create(rewriter, loc, is_inf, infinity, result);\n \n     if (needs_cast) {\n-      result = rewriter.create<CastOp>(loc, original_tensor_type, result);\n+      result = CastOp::create(rewriter, loc, original_tensor_type, result);\n     }\n \n     rewriter.replaceOp(op, result);\n@@ -819,10 +828,11 @@ class LowerPackOp : public RewritePattern {\n     auto op = cast<PackOp>(src_op);\n \n     Location loc = op.getLoc();\n-    auto axis_value = rewriter.create<ConstOp>(\n-        loc, DenseElementsAttr::get(tensorflow::GetTypeFromTFTensorShape(\n-                                        {}, rewriter.getIntegerType(64)),\n-                                    op.getAxis()));\n+    auto axis_value = ConstOp::create(\n+        rewriter, loc,\n+        DenseElementsAttr::get(tensorflow::GetTypeFromTFTensorShape(\n+                                   {}, rewriter.getIntegerType(64)),\n+                               op.getAxis()));\n     int64_t axis = op.getAxis();\n \n     Type prev_input_ty, inferred_ty;\n@@ -838,7 +848,7 @@ class LowerPackOp : public RewritePattern {\n         prev_input_ty = input_ty;\n       }\n       expanded_inputs.push_back(\n-          rewriter.create<ExpandDimsOp>(loc, inferred_ty, input, axis_value));\n+          ExpandDimsOp::create(rewriter, loc, inferred_ty, input, axis_value));\n     }\n \n     rewriter.replaceOpWithNewOp<ConcatV2Op>(op, op.getType(), expanded_inputs,\n@@ -922,28 +932,28 @@ class LowerSpaceToBatchNDOp : public RewritePattern {\n     auto block_shape_i64_type = tensorflow::GetTypeFromTFTensorShape(\n         block_shape_type.getShape(), rewriter.getIntegerType(64));\n     auto block_shape_i64 =\n-        rewriter.create<CastOp>(loc, block_shape_i64_type, op.getBlockShape());\n+        CastOp::create(rewriter, loc, block_shape_i64_type, op.getBlockShape());\n \n     auto paddings_i64_type = tensorflow::GetTypeFromTFTensorShape(\n         paddings_type.getShape(), rewriter.getIntegerType(64));\n     auto paddings_i64 =\n-        rewriter.create<CastOp>(loc, paddings_i64_type, op.getPaddings());\n+        CastOp::create(rewriter, loc, paddings_i64_type, op.getPaddings());\n \n-    auto pad00 = rewriter.create<ConstOp>(\n-        loc, DenseElementsAttr::get<int64_t>(\n-                 tensorflow::GetTypeFromTFTensorShape(\n-                     {1, 2}, rewriter.getIntegerType(64)),\n-                 {0, 0}));\n+    auto pad00 = ConstOp::create(rewriter, loc,\n+                                 DenseElementsAttr::get<int64_t>(\n+                                     tensorflow::GetTypeFromTFTensorShape(\n+                                         {1, 2}, rewriter.getIntegerType(64)),\n+                                     {0, 0}));\n     SmallVector<Value, 4> full_paddings_list{pad00, paddings_i64};\n     full_paddings_list.append(remaining_rank, pad00);\n     auto full_paddings_type = tensorflow::GetTypeFromTFTensorShape(\n         {input_rank, 2}, rewriter.getIntegerType(64));\n-    auto zero_i64 = rewriter.create<ConstOp>(\n-        loc, GetScalarOfType(rewriter.getIntegerType(64), 0));\n+    auto zero_i64 = ConstOp::create(\n+        rewriter, loc, GetScalarOfType(rewriter.getIntegerType(64), 0));\n     // Extends paddings to all dimensions of input by adding 0s to non-block\n     // dimensions.\n-    auto full_paddings = rewriter.create<ConcatV2Op>(\n-        loc, full_paddings_type, full_paddings_list, zero_i64);\n+    auto full_paddings = ConcatV2Op::create(rewriter, loc, full_paddings_type,\n+                                            full_paddings_list, zero_i64);\n \n     // Compute the result type here instead of using shape inference because the\n     // full_paddings won't be available as a constant for shape inference.\n@@ -973,54 +983,53 @@ class LowerSpaceToBatchNDOp : public RewritePattern {\n         tensorflow::GetTypeFromTFTensorShape(padded_shape, element_type);\n     // padded = pad(input, full_paddings)\n     auto padded =\n-        rewriter.create<PadOp>(loc, padded_type, op.getInput(), full_paddings);\n+        PadOp::create(rewriter, loc, padded_type, op.getInput(), full_paddings);\n \n     auto paddings_sum_type = tensorflow::GetTypeFromTFTensorShape(\n         {input_rank}, rewriter.getIntegerType(64));\n     // paddings_sum = paddings[*,0] + paddings[*,1]\n-    auto paddings_split = rewriter.create<UnpackOp>(\n-        loc, TypeRange({paddings_sum_type, paddings_sum_type}), full_paddings,\n-        rewriter.getI64IntegerAttr(1));\n-    auto paddings_sum = rewriter.create<AddV2Op>(\n-        loc, paddings_split.getResult(0), paddings_split.getResult(1));\n-\n-    auto input_shape_tensor = rewriter.create<ConstOp>(\n-        loc,\n+    auto paddings_split = UnpackOp::create(\n+        rewriter, loc, TypeRange({paddings_sum_type, paddings_sum_type}),\n+        full_paddings, rewriter.getI64IntegerAttr(1));\n+    auto paddings_sum =\n+        AddV2Op::create(rewriter, loc, paddings_split.getResult(0),\n+                        paddings_split.getResult(1));\n+\n+    auto input_shape_tensor = ConstOp::create(\n+        rewriter, loc,\n         DenseElementsAttr::get(tensorflow::GetTypeFromTFTensorShape(\n                                    {input_rank}, rewriter.getIntegerType(64)),\n                                input_shape));\n \n     // padded_shape_tensor is the shape of padded.\n     auto padded_shape_tensor =\n-        rewriter.create<AddV2Op>(loc, paddings_sum, input_shape_tensor);\n+        AddV2Op::create(rewriter, loc, paddings_sum, input_shape_tensor);\n \n-    auto zero_i32 = rewriter.create<ConstOp>(\n-        loc, GetScalarOfType(rewriter.getIntegerType(32), 0));\n+    auto zero_i32 = ConstOp::create(\n+        rewriter, loc, GetScalarOfType(rewriter.getIntegerType(32), 0));\n     SmallVector<Type, 4> padded_shape_splits_types(\n         input_rank,\n         tensorflow::GetTypeFromTFTensorShape({1}, rewriter.getIntegerType(64)));\n     SmallVector<Value, 4> padded_shape_splits(\n-        rewriter\n-            .create<SplitOp>(loc, padded_shape_splits_types, zero_i32,\n-                             padded_shape_tensor)\n+        SplitOp::create(rewriter, loc, padded_shape_splits_types, zero_i32,\n+                        padded_shape_tensor)\n             .getOutput());\n \n     SmallVector<Type, 4> block_shape_splits_types(\n         block_rank,\n         tensorflow::GetTypeFromTFTensorShape({1}, rewriter.getIntegerType(64)));\n     SmallVector<Value, 4> block_shape_splits(\n-        rewriter\n-            .create<SplitOp>(loc, block_shape_splits_types, zero_i32,\n-                             block_shape_i64)\n+        SplitOp::create(rewriter, loc, block_shape_splits_types, zero_i32,\n+                        block_shape_i64)\n             .getOutput());\n \n     SmallVector<int64_t, 4> outer_shape_ints;\n     SmallVector<Value, 4> outer_shape_vals;\n     for (int64_t i = 0; i < block_rank; ++i) {\n       // TODO(b/157475606): Insert tf.Assert that the following division has\n       // remainder 0.\n-      outer_shape_vals.push_back(rewriter.create<DivOp>(\n-          loc, padded_shape_splits[1 + i], block_shape_splits[i]));\n+      outer_shape_vals.push_back(DivOp::create(\n+          rewriter, loc, padded_shape_splits[1 + i], block_shape_splits[i]));\n \n       auto padded_shape_i = padded_shape[1 + i];\n       auto block_shape_ints_i = block_shape_ints[i];\n@@ -1049,8 +1058,8 @@ class LowerSpaceToBatchNDOp : public RewritePattern {\n     auto reshaped_shape = ValuesToRank1(\n         rewriter, loc, rewriter.getIntegerType(64), reshaped_shape_vals);\n \n-    auto reshaped = rewriter.create<ReshapeOp>(\n-        loc,\n+    auto reshaped = ReshapeOp::create(\n+        rewriter, loc,\n         tensorflow::GetTypeFromTFTensorShape(reshaped_shape_ints, element_type),\n         padded, reshaped_shape);\n \n@@ -1065,14 +1074,14 @@ class LowerSpaceToBatchNDOp : public RewritePattern {\n     for (int64_t i = 1 + block_rank; i < input_rank; ++i) {\n       permutation_vals.push_back(block_rank + i);\n     }\n-    auto permutation = rewriter.create<ConstOp>(\n-        loc, GetI64ElementsAttr(permutation_vals, &rewriter));\n+    auto permutation = ConstOp::create(\n+        rewriter, loc, GetI64ElementsAttr(permutation_vals, &rewriter));\n \n-    auto permuted = rewriter.create<TransposeOp>(loc, reshaped, permutation);\n+    auto permuted = TransposeOp::create(rewriter, loc, reshaped, permutation);\n     auto output_batch = padded_shape_splits[0];\n     for (int64_t i = 0; i < block_rank; ++i) {\n       output_batch =\n-          rewriter.create<MulOp>(loc, output_batch, block_shape_splits[i]);\n+          MulOp::create(rewriter, loc, output_batch, block_shape_splits[i]);\n     }\n     SmallVector<Value, 4> output_shape_vals{output_batch};\n     for (int64_t i = 0; i < block_rank; ++i) {\n@@ -1163,11 +1172,11 @@ class LowerBatchToSpaceND : public RewritePattern {\n     std::copy(input_shape.begin() + 1, input_shape.end(),\n               reshaped_shape.begin() + block_rank + 1);\n \n-    auto reshaped = rewriter.create<TF::ReshapeOp>(\n-        op.getLoc(),\n+    auto reshaped = TF::ReshapeOp::create(\n+        rewriter, op.getLoc(),\n         tensorflow::GetTypeFromTFTensorShape(reshaped_shape, element_ty), input,\n-        rewriter.create<ConstOp>(op.getLoc(),\n-                                 rewriter.getI64TensorAttr(reshaped_shape)));\n+        ConstOp::create(rewriter, op.getLoc(),\n+                        rewriter.getI64TensorAttr(reshaped_shape)));\n \n     // 2. Permute dimensions of `reshaped` to produce `permuted` of shape\n     //      [batch / prod(block_shape),\n@@ -1191,12 +1200,12 @@ class LowerBatchToSpaceND : public RewritePattern {\n       transpose_shape[it.index()] = reshaped_shape[it.value()];\n     }\n \n-    auto permuted = rewriter.create<TF::TransposeOp>(\n-        op.getLoc(),\n+    auto permuted = TF::TransposeOp::create(\n+        rewriter, op.getLoc(),\n         tensorflow::GetTypeFromTFTensorShape(transpose_shape, element_ty),\n         reshaped,\n-        rewriter.create<ConstOp>(op.getLoc(),\n-                                 rewriter.getI64TensorAttr(permutation)));\n+        ConstOp::create(rewriter, op.getLoc(),\n+                        rewriter.getI64TensorAttr(permutation)));\n \n     // 3. Reshape `permuted` to produce `reshaped_permuted` of shape\n     //      [batch / prod(block_shape),\n@@ -1219,13 +1228,13 @@ class LowerBatchToSpaceND : public RewritePattern {\n     std::copy(remainder_shape.begin(), remainder_shape.end(),\n               reshaped_permuted_shape.begin() + 1 + block_rank);\n \n-    auto reshaped_permuted = rewriter.create<TF::ReshapeOp>(\n-        op.getLoc(),\n+    auto reshaped_permuted = TF::ReshapeOp::create(\n+        rewriter, op.getLoc(),\n         tensorflow::GetTypeFromTFTensorShape(reshaped_permuted_shape,\n                                              element_ty),\n         permuted,\n-        rewriter.create<ConstOp>(\n-            op.getLoc(), rewriter.getI64TensorAttr(reshaped_permuted_shape)));\n+        ConstOp::create(rewriter, op.getLoc(),\n+                        rewriter.getI64TensorAttr(reshaped_permuted_shape)));\n \n     // 4. Crop the start and end of dimensions `[1, ..., M]` of\n     //    `reshaped_permuted` according to `crops` to produce the output of\n@@ -1263,10 +1272,10 @@ class LowerBatchToSpaceND : public RewritePattern {\n     rewriter.replaceOpWithNewOp<TF::SliceOp>(\n         op, tensorflow::GetTypeFromTFTensorShape(slice_sizes, element_ty),\n         reshaped_permuted,\n-        rewriter.create<ConstOp>(op.getLoc(),\n-                                 rewriter.getI64TensorAttr(start_indices)),\n-        rewriter.create<ConstOp>(op.getLoc(),\n-                                 rewriter.getI64TensorAttr(slice_sizes)));\n+        ConstOp::create(rewriter, op.getLoc(),\n+                        rewriter.getI64TensorAttr(start_indices)),\n+        ConstOp::create(rewriter, op.getLoc(),\n+                        rewriter.getI64TensorAttr(slice_sizes)));\n     return success();\n   }\n };\n@@ -1310,11 +1319,11 @@ class LowerSparseMatMulOp : public RewritePattern {\n         tensor_type_f32 = UnrankedTensorType::get(Float32Type::get(context));\n       }\n       // Add cast to f32 to conform with element type of result.\n-      operand = rewriter.create<CastOp>(op.getLoc(), tensor_type_f32, operand);\n+      operand = CastOp::create(rewriter, op.getLoc(), tensor_type_f32, operand);\n     }\n-    Value result = rewriter.create<MatMulOp>(\n-        op.getLoc(), op.getProduct().getType(), operands[0], operands[1],\n-        op.getTransposeA(), op.getTransposeB());\n+    Value result = MatMulOp::create(\n+        rewriter, op.getLoc(), op.getProduct().getType(), operands[0],\n+        operands[1], op.getTransposeA(), op.getTransposeB());\n \n     rewriter.replaceOp(op, {result});\n     return success();\n@@ -1441,20 +1450,22 @@ class LowerResizeNearestNeighbor : public RewritePattern {\n     }\n \n     auto one =\n-        rewriter.create<ConstOp>(loc, GetScalarOfType(out_size_element_ty, 1));\n+        ConstOp::create(rewriter, loc, GetScalarOfType(out_size_element_ty, 1));\n \n     // Extract the image shape.\n-    Value input_shape = rewriter.create<ShapeOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({4}, rewriter.getI64Type()),\n+    Value input_shape = ShapeOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({4}, rewriter.getI64Type()),\n         input);\n-    input_shape = rewriter.create<CastOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({4}, out_size_element_ty),\n+    input_shape = CastOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({4}, out_size_element_ty),\n         input_shape);\n \n     auto scalar_dim_ty =\n         tensorflow::GetTypeFromTFTensorShape({}, out_size_element_ty);\n-    auto split_image_shape = rewriter.create<UnpackOp>(\n-        loc,\n+    auto split_image_shape = UnpackOp::create(\n+        rewriter, loc,\n         TypeRange({scalar_dim_ty, scalar_dim_ty, scalar_dim_ty, scalar_dim_ty}),\n         input_shape);\n \n@@ -1464,151 +1475,156 @@ class LowerResizeNearestNeighbor : public RewritePattern {\n     auto in_x = split_image_shape.getResult(2);\n     auto channels = split_image_shape.getResult(3);\n \n-    auto in_count = rewriter.create<MulOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, out_size_element_ty),\n-        in_y, in_x);\n+    auto in_count = MulOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, out_size_element_ty), in_y,\n+        in_x);\n \n     // Unpack and separate the out width/height.\n-    auto split_out_size = rewriter.create<UnpackOp>(\n-        loc, TypeRange({scalar_dim_ty, scalar_dim_ty}), out_size);\n+    auto split_out_size = UnpackOp::create(\n+        rewriter, loc, TypeRange({scalar_dim_ty, scalar_dim_ty}), out_size);\n \n     auto out_y = split_out_size.getResult(0);\n     auto out_x = split_out_size.getResult(1);\n \n-    auto out_count = rewriter.create<MulOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, out_size_element_ty),\n-        out_y, out_x);\n+    auto out_count = MulOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, out_size_element_ty), out_y,\n+        out_x);\n \n     // Generate what the final output shape will look like.\n-    auto out_shape = rewriter.create<PackOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({4}, out_size_element_ty),\n+    auto out_shape = PackOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({4}, out_size_element_ty),\n         ValueRange({batch, out_y, out_x, channels}));\n \n     // Compute the indices along the vertical dimension.\n-    auto in_y_f32 = rewriter.create<CastOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n-        in_y);\n-    auto out_w_f32 = rewriter.create<CastOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n-        out_y);\n-\n-    Value y_scale = rewriter.create<DivOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n+    auto in_y_f32 = CastOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()), in_y);\n+    auto out_w_f32 = CastOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()), out_y);\n+\n+    Value y_scale = DivOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n         in_y_f32, out_w_f32);\n \n-    Value zero_f32 = rewriter.create<ConstOp>(\n-        loc, GetScalarOfType(rewriter.getF32Type(), 0.0));\n-    Value one_f32 = rewriter.create<ConstOp>(\n-        loc, GetScalarOfType(rewriter.getF32Type(), 1.0));\n-\n-    Value y_range = rewriter.create<RangeOp>(\n-        loc,\n-        tensorflow::GetTypeFromTFTensorShape({out_height_constant},\n-                                             rewriter.getF32Type()),\n-        zero_f32, out_w_f32, one_f32);\n-\n-    y_range = rewriter.create<MulOp>(\n-        loc,\n-        tensorflow::GetTypeFromTFTensorShape({out_height_constant},\n-                                             rewriter.getF32Type()),\n-        y_range, y_scale);\n-\n-    y_range =\n-        rewriter.create<CastOp>(loc,\n-                                tensorflow::GetTypeFromTFTensorShape(\n-                                    {out_height_constant}, out_size_element_ty),\n-                                y_range);\n-\n-    y_range = rewriter.create<ReshapeOp>(\n-        loc,\n+    Value zero_f32 = ConstOp::create(\n+        rewriter, loc, GetScalarOfType(rewriter.getF32Type(), 0.0));\n+    Value one_f32 = ConstOp::create(\n+        rewriter, loc, GetScalarOfType(rewriter.getF32Type(), 1.0));\n+\n+    Value y_range =\n+        RangeOp::create(rewriter, loc,\n+                        tensorflow::GetTypeFromTFTensorShape(\n+                            {out_height_constant}, rewriter.getF32Type()),\n+                        zero_f32, out_w_f32, one_f32);\n+\n+    y_range = MulOp::create(rewriter, loc,\n+                            tensorflow::GetTypeFromTFTensorShape(\n+                                {out_height_constant}, rewriter.getF32Type()),\n+                            y_range, y_scale);\n+\n+    y_range = CastOp::create(rewriter, loc,\n+                             tensorflow::GetTypeFromTFTensorShape(\n+                                 {out_height_constant}, out_size_element_ty),\n+                             y_range);\n+\n+    y_range = ReshapeOp::create(\n+        rewriter, loc,\n         tensorflow::GetTypeFromTFTensorShape({out_height_constant, 1},\n                                              out_size_element_ty),\n         y_range,\n-        rewriter.create<PackOp>(\n-            loc, tensorflow::GetTypeFromTFTensorShape({2}, out_size_element_ty),\n+        PackOp::create(\n+            rewriter, loc,\n+            tensorflow::GetTypeFromTFTensorShape({2}, out_size_element_ty),\n             ValueRange({out_y, one})));\n \n-    Value y_indices = rewriter.create<MulOp>(\n-        loc,\n-        tensorflow::GetTypeFromTFTensorShape({out_height_constant, 1},\n-                                             out_size_element_ty),\n-        y_range, in_x);\n+    Value y_indices =\n+        MulOp::create(rewriter, loc,\n+                      tensorflow::GetTypeFromTFTensorShape(\n+                          {out_height_constant, 1}, out_size_element_ty),\n+                      y_range, in_x);\n \n     // Compute the indices for the nearest neighbour lookup across the width\n     // dim.\n-    auto in_x_f32 = rewriter.create<CastOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n-        in_x);\n-    auto out_h_f32 = rewriter.create<CastOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n-        out_x);\n-\n-    Value x_scale = rewriter.create<DivOp>(\n-        loc, tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n+    auto in_x_f32 = CastOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()), in_x);\n+    auto out_h_f32 = CastOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()), out_x);\n+\n+    Value x_scale = DivOp::create(\n+        rewriter, loc,\n+        tensorflow::GetTypeFromTFTensorShape({}, rewriter.getF32Type()),\n         in_x_f32, out_h_f32);\n \n-    Value x_range = rewriter.create<RangeOp>(\n-        loc,\n-        tensorflow::GetTypeFromTFTensorShape({out_width_constant},\n-                                             rewriter.getF32Type()),\n-        zero_f32, out_h_f32, one_f32);\n-\n-    x_range =\n-        rewriter.create<MulOp>(loc,\n-                               tensorflow::GetTypeFromTFTensorShape(\n-                                   {out_width_constant}, rewriter.getF32Type()),\n-                               x_range, x_scale);\n-\n-    x_range =\n-        rewriter.create<CastOp>(loc,\n-                                tensorflow::GetTypeFromTFTensorShape(\n-                                    {out_width_constant}, out_size_element_ty),\n-                                x_range);\n-\n-    Value x_indices = rewriter.create<ReshapeOp>(\n-        loc,\n+    Value x_range =\n+        RangeOp::create(rewriter, loc,\n+                        tensorflow::GetTypeFromTFTensorShape(\n+                            {out_width_constant}, rewriter.getF32Type()),\n+                        zero_f32, out_h_f32, one_f32);\n+\n+    x_range = MulOp::create(rewriter, loc,\n+                            tensorflow::GetTypeFromTFTensorShape(\n+                                {out_width_constant}, rewriter.getF32Type()),\n+                            x_range, x_scale);\n+\n+    x_range = CastOp::create(rewriter, loc,\n+                             tensorflow::GetTypeFromTFTensorShape(\n+                                 {out_width_constant}, out_size_element_ty),\n+                             x_range);\n+\n+    Value x_indices = ReshapeOp::create(\n+        rewriter, loc,\n         tensorflow::GetTypeFromTFTensorShape({1, out_width_constant},\n                                              out_size_element_ty),\n         x_range,\n-        rewriter.create<PackOp>(\n-            loc, tensorflow::GetTypeFromTFTensorShape({2}, out_size_element_ty),\n+        PackOp::create(\n+            rewriter, loc,\n+            tensorflow::GetTypeFromTFTensorShape({2}, out_size_element_ty),\n             ValueRange({one, out_x})));\n \n     // Generate the combined index array, reshape to be 1-D.\n-    Value indices = rewriter.create<AddV2Op>(\n-        loc,\n+    Value indices = AddV2Op::create(\n+        rewriter, loc,\n         tensorflow::GetTypeFromTFTensorShape(\n             {out_height_constant, out_width_constant}, out_size_element_ty),\n         y_indices, x_indices);\n \n-    indices = rewriter.create<ReshapeOp>(\n-        loc,\n+    indices = ReshapeOp::create(\n+        rewriter, loc,\n         tensorflow::GetTypeFromTFTensorShape({out_spatial_cst},\n                                              out_size_element_ty),\n         indices,\n-        rewriter.create<ReshapeOp>(\n-            loc, tensorflow::GetTypeFromTFTensorShape({1}, out_size_element_ty),\n+        ReshapeOp::create(\n+            rewriter, loc,\n+            tensorflow::GetTypeFromTFTensorShape({1}, out_size_element_ty),\n             out_count,\n-            rewriter.create<ConstOp>(loc, rewriter.getI64TensorAttr({1}))));\n+            ConstOp::create(rewriter, loc, rewriter.getI64TensorAttr({1}))));\n \n     // Group the spatial indices and gather along that combined index.\n-    Value input_collapsed_spatial = rewriter.create<ReshapeOp>(\n-        loc,\n+    Value input_collapsed_spatial = ReshapeOp::create(\n+        rewriter, loc,\n         tensorflow::GetTypeFromTFTensorShape(\n             {batch_cst, in_spatial_cst, channels_cst}, input_element_ty),\n         input,\n-        rewriter.create<PackOp>(\n-            loc, tensorflow::GetTypeFromTFTensorShape({3}, out_size_element_ty),\n+        PackOp::create(\n+            rewriter, loc,\n+            tensorflow::GetTypeFromTFTensorShape({3}, out_size_element_ty),\n             ValueRange({batch, in_count, channels})));\n \n-    Value gathered_values = rewriter.create<GatherV2Op>(\n-        loc,\n+    Value gathered_values = GatherV2Op::create(\n+        rewriter, loc,\n         tensorflow::GetTypeFromTFTensorShape(\n             {batch_cst, out_spatial_cst, channels_cst}, input_element_ty),\n         input_collapsed_spatial, indices, /*axis=*/one);\n \n     gathered_values =\n-        rewriter.create<ReshapeOp>(loc, result_ty, gathered_values, out_shape);\n+        ReshapeOp::create(rewriter, loc, result_ty, gathered_values, out_shape);\n \n     rewriter.replaceOp(op, gathered_values);\n     return success();\n@@ -1681,18 +1697,18 @@ struct LowerRollOp : public RewritePattern {\n       begin_values[axis_i] = begin_i;\n       auto begin_attr = DenseIntElementsAttr::get(axis_type, begin_values);\n       auto begin =\n-          rewriter.create<ConstOp>(op->getLoc(), axis_type, begin_attr);\n+          ConstOp::create(rewriter, op->getLoc(), axis_type, begin_attr);\n \n       SmallVector<int64_t, 4> output_shape;\n       output_shape.append(input_shape.begin(), input_shape.end());\n       output_shape[axis_i] = size_i;\n       auto size_attr = DenseIntElementsAttr::get(axis_type, output_shape);\n-      auto size = rewriter.create<ConstOp>(op->getLoc(), axis_type, size_attr);\n+      auto size = ConstOp::create(rewriter, op->getLoc(), axis_type, size_attr);\n \n       auto slice_op_ty = tensorflow::GetTypeFromTFTensorShape(\n           output_shape, input_ty.getElementType());\n-      return rewriter.create<SliceOp>(op->getLoc(), slice_op_ty, input, begin,\n-                                      size);\n+      return SliceOp::create(rewriter, op->getLoc(), slice_op_ty, input, begin,\n+                             size);\n     };\n \n     auto result = tf_roll_op.getInput();\n@@ -1708,9 +1724,9 @@ struct LowerRollOp : public RewritePattern {\n \n       auto dim_attr = DenseIntElementsAttr::get(scalar_type, {axis_i});\n       auto concat_dim =\n-          rewriter.create<ConstOp>(op->getLoc(), scalar_type, dim_attr);\n-      auto concat_op = rewriter.create<ConcatV2Op>(\n-          op->getLoc(), input_ty,\n+          ConstOp::create(rewriter, op->getLoc(), scalar_type, dim_attr);\n+      auto concat_op = ConcatV2Op::create(\n+          rewriter, op->getLoc(), input_ty,\n           ArrayRef<Value>({slice_op_1.getOutput(), slice_op_2.getOutput()}),\n           concat_dim);\n       result = concat_op.getResult();\n@@ -1741,7 +1757,7 @@ class LowerSoftmaxOp : public OpRewritePattern<OpTy> {\n     // Note that the TensorFlow Softmax op verifies that the input rank is\n     // greater than or equal to one so the following sequence is valid.\n     auto reduce_dim =\n-        rewriter.create<TF::ConstOp>(loc, GetI64ElementsAttr({-1}, &rewriter));\n+        TF::ConstOp::create(rewriter, loc, GetI64ElementsAttr({-1}, &rewriter));\n \n     // Exponential of input values and then their sum can be very large here.\n     // Division with large denominator is numerically unstable. To improve\n@@ -1750,20 +1766,19 @@ class LowerSoftmaxOp : public OpRewritePattern<OpTy> {\n     // after adding or subtracting all inputs in a batch using a common value\n     // gives mathematically equivalent result.\n     auto max_logits =\n-        rewriter.create<TF::MaxOp>(loc, logits, reduce_dim,\n-                                   /*keep_dims=*/rewriter.getBoolAttr(true));\n-    auto shifted_logits = rewriter.create<TF::SubOp>(loc, logits, max_logits);\n+        TF::MaxOp::create(rewriter, loc, logits, reduce_dim,\n+                          /*keep_dims=*/rewriter.getBoolAttr(true));\n+    auto shifted_logits = TF::SubOp::create(rewriter, loc, logits, max_logits);\n \n     // Exponentiate the inputs.\n-    Value exp = rewriter.create<TF::ExpOp>(loc, shifted_logits);\n+    Value exp = TF::ExpOp::create(rewriter, loc, shifted_logits);\n \n     // Compute summation of the exponentials.\n-    Value sum =\n-        rewriter.create<TF::SumOp>(loc, exp, reduce_dim,\n-                                   /*keep_dims=*/rewriter.getBoolAttr(true));\n+    Value sum = TF::SumOp::create(rewriter, loc, exp, reduce_dim,\n+                                  /*keep_dims=*/rewriter.getBoolAttr(true));\n \n     if (use_log) {\n-      Value log = rewriter.create<TF::LogOp>(loc, sum);\n+      Value log = TF::LogOp::create(rewriter, loc, sum);\n       rewriter.replaceOpWithNewOp<TF::SubOp>(op, shifted_logits, log);\n     } else {\n       rewriter.replaceOpWithNewOp<TF::DivOp>(op, exp, sum);"
        },
        {
            "sha": "bd8ae6260ce259d277ee8625bada22304cdc7302",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/prepare_tpu_computation_for_tf_export.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fprepare_tpu_computation_for_tf_export.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fprepare_tpu_computation_for_tf_export.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fprepare_tpu_computation_for_tf_export.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -105,13 +105,13 @@ class RewriteXlaHostComputeMlir\n       rewriter.setInsertionPointToStart(&cloned_func.getBody().front());\n       auto result_type =\n           RankedTensorType::get({3}, rewriter.getType<TF::StringType>());\n-      auto dynamic_key =\n-          rewriter.create<TF::_XlaCompileMlirPlaceholderProgramKeyOp>(\n-              func.getLoc(), /*program=*/result_type, llvm::ArrayRef<Value>{});\n+      auto dynamic_key = TF::_XlaCompileMlirPlaceholderProgramKeyOp::create(\n+          rewriter, func.getLoc(), /*program=*/result_type,\n+          llvm::ArrayRef<Value>{});\n \n-      auto recv_at_host = rewriter.create<TF::_XlaRecvAtHostOp>(\n-          func.getLoc(), op.getOperandTypes(), /*dynamic_key=*/dynamic_key,\n-          op.getSendKeyAttr(),\n+      auto recv_at_host = TF::_XlaRecvAtHostOp::create(\n+          rewriter, func.getLoc(), op.getOperandTypes(),\n+          /*dynamic_key=*/dynamic_key, op.getSendKeyAttr(),\n           /*device_ordinal=*/rewriter.getI64IntegerAttr(0),\n           rewriter.getStringAttr(\"TPU\"));\n       for (auto result :\n@@ -120,8 +120,8 @@ class RewriteXlaHostComputeMlir\n       }\n \n       rewriter.setInsertionPoint(cloned_func.getBody().front().getTerminator());\n-      rewriter.create<TF::_XlaSendFromHostOp>(\n-          func.getLoc(),\n+      TF::_XlaSendFromHostOp::create(\n+          rewriter, func.getLoc(),\n           cloned_func.getBody().front().getTerminator()->getOperands(),\n           /*dynamic_key=*/dynamic_key, op.getRecvKeyAttr(),\n           /*device_ordinal=*/rewriter.getI64IntegerAttr(0),\n@@ -157,8 +157,8 @@ void UpdateArgAttributes(mlir::func::FuncOp func) {\n         // 'sharding' attribute.\n         // TODO(b/414807890): Not sure whether we need to pass a V2 sharding to\n         // the _XlaShardingV2, do this when we actually have a use case.\n-        auto updated_arg = builder.create<TF::XlaShardingOp>(\n-            func.getLoc(), arg.getType(), arg, /*sharding=*/sharding,\n+        auto updated_arg = TF::XlaShardingOp::create(\n+            builder, func.getLoc(), arg.getType(), arg, /*sharding=*/sharding,\n             /*_XlaSharding=*/sharding, /*_XlaShardingV2=*/mlir::StringAttr());\n         func.getArgument(i).replaceAllUsesExcept(\n             updated_arg, llvm::SmallPtrSet<Operation*, 1>({updated_arg}));"
        },
        {
            "sha": "656f87deb0b79f7dd029a1e51f2b65b2d1369f55",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/replicate_invariant_op_hoisting.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Freplicate_invariant_op_hoisting.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Freplicate_invariant_op_hoisting.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Freplicate_invariant_op_hoisting.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -118,8 +118,8 @@ void MakeShapeOpInvariant(tf_device::ReplicateOp replicate_op, int num_replicas,\n     if (block_arg.getOwner() != replicate_block) return;\n \n     OpBuilder builder(shape_op);\n-    auto new_shape_op = builder.create<TF::VariableShapeOp>(\n-        shape_op.getLoc(), shape_op.getType(),\n+    auto new_shape_op = TF::VariableShapeOp::create(\n+        builder, shape_op.getLoc(), shape_op.getType(),\n         replicate_op.GetReplicaOperandForBlockArgument(block_arg,\n                                                        /*replica=*/0));\n     shape_op.replaceAllUsesWith(new_shape_op.getOperation());"
        },
        {
            "sha": "1945aa6d811c19ac695a6b9a9bc6ef79c6cb413a",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/rewrite_tpu_embedding_ops.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Frewrite_tpu_embedding_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Frewrite_tpu_embedding_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Frewrite_tpu_embedding_ops.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -45,8 +45,8 @@ OpT AddOperandAndRewriteAs(Operation* op, Value operand, NamedAttrList attr,\n   builder->setInsertionPoint(op);\n   auto operands = llvm::to_vector<4>(op->getOperands());\n   operands.push_back(operand);\n-  auto new_op = builder->create<OpT>(op->getLoc(), op->getResultTypes(),\n-                                     operands, attr.getAttrs());\n+  auto new_op = OpT::create(*builder, op->getLoc(), op->getResultTypes(),\n+                            operands, attr.getAttrs());\n   op->replaceAllUsesWith(new_op.getOperation()->getResults());\n   op->erase();\n   return new_op;\n@@ -82,8 +82,8 @@ LogicalResult RunOnRegion(Region* region) {\n   OpBuilder builder(region);\n   auto output_ty =\n       RankedTensorType::get({}, VariantType::get(region->getContext()));\n-  auto dedup_op = builder.create<XlaRecvTPUEmbeddingDeduplicationDataOp>(\n-      loc, output_ty, config);\n+  auto dedup_op = XlaRecvTPUEmbeddingDeduplicationDataOp::create(\n+      builder, loc, output_ty, config);\n \n   // Rewrite RecvTPUEmbeddingActivations op to the corresponding internal op.\n   if (recv_op)"
        },
        {
            "sha": "ce3b6bb5dd50707c1de2d81c548475d70fd2a3d3",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/sparsecore/embedding_program_key.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fsparsecore%2Fembedding_program_key.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fsparsecore%2Fembedding_program_key.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fsparsecore%2Fembedding_program_key.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -213,13 +213,13 @@ tf_device::LaunchOp CreateLaunchForBlock(OpBuilder* builder,\n   }\n \n   builder->setInsertionPointAfter(before_op);\n-  auto launch = builder->create<tf_device::LaunchOp>(\n-      before_op->getLoc(), builder->getStringAttr(host_device),\n-      launch_result_types);\n+  auto launch = tf_device::LaunchOp::create(*builder, before_op->getLoc(),\n+                                            builder->getStringAttr(host_device),\n+                                            launch_result_types);\n   launch.getBody().push_back(launch_block);\n \n   builder->setInsertionPointToEnd(&launch.GetBody());\n-  builder->create<tf_device::ReturnOp>(before_op->getLoc(), launch_results);\n+  tf_device::ReturnOp::create(*builder, before_op->getLoc(), launch_results);\n \n   return launch;\n }"
        },
        {
            "sha": "d57390cbc919ad431aecdcae262468e197538781",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/tf_data_optimization.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftf_data_optimization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftf_data_optimization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftf_data_optimization.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -37,16 +37,17 @@ struct FuseParallelMapAndBatch : public OpRewritePattern<BatchDatasetV2Op> {\n \n     // The type of the `num_parallel_calls` argument in ParallelMapDataset\n     // and MapAndBatchDataset is different (int32 and int64 respectively)\n-    auto num_parallel_calls_op = rewriter.create<CastOp>(\n-        op.getLoc(), UnrankedTensorType::get(rewriter.getIntegerType(64)),\n+    auto num_parallel_calls_op = CastOp::create(\n+        rewriter, op.getLoc(),\n+        UnrankedTensorType::get(rewriter.getIntegerType(64)),\n         batchInputOp.getNumParallelCalls(), rewriter.getBoolAttr(false));\n \n     if (op.getMetadata() != batchInputOp.getMetadata()) {\n       return failure();\n     }\n \n-    auto fused_op = rewriter.create<MapAndBatchDatasetOp>(\n-        op.getLoc(), op.getType(), batchInputOp.getInputDataset(),\n+    auto fused_op = MapAndBatchDatasetOp::create(\n+        rewriter, op.getLoc(), op.getType(), batchInputOp.getInputDataset(),\n         batchInputOp.getOtherArguments(), op.getBatchSize(),\n         num_parallel_calls_op.getY(), op.getDropRemainder(),\n         batchInputOp.getF(), op.getOutputTypes(), op.getOutputShapes(),"
        },
        {
            "sha": "2ee19787c7552fce1ab6f0c3b569d1b26785b728",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/tpu_parallel_execute_sink_resource_write.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftpu_parallel_execute_sink_resource_write.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftpu_parallel_execute_sink_resource_write.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftpu_parallel_execute_sink_resource_write.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -131,8 +131,8 @@ void SinkResourceWritesIntoParallelExecute(\n     new_result_types.push_back(old_result.getType());\n \n   OpBuilder builder(parallel_execute);\n-  auto new_parallel_execute = builder.create<tf_device::ParallelExecuteOp>(\n-      parallel_execute.getLoc(), num_regions, new_result_types);\n+  auto new_parallel_execute = tf_device::ParallelExecuteOp::create(\n+      builder, parallel_execute.getLoc(), num_regions, new_result_types);\n \n   for (auto region : llvm::zip(new_parallel_execute.getRegions(),\n                                parallel_execute.getRegions()))"
        },
        {
            "sha": "8cd90d0a96e9e93cd1a74b51958c1fbf4c87dca4",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/tpu_resource_read_for_write.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftpu_resource_read_for_write.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftpu_resource_read_for_write.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Ftpu_resource_read_for_write.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -106,9 +106,9 @@ void TPUResourceReadForWritePass::runOnOperation() {\n       if (!resource_and_type.resource) continue;\n       if (ClusterFuncHasResourceRead(cluster_func, resource_and_type.resource))\n         continue;\n-      auto new_read = builder.create<TF::ReadVariableOp>(\n-          resource_and_type.resource.getLoc(), resource_and_type.subtype,\n-          resource_and_type.resource);\n+      auto new_read = TF::ReadVariableOp::create(\n+          builder, resource_and_type.resource.getLoc(),\n+          resource_and_type.subtype, resource_and_type.resource);\n       read_operands.push_back(new_read.getValue());\n     }\n \n@@ -119,8 +119,9 @@ void TPUResourceReadForWritePass::runOnOperation() {\n     operands.append(read_operands.begin(), read_operands.end());\n \n     auto loc = cluster_func.getLoc();\n-    auto new_cluster_func = builder.create<tf_device::ClusterFuncOp>(\n-        loc, cluster_func.getResultTypes(), operands, cluster_func->getAttrs());\n+    auto new_cluster_func = tf_device::ClusterFuncOp::create(\n+        builder, loc, cluster_func.getResultTypes(), operands,\n+        cluster_func->getAttrs());\n     cluster_func.replaceAllUsesWith(new_cluster_func);\n     func::FuncOp func = cluster_func.getFuncOp();\n     Block& block = func.front();"
        },
        {
            "sha": "85db75ea51a543dd6f620c5a7aadfb8d38527a06",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/unroll_batch_matmul.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Funroll_batch_matmul.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Funroll_batch_matmul.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Funroll_batch_matmul.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -89,9 +89,9 @@ TF::ReshapeOp ConvertTFBatchMatMulOp<BatchMatMulOpType>::createReshapeOp(\n   Type resultType = RankedTensorType::get(shape, element_type);\n   auto constant_attr = DenseElementsAttr::get(shape_spec_type, shape);\n   auto shape_tensor =\n-      rewriter.create<TF::ConstOp>(loc, shape_spec_type, constant_attr);\n-  return rewriter.create<TF::ReshapeOp>(loc, resultType, /*tensor=*/value,\n-                                        /*shape=*/shape_tensor);\n+      TF::ConstOp::create(rewriter, loc, shape_spec_type, constant_attr);\n+  return TF::ReshapeOp::create(rewriter, loc, resultType, /*tensor=*/value,\n+                               /*shape=*/shape_tensor);\n }\n \n template <typename BatchMatMulOpType>\n@@ -122,16 +122,16 @@ std::vector<Value> ConvertTFBatchMatMulOp<BatchMatMulOpType>::sliceInput(\n     auto split_dimension_type =\n         RankedTensorType::get({}, rewriter.getIntegerType(32));\n     auto split_dimension_attr = DenseElementsAttr::get(split_dimension_type, 0);\n-    auto split_dimension_op = rewriter.create<TF::ConstOp>(\n-        loc, split_dimension_type, split_dimension_attr);\n+    auto split_dimension_op = TF::ConstOp::create(\n+        rewriter, loc, split_dimension_type, split_dimension_attr);\n \n     // Split along each batch.\n     SmallVector<int64_t, 3> slice_size = {1, num_rows, num_cols};\n     Type slice_result_type = RankedTensorType::get(slice_size, element_type);\n     llvm::SmallVector<Type, 4> output_types(batch_size, slice_result_type);\n-    auto split_op = rewriter.create<TF::SplitOp>(loc, output_types,\n-                                                 split_dimension_op.getOutput(),\n-                                                 reshape_op.getOutput());\n+    auto split_op = TF::SplitOp::create(rewriter, loc, output_types,\n+                                        split_dimension_op.getOutput(),\n+                                        reshape_op.getOutput());\n \n     // Squeeze each batch, i.e. reshape\n     // [1, num_rows, num_cols] -> [num_rows, num_cols]\n@@ -259,11 +259,11 @@ LogicalResult ConvertTFBatchMatMulOp<BatchMatMulOpType>::matchAndRewrite(\n       lhs_batch_idx = batch_idx;\n       rhs_batch_idx = batch_idx;\n     }\n-    auto matmul = rewriter.create<TF::MatMulOp>(loc, matmul_type,\n-                                                /*a=*/sliced_lhs[lhs_batch_idx],\n-                                                /*b=*/sliced_rhs[rhs_batch_idx],\n-                                                /*transpose_a=*/op.getAdjX(),\n-                                                /*transpose_b=*/op.getAdjY());\n+    auto matmul = TF::MatMulOp::create(rewriter, loc, matmul_type,\n+                                       /*a=*/sliced_lhs[lhs_batch_idx],\n+                                       /*b=*/sliced_rhs[rhs_batch_idx],\n+                                       /*transpose_a=*/op.getAdjX(),\n+                                       /*transpose_b=*/op.getAdjY());\n     matmuls.emplace_back(matmul.getProduct());\n   }\n \n@@ -272,7 +272,7 @@ LogicalResult ConvertTFBatchMatMulOp<BatchMatMulOpType>::matchAndRewrite(\n       {bcast.output_batch_size(), rows, cols}, element_type);\n   const auto axis = rewriter.getI64IntegerAttr(0);\n   auto pack_op =\n-      rewriter.create<TF::PackOp>(loc, packed_type, /*values=*/matmuls, axis);\n+      TF::PackOp::create(rewriter, loc, packed_type, /*values=*/matmuls, axis);\n \n   // Reshape the rank-3 tensor into the correct output shape.\n   const auto& result_batch_shape = bcast.output_batch_shape().dim_sizes();"
        },
        {
            "sha": "52d1bfc8ffde3ae3ae26dec02826beec70cc8a32",
            "filename": "tensorflow/compiler/mlir/tensorflow/utils/parallel_execute_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fparallel_execute_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fparallel_execute_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fparallel_execute_util.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -25,15 +25,15 @@ tf_device::ParallelExecuteOp BuildParallelExecuteOp(\n     tf_device::ClusterFuncOp cluster_func, OpBuilder* builder) {\n   const auto output_types = cluster_func.getResultTypes();\n   builder->setInsertionPoint(cluster_func);\n-  auto parallel_execute = builder->create<tf_device::ParallelExecuteOp>(\n-      cluster_func.getLoc(), 1, output_types);\n+  auto parallel_execute = tf_device::ParallelExecuteOp::create(\n+      *builder, cluster_func.getLoc(), 1, output_types);\n   cluster_func->remove();\n   auto& block = parallel_execute.GetRegionBlockWithIndex(0);\n   builder->setInsertionPointToEnd(&block);\n   builder->insert(cluster_func);\n   cluster_func.replaceAllUsesWith(parallel_execute);\n-  builder->create<tf_device::ReturnOp>(block.getParent()->getLoc(),\n-                                       cluster_func.getResults());\n+  tf_device::ReturnOp::create(*builder, block.getParent()->getLoc(),\n+                              cluster_func.getResults());\n   return parallel_execute;\n }\n "
        },
        {
            "sha": "a7b676d85419094d909a83abab8b9181e9e5e30d",
            "filename": "tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util_test.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Ftpu_rewrite_device_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Ftpu_rewrite_device_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Ftpu_rewrite_device_util_test.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -793,8 +793,8 @@ TEST(TPURewriteDeviceUtilTest, TestHasModelParallelismFalse) {\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n   cluster->setAttr(kTopologyAttr, builder.getStringAttr(\"\"));\n@@ -811,8 +811,8 @@ TEST(TPURewriteDeviceUtilTest, TestHasModelParallelismTrue) {\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 5));\n   cluster->setAttr(kTopologyAttr, builder.getStringAttr(\"\"));\n@@ -830,8 +830,8 @@ TEST(TPURewriteDeviceUtilTest,\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n   cluster->setAttr(kTopologyAttr, builder.getStringAttr(\"\"));\n@@ -848,8 +848,8 @@ TEST(TPURewriteDeviceUtilTest,\n       mlir::ModuleOp::create(mlir::UnknownLoc::get(&context));\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kDeviceAssignmentAttr, builder.getArrayAttr({}));\n \n   mlir::TF::RuntimeDevices devices;\n@@ -865,8 +865,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostFailDeviceMissingAttributes) {\n       mlir::ModuleOp::create(mlir::UnknownLoc::get(&context));\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n \n@@ -884,8 +884,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceFailMissingTopology) {\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n   cluster->setAttr(kDeviceAssignmentAttr, builder.getArrayAttr({}));\n@@ -904,8 +904,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceFailMissingDeviceAssignment) {\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n   cluster->setAttr(kTopologyAttr, builder.getStringAttr(\"\"));\n@@ -924,8 +924,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceFailBadDeviceAssignment) {\n   mlir::OpBuilder builder(module_ref->getBodyRegion());\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n   cluster->setAttr(kTopologyAttr, builder.getStringAttr(\"\"));\n@@ -951,8 +951,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceFailBadDeviceName) {\n                     llvm::ArrayRef<llvm::StringRef>({\"bad_device_name\"})));\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n   cluster->setAttr(kTopologyAttr, builder.getStringAttr(\"\"));\n@@ -974,16 +974,16 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceTPUReplicate) {\n \n   llvm::SmallDenseMap<llvm::StringRef, llvm::SmallVector<llvm::StringRef, 4>>\n       devices;\n-  auto replicate = builder.create<mlir::tf_device::ReplicateOp>(\n-      mlir::UnknownLoc::get(&context), /*num_replicas=*/2, devices,\n+  auto replicate = mlir::tf_device::ReplicateOp::create(\n+      builder, mlir::UnknownLoc::get(&context), /*num_replicas=*/2, devices,\n       llvm::ArrayRef<std::pair<mlir::ValueRange, mlir::Type>>{},\n       mlir::ValueRange{}, mlir::TypeRange{});\n   builder.setInsertionPoint(&replicate.getBody().front(),\n                             replicate.getBody().front().begin());\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n \n   mlir::TF::RuntimeDevices runtime_devices;\n   std::string host_device;\n@@ -1007,8 +1007,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceNotReplicated) {\n                      \"/job:worker/replica:0/task:0/device:CPU:0\"})));\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n   cluster->setAttr(kNumCoresPerReplicaAttr,\n                    builder.getIntegerAttr(builder.getIntegerType(64), 1));\n   cluster->setAttr(kTopologyAttr, builder.getStringAttr(\"\"));\n@@ -1034,8 +1034,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceInGenericPipeline) {\n                     {\"/job:localhost/replica:0/task:0/device:CPU:0\"})));\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n \n   mlir::TF::RuntimeDevices runtime_devices;\n   (void)GetDevicesFromOp(*module_ref, &runtime_devices);\n@@ -1060,8 +1060,8 @@ TEST(TPURewriteDeviceUtilTest, TestGetHostDeviceInGenericPipelineMultiCPUs) {\n                      \"/job:worker/replica:0/task:2/device:CPU:0\"})));\n \n   llvm::SmallVector<mlir::Type, 8> result_types;\n-  auto cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      mlir::UnknownLoc::get(&context), result_types);\n+  auto cluster = mlir::tf_device::ClusterOp::create(\n+      builder, mlir::UnknownLoc::get(&context), result_types);\n \n   mlir::TF::RuntimeDevices runtime_devices;\n   (void)GetDevicesFromOp(*module_ref, &runtime_devices);"
        },
        {
            "sha": "82b7202d6d78e9ea04e20b5bef8d2fe0c5b25310",
            "filename": "tensorflow/compiler/mlir/tensorflow/utils/xla_rewrite_util.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fxla_rewrite_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fxla_rewrite_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fxla_rewrite_util.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -83,8 +83,8 @@ int MovePreservedParallelExecuteChildren(\n   // `num_moved_children` is the number of children that will be preserved.\n   const size_t num_moved_children =\n       old_parallel_execute.getRegions().size() - 1;\n-  *new_parallel_execute = builder->create<mlir::tf_device::ParallelExecuteOp>(\n-      old_parallel_execute->getLoc(),\n+  *new_parallel_execute = mlir::tf_device::ParallelExecuteOp::create(\n+      *builder, old_parallel_execute->getLoc(),\n       num_moved_children + num_cores_per_replica, concatenated_output_types);\n \n   // `cluster_idx` is the index of the child with the `ClusterFuncOp`, which\n@@ -118,12 +118,12 @@ mlir::tf_device::LaunchOp WrapOpInLaunch(mlir::OpBuilder* builder,\n                                          llvm::StringRef device) {\n   mlir::OpBuilder::InsertPoint insert_point = builder->saveInsertionPoint();\n \n-  auto launch = builder->create<mlir::tf_device::LaunchOp>(\n-      loc, builder->getStringAttr(device), op->getResultTypes());\n+  auto launch = mlir::tf_device::LaunchOp::create(\n+      *builder, loc, builder->getStringAttr(device), op->getResultTypes());\n   launch.getBody().push_back(new mlir::Block);\n \n   builder->setInsertionPointToEnd(&launch.GetBody());\n-  builder->create<mlir::tf_device::ReturnOp>(loc, op->getResults());\n+  mlir::tf_device::ReturnOp::create(*builder, loc, op->getResults());\n \n   // Move op inside cluster.\n   op->moveBefore(launch.GetBody().getTerminator());"
        },
        {
            "sha": "3bca701131151fc5f281b39b9804bd3f633d3feb",
            "filename": "tensorflow/compiler/mlir/tensorflow/utils/xla_sharding_util.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 29,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fxla_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fxla_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Futils%2Fxla_sharding_util.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -94,22 +94,23 @@ mlir::TF::SliceOp CreateSliceOp(mlir::OpBuilder* builder,\n   auto start_position_type =\n       mlir::RankedTensorType::get(shape.dims(), builder->getIntegerType(64));\n \n-  auto start_position_op = builder->create<mlir::TF::ConstOp>(\n-      input.getLoc(), mlir::DenseIntElementsAttr::get(start_position_type,\n-                                                      slice_start_position));\n-\n-  auto slice_size_op = builder->create<mlir::TF::ConstOp>(\n-      input.getLoc(), mlir::DenseIntElementsAttr::get(\n-                          mlir::RankedTensorType::get(\n-                              shape.dims(), builder->getIntegerType(64)),\n-                          slice_size));\n+  auto start_position_op =\n+      mlir::TF::ConstOp::create(*builder, input.getLoc(),\n+                                mlir::DenseIntElementsAttr::get(\n+                                    start_position_type, slice_start_position));\n+\n+  auto slice_size_op = mlir::TF::ConstOp::create(\n+      *builder, input.getLoc(),\n+      mlir::DenseIntElementsAttr::get(\n+          mlir::RankedTensorType::get(shape.dims(),\n+                                      builder->getIntegerType(64)),\n+          slice_size));\n \n   auto slice_result_type =\n       mlir::RankedTensorType::get(slice_size, getElementTypeOrSelf(input));\n \n-  return builder->create<mlir::TF::SliceOp>(input.getLoc(), slice_result_type,\n-                                            input, start_position_op,\n-                                            slice_size_op);\n+  return mlir::TF::SliceOp::create(*builder, input.getLoc(), slice_result_type,\n+                                   input, start_position_op, slice_size_op);\n }\n \n mlir::TF::PadOp CreatePadOp(mlir::OpBuilder* builder,\n@@ -135,15 +136,15 @@ mlir::TF::PadOp CreatePadOp(mlir::OpBuilder* builder,\n   auto padding_type =\n       mlir::RankedTensorType::get({num_dims, 2}, builder->getIntegerType(64));\n   auto paddings = mlir::DenseIntElementsAttr::get(padding_type, padding_values);\n-  auto paddings_value = builder->create<mlir::TF::ConstOp>(location, paddings);\n+  auto paddings_value = mlir::TF::ConstOp::create(*builder, location, paddings);\n   mlir::SmallVector<int64_t, 4> expand_shape(padded_shape.begin(),\n                                              padded_shape.end());\n \n   auto expand_result_type =\n       mlir::RankedTensorType::get(expand_shape, input_type.getElementType());\n \n-  return builder->create<mlir::TF::PadOp>(location, expand_result_type,\n-                                          src_input, paddings_value);\n+  return mlir::TF::PadOp::create(*builder, location, expand_result_type,\n+                                 src_input, paddings_value);\n }\n \n // Creates a tf::SplitOp that splits 'src_input' into 'num_splits' ways\n@@ -198,17 +199,18 @@ mlir::LogicalResult CreateSplitOp(\n     output_type = input_type;\n   }\n \n-  auto split_dimension_op = builder->create<mlir::TF::ConstOp>(\n-      location, split_dim_type, split_dimension_attr);\n+  auto split_dimension_op = mlir::TF::ConstOp::create(\n+      *builder, location, split_dim_type, split_dimension_attr);\n   if (is_ici_weight_dist_spmd) {\n     split_dimension_op->setAttr(kICIWeightDistributionMlirBridgeMarker,\n                                 builder->getBoolAttr(true));\n   }\n \n   // Creates a split op that splits |src_input| along |split_dimension|.\n   llvm::SmallVector<mlir::Type, 4> output_types(num_split, output_type);\n-  *split_op = builder->create<mlir::TF::SplitOp>(\n-      location, output_types, split_dimension_op.getOutput(), src_input);\n+  *split_op =\n+      mlir::TF::SplitOp::create(*builder, location, output_types,\n+                                split_dimension_op.getOutput(), src_input);\n   (*split_op)->setAttr(\n       kNumSplitAttr,\n       builder->getIntegerAttr(builder->getIntegerType(32), num_split));\n@@ -230,8 +232,8 @@ mlir::TF::ConcatOp CreateConcatOp(const int concat_dimension,\n       mlir::RankedTensorType::get({}, builder->getIntegerType(32));\n   auto concat_dimension_attr =\n       mlir::DenseElementsAttr::get(concat_dim_type, concat_dimension);\n-  auto concat_dimension_op = builder->create<mlir::TF::ConstOp>(\n-      location, concat_dim_type, concat_dimension_attr);\n+  auto concat_dimension_op = mlir::TF::ConstOp::create(\n+      *builder, location, concat_dim_type, concat_dimension_attr);\n \n   // Correctly set output shapes of concat op output if output shape is\n   // statically known. Since the shape of TPUExecute op must be the same\n@@ -253,8 +255,8 @@ mlir::TF::ConcatOp CreateConcatOp(const int concat_dimension,\n     output_type = input_type;\n   }\n \n-  return builder->create<mlir::TF::ConcatOp>(\n-      location, output_type, concat_dimension_op.getOutput(), inputs);\n+  return mlir::TF::ConcatOp::create(*builder, location, output_type,\n+                                    concat_dimension_op.getOutput(), inputs);\n }\n \n mlir::TF::XlaConcatNDOp CreateXlaConcatNDOp(\n@@ -292,9 +294,9 @@ mlir::TF::XlaConcatNDOp CreateXlaConcatNDOp(\n     output_type = input_slice_type;\n   }\n \n-  auto op = builder.create<mlir::TF::XlaConcatNDOp>(\n-      location, output_type, inputs, builder.getI64ArrayAttr(num_concats),\n-      builder.getI64ArrayAttr(paddings));\n+  auto op = mlir::TF::XlaConcatNDOp::create(\n+      builder, location, output_type, inputs,\n+      builder.getI64ArrayAttr(num_concats), builder.getI64ArrayAttr(paddings));\n   return op;\n }\n \n@@ -338,9 +340,9 @@ mlir::LogicalResult CreateXlaSplitNDOp(const mlir::Location& location,\n           << absl::StrJoin(input_shape, \",\")\n           << \", Padding: \" << absl::StrJoin(paddings, \",\");\n \n-  *xla_split_op = builder->create<mlir::TF::XlaSplitNDOp>(\n-      location, output_types, src_input, builder->getI64ArrayAttr(num_splits),\n-      builder->getI64ArrayAttr(paddings));\n+  *xla_split_op = mlir::TF::XlaSplitNDOp::create(\n+      *builder, location, output_types, src_input,\n+      builder->getI64ArrayAttr(num_splits), builder->getI64ArrayAttr(paddings));\n   if (is_ici_weight_dist_spmd) {\n     (*xla_split_op)\n         ->setAttr(kICIWeightDistributionMlirBridgeMarker,"
        },
        {
            "sha": "e4fe30755c2eb7fae5d94756c7143cd82995ed37",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/legalize_tf.cc",
            "status": "modified",
            "additions": 710,
            "deletions": 653,
            "changes": 1363,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179"
        },
        {
            "sha": "abfcc0d26acc65fb80a932cdc50f145a39f0bb49",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/legalize_tf_collective.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf_collective.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf_collective.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf_collective.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -143,8 +143,9 @@ LogicalResult ConvertAllReduce(OpBuilder& builder, int64_t channel_id,\n   ChannelHandleAttr channel_handle = ConvertChannel(builder, channel_id, mode);\n   Location loc = op->getLoc();\n   Type element_type = getElementTypeOrSelf(input.getType());\n-  auto all_reduce = builder.create<AllReduceOp>(\n-      loc, result_type, input, replica_groups, channel_handle, nullptr);\n+  auto all_reduce =\n+      AllReduceOp::create(builder, loc, result_type, input, replica_groups,\n+                          channel_handle, nullptr);\n \n   if (all_reduce.getNumResults() != 1) {\n     return op->emitOpError()\n@@ -178,8 +179,8 @@ LogicalResult ConvertAllReduce(OpBuilder& builder, int64_t channel_id,\n     auto divisor =\n         GetScalarConstOfType(element_type, loc, replica_group_size, &builder);\n     auto broadcast_dims = builder.getDenseI64ArrayAttr({});\n-    result = builder.create<chlo::BroadcastDivOp>(\n-        loc, all_reduce.getResult(0), divisor.getResult(), broadcast_dims);\n+    result = chlo::BroadcastDivOp::create(builder, loc, all_reduce.getResult(0),\n+                                          divisor.getResult(), broadcast_dims);\n   } else if (final_op != \"Id\") {\n     return op->emitOpError()\n            << \"invalid final_op \" << final_op << \", want one of [Id, Div]\";\n@@ -373,11 +374,12 @@ class ConvertCollectiveAssignGroupV2\n     IntegerAttr group_size = rewriter.getI32IntegerAttr(replica_groups.size());\n     IntegerAttr group_key = rewriter.getI32IntegerAttr(0);\n \n-    auto const_group_size = rewriter.create<TF::ConstOp>(\n-        assign_group->getLoc(), assign_group.getResult(0).getType(),\n-        group_size);\n-    auto const_group_key = rewriter.create<TF::ConstOp>(\n-        assign_group->getLoc(), assign_group.getResult(1).getType(), group_key);\n+    auto const_group_size =\n+        TF::ConstOp::create(rewriter, assign_group->getLoc(),\n+                            assign_group.getResult(0).getType(), group_size);\n+    auto const_group_key =\n+        TF::ConstOp::create(rewriter, assign_group->getLoc(),\n+                            assign_group.getResult(1).getType(), group_key);\n     rewriter.replaceAllUsesWith(assign_group.getResult(0), const_group_size);\n     rewriter.replaceAllUsesWith(assign_group.getResult(1), const_group_key);\n     rewriter.eraseOp(assign_group);"
        },
        {
            "sha": "b1105d1a4e40004352390c31f9325a458da680b7",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/legalize_tf_communication.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf_communication.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf_communication.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalize_tf_communication.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -289,10 +289,10 @@ Value CreateSendOp(OpBuilder& builder, Location loc, Value operand,\n                                                /*handle=*/GetNextChannelId(),\n                                                /*type=*/2);\n   auto empty_source_target_pairs = builder.getI64TensorAttr({});\n-  auto send = builder.create<SendOp>(\n-      loc, token.getType(), operand, token, channel_handle,\n-      /*is_host_transfer=*/builder.getBoolAttr(true),\n-      /*source_target_pairs=*/empty_source_target_pairs);\n+  auto send = SendOp::create(builder, loc, token.getType(), operand, token,\n+                             channel_handle,\n+                             /*is_host_transfer=*/builder.getBoolAttr(true),\n+                             /*source_target_pairs=*/empty_source_target_pairs);\n   SetFrontendAttributes(send, index, key, operand.getType(),\n                         /*device_to_host=*/true, host_handler_name);\n \n@@ -311,10 +311,10 @@ Value CreateRecvOp(OpBuilder& builder, Location loc, Value result,\n                                                /*type=*/3);\n   auto result_type = result.getType();\n   SmallVector<Type, 2> recv_result_type = {result_type, token.getType()};\n-  auto recv = builder.create<RecvOp>(\n-      loc, recv_result_type, token, channel_handle,\n-      /*is_host_transfer=*/builder.getBoolAttr(true),\n-      /*source_target_pairs=*/builder.getI64TensorAttr({}));\n+  auto recv =\n+      RecvOp::create(builder, loc, recv_result_type, token, channel_handle,\n+                     /*is_host_transfer=*/builder.getBoolAttr(true),\n+                     /*source_target_pairs=*/builder.getI64TensorAttr({}));\n \n   SetFrontendAttributes(recv, index, key, result_type,\n                         /*device_to_host=*/false, host_handler_name);\n@@ -336,7 +336,7 @@ Value CreateSinkToken(OpBuilder& builder, Location loc, ArrayRef<Value> tokens,\n   } else if (llvm::hasSingleElement(tokens)) {\n     return tokens[0];\n   } else {\n-    return builder.create<AfterAllOp>(loc, original_token.getType(), tokens)\n+    return AfterAllOp::create(builder, loc, original_token.getType(), tokens)\n         .getResult();\n   }\n }\n@@ -413,8 +413,8 @@ Value RewriteCallOp(OpBuilder& builder, func::CallOp call,\n   new_operands.push_back(token);\n   auto new_result_types = llvm::to_vector(call.getResultTypes());\n   new_result_types.push_back(token.getType());\n-  auto new_call = builder.create<func::CallOp>(\n-      call.getLoc(), new_result_types,\n+  auto new_call = func::CallOp::create(\n+      builder, call.getLoc(), new_result_types,\n       new_symbol ? *new_symbol : call.getCallee(), new_operands);\n \n   for (auto results : llvm::zip(call.getResults(), new_call.getResults()))\n@@ -435,7 +435,7 @@ struct OpVisitorState {\n \n // Creates a tuple from a sequence of values.\n Value CreateTuple(OpBuilder& builder, Location loc, ArrayRef<Value> operands) {\n-  return builder.create<TupleOp>(loc, operands).getResult();\n+  return TupleOp::create(builder, loc, operands).getResult();\n }\n \n // Extends `values` with the value `token` attached. If `flatten_tuple` is\n@@ -480,7 +480,7 @@ SmallVector<Value> GetValueWithToken(\n   SmallVector<Value, 4> tuple_operands;\n   for (auto idx : llvm::seq<int32_t>(0, tuple_type.getTypes().size()))\n     tuple_operands.push_back(\n-        builder.create<GetTupleElementOp>(value.getLoc(), value, idx)\n+        GetTupleElementOp::create(builder, value.getLoc(), value, idx)\n             .getResult());\n \n   tuple_operands.push_back(token);\n@@ -518,7 +518,7 @@ Value CreateSubTuple(OpBuilder& builder, Value value, size_t end) {\n   SmallVector<Value, 4> tuple_operands;\n   for (auto idx : llvm::seq<int32_t>(0, end))\n     tuple_operands.push_back(\n-        builder.create<GetTupleElementOp>(value.getLoc(), value, idx)\n+        GetTupleElementOp::create(builder, value.getLoc(), value, idx)\n             .getResult());\n \n   return CreateTuple(builder, value.getLoc(), tuple_operands);\n@@ -543,8 +543,8 @@ void ReplaceWithTupleResult(OpBuilder& builder, ValueRange values,\n   auto tuple_type = mlir::dyn_cast<TupleType>(value.getType());\n   if (!tuple_type) {\n     if (!value.use_empty()) {\n-      auto new_element = builder.create<GetTupleElementOp>(replacement.getLoc(),\n-                                                           replacement, 0);\n+      auto new_element = GetTupleElementOp::create(\n+          builder, replacement.getLoc(), replacement, 0);\n       value.replaceAllUsesWith(new_element.getResult());\n     }\n     return;\n@@ -620,8 +620,8 @@ void RewriteRegionIfOp(OpBuilder& builder, IfOp region_if,\n                        /*flatten_tuple=*/true);\n \n   // Create new `mhlo.if` op with extra token operands and result.\n-  auto new_if = builder.create<IfOp>(region_if.getLoc(), new_result_types,\n-                                     region_if.getPred());\n+  auto new_if = IfOp::create(builder, region_if.getLoc(), new_result_types,\n+                             region_if.getPred());\n \n   // Move all regions from the old `mhlo.if` op to its replacement.\n   new_if.getTrueBranch().takeBody(region_if.getTrueBranch());\n@@ -745,8 +745,8 @@ void RewriteRegionWhileOp(OpBuilder& builder, WhileOp region_while,\n                        /*flatten_tuple*/ true);\n \n   // Create new `mhlo.while` op with extra token operand and result.\n-  auto new_while = builder.create<WhileOp>(region_while.getLoc(),\n-                                           new_result_types, new_val_operands);\n+  auto new_while = WhileOp::create(builder, region_while.getLoc(),\n+                                   new_result_types, new_val_operands);\n \n   // Move all regions from the old `mhlo.while` op to its replacement.\n   new_while.getCond().takeBody(region_while.getCond());\n@@ -815,7 +815,7 @@ void RewriteFunctionTerminator(OpBuilder& builder,\n   auto new_results = llvm::to_vector(terminator.getOperands());\n   new_results.push_back(token);\n   builder.setInsertionPoint(terminator);\n-  builder.create<mlir::func::ReturnOp>(terminator.getLoc(), new_results);\n+  mlir::func::ReturnOp::create(builder, terminator.getLoc(), new_results);\n   terminator.erase();\n }\n \n@@ -844,7 +844,7 @@ LogicalResult RewriteFunction(\n   // a token will be created. Otherwise a token block argument is inserted.\n   Value init_token =\n       rewrite_block ? func_body.addArgument(token_type, func.getLoc())\n-                    : builder.create<CreateTokenOp>(func.getLoc(), token_type)\n+                    : CreateTokenOp::create(builder, func.getLoc(), token_type)\n                           .getResult();\n \n   // Stack to keep track of region based control flow op nesting and current"
        },
        {
            "sha": "0b0e68548032a9804ac4cf2352ad7d383ebfa881",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/split_into_island_per_op_pass.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fsplit_into_island_per_op_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fsplit_into_island_per_op_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fsplit_into_island_per_op_pass.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -108,15 +108,15 @@ void PopulateEmptyIsland(tf_executor::IslandOp island) {\n   OpBuilder builder(&island.GetBody(), island.GetBody().begin());\n   tf_executor::YieldOp yield = island.GetYield();\n   if (yield.getNumOperands() == 0) {\n-    builder.create<TF::NoOp>(island.getLoc(), TypeRange{}, ValueRange{});\n+    TF::NoOp::create(builder, island.getLoc(), TypeRange{}, ValueRange{});\n   } else if (yield.getNumOperands() == 1) {\n     Value operand = yield.getOperand(0);\n-    auto identity = builder.create<TF::IdentityOp>(island.getLoc(),\n-                                                   operand.getType(), operand);\n+    auto identity = TF::IdentityOp::create(builder, island.getLoc(),\n+                                           operand.getType(), operand);\n     yield.setOperand(0, identity.getOutput());\n   } else {\n-    auto identity_n = builder.create<TF::IdentityNOp>(\n-        island.getLoc(), yield.getOperandTypes(), yield.getOperands());\n+    auto identity_n = TF::IdentityNOp::create(\n+        builder, island.getLoc(), yield.getOperandTypes(), yield.getOperands());\n     for (const auto& it : llvm::enumerate(identity_n.getResults()))\n       yield.setOperand(it.index(), it.value());\n   }\n@@ -128,15 +128,15 @@ tf_executor::IslandOp CreateIsland(TypeRange result_types,\n                                    const Location& loc, Operation& sub_op,\n                                    tf_executor::IslandOp original_island) {\n   OpBuilder builder(original_island);\n-  auto island = builder.create<tf_executor::IslandOp>(\n-      loc, result_types, control_type, mlir::ValueRange{});\n+  auto island = tf_executor::IslandOp::create(builder, loc, result_types,\n+                                              control_type, mlir::ValueRange{});\n   island.getBody().push_back(new Block);\n   Block* block = &island.getBody().back();\n   OpBuilder island_builder(original_island);\n   island_builder.setInsertionPointToEnd(block);\n   sub_op.replaceAllUsesWith(island.getOutputs());\n   sub_op.moveBefore(block, block->begin());\n-  island_builder.create<tf_executor::YieldOp>(loc, sub_op.getResults());\n+  tf_executor::YieldOp::create(island_builder, loc, sub_op.getResults());\n   return island;\n }\n "
        },
        {
            "sha": "2e5e4764f63d34c5422262b1b1227adb868e47d2",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/split_into_island_per_op_pass_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fsplit_into_island_per_op_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fsplit_into_island_per_op_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fsplit_into_island_per_op_pass_test.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -53,12 +53,11 @@ class SplitIntoIslandPerOpPass : public ::testing::Test {\n     llvm::SmallVector<mlir::Type, 1> island_result_types;\n     island_result_types.push_back(op_builder_.getF64Type());\n \n-    mlir::Operation* yield_op = op_builder_.create<mlir::tf_executor::YieldOp>(\n-        op_state.location, mlir::ValueRange{});\n-    mlir::tf_executor::IslandOp island_op =\n-        op_builder_.create<mlir::tf_executor::IslandOp>(\n-            op_state.location, island_result_types, mlir::ValueRange{},\n-            mlir::ArrayRef<mlir::NamedAttribute>{});\n+    mlir::Operation* yield_op = mlir::tf_executor::YieldOp::create(\n+        op_builder_, op_state.location, mlir::ValueRange{});\n+    mlir::tf_executor::IslandOp island_op = mlir::tf_executor::IslandOp::create(\n+        op_builder_, op_state.location, island_result_types, mlir::ValueRange{},\n+        mlir::ArrayRef<mlir::NamedAttribute>{});\n     island_op.getBody().push_back(new mlir::Block);\n     island_op.getBody().back().push_back(yield_op);\n     return island_op;\n@@ -126,13 +125,13 @@ TEST_F(SplitIntoIslandPerOpPass, IslandOpTwoOpsSplitsIntoTwoIslands) {\n   islandOp.getBody().back().push_front(inner_op_2);\n   // Code relies on a parent with a fetch op containing the island op.\n   mlir::tf_executor::GraphOp parent_graph_op =\n-      op_builder_.create<mlir::tf_executor::GraphOp>(\n-          mlir::UnknownLoc::get(&context_),\n+      mlir::tf_executor::GraphOp::create(\n+          op_builder_, mlir::UnknownLoc::get(&context_),\n           mlir::TypeRange{op_builder_.getF64Type()});\n   parent_graph_op.getRegion().push_back(new mlir::Block);\n   parent_graph_op.push_back(islandOp);\n   mlir::tf_executor::FetchOp fetch_op =\n-      op_builder_.create<mlir::tf_executor::FetchOp>(parent_graph_op.getLoc());\n+      mlir::tf_executor::FetchOp::create(op_builder_, parent_graph_op.getLoc());\n   parent_graph_op.GetBody().push_back(fetch_op);\n \n   SplitIsland(islandOp, control_type);"
        },
        {
            "sha": "2f7089edacbe31ad26504b46dfe45e6e0813f193",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/tfxla_device_specific_transforms.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Ftfxla_device_specific_transforms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Ftfxla_device_specific_transforms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Ftfxla_device_specific_transforms.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -57,8 +57,9 @@ LogicalResult TFXLADeviceSpecificTransforms::ConvertGetAlgOp(\n \n   OpBuilder opbuilder(get_alg_op);\n \n-  auto tf_const = opbuilder.create<TF::ConstOp>(\n-      get_alg_op->getLoc(), opbuilder.getI32IntegerAttr((int)tensorflow_rng));\n+  auto tf_const =\n+      TF::ConstOp::create(opbuilder, get_alg_op->getLoc(),\n+                          opbuilder.getI32IntegerAttr((int)tensorflow_rng));\n \n   get_alg_op->replaceAllUsesWith(tf_const);\n   get_alg_op->erase();"
        },
        {
            "sha": "61c8e8e161425d22ddbe436ac61d6bd1361c8718",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/utils.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Futils.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -24,11 +24,11 @@ namespace mhlo {\n \n ConstantOp GetScalarConstOfType(Type ty, Location loc, int64_t raw_value,\n                                 OpBuilder* builder) {\n-  return builder->create<ConstantOp>(loc, hlo::getScalarOfType(ty, raw_value));\n+  return ConstantOp::create(*builder, loc, hlo::getScalarOfType(ty, raw_value));\n }\n \n ConstantOp GetScalarNegZeroOfType(Type ty, Location loc, OpBuilder* builder) {\n-  return builder->create<ConstantOp>(loc, hlo::getScalarNegZeroOfType(ty));\n+  return ConstantOp::create(*builder, loc, hlo::getScalarNegZeroOfType(ty));\n }\n \n DenseIntElementsAttr GetI64ElementsAttr(ArrayAttr attr) {"
        },
        {
            "sha": "a6b848ae2fc27b4e10f1bbf0b7a5f4ffe6f00383",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/utils.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Futils.h?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -42,8 +42,8 @@ void BuildReduceBody(Type element_type, Region* body, OpBuilder* builder) {\n   block->addArguments({type, type}, SmallVector<Location, 2>(2, loc));\n \n   auto reducer =\n-      builder->create<Op>(loc, block->getArgument(0), block->getArgument(1));\n-  builder->create<ReturnOp>(loc, reducer.getResult());\n+      Op::create(*builder, loc, block->getArgument(0), block->getArgument(1));\n+  ReturnOp::create(*builder, loc, reducer.getResult());\n }\n \n ConstantOp GetScalarConstOfType(Type ty, Location loc, int64_t raw_value,"
        },
        {
            "sha": "71dce38198c96ad59cd22521672252a71d10abb5",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/xla_legalize_targets_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fxla_legalize_targets_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fxla_legalize_targets_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fxla_legalize_targets_test.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -63,17 +63,17 @@ class XlaLegalizeTargetsTest : public testing::Test {\n };\n \n TEST_F(XlaLegalizeTargetsTest, CreatesConversionTargets) {\n-  auto const_int = builder_.create<mlir::arith::ConstantIntOp>(\n-      builder_.getUnknownLoc(), builder_.getI32Type(), /*value=*/10);\n+  auto const_int = mlir::arith::ConstantIntOp::create(\n+      builder_, builder_.getUnknownLoc(), builder_.getI32Type(), /*value=*/10);\n \n   ConversionTarget target =\n       GetDefaultLegalConversionTargets(context_, /*legalize_chlo=*/false);\n   EXPECT_TRUE(target.isLegal(const_int));\n }\n \n TEST_F(XlaLegalizeTargetsTest, AllowsCHLODialect) {\n-  auto const_int = builder_.create<chlo::ConstantOp>(\n-      builder_.getUnknownLoc(), builder_.getI32TensorAttr({42}));\n+  auto const_int = chlo::ConstantOp::create(builder_, builder_.getUnknownLoc(),\n+                                            builder_.getI32TensorAttr({42}));\n \n   ConversionTarget target =\n       GetDefaultLegalConversionTargets(context_, /*legalize_chlo=*/true);\n@@ -82,8 +82,8 @@ TEST_F(XlaLegalizeTargetsTest, AllowsCHLODialect) {\n }\n \n TEST_F(XlaLegalizeTargetsTest, DontAllowCHLODialect) {\n-  auto const_int = builder_.create<chlo::ConstantOp>(\n-      builder_.getUnknownLoc(), builder_.getI32TensorAttr({42}));\n+  auto const_int = chlo::ConstantOp::create(builder_, builder_.getUnknownLoc(),\n+                                            builder_.getI32TensorAttr({42}));\n \n   ConversionTarget target =\n       GetDefaultLegalConversionTargets(context_, /*legalize_chlo=*/false);"
        },
        {
            "sha": "66b5167839731b1de69d0fa3a8ad61f044402f12",
            "filename": "tensorflow/compiler/mlir/tfr/ir/tfr_ops.cc",
            "status": "modified",
            "additions": 70,
            "deletions": 60,
            "changes": 130,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fir%2Ftfr_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fir%2Ftfr_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fir%2Ftfr_ops.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -118,10 +118,11 @@ class TFRInlinerInterface : public DialectInlinerInterface {\n     auto result_itype = llvm::cast<IntegerType>(result_type);\n     if (input_itype.getWidth() == result_itype.getWidth()) return nullptr;\n     if (input_itype.getWidth() > result_itype.getWidth()) {\n-      return builder.create<arith::TruncIOp>(conversion_loc, result_type,\n-                                             input);\n+      return arith::TruncIOp::create(builder, conversion_loc, result_type,\n+                                     input);\n     } else {\n-      return builder.create<arith::ExtSIOp>(conversion_loc, result_type, input);\n+      return arith::ExtSIOp::create(builder, conversion_loc, result_type,\n+                                    input);\n     }\n   }\n };\n@@ -148,11 +149,11 @@ TFRDialect::TFRDialect(MLIRContext *context)\n Operation *TFRDialect::materializeConstant(OpBuilder &builder, Attribute value,\n                                            Type type, Location loc) {\n   if (arith::ConstantOp::isBuildableWith(value, type))\n-    return builder.create<arith::ConstantOp>(loc, type,\n-                                             llvm::cast<TypedAttr>(value));\n+    return arith::ConstantOp::create(builder, loc, type,\n+                                     llvm::cast<TypedAttr>(value));\n   if (func::ConstantOp::isBuildableWith(value, type))\n-    return builder.create<func::ConstantOp>(\n-        loc, type, llvm::cast<FlatSymbolRefAttr>(value));\n+    return func::ConstantOp::create(builder, loc, type,\n+                                    llvm::cast<FlatSymbolRefAttr>(value));\n   return nullptr;\n }\n \n@@ -421,9 +422,10 @@ class ConvertConstToTensorConst : public OpRewritePattern<ConstantTensorOp> {\n           {static_cast<int64_t>(array.size())}, *all_types.begin());\n       DenseElementsAttr attr =\n           DenseElementsAttr::get(new_out_type, array.getValue());\n-      new_cst = rewriter.create<TF::ConstOp>(loc, new_out_type, attr);\n+      new_cst = TF::ConstOp::create(rewriter, loc, new_out_type, attr);\n       if (isa<TFRTensorType>(out_type)) {\n-        new_cst = rewriter.create<CastOp>(loc, out_type, new_cst->getResult(0));\n+        new_cst =\n+            CastOp::create(rewriter, loc, out_type, new_cst->getResult(0));\n       }\n       rewriter.replaceOp(cst_tensor_op, new_cst->getResult(0));\n       return success();\n@@ -432,9 +434,10 @@ class ConvertConstToTensorConst : public OpRewritePattern<ConstantTensorOp> {\n     TypedAttr scalar;\n     if (matchPattern(cst_tensor_op.getArg(), m_Constant(&scalar))) {\n       Type new_out_type = RankedTensorType::get({}, scalar.getType());\n-      new_cst = rewriter.create<TF::ConstOp>(loc, new_out_type, scalar);\n+      new_cst = TF::ConstOp::create(rewriter, loc, new_out_type, scalar);\n       if (isa<TFRTensorType>(out_type)) {\n-        new_cst = rewriter.create<CastOp>(loc, out_type, new_cst->getResult(0));\n+        new_cst =\n+            CastOp::create(rewriter, loc, out_type, new_cst->getResult(0));\n       }\n       rewriter.replaceOp(cst_tensor_op, new_cst->getResult(0));\n       return success();\n@@ -481,8 +484,8 @@ class RemoveRedundantCast : public OpRewritePattern<CastOp> {\n     if ((input_tensor_type.getElementType() !=\n          output_tensor_type.getElementType()) &&\n         !isQuantizedType(input_type) && !isQuantizedType(output_type)) {\n-      auto new_tfr_cast = rewriter.create<TFR::CastOp>(\n-          cast_op.getLoc(),\n+      auto new_tfr_cast = TFR::CastOp::create(\n+          rewriter, cast_op.getLoc(),\n           output_tensor_type.clone(input_tensor_type.getElementType()),\n           cast_op.getArg());\n       rewriter.replaceOpWithNewOp<TF::CastOp>(cast_op, output_type,\n@@ -652,8 +655,9 @@ class RemoveRawDataOp : public OpRewritePattern<TFRQuantRawDataOp> {\n       new_list_values.push_back(redundant_cast.getArg());\n     }\n \n-    auto new_list = rewriter.create<BuildListOp>(\n-        raw_data_op.getLoc(), preceding_list.getType(), new_list_values);\n+    auto new_list =\n+        BuildListOp::create(rewriter, raw_data_op.getLoc(),\n+                            preceding_list.getType(), new_list_values);\n     raw_data_op.getOutput().replaceAllUsesWith(new_list.getOut());\n     return success();\n   }\n@@ -679,11 +683,11 @@ class RemoveQParamsOp : public OpRewritePattern<TFRQuantQParamsOp> {\n     rewriter.setInsertionPoint(qparams_op);\n     Location loc = qparams_op->getLoc();\n     if (auto qtype = llvm::dyn_cast<quant::UniformQuantizedType>(cast_qtype)) {\n-      scale_op = rewriter.create<TF::ConstOp>(\n-          loc, RankedTensorType::get({}, rewriter.getF32Type()),\n+      scale_op = TF::ConstOp::create(\n+          rewriter, loc, RankedTensorType::get({}, rewriter.getF32Type()),\n           rewriter.getF32FloatAttr(qtype.getScale()));\n-      zp_op = rewriter.create<TF::ConstOp>(\n-          loc, RankedTensorType::get({}, rewriter.getI32Type()),\n+      zp_op = TF::ConstOp::create(\n+          rewriter, loc, RankedTensorType::get({}, rewriter.getI32Type()),\n           rewriter.getI32IntegerAttr(qtype.getZeroPoint()));\n     } else if (auto qtype = llvm::dyn_cast<quant::UniformQuantizedPerAxisType>(\n                    cast_qtype)) {\n@@ -697,20 +701,20 @@ class RemoveQParamsOp : public OpRewritePattern<TFRQuantQParamsOp> {\n           {static_cast<int64_t>(num_channels)}, rewriter.getF32Type());\n       auto scales_attr =\n           DenseElementsAttr::get(scales_type, llvm::ArrayRef(scales));\n-      scale_op = rewriter.create<TF::ConstOp>(loc, scales_attr);\n+      scale_op = TF::ConstOp::create(rewriter, loc, scales_attr);\n \n       auto zps_type = RankedTensorType::get(\n           {static_cast<int64_t>(num_channels)}, rewriter.getI32Type());\n       auto zps_attr = DenseElementsAttr::get(zps_type, llvm::ArrayRef(zps));\n-      zp_op = rewriter.create<TF::ConstOp>(loc, zps_attr);\n+      zp_op = TF::ConstOp::create(rewriter, loc, zps_attr);\n     }\n     if (!scale_op || !zp_op) {\n       return failure();\n     }\n-    auto scale_cast = rewriter.create<CastOp>(\n-        loc, qparams_op.getScale().getType(), scale_op.getOutput());\n-    auto zp_cast = rewriter.create<CastOp>(loc, qparams_op.getZp().getType(),\n-                                           zp_op.getOutput());\n+    auto scale_cast = CastOp::create(\n+        rewriter, loc, qparams_op.getScale().getType(), scale_op.getOutput());\n+    auto zp_cast = CastOp::create(rewriter, loc, qparams_op.getZp().getType(),\n+                                  zp_op.getOutput());\n \n     qparams_op.getScale().replaceAllUsesWith(scale_cast.getOut());\n     qparams_op.getZp().replaceAllUsesWith(zp_cast.getOut());\n@@ -787,10 +791,11 @@ class RemoveScaleFactorOp : public OpRewritePattern<TFRQuantScaleFactorOp> {\n     }\n     rewriter.setInsertionPoint(scale_factor_op);\n     const Location loc = scale_factor_op->getLoc();\n-    auto result_scale_op = rewriter.create<TF::ConstOp>(\n-        loc, DenseElementsAttr::get(scale_type, llvm::ArrayRef(scale_factors)));\n-    auto result_scale_cast_op = rewriter.create<CastOp>(\n-        loc, scale_factor_op.getType(), result_scale_op.getOutput());\n+    auto result_scale_op = TF::ConstOp::create(\n+        rewriter, loc,\n+        DenseElementsAttr::get(scale_type, llvm::ArrayRef(scale_factors)));\n+    auto result_scale_cast_op = CastOp::create(\n+        rewriter, loc, scale_factor_op.getType(), result_scale_op.getOutput());\n     scale_factor_op.getScaleFactor().replaceAllUsesWith(\n         result_scale_cast_op.getOut());\n     return success();\n@@ -812,50 +817,55 @@ class RemoveRescaleOp : public OpRewritePattern<TFRQuantRescaleOp> {\n     const Location loc = rescale_op->getLoc();\n     const auto result_types = rescale_op->getResultTypes();\n     auto c_false =\n-        rewriter.create<arith::ConstantOp>(loc, rewriter.getBoolAttr(false));\n+        arith::ConstantOp::create(rewriter, loc, rewriter.getBoolAttr(false));\n     TypeAttr f32_attr = TypeAttr::get(rewriter.getF32Type());\n     TFRAttrType output_type = TFRAttrType::get(rewriter.getContext());\n-    auto constant_f32_op = rewriter.create<ConstOp>(loc, output_type, f32_attr);\n+    auto constant_f32_op =\n+        ConstOp::create(rewriter, loc, output_type, f32_attr);\n     TypeAttr i32_attr = TypeAttr::get(rewriter.getI32Type());\n-    auto constant_i32_op = rewriter.create<ConstOp>(loc, output_type, i32_attr);\n+    auto constant_i32_op =\n+        ConstOp::create(rewriter, loc, output_type, i32_attr);\n \n     IntegerAttr zp_attr;\n     if (!matchPattern(zp, m_Constant(&zp_attr))) {\n       return failure();\n     }\n     rewriter.setInsertionPoint(zp.getDefiningOp());\n-    auto zp_tensor = rewriter.create<TF::ConstOp>(\n-        loc, RankedTensorType::get({}, zp.getType()), zp_attr);\n-    auto zp_cast = rewriter.create<CastOp>(\n-        loc, rewriter.getType<TFRTensorType>(), zp_tensor.getOutput());\n+    auto zp_tensor = TF::ConstOp::create(\n+        rewriter, loc, RankedTensorType::get({}, zp.getType()), zp_attr);\n+    auto zp_cast =\n+        CastOp::create(rewriter, loc, rewriter.getType<TFRTensorType>(),\n+                       zp_tensor.getOutput());\n \n     rewriter.setInsertionPoint(rescale_op);\n-    auto cast_input_to_float_op = rewriter.create<CallOp>(\n-        loc, result_types,\n-        SymbolRefAttr::get(rewriter.getContext(), \"tf__cast\"),\n-        ArrayRef<Value>{input, constant_f32_op, c_false},\n-        /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n-    auto input_x_scale_op = rewriter.create<CallOp>(\n-        loc, result_types, SymbolRefAttr::get(rewriter.getContext(), \"tf__mul\"),\n+    auto cast_input_to_float_op =\n+        CallOp::create(rewriter, loc, result_types,\n+                       SymbolRefAttr::get(rewriter.getContext(), \"tf__cast\"),\n+                       ArrayRef<Value>{input, constant_f32_op, c_false},\n+                       /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n+    auto input_x_scale_op = CallOp::create(\n+        rewriter, loc, result_types,\n+        SymbolRefAttr::get(rewriter.getContext(), \"tf__mul\"),\n         ArrayRef<Value>{cast_input_to_float_op.getResult(0), scale},\n         /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n-    auto round_rescaled_op = rewriter.create<CallOp>(\n-        loc, result_types,\n-        SymbolRefAttr::get(rewriter.getContext(), \"tf__round\"),\n-        ArrayRef<Value>{input_x_scale_op->getResult(0)},\n-        /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n-    auto cast_zp_to_float_op = rewriter.create<CallOp>(\n-        loc, result_types,\n-        SymbolRefAttr::get(rewriter.getContext(), \"tf__cast\"),\n-        ArrayRef<Value>{zp_cast, constant_f32_op, c_false},\n-        /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n-    auto recentered_op = rewriter.create<CallOp>(\n-        loc, result_types, SymbolRefAttr::get(rewriter.getContext(), \"tf__add\"),\n-        ArrayRef<Value>{round_rescaled_op->getResult(0),\n-                        cast_zp_to_float_op->getResult(0)},\n-        /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n-    auto cast_output_to_i32 = rewriter.create<CallOp>(\n-        loc, result_types,\n+    auto round_rescaled_op =\n+        CallOp::create(rewriter, loc, result_types,\n+                       SymbolRefAttr::get(rewriter.getContext(), \"tf__round\"),\n+                       ArrayRef<Value>{input_x_scale_op->getResult(0)},\n+                       /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n+    auto cast_zp_to_float_op =\n+        CallOp::create(rewriter, loc, result_types,\n+                       SymbolRefAttr::get(rewriter.getContext(), \"tf__cast\"),\n+                       ArrayRef<Value>{zp_cast, constant_f32_op, c_false},\n+                       /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n+    auto recentered_op =\n+        CallOp::create(rewriter, loc, result_types,\n+                       SymbolRefAttr::get(rewriter.getContext(), \"tf__add\"),\n+                       ArrayRef<Value>{round_rescaled_op->getResult(0),\n+                                       cast_zp_to_float_op->getResult(0)},\n+                       /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);\n+    auto cast_output_to_i32 = CallOp::create(\n+        rewriter, loc, result_types,\n         SymbolRefAttr::get(rewriter.getContext(), \"tf__cast\"),\n         ArrayRef<Value>{recentered_op->getResult(0), constant_i32_op, c_false},\n         /*args_attrs=*/nullptr, /*res_attrs=*/nullptr);"
        },
        {
            "sha": "7a03a46972371c8933a997b6d9ed99b547eb1b67",
            "filename": "tensorflow/compiler/mlir/tfr/passes/canonicalize.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fpasses%2Fcanonicalize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fpasses%2Fcanonicalize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fpasses%2Fcanonicalize.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -75,12 +75,12 @@ class UnrollSCFForOp : public OpRewritePattern<scf::ForOp> {\n     for (auto i = 0; i < trip_count; ++i) {\n       if (!iv.use_empty()) {\n         // iv' = iv + step * i;\n-        Value iter = rewriter.create<arith::ConstantIndexOp>(loc, i);\n+        Value iter = arith::ConstantIndexOp::create(rewriter, loc, i);\n         Value step_cst =\n-            rewriter.create<arith::ConstantIndexOp>(loc, step.getSExtValue());\n-        Value stride = rewriter.create<arith::MulIOp>(loc, step_cst, iter);\n+            arith::ConstantIndexOp::create(rewriter, loc, step.getSExtValue());\n+        Value stride = arith::MulIOp::create(rewriter, loc, step_cst, iter);\n         Value iv_unroll =\n-            rewriter.create<arith::AddIOp>(loc, mapping.lookup(iv), stride);\n+            arith::AddIOp::create(rewriter, loc, mapping.lookup(iv), stride);\n         mapping.map(iv, iv_unroll);\n       }\n "
        },
        {
            "sha": "5dd6a22f90c97227c9633b02b1b60917935c2299",
            "filename": "tensorflow/compiler/mlir/tfr/passes/raise_to_tf.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fpasses%2Fraise_to_tf.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fpasses%2Fraise_to_tf.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfr%2Fpasses%2Fraise_to_tf.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -148,7 +148,7 @@ class RewriteTFRCallOp : public OpRewritePattern<CallOp> {\n         mlir::cast<TypeAttr>(cast_op.getInputElementType()).getValue();\n     if (result_elt_type != original_input_type) {\n       UnrankedTensorType result_type = UnrankedTensorType::get(result_elt_type);\n-      return rewriter.create<TF::CastOp>(loc, result_type, cast_op.getArg());\n+      return TF::CastOp::create(rewriter, loc, result_type, cast_op.getArg());\n     }\n     return cast_op.getArg();\n   }\n@@ -167,7 +167,7 @@ class RewriteTFRCallOp : public OpRewritePattern<CallOp> {\n       Type current_input_type = mlir::cast<TypeAttr>(input_types[i]).getValue();\n       if (current_input_type != target_input_type) {\n         input_values[i] =\n-            rewriter.create<TF::CastOp>(loc, result_type, input_values[i]);\n+            TF::CastOp::create(rewriter, loc, result_type, input_values[i]);\n       }\n     }\n   }\n@@ -397,18 +397,18 @@ LogicalResult RewriteTFRCallOp::CreateAndReplaceOp(\n     Type res_type = res.value();\n     if (mlir::dyn_cast<TFRTensorType>(res_type)) {\n       Value new_res = new_op->getResult(res.index());\n-      auto casted = rewriter.create<CastOp>(loc, res_type, new_res);\n+      auto casted = CastOp::create(rewriter, loc, res_type, new_res);\n       new_results.push_back(casted.getOut());\n     } else if (auto list_type =\n                    mlir::dyn_cast<TFRTensorListType>(res.value())) {\n       SmallVector<Value, 4> tensor_list;\n       for (int i = res.index(); i < new_op->getNumResults(); i++) {\n         Value new_res = new_op->getResult(i);\n         auto casted =\n-            rewriter.create<CastOp>(loc, unconstrainted_type, new_res);\n+            CastOp::create(rewriter, loc, unconstrainted_type, new_res);\n         tensor_list.push_back(casted.getOut());\n       }\n-      auto list_op = rewriter.create<BuildListOp>(loc, res_type, tensor_list);\n+      auto list_op = BuildListOp::create(rewriter, loc, res_type, tensor_list);\n       new_results.push_back(list_op.getOut());\n     }\n   }"
        },
        {
            "sha": "aafd3d958f826bbea109e1684f2a0030360f01b0",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/deduplicate_if_result_pass.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fdeduplicate_if_result_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fdeduplicate_if_result_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fdeduplicate_if_result_pass.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -94,7 +94,7 @@ mlir::func::FuncOp CreateBranchFunctionWithDeduplicatedResults(\n   auto new_func_type = mlir::FunctionType::get(builder.getContext(), arg_types,\n                                                new_result_types);\n \n-  auto new_func = builder.create<mlir::func::FuncOp>(loc, name, new_func_type);\n+  auto new_func = mlir::func::FuncOp::create(builder, loc, name, new_func_type);\n   new_func.setVisibility(mlir::func::FuncOp::Visibility::Private);\n \n   mlir::OpBuilder::InsertionGuard guard(builder);\n@@ -110,8 +110,8 @@ mlir::func::FuncOp CreateBranchFunctionWithDeduplicatedResults(\n \n   // Create the call op to the original func. The arguments are simply\n   // the arguments from the wrapper function.\n-  auto call_op = builder.create<mlir::TF::PartitionedCallOp>(\n-      loc, result_types, block->getArguments(), /*args_attrs=*/nullptr,\n+  auto call_op = mlir::TF::PartitionedCallOp::create(\n+      builder, loc, result_types, block->getArguments(), /*args_attrs=*/nullptr,\n       /*res_attrs=*/nullptr,\n       mlir::FlatSymbolRefAttr::get(func.getSymNameAttr()), empty_string_attr,\n       empty_string_attr, empty_string_attr);\n@@ -120,7 +120,7 @@ mlir::func::FuncOp CreateBranchFunctionWithDeduplicatedResults(\n     results.push_back(call_op.getResult(i));\n   }\n \n-  builder.create<mlir::func::ReturnOp>(loc, results);\n+  mlir::func::ReturnOp::create(builder, loc, results);\n \n   return new_func;\n }\n@@ -183,8 +183,8 @@ void DeduplicateIfOps(mlir::ModuleOp module) {\n         new_result_types.push_back(op->getResult(i).getType());\n       }\n \n-      auto new_if_op = builder.create<mlir::TF::IfOp>(\n-          op.getLoc(), new_result_types, op.getCond(), op.getInput(),\n+      auto new_if_op = mlir::TF::IfOp::create(\n+          builder, op.getLoc(), new_result_types, op.getCond(), op.getInput(),\n           new_then_func.getSymName(), new_else_func.getSymName(),\n           op.getIsStateless());\n "
        },
        {
            "sha": "73d5836fa895a69aeb8ebb06d7563e467b6da63e",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/fuse_tpu_compile_and_execute_ops.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Ffuse_tpu_compile_and_execute_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Ffuse_tpu_compile_and_execute_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Ffuse_tpu_compile_and_execute_ops.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -159,12 +159,11 @@ void FuseCompileAndExecuteOps(\n   auto producer_name =\n       used_exec_op->getAttrOfType<mlir::StringAttr>(\"_producer_name\");\n   if (!producer_name) producer_name = mlir::StringAttr::get(context, \"default\");\n-  auto compile_and_execute_op =\n-      builder.create<mlir::TF::TPUCompileMlirAndExecuteOp>(\n-          used_exec_op.getLoc(), output_types, exec_op_args,\n-          static_shape_tensors,\n-          builder.getI32ArrayAttr(static_shaped_operand_indices_attr),\n-          compile_op.getMlirModule(), compile_op.getMetadata(), producer_name);\n+  auto compile_and_execute_op = mlir::TF::TPUCompileMlirAndExecuteOp::create(\n+      builder, used_exec_op.getLoc(), output_types, exec_op_args,\n+      static_shape_tensors,\n+      builder.getI32ArrayAttr(static_shaped_operand_indices_attr),\n+      compile_op.getMlirModule(), compile_op.getMetadata(), producer_name);\n \n   for (auto exec_op : exec_op_in_group) {\n     exec_op.replaceAllUsesWith(compile_and_execute_op.getResults());"
        },
        {
            "sha": "1e2231f1c59584c0490059d76a4a27c737ccf3b1",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/rewrite_cluster_to_ifrt_call.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Frewrite_cluster_to_ifrt_call.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Frewrite_cluster_to_ifrt_call.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Frewrite_cluster_to_ifrt_call.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -151,8 +151,8 @@ class RewriteClusterToIfrtCallPass\n       // ifrt program already exists\n       builder.setInsertionPoint(cluster_func);\n \n-      mlir::TF::IfrtCallOp ifrt_call_op = builder.create<mlir::TF::IfrtCallOp>(\n-          cluster_func->getLoc(), cluster_func.getResultTypes(),\n+      mlir::TF::IfrtCallOp ifrt_call_op = mlir::TF::IfrtCallOp::create(\n+          builder, cluster_func->getLoc(), cluster_func.getResultTypes(),\n           cluster_func->getOperands());\n \n       int64_t program_id;\n@@ -189,8 +189,8 @@ class RewriteClusterToIfrtCallPass\n     mlir::OpBuilder::InsertionGuard insertion_guard(builder);\n     builder.setInsertionPoint(callee_func);\n \n-    mlir::func::FuncOp cloned_ifrt_program = builder.create<mlir::func::FuncOp>(\n-        callee_func->getLoc(), ifrt_program_name,\n+    mlir::func::FuncOp cloned_ifrt_program = mlir::func::FuncOp::create(\n+        builder, callee_func->getLoc(), ifrt_program_name,\n         callee_func.getFunctionType());\n     mlir::IRMapping mapper;\n     callee_func.cloneInto(cloned_ifrt_program, mapper);\n@@ -226,8 +226,8 @@ class RewriteClusterToIfrtCallPass\n \n     builder.setInsertionPoint(cluster_func);\n \n-    mlir::TF::IfrtCallOp ifrt_call_op = builder.create<mlir::TF::IfrtCallOp>(\n-        cluster_func->getLoc(), cluster_func.getResultTypes(),\n+    mlir::TF::IfrtCallOp ifrt_call_op = mlir::TF::IfrtCallOp::create(\n+        builder, cluster_func->getLoc(), cluster_func.getResultTypes(),\n         cluster_func->getOperands());\n \n     // TODO(b/304839793): populate variable names after adding a variable"
        },
        {
            "sha": "d0c8f03bf7f9c29c35aaa6e51c8b2d6d34fbea7c",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/tf_restore_merging.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf_restore_merging.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf_restore_merging.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf_restore_merging.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -130,15 +130,15 @@ class TfRestoreMergingPass\n     // merged in order to keep the dominance property.\n     mlir::OpBuilder builder(restores_to_merge.front());\n \n-    auto new_tensor_names = builder.create<mlir::TF::ConstOp>(\n-        builder.getFusedLoc(tensor_names_locs),\n+    auto new_tensor_names = mlir::TF::ConstOp::create(\n+        builder, builder.getFusedLoc(tensor_names_locs),\n         GetStringTensorAttr(merged_tensor_names));\n-    auto new_shape_and_slices = builder.create<mlir::TF::ConstOp>(\n-        builder.getFusedLoc(shape_and_slices_locs),\n+    auto new_shape_and_slices = mlir::TF::ConstOp::create(\n+        builder, builder.getFusedLoc(shape_and_slices_locs),\n         GetStringTensorAttr(merged_shape_and_slices));\n \n-    auto new_restore = builder.create<mlir::TF::RestoreV2Op>(\n-        builder.getFusedLoc(restore_locs),\n+    auto new_restore = mlir::TF::RestoreV2Op::create(\n+        builder, builder.getFusedLoc(restore_locs),\n         mlir::TypeRange(mlir::ValueRange(values_to_replace)), prefix,\n         new_tensor_names, new_shape_and_slices);\n     for (auto [old_value, new_value] :"
        },
        {
            "sha": "cb5b3e7afdcc1365aedd08056fb7985347de3d83",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/tf_restore_splitting.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf_restore_splitting.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf_restore_splitting.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf_restore_splitting.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -93,15 +93,15 @@ class TfRestoreSplittingPass\n                    shape_and_slices.getValues<llvm::StringRef>(),\n                    restore.getTensors())) {\n       auto new_tensor_names =\n-          builder.create<mlir::TF::ConstOp>(restore.getTensorNames().getLoc(),\n-                                            GetStringTensorAttr({tensor_name}));\n+          mlir::TF::ConstOp::create(builder, restore.getTensorNames().getLoc(),\n+                                    GetStringTensorAttr({tensor_name}));\n \n-      auto new_shape_and_slices = builder.create<mlir::TF::ConstOp>(\n-          restore.getShapeAndSlices().getLoc(),\n+      auto new_shape_and_slices = mlir::TF::ConstOp::create(\n+          builder, restore.getShapeAndSlices().getLoc(),\n           GetStringTensorAttr({shape_and_slice}));\n \n-      auto new_restore = builder.create<mlir::TF::RestoreV2Op>(\n-          restore.getLoc(), mlir::TypeRange({result.getType()}),\n+      auto new_restore = mlir::TF::RestoreV2Op::create(\n+          builder, restore.getLoc(), mlir::TypeRange({result.getType()}),\n           restore.getPrefix(), new_tensor_names, new_shape_and_slices);\n       result.replaceAllUsesWith(new_restore.getTensors()[0]);\n     }"
        },
        {
            "sha": "916b41620ad33ec90d7a1c569a83c225c0308505",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/lower_saved_model.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Flower_saved_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Flower_saved_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Flower_saved_model.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -440,8 +440,8 @@ void LowerTFSavedModelPass::HoistInvariantOps(mlir::ModuleOp module) {\n   // \"_tfrt_resource_init\" is the special function that executes all invariant\n   // ops (eg. read-only variables) used in the model. This function should be\n   // executed after user-specified initialization.\n-  auto init_func_op = builder.create<mlir::func::FuncOp>(\n-      module.getLoc(), \"_tfrt_resource_init\",\n+  auto init_func_op = mlir::func::FuncOp::create(\n+      builder, module.getLoc(), \"_tfrt_resource_init\",\n       mlir::FunctionType::get(module.getContext(), /*inputs=*/{},\n                               /*results=*/{}));\n   auto *block = init_func_op.addEntryBlock();\n@@ -481,8 +481,8 @@ void LowerTFSavedModelPass::HoistInvariantOps(mlir::ModuleOp module) {\n     auto *new_op = new_value.getDefiningOp();\n     assert(new_op);\n     builder.setInsertionPointAfter(new_op);\n-    auto set_resource_op = builder.create<mlir::TF::_TfrtSetResourceOp>(\n-        new_op->getLoc(), new_value, index);\n+    auto set_resource_op = mlir::TF::_TfrtSetResourceOp::create(\n+        builder, new_op->getLoc(), new_value, index);\n \n     // Preserve the device attribute.\n     llvm::StringRef device = kCpuDeviceName;\n@@ -494,7 +494,7 @@ void LowerTFSavedModelPass::HoistInvariantOps(mlir::ModuleOp module) {\n \n   builder.setInsertionPointToEnd(block);\n   // Finish building the init function by inserting an return op.\n-  builder.create<mlir::func::ReturnOp>(init_func_op.getLoc());\n+  mlir::func::ReturnOp::create(builder, init_func_op.getLoc());\n \n   // Now that we have the index for each value that will be replaced, we can\n   // create the tf._TfrtGetResource op in each function using these indices.\n@@ -568,17 +568,17 @@ void LowerTFSavedModelPass::ReplaceHoistedValues(\n       llvm::SmallVector<mlir::Value> new_values;\n \n       if (fuse_get_resource_ops_) {\n-        auto get_resource_op = builder.create<mlir::TF::_TfrtGetResourceOp>(\n-            block->getParentOp()->getLoc(), old_values.getTypes(),\n+        auto get_resource_op = mlir::TF::_TfrtGetResourceOp::create(\n+            builder, block->getParentOp()->getLoc(), old_values.getTypes(),\n             builder.getI64ArrayAttr(indices),\n             builder.getStrArrayAttr(shared_name_arr),\n             builder.getStrArrayAttr(container_arr));\n         get_resource_op->setAttr(\"device\", builder.getStringAttr(device));\n         new_values = get_resource_op.getResults();\n       } else {\n         for (int i = 0; i < old_values.size(); ++i) {\n-          auto get_resource_op = builder.create<mlir::TF::_TfrtGetResourceOp>(\n-              block->getParentOp()->getLoc(),\n+          auto get_resource_op = mlir::TF::_TfrtGetResourceOp::create(\n+              builder, block->getParentOp()->getLoc(),\n               mlir::TypeRange(old_values[i].getType()),\n               builder.getI64ArrayAttr(indices[i]),\n               builder.getStrArrayAttr(shared_name_arr[i]),\n@@ -670,8 +670,8 @@ mlir::LogicalResult ConvertReferenceVariableToResourceVariable(\n \n   mlir::OpBuilder builder(var_op);\n \n-  auto var_handle_op = builder.create<mlir::TF::VarHandleOp>(\n-      var_op.getLoc(),\n+  auto var_handle_op = mlir::TF::VarHandleOp::create(\n+      builder, var_op.getLoc(),\n       mlir::RankedTensorType::get(\n           {}, mlir::TF::ResourceType::get(\n                   llvm::ArrayRef<mlir::TensorType>{tensor_type},\n@@ -682,8 +682,8 @@ mlir::LogicalResult ConvertReferenceVariableToResourceVariable(\n     // Set insertion point to this identity_op so that the side-effect\n     // visibility is preserved.\n     builder.setInsertionPoint(op);\n-    auto read_var_op = builder.create<mlir::TF::ReadVariableOp>(\n-        op.getLoc(), op.getType(), var_handle_op);\n+    auto read_var_op = mlir::TF::ReadVariableOp::create(\n+        builder, op.getLoc(), op.getType(), var_handle_op);\n     op.replaceAllUsesWith(read_var_op.getValue());\n     op.erase();\n   }\n@@ -692,8 +692,8 @@ mlir::LogicalResult ConvertReferenceVariableToResourceVariable(\n     // Set the insertion point after the assign op so that all operands are\n     // dominating the newly created op.\n     builder.setInsertionPoint(op);\n-    builder.create<mlir::TF::AssignVariableOp>(op.getLoc(), var_handle_op,\n-                                               op.getValue());\n+    mlir::TF::AssignVariableOp::create(builder, op.getLoc(), var_handle_op,\n+                                       op.getValue());\n     op.erase();\n   }\n \n@@ -704,8 +704,8 @@ mlir::LogicalResult ConvertReferenceVariableToResourceVariable(\n     // the newly created op.\n     builder.setInsertionPoint(op);\n     // Create a new read variable op, so that the side-effects are preserved.\n-    auto read_var_op = builder.create<mlir::TF::ReadVariableOp>(\n-        op->getLoc(), tensor_type, var_handle_op);\n+    auto read_var_op = mlir::TF::ReadVariableOp::create(\n+        builder, op->getLoc(), tensor_type, var_handle_op);\n     op->setOperand(idx, read_var_op.getValue());\n   }\n "
        },
        {
            "sha": "38737e22d1c5884403e099fca8e2b14672cad4b2",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/merge_tf_if_ops.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmerge_tf_if_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmerge_tf_if_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmerge_tf_if_ops.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -225,8 +225,8 @@ class MergeTfIfOpsPass\n                     [](mlir::TF::IfOp op) { return op.getIsStateless(); });\n \n     // Create the merged tf.If op using the new branches.\n-    auto new_if_op = builder.create<mlir::TF::IfOp>(\n-        loc, new_result_types, if_ops.front().getCond(),\n+    auto new_if_op = mlir::TF::IfOp::create(\n+        builder, loc, new_result_types, if_ops.front().getCond(),\n         if_ops.front().getInput(), then_branch_name, else_branch_name,\n         is_stateless);\n \n@@ -249,8 +249,8 @@ class MergeTfIfOpsPass\n       llvm::ArrayRef<mlir::TF::IfOp> if_ops,\n       llvm::function_ref<mlir::FlatSymbolRefAttr(mlir::TF::IfOp)> get_branch) {\n     std::string branch_name = absl::StrCat(branch_prefix, branch_suffix);\n-    auto branch = builder.create<mlir::func::FuncOp>(loc, branch_name,\n-                                                     branch_function_type);\n+    auto branch = mlir::func::FuncOp::create(builder, loc, branch_name,\n+                                             branch_function_type);\n     branch.setVisibility(mlir::func::FuncOp::Visibility::Private);\n \n     mlir::OpBuilder::InsertionGuard guard(builder);\n@@ -267,16 +267,17 @@ class MergeTfIfOpsPass\n     for (auto if_op : if_ops) {\n       // Create the call op to the original branch. The arguments are simply\n       // the arguments from the wrapper function.\n-      auto call_op = builder.create<mlir::TF::PartitionedCallOp>(\n-          if_op.getLoc(), if_op.getResultTypes(), block->getArguments(),\n+      auto call_op = mlir::TF::PartitionedCallOp::create(\n+          builder, if_op.getLoc(), if_op.getResultTypes(),\n+          block->getArguments(),\n           /*args_attrs=*/nullptr, /*res_attrs=*/nullptr, get_branch(if_op),\n           empty_string_attr, empty_string_attr, empty_string_attr);\n \n       // The results are the concatenation of the original branches.\n       results.append(call_op.getOutput().begin(), call_op.getOutput().end());\n     }\n \n-    builder.create<mlir::func::ReturnOp>(loc, results);\n+    mlir::func::ReturnOp::create(builder, loc, results);\n \n     return branch.getSymName();\n   }"
        },
        {
            "sha": "b0ad89b6b55d24188a15a2024a902dadbd280397",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/broadcast_propagation_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fbroadcast_propagation_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fbroadcast_propagation_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fbroadcast_propagation_pass.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -280,8 +280,8 @@ DenseMap<BroadcastIntent, Value> realizeBroadcastIntents(\n       setInsertionPointToEarliestPointWithAllValuesAvailable(\n           rewriter, parentBlock,\n           ValueRange{it.targetValue, it.outputDimensions});\n-      realizations[it] = rewriter.create<DynamicBroadcastInDimOp>(\n-          it.targetValue.getLoc(), it.resultType, it.targetValue,\n+      realizations[it] = DynamicBroadcastInDimOp::create(\n+          rewriter, it.targetValue.getLoc(), it.resultType, it.targetValue,\n           it.outputDimensions,\n           mlir::cast<DenseIntElementsAttr>(it.broadcastDimensions));\n       continue;"
        },
        {
            "sha": "18459a9e4e13a8f98d6919fc8fa2da21c2956377",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/embed_tf_framework.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fembed_tf_framework.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fembed_tf_framework.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fembed_tf_framework.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -95,10 +95,10 @@ struct AllocOpConverter : public OpConversionPattern<memref::AllocOp> {\n         alloc, alloc.getType(), *ctx, adaptor.getOperands(),\n         reuse_input_candidates, reuse_output_index);\n     Location loc = buffer.getLoc();\n-    Value cond = rewriter.create<IsValidMemRefOp>(\n-        loc, rewriter.getIntegerType(1), buffer);\n-    rewriter.create<TFAssertOp>(loc, *ctx, cond, ErrorCode::RESOURCE_EXHAUSTED,\n-                                \"failed to allocate memory\");\n+    Value cond = IsValidMemRefOp::create(rewriter, loc,\n+                                         rewriter.getIntegerType(1), buffer);\n+    TFAssertOp::create(rewriter, loc, *ctx, cond, ErrorCode::RESOURCE_EXHAUSTED,\n+                       \"failed to allocate memory\");\n     return success();\n   }\n };"
        },
        {
            "sha": "59792ae7297ce2dd3ba0e0242e4109123e5eaefe",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/func_to_jit_invocations.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ffunc_to_jit_invocations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ffunc_to_jit_invocations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ffunc_to_jit_invocations.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -65,8 +65,8 @@ LogicalResult RewriteToFullJit(func::FuncOp op) {\n                                          old_body->getArgumentTypes(), locs);\n \n   // Create the JIT compile op.\n-  auto jit_compile_op = rewriter.create<tf_framework::JITCompileOp>(\n-      loc, rewriter.getType<tf_framework::JITCallableType>(),\n+  auto jit_compile_op = tf_framework::JITCompileOp::create(\n+      rewriter, loc, rewriter.getType<tf_framework::JITCallableType>(),\n       /*ctx=*/mlir::Value());\n \n   // Move the original functions operations into the body.\n@@ -80,18 +80,18 @@ LogicalResult RewriteToFullJit(func::FuncOp op) {\n \n     Operation *terminator = jit_block->getTerminator();\n     rewriter.setInsertionPointAfter(terminator);\n-    rewriter.create<tf_framework::JITCompileYieldOp>(\n-        loc, terminator->getOperands().front());\n+    tf_framework::JITCompileYieldOp::create(rewriter, loc,\n+                                            terminator->getOperands().front());\n     terminator->erase();\n   }\n \n   // Create JIT execute op.\n-  auto execute = rewriter.create<tf_framework::JITExecuteOp>(\n-      loc, op.getResultTypes().front(), /*ctx=*/Value(),\n+  auto execute = tf_framework::JITExecuteOp::create(\n+      rewriter, loc, op.getResultTypes().front(), /*ctx=*/Value(),\n       jit_compile_op.getResult(), new_body->getArguments());\n \n   // Create a return.\n-  rewriter.create<func::ReturnOp>(loc, execute.getResult());\n+  func::ReturnOp::create(rewriter, loc, execute.getResult());\n   return success();\n }\n \n@@ -111,28 +111,28 @@ LogicalResult RewriteToLargeSizeJit(FuncOp op) {\n \n   // Create large argument condition.\n   auto arg_1 = new_body->getArgument(0);\n-  auto shape_1 = rewriter.create<shape::ShapeOfOp>(loc, arg_1);\n-  auto num_elems_1 = rewriter.create<shape::NumElementsOp>(loc, shape_1);\n-  Value cst_i32_limit = rewriter.create<arith::ConstantIndexOp>(loc, i32Limit);\n-  Value large_tensor_predicate = rewriter.create<arith::CmpIOp>(\n-      loc, arith::CmpIPredicate::sgt, num_elems_1, cst_i32_limit);\n+  auto shape_1 = shape::ShapeOfOp::create(rewriter, loc, arg_1);\n+  auto num_elems_1 = shape::NumElementsOp::create(rewriter, loc, shape_1);\n+  Value cst_i32_limit = arith::ConstantIndexOp::create(rewriter, loc, i32Limit);\n+  Value large_tensor_predicate = arith::CmpIOp::create(\n+      rewriter, loc, arith::CmpIPredicate::sgt, num_elems_1, cst_i32_limit);\n   if (new_body->getNumArguments() > 1) {\n     auto arg_2 = new_body->getArgument(1);\n-    auto shape_2 = rewriter.create<shape::ShapeOfOp>(loc, arg_2);\n-    auto num_elems_2 = rewriter.create<shape::NumElementsOp>(loc, shape_2);\n-    large_tensor_predicate = rewriter.create<arith::OrIOp>(\n-        loc, large_tensor_predicate,\n+    auto shape_2 = shape::ShapeOfOp::create(rewriter, loc, arg_2);\n+    auto num_elems_2 = shape::NumElementsOp::create(rewriter, loc, shape_2);\n+    large_tensor_predicate = arith::OrIOp::create(\n+        rewriter, loc, large_tensor_predicate,\n         // Compare op to check size of the second op\n-        rewriter.create<arith::CmpIOp>(loc, arith::CmpIPredicate::sgt,\n-                                       num_elems_2, cst_i32_limit));\n+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::sgt,\n+                              num_elems_2, cst_i32_limit));\n   }\n \n   // Create dispatch code.\n   auto jit_body_builder_fn = [&](OpBuilder &b, Location loc) {\n     // Create JIT compile op.\n     auto callable_ty = b.getType<tf_framework::JITCallableType>();\n-    auto jit_compile_op =\n-        b.create<tf_framework::JITCompileOp>(loc, callable_ty, /*ctx=*/Value());\n+    auto jit_compile_op = tf_framework::JITCompileOp::create(\n+        b, loc, callable_ty, /*ctx=*/Value());\n     {\n       OpBuilder::InsertionGuard g(b);\n       Block *block = b.createBlock(\n@@ -144,15 +144,15 @@ LogicalResult RewriteToLargeSizeJit(FuncOp op) {\n       for (auto &op : old_body->without_terminator()) {\n         b.clone(op, bvm);\n       }\n-      b.create<tf_framework::JITCompileYieldOp>(\n-          loc, block->back().getResults().front());\n+      tf_framework::JITCompileYieldOp::create(\n+          b, loc, block->back().getResults().front());\n     }\n \n     // Create JIT execute op.\n-    auto jit_execute_op = b.create<tf_framework::JITExecuteOp>(\n-        loc, op.getResultTypes().front(), /*ctx=*/Value(),\n+    auto jit_execute_op = tf_framework::JITExecuteOp::create(\n+        b, loc, op.getResultTypes().front(), /*ctx=*/Value(),\n         jit_compile_op.getResult(), new_body->getArguments());\n-    b.create<scf::YieldOp>(loc, jit_execute_op.getResult());\n+    scf::YieldOp::create(b, loc, jit_execute_op.getResult());\n   };\n   auto aot_body_builder_fn = [&](OpBuilder &b, Location loc) {\n     IRMapping bvm;\n@@ -161,13 +161,13 @@ LogicalResult RewriteToLargeSizeJit(FuncOp op) {\n     for (auto &op : old_body->without_terminator()) {\n       last_clone = b.clone(op, bvm);\n     }\n-    b.create<scf::YieldOp>(loc, last_clone->getResults().front());\n+    scf::YieldOp::create(b, loc, last_clone->getResults().front());\n   };\n \n   // Create the conditional and return operation.\n-  auto ifOp = rewriter.create<scf::IfOp>(\n-      loc, large_tensor_predicate, jit_body_builder_fn, aot_body_builder_fn);\n-  rewriter.create<func::ReturnOp>(loc, ifOp.getResults().front());\n+  auto ifOp = scf::IfOp::create(rewriter, loc, large_tensor_predicate,\n+                                jit_body_builder_fn, aot_body_builder_fn);\n+  func::ReturnOp::create(rewriter, loc, ifOp.getResults().front());\n \n   // Remove the old body.\n   rewriter.eraseBlock(old_body);\n@@ -186,19 +186,19 @@ void PackJITCompileOp(tf_framework::JITCompileOp op,\n   // Temporarily, build the module that would be JIT-compiled. This is only to\n   // obtain the serialized code attribute.\n   auto loc = op->getLoc();\n-  auto jit_module = rewriter.create<ModuleOp>(loc);\n+  auto jit_module = ModuleOp::create(rewriter, loc);\n   {\n     OpBuilder::InsertionGuard g(rewriter);\n     rewriter.setInsertionPointToStart(jit_module.SingleBlock::getBody());\n-    auto jit_function = rewriter.create<func::FuncOp>(\n-        loc, tf_framework::JITCompileFromStrOp::kJITEntryFunctionName,\n+    auto jit_function = func::FuncOp::create(\n+        rewriter, loc, tf_framework::JITCompileFromStrOp::kJITEntryFunctionName,\n         rewriter.getFunctionType(body->getArgumentTypes(),\n                                  yield_op->getOperandTypes()));\n     jit_function->setAttr(tf_framework::TFFrameworkDialect::kTFEntryAttrName,\n                           rewriter.getUnitAttr());\n     jit_function.getBody().takeBody(op.getBodyRegion());\n     rewriter.setInsertionPointToEnd(&jit_function.getBody().front());\n-    rewriter.create<func::ReturnOp>(loc, yield_op.getResult());\n+    func::ReturnOp::create(rewriter, loc, yield_op.getResult());\n     rewriter.eraseOp(yield_op);\n   }\n "
        },
        {
            "sha": "66a455ca71c745c2128c007d03dd729c6c3748aa",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/merge_assuming_ops_pass.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fmerge_assuming_ops_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fmerge_assuming_ops_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Fmerge_assuming_ops_pass.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -73,8 +73,8 @@ struct ShapeReificationPattern : public OpRewritePattern<shape::ShapeOfOp> {\n \n     // Insert cast if needed.\n     if (reifiedShape.getType() != op.getType()) {\n-      reifiedShape = rewriter.create<tensor::CastOp>(op.getLoc(), op.getType(),\n-                                                     reifiedShape);\n+      reifiedShape = tensor::CastOp::create(rewriter, op.getLoc(), op.getType(),\n+                                            reifiedShape);\n     }\n \n     rewriter.replaceOp(op, reifiedShape);\n@@ -148,9 +148,9 @@ LogicalResult moveUpIntoAssumingOpMatchAndRewrite(Operation *op,\n   // Insert the rewritten assuming op right before the old one.\n   OpBuilder::InsertionGuard guard(rewriter);\n   rewriter.setInsertionPoint(assumingOp);\n-  auto newAssumingOp = rewriter.create<shape::AssumingOp>(\n-      assumingOp.getLoc(), assumingOp.getWitness(),\n-      [&](OpBuilder &b, Location) {\n+  auto newAssumingOp = shape::AssumingOp::create(\n+      rewriter, assumingOp.getLoc(), assumingOp.getWitness(),\n+      [&](OpBuilder& b, Location) {\n         // Copy body.\n         IRMapping mapping;\n         for (auto &nested : body->without_terminator())\n@@ -304,9 +304,9 @@ struct MoveUpOutOfAssumingOpPattern : public OpRewritePattern<OpTy> {\n     // explicitly as they are assumed to be independent. The assuming op is\n     // rewritten accordingly.\n     SmallVector<Value, 2> replacementValues;\n-    auto newAssumingOp = rewriter.create<shape::AssumingOp>(\n-        assumingOp.getLoc(), assumingOp.getWitness(),\n-        [&](OpBuilder &b, Location) {\n+    auto newAssumingOp = shape::AssumingOp::create(\n+        rewriter, assumingOp.getLoc(), assumingOp.getWitness(),\n+        [&](OpBuilder& b, Location) {\n           // Copy body.\n           IRMapping mapping;\n           for (Operation &nested : body->without_terminator()) {\n@@ -354,15 +354,16 @@ struct MergeAssumingOpsPattern : public OpRewritePattern<shape::AssumingOp> {\n     // Merge witnesses.\n     OpBuilder::InsertionGuard guard(rewriter);\n     rewriter.setInsertionPoint(precedingOp);\n-    Value newWitness = rewriter.create<shape::AssumingAllOp>(\n-        op.getWitness().getDefiningOp()->getLoc(),\n+    Value newWitness = shape::AssumingAllOp::create(\n+        rewriter, op.getWitness().getDefiningOp()->getLoc(),\n         ValueRange{precedingOp.getWitness(), op.getWitness()});\n \n     // Merge assuming ops.\n     Block *body_a = precedingOp.getBody();\n     Block *body_b = op.getBody();\n-    auto newAssumingOp = rewriter.create<shape::AssumingOp>(\n-        precedingOp.getLoc(), newWitness, [&](OpBuilder &b, Location) {\n+    auto newAssumingOp = shape::AssumingOp::create(\n+        rewriter, precedingOp.getLoc(), newWitness,\n+        [&](OpBuilder& b, Location) {\n           // Copy preceding op's body.\n           IRMapping mapping;\n           for (auto &nested : body_a->without_terminator()) {"
        },
        {
            "sha": "959c56a87982ec2e78004ff06a78f4fabca16323",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/rewrite_tf_framework_assert.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Frewrite_tf_framework_assert.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Frewrite_tf_framework_assert.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Frewrite_tf_framework_assert.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -49,14 +49,14 @@ class TFAssertOpConverter : public OpConversionPattern<TFAssertOp> {\n     auto func = op->getParentOfType<func::FuncOp>();\n     Block *error_reporting_block =\n         rewriter.createBlock(&func.getRegion(), {}, {});\n-    rewriter.create<ReportErrorOp>(loc, adaptor.getCtx(),\n-                                   adaptor.getErrorCode(), adaptor.getMsg());\n+    ReportErrorOp::create(rewriter, loc, adaptor.getCtx(),\n+                          adaptor.getErrorCode(), adaptor.getMsg());\n \n     SmallVector<Value, 2> null_memrefs;\n     for (auto type : func.getFunctionType().getResults()) {\n-      null_memrefs.push_back(rewriter.create<NullMemRefOp>(loc, type));\n+      null_memrefs.push_back(NullMemRefOp::create(rewriter, loc, type));\n     }\n-    rewriter.create<func::ReturnOp>(loc, null_memrefs);\n+    func::ReturnOp::create(rewriter, loc, null_memrefs);\n \n     rewriter.restoreInsertionPoint(ip);\n     rewriter.replaceOpWithNewOp<cf::CondBranchOp>("
        },
        {
            "sha": "2fd419972f4289b47676bb7d3fb92589faf1e8c2",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/tensorflow_abi_knowledge_propagation.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftensorflow_abi_knowledge_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftensorflow_abi_knowledge_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftensorflow_abi_knowledge_propagation.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -116,8 +116,8 @@ struct PropagateTfAbiKnowledgeToKernelsPass\n           Value offset = kernel.getArgument(kernel_p + 2);\n           Value &zero = constants[0];\n           if (!zero) {\n-            zero = b.create<LLVM::ConstantOp>(loc, offset.getType(),\n-                                              b.getIndexAttr(0));\n+            zero = LLVM::ConstantOp::create(b, loc, offset.getType(),\n+                                            b.getIndexAttr(0));\n           }\n           offset.replaceAllUsesWith(zero);\n         }\n@@ -128,9 +128,9 @@ struct PropagateTfAbiKnowledgeToKernelsPass\n               kernel.getArgument(kernel_p + 2 + memref.getRank() * 2);\n           Value &stride_val = constants[const_stride->second];\n           if (!stride_val) {\n-            stride_val = b.create<LLVM::ConstantOp>(\n-                loc, inner_stride.getType(),\n-                b.getIndexAttr(const_stride->second));\n+            stride_val =\n+                LLVM::ConstantOp::create(b, loc, inner_stride.getType(),\n+                                         b.getIndexAttr(const_stride->second));\n           }\n           inner_stride.replaceAllUsesWith(stride_val);\n         }"
        },
        {
            "sha": "21d477b30547c1700e325a337dbf1490ab0cfb46",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/tf_framework_legalize_to_llvm.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 60,
            "changes": 123,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_framework_legalize_to_llvm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_framework_legalize_to_llvm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_framework_legalize_to_llvm.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -70,25 +70,27 @@ class ConvertToLLVMCallOpPattern : public ConvertOpToLLVMPattern<OpTy> {\n     // If the attribute is missing or empty, set the element count to 0 and\n     // return NULL.\n     if (!attr.has_value() || attr.value().empty()) {\n-      Value zero = rewriter->create<LLVM::ConstantOp>(\n-          loc, size_ty, rewriter->getIntegerAttr(size_ty, 0));\n-      Value null_ptr = rewriter->create<LLVM::ZeroOp>(loc, ptr_ty);\n+      Value zero = LLVM::ConstantOp::create(\n+          *rewriter, loc, size_ty, rewriter->getIntegerAttr(size_ty, 0));\n+      Value null_ptr = LLVM::ZeroOp::create(*rewriter, loc, ptr_ty);\n       return std::make_pair(zero, null_ptr);\n     }\n \n     // Allocate array to store the elements.\n     auto &array_attr = attr.value();\n-    Value array_size = rewriter->create<LLVM::ConstantOp>(\n-        loc, size_ty, rewriter->getIntegerAttr(size_ty, array_attr.size()));\n-    Value array_ptr = rewriter->create<LLVM::AllocaOp>(\n-        loc, ptr_ty, element_ty, array_size, /*alignment=*/0);\n+    Value array_size = LLVM::ConstantOp::create(\n+        *rewriter, loc, size_ty,\n+        rewriter->getIntegerAttr(size_ty, array_attr.size()));\n+    Value array_ptr = LLVM::AllocaOp::create(*rewriter, loc, ptr_ty, element_ty,\n+                                             array_size, /*alignment=*/0);\n     for (const auto &e : llvm::enumerate(array_attr)) {\n-      Value index = rewriter->create<LLVM::ConstantOp>(\n-          loc, size_ty, rewriter->getIntegerAttr(size_ty, e.index()));\n-      Value element_ptr = rewriter->create<LLVM::GEPOp>(loc, ptr_ty, element_ty,\n-                                                        array_ptr, index);\n+      Value index = LLVM::ConstantOp::create(\n+          *rewriter, loc, size_ty,\n+          rewriter->getIntegerAttr(size_ty, e.index()));\n+      Value element_ptr = LLVM::GEPOp::create(*rewriter, loc, ptr_ty,\n+                                              element_ty, array_ptr, index);\n       Value element = create_element(e.value());\n-      rewriter->create<LLVM::StoreOp>(loc, element, element_ptr);\n+      LLVM::StoreOp::create(*rewriter, loc, element, element_ptr);\n     }\n     return std::make_pair(array_size, array_ptr);\n   }\n@@ -101,8 +103,8 @@ class ConvertToLLVMCallOpPattern : public ConvertOpToLLVMPattern<OpTy> {\n     assert(mlir::isa<IntegerType>(element_ty) && \"expect integer element type\");\n     return ConvertArrayAttrToStackAllocatedArray(\n         loc, size_ty, element_ty, attr, rewriter, [&](Attribute attr) {\n-          return rewriter->create<LLVM::ConstantOp>(\n-              loc, element_ty,\n+          return LLVM::ConstantOp::create(\n+              *rewriter, loc, element_ty,\n               rewriter->getIntegerAttr(element_ty,\n                                        mlir::cast<IntegerAttr>(attr).getInt()));\n         });\n@@ -136,8 +138,8 @@ class TFAllocOpConverter : public ConvertToLLVMCallOpPattern<TFAllocOp> {\n \n     // Convert `output_index` or set it to -1 if the attribute is missing.\n     Type llvmInt32Type = IntegerType::get(rewriter.getContext(), 32);\n-    Value output_index = rewriter.create<LLVM::ConstantOp>(\n-        loc, llvmInt32Type,\n+    Value output_index = LLVM::ConstantOp::create(\n+        rewriter, loc, llvmInt32Type,\n         rewriter.getI32IntegerAttr(tf_alloc_op.getOutputIndex().has_value()\n                                        ? tf_alloc_op.getOutputIndex().value()\n                                        : -1));\n@@ -152,12 +154,11 @@ class TFAllocOpConverter : public ConvertToLLVMCallOpPattern<TFAllocOp> {\n     FlatSymbolRefAttr tf_func_ref =\n         GetOrInsertLLVMFunction(GetFuncName(), GetFuncType(), op, &rewriter);\n     Value allocated_byte_ptr =\n-        rewriter\n-            .create<LLVM::CallOp>(\n-                loc, getVoidPtrType(), tf_func_ref,\n-                llvm::ArrayRef({adaptor.getCtx(), num_elements, element_size,\n-                                output_index, candidates_count_and_ptr.first,\n-                                candidates_count_and_ptr.second}))\n+        LLVM::CallOp::create(\n+            rewriter, loc, getVoidPtrType(), tf_func_ref,\n+            llvm::ArrayRef({adaptor.getCtx(), num_elements, element_size,\n+                            output_index, candidates_count_and_ptr.first,\n+                            candidates_count_and_ptr.second}))\n             .getResult();\n \n     MemRefDescriptor memRefDescriptor = CreateMemRefDescriptor(\n@@ -213,7 +214,7 @@ class TFAllocOpConverter : public ConvertToLLVMCallOpPattern<TFAllocOp> {\n       // Update stride\n       if (pos > 0) {\n         stride_carried =\n-            rewriter.create<LLVM::MulOp>(loc, stride_carried, size);\n+            LLVM::MulOp::create(rewriter, loc, stride_carried, size);\n       }\n     }\n     return memref_desc;\n@@ -272,12 +273,12 @@ class JITCompileFromStrOpConverter\n         ConvertIntegerArrayAttrToStackAllocatedArray(\n             loc, rewriter.getI64Type(), rewriter.getI64Type(),\n             op.getUnrollFactors(), &rewriter);\n-    Value enable_ftz = rewriter.create<LLVM::ConstantOp>(\n-        loc, rewriter.getI1Type(), op.getEnableFtzAttr());\n-    Value index_64bit = rewriter.create<LLVM::ConstantOp>(\n-        loc, rewriter.getI1Type(), op.getIndex64BitAttr());\n-    Value cpu_codegen = rewriter.create<LLVM::ConstantOp>(\n-        loc, rewriter.getI1Type(), op.getCpuCodegenAttr());\n+    Value enable_ftz = LLVM::ConstantOp::create(\n+        rewriter, loc, rewriter.getI1Type(), op.getEnableFtzAttr());\n+    Value index_64bit = LLVM::ConstantOp::create(\n+        rewriter, loc, rewriter.getI1Type(), op.getIndex64BitAttr());\n+    Value cpu_codegen = LLVM::ConstantOp::create(\n+        rewriter, loc, rewriter.getI1Type(), op.getCpuCodegenAttr());\n     FlatSymbolRefAttr tf_func_ref =\n         GetOrInsertLLVMFunction(GetFuncName(), GetFuncType(), op, &rewriter);\n     rewriter.replaceOpWithNewOp<LLVM::CallOp>(\n@@ -327,40 +328,39 @@ class JITExecuteOpConverter : public ConvertToLLVMCallOpPattern<JITExecuteOp> {\n         getTypeConverter()->convertType(op->getResultTypes().front());\n     Type ptr_ty = LLVM::LLVMPointerType::get(getContext());\n     Type i64_ty = rewriter.getI64Type();\n-    Value one = rewriter.create<LLVM::ConstantOp>(\n-        loc, i64_ty, rewriter.getI64IntegerAttr(1));\n+    Value one = LLVM::ConstantOp::create(rewriter, loc, i64_ty,\n+                                         rewriter.getI64IntegerAttr(1));\n     auto result_ptr =\n-        rewriter.create<LLVM::AllocaOp>(loc, ptr_ty, result_ty, one);\n+        LLVM::AllocaOp::create(rewriter, loc, ptr_ty, result_ty, one);\n \n     // Pass the buffer arguments as a stack-allocated array.\n     Type args_elem_ty = adaptor.getInputs().front().getType();\n-    Value num_args = rewriter.create<LLVM::ConstantOp>(\n-        loc, i64_ty,\n+    Value num_args = LLVM::ConstantOp::create(\n+        rewriter, loc, i64_ty,\n         rewriter.getI64IntegerAttr(\n             static_cast<int64_t>(adaptor.getInputs().size())));\n     Value args_ptr =\n-        rewriter.create<LLVM::AllocaOp>(loc, ptr_ty, args_elem_ty, num_args,\n-                                        /*alignment=*/0);\n+        LLVM::AllocaOp::create(rewriter, loc, ptr_ty, args_elem_ty, num_args,\n+                               /*alignment=*/0);\n     for (const auto &it : llvm::enumerate(adaptor.getInputs())) {\n-      Value index = rewriter.create<LLVM::ConstantOp>(\n-          loc, i64_ty, rewriter.getI64IntegerAttr(it.index()));\n-      Value element_ptr = rewriter.create<LLVM::GEPOp>(\n-          loc, ptr_ty, args_elem_ty, args_ptr, index);\n-      rewriter.create<LLVM::StoreOp>(loc, it.value(), element_ptr);\n+      Value index = LLVM::ConstantOp::create(\n+          rewriter, loc, i64_ty, rewriter.getI64IntegerAttr(it.index()));\n+      Value element_ptr = LLVM::GEPOp::create(rewriter, loc, ptr_ty,\n+                                              args_elem_ty, args_ptr, index);\n+      LLVM::StoreOp::create(rewriter, loc, it.value(), element_ptr);\n     }\n \n     // Materialize runtime call.\n     FlatSymbolRefAttr tf_func_ref =\n         GetOrInsertLLVMFunction(GetFuncName(), GetFuncType(), op, &rewriter);\n-    rewriter.create<LLVM::CallOp>(\n-        loc, mlir::TypeRange(), tf_func_ref,\n-        ValueRange{adaptor.getCtx(), adaptor.getCallable(), result_ptr,\n-                   num_args, args_ptr});\n+    LLVM::CallOp::create(rewriter, loc, mlir::TypeRange(), tf_func_ref,\n+                         ValueRange{adaptor.getCtx(), adaptor.getCallable(),\n+                                    result_ptr, num_args, args_ptr});\n \n     // Copy result (including the descriptor) to a stack-allocated buffer and\n     // free the old descriptor.\n     llvm::SmallVector<Value, 1> final_result = {\n-        rewriter.create<LLVM::LoadOp>(loc, result_ty, result_ptr)};\n+        LLVM::LoadOp::create(rewriter, loc, result_ty, result_ptr)};\n     if (failed(copyUnrankedDescriptors(rewriter, loc, op->getResultTypes(),\n                                        final_result,\n                                        /*toDynamic=*/false))) {\n@@ -402,8 +402,8 @@ class ReportErrorOpConverter\n     // Insert function call.\n     FlatSymbolRefAttr tf_func_ref =\n         GetOrInsertLLVMFunction(GetFuncName(), GetFuncType(), op, &rewriter);\n-    Value error_code = rewriter.create<LLVM::ConstantOp>(\n-        loc, typeConverter->convertType(rewriter.getI32Type()),\n+    Value error_code = LLVM::ConstantOp::create(\n+        rewriter, loc, typeConverter->convertType(rewriter.getI32Type()),\n         adaptor.getErrorCodeAttr());\n     rewriter.replaceOpWithNewOp<LLVM::CallOp>(\n         op, mlir::TypeRange(), tf_func_ref,\n@@ -489,7 +489,7 @@ class NullMemRefOpConverter : public ConvertOpToLLVMPattern<NullMemRefOp> {\n \n       // Prepare packed args [allocatedPtr, alignedPtr, offset, sizes, strides]\n       // to create a memref descriptor.\n-      Value null = rewriter.create<LLVM::ZeroOp>(loc, llvm_ptr_type);\n+      Value null = LLVM::ZeroOp::create(rewriter, loc, llvm_ptr_type);\n       SmallVector<Value, 12> packed_values{null, null, zero};\n       packed_values.append(sizes);\n       packed_values.append(strides);\n@@ -518,11 +518,12 @@ class NullMemRefOpConverter : public ConvertOpToLLVMPattern<NullMemRefOp> {\n     // setting its pointer to NULL.\n     Value alloca_size = UnrankedMemRefDescriptor::computeSize(\n         rewriter, loc, *getTypeConverter(), desc, addressSpace);\n-    Value underlying_desc_ptr = rewriter.create<LLVM::AllocaOp>(\n-        loc, getVoidPtrType(), IntegerType::get(getContext(), 8), alloca_size);\n+    Value underlying_desc_ptr =\n+        LLVM::AllocaOp::create(rewriter, loc, getVoidPtrType(),\n+                               IntegerType::get(getContext(), 8), alloca_size);\n \n     // Populate underlying ranked descriptor.\n-    Value null = rewriter.create<LLVM::ZeroOp>(loc, llvm_ptr_type);\n+    Value null = LLVM::ZeroOp::create(rewriter, loc, llvm_ptr_type);\n     UnrankedMemRefDescriptor::setAllocatedPtr(\n         rewriter, loc, underlying_desc_ptr, llvm_ptr_type, null);\n     UnrankedMemRefDescriptor::setAlignedPtr(rewriter, loc, *getTypeConverter(),\n@@ -551,21 +552,23 @@ class IsValidMemRefOpConverter\n \n     // Compare every size in the descriptor to 0 to check num_elements == 0.\n     int64_t rank = mlir::cast<MemRefType>(op.getArg().getType()).getRank();\n-    Value is_empty_shape = rewriter.create<LLVM::ConstantOp>(\n-        loc, rewriter.getI1Type(), rewriter.getBoolAttr(false));\n+    Value is_empty_shape = LLVM::ConstantOp::create(\n+        rewriter, loc, rewriter.getI1Type(), rewriter.getBoolAttr(false));\n     Value zero = createIndexAttrConstant(rewriter, loc, getIndexType(), 0);\n     for (int i = 0; i < rank; ++i) {\n       Value size = desc.size(rewriter, loc, i);\n-      Value is_zero_size = rewriter.create<LLVM::ICmpOp>(\n-          loc, rewriter.getI1Type(), LLVM::ICmpPredicate::eq, size, zero);\n+      Value is_zero_size =\n+          LLVM::ICmpOp::create(rewriter, loc, rewriter.getI1Type(),\n+                               LLVM::ICmpPredicate::eq, size, zero);\n       is_empty_shape =\n-          rewriter.create<LLVM::OrOp>(loc, is_empty_shape, is_zero_size);\n+          LLVM::OrOp::create(rewriter, loc, is_empty_shape, is_zero_size);\n     }\n \n     Value ptr = desc.allocatedPtr(rewriter, loc);\n-    Value null = rewriter.create<LLVM::ZeroOp>(loc, getVoidPtrType());\n-    Value is_not_nullptr = rewriter.create<LLVM::ICmpOp>(\n-        loc, rewriter.getI1Type(), LLVM::ICmpPredicate::ne, ptr, null);\n+    Value null = LLVM::ZeroOp::create(rewriter, loc, getVoidPtrType());\n+    Value is_not_nullptr =\n+        LLVM::ICmpOp::create(rewriter, loc, rewriter.getI1Type(),\n+                             LLVM::ICmpPredicate::ne, ptr, null);\n \n     // Valid memref = ptr != NULL || num_elements == 0;\n     rewriter.replaceOpWithNewOp<LLVM::OrOp>(op, is_not_nullptr, is_empty_shape);"
        },
        {
            "sha": "e51a397363e01e128ef547cf02a3073559168f47",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/tf_kernel_to_llvm_pass.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_kernel_to_llvm_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_kernel_to_llvm_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_kernel_to_llvm_pass.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -115,27 +115,28 @@ Value ConvertLaunchFuncOpToTfRuntimeCallPattern::generateParamsArray(\n   for (auto argument : arguments) argument_types.push_back(argument.getType());\n   auto struct_type = LLVM::LLVMStructType::getNewIdentified(\n       context_, StringRef(), argument_types);\n-  auto one = builder.create<LLVM::ConstantOp>(loc, llvm_int32_type_,\n-                                              builder.getI32IntegerAttr(1));\n-  auto struct_ptr = builder.create<LLVM::AllocaOp>(\n-      loc, llvm_pointer_type_, struct_type, one, /*alignment=*/0);\n-  auto array_size = builder.create<LLVM::ConstantOp>(\n-      loc, llvm_int32_type_, builder.getI32IntegerAttr(num_arguments));\n-  auto array_ptr = builder.create<LLVM::AllocaOp>(\n-      loc, llvm_pointer_type_, llvm_pointer_type_, array_size, /*alignment=*/0);\n-  auto zero = builder.create<LLVM::ConstantOp>(loc, llvm_int32_type_,\n-                                               builder.getI32IntegerAttr(0));\n+  auto one = LLVM::ConstantOp::create(builder, loc, llvm_int32_type_,\n+                                      builder.getI32IntegerAttr(1));\n+  auto struct_ptr = LLVM::AllocaOp::create(builder, loc, llvm_pointer_type_,\n+                                           struct_type, one, /*alignment=*/0);\n+  auto array_size = LLVM::ConstantOp::create(\n+      builder, loc, llvm_int32_type_, builder.getI32IntegerAttr(num_arguments));\n+  auto array_ptr =\n+      LLVM::AllocaOp::create(builder, loc, llvm_pointer_type_,\n+                             llvm_pointer_type_, array_size, /*alignment=*/0);\n+  auto zero = LLVM::ConstantOp::create(builder, loc, llvm_int32_type_,\n+                                       builder.getI32IntegerAttr(0));\n   for (auto en : llvm::enumerate(arguments)) {\n-    auto index = builder.create<LLVM::ConstantOp>(\n-        loc, llvm_int32_type_, builder.getI32IntegerAttr(en.index()));\n-    auto field_ptr = builder.create<LLVM::GEPOp>(\n-        loc, llvm_pointer_type_, struct_type, struct_ptr,\n+    auto index = LLVM::ConstantOp::create(\n+        builder, loc, llvm_int32_type_, builder.getI32IntegerAttr(en.index()));\n+    auto field_ptr = LLVM::GEPOp::create(\n+        builder, loc, llvm_pointer_type_, struct_type, struct_ptr,\n         ArrayRef<Value>{zero, index.getResult()});\n-    builder.create<LLVM::StoreOp>(loc, en.value(), field_ptr);\n+    LLVM::StoreOp::create(builder, loc, en.value(), field_ptr);\n     auto element_ptr =\n-        builder.create<LLVM::GEPOp>(loc, llvm_pointer_type_, llvm_pointer_type_,\n-                                    array_ptr, index.getResult());\n-    builder.create<LLVM::StoreOp>(loc, field_ptr, element_ptr);\n+        LLVM::GEPOp::create(builder, loc, llvm_pointer_type_,\n+                            llvm_pointer_type_, array_ptr, index.getResult());\n+    LLVM::StoreOp::create(builder, loc, field_ptr, element_ptr);\n   }\n   return array_ptr;\n }\n@@ -220,11 +221,11 @@ LogicalResult ConvertLaunchFuncOpToTfRuntimeCallPattern::matchAndRewrite(\n                          });\n     rewriter.setInsertionPointToStart(\n         launch_op->getParentOfType<ModuleOp>().getBody());\n-    function = rewriter.create<LLVM::LLVMFuncOp>(\n-        loc, kTfWrapperLibaryLaunchHelperName, function_type);\n+    function = LLVM::LLVMFuncOp::create(\n+        rewriter, loc, kTfWrapperLibaryLaunchHelperName, function_type);\n   }\n-  rewriter.create<LLVM::CallOp>(\n-      loc, TypeRange(), mlir::SymbolRefAttr::get(function),\n+  LLVM::CallOp::create(\n+      rewriter, loc, TypeRange(), mlir::SymbolRefAttr::get(function),\n \n       ArrayRef<Value>{context_arg, module_blob, kernel_name_global,\n                       adaptor.getGridSizeX(), adaptor.getGridSizeY(),"
        },
        {
            "sha": "a6ee71bfed73b8d7892f32e3cc908ff2c796e5ab",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/af8c7d0e2e006be6e69b0034d3d2d9fb1e744179/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Futils.cc?ref=af8c7d0e2e006be6e69b0034d3d2d9fb1e744179",
            "patch": "@@ -35,7 +35,7 @@ FlatSymbolRefAttr GetOrInsertLLVMFunction(StringRef func_name, Type func_type,\n   if (!tf_func) {\n     OpBuilder::InsertionGuard guard(*b);\n     b->setInsertionPointToStart(module.getBody());\n-    tf_func = b->create<LLVMFuncOp>(b->getUnknownLoc(), func_name, func_type);\n+    tf_func = LLVMFuncOp::create(*b, b->getUnknownLoc(), func_name, func_type);\n   }\n   return SymbolRefAttr::get(b->getContext(), func_name);\n }\n@@ -55,11 +55,12 @@ Value CreateOrFindGlobalStringConstant(Location loc, StringRef global_name,\n     StringRef symbol_name = global_op.getName();\n     Type symbol_type = global_op.getType();\n     Type ptr_type = LLVM::LLVMPointerType::get(b->getContext());\n-    Value global_ptr = b->create<LLVM::AddressOfOp>(loc, ptr_type, symbol_name);\n+    Value global_ptr =\n+        LLVM::AddressOfOp::create(*b, loc, ptr_type, symbol_name);\n     Value c0 =\n-        b->create<LLVM::ConstantOp>(loc, b->getI64Type(), b->getIndexAttr(0));\n-    return b->create<LLVM::GEPOp>(loc, ptr_type, symbol_type, global_ptr,\n-                                  ValueRange{c0, c0});\n+        LLVM::ConstantOp::create(*b, loc, b->getI64Type(), b->getIndexAttr(0));\n+    return LLVM::GEPOp::create(*b, loc, ptr_type, symbol_type, global_ptr,\n+                               ValueRange{c0, c0});\n   }\n   return LLVM::createGlobalString(loc, *b, global_name, content,\n                                   LLVM::Linkage::Internal);"
        }
    ],
    "stats": {
        "total": 3066,
        "additions": 1581,
        "deletions": 1485
    }
}