{
    "author": "GleasonK",
    "message": "Add CHLO decompositions directly to StableHLO when operands are static shape\n\nPiperOrigin-RevId: 809167113",
    "sha": "fe0f1e5c92f0941710ac8d75d3f429a04b2dd10d",
    "files": [
        {
            "sha": "36b276df5253263fc316130575a64c4dd9171640",
            "filename": "tensorflow/compiler/mlir/tf2xla/tests/legalize-tf-with-tf2xla-hlo-importer.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fe0f1e5c92f0941710ac8d75d3f429a04b2dd10d/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftests%2Flegalize-tf-with-tf2xla-hlo-importer.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fe0f1e5c92f0941710ac8d75d3f429a04b2dd10d/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftests%2Flegalize-tf-with-tf2xla-hlo-importer.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftests%2Flegalize-tf-with-tf2xla-hlo-importer.mlir?ref=fe0f1e5c92f0941710ac8d75d3f429a04b2dd10d",
            "patch": "@@ -655,7 +655,7 @@ module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, pr\n   // CHECK-LABEL: func @tf_mod\n   func.func @tf_mod(%arg1: tensor<2x2xf32>) -> tensor<2x2xf32> {\n     %cst = \"tf.Const\"() {value = dense<7.000000e+00> : tensor<f32>} : () -> tensor<f32>\n-    // CHECK: \"mhlo.dynamic_broadcast_in_dim\"\n+    // CHECK: mhlo.broadcast_in_dim\n     // CHECK: mhlo.remainder\n     %6 = \"tf.Mod\"(%arg1, %cst) {_global_shape = [#tf_type.shape<4x8>], device = \"\"} : (tensor<2x2xf32>, tensor<f32>) -> tensor<2x2xf32>\n     return %6 : tensor<2x2xf32>"
        },
        {
            "sha": "e3c937ed08b063bff9116493a4b38a747e517b1d",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 179,
            "deletions": 0,
            "changes": 179,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fe0f1e5c92f0941710ac8d75d3f429a04b2dd10d/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fe0f1e5c92f0941710ac8d75d3f429a04b2dd10d/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=fe0f1e5c92f0941710ac8d75d3f429a04b2dd10d",
            "patch": "@@ -4871,6 +4871,40 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c\n -\n -}  // namespace stablehlo\n -}  // namespace mlir\n+diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n+--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n++++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n+@@ -11,6 +11,30 @@\n+ }\n+ \n+ // -----\n++\n++// CHECK-LABEL: @addStaticBroadcastExpanding\n++func.func @addStaticBroadcastExpanding(%arg0: tensor<4xf32>, %arg1: tensor<f32>) -> tensor<4xf32> {\n++  // CHECK:      %[[BROADCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n++  // CHECK-NEXT: stablehlo.add %arg0, %[[BROADCAST]]\n++  // CHECK-NOT: shape\n++  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<4xf32>, tensor<f32>) -> tensor<4xf32>\n++  func.return %0 : tensor<4xf32>\n++}\n++\n++// -----\n++\n++// CHECK-LABEL: @addStaticBroadcastSameRank\n++func.func @addStaticBroadcastSameRank(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {\n++  // CHECK:      %[[ARG0_B:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<1x4xf32>) -> tensor<4x4xf32>\n++  // CHECK-NEXT: %[[ARG1_B:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<4x1xf32>) -> tensor<4x4xf32>\n++  // CHECK-NEXT: stablehlo.add %[[ARG0_B]], %[[ARG1_B]] : tensor<4x4xf32>\n++  // CHECK-NOT: shape\n++  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<1x4xf32>, tensor<4x1xf32>) -> tensor<4x4xf32>\n++  func.return %0 : tensor<4x4xf32>\n++}\n++\n++// -----\n++\n+ \n+ // CHECK-LABEL: @dynamicBroadcast\n+ // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n@@ -5329,6 +5363,151 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_o\n +  %result = stablehlo.add %one, %arg_times_zero : tensor<f32>\n +  return %result : tensor<f32>\n +}\n+diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n+--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n++++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n+@@ -24,6 +24,7 @@\n+ #include \"llvm/ADT/STLExtras.h\"\n+ #include \"llvm/ADT/Sequence.h\"\n+ #include \"llvm/ADT/SmallVector.h\"\n++#include \"llvm/Support/Debug.h\"\n+ #include \"mlir/Dialect/Arith/IR/Arith.h\"\n+ #include \"mlir/Dialect/Complex/IR/Complex.h\"\n+ #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+@@ -55,6 +56,8 @@\n+ // compilation, M_PI will not be defined.\n+ #define _USE_MATH_DEFINES\n+ \n++#define DEBUG_TYPE \"chlo-legalize-to-stablehlo\"\n++\n+ namespace mlir {\n+ namespace stablehlo {\n+ \n+@@ -198,6 +201,31 @@\n+       val);\n+ }\n+ \n++// Broadcast using numpy-style broadcasting semantics.\n++// This is only valid if the CHLO op has static shaped operands, and no\n++// explicitly specified broadcast_dimensions.\n++//\n++// Asserts that input is ranked tensor type.\n++Value numpyBroadcastIfNeeded(Value op, RankedTensorType opResultType,\n++                             PatternRewriter& rewriter) {\n++  RankedTensorType inputType = cast<RankedTensorType>(op.getType());\n++  RankedTensorType broadcastedResultType =\n++      opResultType.clone(inputType.getElementType());\n++\n++  // No broadcasting needed if input type matches broadcasted result type.\n++  if (inputType == broadcastedResultType) return op;\n++\n++  // broadcast dims are the last dims for numpy style broadcasting.\n++  int64_t inputRank = inputType.getRank();\n++  int64_t resultRank = opResultType.getRank();\n++  auto broadcastDimensions =\n++      llvm::to_vector(llvm::seq<int64_t>(resultRank - inputRank, resultRank));\n++  return stablehlo::BroadcastInDimOp::create(rewriter, op.getLoc(),\n++                                             broadcastedResultType, op,\n++                                             broadcastDimensions)\n++      .getResult();\n++}\n++\n+ //===----------------------------------------------------------------------===//\n+ // Broadcasting Patterns.\n+ //===----------------------------------------------------------------------===//\n+@@ -215,24 +243,69 @@\n+     // Only rewrite for statically determinable non-broadcasting cases.\n+     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());\n+     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());\n+-    if (!lhsType || !rhsType) return failure();\n+-\n+-    // Requires rank broadcast.\n+-    if (lhsType.getRank() != rhsType.getRank()) return failure();\n+-\n+-    // Any dynamic dimension may require broadcasting and requires more\n+-    // analysis.\n+-    if (!lhsType.hasStaticShape() || !rhsType.hasStaticShape()) {\n+-      return failure();\n+-    }\n+-\n+-    if (!llvm::equal(lhsType.getShape(), rhsType.getShape())) {\n+-      return failure();\n+-    }\n++    if (!lhsType || !rhsType || lhsType.getShape() != rhsType.getShape() ||\n++        !lhsType.hasStaticShape() || !rhsType.hasStaticShape())\n++      return rewriter.notifyMatchFailure(\n++          op,\n++          \"expected LHS and RHS to be ranked tensors with matching shapes that \"\n++          \"are all static\");\n+ \n+     rewriter.replaceOp(\n+         op, ValueRange{Adaptor::createOp(op, op.getType(),\n+                                          adaptor.getOperands(), rewriter)});\n++    return success();\n++  }\n++};\n++\n++// Converts binary ops that statically determined to use default numpy\n++// broadcasting to simple StableHLO broadcasting ops without shape dialect.\n++template <typename ChloOpTy, typename HloOpTy, typename Adaptor>\n++struct ConvertTrivialNumpyBroadcastBinaryOp final\n++    : OpConversionPattern<ChloOpTy> {\n++  using OpConversionPattern<ChloOpTy>::OpConversionPattern;\n++\n++  LogicalResult matchAndRewrite(\n++      ChloOpTy op, typename ChloOpTy::Adaptor adaptor,\n++      ConversionPatternRewriter& rewriter) const override {\n++    // Only rewrite for statically determinable non-broadcasting cases.\n++    auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());\n++    auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());\n++    if (!lhsType || !rhsType || !lhsType.hasStaticShape() ||\n++        !rhsType.hasStaticShape())\n++      return rewriter.notifyMatchFailure(\n++          op,\n++          \"expected LHS and RHS to be ranked tensor types with static \"\n++          \"shape\");\n++\n++    // Rely on CHLO type inference to figure out the proper broadcasted shape.\n++    auto resultType = dyn_cast<RankedTensorType>(op.getResult().getType());\n++    if (!resultType || !resultType.hasStaticShape())\n++      return rewriter.notifyMatchFailure(\n++          op, \"expected result to be a ranked tensor type with static shape\");\n++\n++    auto lhs = adaptor.getLhs();\n++    auto rhs = adaptor.getRhs();\n++    auto broadcastDimensions = adaptor.getBroadcastDimensions();\n++    if (broadcastDimensions &&\n++        !hlo::isLegalNumpyRankedBroadcast(lhs, rhs, *broadcastDimensions))\n++      return rewriter.notifyMatchFailure(\n++          op,\n++          \"expected implicit broadcast_dimensions or numpy-style broadcasting\");\n++\n++    LLVM_DEBUG(llvm::dbgs()\n++               << \"CHLO Decomposing \" << op->getName() << \" with broadcast \"\n++               << lhsType << \" x \" << rhsType << \" -> \" << resultType << \"\\n\");\n++\n++    // If operands are static directly create stablehlo broadcasting ops.\n++    // Use numpy-style broadcasting with using StableHLO broadcast ops,\n++    // when user didn't specify broadcast_dimensions.\n++    auto lhsBroadcast =\n++        numpyBroadcastIfNeeded(adaptor.getLhs(), resultType, rewriter);\n++    auto rhsBroadcast =\n++        numpyBroadcastIfNeeded(adaptor.getRhs(), resultType, rewriter);\n++    auto result = Adaptor::createOp(op, resultType,\n++                                    {lhsBroadcast, rhsBroadcast}, rewriter);\n++    rewriter.replaceOp(op, {result.getResult()});\n+     return success();\n+   }\n+ };\n+@@ -2416,6 +2489,8 @@\n+   // not have special attributes that need to be preserved.\n+   populateForBroadcastingBinaryOp<ConvertTrivialNonBroadcastBinaryOp>(\n+       context, patterns, 10);\n++  populateForBroadcastingBinaryOp<ConvertTrivialNumpyBroadcastBinaryOp>(\n++      context, patterns, 10);\n+   populateForBroadcastingBinaryOp<ConvertRankedDynamicBroadcastBinaryOp>(\n+       context, patterns, 5);\n+   patterns->add<ConvertConstantLikeOp, ConvertSelectOp>(context);\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp"
        }
    ],
    "stats": {
        "total": 181,
        "additions": 180,
        "deletions": 1
    }
}