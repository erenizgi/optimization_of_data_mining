{
    "author": "ermilovmaxim",
    "message": "Automated rollback of rollforward\n\nReverts 90eac795d6e2be1551eda2e1e35485ac37cca3a9\n\nPiperOrigin-RevId: 815970691",
    "sha": "79218fb33e8af939aa452ca2f22347199f3ef562",
    "files": [
        {
            "sha": "5b27e98c72c45c4a92bcedadfd4c6cdcd70218bc",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 2,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=79218fb33e8af939aa452ca2f22347199f3ef562",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/dot_dimension_merger.h\"\n #include \"xla/hlo/transforms/simplifiers/float_normalization.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_folding.h\"\n+#include \"xla/hlo/transforms/simplifiers/reshape_mover.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/call_inliner.h\"\n@@ -147,8 +148,27 @@ absl::Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization(\n   pipeline.AddPass<HloPassFix<GpuAlgebraicSimplifier>>(algsimp_options,\n                                                        gpu_version);\n \n-  pipeline.AddPass<ConvertMover>();\n-  pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  // tf2xla bridge, DepthwiseConvolutionConverter, ConvRewriter, and\n+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover\n+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"reshape_mover_after_conv_canonicalization\")] {\n+    ReshapeMoverOptions reshape_mover_options;\n+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n+\n+  // The reshapes and transposes can possibly be eliminated using\n+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.\n+  // ConvertMover wants to move some converts down the graph, but ReshapeMover\n+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed\n+  // point.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"simplify_after_conv_canonicalization\")] {\n+    pipeline.AddPass<ConvertMover>();\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n \n   // ConvRewriter, ConvPaddingLegalization and\n   // CudnnConvPadForTensorCores may add instructions which can be simplified"
        },
        {
            "sha": "669c07a65a0e16fbfafc84f45080f75a6bda02af",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=79218fb33e8af939aa452ca2f22347199f3ef562",
            "patch": "@@ -931,12 +931,23 @@ absl::Status RunOptimizationPasses(\n     pipeline.AddPass<WhileLoopSimplifier>();\n     pipeline.AddPass<SliceSinker>();\n \n+    ReshapeMoverOptions reshape_mover_options;\n+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n     pipeline.AddPass<HloConstantFolding>();\n     pipeline.AddPass<ConditionalSimplifier>();\n     pipeline.AddPass<RealImagExpander>();\n     pipeline.AddPass<TransposeFolding>(CanFoldTransposeOperandIntoDot);\n     pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/false);\n     pipeline.AddPass<HloDCE>();\n+  }();\n+\n+  // ConvertMover and ReshapeMover fight with each other: ConvertMover wants\n+  // to move some converts down the graph, but ReshapeMover wants to move them\n+  // up the graph.  As a compromise, let ReshapeMover run to a fixed point,\n+  // and then run ConvertMover + algsimp to a fixed point.\n+  [&, &pipeline =\n+          pipeline.AddPass<HloPassFix<HloPassPipeline>>(\"simplification-2\")] {\n     pipeline.AddPass<ConvertMover>();\n     pipeline.AddPass<GpuAlgebraicSimplifier>(layout_insensitive_algsimp_opts,\n                                              gpu_version);"
        },
        {
            "sha": "00d45a9f6f16428d5b57cef25deb1de0f867d94c",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=79218fb33e8af939aa452ca2f22347199f3ef562",
            "patch": "@@ -63,7 +63,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0)\\n  tmp_2 = bf16[] constant({...})\\n  tmp_3 = bf16[4,32,1024,1024]{3,2,1,0} broadcast(bf16[] tmp_2), dimensions={}\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} multiply(bf16[4,32,1024,1024]{3,2,1,0} tmp_1, bf16[4,32,1024,1024]{3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n+  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[] constant({...})\\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n   result {\n     gemm {\n       algorithm: -1"
        },
        {
            "sha": "8b1e5ee5874aada474bba137315143dfcc072134",
            "filename": "third_party/xla/xla/service/gpu/gpu_spmd_pipeline.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc?ref=79218fb33e8af939aa452ca2f22347199f3ef562",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_folding.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_splitter.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n+#include \"xla/hlo/transforms/simplifiers/reshape_mover.h\"\n #include \"xla/hlo/transforms/simplifiers/sort_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/service/call_graph.h\"\n@@ -78,6 +79,9 @@ void AddSPMDPasses(\n   spmd_simplify.AddPass<WhileLoopConstantSinking>();\n   spmd_simplify.AddPass<WhileLoopSimplifier>();\n \n+  ReshapeMoverOptions reshape_mover_options;\n+  reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+  spmd_simplify.AddPass<ReshapeMover>(reshape_mover_options);\n   // Run AlgebraicSimplifier directly before HloConstantFolding, because we\n   // need to simplify DynamicSlice(Broadcast) away. Constant folding of\n   // DynamicSlice can be quite costly, as the whole operand will be evaluated."
        },
        {
            "sha": "b30479144dfb597650d06d0e2777376e160ea4ae",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 2,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=79218fb33e8af939aa452ca2f22347199f3ef562",
            "patch": "@@ -58,6 +58,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/dot_dimension_merger.h\"\n #include \"xla/hlo/transforms/simplifiers/float_normalization.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_folding.h\"\n+#include \"xla/hlo/transforms/simplifiers/reshape_mover.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/call_inliner.h\"\n@@ -233,8 +234,27 @@ absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n     pipeline.AddPass<CudnnSimplifyPadding>();\n   }\n \n-  pipeline.AddPass<ConvertMover>();\n-  pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  // tf2xla bridge, DepthwiseConvolutionConverter, ConvRewriter, and\n+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover\n+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"reshape_mover_after_conv_canonicalization\")] {\n+    ReshapeMoverOptions reshape_mover_options;\n+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n+\n+  // The reshapes and transposes can possibly be eliminated using\n+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.\n+  // ConvertMover wants to move some converts down the graph, but ReshapeMover\n+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed\n+  // point.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"simplify_after_conv_canonicalization\")] {\n+    pipeline.AddPass<ConvertMover>();\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n \n   // ConvRewriter, ConvPaddingLegalization and\n   // CudnnConvPadForTensorCores may add instructions which can be simplified"
        },
        {
            "sha": "7b04315cc624a49198e4bba0b6976e78b84e28f9",
            "filename": "third_party/xla/xla/tests/multioutput_fusion_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79218fb33e8af939aa452ca2f22347199f3ef562/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc?ref=79218fb33e8af939aa452ca2f22347199f3ef562",
            "patch": "@@ -47,12 +47,9 @@ class MultiOutputFusionTest : public HloTestBase {\n   // Layout assignment assumes that there are no fusions in the input graph.\n   // Since the purpose of this test is to send pre-fused graphs to XLA, we have\n   // to do layout assignment ourselves.\n-  // Dot strength reduction replaces dot with a multiply and it does require\n-  // layout assignment to ensure compatible physical layouts.\n   DebugOptions GetDebugOptionsForTest() const override {\n     auto opts = HloTestBase::GetDebugOptionsForTest();\n     opts.add_xla_disable_hlo_passes(\"layout-assignment\");\n-    opts.add_xla_disable_hlo_passes(\"dot-strength-reduction\");\n     return opts;\n   }\n "
        }
    ],
    "stats": {
        "total": 68,
        "additions": 60,
        "deletions": 8
    }
}