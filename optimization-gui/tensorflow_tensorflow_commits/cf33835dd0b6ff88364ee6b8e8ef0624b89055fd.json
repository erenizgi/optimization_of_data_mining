{
    "author": "ZixuanJiang",
    "message": "Simplify `hlo_sharding_util::ReshapeSharding`.\n\nThe new implementation is based on index, which is clearer than the old one with stack.\n\nPiperOrigin-RevId: 810661145",
    "sha": "cf33835dd0b6ff88364ee6b8e8ef0624b89055fd",
    "files": [
        {
            "sha": "29f930b5382b74386457e10bf7119be4fb0a3e54",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 80,
            "deletions": 107,
            "changes": 187,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cf33835dd0b6ff88364ee6b8e8ef0624b89055fd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cf33835dd0b6ff88364ee6b8e8ef0624b89055fd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=cf33835dd0b6ff88364ee6b8e8ef0624b89055fd",
            "patch": "@@ -882,9 +882,8 @@ HloSharding TransposeSharding(const HloSharding& sharding,\n     }\n     return HloSharding::Subgroup(tile_assignment, subgroup_types,\n                                  sharding.metadata());\n-  } else {\n-    return HloSharding::PartialTile(tile_assignment, sharding.metadata());\n   }\n+  return HloSharding::PartialTile(tile_assignment, sharding.metadata());\n }\n \n std::optional<HloSharding> ReshapeSharding(const Shape& source_shape,\n@@ -911,158 +910,132 @@ std::optional<HloSharding> ReshapeSharding(const Shape& source_shape,\n   // For example, given the source_shape f32[6,4], target_shape f32[4,6] and\n   // sharding {devices=[6,1]<=[6]}, the output sharding is {devices=[2,1,3]<=[6]\n   // last_tile_dim_replicate}.\n-  DimensionVector target_tile_assignment_dimensions;\n-  DimensionVector source_dims_stack(source_shape.dimensions().rbegin(),\n-                                    source_shape.dimensions().rend());\n-  DimensionVector target_dims_stack(target_shape.dimensions().rbegin(),\n-                                    target_shape.dimensions().rend());\n-  DimensionVector sharding_tile_dims_stack(\n-      source_sharding.tile_assignment().dimensions().begin(),\n-      source_sharding.tile_assignment().dimensions().begin() +\n-          source_shape.dimensions().size());\n-  std::reverse(sharding_tile_dims_stack.begin(),\n-               sharding_tile_dims_stack.end());\n-  int64_t source_dims_index = -1;\n-  std::vector<int64_t> dims_to_replicate;\n-\n-  auto source_dims_push = [&](int64_t shape_size, int64_t partitions) {\n-    source_dims_stack.push_back(shape_size);\n-    sharding_tile_dims_stack.push_back(partitions);\n-    source_dims_index--;\n-  };\n-  auto source_dims_pop = [&]() {\n-    source_dims_stack.pop_back();\n-    sharding_tile_dims_stack.pop_back();\n-    source_dims_index++;\n-  };\n \n-  bool inplace_add_sharding_dim = false;\n-  auto append_target_sharding_dim = [&](int64_t size) {\n-    if (inplace_add_sharding_dim) {\n-      target_tile_assignment_dimensions.back() *= size;\n-    } else {\n-      target_tile_assignment_dimensions.push_back(size);\n+  int64_t source_rank = source_shape.dimensions().size();\n+  int64_t target_rank = target_shape.dimensions().size();\n+  int64_t source_index = -1;\n+  int64_t target_index = -1;\n+\n+  int64_t source_size;\n+  int64_t target_size;\n+  int64_t source_tile_dim;\n+  DimensionVector target_tile_dims(target_rank, 1);\n+  std::vector<int64_t> source_dims_to_replicate;\n+\n+  auto advance_source = [&]() {\n+    source_index++;\n+    if (source_index == source_rank) {\n+      return false;\n     }\n-    inplace_add_sharding_dim = false;\n+    source_size = source_shape.dimensions()[source_index];\n+    source_tile_dim = source_sharding.tile_assignment().dim(source_index);\n+    return true;\n+  };\n+  auto advance_target = [&]() {\n+    target_index++;\n+    if (target_index == target_rank) {\n+      return false;\n+    }\n+    target_size = target_shape.dimensions()[target_index];\n+    return true;\n   };\n \n-  while (!source_dims_stack.empty() && !target_dims_stack.empty() &&\n-         Product(sharding_tile_dims_stack) != 1) {\n+  advance_source();\n+  advance_target();\n+  while (source_index < source_rank && target_index < target_rank) {\n     int64_t source_dims_product = 1;\n-    while (!sharding_tile_dims_stack.empty() &&\n-           sharding_tile_dims_stack.back() == 1) {\n-      source_dims_product *= source_dims_stack.back();\n-      source_dims_pop();\n-    }\n-    while (!target_dims_stack.empty() && target_dims_stack.back() > 1 &&\n-           source_dims_product % target_dims_stack.back() == 0) {\n-      source_dims_product /= target_dims_stack.back();\n-      target_dims_stack.pop_back();\n-      append_target_sharding_dim(1);\n+    while (source_tile_dim == 1) {\n+      source_dims_product *= source_size;\n+      if (!advance_source()) {\n+        break;\n+      }\n     }\n-    if (source_dims_product != 1) {\n-      source_dims_push(source_dims_product, 1);\n+    while (source_dims_product != 1 && source_dims_product % target_size == 0) {\n+      source_dims_product /= target_size;\n+      if (!advance_target()) {\n+        break;\n+      }\n     }\n-\n-    if (source_dims_stack.empty() || target_dims_stack.empty()) {\n+    if (source_index == source_rank || target_index == target_rank ||\n+        source_dims_product != 1) {\n       break;\n     }\n-    int64_t s_size = source_dims_stack.back();\n-    int64_t s_partitions = sharding_tile_dims_stack.back();\n-    source_dims_pop();\n-\n-    int64_t t_size = target_dims_stack.back();\n-    target_dims_stack.pop_back();\n \n-    if (s_size == t_size) {\n-      // Same dimension size.\n-      if (inplace_add_sharding_dim && s_size % s_partitions != 0) {\n-        append_target_sharding_dim(std::gcd(s_size, s_partitions));\n+    if (source_size == target_size) {\n+      if (target_size != target_shape.dimensions()[target_index] &&\n+          source_size % source_tile_dim != 0) {\n+        target_tile_dims[target_index] *=\n+            std::gcd(source_size, source_tile_dim);\n         break;\n       }\n-      append_target_sharding_dim(s_partitions);\n-    } else if (t_size == 1) {\n-      // Trivial dimension added.\n-      append_target_sharding_dim(1);\n-      source_dims_push(s_size, s_partitions);\n-    } else if (s_size == 1) {\n-      // Trivial dimension removed.\n-      target_dims_stack.push_back(t_size);\n-      if (s_partitions > 1) {\n-        dims_to_replicate.push_back(source_dims_index);\n+      target_tile_dims[target_index] *= source_tile_dim;\n+      advance_source();\n+      advance_target();\n+    } else if (target_size == 1) {\n+      advance_target();\n+    } else if (source_size == 1) {\n+      if (source_tile_dim > 1) {\n+        source_dims_to_replicate.push_back(source_index);\n       }\n-    } else if (s_partitions == 1) {\n-      if (!source_dims_stack.empty() && sharding_tile_dims_stack.back() == 1) {\n-        source_dims_stack.back() *= s_size;\n-      } else {\n-        break;\n-      }\n-    } else if (s_size % s_partitions != 0) {\n-      // TODO(zixuanjiang): Although we can propagate thd gcd(s_size,\n-      // s_partitions), we return std::nullopt since the current partitioner\n+      advance_source();\n+    } else if (source_size % source_tile_dim != 0) {\n+      // TODO(zixuanjiang): Although we can propagate gcd(source_size,\n+      // source_tile_dim), we return std::nullopt since the current partitioner\n       // reply on that to create halo exchange. Revisit it later.\n       return std::nullopt;\n     } else {\n-      int64_t gcd = std::gcd(s_partitions, t_size);\n+      int64_t gcd = std::gcd(source_tile_dim, target_size);\n       if (gcd == 1) {\n         break;\n       }\n \n-      source_dims_push(s_size / gcd, s_partitions / gcd);\n-      target_dims_stack.push_back(t_size / gcd);\n-      append_target_sharding_dim(gcd);\n-      inplace_add_sharding_dim = true;\n+      source_size /= gcd;\n+      source_tile_dim /= gcd;\n+      target_size /= gcd;\n+      target_tile_dims[target_index] *= gcd;\n     }\n   }\n \n-  if (Product(target_tile_assignment_dimensions) == 1) {\n+  if (Product(target_tile_dims) == 1) {\n     return std::nullopt;\n   }\n-  while (target_tile_assignment_dimensions.size() <\n-         target_shape.dimensions().size()) {\n-    target_tile_assignment_dimensions.push_back(1);\n-  }\n \n   // If there is a source dimension satisfying (1) size is 1, (2) partition > 1,\n   // and (3) there is no corresponding target dimension, we replicate the source\n   // sharding along this dimension since the source sharding cannot be\n   // propagated along this dimension.\n-  const HloSharding sharding = !dims_to_replicate.empty()\n-                                   ? PartiallyReplicateTiledShardingOnDims(\n-                                         source_sharding, dims_to_replicate)\n-                                   : source_sharding;\n+  const HloSharding sharding = PartiallyReplicateTiledShardingOnDims(\n+      source_sharding, source_dims_to_replicate);\n \n   for (int64_t i = sharding.TiledDataRank();\n        i < sharding.tile_assignment().num_dimensions(); ++i) {\n-    target_tile_assignment_dimensions.push_back(\n-        i == sharding.SubgroupReplicationDim()\n-            ? 1\n-            : sharding.tile_assignment().dim(i));\n+    target_tile_dims.push_back(i == sharding.SubgroupReplicationDim()\n+                                   ? 1\n+                                   : sharding.tile_assignment().dim(i));\n   }\n \n   auto subgroup_types = sharding.subgroup_types();\n-  auto partially_replicated = std::div(\n-      sharding.TotalNumTiles(), Product(target_tile_assignment_dimensions));\n+  auto partially_replicated =\n+      std::div(sharding.TotalNumTiles(), Product(target_tile_dims));\n   CHECK_EQ(partially_replicated.rem, 0);\n   if (partially_replicated.quot > 1) {\n     if (sharding.ReplicateOnLastTileDim()) {\n-      target_tile_assignment_dimensions.back() = partially_replicated.quot;\n+      target_tile_dims.back() = partially_replicated.quot;\n       subgroup_types.push_back(OpSharding::REPLICATED);\n     } else if (absl::c_linear_search(subgroup_types, OpSharding::REPLICATED)) {\n-      target_tile_assignment_dimensions[sharding.SubgroupReplicationDim() -\n-                                        sharding.TiledDataRank() +\n-                                        target_shape.dimensions().size()] =\n+      target_tile_dims[sharding.SubgroupReplicationDim() -\n+                       sharding.TiledDataRank() +\n+                       target_shape.dimensions().size()] =\n           partially_replicated.quot;\n     } else {\n-      target_tile_assignment_dimensions.push_back(partially_replicated.quot);\n+      target_tile_dims.push_back(partially_replicated.quot);\n       subgroup_types.push_back(OpSharding::REPLICATED);\n     }\n   }\n \n-  auto new_tile_assignment =\n-      sharding.tile_assignment().Reshape(target_tile_assignment_dimensions);\n-  return HloSharding::Subgroup(new_tile_assignment, subgroup_types,\n-                               sharding.metadata());\n+  return HloSharding::Subgroup(\n+      sharding.tile_assignment().Reshape(target_tile_dims), subgroup_types,\n+      sharding.metadata());\n }\n \n HloSharding PropagateShardingThroughReshape(const Shape& source_shape,"
        }
    ],
    "stats": {
        "total": 187,
        "additions": 80,
        "deletions": 107
    }
}