{
    "author": "metaflow",
    "message": "[XLA:GPU] enable dynamic slice support\n\nreplace usages of legacy IsTritonSupportedDynamicSlice\n\nPiperOrigin-RevId: 839186994",
    "sha": "ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
    "files": [
        {
            "sha": "5277588260ac2aee1c0a0b2e333100b16f133202",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
            "patch": "@@ -1097,6 +1097,14 @@ absl::StatusOr<TensorValue> EmitPad(\n           .getResult());\n }\n \n+absl::StatusOr<TensorValue> EmitTiledDynamicSlice(\n+    mlir::ImplicitLocOpBuilder& b,\n+    const TiledHloInstruction& tiled_dynamic_slice,\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n+  // Slicing happens in `ComputeOffsetsForTile` when this value is emitted.\n+  return values[tiled_dynamic_slice.operand(0)];\n+}\n+\n absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n     mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo,\n@@ -1229,9 +1237,7 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kDynamicSlice) {\n-    // Dynamic slice is implemented as a load and does not require any further\n-    // processing.\n-    return values[tiled_hlo.operand(0)];\n+    return EmitTiledDynamicSlice(b, tiled_hlo, values);\n   }\n \n   return absl::UnimplementedError("
        },
        {
            "sha": "fdfce9945be6ed7d997eaa3e7444c5a5a35a1bc1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 6,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
            "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n \n #include <string>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -653,10 +652,8 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n     case HloOpcode::kParameter:\n       return CodegenDecision::Allow();\n     case HloOpcode::kDynamicSlice:\n-      // TODO(b/417172838): enable this once we confirm that no benchmarks were\n-      // regressed.\n-      return CodegenDecision::Forbid(\n-          \"dynamic slice is supported but not enabled yet\");\n+      return IsTritonSupportedDynamicSlice(\n+          *Cast<HloDynamicSliceInstruction>(&instr));\n     case HloOpcode::kBitcast:\n       if (ShapeUtil::ElementsIn(instr.operand(0)->shape()) !=\n           ShapeUtil::ElementsIn(instr.shape())) {\n@@ -704,7 +701,6 @@ namespace internal {\n bool IsTritonUnsupportedOpcode(HloOpcode opcode) {\n   switch (opcode) {\n     case HloOpcode::kDynamicReshape:\n-    case HloOpcode::kDynamicSlice:\n     case HloOpcode::kDynamicUpdateSlice:\n     case HloOpcode::kGather:\n     case HloOpcode::kRaggedDot:\n@@ -743,6 +739,26 @@ absl::Status EnsureTritonSupportsComputeCapability(\n   return absl::OkStatus();\n }\n \n+CodegenDecision IsTritonSupportedDynamicSlice(\n+    const HloDynamicSliceInstruction& instr) {\n+  for (const HloInstruction* index_operand : instr.index_operands()) {\n+    switch (index_operand->shape().element_type()) {\n+      case S8:\n+      case S16:\n+      case S32:\n+      case S64:\n+        break;  // supported\n+      default:\n+        return CodegenDecision::Forbid(\n+            \"Dynamic slice is only supported S8, S16, S32, or S64 offsets.\");\n+    }\n+  }\n+  if (instr.shape().element_type() == PrimitiveType::S4) {\n+    return CodegenDecision::Forbid(\"S4 is not supported.\");\n+  }\n+  return CodegenDecision::Allow();\n+}\n+\n CodegenDecision IsTritonSupportedInstruction(\n     const HloInstruction& instr, const se::GpuComputeCapability& gpu_version) {\n   CodegenDecision decision ="
        },
        {
            "sha": "532b592d26d5fb8a704d2c778c6b9fc8298dcc50",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h?ref=ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/shape.h\"\n@@ -66,6 +67,11 @@ CodegenDecision IsTritonSupportedComputation(\n // `kTritonGemmFusionKind`.\n bool IsTritonFusedComputation(const HloComputation& computation);\n \n+// TODO(b/393299275): this function is only exposed for\n+// triton_tiling_propagation.cc. If possible it should be removed.\n+CodegenDecision IsTritonSupportedDynamicSlice(\n+    const HloDynamicSliceInstruction& instr);\n+\n namespace internal {\n // TODO(b/363981282): Remove the function below once all ops are tested via\n // HLOs. This is exposed for testing purposes only and will be removed in the"
        },
        {
            "sha": "c87d8cfe7f4f8f0258502ee81573a504a8885eb0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 58,
            "deletions": 1,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
            "patch": "@@ -156,6 +156,15 @@ std::vector<xla::PrimitiveType> AllOpSupportedTypes(HloOpcode opcode) {\n   return result;\n }\n \n+std::vector<xla::PrimitiveType> AllIntegralDataTypes() {\n+  std::vector<xla::PrimitiveType> result;\n+  absl::c_copy_if(AllXlaDataTypes(), std::back_inserter(result),\n+                  [&](PrimitiveType data_type) {\n+                    return primitive_util::IsIntegralType(data_type);\n+                  });\n+  return result;\n+}\n+\n std::vector<PrecisionConfig::Algorithm> AllPrecisionAlgorithms() {\n   std::vector<PrecisionConfig::Algorithm> algorithms;\n   const tsl::protobuf::EnumDescriptor* algorithm_descriptor =\n@@ -3090,6 +3099,54 @@ INSTANTIATE_TEST_SUITE_P(SortSuite, SortTest,\n                          AllTestCombinationsForOpcodes({HloOpcode::kSort}),\n                          TritonSupportTestTypeAndOpcodeAndDeviceToString);\n \n+using DynamicSliceTest = TritonSupportTestWithTypeAndDeviceParam;\n+\n+TEST_P(DynamicSliceTest, OperandTypes) {\n+  auto [data_type, cc] = GetParam();\n+  const std::string kHloTestTemplate = R\"(\n+ENTRY triton_computation {\n+  operand = $0[256,256] parameter(0)\n+  start_1 = s32[] parameter(1)\n+  start_2 = s32[] constant(0)\n+  ROOT dynamic_slice_op = $0[32,256] dynamic-slice(operand, start_1, start_2),\n+                          dynamic_slice_sizes={32,256}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti, ParseTemplateAndGetInstruction(\n+                                                    kHloTestTemplate, data_type,\n+                                                    HloOpcode::kDynamicSlice));\n+  RunSupportTest(std::move(ti), /*output_tile_sizes=*/{2, 4}, cc);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    DynamicSliceSuite, DynamicSliceTest,\n+    ::testing::Combine(::testing::ValuesIn(AllXlaDataTypes()),\n+                       ::testing::ValuesIn(AllDevicesToTest())),\n+    TritonSupportTestTypeAndDeviceToString);\n+\n+using DynamicSliceOffsetTypesTest = TritonSupportTestWithTypeAndDeviceParam;\n+\n+TEST_P(DynamicSliceOffsetTypesTest, DynamicSlice2D) {\n+  auto [data_type, cc] = GetParam();\n+  const std::string kHloTestTemplate = R\"(\n+ENTRY triton_computation {\n+  operand = f32[256,256] parameter(0)\n+  start_1 = $0[] parameter(1)\n+  start_2 = $0[] parameter(2)\n+  ROOT dynamic_slice_op = f32[32,64] dynamic-slice(operand, start_1, start_2),\n+                          dynamic_slice_sizes={32,64}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti, ParseTemplateAndGetInstruction(\n+                                                    kHloTestTemplate, data_type,\n+                                                    HloOpcode::kDynamicSlice));\n+  RunSupportTest(std::move(ti), /*output_tile_sizes=*/{2, 4}, cc);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    DynamicSliceOffsetTypesSuite, DynamicSliceOffsetTypesTest,\n+    ::testing::Combine(::testing::ValuesIn(AllIntegralDataTypes()),\n+                       ::testing::ValuesIn(AllDevicesToTest())),\n+    TritonSupportTestTypeAndDeviceToString);\n+\n using RecvOpsTest = TritonSupportTestWithTypeAndDeviceParam;\n \n TEST_P(RecvOpsTest, RecvAndRecvDone) {\n@@ -3477,7 +3534,6 @@ constexpr std::array kUnsupportedOps = {\n     // clang-format off\n     // go/keep-sorted start\n     HloOpcode::kDynamicReshape,\n-    HloOpcode::kDynamicSlice,\n     HloOpcode::kDynamicUpdateSlice,\n     HloOpcode::kGather,\n     HloOpcode::kRaggedDot,\n@@ -3537,6 +3593,7 @@ absl::flat_hash_set<HloOpcode> AllTestedOpcodes() {\n   ret.emplace(HloOpcode::kCustomCall);\n   ret.emplace(HloOpcode::kDomain);\n   ret.emplace(HloOpcode::kDot);\n+  ret.emplace(HloOpcode::kDynamicSlice);\n   ret.emplace(HloOpcode::kFft);\n   ret.emplace(HloOpcode::kFusion);\n   ret.emplace(HloOpcode::kGetDimensionSize);"
        },
        {
            "sha": "3007d7ee5e7b3429313985d7dc5bffb2312df1f4",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 4,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc?ref=ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n+#include \"xla/layout.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_padding_requirements.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n@@ -273,6 +274,36 @@ std::optional<DimOrdersAndReqs> GetUserDimOrdersAndCombinedReqsIfProfitable(\n       std::get<DotRequirements>(combined_reqs)};\n }\n \n+// Checks if a dynamic slice can be fused.\n+bool CanFuseDynamicSlice(const HloDynamicSliceInstruction& dynamic_slice,\n+                         const se::GpuComputeCapability& gpu_version) {\n+  if (CodegenDecision decision =\n+          IsTritonSupportedInstruction(dynamic_slice, gpu_version);\n+      !decision.CanFuse()) {\n+    VLOG(5) << \"Not fusing \" << dynamic_slice.ToString()\n+            << \" to the output due to the decision: \" << decision.Explain();\n+    return false;\n+  }\n+  // TODO(b/417172838): this check replicates the legacy emitter behavior.\n+  // New emitter might support all dimensions but we should verify that.\n+  const HloInstruction* input = dynamic_slice.operand(0);\n+  Layout in_layout = input->shape().layout();\n+  int64_t majormost_dim_id =\n+      in_layout.minor_to_major(in_layout.minor_to_major().size() - 1);\n+  for (int i = 0; i < input->shape().dimensions().size(); ++i) {\n+    if (i == majormost_dim_id) {\n+      continue;\n+    }\n+    if (input->shape().dimensions(i) != dynamic_slice.slice_sizes(i)) {\n+      VLOG(5) << \"Not fusing \" << dynamic_slice.ToString()\n+              << \" to the output due to the unsupported dynamic slice on \"\n+                 \"non-major-most dimension.\";\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n class FusionPlanBuilder {\n  public:\n   // Builds and returns the FusionPlan. Clears internal state.\n@@ -414,10 +445,12 @@ FusionPlanAndRequirements BuildFusionPlanTowardOperands(\n     // replaces unsupported F8E8M0FNU with u8. We should have a more principled\n     // way check if we will be able to emit the triton code for the fusion.\n     if (original_hlo.opcode() == HloOpcode::kDynamicSlice) {\n-      // TODO(b/417172838): support dynamic slice op.\n-      fusion_builder.SetShouldFuseNode(node_id, false);\n-      LOG(INFO) << \"Not fusing dynamic slice: \" << original_hlo.ToString();\n-      continue;\n+      const HloDynamicSliceInstruction& dynamic_slice =\n+          *Cast<HloDynamicSliceInstruction>(&original_hlo);\n+      if (!CanFuseDynamicSlice(dynamic_slice, gpu_version)) {\n+        fusion_builder.SetShouldFuseNode(node_id, false);\n+        continue;\n+      }\n     }\n \n     auto opt_result = GetOperandDimOrdersAndCombinedReqsIfProfitable("
        },
        {
            "sha": "8a0e92dee892de6398dc0e6402224f6d01874b93",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
            "patch": "@@ -264,8 +264,7 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest, DISABLED_DynamicSliceIsFused) {\n+TEST_F(GemmFusionTest, DynamicSliceIsFused) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -289,8 +288,7 @@ ENTRY e {\n                                     m::Parameter(), m::Constant()))));\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest, DISABLED_DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n+TEST_F(GemmFusionTest, DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -321,8 +319,7 @@ ENTRY e {\n                             m::Parameter(), m::Parameter()))));\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest, DISABLED_DoNotFuseDynamicSliceOfNonMajorFragments) {\n+TEST_F(GemmFusionTest, DoNotFuseDynamicSliceOfNonMajorFragments) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -341,9 +338,7 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest,\n-       DISABLED_CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n+TEST_F(GemmFusionTest, CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {"
        },
        {
            "sha": "817967d2e5b39d0bc63e2247453a38296ff13904",
            "filename": "third_party/xla/xla/service/gpu/triton_tiling_propagation.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc?ref=ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194",
            "patch": "@@ -914,7 +914,7 @@ DimOrderMapOrError GetPropagatedDimOrders(const HloInstruction& hlo,\n                                                   properties);\n   } else if (hlo.opcode() == HloOpcode::kDynamicSlice &&\n              direction == TransformDirection::kOutputToInput) {\n-    if (CodegenDecision decision = legacy_triton::IsTritonSupportedDynamicSlice(\n+    if (CodegenDecision decision = IsTritonSupportedDynamicSlice(\n             *Cast<HloDynamicSliceInstruction>(&hlo));\n         !decision.CanFuse()) {\n       // CodegenDecision is actually the same type as FusionDecision."
        }
    ],
    "stats": {
        "total": 161,
        "additions": 137,
        "deletions": 24
    }
}