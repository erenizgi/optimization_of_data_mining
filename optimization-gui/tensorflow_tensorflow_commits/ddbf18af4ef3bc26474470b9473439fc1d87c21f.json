{
    "author": "ZixuanJiang",
    "message": "Prefer `all-gather` over `all-reduce(dynamic-update-slice)` in spmd partitioner.\n\nBefore this change, we can disable creating all-gather explicitly in the test files. With this change, we always enable creating all-gather.\n\nA follow up of cl/820593767.\n\nPiperOrigin-RevId: 842819486",
    "sha": "ddbf18af4ef3bc26474470b9473439fc1d87c21f",
    "files": [
        {
            "sha": "f9f8d1dbdc694c84d8706f77749de55cdef2aae1",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ddbf18af4ef3bc26474470b9473439fc1d87c21f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ddbf18af4ef3bc26474470b9473439fc1d87c21f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=ddbf18af4ef3bc26474470b9473439fc1d87c21f",
            "patch": "@@ -1397,28 +1397,17 @@ HloInstruction* PartitionedHlo::ReplicatePartial(\n     return broadcast;\n   }\n \n-  HloInstruction* result = nullptr;\n-  if (state_.collective_ops_creator.create_cross_partition_all_gather) {\n-    result = state_.partitioner->AllGatherShards(\n-        state_.b, broadcast, sharding(), state_.next_channel_id, ag_dims,\n-        state_.collective_ops_creator);\n-  }\n-\n-  if (result == nullptr) {\n-    // We do not create all-gather instructions.\n-    dus_ar_dims.insert(dus_ar_dims.end(), ag_dims.begin(), ag_dims.end());\n-    result = broadcast;\n-  } else {\n-    // We create all-gather instructions, which may contain padding. Add a slice\n-    // to remove the padding.\n-    if (!ShapeUtil::Compatible(result->shape(), ag_result_shape)) {\n-      std::vector<int64_t> start_indices(ag_result_shape.dimensions().size(),\n-                                         0);\n-      std::vector<int64_t> strides(ag_result_shape.dimensions().size(), 1);\n-      result = state_.b->AddInstruction(\n-          HloInstruction::CreateSlice(ag_result_shape, result, start_indices,\n-                                      ag_result_shape.dimensions(), strides));\n-    }\n+  HloInstruction* result = state_.partitioner->AllGatherShards(\n+      state_.b, broadcast, sharding(), state_.next_channel_id, ag_dims,\n+      state_.collective_ops_creator);\n+  // We create all-gather instructions, which may contain padding. Add a slice\n+  // to remove the padding.\n+  if (!ShapeUtil::Compatible(result->shape(), ag_result_shape)) {\n+    std::vector<int64_t> start_indices(ag_result_shape.dimensions().size(), 0);\n+    std::vector<int64_t> strides(ag_result_shape.dimensions().size(), 1);\n+    result = state_.b->AddInstruction(\n+        HloInstruction::CreateSlice(ag_result_shape, result, start_indices,\n+                                    ag_result_shape.dimensions(), strides));\n   }\n \n   if (!dus_ar_dims.empty()) {"
        },
        {
            "sha": "9e2118ef7d8e162abafae96d63368a279c42462c",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 65,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ddbf18af4ef3bc26474470b9473439fc1d87c21f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ddbf18af4ef3bc26474470b9473439fc1d87c21f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=ddbf18af4ef3bc26474470b9473439fc1d87c21f",
            "patch": "@@ -85,13 +85,10 @@ class SpmdPartitioningTest\n   absl::StatusOr<std::unique_ptr<HloModule>> PartitionComputation(\n       absl::string_view hlo_module, int64_t num_devices,\n       SpmdPartitionerOptions options = SpmdPartitionerOptions(),\n-      bool use_all_gather = true, bool enable_enzyme_opt = false) {\n+      bool enable_enzyme_opt = false) {\n     options.allow_module_signature_change = true;\n     auto collective_ops_creator =\n         GetDefaultCollectiveOpsCreator(num_devices, /*num_replicas=*/1);\n-    if (!use_all_gather) {\n-      collective_ops_creator.create_cross_partition_all_gather = nullptr;\n-    }\n \n     HloModuleConfig config = GetModuleConfigForTest();\n     config.set_use_spmd_partitioning(true);\n@@ -8100,10 +8097,10 @@ TEST_P(SpmdPartitioningTest, DynamicUpdateSliceOfConstantInRange) {\n       dynamic-update-slice(%input, %update, %c59, %c27),\n       sharding={devices=[1,2]<=[2]}\n   })\";\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, PartitionComputation(\n-                       hlo_string, /*num_devices=*/2, SpmdPartitionerOptions(),\n-                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/2,\n+                                               SpmdPartitionerOptions(),\n+                                               /*enable_enzyme_opt=*/true));\n   const auto root = module->entry_computation()->root_instruction();\n   auto sharded_input = AllOf(op::Parameter(0), op::Shape(\"s32[128,32]\"));\n   auto sharded_update = AllOf(op::Parameter(1), op::Shape(\"s32[10,5]\"));\n@@ -8154,10 +8151,10 @@ TEST_P(SpmdPartitioningTest, DynamicUpdateSliceOfConstantOutOfRange) {\n       sharding={devices=[1,2]<=[2]}\n   })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, PartitionComputation(\n-                       hlo_string, /*num_devices=*/2, SpmdPartitionerOptions(),\n-                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/2,\n+                                               SpmdPartitionerOptions(),\n+                                               /*enable_enzyme_opt=*/true));\n   const auto root = module->entry_computation()->root_instruction();\n   auto sharded_input = AllOf(op::Parameter(0), op::Shape(\"s32[128,32]\"));\n   auto sharded_update = AllOf(op::Parameter(1), op::Shape(\"s32[128,10]\"));\n@@ -8187,10 +8184,10 @@ TEST_P(SpmdPartitioningTest, DynamicUpdateSliceSingleDimensionWithEnzymeOpt) {\n         sharding={devices=[4]<=[4]}\n     })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, PartitionComputation(\n-                       hlo_string, /*num_devices=*/4, SpmdPartitionerOptions(),\n-                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/4,\n+                                               SpmdPartitionerOptions(),\n+                                               /*enable_enzyme_opt=*/true));\n   const auto root = module->entry_computation()->root_instruction();\n   auto sharded_input = AllOf(op::Parameter(0), op::Shape(\"s32[4]\"));\n   auto sharded_update = AllOf(op::Parameter(1), op::Shape(\"s32[2]\"));\n@@ -12261,29 +12258,22 @@ TEST_P(SpmdPartitioningTest,\n HloModule module\n \n ENTRY %module {\n-  %parameter.0 = s32[8,4,2,2]{3,2,1,0} parameter(0),\n-    sharding={devices=[2,2,1,1,2]<=[8] last_tile_dim_replicate}\n-  %parameter.1 = s32[2,8,4]{2,1,0} parameter(1),\n-    sharding={devices=[1,2,1,4]<=[8] last_tile_dim_replicate}\n-  ROOT %gather.20 = s32[8,4,2,2]{3,2,1,0} gather(\n-    s32[8,4,2,2]{3,2,1,0} %parameter.0,\n-    s32[2,8,4]{2,1,0} %parameter.1), offset_dims={2,3},\n-    collapsed_slice_dims={0,1}, start_index_map={0,1}, index_vector_dim=0,\n-    slice_sizes={1,1,2,2}, sharding={replicated}\n+  %operand = s32[18,14,2,2] parameter(0), sharding={devices=[2,2,1,1,2]<=[8] last_tile_dim_replicate}\n+  %indices = s32[2,8,4] parameter(1), sharding={devices=[1,2,1,4]<=[8] last_tile_dim_replicate}\n+  ROOT %gather.20 = s32[8,4,2,2] gather(%operand, %indices),\n+    offset_dims={2,3}, collapsed_slice_dims={0,1}, start_index_map={0,1},\n+    index_vector_dim=0, slice_sizes={1,1,2,2}, sharding={replicated}\n })\";\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module,\n-      PartitionComputation(hlo_string, /*num_devices=*/8,\n-                           SpmdPartitionerOptions(), /*use_all_gather=*/false));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n-  auto operand = AllOf(op::Shape(\"s32[4,2,2,2]\"), op::Parameter());\n+  auto operand = AllOf(op::Shape(\"s32[9,7,2,2]\"), op::Parameter());\n   auto indices = AllOf(op::Shape(\"s32[2,4,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[4,4,2,2]\"), op::Gather(operand, indices));\n   EXPECT_THAT(\n-      root, op::AllReduce(op::DynamicUpdateSlice(\n-                _, op::AllReduce(op::AllReduce(op::Select(_, _, gather))), _, _,\n-                _, _)));\n+      root,\n+      op::AllGather(op::AllReduce(op::AllReduce(op::Select(_, _, gather)))));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -13249,21 +13239,19 @@ ENTRY %module {\n     update_window_dims={2,3},\n     inserted_window_dims={0,1},\n     scatter_dims_to_operand_dims={0,1},\n-    index_vector_dim=0, sharding={replicated}\n+    index_vector_dim=0, sharding={devices=[2,2,2,1]<=[8]}\n })\";\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module,\n-      PartitionComputation(hlo_string, /*num_devices=*/8,\n-                           SpmdPartitionerOptions(), /*use_all_gather=*/false));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n   auto operand = AllOf(op::Shape(\"s32[8,4,1,2]\"), op::Select());\n   auto indices = AllOf(op::Shape(\"s32[2,4,2]\"), op::Parameter());\n   auto update = AllOf(op::Shape(\"s32[4,2,1,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[8,4,1,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::DynamicUpdateSlice(\n-                        _, op::AllReduce(op::AllReduce(scatter)), _, _, _, _)));\n+  EXPECT_THAT(root, op::DynamicSlice(op::AllReduce(op::AllReduce(scatter)), _,\n+                                     _, _, _));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -14935,10 +14923,10 @@ ENTRY entry {\n   ROOT c = bf16[16,224,224,384]{3,2,1,0} copy(dynamic-update-slice.128), sharding={devices=[2,2,2,1]<=[8]}\n })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, PartitionComputation(\n-                       hlo_string, /*num_devices=*/8, SpmdPartitionerOptions(),\n-                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8,\n+                                               SpmdPartitionerOptions(),\n+                                               /*enable_enzyme_opt=*/true));\n \n   XLA_VLOG_LINES(1, module->ToString());\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n@@ -16379,27 +16367,6 @@ ENTRY entry {\n   EXPECT_EQ(FindInstruction(module.get(), HloOpcode::kAllReduce), nullptr);\n }\n \n-TEST_P(SpmdPartitioningTest, UnreducedPopulation) {\n-  absl::string_view hlo_string = R\"(\n-HloModule module\n-\n-ENTRY entry {\n-  constant = s32[2,4]{1,0} constant({{1,1,1,1},{1,1,1,1}}), sharding={maximal device=0}\n-  a = s32[2,4]{1,0} parameter(0), sharding={devices=[1,2]0,1}\n-  add = s32[2,4]{1,0} add(constant, a), sharding={unreduced}\n-  ROOT copy = s32[2,4]{1,0} copy(%add), sharding={unreduced}\n-})\";\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module,\n-      PartitionComputation(hlo_string, /*num_devices=*/2,\n-                           SpmdPartitionerOptions(), /*use_all_gather=*/false));\n-  VLOG(1) << module->ToString();\n-  // Check that we use all-reduce to reshard the operands of the add in spite\n-  // that the `add` has unreduced axes.\n-  EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              op::Copy(op::Add(op::AllReduce(), op::AllReduce())));\n-}\n-\n TEST_P(SpmdPartitioningTest, UnreducedParam) {\n   absl::string_view hlo_string = R\"(\n HloModule module"
        },
        {
            "sha": "4fd83f5f10c84fc3bc754863fb92c124710c0492",
            "filename": "third_party/xla/xla/tools/hlo_control_flow_flattening_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ddbf18af4ef3bc26474470b9473439fc1d87c21f/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_control_flow_flattening_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ddbf18af4ef3bc26474470b9473439fc1d87c21f/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_control_flow_flattening_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_control_flow_flattening_test.cc?ref=ddbf18af4ef3bc26474470b9473439fc1d87c21f",
            "patch": "@@ -49,7 +49,6 @@ class HloControlFlowFlatteningTest : public HloHardwareIndependentTestBase {\n     spmd::SpmdPartitionerOptions options;\n     auto collective_ops_creator =\n         spmd::GetDefaultCollectiveOpsCreator(num_devices, /*num_replicas=*/1);\n-    collective_ops_creator.create_cross_partition_all_gather = nullptr;\n \n     HloModuleConfig config = GetModuleConfigForTest();\n     config.set_use_spmd_partitioning(true);"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 43,
        "deletions": 88
    }
}