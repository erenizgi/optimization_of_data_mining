{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 832174563",
    "sha": "069753c571ed000340ce886060f4f4b173c82c7e",
    "files": [
        {
            "sha": "e74d99fd2af2ad0a2105de279a2bf3c3b4742ee7",
            "filename": "tensorflow/core/common_runtime/pool_allocator.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpool_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpool_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpool_allocator.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -37,7 +37,7 @@ namespace tensorflow {\n \n PoolAllocator::PoolAllocator(size_t pool_size_limit, bool auto_resize,\n                              SubAllocator* allocator,\n-                             RoundUpInterface* size_rounder, string name)\n+                             RoundUpInterface* size_rounder, std::string name)\n     : name_(std::move(name)),\n       has_size_limit_(pool_size_limit > 0),\n       auto_resize_(auto_resize),"
        },
        {
            "sha": "69c1e7a75b88d9d9fe6f2108cbb20f04a668d788",
            "filename": "tensorflow/core/common_runtime/pool_allocator.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpool_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpool_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpool_allocator.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -55,10 +55,10 @@ class PoolAllocator : public Allocator {\n   // malloc/free operations.  This object takes ownership of allocator.\n   PoolAllocator(size_t pool_size_limit, bool auto_resize,\n                 SubAllocator* allocator, RoundUpInterface* size_rounder,\n-                string name);\n+                std::string name);\n   ~PoolAllocator() override;\n \n-  string Name() override { return name_; }\n+  std::string Name() override { return name_; }\n \n   void* AllocateRaw(size_t alignment, size_t num_bytes) override;\n \n@@ -121,7 +121,7 @@ class PoolAllocator : public Allocator {\n   // Delete the least recently used record.\n   void EvictOne() TF_EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n \n-  const string name_;\n+  const std::string name_;\n   const bool has_size_limit_;\n   const bool auto_resize_;\n   size_t pool_size_limit_;"
        },
        {
            "sha": "c26495dfa83117c83ba1dad9c6987c2bea3597bd",
            "filename": "tensorflow/core/common_runtime/process_function_library_runtime.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 56,
            "changes": 115,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -92,7 +92,7 @@ int64_t GetParallelSubgraphThreshold() {\n const char ProcessFunctionLibraryRuntime::kDefaultFLRDevice[] = \"null\";\n \n void ProcessFunctionLibraryRuntime::FunctionData::DistributedInit(\n-    DistributedFunctionLibraryRuntime* parent, const string& function_name,\n+    DistributedFunctionLibraryRuntime* parent, const std::string& function_name,\n     const FunctionLibraryDefinition& lib_def, AttrSlice attrs,\n     const FunctionLibraryRuntime::InstantiateOptions& options,\n     FunctionLibraryRuntime::DoneCallback done) {\n@@ -149,16 +149,17 @@ ProcessFunctionLibraryRuntime::ProcessFunctionLibraryRuntime(\n \n /* static */\n absl::Status ProcessFunctionLibraryRuntime::SendTensors(\n-    const string& source_device, const string& target_device,\n-    const string& key_prefix, int64_t src_incarnation,\n+    const std::string& source_device, const std::string& target_device,\n+    const std::string& key_prefix, int64_t src_incarnation,\n     absl::Span<const Tensor> tensors_to_send, DeviceContext* device_context,\n     const std::vector<AllocatorAttributes>& alloc_attrs,\n     RendezvousInterface* rendezvous) {\n-  std::vector<string> keys;\n+  std::vector<std::string> keys;\n   for (int i = 0; i < tensors_to_send.size(); ++i) {\n-    string name = strings::StrCat(key_prefix, i);\n-    string key = Rendezvous::CreateKey(source_device, src_incarnation,\n-                                       target_device, name, FrameAndIter(0, 0));\n+    std::string name = absl::StrCat(key_prefix, i);\n+    std::string key =\n+        Rendezvous::CreateKey(source_device, src_incarnation, target_device,\n+                              name, FrameAndIter(0, 0));\n     keys.push_back(key);\n   }\n   TF_RETURN_IF_ERROR(SendTensorsToRendezvous(\n@@ -168,17 +169,18 @@ absl::Status ProcessFunctionLibraryRuntime::SendTensors(\n \n /* static */\n void ProcessFunctionLibraryRuntime::ReceiveTensorsAsync(\n-    const string& source_device, const string& target_device,\n-    const string& key_prefix, int64_t src_incarnation, int64_t num_tensors,\n+    const std::string& source_device, const std::string& target_device,\n+    const std::string& key_prefix, int64_t src_incarnation, int64_t num_tensors,\n     DeviceContext* device_context,\n     const std::vector<AllocatorAttributes>& alloc_attrs,\n     RendezvousInterface* rendezvous, std::vector<Tensor>* received_tensors,\n     StatusCallback done) {\n-  std::vector<string> keys;\n+  std::vector<std::string> keys;\n   for (int64_t i = 0; i < num_tensors; ++i) {\n-    string name = strings::StrCat(key_prefix, i);\n-    string key = Rendezvous::CreateKey(source_device, src_incarnation,\n-                                       target_device, name, FrameAndIter(0, 0));\n+    std::string name = absl::StrCat(key_prefix, i);\n+    std::string key =\n+        Rendezvous::CreateKey(source_device, src_incarnation, target_device,\n+                              name, FrameAndIter(0, 0));\n     keys.push_back(key);\n   }\n   RecvOutputsFromRendezvousAsync(rendezvous, device_context, alloc_attrs, keys,\n@@ -207,7 +209,7 @@ absl::Status ProcessFunctionLibraryRuntime::GetRetTypes(\n }\n \n absl::Status ProcessFunctionLibraryRuntime::GetDeviceIncarnation(\n-    const string& device_name, int64_t* incarnation) const {\n+    const std::string& device_name, int64_t* incarnation) const {\n   FunctionLibraryRuntime* flr = GetFLR(device_name);\n   if (flr == nullptr) {\n     return errors::InvalidArgument(\"Device name: \", device_name, \" not found.\");\n@@ -217,14 +219,14 @@ absl::Status ProcessFunctionLibraryRuntime::GetDeviceIncarnation(\n }\n \n absl::Status ProcessFunctionLibraryRuntime::GetDeviceContext(\n-    const string& device_name, DeviceContext** device_context) const {\n+    const std::string& device_name, DeviceContext** device_context) const {\n   *device_context = nullptr;\n   FunctionLibraryRuntime* flr = GetFLR(device_name);\n   if (flr == nullptr) {\n     return errors::InvalidArgument(\"Device name: \", device_name, \" not found.\");\n   }\n   Device* device = flr->device();\n-  string device_type = device->parsed_name().type;\n+  std::string device_type = device->parsed_name().type;\n   if (device_type == \"CPU\" || device_type == \"TPU_SYSTEM\") {\n     // \"TPU_SYSTEM\" indicates that `device` is a CPU.\n     return absl::OkStatus();\n@@ -281,7 +283,7 @@ void ProcessFunctionLibraryRuntime::InitializeDeviceAndFlr() {\n }\n \n FunctionLibraryRuntime* ProcessFunctionLibraryRuntime::GetFLR(\n-    const string& device_name) const {\n+    const std::string& device_name) const {\n   Device* device = nullptr;\n   if (device_name != kDefaultFLRDevice) {\n     if (!device_mgr_->LookupDevice(device_name, &device).ok()) {\n@@ -299,14 +301,14 @@ FunctionLibraryRuntime* ProcessFunctionLibraryRuntime::GetFLR(\n }\n \n FunctionLibraryRuntime::Handle ProcessFunctionLibraryRuntime::AddHandle(\n-    const string& function_key, const string& device_name,\n+    const std::string& function_key, const std::string& device_name,\n     FunctionLibraryRuntime::LocalHandle local_handle) {\n   mutex_lock l(mu_);\n   return AddHandleLocked(function_key, device_name, local_handle);\n }\n \n FunctionLibraryRuntime::Handle ProcessFunctionLibraryRuntime::AddHandleLocked(\n-    const string& function_key, const string& device_name,\n+    const std::string& function_key, const std::string& device_name,\n     FunctionLibraryRuntime::LocalHandle local_handle) {\n   auto h = next_handle_;\n   function_data_[h] =\n@@ -318,7 +320,8 @@ FunctionLibraryRuntime::Handle ProcessFunctionLibraryRuntime::AddHandleLocked(\n \n FunctionLibraryRuntime::Handle\n ProcessFunctionLibraryRuntime::AddMultiDeviceHandle(\n-    std::unique_ptr<MultiDeviceFunctionData> data, const string& function_key) {\n+    std::unique_ptr<MultiDeviceFunctionData> data,\n+    const std::string& function_key) {\n   mutex_lock l(mu_);\n   auto h = next_handle_;\n   mdevice_data_[h] = std::move(data);\n@@ -338,14 +341,14 @@ bool ProcessFunctionLibraryRuntime::HasMultiDeviceHandle(\n }\n \n FunctionLibraryRuntime::Handle ProcessFunctionLibraryRuntime::GetHandle(\n-    const string& function_key) const {\n+    const std::string& function_key) const {\n   tf_shared_lock l(mu_);\n   return gtl::FindWithDefault(table_, function_key, kInvalidHandle);\n }\n \n FunctionLibraryRuntime::LocalHandle\n ProcessFunctionLibraryRuntime::GetHandleOnDevice(\n-    const string& device_name, FunctionLibraryRuntime::Handle handle,\n+    const std::string& device_name, FunctionLibraryRuntime::Handle handle,\n     bool include_multi_device) const {\n   tf_shared_lock l(mu_);\n \n@@ -357,7 +360,7 @@ ProcessFunctionLibraryRuntime::GetHandleOnDevice(\n     if (data.glue_.size() != 1) return kInvalidLocalHandle;\n \n     const auto& pair = *data.glue_.begin();\n-    const string& func_device_name = pair.first;\n+    const std::string& func_device_name = pair.first;\n     const ComponentFunctionData& component_data = pair.second;\n     if (func_device_name != device_name) return kInvalidLocalHandle;\n \n@@ -377,7 +380,7 @@ ProcessFunctionLibraryRuntime::GetHandleOnDevice(\n   return function_data->local_handle();\n }\n \n-string ProcessFunctionLibraryRuntime::GetDeviceName(\n+std::string ProcessFunctionLibraryRuntime::GetDeviceName(\n     FunctionLibraryRuntime::Handle handle) const {\n   tf_shared_lock l(mu_);\n   auto iter = function_data_.find(handle);\n@@ -496,11 +499,11 @@ void ProcessFunctionLibraryRuntime::PublishSubgraphs(\n }\n \n absl::Status ProcessFunctionLibraryRuntime::InstantiateMultiDevice(\n-    const string& function_name, AttrSlice attrs,\n+    const std::string& function_name, AttrSlice attrs,\n     const FunctionLibraryRuntime::InstantiateOptions& options,\n     FunctionLibraryRuntime::Handle* handle) {\n   // Check if this function has already been instantiated.\n-  const string& function_key = Canonicalize(function_name, attrs, options);\n+  const std::string& function_key = Canonicalize(function_name, attrs, options);\n \n   {\n     mutex_lock l(mu_);\n@@ -517,12 +520,12 @@ absl::Status ProcessFunctionLibraryRuntime::InstantiateMultiDevice(\n   if (VLOG_IS_ON(3)) {\n     int index = 0;\n     VLOG(3) << \"Requested input devices:\";\n-    for (const string& device : options.input_devices) {\n+    for (const std::string& device : options.input_devices) {\n       VLOG(3) << \"    [input \" << index++ << \"] \" << device;\n     }\n     index = 0;\n     VLOG(3) << \"Requested output devices:\";\n-    for (const string& device : options.output_devices) {\n+    for (const std::string& device : options.output_devices) {\n       VLOG(3) << \"    [output \" << index++ << \"] \" << device;\n     }\n   }\n@@ -552,7 +555,7 @@ absl::Status ProcessFunctionLibraryRuntime::InstantiateMultiDevice(\n   Device* cpu_device;\n   TF_RETURN_IF_ERROR(device_mgr_->LookupDevice(\"CPU:0\", &cpu_device));\n \n-  const uint64 optimization_start_time_usecs = Env::Default()->NowMicros();\n+  const uint64_t optimization_start_time_usecs = Env::Default()->NowMicros();\n   // Look up for optimized function graph in library. If found, skip\n   // `OptimizeFunctionGraph` step.\n   std::optional<absl::StatusOr<OptimizedFunctionGraph>> optimized_graph_proto =\n@@ -593,8 +596,8 @@ absl::Status ProcessFunctionLibraryRuntime::InstantiateMultiDevice(\n                                           function_name, *optimized_graph_info,\n                                           options, *dev_set, lib_def_,\n                                           composite_devices, cpu_device, env_));\n-  const uint64 optimization_end_time_usecs = Env::Default()->NowMicros();\n-  const uint64 graph_optimization_duration =\n+  const uint64_t optimization_end_time_usecs = Env::Default()->NowMicros();\n+  const uint64_t graph_optimization_duration =\n       optimization_end_time_usecs - optimization_start_time_usecs;\n   metrics::UpdateFunctionGraphOptimizationTime(graph_optimization_duration);\n   VLOG(1) << \"Finished graph optimizations for MultiDevice function \\\"\"\n@@ -617,11 +620,11 @@ absl::Status ProcessFunctionLibraryRuntime::InstantiateMultiDevice(\n   // We must preserve control returns in each of the function components,\n   // otherwise after function inlining we might prune side-effectful nodes.\n   const auto control_ret =\n-      [&node_name_to_control_ret](const Node* n) -> std::optional<string> {\n+      [&node_name_to_control_ret](const Node* n) -> std::optional<std::string> {\n     const auto it = node_name_to_control_ret.find(n->name());\n     return it != node_name_to_control_ret.end()\n                // NOLINTNEXTLINE\n-               ? absl::make_optional<string>(it->second)\n+               ? absl::make_optional<std::string>(it->second)\n                // NOLINTNEXTLINE\n                : absl::nullopt;\n   };\n@@ -659,11 +662,11 @@ absl::Status ProcessFunctionLibraryRuntime::InstantiateMultiDevice(\n \n   auto instantiate_component = [this, dev_set, &data_lib_def, &control_ret,\n                                 &options,\n-                                &data](const string& target,\n+                                &data](const std::string& target,\n                                        std::unique_ptr<Graph> subgraph,\n                                        ComponentFunctionData* comp_data,\n                                        std::function<void(absl::Status)> done) {\n-    const string& device_type =\n+    const std::string& device_type =\n         dev_set->FindDeviceByName(target)->device_type();\n \n     bool ints_on_device =\n@@ -854,7 +857,7 @@ absl::Status ProcessFunctionLibraryRuntime::GetOutputDevices(\n       continue;\n     }\n \n-    const string& target = pair.first;\n+    const std::string& target = pair.first;\n     FunctionLibraryRuntime* target_flr = GetFLR(target);\n     Device* target_device = nullptr;\n     Device* host = nullptr;\n@@ -863,7 +866,7 @@ absl::Status ProcessFunctionLibraryRuntime::GetOutputDevices(\n         data->has_remote_outputs = true;\n       }\n       target_device = device_set()->FindDeviceByName(target);\n-      string remote_host;\n+      std::string remote_host;\n       TF_RETURN_IF_ERROR(\n           DeviceNameUtils::DeviceNameToCpuDeviceName(target, &remote_host));\n       host = device_set()->FindDeviceByName(remote_host);\n@@ -917,14 +920,14 @@ absl::Status ProcessFunctionLibraryRuntime::PrepareRunMultiDevice(\n   return absl::OkStatus();\n }\n \n-std::vector<string> ProcessFunctionLibraryRuntime::GetOrderedSubgraphs(\n+std::vector<std::string> ProcessFunctionLibraryRuntime::GetOrderedSubgraphs(\n     const MultiDeviceFunctionData* data) const {\n-  std::vector<string> subgraph_keys;\n+  std::vector<std::string> subgraph_keys;\n   subgraph_keys.reserve(data->glue_.size());\n   for (const auto& pair : data->glue_) {\n     subgraph_keys.push_back(pair.first);\n   }\n-  auto send_first_ordering = [&](const string& a, const string& b) {\n+  auto send_first_ordering = [&](const std::string& a, const std::string& b) {\n     auto a_summary = data->glue_.at(a).async_attributes.summary();\n     auto b_summary = data->glue_.at(b).async_attributes.summary();\n     if (a_summary == b_summary) {\n@@ -969,9 +972,9 @@ absl::Status ProcessFunctionLibraryRuntime::RunMultiDeviceSync(\n   //\n   // We assume that the partitioning has a valid deadlock-free ordering and the\n   // safety of running synchronously has already been confirmed by this point.\n-  std::vector<string> subgraph_keys = GetOrderedSubgraphs(data);\n+  std::vector<std::string> subgraph_keys = GetOrderedSubgraphs(data);\n \n-  for (const string& target : subgraph_keys) {\n+  for (const std::string& target : subgraph_keys) {\n     const ComponentFunctionData& comp_data = data->glue_.at(target);\n     FunctionLibraryRuntime::Handle comp_handle = comp_data.handle;\n \n@@ -1003,9 +1006,9 @@ absl::Status ProcessFunctionLibraryRuntime::RunMultiDeviceSync(\n                        &comp_tensor_rets);\n       if (!run_status.ok()) {\n         VLOG(2) << \"Component function execution failed: \" << run_status;\n-        const string function_and_msg = strings::StrCat(\n-            errors::FormatFunctionForError(data->function_name_), \" \",\n-            run_status.message());\n+        const std::string function_and_msg =\n+            absl::StrCat(errors::FormatFunctionForError(data->function_name_),\n+                         \" \", run_status.message());\n         if (opts.rendezvous != nullptr) opts.rendezvous->StartAbort(run_status);\n         return errors::CreateWithUpdatedMessage(run_status, function_and_msg);\n       } else {\n@@ -1067,7 +1070,7 @@ void ProcessFunctionLibraryRuntime::RunMultiDeviceAsync(\n \n   FunctionLibraryRuntime::Options opts_copy = opts;\n   for (const auto& pair : data->glue_) {\n-    const string& target = pair.first;\n+    const std::string& target = pair.first;\n     const ComponentFunctionData& comp_data = pair.second;\n     FunctionLibraryRuntime::Handle comp_handle = pair.second.handle;\n \n@@ -1094,9 +1097,9 @@ void ProcessFunctionLibraryRuntime::RunMultiDeviceAsync(\n         VLOG(2) << \"Component function execution on target \" << target\n                 << \" from \" << data->function_name_ << \" with handle \"\n                 << comp_handle << \" failed: \" << status;\n-        const string function_and_msg = strings::StrCat(\n-            errors::FormatFunctionForError(data->function_name_), \" \",\n-            status.message());\n+        const std::string function_and_msg =\n+            absl::StrCat(errors::FormatFunctionForError(data->function_name_),\n+                         \" \", status.message());\n         refcounted_done->UpdateStatus(\n             errors::CreateWithUpdatedMessage(status, function_and_msg));\n         // Cancel the execution of other component functions.\n@@ -1147,7 +1150,7 @@ void ProcessFunctionLibraryRuntime::RunMultiDeviceAsync(\n }\n \n absl::Status ProcessFunctionLibraryRuntime::Instantiate(\n-    const string& function_name, AttrSlice attrs,\n+    const std::string& function_name, AttrSlice attrs,\n     const FunctionLibraryRuntime::InstantiateOptions& options,\n     FunctionLibraryRuntime::Handle* handle) {\n   if (options.is_multi_device_function) {\n@@ -1195,7 +1198,7 @@ absl::Status ProcessFunctionLibraryRuntime::IsCrossProcess(\n }\n \n void ProcessFunctionLibraryRuntime::InstantiateRemote(\n-    const string& function_name, AttrSlice attrs,\n+    const std::string& function_name, AttrSlice attrs,\n     const FunctionLibraryRuntime::InstantiateOptions& options,\n     FunctionLibraryRuntime::Handle* handle,\n     FunctionLibraryRuntime::DoneCallback done) {\n@@ -1207,7 +1210,7 @@ void ProcessFunctionLibraryRuntime::InstantiateRemote(\n   }\n   auto target = options.target;\n   VLOG(1) << \"ProcessFLR Instantiate: \" << function_name << \" on: \" << target;\n-  string function_key = Canonicalize(function_name, attrs, options);\n+  std::string function_key = Canonicalize(function_name, attrs, options);\n   FunctionData* f;\n   {\n     mutex_lock l(mu_);\n@@ -1257,7 +1260,7 @@ absl::Status ProcessFunctionLibraryRuntime::ReleaseMultiDeviceHandle(\n   // Release all component function handles.\n   absl::Status overall_status;\n   for (const auto& it : mdata->glue_) {\n-    const string& device = it.first;\n+    const std::string& device = it.first;\n     FunctionLibraryRuntime::Handle flr_handle = it.second.handle;\n     FunctionLibraryRuntime* flr = GetFLR(device);\n     if (flr == nullptr) {\n@@ -1291,7 +1294,7 @@ absl::Status ProcessFunctionLibraryRuntime::ReleaseHandle(\n   }\n \n   FunctionLibraryRuntime* flr = nullptr;\n-  string target_device;\n+  std::string target_device;\n   {\n     mutex_lock l(mu_);\n \n@@ -1455,7 +1458,7 @@ void ProcessFunctionLibraryRuntime::RunInternal(\n     std::vector<std::unique_ptr<CleanUpItem>>* cleanup_items,\n     FunctionLibraryRuntime::DoneCallback done) const {\n   FunctionLibraryRuntime* flr = nullptr;\n-  string target_device;\n+  std::string target_device;\n   FunctionLibraryRuntime::LocalHandle local_handle;\n   {\n     tf_shared_lock l(mu_);\n@@ -1480,7 +1483,7 @@ void ProcessFunctionLibraryRuntime::RunInternal(\n   flr = GetFLR(target_device);\n   if (flr != nullptr) {\n     auto rendezvous = opts.rendezvous;\n-    string source_device = opts.source_device;\n+    std::string source_device = opts.source_device;\n     DeviceContext* device_context;\n     absl::Status s = GetDeviceContext(source_device, &device_context);\n     if (!s.ok()) {"
        },
        {
            "sha": "d37f341ae83531b784527c05bdb561320765c824",
            "filename": "tensorflow/core/common_runtime/process_function_library_runtime.h",
            "status": "modified",
            "additions": 37,
            "deletions": 35,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -94,8 +94,8 @@ class ProcessFunctionLibraryRuntime {\n   // `tensors_to_send` and indicates how the input tensors are allocated. Method\n   // takes references on each of the `tensors_to_send`. Method doesn't block.\n   static absl::Status SendTensors(\n-      const string& source_device, const string& target_device,\n-      const string& key_prefix, int64_t src_incarnation,\n+      const std::string& source_device, const std::string& target_device,\n+      const std::string& key_prefix, int64_t src_incarnation,\n       absl::Span<const Tensor> tensors_to_send, DeviceContext* device_context,\n       const std::vector<AllocatorAttributes>& alloc_attrs,\n       RendezvousInterface* rendezvous);\n@@ -107,35 +107,36 @@ class ProcessFunctionLibraryRuntime {\n   // tensors and should either be empty or `num_tensors` in size. Method doesn't\n   // block and calls `done` when `num_tensors` are fetched.\n   static void ReceiveTensorsAsync(\n-      const string& source_device, const string& target_device,\n-      const string& key_prefix, int64_t src_incarnation, int64_t num_tensors,\n-      DeviceContext* device_context,\n+      const std::string& source_device, const std::string& target_device,\n+      const std::string& key_prefix, int64_t src_incarnation,\n+      int64_t num_tensors, DeviceContext* device_context,\n       const std::vector<AllocatorAttributes>& alloc_attrs,\n       RendezvousInterface* rendezvous, std::vector<Tensor>* received_tensors,\n       StatusCallback done);\n \n   static const char kDefaultFLRDevice[];\n   // Returns the FunctionLibraryRuntime for the corresponding device_name.\n-  FunctionLibraryRuntime* GetFLR(const string& device_name) const;\n+  FunctionLibraryRuntime* GetFLR(const std::string& device_name) const;\n \n   // Returns the return types for the function identified by handle `h`.\n   absl::Status GetRetTypes(FunctionLibraryRuntime::Handle h,\n                            DataTypeVector* ret_types);\n \n   // Returns the device incarnation for the given device_name.\n-  absl::Status GetDeviceIncarnation(const string& device_name,\n+  absl::Status GetDeviceIncarnation(const std::string& device_name,\n                                     int64_t* incarnation) const;\n \n   // For a given canonicalized key signature of the function instantiated\n   // on device `device_name` and a `local_handle`, creates a handle and returns\n   // that value. Uses core/common_runtime/framework/function.h::Canonicalize\n   // to canonicalize the function signature.\n   FunctionLibraryRuntime::Handle AddHandle(\n-      const string& function_key, const string& device_name,\n+      const std::string& function_key, const std::string& device_name,\n       FunctionLibraryRuntime::LocalHandle local_handle);\n \n   // Returns a handle if found for the given key, else returns kInvalidHandle.\n-  FunctionLibraryRuntime::Handle GetHandle(const string& function_key) const;\n+  FunctionLibraryRuntime::Handle GetHandle(\n+      const std::string& function_key) const;\n \n   // For the given handle instantiated on device `device_name` returns the local\n   // index of instantiation of that function. If the function was not\n@@ -146,7 +147,7 @@ class ProcessFunctionLibraryRuntime {\n   // with a single component that is placed on `device_name`, then this method\n   // will return the local handle for that component.\n   FunctionLibraryRuntime::LocalHandle GetHandleOnDevice(\n-      const string& device_name, FunctionLibraryRuntime::Handle handle,\n+      const std::string& device_name, FunctionLibraryRuntime::Handle handle,\n       bool include_multi_device = false) const;\n \n   // Fills `output_devices` with the devices on which the results will\n@@ -161,7 +162,7 @@ class ProcessFunctionLibraryRuntime {\n   // Allows for function_name to be instantiated on different devices\n   // as specified in attrs.\n   absl::Status Instantiate(\n-      const string& function_name, AttrSlice attrs,\n+      const std::string& function_name, AttrSlice attrs,\n       const FunctionLibraryRuntime::InstantiateOptions& options,\n       FunctionLibraryRuntime::Handle* handle);\n \n@@ -273,7 +274,7 @@ class ProcessFunctionLibraryRuntime {\n     // The handle for the instantiated component function.\n     FunctionLibraryRuntime::Handle handle;\n     // The name for the component function.\n-    string name;\n+    std::string name;\n     // arg_indices.size() is the number of arguments to the component function.\n     // The i-th argument of the component function comes from the\n     // `arg_indices[i]`-th argument of the multi-device function.\n@@ -297,8 +298,8 @@ class ProcessFunctionLibraryRuntime {\n   // The fields are filled in during instantiation. Once the object is\n   // added to mdevice_data_, all fields are constant.\n   struct MultiDeviceFunctionData {\n-    MultiDeviceFunctionData(const string& function_name,\n-                            const string& function_key, int num_outputs,\n+    MultiDeviceFunctionData(const std::string& function_name,\n+                            const std::string& function_key, int num_outputs,\n                             DataTypeVector ret_types)\n         : function_name_(function_name),\n           function_key_(function_key),\n@@ -308,9 +309,9 @@ class ProcessFunctionLibraryRuntime {\n           is_cross_process_(false),\n           has_remote_outputs(false) {}\n \n-    const string function_name_;\n-    const string function_key_;\n-    uint64 instantiation_counter_;\n+    const std::string function_name_;\n+    const std::string function_key_;\n+    uint64_t instantiation_counter_;\n     // Stored here to resize the output tensor vector when function is run.\n     const int num_outputs_;\n     DataTypeVector ret_types_;\n@@ -325,12 +326,12 @@ class ProcessFunctionLibraryRuntime {\n \n     // Maps the device name to the information about the component function\n     // be run on this device.\n-    std::unordered_map<string, ComponentFunctionData> glue_;\n+    std::unordered_map<std::string, ComponentFunctionData> glue_;\n   };\n \n   struct CleanUpItem {\n-    string device;\n-    uint64 step_id;\n+    std::string device;\n+    uint64_t step_id;\n     FunctionLibraryRuntime::Handle local_handle;\n   };\n \n@@ -343,18 +344,18 @@ class ProcessFunctionLibraryRuntime {\n \n  private:\n   FunctionLibraryRuntime::Handle AddHandleLocked(\n-      const string& function_key, const string& device_name,\n+      const std::string& function_key, const std::string& device_name,\n       FunctionLibraryRuntime::LocalHandle local_handle)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n   // For a given device_name, returns a DeviceContext for copying\n   // tensors to/from the device.\n-  absl::Status GetDeviceContext(const string& device_name,\n+  absl::Status GetDeviceContext(const std::string& device_name,\n                                 DeviceContext** device_context) const;\n \n   // Looks up the information for the given `handle` and returns the name\n   // of the device where the function is registered.\n-  string GetDeviceName(FunctionLibraryRuntime::Handle handle) const;\n+  std::string GetDeviceName(FunctionLibraryRuntime::Handle handle) const;\n \n   // Removes handle from the state owned by this object.\n   absl::Status RemoveHandle(FunctionLibraryRuntime::Handle handle);\n@@ -380,19 +381,19 @@ class ProcessFunctionLibraryRuntime {\n   absl::Status ReleaseMultiDeviceHandle(FunctionLibraryRuntime::Handle handle);\n \n   absl::Status InstantiateMultiDevice(\n-      const string& function_name, AttrSlice attrs,\n+      const std::string& function_name, AttrSlice attrs,\n       const FunctionLibraryRuntime::InstantiateOptions& options,\n       FunctionLibraryRuntime::Handle* handle);\n \n   void InstantiateRemote(\n-      const string& function_name, AttrSlice attrs,\n+      const std::string& function_name, AttrSlice attrs,\n       const FunctionLibraryRuntime::InstantiateOptions& options,\n       FunctionLibraryRuntime::Handle* handle,\n       FunctionLibraryRuntime::DoneCallback done);\n \n   FunctionLibraryRuntime::Handle AddMultiDeviceHandle(\n       const std::unique_ptr<MultiDeviceFunctionData> data,\n-      const string& function_key);\n+      const std::string& function_key);\n \n   bool HasMultiDeviceHandle(FunctionLibraryRuntime::Handle handle) const;\n \n@@ -426,7 +427,7 @@ class ProcessFunctionLibraryRuntime {\n                                        InternalArgs* comp_args);\n #endif  // IS_MOBILE_PLATFORM\n \n-  std::vector<string> GetOrderedSubgraphs(\n+  std::vector<std::string> GetOrderedSubgraphs(\n       const MultiDeviceFunctionData* data) const;\n \n   absl::Status PrepareRunMultiDevice(\n@@ -458,15 +459,15 @@ class ProcessFunctionLibraryRuntime {\n   // (to be executed on `target_device`) function.\n   class FunctionData {\n    public:\n-    FunctionData(const string& target_device,\n+    FunctionData(const std::string& target_device,\n                  FunctionLibraryRuntime::LocalHandle local_handle,\n-                 const string& function_key)\n+                 const std::string& function_key)\n         : target_device_(target_device),\n           local_handle_(local_handle),\n           function_key_(function_key) {}\n \n-    const string& target_device() { return target_device_; }\n-    const string& function_key() { return function_key_; }\n+    const std::string& target_device() { return target_device_; }\n+    const std::string& function_key() { return function_key_; }\n \n     FunctionLibraryRuntime::LocalHandle local_handle() {\n       mutex_lock l(mu_);\n@@ -476,7 +477,8 @@ class ProcessFunctionLibraryRuntime {\n     // Initializes the FunctionData object by potentially making an Initialize\n     // call to the DistributedFunctionLibraryRuntime.\n     void DistributedInit(\n-        DistributedFunctionLibraryRuntime* parent, const string& function_name,\n+        DistributedFunctionLibraryRuntime* parent,\n+        const std::string& function_name,\n         const FunctionLibraryDefinition& lib_def, AttrSlice attrs,\n         const FunctionLibraryRuntime::InstantiateOptions& options,\n         FunctionLibraryRuntime::DoneCallback done);\n@@ -489,9 +491,9 @@ class ProcessFunctionLibraryRuntime {\n    private:\n     mutex mu_;\n \n-    const string target_device_;\n+    const std::string target_device_;\n     FunctionLibraryRuntime::LocalHandle local_handle_ TF_GUARDED_BY(mu_);\n-    const string function_key_;\n+    const std::string function_key_;\n     bool is_cross_process_ TF_GUARDED_BY(mu_) = false;\n     bool init_started_ TF_GUARDED_BY(mu_) = false;\n     absl::Status init_result_ TF_GUARDED_BY(mu_);\n@@ -516,7 +518,7 @@ class ProcessFunctionLibraryRuntime {\n   std::vector<CompositeDevice*> composite_devices_ TF_GUARDED_BY(mu_);\n \n   // Holds all the function instantiations. Maps function_keys to handles.\n-  std::unordered_map<string, FunctionLibraryRuntime::Handle> table_\n+  std::unordered_map<std::string, FunctionLibraryRuntime::Handle> table_\n       TF_GUARDED_BY(mu_);\n \n   // Function data for instantiated remote functions."
        },
        {
            "sha": "970ac62b197f171d4d8677134d17e711a0e06ed3",
            "filename": "tensorflow/core/common_runtime/process_function_library_runtime_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 22,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_function_library_runtime_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -59,7 +59,7 @@ class TestClusterFLR : public DistributedFunctionLibraryRuntime {\n  public:\n   explicit TestClusterFLR(DeviceMgr* device_mgr) : device_mgr_(device_mgr) {}\n \n-  void Instantiate(const string& function_name,\n+  void Instantiate(const std::string& function_name,\n                    const FunctionLibraryDefinition& lib_def, AttrSlice attrs,\n                    const FunctionLibraryRuntime::InstantiateOptions& options,\n                    FunctionLibraryRuntime::LocalHandle* handle,\n@@ -82,7 +82,7 @@ class TestClusterFLR : public DistributedFunctionLibraryRuntime {\n            absl::Span<const FunctionArg> args, std::vector<FunctionRet>* rets,\n            FunctionLibraryRuntime::DoneCallback done) override {}\n \n-  void CleanUp(uint64 step_id, FunctionLibraryRuntime::LocalHandle handle,\n+  void CleanUp(uint64_t step_id, FunctionLibraryRuntime::LocalHandle handle,\n                FunctionLibraryRuntime::DoneCallback done) override {}\n \n   DeviceMgr* remote_device_mgr() const override { return device_mgr_; }\n@@ -169,7 +169,7 @@ class ProcessFunctionLibraryRuntimeTest : public ::testing::Test {\n   }\n \n   absl::Status Instantiate(\n-      const string& name, test::function::Attrs attrs,\n+      const std::string& name, test::function::Attrs attrs,\n       const FunctionLibraryRuntime::InstantiateOptions& instantiate_opts,\n       FunctionLibraryRuntime::Handle* handle) {\n     return proc_flr_->Instantiate(name, attrs, instantiate_opts, handle);\n@@ -214,7 +214,7 @@ class ProcessFunctionLibraryRuntimeTest : public ::testing::Test {\n \n   template <typename T, typename K>\n   absl::Status RunWithRuntime(\n-      const string& name, FunctionLibraryRuntime::Options opts,\n+      const std::string& name, FunctionLibraryRuntime::Options opts,\n       test::function::Attrs attrs,\n       const FunctionLibraryRuntime::InstantiateOptions& instantiate_opts,\n       const T& args, std::vector<K*> rets,\n@@ -270,7 +270,7 @@ class ProcessFunctionLibraryRuntimeTest : public ::testing::Test {\n   }\n \n   absl::Status Run(\n-      const string& name, FunctionLibraryRuntime::Options opts,\n+      const std::string& name, FunctionLibraryRuntime::Options opts,\n       test::function::Attrs attrs,\n       const FunctionLibraryRuntime::InstantiateOptions& instantiate_opts,\n       const std::vector<Tensor>& args, std::vector<Tensor*> rets,\n@@ -280,7 +280,7 @@ class ProcessFunctionLibraryRuntimeTest : public ::testing::Test {\n   }\n \n   absl::Status RunWithPackedArgs(\n-      const string& name, FunctionLibraryRuntime::Options opts,\n+      const std::string& name, FunctionLibraryRuntime::Options opts,\n       test::function::Attrs attrs,\n       const FunctionLibraryRuntime::InstantiateOptions& instantiate_opts,\n       const FunctionArgsInterface& args, std::vector<FunctionRet*> rets,\n@@ -503,7 +503,7 @@ TEST_F(ProcessFunctionLibraryRuntimeTest, MultipleCallsSameDeviceXTimes) {\n TEST_F(ProcessFunctionLibraryRuntimeTest,\n        SameDeviceXTimesFourInt32MultiDevice) {\n   Init({test::function::XTimesTwoInt32(), test::function::XTimesFourInt32()});\n-  auto x = test::AsTensor<int32>({1, 2, 3, 4});\n+  auto x = test::AsTensor<int32_t>({1, 2, 3, 4});\n   FunctionLibraryRuntime::Options opts;\n   opts.source_device = \"/job:a/replica:0/task:0/cpu:0\";\n   opts.remote_execution = true;\n@@ -515,13 +515,13 @@ TEST_F(ProcessFunctionLibraryRuntimeTest,\n   Tensor y;\n   TF_CHECK_OK(Run(\"XTimesFourInt32\", opts, {{\"T\", DT_INT32}}, instantiate_opts,\n                   {x}, {&y}));\n-  test::ExpectTensorEqual<int32>(y, test::AsTensor<int32>({4, 8, 12, 16}));\n+  test::ExpectTensorEqual<int32_t>(y, test::AsTensor<int32_t>({4, 8, 12, 16}));\n }\n \n TEST_F(ProcessFunctionLibraryRuntimeTest,\n        MultipleCallsSameDeviceXTimesMultiDevice) {\n   Init({test::function::XTimesTwoInt32(), test::function::XTimesFourInt32()});\n-  auto x = test::AsTensor<int32>({1, 2, 3, 4});\n+  auto x = test::AsTensor<int32_t>({1, 2, 3, 4});\n   FunctionLibraryRuntime::Options opts;\n   opts.source_device = \"/job:a/replica:0/task:0/cpu:0\";\n   opts.remote_execution = true;\n@@ -533,10 +533,10 @@ TEST_F(ProcessFunctionLibraryRuntimeTest,\n   Tensor y;\n   TF_CHECK_OK(Run(\"XTimesTwoInt32\", opts, {{\"T\", DT_INT32}}, instantiate_opts,\n                   {x}, {&y}));\n-  test::ExpectTensorEqual<int32>(y, test::AsTensor<int32>({2, 4, 6, 8}));\n+  test::ExpectTensorEqual<int32_t>(y, test::AsTensor<int32_t>({2, 4, 6, 8}));\n   TF_CHECK_OK(Run(\"XTimesFourInt32\", opts, {{\"T\", DT_INT32}}, instantiate_opts,\n                   {x}, {&y}));\n-  test::ExpectTensorEqual<int32>(y, test::AsTensor<int32>({4, 8, 12, 16}));\n+  test::ExpectTensorEqual<int32_t>(y, test::AsTensor<int32_t>({4, 8, 12, 16}));\n }\n \n TEST_F(ProcessFunctionLibraryRuntimeTest, MultipleCallsSameDeviceFindDevice) {\n@@ -668,7 +668,7 @@ bool IsCUDATensor(const Tensor& t) {\n void TestTwoDeviceMult(\n     ProcessFunctionLibraryRuntimeTest* fixture,\n     const FunctionLibraryRuntime::InstantiateOptions& inst_opts,\n-    const string& error = \"\") {\n+    const std::string& error = \"\") {\n   fixture->Init({test::function::TwoDeviceMult()});\n   FunctionLibraryRuntime::Options opts;\n   auto x = test::AsTensor<float>({1, 2, 3});\n@@ -764,18 +764,18 @@ void TestTwoDeviceInputOutput(\n   test::ExpectTensorEqual<float>(y2, test::AsTensor<float>({30, 60}));\n }\n \n-std::vector<string> CompleteDevices(const std::vector<string>& v) {\n-  std::vector<string> result;\n+std::vector<std::string> CompleteDevices(const std::vector<std::string>& v) {\n+  std::vector<std::string> result;\n   result.reserve(v.size());\n-  for (const string& s : v) {\n-    result.push_back(strings::StrCat(\"/job:a/replica:0/task:0/device:\", s));\n+  for (const std::string& s : v) {\n+    result.push_back(absl::StrCat(\"/job:a/replica:0/task:0/device:\", s));\n   }\n   return result;\n }\n \n FunctionLibraryRuntime::InstantiateOptions MakeOptions(\n-    const string& target, const std::vector<string>& input_devices,\n-    const std::vector<string>& output_devices) {\n+    const std::string& target, const std::vector<std::string>& input_devices,\n+    const std::vector<std::string>& output_devices) {\n   FunctionLibraryRuntime::InstantiateOptions inst_opts;\n   inst_opts.target = target;\n   inst_opts.input_devices = CompleteDevices(input_devices);\n@@ -924,8 +924,9 @@ TEST_F(ProcessFunctionLibraryRuntimeTest, MultiDevice_EmptyBodySwap) {\n   test::ExpectTensorEqual<float>(y2, test::AsTensor<float>({1, 2}));\n }\n \n-Tensor GetResourceHandle(const string& var_name, const string& container,\n-                         const string& device_name) {\n+Tensor GetResourceHandle(const std::string& var_name,\n+                         const std::string& container,\n+                         const std::string& device_name) {\n   ResourceHandle handle;\n   handle.set_device(device_name);\n   handle.set_container(container);\n@@ -1189,8 +1190,9 @@ TEST_F(ProcessFunctionLibraryRuntimeTest, MultiDevice_StateHandle) {\n       // Attrs\n       {},\n       // Nodes\n-      {FunctionDefHelper::Const<int32>(\"shape\", absl::Span<const int32>({1})),\n-       FunctionDefHelper::Const<int32>(\"minval\", 0),\n+      {FunctionDefHelper::Const<int32_t>(\"shape\",\n+                                         absl::Span<const int32_t>({1})),\n+       FunctionDefHelper::Const<int32_t>(\"minval\", 0),\n        {{\"maxval\"}, \"ReadVariableOp\", {\"x\"}, {{\"dtype\", T}}, {}},\n        // A stateful node.\n        {{\"y\"},"
        },
        {
            "sha": "c79b42faffe83c40ae0544d727c710630aba752b",
            "filename": "tensorflow/core/common_runtime/process_state.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_state.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -46,7 +46,7 @@ namespace tensorflow {\n ProcessState::ProcessState()\n     : numa_enabled_(false), cpu_allocators_cached_(0) {}\n \n-string ProcessState::MemDesc::DebugString() {\n+std::string ProcessState::MemDesc::DebugString() {\n   return strings::StrCat((loc == CPU ? \"CPU \" : \"GPU \"), dev_index,\n                          \", dma: \", gpu_registered, \", nic: \", nic_registered);\n }"
        },
        {
            "sha": "eb0b7f53a8c7a496800cd0948051719d4b7a6a74",
            "filename": "tensorflow/core/common_runtime/process_state.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_state.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_state.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_state.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -51,7 +51,7 @@ class ProcessState : public ProcessStateInterface {\n           dev_index(0),\n           gpu_registered(false),\n           nic_registered(false) {}\n-    string DebugString();\n+    std::string DebugString();\n   };\n \n   // If NUMA Allocators are desired, call this before calling any\n@@ -122,7 +122,7 @@ class RecordingAllocator : public Allocator {\n                      ProcessState::MemDesc md, mutex* mu)\n       : mm_(mm), a_(a), md_(md), mu_(mu) {}\n \n-  string Name() override { return a_->Name(); }\n+  std::string Name() override { return a_->Name(); }\n   void* AllocateRaw(size_t alignment, size_t num_bytes) override {\n     void* p = a_->AllocateRaw(alignment, num_bytes);\n     mutex_lock l(*mu_);"
        },
        {
            "sha": "233dcde498a6bc9c3e413c4fcf949e19c63cd72d",
            "filename": "tensorflow/core/common_runtime/process_util.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_util.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -35,12 +35,12 @@ namespace tensorflow {\n namespace {\n \n // Use environment setting if specified (init once)\n-int32 GetEnvNumInterOpThreads() {\n+int32_t GetEnvNumInterOpThreads() {\n   static int32_t env_num_threads = NumInterOpThreadsFromEnvironment();\n   return env_num_threads;\n }\n \n-int32 DefaultNumInterOpThreads() {\n+int32_t DefaultNumInterOpThreads() {\n #ifndef __ANDROID__\n   int32_t env_num_threads = GetEnvNumInterOpThreads();\n   if (env_num_threads > 0) {\n@@ -90,13 +90,13 @@ thread::ThreadPool* ComputePool(const SessionOptions& options) {\n   return compute_pool;\n }\n \n-int32 NumInterOpThreadsFromEnvironment() {\n+int32_t NumInterOpThreadsFromEnvironment() {\n   int32_t num;\n   const char* val = std::getenv(\"TF_NUM_INTEROP_THREADS\");\n   return (val && absl::SimpleAtoi(val, &num)) ? num : 0;\n }\n \n-int32 NumIntraOpThreadsFromEnvironment() {\n+int32_t NumIntraOpThreadsFromEnvironment() {\n   int32_t num;\n   const char* val = std::getenv(\"TF_NUM_INTRAOP_THREADS\");\n   return (val && absl::SimpleAtoi(val, &num)) ? num : 0;\n@@ -122,7 +122,7 @@ int32 DefaultNumIntraOpThreads() {\n   return port::MaxParallelism();\n }\n #endif  // defined(ENABLE_ONEDNN_OPENMP) && defined(ENABLE_MKL)\n-int32 NumInterOpThreadsFromSessionOptions(const SessionOptions& options) {\n+int32_t NumInterOpThreadsFromSessionOptions(const SessionOptions& options) {\n   const int32_t inter_op = options.config.inter_op_parallelism_threads();\n   if (inter_op > 0) return inter_op;\n   const int32_t env_inter_op = GetEnvNumInterOpThreads();\n@@ -169,7 +169,7 @@ void SchedClosure(absl::AnyInvocable<void()> closure) {\n   if (!tsl::tracing::EventCollector::IsEnabled()) {\n     return Env::Default()->SchedClosure(std::move(closure));\n   }\n-  uint64 id = tsl::tracing::GetUniqueArg();\n+  uint64_t id = tsl::tracing::GetUniqueArg();\n   tsl::tracing::RecordEvent(tsl::tracing::EventCategory::kScheduleClosure, id);\n \n   Env::Default()->SchedClosure([id, closure = std::move(closure)]() mutable {"
        },
        {
            "sha": "682556d19fbfad38111e700b4a9545ee5a2b9a64",
            "filename": "tensorflow/core/common_runtime/process_util.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprocess_util.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -32,18 +32,18 @@ namespace tensorflow {\n thread::ThreadPool* ComputePool(const SessionOptions& options);\n \n // Returns the TF_NUM_INTEROP_THREADS environment value, or 0 if not specified.\n-int32 NumInterOpThreadsFromEnvironment();\n+int32_t NumInterOpThreadsFromEnvironment();\n \n // Returns the TF_NUM_INTRAOP_THREADS environment value, or 0 if not specified.\n-int32 NumIntraOpThreadsFromEnvironment();\n+int32_t NumIntraOpThreadsFromEnvironment();\n \n // Returns the number of inter op threads specified in `options` or a default.\n // If no value or a negative value is specified in the provided options, then\n // the function returns the value defined in the TF_NUM_INTEROP_THREADS\n // environment variable. If neither a value is specified in the options or in\n // the environment, this function will return a reasonable default value based\n // on the number of schedulable CPUs, and any MKL and OpenMP configurations.\n-int32 NumInterOpThreadsFromSessionOptions(const SessionOptions& options);\n+int32_t NumInterOpThreadsFromSessionOptions(const SessionOptions& options);\n \n // Creates a thread pool with number of inter op threads.\n // The number is set if `num_threads` > 0, otherwise it will be configured by"
        },
        {
            "sha": "28ae706c3f08b760fef4a1a545a1cbf034ce025f",
            "filename": "tensorflow/core/common_runtime/profile_handler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprofile_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fprofile_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fprofile_handler.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -40,9 +40,9 @@ class ProfileHandler {\n   // - label: Extra content for timeline click text.\n   // - op_type: String name of the Op.\n   // - details: Main content for timeline click text.\n-  virtual void RecordOneOp(const string& device, const NodeExecStats& stats,\n-                           bool is_copy, absl::string_view label,\n-                           absl::string_view op_type,\n+  virtual void RecordOneOp(const std::string& device,\n+                           const NodeExecStats& stats, bool is_copy,\n+                           absl::string_view label, absl::string_view op_type,\n                            absl::string_view details) = 0;\n \n   // Records that the current step finished."
        },
        {
            "sha": "6d65024cc4f50a056a1e24e972c9183a42a324d9",
            "filename": "tensorflow/core/common_runtime/propagator_state.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpropagator_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpropagator_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpropagator_state.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -159,7 +159,7 @@ void PropagatorState::PropagateOutputs(const TaggedNode& tagged_node,\n         if (need_create_iter) {\n           tsl::profiler::TraceMe activit1y(\n               [&]() {\n-                return strings::StrCat(\n+                return absl::StrCat(\n                     \"PropagateOutputs::NextIteration::CreateIterationState\");\n               },\n               tsl::profiler::GetTFTraceMeLevel(/*is_expensive=*/false));\n@@ -259,7 +259,7 @@ void PropagatorState::FindOrCreateChildFrame(FrameState* frame,\n   const ImmutableExecutorState::FrameInfo& frame_info =\n       immutable_state_.get_enter_frame_info(node_item);\n \n-  const uint64 child_id = Hash64Combine(\n+  const uint64_t child_id = Hash64Combine(\n       frame->frame_id,\n       Hash64Combine(iter_state->iter_num, Hash64(frame_info.name)));\n \n@@ -275,7 +275,7 @@ void PropagatorState::FindOrCreateChildFrame(FrameState* frame,\n   // Need to create a new frame instance.\n   // Note that this new frame instance is created without any locks.\n   if (vlog_) {\n-    const string child_name = strings::StrCat(\n+    const std::string child_name = strings::StrCat(\n         frame->frame_name, \";\", iter_state->iter_num, \";\", frame_info.name);\n     VLOG(2) << \"Create frame: \" << child_name << \" id: \" << child_id;\n   }"
        },
        {
            "sha": "238cb0552b2c67b37fbe9f59c96dfbed60c88ac7",
            "filename": "tensorflow/core/common_runtime/propagator_state.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpropagator_state.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fpropagator_state.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fpropagator_state.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -255,11 +255,11 @@ class PropagatorState {\n     // The name of this frame, which is the concatenation of its parent\n     // frame name, the iteration of the parent frame when this frame was\n     // created, and the value of the attr 'frame_name'.\n-    string frame_name;\n+    std::string frame_name;\n \n     // The unique id for this frame. Generated by fingerprinting\n     // frame_name.\n-    uint64 frame_id;\n+    uint64_t frame_id;\n \n     // The iteration state of its parent frame when this frame is created.\n     // nullptr if there is no parent frame. The frame_name/parent_iter pair\n@@ -543,7 +543,7 @@ class PropagatorState {\n   // child frame is a hash composed of the ID of the parent frame, the iteration\n   // number at which the parent frame is creating the new frame, and the\n   // name of the new frame from nodedef.\n-  absl::flat_hash_map<uint64, FrameState*> outstanding_frames_\n+  absl::flat_hash_map<uint64_t, FrameState*> outstanding_frames_\n       TF_GUARDED_BY(mu_);\n \n   PropagatorState(const PropagatorState&) = delete;\n@@ -579,12 +579,12 @@ class OrderedPropagatorState : public PropagatorState {\n \n    private:\n     static bool compare(TaggedNode const& lhs, TaggedNode const& rhs) {\n-      std::tuple<int, uint64, int64_t> lhs_prio{lhs.node_item->node_id,\n-                                                lhs.input_frame->frame_id,\n-                                                lhs.input_iter->iter_num};\n-      std::tuple<int, uint64, int64_t> rhs_prio{rhs.node_item->node_id,\n-                                                rhs.input_frame->frame_id,\n-                                                rhs.input_iter->iter_num};\n+      std::tuple<int, uint64_t, int64_t> lhs_prio{lhs.node_item->node_id,\n+                                                  lhs.input_frame->frame_id,\n+                                                  lhs.input_iter->iter_num};\n+      std::tuple<int, uint64_t, int64_t> rhs_prio{rhs.node_item->node_id,\n+                                                  rhs.input_frame->frame_id,\n+                                                  rhs.input_iter->iter_num};\n       return lhs_prio < rhs_prio;\n     }\n "
        },
        {
            "sha": "3459153ed7dace985cd473e5a562986c4160c12b",
            "filename": "tensorflow/core/common_runtime/quantize_training.cc",
            "status": "modified",
            "additions": 74,
            "deletions": 78,
            "changes": 152,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -35,18 +35,18 @@ namespace tensorflow {\n namespace {\n \n // TODO(suharshs): If desired, make these values configurable.\n-const uint32 kAllowedInputs = 2;\n+const uint32_t kAllowedInputs = 2;\n const float kEMADecay = 0.999;\n \n // Node types to rewrite. Insert quantize_and_dequantize op for their inputs.\n const auto* nodes_to_rewrite =\n-    new std::unordered_set<string, StringPieceHasher>{\"MatMul\", \"Conv2D\"};\n+    new std::unordered_set<std::string, StringPieceHasher>{\"MatMul\", \"Conv2D\"};\n \n // Contains necessary parameters to convert an edge.\n struct EdgeToConvert {\n   // edge is not owned here.\n   const Edge* edge;\n-  int32 num_bits;\n+  int32_t num_bits;\n   bool signed_input;\n   bool range_given;\n   float input_min;\n@@ -67,7 +67,7 @@ struct EdgeToConvert {\n // TODO(jmchen): Make this check more robust as it is not guaranteed that the\n // forward node will not be named with a leading \"gradients\".\n inline bool IsGradientNode(const Graph* graph, const Node* node) {\n-  static const string tag = \"gradients\";\n+  static const std::string tag = \"gradients\";\n   return (node->name().compare(0, tag.size(), tag) == 0);\n }\n \n@@ -76,7 +76,7 @@ inline bool IsGradientNode(const Graph* graph, const Node* node) {\n // Returns true if the root tensor op type is known, false otherwise.\n bool FindType(const Graph* graph, const Node* node, bool* signed_input,\n               bool* range_given, float* input_min, float* input_max) {\n-  const string& src_op = node->type_string();\n+  const std::string& src_op = node->type_string();\n   if (src_op == \"Const\" || src_op == \"Variable\" || src_op == \"VariableV2\") {\n     *signed_input = true;\n     *range_given = false;\n@@ -154,7 +154,7 @@ absl::Status FindSaveOp(const Graph* graph, Node** save_op,\n Node* FindRestoreAllOp(const Graph* graph, absl::string_view save_prefix) {\n   for (Node* node : graph->op_nodes()) {\n     // The restore_all op should have the same prefix of the save_op.\n-    if (node->name() == strings::StrCat(save_prefix, \"/restore_all\")) {\n+    if (node->name() == absl::StrCat(save_prefix, \"/restore_all\")) {\n       return node;\n     }\n   }\n@@ -254,21 +254,21 @@ absl::Status AddRestoreVariableSubgraphs(\n   if (restore_all == nullptr) {\n     return errors::InvalidArgument(\"graph has SaveOp, but no restore_all NoOp\");\n   }\n-  const string restore_op_name = strings::StrCat(name_prefix, \"/RestoreV2\");\n-  const string assign_op_name = strings::StrCat(name_prefix, \"/Assign\");\n+  const std::string restore_op_name = absl::StrCat(name_prefix, \"/RestoreV2\");\n+  const std::string assign_op_name = absl::StrCat(name_prefix, \"/Assign\");\n   for (Node* var : variables) {\n     // Add an extra prefix after calling graph->NewName because the \"unique\"\n     // name may conflict with names generated for Send nodes.\n     // TODO(b/77547936): fix this more generally and get rid of the extra prefix\n     // here.\n-    string new_restore_op_name =\n-        strings::StrCat(graph->NewName(restore_op_name), \"_qt\");\n-    string new_assign_op_name =\n-        strings::StrCat(graph->NewName(assign_op_name), \"_qt\");\n-    string tensor_names_op_name =\n-        strings::StrCat(new_restore_op_name, \"/tensor_names\");\n-    string shape_and_slices_op_name =\n-        strings::StrCat(new_restore_op_name, \"/shape_and_slices\");\n+    std::string new_restore_op_name =\n+        absl::StrCat(graph->NewName(restore_op_name), \"_qt\");\n+    std::string new_assign_op_name =\n+        absl::StrCat(graph->NewName(assign_op_name), \"_qt\");\n+    std::string tensor_names_op_name =\n+        absl::StrCat(new_restore_op_name, \"/tensor_names\");\n+    std::string shape_and_slices_op_name =\n+        absl::StrCat(new_restore_op_name, \"/shape_and_slices\");\n \n     // Construct the tensor_names input with the variable name.\n     Node* tensor_names;\n@@ -329,32 +329,32 @@ absl::Status AddSaveAndRestore(Graph* graph,\n \n // Sets output to the Node that computes reduction axes corresponding to all\n // dimensions of input and return.\n-absl::Status MakeReductionAxes(Graph* graph, string name_prefix, Node* input,\n-                               Node** output) {\n-  name_prefix = strings::StrCat(name_prefix, \"/ReductionAxes\");\n+absl::Status MakeReductionAxes(Graph* graph, std::string name_prefix,\n+                               Node* input, Node** output) {\n+  name_prefix = absl::StrCat(name_prefix, \"/ReductionAxes\");\n   Node* start;\n   Tensor zero_tensor(DT_INT32, TensorShape());\n-  zero_tensor.flat<int32>()(0) = 0;\n+  zero_tensor.flat<int32_t>()(0) = 0;\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/RangeStart\"), \"Const\")\n+      NodeBuilder(absl::StrCat(name_prefix, \"/RangeStart\"), \"Const\")\n           .Attr(\"dtype\", DT_INT32)\n           .Attr(\"value\", zero_tensor)\n           .Finalize(graph, &start));\n   Node* delta;\n   Tensor one_tensor(DT_INT32, TensorShape());\n-  one_tensor.flat<int32>()(0) = 1;\n+  one_tensor.flat<int32_t>()(0) = 1;\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/RangeDelta\"), \"Const\")\n+      NodeBuilder(absl::StrCat(name_prefix, \"/RangeDelta\"), \"Const\")\n           .Attr(\"dtype\", DT_INT32)\n           .Attr(\"value\", one_tensor)\n           .Finalize(graph, &delta));\n   Node* rank;\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/InputRank\"), \"Rank\")\n+      NodeBuilder(absl::StrCat(name_prefix, \"/InputRank\"), \"Rank\")\n           .Input(input)\n           .Finalize(graph, &rank));\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/ReductionAxes\"), \"Range\")\n+      NodeBuilder(absl::StrCat(name_prefix, \"/ReductionAxes\"), \"Range\")\n           .Input(start)\n           .Input(rank)\n           .Input(delta)\n@@ -363,45 +363,43 @@ absl::Status MakeReductionAxes(Graph* graph, string name_prefix, Node* input,\n }\n \n // Computes the exponential moving average of input, updated in update_variable.\n-absl::Status MakeExponentialMovingAverage(Graph* graph, string name_prefix,\n+absl::Status MakeExponentialMovingAverage(Graph* graph, std::string name_prefix,\n                                           const NodeBuilder::NodeOut& input,\n                                           Node* decay, Node* update_variable,\n                                           Node** assign_value) {\n   // variable_t+1 = variable_t - [(variable_t - value) * (1 - decay)]\n-  name_prefix = strings::StrCat(name_prefix, \"/EMA\");\n+  name_prefix = absl::StrCat(name_prefix, \"/EMA\");\n   Node* one;\n   Tensor one_tensor(DT_FLOAT, TensorShape());\n   one_tensor.flat<float>()(0) = 1.0;\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/OneConst\"), \"Const\")\n+      NodeBuilder(absl::StrCat(name_prefix, \"/OneConst\"), \"Const\")\n           .Attr(\"dtype\", DT_FLOAT)\n           .Attr(\"value\", one_tensor)\n           .Finalize(graph, &one));\n   Node* decay_complement;\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/DecayComplement\"), \"Sub\")\n+      NodeBuilder(absl::StrCat(name_prefix, \"/DecayComplement\"), \"Sub\")\n           .Input(one)\n           .Input(decay)\n           .Finalize(graph, &decay_complement));\n \n   Node* value_diff;\n-  TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/ValueDiff\"), \"Sub\")\n-          .Input(update_variable)\n-          .Input(input)\n-          .Finalize(graph, &value_diff));\n+  TF_RETURN_IF_ERROR(NodeBuilder(absl::StrCat(name_prefix, \"/ValueDiff\"), \"Sub\")\n+                         .Input(update_variable)\n+                         .Input(input)\n+                         .Finalize(graph, &value_diff));\n   Node* update_value;\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/UpdateValue\"), \"Mul\")\n+      NodeBuilder(absl::StrCat(name_prefix, \"/UpdateValue\"), \"Mul\")\n           .Input(value_diff)\n           .Input(decay_complement)\n           .Finalize(graph, &update_value));\n \n-  TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/EMAValue\"), \"Sub\")\n-          .Input(update_variable)\n-          .Input(update_value)\n-          .Finalize(graph, assign_value));\n+  TF_RETURN_IF_ERROR(NodeBuilder(absl::StrCat(name_prefix, \"/EMAValue\"), \"Sub\")\n+                         .Input(update_variable)\n+                         .Input(update_value)\n+                         .Finalize(graph, assign_value));\n   return absl::OkStatus();\n }\n \n@@ -416,25 +414,24 @@ absl::Status MakeExponentialMovingAverage(Graph* graph, string name_prefix,\n //       |         EMA    init_val\n //       |           \\      /\n //       +----------- assign\n-absl::Status MakeInitializedEMAVariable(Graph* graph, const string& name,\n+absl::Status MakeInitializedEMAVariable(Graph* graph, const std::string& name,\n                                         Node* decay, Node* init_val,\n                                         std::vector<Node*>* added_variables,\n                                         Node** var) {\n   // TODO(suharshs): Update this to use ResourceVariables when they are ready.\n-  TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name, \"/Variable\"), \"VariableV2\")\n-          .Attr(\"shape\", TensorShape())\n-          .Attr(\"dtype\", DT_FLOAT)\n-          .Finalize(graph, var));\n+  TF_RETURN_IF_ERROR(NodeBuilder(absl::StrCat(name, \"/Variable\"), \"VariableV2\")\n+                         .Attr(\"shape\", TensorShape())\n+                         .Attr(\"dtype\", DT_FLOAT)\n+                         .Finalize(graph, var));\n   added_variables->push_back(*var);\n \n   Node* is_initialized;\n-  TF_RETURN_IF_ERROR(NodeBuilder(strings::StrCat(name, \"/IsInitialized\"),\n-                                 \"IsVariableInitialized\")\n-                         .Input(*var)\n-                         .Finalize(graph, &is_initialized));\n+  TF_RETURN_IF_ERROR(\n+      NodeBuilder(absl::StrCat(name, \"/IsInitialized\"), \"IsVariableInitialized\")\n+          .Input(*var)\n+          .Finalize(graph, &is_initialized));\n   Node* switch_node;\n-  TF_RETURN_IF_ERROR(NodeBuilder(strings::StrCat(name, \"/Switch\"), \"Switch\")\n+  TF_RETURN_IF_ERROR(NodeBuilder(absl::StrCat(name, \"/Switch\"), \"Switch\")\n                          .Input(init_val)\n                          .Input(is_initialized)\n                          .Finalize(graph, &switch_node));\n@@ -446,20 +443,19 @@ absl::Status MakeInitializedEMAVariable(Graph* graph, const string& name,\n                                                   decay, *var, &ema_value));\n \n   Node* assign_value;\n-  TF_RETURN_IF_ERROR(NodeBuilder(strings::StrCat(name, \"/Merge\"), \"Merge\")\n+  TF_RETURN_IF_ERROR(NodeBuilder(absl::StrCat(name, \"/Merge\"), \"Merge\")\n                          .Input({output_false, ema_value})\n                          .Finalize(graph, &assign_value));\n \n-  TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name, \"/AssignValue\"), \"Assign\")\n-          .Input(*var)\n-          .Input(assign_value)\n-          .Finalize(graph, var));\n+  TF_RETURN_IF_ERROR(NodeBuilder(absl::StrCat(name, \"/AssignValue\"), \"Assign\")\n+                         .Input(*var)\n+                         .Input(assign_value)\n+                         .Finalize(graph, var));\n   return absl::OkStatus();\n }\n \n // Computes the min and max EMA of input and stores them in min_var and max_var.\n-absl::Status MakeEMAMinMaxVars(Graph* graph, const string& name_prefix,\n+absl::Status MakeEMAMinMaxVars(Graph* graph, const std::string& name_prefix,\n                                Node* input, std::vector<Node*>* added_variables,\n                                Node** min_var, Node** max_var) {\n   // TODO(suharshs): The decay will be constant, so we could make only one for\n@@ -468,23 +464,22 @@ absl::Status MakeEMAMinMaxVars(Graph* graph, const string& name_prefix,\n   Tensor decay_tensor(DT_FLOAT, TensorShape());\n   decay_tensor.flat<float>()(0) = kEMADecay;\n   Node* decay;\n-  TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(name_prefix, \"/Decay\"), \"Const\")\n-          .Attr(\"dtype\", DT_FLOAT)\n-          .Attr(\"value\", decay_tensor)\n-          .Finalize(graph, &decay));\n+  TF_RETURN_IF_ERROR(NodeBuilder(absl::StrCat(name_prefix, \"/Decay\"), \"Const\")\n+                         .Attr(\"dtype\", DT_FLOAT)\n+                         .Attr(\"value\", decay_tensor)\n+                         .Finalize(graph, &decay));\n \n   Node* reduction_axes;\n   TF_RETURN_IF_ERROR(\n       MakeReductionAxes(graph, name_prefix, input, &reduction_axes));\n   Node* min;\n-  string min_name = strings::StrCat(name_prefix, \"/Min\");\n+  std::string min_name = absl::StrCat(name_prefix, \"/Min\");\n   TF_RETURN_IF_ERROR(NodeBuilder(min_name, \"Min\")\n                          .Input(input)\n                          .Input(reduction_axes)\n                          .Finalize(graph, &min));\n   Node* max;\n-  string max_name = strings::StrCat(name_prefix, \"/Max\");\n+  std::string max_name = absl::StrCat(name_prefix, \"/Max\");\n   TF_RETURN_IF_ERROR(NodeBuilder(max_name, \"Max\")\n                          .Input(input)\n                          .Input(reduction_axes)\n@@ -498,7 +493,7 @@ absl::Status MakeEMAMinMaxVars(Graph* graph, const string& name_prefix,\n \n // Makes an input min and max constant if the range is given. Otherwise, makes\n // min and max variables that are updated by an EMA.\n-absl::Status MakeInputMinMax(Graph* graph, const string& name_prefix,\n+absl::Status MakeInputMinMax(Graph* graph, const std::string& name_prefix,\n                              const EdgeToConvert& edge,\n                              std::vector<Node*>* added_variables,\n                              Node** input_min, Node** input_max) {\n@@ -508,14 +503,14 @@ absl::Status MakeInputMinMax(Graph* graph, const string& name_prefix,\n     Tensor input_min_tensor(DT_FLOAT, TensorShape());\n     input_min_tensor.flat<float>()(0) = edge.input_min;\n     TF_RETURN_IF_ERROR(\n-        NodeBuilder(strings::StrCat(name_prefix, \"/InputMin\"), \"Const\")\n+        NodeBuilder(absl::StrCat(name_prefix, \"/InputMin\"), \"Const\")\n             .Attr(\"dtype\", DT_FLOAT)\n             .Attr(\"value\", input_min_tensor)\n             .Finalize(graph, input_min));\n     Tensor input_max_tensor(DT_FLOAT, TensorShape());\n     input_max_tensor.flat<float>()(0) = edge.input_max;\n     TF_RETURN_IF_ERROR(\n-        NodeBuilder(strings::StrCat(name_prefix, \"/InputMax\"), \"Const\")\n+        NodeBuilder(absl::StrCat(name_prefix, \"/InputMax\"), \"Const\")\n             .Attr(\"dtype\", DT_FLOAT)\n             .Attr(\"value\", input_max_tensor)\n             .Finalize(graph, input_max));\n@@ -532,16 +527,16 @@ absl::Status MakeInputMinMax(Graph* graph, const string& name_prefix,\n // Adds a QuantizeAndDequantizeV2 or FakeQuantizeWithMinMaxVars op\n // (and required input nodes) based on edge.\n // The result is stored in convert_node.\n-absl::Status MakeQuantizeOp(Graph* graph, const string& name_prefix,\n-                            const string& quant_op_type,\n+absl::Status MakeQuantizeOp(Graph* graph, const std::string& name_prefix,\n+                            const std::string& quant_op_type,\n                             const EdgeToConvert& edge,\n                             std::vector<Node*>* added_variables,\n                             Node** convert_node) {\n   Node* input_min;\n   Node* input_max;\n   TF_RETURN_IF_ERROR(MakeInputMinMax(graph, name_prefix, edge, added_variables,\n                                      &input_min, &input_max));\n-  string quant_name = strings::StrCat(name_prefix, \"/\", quant_op_type);\n+  std::string quant_name = absl::StrCat(name_prefix, \"/\", quant_op_type);\n   if (quant_op_type == \"QuantizeAndDequantizeV2\") {\n     TF_RETURN_IF_ERROR(NodeBuilder(quant_name, quant_op_type)\n                            .Input(edge.edge->src())\n@@ -566,15 +561,15 @@ absl::Status MakeQuantizeOp(Graph* graph, const string& name_prefix,\n \n // Insert conversion op, connect it to the graph and remove the old edge.\n absl::Status ProcessTargetEdges(\n-    Graph* graph, const string& quant_op_type,\n+    Graph* graph, const std::string& quant_op_type,\n     const std::vector<EdgeToConvert>& target_edges) {\n   // Remember previously converted ops to avoid duplicated conversion on the\n   // same input.\n-  std::unordered_map<string, Node*, StringPieceHasher> name_index;\n+  std::unordered_map<std::string, Node*, StringPieceHasher> name_index;\n   std::vector<Node*> added_variables;\n   for (const EdgeToConvert edge : target_edges) {\n     Node* convert_node;\n-    string name_prefix = edge.edge->src()->name();\n+    std::string name_prefix = edge.edge->src()->name();\n \n     auto iter = name_index.find(name_prefix);\n     if (iter == name_index.end()) {\n@@ -596,7 +591,8 @@ absl::Status ProcessTargetEdges(\n \n }  // namespace\n \n-absl::Status DoQuantizeTraining(int32_t num_bits, const string& quant_op_type,\n+absl::Status DoQuantizeTraining(int32_t num_bits,\n+                                const std::string& quant_op_type,\n                                 Graph* graph) {\n   if (graph == nullptr) {\n     return errors::InvalidArgument(\"Cannot accept empty graph pointer.\");\n@@ -663,7 +659,7 @@ absl::Status DoQuantizeTraining(int32_t num_bits, const string& quant_op_type,\n \n absl::Status DoQuantizeTrainingOnGraphDef(const GraphDef& input_graphdef,\n                                           int32_t num_bits,\n-                                          const string& quant_op_type,\n+                                          const std::string& quant_op_type,\n                                           GraphDef* result_graphdef) {\n   Graph graph(OpRegistry::Global());\n   GraphConstructorOptions opts;\n@@ -678,8 +674,8 @@ absl::Status DoQuantizeTrainingOnGraphDef(const GraphDef& input_graphdef,\n }\n \n absl::Status DoQuantizeTrainingOnSerializedGraphDef(\n-    const string& input_graph_string, int32_t num_bits,\n-    const string& quant_op_type, string* result_graph_string) {\n+    const std::string& input_graph_string, int32_t num_bits,\n+    const std::string& quant_op_type, std::string* result_graph_string) {\n   // First create the graph from the GraphDef.\n   GraphDef input_graphdef;\n   if (!ParseProtoUnlimited(&input_graphdef, input_graph_string)) {"
        },
        {
            "sha": "21f794cbec8f2c9615d0f92f49ed467ca49510ec",
            "filename": "tensorflow/core/common_runtime/quantize_training.h",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -35,21 +35,20 @@ namespace tensorflow {\n //    - num_bits out of range.\n //    - g is null.\n //    - More than 1 unknown ops encountered.\n-absl::Status DoQuantizeTraining(int32_t num_bits, const string& quant_op_type,\n-                                Graph* g);\n+absl::Status DoQuantizeTraining(int32_t num_bits,\n+                                const std::string& quant_op_type, Graph* g);\n \n // Converts the input serialized GraphDef and returns a rewritten serialized\n // GraphDef for quantized training.\n-absl::Status DoQuantizeTrainingOnSerializedGraphDef(const string& input_graph,\n-                                                    int32_t num_bits,\n-                                                    const string& quant_op_type,\n-                                                    string* result_graph);\n+absl::Status DoQuantizeTrainingOnSerializedGraphDef(\n+    const std::string& input_graph, int32_t num_bits,\n+    const std::string& quant_op_type, std::string* result_graph);\n \n // Converts the input GraphDef and returns a rewritten GraphDef for quantized\n // training.\n absl::Status DoQuantizeTrainingOnGraphDef(const GraphDef& input_graphdef,\n                                           int32_t num_bits,\n-                                          const string& quant_op_type,\n+                                          const std::string& quant_op_type,\n                                           GraphDef* result_graphdef);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "5d4a1ac2618de23713e170bf2089069f1db65f67",
            "filename": "tensorflow/core/common_runtime/quantize_training_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 36,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fquantize_training_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -51,7 +51,7 @@ class QuantizeTrainingTest : public ::testing::Test {\n     return test::graph::Constant(g_.get(), test::AsTensor(values, shape));\n   }\n \n-  absl::Status Placeholder(Graph* g, const string& name, TensorShape shape,\n+  absl::Status Placeholder(Graph* g, const std::string& name, TensorShape shape,\n                            Node** out) {\n     TF_RETURN_IF_ERROR(NodeBuilder(name, \"Placeholder\")\n                            .Attr(\"dtype\", DT_FLOAT)\n@@ -60,7 +60,7 @@ class QuantizeTrainingTest : public ::testing::Test {\n     return absl::OkStatus();\n   }\n \n-  absl::Status FindNode(Graph* g, const string& name, Node** out) {\n+  absl::Status FindNode(Graph* g, const std::string& name, Node** out) {\n     for (Node* node : g->nodes()) {\n       if (node->name() == name) {\n         *out = node;\n@@ -111,15 +111,14 @@ TEST_F(QuantizeTrainingTest, SignedInput) {\n   // Quantize_and_dequantize node for identity should have signed_input==true.\n   Node* identity_q_node;\n   TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(identity->name(), \"/QuantizeAndDequantizeV2\"),\n+      FindNode(g, absl::StrCat(identity->name(), \"/QuantizeAndDequantizeV2\"),\n                &identity_q_node));\n   ASSERT_EQ(\"true\",\n             SummarizeAttrValue(*identity_q_node->attrs().Find(\"signed_input\")));\n   // Quantize_and_dequantize node for relu should have signed_input==false.\n   Node* relu_q_node;\n-  TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(relu->name(), \"/QuantizeAndDequantizeV2\"),\n-               &relu_q_node));\n+  TF_ASSERT_OK(FindNode(\n+      g, absl::StrCat(relu->name(), \"/QuantizeAndDequantizeV2\"), &relu_q_node));\n   ASSERT_EQ(\"false\",\n             SummarizeAttrValue(*relu_q_node->attrs().Find(\"signed_input\")));\n }\n@@ -161,16 +160,15 @@ TEST_F(QuantizeTrainingTest, RangeGivenTrue) {\n \n   // Quantize_and_dequantize node for relu6 should have range_given==true.\n   Node* relu6_q_node;\n-  TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(relu6->name(), \"/QuantizeAndDequantizeV2\"),\n-               &relu6_q_node));\n+  TF_ASSERT_OK(FindNode(g,\n+                        absl::StrCat(relu6->name(), \"/QuantizeAndDequantizeV2\"),\n+                        &relu6_q_node));\n   ASSERT_EQ(\"true\",\n             SummarizeAttrValue(*relu6_q_node->attrs().Find(\"range_given\")));\n   // Quantize_and_dequantize node for relu should have range_given==true.\n   Node* relu_q_node;\n-  TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(relu->name(), \"/QuantizeAndDequantizeV2\"),\n-               &relu_q_node));\n+  TF_ASSERT_OK(FindNode(\n+      g, absl::StrCat(relu->name(), \"/QuantizeAndDequantizeV2\"), &relu_q_node));\n   ASSERT_EQ(\"true\",\n             SummarizeAttrValue(*relu_q_node->attrs().Find(\"range_given\")));\n }\n@@ -215,18 +213,17 @@ TEST_F(QuantizeTrainingTest, WithBackwardNodes_QuantizeAndDequantize) {\n   // Ensure that the backwards matmul input was not quantized.\n   Node* found_node;\n   absl::Status s = FindNode(\n-      g, strings::StrCat(d->name(), \"/QuantizeAndDequantizeV2\"), &found_node);\n+      g, absl::StrCat(d->name(), \"/QuantizeAndDequantizeV2\"), &found_node);\n   EXPECT_TRUE(absl::StrContains(s.ToString(), \"not found\")) << s;\n \n   // Ensure that m1 and m2's inputs were quantized.\n+  TF_ASSERT_OK(FindNode(\n+      g, absl::StrCat(relu->name(), \"/QuantizeAndDequantizeV2\"), &found_node));\n   TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(relu->name(), \"/QuantizeAndDequantizeV2\"),\n-               &found_node));\n-  TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(identity->name(), \"/QuantizeAndDequantizeV2\"),\n+      FindNode(g, absl::StrCat(identity->name(), \"/QuantizeAndDequantizeV2\"),\n                &found_node));\n-  TF_ASSERT_OK(FindNode(\n-      g, strings::StrCat(c->name(), \"/QuantizeAndDequantizeV2\"), &found_node));\n+  TF_ASSERT_OK(FindNode(g, absl::StrCat(c->name(), \"/QuantizeAndDequantizeV2\"),\n+                        &found_node));\n }\n \n TEST_F(QuantizeTrainingTest, WithBackwardNodes_FakeQuant) {\n@@ -269,18 +266,17 @@ TEST_F(QuantizeTrainingTest, WithBackwardNodes_FakeQuant) {\n   // Ensure that the backwards matmul input was not quantized.\n   Node* found_node;\n   absl::Status s = FindNode(\n-      g, strings::StrCat(d->name(), \"/FakeQuantWithMinMaxVars\"), &found_node);\n+      g, absl::StrCat(d->name(), \"/FakeQuantWithMinMaxVars\"), &found_node);\n   EXPECT_TRUE(absl::StrContains(s.ToString(), \"not found\")) << s;\n \n   // Ensure that m1 and m2's inputs were quantized.\n+  TF_ASSERT_OK(FindNode(\n+      g, absl::StrCat(relu->name(), \"/FakeQuantWithMinMaxVars\"), &found_node));\n   TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(relu->name(), \"/FakeQuantWithMinMaxVars\"),\n-               &found_node));\n-  TF_ASSERT_OK(\n-      FindNode(g, strings::StrCat(identity->name(), \"/FakeQuantWithMinMaxVars\"),\n+      FindNode(g, absl::StrCat(identity->name(), \"/FakeQuantWithMinMaxVars\"),\n                &found_node));\n-  TF_ASSERT_OK(FindNode(\n-      g, strings::StrCat(c->name(), \"/FakeQuantWithMinMaxVars\"), &found_node));\n+  TF_ASSERT_OK(FindNode(g, absl::StrCat(c->name(), \"/FakeQuantWithMinMaxVars\"),\n+                        &found_node));\n }\n \n TEST_F(QuantizeTrainingTest, QuantizeSerializedGraphDef) {\n@@ -301,10 +297,10 @@ TEST_F(QuantizeTrainingTest, QuantizeSerializedGraphDef) {\n   // Convert the graph to the graphdef string.\n   GraphDef input_graph;\n   graph->ToGraphDef(&input_graph);\n-  string input_string;\n+  std::string input_string;\n   input_graph.SerializeToString(&input_string);\n \n-  string result_string;\n+  std::string result_string;\n   TF_ASSERT_OK(DoQuantizeTrainingOnSerializedGraphDef(\n       input_string, num_bits, \"QuantizeAndDequantizeV2\", &result_string));\n \n@@ -400,8 +396,8 @@ TEST_F(QuantizeTrainingTest, FixedRangeAndEMARange_QuantizeAndDequantize) {\n \n   // The min and max values of the relu6 quantization should be constant values\n   // of 0 and 6.\n-  string min_const_name = strings::StrCat(relu6->name(), \"/InputMin\");\n-  string max_const_name = strings::StrCat(relu6->name(), \"/InputMax\");\n+  std::string min_const_name = absl::StrCat(relu6->name(), \"/InputMin\");\n+  std::string max_const_name = absl::StrCat(relu6->name(), \"/InputMax\");\n   std::vector<Tensor> outputs;\n   TF_ASSERT_OK(sess->Run({}, {min_const_name, max_const_name}, {}, &outputs));\n   EXPECT_EQ(outputs[0].flat<float>()(0), 0.0);\n@@ -416,8 +412,8 @@ TEST_F(QuantizeTrainingTest, FixedRangeAndEMARange_QuantizeAndDequantize) {\n \n   // The value of the min and max should be set to the min and max of a1 since\n   // this is the first run that initializes the EMA variables.\n-  string min_var_name = strings::StrCat(relu->name(), \"/Min/Variable\");\n-  string max_var_name = strings::StrCat(relu->name(), \"/Max/Variable\");\n+  std::string min_var_name = absl::StrCat(relu->name(), \"/Min/Variable\");\n+  std::string max_var_name = absl::StrCat(relu->name(), \"/Max/Variable\");\n   TF_ASSERT_OK(sess->Run({}, {min_var_name, max_var_name}, {}, &outputs));\n   EXPECT_EQ(outputs[0].flat<float>()(0), 0.0);\n   EXPECT_EQ(outputs[1].flat<float>()(0), 3.0);\n@@ -494,8 +490,8 @@ TEST_F(QuantizeTrainingTest, FixedRangeAndEMARange_FakeQuant) {\n \n   // The min and max values of the relu6 quantization should be constant values\n   // of 0 and 6.\n-  string min_const_name = strings::StrCat(relu6->name(), \"/InputMin\");\n-  string max_const_name = strings::StrCat(relu6->name(), \"/InputMax\");\n+  std::string min_const_name = absl::StrCat(relu6->name(), \"/InputMin\");\n+  std::string max_const_name = absl::StrCat(relu6->name(), \"/InputMax\");\n   std::vector<Tensor> outputs;\n   TF_ASSERT_OK(sess->Run({}, {min_const_name, max_const_name}, {}, &outputs));\n   EXPECT_EQ(outputs[0].flat<float>()(0), 0.0);\n@@ -510,8 +506,8 @@ TEST_F(QuantizeTrainingTest, FixedRangeAndEMARange_FakeQuant) {\n \n   // The value of the min and max should be set to the min and max of a1 since\n   // this is the first run that initializes the EMA variables.\n-  string min_var_name = strings::StrCat(relu->name(), \"/Min/Variable\");\n-  string max_var_name = strings::StrCat(relu->name(), \"/Max/Variable\");\n+  std::string min_var_name = absl::StrCat(relu->name(), \"/Min/Variable\");\n+  std::string max_var_name = absl::StrCat(relu->name(), \"/Max/Variable\");\n   TF_ASSERT_OK(sess->Run({}, {min_var_name, max_var_name}, {}, &outputs));\n   EXPECT_EQ(outputs[0].flat<float>()(0), 0.0);\n   EXPECT_EQ(outputs[1].flat<float>()(0), 3.0);"
        },
        {
            "sha": "a4c15f74b49774f8521e086b2b82f535a928b582",
            "filename": "tensorflow/core/common_runtime/renamed_device.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frenamed_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frenamed_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Frenamed_device.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -28,7 +28,7 @@ namespace tensorflow {\n \n /* static */\n std::unique_ptr<Device> RenamedDevice::NewRenamedDevice(\n-    const string& new_base, Device* underlying, bool owns_underlying,\n+    const std::string& new_base, Device* underlying, bool owns_underlying,\n     bool isolate_session_state,\n     thread::ThreadPoolInterface* underlying_threadpool) {\n   DeviceNameUtils::ParsedName parsed_name;\n@@ -39,9 +39,9 @@ std::unique_ptr<Device> RenamedDevice::NewRenamedDevice(\n   CHECK(underlying_parsed_name.has_id);\n   parsed_name.type = underlying_parsed_name.type;\n   parsed_name.id = underlying_parsed_name.id;\n-  string name = DeviceNameUtils::FullName(parsed_name.job, parsed_name.replica,\n-                                          parsed_name.task, parsed_name.type,\n-                                          parsed_name.id);\n+  std::string name = DeviceNameUtils::FullName(\n+      parsed_name.job, parsed_name.replica, parsed_name.task, parsed_name.type,\n+      parsed_name.id);\n   DeviceAttributes attributes(underlying->attributes());\n   attributes.set_name(name);\n   // Call absl::WrapUnique to access private constructor."
        },
        {
            "sha": "687f61f8eff2d89e8d94fe3cb5e2c4b767a2e591",
            "filename": "tensorflow/core/common_runtime/renamed_device.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frenamed_device.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frenamed_device.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Frenamed_device.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -30,7 +30,7 @@ namespace tensorflow {\n class RenamedDevice : public Device {\n  public:\n   static std::unique_ptr<Device> NewRenamedDevice(\n-      const string& new_base, Device* underlying, bool owns_underlying,\n+      const std::string& new_base, Device* underlying, bool owns_underlying,\n       bool isolate_session_state,\n       thread::ThreadPoolInterface* underlying_threadpool = nullptr);\n "
        },
        {
            "sha": "1d6e53c658506808fc7fbb6273503e5c530753bd",
            "filename": "tensorflow/core/common_runtime/rendezvous_mgr.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_mgr.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_mgr.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_mgr.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -99,9 +99,9 @@ void SameWorkerRecvDone(const DeviceMgr* device_mgr,\n   if (in.dtype() != DT_VARIANT) {\n     // Variants are handled by CopyTensor::ViaDMA.\n     AllocationAttributes aa;\n-    uint64 safe_alloc_frontier = dst_device->SafeAllocFrontier(0);\n-    std::function<uint64()> freed_by_func = [dst_device,\n-                                             &safe_alloc_frontier]() {\n+    uint64_t safe_alloc_frontier = dst_device->SafeAllocFrontier(0);\n+    std::function<uint64_t()> freed_by_func = [dst_device,\n+                                               &safe_alloc_frontier]() {\n       safe_alloc_frontier = dst_device->SafeAllocFrontier(safe_alloc_frontier);\n       return safe_alloc_frontier;\n     };"
        },
        {
            "sha": "8f4e7acbf77ed506afdb14d40126e7c455335b4d",
            "filename": "tensorflow/core/common_runtime/rendezvous_util.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -22,7 +22,8 @@ namespace tensorflow {\n absl::Status SendTensorsToRendezvous(\n     RendezvousInterface* rendezvous, DeviceContext* device_context,\n     const std::vector<AllocatorAttributes>& alloc_attrs,\n-    const std::vector<string>& keys, absl::Span<const Tensor> tensors_to_send) {\n+    const std::vector<std::string>& keys,\n+    absl::Span<const Tensor> tensors_to_send) {\n   if (keys.size() != tensors_to_send.size()) {\n     return errors::InvalidArgument(\n         \"keys and tensors_to_send are not the same size. keys.size() = \",\n@@ -56,7 +57,7 @@ absl::Status SendTensorsToRendezvous(\n void RecvOutputsFromRendezvousAsync(\n     RendezvousInterface* rendezvous, DeviceContext* device_context,\n     const std::vector<AllocatorAttributes>& alloc_attrs,\n-    const std::vector<string>& keys, std::vector<Tensor>* received_tensors,\n+    const std::vector<std::string>& keys, std::vector<Tensor>* received_tensors,\n     StatusCallback done) {\n   if (keys.empty()) {\n     done(absl::OkStatus());\n@@ -69,8 +70,8 @@ void RecvOutputsFromRendezvousAsync(\n   }\n \n   received_tensors->reserve(keys.size());\n-  std::vector<\n-      std::tuple<string, Tensor*, Rendezvous::ParsedKey, AllocatorAttributes>>\n+  std::vector<std::tuple<std::string, Tensor*, Rendezvous::ParsedKey,\n+                         AllocatorAttributes>>\n       arguments;\n   for (int i = 0; i < keys.size(); ++i) {\n     Rendezvous::ParsedKey parsed;\n@@ -90,7 +91,7 @@ void RecvOutputsFromRendezvousAsync(\n \n   auto status_cb = new ReffedStatusCallback(std::move(done));\n   for (auto& p : arguments) {\n-    const string& key = std::get<0>(p);\n+    const std::string& key = std::get<0>(p);\n     Tensor* val = std::get<1>(p);\n     Rendezvous::ParsedKey parsed = std::get<2>(p);\n     Rendezvous::Args rendez_args;\n@@ -124,7 +125,7 @@ absl::Status RecvOutputsFromRendezvous(RendezvousInterface* rendezvous,\n   // Receives values requested by the caller.\n   Rendezvous::ParsedKey parsed;\n   for (auto& p : *out) {\n-    const string& key = p.first;\n+    const std::string& key = p.first;\n     Tensor* val = &p.second;\n     bool is_dead = false;\n     TF_RETURN_IF_ERROR(Rendezvous::ParseKey(key, &parsed));"
        },
        {
            "sha": "1c9ac0ef221a5491c9955a74ab06c523eaaf78ef",
            "filename": "tensorflow/core/common_runtime/rendezvous_util.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -22,7 +22,7 @@ limitations under the License.\n \n namespace tensorflow {\n \n-typedef std::map<string, Tensor> NamedTensors;\n+typedef std::map<std::string, Tensor> NamedTensors;\n typedef std::function<void(const absl::Status&)> StatusCallback;\n \n // Uses `rendezvous` to send tensors in `tensors_to_send`. `device_context`\n@@ -33,7 +33,8 @@ typedef std::function<void(const absl::Status&)> StatusCallback;\n absl::Status SendTensorsToRendezvous(\n     RendezvousInterface* rendezvous, DeviceContext* device_context,\n     const std::vector<AllocatorAttributes>& alloc_attrs,\n-    const std::vector<string>& keys, absl::Span<const Tensor> tensors_to_send);\n+    const std::vector<std::string>& keys,\n+    absl::Span<const Tensor> tensors_to_send);\n \n // Uses `rendezvous` to obtain tensors. `device_context` should be the\n // DeviceContext associated with the receiving device. `alloc_attrs` contains\n@@ -42,7 +43,7 @@ absl::Status SendTensorsToRendezvous(\n void RecvOutputsFromRendezvousAsync(\n     RendezvousInterface* rendezvous, DeviceContext* device_context,\n     const std::vector<AllocatorAttributes>& alloc_attrs,\n-    const std::vector<string>& keys, std::vector<Tensor>* received_tensors,\n+    const std::vector<std::string>& keys, std::vector<Tensor>* received_tensors,\n     StatusCallback done);\n \n absl::Status RecvOutputsFromRendezvous(RendezvousInterface* rendezvous,"
        },
        {
            "sha": "f2c866c307905cef4b1d07c8f9585ebc2158cb46",
            "filename": "tensorflow/core/common_runtime/rendezvous_util_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Frendezvous_util_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -32,20 +32,20 @@ class RendezvousUtilTest : public ::testing::Test {\n };\n \n // string -> Tensor<string>\n-Tensor V(const string& content) {\n+Tensor V(const std::string& content) {\n   Tensor tensor(DT_STRING, TensorShape({}));\n   tensor.scalar<tstring>()() = content;\n   return tensor;\n }\n \n // Tensor<string> -> string\n-string V(const Tensor& tensor) {\n+std::string V(const Tensor& tensor) {\n   CHECK_EQ(tensor.dtype(), DT_STRING);\n   CHECK(TensorShapeUtils::IsScalar(tensor.shape()));\n   return tensor.scalar<tstring>()();\n }\n \n-string MakeStringKey(const string& name) {\n+std::string MakeStringKey(const std::string& name) {\n   return Rendezvous::CreateKey(\n       \"/job:localhost/replica:0/task:0/device:CPU:0\", 0,\n       \"/job:localhost/replica:0/task:0/device:GPU:0\", name, FrameAndIter(0, 0));"
        },
        {
            "sha": "9dfa50ae0dc2a4f3ca9eebc3d5c1f33c77900ac4",
            "filename": "tensorflow/core/common_runtime/replicate_constants_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_constants_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_constants_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_constants_pass.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -70,8 +70,8 @@ bool HasCpuDevice(const Node* node) {\n // Convert the CPU device name to the corresponding CPU device name. If\n // multiple local CPU devices are enabled, the CPU device name will also\n // contain the device id.\n-absl::Status DeviceNameToCpuDeviceNameWithDeviceId(const string& device_name,\n-                                                   string* host_device_name) {\n+absl::Status DeviceNameToCpuDeviceNameWithDeviceId(\n+    const std::string& device_name, std::string* host_device_name) {\n   DeviceNameUtils::ParsedName device;\n   if (!DeviceNameUtils::ParseFullName(device_name, &device)) {\n     return absl::InternalError("
        },
        {
            "sha": "e60117f588f8c0cf912ebb692a19ea779a3adf18",
            "filename": "tensorflow/core/common_runtime/replicate_per_replica_nodes.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -45,16 +45,16 @@ class ReplicateHelper {\n \n   // Replicate the given node to an allowed device.\n   absl::Status ReplicateNode(const Node* node,\n-                             const std::vector<string>& allowed_devices,\n+                             const std::vector<std::string>& allowed_devices,\n                              int allowed_device_index, Graph* graph) {\n     auto& replicated_nodes = replicated_nodes_map_.at(node);\n     if (replicated_nodes[allowed_device_index] != nullptr) {\n       return absl::OkStatus();\n     }\n     const auto& device = allowed_devices.at(allowed_device_index);\n     NodeDef node_def = node->def();\n-    const string suffix = strings::StrCat(\"/R\", allowed_device_index);\n-    node_def.set_name(graph->NewName(strings::StrCat(node_def.name(), suffix)));\n+    const std::string suffix = absl::StrCat(\"/R\", allowed_device_index);\n+    node_def.set_name(graph->NewName(absl::StrCat(node_def.name(), suffix)));\n     TF_ASSIGN_OR_RETURN(Node * replicated_node, graph->AddNode(node_def));\n     replicated_node->set_assigned_device_name(device);\n     if (replicated_node->IsArg()) {\n@@ -83,7 +83,7 @@ class ReplicateHelper {\n   // Replace an edge (composite device -> composite device) with\n   // N edges (allowed devices -> allowed devices).\n   absl::Status ReplicateFromCompositeDeviceToCompositeDevice(\n-      const Edge* edge, const std::vector<string>& allowed_devices,\n+      const Edge* edge, const std::vector<std::string>& allowed_devices,\n       Graph* graph) {\n     const std::vector<Node*>& src_replicated_nodes =\n         replicated_nodes_map_.at(edge->src());\n@@ -115,12 +115,12 @@ class ReplicateHelper {\n   // Control edge: replace an edge (composite device -> a regular device) with\n   // N edges (allowed devices -> a regular device).\n   absl::Status ReplicateFromCompositeDeviceToRegularDevice(\n-      const Edge* edge, const std::vector<string>& allowed_devices,\n+      const Edge* edge, const std::vector<std::string>& allowed_devices,\n       Graph* graph) {\n     const std::vector<Node*>& src_replicated_nodes =\n         replicated_nodes_map_.at(edge->src());\n     Node* dst = edge->dst();\n-    const string& dst_device = dst->assigned_device_name();\n+    const std::string& dst_device = dst->assigned_device_name();\n     bool found_src_node = false;\n     for (int i = 0; i < allowed_devices.size(); ++i) {\n       if (allowed_devices.at(i) == dst_device) {\n@@ -198,7 +198,7 @@ class ReplicateHelper {\n \n // Replicate the nodes in cluster_nodes and update edges.\n absl::Status ReplicateNodesAndEdges(\n-    const std::vector<string>& allowed_devices,\n+    const std::vector<std::string>& allowed_devices,\n     absl::flat_hash_map<Node*, int>* cluster_nodes, ReplicateHelper* helper,\n     Graph* graph) {\n   // Contains nodes in cluster_nodes whose out nodes are all on physical\n@@ -253,19 +253,19 @@ absl::Status ReplicateNodesAndEdges(\n }  // namespace\n \n absl::Status ReplicatePerReplicaNodesInFunctionGraph(\n-    const absl::flat_hash_map<string, const std::vector<string>*>&\n+    const absl::flat_hash_map<std::string, const std::vector<std::string>*>&\n         composite_devices,\n     Graph* graph) {\n   VLOG(1) << \"Starting ReplicatePerReplicaNodesInFunctionGraph\";\n   VLOG(1) << \"Graph #nodes \" << graph->num_nodes() << \" #edges \"\n           << graph->num_edges();\n-  std::set<string> composite_device_names;\n+  std::set<std::string> composite_device_names;\n   for (const auto& it : composite_devices) {\n     composite_device_names.insert(it.first);\n   }\n   // Map from a composite device to a cluster of nodes assigned to the\n   // composite device and the numbers of their out edges to process.\n-  absl::flat_hash_map<string, absl::flat_hash_map<Node*, int>>\n+  absl::flat_hash_map<std::string, absl::flat_hash_map<Node*, int>>\n       composite_device_to_cluster_nodes;\n   for (Node* n : graph->op_nodes()) {\n     if (composite_device_names.find(n->assigned_device_name()) !=\n@@ -284,7 +284,7 @@ absl::Status ReplicatePerReplicaNodesInFunctionGraph(\n   }\n \n   for (auto& it : composite_device_to_cluster_nodes) {\n-    const std::vector<string>& allowed_devices =\n+    const std::vector<std::string>& allowed_devices =\n         *composite_devices.at(it.first);\n     if (allowed_devices.empty()) {\n       return errors::InvalidArgument(\"No allowed device of composite device: \","
        },
        {
            "sha": "414bd21de353612d2d6603b233331881a9f236e1",
            "filename": "tensorflow/core/common_runtime/replicate_per_replica_nodes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -35,7 +35,7 @@ namespace tensorflow {\n // dependency.\n // TODO(b/145922293): Register it as a POST_REWRITE_FOR_EXEC pass.\n absl::Status ReplicatePerReplicaNodesInFunctionGraph(\n-    const absl::flat_hash_map<string, const std::vector<string>*>&\n+    const absl::flat_hash_map<std::string, const std::vector<std::string>*>&\n         composite_devices,\n     Graph* graph);\n "
        },
        {
            "sha": "f0a859286fba06e8fd3cc083928ded49f73c4a32",
            "filename": "tensorflow/core/common_runtime/replicate_per_replica_nodes_test.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 24,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Freplicate_per_replica_nodes_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -40,7 +40,7 @@ class GraphHelper {\n     }\n   }\n \n-  Node* GetNodeByName(const string& name) {\n+  Node* GetNodeByName(const std::string& name) {\n     const auto it = nodes_by_name_.find(name);\n     if (it != nodes_by_name_.end()) {\n       return it->second;\n@@ -53,7 +53,8 @@ class GraphHelper {\n     return nullptr;\n   }\n \n-  void SetAssignedDevice(const string& node_name, const string& device_name) {\n+  void SetAssignedDevice(const std::string& node_name,\n+                         const std::string& device_name) {\n     CHECK_NOTNULL(GetNodeByName(node_name))\n         ->set_assigned_device_name(device_name);\n   }\n@@ -68,14 +69,14 @@ class GraphHelper {\n     EXPECT_EQ(arg_num, expected_num);\n   }\n \n-  void CheckAssignedDevice(const string& node_name,\n-                           const string& expected_device_name) {\n+  void CheckAssignedDevice(const std::string& node_name,\n+                           const std::string& expected_device_name) {\n     EXPECT_EQ(expected_device_name,\n               CHECK_NOTNULL(GetNodeByName(node_name))->assigned_device_name());\n   }\n \n-  void CheckAssignedDevicePrefix(const string& node_name,\n-                                 const string& expected_device_name) {\n+  void CheckAssignedDevicePrefix(const std::string& node_name,\n+                                 const std::string& expected_device_name) {\n     auto assigned =\n         CHECK_NOTNULL(GetNodeByName(node_name))->assigned_device_name();\n     EXPECT_EQ(assigned.rfind(expected_device_name, 0), 0);\n@@ -85,21 +86,21 @@ class GraphHelper {\n   const Graph& graph_;\n   // Maps from a node name to a Node* in the graph. We use an ordered map here\n   // to ensure stability of GetNodeByName().\n-  std::map<string, Node*> nodes_by_name_;\n+  std::map<std::string, Node*> nodes_by_name_;\n };\n \n TEST(ReplicatePerReplicaNodesTest, SingleCompositeDevice) {\n   tensorflow::Scope scope = tensorflow::Scope::NewRootScope();\n   Output arg = ops::_Arg(scope.WithOpName(\"arg\"), DT_RESOURCE, 0);\n   auto read = ops::ReadVariableOp(scope.WithOpName(\"read\"), arg, DT_INT32);\n-  auto one = ops::Const<int32>(scope.WithOpName(\"one\"), 1);\n+  auto one = ops::Const<int32_t>(scope.WithOpName(\"one\"), 1);\n   auto write = ops::AssignVariableOp(scope.WithOpName(\"write\"), arg, one);\n   auto ret = ops::_Retval(\n       scope.WithOpName(\"ret\").WithControlDependencies({write}), read, 0);\n \n-  const std::vector<string> underlying_devices = {\"/device:TPU:0\",\n-                                                  \"/device:TPU:1\"};\n-  const absl::flat_hash_map<string, const std::vector<string>*>\n+  const std::vector<std::string> underlying_devices = {\"/device:TPU:0\",\n+                                                       \"/device:TPU:1\"};\n+  const absl::flat_hash_map<std::string, const std::vector<std::string>*>\n       composite_devices = {{\"/device:TPU_COMPOSITE:0\", &underlying_devices}};\n \n   Graph graph(OpRegistry::Global());\n@@ -143,8 +144,8 @@ TEST(ReplicatePerReplicaNodesTest, SingleCompositeDeviceToSingleDevice) {\n   auto read = ops::ReadVariableOp(scope.WithOpName(\"read\"), arg, DT_INT32);\n   auto ret = ops::_Retval(scope.WithOpName(\"ret\"), read, 0);\n \n-  const std::vector<string> underlying_devices = {\"/device:TPU:0\"};\n-  const absl::flat_hash_map<string, const std::vector<string>*>\n+  const std::vector<std::string> underlying_devices = {\"/device:TPU:0\"};\n+  const absl::flat_hash_map<std::string, const std::vector<std::string>*>\n       composite_devices = {{\"/device:TPU_COMPOSITE:0\", &underlying_devices}};\n \n   Graph graph(OpRegistry::Global());\n@@ -183,11 +184,11 @@ TEST(ReplicatePerReplicaNodesTest, MultipleCompositeDevices) {\n   auto add = ops::Add(scope.WithOpName(\"add\"), identity0, identity1);\n   auto ret = ops::_Retval(scope.WithOpName(\"ret\"), add, 0);\n \n-  const std::vector<string> underlying_devices_0 = {\"/device:TPU:0\",\n-                                                    \"/device:TPU:1\"};\n-  const std::vector<string> underlying_devices_1 = {\"/device:TPU:2\",\n-                                                    \"/device:TPU:3\"};\n-  const absl::flat_hash_map<string, const std::vector<string>*>\n+  const std::vector<std::string> underlying_devices_0 = {\"/device:TPU:0\",\n+                                                         \"/device:TPU:1\"};\n+  const std::vector<std::string> underlying_devices_1 = {\"/device:TPU:2\",\n+                                                         \"/device:TPU:3\"};\n+  const absl::flat_hash_map<std::string, const std::vector<std::string>*>\n       composite_devices = {{\"/device:TPU_COMPOSITE:0\", &underlying_devices_0},\n                            {\"/device:TPU_COMPOSITE:1\", &underlying_devices_1}};\n \n@@ -232,9 +233,9 @@ TEST(ReplicatePerReplicaNodesTest, MultipleCompositeDevices) {\n }\n \n TEST(ReplicatePerReplicaNodesTest, NestedFunctions) {\n-  const std::vector<string> underlying_devices = {\"/device:TPU:0\",\n-                                                  \"/device:TPU:1\"};\n-  const absl::flat_hash_map<string, const std::vector<string>*>\n+  const std::vector<std::string> underlying_devices = {\"/device:TPU:0\",\n+                                                       \"/device:TPU:1\"};\n+  const absl::flat_hash_map<std::string, const std::vector<std::string>*>\n       composite_devices = {{\"/device:TPU_COMPOSITE:0\", &underlying_devices}};\n \n   FunctionDefLibrary fdef_lib;\n@@ -311,9 +312,9 @@ TEST(ReplicatePerReplicaNodesTest, DeadArgNodes) {\n   auto read = ops::ReadVariableOp(scope.WithOpName(\"read\"), arg, DT_INT32);\n   auto ret = ops::_Retval(scope.WithOpName(\"ret\"), read, 0);\n \n-  const std::vector<string> underlying_devices = {\"/device:TPU:0\",\n-                                                  \"/device:TPU:1\"};\n-  const absl::flat_hash_map<string, const std::vector<string>*>\n+  const std::vector<std::string> underlying_devices = {\"/device:TPU:0\",\n+                                                       \"/device:TPU:1\"};\n+  const absl::flat_hash_map<std::string, const std::vector<std::string>*>\n       composite_devices = {{\"/device:TPU_COMPOSITE:0\", &underlying_devices}};\n \n   Graph graph(OpRegistry::Global());"
        },
        {
            "sha": "ff44370ecbd451d892c84e868ef15e3bf22a729c",
            "filename": "tensorflow/core/common_runtime/ring_alg.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_alg.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_alg.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fring_alg.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -61,8 +61,8 @@ namespace {\n // RingAlg instances.  Note that the exec_key will differentiate between\n // different instances consequently we don't need to further differentiate\n // between subclasses of RingAlg.\n-string RingAlgBufKey(const string& name, const string& exec_key, int pass,\n-                     int section, int source_rank) {\n+std::string RingAlgBufKey(const std::string& name, const std::string& exec_key,\n+                          int pass, int section, int source_rank) {\n   if (READABLE_KEYS) {\n     return strings::StrCat(name, \"(\", exec_key, \"):pass(\", pass, \"):section(\",\n                            section, \"):srcrank(\", source_rank, \")\");\n@@ -97,7 +97,7 @@ RingAlg::RingField* RingAlg::PCQueue::Dequeue() {\n   return rf;\n }\n \n-RingAlg::RingAlg(CollectiveType type, const string& name)\n+RingAlg::RingAlg(CollectiveType type, const std::string& name)\n     : type_(type),\n       name_(name),\n       col_ctx_(nullptr),\n@@ -163,10 +163,10 @@ absl::Status GenerateSubdivsInCollectiveParams(CollectiveParams* col_params) {\n   }\n \n   if (VLOG_IS_ON(2)) {\n-    string subdiv_buf;\n+    std::string subdiv_buf;\n     for (const int subdiv_offset :\n          col_params->instance.impl_details.subdiv_offsets) {\n-      strings::StrAppend(&subdiv_buf, \" \", subdiv_offset);\n+      absl::StrAppend(&subdiv_buf, \" \", subdiv_offset);\n     }\n     VLOG(2) << \"Dynamically generated \" << num_subdivs\n             << \" subdiv_offsets:\" << subdiv_buf << \" tensor_size \"\n@@ -178,7 +178,7 @@ absl::Status GenerateSubdivsInCollectiveParams(CollectiveParams* col_params) {\n }  // namespace\n \n absl::Status RingAlg::InitializeCollectiveParams(CollectiveParams* col_params) {\n-  const string& device_name =\n+  const std::string& device_name =\n       col_params->group.members[col_params->default_rank].device.name();\n   // Each subdiv permutation is a ring formed by rotating each\n   // single-task subsequence of devices by an offset.  This makes most\n@@ -190,7 +190,7 @@ absl::Status RingAlg::InitializeCollectiveParams(CollectiveParams* col_params) {\n   // Precondition: device_names must be sorted so that all devices in\n   // the same task are adjacent.\n   std::vector<int> dev_per_task;\n-  const string* prior_task_name = &col_params->group.members[0].task;\n+  const std::string* prior_task_name = &col_params->group.members[0].task;\n   int dev_count = 1;\n   for (int di = 1; di < col_params->group.group_size; ++di) {\n     if (col_params->group.members[di].task != *prior_task_name) {\n@@ -265,7 +265,7 @@ absl::Status RingAlg::InitializeCollectiveContext(\n       &col_ctx->device_locality);\n }\n \n-string RingAlg::TensorDebugString(const Tensor& tensor) {\n+std::string RingAlg::TensorDebugString(const Tensor& tensor) {\n   const DeviceBase::AcceleratorDeviceInfo* accelerator_device_info =\n       col_ctx_->op_ctx->device()->tensorflow_accelerator_device_info();\n   if (accelerator_device_info) {\n@@ -383,11 +383,11 @@ void RingAlg::AdvanceToSecondPass(RingField* rf) {\n   VLOG(3) << \"IncrRingField new value \" << rf->DebugString();\n }\n \n-string RingAlg::RingField::DebugString() const {\n-  string rv = strings::StrCat(\"RingField rank=\", rank, \" chunk_idx=\", chunk_idx,\n-                              \" subdiv=\", subdiv_idx, \" sc_idx=\", sc_idx,\n-                              \" action=\", action);\n-  strings::StrAppend(&rv, \" pass=\", second_pass);\n+std::string RingAlg::RingField::DebugString() const {\n+  std::string rv = strings::StrCat(\n+      \"RingField rank=\", rank, \" chunk_idx=\", chunk_idx, \" subdiv=\", subdiv_idx,\n+      \" sc_idx=\", sc_idx, \" action=\", action);\n+  absl::StrAppend(&rv, \" pass=\", second_pass);\n   strings::StrAppend(&rv, \" do_send=\", do_send, \" do_recv=\", do_recv,\n                      \" is_final=\", is_final, \" recv_is_remote=\", recv_is_remote,\n                      \" recv_dev_idx=\", recv_dev_idx, \" sc_idx=\", sc_idx);\n@@ -396,8 +396,8 @@ string RingAlg::RingField::DebugString() const {\n \n void RingAlg::DispatchSend(RingField* rf, const StatusCallback& done) {\n   DCHECK(rf->do_send);\n-  string send_buf_key = RingAlgBufKey(name_, col_ctx_->exec_key,\n-                                      rf->second_pass, rf->sc_idx, rf->rank);\n+  std::string send_buf_key = RingAlgBufKey(\n+      name_, col_ctx_->exec_key, rf->second_pass, rf->sc_idx, rf->rank);\n   VLOG(3) << \"DispatchSend rank=\" << col_params_->default_rank << \" send key \"\n           << send_buf_key << \" chunk \" << ca_->TBounds(rf->chunk) << \" sc_idx \"\n           << rf->sc_idx;\n@@ -415,7 +415,7 @@ void RingAlg::DispatchSend(RingField* rf, const StatusCallback& done) {\n \n void RingAlg::DispatchRecv(RingField* rf, const StatusCallback& done) {\n   DCHECK(rf->do_recv);\n-  string recv_buf_key =\n+  std::string recv_buf_key =\n       RingAlgBufKey(name_, col_ctx_->exec_key, rf->second_pass, rf->sc_idx,\n                     (rf->rank + (group_size_ - 1)) % group_size_);\n   VLOG(3) << \"DispatchRecv rank=\" << col_params_->default_rank << \" recv key \"\n@@ -434,9 +434,9 @@ void RingAlg::DispatchRecv(RingField* rf, const StatusCallback& done) {\n       col_ctx_->op_ctx->cancellation_manager(), done);\n }\n \n-string RingAlg::FieldState() {\n-  string s = strings::StrCat(\n-      \"Ring\", name_, \" \", strings::Hex(reinterpret_cast<uint64>(this)),\n+std::string RingAlg::FieldState() {\n+  std::string s = strings::StrCat(\n+      \"Ring\", name_, \" \", strings::Hex(reinterpret_cast<uint64_t>(this)),\n       \" exec \", col_ctx_->exec_key, \" step_id=\", col_ctx_->step_id,\n       \" state of all \", rfv_.size(), \" fields:\");\n   for (int i = 0; i < rfv_.size(); ++i) {"
        },
        {
            "sha": "b54da03a01a739cb18ee64cbc67a53617798192d",
            "filename": "tensorflow/core/common_runtime/ring_alg.h",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_alg.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_alg.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fring_alg.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -31,7 +31,7 @@ class Device;\n // for specific collective functions.\n class RingAlg : public CollectiveImplementationInterface {\n  public:\n-  explicit RingAlg(CollectiveType type, const string& name);\n+  explicit RingAlg(CollectiveType type, const std::string& name);\n   ~RingAlg() override {}\n \n   // Establishes the requested number of subdivision permutations based on the\n@@ -63,11 +63,11 @@ class RingAlg : public CollectiveImplementationInterface {\n \n   // Tracks progress of actions on a single subfield of the entire tensor.\n   struct RingField {\n-    int16 chunk_idx;     // major division index\n-    int16 subdiv_idx;    // minor division index\n-    int16 sc_idx;        // subchunk index\n-    int16 rank;          // rank within subdiv permutation\n-    int16 recv_dev_idx;  // dev from which value should be recv'd\n+    int16_t chunk_idx;     // major division index\n+    int16_t subdiv_idx;    // minor division index\n+    int16_t sc_idx;        // subchunk index\n+    int16_t rank;          // rank within subdiv permutation\n+    int16_t recv_dev_idx;  // dev from which value should be recv'd\n     RingFieldAction action;\n     bool second_pass;\n     bool recv_is_remote = false;\n@@ -78,7 +78,7 @@ class RingAlg : public CollectiveImplementationInterface {\n     Tensor chunk;           // alias to field values\n     Tensor tmp_chunk;\n     absl::Status status;\n-    string DebugString() const;\n+    std::string DebugString() const;\n   };\n   virtual void InitRingField(RingField* rf, int chunk_idx, int subdiv_idx,\n                              int field_idx);\n@@ -87,8 +87,8 @@ class RingAlg : public CollectiveImplementationInterface {\n   void DispatchRecv(RingField* rf, const StatusCallback& done);\n \n   // For constructing log messages for debugging.\n-  string FieldState();\n-  string TensorDebugString(const Tensor& tensor);\n+  std::string FieldState();\n+  std::string TensorDebugString(const Tensor& tensor);\n \n   // Producer/Consumer Queue of RingField structs.\n   class PCQueue {\n@@ -104,7 +104,7 @@ class RingAlg : public CollectiveImplementationInterface {\n   };\n \n   const CollectiveType type_;\n-  const string name_;\n+  const std::string name_;\n   std::shared_ptr<CollectiveContext> col_ctx_;\n   const CollectiveParams* col_params_;  // Not owned\n   StatusCallback done_;"
        },
        {
            "sha": "bd85f07aef1840127b5db25c0791622db53ff213",
            "filename": "tensorflow/core/common_runtime/ring_gatherer.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_gatherer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_gatherer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fring_gatherer.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -71,18 +71,18 @@ void RingGatherer::Run(StatusCallback done) {\n   DCHECK_GT(num_subdivs_, 0);\n \n   if (VLOG_IS_ON(1)) {\n-    string buf;\n+    std::string buf;\n     for (int r = 0; r < col_params_->group.members.size(); ++r) {\n       strings::StrAppend(&buf, \"dev \", r, \" : \",\n                          col_params_->group.members[r].device.name(), \"\\n\");\n     }\n     for (int sd = 0;\n          sd < col_params_->instance.impl_details.subdiv_permutations.size();\n          ++sd) {\n-      strings::StrAppend(&buf, \"\\nsubdiv \", sd, \" perm: \");\n+      absl::StrAppend(&buf, \"\\nsubdiv \", sd, \" perm: \");\n       for (auto x :\n            col_params_->instance.impl_details.subdiv_permutations[sd]) {\n-        strings::StrAppend(&buf, x, \", \");\n+        absl::StrAppend(&buf, x, \", \");\n       }\n     }\n     VLOG(1) << \"RingGatherer::Run for device \" << col_ctx_->device_name"
        },
        {
            "sha": "884fb17340c4c0cdbefca8260ee924dadf90569d",
            "filename": "tensorflow/core/common_runtime/ring_gatherer_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_gatherer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_gatherer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fring_gatherer_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -105,7 +105,7 @@ class RingGathererTest : public ::testing::Test {\n       // Confirm that every device terminated with the expected error status.\n       for (int di = 0; di < static_cast<int>(instances_.size()); ++di) {\n         EXPECT_NE(instances_[di]->status_.message().find(\"Deliberate failure\"),\n-                  string::npos);\n+                  std::string::npos);\n       }\n     } else {\n       // Confirm that every device accumulated the same set of correct\n@@ -130,7 +130,7 @@ class RingGathererTest : public ::testing::Test {\n             GenerateEvenSubdivOffsets(test_env->num_devices_per_worker,\n                                       num_subdivs);\n       }\n-      string dev_name = col_params_->group.members[rank].device.name();\n+      std::string dev_name = col_params_->group.members[rank].device.name();\n       TF_CHECK_OK(test_env_->device_mgr->LookupDevice(dev_name, &device_))\n           << \"Couldn't find device \" << dev_name\n           << \" existing devices: \" << test_env_->device_mgr->DebugString();"
        },
        {
            "sha": "3ad099caee9b9b70f4d842b5b305bad7f93074c2",
            "filename": "tensorflow/core/common_runtime/ring_reducer.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_reducer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_reducer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fring_reducer.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -67,18 +67,18 @@ void RingReducer::Run(StatusCallback done) {\n   CHECK_GT(num_subdivs_, 0);\n \n   if (VLOG_IS_ON(1)) {\n-    string buf;\n+    std::string buf;\n     for (int r = 0; r < col_params_->group.members.size(); ++r) {\n       strings::StrAppend(&buf, \"dev \", r, \" : \",\n                          col_params_->group.members[r].device.name(), \"\\n\");\n     }\n     for (int sd = 0;\n          sd < col_params_->instance.impl_details.subdiv_permutations.size();\n          ++sd) {\n-      strings::StrAppend(&buf, \"\\nsubdiv \", sd, \" perm: \");\n+      absl::StrAppend(&buf, \"\\nsubdiv \", sd, \" perm: \");\n       for (auto x :\n            col_params_->instance.impl_details.subdiv_permutations[sd]) {\n-        strings::StrAppend(&buf, x, \", \");\n+        absl::StrAppend(&buf, x, \", \");\n       }\n     }\n     VLOG(1) << \"RingReducer::Run for device \" << col_ctx_->device_name\n@@ -129,9 +129,9 @@ void RingReducer::ContinueAfterInputCopy() {\n     // can be provided to the kernel in host memory?\n     Tensor group_size_val = ca_->Scalar(group_size_);\n     if (col_params_->group.device_type != \"CPU\") {\n-      uint64 safe_alloc_frontier = col_ctx_->device->SafeAllocFrontier(0);\n+      uint64_t safe_alloc_frontier = col_ctx_->device->SafeAllocFrontier(0);\n       AllocationAttributes aa;\n-      std::function<uint64()> freed_by_func = [this, &safe_alloc_frontier]() {\n+      std::function<uint64_t()> freed_by_func = [this, &safe_alloc_frontier]() {\n         safe_alloc_frontier =\n             col_ctx_->device->SafeAllocFrontier(safe_alloc_frontier);\n         return safe_alloc_frontier;"
        },
        {
            "sha": "bedfa64134de51f84a09b4a5834f420a97468ed3",
            "filename": "tensorflow/core/common_runtime/ring_reducer_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_reducer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fring_reducer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fring_reducer_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -138,7 +138,7 @@ class RingReducerTest : public ::testing::Test {\n       // Confirm that every device terminated with the expected error status.\n       for (int di = 0; di < static_cast<int>(instances_.size()); ++di) {\n         EXPECT_NE(instances_[di]->status_.message().find(\"Deliberate failure\"),\n-                  string::npos);\n+                  std::string::npos);\n       }\n     } else {\n       // Confirm that every device computed the same correct reduction value.\n@@ -165,7 +165,7 @@ class RingReducerTest : public ::testing::Test {\n             GenerateEvenSubdivOffsets(test_env->num_devices_per_worker,\n                                       num_subdivs);\n       }\n-      string dev_name = col_params_->group.members[rank].device.name();\n+      std::string dev_name = col_params_->group.members[rank].device.name();\n       TF_CHECK_OK(test_env_->device_mgr->LookupDevice(dev_name, &device_))\n           << \"Couldn't find device \" << dev_name\n           << \" existing devices: \" << test_env_->device_mgr->DebugString();\n@@ -200,7 +200,7 @@ class RingReducerTest : public ::testing::Test {\n   std::unique_ptr<CollectiveTestEnv> test_env_;\n   std::vector<std::unique_ptr<DeviceInstance>> instances_;\n   mutex mu_;\n-  int32 reduce_counter_ TF_GUARDED_BY(mu_) = 0;\n+  int32_t reduce_counter_ TF_GUARDED_BY(mu_) = 0;\n };\n \n class RingReducerInitParamsTest : public ::testing::Test {"
        },
        {
            "sha": "24e7e089784e1730eb985fa15468c3b47d990bb9",
            "filename": "tensorflow/core/common_runtime/scoped_allocator.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n namespace tensorflow {\n \n ScopedAllocator::ScopedAllocator(const Tensor& backing_tensor, int32_t scope_id,\n-                                 const string& name,\n+                                 const std::string& name,\n                                  const absl::Span<const Field> fields,\n                                  int32_t expected_call_count,\n                                  ScopedAllocatorContainer* container)\n@@ -69,7 +69,7 @@ void* ScopedAllocator::AllocateRaw(int32_t field_index, size_t num_bytes) {\n       return nullptr;\n     }\n \n-    int32_t num_fields = static_cast<int32>(fields_.size());\n+    int32_t num_fields = static_cast<int32_t>(fields_.size());\n     if (field_index >= num_fields) {\n       LOG(ERROR) << \"ScopedAllocator \" << name_\n                  << \" received unexpected field number \" << field_index;\n@@ -228,8 +228,8 @@ void ScopedAllocatorInstance::DeallocateRaw(void* p) {\n   if (del) delete this;\n }\n \n-string ScopedAllocatorInstance::Name() {\n-  return strings::StrCat(scoped_allocator_->name(), \"_field_\", field_index_);\n+std::string ScopedAllocatorInstance::Name() {\n+  return absl::StrCat(scoped_allocator_->name(), \"_field_\", field_index_);\n }\n \n }  // namespace tensorflow"
        },
        {
            "sha": "8c894372fbee15ba514031e058fae4aaa1d1c316",
            "filename": "tensorflow/core/common_runtime/scoped_allocator.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -33,7 +33,7 @@ class ScopedAllocator {\n   // A subrange of the TensorBuffer associated with this object that\n   // will be the backing memory for one aliased tensor.\n   struct Field {\n-    int32 scope_id;\n+    int32_t scope_id;\n     size_t offset;\n     size_t bytes_requested;\n     size_t bytes_allocated;\n@@ -71,13 +71,13 @@ class ScopedAllocator {\n   void DeallocateRaw(void* p) TF_LOCKS_EXCLUDED(mu_);\n   Tensor backing_tensor_;\n   TensorBuffer* tbuf_;\n-  int32 id_;\n+  int32_t id_;\n   std::string name_;\n   ScopedAllocatorContainer* container_;\n   std::vector<Field> fields_;\n   mutex mu_;\n-  int32 expected_call_count_ TF_GUARDED_BY(mu_);\n-  int32 live_alloc_count_ TF_GUARDED_BY(mu_);\n+  int32_t expected_call_count_ TF_GUARDED_BY(mu_);\n+  int32_t live_alloc_count_ TF_GUARDED_BY(mu_);\n };\n \n // An Allocator that will return a pointer into the backing buffer of\n@@ -117,7 +117,7 @@ class ScopedAllocatorInstance : public Allocator {\n  private:\n   mutex mu_;\n   ScopedAllocator* scoped_allocator_;\n-  int32 field_index_;\n+  int32_t field_index_;\n   bool allocated_ TF_GUARDED_BY(mu_);\n   bool deallocated_ TF_GUARDED_BY(mu_);\n   bool in_table_ TF_GUARDED_BY(mu_);"
        },
        {
            "sha": "d4fe07b5f27d2b7110cf87560225799ba2152e28",
            "filename": "tensorflow/core/common_runtime/scoped_allocator_mgr.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator_mgr.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator_mgr.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator_mgr.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -20,7 +20,8 @@ limitations under the License.\n namespace tensorflow {\n \n absl::Status ScopedAllocatorContainer::AddScopedAllocator(\n-    const Tensor& backing_tensor, int32_t scope_id, const string& scope_name,\n+    const Tensor& backing_tensor, int32_t scope_id,\n+    const std::string& scope_name,\n     const absl::Span<const ScopedAllocator::Field>& fields,\n     int32_t expected_call_count) {\n   VLOG(1) << \"AddScopedAllocator \" << mgr_->device_name()\n@@ -152,7 +153,7 @@ ScopedAllocatorContainer* ScopedAllocatorMgr::GetContainer(int64_t step_id) {\n \n absl::Status ScopedAllocatorMgr::AddScopedAllocator(\n     const Tensor& backing_tensor, int64_t step_id, int32_t scope_id,\n-    const string& scope_name,\n+    const std::string& scope_name,\n     const absl::Span<const ScopedAllocator::Field>& fields,\n     int32_t expected_call_count) {\n   ScopedAllocatorContainer* sac = GetContainer(step_id);\n@@ -164,7 +165,7 @@ absl::Status ScopedAllocatorMgr::AddScopedAllocator(\n size_t ScopedAllocatorMgr::PopulateFields(\n     int32_t scope_id, const absl::Span<const TensorShape>& shapes,\n     const DataType dtype, std::vector<ScopedAllocator::Field>* fields) {\n-  const int32_t num_fields = static_cast<int32>(shapes.size());\n+  const int32_t num_fields = static_cast<int32_t>(shapes.size());\n   fields->resize(num_fields);\n   // At the end of iteration `i`, `offset` points to the offset from the start\n   // of the backing buffer until the end of `field[i].bytes_allocated`.  This"
        },
        {
            "sha": "22924a7005e892b634bb6661091af18b559289bd",
            "filename": "tensorflow/core/common_runtime/scoped_allocator_mgr.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator_mgr.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator_mgr.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fscoped_allocator_mgr.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -54,7 +54,7 @@ class ScopedAllocatorContainer : public core::RefCounted {\n   int64_t step_id_;\n   mutex mu_;\n   struct SAField {\n-    int32 field_index;\n+    int32_t field_index;\n     union {\n       ScopedAllocator* scoped_allocator;\n       ScopedAllocatorInstance* instance;\n@@ -67,7 +67,7 @@ class ScopedAllocatorContainer : public core::RefCounted {\n         : field_index(ScopedAllocator::kBackingIndex),\n           scoped_allocator(nullptr) {}\n   };\n-  std::unordered_map<int32, SAField> allocators_ TF_GUARDED_BY(mu_);\n+  std::unordered_map<int32_t, SAField> allocators_ TF_GUARDED_BY(mu_);\n };\n \n // At most one of these exists per device."
        },
        {
            "sha": "59deffca41c19c7b2612662fdc6bcf4e1ee380d1",
            "filename": "tensorflow/core/common_runtime/session.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsession.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -36,27 +36,29 @@ Session::Session() {}\n \n Session::~Session() {}\n \n-absl::Status Session::Run(const RunOptions& run_options,\n-                          const std::vector<std::pair<string, Tensor> >& inputs,\n-                          const std::vector<string>& output_tensor_names,\n-                          const std::vector<string>& target_tensor_names,\n-                          std::vector<Tensor>* outputs,\n-                          RunMetadata* run_metadata) {\n+absl::Status Session::Run(\n+    const RunOptions& run_options,\n+    const std::vector<std::pair<std::string, Tensor> >& inputs,\n+    const std::vector<std::string>& output_tensor_names,\n+    const std::vector<std::string>& target_tensor_names,\n+    std::vector<Tensor>* outputs, RunMetadata* run_metadata) {\n   return errors::Unimplemented(\n       \"Run with options is not supported for this session.\");\n }\n \n-absl::Status Session::PRunSetup(const std::vector<string>& input_names,\n-                                const std::vector<string>& output_names,\n-                                const std::vector<string>& target_nodes,\n-                                string* handle) {\n+absl::Status Session::PRunSetup(const std::vector<std::string>& input_names,\n+                                const std::vector<std::string>& output_names,\n+                                const std::vector<std::string>& target_nodes,\n+                                std::string* handle) {\n   return errors::Unimplemented(\n       \"Partial run is not supported for this session.\");\n }\n \n absl::Status Session::PRun(\n-    const string& handle, const std::vector<std::pair<string, Tensor> >& inputs,\n-    const std::vector<string>& output_names, std::vector<Tensor>* outputs) {\n+    const std::string& handle,\n+    const std::vector<std::pair<std::string, Tensor> >& inputs,\n+    const std::vector<std::string>& output_names,\n+    std::vector<Tensor>* outputs) {\n   return errors::Unimplemented(\n       \"Partial run is not supported for this session.\");\n }\n@@ -96,7 +98,7 @@ absl::Status NewSession(const SessionOptions& options, Session** out_session) {\n }\n \n absl::Status Reset(const SessionOptions& options,\n-                   const std::vector<string>& containers) {\n+                   const std::vector<std::string>& containers) {\n   SessionFactory* factory;\n   TF_RETURN_IF_ERROR(SessionFactory::GetFactory(options, &factory));\n   return factory->Reset(options, containers);"
        },
        {
            "sha": "fc28ab4e05e887d0a237978564b145b34eccd86e",
            "filename": "tensorflow/core/common_runtime/session_factory.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_factory.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_factory.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_factory.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -33,15 +33,15 @@ static mutex* get_session_factory_lock() {\n   return &session_factory_lock;\n }\n \n-typedef std::unordered_map<string, SessionFactory*> SessionFactories;\n+typedef std::unordered_map<std::string, SessionFactory*> SessionFactories;\n SessionFactories* session_factories() {\n   static SessionFactories* factories = new SessionFactories;\n   return factories;\n }\n \n }  // namespace\n \n-void SessionFactory::Register(const string& runtime_type,\n+void SessionFactory::Register(const std::string& runtime_type,\n                               SessionFactory* factory) {\n   mutex_lock l(*get_session_factory_lock());\n   if (!session_factories()->insert({runtime_type, factory}).second) {\n@@ -51,25 +51,25 @@ void SessionFactory::Register(const string& runtime_type,\n }\n \n namespace {\n-const string RegisteredFactoriesErrorMessageLocked() {\n-  std::vector<string> factory_types;\n+const std::string RegisteredFactoriesErrorMessageLocked() {\n+  std::vector<std::string> factory_types;\n   for (const auto& session_factory : *session_factories()) {\n     factory_types.push_back(session_factory.first);\n   }\n-  return strings::StrCat(\"Registered factories are {\",\n-                         absl::StrJoin(factory_types, \", \"), \"}.\");\n+  return absl::StrCat(\"Registered factories are {\",\n+                      absl::StrJoin(factory_types, \", \"), \"}.\");\n }\n-string SessionOptionsToString(const SessionOptions& options) {\n-  return strings::StrCat(\"target: \\\"\", options.target,\n-                         \"\\\" config: \", options.config.ShortDebugString());\n+std::string SessionOptionsToString(const SessionOptions& options) {\n+  return absl::StrCat(\"target: \\\"\", options.target,\n+                      \"\\\" config: \", options.config.ShortDebugString());\n }\n }  // namespace\n \n absl::Status SessionFactory::GetFactory(const SessionOptions& options,\n                                         SessionFactory** out_factory) {\n   mutex_lock l(*get_session_factory_lock());  // could use reader lock\n \n-  std::vector<std::pair<string, SessionFactory*>> candidate_factories;\n+  std::vector<std::pair<std::string, SessionFactory*>> candidate_factories;\n   for (const auto& session_factory : *session_factories()) {\n     if (session_factory.second->AcceptsOptions(options)) {\n       VLOG(2) << \"SessionFactory type \" << session_factory.first\n@@ -93,7 +93,7 @@ absl::Status SessionFactory::GetFactory(const SessionOptions& options,\n     // the number of sessions grows.\n     // TODO(mrry): Consider providing a system-default fallback option\n     // in this case.\n-    std::vector<string> factory_types;\n+    std::vector<std::string> factory_types;\n     factory_types.reserve(candidate_factories.size());\n     for (const auto& candidate_factory : candidate_factories) {\n       factory_types.push_back(candidate_factory.first);"
        },
        {
            "sha": "3c9d08db121c68e8006e14f43cdf3735e57b7603",
            "filename": "tensorflow/core/common_runtime/session_factory.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_factory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_factory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_factory.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -61,12 +61,13 @@ class SessionFactory {\n   //\n   // Sessions that support resource containers should override this function.\n   virtual absl::Status Reset(const SessionOptions& options,\n-                             const std::vector<string>& containers) {\n+                             const std::vector<std::string>& containers) {\n     return errors::Unimplemented(\"Reset()\");\n   }\n \n   virtual ~SessionFactory() {}\n-  static void Register(const string& runtime_type, SessionFactory* factory);\n+  static void Register(const std::string& runtime_type,\n+                       SessionFactory* factory);\n   static absl::Status GetFactory(const SessionOptions& options,\n                                  SessionFactory** out_factory);\n };"
        },
        {
            "sha": "5a236367357099548e470d453d89974b667f5761",
            "filename": "tensorflow/core/common_runtime/session_state.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsession_state.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -23,7 +23,8 @@ namespace tensorflow {\n // kTensorHandleResourceTypeName.\n const char* SessionState::kTensorHandleResourceTypeName = \"TensorHandle\";\n \n-absl::Status SessionState::GetTensor(const string& handle, Tensor* tensor) {\n+absl::Status SessionState::GetTensor(const std::string& handle,\n+                                     Tensor* tensor) {\n   mutex_lock l(state_lock_);\n   auto it = tensors_.find(handle);\n   if (it == tensors_.end()) {\n@@ -34,7 +35,7 @@ absl::Status SessionState::GetTensor(const string& handle, Tensor* tensor) {\n   return absl::OkStatus();\n }\n \n-absl::Status SessionState::AddTensor(const string& handle,\n+absl::Status SessionState::AddTensor(const std::string& handle,\n                                      const Tensor& tensor) {\n   mutex_lock l(state_lock_);\n   if (!tensors_.insert({handle, tensor}).second) {\n@@ -44,7 +45,7 @@ absl::Status SessionState::AddTensor(const string& handle,\n   return absl::OkStatus();\n }\n \n-absl::Status SessionState::DeleteTensor(const string& handle) {\n+absl::Status SessionState::DeleteTensor(const std::string& handle) {\n   mutex_lock l(state_lock_);\n   if (tensors_.erase(handle) == 0) {\n     return errors::InvalidArgument(\"Failed to delete a tensor with handle '\",\n@@ -58,7 +59,7 @@ int64_t SessionState::GetNewId() {\n   return tensor_id_++;\n }\n \n-absl::Status TensorStore::AddTensor(const string& name,\n+absl::Status TensorStore::AddTensor(const std::string& name,\n                                     const TensorAndKey& tk) {\n   mutex_lock l(lock_);\n   if (!tensors_.insert({name, tk}).second) {\n@@ -69,18 +70,18 @@ absl::Status TensorStore::AddTensor(const string& name,\n   return absl::OkStatus();\n }\n \n-absl::Status TensorStore::SaveTensors(const std::vector<string>& output_names,\n-                                      SessionState* session_state) {\n+absl::Status TensorStore::SaveTensors(\n+    const std::vector<std::string>& output_names, SessionState* session_state) {\n   mutex_lock l(lock_);\n   if (!tensors_.empty()) {\n     // Save only the tensors in output_names in the session.\n-    for (const string& name : output_names) {\n+    for (const std::string& name : output_names) {\n       TensorId id(ParseTensorName(name));\n-      const string op_name(id.first);\n+      const std::string op_name(id.first);\n       auto it = tensors_.find(op_name);\n       if (it != tensors_.end()) {\n         // Save the tensor to the session state.\n-        string key = it->second.GetHandle(op_name);\n+        std::string key = it->second.GetHandle(op_name);\n         TF_RETURN_IF_ERROR(session_state->AddTensor(key, it->second.tensor));\n       }\n     }"
        },
        {
            "sha": "bc4787864315b0ac780873c65963d4069e77228e",
            "filename": "tensorflow/core/common_runtime/shape_refiner.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -162,15 +162,15 @@ absl::Status ShapeRefiner::InferShapesForFunction(\n     const FunctionDef* function_def, AttrSlice attributes,\n     InferenceContext* outer_context) {\n   const Graph* graph;\n-  const string& fname = function_def->signature().name();\n+  const std::string& fname = function_def->signature().name();\n   auto it = functions_.find(fname);\n   if (it != functions_.end()) {\n     graph = it->second.get();\n   } else {\n     InstantiationResult result;\n     TF_RETURN_IF_ERROR(InstantiateFunction(\n         *function_def, attributes,\n-        [this](const string& op, const OpDef** sig) {\n+        [this](const std::string& op, const OpDef** sig) {\n           return this->function_library_->LookUpOpDef(op, sig);\n         },\n         &result));\n@@ -476,7 +476,7 @@ absl::Status ShapeRefiner::EvaluateConstantIntScalarEdge(\n           scalar.NumElements());\n     }\n     if (scalar.dtype() == DT_INT32) {\n-      *result = scalar.scalar<int32>()();\n+      *result = scalar.scalar<int32_t>()();\n     } else {\n       if (scalar.dtype() != DT_INT64) {\n         return errors::InvalidArgument(\n@@ -515,7 +515,7 @@ absl::Status ShapeRefiner::ConstantPartialShape(\n           \"of '-1' is required to represent an unknown shape.\");\n     }\n     if (t.dims() == 0) {\n-      if (t.dtype() == DT_INT32 && t.scalar<int32>()() == -1) {\n+      if (t.dtype() == DT_INT32 && t.scalar<int32_t>()() == -1) {\n         *result = target_context->UnknownShape();\n         return absl::OkStatus();\n       } else if (t.dtype() == DT_INT64 && t.scalar<int64_t>()() == -1) {\n@@ -531,7 +531,7 @@ absl::Status ShapeRefiner::ConstantPartialShape(\n \n   TF_RETURN_IF_ERROR(src_context->WithRank(src_shape, 1, &src_shape));\n \n-  const string& src_op = input_edge->src()->type_string();\n+  const std::string& src_op = input_edge->src()->type_string();\n   if (src_context->Value(src_context->Dim(src_shape, 0)) == 0) {\n     // Source tensor is a vector of length 0, so the shape it\n     // represents is as scalar."
        },
        {
            "sha": "f67e5dd4b388e7bae3d661562e80603f90bf6354",
            "filename": "tensorflow/core/common_runtime/shape_refiner.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -87,7 +87,7 @@ class ShapeRefiner {\n   }\n \n   // Getters and setters for graph_def_version_.\n-  int32 graph_def_version() const { return graph_def_version_; }\n+  int32_t graph_def_version() const { return graph_def_version_; }\n   void set_graph_def_version(int32_t version) { graph_def_version_ = version; }\n \n   void set_require_shape_inference_fns(bool require_shape_inference_fns) {\n@@ -250,7 +250,7 @@ class ShapeRefiner {\n       shape_inference::InferenceContext* context,\n       shape_inference::InferenceContext* outer_context = nullptr);\n \n-  int32 graph_def_version_;\n+  int32_t graph_def_version_;\n   const OpRegistryInterface* const ops_registry_;\n \n   // The lifetime of the tensors are bound to the runner, so it should be the"
        },
        {
            "sha": "580a987b3ccffd00fb3f5663c6b13d2c1191efa3",
            "filename": "tensorflow/core/common_runtime/shape_refiner_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 13,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -281,9 +281,10 @@ TEST_F(ShapeRefinerTest, ExtractConstantSubgraphMultiOutput) {\n   // input_tensor from the shape function.\n   {\n     Scope root = Scope::NewRootScope();\n-    auto small = ops::Const(root, {static_cast<int32>(1), TensorShape({1, 1})});\n+    auto small =\n+        ops::Const(root, {static_cast<int32_t>(1), TensorShape({1, 1})});\n     auto large = ops::Const(\n-        root, {static_cast<int32>(2), TensorShape({4, kMaxTensorSize / 2})});\n+        root, {static_cast<int32_t>(2), TensorShape({4, kMaxTensorSize / 2})});\n     Node* multi;\n     TF_ASSERT_OK(NodeBuilder(\"MI\", \"MultiIdentity\")\n                      .Input(std::vector<NodeBuilder::NodeOut>{small.node(),\n@@ -313,7 +314,7 @@ TEST_F(ShapeRefinerTest, ExtractConstantSubgraphMultiOutput) {\n     // The add adds 1 and 2 together, and its output has kMaxTensorSize*2\n     // elements.\n     shape_inference::InferenceContext* ctx = m.GetContext(shape_v2);\n-    EXPECT_EQ(strings::StrCat(\"[\", kMaxTensorSize * 2 * 3, \"]\"),\n+    EXPECT_EQ(absl::StrCat(\"[\", kMaxTensorSize * 2 * 3, \"]\"),\n               ctx->DebugString(ctx->output(0)));\n   }\n }\n@@ -380,7 +381,7 @@ REGISTER_OP(\"ShapeData\")\n       std::vector<shape_inference::DimensionHandle> dims;\n       dims.reserve(shape_data->NumElements());\n       for (int i = 0; i < shape_data->NumElements(); ++i) {\n-        dims.emplace_back(c->MakeDim(shape_data->flat<int32>()(i)));\n+        dims.emplace_back(c->MakeDim(shape_data->flat<int32_t>()(i)));\n       }\n \n       c->set_output(0, c->MakeShape(dims));\n@@ -418,7 +419,7 @@ REGISTER_OP(\"ShapeVectorForAllElements\")\n       }\n       int64_t total = 0;\n       for (int i = 0; i < shape_data->NumElements(); ++i) {\n-        total += shape_data->flat<int32>()(i);\n+        total += shape_data->flat<int32_t>()(i);\n       }\n \n       c->set_output(0, c->Vector(total));\n@@ -487,7 +488,8 @@ TEST_F(ShapeRefinerTest, PropagateShapeAcrossTensorContentInt64) {\n \n   // Create variable 2x4 tensor.\n   auto input = ops::Variable(\n-      root, {2, 4, static_cast<int64_t>(std::numeric_limits<int32>::max()) * 2},\n+      root,\n+      {2, 4, static_cast<int64_t>(std::numeric_limits<int32_t>::max()) * 2},\n       DT_INT64);\n \n   // Shape is a vector of 2 elements (2,4)\n@@ -521,7 +523,8 @@ TEST_F(ShapeRefinerTest, PropagateShapeAcrossTensorContentInt32Overflow) {\n \n   // Create variable 2x4 tensor.\n   auto input = ops::Variable(\n-      root, {2, 4, static_cast<int64_t>(std::numeric_limits<int32>::max()) * 2},\n+      root,\n+      {2, 4, static_cast<int64_t>(std::numeric_limits<int32_t>::max()) * 2},\n       DT_INT32);\n \n   // Shape is a vector of 2 elements (2,4)\n@@ -607,7 +610,7 @@ TEST_F(ShapeRefinerTest, PropagateSizeAcrossTensorContentInt64) {\n   auto input = ops::Variable(\n       root,\n       {1, 2, 3, 4, 5,\n-       static_cast<int64_t>(std::numeric_limits<int32>::max()) * 2},\n+       static_cast<int64_t>(std::numeric_limits<int32_t>::max()) * 2},\n       DT_INT64);\n \n   // 5! * int32_max_value * 2.\n@@ -638,7 +641,7 @@ TEST_F(ShapeRefinerTest, PropagateSizeAcrossTensorContentInt32Overflow) {\n   auto input = ops::Variable(\n       root,\n       {1, 2, 3, 4, 5,\n-       static_cast<int64_t>(std::numeric_limits<int32>::max()) * 2},\n+       static_cast<int64_t>(std::numeric_limits<int32_t>::max()) * 2},\n       DT_INT32);\n \n   // 5!.\n@@ -845,7 +848,7 @@ absl::Status PartialTensorAsShapeShapeFn(shape_inference::InferenceContext* c) {\n     return absl::OkStatus();\n   }\n   TF_RETURN_IF_ERROR(\n-      c->MakeShapeFromTensorShape(TensorShape({t->flat<int32>()(0)}), &out));\n+      c->MakeShapeFromTensorShape(TensorShape({t->flat<int32_t>()(0)}), &out));\n   c->set_output(0, out);\n   return absl::OkStatus();\n }\n@@ -967,10 +970,10 @@ TEST_F(ShapeRefinerTest, ConstantValueAsShape_PackInt32) {\n \n   InputList inputs{\n       // clang-format off\n-      Input(ops::Const<int32>(root, 10)),\n-      Input(ops::Const<int32>(root, 20)),\n+      Input(ops::Const<int32_t>(root, 10)),\n+      Input(ops::Const<int32_t>(root, 20)),\n       Input(Output(scalar_non_const)),\n-      Input(ops::Const<int32>(root, 40)),\n+      Input(ops::Const<int32_t>(root, 40)),\n   };  // clang-format on\n   auto pack = ops::Stack(root, inputs);\n   TF_ASSERT_OK(root.status());"
        },
        {
            "sha": "3855c6a3d6cfce93f461bc0c67af56b4d3a3a03f",
            "filename": "tensorflow/core/common_runtime/simple_propagator_state.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsimple_propagator_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsimple_propagator_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsimple_propagator_state.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -35,7 +35,7 @@ SimplePropagatorState::SimplePropagatorState(\n       vlog_(vlog || VLOG_IS_ON(1)),\n       input_tensors_(finfo.total_inputs),\n       pending_(\n-          new std::atomic<int32>[immutable_state.graph_view().num_nodes()]),\n+          new std::atomic<int32_t>[immutable_state.graph_view().num_nodes()]),\n       active_(vlog_ ? new std::vector<bool>(\n                           immutable_state.graph_view().num_nodes())\n                     : nullptr),"
        },
        {
            "sha": "8ef9775f93aee80656183215934e5761d8f58a79",
            "filename": "tensorflow/core/common_runtime/simple_propagator_state.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsimple_propagator_state.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsimple_propagator_state.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsimple_propagator_state.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -167,7 +167,7 @@ class SimplePropagatorState {\n   // is never concurrent access to the same entry.\n   std::vector<Entry> input_tensors_;\n \n-  std::unique_ptr<std::atomic<int32>[]> pending_;\n+  std::unique_ptr<std::atomic<int32_t>[]> pending_;\n \n   // If `vlog_` is true, this stores a bit vector of active nodes, indexed by\n   // node ID."
        },
        {
            "sha": "5eb084d0def6295bd63654cd25e2153e9bb4b06a",
            "filename": "tensorflow/core/common_runtime/simplify_ici_dummy_variables_pass.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsimplify_ici_dummy_variables_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsimplify_ici_dummy_variables_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsimplify_ici_dummy_variables_pass.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -109,12 +109,12 @@ void RedirectEdge(Graph* graph, Node* old_src_node, Node* dst_node,\n }\n \n // Find the corresponding host device name from the TPU device name.\n-string GetHostDeviceName(Node* tpu_node) {\n+std::string GetHostDeviceName(Node* tpu_node) {\n   auto device_name = tpu_node->requested_device();\n   if (device_name.empty()) device_name = tpu_node->assigned_device_name();\n   DeviceNameUtils::ParsedName parsed_device_name;\n   DeviceNameUtils::ParseFullName(device_name, &parsed_device_name);\n-  string host_device_name = DeviceNameUtils::FullName(\n+  std::string host_device_name = DeviceNameUtils::FullName(\n       parsed_device_name.job, parsed_device_name.replica,\n       parsed_device_name.task, /*type=*/\"CPU\", /*id=*/0);\n   return host_device_name;\n@@ -143,7 +143,8 @@ int GetTPUTaskId(Node* tpu_node) {\n // Build the fill op. Its value is 0 and the fill op is put on the host device\n // with the same task id as the TPUExecute node.\n Node* BuildFillOp(GraphDefBuilder::Options& bopts, Node* tpu_node,\n-                  Node* in_node, int input_index, string host_device_name) {\n+                  Node* in_node, int input_index,\n+                  std::string host_device_name) {\n   // Find the output_shape vector\n   auto output_shape_vec = GetOutputShapeVec(in_node);\n   if (!output_shape_vec.has_value()) return nullptr;\n@@ -191,7 +192,7 @@ absl::Status ReplaceIciDummyVariables(Graph* graph, int input_index,\n       continue;\n     }\n \n-    string host_device_name = GetHostDeviceName(tpu_node);\n+    std::string host_device_name = GetHostDeviceName(tpu_node);\n \n     // If the node corresponding to host_device_name is already in the graph,\n     // replace the edge from in_node to tpu_node with the edge from"
        },
        {
            "sha": "c737d274fbcd6451942c306d60c5caf6b681cc96",
            "filename": "tensorflow/core/common_runtime/single_threaded_executor.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsingle_threaded_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsingle_threaded_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsingle_threaded_executor.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -65,8 +65,8 @@ namespace {\n typedef absl::InlinedVector<TensorValue, 4UL> TensorValueVec;\n typedef absl::InlinedVector<AllocatorAttributes, 4UL> AllocatorAttributeVec;\n \n-static const string& kSingleThreadedExecutor =\n-    *new string(\"SINGLE_THREADED_EXECUTOR\");\n+static const std::string& kSingleThreadedExecutor =\n+    *new std::string(\"SINGLE_THREADED_EXECUTOR\");\n \n class SingleThreadedExecutorImpl : public Executor {\n  public:"
        },
        {
            "sha": "b081e17d86a97860c36afc3c17c711118867cec3",
            "filename": "tensorflow/core/common_runtime/single_threaded_executor_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsingle_threaded_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fsingle_threaded_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fsingle_threaded_executor_test.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -170,8 +170,9 @@ float V(const Tensor& tensor) {\n   return tensor.scalar<float>()();\n }\n \n-Rendezvous::ParsedKey Key(const string& sender, const uint64 incarnation,\n-                          const string& receiver, const string& name) {\n+Rendezvous::ParsedKey Key(const std::string& sender, const uint64_t incarnation,\n+                          const std::string& receiver,\n+                          const std::string& name) {\n   Rendezvous::ParsedKey result;\n   TF_CHECK_OK(\n       Rendezvous::ParseKey(Rendezvous::CreateKey(sender, incarnation, receiver,\n@@ -363,8 +364,8 @@ void BM_executor(::testing::benchmark::State& state) {\n   Graph* g = new Graph(OpRegistry::Global());\n   random::PhiloxRandom philox(1729, 17);\n   random::SimplePhilox rand(&philox);\n-  uint64 cur = 0;\n-  uint32 r = 1 + rand.Rand32() % width;\n+  uint64_t cur = 0;\n+  uint32_t r = 1 + rand.Rand32() % width;\n   std::vector<Node*> ready_nodes;\n   for (int i = 0; i < r; ++i) {\n     ready_nodes.push_back(test::graph::NoOp(g, {}));\n@@ -392,7 +393,7 @@ void BM_executor(::testing::benchmark::State& state) {\n   test::Benchmark(\"cpu\", g, nullptr, nullptr, nullptr,\n                   \"SINGLE_THREADED_EXECUTOR\", /*old_benchmark_api=*/false)\n       .Run(state);\n-  state.SetLabel(strings::StrCat(\"Nodes = \", cur));\n+  state.SetLabel(absl::StrCat(\"Nodes = \", cur));\n   state.SetItemsProcessed(cur * static_cast<int64_t>(state.iterations()));\n }\n \n@@ -424,7 +425,7 @@ void BM_const_identity(::testing::benchmark::State& state) {\n                   \"SINGLE_THREADED_EXECUTOR\",\n                   /*old_benchmark_api=*/false)\n       .Run(state);\n-  state.SetLabel(strings::StrCat(\"Nodes = \", (1 + outputs_per_const) * width));\n+  state.SetLabel(absl::StrCat(\"Nodes = \", (1 + outputs_per_const) * width));\n   state.SetItemsProcessed((1 + outputs_per_const) * width *\n                           static_cast<int64_t>(state.iterations()));\n }"
        },
        {
            "sha": "610efbdadb7dc86e056f7eac6b7fec88af182023",
            "filename": "tensorflow/core/common_runtime/stats_publisher_interface.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fstats_publisher_interface.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fstats_publisher_interface.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fstats_publisher_interface.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -43,7 +43,8 @@ class NoOpStatsPublisher : public StatsPublisherInterface {\n                              function_records) override {}\n \n   std::unique_ptr<ProfileHandler> GetProfileHandler(\n-      uint64 step, int64_t execution_count, const RunOptions& ropts) override {\n+      uint64_t step, int64_t execution_count,\n+      const RunOptions& ropts) override {\n     return nullptr;\n   }\n \n@@ -74,7 +75,7 @@ StatsPublisherFactory StatsPublisherInterface::GetStatsPublisherFactory() {\n }\n \n std::unique_ptr<StatsPublisherInterface> CreateNoOpStatsPublisher(\n-    const string& session, const BuildGraphOptions& bopts,\n+    const std::string& session, const BuildGraphOptions& bopts,\n     const SessionOptions& sopts) {\n   return std::unique_ptr<StatsPublisherInterface>(new NoOpStatsPublisher);\n }"
        },
        {
            "sha": "2f0e3221be97cbb0fe39f4e45f55800bc2e83fd2",
            "filename": "tensorflow/core/common_runtime/stats_publisher_interface.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fstats_publisher_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fstats_publisher_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fstats_publisher_interface.h?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -61,7 +61,7 @@ class StatsPublisherInterface {\n   //\n   // This method may return a null pointer, if no handler was created.\n   virtual std::unique_ptr<ProfileHandler> GetProfileHandler(\n-      uint64 step, int64_t execution_count, const RunOptions& ropts) = 0;\n+      uint64_t step, int64_t execution_count, const RunOptions& ropts) = 0;\n \n   virtual ~StatsPublisherInterface() {}\n \n@@ -77,7 +77,7 @@ class StatsPublisherInterface {\n };\n \n std::unique_ptr<StatsPublisherInterface> CreateNoOpStatsPublisher(\n-    const string& session, const BuildGraphOptions& bopts,\n+    const std::string& session, const BuildGraphOptions& bopts,\n     const SessionOptions& sopts);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "cc32e668309402e0ce63e4f51a5d0c41efe4a874",
            "filename": "tensorflow/core/common_runtime/step_stats_collector.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 34,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fstep_stats_collector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/069753c571ed000340ce886060f4f4b173c82c7e/tensorflow%2Fcore%2Fcommon_runtime%2Fstep_stats_collector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fstep_stats_collector.cc?ref=069753c571ed000340ce886060f4f4b173c82c7e",
            "patch": "@@ -36,7 +36,7 @@ const int kMaxAllocReportNodes = 100;\n const float kMaxAllocReportFraction = 0.99;\n \n struct AllocStats {\n-  std::map<int64_t, std::vector<string>> nodes_by_size;\n+  std::map<int64_t, std::vector<std::string>> nodes_by_size;\n   int64_t total_bytes = 0;\n   int64_t total_nodes = 0;\n };\n@@ -65,39 +65,39 @@ NodeExecStatsWrapper::NodeExecStatsWrapper(\n       node_(node),\n       step_stats_collector_(step_stats_collector) {}\n \n-void NodeExecStatsWrapper::Done(const string& device) {\n+void NodeExecStatsWrapper::Done(const std::string& device) {\n   // TODO(tucker): merge with the DetailText function in session.cc in a common\n   // location.\n   DCHECK(node_);\n-  string memory;\n+  std::string memory;\n   for (auto& all : stats_->memory()) {\n     int64_t tot = all.total_bytes();\n     if (tot >= 0.1 * 1048576.0) {\n       int64_t peak = all.peak_bytes();\n       if (peak > 0) {\n         memory =\n-            strings::StrCat(memory, \"[\", all.allocator_name(),\n-                            strings::Printf(\" %.1fMB %.1fMB] \", tot / 1048576.0,\n-                                            peak / 1048576.0));\n+            absl::StrCat(memory, \"[\", all.allocator_name(),\n+                         strings::Printf(\" %.1fMB %.1fMB] \", tot / 1048576.0,\n+                                         peak / 1048576.0));\n       } else {\n-        memory = strings::StrCat(memory, \"[\", all.allocator_name(),\n-                                 strings::Printf(\" %.1fMB] \", tot / 1048576.0));\n+        memory = absl::StrCat(memory, \"[\", all.allocator_name(),\n+                              strings::Printf(\" %.1fMB] \", tot / 1048576.0));\n       }\n     }\n   }\n   const AttrSlice attrs(*node_);\n-  string text;\n+  std::string text;\n   if (IsSend(node_)) {\n-    string tensor_name;\n+    std::string tensor_name;\n     TF_CHECK_OK(GetNodeAttr(attrs, \"tensor_name\", &tensor_name));\n-    string recv_device;\n+    std::string recv_device;\n     TF_CHECK_OK(GetNodeAttr(attrs, \"recv_device\", &recv_device));\n     text = strings::StrCat(memory, node_->name(), \" = \", node_->op(), \"(\",\n                            tensor_name, \" @\", recv_device, \")\");\n   } else if (IsRecv(node_)) {\n-    string tensor_name;\n+    std::string tensor_name;\n     TF_CHECK_OK(GetNodeAttr(attrs, \"tensor_name\", &tensor_name));\n-    string send_device;\n+    std::string send_device;\n     TF_CHECK_OK(GetNodeAttr(attrs, \"send_device\", &send_device));\n     text = strings::StrCat(memory, node_->name(), \" = \", node_->op(), \"(\",\n                            tensor_name, \" @\", send_device, \")\");\n@@ -197,7 +197,7 @@ void NodeExecStatsWrapper::Finalize() {\n StepStatsCollector::StepStatsCollector(StepStats* step_stats)\n     : finalized_(false), step_stats_(step_stats) {}\n \n-static int ExtractGpuWithStreamAll(string device_name) {\n+static int ExtractGpuWithStreamAll(std::string device_name) {\n   // Check if the device name matches the \".*gpu:(\\\\d+)/stream:all$\" regexp,\n   // and if it does return the stream index (always positive). If it doesn't\n   // return -1.\n@@ -220,15 +220,15 @@ static int ExtractGpuWithStreamAll(string device_name) {\n   } else {\n     // Convert the captured string into an integer. But first we need to put\n     // the digits back in order\n-    string ordered_capture(capture);\n+    std::string ordered_capture(capture);\n     std::reverse(ordered_capture.begin(), ordered_capture.end());\n     int gpu_id;\n     CHECK(absl::SimpleAtoi(ordered_capture, &gpu_id));\n     return gpu_id;\n   }\n }\n \n-static int ExtractGpuWithoutStream(string device_name) {\n+static int ExtractGpuWithoutStream(std::string device_name) {\n   // Check if the device name matches the \".*gpu:(\\\\d+)$\" regexp,\n   // and if it does return the stream index (always positive). If it doesn't\n   // return -1.\n@@ -249,7 +249,7 @@ static int ExtractGpuWithoutStream(string device_name) {\n   } else {\n     // Convert the captured string into an integer. But first we need to put\n     // the digits back in order\n-    string ordered_capture(capture);\n+    std::string ordered_capture(capture);\n     std::reverse(ordered_capture.begin(), ordered_capture.end());\n     int gpu_id;\n     CHECK(absl::SimpleAtoi(ordered_capture, &gpu_id));\n@@ -259,7 +259,7 @@ static int ExtractGpuWithoutStream(string device_name) {\n \n void StepStatsCollector::BuildCostModel(\n     CostModelManager* cost_model_manager,\n-    const std::unordered_map<string, const Graph*>& device_map) {\n+    const std::unordered_map<std::string, const Graph*>& device_map) {\n   mutex_lock lock(mu_);\n \n   if (!finalized_) {\n@@ -282,7 +282,7 @@ void StepStatsCollector::BuildCostModel(\n \n   for (int i = 0; i < step_stats_->dev_stats_size(); ++i) {\n     const DeviceStepStats& device_stats = step_stats_->dev_stats(i);\n-    const string& device_name = device_stats.device();\n+    const std::string& device_name = device_stats.device();\n     const int gpu_id = ExtractGpuWithStreamAll(device_name);\n     if (gpu_id >= 0) {\n       // These are gpu hardware stats\n@@ -296,7 +296,7 @@ void StepStatsCollector::BuildCostModel(\n \n   for (auto& itr : per_device_stats) {\n     const absl::string_view device_name = itr.first;\n-    const int gpu_id = ExtractGpuWithoutStream(string(device_name));\n+    const int gpu_id = ExtractGpuWithoutStream(std::string(device_name));\n     if (gpu_id >= 0) {\n       // Reference the gpu hardware stats in addition to the regular stats\n       // for this gpu device if they're available.\n@@ -324,10 +324,10 @@ void StepStatsCollector::BuildCostModel(\n \n     const DeviceStats& dev_stats = per_device_stats.find(device)->second;\n \n-    std::unordered_map<string, NodeExecStats> name_to_hw_node_stats;\n+    std::unordered_map<std::string, NodeExecStats> name_to_hw_node_stats;\n     if (dev_stats.hardware_stats) {\n       for (const auto& node_stats : dev_stats.hardware_stats->node_stats()) {\n-        string node_name = node_stats.node_name();\n+        std::string node_name = node_stats.node_name();\n         // Remove the part of op name (e.g. :Conv2D) in the end of a node name.\n         size_t pos = node_name.find_first_of(':');\n         if (pos != std::string::npos) {\n@@ -368,7 +368,8 @@ void StepStatsCollector::BuildCostModel(\n         cm->RecordMemoryStats(node, stats.memory_stats());\n         // Use hardware stats to record the execution time if they're available,\n         // otherwise use the regular (less accurate) stats\n-        string node_name = dev_stats.regular_stats->node_stats(i).node_name();\n+        std::string node_name =\n+            dev_stats.regular_stats->node_stats(i).node_name();\n         if (dev_stats.hardware_stats && name_to_hw_node_stats.find(node_name) !=\n                                             name_to_hw_node_stats.end()) {\n           const NodeExecStats& hw_stats = name_to_hw_node_stats[node_name];\n@@ -383,14 +384,14 @@ void StepStatsCollector::BuildCostModel(\n   }\n }\n \n-void StepStatsCollector::Save(const string& device,\n+void StepStatsCollector::Save(const std::string& device,\n                               NodeExecStats* node_stats_pb) {\n   Save(device,\n        new NodeExecStatsWrapper(std::unique_ptr<NodeExecStats>(node_stats_pb),\n                                 nullptr, this));\n }\n \n-void StepStatsCollector::Save(const string& device,\n+void StepStatsCollector::Save(const std::string& device,\n                               NodeExecStatsWrapper* node_stats) {\n   if (!node_stats) return;\n   VLOG(1) << \"Save dev \" << device << \" node stats \" << node_stats->stats();\n@@ -410,9 +411,9 @@ void StepStatsCollector::Save(const string& device,\n   }\n }\n \n-void StepStatsCollector::SaveThreadName(const string& device,\n-                                        const uint32 thread_id,\n-                                        const string& thread_name) {\n+void StepStatsCollector::SaveThreadName(const std::string& device,\n+                                        const uint32_t thread_id,\n+                                        const std::string& thread_name) {\n   VLOG(1) << \"Save dev \" << device << \" thread id \" << thread_id << \" name \"\n           << thread_name;\n   {\n@@ -434,17 +435,17 @@ NodeExecStatsInterface* StepStatsCollector::CreateNodeExecStats(\n   return new NodeExecStatsWrapper(node, this);\n }\n \n-string StepStatsCollector::ReportAllocsOnResourceExhausted(\n+std::string StepStatsCollector::ReportAllocsOnResourceExhausted(\n     const absl::string_view err) {\n   mutex_lock l(mu_);\n   if (err.find(\"OOM\") == err.npos) {\n     return \"\";\n   }\n   // <device, allocator> -> AllocStats\n-  std::map<std::pair<string, string>, AllocStats> allocs_map;\n-  string report = \"\\n\";\n+  std::map<std::pair<std::string, std::string>, AllocStats> allocs_map;\n+  std::string report = \"\\n\";\n   for (const auto& dev_stat : dev_stats_) {\n-    const string& device = dev_stat.first;\n+    const std::string& device = dev_stat.first;\n     // Only print the device that has OOM.\n     // TODO(xpan): Extract device from err first to speed it up.\n     if (err.find(device) == err.npos) {\n@@ -490,7 +491,7 @@ string StepStatsCollector::ReportAllocsOnResourceExhausted(\n     // Print allocations stats of the <device, allocator> pair.\n     for (auto it = dev_allocs_stats.nodes_by_size.rbegin();\n          it != dev_allocs_stats.nodes_by_size.rend(); ++it) {\n-      for (const string& node_name : it->second) {\n+      for (const std::string& node_name : it->second) {\n         reported_bytes += it->first;\n         strings::StrAppend(&report, \"  \",\n                            strings::HumanReadableNumBytes(it->first), \" from \",\n@@ -532,7 +533,7 @@ void StepStatsCollector::FinalizeInternal() {\n     return;\n   }\n   finalized_ = true;\n-  std::map<string, DeviceStepStats*> dev_stats_pb;\n+  std::map<std::string, DeviceStepStats*> dev_stats_pb;\n   for (auto& ds : *step_stats_->mutable_dev_stats()) {\n     dev_stats_pb[ds.device()] = &ds;\n   }"
        }
    ],
    "stats": {
        "total": 985,
        "additions": 499,
        "deletions": 486
    }
}