{
    "author": "ezhulenev",
    "message": "[xla:cpu] Migrate XLA:CPU to se::DeviceAddress\n\nPiperOrigin-RevId: 841181057",
    "sha": "3d990196e3c12a04a0f62cc6828a7cd10111396e",
    "files": [
        {
            "sha": "b124f4f72962eacd335d0ecb96704f669e69eae7",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -163,7 +163,7 @@ cc_library(\n         \"//xla/backends/cpu/runtime:dot_dims\",\n         \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n@@ -305,7 +305,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "8d29f0afdb15eff5229e09301301987efb2ea582",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/cpu_profiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -53,7 +53,7 @@ static absl::StatusOr<std::unique_ptr<InputBuffers>> PrepareBackedBuffers(\n     Literal literal(shape, true);\n \n     backed_buffers->backing_literals.push_back(std::move(literal));\n-    backed_buffers->buffers.emplace_back(stream_executor::DeviceMemoryBase(\n+    backed_buffers->buffers.emplace_back(stream_executor::DeviceAddressBase(\n         backed_buffers->backing_literals.back().untyped_data(),\n         backed_buffers->backing_literals.back().size_bytes()));\n   }"
        },
        {
            "sha": "abec1d790f9b90f2b79b91e43caa39cae59f281e",
            "filename": "third_party/xla/xla/backends/cpu/collectives/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -143,7 +143,7 @@ cc_library(\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:rendezvous\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/lib/math:math_util\",\n         \"//xla/tsl/platform:errors\",\n@@ -228,7 +228,7 @@ xla_cc_test(\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/runtime:device_id\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n@@ -271,7 +271,7 @@ cc_library(\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -337,7 +337,7 @@ cc_library(\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\","
        },
        {
            "sha": "797b585b24a262b06e749b3d9311bdb5038ac4cc",
            "filename": "third_party/xla/xla/backends/cpu/collectives/gloo_collectives_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_collectives_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_collectives_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_collectives_test.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -94,9 +94,9 @@ RendezvousKey MakeRendezvousKey(std::vector<GlobalDeviceId> global_devices) {\n // TODO(cobley) - add tests for other collectives.\n \n template <typename T>\n-static se::DeviceMemoryBase AsDeviceMemory(const std::vector<T>& data) {\n-  return se::DeviceMemoryBase(const_cast<T*>(data.data()),\n-                              data.size() * sizeof(T));\n+static se::DeviceAddressBase AsDeviceMemory(const std::vector<T>& data) {\n+  return se::DeviceAddressBase(const_cast<T*>(data.data()),\n+                               data.size() * sizeof(T));\n }\n \n absl::StatusOr<std::vector<uint8_t>> AllReduce("
        },
        {
            "sha": "33fddc52052b5db3fb661955657d8516ad2ca7c0",
            "filename": "third_party/xla/xla/backends/cpu/collectives/gloo_communicator.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -47,7 +47,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/types.h\"\n@@ -63,8 +63,8 @@ GlooCommunicator::~GlooCommunicator() = default;\n \n template <typename T>\n static absl::Status SetAllReduceOptions(ReductionKind reduction_kind,\n-                                        se::DeviceMemoryBase input_buffer,\n-                                        se::DeviceMemoryBase output_buffer,\n+                                        se::DeviceAddressBase input_buffer,\n+                                        se::DeviceAddressBase output_buffer,\n                                         size_t num_elements,\n                                         gloo::AllreduceOptions& options) {\n   options.setInput(reinterpret_cast<T*>(  // REINTERPRET_CAST_OK=existing code.\n@@ -103,8 +103,8 @@ static absl::Status SetAllReduceOptions(ReductionKind reduction_kind,\n   return absl::OkStatus();\n }\n \n-Future<> GlooCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n-                                     se::DeviceMemoryBase recv_buffer,\n+Future<> GlooCommunicator::AllReduce(se::DeviceAddressBase send_buffer,\n+                                     se::DeviceAddressBase recv_buffer,\n                                      PrimitiveType dtype, size_t count,\n                                      ReductionKind reduction_kind,\n                                      const Executor& executor) {\n@@ -189,7 +189,7 @@ Future<> GlooCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n static constexpr uint8_t kCollectivePermuteSlotPrefix = 0x40;\n \n Future<> GlooCommunicator::CollectivePermute(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n   uint32_t tag = 0;  // TODO(phawkins): come up with better tags.\n@@ -240,8 +240,8 @@ Future<> GlooCommunicator::CollectivePermute(\n }\n \n Future<> GlooCommunicator::AllToAll(\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n   // We can't use Gloo's all-to-all implementation directly because it assumes\n   // that the inputs and outputs are contiguous. No big deal; it's just built\n@@ -295,8 +295,8 @@ Future<> GlooCommunicator::AllToAll(\n   return absl::OkStatus();\n }\n \n-Future<> GlooCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n-                                     se::DeviceMemoryBase recv_buffer,\n+Future<> GlooCommunicator::AllGather(se::DeviceAddressBase send_buffer,\n+                                     se::DeviceAddressBase recv_buffer,\n                                      PrimitiveType dtype, size_t count,\n                                      const Executor& executor) {\n   uint32_t tag = 0;  // TODO(phawkins): use better tags.\n@@ -369,8 +369,8 @@ absl::Status ReduceScatterHelper(std::shared_ptr<gloo::Context> context,\n   return absl::OkStatus();\n }\n \n-Future<> GlooCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                         se::DeviceMemoryBase recv_buffer,\n+Future<> GlooCommunicator::ReduceScatter(se::DeviceAddressBase send_buffer,\n+                                         se::DeviceAddressBase recv_buffer,\n                                          PrimitiveType dtype, size_t count,\n                                          ReductionKind reduction_kind,\n                                          const Executor& executor) {"
        },
        {
            "sha": "7df290060ad16539085c34fc3146c8b24f71560d",
            "filename": "third_party/xla/xla/backends/cpu/collectives/gloo_communicator.h",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -43,43 +43,43 @@ class GlooCommunicator : public Communicator {\n                    size_t num_ranks);\n   ~GlooCommunicator() override;\n \n-  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllReduce(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, ReductionKind reduction_kind,\n                      const Executor& executor) override;\n \n-  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n-                             se::DeviceMemoryBase recv_buffer,\n+  Future<> CollectivePermute(se::DeviceAddressBase send_buffer,\n+                             se::DeviceAddressBase recv_buffer,\n                              PrimitiveType dtype, size_t count,\n                              std::optional<RankId> source_rank,\n                              absl::Span<const RankId> target_ranks,\n                              const Executor& executor) override;\n \n-  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n                     PrimitiveType dtype, size_t count,\n                     const Executor& executor) override;\n \n-  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllGather(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, const Executor& executor) override;\n \n-  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> ReduceScatter(se::DeviceAddressBase send_buffer,\n+                         se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                          size_t count, ReductionKind reduction_kind,\n                          const Executor& executor) override;\n \n-  Future<> Broadcast(se::DeviceMemoryBase, se::DeviceMemoryBase, PrimitiveType,\n-                     size_t, RankId, const Executor&) override {\n+  Future<> Broadcast(se::DeviceAddressBase, se::DeviceAddressBase,\n+                     PrimitiveType, size_t, RankId, const Executor&) override {\n     return Unimplemented(\"Broadcast is not implemented\");\n   }\n \n-  Future<> Send(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+  Future<> Send(se::DeviceAddressBase, PrimitiveType, size_t, RankId,\n                 const Executor&) override {\n     return Unimplemented(\"Send is not implemented\");\n   }\n \n-  Future<> Recv(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+  Future<> Recv(se::DeviceAddressBase, PrimitiveType, size_t, RankId,\n                 const Executor&) override {\n     return Unimplemented(\"Recv is not implemented\");\n   }"
        },
        {
            "sha": "c159b9f64d1565091740162b5fb757169a187beb",
            "filename": "third_party/xla/xla/backends/cpu/collectives/in_process_communicator.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/rendezvous.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/lib/math/math_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -242,8 +242,8 @@ absl::Status ReduceScatter(ReductionKind reduction_kind,\n \n struct AllReduceParticipant {\n   size_t rank;\n-  se::DeviceMemoryBase src;\n-  se::DeviceMemoryBase dest;\n+  se::DeviceAddressBase src;\n+  se::DeviceAddressBase dest;\n };\n \n static absl::Status AllReduceOp(\n@@ -265,7 +265,7 @@ static absl::Status AllReduceOp(\n   if (chunk_count == 0) return absl::OkStatus();\n \n   // Returns a pointer to the chunk of data for the given participant rank.\n-  auto chunk_ptr = [&](se::DeviceMemoryBase mem) -> void* {\n+  auto chunk_ptr = [&](se::DeviceAddressBase mem) -> void* {\n     std::byte* ptr = static_cast<std::byte*>(mem.opaque());\n     return ptr + rank * chunk_size * primitive_util::ByteWidth(primitive_type);\n   };\n@@ -302,8 +302,8 @@ static absl::Status AllReduceOp(\n \n struct ReduceScatterParticipant {\n   size_t rank;\n-  se::DeviceMemoryBase src;\n-  se::DeviceMemoryBase dest;\n+  se::DeviceAddressBase src;\n+  se::DeviceAddressBase dest;\n };\n \n static absl::Status ReduceScatterOp(\n@@ -345,8 +345,8 @@ static absl::Status ReduceScatterOp(\n \n struct AllGatherParticipant {\n   size_t rank;\n-  se::DeviceMemoryBase src;\n-  se::DeviceMemoryBase dest;\n+  se::DeviceAddressBase src;\n+  se::DeviceAddressBase dest;\n };\n \n static absl::Status AllGatherOp(\n@@ -366,8 +366,8 @@ static absl::Status AllGatherOp(\n struct AllToAllParticipant {\n   size_t rank;\n \n-  std::vector<se::DeviceMemoryBase> src;\n-  std::vector<se::DeviceMemoryBase> dest;\n+  std::vector<se::DeviceAddressBase> src;\n+  std::vector<se::DeviceAddressBase> dest;\n };\n \n static absl::Status AllToAllOp(\n@@ -388,8 +388,8 @@ struct CollectivePermuteParticipant {\n   size_t rank;\n   std::optional<RankId> src_rank;\n \n-  se::DeviceMemoryBase src;\n-  se::DeviceMemoryBase dest;\n+  se::DeviceAddressBase src;\n+  se::DeviceAddressBase dest;\n };\n \n static absl::Status CollectivePermuteOp(\n@@ -415,8 +415,8 @@ static absl::Status CollectivePermuteOp(\n InProcessCommunicator::InProcessCommunicator(size_t rank, size_t num_ranks)\n     : rank_(rank), num_ranks_(num_ranks) {}\n \n-Future<> InProcessCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n-                                          se::DeviceMemoryBase recv_buffer,\n+Future<> InProcessCommunicator::AllReduce(se::DeviceAddressBase send_buffer,\n+                                          se::DeviceAddressBase recv_buffer,\n                                           PrimitiveType dtype, size_t count,\n                                           ReductionKind reduction_kind,\n                                           const Executor& executor) {\n@@ -438,8 +438,8 @@ Future<> InProcessCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n   return Future<>(absl::OkStatus());\n }\n \n-Future<> InProcessCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                              se::DeviceMemoryBase recv_buffer,\n+Future<> InProcessCommunicator::ReduceScatter(se::DeviceAddressBase send_buffer,\n+                                              se::DeviceAddressBase recv_buffer,\n                                               PrimitiveType dtype, size_t count,\n                                               ReductionKind reduction_kind,\n                                               const Executor& executor) {\n@@ -462,7 +462,7 @@ Future<> InProcessCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n }\n \n Future<> InProcessCommunicator::CollectivePermute(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n@@ -486,8 +486,8 @@ Future<> InProcessCommunicator::CollectivePermute(\n }\n \n Future<> InProcessCommunicator::AllToAll(\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n   const RendezvousKey& key = cpu_executor->rendezvous_key();\n@@ -510,8 +510,8 @@ Future<> InProcessCommunicator::AllToAll(\n   return Future<>(absl::OkStatus());\n }\n \n-Future<> InProcessCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n-                                          se::DeviceMemoryBase recv_buffer,\n+Future<> InProcessCommunicator::AllGather(se::DeviceAddressBase send_buffer,\n+                                          se::DeviceAddressBase recv_buffer,\n                                           PrimitiveType dtype, size_t count,\n                                           const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));"
        },
        {
            "sha": "55691f1a0860f660f57b4048272e59b6021f5f60",
            "filename": "third_party/xla/xla/backends/cpu/collectives/in_process_communicator.h",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -28,7 +28,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -40,43 +40,43 @@ class InProcessCommunicator : public Communicator {\n  public:\n   InProcessCommunicator(size_t rank, size_t num_ranks);\n \n-  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllReduce(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, ReductionKind reduction_kind,\n                      const Executor& executor) override;\n \n-  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n-                             se::DeviceMemoryBase recv_buffer,\n+  Future<> CollectivePermute(se::DeviceAddressBase send_buffer,\n+                             se::DeviceAddressBase recv_buffer,\n                              PrimitiveType dtype, size_t count,\n                              std::optional<RankId> source_rank,\n                              absl::Span<const RankId> target_ranks,\n                              const Executor& executor) override;\n \n-  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n                     PrimitiveType dtype, size_t count,\n                     const Executor& executor) override;\n \n-  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllGather(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, const Executor& executor) override;\n \n-  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> ReduceScatter(se::DeviceAddressBase send_buffer,\n+                         se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                          size_t count, ReductionKind reduction_kind,\n                          const Executor& executor) override;\n \n-  Future<> Broadcast(se::DeviceMemoryBase, se::DeviceMemoryBase, PrimitiveType,\n-                     size_t, RankId, const Executor&) override {\n+  Future<> Broadcast(se::DeviceAddressBase, se::DeviceAddressBase,\n+                     PrimitiveType, size_t, RankId, const Executor&) override {\n     return Future<>(Unimplemented(\"Broadcast is not implemented\"));\n   }\n \n-  Future<> Send(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+  Future<> Send(se::DeviceAddressBase, PrimitiveType, size_t, RankId,\n                 const Executor&) override {\n     return Future<>(Unimplemented(\"Send is not implemented\"));\n   }\n \n-  Future<> Recv(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+  Future<> Recv(se::DeviceAddressBase, PrimitiveType, size_t, RankId,\n                 const Executor&) override {\n     return Future<>(Unimplemented(\"Recv is not implemented\"));\n   }"
        },
        {
            "sha": "5b15bfce8ea2a9527a109d0a0ae3593acda82328",
            "filename": "third_party/xla/xla/backends/cpu/collectives/mpi_communicator.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -125,8 +125,8 @@ MpiCommunicator::MpiCommunicator(int color, int key) {\n \n MpiCommunicator::~MpiCommunicator() { MPI_Comm_free(&comm_); };\n \n-Future<> MpiCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n-                                    se::DeviceMemoryBase recv_buffer,\n+Future<> MpiCommunicator::AllReduce(se::DeviceAddressBase send_buffer,\n+                                    se::DeviceAddressBase recv_buffer,\n                                     PrimitiveType dtype, size_t count,\n                                     ReductionKind reduction_kind,\n                                     const Executor& executor) {\n@@ -138,7 +138,7 @@ Future<> MpiCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n }\n \n Future<> MpiCommunicator::CollectivePermute(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    se::DeviceAddressBase send_buffer, se::DeviceAddressBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n   int tag = 0;  // TODO come up with better tags.\n@@ -182,8 +182,8 @@ Future<> MpiCommunicator::CollectivePermute(\n }\n \n Future<> MpiCommunicator::AllToAll(\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n   // We can't use MPI_Alltoall directly because it assumes that the inputs and\n   // outputs are contiguous. Therefore here we implement it using MPI_Sendrecv.\n@@ -218,8 +218,8 @@ Future<> MpiCommunicator::AllToAll(\n   return absl::OkStatus();\n }\n \n-Future<> MpiCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n-                                    se::DeviceMemoryBase recv_buffer,\n+Future<> MpiCommunicator::AllGather(se::DeviceAddressBase send_buffer,\n+                                    se::DeviceAddressBase recv_buffer,\n                                     PrimitiveType dtype, size_t count,\n                                     const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(MPI_Datatype type, PrimitiveTypeToMpiType(dtype));\n@@ -230,8 +230,8 @@ Future<> MpiCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n   return absl::OkStatus();\n }\n \n-Future<> MpiCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                        se::DeviceMemoryBase recv_buffer,\n+Future<> MpiCommunicator::ReduceScatter(se::DeviceAddressBase send_buffer,\n+                                        se::DeviceAddressBase recv_buffer,\n                                         PrimitiveType dtype, size_t count,\n                                         ReductionKind reduction_kind,\n                                         const Executor& executor) {"
        },
        {
            "sha": "b6ae18ac1d6fd80518dadf26be570ff66c3e47d3",
            "filename": "third_party/xla/xla/backends/cpu/collectives/mpi_communicator.h",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -40,41 +40,41 @@ class MpiCommunicator : public Communicator {\n   explicit MpiCommunicator(int color, int key);\n   ~MpiCommunicator() override;\n \n-  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllReduce(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, ReductionKind reduction_kind,\n                      const Executor& executor) override;\n \n-  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n-                             se::DeviceMemoryBase recv_buffer,\n+  Future<> CollectivePermute(se::DeviceAddressBase send_buffer,\n+                             se::DeviceAddressBase recv_buffer,\n                              PrimitiveType dtype, size_t count,\n                              std::optional<RankId> source_rank,\n                              absl::Span<const RankId> target_ranks,\n                              const Executor& executor) override;\n \n-  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceAddressBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceAddressBase, 4> recv_buffers,\n                     PrimitiveType dtype, size_t count,\n                     const Executor& executor) override;\n-  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n-                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> AllGather(se::DeviceAddressBase send_buffer,\n+                     se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                      size_t count, const Executor& executor) override;\n-  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+  Future<> ReduceScatter(se::DeviceAddressBase send_buffer,\n+                         se::DeviceAddressBase recv_buffer, PrimitiveType dtype,\n                          size_t count, ReductionKind reduction_kind,\n                          const Executor& executor) override;\n \n-  Future<> Broadcast(se::DeviceMemoryBase, se::DeviceMemoryBase, PrimitiveType,\n-                     size_t, RankId, const Executor&) override {\n+  Future<> Broadcast(se::DeviceAddressBase, se::DeviceAddressBase,\n+                     PrimitiveType, size_t, RankId, const Executor&) override {\n     return Unimplemented(\"Broadcast is not implemented\");\n   }\n \n-  Future<> Send(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+  Future<> Send(se::DeviceAddressBase, PrimitiveType, size_t, RankId,\n                 const Executor&) override {\n     return Unimplemented(\"Send is not implemented\");\n   }\n \n-  Future<> Recv(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+  Future<> Recv(se::DeviceAddressBase, PrimitiveType, size_t, RankId,\n                 const Executor&) override {\n     return Unimplemented(\"Recv is not implemented\");\n   }"
        },
        {
            "sha": "6215a7c0125e0625e606bc92c68eff6014ee9147",
            "filename": "third_party/xla/xla/backends/cpu/constant_allocation.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fconstant_allocation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fconstant_allocation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fconstant_allocation.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -32,25 +32,25 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n \n-se::DeviceMemoryBase ConstantAllocation::AsDeviceMemoryBase() const {\n+se::DeviceAddressBase ConstantAllocation::AsDeviceMemoryBase() const {\n   if (auto* _ = std::get_if<std::monostate>(&data)) {\n-    return se::DeviceMemoryBase();\n+    return se::DeviceAddressBase();\n   }\n \n   if (auto* owned = std::get_if<std::unique_ptr<Literal>>(&data)) {\n-    return se::DeviceMemoryBase((*owned)->untyped_data(),\n-                                (*owned)->size_bytes());\n+    return se::DeviceAddressBase((*owned)->untyped_data(),\n+                                 (*owned)->size_bytes());\n   }\n \n   auto* view = std::get_if<absl::Span<const uint8_t>>(&data);\n-  return se::DeviceMemoryBase(\n+  return se::DeviceAddressBase(\n       const_cast<void*>(reinterpret_cast<const void*>(view->data())),\n       view->size());\n }"
        },
        {
            "sha": "93c01a5375125e559be9239f576978e45a2e92ea",
            "filename": "third_party/xla/xla/backends/cpu/constant_allocation.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fconstant_allocation.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fconstant_allocation.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fconstant_allocation.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -25,13 +25,13 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/literal.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace xla::cpu {\n \n // A storage (or an alias) for constant allocations data.\n struct ConstantAllocation {\n-  se::DeviceMemoryBase AsDeviceMemoryBase() const;\n+  se::DeviceAddressBase AsDeviceMemoryBase() const;\n \n   BufferAllocation::Index index = -1;\n   std::variant<std::monostate, std::unique_ptr<Literal>,"
        },
        {
            "sha": "43c05c3190e43738abeca4eec25e39af721cc4bb",
            "filename": "third_party/xla/xla/backends/cpu/nanort/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2FBUILD?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -113,7 +113,7 @@ cc_library(\n         \"//xla/service/cpu:cpu_aot_loader\",\n         \"//xla/service/cpu:cpu_executable\",\n         \"//xla/service/cpu:executable_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\","
        },
        {
            "sha": "8cafe89fc3e6cc01e5d79a574679d227fe5fc83d",
            "filename": "third_party/xla/xla/backends/cpu/nanort/nanort_executable.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fnanort_executable.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -46,7 +46,7 @@ limitations under the License.\n #include \"xla/service/hlo_value.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -306,23 +306,23 @@ NanoRtExecutable::NanoRtExecutable(\n       temp_allocation_index_(temp_allocation_index),\n       program_shape_(program_shape) {}\n \n-static se::DeviceMemoryBase ToDeviceMemory(\n+static se::DeviceAddressBase ToDeviceMemory(\n     const NanoRtExecutable::Argument& argument) {\n-  return se::DeviceMemoryBase(\n+  return se::DeviceAddressBase(\n       const_cast<void*>(reinterpret_cast<const void*>(argument.data().data())),\n       argument.data().size());\n }\n \n-static se::DeviceMemoryBase ToDeviceMemory(\n+static se::DeviceAddressBase ToDeviceMemory(\n     const NanoRtExecutable::Result& result) {\n-  return se::DeviceMemoryBase(reinterpret_cast<void*>(result.data().data()),\n-                              result.data().size());\n+  return se::DeviceAddressBase(reinterpret_cast<void*>(result.data().data()),\n+                               result.data().size());\n }\n \n-static se::DeviceMemoryBase ToDeviceMemory(\n+static se::DeviceAddressBase ToDeviceMemory(\n     const NanoRtExecutable::PreallocatedTemp& temp) {\n-  return se::DeviceMemoryBase(reinterpret_cast<void*>(temp.data()),\n-                              temp.size());\n+  return se::DeviceAddressBase(reinterpret_cast<void*>(temp.data()),\n+                               temp.size());\n }\n \n tsl::AsyncValueRef<NanoRtExecutable::ExecuteEvent> NanoRtExecutable::Execute("
        },
        {
            "sha": "1ee4f81b7db5f6321ba39bf9b0e00c07832220d5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -57,7 +57,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:maybe_owning_device_memory\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -73,7 +73,7 @@ xla_cc_test(\n         \":thunk_testlib\",\n         \"//xla:literal_util\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_googletest//:gtest_main\",\n@@ -104,7 +104,7 @@ cc_library(\n         \":work_queue\",\n         \"//xla:util\",\n         \"//xla/runtime:work_group\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:logging\",\n@@ -219,7 +219,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/types:span\",\n@@ -286,7 +286,7 @@ xla_cc_test(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n@@ -329,7 +329,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n@@ -436,7 +436,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n@@ -619,7 +619,7 @@ cc_library(\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:computation_placer\",\n         \"//xla/service:hlo_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n@@ -647,7 +647,7 @@ cc_library(\n         \"//xla/pjrt:transpose\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -701,7 +701,7 @@ cc_library(\n         \"//xla/service:custom_call_status_internal\",\n         \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:hlo_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -795,7 +795,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n@@ -843,7 +843,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n@@ -881,7 +881,7 @@ cc_library(\n         \"//xla/runtime:device_id\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:computation_placer_hdr\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n@@ -923,7 +923,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n@@ -966,7 +966,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:work_group\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n@@ -1003,7 +1003,7 @@ xla_cc_test(\n         \"//xla:literal_util\",\n         \"//xla/runtime:work_group\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1047,7 +1047,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -1087,7 +1087,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n@@ -1145,7 +1145,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -1172,7 +1172,7 @@ xla_cc_test(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1193,7 +1193,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n@@ -1224,7 +1224,7 @@ cc_library(\n         \":topk_lib\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:dynamic_annotations\",\n@@ -1281,7 +1281,7 @@ cc_library(\n         \"//xla/runtime:work_group\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:collective_ops_utils\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "b1f7e8142d2939aac1cd59af7428de47d2a1a368",
            "filename": "third_party/xla/xla/backends/cpu/runtime/buffer_allocations.h",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fbuffer_allocations.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fbuffer_allocations.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fbuffer_allocations.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/maybe_owning_device_memory.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/util.h\"\n \n namespace xla::cpu {\n@@ -36,35 +36,35 @@ namespace xla::cpu {\n // particular XLA execution. Buffers are indexed by the buffer allocation index.\n class BufferAllocations {\n  public:\n-  using Buffers = absl::InlinedVector<se::DeviceMemoryBase, 8>;\n+  using Buffers = absl::InlinedVector<se::DeviceAddressBase, 8>;\n \n   explicit BufferAllocations(Buffers buffers);\n-  explicit BufferAllocations(absl::Span<const se::DeviceMemoryBase> buffers);\n+  explicit BufferAllocations(absl::Span<const se::DeviceAddressBase> buffers);\n   explicit BufferAllocations(absl::Span<const MaybeOwningDeviceMemory> buffers);\n \n   // Returns the device address of buffer at the given index. Returns an error\n   // if the index is out of range.\n-  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+  absl::StatusOr<se::DeviceAddressBase> GetDeviceAddress(\n       BufferAllocation::Index index) const;\n \n   // Same as above, but also adjusts the returned address for the offset and\n   // size contained in the given slice.\n-  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+  absl::StatusOr<se::DeviceAddressBase> GetDeviceAddress(\n       BufferAllocation::Slice slice) const;\n \n   // Unchecked version of `GetDeviceAddress` that does not check the buffer\n   // index and assumes it is valid.\n-  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+  se::DeviceAddressBase GetDeviceAddressUnchecked(\n       BufferAllocation::Index buffer_index) const;\n \n   // Unchecked version of `GetDeviceAddress` that does not check the slice\n   // buffer index, offset and size and assumes they all are valid.\n-  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+  se::DeviceAddressBase GetDeviceAddressUnchecked(\n       BufferAllocation::Slice slice) const;\n \n  private:\n-  absl::InlinedVector<se::DeviceMemoryBase, 8> buffers_;\n-  se::DeviceMemoryBase* buffers_data_;  // buffers_.data()\n+  absl::InlinedVector<se::DeviceAddressBase, 8> buffers_;\n+  se::DeviceAddressBase* buffers_data_;  // buffers_.data()\n   size_t num_buffers_;\n };\n \n@@ -74,7 +74,7 @@ inline BufferAllocations::BufferAllocations(Buffers buffers)\n       num_buffers_(buffers_.size()) {}\n \n inline BufferAllocations::BufferAllocations(\n-    absl::Span<const se::DeviceMemoryBase> buffers)\n+    absl::Span<const se::DeviceAddressBase> buffers)\n     : buffers_(buffers.begin(), buffers.end()),\n       buffers_data_(buffers_.data()),\n       num_buffers_(buffers_.size()) {}\n@@ -89,7 +89,7 @@ inline BufferAllocations::BufferAllocations(\n   }\n }\n \n-inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceAddressBase>\n BufferAllocations::GetDeviceAddress(BufferAllocation::Index index) const {\n   if (ABSL_PREDICT_FALSE(index < 0 || index >= num_buffers_)) {\n     return InvalidArgument(\n@@ -100,13 +100,13 @@ BufferAllocations::GetDeviceAddress(BufferAllocation::Index index) const {\n   return buffers_[index];\n }\n \n-inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceAddressBase>\n BufferAllocations::GetDeviceAddress(BufferAllocation::Slice slice) const {\n   // Handle empty slices explicitly and return a null pointer device memory to\n   // guarantee that we do not accidentally write through the empty slice which\n   // would hide a real bug in the code.\n   if (ABSL_PREDICT_FALSE(slice.size() == 0)) {\n-    return se::DeviceMemoryBase(nullptr, 0);\n+    return se::DeviceAddressBase(nullptr, 0);\n   }\n \n   int64_t index = slice.index();\n@@ -115,7 +115,7 @@ BufferAllocations::GetDeviceAddress(BufferAllocation::Slice slice) const {\n         \"Invalid buffer index %d. It must be in the range [0, %d)\", index,\n         num_buffers_);\n   }\n-  const se::DeviceMemoryBase& base = buffers_data_[index];\n+  const se::DeviceAddressBase& base = buffers_data_[index];\n \n   int64_t offset = slice.offset();\n   int64_t extent = offset + slice.size();\n@@ -140,15 +140,15 @@ BufferAllocations::GetDeviceAddress(BufferAllocation::Slice slice) const {\n   return base.GetByteSlice(offset, slice.size());\n }\n \n-inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceAddressBase\n BufferAllocations::GetDeviceAddressUnchecked(\n     BufferAllocation::Index buffer_index) const {\n   return buffers_data_[buffer_index];\n }\n \n // Unchecked version of `GetDeviceAddress` that does not check the slice\n // buffer index, offset and size and assumes they are valid.\n-inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceAddressBase\n BufferAllocations::GetDeviceAddressUnchecked(\n     BufferAllocation::Slice slice) const {\n   return buffers_data_[slice.index()].GetByteSlice(slice.offset(),"
        },
        {
            "sha": "1d457986c8cbe32300b3b48e4cfb43c8acd3d81f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/buffer_allocations_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fbuffer_allocations_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fbuffer_allocations_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fbuffer_allocations_test.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk_testlib.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n \n@@ -34,11 +34,11 @@ TEST(BufferAllocationsTest, GetDeviceAddress) {\n \n   BufferAllocations allocations = CreateBufferAllocations(data);\n \n-  TF_ASSERT_OK_AND_ASSIGN(se::DeviceMemoryBase alloc_mem,\n+  TF_ASSERT_OK_AND_ASSIGN(se::DeviceAddressBase alloc_mem,\n                           allocations.GetDeviceAddress(0));\n   EXPECT_EQ(alloc_mem.opaque(), &data.data<float>()[0]);\n \n-  TF_ASSERT_OK_AND_ASSIGN(se::DeviceMemoryBase slice_mem,\n+  TF_ASSERT_OK_AND_ASSIGN(se::DeviceAddressBase slice_mem,\n                           allocations.GetDeviceAddress(slice));\n   EXPECT_EQ(slice_mem.opaque(), &data.data<float>()[2]);\n }\n@@ -52,10 +52,11 @@ TEST(BufferAllocationsTest, GetDeviceAddressUnchecked) {\n \n   BufferAllocations allocations = CreateBufferAllocations(data);\n \n-  se::DeviceMemoryBase alloc_mem = allocations.GetDeviceAddressUnchecked(0);\n+  se::DeviceAddressBase alloc_mem = allocations.GetDeviceAddressUnchecked(0);\n   EXPECT_EQ(alloc_mem.opaque(), &data.data<float>()[0]);\n \n-  se::DeviceMemoryBase slice_mem = allocations.GetDeviceAddressUnchecked(slice);\n+  se::DeviceAddressBase slice_mem =\n+      allocations.GetDeviceAddressUnchecked(slice);\n   EXPECT_EQ(slice_mem.opaque(), &data.data<float>()[2]);\n }\n "
        },
        {
            "sha": "3bdd58f8436d6bb77267c18f927e3850de442692",
            "filename": "third_party/xla/xla/backends/cpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -52,7 +52,7 @@ limitations under the License.\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -136,14 +136,14 @@ CollectiveThunk::GetOpDeviceMemory(const ExecuteParams& params) {\n   size_t num_dsts = destination_buffers().size();\n   DCHECK_EQ(num_srcs, num_dsts) << \"Number of src and dst buffers must match\";\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> source_data(num_srcs);\n+  absl::InlinedVector<se::DeviceAddressBase, 4> source_data(num_srcs);\n   for (int i = 0; i < num_srcs; ++i) {\n     TF_ASSIGN_OR_RETURN(\n         source_data[i],\n         params.buffer_allocations->GetDeviceAddress(source_buffer(i)));\n   }\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> destination_data(num_dsts);\n+  absl::InlinedVector<se::DeviceAddressBase, 4> destination_data(num_dsts);\n   for (int i = 0; i < num_dsts; ++i) {\n     TF_ASSIGN_OR_RETURN(\n         destination_data[i],"
        },
        {
            "sha": "0ba4c66178133fab823c6e2fb8331201a509609a",
            "filename": "third_party/xla/xla/backends/cpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -83,8 +83,8 @@ class CollectiveThunk : public Thunk {\n \n   // Device memory resolved for the collective operation buffers.\n   struct OpDeviceMemory {\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> source;\n-    absl::InlinedVector<se::DeviceMemoryBase, 4> destination;\n+    absl::InlinedVector<se::DeviceAddressBase, 4> source;\n+    absl::InlinedVector<se::DeviceAddressBase, 4> destination;\n   };\n \n   CollectiveThunk(CollectiveKind collective_kind, Thunk::Info info,"
        },
        {
            "sha": "17ee2002d1b6b06e3acf091701d693e5d1928f04",
            "filename": "third_party/xla/xla/backends/cpu/runtime/conditional_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconditional_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -93,7 +93,7 @@ ConditionalThunk::ConditionalThunk(Info info,\n tsl::AsyncValueRef<Thunk::ExecuteEvent> ConditionalThunk::Execute(\n     const ExecuteParams& params) {\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase branch_index_data,\n+      se::DeviceAddressBase branch_index_data,\n       params.buffer_allocations->GetDeviceAddress(branch_index_buffer_));\n \n   VLOG(3) << absl::StreamFormat(\"Conditional: #branches=%d\","
        },
        {
            "sha": "420c8ba7d945ec1da0ed0b82eb049d99ad3927e8",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -78,13 +78,13 @@ ConvolutionThunk::ConvolutionThunk(\n \n tsl::AsyncValueRef<Thunk::ExecuteEvent> ConvolutionThunk::Execute(\n     const ExecuteParams& params) {\n-  TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase input_data,\n+  TF_ASSIGN_OR_RETURN(se::DeviceAddressBase input_data,\n                       params.buffer_allocations->GetDeviceAddress(\n                           convolution_slices_.input_buffer));\n-  TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase kernel_data,\n+  TF_ASSIGN_OR_RETURN(se::DeviceAddressBase kernel_data,\n                       params.buffer_allocations->GetDeviceAddress(\n                           convolution_slices_.kernel_buffer));\n-  TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase output_data,\n+  TF_ASSIGN_OR_RETURN(se::DeviceAddressBase output_data,\n                       params.buffer_allocations->GetDeviceAddress(\n                           convolution_slices_.output_buffer));\n \n@@ -115,9 +115,9 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> ConvolutionThunk::Execute(\n \n tsl::AsyncValueRef<Thunk::ExecuteEvent>\n ConvolutionThunk::HandleEigen2DConvolution(const ExecuteParams& params,\n-                                           se::DeviceMemoryBase input,\n-                                           se::DeviceMemoryBase kernel,\n-                                           se::DeviceMemoryBase output) {\n+                                           se::DeviceAddressBase input,\n+                                           se::DeviceAddressBase kernel,\n+                                           se::DeviceAddressBase output) {\n   auto dispatch = [&](auto type_tag) {\n     auto execute_event = tsl::MakeConstructedAsyncValueRef<ExecuteEvent>();\n \n@@ -167,9 +167,9 @@ ConvolutionThunk::HandleEigen2DConvolution(const ExecuteParams& params,\n \n tsl::AsyncValueRef<Thunk::ExecuteEvent>\n ConvolutionThunk::HandleEigen3DConvolution(const ExecuteParams& params,\n-                                           se::DeviceMemoryBase input,\n-                                           se::DeviceMemoryBase kernel,\n-                                           se::DeviceMemoryBase output) {\n+                                           se::DeviceAddressBase input,\n+                                           se::DeviceAddressBase kernel,\n+                                           se::DeviceAddressBase output) {\n   auto dispatch = [&](auto type_tag) {\n     auto execute_event = tsl::MakeConstructedAsyncValueRef<ExecuteEvent>();\n "
        },
        {
            "sha": "130e554b063ebf78e46c4d43dd8a963e0a53dfbc",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -69,12 +69,12 @@ class ConvolutionThunk final : public Thunk {\n                    ConvolutionDimensionNumbers dnums, Window window);\n \n   tsl::AsyncValueRef<Thunk::ExecuteEvent> HandleEigen2DConvolution(\n-      const ExecuteParams& params, se::DeviceMemoryBase input,\n-      se::DeviceMemoryBase kernel, se::DeviceMemoryBase output);\n+      const ExecuteParams& params, se::DeviceAddressBase input,\n+      se::DeviceAddressBase kernel, se::DeviceAddressBase output);\n \n   tsl::AsyncValueRef<Thunk::ExecuteEvent> HandleEigen3DConvolution(\n-      const ExecuteParams& params, se::DeviceMemoryBase input,\n-      se::DeviceMemoryBase kernel, se::DeviceMemoryBase output);\n+      const ExecuteParams& params, se::DeviceAddressBase input,\n+      se::DeviceAddressBase kernel, se::DeviceAddressBase output);\n \n   Options options_;\n   ConvolutionSlices convolution_slices_;"
        },
        {
            "sha": "106d945ddc3981a06a0662a6c5c1491b2df5f5d1",
            "filename": "third_party/xla/xla/backends/cpu/runtime/copy_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -97,7 +97,7 @@ CopyThunk::CopyThunk(Info info, BufferAllocation::Slice src_buffer,\n \n static std::tuple<void*, void*, int64_t> GetBlockCopyParameters(\n     const CopyThunk::ParallelBlockParams& params, int64_t block_index,\n-    se::DeviceMemoryBase destination, se::DeviceMemoryBase source) {\n+    se::DeviceAddressBase destination, se::DeviceAddressBase source) {\n   CHECK_LT(block_index, params.block_count);\n \n   int64_t offset = block_index * params.block_size;\n@@ -143,8 +143,8 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> CopyThunk::Execute(\n \n   const BufferAllocations* allocations = params.buffer_allocations;\n \n-  se::DeviceMemoryBase src_data;\n-  se::DeviceMemoryBase dst_data;\n+  se::DeviceAddressBase src_data;\n+  se::DeviceAddressBase dst_data;\n \n   if constexpr (ShouldCheckBufferSlices()) {\n     TF_ASSIGN_OR_RETURN(src_data, allocations->GetDeviceAddress(src_buffer_));"
        },
        {
            "sha": "612b41622d4a4932c9c42d222bca285d365663b5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcustom_call_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -52,7 +52,7 @@ limitations under the License.\n #include \"xla/service/custom_call_status_internal.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/hlo.pb.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -133,7 +133,7 @@ static absl::StatusOr<ffi::CallFrame> BuildCallFrameForTypedFFI(\n     auto elements = absl::c_accumulate(shape.dimensions(), 1ULL,\n                                        std::multiplies<int64_t>());\n     auto dtype_bytes = primitive_util::ByteWidth(shape.element_type());\n-    se::DeviceMemoryBase placeholder_arg(nullptr, elements * dtype_bytes);\n+    se::DeviceAddressBase placeholder_arg(nullptr, elements * dtype_bytes);\n     builder.AddBufferArg(placeholder_arg, shape.element_type(),\n                          shape.dimensions());\n   }\n@@ -151,7 +151,7 @@ static absl::StatusOr<ffi::CallFrame> BuildCallFrameForTypedFFI(\n     auto elements = absl::c_accumulate(shape.dimensions(), 1ULL,\n                                        std::multiplies<int64_t>());\n     auto dtype_bytes = primitive_util::ByteWidth(shape.element_type());\n-    se::DeviceMemoryBase placeholder_ret(nullptr, elements * dtype_bytes);\n+    se::DeviceAddressBase placeholder_ret(nullptr, elements * dtype_bytes);\n     builder.AddBufferRet(placeholder_ret, shape.element_type(),\n                          shape.dimensions());\n   }\n@@ -302,7 +302,7 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> CustomCallThunk::CallTypedFFI(\n   }\n \n   // Collect argument buffers.\n-  absl::InlinedVector<se::DeviceMemoryBase, 8> arguments;\n+  absl::InlinedVector<se::DeviceAddressBase, 8> arguments;\n   arguments.reserve(op_buffers_.arguments_buffers.size());\n   for (int i = 0; i < op_buffers_.arguments_buffers.size(); ++i) {\n     BufferAllocation::Slice& slice = op_buffers_.arguments_buffers[i];\n@@ -317,7 +317,7 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> CustomCallThunk::CallTypedFFI(\n   }\n \n   // Collect results buffers.\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> results;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> results;\n   results.reserve(op_buffers_.results_buffers.size());\n   for (int i = 0; i < op_buffers_.results_buffers.size(); ++i) {\n     BufferAllocation::Slice& slice = op_buffers_.results_buffers[i];\n@@ -355,7 +355,7 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> CustomCallThunk::CallUntypedAPI(\n   arguments.reserve(op_buffers_.arguments_buffers.size());\n   for (int i = 0; i < op_buffers_.arguments_buffers.size(); ++i) {\n     auto& slice = op_buffers_.arguments_buffers[i];\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase arg,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase arg,\n                         params.buffer_allocations->GetDeviceAddress(slice));\n     arguments.push_back(arg.opaque());\n     VLOG(3) << absl::StreamFormat(\n@@ -370,7 +370,7 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> CustomCallThunk::CallUntypedAPI(\n   results.reserve(op_buffers_.results_buffers.size());\n   for (int i = 0; i < op_buffers_.results_buffers.size(); ++i) {\n     auto& slice = op_buffers_.results_buffers[i];\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase res,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase res,\n                         params.buffer_allocations->GetDeviceAddress(slice));\n     results.push_back(res.opaque());\n     VLOG(3) << absl::StreamFormat(\"  res: %s in slice %s (%p)\","
        },
        {
            "sha": "4b5665dcf3d1b4addf07c3ffb88d0665d81a0638",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -73,15 +73,15 @@ DotThunk::DotThunk(Info info, DotDimensionNumbers dot_dimensions,\n tsl::AsyncValueRef<DotThunk::ExecuteEvent> DotThunk::Execute(\n     const ExecuteParams& params) {\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase lhs_data,\n+      se::DeviceAddressBase lhs_data,\n       params.buffer_allocations->GetDeviceAddress(dot_slices_.lhs_buffer));\n \n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase rhs_data,\n+      se::DeviceAddressBase rhs_data,\n       params.buffer_allocations->GetDeviceAddress(dot_slices_.rhs_buffer));\n \n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase out_data,\n+      se::DeviceAddressBase out_data,\n       params.buffer_allocations->GetDeviceAddress(dot_slices_.out_buffer));\n \n   VLOG(3) << absl::StreamFormat("
        },
        {
            "sha": "b5eaf6f68ca78f0cb3a7c165d2440e50e5c65a83",
            "filename": "third_party/xla/xla/backends/cpu/runtime/fft_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ffft_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ffft_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ffft_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -168,10 +168,10 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> FftThunk::Execute(\n   TF_RET_CHECK(LayoutUtil::IsMonotonicWithDim0Major(output_shape_.layout()));\n \n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase input_data,\n+      se::DeviceAddressBase input_data,\n       params.buffer_allocations->GetDeviceAddress(input_buffer_));\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase output_data,\n+      se::DeviceAddressBase output_data,\n       params.buffer_allocations->GetDeviceAddress(output_buffer_));\n \n   const int fft_rank = fft_length_.size();"
        },
        {
            "sha": "6a5edf3562a8d76474d9b461653227e70cfb9ed5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/infeed_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Finfeed_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Finfeed_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Finfeed_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -65,7 +65,7 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> InfeedThunk::Execute(\n \n   for (InfeedBuffer& infeed_buffer : infeed_buffers_) {\n     TF_ASSIGN_OR_RETURN(\n-        se::DeviceMemoryBase infeed_data,\n+        se::DeviceAddressBase infeed_data,\n         params.buffer_allocations->GetDeviceAddress(infeed_buffer.slice));\n \n     VLOG(3) << absl::StreamFormat(\"  infeed #%d: %s into slice %s (%p)\","
        },
        {
            "sha": "02ba094c053dc888b5180933432419f32567991c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/kernel.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/kernel_c_api.h\"\n #include \"xla/backends/cpu/runtime/work_queue.h\"\n #include \"xla/runtime/work_group.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/util.h\"\n@@ -52,7 +52,7 @@ static tsl::AsyncValueRef<LaunchEvent> OkLaunchEvent() {\n }\n \n static absl::InlinedVector<XLA_CPU_KernelArg, 8> ConvertBuffersToKernelArgs(\n-    absl::Span<const Kernel::DeviceMemoryBase> buffers) {\n+    absl::Span<const Kernel::DeviceAddressBase> buffers) {\n   absl::InlinedVector<XLA_CPU_KernelArg, 8> args(buffers.size());\n   for (size_t i = 0; i < buffers.size(); ++i) {\n     args[i].data = buffers[i].opaque();\n@@ -140,7 +140,7 @@ Kernel::Kernel(unsigned arity, XLA_CPU_Kernel* kernel)\n       arity_(arity) {}\n \n absl::Status Kernel::Launch(const NumWorkGroups& num_workgroups,\n-                            absl::Span<const DeviceMemoryBase> buffers) const {\n+                            absl::Span<const DeviceAddressBase> buffers) const {\n   return Launch(num_workgroups, ConvertBuffersToKernelArgs(buffers));\n }\n \n@@ -171,7 +171,7 @@ absl::Status Kernel::Launch(const NumWorkGroups& num_workgroups,\n \n tsl::AsyncValueRef<LaunchEvent> Kernel::Launch(\n     const NumWorkGroups& num_workgroups,\n-    absl::Span<const DeviceMemoryBase> buffers,\n+    absl::Span<const DeviceAddressBase> buffers,\n     const Eigen::ThreadPoolDevice* device) const {\n   return Launch(num_workgroups, ConvertBuffersToKernelArgs(buffers), device);\n }"
        },
        {
            "sha": "f435d6d0bc37cb2fc676bf5a1c8d972a320c8fe5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/kernel.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/cpu/runtime/kernel_c_api.h\"\n #include \"xla/runtime/work_group.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/chain.h\"\n \n@@ -41,7 +41,7 @@ class Kernel {\n   // A struct to report completion of the kernel execution.\n   using LaunchEvent = tsl::Chain;\n \n-  using DeviceMemoryBase = stream_executor::DeviceMemoryBase;\n+  using DeviceAddressBase = stream_executor::DeviceAddressBase;\n \n   // Virtual base class that owns the function behind the host kernel. It can be\n   // a function in a jit-compiled LLVM module or simply a pointer to the\n@@ -74,7 +74,7 @@ class Kernel {\n   // Launches the kernel on the current thread by iterating over all workgroups\n   // in `num_workgroups` and calling the kernel function.\n   absl::Status Launch(const NumWorkGroups& num_workgroups,\n-                      absl::Span<const DeviceMemoryBase> buffers) const;\n+                      absl::Span<const DeviceAddressBase> buffers) const;\n   absl::Status Launch(const NumWorkGroups& num_workgroups,\n                       absl::Span<const XLA_CPU_KernelArg> args) const;\n \n@@ -86,7 +86,7 @@ class Kernel {\n   // get the number of tasks that are expected to be completed.\n   tsl::AsyncValueRef<LaunchEvent> Launch(\n       const NumWorkGroups& num_workgroups,\n-      absl::Span<const DeviceMemoryBase> buffers,\n+      absl::Span<const DeviceAddressBase> buffers,\n       const Eigen::ThreadPoolDevice* device) const;\n \n   tsl::AsyncValueRef<LaunchEvent> Launch("
        },
        {
            "sha": "5d502aeb8dac84fcbafa1b198288d1de08b8b4f6",
            "filename": "third_party/xla/xla/backends/cpu/runtime/kernel_c_api.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_c_api.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_c_api.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_c_api.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -65,7 +65,7 @@ typedef struct XLA_CPU_WorkGroupId {\n   uint64_t z;\n } XLA_CPU_WorkGroupId;\n \n-// A CPU kernel argument that corresponds to se::DeviceMemoryBase.\n+// A CPU kernel argument that corresponds to se::DeviceAddressBase.\n typedef struct XLA_CPU_KernelArg {\n   void* data;\n   size_t size;"
        },
        {
            "sha": "2ef0c02b44066ee7bc0815b03e0880f83b6eff2c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/kernel_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_test.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -65,10 +65,10 @@ TEST(KernelTest, InternalAddition1D) {\n   std::vector<int32_t> rhs = {5, 6, 7, 8};\n   std::vector<int32_t> out = {0, 0, 0, 0};\n \n-  Kernel::DeviceMemoryBase lhs_mem(lhs.data(), lhs.size() * sizeof(int32_t));\n-  Kernel::DeviceMemoryBase rhs_mem(rhs.data(), rhs.size() * sizeof(int32_t));\n-  Kernel::DeviceMemoryBase out_mem(out.data(), out.size() * sizeof(int32_t));\n-  std::vector<Kernel::DeviceMemoryBase> args = {lhs_mem, rhs_mem, out_mem};\n+  Kernel::DeviceAddressBase lhs_mem(lhs.data(), lhs.size() * sizeof(int32_t));\n+  Kernel::DeviceAddressBase rhs_mem(rhs.data(), rhs.size() * sizeof(int32_t));\n+  Kernel::DeviceAddressBase out_mem(out.data(), out.size() * sizeof(int32_t));\n+  std::vector<Kernel::DeviceAddressBase> args = {lhs_mem, rhs_mem, out_mem};\n \n   TF_ASSERT_OK(kernel.Launch(NumWorkGroups{4, 1, 1}, args));\n \n@@ -84,10 +84,10 @@ TEST(KernelTest, InternalAddition3D) {\n   std::vector<int32_t> rhs = {10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21};\n   std::vector<int32_t> out = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n \n-  Kernel::DeviceMemoryBase lhs_mem(lhs.data(), lhs.size() * sizeof(int32_t));\n-  Kernel::DeviceMemoryBase rhs_mem(rhs.data(), rhs.size() * sizeof(int32_t));\n-  Kernel::DeviceMemoryBase out_mem(out.data(), out.size() * sizeof(int32_t));\n-  std::vector<Kernel::DeviceMemoryBase> args = {lhs_mem, rhs_mem, out_mem};\n+  Kernel::DeviceAddressBase lhs_mem(lhs.data(), lhs.size() * sizeof(int32_t));\n+  Kernel::DeviceAddressBase rhs_mem(rhs.data(), rhs.size() * sizeof(int32_t));\n+  Kernel::DeviceAddressBase out_mem(out.data(), out.size() * sizeof(int32_t));\n+  std::vector<Kernel::DeviceAddressBase> args = {lhs_mem, rhs_mem, out_mem};\n \n   TF_ASSERT_OK(kernel.Launch(NumWorkGroups{2, 2, 3}, args));\n "
        },
        {
            "sha": "74a48f644a9ebb0810c2eb0012b8fb36c4a8e5d5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -46,7 +46,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/work_group.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\""
        },
        {
            "sha": "688e4125b4a52234f0690cf0da3eb70c4ed4cfb1",
            "filename": "third_party/xla/xla/backends/cpu/runtime/kernel_thunk_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fkernel_thunk_test.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -204,8 +204,8 @@ TEST(KernelThunkInvariantBuffersTest,\n   // But runtime output buffer overlaps with invariant input buffer.\n   std::array<float, 5> runtime_buffer;\n   BufferAllocations runtime_allocations(BufferAllocations::Buffers{\n-      se::DeviceMemoryBase(runtime_buffer.data(), 16),\n-      se::DeviceMemoryBase(runtime_buffer.data() + 1, 16)});\n+      se::DeviceAddressBase(runtime_buffer.data(), 16),\n+      se::DeviceAddressBase(runtime_buffer.data() + 1, 16)});\n   Thunk::ExecuteParams params = {&host_kernels, &runtime_allocations};\n \n   auto execute_event = thunk->Execute(params);"
        },
        {
            "sha": "15a42816eeab79880c6c40f2a2a5fc5fdb667522",
            "filename": "third_party/xla/xla/backends/cpu/runtime/logical_id_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Flogical_id_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Flogical_id_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Flogical_id_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -81,7 +81,7 @@ template <LogicalIdKind logical_id_kind>\n tsl::AsyncValueRef<typename LogicalIdThunk<logical_id_kind>::ExecuteEvent>\n LogicalIdThunk<logical_id_kind>::Execute(const ExecuteParams& params) {\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase logical_id_data,\n+      se::DeviceAddressBase logical_id_data,\n       params.buffer_allocations->GetDeviceAddress(logical_id_buffer_));\n \n   TF_RET_CHECK(logical_id_data.size() == sizeof(int32_t))"
        },
        {
            "sha": "d29bbe2ae3f71c9c267190ffbe6d80f22eb66771",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -55,7 +55,7 @@ onednn_graph_cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:object_pool\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/mkl:onednn\",\n         \"//xla/tsl/platform:errors\",\n@@ -133,7 +133,7 @@ cc_library(\n         \"//xla/service/cpu:onednn_matmul\",\n         \"//xla/service/cpu:onednn_memory_util\",\n         \"//xla/service/cpu:onednn_softmax\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/mkl:onednn\",\n         \"//xla/tsl/platform:logging\","
        },
        {
            "sha": "b42d9c2a14b52f3c1bcf7866d88132adb1a73389",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_fusion_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_fusion_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -49,8 +49,8 @@ struct OneDnnFusionThunk::OneDnnRuntime {\n \n   tsl::AsyncValueRef<OneDnnFusionThunk::ExecuteEvent> Invoke(\n       Eigen::ThreadPoolInterface* thread_pool,\n-      absl::Span<se::DeviceMemoryBase> arguments,\n-      absl::Span<se::DeviceMemoryBase> results);\n+      absl::Span<se::DeviceAddressBase> arguments,\n+      absl::Span<se::DeviceAddressBase> results);\n \n   OneDnnFusion fusion;\n \n@@ -72,8 +72,8 @@ OneDnnFusionThunk::OneDnnRuntime::OneDnnRuntime(\n tsl::AsyncValueRef<OneDnnFusionThunk::ExecuteEvent>\n OneDnnFusionThunk::OneDnnRuntime::Invoke(\n     Eigen::ThreadPoolInterface* thread_pool,\n-    absl::Span<se::DeviceMemoryBase> arguments,\n-    absl::Span<se::DeviceMemoryBase> results) {\n+    absl::Span<se::DeviceAddressBase> arguments,\n+    absl::Span<se::DeviceAddressBase> results) {\n   // Number of arguments and results must match the number of logical tensors.\n   TF_RET_CHECK(arguments.size() == fusion.parameters.size())\n       << \"Arguments size mismatch\";\n@@ -172,7 +172,7 @@ tsl::AsyncValueRef<OneDnnFusionThunk::ExecuteEvent> OneDnnFusionThunk::Execute(\n   }\n \n   // Resolve device memory for arguments.\n-  absl::InlinedVector<se::DeviceMemoryBase, 8> arguments_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 8> arguments_buffers;\n   arguments_buffers.resize(arguments_.size());\n   for (size_t i = 0; i < arguments_.size(); ++i) {\n     Argument& argument = arguments_[i];\n@@ -188,7 +188,7 @@ tsl::AsyncValueRef<OneDnnFusionThunk::ExecuteEvent> OneDnnFusionThunk::Execute(\n   }\n \n   // Resolve device memory for results.\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> results_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> results_buffers;\n   results_buffers.resize(results_.size());\n   for (size_t i = 0; i < results_.size(); ++i) {\n     Result& result = results_[i];"
        },
        {
            "sha": "bf5e0df3585419157b8a1a914cefd16478a63c67",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n #include \"xla/service/cpu/onednn_matmul.h\"\n #include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/onednn_softmax.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -167,7 +167,7 @@ tsl::AsyncValueRef<OneDnnOpThunk::ExecuteEvent> OneDnnOpThunk::Execute(\n   runtime->resources.arg_memrefs.reserve(num_operands);\n   for (size_t i = 0; i < num_operands; ++i) {\n     const auto& shape = op_buffers_.arguments_shapes[i];\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase arg,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase arg,\n                         params.buffer_allocations->GetDeviceAddress(\n                             op_buffers_.arguments_buffers[i]));\n \n@@ -185,7 +185,7 @@ tsl::AsyncValueRef<OneDnnOpThunk::ExecuteEvent> OneDnnOpThunk::Execute(\n   runtime->resources.result_memrefs.reserve(num_results);\n   for (size_t i = 0; i < num_results; ++i) {\n     const auto& shape = op_buffers_.results_shapes[i];\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase res,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddressBase res,\n                         params.buffer_allocations->GetDeviceAddress(\n                             op_buffers_.results_buffers[i]));\n "
        },
        {
            "sha": "b77eaeb2b6150cdd5ac0adf1a732bad556a6fd03",
            "filename": "third_party/xla/xla/backends/cpu/runtime/outfeed_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Foutfeed_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -65,7 +65,7 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> OutfeedThunk::Execute(\n \n   for (OutfeedBuffer& outfeed_buffer : outfeed_buffers_) {\n     TF_ASSIGN_OR_RETURN(\n-        se::DeviceMemoryBase outfeed_data,\n+        se::DeviceAddressBase outfeed_data,\n         params.buffer_allocations->GetDeviceAddress(outfeed_buffer.slice));\n \n     VLOG(3) << absl::StreamFormat("
        },
        {
            "sha": "db8286ff34f72f83b0f87da61316c88c4dd452fe",
            "filename": "third_party/xla/xla/backends/cpu/runtime/rng_state_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Frng_state_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Frng_state_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Frng_state_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -50,7 +50,7 @@ RngGetAndUpdateStateThunk::RngGetAndUpdateStateThunk(\n tsl::AsyncValueRef<Thunk::ExecuteEvent> RngGetAndUpdateStateThunk::Execute(\n     const ExecuteParams& params) {\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase state_data,\n+      se::DeviceAddressBase state_data,\n       params.buffer_allocations->GetDeviceAddress(state_buffer_));\n \n   if (state_data.size() != sizeof(absl::int128)) {"
        },
        {
            "sha": "11bd0aa61d2198cef9458c632aa7f5c7fe6952f2",
            "filename": "third_party/xla/xla/backends/cpu/runtime/sort_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fsort_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -46,7 +46,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -164,13 +164,13 @@ SortThunk::SortThunk(Info info, absl::Span<const Input> inputs,\n \n // Sorts `data` of the given `shape` along the `dimension` inplace.\n static void SortInplace(const SortThunk::SortDims& sort_dims,\n-                        absl::Span<se::DeviceMemoryBase> data,\n+                        absl::Span<se::DeviceAddressBase> data,\n                         absl::Span<const Shape> shapes, bool is_stable,\n                         SortThunk::LessThan* less_than,\n                         std::optional<SortThunk::SortDirection> direction) {\n   absl::InlinedVector<std::byte*, 16> raw_data;\n   absl::c_transform(data, std::back_inserter(raw_data),\n-                    [](const se::DeviceMemoryBase& mem) {\n+                    [](const se::DeviceAddressBase& mem) {\n                       return reinterpret_cast<std::byte*>(mem.opaque());\n                     });\n \n@@ -210,7 +210,7 @@ tsl::AsyncValueRef<SortThunk::ExecuteEvent> SortThunk::Execute(\n       \"Sort %d inputs along dimension %d (is_stable=%v)\", inputs_.size(),\n       dimension_, is_stable_);\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 8> data;\n+  absl::InlinedVector<se::DeviceAddressBase, 8> data;\n   data.reserve(inputs_.size());\n \n   absl::InlinedVector<Shape, 8> shapes;"
        },
        {
            "sha": "4c438c59dcf6322ca33fe2097c421d6d90e6a712",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_executor_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor_test.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -41,7 +41,7 @@ limitations under the License.\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -160,10 +160,10 @@ AddI32Thunk::AddI32Thunk(std::string name,\n absl::Status AddI32Thunk::Execute(const BufferAllocations* allocations,\n                                   BufferAllocation::Slice src_slice,\n                                   BufferAllocation::Slice dst_slice) {\n-  TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase src,\n+  TF_ASSIGN_OR_RETURN(se::DeviceAddressBase src,\n                       allocations->GetDeviceAddress(src_slice));\n \n-  TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase dst,\n+  TF_ASSIGN_OR_RETURN(se::DeviceAddressBase dst,\n                       allocations->GetDeviceAddress(dst_slice));\n \n   CHECK_EQ(src.size() % sizeof(int32_t), 0);"
        },
        {
            "sha": "0d515fd787b1fb4e0977d23b7edf9bc801de3364",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_proto_serdes.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -75,7 +75,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -1600,7 +1600,7 @@ static absl::StatusOr<std::unique_ptr<YnnFusionThunk>> YnnFusionThunkFromProto(\n   }\n \n   absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers)>\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers)>\n       builder;\n   absl::Span<const int64_t> captured_arguments_ids;\n   if (hlo->opcode() == HloOpcode::kDot) {"
        },
        {
            "sha": "5a8b956aeb8ac2edc28ee6c4b14c2a087a390ef0",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_testlib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_testlib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_testlib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_testlib.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/buffer_allocations.h\"\n #include \"xla/literal.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace xla::cpu {\n \n@@ -43,7 +43,7 @@ BufferAllocation::Slice CreateBufferAllocationSlice(\n }\n \n BufferAllocations CreateBufferAllocations(absl::Span<Literal*> literals) {\n-  std::vector<se::DeviceMemoryBase> buffers;\n+  std::vector<se::DeviceAddressBase> buffers;\n   buffers.reserve(literals.size());\n \n   for (auto* literal : literals) {"
        },
        {
            "sha": "7a7c9190d3cc4df9fa4b2fd5eec82a87f691ae48",
            "filename": "third_party/xla/xla/backends/cpu/runtime/topk_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Ftopk_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/backends/cpu/runtime/topk_lib.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -53,13 +53,13 @@ absl::StatusOr<std::unique_ptr<TopKThunk>> TopKThunk::Create(\n tsl::AsyncValueRef<Thunk::ExecuteEvent> TopKThunk::Execute(\n     const ExecuteParams& params) {\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase values,\n+      se::DeviceAddressBase values,\n       params.buffer_allocations->GetDeviceAddress(values_buffer_));\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase output,\n+      se::DeviceAddressBase output,\n       params.buffer_allocations->GetDeviceAddress(output_buffer_));\n   TF_ASSIGN_OR_RETURN(\n-      se::DeviceMemoryBase indices,\n+      se::DeviceAddressBase indices,\n       params.buffer_allocations->GetDeviceAddress(indices_buffer_));\n \n   internal::TopK<float>(batch_size_, input_size_, k_,"
        },
        {
            "sha": "bc931048f06475fb3b74209be785e1a3b7e11bbc",
            "filename": "third_party/xla/xla/backends/cpu/runtime/while_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -38,7 +38,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -86,7 +86,7 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> WhileThunk::Execute(\n \n   const BufferAllocations* allocations = params.buffer_allocations;\n \n-  se::DeviceMemoryBase cond_data;\n+  se::DeviceAddressBase cond_data;\n   if (ShouldCheckBufferSlices()) {\n     TF_ASSIGN_OR_RETURN(cond_data, allocations->GetDeviceAddress(cond_buffer_));\n   } else {"
        },
        {
            "sha": "f65492a5da010fbd4bf9af1b81140a974917acdf",
            "filename": "third_party/xla/xla/backends/cpu/runtime/while_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwhile_thunk_test.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -115,7 +115,7 @@ class CondThunk : public Thunk {\n     auto event = tsl::MakeConstructedAsyncValueRef<ExecuteEvent>();\n \n     TF_ASSIGN_OR_RETURN(\n-        se::DeviceMemoryBase predicate_mem,\n+        se::DeviceAddressBase predicate_mem,\n         params.buffer_allocations->GetDeviceAddress(pred_slice_));\n     bool* predicate = reinterpret_cast<bool*>(predicate_mem.opaque());\n \n@@ -146,7 +146,7 @@ class BodyThunk : public Thunk {\n     auto event = tsl::MakeConstructedAsyncValueRef<ExecuteEvent>();\n \n     TF_ASSIGN_OR_RETURN(\n-        se::DeviceMemoryBase counter_mem,\n+        se::DeviceAddressBase counter_mem,\n         params.buffer_allocations->GetDeviceAddress(counter_slice_));\n \n     int32_t* counter = reinterpret_cast<int32_t*>(counter_mem.opaque());"
        },
        {
            "sha": "9cd96a5e90b63b748dbe98bc8417b792da6784d4",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2FBUILD?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -56,7 +56,7 @@ cc_library(\n         \"//xla/backends/cpu/runtime:convolution_dims\",\n         \"//xla/backends/cpu/runtime:thunk\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n@@ -115,7 +115,7 @@ cc_library(\n         \"//xla/backends/cpu/runtime:dot_dims\",\n         \"//xla/backends/cpu/runtime:thunk\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n@@ -165,7 +165,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:object_pool\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\","
        },
        {
            "sha": "0d83fbec77698d76d562ce46783164a81995c428",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -47,7 +47,7 @@ namespace xla::cpu {\n \n absl::StatusOr<XnnSubgraph> XnnConvolutionThunk::BuildConvolutionSubgraph(\n     absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n   TF_ASSIGN_OR_RETURN(XnnSubgraph subgraph,\n                       CreateXnnSubgraph([&](xnn_subgraph_t* subgraph) {\n                         return xnn_create_subgraph("
        },
        {
            "sha": "7269ddff7f20d9145a905ffd3e8d67c491d890d8",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n@@ -73,7 +73,7 @@ class XnnConvolutionThunk final : public XnnFusionThunk {\n \n   absl::StatusOr<XnnSubgraph> BuildConvolutionSubgraph(\n       absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   ConvolutionSlices convolution_slices_;\n   ConvolutionCanonicalDims convolution_canonical_dims_;"
        },
        {
            "sha": "44ec1b8139bfc5dd761724b60e766578709e3e3f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -47,7 +47,7 @@ namespace xla::cpu {\n \n absl::StatusOr<XnnSubgraph> XnnDotThunk::BuildDotSubgraph(\n     absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n   TF_ASSIGN_OR_RETURN(XnnSubgraph subgraph,\n                       CreateXnnSubgraph([](xnn_subgraph_t* subgraph) {\n                         return xnn_create_subgraph("
        },
        {
            "sha": "448897ad0eb662052ebb9f6758be6e018ec2065f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n@@ -64,7 +64,7 @@ class XnnDotThunk final : public XnnFusionThunk {\n \n   absl::StatusOr<XnnSubgraph> BuildDotSubgraph(\n       absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   DotDimensionNumbers dot_dimensions_;\n   DotSlices dot_slices_;"
        },
        {
            "sha": "6ab367af62fce711288c20eea5d74cff4d3c1cd1",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 15,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n #include \"xla/runtime/buffer_use.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -66,8 +66,8 @@ std::ostream& operator<<(std::ostream& os, XnnFusionThunk::XnnFusionKind kind) {\n struct XnnFusionThunk::XnnExecutable {\n   tsl::AsyncValueRef<XnnFusionThunk::ExecuteEvent> Invoke(\n       const XnnThreadpool& threadpool,\n-      absl::Span<se::DeviceMemoryBase> arguments,\n-      absl::Span<se::DeviceMemoryBase> results,\n+      absl::Span<se::DeviceAddressBase> arguments,\n+      absl::Span<se::DeviceAddressBase> results,\n       absl::FunctionRef<bool(size_t)> is_captured_argument);\n \n   // Resets XNNPACK runtime and subgraph.\n@@ -80,13 +80,14 @@ struct XnnFusionThunk::XnnExecutable {\n   // captured argument, and this is not correct as we can have multiple\n   // arguments allocated to the heap address. This is work in progress, and will\n   // be migrated to a buffer identity passed to XLA by the client (PjRt).\n-  std::vector<se::DeviceMemoryBase> captured_arguments;\n+  std::vector<se::DeviceAddressBase> captured_arguments;\n };\n \n tsl::AsyncValueRef<XnnFusionThunk::ExecuteEvent>\n XnnFusionThunk::XnnExecutable::Invoke(\n-    const XnnThreadpool& threadpool, absl::Span<se::DeviceMemoryBase> arguments,\n-    absl::Span<se::DeviceMemoryBase> results,\n+    const XnnThreadpool& threadpool,\n+    absl::Span<se::DeviceAddressBase> arguments,\n+    absl::Span<se::DeviceAddressBase> results,\n     absl::FunctionRef<bool(size_t)> is_captured_argument) {\n   // Create external values for all arguments and results.\n   absl::InlinedVector<xnn_external_value, 8> external_values;\n@@ -95,14 +96,14 @@ XnnFusionThunk::XnnExecutable::Invoke(\n   // External tensor id for arguments and results.\n   uint32_t id = 0;\n \n-  for (const se::DeviceMemoryBase& argument : arguments) {\n+  for (const se::DeviceAddressBase& argument : arguments) {\n     xnn_external_value value{id++, argument.opaque()};\n     if (!is_captured_argument(value.id)) {\n       external_values.push_back(value);\n     }\n   }\n \n-  for (const se::DeviceMemoryBase& result : results) {\n+  for (const se::DeviceAddressBase& result : results) {\n     xnn_external_value value{id++, result.opaque()};\n     external_values.push_back(value);\n   }\n@@ -128,7 +129,7 @@ absl::Status XnnFusionThunk::XnnExecutable::Reset() {\n absl::StatusOr<XnnFusionThunk::XnnExecutable>\n XnnFusionThunk::CreateXnnExecutable(\n     const XnnThreadpool& threadpool,\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n   bool capturing = !captured_arguments_ids_.empty();\n   VLOG(3) << absl::StreamFormat(\n       \"Create %s XNN executable for `%s` operation: num_created=%d\",\n@@ -165,7 +166,7 @@ XnnFusionThunk::CreateXnnExecutable(\n \n absl::Status XnnFusionThunk::UpdateXnnExecutable(\n     const XnnThreadpool& threadpool, XnnExecutable& executable,\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n   DCHECK(capturing_builder_) << \"XNN executable is not capturing arguments\";\n   DCHECK_EQ(executable.captured_arguments.size(),\n             captured_arguments_ids_.size())\n@@ -206,9 +207,9 @@ absl::Status XnnFusionThunk::UpdateXnnExecutable(\n   return absl::OkStatus();\n }\n \n-std::vector<se::DeviceMemoryBase> XnnFusionThunk::CaptureArguments(\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n-  std::vector<se::DeviceMemoryBase> captured_arguments_ids;\n+std::vector<se::DeviceAddressBase> XnnFusionThunk::CaptureArguments(\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n+  std::vector<se::DeviceAddressBase> captured_arguments_ids;\n   captured_arguments_ids.reserve(captured_arguments_ids_.size());\n   for (int64_t i = 0; i < captured_arguments_ids_.size(); ++i) {\n     int32_t arg_index = captured_arguments_ids_[i];\n@@ -298,7 +299,7 @@ tsl::AsyncValueRef<XnnFusionThunk::ExecuteEvent> XnnFusionThunk::Execute(\n   }\n \n   // Resolve device memory for arguments.\n-  absl::InlinedVector<se::DeviceMemoryBase, 8> arguments_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 8> arguments_buffers;\n   arguments_buffers.resize(arguments_.size());\n   for (size_t i = 0; i < arguments_.size(); ++i) {\n     Argument& argument = arguments_[i];\n@@ -314,7 +315,7 @@ tsl::AsyncValueRef<XnnFusionThunk::ExecuteEvent> XnnFusionThunk::Execute(\n   }\n \n   // Resolve device memory for results.\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> results_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> results_buffers;\n   results_buffers.resize(results_.size());\n   for (size_t i = 0; i < results_.size(); ++i) {\n     Result& result = results_[i];"
        },
        {
            "sha": "21deb08bfd6fd6cb895b4e5436caec5bcd0e2c5c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/runtime/object_pool.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n \n namespace xla::cpu {\n@@ -85,7 +85,7 @@ class XnnFusionThunk : public Thunk {\n   // constant, i.e. convolution filters and one of the dot arguments).\n   using CapturingBuilder = absl::AnyInvocable<absl::StatusOr<XnnSubgraph>(\n       absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers)>;\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers)>;\n \n   static absl::StatusOr<std::unique_ptr<XnnFusionThunk>> Create(\n       Options options, Info info, std::vector<Argument> arguments,\n@@ -138,17 +138,17 @@ class XnnFusionThunk : public Thunk {\n   // Creates XnnExecutable for the fusion operation using one of the builders.\n   absl::StatusOr<XnnExecutable> CreateXnnExecutable(\n       const XnnThreadpool& threadpool,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   // Updates XnnExecutable to the XNN subgraph constructed with the given\n   // arguments buffers.\n   absl::Status UpdateXnnExecutable(\n       const XnnThreadpool& threadpool, XnnExecutable& executable,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   // Returns the list of captured arguments buffers.\n-  std::vector<se::DeviceMemoryBase> CaptureArguments(\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+  std::vector<se::DeviceAddressBase> CaptureArguments(\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   XnnFusionKind xnn_fusion_kind_;\n   Options options_;\n@@ -170,7 +170,7 @@ class XnnFusionThunk : public Thunk {\n   // XLA:CPU executable can be called concurrently from multiple threads,\n   // and we need to keep a pool of XNNPACK executables to avoid data races.\n   using XnnExecutablePool = ObjectPool<XnnExecutable, const XnnThreadpool&,\n-                                       absl::Span<const se::DeviceMemoryBase>>;\n+                                       absl::Span<const se::DeviceAddressBase>>;\n   XnnExecutablePool xnn_executable_pool_;\n \n   // The number of XNNPACK executables created for capturing graphs."
        },
        {
            "sha": "5172563e23ca0c2d996205e5a438d820d134e568",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -96,7 +96,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:object_pool\",\n         \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\","
        },
        {
            "sha": "24646f62dc02085cd57048bfb33f130a9e36b1ee",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 15,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/runtime/buffer_use.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -62,8 +62,8 @@ std::ostream& operator<<(std::ostream& os, YnnFusionThunk::YnnFusionKind kind) {\n struct YnnFusionThunk::YnnExecutable {\n   tsl::AsyncValueRef<YnnFusionThunk::ExecuteEvent> Invoke(\n       const YnnThreadpool& threadpool,\n-      absl::Span<se::DeviceMemoryBase> arguments,\n-      absl::Span<se::DeviceMemoryBase> results,\n+      absl::Span<se::DeviceAddressBase> arguments,\n+      absl::Span<se::DeviceAddressBase> results,\n       absl::FunctionRef<bool(size_t)> is_captured_argument);\n \n   // Resets YNNPACK runtime and subgraph.\n@@ -76,7 +76,7 @@ struct YnnFusionThunk::YnnExecutable {\n   // captured argument, and this is not correct as we can have multiple\n   // arguments allocated to the heap address. This is work in progress, and will\n   // be migrated to a buffer identity passed to XLA by the client (PjRt).\n-  std::vector<se::DeviceMemoryBase> captured_arguments;\n+  std::vector<se::DeviceAddressBase> captured_arguments;\n };\n \n namespace {\n@@ -99,8 +99,9 @@ static enum ynn_status set_external_values(\n \n tsl::AsyncValueRef<YnnFusionThunk::ExecuteEvent>\n YnnFusionThunk::YnnExecutable::Invoke(\n-    const YnnThreadpool& threadpool, absl::Span<se::DeviceMemoryBase> arguments,\n-    absl::Span<se::DeviceMemoryBase> results,\n+    const YnnThreadpool& threadpool,\n+    absl::Span<se::DeviceAddressBase> arguments,\n+    absl::Span<se::DeviceAddressBase> results,\n     absl::FunctionRef<bool(size_t)> is_captured_argument) {\n   // Create external values for all arguments and results.\n   absl::InlinedVector<YnnExternalValue, 8> external_values;\n@@ -109,14 +110,14 @@ YnnFusionThunk::YnnExecutable::Invoke(\n   // External tensor id for arguments and results.\n   uint32_t id = 0;\n \n-  for (const se::DeviceMemoryBase& argument : arguments) {\n+  for (const se::DeviceAddressBase& argument : arguments) {\n     YnnExternalValue value{id++, argument.opaque()};\n     if (!is_captured_argument(value.id)) {\n       external_values.push_back(value);\n     }\n   }\n \n-  for (const se::DeviceMemoryBase& result : results) {\n+  for (const se::DeviceAddressBase& result : results) {\n     YnnExternalValue value{id++, result.opaque()};\n     external_values.push_back(value);\n   }\n@@ -143,7 +144,7 @@ absl::Status YnnFusionThunk::YnnExecutable::Reset() {\n absl::StatusOr<YnnFusionThunk::YnnExecutable>\n YnnFusionThunk::CreateYnnExecutable(\n     const YnnThreadpool& threadpool,\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n   bool capturing = !captured_arguments_ids_.empty();\n   VLOG(3) << absl::StreamFormat(\n       \"Create %s YNN executable for `%s` operation: num_created=%d\",\n@@ -179,7 +180,7 @@ YnnFusionThunk::CreateYnnExecutable(\n \n absl::Status YnnFusionThunk::UpdateYnnExecutable(\n     const YnnThreadpool& threadpool, YnnExecutable& executable,\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n   DCHECK(capturing_builder_) << \"YNN executable is not capturing arguments\";\n   DCHECK_EQ(executable.captured_arguments.size(),\n             captured_arguments_ids_.size())\n@@ -219,9 +220,9 @@ absl::Status YnnFusionThunk::UpdateYnnExecutable(\n   return absl::OkStatus();\n }\n \n-std::vector<se::DeviceMemoryBase> YnnFusionThunk::CaptureArguments(\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers) {\n-  std::vector<se::DeviceMemoryBase> captured_arguments_ids;\n+std::vector<se::DeviceAddressBase> YnnFusionThunk::CaptureArguments(\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n+  std::vector<se::DeviceAddressBase> captured_arguments_ids;\n   captured_arguments_ids.reserve(captured_arguments_ids_.size());\n   for (int64_t i = 0; i < captured_arguments_ids_.size(); ++i) {\n     int32_t arg_index = captured_arguments_ids_[i];\n@@ -313,7 +314,7 @@ tsl::AsyncValueRef<YnnFusionThunk::ExecuteEvent> YnnFusionThunk::Execute(\n   }\n \n   // Resolve device memory for arguments.\n-  absl::InlinedVector<se::DeviceMemoryBase, 8> arguments_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 8> arguments_buffers;\n   arguments_buffers.resize(arguments_.size());\n   for (size_t i = 0; i < arguments_.size(); ++i) {\n     Argument& argument = arguments_[i];\n@@ -329,7 +330,7 @@ tsl::AsyncValueRef<YnnFusionThunk::ExecuteEvent> YnnFusionThunk::Execute(\n   }\n \n   // Resolve device memory for results.\n-  absl::InlinedVector<se::DeviceMemoryBase, 4> results_buffers;\n+  absl::InlinedVector<se::DeviceAddressBase, 4> results_buffers;\n   results_buffers.resize(results_.size());\n   for (size_t i = 0; i < results_.size(); ++i) {\n     Result& result = results_[i];"
        },
        {
            "sha": "52148057211b0fa14b6bf62258a913ef69b78e33",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_fusion_thunk.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -38,7 +38,7 @@ limitations under the License.\n #include \"xla/runtime/object_pool.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n \n namespace xla::cpu {\n@@ -84,7 +84,7 @@ class YnnFusionThunk : public Thunk {\n   // constant, i.e. convolution filters and one of the dot arguments).\n   using CapturingBuilder = absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n       absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers)>;\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers)>;\n \n   static absl::StatusOr<std::unique_ptr<YnnFusionThunk>> Create(\n       Options options, Info info, const HloInstruction* hlo,\n@@ -145,17 +145,17 @@ class YnnFusionThunk : public Thunk {\n   // Creates YnnExecutable for the fusion operation using one of the builders.\n   absl::StatusOr<YnnExecutable> CreateYnnExecutable(\n       const YnnThreadpool& threadpool,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   // Updates YnnExecutable to the YNN subgraph constructed with the given\n   // arguments buffers.\n   absl::Status UpdateYnnExecutable(\n       const YnnThreadpool& threadpool, YnnExecutable& executable,\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   // Returns the list of captured arguments buffers.\n-  std::vector<se::DeviceMemoryBase> CaptureArguments(\n-      absl::Span<const se::DeviceMemoryBase> arguments_buffers);\n+  std::vector<se::DeviceAddressBase> CaptureArguments(\n+      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n \n   YnnFusionKind ynn_fusion_kind_;\n   Options options_;\n@@ -181,7 +181,7 @@ class YnnFusionThunk : public Thunk {\n   // XLA:CPU executable can be called concurrently from multiple threads,\n   // and we need to keep a pool of YNNPACK executables to avoid data races.\n   using YnnExecutablePool = ObjectPool<YnnExecutable, const YnnThreadpool&,\n-                                       absl::Span<const se::DeviceMemoryBase>>;\n+                                       absl::Span<const se::DeviceAddressBase>>;\n   YnnExecutablePool ynn_executable_pool_;\n \n   // The number of YNNPACK executables created for capturing graphs."
        },
        {
            "sha": "145fd16e1ac1d870f21bacdbb165ba791dd446e9",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/literal.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -371,7 +371,7 @@ static ynn_status DefineBatchMatrixMultiply(ynn_subgraph_t subgraph,\n static absl::StatusOr<YnnSubgraph> EmitYnnDotSubgraph(\n     const HloDotInstruction* dot,\n     std::vector<std::unique_ptr<Literal>>& literals,\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers,\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers,\n     bool capture_rhs) {\n   TF_ASSIGN_OR_RETURN(\n       YnnSubgraph subgraph, CreateYnnSubgraph([&](ynn_subgraph_t* subgraph) {\n@@ -443,7 +443,7 @@ static absl::StatusOr<YnnSubgraph> EmitYnnDotSubgraph(\n }\n \n absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers)>>\n EmitYnnFusionBuilder(const HloComputation* computation) {\n   // We do not support non-array parameters for YNNPACK operations.\n   for (auto& param : computation->parameter_instructions()) {\n@@ -460,19 +460,22 @@ EmitYnnFusionBuilder(const HloComputation* computation) {\n                            computation->root_instruction()->shape().ToString());\n   }\n \n-  return [computation, literals = std::vector<std::unique_ptr<Literal>>()](\n-             absl::Span<const se::DeviceMemoryBase> arguments_buffers) mutable {\n-    return EmitYnnSubgraph(computation, literals);\n-  };\n+  return\n+      [computation, literals = std::vector<std::unique_ptr<Literal>>()](\n+          absl::Span<const se::DeviceAddressBase> arguments_buffers) mutable {\n+        return EmitYnnSubgraph(computation, literals);\n+      };\n }\n \n absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers)>>\n EmitYnnDotBuilder(const HloDotInstruction* dot, bool capture_rhs) {\n-  return [dot, capture_rhs, literals = std::vector<std::unique_ptr<Literal>>()](\n-             absl::Span<const se::DeviceMemoryBase> arguments_buffers) mutable {\n-    return EmitYnnDotSubgraph(dot, literals, arguments_buffers, capture_rhs);\n-  };\n+  return\n+      [dot, capture_rhs, literals = std::vector<std::unique_ptr<Literal>>()](\n+          absl::Span<const se::DeviceAddressBase> arguments_buffers) mutable {\n+        return EmitYnnDotSubgraph(dot, literals, arguments_buffers,\n+                                  capture_rhs);\n+      };\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "ff8a2949926979b9ac0cb449afc27a091536b02c",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3d990196e3c12a04a0f62cc6828a7cd10111396e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h?ref=3d990196e3c12a04a0f62cc6828a7cd10111396e",
            "patch": "@@ -21,16 +21,16 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace xla::cpu {\n \n absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers)>>\n EmitYnnFusionBuilder(const HloComputation* computation);\n \n absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n-    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n+    absl::Span<const se::DeviceAddressBase> arguments_buffers)>>\n EmitYnnDotBuilder(const HloDotInstruction* dot, bool capture_rhs);\n \n }  // namespace xla::cpu"
        }
    ],
    "stats": {
        "total": 672,
        "additions": 339,
        "deletions": 333
    }
}