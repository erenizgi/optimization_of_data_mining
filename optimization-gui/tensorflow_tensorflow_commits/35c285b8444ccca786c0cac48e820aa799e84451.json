{
    "author": "ezhulenev",
    "message": "[xla:cpu] Remove ACL support from XLA:CPU\n\nPiperOrigin-RevId: 829087448",
    "sha": "35c285b8444ccca786c0cac48e820aa799e84451",
    "files": [
        {
            "sha": "d64cfd799537096ee58a2f72ad90d4f5c4fcf3f3",
            "filename": "tensorflow/python/tools/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/35c285b8444ccca786c0cac48e820aa799e84451/tensorflow%2Fpython%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/35c285b8444ccca786c0cac48e820aa799e84451/tensorflow%2Fpython%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftools%2FBUILD?ref=35c285b8444ccca786c0cac48e820aa799e84451",
            "patch": "@@ -583,7 +583,6 @@ tf_cc_test(\n         \":aot_compiled_x_matmul_y_small\",\n         \":aot_compiled_x_plus_y\",\n         # LINT.ThenChange(//tensorflow/tools/pip_package/xla_build/pip_test/run_xla_aot_test.sh)\n-        \"@local_xla//xla/service/cpu:runtime_matmul_acl\",\n         \"//tensorflow/core:test\",\n         \"//tensorflow/core/platform:logging\",\n         \"@eigen_archive//:eigen3\","
        },
        {
            "sha": "e132ab1ccb3526c06f55431e9b2d89ce50d2d9d3",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/35c285b8444ccca786c0cac48e820aa799e84451/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/35c285b8444ccca786c0cac48e820aa799e84451/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=35c285b8444ccca786c0cac48e820aa799e84451",
            "patch": "@@ -1,10 +1,3 @@\n-# Description:\n-#    LLVM-based CPU backend for XLA.\n-\n-load(\n-    \"//third_party/compute_library:build_defs.bzl\",\n-    \"acl_deps\",\n-)\n load(\n     \"//xla:xla.default.bzl\",\n     \"xla_cc_test\",\n@@ -1123,46 +1116,6 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"runtime_matmul_acl\",\n-    srcs = [\"runtime_matmul_acl.cc\"],\n-    hdrs = [\"runtime_matmul_acl.h\"],\n-    copts = tsl_copts(),\n-    visibility = [\"//visibility:public\"],\n-    deps = [\n-        \":runtime_lightweight_check\",\n-        \":runtime_matmul\",\n-        \"//xla:executable_run_options\",\n-        \"//xla/tsl/platform:dynamic_annotations\",\n-        \"@com_google_absl//absl/base\",\n-        \"@eigen_archive//:eigen3\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:types\",\n-    ] + acl_deps(),\n-)\n-\n-cc_library(\n-    name = \"runtime_conv2d_acl\",\n-    srcs = [\n-        \"runtime_conv2d_acl.cc\",\n-    ],\n-    hdrs = [\"runtime_conv2d_acl.h\"],\n-    copts = tsl_copts(),\n-    visibility = [\"//visibility:public\"],\n-    deps = [\n-        \":runtime_conv2d\",\n-        \":runtime_lightweight_check\",\n-        \":runtime_single_threaded_conv2d\",\n-        \"//xla:executable_run_options\",\n-        \"//xla/tsl/framework/convolution:eigen_helpers\",\n-        \"//xla/tsl/platform:dynamic_annotations\",\n-        \"@com_google_absl//absl/base\",\n-        \"@eigen_archive//:eigen3\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:types\",\n-    ] + acl_deps(),\n-)\n-\n cc_library(\n     name = \"runtime_single_threaded_conv2d\",\n     srcs = [\"runtime_single_threaded_conv2d.cc\"],\n@@ -1280,7 +1233,6 @@ xla_cc_test(\n         \":cpu_runtime\",\n         \":runtime_custom_call_status\",\n         \":runtime_matmul\",\n-        \":runtime_matmul_acl\",\n         \":runtime_single_threaded_matmul\",\n         \"//xla:array2d\",\n         \"//xla:executable_run_options\","
        },
        {
            "sha": "284cc42d9417566b29721fdb396ab5c4af1061e1",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/35c285b8444ccca786c0cac48e820aa799e84451/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/35c285b8444ccca786c0cac48e820aa799e84451/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime_test.cc?ref=35c285b8444ccca786c0cac48e820aa799e84451",
            "patch": "@@ -12,30 +12,31 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n-#include <cstdint>\n-#include <utility>\n-#define EIGEN_USE_THREADS\n+\n #include \"xla/service/cpu/cpu_runtime.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <string>\n #include <tuple>\n+#include <utility>\n \n #include \"absl/strings/str_format.h\"\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n #include \"xla/array2d.h\"\n #include \"xla/client/local_client.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/service/cpu/runtime_custom_call_status.h\"\n #include \"xla/service/cpu/runtime_matmul.h\"\n-#include \"xla/service/cpu/runtime_matmul_acl.h\"\n #include \"xla/service/cpu/runtime_single_threaded_matmul.h\"\n #include \"xla/service/custom_call_status_internal.h\"\n #include \"xla/types.h\"\n #include \"tsl/platform/env.h\"\n #include \"tsl/platform/logging.h\"\n #include \"tsl/platform/test.h\"\n \n+#define EIGEN_USE_THREADS\n+#include \"unsupported/Eigen/CXX11/Tensor\"\n+\n namespace xla {\n namespace {\n "
        },
        {
            "sha": "9db849566865f0c261c9ce639ca6f0f7b386401c",
            "filename": "third_party/xla/xla/service/cpu/runtime_conv2d_acl.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 280,
            "changes": 280,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d_acl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d_acl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d_acl.cc?ref=ea9f8804e917315408ea9fbfd9044e33a0e556fe",
            "patch": "@@ -1,280 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/executable_run_options.h\"\n-#include \"xla/tsl/platform/dynamic_annotations.h\"\n-#include \"tsl/platform/types.h\"\n-#ifdef XLA_CPU_USE_ACL\n-#include \"absl/base/call_once.h\"\n-#include \"xla/service/cpu/runtime_conv2d.h\"\n-#include \"xla/service/cpu/runtime_conv2d_acl.h\"\n-#include \"xla/service/cpu/runtime_lightweight_check.h\"\n-#include \"tsl/platform/logging.h\"\n-#define EIGEN_USE_THREADS\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-\n-namespace {\n-int32_t ACLDepthwiseConvImpl(\n-    const void* run_options_ptr, float* out, float* lhs, float* rhs,\n-    int64_t input_batch, int64_t input_rows, int64_t input_cols,\n-    int64_t input_channels, int64_t kernel_rows, int64_t kernel_cols,\n-    int64_t kernel_channels, int64_t kernel_filters, int64_t output_rows,\n-    int64_t output_cols, int64_t row_stride, int64_t col_stride,\n-    int64_t padding_top, int64_t padding_bottom, int64_t padding_left,\n-    int64_t padding_right, int64_t lhs_row_dilation, int64_t lhs_col_dilation,\n-    int64_t rhs_row_dilation, int64_t rhs_col_dilation,\n-    int64_t feature_group_count) {\n-  VLOG(1) << \"Accelerate with ACLDepthwiseConvImpl\";\n-  /* TODO: optimize this object creation along with tensor init and\n-   * gemm configuration by caching the shapes, similar to onednn\n-   * primitive caching feature\n-   */\n-  struct acl_depthwise_conv_obj_t acl_conv_obj;\n-  struct acl_conv_conf_t acl_conf;\n-\n-  /* ir_emitter HandleConvolution ensures the below preconditions before\n-   * dispatching it to ACL layout: NHWC format: FP32 Number of feature groups\n-   * matches the input channels source and kernel dilation is: 1\n-   */\n-  acl_conf.dilation_info =\n-      arm_compute::Size2D(lhs_col_dilation, lhs_row_dilation);\n-  acl_conf.padstride_info = arm_compute::PadStrideInfo(\n-      col_stride, row_stride, static_cast<unsigned int>(padding_left),\n-      static_cast<unsigned int>(padding_right),\n-      static_cast<unsigned int>(padding_top),\n-      static_cast<unsigned int>(padding_bottom),\n-      arm_compute::DimensionRoundingType::FLOOR);\n-  acl_conf.with_bias = false;\n-\n-  acl_conf.act_info = arm_compute::ActivationLayerInfo();\n-\n-  acl_conf.input_info = arm_compute::TensorInfo(\n-      arm_compute::TensorShape(input_channels, input_cols, input_rows,\n-                               input_batch),\n-      1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);\n-  acl_conf.kernel_info = arm_compute::TensorInfo(\n-      arm_compute::TensorShape(kernel_filters, kernel_cols, kernel_rows), 1,\n-      arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);\n-  acl_conf.output_info = arm_compute::TensorInfo(\n-      arm_compute::TensorShape(kernel_filters, output_cols, output_rows,\n-                               input_batch),\n-      1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);\n-\n-  auto acl_st = arm_compute::NEDepthwiseConvolutionLayer::validate(\n-      &acl_conf.input_info, &acl_conf.kernel_info,\n-      /*acp.with_bias */ nullptr, &acl_conf.output_info,\n-      acl_conf.padstride_info, kernel_channels /*depth_multiplier*/,\n-      acl_conf.act_info, acl_conf.dilation_info);\n-  if (acl_st.error_code() != arm_compute::ErrorCode::OK) {\n-    VLOG(1) << \" Gemm conv validation failed\";\n-    return -1;\n-  }\n-\n-  static absl::once_flag flag_once;\n-  const xla::ExecutableRunOptions* run_options =\n-      static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n-  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n-  const Eigen::ThreadPoolDevice* tpd =\n-      (Eigen::ThreadPoolDevice*)(run_options->intra_op_thread_pool());\n-  // The threads in Compute Library are bound for the cores 0..max_threads-1\n-  const int max_threads = tpd->numThreads();\n-\n-  // arm_compute::Scheduler does not support concurrent access thus a\n-  // workaround here restricts it to only one call\n-  absl::call_once(flag_once, [&]() {\n-    arm_compute::Scheduler::get().set_num_threads(max_threads);\n-  });\n-\n-  // configure the acl obj with the config\n-  acl_conv_obj.input_tensor.allocator()->init(acl_conf.input_info);\n-  acl_conv_obj.kernel_tensor.allocator()->init(acl_conf.kernel_info);\n-  acl_conv_obj.output_tensor.allocator()->init(acl_conf.output_info);\n-\n-  acl_conv_obj.depthwise_conv.configure(\n-      &acl_conv_obj.input_tensor, &acl_conv_obj.kernel_tensor, nullptr,\n-      &acl_conv_obj.output_tensor, acl_conf.padstride_info,\n-      kernel_channels /*depth_multiplier*/, acl_conf.act_info,\n-      acl_conf.dilation_info);\n-\n-  /* import_memory() and free() methods do not allocate/free any additional\n-   * memory, only acquire/release pointers.\n-   */\n-  acl_conv_obj.input_tensor.allocator()->import_memory(lhs);\n-  acl_conv_obj.kernel_tensor.allocator()->import_memory(rhs);\n-  acl_conv_obj.output_tensor.allocator()->import_memory(out);\n-\n-  acl_conv_obj.depthwise_conv.run();\n-\n-  acl_conv_obj.input_tensor.allocator()->free();\n-  acl_conv_obj.kernel_tensor.allocator()->free();\n-  acl_conv_obj.output_tensor.allocator()->free();\n-\n-  return 0;\n-}\n-\n-int32_t ACLGemmConvImpl(const void* run_options_ptr, float* out, float* lhs,\n-                        float* rhs, int64_t input_batch, int64_t input_rows,\n-                        int64_t input_cols, int64_t input_channels,\n-                        int64_t kernel_rows, int64_t kernel_cols,\n-                        int64_t kernel_channels, int64_t kernel_filters,\n-                        int64_t output_rows, int64_t output_cols,\n-                        int64_t row_stride, int64_t col_stride,\n-                        int64_t padding_top, int64_t padding_bottom,\n-                        int64_t padding_left, int64_t padding_right,\n-                        int64_t lhs_row_dilation, int64_t lhs_col_dilation,\n-                        int64_t rhs_row_dilation, int64_t rhs_col_dilation) {\n-  VLOG(1) << \"Accelerate with ACLGemmConvImpl\";\n-  /* TODO: optimize this object creation along with tensor init and\n-   * gemm configuration by caching the shapes, similar to onednn\n-   * primitive caching feature\n-   */\n-  struct acl_gemm_conv_obj_t acl_conv_obj;\n-  struct acl_conv_conf_t acl_conf;\n-\n-  /* TODO: add TF_XLA_* flag for runtime control of fast math mode\n-   */\n-  acl_conf.fast_math = true;\n-\n-  /* ir_emitter HandleConvolution ensures the below preconditions before\n-   * dispatching it to ACL layout: NHWC format: FP32 Number of feature groups: 1\n-   *  source and kernel dilation is: 1\n-   */\n-  acl_conf.dilation_info =\n-      arm_compute::Size2D(lhs_col_dilation, lhs_row_dilation);\n-  acl_conf.padstride_info = arm_compute::PadStrideInfo(\n-      col_stride, row_stride, static_cast<unsigned int>(padding_left),\n-      static_cast<unsigned int>(padding_right),\n-      static_cast<unsigned int>(padding_top),\n-      static_cast<unsigned int>(padding_bottom),\n-      arm_compute::DimensionRoundingType::FLOOR);\n-  acl_conf.with_bias = false;\n-\n-  acl_conf.input_info = arm_compute::TensorInfo(\n-      arm_compute::TensorShape(input_channels, input_cols, input_rows,\n-                               input_batch),\n-      1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);\n-  acl_conf.kernel_info = arm_compute::TensorInfo(\n-      arm_compute::TensorShape(input_channels, kernel_cols, kernel_rows,\n-                               kernel_filters),\n-      1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);\n-  acl_conf.output_info = arm_compute::TensorInfo(\n-      arm_compute::TensorShape(kernel_filters, output_cols, output_rows,\n-                               input_batch),\n-      1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);\n-  acl_conf.act_info = arm_compute::ActivationLayerInfo();\n-\n-  // Validate convolution manually to check for return status\n-  auto acl_st = arm_compute::NEGEMMConvolutionLayer::validate(\n-      &acl_conf.input_info, &acl_conf.kernel_info,\n-      /*acp.with_bias */ nullptr, &acl_conf.output_info,\n-      acl_conf.padstride_info, acl_conf.kernel_wei_info, acl_conf.dilation_info,\n-      acl_conf.act_info, acl_conf.fast_math);\n-  if (acl_st.error_code() != arm_compute::ErrorCode::OK) {\n-    VLOG(1) << \" Gemm conv validation failed\";\n-    return -1;\n-  }\n-\n-  static absl::once_flag flag_once;\n-  const xla::ExecutableRunOptions* run_options =\n-      static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n-  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n-  const Eigen::ThreadPoolDevice* tpd =\n-      (Eigen::ThreadPoolDevice*)(run_options->intra_op_thread_pool());\n-  // The threads in Compute Library are bound for the cores 0..max_threads-1\n-  const int max_threads = tpd->numThreads();\n-\n-  // arm_compute::Scheduler does not support concurrent access thus a\n-  // workaround here restricts it to only one call\n-  absl::call_once(flag_once, [&]() {\n-    arm_compute::Scheduler::get().set_num_threads(max_threads);\n-  });\n-\n-  // configure the acl obj with the config\n-  acl_conv_obj.input_tensor.allocator()->init(acl_conf.input_info);\n-  acl_conv_obj.kernel_tensor.allocator()->init(acl_conf.kernel_info);\n-  acl_conv_obj.output_tensor.allocator()->init(acl_conf.output_info);\n-\n-  // Configure GEMM\n-  acl_conv_obj.gemm_conv.configure(\n-      &acl_conv_obj.input_tensor, &acl_conv_obj.kernel_tensor, nullptr,\n-      &acl_conv_obj.output_tensor, acl_conf.padstride_info,\n-      acl_conf.kernel_wei_info, acl_conf.dilation_info, acl_conf.act_info,\n-      acl_conf.fast_math);\n-\n-  /* import_memory() and free() methods do not allocate/free any additional\n-   * memory, only acquire/release pointers.\n-   */\n-  acl_conv_obj.input_tensor.allocator()->import_memory(lhs);\n-  acl_conv_obj.kernel_tensor.allocator()->import_memory(rhs);\n-  acl_conv_obj.output_tensor.allocator()->import_memory(out);\n-\n-  acl_conv_obj.gemm_conv.run();\n-\n-  acl_conv_obj.input_tensor.allocator()->free();\n-  acl_conv_obj.kernel_tensor.allocator()->free();\n-  acl_conv_obj.output_tensor.allocator()->free();\n-\n-  return 0;\n-}\n-}  // namespace\n-\n-ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_ACLConv2DF32(\n-    const void* run_options_ptr, float* out, float* lhs, float* rhs,\n-    int64_t input_batch, int64_t input_rows, int64_t input_cols,\n-    int64_t input_channels, int64_t kernel_rows, int64_t kernel_cols,\n-    int64_t kernel_channels, int64_t kernel_filters, int64_t output_rows,\n-    int64_t output_cols, int64_t row_stride, int64_t col_stride,\n-    int64_t padding_top, int64_t padding_bottom, int64_t padding_left,\n-    int64_t padding_right, int64_t lhs_row_dilation, int64_t lhs_col_dilation,\n-    int64_t rhs_row_dilation, int64_t rhs_col_dilation,\n-    int64_t feature_group_count) {\n-  bool fallback_to_eigen = false;\n-\n-  if (lhs_row_dilation > 1 || lhs_col_dilation > 1 ||\n-      ((feature_group_count > 1) && (input_channels != feature_group_count))) {\n-    fallback_to_eigen = true;\n-  } else if ((feature_group_count > 1) &&\n-             (input_channels == feature_group_count)) {\n-    if (ACLDepthwiseConvImpl(\n-            run_options_ptr, out, lhs, rhs, input_batch, input_rows, input_cols,\n-            input_channels, kernel_rows, kernel_cols, kernel_channels,\n-            kernel_filters, output_rows, output_cols, row_stride, col_stride,\n-            padding_top, padding_bottom, padding_left, padding_right,\n-            lhs_row_dilation, lhs_col_dilation, rhs_row_dilation,\n-            rhs_col_dilation, feature_group_count) < 0)\n-      fallback_to_eigen = true;\n-  } else {\n-    if (ACLGemmConvImpl(run_options_ptr, out, lhs, rhs, input_batch, input_rows,\n-                        input_cols, input_channels, kernel_rows, kernel_cols,\n-                        kernel_channels, kernel_filters, output_rows,\n-                        output_cols, row_stride, col_stride, padding_top,\n-                        padding_bottom, padding_left, padding_right,\n-                        lhs_row_dilation, lhs_col_dilation, rhs_row_dilation,\n-                        rhs_col_dilation) < 0)\n-      fallback_to_eigen = true;\n-  }\n-\n-  if (fallback_to_eigen) {\n-    VLOG(1) << \"XLA conv2d not supported by ACL, fallback to Eigen runtime\";\n-    __xla_cpu_runtime_EigenConv2DF32(\n-        run_options_ptr, out, lhs, rhs, input_batch, input_rows, input_cols,\n-        input_channels, kernel_rows, kernel_cols, kernel_channels,\n-        kernel_filters, output_rows, output_cols, row_stride, col_stride,\n-        padding_top, padding_bottom, padding_left, padding_right,\n-        lhs_row_dilation, lhs_col_dilation, rhs_row_dilation, rhs_col_dilation,\n-        feature_group_count);\n-  }\n-}\n-#endif  // XLA_CPU_USE_ACL"
        },
        {
            "sha": "69a2429ff49f641dc75428e4e5c3d0767df698fd",
            "filename": "third_party/xla/xla/service/cpu/runtime_conv2d_acl.h",
            "status": "removed",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d_acl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d_acl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d_acl.h?ref=ea9f8804e917315408ea9fbfd9044e33a0e556fe",
            "patch": "@@ -1,96 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_SERVICE_CPU_RUNTIME_CONV2D_ACL_H_\n-#define XLA_SERVICE_CPU_RUNTIME_CONV2D_ACL_H_\n-\n-#include \"tsl/platform/types.h\"\n-\n-#ifdef XLA_CPU_USE_ACL\n-#include \"arm_compute/runtime/NEON/NEFunctions.h\"\n-#include \"arm_compute/runtime/NEON/NEScheduler.h\"\n-#include \"utils/Utils.h\"\n-\n-extern \"C\" {\n-struct acl_depthwise_conv_obj_t {\n-  arm_compute::NEDepthwiseConvolutionLayer depthwise_conv;\n-  arm_compute::NEArithmeticAddition add;\n-  arm_compute::NEActivationLayer act;\n-  arm_compute::Tensor input_tensor;\n-  arm_compute::Tensor kernel_tensor;\n-  arm_compute::Tensor bia_tensor;\n-  arm_compute::Tensor output_tensor;\n-  arm_compute::Tensor output_acc_tensor;\n-};\n-\n-struct acl_gemm_conv_obj_t {\n-  arm_compute::NEGEMMConvolutionLayer gemm_conv;\n-  arm_compute::NEArithmeticAddition add;\n-  arm_compute::NEActivationLayer act;\n-  arm_compute::Tensor input_tensor;\n-  arm_compute::Tensor kernel_tensor;\n-  arm_compute::Tensor bia_tensor;\n-  arm_compute::Tensor output_tensor;\n-  arm_compute::Tensor output_acc_tensor;\n-};\n-\n-struct acl_conv_conf_t {\n-  bool with_bias;\n-  bool is_int8;\n-  bool sum_with_eltwise;\n-  bool fast_math;\n-  arm_compute::TensorInfo input_info;\n-  arm_compute::TensorInfo kernel_info;\n-  arm_compute::TensorInfo bia_info;\n-  arm_compute::TensorInfo output_info;\n-  arm_compute::PadStrideInfo padstride_info;\n-  arm_compute::Size2D dilation_info;\n-  arm_compute::WeightsInfo kernel_wei_info;\n-  arm_compute::ActivationLayerInfo act_info;\n-};\n-\n-extern void __xla_cpu_runtime_ACLConv2DF32(\n-    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,\n-    float* lhs, float* rhs, int64_t input_batch, int64_t input_rows,\n-    int64_t input_cols, int64_t input_channels, int64_t kernel_rows,\n-    int64_t kernel_cols, int64_t kernel_channels, int64_t kernel_filters,\n-    int64_t output_rows, int64_t output_cols, int64_t row_stride,\n-    int64_t col_stride, int64_t padding_top, int64_t padding_bottom,\n-    int64_t padding_left, int64_t padding_right, int64_t lhs_row_dilation,\n-    int64_t lhs_col_dilation, int64_t rhs_row_dilation,\n-    int64_t rhs_col_dilation, int64_t feature_group_count);\n-}\n-#else\n-#include <iostream>\n-\n-extern \"C\" {\n-inline extern void __xla_cpu_runtime_ACLConv2DF32(\n-    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,\n-    float* lhs, float* rhs, int64_t input_batch, int64_t input_rows,\n-    int64_t input_cols, int64_t input_channels, int64_t kernel_rows,\n-    int64_t kernel_cols, int64_t kernel_channels, int64_t kernel_filters,\n-    int64_t output_rows, int64_t output_cols, int64_t row_stride,\n-    int64_t col_stride, int64_t padding_top, int64_t padding_bottom,\n-    int64_t padding_left, int64_t padding_right, int64_t lhs_row_dilation,\n-    int64_t lhs_col_dilation, int64_t rhs_row_dilation,\n-    int64_t rhs_col_dilation, int64_t feature_group_count) {\n-  std::cerr\n-      << \"Attempt to call ACL Conv2D runtime library without defining \"\n-         \"XLA_CPU_USE_ACL. Add --define=build_with_acl=true to build with ACL.\";\n-  exit(1);\n-}\n-}\n-#endif  // XLA_CPU_USE_ACL\n-#endif  // XLA_SERVICE_CPU_RUNTIME_CONV2D_ACL_H_"
        },
        {
            "sha": "fa947ca89ce74f318b7c1618d859842978136c37",
            "filename": "third_party/xla/xla/service/cpu/runtime_matmul_acl.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 207,
            "changes": 207,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_matmul_acl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_matmul_acl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_matmul_acl.cc?ref=ea9f8804e917315408ea9fbfd9044e33a0e556fe",
            "patch": "@@ -1,207 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifdef XLA_CPU_USE_ACL\n-#include \"xla/service/cpu/runtime_matmul_acl.h\"\n-\n-#include \"absl/base/call_once.h\"\n-#include \"xla/executable_run_options.h\"\n-#include \"xla/service/cpu/runtime_lightweight_check.h\"\n-#include \"xla/service/cpu/runtime_matmul.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/types.h\"\n-\n-#define EIGEN_USE_THREADS\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-#include \"xla/tsl/platform/dynamic_annotations.h\"\n-\n-namespace {\n-// ACL GEMM API for 32-bit Matrix Multiplication.\n-\n-// MatMul function is defined as: c = alpha * op(a) * op(b) + beta * c.\n-// Since XLA MatMul does not use alpha, beta, we set them to 1.0 and 0.0.\n-// Matrix lhs, rhs and out are all column-major.\n-int32_t MatMulF32(const void* run_options_ptr, float* out, float* lhs,\n-                  float* rhs, int64_t m, int64_t n, int64_t k,\n-                  int64_t batch_size, int32_t transpose_lhs,\n-                  int32_t transpose_rhs) {\n-  const float alpha = 1.0f, beta = 0.0f;\n-\n-  /* TODO: optimize this object creation along with tensor init and\n-   * gemm configuration by caching the shapes, similar to onednn\n-   * primitive caching feature\n-   */\n-  struct acl_matmul_obj_t acl_obj;\n-  struct acl_matmul_conf_t acl_conf;\n-\n-  acl_conf.is_trans_lhs = (bool)transpose_lhs;\n-  acl_conf.is_trans_rhs = (bool)transpose_rhs;\n-\n-  if (acl_conf.is_trans_lhs) {\n-    acl_conf.lhs_acc_info =\n-        arm_compute::TensorInfo(arm_compute::TensorShape(k, m, batch_size), 1,\n-                                arm_compute::DataType::F32);\n-  }\n-  if (acl_conf.is_trans_rhs) {\n-    acl_conf.rhs_acc_info =\n-        arm_compute::TensorInfo(arm_compute::TensorShape(n, k, 1, batch_size),\n-                                1, arm_compute::DataType::F32);\n-  }\n-\n-  acl_conf.lhs_info =\n-      arm_compute::TensorInfo(arm_compute::TensorShape(m, k, batch_size), 1,\n-                              arm_compute::DataType::F32);\n-  acl_conf.rhs_info =\n-      arm_compute::TensorInfo(arm_compute::TensorShape(k, n, 1, batch_size), 1,\n-                              arm_compute::DataType::F32);\n-  acl_conf.out_info =\n-      arm_compute::TensorInfo(arm_compute::TensorShape(m, n, 1, batch_size), 1,\n-                              arm_compute::DataType::F32);\n-\n-  /* TODO: add TF_XLA_* flag for runtime control of fast math mode*/\n-  bool is_fastmath_enabled = true;\n-  acl_conf.gemm_info.set_fast_math(is_fastmath_enabled);\n-\n-  // Fused ReLU activation\n-  acl_conf.gemm_info.set_activation_info(arm_compute::ActivationLayerInfo());\n-\n-  // Set alpha (output scaling)\n-  acl_conf.alpha = alpha;\n-\n-  // Validate ACL transpose\n-  if (acl_conf.is_trans_lhs) {\n-    auto acl_trans_lhs_st = arm_compute::NETranspose::validate(\n-        &acl_conf.lhs_acc_info, &acl_conf.lhs_info);\n-    if (acl_trans_lhs_st.error_code() != arm_compute::ErrorCode::OK) {\n-      VLOG(1) << \"lhs transpose validation failed\";\n-      return -1;\n-    }\n-  }\n-  if (acl_conf.is_trans_rhs) {\n-    auto acl_trans_rhs_st = arm_compute::NETranspose::validate(\n-        &acl_conf.rhs_acc_info, &acl_conf.rhs_info);\n-    if (acl_trans_rhs_st.error_code() != arm_compute::ErrorCode::OK) {\n-      VLOG(1) << \"rhs transpose validation failed\";\n-      return -1;\n-    }\n-  }\n-\n-  // Validate ACL GEMM\n-  auto acl_st = arm_compute::NEGEMM::validate(\n-      &acl_conf.rhs_info, &acl_conf.lhs_info, nullptr, &acl_conf.out_info,\n-      acl_conf.alpha, 0.0f, acl_conf.gemm_info);\n-  if (acl_st.error_code() != arm_compute::ErrorCode::OK) {\n-    VLOG(1) << \"validate acl GEMM FAILED\";\n-    return -1;\n-  }\n-\n-  static absl::once_flag flag_once;\n-  const xla::ExecutableRunOptions* run_options =\n-      static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n-  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n-  const Eigen::ThreadPoolDevice* tpd =\n-      (Eigen::ThreadPoolDevice*)(run_options->intra_op_thread_pool());\n-  // The threads in Compute Library are bound for the cores 0..max_threads-1\n-  const int max_threads = tpd->numThreads();\n-\n-  // arm_compute::Scheduler does not support concurrent access thus a\n-  // workaround here restricts it to only one call\n-  absl::call_once(flag_once, [&]() {\n-    arm_compute::Scheduler::get().set_num_threads(max_threads);\n-  });\n-\n-  // configure the acl obj with the config\n-  acl_obj.lhs_tensor.allocator()->init(acl_conf.lhs_info);\n-  acl_obj.rhs_tensor.allocator()->init(acl_conf.rhs_info);\n-  acl_obj.out_tensor.allocator()->init(acl_conf.out_info);\n-\n-  // Configure transpose kernel for src, wei or both\n-  if (acl_conf.is_trans_lhs) {\n-    acl_obj.lhs_acc_tensor.allocator()->init(acl_conf.lhs_acc_info);\n-    acl_obj.trans_lhs.configure(&acl_obj.lhs_acc_tensor, &acl_obj.lhs_tensor);\n-  }\n-  if (acl_conf.is_trans_rhs) {\n-    acl_obj.rhs_acc_tensor.allocator()->init(acl_conf.rhs_acc_info);\n-    acl_obj.trans_rhs.configure(&acl_obj.rhs_acc_tensor, &acl_obj.rhs_tensor);\n-  }\n-  // Configure GEMM\n-  acl_obj.gemm.configure(&acl_obj.rhs_tensor, &acl_obj.lhs_tensor, nullptr,\n-                         &acl_obj.out_tensor, acl_conf.alpha, 0.0f,\n-                         acl_conf.gemm_info);\n-\n-  // Run transpose kernel\n-  if (transpose_lhs && !transpose_rhs) {\n-    acl_obj.lhs_tensor.allocator()->allocate();\n-    acl_obj.lhs_acc_tensor.allocator()->import_memory(lhs);\n-    acl_obj.trans_lhs.run();\n-    acl_obj.rhs_tensor.allocator()->import_memory(rhs);\n-  } else if (transpose_rhs && !transpose_lhs) {\n-    acl_obj.rhs_tensor.allocator()->allocate();\n-    acl_obj.rhs_acc_tensor.allocator()->import_memory(rhs);\n-    acl_obj.trans_rhs.run();\n-    acl_obj.lhs_tensor.allocator()->import_memory(lhs);\n-  } else if (transpose_rhs && transpose_lhs) {\n-    acl_obj.lhs_tensor.allocator()->allocate();\n-    acl_obj.lhs_acc_tensor.allocator()->import_memory(lhs);\n-    acl_obj.rhs_tensor.allocator()->allocate();\n-    acl_obj.rhs_acc_tensor.allocator()->import_memory(rhs);\n-    acl_obj.trans_lhs.run();\n-    acl_obj.trans_rhs.run();\n-  } else {\n-    acl_obj.lhs_tensor.allocator()->import_memory(lhs);\n-    acl_obj.rhs_tensor.allocator()->import_memory(rhs);\n-  }\n-\n-  acl_obj.out_tensor.allocator()->import_memory(out);\n-\n-  // Execute the function\n-  acl_obj.gemm.run();\n-\n-  acl_obj.lhs_tensor.allocator()->free();\n-  acl_obj.rhs_tensor.allocator()->free();\n-  acl_obj.out_tensor.allocator()->free();\n-  if (acl_conf.is_trans_lhs) acl_obj.lhs_acc_tensor.allocator()->free();\n-  if (acl_conf.is_trans_rhs) acl_obj.rhs_acc_tensor.allocator()->free();\n-\n-  return 0;\n-}\n-\n-}  // namespace\n-\n-ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_ACLMatMulF32(\n-    const void* run_options_ptr, float* out, float* lhs, float* rhs, int64_t m,\n-    int64_t n, int64_t k, int32_t transpose_lhs, int32_t transpose_rhs) {\n-  if (MatMulF32(run_options_ptr, out, lhs, rhs, m, n, k, 1 /*batch_size*/,\n-                transpose_lhs, transpose_rhs) < 0) {\n-    VLOG(1) << \"ACL matmul failed, fallback to Eigen matmul\";\n-    __xla_cpu_runtime_EigenMatMulF32(run_options_ptr, out, lhs, rhs, m, n, k,\n-                                     transpose_lhs, transpose_rhs);\n-  }\n-}\n-\n-ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_ACLBatchMatMulF32(\n-    const void* run_options_ptr, float* out, float* lhs, float* rhs, int64_t m,\n-    int64_t n, int64_t k, int64_t batch_size, int32_t transpose_lhs,\n-    int32_t transpose_rhs) {\n-  if (MatMulF32(run_options_ptr, out, lhs, rhs, m, n, k, batch_size,\n-                transpose_lhs, transpose_rhs) < 0) {\n-    VLOG(1) << \"ACL batch matmul failed, fallback to Eigen batch matmul\";\n-    __xla_cpu_runtime_EigenBatchMatMulF32(run_options_ptr, out, lhs, rhs, m, n,\n-                                          k, batch_size, transpose_lhs,\n-                                          transpose_rhs);\n-  }\n-}\n-\n-#endif  // XLA_CPU_USE_ACL"
        },
        {
            "sha": "94f4f56d65f100845d703f80e9fab229174fa1ba",
            "filename": "third_party/xla/xla/service/cpu/runtime_matmul_acl.h",
            "status": "removed",
            "additions": 0,
            "deletions": 87,
            "changes": 87,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_matmul_acl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea9f8804e917315408ea9fbfd9044e33a0e556fe/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_matmul_acl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_matmul_acl.h?ref=ea9f8804e917315408ea9fbfd9044e33a0e556fe",
            "patch": "@@ -1,87 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_SERVICE_CPU_RUNTIME_MATMUL_ACL_H_\n-#define XLA_SERVICE_CPU_RUNTIME_MATMUL_ACL_H_\n-\n-#include <iostream>\n-\n-#include \"tsl/platform/types.h\"\n-\n-#ifdef XLA_CPU_USE_ACL\n-#include \"arm_compute/runtime/NEON/NEFunctions.h\"\n-#include \"arm_compute/runtime/NEON/NEScheduler.h\"\n-#include \"utils/Utils.h\"\n-\n-extern \"C\" {\n-struct acl_matmul_obj_t {\n-  arm_compute::NEGEMM gemm;\n-  arm_compute::NETranspose trans_lhs;\n-  arm_compute::NETranspose trans_rhs;\n-  arm_compute::Tensor rhs_tensor;\n-  arm_compute::Tensor rhs_acc_tensor;\n-  arm_compute::Tensor lhs_tensor;\n-  arm_compute::Tensor lhs_acc_tensor;\n-  arm_compute::Tensor out_tensor;\n-};\n-\n-struct acl_matmul_conf_t {\n-  bool with_bias;\n-  bool is_trans_lhs;\n-  bool is_trans_rhs;\n-  arm_compute::TensorInfo lhs_info;\n-  arm_compute::TensorInfo lhs_acc_info;\n-  arm_compute::TensorInfo rhs_info;\n-  arm_compute::TensorInfo rhs_acc_info;\n-  arm_compute::TensorInfo out_info;\n-  arm_compute::GEMMInfo gemm_info;\n-  float alpha;\n-};\n-\n-extern void __xla_cpu_runtime_ACLMatMulF32(\n-    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,\n-    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k,\n-    int32_t transpose_lhs, int32_t transpose_rhs);\n-\n-extern void __xla_cpu_runtime_ACLBatchMatMulF32(\n-    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,\n-    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k, int64_t batch_size,\n-    int32_t transpose_lhs, int32_t transpose_rhs);\n-\n-}  // extern \"C\"\n-#else\n-extern \"C\" {\n-inline extern void __xla_cpu_runtime_ACLMatMulF32(\n-    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,\n-    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k,\n-    int32_t transpose_lhs, int32_t transpose_rhs) {\n-  std::cerr\n-      << \"Attempt to call ACL MatMul runtime library without defining \"\n-         \"XLA_CPU_USE_ACL. Add --define=build_with_acl=true to build with ACL.\";\n-  exit(1);\n-}\n-\n-inline extern void __xla_cpu_runtime_ACLBatchMatMulF32(\n-    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,\n-    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k, int64_t batch_size,\n-    int32_t transpose_lhs, int32_t transpose_rhs) {\n-  std::cerr\n-      << \"Attempt to call ACL MatMul runtime library without defining \"\n-         \"XLA_CPU_USE_ACL. Add --define=build_with_acl=true to build with ACL.\";\n-  exit(1);\n-}\n-}  // extern \"C\"\n-#endif  // XLA_CPU_USE_ACL\n-#endif  // XLA_SERVICE_CPU_RUNTIME_MATMUL_ACL_H_"
        }
    ],
    "stats": {
        "total": 730,
        "additions": 6,
        "deletions": 724
    }
}