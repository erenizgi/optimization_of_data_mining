{
    "author": "Tixxx",
    "message": "PR #31783: [NVIDIA GPU]Use default main stream when no stream borrower is created for executable\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31783\n\nüìù Summary of Changes\nThis is a bug fix to address the xla assert when no stream borrower is created for an executable that contains\nparallel computes. In such case, default compute stream will be used for all parallel compute nodes.\n\nüéØ Justification\nIf no stream borrower is created, currently xla asserts with \"No stream borrower\" error.\n\nüöÄ Kind of Contribution\n üêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nNA\nüß™ Unit Tests:\nAdded a unit test\n\nüß™ Execution Tests:\nUnit test executes impacted code path.\n\nCopybara import of the project:\n\n--\n3cf15be95549cbbc0504c187a2a4290f65e0e754 by TJ Xu <tjx@nvidia.com>:\n\nUse default main stream when no stream borrower is created for executable\n\n--\ne78a48f9d7d338b8a2e27001c10493741d85aff5 by TJ Xu <tjx@nvidia.com>:\n\nadded unit test\n\nMerging this change closes #31783\n\nPiperOrigin-RevId: 815043117",
    "sha": "1559b23bfaa6c8471799439bd8b91c6aa2d0dd78",
    "files": [
        {
            "sha": "e5297800cd9bb29d5f3ad1c63f9b06500bb4a74c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1559b23bfaa6c8471799439bd8b91c6aa2d0dd78/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1559b23bfaa6c8471799439bd8b91c6aa2d0dd78/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=1559b23bfaa6c8471799439bd8b91c6aa2d0dd78",
            "patch": "@@ -346,6 +346,52 @@ TEST_F(AutotunerPassTest, DevicelessUsesDefaultConfigIfNoCache) {\n       gpu_backend_config.gemm_backend_config().has_selected_algorithm());\n }\n \n+TEST_F(AutotunerPassTest, CublasGemmInNonDefaultStreamIsAutotuned) {\n+  const char kCublasCustomNonDefaultStreamCallHlo[] = R\"\"\"(\n+HloModule module, entry_computation_layout={(f32[100,100]{1,0}, f32[100,100]{1,0})->f32[100,100]{1,0}}\n+ENTRY %main (arg0: f32[100,100], arg1: f32[100,100]) -> f32[100,100] {\n+  %arg0 = f32[100,100]{1,0} parameter(0)\n+  %arg1 = f32[100,100]{1,0} parameter(1)\n+  %custom-call.1 = (f32[100,100]{1,0}, s8[80000]{0}) custom-call(%arg0, %arg1),\n+  custom_call_target=\"__cublas$gemm\",\n+  backend_config={\n+    \"operation_queue_id\":\"109\",\n+    \"gemm_backend_config\":{\n+      \"dot_dimension_numbers\":\n+        {\n+          \"lhs_contracting_dimensions\":[\"1\"],\n+          \"rhs_contracting_dimensions\":[\"0\"],\n+          \"lhs_batch_dimensions\":[],\n+          \"rhs_batch_dimensions\":[]\n+      }\n+    }\n+  }\n+  ROOT %get-tuple-element = f32[100,100]{1,0} get-tuple-element(%custom-call.1), index=0\n+}\n+)\"\"\";\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> module,\n+      ParseAndReturnVerifiedModule(kCublasCustomNonDefaultStreamCallHlo));\n+\n+  tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n+                                      /*num_threads=*/4);\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  GpuCompiler::TargetConfig target_config(stream_executor_);\n+\n+  backends.push_back(std::make_unique<CublasBackend>(\n+      stream_executor_, &module->config().debug_options(), &compiler_,\n+      &target_config));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<AutotunerPass> pass,\n+      AutotunerPass::Create(std::move(backends),\n+                            module->config().debug_options(), stream_executor_,\n+                            &thread_pool, IsCublasGemmInstruction,\n+                            &target_config, allocator_.get()));\n+  EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "5445a4efd6c96c2d9472ee076d548ff562fedf63",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1559b23bfaa6c8471799439bd8b91c6aa2d0dd78/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1559b23bfaa6c8471799439bd8b91c6aa2d0dd78/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=1559b23bfaa6c8471799439bd8b91c6aa2d0dd78",
            "patch": "@@ -363,16 +363,25 @@ absl::Status ExecuteThunksImpl(\n   Thunk::ExecutionStreamIdMap additional_execution_streams;\n   std::vector<StreamPool::Ptr> additional_streams;\n   if (!execution_stream_ids.empty()) {\n-    TF_ASSIGN_OR_RETURN(additional_streams, run_options->BorrowStreams(\n-                                                executor->device_ordinal(),\n-                                                execution_stream_ids.size()));\n-    int64_t i = 0;\n-    for (ExecutionStreamId stream_id : execution_stream_ids) {\n-      additional_execution_streams[stream_id] = additional_streams.at(i).get();\n-      i++;\n+    if (run_options->HasStreamBorrower()) {\n+      TF_ASSIGN_OR_RETURN(additional_streams, run_options->BorrowStreams(\n+                                                  executor->device_ordinal(),\n+                                                  execution_stream_ids.size()));\n+      int64_t i = 0;\n+      for (ExecutionStreamId stream_id : execution_stream_ids) {\n+        additional_execution_streams[stream_id] =\n+            additional_streams.at(i).get();\n+        i++;\n+      }\n+      VLOG(2) << \"Using \" << additional_execution_streams.size()\n+              << \" additional compute streams.\";\n+    } else {\n+      VLOG(2) << \"No stream borrower created. \"\n+              << \"Assigning the default stream to all parallel computes.\";\n+      for (ExecutionStreamId stream_id : execution_stream_ids) {\n+        additional_execution_streams[stream_id] = main_stream;\n+      }\n     }\n-    VLOG(2) << \"Using \" << additional_execution_streams.size()\n-            << \" additional compute streams.\";\n   }\n \n   tsl::profiler::TraceMe hlo_module_activity("
        }
    ],
    "stats": {
        "total": 73,
        "additions": 64,
        "deletions": 9
    }
}