{
    "author": "EusebioDM",
    "message": "Introduce new AOT compilation support in `GpuCompiler`\n\nWhen the new `xla_gpu_experimental_aot_compiled_thunks` flag is enabled (off by default), `GpuCompiler::Export` and `GpuCompiler::CompileAheadOfTime` will use the new OAT compilation path.\n\nAdditional changes:\n* Add parametrized tests to test both flows.\n* Fixed the HLOs in the `gpu_aot_compilation_test`, since it was trying to copy tensors with different layouts (which I believe is not valid).\n* Minor fixes to `gpu_aot_compilation_test` (imports, raw string tags).\n* Ran `cclean` in the change files.\n\nPiperOrigin-RevId: 836656156",
    "sha": "6b91a2a6e2bed8dae4ba4b822583f13902afc092",
    "files": [
        {
            "sha": "677f7ff1ef22b62e0ee6e42172ecb7727800ec45",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -1780,6 +1780,15 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n               set_xla_gpu_experimental_enable_nccl_symmetric_buffers),\n       debug_options->xla_gpu_experimental_enable_nccl_symmetric_buffers(),\n       \"Enables NCCL symmetric buffer registration.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_experimental_aot_compiled_thunks\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_gpu_experimental_aot_compiled_thunks),\n+      debug_options->xla_gpu_experimental_aot_compiled_thunks(),\n+      \"Enables an Ahead-of-Time (AOT) compilation flow where the compiled \"\n+      \"binary includes the generated Thunks. In contrast, the legacy flow \"\n+      \"only compiles up to the HLO optimization stage, before Thunk \"\n+      \"generation.\"));\n \n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_experimental_enable_nvshmem\","
        },
        {
            "sha": "0f4049e737a11b21a25bde3e059ba0dc0a9e5b9e",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -859,6 +859,7 @@ xla_cc_test(\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n+        \"//xla/client:executable_build_options\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_ordering\",\n@@ -1648,6 +1649,7 @@ cc_library(\n         \":flag_utils\",\n         \":fusion_dispatch_pipeline\",\n         \":fusion_pipeline\",\n+        \":gpu_aot_compilation_result\",\n         \":gpu_constants\",\n         \":gpu_executable\",\n         \":gpu_executable_proto_cc\",\n@@ -2060,6 +2062,7 @@ cc_library(\n     deps = [\n         \":gpu_executable\",\n         \":gpu_executable_proto_cc\",\n+        \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n@@ -2385,6 +2388,7 @@ xla_cc_test(\n         \":amdgpu_compiler_impl\",\n     ]) + [\n         \":gpu_transfer_manager\",\n+        \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/hlo/ir:hlo\",\n@@ -2400,8 +2404,11 @@ xla_cc_test(\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tests:literal_test_util\",\n         \"//xla/tests:xla_internal_test_main\",  # build_cleaner: keep\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:statusor\",\n     ],"
        },
        {
            "sha": "48ca5e869b37e3d9175d84420ad92cf6bf3f3c06",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_result.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result.h?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -41,7 +41,7 @@ namespace xla::gpu {\n // HLO.\n class GpuAotCompilationResult : public AotCompilationResult {\n  public:\n-  static absl::StatusOr<std::unique_ptr<GpuAotCompilationResult>> Create(\n+  static absl::StatusOr<std::unique_ptr<GpuAotCompilationResult>> FromProto(\n       GpuExecutableProto executable) {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n                         HloModule::CreateFromProtoWithConfig("
        },
        {
            "sha": "7edc828fd9e76a7457d1fd55de18258a54ac086c",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_result_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result_test.cc?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -127,7 +127,7 @@ TEST_F(GpuAotCompilationResultTest, CreateAndSerialize) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<GpuAotCompilationResult> result,\n-      GpuAotCompilationResult::Create(reference_executable));\n+      GpuAotCompilationResult::FromProto(reference_executable));\n   TF_ASSERT_OK_AND_ASSIGN(std::string serialized_result,\n                           result->SerializeAsString());\n   GpuExecutableProto deserialized_executable;\n@@ -142,7 +142,7 @@ TEST_F(GpuAotCompilationResultTest, LoadExecutable) {\n                           CreateGpuExecutableProto());\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<GpuAotCompilationResult> result,\n-      GpuAotCompilationResult::Create(reference_executable));\n+      GpuAotCompilationResult::FromProto(reference_executable));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<Executable> executable,"
        },
        {
            "sha": "bf02596ba3a47734e92523c2fdd1aecc28ead2cc",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_test.cc",
            "status": "modified",
            "additions": 58,
            "deletions": 35,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -21,11 +21,17 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/strings/ascii.h\"\n #include \"absl/strings/escaping.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/Builders.h\"  // from @llvm-project\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n+#include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n@@ -36,22 +42,39 @@ limitations under the License.\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tests/literal_test_util.h\"\n-#include \"tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n \n-using GpuAotCompilationTest = HloTestBase;\n+class GpuAotCompilationTest : public HloTestBase,\n+                              public ::testing::WithParamInterface<bool> {\n+ protected:\n+  void SetUp() override { debug_options_ = GetDebugOptionsForTest(); }\n \n-TEST_F(GpuAotCompilationTest, ExportAndLoadExecutable) {\n-  const absl::string_view hlo_string = R\"(\n-HloModule Test\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = HloTestBase::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_experimental_aot_compiled_thunks(GetParam());\n+    return debug_options;\n+  }\n \n-ENTRY main {\n-  a = f32[100, 200]{1,0} parameter(0)\n-  ROOT b = f32[100, 200]{0,1} copy(a)\n-}\n-)\";\n+  DebugOptions debug_options_;\n+};\n+INSTANTIATE_TEST_SUITE_P(NewAotFlow, GpuAotCompilationTest, ::testing::Bool(),\n+                         [](const ::testing::TestParamInfo<bool>& info) {\n+                           return info.param ? \"NewAotFlowEnabled\"\n+                                             : \"NewAotFlowDisabled\";\n+                         });\n+\n+TEST_P(GpuAotCompilationTest, ExportAndLoadExecutable) {\n+  const absl::string_view hlo_string = R\"hlo(\n+    HloModule Test\n+\n+    ENTRY main {\n+      a = f32[100, 200]parameter(0)\n+      ROOT b = f32[100, 200] copy(a)\n+    }\n+)hlo\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(hlo_string));\n \n@@ -84,15 +107,15 @@ ENTRY main {\n       std::move(*aot_result).LoadExecutable(compiler, stream_exec));\n }\n \n-TEST_F(GpuAotCompilationTest, AotCompilationWithoutGpuDevice) {\n-  const absl::string_view hlo_string = R\"(\n-HloModule Test\n+TEST_P(GpuAotCompilationTest, AotCompilationWithoutGpuDevice) {\n+  const absl::string_view hlo_string = R\"hlo(\n+    HloModule Test\n \n-ENTRY main {\n-  a = f32[100, 200]{1,0} parameter(0)\n-  ROOT b = f32[100, 200]{0,1} copy(a)\n-}\n-)\";\n+    ENTRY main {\n+      a = f32[100, 200] parameter(0)\n+      ROOT b = f32[100, 200] copy(a)\n+    }\n+)hlo\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(hlo_string));\n \n@@ -136,20 +159,20 @@ std::string CreateTritonCustomCallBackendConfig() {\n   mlir::Builder builder(&context_);\n \n   // Create the backend_config for the triton custom call.\n-  const std::string kMLIRText = R\"(\n-  module {\n-    tt.func public @add_one(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}, %arg1: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}, %arg3: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}) {\n-      %0 = tt.get_program_id x : i32\n-      %1 = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<f32>\n-      %2 = tt.load %arg1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<f32>\n-      %cst = arith.constant 1.000000e+00 : f32\n-      %3 = arith.addf %1, %cst : f32\n-      tt.store %arg2, %3 {cache = 1 : i32, evict = 1 : i32} : !tt.ptr<f32>\n-      tt.store %arg3, %2 {cache = 1 : i32, evict = 1 : i32} : !tt.ptr<f32>\n-      tt.return\n+  const std::string kMLIRText = R\"mlir(\n+    module {\n+      tt.func public @add_one(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}, %arg1: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}, %arg3: !tt.ptr<f32, 1> {tt.divisibility = 32 : i32}) {\n+        %0 = tt.get_program_id x : i32\n+        %1 = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<f32>\n+        %2 = tt.load %arg1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<f32>\n+        %cst = arith.constant 1.000000e+00 : f32\n+        %3 = arith.addf %1, %cst : f32\n+        tt.store %arg2, %3 {cache = 1 : i32, evict = 1 : i32} : !tt.ptr<f32>\n+        tt.store %arg3, %2 {cache = 1 : i32, evict = 1 : i32} : !tt.ptr<f32>\n+        tt.return\n+      }\n     }\n-  }\n-  )\";\n+  )mlir\";\n \n   NamedAttribute name =\n       builder.getNamedAttr(\"name\", builder.getStringAttr(\"add_one\"));\n@@ -183,7 +206,7 @@ std::string CreateTritonCustomCallBackendConfig() {\n \n }  // namespace\n \n-TEST_F(GpuAotCompilationTest, ExportAndLoadExecutableWithTriton) {\n+TEST_P(GpuAotCompilationTest, ExportAndLoadExecutableWithTriton) {\n   auto triton_support =\n       EnsureTritonSupportsComputeCapability(backend()\n                                                 .default_stream_executor()\n@@ -193,15 +216,15 @@ TEST_F(GpuAotCompilationTest, ExportAndLoadExecutableWithTriton) {\n     GTEST_SKIP() << triton_support;\n   }\n \n-  const absl::string_view hlo_string_template = R\"(\n+  const absl::string_view hlo_string_template = R\"hlo(\n     HloModule Test\n \n     ENTRY main {\n     a = f32[] parameter(0)\n     b = f32[] parameter(1)\n     ROOT c = (f32[],f32[]) custom-call(a, b), custom_call_target=\"__gpu$xla.gpu.triton\", backend_config=\"%s\"\n     }\n-    )\";\n+    )hlo\";\n \n   std::string hlo_string =\n       absl::StrFormat(hlo_string_template,"
        },
        {
            "sha": "507e787335e402a260d277a25bb3faa99ba0639c",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 57,
            "deletions": 2,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -28,6 +28,8 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/base/call_once.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -40,13 +42,15 @@ limitations under the License.\n #include \"llvm/AsmParser/Parser.h\"\n #include \"llvm/Bitcode/BitcodeReader.h\"\n #include \"llvm/Bitcode/BitcodeWriter.h\"\n+#include \"llvm/IR/Constants.h\"\n #include \"llvm/IR/DataLayout.h\"\n #include \"llvm/IR/DiagnosticInfo.h\"\n #include \"llvm/IR/DiagnosticPrinter.h\"\n #include \"llvm/IR/GlobalValue.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/Error.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"llvm/TargetParser/Triple.h\"\n@@ -55,6 +59,7 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"google/protobuf/text_format.h\"\n #include \"xla/backends/cpu/nanort/nanort_client.h\"\n+#include \"xla/backends/cpu/nanort/nanort_executable.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/backends/gpu/runtime/host_execute_thunk.h\"\n #include \"xla/backends/gpu/runtime/runtime_intrinsics.h\"\n@@ -135,6 +140,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/zero_sized_hlo_elimination.h\"\n #include \"xla/hlo/transforms/while_loop_trip_count_annotator.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/maybe_owning.h\"\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/service/all_reduce_promotion.h\"\n@@ -145,6 +151,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_value.h\"\n #include \"xla/service/call_inliner.h\"\n+#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/collective_permute_decomposer.h\"\n #include \"xla/service/collective_pipeliner.h\"\n #include \"xla/service/collective_pipeliner_utils.h\"\n@@ -171,6 +178,7 @@ limitations under the License.\n #include \"xla/service/gpu/flag_utils.h\"\n #include \"xla/service/gpu/fusion_dispatch_pipeline.h\"\n #include \"xla/service/gpu/fusion_pipeline.h\"\n+#include \"xla/service/gpu/gpu_aot_compilation_result.h\"\n #include \"xla/service/gpu/gpu_executable.h\"\n #include \"xla/service/gpu/gpu_executable.pb.h\"\n #include \"xla/service/gpu/gpu_float_support.h\"\n@@ -2668,6 +2676,34 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n   // compilation.\n   CHECK_EQ(options.PlatformId(), PlatformId());\n \n+  if (hlo_module->config()\n+          .debug_options()\n+          .xla_gpu_experimental_aot_compiled_thunks()) {\n+    return NewCompileAheadOfTime(std::move(hlo_module), options);\n+  }\n+\n+  return LegacyCompileAheadOfTime(std::move(hlo_module), options);\n+}\n+\n+absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>\n+GpuCompiler::NewCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n+                                   const AotCompilationOptions& options) {\n+  CompileOptions compile_options;\n+  compile_options.device_allocator = options.device_allocator();\n+  compile_options.gpu_target_config = options.gpu_target_config();\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<Executable> executable,\n+      RunBackend(std::move(hlo_module), options.executor(), compile_options));\n+\n+  std::vector<std::unique_ptr<AotCompilationResult>> results;\n+  TF_ASSIGN_OR_RETURN(results.emplace_back(), Export(executable.get()));\n+  return results;\n+}\n+\n+absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>\n+GpuCompiler::LegacyCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n+                                      const AotCompilationOptions& options) {\n   std::unique_ptr<HloModule> optimized_module;\n \n   if (!hlo_module->has_schedule()) {\n@@ -2723,6 +2759,14 @@ absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(\n     return Internal(\"GpuExecutable is null\");\n   }\n \n+  if (gpu_executable->module()\n+          .config()\n+          .debug_options()\n+          .xla_gpu_experimental_aot_compiled_thunks()) {\n+    TF_ASSIGN_OR_RETURN(GpuExecutableProto proto, gpu_executable->ToProto());\n+    return GpuAotCompilationResult::FromProto(std::move(proto));\n+  }\n+\n   return LegacyGpuAotCompilationResult::FromModule(\n       &gpu_executable->module(), gpu_executable->buffer_assignment(),\n       gpu_executable->text(), gpu_executable->binary(),\n@@ -2962,8 +3006,19 @@ absl::Status GpuCompiler::SerializeAutotuneResultsToFile(\n absl::StatusOr<std::unique_ptr<AotCompilationResult>>\n GpuCompiler::LoadAotCompilationResult(\n     const std::string& serialized_aot_result) {\n-  return LegacyGpuAotCompilationResult::FromString(serialized_aot_result,\n-                                                   pointer_size_);\n+  GpuExecutableProto gpu_executable_proto;\n+  if (!gpu_executable_proto.ParseFromString(serialized_aot_result)) {\n+    return InvalidArgument(\n+        \"Failed to parse serialized AOT result as GpuExecutableProto.\");\n+  }\n+\n+  // If the proto has a thunk set, it's a new OAT format.\n+  if (gpu_executable_proto.has_thunk()) {\n+    return GpuAotCompilationResult::FromProto(gpu_executable_proto);\n+  }\n+\n+  return LegacyGpuAotCompilationResult::FromProto(gpu_executable_proto,\n+                                                  pointer_size_);\n }\n \n absl::StatusOr<std::unique_ptr<Executable>>"
        },
        {
            "sha": "96b6312d2926553242236b182af98dc18d966196",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -273,6 +273,15 @@ class GpuCompiler : public LLVMCompiler {\n     return Unimplemented(\"LinkModules is not implemented.\");\n   }\n \n+  // New AOT compilation as part of the AOT split project.\n+  absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>\n+  NewCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n+                        const AotCompilationOptions& options);\n+  // Legacy AOT compilation.\n+  absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>\n+  LegacyCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n+                           const AotCompilationOptions& options);\n+\n   se::Platform::Id platform_id_;\n \n   // The triple that represents our target."
        },
        {
            "sha": "91630ab31313fd10672699cf924f991b5134f4d4",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -108,6 +108,7 @@ using ::testing::IsEmpty;\n using ::testing::IsSupersetOf;\n using ::testing::Matches;\n using ::testing::Not;\n+using ::testing::SizeIs;\n using ::testing::StartsWith;\n using ::testing::TempDir;\n using ::tsl::gtl::ValueOrDie;\n@@ -952,6 +953,117 @@ ENTRY main {\n   }\n }\n \n+class AotCompilationTest : public GpuCompilerTest,\n+                           public ::testing::WithParamInterface<bool> {\n+ protected:\n+  void SetUp() override {\n+    stream_exec_ = backend().default_stream_executor();\n+    compiler_ = backend().compiler();\n+    aot_options_ =\n+        std::make_unique<AotCompilationOptions>(compiler_->PlatformId());\n+    aot_options_->set_executor(stream_exec_);\n+  }\n+\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = GpuCompilerTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_experimental_aot_compiled_thunks(GetParam());\n+    return debug_options;\n+  }\n+\n+  se::StreamExecutor* stream_exec_;\n+  Compiler* compiler_;\n+  std::unique_ptr<AotCompilationOptions> aot_options_;\n+};\n+\n+INSTANTIATE_TEST_SUITE_P(NewAotFlow, AotCompilationTest, ::testing::Bool(),\n+                         [](const ::testing::TestParamInfo<bool>& info) {\n+                           return info.param ? \"NewAotFlowEnabled\"\n+                                             : \"NewAotFlowDisabled\";\n+                         });\n+\n+TEST_P(AotCompilationTest, CompileAndLoadAotResult) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> add_1_hlo,\n+      ParseAndReturnVerifiedModule(R\"hlo(\n+    add1 {\n+      p = s32[] parameter(0)\n+      c = s32[] constant(1)\n+      ROOT a = s32[] add(p, c)\n+    }\n+\n+    ENTRY e {\n+      p = s32[] parameter(0)\n+      ROOT r = s32[] fusion(p), kind=kLoop, calls=add1\n+    })hlo\",\n+                                   GetModuleConfigForTest()));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<std::unique_ptr<AotCompilationResult>> aot_results,\n+      compiler_->CompileAheadOfTime(std::move(add_1_hlo), *aot_options_));\n+  ASSERT_THAT(aot_results, SizeIs(1));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::string serialized_aot_result,\n+                          std::move(aot_results[0])->SerializeAsString());\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<AotCompilationResult> aot_result,\n+      compiler_->LoadAotCompilationResult(serialized_aot_result));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Executable> executable,\n+      std::move(*aot_result).LoadExecutable(compiler_, stream_exec_));\n+  std::unique_ptr<OpaqueExecutable> wrapped_executable =\n+      test_runner_as_hlo_runner().WrapExecutable(std::move(executable));\n+\n+  const xla::Literal literal_input = xla::LiteralUtil::CreateR0<int32_t>(1);\n+  const xla::Literal literal_expected_result =\n+      xla::LiteralUtil::CreateR0<int32_t>(2);\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result,\n+                          test_runner_as_hlo_runner().ExecuteWithExecutable(\n+                              wrapped_executable.get(), {&literal_input}));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(result, literal_expected_result));\n+}\n+\n+TEST_P(AotCompilationTest, ExportAndImportAotResult) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> add_1_hlo,\n+      ParseAndReturnVerifiedModule(R\"hlo(\n+    add1 {\n+      p = s32[] parameter(0)\n+      c = s32[] constant(1)\n+      ROOT a = s32[] add(p, c)\n+    }\n+\n+    ENTRY e {\n+      p = s32[] parameter(0)\n+      ROOT r = s32[] fusion(p), kind=kLoop, calls=add1\n+    })hlo\",\n+                                   GetModuleConfigForTest()));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Executable> executable,\n+      compiler_->RunBackend(std::move(add_1_hlo), stream_exec_,\n+                            {/*device_allocator=*/nullptr,\n+                             /*thread_pool=*/nullptr,\n+                             /*layout_canonicalization_callback=*/{},\n+                             /*is_autotuning_compilation=*/false}));\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<AotCompilationResult> aot_result,\n+                          compiler_->Export(executable.get()));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Executable> new_executable,\n+      std::move(*aot_result).LoadExecutable(compiler_, stream_exec_));\n+  std::unique_ptr<OpaqueExecutable> wrapped_executable =\n+      test_runner_as_hlo_runner().WrapExecutable(std::move(new_executable));\n+\n+  const xla::Literal literal_input = xla::LiteralUtil::CreateR0<int32_t>(1);\n+  const xla::Literal literal_expected_result =\n+      xla::LiteralUtil::CreateR0<int32_t>(2);\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result,\n+                          test_runner_as_hlo_runner().ExecuteWithExecutable(\n+                              wrapped_executable.get(), {&literal_input}));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(result, literal_expected_result));\n+}\n+\n class KernelCacheTest : public HloTestBase {\n  public:\n   void SetUp() override {"
        },
        {
            "sha": "0985ad404f4b9eb314265c0bf3d9b784d2466929",
            "filename": "third_party/xla/xla/service/gpu/legacy_gpu_aot_compilation_result.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.cc?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -64,9 +64,17 @@ LegacyGpuAotCompilationResult::FromString(const std::string& serialized,\n   tsl::profiler::TraceMe traceme(\"ResultFromString\");\n   GpuExecutableProto proto;\n   if (!proto.ParseFromString(serialized)) {\n-    return Internal(\"Failed to parse serialized GpuThunkAotCompilationResult.\");\n+    return Internal(\n+        \"Failed to parse serialized LegacyGpuAotCompilationResult.\");\n   }\n \n+  return FromProto(proto, pointer_size);\n+}\n+\n+absl::StatusOr<std::unique_ptr<LegacyGpuAotCompilationResult>>\n+LegacyGpuAotCompilationResult::FromProto(const GpuExecutableProto& proto,\n+                                         int pointer_size) {\n+  tsl::profiler::TraceMe traceme(\"ResultFromProto\");\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<HloModule> module,\n       HloModule::CreateFromProtoWithConfig(proto.hlo_module_with_config()));"
        },
        {
            "sha": "b97cecd5a2ebce85e4afe33af0ee191bdc75648e",
            "filename": "third_party/xla/xla/service/gpu/legacy_gpu_aot_compilation_result.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.h?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -55,6 +55,9 @@ class LegacyGpuAotCompilationResult : public AotCompilationResult {\n   static absl::StatusOr<std::unique_ptr<LegacyGpuAotCompilationResult>>\n   FromString(const std::string& serialized, int pointer_size);\n \n+  static absl::StatusOr<std::unique_ptr<LegacyGpuAotCompilationResult>>\n+  FromProto(const GpuExecutableProto& proto, int pointer_size);\n+\n   absl::StatusOr<std::string> SerializeAsString() const override;\n \n   absl::StatusOr<std::unique_ptr<Executable>> LoadExecutable("
        },
        {
            "sha": "e52bf4808e3ac9bb064571688d6b9a0e89d5b2cf",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b91a2a6e2bed8dae4ba4b822583f13902afc092/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=6b91a2a6e2bed8dae4ba4b822583f13902afc092",
            "patch": "@@ -598,6 +598,11 @@ message DebugOptions {\n   // regressions.\n   optional bool xla_gpu_experimental_allow_unroll_factor_eight = 430;\n \n+  // Enables an Ahead-of-Time (AOT) compilation flow where the compiled binary\n+  // includes the generated Thunks. In contrast, the legacy flow only compiles\n+  // up to the HLO optimization stage, before Thunk generation.\n+  optional bool xla_gpu_experimental_aot_compiled_thunks = 435;\n+\n   // Specifies the behavior of per kernel autotuning cache.\n   optional AutotuneCacheMode xla_gpu_experimental_autotune_cache_mode = 324;\n \n@@ -1320,7 +1325,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 435\n+  // Next id: 436\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 315,
        "additions": 273,
        "deletions": 42
    }
}