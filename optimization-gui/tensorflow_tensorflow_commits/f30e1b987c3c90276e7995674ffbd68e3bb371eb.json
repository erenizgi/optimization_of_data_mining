{
    "author": "hawkinsp",
    "message": "[PJRT:CPU] Parallelize transposes when passing inputs to computations.\n\nThe transpose code already supported parallelization, we should use it.\n\nDefault to a maximum of 8 threads; it is easy to saturate memory bandwidth with transposes anyway.\n\nPiperOrigin-RevId: 834031096",
    "sha": "f30e1b987c3c90276e7995674ffbd68e3bb371eb",
    "files": [
        {
            "sha": "313d71ac20aef75f225951ad0e85eae38388bb4d",
            "filename": "third_party/xla/xla/pjrt/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD?ref=f30e1b987c3c90276e7995674ffbd68e3bb371eb",
            "patch": "@@ -73,6 +73,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/concurrency:ref_count\",\n+        \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\","
        },
        {
            "sha": "433b6c9fac3e3ef368c231fee23df68374345c85",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc?ref=f30e1b987c3c90276e7995674ffbd68e3bb371eb",
            "patch": "@@ -207,7 +207,8 @@ absl::StatusOr<std::unique_ptr<PjRtClient>> GetPjRtCpuClient(\n   return std::unique_ptr<PjRtClient>(new PjRtCpuClient(\n       options.process_id, std::move(devices), std::move(allocator),\n       std::move(options.collectives), num_threads, options.asynchronous,\n-      std::move(options.customize_hlo_module_config)));\n+      std::move(options.customize_hlo_module_config),\n+      options.max_transpose_threads));\n }\n \n // An upper bound on the number of threads to use for intra-op parallelism. It\n@@ -243,7 +244,8 @@ PjRtCpuClient::PjRtCpuClient(\n     std::shared_ptr<CpuDeviceMemory::Allocator> allocator,\n     std::shared_ptr<cpu::CpuCollectives> collectives, size_t num_threads,\n     bool asynchronous,\n-    std::function<void(HloModuleConfig&)> customize_hlo_module_config)\n+    std::function<void(HloModuleConfig&)> customize_hlo_module_config,\n+    int max_transpose_threads)\n     : process_index_(process_index),\n       owned_devices_(std::move(devices)),\n       computation_placer_(std::make_unique<ComputationPlacer>()),\n@@ -270,7 +272,8 @@ PjRtCpuClient::PjRtCpuClient(\n           new tsl::thread::ThreadPool(tsl::Env::Default(), GetThreadOptions(),\n                                       \"XLAPjRtCpuClient\", num_threads)),\n       async_work_runner_(\n-          MakeThreadPoolAsyncWorkRunner(pjrt_client_thread_pool_.get())) {\n+          MakeThreadPoolAsyncWorkRunner(pjrt_client_thread_pool_.get())),\n+      max_transpose_threads_(max_transpose_threads) {\n   for (const std::unique_ptr<PjRtCpuDevice>& device : owned_devices_) {\n     devices_.push_back(device.get());\n     CHECK(\n@@ -923,7 +926,8 @@ PjRtCpuClient::LinearizeHostBufferInto(\n       ->CopyFromHostBuffer(\n           data, type, dims, byte_strides, host_buffer_semantics,\n           std::move(on_done_with_host_buffer), device_shape,\n-          async_work_runner(), &transpose_mu_, &transpose_cache_);\n+          async_work_runner(), &transpose_mu_, &transpose_cache_,\n+          eigen_intraop_pool(), max_transpose_threads_);\n }\n \n absl::StatusOr<tsl::RCReference<PjRtDeviceEvent>> PjRtCpuClient::LinearizeInto("
        },
        {
            "sha": "6b98426b0e98464d6aeaf78a8d16a230e0604fb1",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.h?ref=f30e1b987c3c90276e7995674ffbd68e3bb371eb",
            "patch": "@@ -260,7 +260,8 @@ class PjRtCpuClient final : public CommonPjRtClient {\n       std::shared_ptr<CpuDeviceMemory::Allocator> allocator,\n       std::shared_ptr<cpu::CpuCollectives> collectives, size_t num_threads,\n       bool asynchronous,\n-      std::function<void(HloModuleConfig&)> customize_hlo_module_config);\n+      std::function<void(HloModuleConfig&)> customize_hlo_module_config,\n+      int max_transpose_threads);\n \n   absl::StatusOr<std::unique_ptr<PjRtLoadedExecutable>> CompileInternal(\n       const XlaComputation& computation,\n@@ -334,6 +335,10 @@ class PjRtCpuClient final : public CommonPjRtClient {\n   // Thread pool for running PjRtClient tasks.\n   std::unique_ptr<tsl::thread::ThreadPool> pjrt_client_thread_pool_;\n   std::unique_ptr<AsyncWorkRunner> async_work_runner_;\n+\n+  // Maximum number of threads to use for any one transpose. We will use the\n+  // the lesser of this number and the thread pool size. 1 = no threading.\n+  int max_transpose_threads_;\n };\n \n class PjRtCpuExecutable final : public PjRtLoadedExecutable {"
        },
        {
            "sha": "ee8d25073b28e264118b8767134964565b8216e8",
            "filename": "third_party/xla/xla/pjrt/cpu/raw_buffer.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.cc?ref=f30e1b987c3c90276e7995674ffbd68e3bb371eb",
            "patch": "@@ -15,9 +15,11 @@ limitations under the License.\n \n #include \"xla/pjrt/cpu/raw_buffer.h\"\n \n+#include <algorithm>\n #include <cstddef>\n #include <cstdint>\n #include <cstring>\n+#include <functional>\n #include <memory>\n #include <optional>\n #include <utility>\n@@ -54,6 +56,7 @@ limitations under the License.\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n@@ -199,7 +202,8 @@ CpuRawBuffer::CopyFromHostBuffer(\n     PjRtClient::HostBufferSemantics host_buffer_semantics,\n     absl::AnyInvocable<void() &&> on_done_with_host_buffer, const Shape& shape,\n     AsyncWorkRunner* async_work_runner, absl::Mutex* transpose_mu,\n-    TransposePlanCache* transpose_cache) {\n+    TransposePlanCache* transpose_cache, tsl::thread::ThreadPool* thread_pool,\n+    int max_transpose_threads) {\n   tsl::AsyncValueRef<CpuDeviceMemory> device_buffer = buffer_;\n   bool has_default_layout =\n       !byte_strides || HasMajorToMinorLayout(type, dims, *byte_strides);\n@@ -230,18 +234,28 @@ CpuRawBuffer::CopyFromHostBuffer(\n       if (byte_strides) {\n         options.input_layout = TransposePlan::Striding{*byte_strides};\n       }\n+      if (thread_pool) {\n+        options.num_threads =\n+            std::min(thread_pool->NumThreads(), max_transpose_threads);\n+      }\n       absl::MutexLock lock(*transpose_mu);\n       TF_ASSIGN_OR_RETURN(transpose, transpose_cache->GetOrCreate(options));\n     }\n+    std::function<void(std::function<void(void)>)> schedule_work;\n+    if (thread_pool && max_transpose_threads > 1) {\n+      schedule_work = [thread_pool](std::function<void(void)> work) {\n+        thread_pool->Schedule(std::move(work));\n+      };\n+    }\n     if (!is_packed) {\n-      transpose->Execute(data, dst_data_ptr);\n+      transpose->Execute(data, dst_data_ptr, schedule_work);\n     } else {\n       // First transpose the unpacked data into a new temporary buffer, then\n       // pack the data.\n       // TODO(reedwm): Fuse the transpose and packing by having TransposePlan\n       // support packing.\n       auto data_transposed = std::make_unique<char[]>(byte_size);\n-      transpose->Execute(data, data_transposed.get());\n+      transpose->Execute(data, data_transposed.get(), schedule_work);\n       absl::Span<const char> src_data_span(data_transposed.get(), byte_size);\n       absl::Span<char> dst_data_span(static_cast<char*>(dst_data_ptr),\n                                      dst_byte_size);"
        },
        {
            "sha": "0d5f91fc08f74a5e0a8cf94fce5b5fcea3bb996e",
            "filename": "third_party/xla/xla/pjrt/cpu/raw_buffer.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.h?ref=f30e1b987c3c90276e7995674ffbd68e3bb371eb",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/tsl/concurrency/async_value.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -144,7 +145,8 @@ class CpuRawBuffer : public CommonPjRtRawBuffer {\n       PjRtClient::HostBufferSemantics host_buffer_semantics,\n       absl::AnyInvocable<void() &&> on_done_with_host_buffer,\n       const Shape& shape, AsyncWorkRunner* async_work_runner,\n-      absl::Mutex* transpose_mu, TransposePlanCache* transpose_cache);\n+      absl::Mutex* transpose_mu, TransposePlanCache* transpose_cache,\n+      tsl::thread::ThreadPool* thread_pool, int max_transpose_threads);\n \n   void ReadDynamicShape(tsl::AsyncValueRef<xla::Shape> output_shape,\n                         xla::Shape shape) override;"
        },
        {
            "sha": "6c29cfe70680f8c79bd1892fa23e34a0e319e31a",
            "filename": "third_party/xla/xla/pjrt/plugin/xla_cpu/cpu_client_options.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2Fcpu_client_options.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f30e1b987c3c90276e7995674ffbd68e3bb371eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2Fcpu_client_options.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2Fcpu_client_options.h?ref=f30e1b987c3c90276e7995674ffbd68e3bb371eb",
            "patch": "@@ -60,6 +60,10 @@ struct CpuClientOptions {\n   std::function<absl::StatusOr<std::unique_ptr<CpuMemory>>(size_t size_bytes,\n                                                            size_t alignment)>\n       allocator;\n+\n+  // Maximum number of threads to use for any one transpose. We will use the\n+  // the lesser of this number and the thread pool size. 1 = no threading.\n+  int max_transpose_threads = 8;\n };\n \n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 39,
        "deletions": 9
    }
}