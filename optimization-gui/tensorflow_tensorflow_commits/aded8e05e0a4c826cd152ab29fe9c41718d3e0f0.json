{
    "author": "loislo",
    "message": "[XLA:GPU] add buffer_nan_count_thunk for the buffer_nan_count_kernel\n\nIn the follow up cl we will need to add this thunk to the buffer debug pass.\nAlso there we will need to infer the buffer element type.\nAnother refactoring would be to change the name of the payload which is the checksum at the moment to something more generic like 'value' or 'result'.\nOne more thing we could do is to reduce the code duplication by merging together both thunks, the checksum one and nan counter one.\n\nPiperOrigin-RevId: 824491914",
    "sha": "aded8e05e0a4c826cd152ab29fe9c41718d3e0f0",
    "files": [
        {
            "sha": "12e862381eaa80161080fcd1936b37810fb9c52b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 58,
            "deletions": 0,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=aded8e05e0a4c826cd152ab29fe9c41718d3e0f0",
            "patch": "@@ -3058,6 +3058,64 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"buffers_nan_count_thunk\",\n+    srcs = [\"buffers_nan_count_thunk.cc\"],\n+    hdrs = [\"buffers_nan_count_thunk.h\"],\n+    deps = [\n+        \":thunk\",\n+        \":thunk_buffer_id\",\n+        \"//xla:types\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/cuda:cuda_platform_id\",\n+        \"//xla/stream_executor/gpu:buffer_debug_log\",\n+        \"//xla/stream_executor/gpu:buffer_debug_nan_count_kernel\",\n+        \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+    ] + if_cuda_is_configured([\n+        \"//xla/stream_executor/cuda:buffer_debug_nan_count_kernel_cuda\",\n+    ]),\n+)\n+\n+xla_test(\n+    name = \"buffers_nan_count_thunk_test\",\n+    srcs = [\"buffers_nan_count_thunk_test.cc\"],\n+    backends = [\"gpu\"],\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \":buffer_debug_log_structs\",\n+        \":buffers_nan_count_thunk\",\n+        \":thunk\",\n+        \":thunk_buffer_id\",\n+        \":thunk_id\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service:executable\",\n+        \"//xla/service/gpu:buffer_allocations\",\n+        \"//xla/service/gpu:resource_requests\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n+        \"//xla/stream_executor/gpu:buffer_debug_log\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n tf_proto_library(\n     name = \"buffer_debug_log_proto\",\n     srcs = [\"buffer_debug_log.proto\"],"
        },
        {
            "sha": "30bc069b5acac831997644ca201cffc9d6a26780",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk.cc",
            "status": "added",
            "additions": 128,
            "deletions": 0,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc?ref=aded8e05e0a4c826cd152ab29fe9c41718d3e0f0",
            "patch": "@@ -0,0 +1,128 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/buffers_nan_count_thunk.h\"\n+\n+#include <cstdint>\n+#include <string>\n+\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n+#include \"xla/stream_executor/gpu/buffer_debug_nan_count_kernel.h\"\n+#include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/types.h\"\n+\n+namespace xla::gpu {\n+\n+namespace se = stream_executor;\n+\n+absl::Status BuffersDebugNanCountThunk::Initialize(\n+    const InitializeParams& params) {\n+  if (params.executor->GetPlatform()->id() != se::cuda::kCudaPlatformId) {\n+    VLOG(1)\n+        << \"Buffer nan-counting not supported on non-CUDA platforms, skipping\";\n+    return absl::OkStatus();\n+  }\n+  if (!params.executor->GetDeviceDescription()\n+           .cuda_compute_capability()\n+           .IsAtLeastPascal()) {\n+    VLOG(1)\n+        << \"Buffer nan-counting not supported on CUDA architectures older than \"\n+           \"Pascal due to missing atomic fetch_add with system scope, skipping\";\n+    return absl::OkStatus();\n+  }\n+\n+  se::gpu::GpuKernelRegistry registry =\n+      se::gpu::GpuKernelRegistry::GetGlobalRegistry();\n+  TF_ASSIGN_OR_RETURN(\n+      kernel_f32_, registry.LoadKernel<se::gpu::BufferDebugNanCountF32Kernel>(\n+                       params.executor));\n+  TF_ASSIGN_OR_RETURN(\n+      kernel_bf16_, registry.LoadKernel<se::gpu::BufferDebugNanCountBf16Kernel>(\n+                        params.executor));\n+\n+  VLOG(1) << \"NanCount kernel loaded\";\n+  return absl::OkStatus();\n+}\n+\n+absl::Status BuffersDebugNanCountThunk::ExecuteOnStream(\n+    const ExecuteParams& params) {\n+  se::StreamExecutor* executor = params.stream->parent();\n+  if (!kernel_f32_.has_value()) {\n+    // Initialize didn't load the kernel. This can happen when we're running on\n+    // an unsupported platform.\n+    VLOG(1) << \"NanCount kernel not loaded, skipping\";\n+    return absl::OkStatus();\n+  }\n+\n+  VLOG(1) << \"BuffersDebugNanCountThunk::ExecuteOnStream\";\n+\n+  const se::ThreadDim thread_dim(\n+      executor->GetDeviceDescription().threads_per_block_limit(), 1, 1);\n+\n+  se::DeviceMemory<uint8_t> log_ptr(\n+      params.buffer_allocations->GetDeviceAddress(log_slice_));\n+  se::gpu::BufferDebugLog buffer_debug_log =\n+      se::gpu::BufferDebugLog::FromDeviceMemoryUnchecked(log_ptr);\n+\n+  for (const auto& [entry_id, buffer_slice_pair] : buffers_) {\n+    BufferAllocation::Slice buffer = buffer_slice_pair.buffer;\n+    PrimitiveType buffer_type = buffer_slice_pair.element_type;\n+    se::DeviceMemoryBase device_buffer =\n+        params.buffer_allocations->GetDeviceAddress(buffer);\n+    if (buffer_type == PrimitiveType::F32) {\n+      se::DeviceMemory<float> f32_buffer(device_buffer);\n+      TF_RETURN_IF_ERROR(kernel_f32_->Launch(\n+          thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n+          f32_buffer, f32_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n+          buffer_debug_log.GetDeviceEntries()));\n+    } else if (buffer_type == PrimitiveType::BF16) {\n+      se::DeviceMemory<Eigen::bfloat16> bf16_buffer(device_buffer);\n+      TF_RETURN_IF_ERROR(kernel_bf16_->Launch(\n+          thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n+          bf16_buffer, bf16_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n+          buffer_debug_log.GetDeviceEntries()));\n+    } else {\n+      VLOG(1) << \"Unsupported primitive type for NaN counting: \"\n+              << PrimitiveType_Name(buffer_type);\n+    }\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+std::string BuffersDebugNanCountThunk::ToString(int indent) const {\n+  std::string result;\n+  absl::StrAppend(&result, \", buffers = \", buffers_.size());\n+  for (const auto& [buffer_id, buffer_slice_pair] : buffers_) {\n+    absl::StrAppend(&result, \"\\n\", std::string(indent + 2, ' '),\n+                    \"buffer_id: \", buffer_id,\n+                    \", buffer: \", buffer_slice_pair.buffer.ToString());\n+  }\n+  return result;\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "b9d9f0679b45e67fd2cf728498150852203804f0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk.h",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h?ref=aded8e05e0a4c826cd152ab29fe9c41718d3e0f0",
            "patch": "@@ -0,0 +1,68 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_BUFFERS_NAN_COUNT_THUNK_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_BUFFERS_NAN_COUNT_THUNK_H_\n+\n+#include <optional>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/stream_executor/gpu/buffer_debug_nan_count_kernel.h\"\n+\n+namespace xla::gpu {\n+\n+class BuffersDebugNanCountThunk : public Thunk {\n+ public:\n+  struct BufferToCount {\n+    BufferAllocation::Slice buffer;\n+    PrimitiveType element_type;\n+  };\n+\n+  explicit BuffersDebugNanCountThunk(\n+      ThunkInfo info, BufferAllocation::Slice log_slice,\n+      absl::flat_hash_map<ThunkBufferId, BufferToCount> buffers)\n+      : Thunk(Thunk::Kind::kBuffersDebugNanCount, std::move(info)),\n+        log_slice_(log_slice),\n+        buffers_(std::move(buffers)) {}\n+\n+  absl::Status Initialize(const InitializeParams& params) override;\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+\n+  std::string ToString(int indent) const override;\n+\n+  BufferUses buffer_uses() const override {\n+    // Intentionally left empty to not nan-count the nan-counting thunk.\n+    return {};\n+  }\n+\n+ private:\n+  // Loaded in Initialize.\n+  std::optional<stream_executor::gpu::BufferDebugNanCountF32Kernel::KernelType>\n+      kernel_f32_;\n+  std::optional<stream_executor::gpu::BufferDebugNanCountBf16Kernel::KernelType>\n+      kernel_bf16_;\n+  BufferAllocation::Slice log_slice_;\n+  absl::flat_hash_map<ThunkBufferId, BufferToCount> buffers_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_BUFFERS_NAN_COUNT_THUNK_H_"
        },
        {
            "sha": "98c3bf2f108d06cb08a001790a90a742be952f78",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk_test.cc",
            "status": "added",
            "additions": 152,
            "deletions": 0,
            "changes": 152,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc?ref=aded8e05e0a4c826cd152ab29fe9c41718d3e0f0",
            "patch": "@@ -0,0 +1,152 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/buffers_nan_count_thunk.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <limits>\n+#include <memory>\n+#include <optional>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/buffer_allocations.h\"\n+#include \"xla/service/gpu/resource_requests.h\"\n+#include \"xla/service/service_executable_run_options.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+namespace se = stream_executor;\n+\n+using ::stream_executor::gpu::BufferDebugLog;\n+using ::testing::UnorderedElementsAre;\n+\n+class BuffersDebugNanCountThunkTest : public ::testing::Test {\n+ protected:\n+  void SetUp() override {\n+    TF_ASSERT_OK_AND_ASSIGN(platform_,\n+                            se::PlatformManager::PlatformWithName(\"CUDA\"));\n+    TF_ASSERT_OK_AND_ASSIGN(executor_, platform_->ExecutorForDevice(0));\n+    TF_ASSERT_OK_AND_ASSIGN(stream_, executor_->CreateStream(std::nullopt));\n+    allocator_ =\n+        std::make_unique<se::StreamExecutorMemoryAllocator>(stream_->parent());\n+\n+    if (!executor_->GetDeviceDescription()\n+             .cuda_compute_capability()\n+             .IsAtLeastPascal()) {\n+      GTEST_SKIP()\n+          << \"buffer nan-counting is not supported on CUDA architectures \"\n+             \"older than Pascal due to missing atomic fetch_add with \"\n+             \"system scope\";\n+    }\n+  }\n+\n+  se::Platform* platform_;\n+  se::StreamExecutor* executor_;\n+  std::unique_ptr<se::Stream> stream_;\n+  std::unique_ptr<se::StreamExecutorMemoryAllocator> allocator_;\n+};\n+\n+TEST_F(BuffersDebugNanCountThunkTest, CalculatesNanCounts) {\n+  static constexpr size_t kLogSize = BufferDebugLog::RequiredSizeForEntries(10);\n+  static constexpr size_t kInputElems = 1024;\n+  static constexpr size_t kInputSizeInBytes = kInputElems * sizeof(float);\n+  static constexpr size_t kTotalDeviceMemoryBytes =\n+      kLogSize + kInputSizeInBytes * 2;\n+  // Setup memory allocations for the log and inputs\n+  BufferAllocation alloc(/*index=*/0,\n+                         /*size=*/kTotalDeviceMemoryBytes,\n+                         /*color=*/0);\n+  BufferAllocation::Slice log_slice(&alloc, /*offset=*/0, kLogSize);\n+  BufferAllocation::Slice inputs[2];\n+  for (int i = 0; i < 2; ++i) {\n+    inputs[i] = BufferAllocation::Slice(\n+        &alloc, /*offset=*/kLogSize + i * kInputSizeInBytes, kInputSizeInBytes);\n+  }\n+  BufferAllocations allocations(\n+      {executor_->AllocateArray<uint8_t>(kTotalDeviceMemoryBytes)},\n+      executor_->device_ordinal(), allocator_.get());\n+  se::DeviceMemoryBase log_mem = allocations.GetDeviceAddress(log_slice);\n+  se::DeviceMemoryBase inputs0_mem = allocations.GetDeviceAddress(inputs[0]);\n+  se::DeviceMemoryBase inputs1_mem = allocations.GetDeviceAddress(inputs[1]);\n+  // Initialize the log in device memory\n+  TF_ASSERT_OK_AND_ASSIGN(BufferDebugLog device_log,\n+                          BufferDebugLog::CreateOnDevice(\n+                              *stream_, se::DeviceMemory<uint8_t>(log_mem)));\n+  // Fill inputs with some data\n+  std::vector<float> data(kInputElems, 0);\n+  data[123] = std::numeric_limits<float>::quiet_NaN();\n+  TF_ASSERT_OK(stream_->Memcpy(&inputs0_mem, data.data(), kInputSizeInBytes));\n+  data[123] = 0;\n+  data[456] = std::numeric_limits<float>::quiet_NaN();\n+  data[789] = std::numeric_limits<float>::quiet_NaN();\n+  TF_ASSERT_OK(stream_->Memcpy(&inputs1_mem, data.data(), kInputSizeInBytes));\n+  // Setup parameters for Initialize/Prepare/ExecuteOnStream\n+  Thunk::InitializeParams init_params;\n+  init_params.executor = executor_;\n+  init_params.stream = stream_.get();\n+  ResourceRequests resource_requests;\n+  auto execute_params = Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), allocations, stream_.get(),\n+      /*command_buffer_trace_stream=*/stream_.get(),\n+      /*collective_params=*/nullptr, /*collective_cliques=*/nullptr);\n+\n+  BuffersDebugNanCountThunk thunk(\n+      Thunk::ThunkInfo(), log_slice,\n+      {{ThunkBufferId::Create(ThunkId(123), 4).value(),\n+        {inputs[0], PrimitiveType::F32}},\n+       {ThunkBufferId::Create(ThunkId(456), 8).value(),\n+        {inputs[1], PrimitiveType::F32}}});\n+  TF_ASSERT_OK(thunk.Initialize(init_params));\n+  TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}, resource_requests));\n+  TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));\n+  TF_ASSERT_OK_AND_ASSIGN(std::vector<BufferDebugLogEntry> entries,\n+                          device_log.ReadFromDevice(*stream_));\n+\n+  // BuffersDebugNanCountThunk launches a kernel for each input buffer, they may\n+  // complete in any order.\n+  EXPECT_THAT(\n+      entries,\n+      UnorderedElementsAre(\n+          BufferDebugLogEntry{\n+              /*entry_id=*/ThunkBufferId::Create(ThunkId(123), 4).value(),\n+              /*checksum=*/1,  // We use checksum field for NaN count. It will\n+                               // be generalised in the follow-up commits.\n+          },\n+          BufferDebugLogEntry{\n+              /*entry_id=*/ThunkBufferId::Create(ThunkId(456), 8).value(),\n+              /*checksum=*/2,  // We use checksum field for NaN count. It will\n+                               // be generalised in the follow-up commits.\n+          }));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "79f2e714eecc242a0087d28b5e3d7d12a7181b79",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc?ref=aded8e05e0a4c826cd152ab29fe9c41718d3e0f0",
            "patch": "@@ -260,6 +260,7 @@ Thunk::ExecuteParams::ExecuteParams(\n     CASE(kAllToAllDone);\n     CASE(kAllToAllStart);\n     CASE(kBuffersDebugChecksum);\n+    CASE(kBuffersDebugNanCount);\n     CASE(kCholesky);\n     CASE(kCollectiveBroadcast);\n     CASE(kCollectiveBroadcastDone);"
        },
        {
            "sha": "c0e2a24dae1cdb2729288bd1bb79fd517256344c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aded8e05e0a4c826cd152ab29fe9c41718d3e0f0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h?ref=aded8e05e0a4c826cd152ab29fe9c41718d3e0f0",
            "patch": "@@ -138,6 +138,7 @@ class Thunk {\n     kAllToAllDone,\n     kAllToAllStart,\n     kBuffersDebugChecksum,\n+    kBuffersDebugNanCount,\n     kCholesky,\n     kCollectiveBroadcast,\n     kCollectiveBroadcastDone,"
        }
    ],
    "stats": {
        "total": 408,
        "additions": 408,
        "deletions": 0
    }
}