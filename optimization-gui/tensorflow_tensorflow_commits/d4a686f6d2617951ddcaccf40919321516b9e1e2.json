{
    "author": "ezhulenev",
    "message": "[xla:cpu] Extract dot implementation into dot_lib\n\n<details>\n\n```\nname                                                                                    cpu/op         cpu/op      vs base\nBM_BatchedDot_F32_F32_1x2x2/process_time                                                 5.907µ ±  1%   5.925µ ±  1%       ~ (p=0.640 n=20)\nBM_BatchedDot_F32_F32_1x32x32/process_time                                               8.949µ ±  1%   8.904µ ±  1%       ~ (p=0.149 n=20)\nBM_BatchedDot_F32_F32_1x64x64/process_time                                               14.36µ ±  1%   14.36µ ±  1%       ~ (p=0.620 n=20)\nBM_BatchedDot_F32_F32_1x128x128/process_time                                             159.2µ ±  2%   159.5µ ±  2%       ~ (p=0.445 n=20)\nBM_BatchedDot_F32_F32_1x256x256/process_time                                             1.463m ±  1%   1.462m ±  1%       ~ (p=0.583 n=20)\nBM_BatchedDot_F32_F32_1x512x512/process_time                                             6.128m ±  0%   6.105m ±  0%       ~ (p=0.231 n=20)\nBM_BatchedDot_F32_F32_2x2x2/process_time                                                 5.748µ ±  1%   5.731µ ±  1%       ~ (p=0.512 n=20)\nBM_BatchedDot_F32_F32_2x32x32/process_time                                               10.97µ ±  1%   10.91µ ±  0%       ~ (p=0.114 n=20)\nBM_BatchedDot_F32_F32_2x64x64/process_time                                               22.04µ ±  1%   21.95µ ±  2%       ~ (p=0.883 n=20)\nBM_BatchedDot_F32_F32_2x128x128/process_time                                             301.9µ ±  5%   300.6µ ±  2%       ~ (p=0.904 n=20)\nBM_BatchedDot_F32_F32_2x256x256/process_time                                             3.080m ±  1%   3.094m ±  1%       ~ (p=0.678 n=20)\nBM_BatchedDot_F32_F32_2x512x512/process_time                                             12.72m ±  0%   12.74m ±  0%       ~ (p=0.583 n=20)\nBM_BatchedDot_F32_F32_4x2x2/process_time                                                 5.807µ ±  1%   5.783µ ±  1%       ~ (p=0.231 n=20)\nBM_BatchedDot_F32_F32_4x32x32/process_time                                               15.22µ ±  1%   15.06µ ±  1%       ~ (p=0.086 n=20)\nBM_BatchedDot_F32_F32_4x64x64/process_time                                               37.31µ ±  1%   37.08µ ±  1%       ~ (p=0.127 n=20)\nBM_BatchedDot_F32_F32_4x128x128/process_time                                             618.9µ ±  3%   608.3µ ±  4%       ~ (p=0.478 n=20)\nBM_BatchedDot_F32_F32_4x256x256/process_time                                             6.293m ±  1%   6.301m ±  1%       ~ (p=0.620 n=20)\nBM_BatchedDot_F32_F32_4x512x512/process_time                                             23.66m ±  1%   23.69m ±  1%       ~ (p=0.659 n=20)\nBM_BatchedDot_F32_F32_8x2x2/process_time                                                 5.811µ ±  1%   5.818µ ±  1%       ~ (p=0.968 n=20)\nBM_BatchedDot_F32_F32_8x32x32/process_time                                               23.42µ ±  2%   23.17µ ±  1%       ~ (p=0.253 n=20)\nBM_BatchedDot_F32_F32_8x64x64/process_time                                               68.64µ ±  1%   68.09µ ±  1%  -0.80% (p=0.046 n=20)\nBM_BatchedDot_F32_F32_8x128x128/process_time                                             1.254m ±  4%   1.235m ±  4%       ~ (p=0.142 n=20)\nBM_BatchedDot_F32_F32_8x256x256/process_time                                             12.66m ±  1%   12.75m ±  1%       ~ (p=0.445 n=20)\nBM_BatchedDot_F32_F32_8x512x512/process_time                                             48.77m ±  1%   48.51m ±  0%  -0.53% (p=0.018 n=20)\nBM_BatchedDot_BF16_F32_1x2x2/process_time                                                6.903µ ±  1%   6.846µ ±  1%  -0.83% (p=0.002 n=20)\nBM_BatchedDot_BF16_F32_1x32x32/process_time                                              9.154µ ±  1%   9.123µ ±  1%       ~ (p=0.620 n=20)\nBM_BatchedDot_BF16_F32_1x64x64/process_time                                              19.04µ ±  0%   18.95µ ±  0%       ~ (p=0.183 n=20)\nBM_BatchedDot_BF16_F32_1x128x128/process_time                                            93.79µ ±  1%   93.49µ ±  0%       ~ (p=0.355 n=20)\nBM_BatchedDot_BF16_F32_1x256x256/process_time                                            1.412m ±  0%   1.411m ±  1%       ~ (p=0.369 n=20)\nBM_BatchedDot_BF16_F32_1x512x512/process_time                                            10.27m ±  0%   10.24m ±  0%       ~ (p=0.149 n=20)\nBM_BatchedDot_BF16_F32_2x2x2/process_time                                                6.933µ ±  1%   6.909µ ±  1%       ~ (p=0.127 n=20)\nBM_BatchedDot_BF16_F32_2x32x32/process_time                                              11.23µ ±  1%   11.18µ ±  0%       ~ (p=0.221 n=20)\nBM_BatchedDot_BF16_F32_2x64x64/process_time                                              30.43µ ±  1%   30.43µ ±  0%       ~ (p=0.968 n=20)\nBM_BatchedDot_BF16_F32_2x128x128/process_time                                            180.7µ ±  1%   180.2µ ±  0%       ~ (p=0.211 n=20)\nBM_BatchedDot_BF16_F32_2x256x256/process_time                                            2.559m ±  1%   2.562m ±  0%       ~ (p=0.841 n=20)\nBM_BatchedDot_BF16_F32_2x512x512/process_time                                            19.96m ±  0%   19.92m ±  0%       ~ (p=0.142 n=20)\nBM_BatchedDot_BF16_F32_4x2x2/process_time                                                6.963µ ±  1%   6.924µ ±  1%       ~ (p=0.060 n=20)\nBM_BatchedDot_BF16_F32_4x32x32/process_time                                              15.12µ ±  1%   15.08µ ±  0%       ~ (p=0.445 n=20)\nBM_BatchedDot_BF16_F32_4x64x64/process_time                                              52.97µ ±  0%   52.74µ ±  0%       ~ (p=0.056 n=20)\nBM_BatchedDot_BF16_F32_4x128x128/process_time                                            390.7µ ±  1%   391.6µ ±  3%       ~ (p=0.620 n=20)\nBM_BatchedDot_BF16_F32_4x256x256/process_time                                            4.880m ±  1%   4.874m ±  0%       ~ (p=0.127 n=20)\nBM_BatchedDot_BF16_F32_4x512x512/process_time                                            39.50m ±  1%   39.34m ±  0%       ~ (p=0.157 n=20)\nBM_BatchedDot_BF16_F32_8x2x2/process_time                                                7.073µ ±  1%   6.969µ ±  1%  -1.47% (p=0.007 n=20)\nBM_BatchedDot_BF16_F32_8x32x32/process_time                                              22.05µ ±  1%   22.02µ ±  1%       ~ (p=0.820 n=20)\nBM_BatchedDot_BF16_F32_8x64x64/process_time                                              98.13µ ±  1%   97.93µ ±  1%       ~ (p=0.265 n=20)\nBM_BatchedDot_BF16_F32_8x128x128/process_time                                            800.2µ ±  1%   797.4µ ±  6%       ~ (p=0.820 n=20)\nBM_BatchedDot_BF16_F32_8x256x256/process_time                                            9.546m ±  0%   9.518m ±  0%       ~ (p=0.114 n=20)\nBM_BatchedDot_BF16_F32_8x512x512/process_time                                            81.05m ±  1%   80.00m ±  1%       ~ (p=0.108 n=20)\nBM_BatchedDot_S8_S32_1x2x2/process_time                                                  7.000µ ±  1%   6.922µ ±  1%       ~ (p=0.072 n=20)\nBM_BatchedDot_S8_S32_1x32x32/process_time                                                8.784µ ±  2%   8.696µ ±  1%       ~ (p=0.157 n=20)\nBM_BatchedDot_S8_S32_1x64x64/process_time                                                15.03µ ±  1%   14.95µ ±  0%       ~ (p=0.056 n=20)\nBM_BatchedDot_S8_S32_1x128x128/process_time                                              62.15µ ±  1%   61.96µ ±  0%       ~ (p=0.231 n=20)\nBM_BatchedDot_S8_S32_1x256x256/process_time                                              1.124m ±  1%   1.120m ±  0%       ~ (p=0.068 n=20)\nBM_BatchedDot_S8_S32_1x512x512/process_time                                              7.623m ±  0%   7.597m ±  0%       ~ (p=0.052 n=20)\nBM_BatchedDot_S8_S32_2x2x2/process_time                                                  7.038µ ±  1%   7.018µ ±  1%       ~ (p=0.289 n=20)\nBM_BatchedDot_S8_S32_2x32x32/process_time                                                10.21µ ±  1%   10.18µ ±  1%       ~ (p=0.369 n=20)\nBM_BatchedDot_S8_S32_2x64x64/process_time                                                23.26µ ±  1%   23.18µ ±  0%       ~ (p=0.121 n=20)\nBM_BatchedDot_S8_S32_2x128x128/process_time                                              117.1µ ±  1%   116.7µ ±  0%       ~ (p=0.314 n=20)\nBM_BatchedDot_S8_S32_2x256x256/process_time                                              2.042m ±  1%   2.037m ±  1%       ~ (p=0.211 n=20)\nBM_BatchedDot_S8_S32_2x512x512/process_time                                              14.81m ±  0%   14.78m ±  0%       ~ (p=0.108 n=20)\nBM_BatchedDot_S8_S32_4x2x2/process_time                                                  7.047µ ±  1%   7.033µ ±  1%       ~ (p=0.583 n=20)\nBM_BatchedDot_S8_S32_4x32x32/process_time                                                12.61µ ±  1%   12.55µ ±  1%       ~ (p=0.211 n=20)\nBM_BatchedDot_S8_S32_4x64x64/process_time                                                38.32µ ±  1%   38.12µ ±  0%       ~ (p=0.134 n=20)\nBM_BatchedDot_S8_S32_4x128x128/process_time                                              263.2µ ±  2%   262.6µ ±  2%       ~ (p=0.640 n=20)\nBM_BatchedDot_S8_S32_4x256x256/process_time                                              3.884m ±  1%   3.880m ±  0%       ~ (p=0.301 n=20)\nBM_BatchedDot_S8_S32_4x512x512/process_time                                              29.18m ±  1%   29.08m ±  1%       ~ (p=0.121 n=20)\nBM_BatchedDot_S8_S32_8x2x2/process_time                                                  7.103µ ±  1%   7.034µ ±  1%       ~ (p=0.063 n=20)\nBM_BatchedDot_S8_S32_8x32x32/process_time                                                17.92µ ±  1%   17.79µ ±  1%       ~ (p=0.211 n=20)\nBM_BatchedDot_S8_S32_8x64x64/process_time                                                68.75µ ±  1%   68.50µ ±  1%       ~ (p=0.221 n=20)\nBM_BatchedDot_S8_S32_8x128x128/process_time                                              532.4µ ± 10%   565.4µ ± 11%       ~ (p=0.211 n=20)\nBM_BatchedDot_S8_S32_8x256x256/process_time                                              7.591m ±  1%   7.562m ±  0%       ~ (p=0.265 n=20)\nBM_BatchedDot_S8_S32_8x512x512/process_time                                              59.05m ±  0%   58.90m ±  1%       ~ (p=0.192 n=20)\nBM_BatchedDot_S32_S32_1x2x2/process_time                                                 6.003µ ±  1%   5.937µ ±  1%       ~ (p=0.081 n=20)\nBM_BatchedDot_S32_S32_1x32x32/process_time                                               13.90µ ±  1%   13.91µ ±  0%       ~ (p=0.659 n=20)\nBM_BatchedDot_S32_S32_1x64x64/process_time                                               54.75µ ±  0%   54.60µ ±  0%       ~ (p=0.149 n=20)\nBM_BatchedDot_S32_S32_1x128x128/process_time                                             500.4µ ±  2%   526.6µ ±  5%  +5.24% (p=0.005 n=20)\nBM_BatchedDot_S32_S32_1x256x256/process_time                                             6.008m ±  0%   5.999m ±  0%       ~ (p=0.108 n=20)\nBM_BatchedDot_S32_S32_1x512x512/process_time                                             47.14m ±  1%   47.09m ±  1%       ~ (p=0.398 n=20)\nBM_BatchedDot_S32_S32_2x2x2/process_time                                                 5.984µ ±  1%   5.971µ ±  1%       ~ (p=0.529 n=20)\nBM_BatchedDot_S32_S32_2x32x32/process_time                                               20.87µ ±  1%   20.88µ ±  0%       ~ (p=0.461 n=20)\nBM_BatchedDot_S32_S32_2x64x64/process_time                                               102.9µ ±  0%   102.5µ ±  0%       ~ (p=0.114 n=20)\nBM_BatchedDot_S32_S32_2x128x128/process_time                                             974.2µ ±  5%   972.6µ ±  2%       ~ (p=0.640 n=20)\nBM_BatchedDot_S32_S32_2x256x256/process_time                                             11.73m ±  0%   11.73m ±  0%       ~ (p=0.947 n=20)\nBM_BatchedDot_S32_S32_2x512x512/process_time                                             93.66m ±  1%   93.93m ±  0%       ~ (p=0.414 n=20)\nBM_BatchedDot_S32_S32_4x2x2/process_time                                                 6.127µ ±  1%   6.013µ ±  1%  -1.85% (p=0.002 n=20)\nBM_BatchedDot_S32_S32_4x32x32/process_time                                               34.78µ ±  0%   34.72µ ±  1%       ~ (p=0.495 n=20)\nBM_BatchedDot_S32_S32_4x64x64/process_time                                               198.6µ ±  1%   198.2µ ±  0%       ~ (p=0.157 n=20)\nBM_BatchedDot_S32_S32_4x128x128/process_time                                             2.010m ±  3%   2.006m ±  2%       ~ (p=0.862 n=20)\nBM_BatchedDot_S32_S32_4x256x256/process_time                                             23.14m ±  0%   23.08m ±  0%       ~ (p=0.121 n=20)\nBM_BatchedDot_S32_S32_4x512x512/process_time                                             185.7m ±  0%   185.2m ±  1%       ~ (p=0.478 n=20)\nBM_BatchedDot_S32_S32_8x2x2/process_time                                                 6.030µ ±  1%   6.027µ ±  1%       ~ (p=0.478 n=20)\nBM_BatchedDot_S32_S32_8x32x32/process_time                                               62.41µ ±  1%   62.22µ ±  0%       ~ (p=0.201 n=20)\nBM_BatchedDot_S32_S32_8x64x64/process_time                                               393.0µ ±  1%   390.9µ ±  0%       ~ (p=0.056 n=20)\nBM_BatchedDot_S32_S32_8x128x128/process_time                                             4.115m ±  9%   4.001m ±  9%       ~ (p=0.602 n=20)\nBM_BatchedDot_S32_S32_8x256x256/process_time                                             46.10m ±  1%   46.11m ±  2%       ~ (p=0.183 n=20)\nBM_BatchedDot_S32_S32_8x512x512/process_time                                             369.0m ±  1%   368.8m ±  1%       ~ (p=0.718 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_1x11x1152_2x6912x1152_1x11x2x6912/process_time          65.20m ±  3%   64.68m ±  3%       ~ (p=0.221 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_1x11x1152_4x1152x256_1x11x4x256/process_time            2.664m ±  2%   2.712m ±  2%       ~ (p=0.072 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_1x11x4x11_1x11x256_1x11x4x256/process_time              16.38µ ±  1%   16.30µ ±  0%  -0.45% (p=0.043 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_1x11x4x256_1x11x256_1x11x4x11/process_time              17.67µ ±  2%   17.61µ ±  1%       ~ (p=0.072 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_1x11x4x256_4x256x1152_1x11x1152/process_time            2.290m ±  2%   2.310m ±  3%       ~ (p=0.640 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_1x11x6912_6912x1152_1x11x1152/process_time              19.21m ±  2%   19.24m ±  3%       ~ (p=0.968 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_1x1152_1152x262144_1x262144/process_time                 31.95 ±  3%    32.56 ±  3%       ~ (p=0.265 n=20)\nBM_Gemma3_1B_Call/BF16_BF16_BF16_2x1x1152x256_1x11x1152_2x1x256x1x11/process_time        2.541m ±  2%   2.567m ±  2%       ~ (p=0.620 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_1x1x1152_1152x262144_1x1x262144/process_time       32.86 ±  2%    31.63 ±  5%       ~ (p=0.091 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_1x1x1152_2x6912x1152_1x1x2x6912/process_time      5.941m ±  1%   5.956m ±  1%       ~ (p=0.738 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_1x1x1152_4x1152x256_1x1x4x256/process_time        2.525m ±  0%   2.514m ±  0%  -0.45% (p=0.043 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_1x1x4x256_1x4096x256_1x1x4x4096/process_time      3.658m ±  0%   3.632m ±  1%  -0.71% (p=0.014 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_1x1x4x256_4x256x1152_1x1x1152/process_time        1.898m ±  1%   1.892m ±  0%       ~ (p=0.289 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_1x1x4x4096_1x4096x256_1x1x4x256/process_time      1.917m ±  2%   1.936m ±  3%       ~ (p=0.841 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_1x1x6912_6912x1152_1x1x1152/process_time          14.81m ±  1%   14.81m ±  3%       ~ (p=0.904 n=20)\nBM_Gemma3_1B_SampleLoop/BF16_BF16_BF16_2x1x1152x256_1x1x1152_2x1x256x1x1/process_time    1.471m ±  1%   1.467m ±  1%       ~ (p=0.355 n=20)\ngeomean                                                                                 425.8µ         424.9µ        -0.22%\n```\n\n</details>\n\nPiperOrigin-RevId: 832831271",
    "sha": "d4a686f6d2617951ddcaccf40919321516b9e1e2",
    "files": [
        {
            "sha": "926312ddb8e35283dfeb1373e623d30a5c1f6d23",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 21,
            "deletions": 10,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -732,20 +732,32 @@ cc_library(\n )\n \n cc_library(\n-    name = \"dot_thunk\",\n+    name = \"dot_lib\",\n     srcs = [\n-        \"dot_thunk.cc\",\n-        \"dot_thunk_c128.cc\",\n-        \"dot_thunk_c64.cc\",\n-        \"dot_thunk_f16.cc\",\n-        \"dot_thunk_f32.cc\",\n-        \"dot_thunk_f64.cc\",\n-        \"dot_thunk_s32.cc\",\n-        \"dot_thunk_s8.cc\",\n+        \"dot_lib_c128.cc\",\n+        \"dot_lib_c64.cc\",\n+        \"dot_lib_f16.cc\",\n+        \"dot_lib_f32.cc\",\n+        \"dot_lib_f64.cc\",\n+        \"dot_lib_s32.cc\",\n+        \"dot_lib_s8.cc\",\n+    ],\n+    hdrs = [\"dot_lib.h\"],\n+    deps = [\n+        \"//xla/tsl/framework/contraction:eigen_contraction_kernel\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@eigen_archive//:eigen3\",\n     ],\n+)\n+\n+cc_library(\n+    name = \"dot_thunk\",\n+    srcs = [\"dot_thunk.cc\"],\n     hdrs = [\"dot_thunk.h\"],\n     deps = [\n         \":dot_dims\",\n+        \":dot_lib\",\n         \":thunk\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n@@ -754,7 +766,6 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/tsl/concurrency:async_value\",\n-        \"//xla/tsl/framework/contraction:eigen_contraction_kernel\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\","
        },
        {
            "sha": "363e83609f1eab29a9fd7cbcea03868396ed2ae4",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib.h",
            "status": "added",
            "additions": 141,
            "deletions": 0,
            "changes": 141,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib.h?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -0,0 +1,141 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_DOT_LIB_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_DOT_LIB_H_\n+\n+#include <array>\n+#include <cstdint>\n+#include <utility>\n+\n+#include \"absl/base/optimization.h\"\n+#include \"absl/functional/any_invocable.h\"\n+\n+#define EIGEN_USE_THREADS\n+#include \"Eigen/Core\"\n+#include \"unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace xla::cpu::internal {\n+\n+// Done callback is called when the MatMul computation is complete.\n+using DoneCallback = absl::AnyInvocable<void()>;\n+\n+// Col-major x Col-major MatMul implementation as Eigen contraction.\n+template <typename LhsType, typename RhsType, typename OutType,\n+          Eigen::AlignmentType alignment>\n+void MatMul(const Eigen::ThreadPoolDevice* device, OutType* out, LhsType* lhs,\n+            RhsType* rhs, int64_t m, int64_t n, int64_t k,\n+            int32_t transpose_lhs, int32_t transpose_rhs, DoneCallback done);\n+\n+// Col-major x Col-major MatMul implementation as Eigen contraction.\n+template <typename LhsType, typename RhsType, typename OutType>\n+void TypedMatMul(const Eigen::ThreadPoolDevice* device, void* out, void* lhs,\n+                 void* rhs, int64_t m, int64_t n, int64_t k, bool transpose_lhs,\n+                 bool transpose_rhs, DoneCallback done);\n+\n+//===----------------------------------------------------------------------===//\n+// TypedMatMul/MatMul implementation details.\n+//===----------------------------------------------------------------------===//\n+\n+template <typename LhsType, typename RhsType, typename OutType,\n+          Eigen::AlignmentType alignment>\n+void MatMul(const Eigen::ThreadPoolDevice* device, OutType* out, LhsType* lhs,\n+            RhsType* rhs, int64_t m, int64_t n, int64_t k,\n+            int32_t transpose_lhs, int32_t transpose_rhs, DoneCallback done) {\n+  int64_t lhs_rows = m;\n+  int64_t lhs_cols = k;\n+  if (transpose_lhs) {\n+    std::swap(lhs_rows, lhs_cols);\n+  }\n+\n+  int64_t rhs_rows = k;\n+  int64_t rhs_cols = n;\n+  if (transpose_rhs) {\n+    std::swap(rhs_rows, rhs_cols);\n+  }\n+\n+  const Eigen::TensorMap<Eigen::Tensor<const LhsType, 2>, alignment> a(\n+      lhs, lhs_rows, lhs_cols);\n+  const Eigen::TensorMap<Eigen::Tensor<const RhsType, 2>, alignment> b(\n+      rhs, rhs_rows, rhs_cols);\n+  Eigen::TensorMap<Eigen::Tensor<OutType, 2>, alignment> c(out, m, n);\n+\n+  typedef typename Eigen::Tensor<LhsType, 2>::DimensionPair DimPair;\n+  int lhs_contract_dim = transpose_lhs ? 0 : 1;\n+  int rhs_contract_dim = transpose_rhs ? 1 : 0;\n+\n+  std::array<DimPair, 1> dims({DimPair(lhs_contract_dim, rhs_contract_dim)});\n+\n+  if (device != nullptr) {\n+    c.device(*device, std::move(done)) =\n+        a.contract(b, dims).template cast<OutType>();\n+  } else {\n+    c = a.contract(b, dims).template cast<OutType>();\n+    done();\n+  }\n+}\n+\n+template <typename LhsType, typename RhsType, typename OutType>\n+void TypedMatMul(const Eigen::ThreadPoolDevice* device, void* out, void* lhs,\n+                 void* rhs, int64_t m, int64_t n, int64_t k, bool transpose_lhs,\n+                 bool transpose_rhs, DoneCallback done) {\n+  auto is_16_byte_aligned = [](void* ptr) {\n+    return reinterpret_cast<uintptr_t>(ptr) % 16 == 0;\n+  };\n+\n+  bool is_aligned = is_16_byte_aligned(lhs) && is_16_byte_aligned(rhs) &&\n+                    is_16_byte_aligned(out);\n+\n+  if (ABSL_PREDICT_TRUE(is_aligned)) {\n+    MatMul<LhsType, RhsType, OutType, Eigen::Aligned16>(\n+        device, static_cast<OutType*>(out), static_cast<LhsType*>(lhs),\n+        static_cast<RhsType*>(rhs), m, n, k, transpose_lhs, transpose_rhs,\n+        std::move(done));\n+  } else {\n+    MatMul<LhsType, RhsType, OutType, Eigen::Unaligned>(\n+        device, static_cast<OutType*>(out), static_cast<LhsType*>(lhs),\n+        static_cast<RhsType*>(rhs), m, n, k, transpose_lhs, transpose_rhs,\n+        std::move(done));\n+  }\n+}\n+\n+// Declare TypedMatMul template for all supported data types to enable\n+// parallel compilation.\n+#define DECLARE_TYPED_MATMUL(T)                                                \\\n+  extern template void TypedMatMul<T, T, T>(                                   \\\n+      const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,  \\\n+      int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs, \\\n+      DoneCallback done)\n+\n+DECLARE_TYPED_MATMUL(Eigen::half);\n+DECLARE_TYPED_MATMUL(float);\n+DECLARE_TYPED_MATMUL(double);\n+DECLARE_TYPED_MATMUL(int32_t);\n+DECLARE_TYPED_MATMUL(std::complex<float>);\n+DECLARE_TYPED_MATMUL(std::complex<double>);\n+\n+#define DECLARE_MIXED_MATMUL(LhsType, RhsType, OutType)                        \\\n+  extern template void TypedMatMul<LhsType, RhsType, OutType>(                 \\\n+      const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,  \\\n+      int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs, \\\n+      DoneCallback done)\n+\n+DECLARE_MIXED_MATMUL(int8_t, int8_t, int32_t);\n+\n+#undef DECLARE_TYPED_MATMUL\n+\n+}  // namespace xla::cpu::internal\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_DOT_LIB_H_"
        },
        {
            "sha": "158ee7b82e25d7247d3376768a89f7bceafeaace",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib_c128.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_c128.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_c128.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_c128.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -13,9 +13,9 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/dot_thunk.h\"  // NOLINT IWYU pragma: keep\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"  // IWYU pragma: keep\n \n-template void ::xla::cpu::DotThunk::TypedMatMul<\n+template void ::xla::cpu::internal::TypedMatMul<\n     std::complex<double>, std::complex<double>, std::complex<double>>(\n     const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,\n     int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs,",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk_c128.cc"
        },
        {
            "sha": "6daf5ccc8516cd742d913c897dfc141d917b3555",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib_c64.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_c64.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_c64.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_c64.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -13,9 +13,9 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/dot_thunk.h\"  // NOLINT IWYU pragma: keep\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"  // IWYU pragma: keep\n \n-template void ::xla::cpu::DotThunk::TypedMatMul<\n+template void ::xla::cpu::internal::TypedMatMul<\n     std::complex<float>, std::complex<float>, std::complex<float>>(\n     const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,\n     int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs,",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk_c64.cc"
        },
        {
            "sha": "b9e50c11be59050100f05103942caff0a8cb0c9f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib_f16.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f16.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f16.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f16.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -13,9 +13,9 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/dot_thunk.h\"  // NOLINT IWYU pragma: keep\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"  // IWYU pragma: keep\n \n-template void ::xla::cpu::DotThunk::TypedMatMul<Eigen::half, Eigen::half,\n+template void ::xla::cpu::internal::TypedMatMul<Eigen::half, Eigen::half,\n                                                 Eigen::half>(\n     const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,\n     int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs,",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk_f16.cc"
        },
        {
            "sha": "cdc654c41787b467a1f80adb00c6d579a332699c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib_f32.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f32.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f32.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f32.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -13,13 +13,13 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/dot_thunk.h\"  // NOLINT IWYU pragma: keep\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"  // IWYU pragma: keep\n \n #if defined(TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL)\n #include \"xla/tsl/framework/contraction/eigen_contraction_kernel.h\"  // IWYU pragma: keep\n #endif\n \n-template void ::xla::cpu::DotThunk::TypedMatMul<float, float, float>(\n+template void ::xla::cpu::internal::TypedMatMul<float, float, float>(\n     const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,\n     int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs,\n     DoneCallback done);",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk_f32.cc"
        },
        {
            "sha": "4626c002dab8009b960d4ca1f0dbc4545606dc2e",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib_f64.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f64.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f64.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_f64.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -13,9 +13,9 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/dot_thunk.h\"  // NOLINT IWYU pragma: keep\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"  // IWYU pragma: keep\n \n-template void ::xla::cpu::DotThunk::TypedMatMul<double, double, double>(\n+template void ::xla::cpu::internal::TypedMatMul<double, double, double>(\n     const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,\n     int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs,\n     DoneCallback done);",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk_f64.cc"
        },
        {
            "sha": "ca3f145c05f339cb3469e61eb9e19d50093fa889",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib_s32.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_s32.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_s32.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_s32.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -13,9 +13,9 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/dot_thunk.h\"  // NOLINT IWYU pragma: keep\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"  // IWYU pragma: keep\n \n-template void ::xla::cpu::DotThunk::TypedMatMul<int32_t, int32_t, int32_t>(\n+template void ::xla::cpu::internal::TypedMatMul<int32_t, int32_t, int32_t>(\n     const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,\n     int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs,\n     DoneCallback done);",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk_s32.cc"
        },
        {
            "sha": "122bde428d2517e7a4533eb225f1e82a34972c27",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_lib_s8.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_s8.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_s8.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_lib_s8.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -13,13 +13,13 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/dot_thunk.h\"  // NOLINT IWYU pragma: keep\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"  // IWYU pragma: keep\n \n #if defined(TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL)\n #include \"xla/tsl/framework/contraction/eigen_contraction_kernel.h\"  // IWYU pragma: keep\n #endif\n \n-template void ::xla::cpu::DotThunk::TypedMatMul<int8_t, int8_t, int32_t>(\n+template void ::xla::cpu::internal::TypedMatMul<int8_t, int8_t, int32_t>(\n     const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,\n     int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs,\n     DoneCallback done);",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk_s8.cc"
        },
        {
            "sha": "79a68a40dd7c801e76df5b92a3ed1e11d4490954",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.cc?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/cpu/runtime/dot_dims.h\"\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -71,7 +72,6 @@ DotThunk::DotThunk(Info info, DotDimensionNumbers dot_dimensions,\n \n tsl::AsyncValueRef<DotThunk::ExecuteEvent> DotThunk::Execute(\n     const ExecuteParams& params) {\n-\n   TF_ASSIGN_OR_RETURN(\n       se::DeviceMemoryBase lhs_data,\n       params.buffer_allocations->GetDeviceAddress(dot_slices_.lhs_buffer));\n@@ -169,13 +169,17 @@ tsl::AsyncValueRef<DotThunk::ExecuteEvent> DotThunk::Execute(\n \n   auto dispatch = [&](auto lhs_type, auto rhs_type, auto out_type) {\n     for (int64_t i = 0; i < dot_shape_.batch_size; ++i) {\n-      TypedMatMul<decltype(lhs_type), decltype(rhs_type), decltype(out_type)>(\n+      using LhsType = decltype(lhs_type);\n+      using RhsType = decltype(rhs_type);\n+      using OutType = decltype(out_type);\n+      internal::TypedMatMul<LhsType, RhsType, OutType>(\n           params.intra_op_threadpool, batch_ptr(out, out_stride, i),\n           batch_ptr(lhs, lhs_stride, i), batch_ptr(rhs, rhs_stride, i), m, n, k,\n           transpose_lhs, transpose_rhs,\n           [state]() mutable { state.CountDown(); });\n     }\n   };\n+\n   auto dispatch_same_type = [&](auto type_tag) {\n     dispatch(type_tag, type_tag, type_tag);\n   };\n@@ -205,7 +209,7 @@ tsl::AsyncValueRef<DotThunk::ExecuteEvent> DotThunk::Execute(\n         dispatch_same_type(std::complex<double>{});\n         break;\n       default:\n-        absl::string_view type_name = PrimitiveType_Name(lhs_dtype);\n+        auto type_name = primitive_util::LowercasePrimitiveTypeName(lhs_dtype);\n         return Unimplemented(\n             \"Unsupported element type for DotThunk::Execute: %s x %s = %s\",\n             type_name, type_name, type_name);"
        },
        {
            "sha": "2a57a1ed021bf335a4d0da15e5528dbe1f6a2ad4",
            "filename": "third_party/xla/xla/backends/cpu/runtime/dot_thunk.h",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4a686f6d2617951ddcaccf40919321516b9e1e2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fdot_thunk.h?ref=d4a686f6d2617951ddcaccf40919321516b9e1e2",
            "patch": "@@ -16,14 +16,10 @@ limitations under the License.\n #ifndef XLA_BACKENDS_CPU_RUNTIME_DOT_THUNK_H_\n #define XLA_BACKENDS_CPU_RUNTIME_DOT_THUNK_H_\n \n-#include <array>\n #include <cstdint>\n #include <memory>\n-#include <utility>\n \n-#include \"absl/base/optimization.h\"\n #include \"absl/container/inlined_vector.h\"\n-#include \"absl/functional/any_invocable.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/cpu/runtime/dot_dims.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n@@ -32,10 +28,6 @@ limitations under the License.\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/xla_data.pb.h\"\n \n-#define EIGEN_USE_THREADS\n-#include \"Eigen/Core\"\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-\n namespace xla::cpu {\n \n class DotThunk final : public Thunk {\n@@ -57,22 +49,6 @@ class DotThunk final : public Thunk {\n   DotThunk(Info info, DotDimensionNumbers dot_dimensions, DotSlices dot_slices,\n            DotShape dot_shape, DotCanonicalDims dot_canonical_dims);\n \n-  using DoneCallback = absl::AnyInvocable<void()>;\n-\n-  // Col-major x Col-major MatMul implementation as Eigen contraction.\n-  template <typename LhsType, typename RhsType, typename OutType,\n-            Eigen::AlignmentType alignment>\n-  static void MatMul(const Eigen::ThreadPoolDevice* device, OutType* out,\n-                     LhsType* lhs, RhsType* rhs, int64_t m, int64_t n,\n-                     int64_t k, int32_t transpose_lhs, int32_t transpose_rhs,\n-                     DoneCallback done);\n-\n-  template <typename LhsType, typename RhsType, typename OutType>\n-  static void TypedMatMul(const Eigen::ThreadPoolDevice* device, void* out,\n-                          void* lhs, void* rhs, int64_t m, int64_t n, int64_t k,\n-                          bool transpose_lhs, bool transpose_rhs,\n-                          DoneCallback done);\n-\n   DotDimensionNumbers dot_dimensions_;\n   DotSlices dot_slices_;\n   DotShape dot_shape_;\n@@ -83,95 +59,6 @@ class DotThunk final : public Thunk {\n   absl::InlinedVector<int64_t, 2> rhs_matmul_contracting_dims_;\n };\n \n-//===----------------------------------------------------------------------===//\n-// DotThunk implementation details.\n-//===----------------------------------------------------------------------===//\n-\n-template <typename LhsType, typename RhsType, typename OutType,\n-          Eigen::AlignmentType alignment>\n-void DotThunk::MatMul(const Eigen::ThreadPoolDevice* device, OutType* out,\n-                      LhsType* lhs, RhsType* rhs, int64_t m, int64_t n,\n-                      int64_t k, int32_t transpose_lhs, int32_t transpose_rhs,\n-                      DoneCallback done) {\n-  int64_t lhs_rows = m;\n-  int64_t lhs_cols = k;\n-  if (transpose_lhs) std::swap(lhs_rows, lhs_cols);\n-\n-  int64_t rhs_rows = k;\n-  int64_t rhs_cols = n;\n-  if (transpose_rhs) std::swap(rhs_rows, rhs_cols);\n-\n-  const Eigen::TensorMap<Eigen::Tensor<const LhsType, 2>, alignment> a(\n-      lhs, lhs_rows, lhs_cols);\n-  const Eigen::TensorMap<Eigen::Tensor<const RhsType, 2>, alignment> b(\n-      rhs, rhs_rows, rhs_cols);\n-  Eigen::TensorMap<Eigen::Tensor<OutType, 2>, alignment> c(out, m, n);\n-\n-  typedef typename Eigen::Tensor<LhsType, 2>::DimensionPair DimPair;\n-  int lhs_contract_dim = transpose_lhs ? 0 : 1;\n-  int rhs_contract_dim = transpose_rhs ? 1 : 0;\n-  std::array<DimPair, 1> dims({DimPair(lhs_contract_dim, rhs_contract_dim)});\n-\n-  if (device != nullptr) {\n-    c.device(*device, std::move(done)) =\n-        a.contract(b, dims).template cast<OutType>();\n-  } else {\n-    c = a.contract(b, dims).template cast<OutType>();\n-    done();\n-  }\n-}\n-\n-template <typename LhsType, typename RhsType, typename OutType>\n-void DotThunk::TypedMatMul(const Eigen::ThreadPoolDevice* device, void* out,\n-                           void* lhs, void* rhs, int64_t m, int64_t n,\n-                           int64_t k, bool transpose_lhs, bool transpose_rhs,\n-                           DoneCallback done) {\n-  auto is_16_byte_aligned = [](void* ptr) {\n-    return reinterpret_cast<uintptr_t>(ptr) % 16 == 0;\n-  };\n-\n-  bool is_aligned = is_16_byte_aligned(lhs) && is_16_byte_aligned(rhs) &&\n-                    is_16_byte_aligned(out);\n-\n-  if (ABSL_PREDICT_TRUE(is_aligned)) {\n-    MatMul<LhsType, RhsType, OutType, Eigen::Aligned16>(\n-        device, static_cast<OutType*>(out), static_cast<LhsType*>(lhs),\n-        static_cast<RhsType*>(rhs), m, n, k, transpose_lhs, transpose_rhs,\n-        std::move(done));\n-  } else {\n-    MatMul<LhsType, RhsType, OutType, Eigen::Unaligned>(\n-        device, static_cast<OutType*>(out), static_cast<LhsType*>(lhs),\n-        static_cast<RhsType*>(rhs), m, n, k, transpose_lhs, transpose_rhs,\n-        std::move(done));\n-  }\n-}\n-\n-// Extern DotThunk::TypedMatMul template for all supported data types to enable\n-// parallel compilation.\n-#define DOT_THUNK_EXTERN_MATMUL_TEMPLATE(T)                                    \\\n-  extern template void DotThunk::TypedMatMul<T, T, T>(                         \\\n-      const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,  \\\n-      int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs, \\\n-      DoneCallback done)\n-\n-DOT_THUNK_EXTERN_MATMUL_TEMPLATE(Eigen::half);\n-DOT_THUNK_EXTERN_MATMUL_TEMPLATE(float);\n-DOT_THUNK_EXTERN_MATMUL_TEMPLATE(double);\n-DOT_THUNK_EXTERN_MATMUL_TEMPLATE(int32_t);\n-DOT_THUNK_EXTERN_MATMUL_TEMPLATE(std::complex<float>);\n-DOT_THUNK_EXTERN_MATMUL_TEMPLATE(std::complex<double>);\n-\n-#define DOT_THUNK_EXTERN_MATMUL_MIXED_PRECISION_TEMPLATE(LhsType, RhsType,     \\\n-                                                         OutType)              \\\n-  extern template void DotThunk::TypedMatMul<LhsType, RhsType, OutType>(       \\\n-      const Eigen::ThreadPoolDevice* device, void* out, void* lhs, void* rhs,  \\\n-      int64_t m, int64_t n, int64_t k, bool transpose_lhs, bool transpose_rhs, \\\n-      DoneCallback done)\n-\n-DOT_THUNK_EXTERN_MATMUL_MIXED_PRECISION_TEMPLATE(int8_t, int8_t, int32_t);\n-\n-#undef DOT_THUNK_EXTERN_MATMUL_TEMPLATE\n-\n }  // namespace xla::cpu\n \n #endif  // XLA_BACKENDS_CPU_RUNTIME_DOT_THUNK_H_"
        }
    ],
    "stats": {
        "total": 323,
        "additions": 183,
        "deletions": 140
    }
}