{
    "author": "metaflow",
    "message": "[XLA:GPU] remove is_autotuning_compilation flag\n\nThe flag was used all over the place to make various decisions in\ncompiler that is now aware about existence of autotuner. Instead\nintroduce a new debug flag and compile option to control what do we want\ncompiler to do.\n\nAlso removed conditional logging for estimated memory r+w, clarify message that it's an estimation\n\nPiperOrigin-RevId: 838763713",
    "sha": "51355e39ec059059369adff49b69195f26da39e1",
    "files": [
        {
            "sha": "8645a708d485fa8a37365354161fcad26fb86f7a",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/cpu_codegen_backend.h",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_codegen_backend.h?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n@@ -30,10 +29,8 @@ limitations under the License.\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n \n@@ -66,11 +63,9 @@ class CpuCodegenBackend : public CodegenBackend {\n     TF_RETURN_IF_ERROR(ApplyConfig(\n         *hlo_module->entry_computation()->root_instruction(), config));\n \n-    Compiler::CompileOptions options;\n-    options.is_autotuning_compilation = true;\n-\n     return compiler_->RunBackend(std::move(hlo_module),\n-                                 /*executor=*/nullptr, options);\n+                                 /*executor=*/nullptr,\n+                                 /*device_allocator=*/nullptr);\n   }\n \n   bool CanProduceWrongResults() const override { return false; }"
        },
        {
            "sha": "a0b7612bab1e3f71cf15a0d6850dcfbaacad55d8",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -78,7 +78,7 @@ class GpuCodegenBackend : public CodegenBackend {\n \n     Compiler::CompileOptions options;\n     options.gpu_target_config = target_config_;\n-    options.is_autotuning_compilation = true;\n+    options.embed_hlo_module = false;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));\n     return compiler_->RunBackend(std::move(optimized_module), stream_executor_,\n@@ -108,6 +108,7 @@ class GpuCodegenBackend : public CodegenBackend {\n     debug_options.set_xla_embed_ir_in_executable(false);\n     debug_options.set_xla_gpu_kernel_cache_file(\"\");\n     debug_options.set_xla_enable_scoped_logging_timers(false);\n+    debug_options.set_xla_gpu_executable_embed_debug_info(false);\n     // Don't touch the \"fail on register spilling\" flag if it's already on.\n     if (!debug_options.xla_gpu_fail_ptx_compilation_on_register_spilling()) {\n       debug_options.set_xla_gpu_fail_ptx_compilation_on_register_spilling("
        },
        {
            "sha": "94b0d741c01ad532872855cfb6b4134ae86ddb9a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -226,8 +226,7 @@ TEST_F(NativeEmitterBackendTest, CompileSetsIsAutotuningCompilationOption) {\n       mock_compiler,\n       RunBackend(\n           testing::_, testing::_,\n-          testing::Field(&Compiler::CompileOptions::is_autotuning_compilation,\n-                         true)))\n+          testing::Field(&Compiler::CompileOptions::embed_hlo_module, false)))\n       .WillOnce(testing::Return(std::unique_ptr<Executable>()));\n   // Attempt to compile the fusion using the retrieved backend config.\n   EXPECT_THAT(backend.Compile(*fusion, *config), absl_testing::IsOk());"
        },
        {
            "sha": "e2ba3ec553bff8d95fe10d98cef1385e0b5c4590",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -12,7 +12,6 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n-\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n \n #include <cstdint>"
        },
        {
            "sha": "48263e10ca363ff31cfcfd38ae211c552fbd8a0d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_broadcast_thunk_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk_test.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -179,10 +179,7 @@ ENTRY test_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(compiled_module), executor,\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n \n   // Downcast to GPU executable\n   xla::gpu::GpuExecutable* gpu_executable ="
        },
        {
            "sha": "f9e51f21be014b796d5020b92c6757cb0fc00cdd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk_test.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -183,10 +183,7 @@ ENTRY test_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(compiled_module), executor,\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n   // Downcast to GPU executable\n   xla::gpu::GpuExecutable* gpu_executable =\n       tensorflow::down_cast<xla::gpu::GpuExecutable*>(executable.get());"
        },
        {
            "sha": "58141fe07a98b97f2b342665c571f8d9c0bfff17",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -423,6 +423,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_pgle_accuracy_checker(\n       DebugOptions::PGLE_STRICTNESS_LEVEL_WARN);\n \n+  opts.set_xla_gpu_executable_embed_debug_info(true);\n   opts.set_xla_gpu_executable_warn_stuck_timeout_seconds(10);\n   opts.set_xla_gpu_executable_terminate_timeout_seconds(30);\n \n@@ -2385,7 +2386,12 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       \"missing instructions in the profile, then the compilation will halt \"\n       \"(ERROR), or a warning will be emitted (WARN), or the checker is \"\n       \"disabled (OFF)\"));\n-\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_executable_embed_debug_info\",\n+      bool_setter_for(&DebugOptions::set_xla_gpu_executable_embed_debug_info),\n+      debug_options->xla_gpu_executable_embed_debug_info(),\n+      \"Add debug information to the executable such as HLO module, asm_text \"\n+      \"etc.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_executable_warn_stuck_timeout\",\n       int32_setter_for("
        },
        {
            "sha": "94f6dee26a7f1a64ecb155e4fb01c562a0e71d6f",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -185,9 +185,6 @@ class Compiler {\n         const HloModule& module)>\n         layout_canonicalization_callback = {};\n \n-    ABSL_DEPRECATED(\"This field is being deprecated, please do not rely on it.\")\n-    bool is_autotuning_compilation = false;\n-\n     // AOT device description. If provided, used instead of querying the device\n     // on which compilation is performed.\n     std::optional<GpuTargetConfig> gpu_target_config;\n@@ -199,6 +196,9 @@ class Compiler {\n \n     // The number of devices in a fast-interconnect domain.\n     int64_t slice_size = 0;\n+\n+    // Embed HLO module in the executable. Only used on GPU at the moment.\n+    bool embed_hlo_module = true;\n   };\n \n   virtual ~Compiler() = default;"
        },
        {
            "sha": "e12952732bf7af2faa71d49b553f745e76378bef",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -248,6 +248,7 @@ xla_test(\n         \"//xla:error_spec\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/autotuner:gpu_codegen_backend\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n@@ -256,6 +257,7 @@ xla_test(\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:call_inliner\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:dump\",\n         \"//xla/service:executable\",\n         \"//xla/service:hlo_module_config\",\n@@ -288,7 +290,6 @@ xla_test(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//mlir:IR\",\n-        \"@local_tsl//tsl/platform:env\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:platform_port\",\n     ],"
        },
        {
            "sha": "558b21fe05c27018f6bddffcb584f72cc36d3a51",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_compile_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_compile_util.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -126,11 +126,11 @@ absl::StatusOr<std::unique_ptr<Executable>> AutotunerCompileUtil::Compile(\n   if (!new_hlo_module.status().ok()) {\n     return new_hlo_module.status();\n   }\n+  Compiler::CompileOptions compile_options;\n+  compile_options.device_allocator = &allocator_;\n+  compile_options.embed_hlo_module = false;\n   absl::StatusOr<std::unique_ptr<Executable>> out = compiler_->RunBackend(\n-      std::move(*new_hlo_module), &stream_executor_,\n-      Compiler::CompileOptions{&allocator_, /*thread_pool=*/nullptr,\n-                               /*layout_canonicalization_callback=*/{},\n-                               /*is_autotuning_compilation=*/true});\n+      std::move(*new_hlo_module), &stream_executor_, compile_options);\n   if (out.status().code() == absl::StatusCode::kResourceExhausted ||\n       out.status().code() == absl::StatusCode::kCancelled) {\n     // Being out of shared memory budget or registers is an expected failure."
        },
        {
            "sha": "1744f373b713001fc9197a41d24bbdda3aaf1079",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotune_results.pb.h\"\n #include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -51,6 +52,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/call_inliner.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n@@ -655,14 +657,16 @@ ENTRY %e {\n })\";\n \n   auto module = ParseAndReturnVerifiedModule(kHloText).value();\n+  GpuCodegenBackend::AdjustDebugOptionsForAutotuning(\n+      module->mutable_config().mutable_debug_options(),\n+      /*force_allow_register_spills=*/false);\n+  Compiler::CompileOptions options;\n+  options.embed_hlo_module = false;\n   std::unique_ptr<Executable> executable =\n       backend()\n           .compiler()\n           ->RunBackend(std::move(module), backend().default_stream_executor(),\n-                       {/*device_allocator=*/nullptr,\n-                        /*thread_pool=*/nullptr,\n-                        /*layout_canonicalization_callback=*/{},\n-                        /*is_autotuning_compilation=*/true})\n+                       /*device_allocator=*/nullptr)\n           .value();\n   EXPECT_NE(executable, nullptr);\n }"
        },
        {
            "sha": "f5207214c9c186662dfbf061b37f4087a1b18512",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -2537,14 +2537,10 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n         gpu_device_info.memory_bandwidth());\n     GpuHloCostAnalysis cost_analysis(cost_analysis_options, gpu_device_info);\n     TF_RETURN_IF_ERROR(module->entry_computation()->Accept(&cost_analysis));\n-    if (!options.is_autotuning_compilation) {\n-      VLOG(1) << \"HLO memory read+written: \"\n-              << tsl::strings::HumanReadableNumBytes(\n-                     cost_analysis.bytes_accessed());\n-    }\n-    if (module->config().hlo_profiling_enabled()) {\n-      LOG(ERROR) << \"--xla_hlo_profile for GPU is unsupported.\";\n-    }\n+    VLOG(1) << absl::StrFormat(\n+        \"#module=%s,program_id=%d# estimated memory r+w %s\", module->name(),\n+        module->unique_id(),\n+        tsl::strings::HumanReadableNumBytes(cost_analysis.bytes_accessed()));\n   }\n \n   TF_ASSIGN_OR_RETURN(CompileResultWithMetadata res,\n@@ -2560,6 +2556,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   // The module is being moved into the GpuExecutable below and we need to\n   // read a few config values from the module, before it becomes invalid.\n   bool embed_ir_in_executable = debug_opts.xla_embed_ir_in_executable();\n+  bool embed_debug_info = debug_opts.xla_gpu_executable_embed_debug_info();\n \n   tsl::profiler::ScopedAnnotation annotation([&] {\n     return absl::StrFormat(\"XlaCreateGpuExecutable:#module=%s#\",\n@@ -2571,10 +2568,8 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<GpuExecutable> gpu_executable,\n       GpuExecutable::Create(GpuExecutable::Params{\n-          /*asm_text=*/(options.is_autotuning_compilation &&\n-                        !res.backend_result.binary.empty())\n-              ? std::string()\n-              : std::move(res.backend_result.asm_text),\n+          /*asm_text=*/embed_debug_info ? std::move(res.backend_result.asm_text)\n+                                        : std::string(),\n           /*binary=*/std::move(res.backend_result.binary),\n           /*dnn_compiled_graphs=*/\n           std::move(dnn_compiled_graphs),\n@@ -2593,10 +2588,10 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n           /*alias_info=*/std::move(alias_info),\n           /*debug_options=*/debug_opts,\n           /*device_description=*/gpu_device_info,\n-          /*debug_module=*/options.is_autotuning_compilation\n-              ? std::unique_ptr<HloModule>()\n-              : std::move(module),\n-          /*enable_debug_info_manager=*/!options.is_autotuning_compilation}));\n+          /*debug_module=*/options.embed_hlo_module\n+              ? std::move(module)\n+              : std::unique_ptr<HloModule>(),\n+          /*enable_debug_info_manager=*/embed_debug_info}));\n \n   if (embed_ir_in_executable) {\n     std::string ir_module_string_before_opt =\n@@ -2607,7 +2602,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n \n   IncrementCompiledProgramsCount();\n \n-  if (!options.is_autotuning_compilation && gpu_executable->has_module()) {\n+  if (embed_debug_info && gpu_executable->has_module()) {\n     // Dump computation proto state and buffer assignment for\n     // CompiledMemoryAnalysis.\n     auto hlo_proto = std::make_unique<HloProto>();"
        },
        {
            "sha": "eb40243ac48c0565d186f984abc9b2c6aa040875",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 50,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -180,10 +180,7 @@ ENTRY main {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n   EXPECT_EQ(GetCompiledProgramsCount(), before + 1);\n }\n \n@@ -275,10 +272,7 @@ ENTRY main {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n \n   const std::string kGpuCompilerStacktraceMetricName =\n       \"/xla/service/gpu/compiler_stacktrace_count\";\n@@ -310,10 +304,7 @@ ENTRY main {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n   EXPECT_TRUE(XlaDebugInfoManager::Get()->TracksModule(\n       executable->module().unique_id()));\n }\n@@ -329,14 +320,13 @@ ENTRY main {\n )\";\n   auto module = ParseAndReturnVerifiedModule(hlo_text).value();\n   int module_id = module->unique_id();\n+  Compiler::CompileOptions compile_options;\n+  compile_options.embed_hlo_module = false;\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/true}));\n+                                       compile_options));\n   EXPECT_FALSE(XlaDebugInfoManager::Get()->TracksModule(module_id));\n }\n \n@@ -392,10 +382,7 @@ ENTRY e {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n \n   HloModule& compiled_module = executable->module();\n   const HloInstruction* entry_root =\n@@ -997,10 +984,7 @@ TEST_P(AotCompilationTest, ExportAndImportAotResult) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<Executable> executable,\n       compiler_->RunBackend(std::move(add_1_hlo), stream_exec_,\n-                            {/*device_allocator=*/nullptr,\n-                             /*thread_pool=*/nullptr,\n-                             /*layout_canonicalization_callback=*/{},\n-                             /*is_autotuning_compilation=*/false}));\n+                            /*device_allocator=*/nullptr));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<AotCompilationResult> aot_result,\n                           compiler_->Export(executable.get()));\n \n@@ -1367,12 +1351,9 @@ ENTRY entry {\n               .value();\n       TF_ASSERT_OK_AND_ASSIGN(\n           std::unique_ptr<Executable> executable,\n-          backend().compiler()->RunBackend(\n-              std::move(compiled_module), backend().default_stream_executor(),\n-              {/*device_allocator=*/nullptr,\n-               /*thread_pool=*/nullptr,\n-               /*layout_canonicalization_callback=*/{},\n-               /*is_autotuning_compilation=*/false}));\n+          backend().compiler()->RunBackend(std::move(compiled_module),\n+                                           backend().default_stream_executor(),\n+                                           /*device_allocator=*/nullptr));\n     });\n   }\n }\n@@ -1413,10 +1394,7 @@ ENTRY main {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n   std::unique_ptr<GpuExecutable> gpu_exec(\n       static_cast<GpuExecutable*>(executable.release()));\n \n@@ -1476,10 +1454,7 @@ ENTRY main {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n   std::unique_ptr<GpuExecutable> gpu_exec(\n       static_cast<GpuExecutable*>(executable.release()));\n \n@@ -2132,10 +2107,7 @@ TEST_F(GpuCompilerTest, CompilingAndCollectingMetadata) {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(opt_module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n \n   auto& exe_module = executable->module();\n   const HloModuleMetadataProto& exe_metadata = exe_module.metadata()->proto();\n@@ -2176,10 +2148,7 @@ ENTRY main {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(hlo_module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n   std::unique_ptr<GpuExecutable> gpu_exec(\n       static_cast<GpuExecutable*>(executable.release()));\n   const ThunkSequence& thunks = gpu_exec->GetThunk().thunks();\n@@ -2302,10 +2271,7 @@ ENTRY main {\n       std::unique_ptr<Executable> executable,\n       backend().compiler()->RunBackend(std::move(compiled_module),\n                                        backend().default_stream_executor(),\n-                                       {/*device_allocator=*/nullptr,\n-                                        /*thread_pool=*/nullptr,\n-                                        /*layout_canonicalization_callback=*/{},\n-                                        /*is_autotuning_compilation=*/false}));\n+                                       /*device_allocator=*/nullptr));\n \n   // Downcast to GPU executable\n   xla::gpu::GpuExecutable* gpu_executable ="
        },
        {
            "sha": "0e0f09561e6aa4510530b37e49889eec7b35e7df",
            "filename": "third_party/xla/xla/service/gpu/ptx_compilation_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -251,10 +251,7 @@ class NVPTXCompilationTests\n \n     return compiler.RunBackend(std::move(module),\n                                backend().default_stream_executor(),\n-                               {/*device_allocator=*/nullptr,\n-                                /*thread_pool=*/nullptr,\n-                                /*layout_canonicalization_callback=*/{},\n-                                /*is_autotuning_compilation=*/false});\n+                               /*options=*/{});\n   }\n };\n "
        },
        {
            "sha": "6278e04737e8c4f0201bea705597d3180cd40fd5",
            "filename": "third_party/xla/xla/service/local_service.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -95,9 +95,9 @@ LocalService::CompileExecutables(\n       build_options.device_allocator(),\n       build_options.compile_thread_pool(),\n       build_options.layout_canonicalization_callback(),\n-      false,\n       /*gpu_target_config=*/{},\n       /*cpu_target_config=*/{},\n+      /*key_value_store=*/\n       {build_options.key_value_store(), build_options.process_index(),\n        build_options.process_count()},\n       build_options.slice_size()};"
        },
        {
            "sha": "946ee72f6e22c8d7a7dc0165f11125d8433b75a8",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51355e39ec059059369adff49b69195f26da39e1/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=51355e39ec059059369adff49b69195f26da39e1",
            "patch": "@@ -588,6 +588,8 @@ message DebugOptions {\n   // a deterministic implementation.\n   optional bool xla_gpu_exclude_nondeterministic_ops = 297;\n \n+  optional bool xla_gpu_executable_embed_debug_info = 437;\n+\n   // Timeout to terminate on stuck rendez-vous.\n   optional int32 xla_gpu_executable_terminate_timeout_seconds = 328;\n \n@@ -1324,7 +1326,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 437\n+  // Next id: 438\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 169,
        "additions": 64,
        "deletions": 105
    }
}