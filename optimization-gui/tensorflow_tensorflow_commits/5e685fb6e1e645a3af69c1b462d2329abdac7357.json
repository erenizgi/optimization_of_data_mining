{
    "author": "tensorflower-gardener",
    "message": "Apply llvm-use-new-mlir-op-builder fixes\n\nThis migrates `builder.create<Op>()` => `Op::create()`\n\nPiperOrigin-RevId: 846854812",
    "sha": "5e685fb6e1e645a3af69c1b462d2329abdac7357",
    "files": [
        {
            "sha": "a9337c0c84f94427b126bdbe829158256f86c578",
            "filename": "tensorflow/compiler/mlir/lite/debug/debug_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fdebug%2Fdebug_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fdebug%2Fdebug_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fdebug%2Fdebug_test.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -103,20 +103,21 @@ class InitPassManagerTest : public testing::Test {\n     context_.loadAllAvailableDialects();\n \n     mlir::OpBuilder builder(&context_);\n-    module_ = builder.create<mlir::ModuleOp>(builder.getUnknownLoc());\n+    module_ = mlir::ModuleOp::create(builder, builder.getUnknownLoc());\n \n     builder.setInsertionPointToStart(module_->getBody());\n-    auto func = builder.create<mlir::func::FuncOp>(  //\n-        builder.getUnknownLoc(), \"main\", builder.getFunctionType({}, {}));\n+    auto func = mlir::func::FuncOp::create(builder,  //\n+                                           builder.getUnknownLoc(), \"main\",\n+                                           builder.getFunctionType({}, {}));\n     func->setAttr(\"tfl.func\", builder.getUnitAttr());\n     builder.setInsertionPointToStart(func.addEntryBlock());\n     llvm::SmallVector<int> shape{1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n-    builder.create<mlir::arith::ConstantOp>(\n-        builder.getUnknownLoc(),\n+    mlir::arith::ConstantOp::create(\n+        builder, builder.getUnknownLoc(),\n         mlir::DenseIntElementsAttr::get(\n             mlir::RankedTensorType::get(shape.size(), builder.getI32Type()),\n             shape));\n-    builder.create<mlir::func::ReturnOp>(builder.getUnknownLoc());\n+    mlir::func::ReturnOp::create(builder, builder.getUnknownLoc());\n   }\n \n   absl::Status GetDumpDir(std::string* dump_dir) {"
        },
        {
            "sha": "614f97383560193079bfd5308b9af283a063371e",
            "filename": "tensorflow/compiler/mlir/lite/experimental/common/outline_operations.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Fcommon%2Foutline_operations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Fcommon%2Foutline_operations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Fcommon%2Foutline_operations.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -163,8 +163,8 @@ func::FuncOp BuildFuncOp(const Subgraph& subgraph, OpBuilder& builder,\n     Value cloned_output = values_in_scope.lookup(result);\n     return_operands.push_back(cloned_output);\n   }\n-  function_builder.create<mlir::func::ReturnOp>(new_func.getLoc(),\n-                                                return_operands);\n+  mlir::func::ReturnOp::create(function_builder, new_func.getLoc(),\n+                               return_operands);\n   ops_added.func_op = new_func;\n   module.push_back(new_func);\n   return new_func;\n@@ -179,8 +179,8 @@ void ExtractSubgraphToFunc(const Subgraph& subgraph, OpBuilder& builder,\n   Operation* last_output = subgraph.partition_ops_.back();\n \n   builder.setInsertionPoint(last_output);\n-  auto call_op = builder.create<func::CallOp>(last_output->getLoc(), func,\n-                                              subgraph.FuncArguments());\n+  auto call_op = func::CallOp::create(builder, last_output->getLoc(), func,\n+                                      subgraph.FuncArguments());\n   ops_added.call_op = call_op;\n   // FuncOutputs refer to the original `Values` in input module which are now\n   // invalid after pulling out the defining ops. The values in"
        },
        {
            "sha": "c5c8c040c2bb283527ce20907473e316b33fa273",
            "filename": "tensorflow/compiler/mlir/lite/experimental/tac/transforms/device_transform.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fdevice_transform.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fdevice_transform.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fdevice_transform.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -89,8 +89,8 @@ void ConvertQuantizedOpToFloat(mlir::func::FuncOp func, OpBuilder* builder) {\n         auto dequantized_input_type =\n             mlir::quant::QuantizedType::castToExpressedType(input_type);\n         builder->setInsertionPoint(op);\n-        auto dequantize_op = builder->create<TFL::DequantizeOp>(\n-            op->getLoc(), dequantized_input_type, input.get());\n+        auto dequantize_op = TFL::DequantizeOp::create(\n+            *builder, op->getLoc(), dequantized_input_type, input.get());\n         dequantized_inputs.push_back(dequantize_op);\n       } else {\n         dequantized_inputs.push_back(input.get());\n@@ -126,8 +126,9 @@ void ConvertQuantizedOpToFloat(mlir::func::FuncOp func, OpBuilder* builder) {\n       Value new_result = new_op->getResult(i);\n       if (IsQI8Type(result_type) || IsQUI8Type(result_type)) {\n         builder->setInsertionPoint(op);\n-        TFL::QuantizeOp quant_op = builder->create<TFL::QuantizeOp>(\n-            op->getLoc(), result_type, new_result, TypeAttr::get(result_type));\n+        TFL::QuantizeOp quant_op =\n+            TFL::QuantizeOp::create(*builder, op->getLoc(), result_type,\n+                                    new_result, TypeAttr::get(result_type));\n         new_result = quant_op.getResult();\n       }\n "
        },
        {
            "sha": "e6d7c6425abafe082f8e7e1579632f7f966c8e79",
            "filename": "tensorflow/compiler/mlir/lite/experimental/tac/transforms/device_transform_patterns.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fdevice_transform_patterns.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fdevice_transform_patterns.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fdevice_transform_patterns.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -85,11 +85,11 @@ TFL::ReshapeOp InsertReshapeOp(Location loc, Value input, Type element_type,\n   auto new_shape_attr =\n       mlir::DenseIntElementsAttr::get(reshape_shape_type, new_shape_array_i32);\n \n-  auto new_shape = builder->create<TFL::ConstOp>(loc, new_shape_attr);\n+  auto new_shape = TFL::ConstOp::create(*builder, loc, new_shape_attr);\n \n   auto reshape_out_type = RankedTensorType::get(new_shape_array, element_type);\n-  return builder->create<TFL::ReshapeOp>(loc, reshape_out_type, input,\n-                                         new_shape);\n+  return TFL::ReshapeOp::create(*builder, loc, reshape_out_type, input,\n+                                new_shape);\n }\n \n LogicalResult EnsureBias(Operation* op, int bias_idx,\n@@ -148,7 +148,7 @@ TF::ConstOp PadConstValues(Operation* input_op, int value_to_pad,\n   auto new_value_i32_attr =\n       mlir::DenseIntElementsAttr::get(value_shape_type, value_i32);\n \n-  return builder->create<TF::ConstOp>(loc, new_value_i32_attr);\n+  return TF::ConstOp::create(*builder, loc, new_value_i32_attr);\n }\n \n SmallVector<Value, 4> SliceOutputs(Operation* split_op, Value input,\n@@ -186,13 +186,13 @@ SmallVector<Value, 4> SliceOutputs(Operation* split_op, Value input,\n         mlir::DenseIntElementsAttr::get(slice_type, slice_size);\n \n     auto slice_begin_const =\n-        rewriter->create<TFL::ConstOp>(split_op->getLoc(), slice_begin_attr);\n+        TFL::ConstOp::create(*rewriter, split_op->getLoc(), slice_begin_attr);\n     auto slice_size_const =\n-        rewriter->create<TFL::ConstOp>(split_op->getLoc(), slice_size_attr);\n+        TFL::ConstOp::create(*rewriter, split_op->getLoc(), slice_size_attr);\n \n-    auto slice_op = rewriter->create<TFL::SliceOp>(\n-        split_op->getLoc(), current_output_type, input, slice_begin_const,\n-        slice_size_const);\n+    auto slice_op =\n+        TFL::SliceOp::create(*rewriter, split_op->getLoc(), current_output_type,\n+                             input, slice_begin_const, slice_size_const);\n \n     // Rewire output.\n     slice_outputs.push_back(slice_op.getResult());"
        },
        {
            "sha": "300daee0f9a40d159671952b92ca729281612824",
            "filename": "tensorflow/compiler/mlir/lite/experimental/tac/transforms/pick_subgraphs.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fpick_subgraphs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fpick_subgraphs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fpick_subgraphs.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -430,8 +430,8 @@ void PickSubgraphsPass::RewireSubgraphs(\n         if (call.getCallee() != impl.getName()) {\n           // We need to rebuild the call op. :(\n           builder->setInsertionPoint(call);\n-          auto new_call = builder->create<func::CallOp>(call.getLoc(), impl,\n-                                                        call.getOperands());\n+          auto new_call = func::CallOp::create(*builder, call.getLoc(), impl,\n+                                               call.getOperands());\n \n           // Set interface_name & target to the call_op as well.\n           new_call->setAttr(kInterfaceNameAttr,"
        },
        {
            "sha": "1da38c2c9f466ef4f5bde75fa03db59eeeb57b61",
            "filename": "tensorflow/compiler/mlir/lite/quantization/common/quantization_lib/quantization_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fquantization%2Fcommon%2Fquantization_lib%2Fquantization_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fquantization%2Fcommon%2Fquantization_lib%2Fquantization_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fquantization%2Fcommon%2Fquantization_lib%2Fquantization_utils.h?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -346,10 +346,10 @@ void CreateVerifier(mlir::Operation* quantizing_op,\n   BoolAttr log =\n       rewriter.getBoolAttr(quant_params.numeric_verify_spec.log_if_failed_flag);\n   // Verify the quantized value by sending the result to the verifier.\n-  rewriter.create<VerifierT>(\n-      quantizing_op->getLoc(), quantized_op->getResult(result_idx).getType(),\n-      quantized_op->getResult(result_idx), quantizing_op->getResult(result_idx),\n-      tolerance, log);\n+  VerifierT::create(rewriter, quantizing_op->getLoc(),\n+                    quantized_op->getResult(result_idx).getType(),\n+                    quantized_op->getResult(result_idx),\n+                    quantizing_op->getResult(result_idx), tolerance, log);\n }\n \n template <>"
        },
        {
            "sha": "529b5d2161be32d62be705ec7bf54b75d9e47be5",
            "filename": "tensorflow/compiler/mlir/lite/quantization/tensorflow/tf_to_quant.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fquantization%2Ftensorflow%2Ftf_to_quant.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fquantization%2Ftensorflow%2Ftf_to_quant.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fquantization%2Ftensorflow%2Ftf_to_quant.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -150,10 +150,10 @@ struct InsertQuantOpsAfterTFFakeQuantOp\n     // dequantize ops, and insert them between the tf.FakeQuantWithMinMaxVarsOp\n     // and its users.\n     Value value = tf_op.getOutputs();\n-    auto quantize = rewriter.create<quantfork::QuantizeCastOp>(\n-        tf_op.getLoc(), qtype.getValue(), value);\n-    auto dequantize = rewriter.create<quantfork::DequantizeCastOp>(\n-        tf_op.getLoc(), res_type, quantize.getResult());\n+    auto quantize = quantfork::QuantizeCastOp::create(rewriter, tf_op.getLoc(),\n+                                                      qtype.getValue(), value);\n+    auto dequantize = quantfork::DequantizeCastOp::create(\n+        rewriter, tf_op.getLoc(), res_type, quantize.getResult());\n     value.replaceAllUsesWith(dequantize);\n     quantize.getOperation()->replaceUsesOfWith(dequantize, value);\n "
        },
        {
            "sha": "0dd7e1f3b97a1ca487fd26980eea68dfdbd989c5",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/compose_uniform_quantized_type_pass.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 52,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fcompose_uniform_quantized_type_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fcompose_uniform_quantized_type_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fcompose_uniform_quantized_type_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -677,13 +677,12 @@ class ComposeUniformQuantizedConvolutionOp\n         CreateI8F32UniformQuantizedType(\n             uniform_quantize_call_op.getLoc(), *rewriter.getContext(),\n             input_scale_value, input_zero_point_value);\n-    auto input_uniform_quantize_op =\n-        rewriter.create<stablehlo::UniformQuantizeOp>(\n-            uniform_quantize_call_op.getLoc(),\n-            /*result=*/\n-            mlir::cast<TensorType>(input_value.getType())\n-                .clone(input_quantized_element_type),\n-            /*operand=*/input_value);\n+    auto input_uniform_quantize_op = stablehlo::UniformQuantizeOp::create(\n+        rewriter, uniform_quantize_call_op.getLoc(),\n+        /*result=*/\n+        mlir::cast<TensorType>(input_value.getType())\n+            .clone(input_quantized_element_type),\n+        /*operand=*/input_value);\n \n     rewriter.replaceAllUsesWith(input_i8_to_f32_convert_op.getResult(),\n                                 input_uniform_quantize_op.getResult());\n@@ -754,8 +753,8 @@ class ComposeUniformQuantizedConvolutionOp\n             /*quantization_dimension=*/3);\n \n     // Create a new constant op for the filter in i8.\n-    auto quantized_filter_constant_op = rewriter.create<stablehlo::ConstantOp>(\n-        filter_op->getLoc(),\n+    auto quantized_filter_constant_op = stablehlo::ConstantOp::create(\n+        rewriter, filter_op->getLoc(),\n         /*output=*/\n         filter_i8_value_attr.getType().clone(filter_quantized_element_type),\n         /*value=*/filter_i8_value_attr);\n@@ -797,18 +796,16 @@ class ComposeUniformQuantizedConvolutionOp\n \n     SmallVector<Type> new_conv_output_types = {\n         output_uniform_quantized_tensor_type};\n-    auto new_conv_op_with_output_type =\n-        rewriter.create<stablehlo::ConvolutionOp>(\n-            op.getLoc(), new_conv_output_types, op.getOperands(),\n-            op->getAttrs());\n+    auto new_conv_op_with_output_type = stablehlo::ConvolutionOp::create(\n+        rewriter, op.getLoc(), new_conv_output_types, op.getOperands(),\n+        op->getAttrs());\n \n     rewriter.replaceAllUsesWith(op.getResult(),\n                                 new_conv_op_with_output_type.getResult());\n \n-    auto new_output_dequant_op =\n-        rewriter.create<stablehlo::UniformDequantizeOp>(\n-            rewriter.getUnknownLoc(),\n-            /*operand=*/new_conv_op_with_output_type);\n+    auto new_output_dequant_op = stablehlo::UniformDequantizeOp::create(\n+        rewriter, rewriter.getUnknownLoc(),\n+        /*operand=*/new_conv_op_with_output_type);\n \n     auto output_uniform_dequantize_call_op = cast<func::CallOp>(\n         *output_uniform_quantize_call_op.getResult(0).user_begin());\n@@ -1035,13 +1032,12 @@ class ComposeUniformQuantizedDotGeneralOp\n             input_scale_value, input_zero_point_value);\n \n     Value input_value = input_uniform_quantize_call_pattern->GetInputValue();\n-    auto input_uniform_quantize_op =\n-        rewriter.create<stablehlo::UniformQuantizeOp>(\n-            input_i8_to_f32_convert_op.getLoc(),\n-            /*result=*/\n-            mlir::cast<TensorType>(input_value.getType())\n-                .clone(input_uniform_quantized_type),\n-            /*operand=*/input_value);\n+    auto input_uniform_quantize_op = stablehlo::UniformQuantizeOp::create(\n+        rewriter, input_i8_to_f32_convert_op.getLoc(),\n+        /*result=*/\n+        mlir::cast<TensorType>(input_value.getType())\n+            .clone(input_uniform_quantized_type),\n+        /*operand=*/input_value);\n \n     rewriter.replaceAllUsesWith(input_i8_to_f32_convert_op.getResult(),\n                                 input_uniform_quantize_op.getResult());\n@@ -1116,8 +1112,8 @@ class ComposeUniformQuantizedDotGeneralOp\n             quantization_dimension);\n \n     // Create a new constant op for the filter in i8.\n-    auto quantized_filter_constant_op = rewriter.create<stablehlo::ConstantOp>(\n-        filter_constant_op.getLoc(),\n+    auto quantized_filter_constant_op = stablehlo::ConstantOp::create(\n+        rewriter, filter_constant_op.getLoc(),\n         /*output=*/\n         mlir::cast<TensorType>(filter_constant_op.getResult().getType())\n             .clone(filter_uniform_quantized_type),\n@@ -1157,8 +1153,8 @@ class ComposeUniformQuantizedDotGeneralOp\n             output_uniform_quantize_call_op.getLoc(), *rewriter.getContext(),\n             output_scale_value, output_zero_point_value);\n \n-    auto new_dot_general_op = rewriter.create<stablehlo::DotGeneralOp>(\n-        op.getLoc(), /*resultType0=*/\n+    auto new_dot_general_op = stablehlo::DotGeneralOp::create(\n+        rewriter, op.getLoc(), /*resultType0=*/\n         mlir::cast<TensorType>(op.getResult().getType())\n             .clone(output_uniform_quantized_type),\n         /*lhs=*/op.getLhs(), /*rhs=*/op.getRhs(),\n@@ -1168,10 +1164,9 @@ class ComposeUniformQuantizedDotGeneralOp\n \n     rewriter.replaceAllUsesWith(op.getResult(), new_dot_general_op.getResult());\n \n-    auto new_output_dequant_op =\n-        rewriter.create<stablehlo::UniformDequantizeOp>(\n-            output_uniform_dequantize_call_op.getLoc(),\n-            /*operand=*/new_dot_general_op);\n+    auto new_output_dequant_op = stablehlo::UniformDequantizeOp::create(\n+        rewriter, output_uniform_dequantize_call_op.getLoc(),\n+        /*operand=*/new_dot_general_op);\n \n     rewriter.replaceAllUsesWith(output_uniform_dequantize_call_op.getResult(0),\n                                 new_output_dequant_op.getResult());\n@@ -1423,13 +1418,12 @@ class ComposeUniformQuantizedDotGeneralOpWithTwoQuantizedActivations\n             input1_scale_value, input1_zero_point_value);\n \n     Value input1_value = input1_uniform_quantize_call_pattern->GetInputValue();\n-    auto input1_uniform_quantize_op =\n-        rewriter.create<stablehlo::UniformQuantizeOp>(\n-            input1_uniform_quantize_call_op.getLoc(),\n-            /*result=*/\n-            mlir::cast<TensorType>(input1_value.getType())\n-                .clone(input1_uniform_quantized_type),\n-            /*operand=*/input1_value);\n+    auto input1_uniform_quantize_op = stablehlo::UniformQuantizeOp::create(\n+        rewriter, input1_uniform_quantize_call_op.getLoc(),\n+        /*result=*/\n+        mlir::cast<TensorType>(input1_value.getType())\n+            .clone(input1_uniform_quantized_type),\n+        /*operand=*/input1_value);\n \n     rewriter.replaceAllUsesWith(input1_zero_point_subtract_op.getResult(),\n                                 input1_uniform_quantize_op.getResult());\n@@ -1462,13 +1456,12 @@ class ComposeUniformQuantizedDotGeneralOpWithTwoQuantizedActivations\n             input2_scale_value, input2_zero_point_value);\n \n     Value input2_value = input2_uniform_quantize_call_pattern->GetInputValue();\n-    auto input2_uniform_quantize_op =\n-        rewriter.create<stablehlo::UniformQuantizeOp>(\n-            input2_uniform_quantize_call_op.getLoc(),\n-            /*result=*/\n-            mlir::cast<TensorType>(input2_value.getType())\n-                .clone(input2_uniform_quantized_type),\n-            /*operand=*/input2_value);\n+    auto input2_uniform_quantize_op = stablehlo::UniformQuantizeOp::create(\n+        rewriter, input2_uniform_quantize_call_op.getLoc(),\n+        /*result=*/\n+        mlir::cast<TensorType>(input2_value.getType())\n+            .clone(input2_uniform_quantized_type),\n+        /*operand=*/input2_value);\n \n     rewriter.replaceAllUsesWith(input2_zero_point_subtract_op.getResult(),\n                                 input2_uniform_quantize_op.getResult());\n@@ -1512,8 +1505,8 @@ class ComposeUniformQuantizedDotGeneralOpWithTwoQuantizedActivations\n             output_uniform_quantize_call_op.getLoc(), *rewriter.getContext(),\n             output_scale_value, output_zero_point_value);\n \n-    auto new_dot_general_op = rewriter.create<stablehlo::DotGeneralOp>(\n-        op.getLoc(), /*resultType0=*/\n+    auto new_dot_general_op = stablehlo::DotGeneralOp::create(\n+        rewriter, op.getLoc(), /*resultType0=*/\n         mlir::cast<TensorType>(op.getResult().getType())\n             .clone(output_uniform_quantized_type),\n         /*lhs=*/op.getLhs(), /*rhs=*/op.getRhs(),\n@@ -1523,10 +1516,9 @@ class ComposeUniformQuantizedDotGeneralOpWithTwoQuantizedActivations\n \n     rewriter.replaceAllUsesWith(op.getResult(), new_dot_general_op.getResult());\n \n-    auto new_output_dequant_op =\n-        rewriter.create<stablehlo::UniformDequantizeOp>(\n-            output_uniform_dequantize_call_op.getLoc(),\n-            /*operand=*/new_dot_general_op);\n+    auto new_output_dequant_op = stablehlo::UniformDequantizeOp::create(\n+        rewriter, output_uniform_dequantize_call_op.getLoc(),\n+        /*operand=*/new_dot_general_op);\n \n     rewriter.replaceAllUsesWith(output_uniform_dequantize_call_op.getResult(0),\n                                 new_output_dequant_op.getResult());"
        },
        {
            "sha": "0d8688b2c8855a1c61682f5cc7b7252350ded8a2",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo.cc",
            "status": "modified",
            "additions": 268,
            "deletions": 252,
            "changes": 520,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -328,22 +328,22 @@ class ConvertNdConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n       size.push_back(input_shape[i] - pre_slice - post_slice);\n     }\n \n-    auto start_attr = rewriter.create<TF::ConstOp>(\n-        value.getLoc(),\n+    auto start_attr = TF::ConstOp::create(\n+        rewriter, value.getLoc(),\n         DenseIntElementsAttr::get(\n             RankedTensorType::get({static_cast<int64_t>(start.size())},\n                                   rewriter.getI64Type()),\n             start));\n-    auto size_attr = rewriter.create<TF::ConstOp>(\n-        value.getLoc(),\n+    auto size_attr = TF::ConstOp::create(\n+        rewriter, value.getLoc(),\n         DenseIntElementsAttr::get(\n             RankedTensorType::get({static_cast<int64_t>(size.size())},\n                                   rewriter.getI64Type()),\n             size));\n     auto output_type = RankedTensorType::get(size, input_type.getElementType());\n \n-    return rewriter.create<TF::SliceOp>(value.getLoc(), output_type, value,\n-                                        start_attr, size_attr);\n+    return TF::SliceOp::create(rewriter, value.getLoc(), output_type, value,\n+                               start_attr, size_attr);\n   }\n \n   void CreateConvOp(mhlo::ConvolutionOp conv_op, ArrayRef<int64_t> strides,\n@@ -381,14 +381,15 @@ class ConvertNdConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n           mlir::dyn_cast<RankedTensorType>(conv_op.getLhs().getType());\n       RankedTensorType padding_attr_type = mlir::RankedTensorType::get(\n           {lhs_type.getRank(), 2}, rewriter.getIntegerType(64));\n-      auto padding_const = rewriter.create<TF::ConstOp>(\n-          conv_op->getLoc(),\n+      auto padding_const = TF::ConstOp::create(\n+          rewriter, conv_op->getLoc(),\n           mlir::DenseElementsAttr::get(padding_attr_type,\n                                        ArrayRef<int64_t>(new_padding)));\n       // Add Pad op.\n       auto pad_output_type = UnrankedTensorType::get(lhs_type.getElementType());\n-      sliced_lhs = rewriter.create<TF::PadOp>(\n-          conv_op->getLoc(), pad_output_type, sliced_lhs, padding_const);\n+      sliced_lhs =\n+          TF::PadOp::create(rewriter, conv_op->getLoc(), pad_output_type,\n+                            sliced_lhs, padding_const);\n       padding = \"VALID\";\n     }\n \n@@ -422,28 +423,28 @@ class ConvertNdConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n                                                     hlo_filter_shape.end());\n       tf_filter_shape[2] = input_channels;\n       tf_filter_shape[3] = hlo_filter_shape.back() / input_channels;\n-      auto reshaped_filter = rewriter.create<mhlo::ReshapeOp>(\n-          rhs.getLoc(),\n+      auto reshaped_filter = mhlo::ReshapeOp::create(\n+          rewriter, rhs.getLoc(),\n           RankedTensorType::get(tf_filter_shape, filter_type.getElementType()),\n           rhs);\n \n-      output = rewriter.create<TF::DepthwiseConv2dNativeOp>(\n-          conv_op.getLoc(), conv_output_type, sliced_lhs, reshaped_filter,\n-          rewriter.getI64ArrayAttr(strides),\n+      output = TF::DepthwiseConv2dNativeOp::create(\n+          rewriter, conv_op.getLoc(), conv_output_type, sliced_lhs,\n+          reshaped_filter, rewriter.getI64ArrayAttr(strides),\n           /*padding=*/rewriter.getStringAttr(padding),\n           /*explicit_paddings=*/rewriter.getI64ArrayAttr(new_padding),\n           /*data_format=*/rewriter.getStringAttr(\"NHWC\"),\n           /*dilations=*/rewriter.getI64ArrayAttr(dilation));\n     } else if (num_spatial_dims == 3) {\n-      output = rewriter.create<TF::Conv3DOp>(\n-          conv_op.getLoc(), conv_output_type, sliced_lhs, rhs,\n+      output = TF::Conv3DOp::create(\n+          rewriter, conv_op.getLoc(), conv_output_type, sliced_lhs, rhs,\n           rewriter.getI64ArrayAttr(strides),\n           /*padding=*/rewriter.getStringAttr(padding),\n           /*data_format=*/rewriter.getStringAttr(\"NDHWC\"),\n           /*dilations=*/rewriter.getI64ArrayAttr(dilation));\n     } else {\n-      output = rewriter.create<TF::Conv2DOp>(\n-          conv_op.getLoc(), conv_output_type, sliced_lhs, rhs,\n+      output = TF::Conv2DOp::create(\n+          rewriter, conv_op.getLoc(), conv_output_type, sliced_lhs, rhs,\n           rewriter.getI64ArrayAttr(strides),\n           /*use_cudnn_on_gpu=*/rewriter.getBoolAttr(true),\n           /*padding=*/rewriter.getStringAttr(padding),\n@@ -462,8 +463,8 @@ class ConvertNdConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n               dnums.getOutputFeatureDimension(),\n               *dnums.getOutputSpatialDimensions().begin(), num_spatial_dims,\n               conv_output_type, rewriter);\n-      output = rewriter.create<mhlo::TransposeOp>(\n-          conv_op.getLoc(), conv_op.getType(), output, permutation);\n+      output = mhlo::TransposeOp::create(\n+          rewriter, conv_op.getLoc(), conv_op.getType(), output, permutation);\n     }\n     rewriter.replaceOp(conv_op, {output});\n   }\n@@ -513,8 +514,8 @@ class Convert1DConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n     auto image_2d_type =\n         RankedTensorType::get(image_2d_shape, image_type.getElementType());\n     auto loc = conv_op.getLoc();\n-    auto image_2d_op = rewriter.create<mhlo::ReshapeOp>(\n-        conv_op.getLoc(), image_2d_type, conv_op.getLhs());\n+    auto image_2d_op = mhlo::ReshapeOp::create(rewriter, conv_op.getLoc(),\n+                                               image_2d_type, conv_op.getLhs());\n \n     // Transpose image to get it into NWHC form (where H is the added dim).\n     SmallVector<int64_t, 4> image_permutation = {\n@@ -523,9 +524,9 @@ class Convert1DConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n         dnums.getInputFeatureDimension()};\n     auto image_permutation_and_shape = GetPermutationAndTransposedShape(\n         image_permutation, image_2d_type, rewriter);\n-    auto transposed_image_2d_op = rewriter.create<mhlo::TransposeOp>(\n-        loc, image_permutation_and_shape.shape, image_2d_op->getResult(0),\n-        image_permutation_and_shape.permutation);\n+    auto transposed_image_2d_op = mhlo::TransposeOp::create(\n+        rewriter, loc, image_permutation_and_shape.shape,\n+        image_2d_op->getResult(0), image_permutation_and_shape.permutation);\n \n     // Reshape kernel to add a new spatial dimension.\n     auto kernel_type = mlir::cast<ShapedType>(conv_op.getRhs().getType());\n@@ -536,8 +537,8 @@ class Convert1DConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n     kernel_2d_shape.push_back(1);\n     auto kernel_2d_type =\n         RankedTensorType::get(kernel_2d_shape, kernel_type.getElementType());\n-    auto kernel_2d_op =\n-        rewriter.create<mhlo::ReshapeOp>(loc, kernel_2d_type, conv_op.getRhs());\n+    auto kernel_2d_op = mhlo::ReshapeOp::create(rewriter, loc, kernel_2d_type,\n+                                                conv_op.getRhs());\n \n     // Transpose kernel to get it into WHIO form (where H is the added dim).\n     SmallVector<int64_t, 4> kernel_permutation = {\n@@ -547,9 +548,9 @@ class Convert1DConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n         dnums.getKernelOutputFeatureDimension()};\n     auto kernel_permutation_and_shape = GetPermutationAndTransposedShape(\n         kernel_permutation, kernel_2d_type, rewriter);\n-    auto transposed_kernel_2d_op = rewriter.create<mhlo::TransposeOp>(\n-        loc, kernel_permutation_and_shape.shape, kernel_2d_op->getResult(0),\n-        kernel_permutation_and_shape.permutation);\n+    auto transposed_kernel_2d_op = mhlo::TransposeOp::create(\n+        rewriter, loc, kernel_permutation_and_shape.shape,\n+        kernel_2d_op->getResult(0), kernel_permutation_and_shape.permutation);\n \n     //\n     // Create 2d equivalents for 1d convolution attributes.\n@@ -638,12 +639,12 @@ class Convert1DConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n                                          rewriter)\n             .shape;\n \n-    auto conv2d_op = rewriter.create<mhlo::ConvolutionOp>(\n-        loc, transposed_output_2d_shape, transposed_image_2d_op.getResult(),\n-        transposed_kernel_2d_op.getResult(), window_strides_2d, padding_2d,\n-        lhs_dilation_2d, rhs_dilation_2d, window_reversal_2d, dnums_2d,\n-        conv_op.getFeatureGroupCount(), conv_op.getBatchGroupCount(),\n-        conv_op.getPrecisionConfigAttr());\n+    auto conv2d_op = mhlo::ConvolutionOp::create(\n+        rewriter, loc, transposed_output_2d_shape,\n+        transposed_image_2d_op.getResult(), transposed_kernel_2d_op.getResult(),\n+        window_strides_2d, padding_2d, lhs_dilation_2d, rhs_dilation_2d,\n+        window_reversal_2d, dnums_2d, conv_op.getFeatureGroupCount(),\n+        conv_op.getBatchGroupCount(), conv_op.getPrecisionConfigAttr());\n \n     OpResult conv2d_output = conv2d_op->getResult(0);\n     auto conv2d_output_type = mlir::cast<ShapedType>(conv2d_output.getType());\n@@ -656,8 +657,8 @@ class Convert1DConvOp : public OpConversionPattern<mhlo::ConvolutionOp> {\n     // affectively applied.\n     auto output_permutation_and_shape = GetInversePermutationAndShape(\n         output_permutation, conv2d_output_type, rewriter);\n-    auto transposed_output_2d_op = rewriter.create<mhlo::TransposeOp>(\n-        loc, output_permutation_and_shape.shape, conv2d_output,\n+    auto transposed_output_2d_op = mhlo::TransposeOp::create(\n+        rewriter, loc, output_permutation_and_shape.shape, conv2d_output,\n         output_permutation_and_shape.permutation);\n \n     // Drop the trailing spatial dimension from the output.\n@@ -804,11 +805,10 @@ class ConvertToResizeBilinearOpOrDepthwiseTransposedConvOp\n       } else {\n         limit_indices[channel_idx] = depth_idx + 1;\n       }\n-      return rewriter.create<mhlo::SliceOp>(\n-          conv_op.getLoc(), tensor,\n-          GetI64ElementsAttr(start_indices, &rewriter),\n-          GetI64ElementsAttr(limit_indices, &rewriter),\n-          GetI64ElementsAttr(strides, &rewriter));\n+      return mhlo::SliceOp::create(rewriter, conv_op.getLoc(), tensor,\n+                                   GetI64ElementsAttr(start_indices, &rewriter),\n+                                   GetI64ElementsAttr(limit_indices, &rewriter),\n+                                   GetI64ElementsAttr(strides, &rewriter));\n     };\n \n     // Storage for smaller convolution results\n@@ -832,18 +832,19 @@ class ConvertToResizeBilinearOpOrDepthwiseTransposedConvOp\n           RankedTensorType::get(new_output_shape, output_type.getElementType());\n \n       // Create a Smaller Convolution (Ensure compatibility)\n-      auto conv_result = rewriter.create<mhlo::ConvolutionOp>(\n-          conv_op.getLoc(), new_output_type, sliced_input, sliced_kernel,\n-          conv_op.getWindowStridesAttr(), conv_op.getPaddingAttr(),\n-          conv_op.getLhsDilationAttr(), conv_op.getRhsDilationAttr(),\n-          conv_op.getWindowReversalAttr(), conv_op.getDimensionNumbers(), 1, 1,\n+      auto conv_result = mhlo::ConvolutionOp::create(\n+          rewriter, conv_op.getLoc(), new_output_type, sliced_input,\n+          sliced_kernel, conv_op.getWindowStridesAttr(),\n+          conv_op.getPaddingAttr(), conv_op.getLhsDilationAttr(),\n+          conv_op.getRhsDilationAttr(), conv_op.getWindowReversalAttr(),\n+          conv_op.getDimensionNumbers(), 1, 1,\n           conv_op.getPrecisionConfigAttr());\n \n       conv_results.push_back(conv_result);\n     }\n \n-    auto final_output = rewriter.create<mhlo::ConcatenateOp>(\n-        conv_op.getLoc(), conv_results,\n+    auto final_output = mhlo::ConcatenateOp::create(\n+        rewriter, conv_op.getLoc(), conv_results,\n         rewriter.getI64IntegerAttr(dnums.getOutputFeatureDimension()));\n     rewriter.replaceOp(conv_op, final_output.getResult());\n     return mlir::success();\n@@ -854,17 +855,17 @@ class ConvertToResizeBilinearOpOrDepthwiseTransposedConvOp\n                               llvm::ArrayRef<int32_t> output_sizes,\n                               bool align_corners,\n                               ConversionPatternRewriter& rewriter) const {\n-    Value output_sizes_attr = rewriter.create<TF::ConstOp>(\n-        conv_op.getLoc(),\n+    Value output_sizes_attr = TF::ConstOp::create(\n+        rewriter, conv_op.getLoc(),\n         DenseIntElementsAttr::get(\n             RankedTensorType::get({static_cast<int64_t>(output_sizes.size())},\n                                   rewriter.getI32Type()),\n             output_sizes));\n     // The value of half_pixel_centers couldn't be inferred from the IR and XLA\n     // only support half_pixel_centers=True as in 01/11/2022. Here\n     // half_pixel_centers=False is hardcoded.\n-    Value output = rewriter.create<TF::ResizeBilinearOp>(\n-        conv_op.getLoc(), conv_op.getType(), conv_op.getLhs(),\n+    Value output = TF::ResizeBilinearOp::create(\n+        rewriter, conv_op.getLoc(), conv_op.getType(), conv_op.getLhs(),\n         output_sizes_attr,\n         /*align_corners=*/rewriter.getBoolAttr(align_corners),\n         /*half_pixel_centers=*/rewriter.getBoolAttr(false));\n@@ -1071,8 +1072,8 @@ class ConvertNonTrivialConvOp\n       permutation.push_back(dnums.getKernelOutputFeatureDimension());\n       permutation.push_back(dnums.getKernelInputFeatureDimension());\n \n-      auto filter_transposed = rewriter.create<mhlo::TransposeOp>(\n-          conv_op.getLoc(), conv_op.getRhs(),\n+      auto filter_transposed = mhlo::TransposeOp::create(\n+          rewriter, conv_op.getLoc(), conv_op.getRhs(),\n           DenseIntElementsAttr::get(\n               RankedTensorType::get({static_cast<int64_t>(permutation.size())},\n                                     rewriter.getI64Type()),\n@@ -1082,8 +1083,9 @@ class ConvertNonTrivialConvOp\n \n     // Lets hard-code the reverse indexes to be {0, 1} as the expectation is\n     // that the kernel is always in HWOI format, with the above code.\n-    mhlo::ReverseOp filter = rewriter.create<mhlo::ReverseOp>(\n-        conv_op.getLoc(), reverse_filter_in, rewriter.getI64TensorAttr({0, 1}));\n+    mhlo::ReverseOp filter =\n+        mhlo::ReverseOp::create(rewriter, conv_op.getLoc(), reverse_filter_in,\n+                                rewriter.getI64TensorAttr({0, 1}));\n \n     // if output is not in [b, 0, 1, f] format, insert transpose to go back\n     if (dnums.getOutputBatchDimension() != 0 ||\n@@ -1112,23 +1114,23 @@ class ConvertNonTrivialConvOp\n       auto output_type = RankedTensorType::get(\n           transposed_output_shape,\n           mlir::cast<ShapedType>(conv_op.getRhs().getType()).getElementType());\n-      auto output_sizes = rewriter.create<TF::ConstOp>(\n-          conv_op.getLoc(),\n+      auto output_sizes = TF::ConstOp::create(\n+          rewriter, conv_op.getLoc(),\n           DenseIntElementsAttr::get(\n               RankedTensorType::get(\n                   {static_cast<int64_t>(transposed_output_shape_i32.size())},\n                   rewriter.getI32Type()),\n               transposed_output_shape_i32));\n-      auto new_conv = rewriter.create<TF::Conv2DBackpropInputOp>(\n-          conv_op.getLoc(), output_type, output_sizes, filter, conv_input,\n-          rewriter.getI64ArrayAttr(strides),\n+      auto new_conv = TF::Conv2DBackpropInputOp::create(\n+          rewriter, conv_op.getLoc(), output_type, output_sizes, filter,\n+          conv_input, rewriter.getI64ArrayAttr(strides),\n           /*use_cudnn_on_gpu=*/rewriter.getBoolAttr(true),\n           /*padding=*/rewriter.getStringAttr(padding),\n           /*explicit_paddings=*/rewriter.getI64ArrayAttr({}),\n           /*data_format=*/rewriter.getStringAttr(\"NHWC\"),\n           /*dilations=*/rewriter.getI64ArrayAttr(dilation));\n-      auto output_transpose = rewriter.create<mhlo::TransposeOp>(\n-          conv_op.getLoc(), new_conv.getResult(),\n+      auto output_transpose = mhlo::TransposeOp::create(\n+          rewriter, conv_op.getLoc(), new_conv.getResult(),\n           rewriter.getI64TensorAttr(transpose_order));\n       conv_op->replaceAllUsesWith(output_transpose);\n       rewriter.eraseOp(conv_op);\n@@ -1139,8 +1141,8 @@ class ConvertNonTrivialConvOp\n                .getShape()) {\n         output_shape_i32.push_back(dim);\n       }\n-      auto output_sizes = rewriter.create<TF::ConstOp>(\n-          conv_op.getLoc(),\n+      auto output_sizes = TF::ConstOp::create(\n+          rewriter, conv_op.getLoc(),\n           DenseIntElementsAttr::get(\n               RankedTensorType::get(\n                   {static_cast<int64_t>(output_shape_i32.size())},\n@@ -1255,12 +1257,12 @@ class ConvertSliceOp : public OpConversionPattern<mhlo::SliceOp> {\n   LogicalResult matchAndRewrite(\n       mhlo::SliceOp slice_op, OpAdaptor adaptor,\n       ConversionPatternRewriter& rewriter) const final {\n-    auto begin = rewriter.create<TF::ConstOp>(slice_op.getLoc(),\n-                                              slice_op.getStartIndices());\n-    auto end = rewriter.create<TF::ConstOp>(slice_op.getLoc(),\n-                                            slice_op.getLimitIndices());\n+    auto begin = TF::ConstOp::create(rewriter, slice_op.getLoc(),\n+                                     slice_op.getStartIndices());\n+    auto end = TF::ConstOp::create(rewriter, slice_op.getLoc(),\n+                                   slice_op.getLimitIndices());\n     auto strides =\n-        rewriter.create<TF::ConstOp>(slice_op.getLoc(), slice_op.getStrides());\n+        TF::ConstOp::create(rewriter, slice_op.getLoc(), slice_op.getStrides());\n     rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n         slice_op, slice_op.getType(), slice_op.getOperand(), begin, end,\n         strides);\n@@ -1294,34 +1296,37 @@ class ConvertDynamicSliceOp : public OpConversionPattern<mhlo::DynamicSliceOp> {\n     // Clamp indices to [0, input_size - output_size]\n     llvm::SmallVector<Value, 4> start_indices_vector;\n     start_indices_vector.reserve(op.getStartIndices().size());\n-    Value clamp_min = rewriter.create<TF::ConstOp>(\n-        op.getLoc(),\n+    Value clamp_min = TF::ConstOp::create(\n+        rewriter, op.getLoc(),\n         rewriter.getIntegerAttr(signed_start_indices_element_type, 0));\n     for (uint64_t i = 0, e = op.getStartIndices().size(); i < e; ++i) {\n       // Always put a cast there.\n       auto start = op.getStartIndices()[i];\n       auto cast_type = mlir::cast<ShapedType>(start.getType())\n                            .clone(signed_start_indices_element_type);\n-      auto cast_op = rewriter.create<TF::CastOp>(op.getLoc(), cast_type, start);\n-      Value clamp_max = rewriter.create<TF::ConstOp>(\n-          op.getLoc(), rewriter.getIntegerAttr(\n-                           signed_start_indices_element_type,\n-                           input_type.getShape()[i] -\n-                               op.getSliceSizes().getValues<int64_t>()[i]));\n-      Value clamped_index = rewriter.create<mhlo::ClampOp>(\n-          op.getLoc(), cast_type, clamp_min, cast_op, clamp_max);\n+      auto cast_op =\n+          TF::CastOp::create(rewriter, op.getLoc(), cast_type, start);\n+      Value clamp_max = TF::ConstOp::create(\n+          rewriter, op.getLoc(),\n+          rewriter.getIntegerAttr(\n+              signed_start_indices_element_type,\n+              input_type.getShape()[i] -\n+                  op.getSliceSizes().getValues<int64_t>()[i]));\n+      Value clamped_index = mhlo::ClampOp::create(\n+          rewriter, op.getLoc(), cast_type, clamp_min, cast_op, clamp_max);\n       start_indices_vector.push_back(clamped_index);\n     }\n \n     // Pack individual start indices to start indices tensor.\n     Type start_indices_type = RankedTensorType::get(\n         {static_cast<int64_t>(start_indices_vector.size())},\n         signed_start_indices_element_type);\n-    Value start_indices_op = rewriter.create<TF::PackOp>(\n-        op.getLoc(), start_indices_type, ValueRange(start_indices_vector));\n+    Value start_indices_op =\n+        TF::PackOp::create(rewriter, op.getLoc(), start_indices_type,\n+                           ValueRange(start_indices_vector));\n \n     Value slice_sices_op =\n-        rewriter.create<TF::ConstOp>(op.getLoc(), op.getSliceSizes());\n+        TF::ConstOp::create(rewriter, op.getLoc(), op.getSliceSizes());\n     rewriter.replaceOpWithNewOp<TF::SliceOp>(op, op.getType(), op.getOperand(),\n                                              start_indices_op, slice_sices_op);\n     return success();\n@@ -1378,8 +1383,8 @@ Value BuildReshapeOp(ImplicitLocOpBuilder& builder,\n                      ArrayRef<int64_t> shape, Type idx_type,\n                      Type element_type) {\n   Value shape_cst = BuildIntArrayConstOp(builder, rewriter, shape, idx_type);\n-  Value reshaped_input = builder.create<TF::ReshapeOp>(\n-      RankedTensorType::get(shape, element_type), input, shape_cst);\n+  Value reshaped_input = TF::ReshapeOp::create(\n+      builder, RankedTensorType::get(shape, element_type), input, shape_cst);\n   return reshaped_input;\n }\n \n@@ -1389,8 +1394,9 @@ Value BuildSliceOp(ImplicitLocOpBuilder& builder,\n                    Value begin, ArrayRef<int64_t> shape, Type idx_type,\n                    Type element_type) {\n   Value shape_cst = BuildIntArrayConstOp(builder, rewriter, shape, idx_type);\n-  Value slice_result = builder.create<TF::SliceOp>(\n-      RankedTensorType::get(shape, element_type), input, begin, shape_cst);\n+  Value slice_result =\n+      TF::SliceOp::create(builder, RankedTensorType::get(shape, element_type),\n+                          input, begin, shape_cst);\n   return slice_result;\n }\n \n@@ -1416,8 +1422,8 @@ class ConvertDynamicUpdateSliceOp\n     llvm::SmallVector<Value> start_indices_vector;\n     Append(start_indices_vector, op.getStartIndices());\n     auto shape_tensor_type = RankedTensorType::get({shape_dim}, idx_type);\n-    Value start_indices_tensor = rewriter.create<TF::PackOp>(\n-        op.getLoc(), shape_tensor_type, start_indices_vector);\n+    Value start_indices_tensor = TF::PackOp::create(\n+        rewriter, op.getLoc(), shape_tensor_type, start_indices_vector);\n     rewriter.replaceOpWithNewOp<TF::XlaDynamicUpdateSliceOp>(\n         op, op.getType(), op.getOperand(), op.getUpdate(),\n         start_indices_tensor);\n@@ -1584,7 +1590,7 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n                                       bool is_lhs) {\n   auto operand_type = mlir::cast<ShapedType>(operand.getType());\n   BoolAttr true_attr = builder.getBoolAttr(true);\n-  auto operand_shape = builder.create<TF::ShapeOp>(operand, true_attr);\n+  auto operand_shape = TF::ShapeOp::create(builder, operand, true_attr);\n   const int64_t operand_rank = operand_type.getRank();\n   // Compute flattened out dimension and contracting dimension using\n   // TF::UnsortedSegmentProdOp.\n@@ -1600,26 +1606,28 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n   }\n   auto seg_prod_result_type =\n       RankedTensorType::get(static_cast<int32_t>(1), builder.getI32Type());\n-  auto out_segids_cst = builder.create<TF::ConstOp>(\n-      builder.getI32TensorAttr(flattened_out_segids));\n-  auto contracting_segids_cst = builder.create<TF::ConstOp>(\n-      builder.getI32TensorAttr(flattened_contracting_segids));\n+  auto out_segids_cst = TF::ConstOp::create(\n+      builder, builder.getI32TensorAttr(flattened_out_segids));\n+  auto contracting_segids_cst = TF::ConstOp::create(\n+      builder, builder.getI32TensorAttr(flattened_contracting_segids));\n   auto num_segids_tensor =\n-      builder.create<TF::ConstOp>(builder.getI32IntegerAttr(1));\n-  auto flattened_out_dims = builder.create<TF::UnsortedSegmentProdOp>(\n-      seg_prod_result_type, operand_shape, out_segids_cst, num_segids_tensor);\n-  auto flattened_contracting_dims = builder.create<TF::UnsortedSegmentProdOp>(\n-      seg_prod_result_type, operand_shape, contracting_segids_cst,\n+      TF::ConstOp::create(builder, builder.getI32IntegerAttr(1));\n+  auto flattened_out_dims = TF::UnsortedSegmentProdOp::create(\n+      builder, seg_prod_result_type, operand_shape, out_segids_cst,\n+      num_segids_tensor);\n+  auto flattened_contracting_dims = TF::UnsortedSegmentProdOp::create(\n+      builder, seg_prod_result_type, operand_shape, contracting_segids_cst,\n       num_segids_tensor);\n   llvm::SmallVector<Value, 3> flattend_shape_values;\n   // Gather the batch dimensions.\n   if (!dot_dimensions_info.batch_dimensions().AxesArray().empty()) {\n     if (ShapedType::isDynamicShape(\n             dot_dimensions_info.batch_dimensions().SizesArray())) {\n-      auto batch_axes_tensor =\n-          builder.create<TF::ConstOp>(builder.getI64TensorAttr(\n-              dot_dimensions_info.batch_dimensions().AxesArray()));\n-      auto batch_dims = builder.create<TF::GatherOp>(\n+      auto batch_axes_tensor = TF::ConstOp::create(\n+          builder, builder.getI64TensorAttr(\n+                       dot_dimensions_info.batch_dimensions().AxesArray()));\n+      auto batch_dims = TF::GatherOp::create(\n+          builder,\n           RankedTensorType::get(\n               {static_cast<int>(\n                   dot_dimensions_info.batch_dimensions().AxesArray().size())},\n@@ -1633,7 +1641,7 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n         batch_i32_vec.push_back(static_cast<int32_t>(element));\n       }\n       auto batch_dims =\n-          builder.create<TF::ConstOp>(builder.getI32TensorAttr(batch_i32_vec));\n+          TF::ConstOp::create(builder, builder.getI32TensorAttr(batch_i32_vec));\n       flattend_shape_values.push_back(batch_dims);\n     }\n   }\n@@ -1649,9 +1657,9 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n       builder.getIntegerType(32));\n   // Concatenate the batch dimensions, flattened out dimension and flattened\n   // contracting dimension.\n-  return builder.create<TF::ConcatOp>(\n-      concat_result_type,\n-      builder.create<TF::ConstOp>(builder.getI32IntegerAttr(0)),\n+  return TF::ConcatOp::create(\n+      builder, concat_result_type,\n+      TF::ConstOp::create(builder, builder.getI32IntegerAttr(0)),\n       flattend_shape_values);\n }\n \n@@ -1682,8 +1690,8 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n       lhs_dot_dimensions_info.batch_dimensions().SizesArray(),\n       lhs_dot_dimensions_info.out_dimensions().SizesArray(),\n       lhs_dot_dimensions_info.contracting_dimensions().SizesArray());\n-  auto lhs_transposed = rewriter.create<mhlo::TransposeOp>(\n-      loc,\n+  auto lhs_transposed = mhlo::TransposeOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(lhs_transposed_shape, lhs_type.getElementType()),\n       lhs,\n       DenseIntElementsAttr::get(\n@@ -1700,8 +1708,8 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n       rhs_dot_dimensions_info.batch_dimensions().SizesArray(),\n       rhs_dot_dimensions_info.contracting_dimensions().SizesArray(),\n       rhs_dot_dimensions_info.out_dimensions().SizesArray());\n-  auto rhs_transposed = rewriter.create<mhlo::TransposeOp>(\n-      loc,\n+  auto rhs_transposed = mhlo::TransposeOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(rhs_transposed_shape, rhs_type.getElementType()),\n       rhs,\n       DenseIntElementsAttr::get(\n@@ -1717,15 +1725,15 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n           lhs_dot_dimensions_info.FlattenedContractingDimensionSize()});\n   Value lhs_flattend;\n   if (lhs_type.hasStaticShape()) {\n-    lhs_flattend = rewriter.create<mhlo::ReshapeOp>(\n-        loc,\n+    lhs_flattend = mhlo::ReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(lhs_flattened_shape, lhs_type.getElementType()),\n         lhs_transposed.getResult());\n   } else {\n     auto lhs_flattend_shape_op = BuildDotOperandFlattenedShapeOp(\n         lhs, lhs_dot_dimensions_info, builder, /*is_lhs=*/true);\n-    lhs_flattend = rewriter.create<mhlo::DynamicReshapeOp>(\n-        loc,\n+    lhs_flattend = mhlo::DynamicReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(lhs_flattened_shape, lhs_type.getElementType()),\n         lhs_transposed, lhs_flattend_shape_op);\n   }\n@@ -1739,15 +1747,15 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n           rhs_dot_dimensions_info.FlattenedOutDimensionSize()});\n   Value rhs_flattend;\n   if (rhs_type.hasStaticShape()) {\n-    rhs_flattend = rewriter.create<mhlo::ReshapeOp>(\n-        loc,\n+    rhs_flattend = mhlo::ReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(rhs_flattened_shape, rhs_type.getElementType()),\n         rhs_transposed.getResult());\n   } else {\n     auto rhs_flattend_shape_op = BuildDotOperandFlattenedShapeOp(\n         rhs, rhs_dot_dimensions_info, builder, /*is_lhs=*/false);\n-    rhs_flattend = rewriter.create<mhlo::DynamicReshapeOp>(\n-        loc,\n+    rhs_flattend = mhlo::DynamicReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(rhs_flattened_shape, rhs_type.getElementType()),\n         rhs_transposed, rhs_flattend_shape_op);\n   }\n@@ -1759,36 +1767,38 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n                           lhs_dot_dimensions_info.FlattenedOutDimensionSize()},\n                       llvm::ArrayRef<int64_t>{\n                           rhs_dot_dimensions_info.FlattenedOutDimensionSize()});\n-  auto matmul = rewriter.create<TF::BatchMatMulV3Op>(\n-      loc, RankedTensorType::get(matmul_shape, result_type.getElementType()),\n+  auto matmul = TF::BatchMatMulV3Op::create(\n+      rewriter, loc,\n+      RankedTensorType::get(matmul_shape, result_type.getElementType()),\n       lhs_flattend, rhs_flattend);\n \n   if (result_type.hasStaticShape()) {\n     auto reshaped =\n-        rewriter.create<mhlo::ReshapeOp>(loc, result_type, matmul.getResult());\n+        mhlo::ReshapeOp::create(rewriter, loc, result_type, matmul.getResult());\n     return reshaped.getResult();\n   }\n \n   // Reshape for dynamic shaped operands. The result shape is\n   // [lhs_batch_dimensions, lhs_out_dimensions, rhs_out_dimensions].\n   BoolAttr true_attr = rewriter.getBoolAttr(true);\n-  auto lhs_shape = rewriter.create<TF::ShapeOp>(loc, lhs, true_attr);\n-  auto rhs_shape = rewriter.create<TF::ShapeOp>(loc, rhs, true_attr);\n+  auto lhs_shape = TF::ShapeOp::create(rewriter, loc, lhs, true_attr);\n+  auto rhs_shape = TF::ShapeOp::create(rewriter, loc, rhs, true_attr);\n   llvm::SmallVector<int64_t, 4> lhs_batch_and_out =\n       Concat<int64_t>(lhs_dot_dimensions_info.batch_dimensions().AxesArray(),\n                       lhs_dot_dimensions_info.out_dimensions().AxesArray());\n-  auto lhs_batch_and_out_cst = rewriter.create<TF::ConstOp>(\n-      loc, rewriter.getI64TensorAttr(lhs_batch_and_out));\n-  auto lhs_batch_and_out_dims = rewriter.create<TF::GatherOp>(\n-      loc,\n+  auto lhs_batch_and_out_cst = TF::ConstOp::create(\n+      rewriter, loc, rewriter.getI64TensorAttr(lhs_batch_and_out));\n+  auto lhs_batch_and_out_dims = TF::GatherOp::create(\n+      rewriter, loc,\n       RankedTensorType::get({static_cast<int>(lhs_batch_and_out.size())},\n                             rewriter.getIntegerType(32)),\n       lhs_shape, lhs_batch_and_out_cst, true_attr);\n-  auto rhs_out_cst = rewriter.create<TF::ConstOp>(\n-      loc, rewriter.getI64TensorAttr(\n-               rhs_dot_dimensions_info.out_dimensions().AxesArray()));\n-  auto rhs_out_dims = rewriter.create<TF::GatherOp>(\n-      loc,\n+  auto rhs_out_cst = TF::ConstOp::create(\n+      rewriter, loc,\n+      rewriter.getI64TensorAttr(\n+          rhs_dot_dimensions_info.out_dimensions().AxesArray()));\n+  auto rhs_out_dims = TF::GatherOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(\n           {static_cast<int32_t>(\n               rhs_dot_dimensions_info.out_dimensions().AxesArray().size())},\n@@ -1800,13 +1810,13 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n           lhs_dot_dimensions_info.out_dimensions().AxesArray().size() +\n           rhs_dot_dimensions_info.out_dimensions().AxesArray().size())},\n       rewriter.getIntegerType(32));\n-  auto result_shape = rewriter.create<TF::ConcatOp>(\n-      loc, result_shape_type,\n-      rewriter.create<TF::ConstOp>(loc, rewriter.getI32IntegerAttr(0)),\n+  auto result_shape = TF::ConcatOp::create(\n+      rewriter, loc, result_shape_type,\n+      TF::ConstOp::create(rewriter, loc, rewriter.getI32IntegerAttr(0)),\n       ValueRange{lhs_batch_and_out_dims, rhs_out_dims});\n \n-  auto reshaped = rewriter.create<mhlo::DynamicReshapeOp>(\n-      loc, result_type, matmul.getResult(), result_shape);\n+  auto reshaped = mhlo::DynamicReshapeOp::create(\n+      rewriter, loc, result_type, matmul.getResult(), result_shape);\n   return reshaped.getResult();\n }\n \n@@ -1844,9 +1854,10 @@ template <typename TfReduceOp, typename TfBinOp>\n LogicalResult rewriteNonMatchInitValue(mhlo::ReduceOp reduce_op, Value input,\n                                        TF::ConstOp reduction_indices,\n                                        ConversionPatternRewriter& rewriter) {\n-  Value reduce_result = rewriter.create<TfReduceOp>(\n-      reduce_op.getLoc(), reduce_op.getType(0), input, reduction_indices,\n-      /*keep_dim=*/rewriter.getBoolAttr(false));\n+  Value reduce_result =\n+      TfReduceOp::create(rewriter, reduce_op.getLoc(), reduce_op.getType(0),\n+                         input, reduction_indices,\n+                         /*keep_dim=*/rewriter.getBoolAttr(false));\n   rewriter.replaceOpWithNewOp<TfBinOp>(reduce_op, reduce_op.getType(0),\n                                        reduce_result,\n                                        reduce_op.getInitValues()[0]);\n@@ -1902,8 +1913,9 @@ class ConvertReduceOpToTfOp : public OpConversionPattern<mhlo::ReduceOp> {\n     }\n     auto dim_type = RankedTensorType::get(\n         {static_cast<int64_t>(reduce_dims.size())}, rewriter.getI64Type());\n-    auto reduction_indices = rewriter.create<TF::ConstOp>(\n-        reduce_op.getLoc(), dim_type, rewriter.getI64TensorAttr(reduce_dims));\n+    auto reduction_indices =\n+        TF::ConstOp::create(rewriter, reduce_op.getLoc(), dim_type,\n+                            rewriter.getI64TensorAttr(reduce_dims));\n \n     // In `MatchReduceOpOperand` function, we already match that the\n     // \"mhlo::ReduceOp\" only has one operand, one init_value and one result.\n@@ -2103,25 +2115,26 @@ class ConvertIotaOpToTfRange : public OpConversionPattern<mhlo::IotaOp> {\n \n     auto range_type =\n         RankedTensorType::get({type.getShape()[dimension]}, element_type);\n-    Value start_op = rewriter.create<TF::ConstOp>(iota_op.getLoc(), start);\n-    Value limit_op = rewriter.create<TF::ConstOp>(iota_op.getLoc(), limit);\n-    Value delta_op = rewriter.create<TF::ConstOp>(iota_op.getLoc(), delta);\n-    Value result = rewriter.create<TF::RangeOp>(iota_op.getLoc(), range_type,\n-                                                start_op, limit_op, delta_op);\n+    Value start_op = TF::ConstOp::create(rewriter, iota_op.getLoc(), start);\n+    Value limit_op = TF::ConstOp::create(rewriter, iota_op.getLoc(), limit);\n+    Value delta_op = TF::ConstOp::create(rewriter, iota_op.getLoc(), delta);\n+    Value result = TF::RangeOp::create(rewriter, iota_op.getLoc(), range_type,\n+                                       start_op, limit_op, delta_op);\n \n     if (type.getRank() > 1) {\n       std::vector<int64_t> reshape_shape(type.getRank(), 1);\n       reshape_shape[iota_op.getIotaDimension()] = type.getShape()[dimension];\n       auto reshape_type = RankedTensorType::get(reshape_shape, element_type);\n-      Value reshape_shape_op = rewriter.create<TF::ConstOp>(\n-          iota_op.getLoc(), rewriter.getI64TensorAttr(reshape_shape));\n-      result = rewriter.create<TF::ReshapeOp>(iota_op.getLoc(), reshape_type,\n-                                              result, reshape_shape_op);\n+      Value reshape_shape_op = TF::ConstOp::create(\n+          rewriter, iota_op.getLoc(), rewriter.getI64TensorAttr(reshape_shape));\n+      result = TF::ReshapeOp::create(rewriter, iota_op.getLoc(), reshape_type,\n+                                     result, reshape_shape_op);\n \n-      Value broadcast_shape_op = rewriter.create<TF::ConstOp>(\n-          iota_op.getLoc(), rewriter.getI64TensorAttr(type.getShape()));\n-      result = rewriter.create<TF::BroadcastToOp>(iota_op.getLoc(), type,\n-                                                  result, broadcast_shape_op);\n+      Value broadcast_shape_op =\n+          TF::ConstOp::create(rewriter, iota_op.getLoc(),\n+                              rewriter.getI64TensorAttr(type.getShape()));\n+      result = TF::BroadcastToOp::create(rewriter, iota_op.getLoc(), type,\n+                                         result, broadcast_shape_op);\n     }\n \n     rewriter.replaceOp(iota_op, result);\n@@ -2314,8 +2327,8 @@ class ConvertLoweredCumOp : public OpConversionPattern<mhlo::ReduceWindowOp> {\n       if (right_padding != 0) return failure();\n     }\n \n-    auto axis = rewriter.create<TF::ConstOp>(\n-        rw->getLoc(),\n+    auto axis = TF::ConstOp::create(\n+        rewriter, rw->getLoc(),\n         rewriter.getIntegerAttr(rewriter.getIntegerType(64), cumulative_axis));\n \n     rewriter.replaceOpWithNewOp<TfCumOp>(rw, rw.getType(0), rw.getInputs()[0],\n@@ -2585,7 +2598,7 @@ arith::ConstantOp ShapeToConst(PatternRewriter& rewriter, Value value) {\n   auto attr_type = RankedTensorType::get({static_cast<int64_t>(shape.size())},\n                                          rewriter.getIntegerType(64));\n   auto attr = DenseElementsAttr::get(attr_type, shape);\n-  return rewriter.create<arith::ConstantOp>(value.getLoc(), attr_type, attr);\n+  return arith::ConstantOp::create(rewriter, value.getLoc(), attr_type, attr);\n }\n \n bool IsSign(APInt a, APInt sign) {\n@@ -2841,8 +2854,8 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n \n     TF::CastOp cast_op = nullptr;\n     if (canonical_start_indices_type.getElementType().isUnsignedInteger(32)) {\n-      cast_op = rewriter.create<TF::CastOp>(\n-          gather_op->getLoc(),\n+      cast_op = TF::CastOp::create(\n+          rewriter, gather_op->getLoc(),\n           RankedTensorType::get(canonical_start_indices_type.getShape(),\n                                 rewriter.getI64Type()),\n           canonical_start_indices);\n@@ -2861,8 +2874,8 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n \n     auto canonical_result_type = RankedTensorType::get(\n         canonical_result_shape, result_type.getElementType());\n-    auto canonical_result = rewriter.create<TF::GatherNdOp>(\n-        gather_op->getLoc(), canonical_result_type, canonical_operand,\n+    auto canonical_result = TF::GatherNdOp::create(\n+        rewriter, gather_op->getLoc(), canonical_result_type, canonical_operand,\n         cast_op ? cast_op.getResult() : canonical_start_indices);\n \n     auto offset_dims = gather_op.getDimensionNumbers().getOffsetDims();\n@@ -2968,24 +2981,24 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n     auto min_start_indices = BuildIntArrayConstOp(\n         builder, rewriter, llvm::SmallVector<int64_t>({0, 0}),\n         start_indices_type.getElementType());\n-    auto start_indices_max_op = rewriter.create<TF::MaximumOp>(\n-        gather_op.getLoc(), start_indices, min_start_indices);\n-    auto clamped_start_indices_op = rewriter.create<TF::MinimumOp>(\n-        gather_op.getLoc(), start_indices_max_op, max_start_indices);\n+    auto start_indices_max_op = TF::MaximumOp::create(\n+        rewriter, gather_op.getLoc(), start_indices, min_start_indices);\n+    auto clamped_start_indices_op = TF::MinimumOp::create(\n+        rewriter, gather_op.getLoc(), start_indices_max_op, max_start_indices);\n \n     int64_t batch_size = start_indices_type.getDimSize(batch_dim);\n     auto slice_size = BuildIntArrayConstOp(\n         builder, rewriter, slice_sizes_vector, rewriter.getI32Type());\n     if (batch_size == 1) {\n-      auto squeeze_op = rewriter.create<TF::SqueezeOp>(\n-          gather_op.getLoc(),\n+      auto squeeze_op = TF::SqueezeOp::create(\n+          rewriter, gather_op.getLoc(),\n           RankedTensorType::get({rank_two},\n                                 start_indices_type.getElementType()),\n           clamped_start_indices_op,\n           rewriter.getI64ArrayAttr(llvm::ArrayRef<int64_t>({batch_dim})));\n       auto slice_op =\n-          rewriter.create<TF::SliceOp>(gather_op.getLoc(), gather_op.getType(),\n-                                       operand, squeeze_op, slice_size);\n+          TF::SliceOp::create(rewriter, gather_op.getLoc(), gather_op.getType(),\n+                              operand, squeeze_op, slice_size);\n       rewriter.replaceOp(gather_op, slice_op);\n       return mlir::success();\n     }\n@@ -2999,29 +3012,29 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n       auto two = BuildIntArrayConstOp(builder, rewriter,\n                                       llvm::SmallVector<int64_t>({1, 2}),\n                                       rewriter.getI32Type());\n-      auto begin = rewriter.create<TF::SliceOp>(\n-          gather_op.getLoc(),\n+      auto begin = TF::SliceOp::create(\n+          rewriter, gather_op.getLoc(),\n           RankedTensorType::get({1, 2}, start_indices_type.getElementType()),\n           clamped_start_indices_op, zero, two);\n-      auto squeeze_op = rewriter.create<TF::SqueezeOp>(\n-          gather_op.getLoc(),\n+      auto squeeze_op = TF::SqueezeOp::create(\n+          rewriter, gather_op.getLoc(),\n           RankedTensorType::get({rank_two},\n                                 start_indices_type.getElementType()),\n           begin,\n           rewriter.getI64ArrayAttr(llvm::ArrayRef<int64_t>({batch_dim})));\n-      auto slice_op = rewriter.create<TF::SliceOp>(\n-          gather_op.getLoc(),\n+      auto slice_op = TF::SliceOp::create(\n+          rewriter, gather_op.getLoc(),\n           RankedTensorType::get({1, slice_sizes_vector[1]},\n                                 operand_type.getElementType()),\n           operand, squeeze_op, slice_size);\n       slices.push_back(slice_op);\n     }\n     auto scalar_type = RankedTensorType::get({}, rewriter.getI32Type());\n-    auto zero_scalar = rewriter.create<TF::ConstOp>(\n-        gather_op.getLoc(),\n+    auto zero_scalar = TF::ConstOp::create(\n+        rewriter, gather_op.getLoc(),\n         DenseIntElementsAttr::get(scalar_type, static_cast<int32_t>(0)));\n-    auto concat_op = rewriter.create<TF::ConcatV2Op>(\n-        gather_op.getLoc(), result_type, slices, zero_scalar);\n+    auto concat_op = TF::ConcatV2Op::create(rewriter, gather_op.getLoc(),\n+                                            result_type, slices, zero_scalar);\n     rewriter.replaceOp(gather_op, concat_op);\n     return mlir::success();\n   }\n@@ -3116,12 +3129,13 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n     if (canonical_result_type.hasStaticShape()) {\n       auto unflattened_result_type = RankedTensorType::get(\n           unflattened_shape, original_result_type.getElementType());\n-      canonical_result = rewriter.create<mhlo::ReshapeOp>(\n-          gather_op.getLoc(), unflattened_result_type, canonical_result);\n+      canonical_result =\n+          mhlo::ReshapeOp::create(rewriter, gather_op.getLoc(),\n+                                  unflattened_result_type, canonical_result);\n     }\n     // Transpose back to the original result shape.\n-    return rewriter.create<mhlo::TransposeOp>(\n-        gather_op.getLoc(), original_result_type, canonical_result,\n+    return mhlo::TransposeOp::create(\n+        rewriter, gather_op.getLoc(), original_result_type, canonical_result,\n         rewriter.getI64TensorAttr(\n             GetInversePermutationArray(permutation_to_canonical)));\n   }\n@@ -3168,13 +3182,13 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n     // Transpose the dimensions and flatten the batching dimensions.\n     RankedTensorType transposed_type =\n         RankedTensorType::get(transposed_shape, operand_type.getElementType());\n-    auto transposed_operand = rewriter.create<mhlo::TransposeOp>(\n-        gather_op.getLoc(), transposed_type, operand,\n+    auto transposed_operand = mhlo::TransposeOp::create(\n+        rewriter, gather_op.getLoc(), transposed_type, operand,\n         rewriter.getI64TensorAttr(permutation));\n     auto flattened_type =\n         RankedTensorType::get(flattened_shape, operand_type.getElementType());\n-    auto flattened_operand = rewriter.create<mhlo::ReshapeOp>(\n-        gather_op.getLoc(), flattened_type, transposed_operand);\n+    auto flattened_operand = mhlo::ReshapeOp::create(\n+        rewriter, gather_op.getLoc(), flattened_type, transposed_operand);\n     return flattened_operand;\n   }\n \n@@ -3233,13 +3247,13 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n     reshaped_shape.push_back(index_vector_size);\n \n     // Transpose the dimensions and flatten the batching dimensions.\n-    auto transposed_start_indices = rewriter.create<mhlo::TransposeOp>(\n-        gather_op.getLoc(),\n+    auto transposed_start_indices = mhlo::TransposeOp::create(\n+        rewriter, gather_op.getLoc(),\n         RankedTensorType::get(transposed_shape,\n                               start_indices_type.getElementType()),\n         start_indices, rewriter.getI64TensorAttr(permutation));\n-    start_indices = rewriter.create<mhlo::ReshapeOp>(\n-        gather_op.getLoc(),\n+    start_indices = mhlo::ReshapeOp::create(\n+        rewriter, gather_op.getLoc(),\n         RankedTensorType::get(reshaped_shape,\n                               start_indices_type.getElementType()),\n         transposed_start_indices);\n@@ -3275,48 +3289,49 @@ class ConvertGatherOp : public OpConversionPattern<mhlo::GatherOp> {\n       llvm::SmallVector<int64_t> offsets_shape(start_indices_shape.size(), 1);\n       offsets_shape[non_trivial_sliced_dim] = slice_sizes[operand_dim];\n       start_indices_shape[non_trivial_sliced_dim] = slice_sizes[operand_dim];\n-      auto offsets = rewriter.create<mhlo::IotaOp>(\n-          gather_op.getLoc(),\n+      auto offsets = mhlo::IotaOp::create(\n+          rewriter, gather_op.getLoc(),\n           RankedTensorType::get(offsets_shape,\n                                 start_indices_type.getElementType()),\n           rewriter.getI64IntegerAttr(non_trivial_sliced_dim));\n       non_trivial_sliced_dim++;\n \n       // Pad with 0s on the other operand dimensions.\n-      Value zero = rewriter.create<arith::ConstantOp>(\n-          gather_op.getLoc(), rewriter.getZeroAttr(RankedTensorType::get(\n-                                  {}, start_indices_type.getElementType())));\n+      Value zero = arith::ConstantOp::create(\n+          rewriter, gather_op.getLoc(),\n+          rewriter.getZeroAttr(\n+              RankedTensorType::get({}, start_indices_type.getElementType())));\n       int rank = offsets_shape.size();\n       llvm::SmallVector<int64_t> padding_low(rank, 0);\n       llvm::SmallVector<int64_t> padding_high(rank, 0);\n       llvm::SmallVector<int64_t> padding_interior(rank, 0);\n       padding_low.back() = i;\n       padding_high.back() = start_indices_shape.back() - i - 1;\n-      auto padded_offsets = rewriter.create<mhlo::PadOp>(\n-          gather_op.getLoc(), offsets, zero,\n-          GetI64ElementsAttr(padding_low, &rewriter),\n-          GetI64ElementsAttr(padding_high, &rewriter),\n-          GetI64ElementsAttr(padding_interior, &rewriter));\n+      auto padded_offsets =\n+          mhlo::PadOp::create(rewriter, gather_op.getLoc(), offsets, zero,\n+                              GetI64ElementsAttr(padding_low, &rewriter),\n+                              GetI64ElementsAttr(padding_high, &rewriter),\n+                              GetI64ElementsAttr(padding_interior, &rewriter));\n \n       // Add the padded offsets to the start indices (with broadcasting).\n-      start_indices = rewriter.create<TF::AddOp>(gather_op.getLoc(),\n-                                                 start_indices, padded_offsets);\n+      start_indices = TF::AddOp::create(rewriter, gather_op.getLoc(),\n+                                        start_indices, padded_offsets);\n     }\n \n     if (!start_indices_batching_dims.empty()) {\n       // Concat iota values for indexing into the batching dimensions of the\n       // operand.\n       llvm::SmallVector<int64_t> offsets_shape = start_indices_shape;\n       offsets_shape.back() = 1;\n-      auto offsets = rewriter.create<mhlo::IotaOp>(\n-          gather_op.getLoc(),\n+      auto offsets = mhlo::IotaOp::create(\n+          rewriter, gather_op.getLoc(),\n           RankedTensorType::get(offsets_shape,\n                                 start_indices_type.getElementType()),\n           rewriter.getI64IntegerAttr(0));\n \n       start_indices_shape.back()++;\n-      start_indices = rewriter.create<mhlo::ConcatenateOp>(\n-          gather_op.getLoc(),\n+      start_indices = mhlo::ConcatenateOp::create(\n+          rewriter, gather_op.getLoc(),\n           RankedTensorType::get(start_indices_shape,\n                                 start_indices_type.getElementType()),\n           ValueRange{offsets, start_indices},\n@@ -3345,8 +3360,9 @@ class ConvertWhileOp : public OpConversionPattern<mhlo::WhileOp> {\n     // Creates a TF::WhileRegionOp to replace the mhlo::WhileOp. HLO WhileOp\n     // currently doesn't support stateless and shape invariant, so these\n     // parameters are set to the default values.\n-    auto new_while = rewriter.create<TF::WhileRegionOp>(\n-        while_op.getLoc(), while_op->getResultTypes(), while_op->getOperands(),\n+    auto new_while = TF::WhileRegionOp::create(\n+        rewriter, while_op.getLoc(), while_op->getResultTypes(),\n+        while_op->getOperands(),\n         /*parallel_iterations=*/10,\n         /*is_stateless=*/false, /*shape_invariant=*/false);\n     new_while.getCond().takeBody(while_op.getCond());\n@@ -3366,8 +3382,8 @@ class ConvertIfOp : public OpConversionPattern<mhlo::IfOp> {\n       mhlo::IfOp op, OpAdaptor adaptor,\n       ConversionPatternRewriter& rewriter) const final {\n     // HLO IfOp currently doesn't support stateless\n-    auto new_op = rewriter.create<TF::IfRegionOp>(\n-        op.getLoc(), op->getResultTypes(), op.getPred(),\n+    auto new_op = TF::IfRegionOp::create(\n+        rewriter, op.getLoc(), op->getResultTypes(), op.getPred(),\n         /*is_stateless=*/false, /*_then_func_name=*/nullptr,\n         /*_else_func_name=*/nullptr);\n     new_op.getThenBranch().takeBody(op.getTrueBranch());\n@@ -3427,26 +3443,25 @@ Value ConvertPadOp(PatternRewriter& rewriter, Operation* old_op) {\n       {pad_op.getEdgePaddingLow().size(), 2}, rewriter.getI64Type());\n   auto padding_attr = DenseIntElementsAttr::get(padding_attr_type, padding);\n   auto padding_amount_const_op =\n-      rewriter.create<arith::ConstantOp>(loc, padding_attr_type, padding_attr);\n-  auto new_pad_op = rewriter.create<TF::PadV2Op>(\n-      loc, pad_op.getType().clone(pad_output_shape), pad_op.getOperand(),\n-      padding_amount_const_op, pad_op.getPaddingValue());\n+      arith::ConstantOp::create(rewriter, loc, padding_attr_type, padding_attr);\n+  auto new_pad_op = TF::PadV2Op::create(\n+      rewriter, loc, pad_op.getType().clone(pad_output_shape),\n+      pad_op.getOperand(), padding_amount_const_op, pad_op.getPaddingValue());\n   if (!has_negative_padding_amount) {\n     return new_pad_op;\n   }\n \n   // Convert negative padding amount into slice.\n   auto slice_attr_type = RankedTensorType::get(\n       {pad_op.getEdgePaddingLow().size()}, rewriter.getI64Type());\n-  auto slice_begins_const_op = rewriter.create<arith::ConstantOp>(\n-      loc, slice_attr_type,\n+  auto slice_begins_const_op = arith::ConstantOp::create(\n+      rewriter, loc, slice_attr_type,\n       DenseIntElementsAttr::get(slice_attr_type, slice_begins));\n-  auto slice_sizes_const_op = rewriter.create<arith::ConstantOp>(\n-      loc, slice_attr_type,\n+  auto slice_sizes_const_op = arith::ConstantOp::create(\n+      rewriter, loc, slice_attr_type,\n       DenseIntElementsAttr::get(slice_attr_type, slice_sizes));\n-  return rewriter.create<TF::SliceOp>(loc, pad_op.getType(), new_pad_op,\n-                                      slice_begins_const_op,\n-                                      slice_sizes_const_op);\n+  return TF::SliceOp::create(rewriter, loc, pad_op.getType(), new_pad_op,\n+                             slice_begins_const_op, slice_sizes_const_op);\n }\n \n class ConvertPopulationCountOp\n@@ -3459,8 +3474,8 @@ class ConvertPopulationCountOp\n       ConversionPatternRewriter& rewriter) const final {\n     auto output_type = op.getType().clone(\n         rewriter.getIntegerType(/*width=*/8, /*isSigned=*/false));\n-    auto pop_cnt = rewriter.create<TF::PopulationCountOp>(\n-        op.getLoc(), output_type, op.getOperand());\n+    auto pop_cnt = TF::PopulationCountOp::create(rewriter, op.getLoc(),\n+                                                 output_type, op.getOperand());\n     auto cast_or_pop_cnt =\n         rewriter.createOrFold<TF::CastOp>(op.getLoc(), op.getType(), pop_cnt);\n     rewriter.replaceOp(op, {cast_or_pop_cnt});\n@@ -3608,9 +3623,9 @@ class ConvertCustomCallWithApproxTopK\n     }\n     auto is_max_k = rewriter.getBoolAttr(true);\n \n-    auto approx_top_k = rewriter.create<TF::ApproxTopKOp>(\n-        op.getLoc(), op->getResultTypes(), op.getInputs()[0], top_k_attr,\n-        reduction_dim_attr, recall_target_attr, is_max_k,\n+    auto approx_top_k = TF::ApproxTopKOp::create(\n+        rewriter, op.getLoc(), op->getResultTypes(), op.getInputs()[0],\n+        top_k_attr, reduction_dim_attr, recall_target_attr, is_max_k,\n         reduction_input_size_override_attr, aggregate_to_topk_attr);\n \n     rewriter.replaceOp(op, approx_top_k.getResults());\n@@ -3661,22 +3676,22 @@ class ConvertGetDimensionSizeOp\n       mhlo::GetDimensionSizeOp op, OpAdaptor adaptor,\n       ConversionPatternRewriter& rewriter) const final {\n     ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n-    Value shape_op = rewriter.create<TF::ShapeOp>(op.getLoc(), op.getOperand(),\n-                                                  rewriter.getBoolAttr(true));\n+    Value shape_op = TF::ShapeOp::create(rewriter, op.getLoc(), op.getOperand(),\n+                                         rewriter.getBoolAttr(true));\n     Value size =\n         BuildIntArrayConstOp(builder, rewriter, llvm::SmallVector<int64_t>({1}),\n                              rewriter.getI32Type());\n     Value begin = BuildIntArrayConstOp(\n         builder, rewriter,\n         llvm::SmallVector<int64_t>({static_cast<int64_t>(op.getDimension())}),\n         rewriter.getI64Type());\n-    Value slice_op = rewriter.create<TF::SliceOp>(\n-        op.getLoc(),\n+    Value slice_op = TF::SliceOp::create(\n+        rewriter, op.getLoc(),\n         RankedTensorType::get({static_cast<int64_t>(1)},\n                               op.getType().getElementType()),\n         shape_op, begin, size);\n-    Value squeeze_op = rewriter.create<TF::SqueezeOp>(\n-        op.getLoc(), op.getType(), slice_op,\n+    Value squeeze_op = TF::SqueezeOp::create(\n+        rewriter, op.getLoc(), op.getType(), slice_op,\n         rewriter.getI64ArrayAttr(llvm::ArrayRef<int64_t>({0})));\n     rewriter.replaceOp(op, {squeeze_op});\n     return success();\n@@ -3749,25 +3764,26 @@ class ConvertDynamicIotaOp : public OpConversionPattern<mhlo::DynamicIotaOp> {\n     if (mlir::isa<FloatType>(element_type)) {\n       auto cast_type =\n           mlir::cast<ShapedType>(output_shape.getType()).clone(element_type);\n-      output_shape = rewriter.create<TF::CastOp>(dynamic_iota_op.getLoc(),\n-                                                 cast_type, output_shape);\n+      output_shape = TF::CastOp::create(rewriter, dynamic_iota_op.getLoc(),\n+                                        cast_type, output_shape);\n     }\n     DenseIntElementsAttr scalar_attr = DenseIntElementsAttr::get(\n         RankedTensorType::get({0}, rewriter.getI32Type()),\n         llvm::ArrayRef<int32_t>({}));\n     auto scalar_shape =\n-        rewriter.create<TF::ConstOp>(dynamic_iota_op.getLoc(), scalar_attr);\n-    auto limit_scalar = rewriter.create<TF::ReshapeOp>(\n-        dynamic_iota_op.getLoc(), RankedTensorType::get({}, element_type),\n-        output_shape, scalar_shape);\n+        TF::ConstOp::create(rewriter, dynamic_iota_op.getLoc(), scalar_attr);\n+    auto limit_scalar = TF::ReshapeOp::create(\n+        rewriter, dynamic_iota_op.getLoc(),\n+        RankedTensorType::get({}, element_type), output_shape, scalar_shape);\n     auto range_type =\n         RankedTensorType::get({type.getShape()[dimension]}, element_type);\n     Value start_op =\n-        rewriter.create<TF::ConstOp>(dynamic_iota_op.getLoc(), start);\n+        TF::ConstOp::create(rewriter, dynamic_iota_op.getLoc(), start);\n     Value delta_op =\n-        rewriter.create<TF::ConstOp>(dynamic_iota_op.getLoc(), delta);\n-    Value range_op = rewriter.create<TF::RangeOp>(\n-        dynamic_iota_op.getLoc(), range_type, start_op, limit_scalar, delta_op);\n+        TF::ConstOp::create(rewriter, dynamic_iota_op.getLoc(), delta);\n+    Value range_op =\n+        TF::RangeOp::create(rewriter, dynamic_iota_op.getLoc(), range_type,\n+                            start_op, limit_scalar, delta_op);\n     rewriter.replaceOp(dynamic_iota_op, range_op);\n     return success();\n   }\n@@ -3820,7 +3836,7 @@ arith::ConstantOp ExpandedShape(PatternRewriter& rewriter, Value input,\n       RankedTensorType::get({static_cast<int64_t>(expanded_shape.size())},\n                             rewriter.getIntegerType(64));\n   auto attr = DenseElementsAttr::get(attr_type, expanded_shape);\n-  return rewriter.create<arith::ConstantOp>(output.getLoc(), attr_type, attr);\n+  return arith::ConstantOp::create(rewriter, output.getLoc(), attr_type, attr);\n }\n \n Value ExpandedDynamicShape(PatternRewriter& rewriter, Value input,\n@@ -3843,9 +3859,9 @@ Value ExpandedDynamicShape(PatternRewriter& rewriter, Value input,\n   for (int64_t i : expanded_dimensions) {\n     auto index_attr = DenseIntElementsAttr::get(\n         RankedTensorType::get({}, rewriter.getI64Type()), {i});\n-    Value index = rewriter.create<TF::ConstOp>(output.getLoc(), index_attr);\n-    expanded_input = rewriter.create<TF::ExpandDimsOp>(output.getLoc(),\n-                                                       expanded_input, index);\n+    Value index = TF::ConstOp::create(rewriter, output.getLoc(), index_attr);\n+    expanded_input = TF::ExpandDimsOp::create(rewriter, output.getLoc(),\n+                                              expanded_input, index);\n   }\n   return expanded_input;\n }"
        },
        {
            "sha": "096de88c16055fa7162db12e94aacdf0fdba6461",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/conv_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fconv_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fconv_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fconv_util.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -111,11 +111,11 @@ Value CreatePadOpFromConvPadding(OpBuilder& b, mhlo::ConvolutionOp op) {\n   auto padding_value_type = RankedTensorType::get({}, data.ElementType());\n   auto padding_value_attr = b.getZeroAttr(padding_value_type);\n   auto padding_value_op =\n-      b.create<arith::ConstantOp>(op->getLoc(), padding_value_attr);\n+      arith::ConstantOp::create(b, op->getLoc(), padding_value_attr);\n \n-  auto pad_op = b.create<mhlo::PadOp>(padding_value_op->getLoc(), op.getLhs(),\n-                                      padding_value_op, lo_padding_attr,\n-                                      hi_padding_attr, interior_padding_attr);\n+  auto pad_op = mhlo::PadOp::create(b, padding_value_op->getLoc(), op.getLhs(),\n+                                    padding_value_op, lo_padding_attr,\n+                                    hi_padding_attr, interior_padding_attr);\n \n   return pad_op;\n }"
        },
        {
            "sha": "18d9b10d677259b529f81e5ee24a717f8fdab63d",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/custom_call.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fcustom_call.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fcustom_call.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fcustom_call.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -82,9 +82,9 @@ LogicalResult ConvertCustomCallOp::matchAndRewrite(\n   if (!call_target_name.starts_with(\"custom_call.\")) {\n     return failure();\n   }\n-  auto tfl_custom = rewriter.create<TFL::CustomOp>(\n-      mhlo_custom_call.getLoc(), mhlo_custom_call.getResultTypes(),\n-      mhlo_custom_call.getInputs());\n+  auto tfl_custom = TFL::CustomOp::create(rewriter, mhlo_custom_call.getLoc(),\n+                                          mhlo_custom_call.getResultTypes(),\n+                                          mhlo_custom_call.getInputs());\n   tfl_custom.setCustomCodeAttr(rewriter.getStringAttr(call_target_name));\n \n   if (auto bc = mhlo_custom_call.getBackendConfig()) {"
        },
        {
            "sha": "347817d3cc6d59c72e092c720c4c218b5c0219d0",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/dot_general.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 54,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fdot_general.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fdot_general.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fdot_general.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -178,7 +178,8 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n                                       ImplicitLocOpBuilder& builder,\n                                       bool is_lhs) {\n   auto operand_type = mlir::cast<ShapedType>(operand.getType());\n-  auto operand_shape = builder.create<TFL::ShapeOp>(\n+  auto operand_shape = TFL::ShapeOp::create(\n+      builder,\n       RankedTensorType::get(static_cast<int32_t>(operand_type.getRank()),\n                             builder.getIntegerType(32)),\n       operand);\n@@ -197,27 +198,29 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n   }\n   auto seg_prod_result_type =\n       RankedTensorType::get(static_cast<int32_t>(1), builder.getI32Type());\n-  auto out_segids_cst = builder.create<TFL::ConstOp>(\n-      builder.getI32TensorAttr(flattened_out_segids));\n-  auto contracting_segids_cst = builder.create<TFL::ConstOp>(\n-      builder.getI32TensorAttr(flattened_contracting_segids));\n-  auto num_segids_tensor =\n-      builder.create<TFL::ConstOp>(DenseIntElementsAttr::get(\n-          RankedTensorType::get({}, builder.getIntegerType(32)), 1));\n-  auto flattened_out_dims = builder.create<TFL::UnsortedSegmentProdOp>(\n-      seg_prod_result_type, operand_shape, out_segids_cst, num_segids_tensor);\n-  auto flattened_contracting_dims = builder.create<TFL::UnsortedSegmentProdOp>(\n-      seg_prod_result_type, operand_shape, contracting_segids_cst,\n+  auto out_segids_cst = TFL::ConstOp::create(\n+      builder, builder.getI32TensorAttr(flattened_out_segids));\n+  auto contracting_segids_cst = TFL::ConstOp::create(\n+      builder, builder.getI32TensorAttr(flattened_contracting_segids));\n+  auto num_segids_tensor = TFL::ConstOp::create(\n+      builder, DenseIntElementsAttr::get(\n+                   RankedTensorType::get({}, builder.getIntegerType(32)), 1));\n+  auto flattened_out_dims = TFL::UnsortedSegmentProdOp::create(\n+      builder, seg_prod_result_type, operand_shape, out_segids_cst,\n+      num_segids_tensor);\n+  auto flattened_contracting_dims = TFL::UnsortedSegmentProdOp::create(\n+      builder, seg_prod_result_type, operand_shape, contracting_segids_cst,\n       num_segids_tensor);\n   llvm::SmallVector<Value, 3> flattend_shape_values;\n   // Gather the batch dimensions.\n   if (!dot_dimensions_info.batch_dimensions().AxesArray().empty()) {\n     if (ShapedType::isDynamicShape(\n             dot_dimensions_info.batch_dimensions().SizesArray())) {\n-      auto batch_axes_tensor =\n-          builder.create<TFL::ConstOp>(builder.getI64TensorAttr(\n-              dot_dimensions_info.batch_dimensions().AxesArray()));\n-      auto batch_dims = builder.create<TFL::GatherOp>(\n+      auto batch_axes_tensor = TFL::ConstOp::create(\n+          builder, builder.getI64TensorAttr(\n+                       dot_dimensions_info.batch_dimensions().AxesArray()));\n+      auto batch_dims = TFL::GatherOp::create(\n+          builder,\n           RankedTensorType::get(\n               {static_cast<int>(\n                   dot_dimensions_info.batch_dimensions().AxesArray().size())},\n@@ -230,8 +233,8 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n            dot_dimensions_info.batch_dimensions().SizesArray()) {\n         batch_i32_vec.push_back(static_cast<int32_t>(element));\n       }\n-      auto batch_dims =\n-          builder.create<TFL::ConstOp>(builder.getI32TensorAttr(batch_i32_vec));\n+      auto batch_dims = TFL::ConstOp::create(\n+          builder, builder.getI32TensorAttr(batch_i32_vec));\n       flattend_shape_values.push_back(batch_dims);\n     }\n   }\n@@ -247,9 +250,9 @@ Value BuildDotOperandFlattenedShapeOp(Value operand,\n       builder.getIntegerType(32));\n   // Concatenate the batch dimensions, flattened out dimension and flattened\n   // contracting dimension.\n-  return builder.create<TFL::ConcatenationOp>(\n-      concat_result_type, flattend_shape_values, /*axis*/ 0,\n-      /*fused_activation_function*/ \"NONE\");\n+  return TFL::ConcatenationOp::create(builder, concat_result_type,\n+                                      flattend_shape_values, /*axis*/ 0,\n+                                      /*fused_activation_function*/ \"NONE\");\n }\n }  // namespace\n \n@@ -280,8 +283,8 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n       lhs_dot_dimensions_info.batch_dimensions().SizesArray(),\n       lhs_dot_dimensions_info.out_dimensions().SizesArray(),\n       lhs_dot_dimensions_info.contracting_dimensions().SizesArray());\n-  auto lhs_transposed = rewriter.create<mhlo::TransposeOp>(\n-      loc,\n+  auto lhs_transposed = mhlo::TransposeOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(lhs_transposed_shape, lhs_type.getElementType()),\n       lhs,\n       DenseIntElementsAttr::get(\n@@ -298,8 +301,8 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n       rhs_dot_dimensions_info.batch_dimensions().SizesArray(),\n       rhs_dot_dimensions_info.contracting_dimensions().SizesArray(),\n       rhs_dot_dimensions_info.out_dimensions().SizesArray());\n-  auto rhs_transposed = rewriter.create<mhlo::TransposeOp>(\n-      loc,\n+  auto rhs_transposed = mhlo::TransposeOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(rhs_transposed_shape, rhs_type.getElementType()),\n       rhs,\n       DenseIntElementsAttr::get(\n@@ -314,15 +317,15 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n           lhs_dot_dimensions_info.FlattenedContractingDimensionSize()});\n   Value lhs_flattend;\n   if (lhs_type.hasStaticShape()) {\n-    lhs_flattend = rewriter.create<mhlo::ReshapeOp>(\n-        loc,\n+    lhs_flattend = mhlo::ReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(lhs_flattened_shape, lhs_type.getElementType()),\n         lhs_transposed.getResult());\n   } else {\n     auto lhs_flattend_shape_op = BuildDotOperandFlattenedShapeOp(\n         lhs, lhs_dot_dimensions_info, builder, /*is_lhs=*/true);\n-    lhs_flattend = rewriter.create<mhlo::DynamicReshapeOp>(\n-        loc,\n+    lhs_flattend = mhlo::DynamicReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(lhs_flattened_shape, lhs_type.getElementType()),\n         lhs_transposed, lhs_flattend_shape_op);\n   }\n@@ -336,15 +339,15 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n           rhs_dot_dimensions_info.FlattenedOutDimensionSize()});\n   Value rhs_flattend;\n   if (rhs_type.hasStaticShape()) {\n-    rhs_flattend = rewriter.create<mhlo::ReshapeOp>(\n-        loc,\n+    rhs_flattend = mhlo::ReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(rhs_flattened_shape, rhs_type.getElementType()),\n         rhs_transposed.getResult());\n   } else {\n     auto rhs_flattend_shape_op = BuildDotOperandFlattenedShapeOp(\n         rhs, rhs_dot_dimensions_info, builder, /*is_lhs=*/false);\n-    rhs_flattend = rewriter.create<mhlo::DynamicReshapeOp>(\n-        loc,\n+    rhs_flattend = mhlo::DynamicReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get(rhs_flattened_shape, rhs_type.getElementType()),\n         rhs_transposed, rhs_flattend_shape_op);\n   }\n@@ -357,44 +360,46 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n                       llvm::ArrayRef<int64_t>{\n                           rhs_dot_dimensions_info.FlattenedOutDimensionSize()});\n   BoolAttr false_attr = rewriter.getBoolAttr(false);\n-  auto matmul = rewriter.create<TFL::BatchMatMulOp>(\n-      loc, RankedTensorType::get(matmul_shape, result_type.getElementType()),\n+  auto matmul = TFL::BatchMatMulOp::create(\n+      rewriter, loc,\n+      RankedTensorType::get(matmul_shape, result_type.getElementType()),\n       lhs_flattend, rhs_flattend, /*adj_x*/ false_attr, /*adj_y*/ false_attr,\n       /*asym_quant_input*/ false_attr);\n   if (result_type.hasStaticShape()) {\n     auto reshaped =\n-        rewriter.create<mhlo::ReshapeOp>(loc, result_type, matmul.getResult());\n+        mhlo::ReshapeOp::create(rewriter, loc, result_type, matmul.getResult());\n     return reshaped.getResult();\n   }\n \n   // Reshape for dynamic shaped operands. The result shape is\n   // [lhs_batch_dimensions, lhs_out_dimensions, rhs_out_dimensions].\n-  auto lhs_shape = rewriter.create<TFL::ShapeOp>(\n-      loc,\n+  auto lhs_shape = TFL::ShapeOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(static_cast<int32_t>(lhs_type.getRank()),\n                             builder.getIntegerType(32)),\n       lhs);\n-  auto rhs_shape = rewriter.create<TFL::ShapeOp>(\n-      loc,\n+  auto rhs_shape = TFL::ShapeOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(static_cast<int32_t>(rhs_type.getRank()),\n                             builder.getIntegerType(32)),\n       rhs);\n   llvm::SmallVector<int64_t, 4> lhs_batch_and_out =\n       Concat<int64_t>(lhs_dot_dimensions_info.batch_dimensions().AxesArray(),\n                       lhs_dot_dimensions_info.out_dimensions().AxesArray());\n-  auto lhs_batch_and_out_cst = rewriter.create<TFL::ConstOp>(\n-      loc, rewriter.getI64TensorAttr(lhs_batch_and_out));\n-  auto lhs_batch_and_out_dims = rewriter.create<TFL::GatherOp>(\n-      loc,\n+  auto lhs_batch_and_out_cst = TFL::ConstOp::create(\n+      rewriter, loc, rewriter.getI64TensorAttr(lhs_batch_and_out));\n+  auto lhs_batch_and_out_dims = TFL::GatherOp::create(\n+      rewriter, loc,\n       RankedTensorType::get({static_cast<int>(lhs_batch_and_out.size())},\n                             rewriter.getIntegerType(32)),\n       lhs_shape, lhs_batch_and_out_cst,\n       /*axis*/ 0, /*batch_dims*/ 0);\n-  auto rhs_out_cst = rewriter.create<TFL::ConstOp>(\n-      loc, rewriter.getI64TensorAttr(\n-               rhs_dot_dimensions_info.out_dimensions().AxesArray()));\n-  auto rhs_out_dims = rewriter.create<TFL::GatherOp>(\n-      loc,\n+  auto rhs_out_cst = TFL::ConstOp::create(\n+      rewriter, loc,\n+      rewriter.getI64TensorAttr(\n+          rhs_dot_dimensions_info.out_dimensions().AxesArray()));\n+  auto rhs_out_dims = TFL::GatherOp::create(\n+      rewriter, loc,\n       RankedTensorType::get(\n           {static_cast<int32_t>(\n               rhs_dot_dimensions_info.out_dimensions().AxesArray().size())},\n@@ -407,12 +412,12 @@ Value ConvertDot(PatternRewriter& rewriter, Value lhs, Value rhs,\n           lhs_dot_dimensions_info.out_dimensions().AxesArray().size() +\n           rhs_dot_dimensions_info.out_dimensions().AxesArray().size())},\n       rewriter.getIntegerType(32));\n-  auto result_shape = rewriter.create<TFL::ConcatenationOp>(\n-      loc, result_shape_type, ValueRange{lhs_batch_and_out_dims, rhs_out_dims},\n-      0, \"NONE\");\n+  auto result_shape = TFL::ConcatenationOp::create(\n+      rewriter, loc, result_shape_type,\n+      ValueRange{lhs_batch_and_out_dims, rhs_out_dims}, 0, \"NONE\");\n \n-  auto reshaped = rewriter.create<mhlo::DynamicReshapeOp>(\n-      loc, result_type, matmul.getResult(), result_shape);\n+  auto reshaped = mhlo::DynamicReshapeOp::create(\n+      rewriter, loc, result_type, matmul.getResult(), result_shape);\n   return reshaped.getResult();\n }\n "
        },
        {
            "sha": "34b1b60fd1b825b06977615049c2e6ea52561063",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/fft.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 23,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Ffft.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Ffft.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Ffft.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -74,9 +74,11 @@ bool IsSupportedRfftOp(mhlo::FftOp fft_op) {\n // concatenate with other dimension sizes.\n Value GetDimensionSizeTensor(OpBuilder& rewriter, Location loc, Value input,\n                              int64_t dim) {\n-  auto size_scalar = rewriter.create<mhlo::GetDimensionSizeOp>(loc, input, dim);\n-  return rewriter.create<mhlo::ReshapeOp>(\n-      loc, RankedTensorType::get({1}, rewriter.getI32Type()), size_scalar);\n+  auto size_scalar =\n+      mhlo::GetDimensionSizeOp::create(rewriter, loc, input, dim);\n+  return mhlo::ReshapeOp::create(\n+      rewriter, loc, RankedTensorType::get({1}, rewriter.getI32Type()),\n+      size_scalar);\n }\n \n // Convert rfft to rfft2d.\n@@ -154,26 +156,26 @@ class ConvertNDFftTo2DFftOp : public OpRewritePattern<mhlo::FftOp> {\n           expanded_input_shape_values.push_back(GetDimensionSizeTensor(\n               rewriter, fft_op.getLoc(), fft_operand, i));\n         }\n-        expanded_input_shape_values.push_back(rewriter.create<mhlo::ConstantOp>(\n-            fft_op.getLoc(), rewriter.getI32TensorAttr({1})));\n+        expanded_input_shape_values.push_back(mhlo::ConstantOp::create(\n+            rewriter, fft_op.getLoc(), rewriter.getI32TensorAttr({1})));\n         expanded_input_shape_values.push_back(GetDimensionSizeTensor(\n             rewriter, fft_op.getLoc(), fft_operand, input_shape.size() - 1));\n \n-        auto expanded_input_shape_tensor = rewriter.create<mhlo::ConcatenateOp>(\n-            fft_op.getLoc(),\n+        auto expanded_input_shape_tensor = mhlo::ConcatenateOp::create(\n+            rewriter, fft_op.getLoc(),\n             RankedTensorType::get(\n                 {static_cast<int64_t>(expanded_input_shape_values.size())},\n                 rewriter.getI32Type()),\n             expanded_input_shape_values, 0);\n \n         // Create a new mhlo.dynamic_reshape op with the expanded input and\n         // expanded input shape. SHAPE tensor is created in the previous step.\n-        fft_operand = rewriter.create<mhlo::DynamicReshapeOp>(\n-            fft_op.getLoc(), expanded_input_type, fft_operand,\n+        fft_operand = mhlo::DynamicReshapeOp::create(\n+            rewriter, fft_op.getLoc(), expanded_input_type, fft_operand,\n             expanded_input_shape_tensor);\n       } else {\n-        fft_operand = rewriter.create<mhlo::ReshapeOp>(\n-            fft_op.getLoc(), expanded_input_type, fft_operand);\n+        fft_operand = mhlo::ReshapeOp::create(rewriter, fft_op.getLoc(),\n+                                              expanded_input_type, fft_operand);\n       }\n \n       SmallVector<int64_t, 6> new_output_shape = {output_shape.begin(),\n@@ -186,8 +188,8 @@ class ConvertNDFftTo2DFftOp : public OpRewritePattern<mhlo::FftOp> {\n     }\n \n     auto new_fft =\n-        rewriter.create<mhlo::FftOp>(fft_op.getLoc(), output_type, fft_operand,\n-                                     fft_op.getFftType(), new_fft_lengths_attr);\n+        mhlo::FftOp::create(rewriter, fft_op.getLoc(), output_type, fft_operand,\n+                            fft_op.getFftType(), new_fft_lengths_attr);\n \n     if (input_shape[input_shape.size() - 2] != 1) {\n       // Squeeze the output dimensions back to 2D.\n@@ -202,19 +204,20 @@ class ConvertNDFftTo2DFftOp : public OpRewritePattern<mhlo::FftOp> {\n             rewriter, fft_op.getLoc(), new_fft.getResult(),\n             new_fft.getResult().getType().getShape().size() - 1));\n \n-        auto shape_tensor = rewriter.create<mhlo::ConcatenateOp>(\n-            fft_op.getLoc(),\n+        auto shape_tensor = mhlo::ConcatenateOp::create(\n+            rewriter, fft_op.getLoc(),\n             RankedTensorType::get(\n                 {static_cast<int64_t>(output_shape_values.size())},\n                 rewriter.getI32Type()),\n             output_shape_values, 0);\n-        auto squeeze_op = rewriter.create<mhlo::DynamicReshapeOp>(\n-            fft_op.getLoc(), fft_op.getResult().getType(), new_fft.getResult(),\n-            shape_tensor);\n+        auto squeeze_op = mhlo::DynamicReshapeOp::create(\n+            rewriter, fft_op.getLoc(), fft_op.getResult().getType(),\n+            new_fft.getResult(), shape_tensor);\n         rewriter.replaceOp(fft_op, squeeze_op.getResult());\n       } else {\n-        auto squeeze_op = rewriter.create<mhlo::ReshapeOp>(\n-            fft_op.getLoc(), fft_op.getResult().getType(), new_fft.getResult());\n+        auto squeeze_op = mhlo::ReshapeOp::create(rewriter, fft_op.getLoc(),\n+                                                  fft_op.getResult().getType(),\n+                                                  new_fft.getResult());\n         rewriter.replaceOp(fft_op, squeeze_op.getResult());\n       }\n     } else {\n@@ -256,9 +259,10 @@ class LegalizeRfftOp : public OpConversionPattern<mhlo::FftOp> {\n \n     auto output_type = mlir::cast<ShapedType>(fft_op.getResult().getType());\n     auto fft_len_const =\n-        rewriter.create<arith::ConstantOp>(fft_op.getLoc(), fft_len_f32_attr);\n-    auto tfl_rfft2d = rewriter.create<TFL::RFFT2dOp>(\n-        fft_op.getLoc(), output_type, fft_op.getOperand(), fft_len_const);\n+        arith::ConstantOp::create(rewriter, fft_op.getLoc(), fft_len_f32_attr);\n+    auto tfl_rfft2d =\n+        TFL::RFFT2dOp::create(rewriter, fft_op.getLoc(), output_type,\n+                              fft_op.getOperand(), fft_len_const);\n \n     rewriter.replaceOp(fft_op, tfl_rfft2d.getResult());\n "
        },
        {
            "sha": "9833b3415f3059ea7c62ba6082f2d635e61c2ad8",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/gelu.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fgelu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fgelu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fgelu.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -119,9 +119,9 @@ LogicalResult LowerGELU::matchAndRewrite(Operation* op,\n   if (!HasSplatArg(rhs_mul, kOneOverRoot2, 1)) return failure();\n \n   auto is_approx_attr = rewriter.getBoolAttr(false);\n-  auto gelu = rewriter.create<TFL::GeluOp>(\n-      output_mul.getLoc(), output_mul.getResult().getType(),\n-      erf_input->getOperand(0), is_approx_attr);\n+  auto gelu = TFL::GeluOp::create(rewriter, output_mul.getLoc(),\n+                                  output_mul.getResult().getType(),\n+                                  erf_input->getOperand(0), is_approx_attr);\n   rewriter.replaceAllOpUsesWith(output_mul, gelu);\n   // Note these must be erased in reverse topo order to avoid\n   // failing in debug mode."
        },
        {
            "sha": "6b377c0eee933c73f7a45283b1ab1f47bb62ec97",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/if.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fif.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fif.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fif.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -33,8 +33,8 @@ class LegalizeIfOp : public OpConversionPattern<mhlo::IfOp> {\n   LogicalResult matchAndRewrite(\n       mhlo::IfOp if_op, OpAdaptor adaptor,\n       ConversionPatternRewriter& rewriter) const final {\n-    auto new_op = rewriter.create<TFL::IfOp>(\n-        if_op.getLoc(), if_op.getResultTypes(), if_op.getPred());\n+    auto new_op = TFL::IfOp::create(rewriter, if_op.getLoc(),\n+                                    if_op.getResultTypes(), if_op.getPred());\n \n     new_op.getThenRegion().takeBody(if_op.getTrueBranch());\n     new_op.getElseRegion().takeBody(if_op.getFalseBranch());"
        },
        {
            "sha": "5b5368ac1f55221321b7e478312b67d1fbcd79b2",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/reduce.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 18,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Freduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Freduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Freduce.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -248,8 +248,8 @@ LogicalResult ConvertReduceOpToArgMinMax<\n   int64_t axis = reduce_op.getDimensions().getValues<int64_t>()[0];\n \n   auto dim_type = RankedTensorType::get({1}, rewriter.getI32Type());\n-  auto reduction_indices = rewriter.create<arith::ConstantOp>(\n-      reduce_op.getLoc(), dim_type,\n+  auto reduction_indices = arith::ConstantOp::create(\n+      rewriter, reduce_op.getLoc(), dim_type,\n       rewriter.getI32TensorAttr({static_cast<int32_t>(axis)}));\n \n   // Generate a Max and an ArgMax of as the mhlo op returns both while in TF\n@@ -260,24 +260,24 @@ LogicalResult ConvertReduceOpToArgMinMax<\n   if (operand_type.getElementType().isInteger(1)) {\n     // TF does not support min or max on boolean (int1) arguments.\n     // Use AnyOp for MaxOp and AllOp for MinOp.\n-    auto tf_reduce_op = rewriter.create<BooleanReduce>(\n-        reduce_op.getLoc(), reduce_op->getResult(0).getType(), operand,\n-        reduction_indices,\n+    auto tf_reduce_op = BooleanReduce::create(\n+        rewriter, reduce_op.getLoc(), reduce_op->getResult(0).getType(),\n+        operand, reduction_indices,\n         /*keep_dim=*/rewriter.getBoolAttr(false));\n-    auto tf_argreduce_op = rewriter.create<ArgReduce>(\n-        reduce_op.getLoc(), reduce_op->getResult(1).getType(), operand,\n-        reduction_indices);\n+    auto tf_argreduce_op = ArgReduce::create(rewriter, reduce_op.getLoc(),\n+                                             reduce_op->getResult(1).getType(),\n+                                             operand, reduction_indices);\n \n     rewriter.replaceOp(reduce_op, {tf_reduce_op, tf_argreduce_op});\n   } else {\n-    auto tf_reduce_op = rewriter.create<Reduce>(\n-        reduce_op.getLoc(), reduce_op->getResult(0).getType(), operand,\n-        reduction_indices,\n+    auto tf_reduce_op = Reduce::create(\n+        rewriter, reduce_op.getLoc(), reduce_op->getResult(0).getType(),\n+        operand, reduction_indices,\n         /*keep_dim=*/rewriter.getBoolAttr(false));\n \n-    auto tf_argreduce_op = rewriter.create<ArgReduce>(\n-        reduce_op.getLoc(), reduce_op->getResult(1).getType(), operand,\n-        reduction_indices);\n+    auto tf_argreduce_op = ArgReduce::create(rewriter, reduce_op.getLoc(),\n+                                             reduce_op->getResult(1).getType(),\n+                                             operand, reduction_indices);\n \n     rewriter.replaceOp(reduce_op, {tf_reduce_op, tf_argreduce_op});\n   }\n@@ -366,9 +366,10 @@ template <typename ReduceOp, typename BinaryOp, bool BuilderHasFAF = false>\n LogicalResult rewriteNonMatchInitValue(mhlo::ReduceOp reduce_op, Value input,\n                                        arith::ConstantOp reduction_indices,\n                                        ConversionPatternRewriter& rewriter) {\n-  Value reduce_result = rewriter.create<ReduceOp>(\n-      reduce_op.getLoc(), reduce_op.getType(0), input, reduction_indices,\n-      /*keep_dim=*/rewriter.getBoolAttr(false));\n+  Value reduce_result =\n+      ReduceOp::create(rewriter, reduce_op.getLoc(), reduce_op.getType(0),\n+                       input, reduction_indices,\n+                       /*keep_dim=*/rewriter.getBoolAttr(false));\n \n   if constexpr (BuilderHasFAF) {\n     rewriter.replaceOpWithNewOp<BinaryOp>(reduce_op, reduce_result,\n@@ -455,7 +456,7 @@ class ConvertReduce : public OpConversionPattern<mhlo::ReduceOp> {\n \n     auto tfl_dims = GetDimsAsI32Elements(rewriter, reduce_op);\n     auto tfl_dims_op =\n-        rewriter.create<arith::ConstantOp>(reduce_op.getLoc(), tfl_dims);\n+        arith::ConstantOp::create(rewriter, reduce_op.getLoc(), tfl_dims);\n \n     //\n     // replace with new reduce op, chaining binary op if needed."
        },
        {
            "sha": "c4a3dc62fd58f074944f353c803bcf1a4624a450",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/reduce_window.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Freduce_window.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Freduce_window.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Freduce_window.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -126,7 +126,7 @@ Value TransposeTensor(OpBuilder& b, Value tensor,\n   const int64_t perm_size = perm.size();\n   auto perm_attr_type = RankedTensorType::get({perm_size}, b.getI64Type());\n   auto perm_attr = DenseIntElementsAttr::get(perm_attr_type, perm);\n-  return b.create<mhlo::TransposeOp>(tensor.getLoc(), tensor, perm_attr);\n+  return mhlo::TransposeOp::create(b, tensor.getLoc(), tensor, perm_attr);\n }\n \n DenseIntElementsAttr BuildDenseI64(OpBuilder& b, ArrayRef<int64_t> shape,\n@@ -289,9 +289,10 @@ LogicalResult RelayoutReduceWindow::matchAndRewrite(\n \n   // transpose input and build new reduce_window\n   auto new_input = TransposeTensor(rewriter, input, perm_for_inputs);\n-  auto new_rw = rewriter.create<mhlo::ReduceWindowOp>(\n-      op.getLoc(), new_out_type, new_input, init_val, new_window_dims_attr,\n-      new_window_strides_attr, BuildDenseI64(rewriter, view.BaseDilations()),\n+  auto new_rw = mhlo::ReduceWindowOp::create(\n+      rewriter, op.getLoc(), new_out_type, new_input, init_val,\n+      new_window_dims_attr, new_window_strides_attr,\n+      BuildDenseI64(rewriter, view.BaseDilations()),\n       BuildDenseI64(rewriter, view.WindowDilations()), new_paddings_attr);\n   IRMapping ir_map;\n   op.getBody().cloneInto(&new_rw.getBody(), ir_map);\n@@ -412,7 +413,7 @@ LogicalResult LegalizeCumSum::matchAndRewrite(\n       RankedTensorType::get({}, rewriter.getI32Type()),\n       static_cast<int32_t>(axis));\n   auto axis_cst =\n-      rewriter.create<arith::ConstantOp>(op->getLoc(), axis_cst_attr);\n+      arith::ConstantOp::create(rewriter, op->getLoc(), axis_cst_attr);\n \n   auto tfl_exclusive_attr = rewriter.getBoolAttr(false);\n   auto tfl_reverse_attr = rewriter.getBoolAttr(false);\n@@ -476,7 +477,7 @@ TFL::PadV2Op LegalizeMaxPool::BuildExplicitPadOp(\n       llvm::ArrayRef<int64_t>(padding_values));\n \n   auto padding_values_op =\n-      rewriter.create<arith::ConstantOp>(op.getLoc(), padding_dense_attr);\n+      arith::ConstantOp::create(rewriter, op.getLoc(), padding_dense_attr);\n \n   llvm::SmallVector<int64_t, 4> pad_output_shape_vector;\n   pad_output_shape_vector.push_back(input_type.getDimSize(0));\n@@ -489,8 +490,8 @@ TFL::PadV2Op LegalizeMaxPool::BuildExplicitPadOp(\n   pad_output_shape_vector.push_back(input_type.getDimSize(3));\n   auto pad_output_type = mlir::RankedTensorType::get(\n       pad_output_shape_vector, output_type.getElementType());\n-  return rewriter.create<TFL::PadV2Op>(op.getLoc(), pad_output_type, input,\n-                                       padding_values_op, init);\n+  return TFL::PadV2Op::create(rewriter, op.getLoc(), pad_output_type, input,\n+                              padding_values_op, init);\n }\n \n LogicalResult LegalizeMaxPool::matchAndRewrite(\n@@ -575,13 +576,12 @@ void ReplaceWithAvgPool(mhlo::DivOp op, Value rw_lhs_input,\n \n   auto [fh, fw, sh, sw, p, faf] =\n       BuildTFLPoolAttrs(rewriter, lhs_view, padding);\n-  Value final_op = rewriter.create<TFL::AveragePool2DOp>(\n-      op->getLoc(), out_type, rw_lhs_input, fh, fw, p, sh, sw, faf);\n+  Value final_op = TFL::AveragePool2DOp::create(\n+      rewriter, op->getLoc(), out_type, rw_lhs_input, fh, fw, p, sh, sw, faf);\n \n   if (opt_final_tpose) {\n-    final_op = rewriter\n-                   .create<mhlo::TransposeOp>(final_op.getLoc(), final_op,\n-                                              opt_final_tpose.getPermutation())\n+    final_op = mhlo::TransposeOp::create(rewriter, final_op.getLoc(), final_op,\n+                                         opt_final_tpose.getPermutation())\n                    .getResult();\n   }\n "
        },
        {
            "sha": "303c446d536b4725c7c9bf75890a8f62d37b3079",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/scatter.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fscatter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fscatter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fscatter.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -69,8 +69,8 @@ LogicalResult CanonicalizeScatterUpdates(\n   auto permutation_and_shape = GetPermutationAndTransposedShape(\n       permutation_array, updates_type, rewriter);\n \n-  auto transposed_updates = rewriter.create<mhlo::TransposeOp>(\n-      scatter_op->getLoc(), permutation_and_shape.shape, updates,\n+  auto transposed_updates = mhlo::TransposeOp::create(\n+      rewriter, scatter_op->getLoc(), permutation_and_shape.shape, updates,\n       permutation_and_shape.permutation);\n \n   updates = transposed_updates;\n@@ -163,9 +163,9 @@ LogicalResult ConvertScatterOp<BinaryOp, TfOp>::matchAndRewrite(\n       permutation_array, operand_type, rewriter);\n \n   Location loc = scatter_op.getLoc();\n-  auto transposed_operand = rewriter.create<mhlo::TransposeOp>(\n-      loc, permutation_and_shape.shape, operands[0],\n-      permutation_and_shape.permutation);\n+  auto transposed_operand =\n+      mhlo::TransposeOp::create(rewriter, loc, permutation_and_shape.shape,\n+                                operands[0], permutation_and_shape.permutation);\n \n   Value new_indices = indices;\n   int64_t index_depth =\n@@ -181,17 +181,17 @@ LogicalResult ConvertScatterOp<BinaryOp, TfOp>::matchAndRewrite(\n         builder, rewriter,\n         llvm::SmallVector<int64_t>({num_updates, index_depth}),\n         rewriter.getI32Type());\n-    new_indices = rewriter.create<TF::ReshapeOp>(\n-        loc,\n+    new_indices = TF::ReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get({num_updates, index_depth},\n                               indices_type.getElementType()),\n         indices, indices_shape);\n     auto updates_shape = BuildIntArrayConstOp(\n         builder, rewriter,\n         llvm::SmallVector<int64_t>({num_updates, updates_type.getDimSize(0)}),\n         rewriter.getI32Type());\n-    new_updates = rewriter.create<TF::ReshapeOp>(\n-        loc,\n+    new_updates = TF::ReshapeOp::create(\n+        rewriter, loc,\n         RankedTensorType::get({1, updates_type.getDimSize(0)},\n                               updates_type.getElementType()),\n         new_updates, updates_shape);\n@@ -200,8 +200,8 @@ LogicalResult ConvertScatterOp<BinaryOp, TfOp>::matchAndRewrite(\n   // Apply TF scatter to update the trailing dimensions of the\n   // transposed operand.\n   auto tf_scatter_op =\n-      rewriter.create<TfOp>(loc, permutation_and_shape.shape,\n-                            transposed_operand, new_indices, new_updates);\n+      TfOp::create(rewriter, loc, permutation_and_shape.shape,\n+                   transposed_operand, new_indices, new_updates);\n \n   // Reverse the earlier transpose.\n   auto inverse_permutation = GetInversePermutation(permutation_array, rewriter);"
        },
        {
            "sha": "548951c1ae43e0acef151d0fd342080cc695bbf8",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/slice.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fslice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fslice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fslice.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -46,8 +46,8 @@ Value PackScalarIndices(mlir::ValueRange indices, OpBuilder& b) {\n   auto values_count_attr = b.getI32IntegerAttr(num_indices);\n   auto pack_axis_attr = b.getI32IntegerAttr(0);\n \n-  return b.create<TFL::PackOp>(indices.back().getLoc(), packed_indices_type,\n-                               indices, values_count_attr, pack_axis_attr);\n+  return TFL::PackOp::create(b, indices.back().getLoc(), packed_indices_type,\n+                             indices, values_count_attr, pack_axis_attr);\n }\n \n //===----------------------------------------------------------------------===//\n@@ -56,8 +56,8 @@ Value PackScalarIndices(mlir::ValueRange indices, OpBuilder& b) {\n \n // Cast the value to i32.\n Value BuildTFLCastOp(OpBuilder& b, Value value) {\n-  return b.create<TFL::CastOp>(\n-      value.getLoc(),\n+  return TFL::CastOp::create(\n+      b, value.getLoc(),\n       RankedTensorType::get(llvm::cast<ShapedType>(value.getType()).getShape(),\n                             b.getI32Type()),\n       value);\n@@ -70,12 +70,12 @@ class LegalizeSliceOp : public OpConversionPattern<mhlo::SliceOp> {\n   LogicalResult matchAndRewrite(\n       mhlo::SliceOp slice_op, OpAdaptor adaptor,\n       ConversionPatternRewriter& rewriter) const final {\n-    auto begin = rewriter.create<arith::ConstantOp>(slice_op.getLoc(),\n-                                                    slice_op.getStartIndices());\n-    auto end = rewriter.create<arith::ConstantOp>(slice_op.getLoc(),\n-                                                  slice_op.getLimitIndices());\n-    auto strides = rewriter.create<arith::ConstantOp>(slice_op.getLoc(),\n-                                                      slice_op.getStrides());\n+    auto begin = arith::ConstantOp::create(rewriter, slice_op.getLoc(),\n+                                           slice_op.getStartIndices());\n+    auto end = arith::ConstantOp::create(rewriter, slice_op.getLoc(),\n+                                         slice_op.getLimitIndices());\n+    auto strides = arith::ConstantOp::create(rewriter, slice_op.getLoc(),\n+                                             slice_op.getStrides());\n     auto zero = rewriter.getIntegerAttr(rewriter.getI32Type(), 0);\n     auto no_offset = rewriter.getBoolAttr(false);\n \n@@ -116,8 +116,8 @@ LogicalResult CastSliceIndicesToSignless::matchAndRewrite(\n \n   llvm::SmallVector<Value> casted_start_inds;\n   for (auto start_ind_opr : op.getStartIndices()) {\n-    auto casted_start_ind_opr = rewriter.create<mhlo::ConvertOp>(\n-        start_ind_opr.getLoc(), start_ind_opr, new_start_e_type);\n+    auto casted_start_ind_opr = mhlo::ConvertOp::create(\n+        rewriter, start_ind_opr.getLoc(), start_ind_opr, new_start_e_type);\n     casted_start_inds.push_back(casted_start_ind_opr.getResult());\n   }\n \n@@ -161,24 +161,24 @@ LogicalResult LegalizeDynamicSliceOp::matchAndRewrite(\n   // clamp start indices between zero and shape(operand) - slice_sizes\n   //=-----\n \n-  Value clamp_left_cst = rewriter.create<arith::ConstantOp>(\n-      op->getLoc(), rewriter.getZeroAttr(start_type));\n+  Value clamp_left_cst = arith::ConstantOp::create(\n+      rewriter, op->getLoc(), rewriter.getZeroAttr(start_type));\n \n   llvm::SmallVector<Value> new_start_indices;\n   const auto stride_sizes = UnrollI64Splat(op.getSliceSizes());\n \n   for (auto [dim_size, start_ind_opr, stride_size] :\n        llvm::zip(input_type.getShape(), op.getStartIndices(), stride_sizes)) {\n     const int64_t clamp_right_val = dim_size - stride_size;\n-    auto clamp_right_cst = rewriter.create<arith::ConstantOp>(\n-        op->getLoc(),\n+    auto clamp_right_cst = arith::ConstantOp::create(\n+        rewriter, op->getLoc(),\n         DenseElementsAttr::get(start_type, rewriter.getIntegerAttr(\n                                                start_e_type, clamp_right_val)));\n \n-    Value new_start_ind = rewriter.create<TFL::MaximumOp>(\n-        op->getLoc(), start_type, clamp_left_cst, start_ind_opr);\n-    new_start_ind = rewriter.create<TFL::MinimumOp>(\n-        op->getLoc(), start_type, clamp_right_cst, new_start_ind);\n+    Value new_start_ind = TFL::MaximumOp::create(\n+        rewriter, op->getLoc(), start_type, clamp_left_cst, start_ind_opr);\n+    new_start_ind = TFL::MinimumOp::create(rewriter, op->getLoc(), start_type,\n+                                           clamp_right_cst, new_start_ind);\n \n     new_start_indices.push_back(new_start_ind);\n   }\n@@ -190,7 +190,7 @@ LogicalResult LegalizeDynamicSliceOp::matchAndRewrite(\n   auto packed_indices = PackScalarIndices(new_start_indices, rewriter);\n \n   auto slice_sizes_cst =\n-      rewriter.create<arith::ConstantOp>(op->getLoc(), op.getSliceSizes());\n+      arith::ConstantOp::create(rewriter, op->getLoc(), op.getSliceSizes());\n \n   rewriter.replaceOpWithNewOp<TFL::SliceOp>(op, op.getType(), op.getOperand(),\n                                             packed_indices, slice_sizes_cst);"
        },
        {
            "sha": "6dcf03b1600244c3ea748928b75879d984924ce0",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/util.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Futil.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Futil.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Futil.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -70,7 +70,7 @@ Value BuildIntConstOp(ImplicitLocOpBuilder& builder,\n                       ConversionPatternRewriter& rewriter, int64_t const_value,\n                       Type type) {\n   Value result_const =\n-      builder.create<TF::ConstOp>(rewriter.getIntegerAttr(type, const_value));\n+      TF::ConstOp::create(builder, rewriter.getIntegerAttr(type, const_value));\n   return result_const;\n }\n \n@@ -115,8 +115,8 @@ LogicalResult NormalizeIndexVector(Operation* parent_op, Value& indices,\n     new_start_indices_shape.push_back(1);\n     indices_type = RankedTensorType::get(new_start_indices_shape,\n                                          indices_type.getElementType());\n-    indices = rewriter.create<mhlo::ReshapeOp>(parent_op->getLoc(),\n-                                               indices_type, indices);\n+    indices = mhlo::ReshapeOp::create(rewriter, parent_op->getLoc(),\n+                                      indices_type, indices);\n   } else if (index_vector_dim != indices_type.getRank() - 1) {\n     // If index_vector_dim isn't the last dimension in indices then it isn't\n     // supported yet.\n@@ -197,19 +197,19 @@ Value InsertTranspose(Value value, int batch_dim, int feature_dim,\n                                     default_batch_dim, default_feature_dim,\n                                     default_spatial_dim_start, num_spatial_dims,\n                                     type, rewriter);\n-  return rewriter.create<mhlo::TransposeOp>(value.getLoc(), type, value,\n-                                            permutation);\n+  return mhlo::TransposeOp::create(rewriter, value.getLoc(), type, value,\n+                                   permutation);\n }\n \n Value CreateCastToInt32(Value val, Location loc, PatternRewriter& rewriter) {\n   IntegerType new_ele_type = rewriter.getIntegerType(32);\n   if (auto shaped_type = mlir::dyn_cast<RankedTensorType>(val.getType())) {\n     ShapedType new_type =\n         RankedTensorType::get(shaped_type.getShape(), new_ele_type);\n-    return rewriter.create<TFL::CastOp>(loc, new_type, val);\n+    return TFL::CastOp::create(rewriter, loc, new_type, val);\n   }\n-  return rewriter.create<TFL::CastOp>(\n-      loc, UnrankedTensorType::get(new_ele_type), val);\n+  return TFL::CastOp::create(rewriter, loc,\n+                             UnrankedTensorType::get(new_ele_type), val);\n }\n \n // Replaces `region`'s terminator to TFL::Yield."
        },
        {
            "sha": "1bf33c1d0d993e45cc97bdd8ef5c2f73fa661d86",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Futil.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Futil.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Futil.h?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -75,7 +75,7 @@ Value BuildIntArrayConstOp(ImplicitLocOpBuilder& builder,\n     }\n     const_value_raw = rewriter.getI32TensorAttr(const_i32_vec);\n   }\n-  Value result_const = builder.create<ConstOpT>(const_value_raw);\n+  Value result_const = ConstOpT::create(builder, const_value_raw);\n   return result_const;\n }\n "
        },
        {
            "sha": "0de2ccafedbe16c03cbf8a2e0a9ca35bcbf52392",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/while.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fwhile.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fwhile.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_hlo_conversions%2Fwhile.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -51,9 +51,10 @@ class LeagalizeWhileOp : public OpConversionPattern<mhlo::WhileOp> {\n     // currently doesn't support stateless, so this\n     // parameters are set to the default values.\n     auto is_stateless = rewriter.getBoolAttr(false);\n-    auto new_while = rewriter.create<TFL::WhileOp>(\n-        while_op.getLoc(), while_op->getResultTypes(), while_op->getOperands(),\n-        /*is_stateless=*/is_stateless);\n+    auto new_while = TFL::WhileOp::create(rewriter, while_op.getLoc(),\n+                                          while_op->getResultTypes(),\n+                                          while_op->getOperands(),\n+                                          /*is_stateless=*/is_stateless);\n     new_while.getCond().takeBody(while_op.getCond());\n     new_while.getBody().takeBody(while_op.getBody());\n     TFLReplaceReturnOp(new_while.getCond(), rewriter);"
        },
        {
            "sha": "c7f88bb2ebeebcafcc3020807c561136f903a84b",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_stablehlo_custom_call_to_composite.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_stablehlo_custom_call_to_composite.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_stablehlo_custom_call_to_composite.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_stablehlo_custom_call_to_composite.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -69,9 +69,9 @@ struct ReplaceCustomCallWithComposite final\n \n     auto decomposition = mlir::cast<FlatSymbolRefAttr>(calledComputations[0]);\n \n-    auto composite = rewriter.create<mlir::stablehlo::CompositeOp>(\n-        op.getLoc(), op.getResultTypes(), op.getOperands(), name.str(), attrs,\n-        decomposition.getValue());\n+    auto composite = mlir::stablehlo::CompositeOp::create(\n+        rewriter, op.getLoc(), op.getResultTypes(), op.getOperands(),\n+        name.str(), attrs, decomposition.getValue());\n     rewriter.replaceOp(op, composite.getResults());\n     return success();\n   }"
        },
        {
            "sha": "836598d19a7516df28924e52a44b21e3bd9c136d",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_stablehlo_to_vhlo.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_stablehlo_to_vhlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_stablehlo_to_vhlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_stablehlo_to_vhlo.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -59,7 +59,7 @@ namespace {\n \n Value MaterializeIllegalCast(OpBuilder &builder, Type type,\n                                             ValueRange inputs, Location loc) {\n-  return builder.create<UnrealizedConversionCastOp>(loc, type, inputs)\n+  return UnrealizedConversionCastOp::create(builder, loc, type, inputs)\n       ->getResult(0);\n }\n "
        },
        {
            "sha": "614bd07074826779a49d696eb8decc067e6e9634",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_tf_xla_call_module_to_stablehlo_pass.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_tf_xla_call_module_to_stablehlo_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_tf_xla_call_module_to_stablehlo_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Flegalize_tf_xla_call_module_to_stablehlo_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -177,8 +177,8 @@ class ConvertTFXlaCallModuleOp : public OpRewritePattern<TF::XlaCallModuleOp> {\n \n     SmallVector<Value, 4> call_op_operands(op.getOperands());\n     if (ContainsPlatformIndexArg(op)) {\n-      Value dummy_const = rewriter.create<TF::ConstOp>(\n-          op.getLoc(),\n+      Value dummy_const = TF::ConstOp::create(\n+          rewriter, op.getLoc(),\n           DenseIntElementsAttr::get(\n               RankedTensorType::get({}, rewriter.getIntegerType(32)), {0}));\n       call_op_operands.insert(call_op_operands.begin(), dummy_const);\n@@ -196,16 +196,16 @@ class ConvertTFXlaCallModuleOp : public OpRewritePattern<TF::XlaCallModuleOp> {\n       Value operand = std::get<0>(operand_and_type);\n       Type expected_type = std::get<1>(operand_and_type);\n       if (operand.getType() != expected_type) {\n-        operand = rewriter.create<TF::CastOp>(\n-            op.getLoc(), expected_type, operand,\n-            /*Truncate=*/rewriter.getBoolAttr(false));\n+        operand =\n+            TF::CastOp::create(rewriter, op.getLoc(), expected_type, operand,\n+                               /*Truncate=*/rewriter.getBoolAttr(false));\n       }\n       casted_operands.push_back(operand);\n     }\n \n-    auto call = rewriter.create<func::CallOp>(\n-        op->getLoc(), main_fn.getSymName(), main_fn.getResultTypes(),\n-        casted_operands);\n+    auto call =\n+        func::CallOp::create(rewriter, op->getLoc(), main_fn.getSymName(),\n+                             main_fn.getResultTypes(), casted_operands);\n     rewriter.replaceOp(op, call->getResults());\n \n     return success();"
        },
        {
            "sha": "1effffd9aa00e3d2605108804d06816cbe3c8c51",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/optimize.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Foptimize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Foptimize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Foptimize.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -269,9 +269,9 @@ LogicalResult LiftDotConcatLHS(mhlo::ConcatenateOp concat,\n         mlir::dyn_cast<ShapedType>(v.getType()).getShape()[new_concat_dim];\n   }\n \n-  auto new_concat = rewriter.create<mhlo::ConcatenateOp>(\n-      concat->getLoc(), concat.getType().clone(new_concat_shape), all_dot_lhs,\n-      rewriter.getI64IntegerAttr(new_concat_dim));\n+  auto new_concat = mhlo::ConcatenateOp::create(\n+      rewriter, concat->getLoc(), concat.getType().clone(new_concat_shape),\n+      all_dot_lhs, rewriter.getI64IntegerAttr(new_concat_dim));\n   rewriter.replaceOpWithNewOp<mhlo::DotGeneralOp>(\n       concat, concat.getType(), new_concat, first_dot.getRhs(),\n       first_dot.getDotDimensionNumbers(), first_dot.getPrecisionConfigAttr(),\n@@ -368,11 +368,11 @@ LogicalResult LiftDotConcatLHSAndRHS(mhlo::ConcatenateOp concat,\n         mlir::dyn_cast<ShapedType>(v.getType()).getShape()[rhs_batch_dim];\n   }\n \n-  auto lhs_new_concat = rewriter.create<mhlo::ConcatenateOp>(\n-      concat->getLoc(), concat.getType().clone(lhs_new_concat_shape),\n+  auto lhs_new_concat = mhlo::ConcatenateOp::create(\n+      rewriter, concat->getLoc(), concat.getType().clone(lhs_new_concat_shape),\n       all_dot_lhs, rewriter.getI64IntegerAttr(lhs_batch_dim));\n-  auto rhs_new_concat = rewriter.create<mhlo::ConcatenateOp>(\n-      concat->getLoc(), concat.getType().clone(rhs_new_concat_shape),\n+  auto rhs_new_concat = mhlo::ConcatenateOp::create(\n+      rewriter, concat->getLoc(), concat.getType().clone(rhs_new_concat_shape),\n       all_dot_rhs, rewriter.getI64IntegerAttr(rhs_batch_dim));\n   rewriter.replaceOpWithNewOp<mhlo::DotGeneralOp>(\n       concat, concat.getType(), lhs_new_concat, rhs_new_concat,\n@@ -439,7 +439,8 @@ LogicalResult FuseSliceConcat(mhlo::ConcatenateOp concat,\n     new_slice_shape.push_back(second_limit - first_start);\n   }\n \n-  auto new_slice = rewriter.create<mhlo::SliceOp>(\n+  auto new_slice = mhlo::SliceOp::create(\n+      rewriter,\n       FusedLoc::get(first->getContext(), {first.getLoc(), second.getLoc()}),\n       first.getType().clone(new_slice_shape), first.getOperand(),\n       /*start_indices=*/rewriter.getI64TensorAttr(new_start),\n@@ -730,8 +731,8 @@ class SimplifyBroadcastInDimsReshape\n \n     auto new_broadcast_input_type = RankedTensorType::get(\n         new_broadcast_input_shape, broadcast_type.getElementType());\n-    auto new_broadcast_input = rewriter.create<mhlo::ReshapeOp>(\n-        op->getLoc(), new_broadcast_input_type, op.getOperand());\n+    auto new_broadcast_input = mhlo::ReshapeOp::create(\n+        rewriter, op->getLoc(), new_broadcast_input_type, op.getOperand());\n     auto new_broadcast_dims_attr =\n         rewriter.getI64TensorAttr(new_broadcast_dims);\n "
        },
        {
            "sha": "13f981c8714f46d00d2bc0424df74d41728feac2",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/smuggle_disallowed_ops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fsmuggle_disallowed_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fsmuggle_disallowed_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fsmuggle_disallowed_ops.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -44,8 +44,8 @@ LogicalResult SmuggleOp(Operation* op, PatternRewriter& rewriter) {\n       rewriter.getNamedAttr(\"call_target_name\", op->getName().getIdentifier());\n   SmallVector<NamedAttribute> attrs{op->getAttrs()};\n   attrs.push_back(call_target);\n-  auto custom_call = rewriter.create<mlir::stablehlo::CustomCallOp>(\n-      op->getLoc(), op->getResultTypes(), op->getOperands(), attrs);\n+  auto custom_call = mlir::stablehlo::CustomCallOp::create(\n+      rewriter, op->getLoc(), op->getResultTypes(), op->getOperands(), attrs);\n   rewriter.replaceOp(op, custom_call.getResults());\n   return success();\n }"
        },
        {
            "sha": "557b721bfaf35f7c4bd7e51f0dd7dbb8351a99e5",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/stablehlo_fuse_convolution_pass.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fstablehlo_fuse_convolution_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fstablehlo_fuse_convolution_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fstablehlo_fuse_convolution_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -143,13 +143,15 @@ class FuseStablehloMulAndConvolutionPattern\n       broadcast_dims =\n           DenseI64ArrayAttr::get(rewriter.getContext(), {filter_rank - 1});\n     }\n-    Value broadcast_multiplier = rewriter.create<stablehlo::BroadcastInDimOp>(\n-        mul_op.getLoc(), filter.getType(), multiplier, broadcast_dims);\n-    Value new_filter = rewriter.create<stablehlo::MulOp>(\n-        mul_op.getLoc(), filter.getType(), filter, broadcast_multiplier);\n-    Value new_conv = rewriter.create<stablehlo::ConvolutionOp>(\n-        mul_op.getLoc(), conv_op.getType(), conv_op.getLhs(), new_filter,\n-        conv_op.getWindowStridesAttr(), conv_op.getPaddingAttr(),\n+    Value broadcast_multiplier = stablehlo::BroadcastInDimOp::create(\n+        rewriter, mul_op.getLoc(), filter.getType(), multiplier,\n+        broadcast_dims);\n+    Value new_filter =\n+        stablehlo::MulOp::create(rewriter, mul_op.getLoc(), filter.getType(),\n+                                 filter, broadcast_multiplier);\n+    Value new_conv = stablehlo::ConvolutionOp::create(\n+        rewriter, mul_op.getLoc(), conv_op.getType(), conv_op.getLhs(),\n+        new_filter, conv_op.getWindowStridesAttr(), conv_op.getPaddingAttr(),\n         conv_op.getLhsDilationAttr(), conv_op.getRhsDilationAttr(),\n         conv_op.getWindowReversalAttr(), conv_op.getDimensionNumbers(),\n         conv_op.getFeatureGroupCount(), conv_op.getBatchGroupCount(),\n@@ -169,8 +171,8 @@ class FuseStablehloMulAndConvolutionPattern\n               conv_op) {\n         return failure();\n       }\n-      Value new_shape_of = rewriter.create<shape::ShapeOfOp>(\n-          mul_op.getLoc(), shape_of_op.getType(), new_conv);\n+      Value new_shape_of = shape::ShapeOfOp::create(\n+          rewriter, mul_op.getLoc(), shape_of_op.getType(), new_conv);\n       shape_of_op.replaceAllUsesWith(new_shape_of);\n       rewriter.replaceOp(mul_op, {new_conv});\n     }"
        },
        {
            "sha": "b283dea3098232b2fd9cb5d35d2ebd6b1fc4ba93",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/tflite_legalize_hlo.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Ftflite_legalize_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Ftflite_legalize_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Ftflite_legalize_hlo.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -69,7 +69,7 @@ arith::ConstantOp ShapeToConst(PatternRewriter& rewriter, Value value) {\n   auto attr_type = RankedTensorType::get({static_cast<int64_t>(shape.size())},\n                                          rewriter.getIntegerType(64));\n   auto attr = DenseElementsAttr::get(attr_type, shape);\n-  return rewriter.create<arith::ConstantOp>(value.getLoc(), attr_type, attr);\n+  return arith::ConstantOp::create(rewriter, value.getLoc(), attr_type, attr);\n }\n \n // Returns true if broadcast_dimensions obey Tensorflow convention, as in new\n@@ -107,7 +107,7 @@ arith::ConstantOp ExpandedShape(OpBuilder& b, Value input,\n   auto attr_type = RankedTensorType::get(\n       {static_cast<int64_t>(expanded_shape.size())}, b.getIntegerType(32));\n   auto attr = DenseElementsAttr::get(attr_type, expanded_shape);\n-  return b.create<arith::ConstantOp>(output.getLoc(), attr_type, attr);\n+  return arith::ConstantOp::create(b, output.getLoc(), attr_type, attr);\n }\n \n Value ExpandedDynamicShape(OpBuilder& b, Value input,\n@@ -132,7 +132,7 @@ Value ExpandedDynamicShape(OpBuilder& b, Value input,\n   for (int64_t i : expanded_dimensions) {\n     auto index_attr = DenseIntElementsAttr::get(\n         RankedTensorType::get({}, b.getI64Type()), {i});\n-    Value index = b.create<arith::ConstantOp>(output.getLoc(), index_attr);\n+    Value index = arith::ConstantOp::create(b, output.getLoc(), index_attr);\n \n     auto cur_type = llvm::cast<ShapedType>(expanded_input.getType());\n     auto cur_shape = cur_type.getShape();\n@@ -145,8 +145,8 @@ Value ExpandedDynamicShape(OpBuilder& b, Value input,\n \n     auto new_type = RankedTensorType::get(new_shape, cur_type.getElementType());\n \n-    expanded_input = b.create<TFL::ExpandDimsOp>(output.getLoc(), new_type,\n-                                                 expanded_input, index);\n+    expanded_input = TFL::ExpandDimsOp::create(b, output.getLoc(), new_type,\n+                                               expanded_input, index);\n   }\n \n   return expanded_input;"
        },
        {
            "sha": "b5aded528cdc25ea2bc05e4d80ab7160fb5b6de6",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/unfold_splat_constant_pass.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Funfold_splat_constant_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Funfold_splat_constant_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Funfold_splat_constant_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -74,18 +74,17 @@ class UnfoldSplatConstantPass\n       return;\n     }\n     op_builder->setInsertionPoint(const_op);\n-    Value scalar = op_builder->create<mhlo::ConstantOp>(\n-        const_op->getLoc(),\n+    Value scalar = mhlo::ConstantOp::create(\n+        *op_builder, const_op->getLoc(),\n         DenseElementsAttr::get(\n             RankedTensorType::get(/*shape=*/{}, element_type),\n             splat_elements_attr.getSplatValue<Attribute>()));\n     auto broadcast_dims = DenseIntElementsAttr::get(\n         RankedTensorType::get(/*shape=*/{0}, op_builder->getI64Type()),\n         llvm::SmallVector<int64_t>{});\n-    mhlo::BroadcastInDimOp broadcast_in_dim_op =\n-        op_builder->create<mhlo::BroadcastInDimOp>(\n-            const_op->getLoc(), splat_elements_attr.getType(), scalar,\n-            broadcast_dims);\n+    mhlo::BroadcastInDimOp broadcast_in_dim_op = mhlo::BroadcastInDimOp::create(\n+        *op_builder, const_op->getLoc(), splat_elements_attr.getType(), scalar,\n+        broadcast_dims);\n     const_op->replaceAllUsesWith(broadcast_in_dim_op);\n     const_op->erase();\n   }"
        },
        {
            "sha": "e04be6148b7b1def6b24c93cb475887e8d14e705",
            "filename": "tensorflow/compiler/mlir/lite/transforms/decompose_hybrid_quantization.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fdecompose_hybrid_quantization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fdecompose_hybrid_quantization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fdecompose_hybrid_quantization.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -93,7 +93,7 @@ class DequantizeConverter : public OpRewritePattern<SrcOp> {\n       if (QuantizedType::getQuantizedElementType(operand.getType())) {\n         auto newTy = QuantizedType::castToExpressedType(operand.getType());\n         newOperands.push_back(\n-            rewriter.create<TFL::DequantizeOp>(loc, newTy, operand));\n+            TFL::DequantizeOp::create(rewriter, loc, newTy, operand));\n         continue;\n       }\n \n@@ -109,9 +109,8 @@ class DequantizeConverter : public OpRewritePattern<SrcOp> {\n       newResultTys.push_back(resultTy);\n     }\n \n-    auto newResults = rewriter\n-                          .create<SrcOp>(loc, newResultTys, newOperands,\n-                                         op->getAttrDictionary().getValue())\n+    auto newResults = SrcOp::create(rewriter, loc, newResultTys, newOperands,\n+                                    op->getAttrDictionary().getValue())\n                           .getOperation()\n                           ->getResults();\n \n@@ -120,8 +119,8 @@ class DequantizeConverter : public OpRewritePattern<SrcOp> {\n       Value result = newResults[i];\n       Type resultTy = op->getOpResult(i).getType();\n       if (QuantizedType::getQuantizedElementType(resultTy)) {\n-        replaceResults.push_back(rewriter.create<TFL::QuantizeOp>(\n-            loc, resultTy, result, TypeAttr::get(resultTy)));\n+        replaceResults.push_back(TFL::QuantizeOp::create(\n+            rewriter, loc, resultTy, result, TypeAttr::get(resultTy)));\n         continue;\n       }\n "
        },
        {
            "sha": "c45d5f74b8988d4c631d296fe48933a2b269b45d",
            "filename": "tensorflow/compiler/mlir/lite/transforms/if_outline.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fif_outline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fif_outline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fif_outline.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -84,7 +84,7 @@ func::FuncOp CreateOutlineFuncAndEraseRegion(\n   type = FunctionType::get(context, types, result_types);\n \n   // Create outlined function and move region body to it.\n-  auto outlined_func = func_builder.create<func::FuncOp>(loc, name, type);\n+  auto outlined_func = func::FuncOp::create(func_builder, loc, name, type);\n   outlined_func.getBody().takeBody(region);\n   Region& func_region = outlined_func.getBody();\n \n@@ -97,8 +97,8 @@ func::FuncOp CreateOutlineFuncAndEraseRegion(\n   // Replace yield op with return.\n   Operation* yield_op = outlined_func.getBody().front().getTerminator();\n   OpBuilder return_builder(yield_op);\n-  return_builder.create<func::ReturnOp>(yield_op->getLoc(),\n-                                        yield_op->getOperands());\n+  func::ReturnOp::create(return_builder, yield_op->getLoc(),\n+                         yield_op->getOperands());\n   yield_op->erase();\n \n   SymbolTable(region.getParentOfType<ModuleOp>()).insert(outlined_func);\n@@ -121,8 +121,8 @@ void ReplaceRegionWithCall(StringRef name, Region& region,\n     new_operands.push_back(block->addArgument(t, loc));\n   }\n   new_operands.append(extern_values.begin(), extern_values.end());\n-  auto call = b.create<func::CallOp>(loc, func, new_operands);\n-  b.create<YieldOp>(loc, call.getResults());\n+  auto call = func::CallOp::create(b, loc, func, new_operands);\n+  YieldOp::create(b, loc, call.getResults());\n }\n \n void IfOutlinePass::OutlineIf(IfOp if_op) {"
        },
        {
            "sha": "7a85d60b51d6eba4dd8da9fa2d24e2a18e870e6c",
            "filename": "tensorflow/compiler/mlir/lite/transforms/insert_call_once_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Finsert_call_once_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Finsert_call_once_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Finsert_call_once_op.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -51,7 +51,7 @@ void InsertCallOnceOpFromSessionInitializerPass::runOnOperation() {\n \n       OpBuilder builder(func.getContext());\n       builder.setInsertionPointToStart(&func.getBlocks().front());\n-      builder.create<TFL::CallOnceOp>(func.getLoc(), init_func_op.getName());\n+      TFL::CallOnceOp::create(builder, func.getLoc(), init_func_op.getName());\n     }\n   }\n }"
        },
        {
            "sha": "668493eca931e7a74bb7629660abc6dbc1a8e86d",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_batch_matmul_pass.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 25,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_batch_matmul_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_batch_matmul_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_batch_matmul_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -135,8 +135,8 @@ struct ConvertBatchMatMulOp2FullyConnectedOp_Rank2ConstantRhs\n       // mapped to X and Z dimension.\n       std::iter_swap(permute.begin() + input_rank - 1,\n                      permute.begin() + input_rank - 2);\n-      auto permutation_tensor_op = rewriter.create<arith::ConstantOp>(\n-          bmm_op->getLoc(), permuation_tensor_type,\n+      auto permutation_tensor_op = arith::ConstantOp::create(\n+          rewriter, bmm_op->getLoc(), permuation_tensor_type,\n           DenseElementsAttr::get(permuation_tensor_type, permute));\n \n       auto input_shape = input_type.getShape();\n@@ -181,9 +181,8 @@ struct ConvertBatchMatMulOp2FullyConnectedOp_Rank2ConstantRhs\n             RankedTensorType::get(permuted_shape, input_type.getElementType());\n       }\n \n-      return rewriter.create<TFL::TransposeOp>(\n-          bmm_op->getLoc(), output_type, input,\n-          permutation_tensor_op.getResult());\n+      return TFL::TransposeOp::create(rewriter, bmm_op->getLoc(), output_type,\n+                                      input, permutation_tensor_op.getResult());\n     };\n \n     Value input_lhs = bmm_op.getX();\n@@ -198,10 +197,11 @@ struct ConvertBatchMatMulOp2FullyConnectedOp_Rank2ConstantRhs\n         !bmm_op.getAdjY() ? create_z_x_transpose_op(input_rhs) : input_rhs;\n \n     Type output_type = bmm_op.getResult().getType();\n-    auto no_input = rewriter.create<TFL::NoValueOp>(\n-        bmm_op->getLoc(), rewriter.getNoneType(), rewriter.getUnitAttr());\n-    auto fc_op = rewriter.create<TFL::FullyConnectedOp>(\n-        bmm_op->getLoc(), ArrayRef<Type>{output_type},\n+    auto no_input =\n+        TFL::NoValueOp::create(rewriter, bmm_op->getLoc(),\n+                               rewriter.getNoneType(), rewriter.getUnitAttr());\n+    auto fc_op = TFL::FullyConnectedOp::create(\n+        rewriter, bmm_op->getLoc(), ArrayRef<Type>{output_type},\n         /*input=*/output_lhs, /*filter=*/output_rhs, /*bias=*/no_input,\n         /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n         /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n@@ -257,13 +257,14 @@ struct ConvertBatchMatMulOpToReduceSum\n       cY = rhs_shape.size() - 1;\n     }\n \n-    auto reduce_dim_op = rewriter.create<TFL::ConstOp>(\n-        bmm_op->getLoc(),\n+    auto reduce_dim_op = TFL::ConstOp::create(\n+        rewriter, bmm_op->getLoc(),\n         DenseIntElementsAttr::get(\n             RankedTensorType::get({1}, rewriter.getI32Type()), {cY}));\n-    auto sum_op = rewriter.create<TFL::SumOp>(\n-        bmm_op->getLoc(), bmm_op.getType(), bmm_op.getY(), reduce_dim_op,\n-        /*keep_dims=*/rewriter.getBoolAttr(true));\n+    auto sum_op =\n+        TFL::SumOp::create(rewriter, bmm_op->getLoc(), bmm_op.getType(),\n+                           bmm_op.getY(), reduce_dim_op,\n+                           /*keep_dims=*/rewriter.getBoolAttr(true));\n     rewriter.replaceOp(bmm_op, sum_op);\n     return success();\n   };\n@@ -368,19 +369,21 @@ struct FuseRhsTransposeIntoBatchMatMulOp\n     new_reshape_input_shape.push_back(\n         rhs_contracting_dimensions.SizesArray().front());\n \n-    Value new_reshape_shape_value = rewriter.create<arith::ConstantOp>(\n-        bmm_op->getLoc(),\n+    Value new_reshape_shape_value = arith::ConstantOp::create(\n+        rewriter, bmm_op->getLoc(),\n         GetI32ElementsAttr(new_reshape_input_shape, &rewriter));\n-    auto new_reshape_value = rewriter.create<TFL::ReshapeOp>(\n-        bmm_op->getLoc(), transpose_op.getInput(), new_reshape_shape_value);\n+    auto new_reshape_value = TFL::ReshapeOp::create(rewriter, bmm_op->getLoc(),\n+                                                    transpose_op.getInput(),\n+                                                    new_reshape_shape_value);\n \n     // Replace the BatchMatMulOp with a FullyConnectedOp, if the RHS of BMM has\n     // no broadcasting dimensions. I.e. RHS of BMM is of Rank 2.\n     if (rhs_dimensions_info.batch_dimensions().AxesArray().empty()) {\n-      auto no_input = rewriter.create<TFL::NoValueOp>(\n-          bmm_op->getLoc(), rewriter.getNoneType(), rewriter.getUnitAttr());\n-      auto fc_op = rewriter.create<TFL::FullyConnectedOp>(\n-          bmm_op->getLoc(), ArrayRef<Type>{bmm_op.getType()},\n+      auto no_input = TFL::NoValueOp::create(rewriter, bmm_op->getLoc(),\n+                                             rewriter.getNoneType(),\n+                                             rewriter.getUnitAttr());\n+      auto fc_op = TFL::FullyConnectedOp::create(\n+          rewriter, bmm_op->getLoc(), ArrayRef<Type>{bmm_op.getType()},\n           /*input=*/bmm_op.getX(), /*filter=*/new_reshape_value,\n           /*bias=*/no_input,\n           /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n@@ -391,9 +394,10 @@ struct FuseRhsTransposeIntoBatchMatMulOp\n     } else {\n       // Replace the BatchMatMulOp with a BatchMatMulOp with adj_y = true and\n       // transpose fused into RHS.\n-      auto bmm_op_with_adj_y = rewriter.create<TFL::BatchMatMulOp>(\n-          bmm_op->getLoc(), bmm_op.getType(), bmm_op.getX(), new_reshape_value,\n-          bmm_op.getAdjX(), /*adj_y=*/true, mlir::BoolAttr());\n+      auto bmm_op_with_adj_y = TFL::BatchMatMulOp::create(\n+          rewriter, bmm_op->getLoc(), bmm_op.getType(), bmm_op.getX(),\n+          new_reshape_value, bmm_op.getAdjX(), /*adj_y=*/true,\n+          mlir::BoolAttr());\n       rewriter.replaceOp(bmm_op, {bmm_op_with_adj_y.getResult()});\n     }\n "
        },
        {
            "sha": "21b1963998d0d5a82581b53ad047ae501ed3fc6c",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_broadcast_like_pass.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_broadcast_like_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_broadcast_like_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_broadcast_like_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -131,8 +131,9 @@ Value PrepareBroadcastLikeOpInput(Operation* op, PatternRewriter& rewriter) {\n         RankedTensorType::get({}, elements_attr.getType().getElementType()),\n         elements_attr.getSplatValue<mlir::Attribute>());\n \n-    return rewriter.create<arith::ConstantOp>(\n-        op->getLoc(), scalar_elements_attr.getType(), scalar_elements_attr);\n+    return arith::ConstantOp::create(rewriter, op->getLoc(),\n+                                     scalar_elements_attr.getType(),\n+                                     scalar_elements_attr);\n   }\n   return nullptr;\n }\n@@ -380,10 +381,10 @@ LogicalResult ReorderBroadcastToCast::matchAndRewrite(\n           : static_cast<TensorType>(UnrankedTensorType::get(\n                 old_cast_op_output_type.getElementType()));\n \n-  auto new_cast_op = rewriter.create<TFL::CastOp>(\n-      fused_loc, new_cast_op_output_type, input_value);\n-  auto new_broadcast_to_op = rewriter.create<TFL::BroadcastToOp>(\n-      fused_loc, old_cast_op_output_type, new_cast_op.getOutput(),\n+  auto new_cast_op = TFL::CastOp::create(rewriter, fused_loc,\n+                                         new_cast_op_output_type, input_value);\n+  auto new_broadcast_to_op = TFL::BroadcastToOp::create(\n+      rewriter, fused_loc, old_cast_op_output_type, new_cast_op.getOutput(),\n       broadcast_to_op.getShape());\n \n   rewriter.replaceOp(cast_op, new_broadcast_to_op.getOutput());"
        },
        {
            "sha": "062d9c1e712de276d18cd994fa831f9737eac82c",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_pass.cc",
            "status": "modified",
            "additions": 140,
            "deletions": 127,
            "changes": 267,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -502,8 +502,8 @@ Value GetBiasMultiplier(OpBuilder& builder, Value binary_op,\n       (llvm::isa<mlir::TFL::AddOp>(binary_op.getDefiningOp()) ? 1.0 : -1.0);\n   Attribute constant_attr = FloatAttr::get(element_type, multiplier);\n \n-  return builder.create<arith::ConstantOp>(\n-      binary_op.getLoc(),\n+  return arith::ConstantOp::create(\n+      builder, binary_op.getLoc(),\n       DenseFPElementsAttr::get(RankedTensorType::get({}, element_type),\n                                constant_attr));\n }\n@@ -677,10 +677,10 @@ Value ReshapeValueDroppingLastDim(OpBuilder& builder, Value value) {\n   } else {\n     new_shape.push_back(-1);\n   }\n-  return builder.create<ReshapeOp>(\n-      value.getLoc(), value,\n-      builder.create<arith::ConstantOp>(\n-          value.getLoc(),\n+  return ReshapeOp::create(\n+      builder, value.getLoc(), value,\n+      arith::ConstantOp::create(\n+          builder, value.getLoc(),\n           DenseIntElementsAttr::get(\n               RankedTensorType::get(type.getRank() - 1, builder.getI32Type()),\n               new_shape)));\n@@ -754,9 +754,8 @@ Value Get1DShapeValue(OpBuilder& builder, Value value) {\n   }\n   auto output_type = RankedTensorType::get({1}, builder.getI32Type());\n   const int num_elements = type.getNumElements();\n-  return builder.create<ConstOp>(\n-      value.getLoc(), output_type,\n-      DenseIntElementsAttr::get(output_type, num_elements));\n+  return ConstOp::create(builder, value.getLoc(), output_type,\n+                         DenseIntElementsAttr::get(output_type, num_elements));\n }\n \n Type GetEmbeddingLookupShape(Value lookup, Value value) {\n@@ -780,8 +779,8 @@ mlir::Value GetFcOutput(OpBuilder* builder,\n                         StringAttr fused_activation_function,\n                         StringAttr weights_format, BoolAttr keep_num_dims,\n                         BoolAttr asymmetric_quantize_inputs) {\n-  auto fc_op = builder->create<FullyConnectedOp>(\n-      result[0].getLoc(), result.getTypes(), input, filter, bias,\n+  auto fc_op = FullyConnectedOp::create(\n+      *builder, result[0].getLoc(), result.getTypes(), input, filter, bias,\n       fused_activation_function, weights_format, keep_num_dims,\n       asymmetric_quantize_inputs);\n   return fc_op->getResult(0);\n@@ -973,13 +972,13 @@ struct SqueezeReshapesAroundBroadcastOp\n             .drop_back(num_trailing_broadcast_dims)\n             .drop_front(num_leading_broadcast_dims)};\n \n-    Value new_reshape_shape_value = rewriter.create<arith::ConstantOp>(\n-        inner_reshape_op->getLoc(),\n+    Value new_reshape_shape_value = arith::ConstantOp::create(\n+        rewriter, inner_reshape_op->getLoc(),\n         GetI32ElementsAttr(new_reshape_shape_i32, &rewriter));\n \n-    auto new_inner_reshape_op = rewriter.create<TFL::ReshapeOp>(\n-        inner_reshape_op->getLoc(), inner_reshape_input,\n-        new_reshape_shape_value);\n+    auto new_inner_reshape_op =\n+        TFL::ReshapeOp::create(rewriter, inner_reshape_op->getLoc(),\n+                               inner_reshape_input, new_reshape_shape_value);\n \n     // Create a new reshape_op to replace the old inner reshape_op.\n     rewriter.replaceOp(inner_reshape_op, new_inner_reshape_op.getResult());\n@@ -990,11 +989,12 @@ struct SqueezeReshapesAroundBroadcastOp\n             .drop_back(num_trailing_broadcast_dims)\n             .drop_front(num_leading_broadcast_dims)};\n \n-    Value new_broadcast_shape_value = rewriter.create<arith::ConstantOp>(\n-        loc, GetI64ElementsAttr(new_broadcast_shape, &rewriter));\n+    Value new_broadcast_shape_value = arith::ConstantOp::create(\n+        rewriter, loc, GetI64ElementsAttr(new_broadcast_shape, &rewriter));\n \n-    auto new_broadcast_to_op = rewriter.create<TFL::BroadcastToOp>(\n-        loc, RankedTensorType::get(new_broadcast_shape, rewriter.getF32Type()),\n+    auto new_broadcast_to_op = TFL::BroadcastToOp::create(\n+        rewriter, loc,\n+        RankedTensorType::get(new_broadcast_shape, rewriter.getF32Type()),\n         new_inner_reshape_op.getOutput(), new_broadcast_shape_value);\n \n     // Create a new broadcast_op to replace the old broadcast_op.\n@@ -1055,18 +1055,19 @@ struct FuseAddAndStridedSlice : public OpRewritePattern<TFL::StridedSliceOp> {\n         added_value.reshape(RankedTensorType::get(\n             {num_dims},\n             mlir::cast<ShapedType>(added_value.getType()).getElementType()));\n-    ::mlir::arith::ConstantOp new_end = rewriter.create<arith::ConstantOp>(\n-        strided_slice_op.getEnd().getLoc(), new_added_value);\n+    ::mlir::arith::ConstantOp new_end = arith::ConstantOp::create(\n+        rewriter, strided_slice_op.getEnd().getLoc(), new_added_value);\n \n     if (strided_slice_op.getBeginMask() != 0) return failure();\n     if (strided_slice_op.getEndMask() != 0) return failure();\n     if (strided_slice_op.getEllipsisMask() != 0) return failure();\n     mlir::TFL::StridedSliceOp new_strided_slice_op =\n-        rewriter.create<TFL::StridedSliceOp>(\n-            strided_slice_op.getLoc(), strided_slice_op.getOutput().getType(),\n-            strided_slice_op.getInput(), strided_slice_op.getBegin(), new_end,\n-            strided_slice_op.getStrides(), strided_slice_op.getBeginMask(),\n-            strided_slice_op.getEndMask(), strided_slice_op.getEllipsisMask(),\n+        TFL::StridedSliceOp::create(\n+            rewriter, strided_slice_op.getLoc(),\n+            strided_slice_op.getOutput().getType(), strided_slice_op.getInput(),\n+            strided_slice_op.getBegin(), new_end, strided_slice_op.getStrides(),\n+            strided_slice_op.getBeginMask(), strided_slice_op.getEndMask(),\n+            strided_slice_op.getEllipsisMask(),\n             strided_slice_op.getNewAxisMask(),\n             strided_slice_op.getShrinkAxisMask(),\n             /*offset=*/true);\n@@ -1186,24 +1187,26 @@ struct Convert2DUpscalingToResizeNearestNeighor\n     SmallVector<int64_t, 4> reshape_shape_in_int64(\n         {1, image_size, image_size, feature_size});\n \n-    auto reshape_shape_const_op = rewriter.create<TFL::ConstOp>(\n-        gather_nd_first->getLoc(),\n-        GetI32ElementsAttr(reshape_shape, &rewriter));\n+    auto reshape_shape_const_op =\n+        TFL::ConstOp::create(rewriter, gather_nd_first->getLoc(),\n+                             GetI32ElementsAttr(reshape_shape, &rewriter));\n \n-    auto reshape_op = rewriter.create<TFL::ReshapeOp>(\n-        gather_nd_first->getLoc(),\n+    auto reshape_op = TFL::ReshapeOp::create(\n+        rewriter, gather_nd_first->getLoc(),\n         tensorflow::GetTypeFromTFTensorShape(reshape_shape_in_int64,\n                                              result_type.getElementType()),\n         params_value, reshape_shape_const_op.getResult());\n \n     // Add TFL::resize_nearest_neighor op for 2x upscaling.\n     SmallVector<int32_t, 2> size_vec = {image_size * 2, image_size * 2};\n-    auto size_const_op = rewriter.create<TFL::ConstOp>(\n-        gather_nd_first->getLoc(), GetI32ElementsAttr(size_vec, &rewriter));\n+    auto size_const_op =\n+        TFL::ConstOp::create(rewriter, gather_nd_first->getLoc(),\n+                             GetI32ElementsAttr(size_vec, &rewriter));\n \n-    auto resize = rewriter.create<TFL::ResizeNearestNeighborOp>(\n-        gather_nd_first->getLoc(), transpose_second.getResult().getType(),\n-        reshape_op.getResult(), size_const_op.getResult(), false, false);\n+    auto resize = TFL::ResizeNearestNeighborOp::create(\n+        rewriter, gather_nd_first->getLoc(),\n+        transpose_second.getResult().getType(), reshape_op.getResult(),\n+        size_const_op.getResult(), false, false);\n \n     rewriter.replaceOp(transpose_second, resize.getResult());\n     return success();\n@@ -1233,13 +1236,13 @@ static std::optional<Value> GetAs1DValue(PatternRewriter& rewriter, Value value,\n           RankedTensorType::get({num_channels}, type.getElementType());\n       auto splat_attr =\n           DenseElementsAttr::get(splat_type, attr.getSplatValue<Attribute>());\n-      return rewriter.create<arith::ConstantOp>(value.getLoc(), splat_attr);\n+      return arith::ConstantOp::create(rewriter, value.getLoc(), splat_attr);\n     }\n \n     if (HasOneTailUnitDimension(attr) &&\n         attr.getNumElements() == num_channels) {\n       auto flattened = FlattenTo1D(attr);\n-      return rewriter.create<arith::ConstantOp>(value.getLoc(), flattened);\n+      return arith::ConstantOp::create(rewriter, value.getLoc(), flattened);\n     }\n   }\n \n@@ -1259,7 +1262,7 @@ static std::optional<Value> GetBiasIn1D(PatternRewriter& rewriter, Value bias,\n     RankedTensorType type =\n         RankedTensorType::get({num_channels}, fallback_element_type);\n     auto attr = rewriter.getZeroAttr(type);\n-    return rewriter.create<arith::ConstantOp>(bias.getLoc(), type, attr);\n+    return arith::ConstantOp::create(rewriter, bias.getLoc(), type, attr);\n   }\n \n   auto bias_type = mlir::dyn_cast<RankedTensorType>(bias.getType());\n@@ -1377,34 +1380,34 @@ struct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n     }\n \n     auto new_bias =\n-        rewriter\n-            .create<AddOp>(add_op.getLoc(), bias_1d.value(), add_rhs_1d.value(),\n-                           rewriter.getStringAttr(\"NONE\"))\n+        AddOp::create(rewriter, add_op.getLoc(), bias_1d.value(),\n+                      add_rhs_1d.value(), rewriter.getStringAttr(\"NONE\"))\n             .getOutput();\n     mlir::Value out =\n-        rewriter\n-            .create<TFL::FullyConnectedOp>(\n-                mlir::FusedLoc::get(fc_op.getContext(),\n-                                    {fc_op.getLoc(), add_op.getLoc()}),\n-                fc_output_type,\n-                /*input=*/fc_op.getInput(),\n-                /*filter=*/filter,\n-                /*bias=*/new_bias,\n-                /*fused_activation_function=*/\n-                rewriter.getStringAttr(add_op.getFusedActivationFunction()),\n-                /*weights_format=*/\n-                rewriter.getStringAttr(fc_op.getWeightsFormat()),\n-                /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.getKeepNumDims()),\n-                /*asymmetric_quantize_inputs=*/\n-                fc_op.getAsymmetricQuantizeInputsAttr())\n+        TFL::FullyConnectedOp::create(\n+            rewriter,\n+            mlir::FusedLoc::get(fc_op.getContext(),\n+                                {fc_op.getLoc(), add_op.getLoc()}),\n+            fc_output_type,\n+            /*input=*/fc_op.getInput(),\n+            /*filter=*/filter,\n+            /*bias=*/new_bias,\n+            /*fused_activation_function=*/\n+            rewriter.getStringAttr(add_op.getFusedActivationFunction()),\n+            /*weights_format=*/\n+            rewriter.getStringAttr(fc_op.getWeightsFormat()),\n+            /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.getKeepNumDims()),\n+            /*asymmetric_quantize_inputs=*/\n+            fc_op.getAsymmetricQuantizeInputsAttr())\n             .getOutput()[0];\n \n     if (fc_output_type.getShape() != add_output_type.getShape()) {\n-      auto target_shape = rewriter.create<arith::ConstantOp>(\n-          add_op.getLoc(), rewriter.getI32TensorAttr(llvm::SmallVector<int32_t>(\n-                               add_output_type.getShape())));\n-      out = rewriter.create<ReshapeOp>(add_op.getLoc(), add_output_type, out,\n-                                       target_shape);\n+      auto target_shape = arith::ConstantOp::create(\n+          rewriter, add_op.getLoc(),\n+          rewriter.getI32TensorAttr(\n+              llvm::SmallVector<int32_t>(add_output_type.getShape())));\n+      out = ReshapeOp::create(rewriter, add_op.getLoc(), add_output_type, out,\n+                              target_shape);\n     }\n     rewriter.replaceOp(add_op, out);\n \n@@ -1471,8 +1474,8 @@ struct FuseAddAndFullyConnected\n       return failure();\n     }\n \n-    auto new_bias = rewriter.create<TFL::FullyConnectedOp>(\n-        fc_op.getLoc(), old_bias.getType(),\n+    auto new_bias = TFL::FullyConnectedOp::create(\n+        rewriter, fc_op.getLoc(), old_bias.getType(),\n         /*input=*/add_op.getRhs(),\n         /*filter=*/fc_op.getFilter(),\n         /*bias=*/old_bias,\n@@ -1482,7 +1485,8 @@ struct FuseAddAndFullyConnected\n         /*asymmetric_quantize_inputs=*/fc_op.getAsymmetricQuantizeInputsAttr());\n \n     // Create the updated FC.\n-    auto new_fc = rewriter.create<TFL::FullyConnectedOp>(\n+    auto new_fc = TFL::FullyConnectedOp::create(\n+        rewriter,\n         FusedLoc::get(add_op.getContext(), {add_op.getLoc(), fc_op.getLoc()}),\n         fc_op.getOutput().getTypes(),\n         /*input=*/add_op.getLhs(),\n@@ -1557,14 +1561,14 @@ struct FuseMulAndFullyConnected\n     auto location =\n         FusedLoc::get(mul_op.getContext(), {mul_op.getLoc(), fc_op.getLoc()});\n \n-    auto new_filter = rewriter.create<TFL::MulOp>(\n-        location,\n+    auto new_filter = TFL::MulOp::create(\n+        rewriter, location,\n         /*lhs=*/fc_op.getFilter(),\n         /*rhs=*/mul_op.getRhs(),\n         /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"));\n     // Create the updated FC.\n-    auto new_fc = rewriter.create<TFL::FullyConnectedOp>(\n-        location, fc_op.getOutput().getTypes(),\n+    auto new_fc = TFL::FullyConnectedOp::create(\n+        rewriter, location, fc_op.getOutput().getTypes(),\n         /*input=*/mul_op.getLhs(),\n         /*filter=*/new_filter,\n         /*bias=*/fc_op.getBias(),\n@@ -1597,7 +1601,8 @@ struct FuseFullyConnectedAndReluX : public OpRewritePattern<ReluXOp> {\n         rewriter.getStringAttr(fully_connected_op.getWeightsFormat());\n     auto new_keep_num_dims =\n         rewriter.getBoolAttr(fully_connected_op.getKeepNumDims());\n-    auto fc = rewriter.create<FullyConnectedOp>(\n+    auto fc = FullyConnectedOp::create(\n+        rewriter,\n         FusedLoc::get(relu_op.getContext(),\n                       {fully_connected_op.getLoc(), relu_op.getLoc()}),\n         relu_op.getType(), /*input=*/fully_connected_op.getInput(),\n@@ -1674,7 +1679,7 @@ struct FuseFullyConnectedAndMul : public OpRewritePattern<TFL::MulOp> {\n     }\n \n     auto new_op =\n-        rewriter.create<arith::ConstantOp>(mul_op.getLoc(), new_type, new_cst);\n+        arith::ConstantOp::create(rewriter, mul_op.getLoc(), new_type, new_cst);\n     Value new_const_val = new_op.getResult();\n \n     // Rewrite. Since the folder of TFL::MulOp couldn't broadcast the operands,\n@@ -1689,15 +1694,16 @@ struct FuseFullyConnectedAndMul : public OpRewritePattern<TFL::MulOp> {\n       if (size > (1 << 30)) return failure();\n     }\n     auto new_filter =\n-        rewriter.create<TF::MulOp>(mul_op.getLoc(), filter, new_const_val)\n+        TF::MulOp::create(rewriter, mul_op.getLoc(), filter, new_const_val)\n             .getZ();\n     // If bias isn't None, it needs to be multiplied as well.\n     if (!mlir::isa<NoneType>(bias.getType())) {\n-      bias = rewriter.create<TF::MulOp>(mul_op.getLoc(), bias, constant_val)\n+      bias = TF::MulOp::create(rewriter, mul_op.getLoc(), bias, constant_val)\n                  .getZ();\n     }\n \n-    auto fc = rewriter.create<TFL::FullyConnectedOp>(\n+    auto fc = TFL::FullyConnectedOp::create(\n+        rewriter,\n         FusedLoc::get(fc_op.getContext(), {fc_op.getLoc(), mul_op.getLoc()}),\n         mul_op.getType(),\n         /*input=*/fc_op.getInput(),\n@@ -1848,13 +1854,13 @@ struct FuseAffinOpAndMulWithQDQs : public OpRewritePattern<TFL::MulOp> {\n     DenseElementsAttr broadcasted_gamma_attr =\n         ExpandTo4DForConv(gamma_cst, filter_output_dim);\n     auto broadcasted_gamma =\n-        rewriter.create<ConstOp>(loc, broadcasted_gamma_attr);\n+        ConstOp::create(rewriter, loc, broadcasted_gamma_attr);\n \n     // Inject a mul between the filter constant and the quantize op.\n-    auto new_filter = rewriter\n-                          .create<TFL::MulOp>(loc, filter, broadcasted_gamma,\n-                                              rewriter.getStringAttr(\"NONE\"))\n-                          .getResult();\n+    auto new_filter =\n+        TFL::MulOp::create(rewriter, loc, filter, broadcasted_gamma,\n+                           rewriter.getStringAttr(\"NONE\"))\n+            .getResult();\n     // Update the scale in the quantize op.\n     auto new_qtype = RescaleQtype(q_op.getQtype(), gamma_cst);\n     if (!new_qtype) {\n@@ -1869,11 +1875,11 @@ struct FuseAffinOpAndMulWithQDQs : public OpRewritePattern<TFL::MulOp> {\n \n       auto squeezed_gamma = FlattenTo1D(gamma_cst);\n       auto squeezed_gamma_type = squeezed_gamma.getType();\n-      auto squeezed_gamma_op = rewriter.create<arith::ConstantOp>(\n-          affine_op.getLoc(), squeezed_gamma_type, squeezed_gamma);\n+      auto squeezed_gamma_op = arith::ConstantOp::create(\n+          rewriter, affine_op.getLoc(), squeezed_gamma_type, squeezed_gamma);\n \n-      auto new_bias = rewriter.create<TFL::MulOp>(\n-          loc, bias, squeezed_gamma_op, rewriter.getStringAttr(\"NONE\"));\n+      auto new_bias = TFL::MulOp::create(rewriter, loc, bias, squeezed_gamma_op,\n+                                         rewriter.getStringAttr(\"NONE\"));\n       affine_op.getOperation()->replaceUsesOfWith(bias, new_bias);\n     }\n \n@@ -1977,7 +1983,7 @@ struct FuseBinaryOpToFollowingAffineOp : public OpRewritePattern<AffineOpType> {\n       }\n       auto new_bias = DenseFPElementsAttr::get(new_bias_type, new_bias_values);\n       auto new_bias_op =\n-          rewriter.create<ConstOp>(fc_op.getLoc(), new_bias_type, new_bias);\n+          ConstOp::create(rewriter, fc_op.getLoc(), new_bias_type, new_bias);\n       fc_op.setOperand(0, binary_op->getOperand(0));\n       fc_op.setOperand(2, new_bias_op);\n     } else if (llvm::isa<MulOp, DivOp>(binary_op)) {\n@@ -1992,8 +1998,8 @@ struct FuseBinaryOpToFollowingAffineOp : public OpRewritePattern<AffineOpType> {\n           });\n       // We recreate the constant op in case it is shared by the other ops. This\n       // might increase the model size.\n-      auto new_filter_op = rewriter.create<ConstOp>(\n-          fc_op.getLoc(), filter.getType(), new_filter);\n+      auto new_filter_op = ConstOp::create(rewriter, fc_op.getLoc(),\n+                                           filter.getType(), new_filter);\n       fc_op.setOperand(0, binary_op->getOperand(0));\n       if (fc_op.getFilter() != filter) {\n         // This filter goes through quantize and dequantize ops. Then we just\n@@ -2186,8 +2192,9 @@ struct FuseUnpackAndConcatToReshape\n       new_shape_array_i32.push_back(\n           ShapedType::isDynamic(size) ? -1 : static_cast<int32_t>(size));\n     }\n-    auto new_shape = rewriter.create<TFL::ConstOp>(\n-        concat_op.getLoc(), GetI32ElementsAttr(new_shape_array_i32, &rewriter));\n+    auto new_shape = TFL::ConstOp::create(\n+        rewriter, concat_op.getLoc(),\n+        GetI32ElementsAttr(new_shape_array_i32, &rewriter));\n \n     rewriter.replaceOpWithNewOp<TFL::ReshapeOp>(\n         concat_op, output_type, unpack_op.getInput(), new_shape);\n@@ -2273,8 +2280,8 @@ struct OptimizeTopK : public OpRewritePattern<TFL::TopKV2Op> {\n     auto k = !values.use_empty() ? k_values : k_indices;\n     // Build scalar tensor k.\n     auto k_ty = mlir::RankedTensorType::get({}, rewriter.getIntegerType(32));\n-    Value k_cst = rewriter.create<TFL::ConstOp>(\n-        op.getLoc(), DenseElementsAttr::get(k_ty, k));\n+    Value k_cst = TFL::ConstOp::create(rewriter, op.getLoc(),\n+                                       DenseElementsAttr::get(k_ty, k));\n     // Compute new result types.\n     auto values_ty = mlir::dyn_cast<ShapedType>(values.getType());\n     auto indices_ty = mlir::dyn_cast<ShapedType>(indices.getType());\n@@ -2287,8 +2294,9 @@ struct OptimizeTopK : public OpRewritePattern<TFL::TopKV2Op> {\n         mlir::RankedTensorType::get(shape, values_ty.getElementType());\n     auto new_indices_ty =\n         mlir::RankedTensorType::get(shape, indices_ty.getElementType());\n-    TFL::TopKV2Op top_k_op = rewriter.create<TFL::TopKV2Op>(\n-        op.getLoc(), new_values_ty, new_indices_ty, op->getOperand(0), k_cst);\n+    TFL::TopKV2Op top_k_op =\n+        TFL::TopKV2Op::create(rewriter, op.getLoc(), new_values_ty,\n+                              new_indices_ty, op->getOperand(0), k_cst);\n \n     // Remove original ops (topk, Slice, Slice).\n     if (!values.use_empty()) {\n@@ -2376,10 +2384,12 @@ struct FuseReshapeAndTransposeAroundBatchMatmul\n         static_cast<int>(std::accumulate(\n             transpose_input.getType().getShape().begin() + 2,\n             transpose_input.getType().getShape().end(), 1, std::multiplies()))};\n-    auto shape_constant = rewriter.create<ConstOp>(\n-        batch_matmul.getLoc(), GetI32ElementsAttr(new_shape, &rewriter));\n-    auto reshaped_input = rewriter.create<ReshapeOp>(\n-        batch_matmul.getLoc(), transpose_op.getInput(), shape_constant);\n+    auto shape_constant =\n+        ConstOp::create(rewriter, batch_matmul.getLoc(),\n+                        GetI32ElementsAttr(new_shape, &rewriter));\n+    auto reshaped_input =\n+        ReshapeOp::create(rewriter, batch_matmul.getLoc(),\n+                          transpose_op.getInput(), shape_constant);\n     rewriter.replaceOpWithNewOp<BatchMatMulOp>(\n         op, op.getType(), reshaped_input, batch_matmul.getX(),\n         /*adj_x=*/false, /*adj_y=*/!batch_matmul.getAdjX(),\n@@ -2438,10 +2448,10 @@ struct FuseTransposeReshapeIntoBatchMatmul\n         reshape_op.getType().getShape().drop_front().begin(),\n         reshape_op.getType().getShape().drop_front().end());\n     new_shape.push_back(reshape_op.getType().getDimSize(0));\n-    auto shape_constant = rewriter.create<ConstOp>(\n-        op.getLoc(), GetI32ElementsAttr(new_shape, &rewriter));\n-    auto new_reshape = rewriter.create<ReshapeOp>(\n-        op.getLoc(), transpose_op.getInput(), shape_constant);\n+    auto shape_constant = ConstOp::create(\n+        rewriter, op.getLoc(), GetI32ElementsAttr(new_shape, &rewriter));\n+    auto new_reshape = ReshapeOp::create(\n+        rewriter, op.getLoc(), transpose_op.getInput(), shape_constant);\n     rewriter.replaceOpWithNewOp<BatchMatMulOp>(\n         op, op.getType(), op.getX(), new_reshape, op.getAdjX(), !op.getAdjY(),\n         op.getAsymmetricQuantizeInputsAttr());\n@@ -2647,8 +2657,8 @@ struct UndoBroadcastFullyConnectedBiasAddWithQDQs\n \n     auto new_bias = FlattenTo1D(bias_op.getValueAttr());\n     auto new_bias_type = new_bias.getType();\n-    auto new_bias_op = rewriter.create<arith::ConstantOp>(\n-        bias_op.getLoc(), new_bias_type, new_bias);\n+    auto new_bias_op = arith::ConstantOp::create(rewriter, bias_op.getLoc(),\n+                                                 new_bias_type, new_bias);\n \n     // Update QuantizeOp with the new bias and its output shape\n     q_op.setOperand(new_bias_op);\n@@ -2717,10 +2727,11 @@ struct MoveReshapeAfterFullyConnected\n     new_input_shape.pop_back();\n     new_input_shape.push_back(input_ty.getShape().back());\n \n-    auto reshape_before = rewriter.create<TFL::ReshapeOp>(\n-        fc.getLoc(), fc.getInput(),\n-        rewriter.create<arith::ConstantOp>(\n-            fc->getLoc(), GetI32ElementsAttr(new_input_shape, &rewriter)));\n+    auto reshape_before = TFL::ReshapeOp::create(\n+        rewriter, fc.getLoc(), fc.getInput(),\n+        arith::ConstantOp::create(\n+            rewriter, fc->getLoc(),\n+            GetI32ElementsAttr(new_input_shape, &rewriter)));\n \n     rewriter.replaceOpWithNewOp<TFL::FullyConnectedOp>(\n         reshape,\n@@ -2864,16 +2875,16 @@ struct PushTransposeThroughSqueeze : public RewritePattern {\n             transpose.getInput().getType().getDimSize(i));\n       }\n     }\n-    auto new_squeeze = rewriter.create<TFL::SqueezeOp>(\n-        squeeze->getLoc(),\n+    auto new_squeeze = TFL::SqueezeOp::create(\n+        rewriter, squeeze->getLoc(),\n         mlir::RankedTensorType::get(new_squeeze_shape,\n                                     squeeze.getType().getElementType()),\n         transpose.getInput(), rewriter.getI32ArrayAttr(new_squeeze_dims));\n \n-    auto new_transpose = rewriter.create<TFL::TransposeOp>(\n-        squeeze->getLoc(), squeeze.getType(), new_squeeze,\n-        rewriter.create<arith::ConstantOp>(\n-            squeeze->getLoc(), GetI32ElementsAttr(new_perm, &rewriter)));\n+    auto new_transpose = TFL::TransposeOp::create(\n+        rewriter, squeeze->getLoc(), squeeze.getType(), new_squeeze,\n+        arith::ConstantOp::create(rewriter, squeeze->getLoc(),\n+                                  GetI32ElementsAttr(new_perm, &rewriter)));\n \n     rewriter.replaceOp(squeeze, new_transpose);\n     return success();\n@@ -3000,17 +3011,18 @@ struct ReorderTransposeReshapeTranspose\n         mlir::dyn_cast_or_null<RankedTensorType>(reshape.getType());\n     if (!reshape_type) return failure();\n \n-    auto new_reshape_shape_const = rewriter.create<arith::ConstantOp>(\n-        reshape.getLoc(), GetI32ElementsAttr(new_reshape_shape, &rewriter));\n+    auto new_reshape_shape_const = arith::ConstantOp::create(\n+        rewriter, reshape.getLoc(),\n+        GetI32ElementsAttr(new_reshape_shape, &rewriter));\n \n-    auto new_inner_reshape = rewriter.create<TFL::ReshapeOp>(\n-        reshape.getLoc(),\n+    auto new_inner_reshape = TFL::ReshapeOp::create(\n+        rewriter, reshape.getLoc(),\n         RankedTensorType::get(new_reshape_shape, reshape_type.getElementType()),\n         input, new_reshape_shape_const.getResult());\n-    auto new_inner_tpose = rewriter.create<TFL::TransposeOp>(\n-        inner_tpose.getLoc(), reshape_type, new_inner_reshape,\n-        rewriter.create<arith::ConstantOp>(\n-            inner_tpose.getLoc(),\n+    auto new_inner_tpose = TFL::TransposeOp::create(\n+        rewriter, inner_tpose.getLoc(), reshape_type, new_inner_reshape,\n+        arith::ConstantOp::create(\n+            rewriter, inner_tpose.getLoc(),\n             GetI32ElementsAttr(new_inner_perm, &rewriter)));\n \n     rewriter.replaceOp(reshape, new_inner_tpose);\n@@ -3079,8 +3091,8 @@ struct FullyConnectedSwapOperandsWhenLHSIsConst\n     RankedTensorType intermediate_type =\n         RankedTensorType::get({O, B}, element_type);\n \n-    auto new_fc = rewriter.create<TFL::FullyConnectedOp>(\n-        loc,\n+    auto new_fc = TFL::FullyConnectedOp::create(\n+        rewriter, loc,\n         /*resultTypes=*/intermediate_type,\n         /*input=*/filter,  // Original Filter V[O, I]\n         /*filter=*/input,  // Original Input C[B, I]\n@@ -3096,10 +3108,11 @@ struct FullyConnectedSwapOperandsWhenLHSIsConst\n     RankedTensorType final_shape_type =\n         RankedTensorType::get({B, O}, element_type);\n \n-    Value transposed_result = rewriter.create<TFL::TransposeOp>(\n-        loc, final_shape_type, new_fc.getResult(0),\n-        rewriter.create<arith::ConstantOp>(\n-            loc, GetI32ElementsAttr(ArrayRef<int32_t>({1, 0}), &rewriter)));\n+    Value transposed_result = TFL::TransposeOp::create(\n+        rewriter, loc, final_shape_type, new_fc.getResult(0),\n+        arith::ConstantOp::create(\n+            rewriter, loc,\n+            GetI32ElementsAttr(ArrayRef<int32_t>({1, 0}), &rewriter)));\n \n     rewriter.replaceOp(fc, transposed_result);\n "
        },
        {
            "sha": "b93422d3812f6cb95a20d98233d4cb42cb8c854e",
            "filename": "tensorflow/compiler/mlir/lite/transforms/pin_ops_with_side_effects.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpin_ops_with_side_effects.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpin_ops_with_side_effects.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpin_ops_with_side_effects.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -140,15 +140,15 @@ void PinOpsWithSideEffectsPass::runOnOperation() {\n     // Wrap all side-effect producing/dependent operations in a ControlNodeOp.\n     builder.setInsertionPoint(op);\n     Location loc = op->getLoc();\n-    auto outer_op = builder.create<ControlNodeOp>(\n-        loc, op->getResultTypes(), ControlType::get(op->getContext()),\n-        control_tokens);\n+    auto outer_op = ControlNodeOp::create(builder, loc, op->getResultTypes(),\n+                                          ControlType::get(op->getContext()),\n+                                          control_tokens);\n     Region region;\n     Block *new_block = new Block;\n     region.push_back(new_block);\n     builder.setInsertionPointToEnd(&region.front());\n     Operation *inner_op = builder.clone(*op);\n-    builder.create<YieldOp>(loc, inner_op->getResults());\n+    YieldOp::create(builder, loc, inner_op->getResults());\n     outer_op.getBody().takeBody(region);\n     // Careful: We can't use outer_op.getResults(), because that also includes\n     // the control token."
        },
        {
            "sha": "0cf34df94faf6ca9c8cb6c3cc17a980616e7bdd4",
            "filename": "tensorflow/compiler/mlir/lite/transforms/post_quantize.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpost_quantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpost_quantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpost_quantize.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -183,7 +183,7 @@ std::optional<mlir::Value> GetConstTensor(PatternRewriter& rewriter,\n   auto const_attr = DenseElementsAttr::get(const_type, vec);\n \n   auto const_op =\n-      rewriter.create<arith::ConstantOp>(loc, const_type, const_attr);\n+      arith::ConstantOp::create(rewriter, loc, const_type, const_attr);\n   return const_op.getResult();\n }\n \n@@ -207,8 +207,8 @@ std::optional<mlir::Value> ConvertDequantizeOp(\n     auto const_attr =\n         DenseElementsAttr::get(const_type, static_cast<float>(zeropoint[0]));\n \n-    auto const_op = rewriter.create<arith::ConstantOp>(op->getLoc(), const_type,\n-                                                       const_attr);\n+    auto const_op = arith::ConstantOp::create(rewriter, op->getLoc(),\n+                                              const_type, const_attr);\n     zp_val = const_op.getResult();\n   } else {\n     SmallVector<int64_t> shape;\n@@ -224,8 +224,8 @@ std::optional<mlir::Value> ConvertDequantizeOp(\n     auto const_attr =\n         DenseElementsAttr::get(const_type, static_cast<float>(scale[0]));\n \n-    auto const_op = rewriter.create<arith::ConstantOp>(op->getLoc(), const_type,\n-                                                       const_attr);\n+    auto const_op = arith::ConstantOp::create(rewriter, op->getLoc(),\n+                                              const_type, const_attr);\n     scale_val = const_op.getResult();\n   } else {\n     SmallVector<int64_t> shape;\n@@ -237,16 +237,17 @@ std::optional<mlir::Value> ConvertDequantizeOp(\n   if (!zp_val || !scale_val) return std::nullopt;\n \n   auto op1_cast_in =\n-      rewriter.create<TFL::CastOp>(op->getLoc(), output_type, input_value);\n+      TFL::CastOp::create(rewriter, op->getLoc(), output_type, input_value);\n \n-  auto op2_sub_op1 = rewriter.create<TFL::SubOp>(\n-      op->getLoc(), output_type, op1_cast_in.getResult(), zp_val.value(),\n+  auto op2_sub_op1 = TFL::SubOp::create(\n+      rewriter, op->getLoc(), output_type, op1_cast_in.getResult(),\n+      zp_val.value(),\n       /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"));\n \n-  return rewriter\n-      .create<TFL::MulOp>(\n-          op->getLoc(), output_type, op2_sub_op1.getResult(), scale_val.value(),\n-          /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"))\n+  return TFL::MulOp::create(\n+             rewriter, op->getLoc(), output_type, op2_sub_op1.getResult(),\n+             scale_val.value(),\n+             /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"))\n       .getResult();\n }\n \n@@ -313,8 +314,8 @@ struct RemoveVolatileOps : public OpRewritePattern<DequantizeOp> {\n \n       auto const_type = tensorflow::GetTypeFromTFTensorShape(\n           output_type.getShape(), qtype.getStorageType());\n-      auto const_op = rewriter.create<arith::ConstantOp>(\n-          op->getLoc(), const_type, qconst_op.getValue());\n+      auto const_op = arith::ConstantOp::create(\n+          rewriter, op->getLoc(), const_type, qconst_op.getValue());\n \n       auto new_value =\n           ConvertDequantizeOp(rewriter, op, output_type, const_op.getResult(),"
        },
        {
            "sha": "899e4e9e088312f6816eb9e469b9d66456460aa0",
            "filename": "tensorflow/compiler/mlir/lite/transforms/prepare_tf.cc",
            "status": "modified",
            "additions": 77,
            "deletions": 74,
            "changes": 151,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fprepare_tf.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fprepare_tf.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fprepare_tf.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -102,7 +102,7 @@ static Value CreateTFCastOpI32(OpBuilder *builder, Location loc, Value x,\n   auto x_type = mlir::dyn_cast_or_null<ShapedType>(x.getType());\n   if (!x_type) llvm_unreachable(\"unsupported type\");\n   Type type = x_type.clone(builder->getI32Type());\n-  return builder->create<TF::CastOp>(loc, type, x, truncate);\n+  return TF::CastOp::create(*builder, loc, type, x, truncate);\n }\n }  // namespace\n \n@@ -253,7 +253,7 @@ class ConvertTFConvOp : public RewritePattern {\n         tensorflow::GetTypeFromTFTensorShape({bias_dim}, elem_type);\n     auto bias_attr = rewriter.getZeroAttr(bias_type);\n     auto bias =\n-        rewriter.create<TF::ConstOp>(op->getLoc(), bias_type, bias_attr);\n+        TF::ConstOp::create(rewriter, op->getLoc(), bias_type, bias_attr);\n \n     if (op->getAttrOfType<StringAttr>(\"padding\").getValue() == \"EXPLICIT\") {\n       // Add Const op for padding value.\n@@ -276,12 +276,12 @@ class ConvertTFConvOp : public RewritePattern {\n           mlir::DenseIntElementsAttr::get(padding_attr_type, padding_values);\n \n       auto padding_const =\n-          rewriter.create<TF::ConstOp>(op->getLoc(), padding_attr);\n+          TF::ConstOp::create(rewriter, op->getLoc(), padding_attr);\n \n       // Add Pad op.\n       auto pad_output_type = UnrankedTensorType::get(elem_type);\n-      input = rewriter.create<TF::PadOp>(op->getLoc(), pad_output_type, input,\n-                                         padding_const);\n+      input = TF::PadOp::create(rewriter, op->getLoc(), pad_output_type, input,\n+                                padding_const);\n \n       // Set Conv padding to `VALID` since padding has been handled by Pad op.\n       state.padding = rewriter.getStringAttr(\"VALID\");\n@@ -315,8 +315,8 @@ class ConvertTFConv2D : public ConvertTFConvOp<ConvertTFConv2D, TF::Conv2DOp> {\n                             Type result_type, Value input, Value filter,\n                             Value bias) const {\n     filter = legalizeFilter(rewriter, loc, filter);\n-    return rewriter.create<TFL::Conv2DOp>(\n-        loc, result_type, input, filter, bias,\n+    return TFL::Conv2DOp::create(\n+        rewriter, loc, result_type, input, filter, bias,\n         /*dilation_h_factor=*/state->dilation_height_factor,\n         /*dilation_w_factor=*/state->dilation_width_factor,\n         /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n@@ -338,7 +338,7 @@ class ConvertTFConv2D : public ConvertTFConvOp<ConvertTFConv2D, TF::Conv2DOp> {\n         {static_cast<int>(perm.size())}, rewriter.getIntegerType(32));\n     auto perm_attr =\n         DenseElementsAttr::get(perm_type, llvm::ArrayRef<int>(perm));\n-    auto perm_op = rewriter.create<TF::ConstOp>(loc, perm_type, perm_attr);\n+    auto perm_op = TF::ConstOp::create(rewriter, loc, perm_type, perm_attr);\n \n     // Create tensor type for the transpose result.\n     auto filter_type = mlir::cast<RankedTensorType>(filter.getType());\n@@ -350,7 +350,7 @@ class ConvertTFConv2D : public ConvertTFConvOp<ConvertTFConv2D, TF::Conv2DOp> {\n     auto result_type =\n         tensorflow::GetTypeFromTFTensorShape(result_shape, elem_type);\n \n-    return rewriter.create<TF::TransposeOp>(loc, result_type, filter, perm_op);\n+    return TF::TransposeOp::create(rewriter, loc, result_type, filter, perm_op);\n   }\n };\n \n@@ -382,8 +382,8 @@ class ConvertTFDepthwiseConv2dNative\n         mlir::cast<RankedTensorType>(filter.getType()).getDimSize(3);\n \n     filter = legalizeFilter(rewriter, loc, filter);\n-    return rewriter.create<TFL::DepthwiseConv2DOp>(\n-        loc, result_type, input, filter, bias,\n+    return TFL::DepthwiseConv2DOp::create(\n+        rewriter, loc, result_type, input, filter, bias,\n         /*dilation_h_factor=*/state->dilation_height_factor,\n         /*dilation_w_factor=*/state->dilation_width_factor,\n         /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n@@ -420,9 +420,9 @@ class ConvertTFDepthwiseConv2dNative\n           rewriter.getI32IntegerAttr(ConvertToTfliteSize(size));\n     }\n     auto shape_attr = DenseElementsAttr::get(shape_type, result_shape_data);\n-    auto shape = rewriter.create<TF::ConstOp>(loc, shape_type, shape_attr);\n+    auto shape = TF::ConstOp::create(rewriter, loc, shape_type, shape_attr);\n \n-    return rewriter.create<TF::ReshapeOp>(loc, result_type, filter, shape);\n+    return TF::ReshapeOp::create(rewriter, loc, result_type, filter, shape);\n   }\n };\n \n@@ -495,11 +495,11 @@ struct ConvertTFStridedSlice : public RewritePattern {\n \n     auto shape_attr = DenseElementsAttr::get(shape_type, result_shape_data);\n     auto shape =\n-        rewriter.create<arith::ConstantOp>(loc, shape_type, shape_attr);\n+        arith::ConstantOp::create(rewriter, loc, shape_type, shape_attr);\n     auto revised_output_type = tensorflow::GetTypeFromTFTensorShape(\n         revised_shape, original_input_type.getElementType());\n-    TF::ReshapeOp reshape = rewriter.create<TF::ReshapeOp>(\n-        loc, revised_output_type, original_input, shape);\n+    TF::ReshapeOp reshape = TF::ReshapeOp::create(\n+        rewriter, loc, revised_output_type, original_input, shape);\n \n     // Replace the original strided_slice.\n     uint64_t revised_begin_mask = strided_slice_op.getBeginMask();\n@@ -656,13 +656,13 @@ struct ConvertTFStridedSlice : public RewritePattern {\n \n     auto begin_attr = DenseElementsAttr::get<int32_t>(type, padded_begin);\n     auto begin_op =\n-        rewriter.create<arith::ConstantOp>(op->getLoc(), type, begin_attr);\n+        arith::ConstantOp::create(rewriter, op->getLoc(), type, begin_attr);\n     auto end_attr = DenseElementsAttr::get<int32_t>(type, padded_end);\n     auto end_op =\n-        rewriter.create<arith::ConstantOp>(op->getLoc(), type, end_attr);\n+        arith::ConstantOp::create(rewriter, op->getLoc(), type, end_attr);\n     auto stride_attr = DenseElementsAttr::get<int32_t>(type, padded_stride);\n     auto stride_op =\n-        rewriter.create<arith::ConstantOp>(op->getLoc(), type, stride_attr);\n+        arith::ConstantOp::create(rewriter, op->getLoc(), type, stride_attr);\n \n     rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n         op, strided_slice_op.getType(), input, begin_op.getResult(),\n@@ -767,17 +767,17 @@ struct ConvertTFStridedSlice : public RewritePattern {\n \n     auto begin_end_type = tensorflow::GetTypeFromTFTensorShape(\n         {num_input_dims}, rewriter.getIntegerType(32));\n-    auto new_begin_attr = rewriter.create<arith::ConstantOp>(\n-        op->getLoc(), begin_end_type,\n+    auto new_begin_attr = arith::ConstantOp::create(\n+        rewriter, op->getLoc(), begin_end_type,\n         DenseElementsAttr::get<int32_t>(begin_end_type, padded_begin));\n-    auto new_end_attr = rewriter.create<arith::ConstantOp>(\n-        op->getLoc(), begin_end_type,\n+    auto new_end_attr = arith::ConstantOp::create(\n+        rewriter, op->getLoc(), begin_end_type,\n         DenseElementsAttr::get<int32_t>(begin_end_type, padded_end));\n     auto strides_type = tensorflow::GetTypeFromTFTensorShape(\n         {static_cast<int64_t>(padded_strides.size())},\n         rewriter.getIntegerType(32));\n-    auto new_strides_attr = rewriter.create<arith::ConstantOp>(\n-        op->getLoc(), strides_type,\n+    auto new_strides_attr = arith::ConstantOp::create(\n+        rewriter, op->getLoc(), strides_type,\n         DenseElementsAttr::get<int32_t>(strides_type, padded_strides));\n \n     auto attribute_type = rewriter.getIntegerType(64);\n@@ -1043,18 +1043,19 @@ struct FusedBatchNormV3Pat : public ::mlir::RewritePattern {\n         auto reduce_dim_type = tensorflow::GetTypeFromTFTensorShape(\n             {3}, rewriter.getIntegerType(32));\n         ::mlir::SmallVector<int32_t, 3> reduce_dim_values = {0, 1, 2};\n-        reduce_dim_op = rewriter.create<TF::ConstOp>(\n-            odsLoc, ::mlir::DenseIntElementsAttr::get(reduce_dim_type,\n-                                                      reduce_dim_values));\n+        reduce_dim_op =\n+            TF::ConstOp::create(rewriter, odsLoc,\n+                                ::mlir::DenseIntElementsAttr::get(\n+                                    reduce_dim_type, reduce_dim_values));\n       }\n \n       auto new_mean_type = tensorflow::GetTypeFromTFTensorShape(\n           {last_dim}, rewriter.getF32Type());\n       ::mlir::TF::MeanOp mean_op_1;\n       {\n         ::mlir::Value x_value = (*x.begin());\n-        mean_op_1 = rewriter.create<TF::MeanOp>(\n-            odsLoc, new_mean_type, x_value, reduce_dim_op,\n+        mean_op_1 = TF::MeanOp::create(\n+            rewriter, odsLoc, new_mean_type, x_value, reduce_dim_op,\n             /*keep_dims=*/rewriter.getBoolAttr(false));\n       }\n \n@@ -1064,15 +1065,15 @@ struct FusedBatchNormV3Pat : public ::mlir::RewritePattern {\n         ::mlir::Value tblgen_value_1 = (*mean_op_1.getODSResults(0).begin());\n         // If x has shape of [b, h, w, c], the result of mean_op_1 will have\n         // shape of [c]. Therefore, their shapes are always compatible.\n-        square_diff_op = rewriter.create<::mlir::TF::SquaredDifferenceOp>(\n-            odsLoc, tblgen_value_0, tblgen_value_1);\n+        square_diff_op = ::mlir::TF::SquaredDifferenceOp::create(\n+            rewriter, odsLoc, tblgen_value_0, tblgen_value_1);\n       }\n \n       ::mlir::TF::MeanOp mean_op_2;\n       {\n         ::mlir::Value input_value = (*square_diff_op.getODSResults(0).begin());\n-        mean_op_2 = rewriter.create<TF::MeanOp>(\n-            odsLoc, new_mean_type, input_value, reduce_dim_op,\n+        mean_op_2 = TF::MeanOp::create(\n+            rewriter, odsLoc, new_mean_type, input_value, reduce_dim_op,\n             /*keep_dims=*/rewriter.getBoolAttr(false));\n       }\n \n@@ -1083,57 +1084,56 @@ struct FusedBatchNormV3Pat : public ::mlir::RewritePattern {\n     ::llvm::SmallVector<::mlir::Value, 4> replace_values;\n     ::mlir::TF::ConstOp epsilon_const_op;\n     {\n-      epsilon_const_op =\n-          rewriter.create<::mlir::TF::ConstOp>(odsLoc,\n-                                               /*value=*/epsilon);\n+      epsilon_const_op = ::mlir::TF::ConstOp::create(rewriter, odsLoc,\n+                                                     /*value=*/epsilon);\n     }\n     ::mlir::TF::AddOp add_op_1;\n     {\n       ::mlir::Value epsilon_value =\n           (*epsilon_const_op.getODSResults(0).begin());\n       // Multiplying with a constant, no need to check broadcastibility.\n-      add_op_1 = rewriter.create<::mlir::TF::AddOp>(odsLoc,\n-                                                    /*x=*/variance_value,\n-                                                    /*y=*/epsilon_value);\n+      add_op_1 = ::mlir::TF::AddOp::create(rewriter, odsLoc,\n+                                           /*x=*/variance_value,\n+                                           /*y=*/epsilon_value);\n     }\n     ::mlir::TF::RsqrtOp rsqrt_op;\n     {\n       ::mlir::SmallVector<::mlir::Value, 4> tblgen_values;\n       ::mlir::SmallVector<::mlir::NamedAttribute, 4> tblgen_attrs;\n       tblgen_values.push_back((*add_op_1.getODSResults(0).begin()));\n-      rsqrt_op = rewriter.create<::mlir::TF::RsqrtOp>(odsLoc, tblgen_values,\n-                                                      tblgen_attrs);\n+      rsqrt_op = ::mlir::TF::RsqrtOp::create(rewriter, odsLoc, tblgen_values,\n+                                             tblgen_attrs);\n     }\n     ::mlir::TF::MulOp multiplier;\n     {\n       ::mlir::Value tblgen_value_0 = (*scale.begin());\n       ::mlir::Value tblgen_value_1 = (*rsqrt_op.getODSResults(0).begin());\n-      multiplier = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n-                                                      /*x=*/tblgen_value_0,\n-                                                      /*y=*/tblgen_value_1);\n+      multiplier = ::mlir::TF::MulOp::create(rewriter, odsLoc,\n+                                             /*x=*/tblgen_value_0,\n+                                             /*y=*/tblgen_value_1);\n     }\n     ::mlir::TF::MulOp mul_op_1;\n     {\n       ::mlir::Value tblgen_value_0 = (*x.begin());\n       ::mlir::Value tblgen_value_1 = (*multiplier.getODSResults(0).begin());\n-      mul_op_1 = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n-                                                    /*x=*/tblgen_value_0,\n-                                                    /*y=*/tblgen_value_1);\n+      mul_op_1 = ::mlir::TF::MulOp::create(rewriter, odsLoc,\n+                                           /*x=*/tblgen_value_0,\n+                                           /*y=*/tblgen_value_1);\n     }\n     ::mlir::TF::MulOp mul_op_2;\n     {\n       ::mlir::Value multiplier_value = (*multiplier.getODSResults(0).begin());\n-      mul_op_2 = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n-                                                    /*x=*/mean_value,\n-                                                    /*y=*/multiplier_value);\n+      mul_op_2 = ::mlir::TF::MulOp::create(rewriter, odsLoc,\n+                                           /*x=*/mean_value,\n+                                           /*y=*/multiplier_value);\n     }\n     ::mlir::TF::SubOp sub_op;\n     {\n       ::mlir::Value tblgen_value_0 = (*offset.begin());\n       ::mlir::Value tblgen_value_1 = (*mul_op_2.getODSResults(0).begin());\n-      sub_op = rewriter.create<::mlir::TF::SubOp>(odsLoc,\n-                                                  /*x=*/tblgen_value_0,\n-                                                  /*y=*/tblgen_value_1);\n+      sub_op = ::mlir::TF::SubOp::create(rewriter, odsLoc,\n+                                         /*x=*/tblgen_value_0,\n+                                         /*y=*/tblgen_value_1);\n     }\n     ::mlir::TF::AddOp add_op_2;\n     {\n@@ -1145,8 +1145,8 @@ struct FusedBatchNormV3Pat : public ::mlir::RewritePattern {\n       for (auto v : fused_batch_norm_op.getODSResults(0)) {\n         tblgen_types.push_back(v.getType());\n       }\n-      add_op_2 = rewriter.create<::mlir::TF::AddOp>(\n-          odsLoc, tblgen_types, tblgen_values, tblgen_attrs);\n+      add_op_2 = ::mlir::TF::AddOp::create(rewriter, odsLoc, tblgen_types,\n+                                           tblgen_values, tblgen_attrs);\n     }\n     for (auto v :\n          ::llvm::SmallVector<::mlir::Value, 4>{add_op_2.getODSResults(0)}) {\n@@ -1261,9 +1261,9 @@ struct ReorderFakeQuantPattern : public RewritePattern {\n                                 ReorderOp &new_reorder_op) const {\n     Value tensor_value = (*input.begin());\n     Value shape_value = (*shape.begin());\n-    new_reorder_op = rewriter.create<ReorderOp>(ods_loc,\n-                                                /*tensor=*/tensor_value,\n-                                                /*shape=*/shape_value);\n+    new_reorder_op = ReorderOp::create(rewriter, ods_loc,\n+                                       /*tensor=*/tensor_value,\n+                                       /*shape=*/shape_value);\n     return success();\n   }\n \n@@ -1289,8 +1289,8 @@ struct ReorderFakeQuantPattern : public RewritePattern {\n     for (auto v : casted_op.getODSResults(0)) {\n       target_types.push_back(v.getType());\n     }\n-    fakequant_op = rewriter.create<TF::FakeQuantWithMinMaxVarsOp>(\n-        ods_loc, target_types, target_values, target_attrs);\n+    fakequant_op = TF::FakeQuantWithMinMaxVarsOp::create(\n+        rewriter, ods_loc, target_types, target_values, target_attrs);\n     return success();\n   }\n \n@@ -1442,35 +1442,37 @@ struct ConvertRfftToRfft2d : public RewritePattern {\n \n     auto expaned_input_type = tensorflow::GetTypeFromTFTensorShape(\n         expanded_input_shape, input_type.getElementType());\n-    TF::ExpandDimsOp expanded_input = rewriter.create<TF::ExpandDimsOp>(\n-        rfft_op.getLoc(), expaned_input_type, input, minus_two->getResult());\n+    TF::ExpandDimsOp expanded_input =\n+        TF::ExpandDimsOp::create(rewriter, rfft_op.getLoc(), expaned_input_type,\n+                                 input, minus_two->getResult());\n \n     // Expanded fft_len.\n     auto one_attr = mlir::DenseIntElementsAttr::get(one_ele_type, {1});\n \n-    auto one = rewriter.create<TF::ConstOp>(rfft_op.getLoc(), one_attr);\n+    auto one = TF::ConstOp::create(rewriter, rfft_op.getLoc(), one_attr);\n \n     auto zero = CreateConstOpWithSingleValue(&rewriter, rfft_op.getLoc(),\n                                              one_ele_type, 0);\n \n     auto expanded_fft_len_type = tensorflow::GetTypeFromTFTensorShape(\n         {2}, fft_len_type.getElementType());\n \n-    TF::ConcatV2Op expanded_fft_len = rewriter.create<TF::ConcatV2Op>(\n-        rfft_op.getLoc(), expanded_fft_len_type,\n+    TF::ConcatV2Op expanded_fft_len = TF::ConcatV2Op::create(\n+        rewriter, rfft_op.getLoc(), expanded_fft_len_type,\n         SmallVector<Value, 2>({one.getResult(), fft_len}), zero->getResult());\n \n     // Insert the rfft_2d.\n     auto rfft2d_out_type = tensorflow::GetTypeFromTFTensorShape(\n         expanded_output_shape, output_type.getElementType());\n-    TF::RFFT2DOp rfft2d = rewriter.create<TF::RFFT2DOp>(\n-        rfft_op.getLoc(), rfft2d_out_type, expanded_input.getResult(),\n+    TF::RFFT2DOp rfft2d = TF::RFFT2DOp::create(\n+        rewriter, rfft_op.getLoc(), rfft2d_out_type, expanded_input.getResult(),\n         expanded_fft_len.getResult());\n \n     // Insert the squeeze op.\n     auto squeeze_dim = rewriter.getI64ArrayAttr({-2});\n-    TF::SqueezeOp squeeze = rewriter.create<TF::SqueezeOp>(\n-        rfft_op.getLoc(), output_type, rfft2d.getResult(), squeeze_dim);\n+    TF::SqueezeOp squeeze =\n+        TF::SqueezeOp::create(rewriter, rfft_op.getLoc(), output_type,\n+                              rfft2d.getResult(), squeeze_dim);\n \n     rewriter.replaceOp(op, squeeze.getResult());\n \n@@ -1614,8 +1616,8 @@ class QuantizeConcatResult : public OpRewritePattern<TF::ConcatV2Op> {\n     llvm::SmallVector<Value, 4> inputs{concat_result, min_v, max_v};\n \n     rewriter.setInsertionPointAfter(concat.getOperation());\n-    auto new_fake_quant_op = rewriter.create<TF::FakeQuantWithMinMaxVarsOp>(\n-        concat.getLoc(), concat->getResultTypes(), inputs,\n+    auto new_fake_quant_op = TF::FakeQuantWithMinMaxVarsOp::create(\n+        rewriter, concat.getLoc(), concat->getResultTypes(), inputs,\n         (*fake_quant_ops.begin())->getAttrs());\n \n     for (OpOperand *use : uses) {\n@@ -1673,8 +1675,9 @@ class QuantizeMeanResult : public OpRewritePattern<TF::MeanOp> {\n     llvm::SmallVector<Value, 4> inputs{mean_result, fq.getMin(), fq.getMax()};\n \n     rewriter.setInsertionPointAfter(mean.getOperation());\n-    auto new_fake_quant_op = rewriter.create<TF::FakeQuantWithMinMaxVarsOp>(\n-        mean.getLoc(), mean->getResultTypes(), inputs, fq->getAttrs());\n+    auto new_fake_quant_op = TF::FakeQuantWithMinMaxVarsOp::create(\n+        rewriter, mean.getLoc(), mean->getResultTypes(), inputs,\n+        fq->getAttrs());\n \n     for (OpOperand *use : uses) {\n       use->assign(new_fake_quant_op);"
        },
        {
            "sha": "82803f6de927cb96c5068810abba2d024a8f7a38",
            "filename": "tensorflow/compiler/mlir/lite/transforms/push_transpose_through_ewise_pass.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpush_transpose_through_ewise_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpush_transpose_through_ewise_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fpush_transpose_through_ewise_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -173,8 +173,8 @@ class CommuteBothInputsTransposedWithEwiseOps : public RewritePattern {\n                         new_out_type, op->getAttrs());\n \n     // Apply original tranpose to output of ewise op.\n-    auto out_tpose_op = rewriter.create<TFL::TransposeOp>(\n-        new_ewise_op->getLoc(), op->getResult(0).getType(),\n+    auto out_tpose_op = TFL::TransposeOp::create(\n+        rewriter, new_ewise_op->getLoc(), op->getResult(0).getType(),\n         new_ewise_op->getResults()[0], perm1);\n     rewriter.replaceOp(op, out_tpose_op.getOperation());\n     return success();\n@@ -273,7 +273,7 @@ class CommuteTransposeWithEwiseOps : public RewritePattern {\n           RankedTensorType::get(inverse_perm.size(), rewriter.getI32Type()),\n           inverse_perm);\n       auto inverse_perm_op =\n-          rewriter.create<arith::ConstantOp>(perm.getLoc(), inverse_perm_attr);\n+          arith::ConstantOp::create(rewriter, perm.getLoc(), inverse_perm_attr);\n \n       // Transpose the input constant.\n       auto in_rtt =\n@@ -283,9 +283,9 @@ class CommuteTransposeWithEwiseOps : public RewritePattern {\n           RankedTensorType::get(PermuteShape(in_rtt.getShape(), inverse_perm),\n                                 in_rtt.getElementType());\n \n-      tposed_const = rewriter.create<TFL::TransposeOp>(\n-          cst_arg->getLoc(), inverse_type, cst_arg->getResult(0),\n-          inverse_perm_op);\n+      tposed_const =\n+          TFL::TransposeOp::create(rewriter, cst_arg->getLoc(), inverse_type,\n+                                   cst_arg->getResult(0), inverse_perm_op);\n     }\n \n     auto current_out_type =\n@@ -301,8 +301,8 @@ class CommuteTransposeWithEwiseOps : public RewritePattern {\n                         new_out_type, op->getAttrs());\n \n     // Apply original tranpose to output of ewise op.\n-    auto out_tpose_op = rewriter.create<TFL::TransposeOp>(\n-        new_ewise_op->getLoc(), op->getResult(0).getType(),\n+    auto out_tpose_op = TFL::TransposeOp::create(\n+        rewriter, new_ewise_op->getLoc(), op->getResult(0).getType(),\n         new_ewise_op->getResults()[0], perm);\n     rewriter.replaceOp(op, out_tpose_op.getOperation());\n     return success();"
        },
        {
            "sha": "c213c1ee4982501d486f4b3a8a7f636bbdf09f1e",
            "filename": "tensorflow/compiler/mlir/lite/transforms/quantize.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -207,8 +207,9 @@ class PushForwardDrqFQ : public OpRewritePattern<stablehlo::CompositeOp> {\n     Value float_input = drq_fq_op.getOperand(drq_fq_op.getNumOperands() - 1);\n \n     // Create a new pad op.\n-    auto new_pad_op = rewriter.create<TFL::PadOp>(\n-        pad_op.getLoc(), pad_op.getType(), float_input, pad_op.getPadding());\n+    auto new_pad_op =\n+        TFL::PadOp::create(rewriter, pad_op.getLoc(), pad_op.getType(),\n+                           float_input, pad_op.getPadding());\n \n     // Create a new drq fake quant op.\n     // Operands are the same, except for the last one.\n@@ -218,8 +219,8 @@ class PushForwardDrqFQ : public OpRewritePattern<stablehlo::CompositeOp> {\n     }\n     new_drq_operands.push_back(new_pad_op.getResult());\n \n-    auto new_drq_fq_op = rewriter.create<stablehlo::CompositeOp>(\n-        drq_fq_op.getLoc(), pad_op.getType(), new_drq_operands,\n+    auto new_drq_fq_op = stablehlo::CompositeOp::create(\n+        rewriter, drq_fq_op.getLoc(), pad_op.getType(), new_drq_operands,\n         drq_fq_op->getAttrs());\n \n     rewriter.replaceOp(pad_op, new_drq_fq_op.getResult(0));"
        },
        {
            "sha": "d6e18dc415850816ce8cbe933abf46f29725ec5b",
            "filename": "tensorflow/compiler/mlir/lite/transforms/quantize_variables.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 21,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize_variables.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize_variables.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize_variables.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -114,11 +114,12 @@ void QuantizeVariablesPass::QuantizeVariable(\n       // Add dequantize.\n       builder.setInsertionPointAfter(read_variable_op);\n       auto new_read_variable_op =\n-          builder.create<ReadVariableOp>(read_variable_op.getLoc(), ref_qtype,\n-                                         read_variable_op.getResourceId());\n-      auto new_dq_op = builder.create<DequantizeOp>(\n-          read_variable_op.getLoc(), read_variable_op.getResult().getType(),\n-          new_read_variable_op.getResult());\n+          ReadVariableOp::create(builder, read_variable_op.getLoc(), ref_qtype,\n+                                 read_variable_op.getResourceId());\n+      auto new_dq_op =\n+          DequantizeOp::create(builder, read_variable_op.getLoc(),\n+                               read_variable_op.getResult().getType(),\n+                               new_read_variable_op.getResult());\n       read_variable_op->replaceAllUsesWith(new_dq_op);\n       read_variable_op.erase();\n     }\n@@ -135,32 +136,32 @@ void QuantizeVariablesPass::QuantizeVariable(\n         if (qtype == quant::QuantizedType::getQuantizedElementType(ref_qtype)) {\n           // Same quantization parameters, remove it.\n           builder.setInsertionPoint(assign_variable_op);\n-          auto new_assign_variable_op = builder.create<AssignVariableOp>(\n-              assign_variable_op.getLoc(), assign_variable_op.getResourceId(),\n-              dq_op.getInput());\n+          auto new_assign_variable_op = AssignVariableOp::create(\n+              builder, assign_variable_op.getLoc(),\n+              assign_variable_op.getResourceId(), dq_op.getInput());\n           assign_variable_op->replaceAllUsesWith(new_assign_variable_op);\n         } else {\n           // Otherwise, apply re-quantization.\n           builder.setInsertionPoint(assign_variable_op);\n-          auto new_q_op = builder.create<QuantizeOp>(\n-              assign_variable_op.getLoc(), ref_qtype, dq_op.getInput(),\n+          auto new_q_op = QuantizeOp::create(\n+              builder, assign_variable_op.getLoc(), ref_qtype, dq_op.getInput(),\n               TypeAttr::get(ref_qtype));\n-          auto new_assign_variable_op = builder.create<AssignVariableOp>(\n-              assign_variable_op.getLoc(), assign_variable_op.getResourceId(),\n-              new_q_op.getResult());\n+          auto new_assign_variable_op = AssignVariableOp::create(\n+              builder, assign_variable_op.getLoc(),\n+              assign_variable_op.getResourceId(), new_q_op.getResult());\n           assign_variable_op->replaceAllUsesWith(new_assign_variable_op);\n         }\n         assign_variable_op.erase();\n         dq_op.erase();\n       } else {\n         // Add quantize op.\n         builder.setInsertionPoint(assign_variable_op);\n-        auto new_q_op = builder.create<QuantizeOp>(\n-            assign_variable_op.getLoc(), ref_qtype,\n+        auto new_q_op = QuantizeOp::create(\n+            builder, assign_variable_op.getLoc(), ref_qtype,\n             assign_variable_op.getValue(), TypeAttr::get(ref_qtype));\n-        auto new_assign_variable_op = builder.create<AssignVariableOp>(\n-            assign_variable_op.getLoc(), assign_variable_op.getResourceId(),\n-            new_q_op.getResult());\n+        auto new_assign_variable_op = AssignVariableOp::create(\n+            builder, assign_variable_op.getLoc(),\n+            assign_variable_op.getResourceId(), new_q_op.getResult());\n         assign_variable_op->replaceAllUsesWith(new_assign_variable_op);\n         assign_variable_op.erase();\n       }\n@@ -171,9 +172,9 @@ void QuantizeVariablesPass::QuantizeVariable(\n     builder.setInsertionPoint(var_handle_op);\n     auto output_type = UnrankedTensorType::get(TF::ResourceType::get(\n         {mlir::cast<TensorType>(ref_qtype)}, builder.getContext()));\n-    auto new_var_handle_op = builder.create<VarHandleOp>(\n-        var_handle_op.getLoc(), output_type, var_handle_op.getContainer(),\n-        var_handle_op.getSharedName());\n+    auto new_var_handle_op = VarHandleOp::create(\n+        builder, var_handle_op.getLoc(), output_type,\n+        var_handle_op.getContainer(), var_handle_op.getSharedName());\n     var_handle_op->replaceAllUsesWith(new_var_handle_op);\n     var_handle_op.erase();\n   }"
        },
        {
            "sha": "58fff203b9fb3e64489d3555802d668625b971a9",
            "filename": "tensorflow/compiler/mlir/lite/transforms/raise_custom_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fraise_custom_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fraise_custom_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fraise_custom_ops.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -80,8 +80,8 @@ void RaiseCustomOpsPass::runOnOperation() {\n   for (auto *op : custom_ops) {\n     builder.setInsertionPoint(op);\n     Location loc = op->getLoc();\n-    auto custom_op = builder.create<CustomTfOp>(loc, op->getResultTypes(),\n-                                                op->getOperands());\n+    auto custom_op = CustomTfOp::create(builder, loc, op->getResultTypes(),\n+                                        op->getOperands());\n     Region region;\n     Block *new_block = new Block;\n     region.push_back(new_block);\n@@ -95,7 +95,7 @@ void RaiseCustomOpsPass::runOnOperation() {\n       inner_op->setOperand(idx_args.index(), idx_args.value());\n     }\n     custom_op->setAttrs(inner_op->getAttrs());\n-    builder.create<YieldOp>(loc, inner_op->getResults());\n+    YieldOp::create(builder, loc, inner_op->getResults());\n     custom_op.getBody().takeBody(region);\n \n     op->replaceAllUsesWith(custom_op);"
        },
        {
            "sha": "a0a6df9cf4feef638be7e2420be50584d4ecf986",
            "filename": "tensorflow/compiler/mlir/lite/transforms/reduce_type_precision.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Freduce_type_precision.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Freduce_type_precision.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Freduce_type_precision.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -106,12 +106,12 @@ class SanitizeGatherOpOutputToI4 : public OpRewritePattern<TFL::GatherOp> {\n     }\n \n     Builder builder(op.getContext());\n-    auto new_gather_op = rewriter.create<TFL::GatherOp>(\n-        op.getLoc(),\n-        /*result=*/\n-        mlir::cast<TensorType>(op.getResult().getType())\n-            .clone(builder.getI4Type()),\n-        /*operand=*/op.getOperands(), op->getAttrs());\n+    auto new_gather_op =\n+        TFL::GatherOp::create(rewriter, op.getLoc(),\n+                              /*result=*/\n+                              mlir::cast<TensorType>(op.getResult().getType())\n+                                  .clone(builder.getI4Type()),\n+                              /*operand=*/op.getOperands(), op->getAttrs());\n     rewriter.replaceAllUsesWith(op.getResult(), new_gather_op.getResult());\n \n     return success();"
        },
        {
            "sha": "6f476ded0a1a6218396895bb735a5f56733a4a6d",
            "filename": "tensorflow/compiler/mlir/lite/transforms/tflite_passes/unfold_large_splat_constants_pass.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 17,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Ftflite_passes%2Funfold_large_splat_constants_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Ftflite_passes%2Funfold_large_splat_constants_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Ftflite_passes%2Funfold_large_splat_constants_pass.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -60,23 +60,21 @@ void MaybeUnfoldLargeSplatConstant(mlir::OpBuilder* op_builder,\n   }\n \n   op_builder->setInsertionPoint(const_op);\n-  mlir::arith::ConstantOp fill_shape =\n-      op_builder->create<mlir::arith::ConstantOp>(\n-          const_op->getLoc(), DenseIntElementsAttr::get(\n-                                  tensorflow::GetTypeFromTFTensorShape(\n-                                      {splat_elements_attr.getType().getRank()},\n-                                      op_builder->getI64Type()),\n-                                  splat_elements_attr.getType().getShape()));\n-  mlir::arith::ConstantOp fill_value =\n-      op_builder->create<mlir::arith::ConstantOp>(\n-          const_op->getLoc(),\n-          DenseElementsAttr::get(\n-              tensorflow::GetTypeFromTFTensorShape(\n-                  {}, splat_elements_attr.getType().getElementType()),\n-              splat_elements_attr.getSplatValue<Attribute>()));\n-  TFL::FillOp fill = op_builder->create<TFL::FillOp>(\n-      const_op->getLoc(), splat_elements_attr.getType(), fill_shape,\n-      fill_value);\n+  mlir::arith::ConstantOp fill_shape = mlir::arith::ConstantOp::create(\n+      *op_builder, const_op->getLoc(),\n+      DenseIntElementsAttr::get(tensorflow::GetTypeFromTFTensorShape(\n+                                    {splat_elements_attr.getType().getRank()},\n+                                    op_builder->getI64Type()),\n+                                splat_elements_attr.getType().getShape()));\n+  mlir::arith::ConstantOp fill_value = mlir::arith::ConstantOp::create(\n+      *op_builder, const_op->getLoc(),\n+      DenseElementsAttr::get(\n+          tensorflow::GetTypeFromTFTensorShape(\n+              {}, splat_elements_attr.getType().getElementType()),\n+          splat_elements_attr.getSplatValue<Attribute>()));\n+  TFL::FillOp fill = TFL::FillOp::create(*op_builder, const_op->getLoc(),\n+                                         splat_elements_attr.getType(),\n+                                         fill_shape, fill_value);\n   const_op->replaceAllUsesWith(fill);\n   const_op->erase();\n }"
        },
        {
            "sha": "d9cab52085ef5b515a3dd4b0c06906cbd48063a5",
            "filename": "tensorflow/compiler/mlir/lite/utils/fake_quant_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Ffake_quant_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Ffake_quant_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Ffake_quant_utils.h?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -150,10 +150,10 @@ class InsertTFLQuantOpsAfterTFFakeQuantOp {\n     // dequantize ops, and insert them between the tf.FakeQuantWithMinMaxVarsOp\n     // and its users.\n     Value value = tf_op.getOutputs();\n-    auto quantize = rewriter.create<TFL::QuantizeOp>(\n-        tf_op.getLoc(), qtype.getValue(), value, qtype);\n-    auto dequantize = rewriter.create<TFL::DequantizeOp>(\n-        tf_op.getLoc(), res_type, quantize.getOutput());\n+    auto quantize = TFL::QuantizeOp::create(rewriter, tf_op.getLoc(),\n+                                            qtype.getValue(), value, qtype);\n+    auto dequantize = TFL::DequantizeOp::create(rewriter, tf_op.getLoc(),\n+                                                res_type, quantize.getOutput());\n     value.replaceAllUsesWith(dequantize);\n     quantize.getOperation()->replaceUsesOfWith(dequantize, value);\n "
        },
        {
            "sha": "f94cad6b5eabe7009279b38704ec144b6e8e58eb",
            "filename": "tensorflow/compiler/mlir/lite/utils/lstm_utils.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 28,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Flstm_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Flstm_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Flstm_utils.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -52,43 +52,43 @@ Value CreateI32SplatConst(OpBuilder* builder, ArrayRef<int64_t> shape,\n                           int32_t val, mlir::Location location) {\n   auto type = RankedTensorType::get(shape, builder->getIntegerType(32));\n   auto attr = DenseElementsAttr::get(type, val);\n-  return builder->create<arith::ConstantOp>(location, type, attr);\n+  return arith::ConstantOp::create(*builder, location, type, attr);\n }\n \n Value CreateF32SplatConst(OpBuilder* builder, ArrayRef<int64_t> shape,\n                           float val, mlir::Location location) {\n   auto type = RankedTensorType::get(shape, builder->getF32Type());\n   auto attr = DenseElementsAttr::get(type, val);\n-  return builder->create<arith::ConstantOp>(location, type, attr);\n+  return arith::ConstantOp::create(*builder, location, type, attr);\n }\n \n Value CreatTfF32ConstOp(OpBuilder* builder, ArrayRef<int64_t> shape, float val,\n                         mlir::Location location) {\n   auto type = RankedTensorType::get(shape, builder->getF32Type());\n   auto ele_type = RankedTensorType::get({1}, builder->getF32Type());\n   auto attr = DenseElementsAttr::get(ele_type, val);\n-  return builder->create<TF::ConstOp>(location, type, attr);\n+  return TF::ConstOp::create(*builder, location, type, attr);\n }\n \n Value CreateI64DenseConst(OpBuilder* builder, ArrayRef<int64_t> shape,\n                           ArrayRef<int64_t> values, mlir::Location location) {\n   auto type = RankedTensorType::get(static_cast<int>(shape.size()),\n                                     builder->getIntegerType(64));\n   auto attr = DenseElementsAttr::get(type, values);\n-  return builder->create<arith::ConstantOp>(location, type, attr);\n+  return arith::ConstantOp::create(*builder, location, type, attr);\n }\n \n Value CreateI32DenseConst(OpBuilder* builder, ArrayRef<int32_t> values,\n                           mlir::Location location) {\n   auto type = RankedTensorType::get(static_cast<int>(values.size()),\n                                     builder->getIntegerType(32));\n   auto attr = DenseElementsAttr::get(type, values);\n-  return builder->create<arith::ConstantOp>(location, type, attr);\n+  return arith::ConstantOp::create(*builder, location, type, attr);\n }\n \n Value CreateNoneValue(OpBuilder* builder, mlir::Location location) {\n-  return builder->create<TFL::NoValueOp>(location, builder->getNoneType(),\n-                                         builder->getUnitAttr());\n+  return TFL::NoValueOp::create(*builder, location, builder->getNoneType(),\n+                                builder->getUnitAttr());\n }\n \n Value Transpose(OpBuilder* builder, Value value_to_transpose,\n@@ -106,8 +106,8 @@ Value Transpose(OpBuilder* builder, Value value_to_transpose,\n   auto elem_type = transpose_type.getElementType();\n   auto result_type = RankedTensorType::get(transpose_shape, elem_type);\n \n-  return builder->create<TF::TransposeOp>(location, result_type,\n-                                          value_to_transpose, perm_op);\n+  return TF::TransposeOp::create(*builder, location, result_type,\n+                                 value_to_transpose, perm_op);\n }\n \n Value Transpose2D(OpBuilder* builder, Value value_to_transpose,\n@@ -121,8 +121,8 @@ Value Reverse(OpBuilder* builder, Value value_to_reverse, int axis,\n               RankedTensorType type, mlir::Location location) {\n   auto axis_op = CreateI32SplatConst(builder, {1}, axis, location);\n   // The result type will be the same as the input.\n-  return builder->create<TF::ReverseV2Op>(location, type, value_to_reverse,\n-                                          axis_op);\n+  return TF::ReverseV2Op::create(*builder, location, type, value_to_reverse,\n+                                 axis_op);\n }\n \n ArrayRef<int64_t> GetRankedTensorShape(Value value) {\n@@ -154,8 +154,8 @@ Value SliceRankedTensor(OpBuilder* builder, Value input,\n   auto slice_i2c_size =\n       CreateI64DenseConst(builder, size_shape, size_values, location);\n \n-  return builder->create<TF::SliceOp>(\n-      location,\n+  return TF::SliceOp::create(\n+      *builder, location,\n       RankedTensorType::get(\n           size_values,\n           mlir::cast<RankedTensorType>(input.getType()).getElementType()),\n@@ -175,9 +175,9 @@ Value CreateStridedSliceOp(mlir::Location loc, ArrayRef<int64_t> output_shape,\n   auto end_tensor = CreateI32DenseConst(builder, end, loc);\n   auto strides_tensor = CreateI32DenseConst(builder, strides, loc);\n \n-  return builder->create<TF::StridedSliceOp>(\n-      loc, output_type, input, begin_tensor, end_tensor, strides_tensor,\n-      builder->getI64IntegerAttr(begin_mask),\n+  return TF::StridedSliceOp::create(\n+      *builder, loc, output_type, input, begin_tensor, end_tensor,\n+      strides_tensor, builder->getI64IntegerAttr(begin_mask),\n       builder->getI64IntegerAttr(end_mask),\n       builder->getI64IntegerAttr(ellipsis_mask),\n       builder->getI64IntegerAttr(new_axis_mask),\n@@ -590,21 +590,20 @@ TF::ConstOp Create1DConstantOp(const std::vector<int>& value, Location loc,\n   auto type =\n       mlir::RankedTensorType::get(value.size(), builder->getIntegerType(32));\n   auto dense_values = mlir::DenseIntElementsAttr::get(type, value);\n-  return builder->create<TF::ConstOp>(loc, dense_values);\n+  return TF::ConstOp::create(*builder, loc, dense_values);\n }\n \n TF::ConstOp CreateScalarConstantOp(int value, Location loc,\n                                    OpBuilder* builder) {\n-  return builder->create<TF::ConstOp>(loc, builder->getI32IntegerAttr(value));\n+  return TF::ConstOp::create(*builder, loc, builder->getI32IntegerAttr(value));\n }\n \n TF::ReshapeOp CreateFlattenOP(const Value& input, Location loc,\n                               OpBuilder* builder) {\n   auto output_shape = Create1DConstantOp({-1}, loc, builder);\n-  return builder->create<mlir::TF::ReshapeOp>(\n-      loc,\n-      /*tensor=*/input,\n-      /*shape=*/output_shape.getResult());\n+  return mlir::TF::ReshapeOp::create(*builder, loc,\n+                                     /*tensor=*/input,\n+                                     /*shape=*/output_shape.getResult());\n }\n \n LogicalResult CreateEqualSizeSplitVOp(Value input, int axis, int splits,\n@@ -637,9 +636,9 @@ LogicalResult CreateEqualSizeSplitVOp(Value input, int axis, int splits,\n       builder);\n \n   auto axis_op = CreateScalarConstantOp(axis, loc, builder);\n-  *result = builder->create<TF::SplitVOp>(loc, output_types, input,\n-                                          size_of_splits_op.getResult(),\n-                                          axis_op.getResult());\n+  *result =\n+      TF::SplitVOp::create(*builder, loc, output_types, input,\n+                           size_of_splits_op.getResult(), axis_op.getResult());\n   return success();\n }\n \n@@ -771,8 +770,8 @@ LogicalResult ConvertKerasLSTMLayer(mlir::func::FuncOp func_op,\n       mlir::cast<RankedTensorType>(final_inputs.getType()).getElementType());\n \n   Value none = CreateNoneValue(builder, func_op.getLoc());\n-  auto lstm = builder->create<mlir::TFL::UnidirectionalSequenceLSTMOp>(\n-      func_op.getLoc(), result_type, /*input=*/final_inputs,\n+  auto lstm = mlir::TFL::UnidirectionalSequenceLSTMOp::create(\n+      *builder, func_op.getLoc(), result_type, /*input=*/final_inputs,\n       /*input_to_input_weights=*/weights_array->getResult(0),\n       /*input_to_forget_weights=*/weights_array->getResult(1),\n       /*input_to_cell_weights=*/weights_array->getResult(2),\n@@ -881,7 +880,7 @@ LogicalResult ConvertKerasLSTMLayer(mlir::func::FuncOp func_op,\n                                           func_op.getFunctionType().getInputs(),\n                                           output_types));\n \n-  builder->create<mlir::func::ReturnOp>(func_op.getLoc(), outputs);\n+  mlir::func::ReturnOp::create(*builder, func_op.getLoc(), outputs);\n   return success();\n }\n "
        },
        {
            "sha": "59c3f883411221f1be9bde0c97e8491fd332bd5d",
            "filename": "tensorflow/compiler/mlir/lite/utils/nms_utils.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fnms_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fnms_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fnms_utils.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -57,11 +57,11 @@ void ConvertNMSPaddedFunc::RewriteFunc() {\n   auto output_type1 = func_.getFunctionType().getResult(1);\n \n   OpBuilder builder(func_.getBody());\n-  auto op = builder.create<mlir::TFL::NonMaxSuppressionV4Op>(\n-      func_.getLoc(), output_type0, output_type1, boxes, scores,\n+  auto op = mlir::TFL::NonMaxSuppressionV4Op::create(\n+      builder, func_.getLoc(), output_type0, output_type1, boxes, scores,\n       max_output_size, iou_threshold, score_threshold);\n \n-  builder.create<mlir::func::ReturnOp>(func_.getLoc(), op.getResults());\n+  mlir::func::ReturnOp::create(builder, func_.getLoc(), op.getResults());\n }\n \n LogicalResult ConvertNMSPaddedFunc::VerifySignature() {\n@@ -102,11 +102,11 @@ LogicalResult ConvertSSDPostProcessFunc::RewriteFunc() {\n                                     custom_option_buffer))) {\n     return failure();\n   }\n-  auto op = builder.create<CustomOp>(\n-      func_.getLoc(), func_.getFunctionType().getResults(),\n-      func_.getArguments(), kCustomSSDPostprocessing,\n-      CustomOption(&builder, custom_option_buffer));\n-  builder.create<func::ReturnOp>(func_.getLoc(), op.getResults());\n+  auto op = CustomOp::create(builder, func_.getLoc(),\n+                             func_.getFunctionType().getResults(),\n+                             func_.getArguments(), kCustomSSDPostprocessing,\n+                             CustomOption(&builder, custom_option_buffer));\n+  func::ReturnOp::create(builder, func_.getLoc(), op.getResults());\n \n   return success();\n }"
        },
        {
            "sha": "4bcf4b86e0ea173ce02dee18e888eec61d73c0e1",
            "filename": "tensorflow/compiler/mlir/lite/utils/perception_ops_utils.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fperception_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fperception_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fperception_ops_utils.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -104,11 +104,11 @@ LogicalResult ConvertMaxUnpoolingFunc::RewriteFunc() {\n   if (failed(CreateCustomOptions(custom_option_buffer))) {\n     return failure();\n   }\n-  auto op = builder.create<CustomOp>(\n-      func_.getLoc(), func_.getFunctionType().getResults(),\n-      func_.getArguments(), kMaxUnpooling,\n-      CustomOption(&builder, custom_option_buffer));\n-  builder.create<func::ReturnOp>(func_.getLoc(), op.getResults());\n+  auto op = CustomOp::create(builder, func_.getLoc(),\n+                             func_.getFunctionType().getResults(),\n+                             func_.getArguments(), kMaxUnpooling,\n+                             CustomOption(&builder, custom_option_buffer));\n+  func::ReturnOp::create(builder, func_.getLoc(), op.getResults());\n \n   return success();\n }\n@@ -205,11 +205,11 @@ LogicalResult ConvertDenseImageWarpFunc::RewriteFunc() {\n                  StringAttr::get(func_.getContext(), kImageWarping));\n \n   OpBuilder builder(func_.getBody());\n-  auto op = builder.create<CustomOp>(func_.getLoc(),\n-                                     func_.getFunctionType().getResults(),\n-                                     func_.getArguments(), kImageWarping,\n-                                     CustomOption(&builder, /*content=*/\"\"));\n-  builder.create<func::ReturnOp>(func_.getLoc(), op.getResults());\n+  auto op = CustomOp::create(builder, func_.getLoc(),\n+                             func_.getFunctionType().getResults(),\n+                             func_.getArguments(), kImageWarping,\n+                             CustomOption(&builder, /*content=*/\"\"));\n+  func::ReturnOp::create(builder, func_.getLoc(), op.getResults());\n \n   return success();\n }"
        },
        {
            "sha": "f3917e32d9112672e48a6d6887f6b7a34f9269f0",
            "filename": "tensorflow/compiler/mlir/lite/utils/region_isolation_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fregion_isolation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fregion_isolation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Fregion_isolation_test.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -85,7 +85,7 @@ TEST(RegionIsolationTest, CaseOp) {\n \n   OpBuilder b(&ctx);\n \n-  OwningOpRef<ModuleOp> root(b.create<ModuleOp>(b.getUnknownLoc()));\n+  OwningOpRef<ModuleOp> root(ModuleOp::create(b, b.getUnknownLoc()));\n \n   {\n     auto& block = root->getBodyRegion().front();"
        },
        {
            "sha": "a402deb4bc230effa2bd3109b7f31da9ad36507c",
            "filename": "tensorflow/compiler/mlir/lite/utils/tftext_utils.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Ftftext_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Ftftext_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Futils%2Ftftext_utils.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -136,10 +136,10 @@ LogicalResult ConvertWhitespaceTokenizer(func::FuncOp func, llvm::StringRef api,\n   func->setAttr(kTFImplements, attr);\n   OpBuilder builder(func.getBody());\n   std::string empty_option_buffer;\n-  auto op = builder.create<CustomOp>(\n-      func.getLoc(), func.getFunctionType().getResults(), func.getArguments(),\n-      api, CustomOption(&builder, empty_option_buffer));\n-  builder.create<func::ReturnOp>(func.getLoc(), op.getResults());\n+  auto op = CustomOp::create(\n+      builder, func.getLoc(), func.getFunctionType().getResults(),\n+      func.getArguments(), api, CustomOption(&builder, empty_option_buffer));\n+  func::ReturnOp::create(builder, func.getLoc(), op.getResults());\n   return success();\n }\n \n@@ -267,10 +267,10 @@ LogicalResult ConvertNgrams(func::FuncOp func, llvm::StringRef api,\n                                       custom_option_buffer))) {\n     return failure();\n   }\n-  auto op = builder.create<CustomOp>(\n-      func.getLoc(), func.getFunctionType().getResults(), func.getArguments(),\n-      api, CustomOption(&builder, custom_option_buffer));\n-  builder.create<func::ReturnOp>(func.getLoc(), op.getResults());\n+  auto op = CustomOp::create(\n+      builder, func.getLoc(), func.getFunctionType().getResults(),\n+      func.getArguments(), api, CustomOption(&builder, custom_option_buffer));\n+  func::ReturnOp::create(builder, func.getLoc(), op.getResults());\n   return success();\n }\n \n@@ -350,10 +350,10 @@ LogicalResult ConvertSgnnProjection(func::FuncOp func, llvm::StringRef api,\n                                               custom_option_buffer))) {\n     return failure();\n   }\n-  auto op = builder.create<CustomOp>(\n-      func.getLoc(), func.getFunctionType().getResults(), func.getArguments(),\n-      api, CustomOption(&builder, custom_option_buffer));\n-  builder.create<func::ReturnOp>(func.getLoc(), op.getResults());\n+  auto op = CustomOp::create(\n+      builder, func.getLoc(), func.getFunctionType().getResults(),\n+      func.getArguments(), api, CustomOption(&builder, custom_option_buffer));\n+  func::ReturnOp::create(builder, func.getLoc(), op.getResults());\n   return success();\n }\n }  // namespace"
        },
        {
            "sha": "89896d69079c28cf46d46bf3d7595895d7744822",
            "filename": "tensorflow/compiler/mlir/quantization/common/attrs_and_constraints.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fcommon%2Fattrs_and_constraints.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fcommon%2Fattrs_and_constraints.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fcommon%2Fattrs_and_constraints.h?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -89,12 +89,12 @@ Value CreateConstValue(OpBuilder& builder, const Location loc,\n         RankedTensorType::get(shape, builder.getIntegerType(sizeof(T) * 8));\n \n     const auto attr = DenseIntElementsAttr::get(shape_type, values);\n-    return builder.create<TF::ConstOp>(loc, attr);\n+    return TF::ConstOp::create(builder, loc, attr);\n   }\n \n   const auto type = RankedTensorType::get(shape, builder.getF32Type());\n   const auto value_attr = DenseFPElementsAttr::get(type, values);\n-  return builder.create<TF::ConstOp>(loc, value_attr);\n+  return TF::ConstOp::create(builder, loc, value_attr);\n }\n \n // Creates a 1D array with integer/float type."
        },
        {
            "sha": "5f43083540831f817be2b9f2342491329eb24e30",
            "filename": "tensorflow/compiler/mlir/quantization/common/quantization_lib/quantization_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fcommon%2Fquantization_lib%2Fquantization_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fcommon%2Fquantization_lib%2Fquantization_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fcommon%2Fquantization_lib%2Fquantization_utils.h?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -288,10 +288,10 @@ struct ConvertStatsToQDQs\n     rewriter.setInsertionPointAfter(op.getOperation());\n     Type result_type = quant_type.castFromExpressedType(op.getType());\n     auto q =\n-        rewriter.create<QuantizeOpT>(op.getLoc(), result_type, op.getArg());\n+        QuantizeOpT::create(rewriter, op.getLoc(), result_type, op.getArg());\n     q->setAttr(kVolatileOpAttrName, rewriter.getUnitAttr());\n \n-    auto dq = rewriter.create<DequantizeOpT>(op.getLoc(), op.getType(), q);\n+    auto dq = DequantizeOpT::create(rewriter, op.getLoc(), op.getType(), q);\n     op.getResult().replaceAllUsesWith(dq);\n     q.getOperation()->replaceUsesOfWith(dq, op.getArg());\n     op.erase();\n@@ -644,8 +644,8 @@ class QuantizationPattern : public RewritePattern {\n             if (!matchPattern(q.getOperand(), m_Constant(&attr))) {\n               continue;\n             }\n-            auto cst = rewriter.create<arith::ConstantOp>(\n-                quantized_op->getLoc(), attr);\n+            auto cst = arith::ConstantOp::create(rewriter,\n+                                                 quantized_op->getLoc(), attr);\n             quantizing_op->setOperand(i, cst.getResult());\n           }\n         }"
        },
        {
            "sha": "4203d7824844f9225ec70b7164d33d309d2a2a13",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/bridge/convert_tf_quant_to_mhlo_int_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fbridge%2Fconvert_tf_quant_to_mhlo_int_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fbridge%2Fconvert_tf_quant_to_mhlo_int_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fbridge%2Fconvert_tf_quant_to_mhlo_int_test.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -125,9 +125,9 @@ class ConvertTfQuantToMhloIntTest : public Test {\n       // can't lower tf.Const.\n       Value cst;\n       if (use_mhlo_const) {\n-        cst = builder.create<mhlo::ConstantOp>(func_op->getLoc(), attrs);\n+        cst = mhlo::ConstantOp::create(builder, func_op->getLoc(), attrs);\n       } else {\n-        cst = builder.create<TF::ConstOp>(func_op->getLoc(), attrs);\n+        cst = TF::ConstOp::create(builder, func_op->getLoc(), attrs);\n       }\n       func_op.getArgument(i).replaceAllUsesWith(cst);\n     }"
        },
        {
            "sha": "cc63c2464349345f0770f17ffc8863774a00d6ef",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/bridge/convert_tf_quant_types.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fbridge%2Fconvert_tf_quant_types.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fbridge%2Fconvert_tf_quant_types.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fbridge%2Fconvert_tf_quant_types.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -241,8 +241,8 @@ class TFUniformQuantizedOpsPattern : public ConversionPattern {\n       Type orig_op_type = op->getOperandTypes()[i];\n       if (IsIllegalType(orig_op_type) &&\n           !IsQintValueDefinedByIntToQintCast(op->getOperand(i))) {\n-        new_operands.push_back(rewriter.create<TF::CastOp>(\n-            op->getLoc(), orig_op_type, operands[i]));\n+        new_operands.push_back(TF::CastOp::create(rewriter, op->getLoc(),\n+                                                  orig_op_type, operands[i]));\n       } else {\n         new_operands.push_back(operands[i]);\n       }\n@@ -261,8 +261,8 @@ class TFUniformQuantizedOpsPattern : public ConversionPattern {\n       Value &result = new_results[i];\n       if (IsIllegalType(result.getType()) &&\n           !IsQintValueQintToIntCast(op->getResult(i))) {\n-        result = rewriter.create<TF::CastOp>(\n-            op->getLoc(), ToLegalType(result.getType()), result);\n+        result = TF::CastOp::create(rewriter, op->getLoc(),\n+                                    ToLegalType(result.getType()), result);\n       }\n       // If the result is already consumed by qint->int CastOp, manually replace\n       // its use by the new UQ op. This is because such CastOp is already legal,"
        },
        {
            "sha": "1dd93a9b2c165e7dceee7c98113d858e7cd4d256",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/convert_func_to_bfloat16.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fconvert_func_to_bfloat16.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fconvert_func_to_bfloat16.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fconvert_func_to_bfloat16.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -177,15 +177,17 @@ class BitcastConvertOpPattern\n       return failure();\n     } else if (is_input_legal) {\n       // output is f32, we bitcast_convert to f32 and then convert to bf16.\n-      const Value output = rewriter.create<mlir::stablehlo::BitcastConvertOp>(\n-          op->getLoc(), op.getResult().getType(), adaptor.getOperand());\n+      const Value output = mlir::stablehlo::BitcastConvertOp::create(\n+          rewriter, op->getLoc(), op.getResult().getType(),\n+          adaptor.getOperand());\n       rewriter.replaceOpWithNewOp<mlir::stablehlo::ConvertOp>(\n           op, getTypeConverter()->convertType(op.getResult().getType()),\n           output);\n     } else if (is_output_legal) {\n       // input is f32, we convert from bf16 and then bitcast_convert.\n-      const Value output = rewriter.create<mlir::stablehlo::ConvertOp>(\n-          op->getLoc(), op.getOperand().getType(), adaptor.getOperand());\n+      const Value output = mlir::stablehlo::ConvertOp::create(\n+          rewriter, op->getLoc(), op.getOperand().getType(),\n+          adaptor.getOperand());\n       rewriter.replaceOpWithNewOp<mlir::stablehlo::BitcastConvertOp>(\n           op, op.getResult().getType(), output);\n     } else {"
        },
        {
            "sha": "a63ffb1504bd855bf9bbb77a11f36e5252a7e1db",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/convert_xla_call_module_op_to_bfloat16.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fconvert_xla_call_module_op_to_bfloat16.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fconvert_xla_call_module_op_to_bfloat16.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fconvert_xla_call_module_op_to_bfloat16.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -123,8 +123,8 @@ void ConvertXlaCallModuleOpToBfloat16Pass::runOnOperation() {\n     builder.setInsertionPoint(op);\n     for (auto& op_operand : op->getOpOperands()) {\n       if (quant::stablehlo::IsLargeFloatType(op_operand.get().getType())) {\n-        op_operand.set(builder.create<TF::CastOp>(\n-            op->getLoc(),\n+        op_operand.set(TF::CastOp::create(\n+            builder, op->getLoc(),\n             quant::stablehlo::ToBfloat16Type(op_operand.get().getType()),\n             op_operand.get()));\n       }\n@@ -135,7 +135,7 @@ void ConvertXlaCallModuleOpToBfloat16Pass::runOnOperation() {\n         const Type original_type = op_result.getType();\n         op_result.setType(quant::stablehlo::ToBfloat16Type(original_type));\n         const Value cast =\n-            builder.create<TF::CastOp>(op->getLoc(), original_type, op_result);\n+            TF::CastOp::create(builder, op->getLoc(), original_type, op_result);\n         op_result.replaceAllUsesExcept(cast, cast.getDefiningOp());\n       }\n     }"
        },
        {
            "sha": "08befa7708297cbf4e5910b70ffe0f08f7bc4b64",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/defer_activation_transpose.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fdefer_activation_transpose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fdefer_activation_transpose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fdefer_activation_transpose.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -58,8 +58,8 @@ LogicalResult IsTransposeOpWithPermuation(Operation* absl_nullable op,\n // The Location is set as `input`'s loc.\n TransposeOp CreateTransposeOp(Value input, const ArrayRef<int64_t> permutation,\n                               PatternRewriter& rewriter) {\n-  return rewriter.create<TransposeOp>(\n-      input.getLoc(), input, rewriter.getDenseI64ArrayAttr(permutation));\n+  return TransposeOp::create(rewriter, input.getLoc(), input,\n+                             rewriter.getDenseI64ArrayAttr(permutation));\n }\n \n // Defers the transpose of the left-hand side (LHS) to the right-hand side and\n@@ -77,7 +77,7 @@ void DeferRhsTransposeForBinaryOp(OpT op, PatternRewriter& rewriter) {\n       /*input=*/rhs, kNchwToNhwcPermutation, rewriter);\n \n   auto new_binary_op =\n-      rewriter.create<OpT>(op.getLoc(), lhs_pre_transpose, rhs_transpose_op);\n+      OpT::create(rewriter, op.getLoc(), lhs_pre_transpose, rhs_transpose_op);\n \n   // NHWC -> NCHW for the output, to match the shapes of `op`'s users.\n   TransposeOp output_transpose_op = CreateTransposeOp(\n@@ -166,23 +166,22 @@ class DeferActivationTransposeForMaxPoolReduceWindowOp\n \n     // Create a new `stablehlo.reduce_window` with all relevant attributes\n     // permutated to match the new operand & result type.\n-    auto new_reduce_window_op =\n-        rewriter.create<mlir::stablehlo::ReduceWindowOp>(\n-            op.getLoc(), new_result_type, transpose_op.getOperand(),\n-            /*init_value=*/op.getOperand(1),\n-            /*window_dimensions=*/\n-            PermuteI64ArrayAttr(rewriter, op.getWindowDimensions(),\n-                                kNchwToNhwcPermutation),\n-            /*window_strides=*/\n-            PermuteI64ArrayAttr(rewriter, op.getWindowStrides(),\n-                                kNchwToNhwcPermutation),\n-            /*base_dilations=*/\n-            PermuteI64ArrayAttr(rewriter, op.getBaseDilations(),\n-                                kNchwToNhwcPermutation),\n-            /*window_dilations=*/\n-            PermuteI64ArrayAttr(rewriter, op.getWindowDilations(),\n-                                kNchwToNhwcPermutation),\n-            /*padding=*/DenseIntElementsAttr(nullptr));\n+    auto new_reduce_window_op = mlir::stablehlo::ReduceWindowOp::create(\n+        rewriter, op.getLoc(), new_result_type, transpose_op.getOperand(),\n+        /*init_value=*/op.getOperand(1),\n+        /*window_dimensions=*/\n+        PermuteI64ArrayAttr(rewriter, op.getWindowDimensions(),\n+                            kNchwToNhwcPermutation),\n+        /*window_strides=*/\n+        PermuteI64ArrayAttr(rewriter, op.getWindowStrides(),\n+                            kNchwToNhwcPermutation),\n+        /*base_dilations=*/\n+        PermuteI64ArrayAttr(rewriter, op.getBaseDilations(),\n+                            kNchwToNhwcPermutation),\n+        /*window_dilations=*/\n+        PermuteI64ArrayAttr(rewriter, op.getWindowDilations(),\n+                            kNchwToNhwcPermutation),\n+        /*padding=*/DenseIntElementsAttr(nullptr));\n \n     // Clone the reduce body. It is not affected by the permutation.\n     IRMapping mapping;"
        },
        {
            "sha": "f4648f9a0a0362d83dc13f64e167190d38ea6bb3",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/fold_constant_transpose.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Ffold_constant_transpose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Ffold_constant_transpose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Ffold_constant_transpose.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -160,8 +160,8 @@ class FoldTransposedConstantOp\n                                      /*elementType=*/rewriter.getF32Type());\n     auto new_value_attr =\n         DenseFPElementsAttr::get(new_value_type, std::move(transposed_values));\n-    auto new_const_op = rewriter.create<mlir::stablehlo::ConstantOp>(\n-        combined_loc, new_value_attr);\n+    auto new_const_op = mlir::stablehlo::ConstantOp::create(\n+        rewriter, combined_loc, new_value_attr);\n \n     rewriter.replaceAllUsesWith(op, new_const_op);\n     return success();"
        },
        {
            "sha": "05a826b14b010a7f49c80a998917e874325f5617",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/insert_calibration_statistics_saver.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Finsert_calibration_statistics_saver.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Finsert_calibration_statistics_saver.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Finsert_calibration_statistics_saver.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -86,9 +86,9 @@ LogicalResult InsertCalibrationStatisticsSaverOp(\n   ArrayAttr ids_attr = builder.getStrArrayAttr(ids);\n   ArrayAttr calibration_methods_attr =\n       builder.getI32ArrayAttr(calibration_methods);\n-  builder.create<TF::CalibrationStatisticsSaverOp>(\n-      region.getLoc(), statistics_outputs, output_file_path_attr, ids_attr,\n-      calibration_methods_attr);\n+  TF::CalibrationStatisticsSaverOp::create(\n+      builder, region.getLoc(), statistics_outputs, output_file_path_attr,\n+      ids_attr, calibration_methods_attr);\n   return success();\n }\n "
        },
        {
            "sha": "71a5b35e3514951649ac2631c656545df48e253c",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/merge_fusion_with_dequantize.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fmerge_fusion_with_dequantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fmerge_fusion_with_dequantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fmerge_fusion_with_dequantize.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -114,20 +114,21 @@ class MergeFusionWithUniformDequantizePattern\n \n     // Modify the quantized fused function to do dequantize+relu(6).\n     rewriter.setInsertionPoint(req_op);\n-    Value new_result = rewriter.create<mlir::stablehlo::UniformDequantizeOp>(\n-        req_op.getLoc(), func_op.getResultTypes()[0], req_op.getOperand());\n+    Value new_result = mlir::stablehlo::UniformDequantizeOp::create(\n+        rewriter, req_op.getLoc(), func_op.getResultTypes()[0],\n+        req_op.getOperand());\n     if (func_name.contains(\"_relu6_\")) {\n-      auto min = rewriter.create<mlir::stablehlo::ConstantOp>(\n-          req_op.getLoc(), rewriter.getF32FloatAttr(0));\n-      auto max = rewriter.create<mlir::stablehlo::ConstantOp>(\n-          req_op.getLoc(), rewriter.getF32FloatAttr(6));\n-      new_result = rewriter.create<mlir::stablehlo::ClampOp>(\n-          req_op.getLoc(), min, new_result, max);\n+      auto min = mlir::stablehlo::ConstantOp::create(\n+          rewriter, req_op.getLoc(), rewriter.getF32FloatAttr(0));\n+      auto max = mlir::stablehlo::ConstantOp::create(\n+          rewriter, req_op.getLoc(), rewriter.getF32FloatAttr(6));\n+      new_result = mlir::stablehlo::ClampOp::create(rewriter, req_op.getLoc(),\n+                                                    min, new_result, max);\n     } else if (func_name.contains(\"_relu_\")) {\n-      auto min = rewriter.create<mlir::stablehlo::ConstantOp>(\n-          req_op.getLoc(), rewriter.getF32FloatAttr(0));\n-      new_result = rewriter.create<mlir::chlo::BroadcastMaxOp>(\n-          req_op.getLoc(), min, new_result, nullptr);\n+      auto min = mlir::stablehlo::ConstantOp::create(\n+          rewriter, req_op.getLoc(), rewriter.getF32FloatAttr(0));\n+      new_result = mlir::chlo::BroadcastMaxOp::create(rewriter, req_op.getLoc(),\n+                                                      min, new_result, nullptr);\n     }\n     return_op->setOperand(0, new_result);\n     rewriter.eraseOp(req_op);"
        },
        {
            "sha": "1c425487799962058e0801202ea0708862837850",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/nchw_convolution_to_nhwc.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fnchw_convolution_to_nhwc.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fnchw_convolution_to_nhwc.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fnchw_convolution_to_nhwc.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -76,17 +76,19 @@ class RewriteNchwConvolutionToNhwc\n     const TensorType new_input_tensor_type = GetTransposedTensorType(\n         mlir::cast<TensorType>(input.getType()), kNchwToNhwcPermutation);\n \n-    auto input_transpose_op = rewriter.create<mlir::stablehlo::TransposeOp>(\n-        op.getLoc(), /*resultType0=*/new_input_tensor_type, /*operand=*/input,\n+    auto input_transpose_op = mlir::stablehlo::TransposeOp::create(\n+        rewriter, op.getLoc(), /*resultType0=*/new_input_tensor_type,\n+        /*operand=*/input,\n         rewriter.getDenseI64ArrayAttr(kNchwToNhwcPermutation));\n \n     // Transpose the filter tensor: [o, i, 0, 1] => [0, 1, i, o]\n     Value filter = op->getOperand(1);\n     const TensorType new_filter_tensor_type = GetTransposedTensorType(\n         mlir::cast<TensorType>(filter.getType()), kOihwToHwioPermutation);\n \n-    auto filter_transpose_op = rewriter.create<mlir::stablehlo::TransposeOp>(\n-        op.getLoc(), /*resultType0=*/new_filter_tensor_type, /*operand=*/filter,\n+    auto filter_transpose_op = mlir::stablehlo::TransposeOp::create(\n+        rewriter, op.getLoc(), /*resultType0=*/new_filter_tensor_type,\n+        /*operand=*/filter,\n         rewriter.getDenseI64ArrayAttr(kOihwToHwioPermutation));\n \n     // [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]\n@@ -108,8 +110,8 @@ class RewriteNchwConvolutionToNhwc\n     // reused without modification because the ordering of spatial dimensions\n     // is not modified (i.e. before: [b, f, 0, 1], after: [b, 0, 1, f] => the\n     // spatial dimension is still ordered as {0, 1}).\n-    auto new_convolution_op = rewriter.create<mlir::stablehlo::ConvolutionOp>(\n-        op.getLoc(), /*resultType0=*/new_conv_output_tensor_type,\n+    auto new_convolution_op = mlir::stablehlo::ConvolutionOp::create(\n+        rewriter, op.getLoc(), /*resultType0=*/new_conv_output_tensor_type,\n         /*lhs=*/input_transpose_op,\n         /*rhs=*/filter_transpose_op,\n         /*window_strides=*/op.getWindowStridesAttr(),\n@@ -125,8 +127,9 @@ class RewriteNchwConvolutionToNhwc\n     // Transpose the output of the `ConvolutionOp` back to the original op's\n     // output shape so that users' shapes match.\n     // [b, 0, 1, f] => [b, f, 0, 1]\n-    auto output_transpose_op = rewriter.create<mlir::stablehlo::TransposeOp>(\n-        new_convolution_op.getLoc(), /*resultType0=*/output_tensor_type,\n+    auto output_transpose_op = mlir::stablehlo::TransposeOp::create(\n+        rewriter, new_convolution_op.getLoc(),\n+        /*resultType0=*/output_tensor_type,\n         /*operand=*/new_convolution_op,\n         rewriter.getDenseI64ArrayAttr(kNhwcToNchwPermutation));\n "
        },
        {
            "sha": "4dff113b6427c953de219cfeb6a491a64e939790",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/prepare_quantize.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fprepare_quantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fprepare_quantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fprepare_quantize.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -95,8 +95,8 @@ class MergeConsecutiveQuantizeCast\n         q_op.getArg().getDefiningOp<mlir::quant::ir::QuantizeCastOp>();\n     if (!preceding_qcast) return failure();\n \n-    auto new_qcast = rewriter.create<mlir::quant::ir::QuantizeCastOp>(\n-        q_op.getLoc(), q_op.getType(), preceding_qcast.getArg());\n+    auto new_qcast = mlir::quant::ir::QuantizeCastOp::create(\n+        rewriter, q_op.getLoc(), q_op.getType(), preceding_qcast.getArg());\n     new_qcast->setAttr(kVolatileOpAttrName, rewriter.getUnitAttr());\n     q_op->replaceAllUsesWith(new_qcast);\n     return success();"
        },
        {
            "sha": "e65d5423458f501bd4cd8d3aa39f696e470195ad",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/quantize_weight.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fquantize_weight.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fquantize_weight.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Fquantize_weight.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -163,8 +163,8 @@ class QuantizeWeight : public OpRewritePattern<ConstantOp> {\n       }\n     }\n     rewriter.setInsertionPointAfter(op);\n-    ConvertOp new_convert_op = rewriter.create<ConvertOp>(\n-        op->getLoc(), new_result_type, op.getResult());\n+    ConvertOp new_convert_op = ConvertOp::create(\n+        rewriter, op->getLoc(), new_result_type, op.getResult());\n     quantizable_op->setOperand(quantize_operand_num,\n                                new_convert_op.getResult());\n   }\n@@ -203,10 +203,10 @@ class QuantizeWeight : public OpRewritePattern<ConstantOp> {\n       // of its number of users.\n       rewriter.setInsertionPointAfter(op);\n       // create new F16 constant op in that location\n-      ConstantOp new_const = rewriter.create<ConstantOp>(\n-          op->getLoc(), new_result_type, new_value_attr);\n+      ConstantOp new_const = ConstantOp::create(\n+          rewriter, op->getLoc(), new_result_type, new_value_attr);\n       ConvertOp dcast =\n-          rewriter.create<ConvertOp>(op->getLoc(), old_result_type, new_const);\n+          ConvertOp::create(rewriter, op->getLoc(), old_result_type, new_const);\n       // replace all convert ops with dq op.\n       convert_op->replaceAllUsesWith(dcast);\n       // Return without scanning for the next ConvertOp as only one ConvertOp is"
        },
        {
            "sha": "46da2a3f25b82cc497db9372f29f49e6be2eff5a",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/passes/unwrap_xla_call_module_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Funwrap_xla_call_module_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Funwrap_xla_call_module_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpasses%2Funwrap_xla_call_module_op.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -82,8 +82,8 @@ void UnwrapXlaCallModuleOp(TF::XlaCallModuleOp call_op,\n   // TODO: b/310291615 - find a better way for multi-platform support.\n   if (call_op_has_platform_index_arg) {\n     arg_mapper.map(func_op.getArgument(0),\n-                   builder.create<mhlo::ConstantOp>(\n-                       func_op.getLoc(), builder.getI16IntegerAttr(0)));\n+                   mhlo::ConstantOp::create(builder, func_op.getLoc(),\n+                                            builder.getI16IntegerAttr(0)));\n   }\n   for (auto [func_arg, operand] : llvm::zip_equal(\n            func_op.getArguments().take_back(call_op.getNumOperands()),"
        },
        {
            "sha": "42bf32a27e7bee3f76eb06468ae42e53b02c972a",
            "filename": "tensorflow/compiler/mlir/quantization/tensorflow/cc/constant_fold.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fcc%2Fconstant_fold.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fcc%2Fconstant_fold.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fcc%2Fconstant_fold.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -67,7 +67,7 @@ LogicalResult FoldOperation(OpBuilder& builder, Operation* op,\n   results.clear();\n   builder.setInsertionPointAfter(op);\n   for (const auto& result_value : result_values) {\n-    results.push_back(builder.create<TF::ConstOp>(op->getLoc(), result_value));\n+    results.push_back(TF::ConstOp::create(builder, op->getLoc(), result_value));\n   }\n   return success();\n }"
        },
        {
            "sha": "c2339fe044edd73b1baf03696672aeeccbab87d5",
            "filename": "tensorflow/compiler/mlir/quantization/tensorflow/passes/cast_bf16_ops_to_f32.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fpasses%2Fcast_bf16_ops_to_f32.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fpasses%2Fcast_bf16_ops_to_f32.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fpasses%2Fcast_bf16_ops_to_f32.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -91,8 +91,8 @@ class CastBf16OpsToF32 : public RewritePattern {\n     for (int i = 0; i < op->getNumOperands(); i++) {\n       Value input = op->getOperand(i);\n       if (getElementTypeOrSelf(input).isBF16()) {\n-        Value f32_cast = rewriter.create<TF::CastOp>(\n-            op->getLoc(),\n+        Value f32_cast = TF::CastOp::create(\n+            rewriter, op->getLoc(),\n             CloneTypeWithNewElementType(input.getType(), rewriter.getF32Type()),\n             input);\n         op->setOperand(i, f32_cast);\n@@ -108,8 +108,8 @@ class CastBf16OpsToF32 : public RewritePattern {\n         for (Operation* user : op->getUsers()) {\n           for (int i = 0; i < user->getNumOperands(); i++) {\n             if (user->getOperand(i) == value) {\n-              Value bf16_cast = rewriter.create<TF::CastOp>(\n-                  user->getLoc(),\n+              Value bf16_cast = TF::CastOp::create(\n+                  rewriter, user->getLoc(),\n                   CloneTypeWithNewElementType(value.getType(),\n                                               rewriter.getBF16Type()),\n                   value);"
        },
        {
            "sha": "2ae814880fc2ff58f2ec4b2d6e38695b825339b9",
            "filename": "tensorflow/compiler/mlir/quantization/tensorflow/passes/prepare_quantize_drq.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fpasses%2Fprepare_quantize_drq.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fpasses%2Fprepare_quantize_drq.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fpasses%2Fprepare_quantize_drq.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -241,10 +241,10 @@ class PrepareDRQQuantizableOp : public OpRewritePattern<arith::ConstantOp> {\n       }\n     }\n     rewriter.setInsertionPointAfter(op);\n-    auto q = rewriter.create<mlir::quant::ir::QuantizeCastOp>(\n-        op->getLoc(), cast_type, op.getResult());\n-    auto dq = rewriter.create<mlir::quant::ir::DequantizeCastOp>(\n-        op->getLoc(), expressed_type, q);\n+    auto q = mlir::quant::ir::QuantizeCastOp::create(rewriter, op->getLoc(),\n+                                                     cast_type, op.getResult());\n+    auto dq = mlir::quant::ir::DequantizeCastOp::create(rewriter, op->getLoc(),\n+                                                        expressed_type, q);\n     quantize_op->setOperand(quantize_operand_num, dq.getResult());\n     return true;\n   }"
        },
        {
            "sha": "0c42b760557c518a5ffffd6931fe433c17a18b75",
            "filename": "tensorflow/compiler/mlir/quantization/tensorflow/utils/tf_to_xla_attribute_utils.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 37,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Futils%2Ftf_to_xla_attribute_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e685fb6e1e645a3af69c1b462d2329abdac7357/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Futils%2Ftf_to_xla_attribute_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Futils%2Ftf_to_xla_attribute_utils.cc?ref=5e685fb6e1e645a3af69c1b462d2329abdac7357",
            "patch": "@@ -32,8 +32,8 @@ namespace {\n Value GetDimValue(OpBuilder &builder, Location loc, Value shape_value,\n                   int32_t dim) {\n   Type attribute_type = builder.getI64Type();\n-  return builder.create<TF::StridedSliceOp>(\n-      loc,\n+  return TF::StridedSliceOp::create(\n+      builder, loc,\n       RankedTensorType::get(\n           {}, mlir::cast<ShapedType>(shape_value.getType()).getElementType()),\n       /*input=*/shape_value,\n@@ -60,16 +60,16 @@ void GetSamePaddingValues(OpBuilder &builder, Location loc, Value input_size,\n   Type int32_scalar_type = zero.getType();\n \n   auto scalar_add = [&](Value lhs, Value rhs) {\n-    return builder.create<TF::AddOp>(loc, int32_scalar_type, lhs, rhs);\n+    return TF::AddOp::create(builder, loc, int32_scalar_type, lhs, rhs);\n   };\n   auto scalar_mul = [&](Value lhs, Value rhs) {\n-    return builder.create<TF::MulOp>(loc, int32_scalar_type, lhs, rhs);\n+    return TF::MulOp::create(builder, loc, int32_scalar_type, lhs, rhs);\n   };\n   auto scalar_sub = [&](Value lhs, Value rhs) {\n-    return builder.create<TF::SubOp>(loc, int32_scalar_type, lhs, rhs);\n+    return TF::SubOp::create(builder, loc, int32_scalar_type, lhs, rhs);\n   };\n   auto scalar_div = [&](Value lhs, Value rhs) {\n-    return builder.create<TF::DivOp>(loc, int32_scalar_type, lhs, rhs);\n+    return TF::DivOp::create(builder, loc, int32_scalar_type, lhs, rhs);\n   };\n \n   // effective_filter_size = (filter_size - 1) * dilation_rate + 1\n@@ -90,7 +90,7 @@ void GetSamePaddingValues(OpBuilder &builder, Location loc, Value input_size,\n       scalar_add(effective_filter_size_op,\n                  scalar_mul(stride_value, scalar_sub(output_size, one))),\n       input_size);\n-  padding_needed = builder.create<TF::MaximumOp>(loc, padding_needed, zero);\n+  padding_needed = TF::MaximumOp::create(builder, loc, padding_needed, zero);\n   padding_low = scalar_div(padding_needed, two);\n   padding_high = scalar_sub(padding_needed, padding_low);\n }\n@@ -104,14 +104,15 @@ Value PadForDynamicShapedInputSamePadding(\n \n   auto reshape_op = [&](Value value, const SmallVector<int64_t> &shape) {\n     const int64_t rank = shape.size();\n-    return builder.create<TF::ReshapeOp>(\n-        loc, RankedTensorType::get(shape, builder.getI32Type()), value,\n+    return TF::ReshapeOp::create(\n+        builder, loc, RankedTensorType::get(shape, builder.getI32Type()), value,\n         CreateConstValue<int64_t>(builder, loc, {rank}, shape));\n   };\n \n   ShapedType filter_shape = mlir::cast<ShapedType>(filter.getType());\n-  Value input_shape_value = builder.create<TF::ShapeOp>(\n-      loc, RankedTensorType::get({num_dims}, builder.getI32Type()), input);\n+  Value input_shape_value = TF::ShapeOp::create(\n+      builder, loc, RankedTensorType::get({num_dims}, builder.getI32Type()),\n+      input);\n   auto scalar_to_rank1 = [&](Value value) { return reshape_op(value, {1}); };\n   for (int i : llvm::seq<int>(1, num_dims - 1)) {\n     Value input_size_i = GetDimValue(builder, loc, input_shape_value, i);\n@@ -131,12 +132,12 @@ Value PadForDynamicShapedInputSamePadding(\n       builder, loc, /*shape=*/{num_dims - 2, 2},\n       /*values=*/SmallVector<int32_t>(2 * (num_dims - 2), 0));\n   Value zero = CreateScalarConstValue(builder, loc, 0);\n-  Value temp_padding_rank1 = builder.create<TF::ConcatOp>(\n-      loc, RankedTensorType::get({2 * num_dims}, builder.getI32Type()), zero,\n-      temp_padding_values);\n+  Value temp_padding_rank1 = TF::ConcatOp::create(\n+      builder, loc, RankedTensorType::get({2 * num_dims}, builder.getI32Type()),\n+      zero, temp_padding_values);\n   Value temp_padding = reshape_op(temp_padding_rank1, {num_dims, 2});\n-  return builder.create<TF::PadV2Op>(\n-      loc, input.getType(), input, temp_padding,\n+  return TF::PadV2Op::create(\n+      builder, loc, input.getType(), input, temp_padding,\n       CreateScalarConstValue<int8_t>(builder, loc, input_zp_value));\n }\n \n@@ -224,9 +225,9 @@ Value CalculatePaddingAndPadIfNeeded(OpBuilder &builder, Location loc,\n     output_shape[i] += padding_values[2 * i] + padding_values[2 * i + 1];\n   }\n \n-  return builder.create<TF::PadV2Op>(\n-      loc, RankedTensorType::get(output_shape, builder.getI8Type()), input,\n-      temp_padding,\n+  return TF::PadV2Op::create(\n+      builder, loc, RankedTensorType::get(output_shape, builder.getI8Type()),\n+      input, temp_padding,\n       CreateScalarConstValue<int8_t>(builder, loc, input_zp_value));\n }\n \n@@ -254,7 +255,7 @@ Value PackOperand(OpBuilder &builder, Location loc, Value value, int pack_dim) {\n                                     value_type.getShape().end());\n   RankedTensorType shape_type =\n       RankedTensorType::get({rank}, builder.getI64Type());\n-  Value shape_value = builder.create<TF::ShapeOp>(loc, shape_type, value);\n+  Value shape_value = TF::ShapeOp::create(builder, loc, shape_type, value);\n \n   // It is guaranteed that packed_shape[pack_dim] is known.\n   if (packed_shape[pack_dim] % 2 != 0) {\n@@ -263,14 +264,14 @@ Value PackOperand(OpBuilder &builder, Location loc, Value value, int pack_dim) {\n     padding[pack_dim * 2 + 1] = 1;\n     Value padding_value =\n         CreateConstValue<int32_t>(builder, loc, {rank, 2}, padding);\n-    value = builder.create<TF::PadV2Op>(\n-        loc, RankedTensorType::get(packed_shape, builder.getI8Type()), value,\n-        padding_value, CreateScalarConstValue<int8_t>(builder, loc, 0));\n+    value = TF::PadV2Op::create(\n+        builder, loc, RankedTensorType::get(packed_shape, builder.getI8Type()),\n+        value, padding_value, CreateScalarConstValue<int8_t>(builder, loc, 0));\n \n     SmallVector<int64_t> shape_add(rank, 0);\n     shape_add[pack_dim] = 1;\n-    shape_value = builder.create<TF::AddOp>(\n-        loc, shape_type, shape_value,\n+    shape_value = TF::AddOp::create(\n+        builder, loc, shape_type, shape_value,\n         CreateConstValue<int64_t>(builder, loc, {rank}, shape_add));\n   }\n   packed_shape[pack_dim] /= 2;\n@@ -279,32 +280,32 @@ Value PackOperand(OpBuilder &builder, Location loc, Value value, int pack_dim) {\n \n   RankedTensorType packed_output_type =\n       RankedTensorType::get(packed_shape, builder.getI8Type());\n-  Value packed_shape_value = builder.create<TF::DivOp>(\n-      loc, shape_type, shape_value,\n+  Value packed_shape_value = TF::DivOp::create(\n+      builder, loc, shape_type, shape_value,\n       CreateConstValue<int64_t>(builder, loc, {rank}, divisor));\n \n   Value packed_low_begin_value = CreateConstValue<int64_t>(\n       builder, loc, {rank}, SmallVector<int64_t>(rank, 0));\n   Value packed_low_value =\n-      builder.create<TF::SliceOp>(loc, packed_output_type, value,\n-                                  packed_low_begin_value, packed_shape_value);\n-  packed_low_value = builder.create<TF::BitwiseAndOp>(\n-      loc, packed_output_type, packed_low_value,\n+      TF::SliceOp::create(builder, loc, packed_output_type, value,\n+                          packed_low_begin_value, packed_shape_value);\n+  packed_low_value = TF::BitwiseAndOp::create(\n+      builder, loc, packed_output_type, packed_low_value,\n       CreateScalarConstValue<int8_t>(builder, loc, 0x0F));\n \n   SmallVector<int64_t> packed_high_begin(rank, 0);\n   packed_high_begin[pack_dim] = packed_shape[pack_dim];\n   Value packed_high_begin_value =\n       CreateConstValue<int64_t>(builder, loc, {rank}, packed_high_begin);\n   Value packed_high_value =\n-      builder.create<TF::SliceOp>(loc, packed_output_type, value,\n-                                  packed_high_begin_value, packed_shape_value);\n-  packed_high_value = builder.create<TF::LeftShiftOp>(\n-      loc, packed_output_type, packed_high_value,\n+      TF::SliceOp::create(builder, loc, packed_output_type, value,\n+                          packed_high_begin_value, packed_shape_value);\n+  packed_high_value = TF::LeftShiftOp::create(\n+      builder, loc, packed_output_type, packed_high_value,\n       CreateScalarConstValue<int8_t>(builder, loc, 4));\n \n-  Operation *packed = builder.create<TF::BitwiseOrOp>(\n-      loc, packed_output_type, packed_low_value, packed_high_value);\n+  Operation* packed = TF::BitwiseOrOp::create(\n+      builder, loc, packed_output_type, packed_low_value, packed_high_value);\n   return ConstantFoldOpIfPossible(packed).front();\n }\n "
        }
    ],
    "stats": {
        "total": 2145,
        "additions": 1097,
        "deletions": 1048
    }
}