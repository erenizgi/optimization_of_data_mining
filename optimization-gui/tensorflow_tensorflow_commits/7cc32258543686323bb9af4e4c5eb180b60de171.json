{
    "author": "ezhulenev",
    "message": "[xla:cpu] Delete runtime_fp16 library\n\nPiperOrigin-RevId: 828180411",
    "sha": "7cc32258543686323bb9af4e4c5eb180b60de171",
    "files": [
        {
            "sha": "ab827f1894011e0dc8dd80b7b3de7607fe834cd1",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cc32258543686323bb9af4e4c5eb180b60de171/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cc32258543686323bb9af4e4c5eb180b60de171/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=7cc32258543686323bb9af4e4c5eb180b60de171",
            "patch": "@@ -59,7 +59,6 @@ filegroup(\n     srcs = [\n         # Single-threaded support.\n         \"runtime_custom_call_status.cc\",\n-        \"runtime_fp16.cc\",\n         \"runtime_key_value_sort.cc\",\n         \"runtime_pow.cc\",\n         \"runtime_single_threaded_conv2d.cc\",\n@@ -94,7 +93,6 @@ filegroup(\n     srcs = [\n         # Single-threaded support.\n         \"runtime_custom_call_status.h\",\n-        \"runtime_fp16.h\",\n         \"runtime_key_value_sort.h\",\n         \"runtime_pow.h\",\n         \"runtime_single_threaded_conv2d.h\",\n@@ -592,18 +590,6 @@ cc_library(\n     copts = runtime_copts(),\n )\n \n-cc_library(\n-    name = \"runtime_fp16\",\n-    srcs = [\n-        \"runtime_fp16.cc\",\n-    ],\n-    hdrs = [\n-        \"runtime_fp16.h\",\n-    ],\n-    copts = runtime_copts(),\n-    deps = [\"@com_google_absl//absl/base:core_headers\"],\n-)\n-\n cc_library(\n     name = \"runtime_pow\",\n     srcs = ["
        },
        {
            "sha": "5bca7fecee69e592f196a0b934c94330d07c1ccf",
            "filename": "third_party/xla/xla/service/cpu/runtime_fp16.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 146,
            "changes": 146,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86bf7684e5bb92fc8ffdc5e1c402e3678a778158/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_fp16.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86bf7684e5bb92fc8ffdc5e1c402e3678a778158/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_fp16.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_fp16.cc?ref=86bf7684e5bb92fc8ffdc5e1c402e3678a778158",
            "patch": "@@ -1,146 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/cpu/runtime_fp16.h\"\n-\n-#include <cstdint>\n-#include <cstring>\n-\n-#include \"absl/base/attributes.h\"\n-\n-namespace {\n-\n-// Helper class that lets us access the underlying bit representation\n-// of a float without breaking C++ strict aliasing.\n-class AliasedFloatInt {\n- public:\n-  static_assert(sizeof(float) == sizeof(uint32_t), \"\");\n-\n-  static AliasedFloatInt FromFloat(float f) {\n-    AliasedFloatInt value;\n-    value.set_float(f);\n-    return value;\n-  }\n-\n-  static AliasedFloatInt FromUInt(uint32_t u) {\n-    AliasedFloatInt value;\n-    value.set_uint(u);\n-    return value;\n-  }\n-\n-  void set_float(float f) { memcpy(&value_, &f, sizeof(f)); }\n-  float as_float() const {\n-    float f;\n-    memcpy(&f, &value_, sizeof(f));\n-    return f;\n-  }\n-\n-  void set_uint(uint32_t u) { value_ = u; }\n-  uint32_t as_uint() const { return value_; }\n-\n- private:\n-  uint32_t value_;\n-};\n-}  // namespace\n-\n-// __gnu_f2h_ieee and __gnu_h2f_ieee are marked as weak symbols so if XLA is\n-// built with compiler-rt (that also defines these symbols) we don't get a\n-// duplicate definition linker error.  Making these symbols weak also ensures\n-// that the compiler-rt definitions \"win\", but that isn't essential.\n-\n-// Algorithm copied from Eigen.\n-XlaF16ABIType ABSL_ATTRIBUTE_WEAK __gnu_f2h_ieee(float float_value) {\n-  AliasedFloatInt f = AliasedFloatInt::FromFloat(float_value);\n-\n-  const AliasedFloatInt f32infty = AliasedFloatInt::FromUInt(255 << 23);\n-  const AliasedFloatInt f16max = AliasedFloatInt::FromUInt((127 + 16) << 23);\n-  const AliasedFloatInt denorm_magic =\n-      AliasedFloatInt::FromUInt(((127 - 15) + (23 - 10) + 1) << 23);\n-  unsigned int sign_mask = 0x80000000u;\n-  uint32_t o = static_cast<uint16_t>(0x0u);\n-\n-  unsigned int sign = f.as_uint() & sign_mask;\n-  f.set_uint(f.as_uint() ^ sign);\n-\n-  // NOTE all the integer compares in this function can be safely\n-  // compiled into signed compares since all operands are below\n-  // 0x80000000. Important if you want fast straight SSE2 code\n-  // (since there's no unsigned PCMPGTD).\n-\n-  if (f.as_uint() >=\n-      f16max.as_uint()) {  // result is Inf or NaN (all exponent bits set)\n-    o = (f.as_uint() > f32infty.as_uint()) ? 0x7e00\n-                                           : 0x7c00;  // NaN->qNaN and Inf->Inf\n-  } else {                            // (De)normalized number or zero\n-    if (f.as_uint() < (113 << 23)) {  // resulting FP16 is subnormal or zero\n-      // use a magic value to align our 10 mantissa bits at the bottom of\n-      // the float. as long as FP addition is round-to-nearest-even this\n-      // just works.\n-      f.set_float(f.as_float() + denorm_magic.as_float());\n-\n-      // and one integer subtract of the bias later, we have our final float!\n-      o = static_cast<uint16_t>(f.as_uint() - denorm_magic.as_uint());\n-    } else {\n-      unsigned int mant_odd =\n-          (f.as_uint() >> 13) & 1;  // resulting mantissa is odd\n-\n-      // update exponent, rounding bias part 1\n-      f.set_uint(f.as_uint() + (static_cast<unsigned int>(15 - 127) << 23) +\n-                 0xfff);\n-      // rounding bias part 2\n-      f.set_uint(f.as_uint() + mant_odd);\n-      // take the bits!\n-      o = static_cast<uint16_t>(f.as_uint() >> 13);\n-    }\n-  }\n-\n-  o |= static_cast<uint16_t>(sign >> 16);\n-  // The output can be a float type, bitcast it from uint16_t.\n-  auto ho = static_cast<uint16_t>(o);\n-  XlaF16ABIType ret = 0;\n-  std::memcpy(&ret, &ho, sizeof(ho));\n-  return ret;\n-}\n-\n-// Algorithm copied from Eigen.\n-float ABSL_ATTRIBUTE_WEAK __gnu_h2f_ieee(XlaF16ABIType hf) {\n-  const AliasedFloatInt magic = AliasedFloatInt::FromUInt(113 << 23);\n-  const unsigned int shifted_exp = 0x7c00 << 13;  // exponent mask after shift\n-  AliasedFloatInt o;\n-\n-  // The input can be a float type, bitcast it to uint16_t.\n-  uint16_t h;\n-  std::memcpy(&h, &hf, sizeof(h));\n-  o.set_uint((h & 0x7fff) << 13);                // exponent/mantissa bits\n-  unsigned int exp = shifted_exp & o.as_uint();  // just the exponent\n-  o.set_uint(o.as_uint() + ((127 - 15) << 23));  // exponent adjust\n-\n-  // handle exponent special cases\n-  if (exp == shifted_exp) {                        // Inf/NaN?\n-    o.set_uint(o.as_uint() + ((128 - 16) << 23));  // extra exp adjust\n-  } else if (exp == 0) {                           // Zero/Denormal?\n-    o.set_uint(o.as_uint() + (1 << 23));           // extra exp adjust\n-    o.set_float(o.as_float() - magic.as_float());  // renormalize\n-  }\n-\n-  o.set_uint(o.as_uint() | (h & 0x8000) << 16);  // sign bit\n-  return o.as_float();\n-}\n-\n-XlaF16ABIType ABSL_ATTRIBUTE_WEAK __truncdfhf2(double d) {\n-  // This does a double rounding step, but it's precise enough for our use\n-  // cases.\n-  return __gnu_f2h_ieee(static_cast<float>(d));\n-}"
        },
        {
            "sha": "c86d6dc37f0d5cbe95baea0d38c7f3f987e6328f",
            "filename": "third_party/xla/xla/service/cpu/runtime_fp16.h",
            "status": "removed",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86bf7684e5bb92fc8ffdc5e1c402e3678a778158/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_fp16.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86bf7684e5bb92fc8ffdc5e1c402e3678a778158/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_fp16.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_fp16.h?ref=86bf7684e5bb92fc8ffdc5e1c402e3678a778158",
            "patch": "@@ -1,46 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_SERVICE_CPU_RUNTIME_FP16_H_\n-#define XLA_SERVICE_CPU_RUNTIME_FP16_H_\n-\n-#include <stdint.h>\n-\n-// _Float16 always gets us the correct ABI type, so use that if available.\n-// AArch64 GCC defines __FLT16_MANT_DIG__ even when _Float16 is not available.\n-#if defined(__FLT16_MANT_DIG__) && \\\n-    (defined(__clang__) || !(defined(__GNUC__) && defined(__aarch64__)))\n-using XlaF16ABIType = _Float16;\n-#elif defined(__x86_64__)\n-// Older versions of Clang don't have _Float16. Since both float and _Float16\n-// are passed in the same register we can use the wider type and careful casting\n-// to conform to x86_64 psABI. This only works with the assumption that we're\n-// dealing with little-endian values passed in wider registers.\n-using XlaF16ABIType = float;\n-#else\n-// Default to uint16_t if we have nothing else.\n-using XlaF16ABIType = uint16_t;\n-#endif\n-\n-// Converts an F32 value to a F16.\n-extern \"C\" XlaF16ABIType __gnu_f2h_ieee(float);\n-\n-// Converts an F16 value to a F32.\n-extern \"C\" float __gnu_h2f_ieee(XlaF16ABIType);\n-\n-// Converts an F64 value to a F16.\n-extern \"C\" XlaF16ABIType __truncdfhf2(double);\n-\n-#endif  // XLA_SERVICE_CPU_RUNTIME_FP16_H_"
        }
    ],
    "stats": {
        "total": 206,
        "additions": 0,
        "deletions": 206
    }
}