{
    "author": "tensorflower-gardener",
    "message": "Reverts 677d64b51b3c72760aeac1994f17fc8665f97205\n\nPiperOrigin-RevId: 829125592",
    "sha": "e6aa1c1345c09bafcc7a199fae3b981eab756107",
    "files": [
        {
            "sha": "2e8b47a2d9116257478137bd245e59f4b128de07",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 14,
            "deletions": 45,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e6aa1c1345c09bafcc7a199fae3b981eab756107/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e6aa1c1345c09bafcc7a199fae3b981eab756107/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=e6aa1c1345c09bafcc7a199fae3b981eab756107",
            "patch": "@@ -872,6 +872,7 @@ xla_test(\n         \"//xla/service\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n     ],\n )\n@@ -1030,16 +1031,13 @@ xla_test(\n         # TODO(b/445172709): Re-enable once fixed.\n         \"b200\": [\"broken\"],\n     },\n-    precompile_test = False,\n     shard_count = 20,\n     tags = [\n         \"optonly\",\n-        \"test_migrated_to_hlo_runner_pjrt\",\n     ],\n     deps = [\n-        \":client_library_test_runner_mixin\",\n-        \":hlo_pjrt_interpreter_reference_mixin\",\n-        \":hlo_pjrt_test_base\",\n+        \":client_library_test_base\",\n+        \":hlo_test_base\",\n         \":xla_internal_test_main\",\n         \"//xla:array2d\",\n         \"//xla:array3d\",\n@@ -1060,18 +1058,15 @@ xla_test(\n         \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/hlo/testlib:test_helpers\",\n         \"//xla/service\",\n-        \"//xla/service:hlo_runner_interface\",\n         \"//xla/service:platform_util\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tests:xla_test_backend_predicates\",\n-        \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/platform:test_benchmark\",\n         \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n@@ -1097,43 +1092,33 @@ xla_test(\n         \"b200\": [\"broken\"],\n     },\n     backends = [\"gpu\"],\n-    precompile_test = False,\n     shard_count = 20,\n     tags = [\n         \"optonly\",\n         # TODO(b/151340488): Timed out on 2020-03-12.\n         \"nozapfhahn\",\n-        \"test_migrated_to_hlo_runner_pjrt\",\n     ],\n     deps = [\n-        \":client_library_test_runner_mixin\",\n-        \":hlo_pjrt_interpreter_reference_mixin\",\n-        \":hlo_pjrt_test_base\",\n+        \":client_library_test_base\",\n+        \":hlo_test_base\",\n         \":xla_internal_test_main\",\n         \"//xla:array2d\",\n         \"//xla:array3d\",\n         \"//xla:error_spec\",\n-        \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla:reference_util\",\n         \"//xla:shape_util\",\n-        \"//xla/client:client_library\",\n         \"//xla/client:local_client\",\n         \"//xla/hlo/builder:xla_builder\",\n         \"//xla/hlo/builder/lib:arithmetic\",\n         \"//xla/hlo/builder/lib:matrix\",\n         \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/service:hlo_runner_interface\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tests:xla_test_backend_predicates\",\n-        \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/platform:test_benchmark\",\n-        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@eigen_archive//:eigen3\",\n         \"@local_tsl//tsl/platform:ml_dtypes\",\n         \"@local_tsl//tsl/platform:test\",\n         \"@local_tsl//tsl/platform:test_benchmark\",\n@@ -1157,42 +1142,35 @@ xla_test(\n         \"gpu\",\n         \"interpreter\",\n     ],\n-    precompile_test = False,\n     shard_count = 50,\n     tags = [\n         \"nozapfhahn\",\n         \"optonly\",\n-        \"test_migrated_to_hlo_runner_pjrt\",\n     ],\n     deps = [\n-        \":client_library_test_runner_mixin\",\n-        \":hlo_pjrt_interpreter_reference_mixin\",\n-        \":hlo_pjrt_test_base\",\n+        \":client_library_test_base\",\n+        \":hlo_test_base\",\n         \":xla_internal_test_main\",\n         \"//xla:array2d\",\n         \"//xla:array3d\",\n         \"//xla:error_spec\",\n-        \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla:reference_util\",\n         \"//xla:shape_util\",\n-        \"//xla/client:client_library\",\n+        \"//xla/client:local_client\",\n         \"//xla/hlo/builder:xla_builder\",\n         \"//xla/hlo/builder/lib:arithmetic\",\n         \"//xla/hlo/builder/lib:matrix\",\n         \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/service:hlo_runner_interface\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tests:xla_test_backend_predicates\",\n-        \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/platform:test_benchmark\",\n-        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@eigen_archive//:eigen3\",\n         \"@local_tsl//tsl/platform:ml_dtypes\",\n+        \"@local_tsl//tsl/platform:test\",\n+        \"@local_tsl//tsl/platform:test_benchmark\",\n     ] + if_rocm_is_configured([\n         # keep sorted\n         \"@local_config_rocm//rocm:rocm_headers\",\n@@ -1326,42 +1304,31 @@ xla_test(\n         # TODO(b/445172709): Re-enable once fixed.\n         \"b200\": [\"broken\"],\n     },\n-    precompile_test = False,\n     shard_count = 50,\n     tags = [\n         \"optonly\",\n-        \"test_migrated_to_hlo_runner_pjrt\",\n     ],\n     deps = [\n-        \":client_library_test_runner_mixin\",\n-        \":hlo_pjrt_interpreter_reference_mixin\",\n-        \":hlo_pjrt_test_base\",\n+        \":client_library_test_base\",\n+        \":hlo_test_base\",\n         \":xla_internal_test_main\",\n         \"//xla:array2d\",\n         \"//xla:array3d\",\n         \"//xla:error_spec\",\n-        \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla:reference_util\",\n         \"//xla:shape_util\",\n-        \"//xla/client:client_library\",\n         \"//xla/client:local_client\",\n         \"//xla/hlo/builder:xla_builder\",\n         \"//xla/hlo/builder/lib:arithmetic\",\n         \"//xla/hlo/builder/lib:matrix\",\n         \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/pjrt/plugin/xla_gpu:xla_gpu_pjrt_client\",\n-        \"//xla/service:hlo_runner_interface\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tests:xla_test_backend_predicates\",\n-        \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/platform:test_benchmark\",\n-        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@eigen_archive//:eigen3\",\n         \"@local_tsl//tsl/platform:ml_dtypes\",\n         \"@local_tsl//tsl/platform:test\",\n         \"@local_tsl//tsl/platform:test_benchmark\",\n@@ -1925,6 +1892,7 @@ xla_test(\n     name = \"dynamic_ops_test\",\n     timeout = \"moderate\",\n     srcs = [\"dynamic_ops_test.cc\"],\n+    precompile_test = False,\n     shard_count = 4,\n     tags = [\n         \"test_migrated_to_hlo_runner_pjrt\",\n@@ -3703,6 +3671,7 @@ xla_test(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n "
        },
        {
            "sha": "268482a3ec05eee30d058e8997caf56a88797edd",
            "filename": "third_party/xla/xla/tests/dot_operation_test.cc",
            "status": "modified",
            "additions": 430,
            "deletions": 316,
            "changes": 746,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e6aa1c1345c09bafcc7a199fae3b981eab756107/third_party%2Fxla%2Fxla%2Ftests%2Fdot_operation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e6aa1c1345c09bafcc7a199fae3b981eab756107/third_party%2Fxla%2Fxla%2Ftests%2Fdot_operation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fdot_operation_test.cc?ref=e6aa1c1345c09bafcc7a199fae3b981eab756107",
            "patch": "@@ -15,37 +15,26 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n-#include <utility>\n #include <vector>\n \n #include \"xla/tests/xla_test_backend_predicates.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"Eigen/Core\"\n #include \"xla/array2d.h\"\n #include \"xla/array3d.h\"\n-#include \"xla/client/client_library.h\"\n+#include \"xla/client/local_client.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/builder/lib/arithmetic.h\"\n #include \"xla/hlo/builder/lib/matrix.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n-#include \"xla/layout_util.h\"\n-#include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/reference_util.h\"\n-#include \"xla/service/hlo_runner_interface.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n-#include \"xla/tests/client_library_test_runner_mixin.h\"\n-#include \"xla/tests/hlo_pjrt_interpreter_reference_mixin.h\"\n-#include \"xla/tests/hlo_pjrt_test_base.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tests/client_library_test_base.h\"\n+#include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/tsl/platform/test_benchmark.h\"\n #include \"tsl/platform/ml_dtypes.h\"\n@@ -57,9 +46,7 @@ limitations under the License.\n namespace xla {\n namespace {\n \n-class DotOperationTest\n-    : public ClientLibraryTestRunnerMixin<\n-          HloPjRtInterpreterReferenceMixin<HloPjRtTestBase>> {\n+class DotOperationTest : public ClientLibraryTestBase {\n  public:\n   ErrorSpec error_spec_{0.0001, 1e-5};\n };\n@@ -87,23 +74,24 @@ using TypesF8 = ::testing::Types<tsl::float8_e4m3fnuz>;\n \n // Check that we can safely pass an input tuple's elements to a dot operation.\n TEST_F(DotOperationTest, DotOfInputTupleElem) {\n-  XlaBuilder builder(\"DotOfInputTupleElem\");\n+  XlaBuilder builder(TestName());\n \n   XlaOp param;\n-  Literal param_data = this->CreateParameterAndTransferLiteral(\n-      0,\n-      LiteralUtil::MakeTupleFromSlices(\n-          {LiteralUtil::CreateR2<float>({{1, 2}, {3, 4}}),\n-           LiteralUtil::CreateR2<float>({{5, 6}, {7, 8}})}),\n-      \"arg0\", &builder, &param);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto param_data,\n+      CreateParameterAndTransferLiteral(\n+          0,\n+          LiteralUtil::MakeTupleFromSlices(\n+              {LiteralUtil::CreateR2<float>({{1, 2}, {3, 4}}),\n+               LiteralUtil::CreateR2<float>({{5, 6}, {7, 8}})}),\n+          \"arg0\", &builder, &param));\n   auto lhs = GetTupleElement(param, 0);\n   auto rhs = GetTupleElement(param, 1);\n   Dot(lhs, rhs);\n \n-  Literal expected_literal = LiteralUtil::CreateR2<float>({{19, 22}, {43, 50}});\n-\n-  ComputeAndCompareLiteral(&builder, expected_literal, {&param_data},\n-                           &expected_literal.shape());\n+  ComputeAndCompareLiteral(&builder,\n+                           LiteralUtil::CreateR2<float>({{19, 22}, {43, 50}}),\n+                           {param_data.get()});\n }\n \n template <typename T>\n@@ -234,37 +222,51 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64, FusedDot) {\n   auto exp0 = Exp(param0);\n   Dot(exp0, param1);\n \n-  Literal lhs_handle = LiteralUtil::CreateR2FromArray2D<T>(\n-      {{1.0f, 2.0f, 3.0f, 4.0f}, {-1.0f, -2.0f, -3.0f, -4.0f}});\n-  Literal rhs_handle =\n-      LiteralUtil::CreateR2FromArray2D<T>({{1.0f}, {2.0f}, {3.0f}, {4.0f}});\n+  auto lhs_handle =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+              {{1.0f, 2.0f, 3.0f, 4.0f}, {-1.0f, -2.0f, -3.0f, -4.0f}}))\n+          .value();\n+  auto rhs_handle = this->client_\n+                        ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+                            {{1.0f}, {2.0f}, {3.0f}, {4.0f}}))\n+                        .value();\n+\n   if (std::is_same<Eigen::half, T>::value) {\n     this->error_spec_ = ErrorSpec{0.0001, 1e-3};\n   }\n \n   this->template ComputeAndCompareR2<T>(\n       &builder, Array2D<T>({{296.14560492846033f}, {0.8611737683031964f}}),\n-      {&lhs_handle, &rhs_handle}, this->error_spec_);\n+      {lhs_handle.get(), rhs_handle.get()}, this->error_spec_);\n }\n \n template <typename T>\n class SquareMatrixDot : public DotOperationTest {\n  public:\n   void TestImpl(bool lhs_row_major, bool rhs_row_major) {\n-    Literal lhs_handle = LiteralUtil::CreateFromArrayWithLayout<T>(\n-        {{1.0f, 2.0f}, {3.0f, -4.0f}},\n-        LayoutUtil::MakeLayout(MinorToMajorForIsRowMajor(lhs_row_major)));\n-    Literal rhs_handle = LiteralUtil::CreateFromArrayWithLayout<T>(\n-        {{1.0f, 6.0f}, {7.0f, -4.0f}},\n-        LayoutUtil::MakeLayout(MinorToMajorForIsRowMajor(rhs_row_major)));\n+    auto lhs_handle =\n+        client_\n+            ->TransferToServer(LiteralUtil::CreateFromArrayWithLayout<T>(\n+                {{1.0f, 2.0f}, {3.0f, -4.0f}},\n+                LayoutUtil::MakeLayout(\n+                    MinorToMajorForIsRowMajor(lhs_row_major))))\n+            .value();\n+    auto rhs_handle =\n+        client_\n+            ->TransferToServer(LiteralUtil::CreateFromArrayWithLayout<T>(\n+                {{1.0f, 6.0f}, {7.0f, -4.0f}},\n+                LayoutUtil::MakeLayout(\n+                    MinorToMajorForIsRowMajor(rhs_row_major))))\n+            .value();\n     XlaBuilder builder(TestName());\n     auto prim_type = primitive_util::NativeToPrimitiveType<T>();\n     Dot(Parameter(&builder, 0, ShapeUtil::MakeShape(prim_type, {2, 2}), \"lhs\"),\n         Parameter(&builder, 1, ShapeUtil::MakeShape(prim_type, {2, 2}), \"rhs\"));\n \n     Array2D<T> expected({{15.0f, -2.0f}, {-25.0f, 34.0f}});\n-    ComputeAndCompareR2<T>(&builder, expected, {&lhs_handle, &rhs_handle},\n-                           error_spec_);\n+    ComputeAndCompareR2<T>(&builder, expected,\n+                           {lhs_handle.get(), rhs_handle.get()}, error_spec_);\n   }\n \n  protected:\n@@ -310,22 +312,29 @@ class ParametricDotTest : public DotOperationTest,\n                           public ::testing::WithParamInterface<DotTestParam> {\n  protected:\n   // This method runs before each test runs.\n-  bool IsRocm() {\n-    return test_runner().HasProperty(HloRunnerPropertyTag::kUsingGpuRocm);\n-  }\n-\n   void SetUp() override {\n     // Several F16 tests are subject to denormal issues on MI210 architecture.\n     // For that matter, we set propagate_grad_xy_ flag for these tests, which\n     // activates adapted GEMM algorithm on ROCM. Besides, the adapted algorithm\n     // does not work well with ROCBLAS autotuning, hence we also disable it.\n     // This also serves as a test that grad_x/y attributes are correctly\n     // propagated down to a GEMM routine.\n-    if (IsRocm()) {\n+    const auto& gpu_comp = client_->backend()\n+                               .default_stream_executor()\n+                               ->GetDeviceDescription()\n+                               .gpu_compute_capability();\n+    if (gpu_comp.IsRocm()) {\n       absl::string_view name(\n           ::testing::UnitTest::GetInstance()->current_test_info()->name());\n-      if (absl::StrContains(name, \"TestF16/270x270x520_MajorToMinor\")) {\n+      if (name.find(\"TestF16/270x270x520_MajorToMinor\") != std::string::npos) {\n         GTEST_SKIP() << \"Not supported on ROCm until Triton is re-enabled.\";\n+        execution_options_.mutable_debug_options()->set_xla_gpu_autotune_level(\n+            0);\n+        DotTestParam param = GetParam();\n+        // In order to test both grad_x and grad_y attributes, we set\n+        // propagate_grad_xy_ to 1 or 2 based on some alternating parameter\n+        // to set it deterministically.\n+        propagate_grad_xy_ = param.dot_lhs_row_major ? 1 : 2;\n       }\n     }\n   }\n@@ -336,45 +345,45 @@ class ParametricDotTest : public DotOperationTest,\n   template <typename NativeT>\n   void ComputeAndCompareR2WithError(XlaBuilder* builder,\n                                     const Array2D<NativeT>& expected,\n-                                    absl::Span<Literal* const> arguments);\n+                                    absl::Span<GlobalData* const> arguments);\n \n   int32_t propagate_grad_xy_ = 0;\n };\n \n template <typename NativeT>\n void ParametricDotTest::ComputeAndCompareR2WithError(\n     XlaBuilder* builder, const Array2D<NativeT>& expected,\n-    absl::Span<Literal* const> arguments) {\n+    absl::Span<GlobalData* const> arguments) {\n   ErrorSpec error_spec(0.3, 3e-3);\n   ComputeAndCompareR2(builder, expected, arguments, error_spec);\n }\n \n template <>\n void ParametricDotTest::ComputeAndCompareR2WithError<Eigen::half>(\n     XlaBuilder* builder, const Array2D<Eigen::half>& expected,\n-    absl::Span<Literal* const> arguments) {\n+    absl::Span<GlobalData* const> arguments) {\n   ErrorSpec error_spec(0.3, 7e-3);\n   ComputeAndCompareR2(builder, expected, arguments, error_spec);\n }\n \n template <>\n void ParametricDotTest::ComputeAndCompareR2WithError<int32_t>(\n     XlaBuilder* builder, const Array2D<int32_t>& expected,\n-    absl::Span<Literal* const> arguments) {\n+    absl::Span<GlobalData* const> arguments) {\n   ComputeAndCompareR2(builder, expected, arguments);\n }\n \n template <>\n void ParametricDotTest::ComputeAndCompareR2WithError<uint8_t>(\n     XlaBuilder* builder, const Array2D<uint8_t>& expected,\n-    absl::Span<Literal* const> arguments) {\n+    absl::Span<GlobalData* const> arguments) {\n   ComputeAndCompareR2(builder, expected, arguments);\n }\n \n template <>\n void ParametricDotTest::ComputeAndCompareR2WithError(\n     XlaBuilder* builder, const Array2D<tsl::float8_e5m2>& expected,\n-    absl::Span<Literal* const> arguments) {\n+    absl::Span<GlobalData* const> arguments) {\n   ErrorSpec error_spec(0.3, 3e-3);\n   error_spec.low_precision_fp_error_spec.type =\n       primitive_util::NativeToPrimitiveType<tsl::float8_e5m2>();\n@@ -385,7 +394,7 @@ void ParametricDotTest::ComputeAndCompareR2WithError(\n template <>\n void ParametricDotTest::ComputeAndCompareR2WithError(\n     XlaBuilder* builder, const Array2D<tsl::float8_e4m3fn>& expected,\n-    absl::Span<Literal* const> arguments) {\n+    absl::Span<GlobalData* const> arguments) {\n   ErrorSpec error_spec(0.3, 3e-3);\n   error_spec.low_precision_fp_error_spec.type =\n       primitive_util::NativeToPrimitiveType<tsl::float8_e4m3fn>();\n@@ -398,25 +407,31 @@ void ParametricDotTest::TestImpl() {\n \n   std::unique_ptr<Array2D<NativeT>> dot_lhs_data =\n       MakeLinspaceArray2D<NativeT>(0.0, 1.0, param.m, param.k);\n-  Literal dot_lhs_handle = LiteralUtil::CreateR2FromArray2DWithLayout(\n+  Literal dot_lhs_lit = LiteralUtil::CreateR2FromArray2DWithLayout(\n       *dot_lhs_data, LayoutUtil::MakeLayout(\n                          MinorToMajorForIsRowMajor(param.dot_lhs_row_major)));\n+  std::unique_ptr<GlobalData> dot_lhs_handle =\n+      client_->TransferToServer(dot_lhs_lit).value();\n \n   std::unique_ptr<Array2D<NativeT>> dot_rhs_data =\n       MakeLinspaceArray2D<NativeT>(0.0, 1.0, param.k, param.n);\n   Layout rhs_layout = LayoutUtil::MakeLayout(\n       MinorToMajorForIsRowMajor(param.dot_rhs_row_major));\n-  Literal dot_rhs_handle =\n+  Literal dot_rhs_lit =\n       LiteralUtil::CreateR2FromArray2DWithLayout(*dot_rhs_data, rhs_layout);\n+  std::unique_ptr<GlobalData> dot_rhs_handle =\n+      client_->TransferToServer(dot_rhs_lit).value();\n \n   std::unique_ptr<Array2D<NativeT>> addend_data;\n   Literal addend_lit;\n+  std::unique_ptr<GlobalData> addend_handle;\n \n   if (param.has_addend) {\n     addend_data = MakeLinspaceArray2D<NativeT>(0.0, 1.0, param.m, param.n);\n     addend_lit = LiteralUtil::CreateR2FromArray2DWithLayout(\n         *addend_data, LayoutUtil::MakeLayout(\n                           MinorToMajorForIsRowMajor(param.addend_row_major)));\n+    addend_handle = client_->TransferToServer(addend_lit).value();\n   }\n \n   XlaBuilder builder(TestName());\n@@ -465,9 +480,9 @@ void ParametricDotTest::TestImpl() {\n     expected = ReferenceUtil::MatmulArray2D(*dot_lhs_data, *dot_rhs_data);\n   }\n \n-  std::vector<Literal*> args = {&dot_lhs_handle, &dot_rhs_handle};\n+  std::vector<GlobalData*> args = {dot_lhs_handle.get(), dot_rhs_handle.get()};\n   if (param.has_addend) {\n-    args.push_back(&addend_lit);\n+    args.push_back(addend_handle.get());\n   }\n   ComputeAndCompareR2WithError<NativeT>(&builder, *expected, args);\n }\n@@ -527,11 +542,14 @@ INSTANTIATE_TEST_CASE_P(DotTests, ParametricDotTest,\n class ParametricDotTestWithoutLayoutAssignment : public ParametricDotTest {\n  public:\n   ParametricDotTestWithoutLayoutAssignment() {\n-    mutable_debug_options()->add_xla_disable_hlo_passes(\"layout-assignment\");\n-    mutable_debug_options()->add_xla_disable_hlo_passes(\"hlo-verifier\");\n+    execution_options_.mutable_debug_options()->add_xla_disable_hlo_passes(\n+        \"layout-assignment\");\n+    execution_options_.mutable_debug_options()->add_xla_disable_hlo_passes(\n+        \"hlo-verifier\");\n     // Disable algebraic simplification because the pass may replace a dot\n     // instruction with a layout-changing multiplication instruction.\n-    mutable_debug_options()->add_xla_disable_hlo_passes(\"algsimp\");\n+    execution_options_.mutable_debug_options()->add_xla_disable_hlo_passes(\n+        \"algsimp\");\n   }\n };\n \n@@ -596,12 +614,20 @@ template <typename T>\n class NonsquareMatrixDot : public DotOperationTest {\n  public:\n   void TestImpl(bool lhs_row_major, bool rhs_row_major) {\n-    Literal lhs_handle = LiteralUtil::CreateFromArrayWithLayout<T>(\n-        {{1.0f, 2.0f, 3.0f}, {3.0f, -4.0f, -1.0f}},\n-        LayoutUtil::MakeLayout(MinorToMajorForIsRowMajor(lhs_row_major)));\n-    Literal rhs_handle = LiteralUtil::CreateFromArrayWithLayout<T>(\n-        {{1.0f, 6.0f}, {2.0f, 3.0f}, {7.0f, -4.0f}},\n-        LayoutUtil::MakeLayout(MinorToMajorForIsRowMajor(rhs_row_major)));\n+    auto lhs_handle =\n+        client_\n+            ->TransferToServer(LiteralUtil::CreateFromArrayWithLayout<T>(\n+                {{1.0f, 2.0f, 3.0f}, {3.0f, -4.0f, -1.0f}},\n+                LayoutUtil::MakeLayout(\n+                    MinorToMajorForIsRowMajor(lhs_row_major))))\n+            .value();\n+    auto rhs_handle =\n+        client_\n+            ->TransferToServer(LiteralUtil::CreateFromArrayWithLayout<T>(\n+                {{1.0f, 6.0f}, {2.0f, 3.0f}, {7.0f, -4.0f}},\n+                LayoutUtil::MakeLayout(\n+                    MinorToMajorForIsRowMajor(rhs_row_major))))\n+            .value();\n \n     XlaBuilder builder(TestName());\n     auto prim_type = primitive_util::NativeToPrimitiveType<T>();\n@@ -610,8 +636,8 @@ class NonsquareMatrixDot : public DotOperationTest {\n \n     Array2D<T> expected({{26.0f, 0.0f}, {-12.0f, 10.0f}});\n \n-    ComputeAndCompareR2<T>(&builder, expected, {&lhs_handle, &rhs_handle},\n-                           error_spec_);\n+    ComputeAndCompareR2<T>(&builder, expected,\n+                           {lhs_handle.get(), rhs_handle.get()}, error_spec_);\n   }\n \n  protected:\n@@ -630,11 +656,17 @@ TYPED_TEST(NonsquareMatrixDot, TestTF) { this->TestImpl(true, false); }\n TYPED_TEST(NonsquareMatrixDot, TestTT) { this->TestImpl(true, true); }\n \n TEST_F(DotOperationTest, MatrixVectorC64) {\n-  auto lhs_handle = LiteralUtil::CreateR2WithLayout<complex64>(\n-      {{1.0, 2.0, 3.0, -4.0}}, LayoutUtil::MakeLayout({1, 0}));\n-  auto rhs_handle = LiteralUtil::CreateR2WithLayout<complex64>(\n-      {{1.0, 1.0}, {2.0, 2.0}, {3.0, 3.0}, {-4.0, 4.0}},\n-      LayoutUtil::MakeLayout({1, 0}));\n+  auto lhs_handle =\n+      client_\n+          ->TransferToServer(LiteralUtil::CreateR2WithLayout<complex64>(\n+              {{1.0, 2.0, 3.0, -4.0}}, LayoutUtil::MakeLayout({1, 0})))\n+          .value();\n+  auto rhs_handle =\n+      client_\n+          ->TransferToServer(LiteralUtil::CreateR2WithLayout<complex64>(\n+              {{1.0, 1.0}, {2.0, 2.0}, {3.0, 3.0}, {-4.0, 4.0}},\n+              LayoutUtil::MakeLayout({1, 0})))\n+          .value();\n \n   XlaBuilder builder(TestName());\n   auto prim_type = primitive_util::NativeToPrimitiveType<complex64>();\n@@ -643,8 +675,8 @@ TEST_F(DotOperationTest, MatrixVectorC64) {\n \n   Array2D<complex64> expected({{30.0, -2.0}});\n \n-  ComputeAndCompareR2<complex64>(&builder, expected, {&lhs_handle, &rhs_handle},\n-                                 error_spec_);\n+  ComputeAndCompareR2<complex64>(\n+      &builder, expected, {lhs_handle.get(), rhs_handle.get()}, error_spec_);\n }\n \n TYPED_TEST(DotOperationTest_F16F32F64CF64, ConcurrentMatMult) {\n@@ -684,13 +716,13 @@ TYPED_TEST(DotOperationTestForBatchMatMul, Types) {\n   }\n   using T = TypeParam;\n   XlaBuilder builder(this->TestName());\n-  XlaOp x = Parameter(&builder, 0,\n-                      ShapeUtil::MakeShapeWithType<T>({2, 2, 2, 2}), \"x\");\n-  XlaOp y = Parameter(&builder, 1,\n-                      ShapeUtil::MakeShapeWithType<T>({2, 2, 2, 2}), \"y\");\n+  auto x = Parameter(&builder, 0, ShapeUtil::MakeShapeWithType<T>({2, 2, 2, 2}),\n+                     \"x\");\n+  auto y = Parameter(&builder, 1, ShapeUtil::MakeShapeWithType<T>({2, 2, 2, 2}),\n+                     \"y\");\n \n-  XlaOp x_flat = Reshape(x, {4, 2, 2});\n-  XlaOp y_flat = Reshape(y, {4, 2, 2});\n+  auto x_flat = Reshape(x, {4, 2, 2});\n+  auto y_flat = Reshape(y, {4, 2, 2});\n \n   // Slice batches into individual matrices and multiply them.\n   std::vector<XlaOp> out_slices;\n@@ -710,13 +742,20 @@ TYPED_TEST(DotOperationTestForBatchMatMul, Types) {\n   auto out_flat = ConcatInDim(&builder, out_slices, 0);\n   Reshape(out_flat, {2, 2, 2, 2});\n \n-  Literal x_data = LiteralUtil::CreateR4FromArray4D<T>(\n-      {{{{1000.0f, 100.0f}, {10.0f, 1.0f}}, {{2000.0f, 200.0f}, {20.0f, 2.0f}}},\n-       {{{3000.0f, 300.0f}, {30.0f, 3.0f}},\n-        {{4000.0f, 400.0f}, {40.0f, 4.0f}}}});\n-  Literal y_data = LiteralUtil::CreateR4FromArray4D<T>(\n-      {{{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}},\n-       {{{11.0f, 22.0f}, {33.0f, 44.0f}}, {{55.0f, 66.0f}, {77.0f, 88.0f}}}});\n+  auto x_data = this->client_\n+                    ->TransferToServer(LiteralUtil::CreateR4FromArray4D<T>(\n+                        {{{{1000.0f, 100.0f}, {10.0f, 1.0f}},\n+                          {{2000.0f, 200.0f}, {20.0f, 2.0f}}},\n+                         {{{3000.0f, 300.0f}, {30.0f, 3.0f}},\n+                          {{4000.0f, 400.0f}, {40.0f, 4.0f}}}}))\n+                    .value();\n+  auto y_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR4FromArray4D<T>(\n+              {{{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}},\n+               {{{11.0f, 22.0f}, {33.0f, 44.0f}},\n+                {{55.0f, 66.0f}, {77.0f, 88.0f}}}}))\n+          .value();\n \n   if (std::is_same<Eigen::half, T>::value) {\n     this->error_spec_ = ErrorSpec{0.0001, 1e-3};\n@@ -728,7 +767,7 @@ TYPED_TEST(DotOperationTestForBatchMatMul, Types) {\n         {{11400.0f, 13600.0f}, {114.0f, 136.0f}}},\n        {{{42900.0f, 79200.0f}, {429.0f, 792.0f}},\n         {{250800.0f, 299200.0f}, {2508.0f, 2992.0f}}}},\n-      {&x_data, &y_data}, this->error_spec_);\n+      {x_data.get(), y_data.get()}, this->error_spec_);\n }\n \n TYPED_TEST(DotOperationTest_F16F32F64CF64, GeneralMatMul) {\n@@ -748,17 +787,23 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64, GeneralMatMul) {\n \n   DotGeneral(x, y, dnums);\n \n-  Literal x_data = LiteralUtil::CreateR3FromArray3D<T>(\n-      {{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}});\n+  auto x_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR3FromArray3D<T>(\n+              {{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}}))\n+          .value();\n \n-  Literal y_data = LiteralUtil::CreateR3FromArray3D<T>(\n-      {{{1.0f, 0.0f}, {0.0f, 1.0f}}, {{1.0f, 0.0f}, {0.0f, 1.0f}}});\n+  auto y_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR3FromArray3D<T>(\n+              {{{1.0f, 0.0f}, {0.0f, 1.0f}}, {{1.0f, 0.0f}, {0.0f, 1.0f}}}))\n+          .value();\n \n   this->template ComputeAndCompareR3<T>(\n       &builder,\n       /*expected=*/\n       {{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}},\n-      {&x_data, &y_data}, this->error_spec_);\n+      {x_data.get(), y_data.get()}, this->error_spec_);\n }\n \n #if GOOGLE_CUDA || (TF_HIPBLASLT && TF_ROCM_VERSION >= 60000)\n@@ -768,7 +813,8 @@ class DotOperationTestWithCublasLt_F16F32F64CF64 : public DotOperationTest {\n   DotOperationTestWithCublasLt_F16F32F64CF64() {\n     bool enable_cublas_lt = true;\n \n-    mutable_debug_options()->set_xla_gpu_enable_cublaslt(enable_cublas_lt);\n+    execution_options_.mutable_debug_options()->set_xla_gpu_enable_cublaslt(\n+        enable_cublas_lt);\n   }\n \n  protected:\n@@ -799,11 +845,17 @@ TYPED_TEST(DotOperationTestWithCublasLt_F16F32F64CF64,\n \n   auto dot = DotGeneral(x, y, dnums);\n   auto prim_type = primitive_util::NativeToPrimitiveType<T>();\n-  Literal x_data = LiteralUtil::CreateR3FromArray3D<T>(\n-      {{{-1.0f, 2.0f}, {3.0f, -4.0f}}, {{5.0f, 6.0f}, {-7.0f, 8.0f}}});\n+  auto x_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR3FromArray3D<T>(\n+              {{{-1.0f, 2.0f}, {3.0f, -4.0f}}, {{5.0f, 6.0f}, {-7.0f, 8.0f}}}))\n+          .value();\n \n-  Literal y_data = LiteralUtil::CreateR3FromArray3D<T>(\n-      {{{1.0f, 0.0f}, {0.0f, -1.0f}}, {{1.0f, 0.0f}, {0.0f, 1.0f}}});\n+  auto y_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR3FromArray3D<T>(\n+              {{{1.0f, 0.0f}, {0.0f, -1.0f}}, {{1.0f, 0.0f}, {0.0f, 1.0f}}}))\n+          .value();\n   Array3D<T> expected(\n       {{{-1.0f, -2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {-7.0f, 8.0f}}});\n   if (prim_type != C64) {\n@@ -813,8 +865,8 @@ TYPED_TEST(DotOperationTestWithCublasLt_F16F32F64CF64,\n     expected = Array3D<T>(\n         {{{0.0f, 0.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {0.0f, 8.0f}}});\n   }\n-  this->template ComputeAndCompareR3<T>(&builder, expected, {&x_data, &y_data},\n-                                        this->error_spec_);\n+  this->template ComputeAndCompareR3<T>(\n+      &builder, expected, {x_data.get(), y_data.get()}, this->error_spec_);\n }\n #endif  // GOOGLE_CUDA || TF_HIPBLASLT\n \n@@ -823,7 +875,8 @@ template <typename T>\n class DotOperationTestWithCublasLt_F8 : public DotOperationTest {\n  public:\n   DotOperationTestWithCublasLt_F8() {\n-    mutable_debug_options()->set_xla_gpu_enable_cublaslt(true);\n+    execution_options_.mutable_debug_options()->set_xla_gpu_enable_cublaslt(\n+        true);\n   }\n };\n TYPED_TEST_CASE(DotOperationTestWithCublasLt_F8, TypesF8);\n@@ -855,74 +908,82 @@ TYPED_TEST(DotOperationTestWithCublasLt_F8, ScaledABUnscaledDF8) {\n \n   DotGeneral(a_scaled_f32, b_scaled_f32, dnums);\n \n-  Literal a_data = LiteralUtil::CreateR2FromArray2D<T>(\n-      {{2.0f, 3.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {5.0f, 7.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 7.0f, 5.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 3.0f, 2.0f}});\n-  Literal b_data = LiteralUtil::CreateR2FromArray2D<T>(\n-      {{11.0f, 13.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n-       {17.0f, 19.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 19.0f, 17.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 13.0f, 11.0f}});\n-  Literal a_scale_data = LiteralUtil::CreateR0<float>(2.0f);\n-  Literal b_scale_data = LiteralUtil::CreateR0<float>(4.0f);\n+  auto a_data = this->client_\n+                    ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+                        {{2.0f, 3.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {5.0f, 7.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 7.0f, 5.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 3.0f, 2.0f}}))\n+                    .value();\n+  auto b_data = this->client_\n+                    ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+                        {{11.0f, 13.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {17.0f, 19.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 19.0f, 17.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 13.0f, 11.0f}}))\n+                    .value();\n+  auto a_scale_data =\n+      this->client_->TransferToServer(LiteralUtil::CreateR0<float>(2.0f))\n+          .value();\n+  auto b_scale_data =\n+      this->client_->TransferToServer(LiteralUtil::CreateR0<float>(4.0f))\n+          .value();\n \n   Literal expected_d = LiteralUtil::CreateR2FromArray2D<float>(\n       {{560.0f, 688.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n@@ -958,9 +1019,10 @@ TYPED_TEST(DotOperationTestWithCublasLt_F8, ScaledABUnscaledDF8) {\n        {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n         0.0f, 0.0f, 688.0f, 560.0f}});\n \n-  this->ComputeAndCompareTuple(&builder, expected_d,\n-                               {&a_data, &b_data, &a_scale_data, &b_scale_data},\n-                               this->error_spec_);\n+  this->ComputeAndCompareTuple(\n+      &builder, expected_d,\n+      {a_data.get(), b_data.get(), a_scale_data.get(), b_scale_data.get()},\n+      this->error_spec_);\n }\n \n TYPED_TEST(DotOperationTestWithCublasLt_F8, ScaledABScaledDWithDAmaxF8) {\n@@ -1009,75 +1071,85 @@ TYPED_TEST(DotOperationTestWithCublasLt_F8, ScaledABScaledDWithDAmaxF8) {\n       d_clamped_f32, primitive_util::NativeToPrimitiveType<T>());\n   Tuple(&builder, {d_f8, d_amax});\n \n-  Literal a_data = LiteralUtil::CreateR2FromArray2D<T>(\n-      {{2.0f, 3.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {5.0f, 7.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 7.0f, 5.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 3.0f, 2.0f}});\n-  auto b_data = LiteralUtil::CreateR2FromArray2D<T>(\n-      {{11.0f, 13.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n-       {17.0f, 19.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 0.0f, 0.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 19.0f, 17.0f},\n-       {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n-        0.0f, 0.0f, 13.0f, 11.0f}});\n-  Literal a_scale_data = LiteralUtil::CreateR0<float>(2.0f);\n-  Literal b_scale_data = LiteralUtil::CreateR0<float>(4.0f);\n-  auto d_scale_data = LiteralUtil::CreateR0<float>(8.0f);\n+  auto a_data = this->client_\n+                    ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+                        {{2.0f, 3.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {5.0f, 7.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 7.0f, 5.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 3.0f, 2.0f}}))\n+                    .value();\n+  auto b_data = this->client_\n+                    ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+                        {{11.0f, 13.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {17.0f, 19.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 19.0f, 17.0f},\n+                         {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n+                          0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 13.0f, 11.0f}}))\n+                    .value();\n+  auto a_scale_data =\n+      this->client_->TransferToServer(LiteralUtil::CreateR0<float>(2.0f))\n+          .value();\n+  auto b_scale_data =\n+      this->client_->TransferToServer(LiteralUtil::CreateR0<float>(4.0f))\n+          .value();\n+  auto d_scale_data =\n+      this->client_->TransferToServer(LiteralUtil::CreateR0<float>(8.0f))\n+          .value();\n \n   Literal expected_d = LiteralUtil::CreateR2FromArray2D<T>(\n       {{72.0f, 88.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,\n@@ -1115,10 +1187,10 @@ TYPED_TEST(DotOperationTestWithCublasLt_F8, ScaledABScaledDWithDAmaxF8) {\n   Literal expected_amax = LiteralUtil::CreateR0<float>(1640.0f);\n   Literal expected = LiteralUtil::MakeTuple({&expected_d, &expected_amax});\n \n-  this->ComputeAndCompareTuple(\n-      &builder, expected,\n-      {&a_data, &b_data, &a_scale_data, &b_scale_data, &d_scale_data},\n-      this->error_spec_);\n+  this->ComputeAndCompareTuple(&builder, expected,\n+                               {a_data.get(), b_data.get(), a_scale_data.get(),\n+                                b_scale_data.get(), d_scale_data.get()},\n+                               this->error_spec_);\n }\n #endif  // GOOGLE_CUDA || TF_HIPBLASLT\n \n@@ -1138,15 +1210,20 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64, GeneralMatMulR3LhsR2Rhs) {\n \n   DotGeneral(x, y, dnums);\n \n-  Literal x_data = LiteralUtil::CreateR3FromArray3D<T>(\n-      {{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}});\n+  auto x_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR3FromArray3D<T>(\n+              {{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}}))\n+          .value();\n \n-  Literal y_data =\n-      LiteralUtil::CreateR2FromArray2D<T>({{1.0f, 0.0f}, {0.0f, 1.0f}});\n+  auto y_data = this->client_\n+                    ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+                        {{1.0f, 0.0f}, {0.0f, 1.0f}}))\n+                    .value();\n \n   this->template ComputeAndCompareR2<T>(\n       &builder,\n-      /*expected=*/{{1.0f, 2.0f}, {7.0f, 8.0f}}, {&x_data, &y_data},\n+      /*expected=*/{{1.0f, 2.0f}, {7.0f, 8.0f}}, {x_data.get(), y_data.get()},\n       this->error_spec_);\n }\n \n@@ -1166,15 +1243,20 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64, GeneralMatMulR2LhsR3Rhs) {\n \n   DotGeneral(x, y, dnums);\n \n-  Literal x_data =\n-      LiteralUtil::CreateR2FromArray2D<T>({{1.0f, 0.0f}, {0.0f, 1.0f}});\n+  auto x_data = this->client_\n+                    ->TransferToServer(LiteralUtil::CreateR2FromArray2D<T>(\n+                        {{1.0f, 0.0f}, {0.0f, 1.0f}}))\n+                    .value();\n \n-  auto y_data = LiteralUtil::CreateR3FromArray3D<T>(\n-      {{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}});\n+  auto y_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR3FromArray3D<T>(\n+              {{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}}))\n+          .value();\n \n   this->template ComputeAndCompareR2<T>(\n       &builder,\n-      /*expected=*/{{1.0f, 2.0f}, {7.0f, 8.0f}}, {&x_data, &y_data},\n+      /*expected=*/{{1.0f, 2.0f}, {7.0f, 8.0f}}, {x_data.get(), y_data.get()},\n       this->error_spec_);\n }\n \n@@ -1197,20 +1279,27 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64, GeneralMatMulMultipleBatch) {\n \n   DotGeneral(x, y, dnums);\n \n-  Literal x_data = LiteralUtil::CreateR4FromArray4D<T>(\n-      {{{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}},\n-       {{{9.0f, 10.0f}, {11.0f, 12.0f}}, {{13.0f, 14.0f}, {15.0f, 16.0f}}}});\n+  auto x_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR4FromArray4D<T>(\n+              {{{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}},\n+               {{{9.0f, 10.0f}, {11.0f, 12.0f}},\n+                {{13.0f, 14.0f}, {15.0f, 16.0f}}}}))\n+          .value();\n \n-  Literal y_data = LiteralUtil::CreateR4FromArray4D<T>(\n-      {{{{1.0f, 0.0f}, {0.0f, 1.0f}}, {{1.0f, 0.0f}, {0.0f, 1.0f}}},\n-       {{{0.0f, 1.0f}, {1.0f, 0.0f}}, {{0.0f, 1.0f}, {1.0f, 0.0f}}}});\n+  auto y_data =\n+      this->client_\n+          ->TransferToServer(LiteralUtil::CreateR4FromArray4D<T>(\n+              {{{{1.0f, 0.0f}, {0.0f, 1.0f}}, {{1.0f, 0.0f}, {0.0f, 1.0f}}},\n+               {{{0.0f, 1.0f}, {1.0f, 0.0f}}, {{0.0f, 1.0f}, {1.0f, 0.0f}}}}))\n+          .value();\n \n   this->template ComputeAndCompareR4<T>(\n       &builder,\n       /*expected=*/\n       {{{{1.0f, 2.0f}, {3.0f, 4.0f}}, {{5.0f, 6.0f}, {7.0f, 8.0f}}},\n        {{{10.0f, 9.0f}, {12.0f, 11.0f}}, {{14.0f, 13.0f}, {16.0f, 15.0f}}}},\n-      {&x_data, &y_data}, this->error_spec_);\n+      {x_data.get(), y_data.get()}, this->error_spec_);\n }\n \n TYPED_TEST(DotOperationTest_F16F32F64CF64, TransposeFolding) {\n@@ -1229,10 +1318,20 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64, TransposeFolding) {\n         if (transpose_rhs) {\n           rhs = ReferenceUtil::TransposeArray2D(*rhs);\n         }\n-        Literal lhs_handle = LiteralUtil::CreateR2FromArray2DWithLayout<T>(\n-            *lhs, LayoutUtil::MakeLayout(MinorToMajorForIsRowMajor(row_major)));\n-        Literal rhs_handle = LiteralUtil::CreateR2FromArray2DWithLayout<T>(\n-            *rhs, LayoutUtil::MakeLayout(MinorToMajorForIsRowMajor(row_major)));\n+        auto lhs_handle =\n+            this->client_\n+                ->TransferToServer(\n+                    LiteralUtil::CreateR2FromArray2DWithLayout<T>(\n+                        *lhs, LayoutUtil::MakeLayout(\n+                                  MinorToMajorForIsRowMajor(row_major))))\n+                .value();\n+        auto rhs_handle =\n+            this->client_\n+                ->TransferToServer(\n+                    LiteralUtil::CreateR2FromArray2DWithLayout<T>(\n+                        *rhs, LayoutUtil::MakeLayout(\n+                                  MinorToMajorForIsRowMajor(row_major))))\n+                .value();\n \n         XlaBuilder builder(this->TestName());\n         auto prim_type = primitive_util::NativeToPrimitiveType<T>();\n@@ -1256,7 +1355,8 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64, TransposeFolding) {\n         VLOG(1) << \"TestTransposeFolding \" << transpose_lhs << \" \"\n                 << transpose_rhs << \" \" << row_major;\n         this->template ComputeAndCompareR2<T>(\n-            &builder, expected, {&lhs_handle, &rhs_handle}, this->error_spec_);\n+            &builder, expected, {lhs_handle.get(), rhs_handle.get()},\n+            this->error_spec_);\n       }\n     }\n   }\n@@ -1288,14 +1388,23 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64,\n       new Array2D<T>({{1.0f, 2.0f}, {3.0f, 4.0f}, {5.0f, 6.0f}}));\n   std::unique_ptr<Array2D<T>> arg_2_value_array(new Array2D<T>({{1.0f, 2.0f}}));\n \n-  Literal arg_0_value = LiteralUtil::CreateR2FromArray2D<T>(*arg_0_value_array);\n-\n-  Literal arg_1_value = LiteralUtil::CreateR2FromArray2D<T>(*arg_1_value_array);\n-  Literal arg_2_value = LiteralUtil::CreateR2FromArray2D<T>(*arg_2_value_array);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto arg_0_value,\n+      this->client_->TransferToServer(\n+          LiteralUtil::CreateR2FromArray2D<T>(*arg_0_value_array)));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto arg_1_value,\n+      this->client_->TransferToServer(\n+          LiteralUtil::CreateR2FromArray2D<T>(*arg_1_value_array)));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto arg_2_value,\n+      this->client_->TransferToServer(\n+          LiteralUtil::CreateR2FromArray2D<T>(*arg_2_value_array)));\n \n   Array2D<T> expected({{53.0f, 74.0f}, {45.0f, 66.0f}});\n   this->template ComputeAndCompareR2<T>(\n-      &builder, expected, {&arg_0_value, &arg_1_value, &arg_2_value},\n+      &builder, expected,\n+      {arg_0_value.get(), arg_1_value.get(), arg_2_value.get()},\n       this->error_spec_);\n }\n \n@@ -1328,13 +1437,23 @@ TYPED_TEST(DotOperationTest_F16F32F64CF64,\n   std::unique_ptr<Array2D<T>> arg_2_value_array(\n       new Array2D<T>({{1.0f}, {2.0f}}));\n \n-  auto arg_0_value = LiteralUtil::CreateR2FromArray2D<T>(*arg_0_value_array);\n-  auto arg_1_value = LiteralUtil::CreateR2FromArray2D<T>(*arg_1_value_array);\n-  auto arg_2_value = LiteralUtil::CreateR2FromArray2D<T>(*arg_2_value_array);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto arg_0_value,\n+      this->client_->TransferToServer(\n+          LiteralUtil::CreateR2FromArray2D<T>(*arg_0_value_array)));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto arg_1_value,\n+      this->client_->TransferToServer(\n+          LiteralUtil::CreateR2FromArray2D<T>(*arg_1_value_array)));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto arg_2_value,\n+      this->client_->TransferToServer(\n+          LiteralUtil::CreateR2FromArray2D<T>(*arg_2_value_array)));\n \n   Array2D<T> expected({{38.0f, 36.0f}, {93.0f, 91.0f}});\n   this->template ComputeAndCompareR2<T>(\n-      &builder, expected, {&arg_0_value, &arg_1_value, &arg_2_value},\n+      &builder, expected,\n+      {arg_0_value.get(), arg_1_value.get(), arg_2_value.get()},\n       this->error_spec_);\n }\n \n@@ -1597,21 +1716,21 @@ class EinsumTest : public DotOperationTest,\n                    public ::testing::WithParamInterface<EinsumParamType> {};\n TEST_P(EinsumTest, SimpleEinsumTest) {\n   XlaBuilder builder(TestName());\n-  Literal x_literal =\n+  auto x = AddParam(\n       MakeFakeLiteral(ShapeUtil::MakeShape(F32, std::get<0>(GetParam())))\n-          .value();\n-  XlaOp x = Parameter(&builder, 0, x_literal.shape(), \"parameter1\");\n-  Literal y_literal =\n+          .value(),\n+      &builder);\n+  auto y = AddParam(\n       MakeFakeLiteral(ShapeUtil::MakeShape(F32, std::get<1>(GetParam())))\n-          .value();\n-  XlaOp y = Parameter(&builder, 1, y_literal.shape(), \"parameter2\");\n+          .value(),\n+      &builder);\n   auto config = std::get<2>(GetParam());\n   if (config.find(',') == config.npos) {\n     Einsum(x, config);\n   } else {\n     Einsum(x, y, config);\n   }\n-  ComputeAndCompare(&builder, {&x_literal, &y_literal}, ErrorSpec{1e-3, 1e-3});\n+  ComputeAndCompare(&builder, {}, ErrorSpec{1e-3, 1e-3});\n }\n \n std::vector<EinsumParamType> GetEinsumTestCases() {\n@@ -1684,21 +1803,20 @@ using BatchDotParamType = std::tuple<std::vector<int64_t>, std::vector<int64_t>,\n                                      std::vector<int64_t>>;\n class BatchDotTest : public DotOperationTest,\n                      public ::testing::WithParamInterface<BatchDotParamType> {};\n-\n TEST_P(BatchDotTest, BroadcastingBatchDotTest) {\n   XlaBuilder builder(TestName());\n-  Literal x_literal =\n+  auto x = AddParam(\n       MakeFakeLiteral(ShapeUtil::MakeShape(F32, std::get<0>(GetParam())))\n-          .value();\n-  XlaOp x = Parameter(&builder, 0, x_literal.shape(), \"parameter1\");\n-  Literal y_literal =\n+          .value(),\n+      &builder);\n+  auto y = AddParam(\n       MakeFakeLiteral(ShapeUtil::MakeShape(F32, std::get<1>(GetParam())))\n-          .value();\n-  XlaOp y = Parameter(&builder, 1, y_literal.shape(), \"parameter2\");\n+          .value(),\n+      &builder);\n   auto batch_dot = BatchDot(x, y);\n   auto output_shape = builder.GetShape(batch_dot).value();\n   EXPECT_EQ(output_shape.dimensions(), std::get<2>(GetParam()));\n-  ComputeAndCompare(&builder, {&x_literal, &y_literal}, ErrorSpec{1e-3, 1e-3});\n+  ComputeAndCompare(&builder, {}, ErrorSpec{1e-3, 1e-3});\n }\n \n std::vector<BatchDotParamType> GetBatchDotTestCases() {\n@@ -1720,8 +1838,7 @@ std::vector<BatchDotParamType> GetBatchDotTestCases() {\n INSTANTIATE_TEST_SUITE_P(BatchDot, BatchDotTest,\n                          ::testing::ValuesIn(GetBatchDotTestCases()));\n \n-class DotOperationTextTest\n-    : public HloPjRtInterpreterReferenceMixin<HloPjRtTestBase> {};\n+class DotOperationTextTest : public HloTestBase {};\n \n TEST_F(DotOperationTextTest, DotReorderedDotDims) {\n   absl::string_view hlo_string =\n@@ -1963,10 +2080,9 @@ ENTRY SmallIntegerDot {\n }\n \n TEST_F(DotOperationTextTest, S4Dot) {\n-  // TODO (b/456833594): reenable once the missing logic in tfrt_gpu_client\n-  // to pack int4 type for host literals has been added.\n-  GTEST_SKIP();\n-\n+  if (test::DeviceTypeIs(test::kTpu)) {\n+    GTEST_SKIP();\n+  }\n   absl::string_view hlo_string =\n       R\"(\n HloModule SmallIntegerDot\n@@ -2133,16 +2249,14 @@ TEST_F(DotOperationTest, ReorderContractingDimsConstLHS_RL) {\n   const_arr.FillIota(0);\n \n   XlaBuilder builder(TestName());\n-  Literal t0_literal = LiteralUtil::CreateR3FromArray3D<float>(input_arr);\n-  XlaOp t0 = Parameter(&builder, 0, t0_literal.shape(), \"parameter\");\n-  Literal y_literal =\n-      MakeFakeLiteral(ShapeUtil::MakeShape(F32, {2, 6})).value();\n+  auto t0 =\n+      AddParam(LiteralUtil::CreateR3FromArray3D<float>(input_arr), &builder);\n   auto t1 = Transpose(t0, {1, 0, 2});\n   auto rhs = Reshape(t1, {6, 2});\n   auto lhs = ConstantR2FromArray2D(&builder, const_arr);\n   Dot(lhs, rhs);\n \n-  ComputeAndCompare(&builder, {&t0_literal}, error_spec_);\n+  ComputeAndCompare(&builder, {}, error_spec_);\n }\n \n TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_LR) {\n@@ -2152,8 +2266,8 @@ TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_LR) {\n   const_arr.FillIota(0);\n \n   XlaBuilder builder(TestName());\n-  Literal t0_literal = LiteralUtil::CreateR3FromArray3D<float>(input_arr);\n-  XlaOp t0 = Parameter(&builder, 0, t0_literal.shape(), \"parameter\");\n+  auto t0 =\n+      AddParam(LiteralUtil::CreateR3FromArray3D<float>(input_arr), &builder);\n   auto t1 = Transpose(t0, {1, 0, 2});\n   auto lhs = Reshape(t1, {6, 2});\n   auto rhs = ConstantR2FromArray2D(&builder, const_arr);\n@@ -2163,7 +2277,7 @@ TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_LR) {\n   dims.add_rhs_contracting_dimensions(1);\n   DotGeneral(lhs, rhs, dims);\n \n-  ComputeAndCompare(&builder, {&t0_literal}, error_spec_);\n+  ComputeAndCompare(&builder, {}, error_spec_);\n }\n \n TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_RL) {\n@@ -2173,14 +2287,14 @@ TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_RL) {\n   const_arr.FillIota(0);\n \n   XlaBuilder builder(TestName());\n-  Literal t0_literal = LiteralUtil::CreateR4FromArray4D<float>(input_arr);\n-  XlaOp t0 = Parameter(&builder, 0, t0_literal.shape(), \"parameter\");\n+  auto t0 =\n+      AddParam(LiteralUtil::CreateR4FromArray4D<float>(input_arr), &builder);\n   auto t1 = Transpose(t0, {0, 2, 3, 1});\n   auto lhs = Reshape(t1, {2, 24});\n   auto rhs = ConstantR2FromArray2D(&builder, const_arr);\n   Dot(lhs, rhs);\n \n-  ComputeAndCompare(&builder, {&t0_literal}, error_spec_);\n+  ComputeAndCompare(&builder, {}, error_spec_);\n }\n \n TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_MM) {\n@@ -2190,8 +2304,8 @@ TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_MM) {\n   const_arr.FillIota(0);\n \n   XlaBuilder builder(TestName());\n-  Literal t0_literal = LiteralUtil::CreateR3FromArray3D<float>(input_arr);\n-  XlaOp t0 = Parameter(&builder, 0, t0_literal.shape(), \"parameter\");\n+  auto t0 =\n+      AddParam(LiteralUtil::CreateR3FromArray3D<float>(input_arr), &builder);\n   auto t1 = Reshape(t0, {2, 2, 3, 2});\n   auto t2 = Transpose(t1, {0, 2, 1, 3});\n   auto lhs = Reshape(t2, {2, 6, 2});\n@@ -2204,7 +2318,7 @@ TEST_F(DotOperationTest, ReorderContractingDimsConstRHS_MM) {\n   dims.add_rhs_batch_dimensions(0);\n   DotGeneral(lhs, rhs, dims);\n \n-  ComputeAndCompare(&builder, {&t0_literal}, error_spec_);\n+  ComputeAndCompare(&builder, {}, error_spec_);\n }\n \n TEST_F(DotOperationTest, ReorderContractingDims_Multipass) {\n@@ -2214,8 +2328,8 @@ TEST_F(DotOperationTest, ReorderContractingDims_Multipass) {\n   const_arr.FillIota(0);\n \n   XlaBuilder builder(TestName());\n-  Literal t0_literal = LiteralUtil::CreateR4FromArray4D<float>(input_arr);\n-  XlaOp t0 = Parameter(&builder, 0, t0_literal.shape(), \"parameter\");\n+  auto t0 =\n+      AddParam(LiteralUtil::CreateR4FromArray4D<float>(input_arr), &builder);\n   auto t1 = Transpose(t0, {0, 2, 1, 3});\n   auto t2 = Reshape(t1, {2, 6, 5});\n   auto t3 = Transpose(t2, {0, 2, 1});\n@@ -2231,7 +2345,7 @@ TEST_F(DotOperationTest, ReorderContractingDims_Multipass) {\n   // optimization can be applied multiple times if we fold the transpose\n   // and reshape that are moved to the constant side of the dot.\n   mutable_debug_options()->clear_xla_disable_hlo_passes();\n-  ComputeAndCompare(&builder, {&t0_literal}, error_spec_);\n+  ComputeAndCompare(&builder, {}, error_spec_);\n }\n \n TEST_F(DotOperationTextTest, WiderIntegralResultAccumulation) {"
        }
    ],
    "stats": {
        "total": 805,
        "additions": 444,
        "deletions": 361
    }
}