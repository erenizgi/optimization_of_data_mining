{
    "author": "tensorflower-gardener",
    "message": "Decouples instruction unique id from the instruction's module, making its assignment, uniquification and update only possible from within its parent computation.\n\nInstruction unique IDs are now an int64 with the most significant 32 bits being the computation's unique id and the least significant 32 bits being the local unique id of the instruction within its parent computation. This automatically makes them unique module-wise. Deprecates unique_id_64_bits since all unique ids are being made int64.\n\nChanges SetUniqueId to private in HloInstruction. That means only HloInstruction and its friend class HloComputation can set instruction IDs. Moves code in HLO Module that sets instruction unique ids to HloComputation.\n\nMakes it so that the local id of a instruction is always equivalent to its physical location in the instructions vector of the computation, therefore making local_index obsolete. Modifies XLA Builder to comply with the new behavior, re-organizing the uniquification mechanisms.\n\nModifies a number of different tests due to uniquifier prefix changes in instructions caused by the XLA Builder changes. Modifies certain data structures using int32 for instruction unique ids to int64.\n\nAdds a method to re-assign ids in a module and another to update a computation's schedule when instruction ids are re-assigned.\n\nFixes miscellaneous code that attempts to make use of an instruction's unique id without having the instruction in a computation.\n\nPiperOrigin-RevId: 805910956",
    "sha": "4208b67d6cd7348f0db331087277809db697c27c",
    "files": [
        {
            "sha": "b6ba6815a8bbe1481c9ba7cc9ba9fb8e5a337fcc",
            "filename": "tensorflow/compiler/mlir/tf2xla/tests/legalize-tf-with-tf2xla-hlo-importer.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftests%2Flegalize-tf-with-tf2xla-hlo-importer.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftests%2Flegalize-tf-with-tf2xla-hlo-importer.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftests%2Flegalize-tf-with-tf2xla-hlo-importer.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -696,13 +696,13 @@ module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, pr\n     //     return %0 : tensor<f32>\n     //   }\n     // }\n-    // CHECK: call @main.2\n+    // CHECK: call @main.1\n     %0 = \"tf.XlaCallModule\"(%arg0) {Sout = [#tf_type.shape<*>], device = \"\", dim_args_spec = [], function_list = [], disabled_checks = [], has_token_input_output = false, module = \"ML\\EFR\\03MLIRxxx-trunk\\00\\01\\17\\05\\01\\05\\01\\03\\05\\03\\07\\07\\t\\0B\\03K5\\07\\01\\1B\\07\\0B\\13\\0B3\\0B\\0B\\0B\\0B\\0F\\0B\\13\\0B\\03\\1B\\0F\\1B\\0B\\0B\\0B\\0B\\0B\\0F\\13\\0B\\0B\\0B\\0B\\03\\07\\0F\\17\\07\\02\\A7\\1F\\05\\0D\\03\\03\\03\\07\\05\\0F\\03\\0B\\0B\\1B\\0D'\\0F)\\031\\113\\05\\11\\05\\13\\05\\15\\05\\17\\1D\\15\\17\\05\\19\\17\\19\\EF\\01\\05\\1B\\03\\03\\1D\\0D\\05\\1F!#%\\1D\\1D\\1D\\1F\\1D!\\1D##\\03\\03\\03+\\0D\\03-/\\1D%\\1D'\\1D)\\1D+)\\01\\05\\11\\03\\01\\03\\01\\t\\04A\\05\\01\\11\\01\\05\\07\\03\\01\\05\\03\\11\\01\\t\\05\\03\\05\\0B\\03\\01\\01\\05\\06\\13\\03\\01\\03\\01\\07\\04\\01\\03\\03\\06\\03\\01\\05\\01\\00\\9A\\04-\\0F\\0B\\03!\\1B\\1D\\05\\1B\\83/\\1F\\15\\1D\\15\\11\\13\\15\\11\\11\\0F\\0B\\11builtin\\00vhlo\\00module\\00func_v1\\00sine_v1\\00return_v1\\00sym_name\\00jit_sin\\00arg_attrs\\00function_type\\00res_attrs\\00sym_visibility\\00jit(sin)/jit(main)/sin\\00third_party/py/jax/experimental/jax2tf/tests/back_compat_test.py\\00jax.arg_info\\00x\\00mhlo.sharding\\00{replicated}\\00jax.result_info\\00\\00main\\00public\\00\", platforms = [\"CPU\"], version = 6 : i64} : (tensor<f32>) -> tensor<*xf32>\n     func.return %0 : tensor<*xf32>\n   }\n \n   // Verifies that the following functions are added from xla_call_module. Note this must be at the end of the file.\n-  // CHECK: func.func private @main.2(%arg0: tensor<f32> {mhlo.sharding = \"{replicated}\"}) -> tensor<f32> {\n+  // CHECK: func.func private @main.1(%arg0: tensor<f32> {mhlo.sharding = \"{replicated}\"}) -> tensor<f32> {\n   // CHECK:   %0 = mhlo.sine %arg0 : tensor<f32>\n   // CHECK:   return %0 : tensor<f32>\n   // CHECK: }"
        },
        {
            "sha": "a3090e81f84a825f53406b91d11695bf951a269b",
            "filename": "tensorflow/compiler/tf2xla/xla_compiler_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler_test.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -1960,7 +1960,9 @@ TEST_F(XlaCompilerTest, SetShardingForReturnedTuple) {\n   const auto& hlo_computation_proto = hlo_module_proto.computations(0);\n   std::optional<xla::HloInstructionProto> root_instruction_proto;\n   for (const auto& inst : hlo_computation_proto.instructions()) {\n-    if (inst.id() == hlo_computation_proto.root_id()) {\n+    if (xla::HloInstruction::CalculateLocalId(inst.id()) ==\n+        xla::HloInstruction::CalculateLocalId(\n+            hlo_computation_proto.root_id())) {\n       root_instruction_proto = inst;\n       break;\n     }\n@@ -2196,13 +2198,13 @@ TEST_F(XlaCompilerTest, SetShardingForParametersAndReturnValues) {\n \n   // Expected HLO module with Shardy attributes\n   const char* const expected = R\"(\n-    // CHECK:                HloModule test.6, entry_computation_layout={{.*}}, frontend_attributes=\n+    // CHECK:                HloModule test.1, entry_computation_layout={{.*}}, frontend_attributes=\n     // CHECK-SAME:             {xla.sdy.tuple_results_shardings=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}\n     //\n-    // CHECK:                ENTRY %test.6 (arg0.1: s32[2,2]) -> (s32[2,2]) {\n+    // CHECK:                ENTRY %test.1 (arg0.1: s32[2,2]) -> (s32[2,2]) {\n     // CHECK-NEXT:             %arg0.1 = s32[2,2]{1,0} parameter(0), parameter_replication={false}, sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>\"}, metadata={op_name=\"XLA_Args\"}\n-    // CHECK-NEXT:             %reshape.2 = s32[2,2]{1,0} reshape(%arg0.1)\n-    // CHECK-NEXT:             %XLA_Retvals.3 = s32[2,2]{1,0} reshape(%reshape.2), metadata={op_name=\"XLA_Retvals\"}\n+    // CHECK-NEXT:             %reshape.1 = s32[2,2]{1,0} reshape(%arg0.1)\n+    // CHECK-NEXT:             %XLA_Retvals.3 = s32[2,2]{1,0} reshape(%reshape.1), metadata={op_name=\"XLA_Retvals\"}\n     // CHECK-NEXT:             %XLA_Retvals.4 = s32[2,2]{1,0} copy(%XLA_Retvals.3), sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}, metadata={op_name=\"XLA_Retvals\"}\n     // CHECK-NEXT{LITERAL}:    ROOT %XLA_Retvals.5 = (s32[2,2]{1,0}) tuple(%XLA_Retvals.4), sharding={{devices=[1,2]<=[2]}}, metadata={op_name=\"XLA_Retvals\"}\n   })\";\n@@ -2273,22 +2275,22 @@ TEST_F(XlaCompilerTest, SetShardingForTupleArguments) {\n \n   // Expected HLO module with Shardy attributes\n   const char* const expected = R\"(\n-    // CHECK:                HloModule test.11, entry_computation_layout={{.*}}, frontend_attributes={\n+    // CHECK:                HloModule test.1, entry_computation_layout={{.*}}, frontend_attributes={\n     // CHECK-SAME:             xla.sdy.tuple_args_shardings=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>, <mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\",\n     // CHECK-SAME:             xla.sdy.tuple_results_shardings=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>, <mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\",\n     // CHECK-SAME:             xla.sdy.use_tuple_args=\"True\"}\n     //\n-    // CHECK:                ENTRY %test.11 (arg_tuple.1: (s32[2,2], s32[2,2])) -> (s32[2,2], s32[2,2]) {\n+    // CHECK:                ENTRY %test.1 (arg_tuple.1: (s32[2,2], s32[2,2])) -> (s32[2,2], s32[2,2]) {\n     // CHECK-NEXT{LITERAL}:    %arg_tuple.1 = (s32[2,2]{1,0}, s32[2,2]{1,0}) parameter(0), parameter_replication={false,false}, sharding={{devices=[1,2]<=[2]}, {devices=[1,2]<=[2]}}, metadata={op_name=\"XLA_Args\"}\n     // CHECK-NEXT:             %get-tuple-element.2 = s32[2,2]{1,0} get-tuple-element(%arg_tuple.1), index=0, sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}\n-    // CHECK-NEXT:             %reshape.4 = s32[2,2]{1,0} reshape(%get-tuple-element.2)\n-    // CHECK-NEXT:             %XLA_Retvals.6 = s32[2,2]{1,0} reshape(%reshape.4), metadata={op_name=\"XLA_Retvals\"}\n-    // CHECK-NEXT:             %XLA_Retvals.7 = s32[2,2]{1,0} copy(%XLA_Retvals.6), sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}, metadata={op_name=\"XLA_Retvals\"}\n+    // CHECK-NEXT:             %reshape.2 = s32[2,2]{1,0} reshape(%get-tuple-element.2)\n+    // CHECK-NEXT:             %XLA_Retvals.5 = s32[2,2]{1,0} reshape(%reshape.2), metadata={op_name=\"XLA_Retvals\"}\n+    // CHECK-NEXT:             %XLA_Retvals.6 = s32[2,2]{1,0} copy(%XLA_Retvals.5), sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}, metadata={op_name=\"XLA_Retvals\"}\n     // CHECK-NEXT:             %get-tuple-element.3 = s32[2,2]{1,0} get-tuple-element(%arg_tuple.1), index=1, sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}\n-    // CHECK-NEXT:             %reshape.5 = s32[2,2]{1,0} reshape(%get-tuple-element.3)\n-    // CHECK-NEXT:             %XLA_Retvals.8 = s32[2,2]{1,0} reshape(%reshape.5), metadata={op_name=\"XLA_Retvals\"}\n-    // CHECK-NEXT:             %XLA_Retvals.9 = s32[2,2]{1,0} copy(%XLA_Retvals.8), sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}, metadata={op_name=\"XLA_Retvals\"}\n-    // CHECK-NEXT{LITERAL}:    ROOT %XLA_Retvals.10 = (s32[2,2]{1,0}, s32[2,2]{1,0}) tuple(%XLA_Retvals.7, %XLA_Retvals.9), sharding={{devices=[1,2]<=[2]}, {devices=[1,2]<=[2]}}, metadata={op_name=\"XLA_Retvals\"}\n+    // CHECK-NEXT:             %reshape.3 = s32[2,2]{1,0} reshape(%get-tuple-element.3)\n+    // CHECK-NEXT:             %XLA_Retvals.7 = s32[2,2]{1,0} reshape(%reshape.3), metadata={op_name=\"XLA_Retvals\"}\n+    // CHECK-NEXT:             %XLA_Retvals.8 = s32[2,2]{1,0} copy(%XLA_Retvals.7), sharding={devices=[1,2]<=[2]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"_axis_0\\\"=2]>, [{}, {\\\"_axis_0\\\"}]>]>\"}, metadata={op_name=\"XLA_Retvals\"}\n+    // CHECK-NEXT{LITERAL}:    ROOT %XLA_Retvals.9 = (s32[2,2]{1,0}, s32[2,2]{1,0}) tuple(%XLA_Retvals.6, %XLA_Retvals.8), sharding={{devices=[1,2]<=[2]}, {devices=[1,2]<=[2]}}, metadata={op_name=\"XLA_Retvals\"}\n   })\";\n \n   XlaCompiler::Options compiler_options = DefaultOptions();"
        },
        {
            "sha": "ada6c301033fdd747780b3eb291a72c025a979f7",
            "filename": "third_party/xla/xla/hlo/analysis/hlo_reachability.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_reachability.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_reachability.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_reachability.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_HLO_ANALYSIS_HLO_REACHABILITY_H_\n #define XLA_HLO_ANALYSIS_HLO_REACHABILITY_H_\n \n+#include <cstdint>\n #include <memory>\n #include <utility>\n #include <vector>\n@@ -200,7 +201,7 @@ class HloReachabilityMap {\n \n   friend class HloReachabilityMapBitSetBenchmark;\n \n-  using Key = std::pair<int, int>;  // module ID, instruction ID.\n+  using Key = std::pair<int64_t, int64_t>;  // module ID, instruction ID.\n   static Key GetKey(const HloInstruction* instruction) {\n     return {instruction->GetModule()->unique_id(), instruction->unique_id()};\n   }"
        },
        {
            "sha": "4b6025a1b72ebf0d3367825e6d2daf56e0b57a63",
            "filename": "third_party/xla/xla/hlo/analysis/tuple_points_to_analysis.h",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Ftuple_points_to_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Ftuple_points_to_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Ftuple_points_to_analysis.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include <stddef.h>\n \n+#include <cstdint>\n #include <iosfwd>\n #include <memory>\n #include <set>\n@@ -308,7 +309,7 @@ class TuplePointsToAnalysis : public DfsHloVisitorWithDefault {\n   };\n \n   const PerInstruction* PerInst(const HloInstruction* inst) const {\n-    int id = inst->unique_id();\n+    int64_t id = inst->unique_id();\n     DCHECK_GE(id, 0);\n     auto iter = per_instruction_.find(id);\n     if (iter == per_instruction_.end()) {\n@@ -317,7 +318,7 @@ class TuplePointsToAnalysis : public DfsHloVisitorWithDefault {\n     return iter->second.get();\n   }\n   PerInstruction* PerInst(const HloInstruction* inst) {\n-    int id = inst->unique_id();\n+    int64_t id = inst->unique_id();\n     DCHECK_GE(id, 0);\n     auto iter = per_instruction_.find(id);\n     if (iter == per_instruction_.end()) {\n@@ -334,7 +335,8 @@ class TuplePointsToAnalysis : public DfsHloVisitorWithDefault {\n   const std::unique_ptr<LogicalBufferAnalysis> logical_buffer_analysis_;\n \n   // A map from instruction->unique_id() to\n-  absl::flat_hash_map<int, std::unique_ptr<PerInstruction>> per_instruction_;\n+  absl::flat_hash_map<int64_t, std::unique_ptr<PerInstruction>>\n+      per_instruction_;\n \n   // A map from LogicalBuffer->id() to alias information about that logical\n   // buffer"
        },
        {
            "sha": "2431b12feac3ac36a2252226896f789dcfba5976",
            "filename": "third_party/xla/xla/hlo/builder/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2FBUILD?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -160,11 +160,11 @@ cc_library(\n         \"//xla:window_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/service:collective_ops_utils\",\n         \"//xla/service:hlo_proto_cc\",\n+        \"//xla/service:name_uniquer\",\n         \"//xla/service:shape_inference\",\n-        \"//xla/service:source_target_pairs\",\n         \"//xla/tsl/lib/core:bitmap\",\n+        \"//xla/tsl/platform:errors\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "584cf6b173798c96665374c14b2f1070ebb14966",
            "filename": "third_party/xla/xla/hlo/builder/value_inference.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fvalue_inference.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fvalue_inference.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fvalue_inference.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -125,10 +125,6 @@ struct HloProtoEvaluator {\n   HloProtoEvaluator& WithComputation(\n       std::unique_ptr<HloComputation> new_computation) {\n     computation = new_computation.get();\n-    computation->ClearUniqueIdInternal();\n-    for (HloInstruction* inst : computation->instructions()) {\n-      inst->ClearUniqueIdInternal();\n-    }\n     module.AddEmbeddedComputation(std::move(new_computation));\n     return *this;\n   }\n@@ -168,7 +164,10 @@ struct HloProtoEvaluator {\n       int64_t operand_handle = inst.operand_ids(i);\n       std::unique_ptr<HloInstruction> operand =\n           HloInstruction::CreateConstant(operands[i].Clone());\n-      operand_map[operand_handle] = operand.get();\n+      // FromProto uses local ids, so explicitly downcasts the unique id to\n+      // a local id to avoid issues.\n+      operand_map[HloInstruction::CalculateLocalId(operand_handle)] =\n+          operand.get();\n       builder.AddInstruction(std::move(operand));\n     }\n \n@@ -190,7 +189,6 @@ struct HloProtoEvaluator {\n     TF_ASSIGN_OR_RETURN(\n         auto new_instruction,\n         HloInstruction::CreateFromProto(inst, operand_map, computation_map));\n-    new_instruction->ClearUniqueIdInternal();\n     builder.AddInstruction(std::move(new_instruction));\n     auto computation = builder.Build();\n     module.AddEntryComputation(std::move(computation));"
        },
        {
            "sha": "cba9bdf78188dcfa2cb268155e90f1745a813961",
            "filename": "third_party/xla/xla/hlo/builder/xla_builder.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 23,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -63,10 +63,10 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/sharding_op_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/util.h\"\n #include \"xla/window_util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/errors.h\"\n #include \"tsl/platform/stacktrace.h\"\n #include \"tsl/platform/statusor.h\"\n \n@@ -922,16 +922,18 @@ absl::Status XlaBuilder::BuildComputationProto(int64_t root_id,\n           instruction_shapes_[index]->ToProto();\n     }\n   }\n-\n-  SetProtoIdAndName(&proto, name_, kNameSeparator, GetNextId());\n+  int64_t computation_id = GetNextComputationId();\n+  SetProtoIdAndName(&proto, name_, kNameSeparator, computation_id);\n   TF_ASSIGN_OR_RETURN(ProgramShape program_shape, GetProgramShape(root_id));\n   *proto.mutable_program_shape() = program_shape.ToProto();\n   proto.set_root_id(root_id);\n \n   for (auto& instruction : instructions_) {\n     // Ensures that the instruction names are unique among the whole graph.\n-    instruction.set_name(\n-        GetFullName(instruction.name(), kNameSeparator, instruction.id()));\n+    instruction.set_name(UniquifyInstructionName(instruction.name()));\n+    // Creates a unique ID for the instruction.\n+    instruction.set_id(\n+        HloInstruction::CalculateUniqueId(computation_id, instruction.id()));\n     proto.add_instructions()->Swap(&instruction);\n   }\n \n@@ -1637,7 +1639,7 @@ XlaOp XlaBuilder::Parameter(\n                              parameter_number);\n     }\n     instr.set_parameter_number(parameter_number);\n-    instr.set_name(name);\n+    instr.set_name(UniquifyInstructionName(name));\n     *instr.mutable_shape() = shape.ToProto();\n     if (!replicated_at_leaf_buffers.empty()) {\n       auto replication = instr.mutable_parameter_replication();\n@@ -4702,7 +4704,7 @@ absl::StatusOr<XlaComputation> XlaBuilder::BuildConstantSubGraph(\n \n   HloComputationProto entry;\n   SetProtoIdAndName(&entry, StrCat(name_, \"_compute_constant\"), kNameSeparator,\n-                    GetNextId());\n+                    GetNextComputationId());\n   ProgramShapeProto* program_shape = entry.mutable_program_shape();\n   *program_shape->mutable_result() = root->shape();\n \n@@ -4775,7 +4777,7 @@ absl::StatusOr<XlaComputation> XlaBuilder::BuildConstantSubGraph(\n       }\n       *const_instr.mutable_opcode() =\n           std::string(HloOpcodeString(HloOpcode::kConstant));\n-      const_instr.set_id(handle);\n+      const_instr.set_id(HloInstruction::CalculateUniqueId(entry.id(), handle));\n       *const_instr.mutable_name() =\n           GetFullName(const_instr.opcode(), kNameSeparator, const_instr.id());\n       *entry.add_instructions() =\n@@ -4812,7 +4814,7 @@ absl::StatusOr<XlaComputation> XlaBuilder::BuildConstantSubGraph(\n     root_id = it->second;\n     it = substitutions.find(root_id);\n   }\n-  entry.set_root_id(root_id);\n+  entry.set_root_id(HloInstruction::CalculateUniqueId(entry.id(), root_id));\n \n   // Add related ops to the computation.\n   for (int64_t id : related_ops) {\n@@ -4947,8 +4949,7 @@ absl::StatusOr<XlaOp> XlaBuilder::AddInstruction(\n     HloInstructionProto&& instr, HloOpcode opcode,\n     absl::Span<const XlaOp> operands) {\n   TF_RETURN_IF_ERROR(first_error_);\n-\n-  const int64_t handle = GetNextId();\n+  const int64_t handle = GetNextInstructionId();\n   instr.set_id(handle);\n   *instr.mutable_opcode() = std::string(HloOpcodeString(opcode));\n   for (const auto& operand : operands) {\n@@ -4979,10 +4980,10 @@ absl::StatusOr<XlaOp> XlaBuilder::AddInstruction(\n       absl::string_view name = (last_slash_pos == absl::string_view::npos)\n                                    ? op_name\n                                    : op_name.substr(last_slash_pos + 1);\n-      instr.set_name(\n-          xla::SanitizeOpName(std::string(name), kNameSeparator, \"_\"));\n+      instr.set_name(UniquifyInstructionName(\n+          xla::SanitizeOpName(std::string(name), kNameSeparator, \"_\")));\n     } else {\n-      instr.set_name(instr.opcode());\n+      instr.set_name(UniquifyInstructionName(instr.opcode()));\n     }\n   }\n \n@@ -5034,32 +5035,38 @@ XlaComputationId XlaBuilder::AddSubComputation(\n   // old->new mappings in remapped_ids.\n   for (const HloComputationProto& e : computation.proto().computations()) {\n     HloComputationProto new_computation(e);\n-    int64_t computation_id = GetNextId();\n+    int64_t computation_id = GetNextComputationId();\n     remapped_ids[new_computation.id()] = computation_id;\n     SetProtoIdAndName(&new_computation,\n                       GetBaseName(new_computation.name(), kNameSeparator),\n                       kNameSeparator, computation_id);\n+    // No ID remapping needed for instructions just adding the computation\n+    // prefix.\n     for (auto& instruction : *new_computation.mutable_instructions()) {\n-      int64_t instruction_id = GetNextId();\n-      remapped_ids[instruction.id()] = instruction_id;\n-      SetProtoIdAndName(&instruction,\n-                        GetBaseName(instruction.name(), kNameSeparator),\n-                        kNameSeparator, instruction_id);\n+      instruction.set_name(UniquifyInstructionName(instruction.name()));\n+      instruction.set_id(\n+          HloInstruction::CalculateUniqueId(computation_id, instruction.id()));\n     }\n-    new_computation.set_root_id(remapped_ids.at(new_computation.root_id()));\n+    new_computation.set_root_id(HloInstruction::CalculateUniqueId(\n+        computation_id, new_computation.root_id()));\n \n     imported_computations.push_back(std::move(new_computation));\n   }\n   // Once we have imported all the computations, and captured all the ID\n   // mappings, we go back and fixup the IDs in the imported computations.\n   for (auto& imported_computation : imported_computations) {\n+    // No ID remapping needed for instructions only for called_computation_ids.\n+    // Operands and control_predecessors need to have the new computation ID\n+    // prepended.\n     for (auto& instruction : *imported_computation.mutable_instructions()) {\n       for (auto& operand_id : *instruction.mutable_operand_ids()) {\n-        operand_id = remapped_ids.at(operand_id);\n+        operand_id = HloInstruction::CalculateUniqueId(\n+            imported_computation.id(), operand_id);\n       }\n       for (auto& control_predecessor_id :\n            *instruction.mutable_control_predecessor_ids()) {\n-        control_predecessor_id = remapped_ids.at(control_predecessor_id);\n+        control_predecessor_id = HloInstruction::CalculateUniqueId(\n+            imported_computation.id(), control_predecessor_id);\n       }\n       for (auto& called_computation_id :\n            *instruction.mutable_called_computation_ids()) {"
        },
        {
            "sha": "9330840ce877bca08eea6a56af7f5959063edc90",
            "filename": "third_party/xla/xla/hlo/builder/xla_builder.h",
            "status": "modified",
            "additions": 34,
            "deletions": 4,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -51,6 +51,7 @@ limitations under the License.\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/hlo.pb.h\"\n+#include \"xla/service/name_uniquer.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/bitmap.h\"\n@@ -1224,8 +1225,19 @@ class XlaBuilder {\n       const Shape& lhs_shape, const Shape& rhs_shape,\n       const ConvolutionDimensionNumbers& dimension_numbers) const;\n \n-  int64_t GetNextId() {\n-    return parent_builder_ ? parent_builder_->GetNextId() : ++next_id_;\n+  int64_t GetNextInstructionId() {\n+    // Instruction Ids exist within the context of its parent computation. No\n+    // need to uniquify across computations.\n+    return ++next_instruction_id_;\n+  }\n+\n+  int64_t GetNextComputationId() {\n+    // Computation Ids exist within the context of its parent module. No need to\n+    // uniquify across modules.\n+    if (parent_builder_ == nullptr) {\n+      return ++next_computation_id_;\n+    }\n+    return parent_builder_->GetNextComputationId();\n   }\n \n   // Populates the module with the input/output alias information stored within\n@@ -1238,9 +1250,27 @@ class XlaBuilder {\n \n   std::string name_;  // Name to use for the built computation.\n \n-  // The next sequential ID for every instruction/computation contained within\n+  // The next sequential ID for every instruction contained within\n   // this computation. Unused if this builder has a parent.\n-  int64_t next_id_ = 0;\n+  int64_t next_instruction_id_ = 0;\n+\n+  // The next sequential ID for every computation contained within\n+  // this module.\n+  int64_t next_computation_id_ = 0;\n+\n+  // For uniquifying instruction names within this computation / module.\n+  NameUniquer instruction_name_uniquer_ = NameUniquer(\".\");\n+\n+  // Uniquifies the names of instructions within this computation / module,\n+  // using always the highest available parent builder.\n+  std::string UniquifyInstructionName(absl::string_view name) {\n+    XlaBuilder* highest_parent_builder = this;\n+    while (highest_parent_builder->parent_builder_ != nullptr) {\n+      highest_parent_builder = highest_parent_builder->parent_builder_;\n+    }\n+    return highest_parent_builder->instruction_name_uniquer_.GetUniqueName(\n+        name);\n+  }\n \n   // The first error encountered while building the computation.\n   // This is OK until the first error is encountered."
        },
        {
            "sha": "a2ee4c91802570f87922dcc7e9584fc040b6b0a1",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/stablehlo_utils_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fstablehlo_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fstablehlo_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fstablehlo_utils_test.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -110,7 +110,7 @@ TEST_F(StablehloUtilsTest, ConvertShardyToHloNoSharding) {\n \n   const std::string kExpectedHloPattern = R\"(\n   CHECK: Arg_0.1 = f32[400,400]{1,0} parameter(0), sharding={replicated}\n-  CHECK: Arg_1.2 = f32[400,400]{1,0} parameter(1), sharding={replicated}\n+  CHECK: Arg_1.1 = f32[400,400]{1,0} parameter(1), sharding={replicated}\n   )\";\n \n   ConvertShardyAndCompare(kShardyMlirString, kExpectedHloPattern);\n@@ -123,7 +123,7 @@ TEST_F(StablehloUtilsTest, ConvertShardyToHlo1DSharding) {\n \n   const std::string kExpectedHloPattern = R\"(\n   CHECK: %Arg_0.1 = f32[400,400]{1,0} parameter(0), sharding={devices=[4,1,2]<=[8] last_tile_dim_replicate}\n-  CHECK: %Arg_1.2 = f32[400,400]{1,0} parameter(1), sharding={replicated}\n+  CHECK: %Arg_1.1 = f32[400,400]{1,0} parameter(1), sharding={replicated}\n   )\";\n \n   ConvertShardyAndCompare(kShardyMlirString, kExpectedHloPattern);\n@@ -136,7 +136,7 @@ TEST_F(StablehloUtilsTest, ConvertShardyToHlo2DSharding) {\n \n   const std::string kExpectedHloPattern = R\"(\n   CHECK: %Arg_0.1 = f32[400,400]{1,0} parameter(0), sharding={devices=[4,1,2]<=[8] last_tile_dim_replicate}\n-  CHECK:  %Arg_1.2 = f32[400,400]{1,0} parameter(1), sharding={devices=[1,2,4]<=[4,2]T(1,0) last_tile_dim_replicate}\n+  CHECK:  %Arg_1.1 = f32[400,400]{1,0} parameter(1), sharding={devices=[1,2,4]<=[4,2]T(1,0) last_tile_dim_replicate}\n   )\";\n \n   ConvertShardyAndCompare(kShardyMlirString, kExpectedHloPattern);\n@@ -158,10 +158,10 @@ TEST_F(StablehloUtilsTest, ConvertShardyToHloDataFlowEdge) {\n   )MLIR\";\n \n   const std::string kExpectedHloPattern = R\"(\n-  CHECK: %region_0.7 (Arg_.4: s64[4]) -> s64[4]\n-  CHECK: %region_1.11 (Arg_.8: s64[4]) -> s64[4]\n-  CHECK: ENTRY %main.13 (Arg_0.1: s32[], Arg_1.2: s64[4], Arg_2.3: s64[4]) -> s64[4]\n-  CHECK: ROOT %conditional.12 = s64[4]{0} conditional(%Arg_0.1, %Arg_1.2, %Arg_2.3), branch_computations={%region_0.7, %region_1.11}, sharding={replicated}\n+  CHECK: %region_0.1 (Arg_.1: s64[4]) -> s64[4]\n+  CHECK: %region_1.2 (Arg_.3: s64[4]) -> s64[4]\n+  CHECK: ENTRY %main.3 (Arg_0.1: s32[], Arg_1.1: s64[4], Arg_2.1: s64[4]) -> s64[4]\n+  CHECK: ROOT %conditional.1 = s64[4]{0} conditional(%Arg_0.1, %Arg_1.1, %Arg_2.1), branch_computations={%region_0.1, %region_1.2}, sharding={replicated}\n   )\";\n \n   ConvertShardyAndCompare(kShardyMlirString, kExpectedHloPattern);"
        },
        {
            "sha": "f29c9ae560b25ed56bfb47c5cf818b551ce3ad2c",
            "filename": "third_party/xla/xla/hlo/ir/dfs_hlo_visitor.h",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fdfs_hlo_visitor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fdfs_hlo_visitor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fdfs_hlo_visitor.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -17,17 +17,11 @@ limitations under the License.\n #define XLA_HLO_IR_DFS_HLO_VISITOR_H_\n \n #include <cstddef>\n+#include <cstdint>\n #include <type_traits>\n-#include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/tsl/platform/status.h\"\n-#include \"xla/types.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -344,7 +338,7 @@ class DfsHloVisitorBase {\n     kVisited = 2,\n   };\n \n-  VisitState GetVisitState(int id) {\n+  VisitState GetVisitState(int64_t id) {\n     auto iter = visit_state_.find(id);\n     if (iter == visit_state_.end()) {\n       return VisitState::kNotVisited;\n@@ -371,10 +365,10 @@ class DfsHloVisitorBase {\n   // Useful when we want to free up the memory used by the visit state without\n   // destroying the actual visitor subclass.\n   void DestroyVisitState() {\n-    visit_state_ = absl::flat_hash_map<int, VisitState>{};\n+    visit_state_ = absl::flat_hash_map<int64_t, VisitState>{};\n   }\n \n-  void SetVisitState(int id, VisitState state) { visit_state_[id] = state; }\n+  void SetVisitState(int64_t id, VisitState state) { visit_state_[id] = state; }\n \n   // Sets the visitation state of the given instruction as kVisiting.\n   //\n@@ -428,7 +422,7 @@ class DfsHloVisitorBase {\n   virtual bool ShouldProcessNode(HloInstructionPtr hlo) { return true; }\n \n  private:\n-  absl::flat_hash_map<int, VisitState> visit_state_;\n+  absl::flat_hash_map<int64_t, VisitState> visit_state_;\n \n   DfsHloVisitorBase(const DfsHloVisitorBase&) = delete;\n   DfsHloVisitorBase& operator=(const DfsHloVisitorBase&) = delete;"
        },
        {
            "sha": "96367c9f03ee4ec1126e2a7a0b66d1bf312ed243",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "status": "modified",
            "additions": 170,
            "deletions": 42,
            "changes": 212,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -72,6 +72,7 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/protobuf.h\"\n \n namespace xla {\n \n@@ -151,14 +152,34 @@ std::unique_ptr<HloComputation> HloComputation::Builder::Build(\n HloComputation::HloComputation(\n     const std::string& name, int parameter_count,\n     std::vector<std::unique_ptr<HloInstruction>>* instructions,\n-    HloInstruction* root_instruction)\n+    HloInstruction* root_instruction, bool from_proto)\n     : unique_id_(-1),\n       root_instruction_(root_instruction),\n       instruction_count_(0),\n       name_(NameUniquer::GetSanitizedName(name)) {\n   param_instructions_.resize(parameter_count, nullptr);\n   bool root_found = false;\n+\n+  if (from_proto) {\n+    // Pre-allocate all instructions in the vector since it state should be\n+    // identical.\n+    int32_t max_instruction_local_id = 0;\n+    for (auto& instruction : *instructions) {\n+      if (instruction != nullptr) {\n+        max_instruction_local_id =\n+            std::max(max_instruction_local_id, instruction->local_id());\n+      }\n+    }\n+    instructions_.resize(max_instruction_local_id + 1);\n+  }\n+\n   for (auto& instruction : *instructions) {\n+    if (instruction == nullptr) {\n+      // The instruction is actually a gap left in the instructions vector which\n+      // should be represented by an empty HloInstructionInfo.\n+      instructions_.push_back(HloInstructionInfo());\n+      continue;\n+    }\n     if (instruction->opcode() == HloOpcode::kParameter) {\n       int64_t param_no = instruction->parameter_number();\n       CHECK(param_no >= 0 && param_no < parameter_count)\n@@ -170,7 +191,7 @@ HloComputation::HloComputation(\n       param_instructions_[param_no] = instruction.get();\n     }\n     root_found |= instruction.get() == root_instruction_;\n-    AddInstructionInternal(std::move(instruction));\n+    AddInstructionInternal(std::move(instruction), from_proto);\n   }\n   CHECK(root_found)\n       << \"\\nERROR: root instruction is not present in computation.\";\n@@ -271,6 +292,45 @@ HloInstruction* HloComputation::AddInstruction(\n   return AddInstruction(std::move(instruction));\n }\n \n+void HloComputation::CopyLocalIdsFromComputation(\n+    const HloComputation& source_computation, const HloCloneContext& context) {\n+  ClearUniqueIdInternal();\n+  SetUniqueIdHelper(source_computation.unique_id_);\n+  VLOG(2) << \"Copying local ids from computation \" << source_computation.name()\n+          << \" to computation \" << name();\n+  // Rewrites the instructions_ vector to copy the local_id_ from the source\n+  // computation to the destination computation since the local_id_ represents\n+  // physical order of the instructions in the computation.\n+  HloInstructionList cloned_instructions(\n+      source_computation.instructions_.size());\n+  for (HloInstruction* source_instruction : source_computation.instructions()) {\n+    if (source_instruction == nullptr) {\n+      // Instruction is a gap left in the instructions vector.\n+      continue;\n+    }\n+    HloInstruction* destination_instruction =\n+        context.FindInstruction(source_instruction);\n+    if (destination_instruction == nullptr) {\n+      VLOG(2) << \"Destination instruction is nullptr for source instruction \"\n+              << source_instruction->ToString();\n+      continue;\n+    }\n+\n+    int32_t original_destination_local_id = destination_instruction->local_id_;\n+    CHECK(cloned_instructions.at(source_instruction->local_id_).inst() ==\n+          nullptr)\n+        << \"Instruction local_id_ conflict at index \"\n+        << source_instruction->local_id_;\n+    destination_instruction->ClearUniqueIdInternal();\n+    destination_instruction->SetLocalId(source_instruction->local_id_);\n+    cloned_instructions[source_instruction->local_id_] =\n+        std::move(instructions_[original_destination_local_id]);\n+  }\n+\n+  instructions_ = std::move(cloned_instructions);\n+  next_instruction_unique_id_ = instructions_.size();\n+}\n+\n static void IncrementCount(\n     absl::btree_map<HloComputation*, int, HloComputation::UniqueIdComparator>&\n         map,\n@@ -360,22 +420,41 @@ absl::flat_hash_map<HloInstruction*, int>* const HloComputation::GetCallersMap()\n }\n \n HloInstruction* HloComputation::AddInstructionInternal(\n-    std::unique_ptr<HloInstruction> instruction) {\n+    std::unique_ptr<HloInstruction> instruction, bool from_proto) {\n   if (parent() != nullptr) {\n     instruction->UniquifyName(parent());\n-    instruction->UniquifyId(parent());\n   }\n   instruction->set_parent(this);\n   HloInstruction* pinst = instruction.release();  // Take ownership\n   HloInstructionInfo info;\n   info.opcode_ = pinst->opcode();\n   info.inst_ = pinst;\n-  VLOG(2) << \"Adding instruction \" << pinst << \" \" << pinst->name()\n-          << \" from computation \" << name() << \" opcode \" << info.opcode();\n-  uint32_t index = instructions_.size();\n+\n+  if (from_proto && pinst->local_id_ >= 0) {\n+    // Already set unique id from proto sources therefore it is preserved.\n+    // Calls for AddInstructionInternal from proto sources assume that all space\n+    // in instructions_ vector has been pre-allocated.\n+    CHECK(pinst->local_id() < instructions_.size())\n+        << \"Instruction local_id \" << pinst->local_id()\n+        << \" is out of range [0, \" << instructions_.size()\n+        << \") when adding instruction from proto\";\n+    next_instruction_unique_id_ = instructions_.size();\n+    instructions_[pinst->local_id()] = info;\n+  } else {\n+    // Unset instructions from proto sources and regular instructions get\n+    // assigned a new unique id.\n+    pinst->ClearUniqueIdInternal();\n+    // Must match the size of the instructions_ vector.\n+    CHECK_EQ(next_instruction_unique_id_, instructions_.size())\n+        << \"Mismatch between next_instruction_unique_id_ and instructions_ \"\n+           \"vector size at computation: \"\n+        << name();\n+    pinst->SetLocalId(next_instruction_unique_id_++);\n+    instructions_.push_back(info);\n+  }\n+\n   instruction_count_++;\n-  pinst->index_in_parent_ = index;\n-  instructions_.push_back(info);\n+\n   for (HloComputation* called_computation : pinst->called_computations()) {\n     CHECK(called_computation);\n     // TODO(b/399394039): Consider enforcing that\n@@ -709,7 +788,7 @@ absl::Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,\n       << \"instruction \" << instruction->name()\n       << \" has control successors and cannot be removed\";\n \n-  HloInstructionInfo* info = &instructions_[instruction->index_in_parent_];\n+  HloInstructionInfo* info = &instructions_[instruction->local_id_];\n   DCHECK_EQ(info->inst(), instruction);\n   to_be_deleted_.push_back(info->inst());  // Takes ownership\n   to_be_deleted_.back()->DetachFromOperandsAndUsers();\n@@ -728,11 +807,9 @@ absl::Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,\n   // TODO(jeff): should we set info->opcode to something?\n   info->inst_ =\n       nullptr;  // Leave a hole: this is no longer part of \"instructions()\"\n-  instruction->index_in_parent_ = ~0u;\n+  instruction->local_id_ = -1;\n   instruction_count_--;\n-  DCHECK_EQ(instructions_.size() - to_be_deleted_.size(), instruction_count())\n-      << \"instructions_.size(): \" << instructions_.size()\n-      << \", to_be_deleted_.size(): \" << to_be_deleted_.size();\n+\n   return absl::OkStatus();\n }\n \n@@ -760,22 +837,19 @@ void HloComputation::Cleanup() {\n     }\n     // Update reverse mapping and overwrite the 'marked' entry.\n     HloInstruction* unmarked_instruction = it->inst();\n-    unmarked_instruction->index_in_parent_ =\n+    // Changes the unique_id of the instruction due to recompaction.\n+    unmarked_instruction->local_id_ =\n         std::distance(instructions_.begin(), marked_it);\n     *marked_it++ = std::move(*it);\n   }\n \n   DCHECK(marked_it < instructions_.end());\n-  DCHECK_EQ(std::distance(marked_it, instructions_.end()),\n-            to_be_deleted_.size());\n-  DCHECK_EQ(instructions_.size() - to_be_deleted_.size(), instruction_count())\n-      << \"instructions_.size(): \" << instructions_.size()\n-      << \", to_be_deleted_.size(): \" << to_be_deleted_.size();\n   for (HloInstruction* marked_instruction : to_be_deleted_) {\n     delete marked_instruction;\n   }\n   to_be_deleted_.clear();\n   instructions_.resize(instruction_count());\n+  next_instruction_unique_id_ = instructions_.size();\n }\n \n void HloComputation::set_root_instruction(HloInstruction* new_root_instruction,\n@@ -833,7 +907,7 @@ void HloComputation::ForEachInstructionPostOrderImpl(\n \n   // Pushes instruction to dfs stack only if it was not already processed.\n   auto dfs_stack_push = [&](HloInstruction* instr) {\n-    VisitState state = visited.GetState(instr->index_in_parent_);\n+    VisitState state = visited.GetState(instr->local_id_);\n     if (state != VisitState::kVisited) {\n       dfs_stack->push_back(instr);\n     }\n@@ -846,7 +920,7 @@ void HloComputation::ForEachInstructionPostOrderImpl(\n         << \"Instruction \" << current->name()\n         << \" is not in the current computation (\" << name() << \").\";\n \n-    VisitMap::Handle h = current->index_in_parent_;\n+    VisitMap::Handle h = current->local_id_;\n     VisitState state = visited.GetState(h);\n     if (state == VisitState::kNew) {\n       visited.SetState(h, VisitState::kVisiting);\n@@ -964,7 +1038,7 @@ HloComputation::MakeInstructionPostOrderWithReshapeFirst() const {\n   std::vector<HloInstruction*> frontier_std;\n   std::vector<HloInstruction*> frontier_reshapes;\n   std::vector<HloInstruction*> sorted;\n-  absl::flat_hash_map<int, uint64_t> visitations;\n+  absl::flat_hash_map<int64_t, uint32_t> visitations;\n   sorted.reserve(instruction_count());\n   visitations.reserve(instruction_count());\n \n@@ -1010,8 +1084,8 @@ HloComputation::MakeInstructionPostOrderWithReshapeFirst() const {\n     sorted.push_back(inst);\n     for (HloInstruction* const child : inst->operands()) {\n       // Will increment, or set to 1 if not present\n-      visitations[child->unique_id_64_bits()]++;\n-      if (child->user_count() == visitations[child->unique_id_64_bits()]) {\n+      visitations[child->unique_id()]++;\n+      if (child->user_count() == visitations[child->unique_id()]) {\n         add_to_frontier(child);\n       }\n     }\n@@ -1222,7 +1296,7 @@ HloComputationProto HloComputation::ToProto() const {\n     HloInstructionProto instruction_proto = instruction->ToProto();\n     proto.add_instructions()->Swap(&instruction_proto);\n   }\n-  proto.set_root_id(root_instruction()->unique_id_64_bits());\n+  proto.set_root_id(root_instruction()->unique_id());\n   *proto.mutable_program_shape() = ComputeProgramShape().ToProto();\n   proto.set_is_fusion_computation(IsFusionComputation());\n   proto.set_execution_thread(IsMainThread() ? \"\"\n@@ -1238,7 +1312,21 @@ HloComputation::CreateFromProto(\n   absl::flat_hash_map<int64_t, HloInstruction*> instruction_map;\n   absl::flat_hash_map<HloInstruction*, int64_t> to_proto_id;\n   std::vector<std::unique_ptr<HloInstruction>> instructions;\n+  tsl::protobuf::internal::RepeatedPtrIterator<const xla::HloInstructionProto>\n+      instruction_with_max_id = absl::c_max_element(\n+          proto.instructions(),\n+          [](const HloInstructionProto& a, const HloInstructionProto& b) {\n+            return HloInstruction::CalculateLocalId(a.id()) <\n+                   HloInstruction::CalculateLocalId(b.id());\n+          });\n+  int32_t max_proto_instruction_local_id =\n+      instruction_with_max_id == proto.instructions().end()\n+          ? 0\n+          : HloInstruction::CalculateLocalId(instruction_with_max_id->id());\n+  instructions.resize(max_proto_instruction_local_id + 1);\n+\n   int64_t parameter_count = 0;\n+\n   for (const HloInstructionProto& instruction_proto : proto.instructions()) {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<HloInstruction> instruction,\n                         HloInstruction::CreateFromProto(\n@@ -1247,26 +1335,59 @@ HloComputation::CreateFromProto(\n     if (instruction->opcode() == HloOpcode::kParameter) {\n       parameter_count++;\n     }\n-    TF_RET_CHECK(!ContainsKey(instruction_map, instruction_proto.id()));\n-    instruction_map[instruction_proto.id()] = instruction.get();\n-    to_proto_id[instruction.get()] = instruction_proto.id();\n-    instructions.push_back(std::move(instruction));\n+    int32_t local_proto_id =\n+        HloInstruction::CalculateLocalId(instruction_proto.id());\n+    TF_RET_CHECK(!ContainsKey(instruction_map, local_proto_id));\n+    instruction_map[local_proto_id] = instruction.get();\n+    to_proto_id[instruction.get()] = local_proto_id;\n+    // The instruction's id is the same as the index in the instructions\n+    // vector. This will be reproduced when placing the instruction in the\n+    // instructions vector.\n+    TF_RET_CHECK(instruction->local_id_ >= 0 &&\n+                 instruction->local_id_ < instructions.size())\n+        << \"Instruction local id is out of bounds\" << \" Value is \"\n+        << instruction->local_id_ << \" and size is \" << instructions.size();\n+    TF_RET_CHECK(instructions[instruction->local_id_] == nullptr)\n+        << \"Instruction \" << instruction->name() << \" has duplicate local id \"\n+        << instruction->local_id_;\n+    instructions[instruction->local_id_] = std::move(instruction);\n   }\n \n   TF_RET_CHECK(proto.root_id() != -1);\n-  TF_RET_CHECK(ContainsKey(instruction_map, proto.root_id()));\n-  HloInstruction* root = instruction_map.at(proto.root_id());\n-\n-  // Sort the instructions in the proto id's order.\n-  absl::c_sort(instructions, [&](const std::unique_ptr<HloInstruction>& a,\n-                                 const std::unique_ptr<HloInstruction>& b) {\n-    return to_proto_id[a.get()] < to_proto_id[b.get()];\n-  });\n-\n+  int32_t root_local_id = HloInstruction::CalculateLocalId(proto.root_id());\n+  TF_RET_CHECK(ContainsKey(instruction_map, root_local_id));\n+  HloInstruction* root = instruction_map.at(root_local_id);\n+\n+  // Check if each computation's instructions are unique in their local id\n+  // (lowers 32bits)\n+  absl::flat_hash_set<int32_t> instruction_local_ids;\n+  for (const auto& instruction : instructions) {\n+    // Since the instructions vector replicates the instructions from the proto,\n+    // if there are any gaps in the sequence of instruction ids in the proto, it\n+    // will be represented as a null instruction in the vector.\n+    if (instruction == nullptr) {\n+      continue;\n+    }\n+    TF_RET_CHECK(to_proto_id.contains(instruction.get()))\n+        << \"Instruction not found in to_proto_id map: \" << instruction->name()\n+        << \" local_id: \" << instruction->local_id_;\n+    int32_t local_id_from_proto =\n+        HloInstruction::CalculateLocalId(to_proto_id[instruction.get()]);\n+    TF_RET_CHECK(local_id_from_proto == instruction->local_id_)\n+        << \"Instruction has different local id from proto: proto: \"\n+        << local_id_from_proto << \" vs local: \" << instruction->local_id_;\n+    TF_RET_CHECK(!instruction_local_ids.contains(local_id_from_proto))\n+        << \"Instruction \" << instruction->name()\n+        << \" has duplicate internal unique id \" << local_id_from_proto;\n+    instruction_local_ids.insert(local_id_from_proto);\n+  }\n   TF_RETURN_IF_ERROR([&]() -> absl::Status {\n     std::vector<bool> parameters_seen(parameter_count);\n     int parameters_seen_count = 0;\n     for (auto& instruction : instructions) {\n+      if (instruction == nullptr) {\n+        continue;\n+      }\n       if (instruction->opcode() == HloOpcode::kParameter) {\n         int64_t param_no = instruction->parameter_number();\n         TF_RET_CHECK(param_no >= 0 && param_no < parameter_count)\n@@ -1285,8 +1406,8 @@ HloComputation::CreateFromProto(\n     return absl::OkStatus();\n   }());\n \n-  auto computation = absl::WrapUnique(\n-      new HloComputation(proto.name(), parameter_count, &instructions, root));\n+  auto computation = absl::WrapUnique(new HloComputation(\n+      proto.name(), parameter_count, &instructions, root, /*from_proto=*/true));\n   computation->SetUniqueIdHelper(proto.id());\n   if (proto.is_fusion_computation()) {\n     computation->instruction_and_type_ =\n@@ -1896,7 +2017,6 @@ std::unique_ptr<HloComputation> HloComputation::CloneInContext(\n     auto it = replacements->find(instr);\n     return it != replacements->end() ? it->second.get() : instr;\n   };\n-\n   VLOG(1) << \"Cloning \" << name() << \" --> \" << suffix << \"\\n\";\n \n   // We want to do a postorder walk over [replace(i) for i in instructions_].\n@@ -2030,11 +2150,19 @@ bool HloComputation::CanExpandIntoSingleInstruction() const {\n       });\n }\n \n+HloInstruction* HloComputation::GetInstructionWithLocalId(int32_t local_id) {\n+  if (local_id >= instructions_.size()) {\n+    return nullptr;\n+  }\n+  return instructions_[local_id].get();\n+}\n+\n void HloComputation::ClearUniqueIdInternal() { SetUniqueIdHelper(-1); }\n \n void HloComputation::SetUniqueId(int64_t id) {\n   CHECK_EQ(unique_id_, -1);\n   CHECK_GE(id, 0);\n+  CHECK_LT(id, INT32_MAX) << \"Unique ID must fit in an int32_t\";\n   SetUniqueIdHelper(id);\n }\n "
        },
        {
            "sha": "5717e176a73c890b757565941e8b3eb776c71768",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "status": "modified",
            "additions": 35,
            "deletions": 6,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/btree_map.h\"\n+#include \"absl/container/btree_set.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n@@ -218,7 +219,6 @@ class HloComputation {\n     // Last Value for range checking.\n     kLast = kFusion,\n   };\n-  static constexpr uintptr_t kInstructionTypeMask = 0b111;\n   static_assert(static_cast<int>(InstructionType::kUnset) == 0,\n                 \"kUnset must be 0.\");\n \n@@ -244,6 +244,23 @@ class HloComputation {\n                                  const OpMetadata* metadata,\n                                  const FrontendAttributes* frontend_attributes);\n \n+  // Returns the next unique id to be assigned to an instruction in this\n+  // computation without incrementing the counter.\n+  int32_t next_unique_instruction_internal_id() const {\n+    return next_instruction_unique_id_;\n+  }\n+\n+  // Copy unique ids from the source computation to the current computation.\n+  // The source computation should be the computation from which the current\n+  // computation is cloned. The context is used to find the corresponding\n+  // instruction in the current computation. If the corresponding instruction\n+  // is not found, the unique id is not copied. Does not copy the\n+  // next_unique_id_ counter of the source computation but updates the counter\n+  // of the current computation to accommodate the cloned instruction ids. Used\n+  // for cloning a Module.\n+  void CopyLocalIdsFromComputation(const HloComputation& source_computation,\n+                                   const HloCloneContext& context);\n+\n   // Replace the old parameter at index param_no with\n   // `instruction`. Updates uses and root instruction. Removes old\n   // instruction from computation. No check is done on the shape.\n@@ -835,6 +852,12 @@ class HloComputation {\n   // null if there is no such computation.\n   HloInstruction* GetInstructionWithName(absl::string_view name);\n \n+  // Returns the instruction in this computation that has local id `local_id`.\n+  // Returns null if there is no such instruction.\n+  HloInstruction* GetInstructionWithLocalId(int32_t local_id);\n+\n+  // Returns the unique ID of this computation. The id is stored internally as\n+  // int32_t.\n   int64_t unique_id() const { return unique_id_; }\n \n   void SetExecutionThread(absl::string_view execution_thread) {\n@@ -849,7 +872,8 @@ class HloComputation {\n \n   // Deallocates instructions that are marked by \"RemoveInstruction\" and\n   // compacts the instructions_ vector by removing the deleted instructions'\n-  // entries (a.k.a. tombstones).\n+  // entries (a.k.a. tombstones). This will likely change the unique ids of the\n+  // instructions.\n   // This two-stage clean up process is designed such that HloPass can have\n   // stable internal pointers to HloInstructions while we create and remove\n   // HloInstructions in a pass.\n@@ -941,11 +965,12 @@ class HloComputation {\n   explicit HloComputation(\n       const std::string& name, int parameter_count,\n       std::vector<std::unique_ptr<HloInstruction>>* instructions,\n-      HloInstruction* root_instruction);\n+      HloInstruction* root_instruction, bool from_proto = false);\n \n-  // Internal helper for adding instructions.\n+  // Internal helper for adding instructions. Only assigns a unique id if it is\n+  // not already set.\n   HloInstruction* AddInstructionInternal(\n-      std::unique_ptr<HloInstruction> instruction);\n+      std::unique_ptr<HloInstruction> instruction, bool from_proto = false);\n \n   // Internal helper for comparison with different options.\n   bool EqualInternal(\n@@ -1015,6 +1040,10 @@ class HloComputation {\n   // This is set to -1 if the computation is not in a module. Should only be\n   // updated by SetUniqueIdHelper().\n   int64_t unique_id_;\n+\n+  // The next unique ID to be assigned to an instruction in this computation.\n+  int32_t next_instruction_unique_id_ = 0;\n+\n   HloInstruction* root_instruction_;\n \n   // Module containing this computation.\n@@ -1033,7 +1062,7 @@ class HloComputation {\n \n   // Store instructions in std::vector as they can be added and removed\n   // arbitrarily and we want a stable iteration order.\n-  // For the reverse mapping we use HloInstruction::index_in_parent_.\n+  // For the reverse mapping we use HloInstruction::local_id_.\n   //\n   // Note: removals from this vector must be stable because some users depend on\n   // it. See the Cleanup() method for details on the two-stage removal process."
        },
        {
            "sha": "e9ae023a54268596d5fc3e672ecfb0fc4aec9837",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "status": "modified",
            "additions": 67,
            "deletions": 42,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n \n #include <algorithm>\n-#include <climits>\n #include <cstddef>\n #include <cstdint>\n #include <functional>\n@@ -318,13 +317,13 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n \n   std::unique_ptr<HloInstruction> instruction;\n   const auto operands = [&instruction_map, &proto](int index) {\n-    return instruction_map.at(proto.operand_ids(index));\n+    return instruction_map.at(CalculateLocalId(proto.operand_ids(index)));\n   };\n   const auto all_operands = [&instruction_map, &proto]() {\n     std::vector<HloInstruction*> result(proto.operand_ids_size());\n     std::transform(proto.operand_ids().begin(), proto.operand_ids().end(),\n                    result.begin(), [&instruction_map](int64_t operand_id) {\n-                     return instruction_map.at(operand_id);\n+                     return instruction_map.at(CalculateLocalId(operand_id));\n                    });\n     return result;\n   };\n@@ -354,14 +353,17 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n     return result;\n   };\n \n-  TF_RET_CHECK(\n-      absl::c_all_of(proto.operand_ids(),\n-                     [&](int64_t id) { return instruction_map.contains(id); }))\n+  TF_RET_CHECK(absl::c_all_of(proto.operand_ids(),\n+                              [&](int64_t id) {\n+                                return instruction_map.contains(\n+                                    CalculateLocalId(id));\n+                              }))\n       << proto.name() << \" instruction contains invalid operand id(s)\";\n-\n-  TF_RET_CHECK(\n-      absl::c_all_of(proto.called_computation_ids(),\n-                     [&](int64_t id) { return computation_map.contains(id); }))\n+  TF_RET_CHECK(absl::c_all_of(proto.called_computation_ids(),\n+                              [&](int64_t id) {\n+                                return computation_map.contains(\n+                                    CalculateLocalId(id));\n+                              }))\n       << proto.name() << \" instruction references invalid computation id(s)\";\n \n   TF_ASSIGN_OR_RETURN(Shape shape, Shape::FromProto(proto.shape()));\n@@ -1337,7 +1339,8 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n       }\n \n       for (const int64_t operand_id : proto.operand_ids()) {\n-        instruction->AppendOperand(instruction_map.at(operand_id));\n+        instruction->AppendOperand(\n+            instruction_map.at(CalculateLocalId(operand_id)));\n       }\n       for (const int64_t computation_id : proto.called_computation_ids()) {\n         instruction->AppendComputation(computation_map.at(computation_id));\n@@ -1351,9 +1354,12 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n   }\n \n   for (const int64_t predecessor_id : proto.control_predecessor_ids()) {\n-    TF_RET_CHECK(ContainsKey(instruction_map, predecessor_id))\n-        << \"No instruction with id \" << predecessor_id;\n-    TF_RETURN_IF_ERROR(instruction_map.at(predecessor_id)\n+    int32_t local_predecessor_id = CalculateLocalId(predecessor_id);\n+    TF_RET_CHECK(ContainsKey(instruction_map, local_predecessor_id))\n+        << \"No instruction with id \" << predecessor_id\n+        << \" (local id: \" << local_predecessor_id << \") in computation \"\n+        << proto.name();\n+    TF_RETURN_IF_ERROR(instruction_map.at(local_predecessor_id)\n                            ->AddControlDependencyTo(instruction.get()));\n   }\n \n@@ -1367,15 +1373,8 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n \n   TF_RET_CHECK(proto.id() >= 0)\n       << \"Instruction with negative id: \" << proto.id();\n-  // TODO(b/399394039): Reinforce the condition on INT64_MAX when upgrading\n-  // unique_id_ to int64_t.\n-  if (proto.id() > INT_MAX) {\n-    LOG_EVERY_N(WARNING, 10000)\n-        << \"Instruction with id > INT_MAX: \" << proto.id()\n-        << \" this is not intended behavior and might indicate a bug in the HLO \"\n-           \"proto serialization.\";\n-  }\n-  instruction->unique_id_ = proto.id();\n+  // Drops the most significant 32 bits to ignore parent prefix.\n+  instruction->local_id_ = CalculateLocalId(proto.id());\n \n   if (proto.has_sharding()) {\n     TF_ASSIGN_OR_RETURN(HloSharding sharding,\n@@ -3923,7 +3922,7 @@ void HloInstruction::PrintWithCanonicalNameMap(\n       // If we are canonicalizing instruction names and this is a top-level\n       // HloInstruction::ToString() call, don't print an instruction name.\n       DCHECK(!options.print_percent());  // no need to call PrintNameInternal\n-      printer->Append(canonical_name_map->LookupOrInsert(unique_id_64_bits()));\n+      printer->Append(canonical_name_map->LookupOrInsert(unique_id()));\n       printer->Append(\" = \");\n     }\n   } else {\n@@ -4049,7 +4048,7 @@ void HloInstruction::PrintOperandsWithCanonicalNameMap(\n           printer->Append(\" \");\n         }\n         printer->Append(\n-            canonical_name_map->LookupOrInsert(operand->unique_id_64_bits()));\n+            canonical_name_map->LookupOrInsert(operand->unique_id()));\n       }\n     } else if (options.print_operand_names()) {\n       if (add_space) {\n@@ -4377,18 +4376,18 @@ std::string HloInstruction::ToShortString() const {\n \n HloInstructionProto HloInstruction::ToProto() const {\n   HloInstructionProto proto;\n-  CHECK(unique_id_ != -1)\n+  CHECK(local_id_ != -1)\n       << \"This instruction does not have a valid id. Please make sure the \"\n          \"instruction is inside a module before dumping it.\";\n-  proto.set_id(unique_id_);\n+  proto.set_id(unique_id());\n   proto.set_name(name_);\n   *proto.mutable_opcode() = std::string(HloOpcodeString(opcode_));\n   *proto.mutable_shape() = shape().ToProto();\n   for (const HloInstruction* operand : operands_) {\n-    proto.add_operand_ids(operand->unique_id_64_bits());\n+    proto.add_operand_ids(operand->unique_id());\n   }\n   for (const HloInstruction* control : control_predecessors()) {\n-    proto.add_control_predecessor_ids(control->unique_id_64_bits());\n+    proto.add_control_predecessor_ids(control->unique_id());\n   }\n \n   *proto.mutable_metadata() = metadata();\n@@ -4488,8 +4487,7 @@ bool HloInstruction::IsFusible() const {\n }\n \n HloInstruction::HloInstruction(HloOpcode opcode, const Shape& shape)\n-    : unique_id_(-1),\n-      index_in_parent_(~0u),\n+    : local_id_(-1),\n       opcode_(opcode),\n       is_default_config_(false),\n       cleaned_up_(false),\n@@ -4773,7 +4771,7 @@ template <typename Visitor>\n inline bool PushDFSChild(Visitor* visitor, DFSStack* dfs_stack,\n                          HloInstruction* child) {\n   CHECK(child != nullptr);\n-  const int64_t id = child->unique_id_64_bits();\n+  const int64_t id = child->unique_id();\n   CHECK_GE(id, 0) << \"instruction may not have a parent computation\";\n   switch (visitor->GetVisitState(id)) {\n     case Visitor::kVisiting:\n@@ -4790,28 +4788,28 @@ inline bool PushDFSChild(Visitor* visitor, DFSStack* dfs_stack,\n }\n \n using InternalCompareFunction =\n-    absl::FunctionRef<bool(std::pair<int, const HloInstruction*>,\n-                           std::pair<int, const HloInstruction*>)>;\n+    absl::FunctionRef<bool(std::pair<int64_t, const HloInstruction*>,\n+                           std::pair<int64_t, const HloInstruction*>)>;\n template <typename Visitor>\n static absl::Status PostOrderDFS(\n     HloInstruction* root, Visitor* visitor,\n     std::optional<InternalCompareFunction> operand_order,\n     bool ignore_control_predecessors, bool cross_computation) {\n   visitor->ReserveVisitStates(root->parent()->instruction_count());\n \n-  // dfs_stack holds pairs of <HloInstruction*->unique_id(), HloInstruction*>.\n+  // dfs_stack holds pairs of <HloInstruction*->unique_id, HloInstruction*>.\n   //\n   // We need to keep track of both the id and the instruction because\n   // instructions can get deleted while they are on the stack, so we\n   // can't always use the (potentially dead) instruction object to grab\n   // its id.\n   DFSStack dfs_stack;\n-  dfs_stack.emplace_back(root->unique_id_64_bits(), root);\n+  dfs_stack.emplace_back(root->unique_id(), root);\n \n   do {\n     DCHECK(!dfs_stack.empty());\n \n-    int current_id = dfs_stack.back().first;\n+    int64_t current_id = dfs_stack.back().first;\n     HloInstruction* current_node = dfs_stack.back().second;\n     CHECK_GE(current_id, 0) << current_id << \": \" << current_node->name()\n                             << \": instruction may not have parent computation\";\n@@ -4917,8 +4915,8 @@ absl::Status HloInstruction::AcceptWithOperandOrder(\n     DfsHloVisitor* visitor, CompareFunction operand_order,\n     bool call_finish_visit) {\n   VLOG(2) << \"HloInstruction::AcceptWithOperandOrder(%\" << name() << \")\";\n-  auto func = [operand_order](std::pair<int, const HloInstruction*> a,\n-                              std::pair<int, const HloInstruction*> b) {\n+  auto func = [operand_order](std::pair<int64_t, const HloInstruction*> a,\n+                              std::pair<int64_t, const HloInstruction*> b) {\n     // Call the client's comparison function on the actual HloInstruction*\n     // objects (ignoring the internal ids we also have in our stack entries)\n     return operand_order(a.second, b.second);\n@@ -5408,7 +5406,21 @@ bool HloPtrComparator::operator()(const HloInstruction* const& lhs,\n       lhs_module->unique_id() != rhs_module->unique_id()) {\n     return lhs_module->unique_id() < rhs_module->unique_id();\n   }\n-  return lhs->unique_id_64_bits() < rhs->unique_id_64_bits();\n+  return lhs->unique_id() < rhs->unique_id();\n+}\n+\n+bool HloPtrComparatorInternal::operator()(\n+    const HloInstruction* const& lhs, const HloInstruction* const& rhs) const {\n+  // Nothing compares less than nullptr.\n+  if (rhs == nullptr) {\n+    return false;\n+  }\n+  if (lhs == nullptr) {\n+    return true;\n+  }\n+  CHECK(lhs->GetModule() == rhs->GetModule())\n+      << \"Instructions are not in the same module or both outside a module.\";\n+  return lhs->local_id() < rhs->local_id();\n }\n \n const PrecisionConfig& HloInstruction::precision_config() const {\n@@ -5462,8 +5474,21 @@ void HloInstruction::UniquifyName(HloModule* module) {\n   UniquifyName(&module->instruction_name_uniquer());\n }\n \n-void HloInstruction::UniquifyId(HloModule* module) {\n-  SetUniqueId(module->NewUniqueInstructionId());\n+int64_t HloInstruction::unique_id() const {\n+  CHECK(parent_ != nullptr)\n+      << \"Instruction \" << name()\n+      << \" must have a parent in order to have a unique ID.\";\n+  // The parent's unique ID is stored in the most significant 32 bits of the\n+  // unique ID.\n+  return CalculateUniqueId(parent_->unique_id(), local_id_);\n+}\n+\n+int32_t HloInstruction::local_id() const { return local_id_; }\n+\n+int64_t HloInstruction::CalculateUniqueId(int32_t computation_unique_id,\n+                                          int32_t instruction_local_id) {\n+  return ((static_cast<int64_t>(computation_unique_id) << 32) |\n+          static_cast<int64_t>(instruction_local_id));\n }\n \n void HloInstruction::SortInstructionUsersAndControlLists("
        },
        {
            "sha": "1aed245bae2222f5503987bef087eb06719098a1",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.h",
            "status": "modified",
            "additions": 56,
            "deletions": 35,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/function_ref.h\"\n #include \"absl/hash/hash.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -204,6 +205,8 @@ using HloInstructionUnwrappingIterator =\n using HloInstructionUnwrappingConstIterator =\n     HloInstructionUnwrappingIteratorBase<HloInstructionConstIterator>;\n \n+static constexpr uintptr_t kInstructionTypeMask = 0b111;\n+\n // HLO instructions are the atomic unit of the high-level compiler's IR.\n //\n // HloInstructions live inside of an HloComputation, which is analogous to a\n@@ -219,7 +222,8 @@ using HloInstructionUnwrappingConstIterator =\n // HLO is pure (mostly).  It has no concept of mutable state.  Instead, data\n // values are produced by one HLO and flow into consumers across dependency\n // edges.\n-class HloInstruction {\n+// Alignment must be explicitly specified due to ARM 32 platforms.\n+class alignas(kInstructionTypeMask + 1) HloInstruction {\n  public:\n   // A fusion node computes the same value a call to its fusion computation\n   // would compute.  However, the choice of fusion kind dictates codegen\n@@ -311,8 +315,8 @@ class HloInstruction {\n   // Creates an instruction from the given proto. Arguments:\n   //\n   //   proto: the proto to convert from.\n-  //   instruction_map: a map from instruction id to HloInstruction*. This map\n-  //     must contain all operands of the newly constructed instruction.\n+  //   instruction_map: a map from local instruction id to HloInstruction*. This\n+  //     map must contain all operands of the newly constructed instruction.\n   //   computation_map: a map from computation id to HloComputation*. This map\n   //     must contain all computations which the newly constructed instruction\n   //     calls.\n@@ -1858,41 +1862,31 @@ class HloInstruction {\n   // instruction based on the instruction's existing name.\n   void UniquifyName(HloModule* module);\n \n-  // Use the `module`s `NewUniqueInstructionId` to set the id of this\n-  // instruction.\n-  void UniquifyId(HloModule* module);\n-\n-  // Clear the unique ID of the instruction so that it can be re-assigned, such\n-  // as for the purpose of compacting the instruction unique IDs.\n-  void ClearUniqueIdInternal() { unique_id_ = -1; }\n+  // Return the unique ID assigned to this node via SetLocalId (or -1 if no id\n+  // has been assigned yet) prefixed by the parent's unique ID. This is a\n+  // concatenation of the parent's unique ID and the local ID. This is\n+  // guaranteed to be unique across all instructions in a module.\n+  int64_t unique_id() const;\n \n-  // Set the unique id for this instruction to \"id\"\n-  // TODO(b/399394039): Remove this function once the bug is fixed.\n-  void SetUniqueId(int id) { SetUniqueId(static_cast<int64_t>(id)); }\n+  // Returns the computation-specific ID assigned to this node. This is\n+  // equivalent to the least significant 32 bits of the unique ID. This is not\n+  // guaranteed to be unique across all instructions in a module, only within\n+  // the computation.\n+  int32_t local_id() const;\n \n-  // Set the unique id for this instruction to \"id\"\n-  void SetUniqueId(int64_t id) {\n-    CHECK_EQ(unique_id_, -1);  // Should not be assigned already\n-    CHECK_GE(id, 0);\n-    unique_id_ = id;\n-  }\n+  // Returns the unique ID that would be assigned to this instruction if it were\n+  // inserted into the given parent without an internal ID change. Can be used\n+  // to recreate the unique ID of an instruction that has been removed from its\n+  // parent.\n+  static int64_t CalculateUniqueId(int32_t computation_unique_id,\n+                                   int32_t instruction_local_id);\n \n-  // Return the unique ID assigned to this node via SetUniqueId (or -1\n-  // if no id has been assigned yet).\n-  int unique_id() const {\n-    CHECK_LT(unique_id_, INT32_MAX)\n-        << \"int32_t unique_id was requested but unique_id was written as a \"\n-           \"64-bit integer: \"\n-        << unique_id_;\n-    return static_cast<int>(unique_id_);\n+  // Returns the local ID of the instruction by extracting it from the more\n+  // general unique ID.\n+  static int32_t CalculateLocalId(int64_t unique_id) {\n+    return static_cast<int32_t>(unique_id & 0xFFFFFFFF);\n   }\n \n-  // Return the unique ID assigned to this node via SetUniqueId (or -1\n-  // if no id has been assigned yet).Returns the entire unique ID as a 64-bit\n-  // integer.\n-  // TODO(b/399394039): Remove this function once the bug is fixed.\n-  int64_t unique_id_64_bits() const { return unique_id_; }\n-\n   bool has_backend_config() const { return !backend_config_.empty(); }\n \n   void clear_backend_config() { backend_config_ = BackendConfigWrapper(); }\n@@ -2560,6 +2554,19 @@ class HloInstruction {\n   // HloInstruction.\n   bool IsMarkedAsDead() const { return marked_as_dead_; }\n \n+  // Set the unique id for this instruction to \"id\". Should only be called by\n+  // the instruction's parent computation to set an internal unique id that fits\n+  // in an int32_t.\n+  void SetLocalId(int32_t id) {\n+    CHECK_EQ(local_id_, -1);  // Should not be assigned already\n+    CHECK_GE(id, 0);\n+    local_id_ = id;\n+  }\n+\n+  // Clear the unique ID of the instruction so that it can be re-assigned, such\n+  // as for the purpose of compacting the instruction unique IDs.\n+  void ClearUniqueIdInternal() { local_id_ = -1; }\n+\n   // Rare is allocated lazily, only when any of its constituent fields are\n   // non-empty.  This reduces the memory footprint of HloInstruction objects.\n   struct Rare {\n@@ -2656,8 +2663,9 @@ class HloInstruction {\n         user_map_;\n   };\n \n-  int64_t unique_id_;  // Unique to this HloInstruction within a HloModule\n-  uint32_t index_in_parent_;  // Index that identifies inst in HloComputation\n+  int32_t local_id_;  // Unique to this HloInstruction within a HloComputation.\n+                      // Index that identifies where the inst is stored in the\n+                      // parent computation.\n \n   // Opcode for this instruction.\n   HloOpcode opcode_;\n@@ -2787,9 +2795,22 @@ struct HloPtrComparator {\n                   const HloInstruction* const& rhs) const;\n };\n \n+// Same as HloPtrComparator but uses internal ids only, intended for use within\n+// a computation only.\n+struct HloPtrComparatorInternal {\n+  bool operator()(const HloInstruction* const& lhs,\n+                  const HloInstruction* const& rhs) const;\n+};\n+\n template <typename ValueT>\n using HloInstructionMap = std::map<HloInstruction*, ValueT, HloPtrComparator>;\n \n+// Same as HloInstructionMap but uses internal ids, intended for use within a\n+// computation only.\n+template <typename ValueT>\n+using HloInstructionMapInternal =\n+    std::map<HloInstruction*, ValueT, HloPtrComparatorInternal>;\n+\n template <typename ValueT>\n using ConstHloInstructionMap =\n     std::map<const HloInstruction*, ValueT, HloPtrComparator>;"
        },
        {
            "sha": "9829a402afdd5673c60b49c39dbc5af655383397",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction_test.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 36,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_test.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -168,26 +168,6 @@ ENTRY main {\n   EXPECT_EQ(clone->operand_count(), 2);\n }\n \n-TEST_F(HloInstructionTest, ComparatorWorksWith64BitUniqueIds) {\n-  std::unique_ptr<HloInstruction> param1 =\n-      HloInstruction::CreateParameter(0, Shape(F32, {4}), \"param1\");\n-  std::unique_ptr<HloInstruction> param2 =\n-      HloInstruction::CreateParameter(0, Shape(F32, {4}), \"param2\");\n-  std::unique_ptr<HloInstruction> param3 =\n-      HloInstruction::CreateParameter(0, Shape(F32, {4}), \"param3\");\n-\n-  param1->SetUniqueId(1 + (static_cast<int64_t>(1) << 32));\n-  param2->SetUniqueId(1 + (static_cast<int64_t>(2) << 32));\n-  param3->SetUniqueId(1 + (static_cast<int64_t>(3) << 32));\n-\n-  std::vector<const HloInstruction*> instructions = {param3.get(), param1.get(),\n-                                                     param2.get()};\n-\n-  absl::c_sort(instructions, HloPtrComparator());\n-  EXPECT_THAT(instructions,\n-              ElementsAre(param1.get(), param2.get(), param3.get()));\n-}\n-\n TEST_F(HloInstructionTest, PrintCompareOpWorksIfDead) {\n   const char* const kModuleStr = R\"(\n     HloModule m\n@@ -217,37 +197,46 @@ TEST_F(HloInstructionTest, PrintCompareOpWorksIfDead) {\n }\n \n TEST_F(HloInstructionTest, CanonicalPrintingSupportsInt64) {\n-  std::unique_ptr<HloInstruction> param1 =\n-      HloInstruction::CreateParameter(0, Shape(F32, {4}), \"param1\");\n-  std::unique_ptr<HloInstruction> param2 =\n-      HloInstruction::CreateParameter(0, Shape(F32, {2}), \"param2\");\n-  std::unique_ptr<HloInstruction> param3 =\n-      HloInstruction::CreateParameter(0, Shape(F32, {6}), \"param3\");\n-\n-  param1->SetUniqueId(1 + (static_cast<int64_t>(1) << 32));\n-  param2->SetUniqueId(2 + (static_cast<int64_t>(2) << 32));\n-  param3->SetUniqueId(3 + (static_cast<int64_t>(3) << 32));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           R\"(\n+    HloModule m\n+    ENTRY main {\n+      p0 = f32[] parameter(0)\n+      p1 = f32[] parameter(1)\n+      ROOT result = pred[] compare(p0, p1), direction=GT, type=TOTALORDER\n+    }\n+  )\"));\n \n   xla::HloPrintOptions hlo_print_options =\n       xla::HloPrintOptions(xla::HloPrintOptions::Canonical());\n   hlo_print_options.set_is_in_nested_computation(true);\n \n   xla::CanonicalNameMap new_map;\n   xla::StringPrinter printer;\n-  param1->PrintWithCanonicalNameMap(&printer, hlo_print_options, &new_map);\n+  // Param 0\n+  module->entry_computation()\n+      ->parameter_instruction(0)\n+      ->PrintWithCanonicalNameMap(&printer, hlo_print_options, &new_map);\n   std::string param1_to_string = std::move(printer).ToString();\n \n   printer = StringPrinter();\n-  param2->PrintWithCanonicalNameMap(&printer, hlo_print_options, &new_map);\n+  // Param 1\n+  module->entry_computation()\n+      ->parameter_instruction(1)\n+      ->PrintWithCanonicalNameMap(&printer, hlo_print_options, &new_map);\n   std::string param2_to_string = std::move(printer).ToString();\n \n   printer = StringPrinter();\n-  param3->PrintWithCanonicalNameMap(&printer, hlo_print_options, &new_map);\n+  // Result Root Instruction\n+  module->entry_computation()->root_instruction()->PrintWithCanonicalNameMap(\n+      &printer, hlo_print_options, &new_map);\n   std::string param3_to_string = std::move(printer).ToString();\n \n-  EXPECT_EQ(param1_to_string, \"tmp_0 = f32[4] parameter(0)\");\n-  EXPECT_EQ(param2_to_string, \"tmp_1 = f32[2] parameter(0)\");\n-  EXPECT_EQ(param3_to_string, \"tmp_2 = f32[6] parameter(0)\");\n+  EXPECT_EQ(param1_to_string, \"tmp_0 = f32[] parameter(0)\");\n+  EXPECT_EQ(param2_to_string, \"tmp_1 = f32[] parameter(1)\");\n+  EXPECT_EQ(param3_to_string,\n+            \"tmp_2 = pred[] compare(f32[] tmp_0, f32[] tmp_1), direction=GT, \"\n+            \"type=TOTALORDER\");\n }\n \n }  // namespace"
        },
        {
            "sha": "f98c78e92f2475405e3421e4e1c84b8d84a8b624",
            "filename": "third_party/xla/xla/hlo/ir/hlo_module.cc",
            "status": "modified",
            "additions": 74,
            "deletions": 42,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -180,28 +180,21 @@ HloComputation* HloModule::AddComputationInternal(\n       instruction->UniquifyName(&instruction_name_uniquer());\n     }\n \n-    // Pick unique IDs for each instruction.\n-    for (auto* instruction : computation->instructions()) {\n-      instruction->SetUniqueId(NewUniqueInstructionId());\n-    }\n     // Set unique id to this computation.\n-    CHECK_NE(computation->root_instruction()->unique_id(), -1)\n-        << \"Root has no valid id: \" << computation->ToString();\n-    computation->SetUniqueId(computation->root_instruction()->unique_id());\n+    computation->ClearUniqueIdInternal();\n+    computation->SetUniqueId(ReadAndIncrementNextUniqueComputationId());\n+    // Computation sets unique ID internally in sequence\n+    // Recompacts the instructions vector to remove nullptr entries.\n+    // computation->RecompactInstructions();\n+    computation->Cleanup();\n   } else {\n     // Don't uniquify the names of the computation or instruction, but we must\n     // run the names through the uniquifiers to prevent future name collisions\n-    // for computations and instructions created later. Also, set the\n-    // next_unique_id_ to the one greater than the max unique id of any\n-    // instruction (or the computation) to avoid ID collisions.\n+    // for computations and instructions created later.\n+    ResyncNextUniqueComputationId(computation->unique_id());\n     computation_name_uniquer().GetUniqueName(computation->name());\n     for (auto* instruction : computation->instructions()) {\n       instruction_name_uniquer().GetUniqueName(instruction->name());\n-      next_unique_id_ =\n-          std::max(next_unique_id_, instruction->unique_id_64_bits() + 1);\n-    }\n-    if (next_unique_id_ < computation->unique_id() + 1) {\n-      next_unique_id_ = computation->unique_id() + 1;\n     }\n   }\n \n@@ -292,13 +285,10 @@ void HloModule::MarkFusionDuplications(\n \n void HloModule::MoveComputationsFrom(HloModule* module,\n                                      bool make_names_unique) {\n-  for (size_t i = 0; i < module->computations_.size(); ++i) {\n+  for (size_t i = 0; i < module->computation_count(); ++i) {\n     if (module->computations_[i] == nullptr) {\n       continue;\n     }\n-    for (auto* instruction : module->computations_[i]->instructions()) {\n-      instruction->ClearUniqueIdInternal();\n-    }\n     module->computations_[i]->ClearUniqueIdInternal();\n     auto computation_raw_ptr = module->computations_[i].get();\n     if (computation_raw_ptr->IsEntryComputation()) {\n@@ -316,15 +306,8 @@ void HloModule::MoveComputationsFrom(HloModule* module,\n         instruction->UniquifyName(&instruction_name_uniquer());\n       }\n     }\n-    // Pick unique IDs for each instruction.\n-    for (auto* instruction : computation_raw_ptr->instructions()) {\n-      instruction->SetUniqueId(NewUniqueInstructionId());\n-    }\n     // Set unique id to this computation_raw_ptr.\n-    CHECK_NE(computation_raw_ptr->root_instruction()->unique_id(), -1)\n-        << \"Root has no valid id: \" << computation_raw_ptr->ToString();\n-    computation_raw_ptr->SetUniqueId(\n-        computation_raw_ptr->root_instruction()->unique_id());\n+    computation_raw_ptr->SetUniqueId(ReadAndIncrementNextUniqueComputationId());\n   }\n   // Since the computations no longer belong to the old module, clear the list.\n   module->computations_.clear();\n@@ -639,16 +622,73 @@ absl::Status HloModule::CheckUniqueNamesAndIdsForComputationsAndInstructions()\n           << \"Instruction name is not unique: \" << instruction->name();\n       instruction_names.insert(instruction->name());\n \n-      TF_RET_CHECK(\n-          !ContainsKey(instruction_ids, instruction->unique_id_64_bits()))\n-          << \"Instruction id is not unique: \"\n-          << instruction->unique_id_64_bits();\n-      instruction_ids.insert(instruction->unique_id_64_bits());\n+      TF_RET_CHECK(!ContainsKey(instruction_ids, instruction->unique_id()))\n+          << \"Instruction id is not unique: \" << instruction->unique_id()\n+          << \" name: \" << instruction->name()\n+          << \" Parent: \" << computation->name()\n+          << \" Parent id: \" << computation->unique_id();\n+      instruction_ids.insert(instruction->unique_id());\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n+/* static */\n+absl::Status HloModule::UpdateIdsInSchedules(\n+    HloModuleProto& proto,\n+    absl::flat_hash_map<int64_t, int64_t>& old_instr_id_to_new_id) {\n+  for (HloComputationProto& computation_proto : *proto.mutable_computations()) {\n+    if (proto.schedule().sequences().contains(computation_proto.id())) {\n+      HloScheduleProto::InstructionSequence& sequence =\n+          (*proto.mutable_schedule()\n+                ->mutable_sequences())[computation_proto.id()];\n+      for (int64_t& instr_id : *sequence.mutable_instruction_ids()) {\n+        TF_RET_CHECK(old_instr_id_to_new_id.contains(instr_id))\n+            << \"Instruction id \" << instr_id\n+            << \" not found in map when updating schedule ids.\";\n+        instr_id = old_instr_id_to_new_id[instr_id];\n+      }\n     }\n   }\n   return absl::OkStatus();\n }\n \n+/* static */\n+absl::StatusOr<HloModuleProto> HloModule::RemapInstructionIds(\n+    const HloModuleProto& proto) {\n+  absl::flat_hash_map<int64_t, int64_t> old_instr_id_to_new_id;\n+  HloModuleProto proto_copy = proto;\n+  for (HloComputationProto& computation_proto :\n+       *proto_copy.mutable_computations()) {\n+    int64_t next_instr_id = 0;\n+    for (HloInstructionProto& instr_proto :\n+         *computation_proto.mutable_instructions()) {\n+      int64_t old_instr_id = instr_proto.id();\n+      instr_proto.set_id(next_instr_id++);\n+      old_instr_id_to_new_id[old_instr_id] = instr_proto.id();\n+    }\n+    // Fix operands and control_predecessors.\n+    for (HloInstructionProto& instr_proto :\n+         *computation_proto.mutable_instructions()) {\n+      for (int64_t& operand_id : *instr_proto.mutable_operand_ids()) {\n+        operand_id = old_instr_id_to_new_id[operand_id];\n+      }\n+      for (int64_t& control_predecessor_id :\n+           *instr_proto.mutable_control_predecessor_ids()) {\n+        control_predecessor_id = old_instr_id_to_new_id[control_predecessor_id];\n+      }\n+    }\n+    // Fix root_id.\n+    TF_RET_CHECK(old_instr_id_to_new_id.contains(computation_proto.root_id()))\n+        << \"Root id \" << computation_proto.root_id()\n+        << \" not found in computation proto.\";\n+    computation_proto.set_root_id(\n+        old_instr_id_to_new_id[computation_proto.root_id()]);\n+  }\n+  TF_RETURN_IF_ERROR(UpdateIdsInSchedules(proto_copy, old_instr_id_to_new_id));\n+  return proto_copy;\n+}\n+\n /* static */\n absl::StatusOr<std::unique_ptr<HloModule>> HloModule::CreateFromProto(\n     const HloModuleProto& proto, const HloModuleConfig& module_config,\n@@ -1256,15 +1296,7 @@ void CopyUniqueIds(const HloModule& source, HloModule* clone,\n     if (new_computation == nullptr) {\n       continue;\n     }\n-    new_computation->ClearUniqueIdInternal();\n-    new_computation->SetUniqueId(computation->unique_id());\n-    for (HloInstruction* instruction : computation->instructions()) {\n-      HloInstruction* new_instruction = context.FindInstruction(instruction);\n-      if (new_instruction != nullptr) {\n-        new_instruction->ClearUniqueIdInternal();\n-        new_instruction->SetUniqueId(instruction->unique_id());\n-      }\n-    }\n+    new_computation->CopyLocalIdsFromComputation(*computation, context);\n   }\n }\n \n@@ -1280,7 +1312,7 @@ void HloModule::Clone(const std::string& suffix, HloCloneContext* context,\n \n   // Preserve original instruction and computation ids.\n   CopyUniqueIds(*this, module, *context);\n-  module->next_unique_id_ = next_unique_id_;\n+  module->SetNextUniqueComputationId(ReadNextUniqueComputationId());\n \n   module->input_output_alias_config() = input_output_alias_config();\n   module->buffer_donor_config() = buffer_donor_config();"
        },
        {
            "sha": "b940e2b6bcfa69bd9caca5792319c3d8f78eda52",
            "filename": "third_party/xla/xla/hlo/ir/hlo_module.h",
            "status": "modified",
            "additions": 55,
            "deletions": 12,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_HLO_IR_HLO_MODULE_H_\n #define XLA_HLO_IR_HLO_MODULE_H_\n \n+#include <algorithm>\n #include <atomic>\n #include <cstddef>\n #include <cstdint>\n@@ -27,6 +28,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/status.h\"\n@@ -329,11 +331,16 @@ class HloModule {\n   void CleanupComputations();\n \n   // Runs HloModule::CleanupComputations() and HloComputation::Cleanup() on all\n-  // computations.\n+  // computations. Cleanup can cause instruction's unique ids to change so it\n+  // will also update the schedule's ids.\n   void Cleanup() {\n     CleanupComputations();\n     for (HloComputation* comp : computations()) {\n       comp->Cleanup();\n+      if (schedule_.has_value() && schedule_->is_computation_scheduled(comp)) {\n+        // Update the schedule's instruction unique IDs.\n+        schedule_->GetOrCreateSequence(comp).update_id_sequence();\n+      }\n     }\n   }\n \n@@ -457,6 +464,18 @@ class HloModule {\n   // Returns a stable fingerprint of the module using the given print options.\n   uint64_t ToFingerprint(const HloPrintOptions& options) const;\n \n+  // Remaps the instruction ids in the proto to be consecutive. This is useful\n+  // for loading a proto that had its ids manually created or created\n+  // incorrectly.\n+  static absl::StatusOr<HloModuleProto> RemapInstructionIds(\n+      const HloModuleProto& proto);\n+\n+  // Updates the instruction ids in the module's schedules to match the new\n+  // instruction ids as defined by the old_instr_id_to_new_id map.\n+  static absl::Status UpdateIdsInSchedules(\n+      HloModuleProto& proto,\n+      absl::flat_hash_map<int64_t, int64_t>& old_instr_id_to_new_id);\n+\n   // Convert an HloModule to or from a proto.\n   HloModuleProto ToProto() const;\n   static absl::StatusOr<std::unique_ptr<HloModule>> CreateFromProto(\n@@ -509,12 +528,10 @@ class HloModule {\n         << \"Can't get computation name uniquer after HloModule was finalized\";\n     return *computation_name_uniquer_;\n   }\n-\n-  // Assign a new unique dense id for an instruction\n-  int64_t NewUniqueInstructionId() {\n-    int64_t result = next_unique_id_;\n-    next_unique_id_++;\n-    return result;\n+  // Returns the next unique computation id that will be handed out by this\n+  // module.\n+  int64_t next_unique_computation_id() const {\n+    return ReadNextUniqueComputationId();\n   }\n \n   // input_output_alias_config indicates the list of aliased buffers that are\n@@ -565,10 +582,6 @@ class HloModule {\n \n   HloComputation* AddComputationAndUnifyNamesAndIds(\n       std::unique_ptr<HloComputation> computation, bool is_entry) {\n-    computation->ClearUniqueIdInternal();\n-    for (auto* instruction : computation->instructions()) {\n-      instruction->ClearUniqueIdInternal();\n-    }\n     return AddComputationInternal(std::move(computation), is_entry,\n                                   /*uniquify_identifiers=*/true,\n                                   /*preserve_entry_layouts=*/true);\n@@ -782,10 +795,40 @@ class HloModule {\n   // unique per module. Will be reset to nullopt when Finalize() is called.\n   std::optional<NameUniquer> computation_name_uniquer_{/*separator=*/\".\"};\n   std::optional<NameUniquer> instruction_name_uniquer_{/*separator=*/\".\"};\n-  int64_t next_unique_id_ = 0;\n \n   // Used to keep track of the next unique module id that should be assigned.\n   static std::atomic<int> next_unique_module_id_;\n+\n+  // Used to keep track of the next unique computation id that should be\n+  // assigned to computations in this module.\n+  mutable absl::Mutex next_unique_computation_id_mutex_;\n+  int32_t next_unique_computation_id_\n+      ABSL_GUARDED_BY(next_unique_computation_id_mutex_) = 0;\n+\n+  void SetNextUniqueComputationId(int32_t next_unique_computation_id)\n+      ABSL_LOCKS_EXCLUDED(next_unique_computation_id_mutex_) {\n+    absl::MutexLock mx_lock(&next_unique_computation_id_mutex_);\n+    next_unique_computation_id_ = next_unique_computation_id;\n+  }\n+\n+  void ResyncNextUniqueComputationId(int32_t last_assigned_unique_id)\n+      ABSL_LOCKS_EXCLUDED(next_unique_computation_id_mutex_) {\n+    absl::MutexLock mx_lock(&next_unique_computation_id_mutex_);\n+    next_unique_computation_id_ =\n+        std::max(next_unique_computation_id_, last_assigned_unique_id + 1);\n+  }\n+\n+  int32_t ReadAndIncrementNextUniqueComputationId()\n+      ABSL_LOCKS_EXCLUDED(next_unique_computation_id_mutex_) {\n+    absl::MutexLock mx_lock(&next_unique_computation_id_mutex_);\n+    return next_unique_computation_id_++;\n+  }\n+\n+  int32_t ReadNextUniqueComputationId() const\n+      ABSL_LOCKS_EXCLUDED(next_unique_computation_id_mutex_) {\n+    absl::MutexLock mx_lock(&next_unique_computation_id_mutex_);\n+    return next_unique_computation_id_;\n+  }\n   // A unique id to label modules with.\n   const int unique_id_;\n "
        },
        {
            "sha": "9b797c4c0b22613e411ca1f828a1786a2f746f3d",
            "filename": "third_party/xla/xla/hlo/ir/hlo_module_test.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 13,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_module_test.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -282,6 +282,36 @@ TEST(HloModuleTest, CloneWithNewConfig) {\n             m1.config().device_memory_size());\n }\n \n+TEST(HloModuleTest, UniqueIdProvidesComputationPrefix) {\n+  HloModule m1(\"temp_module\", HloModuleConfig());\n+  HloSchedule schedule(&m1);\n+  CreateComputation(m1, \"TestComputation1\", true, schedule);\n+  CreateComputation(m1, \"TestComputation2\", false, schedule);\n+  CreateComputation(m1, \"TestComputation3\", false, schedule);\n+  TF_EXPECT_OK(m1.set_schedule(schedule));\n+\n+  EXPECT_EQ(m1.GetComputationWithName(\"TestComputation1\")\n+                ->GetInstructionWithName(\"p0\")\n+                ->unique_id(),\n+            0);\n+  EXPECT_EQ(m1.GetComputationWithName(\"TestComputation2\")\n+                ->GetInstructionWithName(\"p0.1\")\n+                ->unique_id(),\n+            (static_cast<int64_t>(1) << 32) + 0);\n+  EXPECT_EQ(m1.GetComputationWithName(\"TestComputation1\")\n+                ->GetInstructionWithName(\"call\")\n+                ->unique_id(),\n+            1);\n+  EXPECT_EQ(m1.GetComputationWithName(\"TestComputation3\")\n+                ->GetInstructionWithName(\"p0.2\")\n+                ->unique_id(),\n+            (static_cast<int64_t>(2) << 32) + 0);\n+  EXPECT_EQ(m1.GetComputationWithName(\"TestComputation1\")\n+                ->GetInstructionWithName(\"call.1\")\n+                ->unique_id(),\n+            2);\n+}\n+\n TEST(HloModuleTest, ClonePreservesUniqueId) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnUnverifiedModule(R\"(\n@@ -709,20 +739,12 @@ TEST(HloModuleTest, TestUniqueIdIs64Bits) {\n   HloInstruction* tparam = f->GetInstructionWithName(\"tparam\");\n   HloComputation* g = module->GetComputationWithName(\"g\");\n   HloInstruction* fparam = g->GetInstructionWithName(\"fparam\");\n-  int64_t new_tparam_unique_id = 1 + (static_cast<int64_t>(1) << 32);\n-  int64_t new_fparam_unique_id = 1 + (static_cast<int64_t>(2) << 32);\n-\n-  tparam->ClearUniqueIdInternal();\n-  tparam->SetUniqueId(new_tparam_unique_id);\n-  fparam->ClearUniqueIdInternal();\n-  fparam->SetUniqueId(new_fparam_unique_id);\n-  // Upper 32 bits should be preserved\n-  EXPECT_EQ(tparam->unique_id_64_bits(), new_tparam_unique_id);\n-  EXPECT_EQ(fparam->unique_id_64_bits(), new_fparam_unique_id);\n-  TF_EXPECT_OK(module->CheckUniqueNamesAndIdsForComputationsAndInstructions());\n+\n+  // Upper 32 bits should make them different\n+  EXPECT_NE(tparam->unique_id(), fparam->unique_id());\n   // Lower 32 bits should be preserved and therefore the same\n-  EXPECT_EQ(tparam->unique_id_64_bits() & 0xFFFFFFFF,\n-            fparam->unique_id_64_bits() & 0xFFFFFFFF);\n+  EXPECT_EQ(tparam->unique_id() & 0xFFFFFFFF, fparam->unique_id() & 0xFFFFFFFF);\n+  TF_EXPECT_OK(module->CheckUniqueNamesAndIdsForComputationsAndInstructions());\n }\n \n }  // namespace"
        },
        {
            "sha": "ed4d2662c506bd66b748a6dc73f3794fd5f951b9",
            "filename": "third_party/xla/xla/hlo/ir/hlo_schedule.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -66,16 +66,19 @@ namespace xla {\n \n     absl::flat_hash_map<int64_t, HloInstruction*> id_to_instruction;\n     for (HloInstruction* instruction : computation->instructions()) {\n-      id_to_instruction[instruction->unique_id_64_bits()] = instruction;\n+      id_to_instruction[instruction->unique_id()] = instruction;\n     }\n \n     HloInstructionSequence& sequence =\n         schedule.GetOrCreateSequence(computation);\n     for (const int64_t instruction_id : id_sequence.second.instruction_ids()) {\n-      auto instr_it = id_to_instruction.find(instruction_id);\n+      int64_t complete_unique_id = HloInstruction::CalculateUniqueId(\n+          computation->unique_id(), instruction_id);\n+      auto instr_it = id_to_instruction.find(complete_unique_id);\n       TF_RET_CHECK(instr_it != id_to_instruction.end())\n           << \"No instruction exists in HLO computation \" << computation->name()\n-          << \" with id \" << instruction_id;\n+          << \" with unique id \" << instruction_id << \" (complete unique id \"\n+          << complete_unique_id << \")\";\n       sequence.push_back(instr_it->second);\n     }\n   }\n@@ -136,13 +139,12 @@ absl::Status HloSchedule::UpdateComputationSchedule(\n   // computation.\n   absl::flat_hash_map<int64_t, HloInstruction*> id_to_instruction;\n   for (HloInstruction* instruction : computation->instructions()) {\n-    InsertOrDie(&id_to_instruction, instruction->unique_id_64_bits(),\n-                instruction);\n+    InsertOrDie(&id_to_instruction, instruction->unique_id(), instruction);\n   }\n \n   // Set of all HloInstructions in the schedule.\n   absl::flat_hash_set<int64_t> ids_in_schedule;\n-  for (int id : sequences_.at(computation->unique_id()).ids()) {\n+  for (int64_t id : sequences_.at(computation->unique_id()).ids()) {\n     InsertOrDie(&ids_in_schedule, id);\n   }\n \n@@ -164,7 +166,7 @@ absl::Status HloSchedule::UpdateComputationSchedule(\n   std::queue<HloInstruction*> worklist;\n \n   for (HloInstruction* instruction : computation->instructions()) {\n-    if (!ids_in_schedule.contains(instruction->unique_id_64_bits())) {\n+    if (!ids_in_schedule.contains(instruction->unique_id())) {\n       // `instruction` is a newly added instruction which is not in the\n       // schedule.\n       if (instruction->operands().empty() &&\n@@ -410,7 +412,7 @@ std::string HloSchedule::ToString() const {\n       // stored in this object.\n       pieces.push_back(absl::StrFormat(\n           \"computation with id %d (no longer in HLO module):\", id));\n-      for (int id : sequence.ids()) {\n+      for (int64_t id : sequence.ids()) {\n         pieces.push_back(absl::StrCat(\"  \", id));\n       }\n     } else {"
        },
        {
            "sha": "63078c4d0a3b24e2aa011e5c0b13da15f71a6717",
            "filename": "third_party/xla/xla/hlo/ir/hlo_schedule.h",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -63,7 +63,7 @@ class HloInstructionSequence {\n   // Adds the instruction to the end of the sequence.\n   void push_back(HloInstruction* instruction) {\n     instruction_sequence_.push_back(instruction);\n-    id_sequence_.push_back(instruction->unique_id_64_bits());\n+    id_sequence_.push_back(instruction->unique_id());\n   }\n \n   void reserve(int64_t size) {\n@@ -75,9 +75,10 @@ class HloInstructionSequence {\n   void remove_instruction(HloInstruction* instruction) {\n     auto instruction_it = std::find(instruction_sequence_.begin(),\n                                     instruction_sequence_.end(), instruction);\n-    if (instruction_it != instruction_sequence_.end()) {\n+    if (instruction_it != instruction_sequence_.end() &&\n+        instruction->parent() != nullptr) {\n       auto id_it = std::find(id_sequence_.begin(), id_sequence_.end(),\n-                             instruction->unique_id_64_bits());\n+                             instruction->unique_id());\n       instruction_sequence_.erase(instruction_it);\n       id_sequence_.erase(id_it);\n     }\n@@ -90,22 +91,20 @@ class HloInstructionSequence {\n         std::find(instruction_sequence_.begin(), instruction_sequence_.end(),\n                   old_instruction);\n     auto id_it = std::find(id_sequence_.begin(), id_sequence_.end(),\n-                           old_instruction->unique_id_64_bits());\n+                           old_instruction->unique_id());\n     CHECK(instruction_it != instruction_sequence_.end())\n-        << \"Do not find instruction id \"\n-        << old_instruction->unique_id_64_bits();\n+        << \"Do not find instruction id \" << old_instruction->unique_id();\n     CHECK(id_it != id_sequence_.end());\n     *instruction_it = new_instruction;\n-    *id_it = new_instruction->unique_id_64_bits();\n+    *id_it = new_instruction->unique_id();\n   }\n \n   // Adds the instruction to the sequence at a specified index,\n   void insert_instruction(HloInstruction* instruction, int64_t index) {\n     CHECK(0 <= index && index < size()) << \"Index out of bounds\";\n     instruction_sequence_.insert(instruction_sequence_.begin() + index,\n                                  instruction);\n-    id_sequence_.insert(id_sequence_.begin() + index,\n-                        instruction->unique_id_64_bits());\n+    id_sequence_.insert(id_sequence_.begin() + index, instruction->unique_id());\n   }\n \n   bool contains(const HloInstruction* inst) const {\n@@ -129,6 +128,16 @@ class HloInstructionSequence {\n   // Returns the unique IDs of the instructions in the sequence (in order).\n   const std::vector<int64_t>& ids() const { return id_sequence_; }\n \n+  // Updates the sequence of unique IDs to match the sequence of instructions.\n+  // This is required when the HLO Module calls Cleanup(), which invalidates\n+  // the old unique IDs.\n+  void update_id_sequence() {\n+    id_sequence_.clear();\n+    for (HloInstruction* instruction : instruction_sequence_) {\n+      id_sequence_.push_back(instruction->unique_id());\n+    }\n+  }\n+\n  private:\n   // The sequence as HloInstructions.\n   std::vector<HloInstruction*> instruction_sequence_;"
        },
        {
            "sha": "d47a0211fae235689b5b0617b3b76b9ced743533",
            "filename": "third_party/xla/xla/hlo/tools/tests/hlo_opt_emit_proto.hlo",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Ftests%2Fhlo_opt_emit_proto.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Ftests%2Fhlo_opt_emit_proto.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Ftests%2Fhlo_opt_emit_proto.hlo?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -81,7 +81,6 @@\n // CHECK-NEXT:       parameter_names: \"a\"\n // CHECK-NEXT:       parameter_names: \"b\"\n // CHECK-NEXT:     }\n-// CHECK-NEXT:     id: 2\n // CHECK-NEXT:     root_id: 2\n // CHECK-NEXT:   }\n // CHECK-NEXT:   host_program_shape {\n@@ -106,7 +105,6 @@\n // CHECK-NEXT:     parameter_names: \"p0\"\n // CHECK-NEXT:     parameter_names: \"p1\"\n // CHECK-NEXT:   }\n-// CHECK-NEXT:   entry_computation_id: 2\n // CHECK-NEXT:   input_output_alias {\n // CHECK-NEXT:   }\n // CHECK-NEXT:   buffer_donor {"
        },
        {
            "sha": "791ae1fb12b11029ee23a67af1364e9e447e8e70",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/add.mlir",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fadd.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fadd.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fadd.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -3,19 +3,19 @@\n // RUN: xla-translate -mlir-hlo-to-hlo-text -emit-return-tuple %s | FileCheck %s --check-prefix=TUPLE-RET\n // RUN: xla-translate -mlir-hlo-to-hlo-text -emit-use-tuple-args -emit-return-tuple %s | FileCheck %s --check-prefix=TUPLES\n \n-// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.2: f32[4]) -> f32[4]\n+// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.1: f32[4]) -> f32[4]\n // TUPLE-ARG-LABEL: ENTRY %main.{{.*}} (arg_tuple.1: (f32[4], f32[4])) -> f32[4]\n-// TUPLE-RET-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.2: f32[4]) -> (f32[4])\n+// TUPLE-RET-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.1: f32[4]) -> (f32[4])\n // TUPLES-LABEL: ENTRY %main.{{.*}} (arg_tuple.1: (f32[4], f32[4])) -> (f32[4])\n \n func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n   // CHECK-NEXT: %Arg_0.1 = f32[4] parameter(0)\n-  // CHECK-NEXT: %Arg_1.2 = f32[4] parameter(1)\n+  // CHECK-NEXT: %Arg_1.1 = f32[4] parameter(1)\n \n-  // CHECK-NEXT: %add.3 = f32[4] add(%Arg_0.1, %Arg_1.2)\n+  // CHECK-NEXT: %add.2 = f32[4] add(%Arg_0.1, %Arg_1.1)\n   %0 = \"mhlo.add\"(%arg0, %arg1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n \n-  // CHECK-NEXT: ROOT %add.4 = f32[4] add(%add.3, %Arg_1.2)\n+  // CHECK-NEXT: ROOT %add.3 = f32[4] add(%add.2, %Arg_1.1)\n   %1 = \"mhlo.add\"(%0, %arg1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n   func.return %1 : tensor<4xf32>\n }"
        },
        {
            "sha": "4c168a2d9b8fd41f0fc0fb6287929f883ca4ec47",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/composite.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fcomposite.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fcomposite.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fcomposite.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -41,7 +41,7 @@ module @composite {\n   //CHECK: }\n   //CHECK: ENTRY %main.{{[0-9]+}} () -> () {\n   //CHECK:   %[[CONSTANT:constant.[0-9]+]] = f32[] constant(42)\n-  //CHECK:   %call.5 = () call(%[[CONSTANT]]), to_apply=%[[RETURN]], is_composite=true, frontend_attributes={composite.attributes={n = 1 : i32, tensor = dense<1> : tensor<i32>},composite.name=\"foo.bar\",composite.version=\"1\"}\n+  //CHECK:   %call.{{[0-9]+}} = () call(%[[CONSTANT]]), to_apply=%[[RETURN]], is_composite=true, frontend_attributes={composite.attributes={n = 1 : i32, tensor = dense<1> : tensor<i32>},composite.name=\"foo.bar\",composite.version=\"1\"}\n   //CHECK:   ROOT %tuple.{{[0-9]+}} = () tuple()\n   //CHECK: }\n   func.func @main() -> () {"
        },
        {
            "sha": "6983894355eb1210ca2be8e568e8076497dc10c2",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/dynamic.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fdynamic.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fdynamic.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fdynamic.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -17,5 +17,5 @@ func.func @main(%arg0: tensor<?x1xi64, #mhlo.type_extensions<bounds = [4, ?]>>)\n   // CHECK-NEXT: %[[SHAPE0:.*]] = s32[] reshape(%[[SHAPE0x1]])\n   // CHECK-NEXT: %[[SHAPE1x1:.*]] = s32[1] slice(%[[SHAPE]]), slice={[1:2]}\n   // CHECK-NEXT: %[[SHAPE1:.*]] = s32[] reshape(%[[SHAPE1x1]])\n-  // CHECK-NEXT: ROOT %dynamic-reshape.10 = s64[1,<=4] dynamic-reshape(%[[ARG0]], %[[SHAPE0]], %[[SHAPE1]])\n+  // CHECK-NEXT: ROOT %dynamic-reshape.1 = s64[1,<=4] dynamic-reshape(%[[ARG0]], %[[SHAPE0]], %[[SHAPE1]])\n }"
        },
        {
            "sha": "633e7d51b146de73f0309d1df407b3b173a4adef",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/export.mlir",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -388,7 +388,7 @@ func.func @main(%arg0: tensor<2xi32>) -> tensor<2xf32> {\n // CHECK:  HloModule\n func.func @main(%arg0: tensor<4xi32>) -> tensor<1x2x3x4xi32> {\n   // CHECK:  [[ARG:%.*]] = s32[4] parameter(0)\n-  // CHECK-NEXT:  ROOT %broadcast.2 = s32[1,2,3,4] broadcast([[ARG]]), dimensions={3}\n+  // CHECK-NEXT:  ROOT %broadcast.1 = s32[1,2,3,4] broadcast([[ARG]]), dimensions={3}\n   %0 = \"mhlo.broadcast\"(%arg0) <{broadcast_sizes = dense<[1,2,3]> : tensor<3xi64>}> : (tensor<4xi32>) -> tensor<1x2x3x4xi32>\n   func.return %0 : tensor<1x2x3x4xi32>\n }\n@@ -405,7 +405,7 @@ func.func @main(%arg0: tensor<1xf32>) -> tensor<1x10xf32> {\n \n // CHECK:  ENTRY\n // CHECK:  [[ARG:%.*]] = f32[1] parameter(0)\n-// CHECK:  ROOT %broadcast.2 = f32[1,10] broadcast([[ARG]]), dimensions={0}\n+// CHECK:  ROOT %broadcast.1 = f32[1,10] broadcast([[ARG]]), dimensions={0}\n \n // -----\n \n@@ -2132,7 +2132,7 @@ func.func @main(%arg0: tensor<2x17x31x7xi32>) -> tensor<2x5x8x7xi32> {\n // CHECK:  ENTRY\n // CHECK-DAG:  %[[ARG0:.*]] = s32[2,17,31,7] parameter(0)\n // CHECK-DAG:  %[[INIT:.*]] = s32[] constant(-2147483648)\n-// CHECK:  ROOT %[[RESULT:.*]] = s32[2,5,8,7] reduce-window(%[[ARG0]], %constant.2),\n+// CHECK:  ROOT %[[RESULT:.*]] = s32[2,5,8,7] reduce-window(%[[ARG0]], %constant.1),\n // CHECK-SAME:  window={size=1x2x2x1 stride=1x4x4x1 pad=0_0x2_0x0_2x0_0 rhs_dilate=1x2x2x1},\n // CHECK-SAME:  to_apply=%[[MAX_COMPUTATION]]\n \n@@ -2468,7 +2468,7 @@ func.func @main(%arg: tensor<3x4xi32>, %start1: tensor<i64>, %start2: tensor<i64\n func.func @main(%arg0: tensor<1x2x3x4xi32>) -> tensor<2x1x4x3xi32> {\n   // CHECK:  [[ARG:%.*]] = s32[1,2,3,4] parameter(0)\n \n-  // CHECK-NEXT:  ROOT %transpose.2 = s32[2,1,4,3] transpose([[ARG]]), dimensions={1,0,3,2}\n+  // CHECK-NEXT:  ROOT %transpose.1 = s32[2,1,4,3] transpose([[ARG]]), dimensions={1,0,3,2}\n   %0 = \"mhlo.transpose\"(%arg0) <{permutation = dense<[1, 0, 3, 2]> : tensor<4xi64>}> : (tensor<1x2x3x4xi32>) -> tensor<2x1x4x3xi32>\n   func.return %0 : tensor<2x1x4x3xi32>\n }\n@@ -2546,7 +2546,7 @@ func.func @main(%input0: tensor<16x16xf32>, %input1: tensor<16x16xi32>) {\n // CHECK: %[[SORT_CMP:.*]] ([[ARG0:.*]]: f32[], [[ARG1:.*]]: f32[], {{.*}}: s32[], {{.*}}: s32[]) -> pred[] {\n // CHECK:   ROOT %compare.{{[0-9+]}} = pred[] compare(%[[ARG0]], %[[ARG1]]), direction=GT\n \n-// CHECK: [[SORT:%.+]] = (f32[16,16], s32[16,16]) sort(%Arg_0.1, %Arg_1.2), dimensions={1}, is_stable=true, to_apply=%[[SORT_CMP]]\n+// CHECK: [[SORT:%.+]] = (f32[16,16], s32[16,16]) sort(%Arg_0.3, %Arg_1.3), dimensions={1}, is_stable=true, to_apply=%[[SORT_CMP]]\n // CHECK: [[GET0:%.+]] = f32[16,16] get-tuple-element([[SORT]]), index=0\n // CHECK: [[GET1:%.+]] = s32[16,16] get-tuple-element([[SORT]]), index=1\n \n@@ -2565,7 +2565,7 @@ func.func @main(%input0: tensor<16x16xf32>) {\n // CHECK: %[[SORT_CMP:.*]] ([[ARG0:.*]]: f32[], [[ARG1:.*]]: f32[]) -> pred[] {\n // CHECK:   ROOT %[[CMP:.*]] = pred[] compare(%[[ARG0]], %[[ARG1]]), direction=GT\n \n-// CHECK: %[[RESULT:.*]] = f32[16,16] sort(%Arg_0.1), dimensions={1}, is_stable=true, to_apply=%[[SORT_CMP]]\n+// CHECK: %[[RESULT:.*]] = f32[16,16] sort(%Arg_0.3), dimensions={1}, is_stable=true, to_apply=%[[SORT_CMP]]\n \n // -----\n "
        },
        {
            "sha": "97919934962c0b174e25d71d9ec9c4da75103afd",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/export_entry_computation_layout.mlir",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport_entry_computation_layout.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport_entry_computation_layout.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport_entry_computation_layout.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -42,12 +42,12 @@ module @entry attributes {\n // CHECK-SAME:   s32[]\n // CHECK-SAME: )->f32[2,3,4]{2,0,1:T(2,128)}}\n \n-// CHECK: ENTRY %main.6 (Arg_0.1: f32[2,3,4], Arg_1.2: f32[2,3,4], Arg_2.3: (f32[1,2], f32[1,2]), Arg_3.4: s32[]) -> f32[2,3,4] {\n-// CHECK:   %Arg_2.3 = (f32[1,2]{1,0}, f32[1,2]{1,0}) parameter(2)\n-// CHECK:   %Arg_3.4 = s32[] parameter(3)\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[2,3,4], Arg_1.1: f32[2,3,4], Arg_2.1: (f32[1,2], f32[1,2]), Arg_3.1: s32[]) -> f32[2,3,4] {\n+// CHECK:   %Arg_2.1 = (f32[1,2]{1,0}, f32[1,2]{1,0}) parameter(2)\n+// CHECK:   %Arg_3.1 = s32[] parameter(3)\n // CHECK:   %Arg_0.1 = f32[2,3,4]{2,1,0} parameter(0)\n-// CHECK:   %Arg_1.2 = f32[2,3,4]{2,1,0} parameter(1)\n-// CHECK:   ROOT %add.5 = f32[2,3,4]{2,1,0} add(%Arg_0.1, %Arg_1.2)\n+// CHECK:   %Arg_1.1 = f32[2,3,4]{2,1,0} parameter(1)\n+// CHECK:   ROOT %add.1 = f32[2,3,4]{2,1,0} add(%Arg_0.1, %Arg_1.1)\n // CHECK: }\n \n // -----\n@@ -71,7 +71,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[2,3,4]{1,0,2}\n // CHECK-SAME: )->f32[2,3,4]{2,1,0}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[2,3,4]) -> f32[2,3,4] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[2,3,4]) -> f32[2,3,4] {\n // CHECK:   ROOT %Arg_0.1 = f32[2,3,4]{2,1,0} parameter(0)\n // CHECK: }\n \n@@ -89,7 +89,7 @@ module @entry attributes {} {\n // CHECK-SAME:   f32[2,3,4]{2,1,0}\n // CHECK-SAME: )->f32[2,3,4]{2,1,0}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[2,3,4]) -> f32[2,3,4] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[2,3,4]) -> f32[2,3,4] {\n // CHECK:   ROOT %Arg_0.1 = f32[2,3,4]{2,1,0} parameter(0)\n // CHECK: }\n \n@@ -110,7 +110,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[2,3,4]\n // CHECK-SAME: )->f32[2,3,4]{2,1,0}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[2,3,4]) -> f32[2,3,4] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[2,3,4]) -> f32[2,3,4] {\n // CHECK:   ROOT %Arg_0.1 = f32[2,3,4]{2,1,0} parameter(0)\n // CHECK: }\n \n@@ -143,7 +143,7 @@ module @entry attributes {\n // CHECK-SAME:   )\n // CHECK-SAME: )->((f32[1,2]{1,0}, f32[1,2]{1,0}))}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: ((f32[1,2], f32[1,2]))) -> ((f32[1,2], f32[1,2])) {\n+// CHECK: ENTRY %main.1 (Arg_0.1: ((f32[1,2], f32[1,2]))) -> ((f32[1,2], f32[1,2])) {\n // CHECK:   ROOT %Arg_0.1 = ((f32[1,2]{1,0}, f32[1,2]{1,0})) parameter(0)\n // CHECK: }\n \n@@ -169,9 +169,9 @@ module @entry attributes {\n // CHECK-SAME:   f32[2,3,4]{0,1,2}\n // CHECK-SAME: )->f32[2,3,4]{2,1,0}}\n \n-// CHECK: ENTRY %main.3 (Arg_0.1: f32[2,3,4], Arg_1.2: f32[2,3,4]) -> f32[2,3,4] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[2,3,4], Arg_1.1: f32[2,3,4]) -> f32[2,3,4] {\n // CHECK:   ROOT %Arg_0.1 = f32[2,3,4]{2,1,0} parameter(0)\n-// CHECK:   %Arg_1.2 = f32[2,3,4]{2,1,0} parameter(1)\n+// CHECK:   %Arg_1.1 = f32[2,3,4]{2,1,0} parameter(1)\n // CHECK: }\n \n // -----\n@@ -195,7 +195,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[1,2]{1,0:T(2,128)}\n // CHECK-SAME: )->f32[1,2]{1,0}}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n // CHECK:   ROOT %Arg_0.1 = f32[1,2]{1,0} parameter(0)\n // CHECK: }\n \n@@ -216,7 +216,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[1,2]{1,0}\n // CHECK-SAME: )->f32[1,2]{1,0}}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n // CHECK:   ROOT %Arg_0.1 = f32[1,2]{1,0} parameter(0)\n // CHECK: }\n \n@@ -237,7 +237,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[1,2]{1,0}\n // CHECK-SAME: )->f32[1,2]{1,0}}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n // CHECK:   ROOT %Arg_0.1 = f32[1,2]{1,0} parameter(0)\n // CHECK: }\n \n@@ -268,7 +268,7 @@ module @entry attributes {\n // CHECK-SAME:   )\n // CHECK-SAME: )->((f32[1,2]{1,0}))}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: ((f32[1,2]))) -> ((f32[1,2])) {\n+// CHECK: ENTRY %main.1 (Arg_0.1: ((f32[1,2]))) -> ((f32[1,2])) {\n // CHECK:   ROOT %Arg_0.1 = ((f32[1,2]{1,0})) parameter(0)\n // CHECK: }\n \n@@ -294,9 +294,9 @@ module @entry attributes {\n // CHECK-SAME:   f32[1,2]{1,0:T(4,128)}\n // CHECK-SAME: )->f32[1,2]{1,0}}\n \n-// CHECK: ENTRY %main.3 (Arg_0.1: f32[1,2], Arg_1.2: f32[1,2]) -> f32[1,2] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[1,2], Arg_1.1: f32[1,2]) -> f32[1,2] {\n // CHECK:   ROOT %Arg_0.1 = f32[1,2]{1,0} parameter(0)\n-// CHECK:   %Arg_1.2 = f32[1,2]{1,0} parameter(1)\n+// CHECK:   %Arg_1.1 = f32[1,2]{1,0} parameter(1)\n // CHECK: }\n \n // -----\n@@ -316,7 +316,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[1,2]{1,0:T(128)(2)}\n // CHECK-SAME: )->f32[1,2]{1,0}}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n // CHECK:   ROOT %Arg_0.1 = f32[1,2]{1,0} parameter(0)\n // CHECK: }\n \n@@ -341,7 +341,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[1,2]{1,0}\n // CHECK-SAME: )->f32[1,2]{0,1}}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n // CHECK:   ROOT %Arg_0.1 = f32[1,2]{1,0} parameter(0)\n // CHECK: }\n \n@@ -374,7 +374,7 @@ module @entry attributes {\n // CHECK-SAME:   )->((f32[1,2]{1,0}, f32[1,2]{0,1}))\n // CHECK-SAME: }\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: ((f32[1,2], f32[1,2]))) -> ((f32[1,2], f32[1,2])) {\n+// CHECK: ENTRY %main.1 (Arg_0.1: ((f32[1,2], f32[1,2]))) -> ((f32[1,2], f32[1,2])) {\n // CHECK:   ROOT %Arg_0.1 = ((f32[1,2]{1,0}, f32[1,2]{1,0})) parameter(0)\n // CHECK: }\n \n@@ -399,7 +399,7 @@ module @entry attributes {\n // CHECK-SAME:   f32[1,2]{1,0}\n // CHECK-SAME: )->f32[1,2]{1,0:T(2,128)}}\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n+// CHECK: ENTRY %main.1 (Arg_0.1: f32[1,2]) -> f32[1,2] {\n // CHECK:   ROOT %Arg_0.1 = f32[1,2]{1,0} parameter(0)\n // CHECK: }\n \n@@ -432,6 +432,6 @@ module @entry attributes {\n // CHECK-SAME:   ))->((f32[1,2]{1,0:T(2,128)}, f32[1,2]{1,0:T(2,1)}))\n // CHECK-SAME: }\n \n-// CHECK: ENTRY %main.2 (Arg_0.1: ((f32[1,2], f32[1,2]))) -> ((f32[1,2], f32[1,2])) {\n+// CHECK: ENTRY %main.1 (Arg_0.1: ((f32[1,2], f32[1,2]))) -> ((f32[1,2], f32[1,2])) {\n // CHECK:   ROOT %Arg_0.1 = ((f32[1,2]{1,0}, f32[1,2]{1,0})) parameter(0)\n // CHECK: }"
        },
        {
            "sha": "a5755902ec6120d900d2da93e3ea9e769dcddb28",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/location_to_stacktrace.mlir",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Flocation_to_stacktrace.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Flocation_to_stacktrace.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Flocation_to_stacktrace.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -3,7 +3,7 @@\n // Checks no locations\n \n // CHECK-LABEL: hlo_module       {\n-// CHECK: name: \"after-all.2\"\n+// CHECK: name: \"after-all.1\"\n // CHECK-NEXT: opcode: \"after-all\"\n // CHECK-NEXT: shape {\n // CHECK-NEXT: element_type: TOKEN\n@@ -26,7 +26,7 @@ module @main attributes {mhlo.cross_program_prefetches = [], mhlo.is_dynamic = f\n \n // CHECK-LABEL: hlo_module       {\n \n-// CHECK: name: \"name_anothername_.2\"\n+// CHECK: name: \"name_anothername_.1\"\n // CHECK-NEXT: opcode: \"after-all\"\n // CHECK-NEXT: shape {\n // CHECK-NEXT:   element_type: TOKEN\n@@ -71,7 +71,7 @@ module @main attributes {mhlo.cross_program_prefetches = [], mhlo.is_dynamic = f\n \n // CHECK-LABEL: hlo_module       {\n \n-// CHECK: name: \"name_anothername_.2\"\n+// CHECK: name: \"name_anothername_.1\"\n // CHECK-NEXT: opcode: \"after-all\"\n // CHECK-NEXT: shape {\n // CHECK-NEXT:   element_type: TOKEN\n@@ -134,10 +134,10 @@ module @main attributes {mhlo.cross_program_prefetches = [], mhlo.is_dynamic = f\n // -----\n \n // Checks how stacks get collapsed into a signel flat-line debug metadata string\n-// i.e \"jit(my_add)/jit(main)/add\" -> \"add.3\"\n+// i.e \"jit(my_add)/jit(main)/add\" -> \"add.1\"\n \n // CHECK-LABEL: hlo_module       {\n-// CHECK: name: \"add.3\"\n+// CHECK: name: \"add.1\"\n #loc1 = loc(\"x\")\n #loc2 = loc(\"y\")\n module @jit_my_add attributes {jax.uses_shape_polymorphism = false, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {"
        },
        {
            "sha": "6d90da03e8737d951a5a830a19951276268d8992",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/multiple_return_tuple.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fmultiple_return_tuple.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fmultiple_return_tuple.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fmultiple_return_tuple.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -8,7 +8,7 @@\n // TUPLE-LABEL: ENTRY %main.{{.*}} (arg_tuple.1: (s32[4])) -> (s32[4], s32[1,2,3,4])\n func.func @main(%arg0: tensor<4xi32>) -> (tensor<4xi32>, tensor<1x2x3x4xi32>) {\n   // CHECK-NEXT: %Arg_0.1 = s32[4] parameter(0)\n-  // CHECK-NEXT: %broadcast.2 = s32[1,2,3,4] broadcast(%Arg_0.1), dimensions={3}\n+  // CHECK-NEXT: %broadcast.1 = s32[1,2,3,4] broadcast(%Arg_0.1), dimensions={3}\n   %0 = \"mhlo.broadcast\"(%arg0) <{broadcast_sizes = dense<[1,2,3]> : tensor<3xi64>}> : (tensor<4xi32>) -> tensor<1x2x3x4xi32>\n   func.return %arg0, %0 : tensor<4xi32>, tensor<1x2x3x4xi32>\n }"
        },
        {
            "sha": "75f04c22a9c1a0125334cbaea0bf28c13a3d2cae",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/sharding.mlir",
            "status": "modified",
            "additions": 37,
            "deletions": 37,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fsharding.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fsharding.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fsharding.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -1,8 +1,8 @@\n // RUN: xla-translate -split-input-file -mlir-hlo-to-hlo-text %s | FileCheck %s\n \n-// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[], Arg_1.2: f32[4]) -> f32[4,4]\n+// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[], Arg_1.1: f32[4]) -> f32[4,4]\n func.func public @main(%arg0: tensor<f32> {mhlo.sharding = \"\"}, %arg1: tensor<4xf32> {mhlo.sharding = \"\\08\\03\\1A\\01\\02\\22\\02\\00\\01\"}) -> (tensor<4x4xf32> {mhlo.sharding = \"\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01\"}) {\n-  // CHECK-NEXT: %Arg_1.2 = f32[4] parameter(1), sharding={devices=[2]0,1}\n+  // CHECK-NEXT: %Arg_1.1 = f32[4] parameter(1), sharding={devices=[2]0,1}\n   // CHECK-NEXT: %Arg_0.1 = f32[] parameter(0), sharding={replicated}\n   %0 = \"mhlo.broadcast_in_dim\"(%arg0) <{broadcast_dimensions = dense<> : tensor<0xi64>}> : (tensor<f32>) -> tensor<4xf32>\n   %1 = mhlo.multiply %arg1, %0 : tensor<4xf32>\n@@ -16,10 +16,10 @@ func.func public @main(%arg0: tensor<f32> {mhlo.sharding = \"\"}, %arg1: tensor<4x\n // CHECK-LABEL: ENTRY %main.{{.*}} ({{[^,]*}}: f32[5,8,128]) -> f32[5,8,128]\n func.func @main(%arg0: tensor<5x8x128xf32> {mhlo.sharding = \"\\08\\03\\1A\\03\\01\\02\\01\\22\\02\\00\\01\"}) -> (tensor<5x8x128xf32> {mhlo.sharding = \"\\08\\03\\1A\\03\\01\\02\\01\\22\\02\\00\\01\"}) {\n   // CHECK-NEXT: %Arg_0.1 = f32[5,8,128] parameter(0), sharding={devices=[1,2,1]0,1}\n-  // CHECK-NEXT: %custom-call.2 = f32[5,8,128] custom-call(%Arg_0.1), custom_call_target=\"Sharding\", sharding={devices=[1,2,1]0,1}\n-  // CHECK-NEXT: %tuple.3 = (f32[5,8,128]) tuple(%custom-call.2)\n+  // CHECK-NEXT: %custom-call.1 = f32[5,8,128] custom-call(%Arg_0.1), custom_call_target=\"Sharding\", sharding={devices=[1,2,1]0,1}\n+  // CHECK-NEXT: %tuple.1 = (f32[5,8,128]) tuple(%custom-call.1)\n   // CHECK-SAME: sharding={{\\{}}{devices=[1,2,1]0,1}}\n-  // CHECK-NEXT: ROOT %get-tuple-element.4 = f32[5,8,128] get-tuple-element(%tuple.3), index=0\n+  // CHECK-NEXT: ROOT %get-tuple-element.1 = f32[5,8,128] get-tuple-element(%tuple.1), index=0\n   // CHECK-SAME: sharding={devices=[1,2,1]0,1}\n   %0 = \"mhlo.custom_call\"(%arg0) {call_target_name = \"Sharding\",\n \t\t\t\t  mhlo.sharding = \"\\08\\03\\1A\\03\\01\\02\\01\\22\\02\\00\\01\"\n@@ -45,8 +45,8 @@ func.func @main(%arg0: tensor<4x4xf32>) -> (tensor<4x4xf32> {mhlo.sharding = \"\\0\n // CHECK-LABEL: ENTRY %main.{{.*}} () -> f32[4]\n func.func @main() -> (tensor<4xf32>) {\n   // CHECK-NEXT: %constant.1 = f32[] constant(3.1415925)\n-  // CHECK-NEXT: %broadcast.2 = f32[4] broadcast(%constant.1), dimensions={}, sharding={devices=[2]0,1}\n-  // CHECK-NEXT: ROOT %add.3 = f32[4] add(%broadcast.2, %broadcast.2)\n+  // CHECK-NEXT: %broadcast.1 = f32[4] broadcast(%constant.1), dimensions={}, sharding={devices=[2]0,1}\n+  // CHECK-NEXT: ROOT %add.1 = f32[4] add(%broadcast.1, %broadcast.1)\n   %0 = mhlo.constant {mhlo.sharding = \"{devices=[2]0,1}\"} dense<3.1415926> : tensor<4xf32>\n   %1 = mhlo.add %0, %0 : tensor<4xf32>\n   return %1 : tensor<4xf32>\n@@ -57,8 +57,8 @@ func.func @main() -> (tensor<4xf32>) {\n // CHECK-LABEL: ENTRY %main.{{.*}} () -> f32[12,24,36]\n func.func @main() -> (tensor<12x24x36xf32>) {\n   // CHECK-NEXT: %constant.1 = f32[] constant(3.1415925)\n-  // CHECK-NEXT: %broadcast.2 = f32[12,24,36] broadcast(%constant.1), dimensions={}, sharding={devices=[1,2,1]0,1}\n-  // CHECK-NEXT: ROOT %add.3 = f32[12,24,36] add(%broadcast.2, %broadcast.2)\n+  // CHECK-NEXT: %broadcast.1 = f32[12,24,36] broadcast(%constant.1), dimensions={}, sharding={devices=[1,2,1]0,1}\n+  // CHECK-NEXT: ROOT %add.1 = f32[12,24,36] add(%broadcast.1, %broadcast.1)\n   %0 = mhlo.constant {mhlo.sharding = \"{devices=[1,2,1]0,1}\"} dense<3.1415926> : tensor<12x24x36xf32>\n   %1 = mhlo.add %0, %0 : tensor<12x24x36xf32>\n   return %1 : tensor<12x24x36xf32>\n@@ -69,13 +69,13 @@ func.func @main() -> (tensor<12x24x36xf32>) {\n // CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: u64[2]) -> (u64[2], u32[512,4])\n func.func @main(%arg0: tensor<2xui64>) -> (tensor<2xui64> {mhlo.sharding = \"{devices=[2,16]<=[32] last_tile_dim_replicate}\"}, tensor<512x4xui32> {mhlo.sharding = \"{devices=[4,8]<=[32]}\"}) {\n   // CHECK-NEXT: %Arg_0.1 = u64[2] parameter(0)\n-  // CHECK-NEXT: %rng-bit-generator.2 = (u64[2], u32[512,4]) rng-bit-generator(%Arg_0.1), algorithm=rng_default, sharding={{\\{}}{replicated}, {devices=[8,4]<=[32]}}\n-  // CHECK-NEXT: %get-tuple-element.3 = u64[2] get-tuple-element(%rng-bit-generator.2), index=0, sharding={replicated}\n-  // CHECK-NEXT: %add.5 = u64[2] add(%get-tuple-element.3, %get-tuple-element.3)\n-  // CHECK-NEXT: %reshape.6 = u64[2] reshape(%add.5)\n-  // CHECK-NEXT: %get-tuple-element.4 = u32[512,4] get-tuple-element(%rng-bit-generator.2), index=1, sharding={devices=[8,4]<=[32]}\n-  // CHECK-NEXT: %reshape.7 = u32[512,4] reshape(%get-tuple-element.4)\n-  // CHECK-NEXT: ROOT %tuple.8 = (u64[2], u32[512,4]) tuple(%add.5, %get-tuple-element.4), sharding={{\\{}}{devices=[2,16]<=[32] last_tile_dim_replicate}, {devices=[4,8]<=[32]}}\n+  // CHECK-NEXT: %rng-bit-generator.1 = (u64[2], u32[512,4]) rng-bit-generator(%Arg_0.1), algorithm=rng_default, sharding={{\\{}}{replicated}, {devices=[8,4]<=[32]}}\n+  // CHECK-NEXT: %get-tuple-element.2 = u64[2] get-tuple-element(%rng-bit-generator.1), index=0, sharding={replicated}\n+  // CHECK-NEXT: %add.1 = u64[2] add(%get-tuple-element.2, %get-tuple-element.2)\n+  // CHECK-NEXT: %reshape.2 = u64[2] reshape(%add.1)\n+  // CHECK-NEXT: %get-tuple-element.3 = u32[512,4] get-tuple-element(%rng-bit-generator.1), index=1, sharding={devices=[8,4]<=[32]}\n+  // CHECK-NEXT: %reshape.3 = u32[512,4] reshape(%get-tuple-element.3)\n+  // CHECK-NEXT: ROOT %tuple.1 = (u64[2], u32[512,4]) tuple(%add.1, %get-tuple-element.3), sharding={{\\{}}{devices=[2,16]<=[32] last_tile_dim_replicate}, {devices=[4,8]<=[32]}}\n   %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<DEFAULT>}> {mhlo.sharding = \"{{replicated}, {devices=[8,4]<=[32]}}\"} : (tensor<2xui64>) -> (tensor<2xui64>, tensor<512x4xui32>)\n   %0 = mhlo.add %output_state, %output_state : tensor<2xui64>\n   return %0, %output : tensor<2xui64>, tensor<512x4xui32>\n@@ -86,11 +86,11 @@ func.func @main(%arg0: tensor<2xui64>) -> (tensor<2xui64> {mhlo.sharding = \"{dev\n // CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: u64[2]) -> (u64[2], u32[512,4])\n func.func @main(%arg0: tensor<2xui64>) -> (tensor<2xui64>, tensor<512x4xui32>) {\n   // CHECK-NEXT: %Arg_0.1 = u64[2] parameter(0)\n-  // CHECK-NEXT: %rng-bit-generator.2 = (u64[2], u32[512,4]) rng-bit-generator(%Arg_0.1), algorithm=rng_default, sharding={{\\{}}{replicated}, {replicated}}\n-  // CHECK-NEXT: %get-tuple-element.3 = u64[2] get-tuple-element(%rng-bit-generator.2), index=0, sharding={replicated}\n-  // CHECK-NEXT: %add.5 = u64[2] add(%get-tuple-element.3, %get-tuple-element.3)\n-  // CHECK-NEXT: %get-tuple-element.4 = u32[512,4] get-tuple-element(%rng-bit-generator.2), index=1, sharding={replicated}\n-  // CHECK-NEXT: ROOT %tuple.6 = (u64[2], u32[512,4]) tuple(%add.5, %get-tuple-element.4)\n+  // CHECK-NEXT: %rng-bit-generator.1 = (u64[2], u32[512,4]) rng-bit-generator(%Arg_0.1), algorithm=rng_default, sharding={{\\{}}{replicated}, {replicated}}\n+  // CHECK-NEXT: %get-tuple-element.2 = u64[2] get-tuple-element(%rng-bit-generator.1), index=0, sharding={replicated}\n+  // CHECK-NEXT: %add.1 = u64[2] add(%get-tuple-element.2, %get-tuple-element.2)\n+  // CHECK-NEXT: %get-tuple-element.3 = u32[512,4] get-tuple-element(%rng-bit-generator.1), index=1, sharding={replicated}\n+  // CHECK-NEXT: ROOT %tuple.1 = (u64[2], u32[512,4]) tuple(%add.1, %get-tuple-element.3)\n   %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<DEFAULT>}> {mhlo.sharding = \"{replicated}\"} : (tensor<2xui64>) -> (tensor<2xui64>, tensor<512x4xui32>)\n   %0 = mhlo.add %output_state, %output_state : tensor<2xui64>\n   return %0, %output : tensor<2xui64>, tensor<512x4xui32>\n@@ -112,7 +112,7 @@ func.func @main(%arg0: tensor<2xui64>) -> (tensor<2xui64>, tensor<512x4xui32>) {\n \n // CHECK:      ENTRY %main.{{[0-9]+}} ([[ARG:Arg_0.[0-9]+]]: s32[]) -> s32[] {\n // CHECK-NEXT:   %[[ARG]] = s32[] parameter(0)\n-// CHECK-NEXT:   ROOT %while.10 = s32[] while(%[[ARG]]), condition=%[[COND]], body=%[[BODY]], sharding={replicated}\n+// CHECK-NEXT:   ROOT %while.1 = s32[] while(%[[ARG]]), condition=%[[COND]], body=%[[BODY]], sharding={replicated}\n \n func.func @main(%arg0: tensor<i32>) -> tensor<i32> {\n   %0 = mhlo.while(%iterArg = %arg0) : tensor<i32> attributes {mhlo.sharding = \"{replicated}\"}\n@@ -364,10 +364,10 @@ func.func @main(%arg0: tensor<i32>,\n // CHECK-NEXT:   %[[ARG3]] = f32[4] parameter(3), sharding={replicated}\n // CHECK-NEXT:   %[[ARG4]] = f32[4] parameter(4), sharding={devices=[4]<=[4]}\n // CHECK-NEXT:   %[[TUPLE7:tuple.*]] = (f32[4], f32[4]) tuple(%[[ARG3]], %[[ARG4]]), sharding={{\\{}}{replicated}, {devices=[4]<=[4]}}\n-// CHECK-NEXT:   %conditional.18 = (f32[4], f32[4]) conditional(%[[ARG0]], %[[TUPLE6]], %[[TUPLE7]]), true_computation=%[[BRANCH0]], false_computation=%[[BRANCH1]],\n+// CHECK-NEXT:   %conditional.1 = (f32[4], f32[4]) conditional(%[[ARG0]], %[[TUPLE6]], %[[TUPLE7]]), true_computation=%[[BRANCH0]], false_computation=%[[BRANCH1]],\n // CHECK-SAME:     sharding={{\\{}}{replicated}, {devices=[4]<=[4]}}\n-// CHECK-NEXT:   %[[GTE19:get-tuple-element.*]] = f32[4] get-tuple-element(%conditional.18), index=0, sharding={replicated}\n-// CHECK-NEXT:   %[[GTE20:get-tuple-element.*]] = f32[4] get-tuple-element(%conditional.18), index=1, sharding={devices=[4]<=[4]}\n+// CHECK-NEXT:   %[[GTE19:get-tuple-element.*]] = f32[4] get-tuple-element(%conditional.1), index=0, sharding={replicated}\n+// CHECK-NEXT:   %[[GTE20:get-tuple-element.*]] = f32[4] get-tuple-element(%conditional.1), index=1, sharding={devices=[4]<=[4]}\n // CHECK-NEXT:   ROOT %tuple.{{.*}} = (f32[4], f32[4]) tuple(%[[GTE19]], %[[GTE20]])\n \n func.func @main(%arg0: tensor<i1>,\n@@ -415,10 +415,10 @@ func.func @main(%arg0: tensor<i1>,\n // CHECK-LABEL: ENTRY %main.{{.*}} ({{[^,]*}}: f32[5,8,128]) -> (f32[5,8,128], f32[5,8,128])\n func.func @main(%arg0: tensor<5x8x128xf32> {mhlo.sharding = \"{devices=[1,2,1]0,1}\"}) -> (tuple<tensor<5x8x128xf32>, tensor<5x8x128xf32>> {mhlo.sharding = \"{{devices=[1,2,1]0,1}, {replicated}}\"}) {\n   // CHECK-NEXT: %Arg_0.1 = f32[5,8,128] parameter(0), sharding={devices=[1,2,1]0,1}\n-  // CHECK-NEXT: %custom-call.2 = (f32[5,8,128], f32[5,8,128]) custom-call(%Arg_0.1), custom_call_target=\"Sharding\", sharding={{\\{}}{devices=[1,2,1]0,1}, {replicated}}\n-  // CHECK-NEXT: %tuple.3 = ((f32[5,8,128], f32[5,8,128])) tuple(%custom-call.2)\n+  // CHECK-NEXT: %custom-call.1 = (f32[5,8,128], f32[5,8,128]) custom-call(%Arg_0.1), custom_call_target=\"Sharding\", sharding={{\\{}}{devices=[1,2,1]0,1}, {replicated}}\n+  // CHECK-NEXT: %tuple.1 = ((f32[5,8,128], f32[5,8,128])) tuple(%custom-call.1)\n   // CHECK-SAME: sharding={{\\{}}{devices=[1,2,1]0,1}, {replicated}}\n-  // CHECK-NEXT: ROOT %get-tuple-element.4 = (f32[5,8,128], f32[5,8,128]) get-tuple-element(%tuple.3), index=0\n+  // CHECK-NEXT: ROOT %get-tuple-element.1 = (f32[5,8,128], f32[5,8,128]) get-tuple-element(%tuple.1), index=0\n   // CHECK-SAME: sharding={{\\{}}{devices=[1,2,1]0,1}, {replicated}}\n   %0 = \"mhlo.custom_call\"(%arg0) {call_target_name = \"Sharding\",\n \t\t\t\t  mhlo.sharding = \"{{devices=[1,2,1]0,1}, {replicated}}\"\n@@ -431,9 +431,9 @@ func.func @main(%arg0: tensor<5x8x128xf32> {mhlo.sharding = \"{devices=[1,2,1]0,1\n // CHECK-LABEL: ENTRY %main.{{.*}} ({{[^,]*}}: f32[5,8,128]) -> f32[5,8,128]\n func.func @main(%arg0: tensor<5x8x128xf32> {mhlo.sharding = \"{devices=[1,2,1]0,1}\"}) -> (tensor<5x8x128xf32> {mhlo.sharding = \"{devices=[2,1,1]0,1}\"}) {\n   // CHECK-NEXT: %Arg_0.1 = f32[5,8,128] parameter(0), sharding={devices=[1,2,1]0,1}\n-  // CHECK-NEXT: %tuple.2 = (f32[5,8,128]) tuple(%Arg_0.1)\n+  // CHECK-NEXT: %tuple.1 = (f32[5,8,128]) tuple(%Arg_0.1)\n   // CHECK-SAME: sharding={{\\{}}{devices=[1,2,1]0,1}}\n-  // CHECK-NEXT: ROOT %get-tuple-element.3 = f32[5,8,128] get-tuple-element(%tuple.2), index=0\n+  // CHECK-NEXT: ROOT %get-tuple-element.1 = f32[5,8,128] get-tuple-element(%tuple.1), index=0\n   // CHECK-SAME: sharding={devices=[2,1,1]0,1}\n   func.return %arg0 : tensor<5x8x128xf32>\n }\n@@ -444,10 +444,10 @@ func.func @main(%arg0: tensor<5x8x128xf32> {mhlo.sharding = \"{devices=[1,2,1]0,1\n // CHECK-NOT: manual\n func.func @main() -> tensor<2048x128xf32> {\n   // CHECK-NEXT: %after-all.1 = token[] after-all(), sharding={manual}\n-  // CHECK-NEXT: %infeed.2 = ((f32[2048,128]), token[]) infeed(%after-all.1), sharding={{[{][{]manual}, {manual[}][}]}}\n-  // CHECK-NEXT: %get-tuple-element.3 = (f32[2048,128]) get-tuple-element(%infeed.2), index=0, sharding={{[{][{]manual[}][}]}}\n+  // CHECK-NEXT: %infeed.1 = ((f32[2048,128]), token[]) infeed(%after-all.1), sharding={{[{][{]manual}, {manual[}][}]}}\n+  // CHECK-NEXT: %get-tuple-element.3 = (f32[2048,128]) get-tuple-element(%infeed.1), index=0, sharding={{[{][{]manual[}][}]}}\n   // CHECK-NEXT: ROOT %get-tuple-element.4 = f32[2048,128] get-tuple-element(%get-tuple-element.3), index=0, sharding={manual}\n-  // CHECK-NEXT: %get-tuple-element.5 = token[] get-tuple-element(%infeed.2), index=1, sharding={manual}\n+  // CHECK-NEXT: %get-tuple-element.5 = token[] get-tuple-element(%infeed.1), index=1, sharding={manual}\n   %0 = mhlo.create_token {mhlo.sharding = \"{manual}\", xla_shape = \"token[]\"} : !mhlo.token\n   %1:2 = \"mhlo.infeed\"(%0) <{infeed_config = \"\", layout = [[1, 0]]}> {mhlo.sharding = \"{{manual}, {manual}}\"} : (!mhlo.token) -> (tensor<2048x128xf32>, !mhlo.token)\n   return %1#0 : tensor<2048x128xf32>\n@@ -457,10 +457,10 @@ func.func @main() -> tensor<2048x128xf32> {\n // CHECK: HloModule\n // CHECK: ENTRY\n func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> () {\n-  // CHECK: %tuple.6 = (f32[4], f32[4]) tuple(%add.3, %add.4), sharding={{[{][{]manual}, {manual[}][}]}}\n-  // CHECK-NEXT: %after-all.5 = token[] after-all(), sharding={manual}\n-  // CHECK-NEXT:  %outfeed.7 = token[] outfeed(%tuple.6, %after-all.5), outfeed_shape=(f32[4]{0}, f32[4]{0}), sharding={manual}\n-  // CHECK-NEXT: ROOT %tuple.8 = () tuple()\n+  // CHECK: %tuple.2 = (f32[4], f32[4]) tuple(%add.2, %add.3), sharding={{[{][{]manual}, {manual[}][}]}}\n+  // CHECK-NEXT: %after-all.1 = token[] after-all(), sharding={manual}\n+  // CHECK-NEXT:  %outfeed.1 = token[] outfeed(%tuple.2, %after-all.1), outfeed_shape=(f32[4]{0}, f32[4]{0}), sharding={manual}\n+  // CHECK-NEXT: ROOT %tuple.3 = () tuple()\n   %0 = \"mhlo.add\"(%arg0, %arg1) {mhlo.sharding = \"{manual}\"} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n   %1 = \"mhlo.add\"(%arg1, %arg0) {mhlo.sharding = \"{manual}\"} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n   %2 = mhlo.create_token {mhlo.sharding = \"{manual}\", xla_shape = \"token[]\"} : !mhlo.token"
        },
        {
            "sha": "61f46192ce63e24699801293aa8c704385f89d2f",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/shardy.mlir",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fshardy.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fshardy.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fshardy.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -26,15 +26,15 @@ module @sdy_frontend_attributes attributes {mhlo.frontend_attributes = {xla.sdy.\n       // CHECK: %Arg_0.1 = f32[8,8] parameter(0)\n       // CHECK-SAME: sharding={devices=[2,1,16]<=[32] last_tile_dim_replicate}\n       // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@mesh, [{\\\"x\\\"}, {}]>\"}\n-      // CHECK-NEXT: %Arg_1.2 = f32[8,8] parameter(1)\n+      // CHECK-NEXT: %Arg_1.1 = f32[8,8] parameter(1)\n       // CHECK-SAME: sharding={devices=[1,4,8]<=[2,4,4]T(1,0,2) last_tile_dim_replicate}\n       // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@mesh, [{}, {\\\"y\\\"}]>\"}\n         %0 = mhlo.add %arg0, %arg1\n           {mhlo.frontend_attributes = {\n             xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{\\\"x\\\"}, {}]>]>\"\n           }}\n           : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>\n-        // CHECK: %add.3 = f32[8,8] add(%Arg_0.1, %Arg_1.2)\n+        // CHECK: %add.1 = f32[8,8] add(%Arg_0.1, %Arg_1.1)\n         // CHECK-NOT: sharding={\n         // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@mesh, [{\\\"x\\\"}, {}]>]>\"}\n         %1 = \"mhlo.custom_call\"(%0) {\n@@ -43,7 +43,7 @@ module @sdy_frontend_attributes attributes {mhlo.frontend_attributes = {xla.sdy.\n             xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"\n           }\n         } : (tensor<8x8xf32>) -> tensor<8x8xf32>\n-        // CHECK: %[[CUSTOM_CALL_0:.*]] = f32[8,8] custom-call(%add.3)\n+        // CHECK: %[[CUSTOM_CALL_0:.*]] = f32[8,8] custom-call(%add.1)\n         // CHECK-SAME: custom_call_target=\"xla.sdy.FuncResultSharding\"\n         // CHECK-NOT: sharding={\n         // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@mesh, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"}\n@@ -55,14 +55,14 @@ module @sdy_frontend_attributes attributes {mhlo.frontend_attributes = {xla.sdy.\n             xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>\"\n           }\n         } : (tensor<8x8xf32>) -> tensor<8x8xf32>\n-        // CHECK: %[[CUSTOM_CALL_1:.*]] = f32[8,8] custom-call(%add.3)\n+        // CHECK: %[[CUSTOM_CALL_1:.*]] = f32[8,8] custom-call(%add.1)\n         // CHECK-SAME: custom_call_target=\"xla.sdy.FuncResultSharding\"\n         // CHECK-NOT: sharding={\n         // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>\"}\n         // CHECK-NEXT: %[[RESHAPE_1:.*]] = f32[8,8] reshape(%[[CUSTOM_CALL_1]])\n         // CHECK-SAME: sharding={replicated}\n         return %1, %2 : tensor<8x8xf32>, tensor<8x8xf32>\n-        // CHECK: ROOT %tuple.8 = (f32[8,8], f32[8,8]) tuple(%[[CUSTOM_CALL_0:.*]], %[[CUSTOM_CALL_1:.*]])\n+        // CHECK: ROOT %tuple.1 = (f32[8,8], f32[8,8]) tuple(%[[CUSTOM_CALL_0:.*]], %[[CUSTOM_CALL_1:.*]])\n         // CHECK-SAME{LITERAL}: sharding={{devices=[8,4]<=[32]}, {replicated}}\n       }\n     }\n@@ -77,15 +77,15 @@ module @sdy_frontend_attributes_inlined_meshes {\n       // CHECK: %Arg_0.1 = f32[8,8] parameter(0)\n       // CHECK-SAME: sharding={devices=[2,1,2]<=[4] last_tile_dim_replicate}\n       // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<mesh<[\\\"x\\\"=2, \\\"y\\\"=2]>, [{\\\"x\\\"}, {}]>\"}\n-      // CHECK-NEXT: %Arg_1.2 = f32[8,8] parameter(1)\n+      // CHECK-NEXT: %Arg_1.1 = f32[8,8] parameter(1)\n       // CHECK-SAME: sharding={devices=[1,2,2]<=[2,2]T(1,0) last_tile_dim_replicate}\n       // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<mesh<[\\\"x\\\"=2, \\\"y\\\"=2]>, [{}, {\\\"y\\\"}]>\"}\n         %0 = mhlo.add %arg0, %arg1\n           {mhlo.frontend_attributes = {\n             xla.sdy.sharding = \"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=2]>, [{\\\"x\\\"}, {\\\"y\\\"}]>]>\"\n           }}\n           : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>\n-        // CHECK: %add.3 = f32[8,8] add(%Arg_0.1, %Arg_1.2)\n+        // CHECK: %add.1 = f32[8,8] add(%Arg_0.1, %Arg_1.1)\n         // CHECK-NOT: sharding={\n         // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=2]>, [{\\\"x\\\"}, {\\\"y\\\"}]>]>\"}\n         %1 = \"mhlo.custom_call\"(%0) {\n@@ -94,7 +94,7 @@ module @sdy_frontend_attributes_inlined_meshes {\n             xla.sdy.sharding = \"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"\n           }\n         } : (tensor<8x8xf32>) -> tensor<8x8xf32>\n-        // CHECK: %[[CUSTOM_CALL_0:.*]] = f32[8,8] custom-call(%add.3)\n+        // CHECK: %[[CUSTOM_CALL_0:.*]] = f32[8,8] custom-call(%add.1)\n         // CHECK-SAME: custom_call_target=\"xla.sdy.FuncResultSharding\"\n         // CHECK-NOT: sharding={\n         // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"}\n@@ -106,14 +106,14 @@ module @sdy_frontend_attributes_inlined_meshes {\n             xla.sdy.sharding = \"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{}, {}]>]>\"\n           }\n         } : (tensor<8x8xf32>) -> tensor<8x8xf32>\n-        // CHECK: %[[CUSTOM_CALL_1:.*]] = f32[8,8] custom-call(%add.3)\n+        // CHECK: %[[CUSTOM_CALL_1:.*]] = f32[8,8] custom-call(%add.1)\n         // CHECK-SAME: custom_call_target=\"xla.sdy.FuncResultSharding\"\n         // CHECK-NOT: sharding={\n         // CHECK-SAME: frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{}, {}]>]>\"}\n         // CHECK-NEXT: %[[RESHAPE_1:.*]] = f32[8,8] reshape(%[[CUSTOM_CALL_1]])\n         // CHECK-SAME: sharding={replicated}\n         return %1, %2 : tensor<8x8xf32>, tensor<8x8xf32>\n-        // CHECK: ROOT %tuple.8 = (f32[8,8], f32[8,8]) tuple(%[[CUSTOM_CALL_0:.*]], %[[CUSTOM_CALL_1:.*]])\n+        // CHECK: ROOT %tuple.1 = (f32[8,8], f32[8,8]) tuple(%[[CUSTOM_CALL_0:.*]], %[[CUSTOM_CALL_1:.*]])\n         // CHECK-SAME{LITERAL}: sharding={{devices=[8,4]<=[32]}, {replicated}}\n       }\n     }"
        },
        {
            "sha": "12f39ce04b595060c6206ff20e3301fc3290df1a",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/while_free_vars.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fwhile_free_vars.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fwhile_free_vars.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fwhile_free_vars.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -29,7 +29,7 @@\n // CHECK-NEXT:   %[[ARG2]] = f32[4] parameter(2)\n // CHECK-NEXT:   %[[TUPLE:tuple.*]] = (s32[], f32[4], s32[], s32[], f32[4]) tuple(%[[ARG0]], %[[ARG1]], %[[CONSTANT4]], %[[CONSTANT5]], %[[ARG2]])\n // CHECK-SAME:     sharding={{\\{}}{replicated}, {devices=[2,2]<=[4] last_tile_dim_replicate}, {replicated}, {replicated}, {devices=[4]<=[4]}}\n-// CHECK-NEXT:   %[[WHILE:while.25]] = (s32[], f32[4], s32[], s32[], f32[4]) while(%[[TUPLE]]), condition=%[[COND]], body=%[[BODY]]\n+// CHECK-NEXT:   %[[WHILE:while.1]] = (s32[], f32[4], s32[], s32[], f32[4]) while(%[[TUPLE]]), condition=%[[COND]], body=%[[BODY]]\n // CHECK-SAME:     sharding={{\\{}}{replicated}, {devices=[2,2]<=[4] last_tile_dim_replicate}, {replicated}, {replicated}, {devices=[4]<=[4]}}\n // CHECK-NEXT:   %[[GTE26:get-tuple-element.*]] = s32[] get-tuple-element(%[[WHILE]]), index=0\n // CHECK-NEXT:   ROOT %[[GTE27:get-tuple-element.*]] = f32[4] get-tuple-element(%[[WHILE]]), index=1, sharding={devices=[2,2]<=[4] last_tile_dim_replicate}"
        },
        {
            "sha": "e5af6c9e1ee63dde0751dba2bc9a2071e8834a59",
            "filename": "third_party/xla/xla/hlo/translate/tests/simple.mlir",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fsimple.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fsimple.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fsimple.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -1,26 +1,26 @@\n // RUN: hlo-translate -mlir-to-hlo -split-input-file %s | FileCheck %s\n \n-// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.2: f32[4]) -> f32[]\n+// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.1: f32[4]) -> f32[]\n func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<f32> {\n   // CHECK: %Arg_0.1 = f32[4] parameter(0)\n-  // CHECK: %Arg_1.2 = f32[4] parameter(1)\n-  // CHECK: %add.3 = f32[4] add(%Arg_0.1, %Arg_1.2)\n+  // CHECK: %Arg_1.1 = f32[4] parameter(1)\n+  // CHECK: %add.1 = f32[4] add(%Arg_0.1, %Arg_1.1)\n   %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n-  // CHECK: ROOT %dot.4 = f32[] dot(%add.3, %Arg_1.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+  // CHECK: ROOT %dot.1 = f32[] dot(%add.1, %Arg_1.1), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n   %1 = stablehlo.dot %0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<f32>\n   func.return %1 : tensor<f32>\n }\n \n // -----\n // MHLO to HLO\n \n-// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.2: f32[4]) -> f32[4]\n+// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.1: f32[4]) -> f32[4]\n func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n   // CHECK-NEXT: %Arg_0.1 = f32[4] parameter(0)\n-  // CHECK-NEXT: %Arg_1.2 = f32[4] parameter(1)\n-  // CHECK-NEXT: %add.3 = f32[4] add(%Arg_0.1, %Arg_1.2)\n+  // CHECK-NEXT: %Arg_1.1 = f32[4] parameter(1)\n+  // CHECK-NEXT: %add.2 = f32[4] add(%Arg_0.1, %Arg_1.1)\n   %0 = \"mhlo.add\"(%arg0, %arg1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-  // CHECK-NEXT: ROOT %add.4 = f32[4] add(%add.3, %Arg_1.2)\n+  // CHECK-NEXT: ROOT %add.3 = f32[4] add(%add.2, %Arg_1.1)\n   %1 = \"mhlo.add\"(%0, %arg1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n   func.return %1 : tensor<4xf32>\n }"
        },
        {
            "sha": "48842d7e4bbe68d74990c8d016e55ea3f852dec8",
            "filename": "third_party/xla/xla/hlo/translate/tests/stablehlo.mlir",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -8,7 +8,7 @@\n \n // CHECK: %[[ARG0:.*]] = f32[4] parameter(0)\n // CHECK: %[[ARG1:.*]] = f32[4] parameter(1)\n-// CHECK: ROOT %add.3 = f32[4] add(%[[ARG0]], %[[ARG1]])\n+// CHECK: ROOT %add.1 = f32[4] add(%[[ARG0]], %[[ARG1]])\n func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n   %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>  func.return %0 : tensor<4xf32>\n }\n@@ -1954,11 +1954,11 @@ module {\n // CHECK: ENTRY\n func.func @main() -> tensor<128x2048xf32> {\n   // CHECK-NEXT: %after-all.1 = token[] after-all(), sharding={manual}\n-  // CHECK-NEXT: %infeed.2 = ((f32[2048,128]), token[]) infeed(%after-all.1), sharding={{[{][{]manual}, {manual[}][}]}}\n-  // CHECK-NEXT: %get-tuple-element.5 = token[] get-tuple-element(%infeed.2), index=1, sharding={manual}\n-  // CHECK-NEXT: %get-tuple-element.3 = (f32[2048,128]) get-tuple-element(%infeed.2), index=0, sharding={{[{][{]manual[}][}]}}\n+  // CHECK-NEXT: %infeed.1 = ((f32[2048,128]), token[]) infeed(%after-all.1), sharding={{[{][{]manual}, {manual[}][}]}}\n+  // CHECK-NEXT: %get-tuple-element.5 = token[] get-tuple-element(%infeed.1), index=1, sharding={manual}\n+  // CHECK-NEXT: %get-tuple-element.3 = (f32[2048,128]) get-tuple-element(%infeed.1), index=0, sharding={{[{][{]manual[}][}]}}\n   // CHECK-NEXT: %get-tuple-element.4 = f32[2048,128] get-tuple-element(%get-tuple-element.3), index=0, sharding={manual}\n-  // CHECK-NEXT: ROOT %transpose.6 = f32[128,2048] transpose(%get-tuple-element.4), dimensions={1,0}, sharding={manual}\n+  // CHECK-NEXT: ROOT %transpose.1 = f32[128,2048] transpose(%get-tuple-element.4), dimensions={1,0}, sharding={manual}\n   %0 = stablehlo.create_token {mhlo.sharding = \"{manual}\", xla_shape = \"token[]\"} : !stablehlo.token\n   %1:2 = \"stablehlo.infeed\"(%0) <{infeed_config = \"\", layout = [[1, 0]]}> {mhlo.sharding = \"{{manual}, {manual}}\"} : (!stablehlo.token) -> (tensor<2048x128xf32>, !stablehlo.token)\n   %2 = stablehlo.transpose %1#0, dims = [1, 0] {mhlo.sharding = \"{manual}\", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = \"f32[128,2048]{0,1}\"} : (tensor<2048x128xf32>) -> tensor<128x2048xf32>"
        },
        {
            "sha": "8ac875b77edb9662bc7a47bc088dbe2b4a13d42d",
            "filename": "third_party/xla/xla/hlo/translate/tests/stablehlo_while_free_vars.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo_while_free_vars.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo_while_free_vars.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo_while_free_vars.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -30,7 +30,7 @@\n // CHECK-NEXT:   %[[ARG2]] = f32[4] parameter(2)\n // CHECK-NEXT:   %[[TUPLE:tuple.*]] = (s32[], f32[4], s32[], s32[], f32[4]) tuple(%[[ARG0]], %[[ARG1]], %[[CONSTANT4]], %[[CONSTANT5]], %[[ARG2]])\n // CHECK-SAME:     sharding={{\\{}}{replicated}, {devices=[2,2]<=[4] last_tile_dim_replicate}, {replicated}, {replicated}, {devices=[4]<=[4]}}\n-// CHECK-NEXT:   %[[WHILE:while.25]] = (s32[], f32[4], s32[], s32[], f32[4]) while(%[[TUPLE]]), condition=%[[COND]], body=%[[BODY]]\n+// CHECK-NEXT:   %[[WHILE:while.1]] = (s32[], f32[4], s32[], s32[], f32[4]) while(%[[TUPLE]]), condition=%[[COND]], body=%[[BODY]]\n // CHECK-SAME:     sharding={{\\{}}{replicated}, {devices=[2,2]<=[4] last_tile_dim_replicate}, {replicated}, {replicated}, {devices=[4]<=[4]}}\n // CHECK-NEXT:   %[[GTE26:get-tuple-element.*]] = s32[] get-tuple-element(%[[WHILE]]), index=0\n // CHECK-NEXT:   ROOT %[[GTE27:get-tuple-element.*]] = f32[4] get-tuple-element(%[[WHILE]]), index=1, sharding={devices=[2,2]<=[4] last_tile_dim_replicate}"
        },
        {
            "sha": "033217dc56656b4b57239fcc9abb2fa0e5e9d48e",
            "filename": "third_party/xla/xla/hlo/translate/tests/vhlo_input.mlir",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fvhlo_input.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fvhlo_input.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fvhlo_input.mlir?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -6,13 +6,13 @@\n // The `.mlir.bc` file is used in the above RUN command, along with the\n // filechecks specified in this file.\n \n-// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.2: f32[4]) -> f32[]\n+// CHECK-LABEL: ENTRY %main.{{.*}} (Arg_0.1: f32[4], Arg_1.1: f32[4]) -> f32[]\n func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<f32> {\n   // CHECK: %Arg_0.1 = f32[4] parameter(0)\n-  // CHECK: %Arg_1.2 = f32[4] parameter(1)\n-  // CHECK: %add.3 = f32[4] add(%Arg_0.1, %Arg_1.2)\n+  // CHECK: %Arg_1.1 = f32[4] parameter(1)\n+  // CHECK: %add.1 = f32[4] add(%Arg_0.1, %Arg_1.1)\n   %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n-  // CHECK: ROOT %dot.4 = f32[] dot(%add.3, %Arg_1.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+  // CHECK: ROOT %dot.1 = f32[] dot(%add.1, %Arg_1.1), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n   %1 = stablehlo.dot %0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<f32>\n   func.return %1 : tensor<f32>\n }\n\\ No newline at end of file"
        },
        {
            "sha": "62b7974a22f631306a47fd28127230ae6b1b6598",
            "filename": "third_party/xla/xla/pjrt/utils.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fpjrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fpjrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Futils.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -119,7 +119,8 @@ absl::StatusOr<std::pair<std::vector<Shape>, Shape>> GetShardedProgramShapes(\n         TF_ASSIGN_OR_RETURN(arg_shapes[instr.parameter_number()],\n                             GetShardedShape(instr));\n       }\n-      if (instr.id() == comp.root_id()) {\n+      if (HloInstruction::CalculateLocalId(instr.id()) ==\n+          HloInstruction::CalculateLocalId(comp.root_id())) {\n         if (result_shape.element_type() != PRIMITIVE_TYPE_INVALID) {\n           return InvalidArgument(\"Found multiple root instructions\");\n         }"
        },
        {
            "sha": "cdbf21ebf7bd41c20c461622aae8afed0abde8e4",
            "filename": "third_party/xla/xla/service/dynamic_dimension_inference.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -169,7 +169,7 @@ class DynamicDimensionInference {\n              lhs.dim == rhs.dim;\n     }\n \n-    std::tuple<int, int, std::string, int64_t> ToTuple() const {\n+    std::tuple<int, int64_t, std::string, int64_t> ToTuple() const {\n       return std::make_tuple(\n           inst && inst->GetModule() ? inst->GetModule()->unique_id() : -1,\n           inst ? inst->unique_id() : -1, index.ToString(), dim);"
        },
        {
            "sha": "54766a0871c53bf849f240aac9b598b035f1ad00",
            "filename": "third_party/xla/xla/service/gpu/model/fusion_analysis_cache.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ffusion_analysis_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ffusion_analysis_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ffusion_analysis_cache.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n \n+#include <cstdint>\n #include <utility>\n \n #include \"absl/synchronization/mutex.h\"\n@@ -50,7 +51,7 @@ const HloFusionAnalysis& HloFusionAnalysisCache::Get(\n \n const HloFusionAnalysis& HloFusionAnalysisCache::Get(\n     const HloInstruction& producer, const HloInstruction& consumer) {\n-  std::pair<int, int> key{producer.unique_id(), consumer.unique_id()};\n+  std::pair<int64_t, int64_t> key{producer.unique_id(), consumer.unique_id()};\n   {\n     absl::MutexLock lock(&mutex_);\n     auto it = producer_consumer_analyses_.find(key);\n@@ -79,18 +80,20 @@ const HloFusionAnalysis& HloFusionAnalysisCache::Get(\n }\n \n void HloFusionAnalysisCache::Invalidate(const HloInstruction& instruction) {\n-  analyses_.erase(instruction.unique_id());\n+  Invalidate(instruction.unique_id());\n+}\n+\n+void HloFusionAnalysisCache::Invalidate(const int64_t instruction_id) {\n+  analyses_.erase(instruction_id);\n \n-  if (auto consumers =\n-          consumers_for_producers_.extract(instruction.unique_id())) {\n+  if (auto consumers = consumers_for_producers_.extract(instruction_id)) {\n     for (const auto consumer : consumers.mapped()) {\n-      producer_consumer_analyses_.erase({instruction.unique_id(), consumer});\n+      producer_consumer_analyses_.erase({instruction_id, consumer});\n     }\n   }\n-  if (auto producers =\n-          producers_for_consumers_.extract(instruction.unique_id())) {\n+  if (auto producers = producers_for_consumers_.extract(instruction_id)) {\n     for (const auto producer : producers.mapped()) {\n-      producer_consumer_analyses_.erase({producer, instruction.unique_id()});\n+      producer_consumer_analyses_.erase({producer, instruction_id});\n     }\n   }\n }"
        },
        {
            "sha": "93190e54532d3ed4c074c360157cc7b7abef0e3c",
            "filename": "third_party/xla/xla/service/gpu/model/fusion_analysis_cache.h",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ffusion_analysis_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ffusion_analysis_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ffusion_analysis_cache.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_MODEL_FUSION_ANALYSIS_CACHE_H_\n #define XLA_SERVICE_GPU_MODEL_FUSION_ANALYSIS_CACHE_H_\n \n+#include <cstdint>\n #include <utility>\n #include <vector>\n \n@@ -49,6 +50,10 @@ class HloFusionAnalysisCache {\n   // removes all producer-consumer fusions that involve this instruction.\n   void Invalidate(const HloInstruction& instruction);\n \n+  // Removes the cache entry for the given instruction, if it exists. Also\n+  // removes all producer-consumer fusions that involve this instruction.\n+  void Invalidate(int64_t instruction_id);\n+\n   // Delete all cache entries.\n   void Clear();\n \n@@ -58,16 +63,16 @@ class HloFusionAnalysisCache {\n   absl::Mutex mutex_;\n \n   // All `int` keys and values here are unique instruction IDs.\n-  absl::node_hash_map<int, HloFusionAnalysis> analyses_;\n-  absl::node_hash_map<std::pair<int, int>, HloFusionAnalysis>\n+  absl::node_hash_map<int64_t, HloFusionAnalysis> analyses_;\n+  absl::node_hash_map<std::pair<int64_t, int64_t>, HloFusionAnalysis>\n       producer_consumer_analyses_;\n \n   // For each instruction `producer`, contains the `consumer`s for which we have\n   // entries {`producer`, `consumer`} in `producer_consumer_analyses_`.\n-  absl::flat_hash_map<int, std::vector<int>> consumers_for_producers_;\n+  absl::flat_hash_map<int64_t, std::vector<int64_t>> consumers_for_producers_;\n   // For each instruction `consumer`, contains the `producer`s for which we have\n   // entries {`producer`, `consumer`} in `producer_consumer_analyses_`.\n-  absl::flat_hash_map<int, std::vector<int>> producers_for_consumers_;\n+  absl::flat_hash_map<int64_t, std::vector<int64_t>> producers_for_consumers_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "f7794be02f205a2a4b8e20e5a380f62a73375f66",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -729,8 +729,6 @@ absl::StatusOr<CommandBuffer> CommandBufferScheduling::PrepareCommandBuffer(\n           builder.AddInstruction(HloInstruction::CreateParameter(\n               parameter_id, operand->shape(), \"p\")));\n \n-      parameter->UniquifyName(module);\n-      parameter->UniquifyId(module);\n       inst_mapping[operand] = parameters[operand] = parameter;\n     }\n   }\n@@ -746,7 +744,6 @@ absl::StatusOr<CommandBuffer> CommandBufferScheduling::PrepareCommandBuffer(\n     }\n     inst_mapping[inst] = builder.AddInstruction(\n         inst->CloneWithNewOperands(inst->shape(), mapped_operands(inst), &ctx));\n-    inst_mapping[inst]->UniquifyId(module);\n \n     // Clear the called computations of the old instruction, because it is\n     // typically not legal for one computation to have more than one caller.\n@@ -779,15 +776,10 @@ absl::StatusOr<CommandBuffer> CommandBufferScheduling::PrepareCommandBuffer(\n \n   // If we return multiple results wrap them into tuple.\n   if (returned.size() > 1) {\n-    HloInstruction* inst =\n-        builder.AddInstruction(HloInstruction::CreateTuple(returned));\n-    inst->UniquifyName(module);\n-    inst->UniquifyId(module);\n+    builder.AddInstruction(HloInstruction::CreateTuple(returned));\n   }\n \n   std::unique_ptr<HloComputation> comp = builder.Build();\n-  comp->UniquifyName(module);\n-  comp->SetUniqueId(comp->root_instruction()->unique_id());\n \n   return CommandBuffer{std::move(arguments), std::move(results),\n                        std::move(comp), std::move(inst_mapping)};\n@@ -822,8 +814,9 @@ absl::StatusOr<HloComputation*> CommandBufferScheduling::RewriteCommandBuffer(\n   }\n \n   HloComputation* computation =\n-      parent->parent()->AddComputation(std::move(command_buffer.computation),\n-                                       /*is_entry=*/false);\n+      parent->parent()->AddComputationAndUnifyNamesAndIds(\n+          std::move(command_buffer.computation),\n+          /*is_entry=*/false);\n \n   HloInstruction* call = parent->AddInstruction(HloInstruction::CreateCall(\n       cmd_buffer_result_shape, command_buffer.arguments, computation));"
        },
        {
            "sha": "47e797003ad48dcfa68cc55d5ba81def2cff6b3b",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -103,9 +103,9 @@ TEST_F(CommandBufferSchedulingTest, SingleCommandBuffer) {\n // CHECK: %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> (s32[], s32[]) {\n // CHECK:   %[[P0]] = s32[] parameter(0)\n // CHECK:   %[[P1]] = s32[] parameter(1)\n-// CHECK:   %fusion = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n-// CHECK:   %fusion.1 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.1\n-// CHECK:   ROOT %tuple = (s32[], s32[]) tuple(%fusion, %fusion.1)\n+// CHECK:   %fusion.2 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n+// CHECK:   %fusion.3 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.1\n+// CHECK:   ROOT %tuple = (s32[], s32[]) tuple(%fusion.2, %fusion.3)\n // CHECK: }\n //\n // CHECK: ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n@@ -175,7 +175,7 @@ TEST_F(CommandBufferSchedulingTest, MultipleCommandBuffers) {\n // CHECK:    ROOT {{.*}} = s32[] fusion(%[[F0]], %[[P2]]), kind=kLoop, calls=%fused_computation.1\n // CHECK:  }\n \n-// CHECK:  %command_buffer.2 ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n+// CHECK:  %command_buffer.1 ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n // CHECK:    %[[P0]] = s32[] parameter(0)\n // CHECK:    %[[P1]] = s32[] parameter(1)\n // CHECK:    %[[F2:.+]] = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.2\n@@ -190,7 +190,7 @@ TEST_F(CommandBufferSchedulingTest, MultipleCommandBuffers) {\n // CHECK:    %e = s32[] get-tuple-element(%c), index=1\n // CHECK:    %[[CMD0:.+]] = s32[] call(%a, %b, %d), to_apply=%command_buffer\n // CHECK:    %[[CALL:.+]] = s32[] custom-call(%[[CMD0]], %e), custom_call_target=\"some target\"\n-// CHECK:    %[[CMD1:.+]] = s32[] call(%[[CALL]], %a), to_apply=%command_buffer.2\n+// CHECK:    %[[CMD1:.+]] = s32[] call(%[[CALL]], %a), to_apply=%command_buffer.1\n // CHECK:    ROOT {{.*}} = s32[] custom-call(%[[CMD1]]), custom_call_target=\"some target\"\n // CHECK:  })\";\n \n@@ -439,8 +439,8 @@ TEST_F(CommandBufferSchedulingTest, DoNotCaptureUnmatchedAsyncDone) {\n     CHECK: %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n     CHECK:   %[[P0]] = s32[] parameter(0)\n     CHECK:   %[[P1]] = s32[] parameter(1)\n-    CHECK:   %fusion = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n-    CHECK:   ROOT %fusion.1 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.1\n+    CHECK:   %fusion.2 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n+    CHECK:   ROOT %fusion.3 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.1\n     CHECK: }\n \n     CHECK: ENTRY %main (a: s32[4], b: s32[]) -> s32[] {\n@@ -1375,8 +1375,8 @@ TEST_F(CommandBufferSchedulingTest, MoveGTEs) {\n // CHECK:  %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n // CHECK:    %[[P0]] = s32[] parameter(0)\n // CHECK:    %[[P1]] = s32[] parameter(1)\n-// CHECK:    %fusion0 = s32[] fusion(%[[P0]], %[[P0]]), kind=kLoop, calls=%fused_computation\n-// CHECK:    ROOT %fusion1 = s32[] fusion(%fusion0, %[[P1]]), kind=kLoop, calls=%fused_computation.1\n+// CHECK:    %fusion0.1 = s32[] fusion(%[[P0]], %[[P0]]), kind=kLoop, calls=%fused_computation\n+// CHECK:    ROOT %fusion1.1 = s32[] fusion(%fusion0.1, %[[P1]]), kind=kLoop, calls=%fused_computation.1\n // CHECK:  }\n \n // CHECK:  ENTRY %main (x: s32[]) -> s32[] {"
        },
        {
            "sha": "e86e169822f2bbcea5fe8c0ed2ae7377795332e4",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -366,7 +366,6 @@ class PriorityFusionQueue {\n     }\n \n     gpu_performance_model_cache_.Invalidate(*instruction);\n-    fusion_analysis_cache_.Invalidate(*instruction);\n     fusion_info_cache_.Invalidate(instruction);\n   }\n \n@@ -418,7 +417,8 @@ class PriorityFusionQueue {\n   void OnFusingInstruction(HloInstruction* fusion,\n                            HloInstruction* original_producer,\n                            HloInstruction* original_consumer,\n-                           int64_t original_consumer_operand_index) {\n+                           int64_t original_consumer_operand_index,\n+                           int64_t original_consumer_unique_id) {\n     bool creates_multi_output_fusion = false;\n     {\n       absl::MutexLock lock(&preferred_consumer_mutex_);\n@@ -463,6 +463,7 @@ class PriorityFusionQueue {\n       reachability_->OnInstructionReplaced(/*previous=*/original_consumer,\n                                            /*now=*/fusion);\n       RemoveInstruction(original_consumer);\n+      fusion_analysis_cache_.Invalidate(original_consumer_unique_id);\n     }\n     if (creates_multi_output_fusion) {\n       // After a multi-output fusion was created, we need to rebuild the\n@@ -506,7 +507,6 @@ class PriorityFusionQueue {\n   // Removes data for the instruction.\n   void RemoveInstruction(HloInstruction* instruction) {\n     to_update_priority_.erase(instruction);\n-    fusion_analysis_cache_.Invalidate(*instruction);\n \n     auto reverse_it = reverse_map_.find(instruction);\n     if (reverse_it == reverse_map_.end()) {\n@@ -992,7 +992,7 @@ class PriorityFusionQueue {\n   // The priority queue of producers, implemented as an ordered map, where a\n   // key is a pair: the first element is the priority and the second element is\n   // the unique ID of the instruction to break ties.\n-  using PriorityQueue = std::map<std::pair<Priority, int>, HloInstruction*>;\n+  using PriorityQueue = std::map<std::pair<Priority, int64_t>, HloInstruction*>;\n   PriorityQueue producer_priority_queue_;\n \n   // A reverse map that helps find an instruction in the priority queue.\n@@ -1176,7 +1176,8 @@ absl::StatusOr<bool> PriorityFusion::Run(\n       std::vector<HloInstruction*> consumers =\n           fusion_queue->current_consumers();\n       bool use_multi_output_fusion = preferred_consumer.has_value();\n-\n+      int64_t pre_fusion_producer_id = producer->unique_id();\n+      absl::flat_hash_set<int64_t> pre_fusion_consumer_ids;\n       for (auto* consumer : consumers) {\n         // Don't fuse into single bitcasts. We ignore them in the check\n         // CanFuseWithAllNonBitcastUsers(), so we need to check it here.\n@@ -1193,6 +1194,8 @@ absl::StatusOr<bool> PriorityFusion::Run(\n         int64_t consumer_operand_index = consumer->operand_index(producer);\n \n         fusion_queue->PreFusion(producer, consumer);\n+        int64_t consumer_pre_fusion_id = consumer->unique_id();\n+        pre_fusion_consumer_ids.insert(consumer_pre_fusion_id);\n         auto fusion_instruction =\n             Fuse(producer, consumer, use_multi_output_fusion);\n         auto backend_config_it = block_level_parameters_map.find(consumer);\n@@ -1203,7 +1206,8 @@ absl::StatusOr<bool> PriorityFusion::Run(\n               HloInstruction::FusionKind::kCustom);\n         }\n         fusion_queue->OnFusingInstruction(fusion_instruction, producer,\n-                                          consumer, consumer_operand_index);\n+                                          consumer, consumer_operand_index,\n+                                          consumer_pre_fusion_id);\n \n         changed = true;\n       }\n@@ -1212,6 +1216,7 @@ absl::StatusOr<bool> PriorityFusion::Run(\n       if (use_multi_output_fusion || producer->user_count() == 0) {\n         fusion_queue->InvalidateCaches(producer);\n         fusion_queue->RemoveInstruction(producer);\n+        fusion_analysis_cache_.Invalidate(pre_fusion_producer_id);\n         // When we use ProducerConsumer multi-output fusion, `producer` will\n         // have been removed already.\n         if (!use_multi_output_fusion) {\n@@ -1223,6 +1228,9 @@ absl::StatusOr<bool> PriorityFusion::Run(\n       for (auto* consumer : consumers) {\n         fusion_queue->InvalidateCaches(consumer);\n       }\n+      for (auto consumer_id : pre_fusion_consumer_ids) {\n+        fusion_analysis_cache_.Invalidate(consumer_id);\n+      }\n       TF_RETURN_IF_ERROR(fusion_queue->UpdatePriorities());\n     }\n "
        },
        {
            "sha": "9675d51de21bc7eefaa93b618d4b062959c9c12a",
            "filename": "third_party/xla/xla/service/hlo_module_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_test.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -509,13 +509,19 @@ ENTRY ReduceR3ToR2.v3 {\n     }\n   }\n \n-  // Verify that the next unique ID which the module would have handed out is\n-  // greater than the unique id of any instruction.\n-  int next_id = module_copy->NewUniqueInstructionId();\n+  // Verify that the next unique ID which any computation would have handed out\n+  // is greater than the unique id of any existing instruction in that\n+  // computation.\n+  int32_t next_module_unique_id = module->next_unique_computation_id();\n   for (const HloComputation* computation : module_copy->computations()) {\n+    int32_t next_instruction_id_internal =\n+        computation->next_unique_instruction_internal_id();\n     for (const HloInstruction* instruction : computation->instructions()) {\n-      EXPECT_GT(next_id, instruction->unique_id());\n+      EXPECT_GT(next_instruction_id_internal, instruction->local_id());\n     }\n+    // Also verify that the next module unique id is greater than the module\n+    // unique ids already present in the computation.\n+    EXPECT_GT(next_module_unique_id, computation->unique_id());\n   }\n }\n "
        },
        {
            "sha": "6e7d5809cab379f5704fe57e551a6501c54e6daf",
            "filename": "third_party/xla/xla/service/hlo_module_util.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_util.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -91,10 +91,16 @@ absl::StatusOr<std::unique_ptr<HloModule>> CreateModuleFromProto(\n }\n \n absl::StatusOr<std::unique_ptr<HloModule>> ReadModuleFromBinaryProtoFile(\n-    absl::string_view filename, const DebugOptions& debug_options) {\n+    absl::string_view filename, const DebugOptions& debug_options,\n+    bool remap_instruction_ids) {\n   HloProto proto;\n   TF_RETURN_IF_ERROR(\n       tsl::ReadBinaryProto(tsl::Env::Default(), std::string(filename), &proto));\n+  if (remap_instruction_ids) {\n+    TF_ASSIGN_OR_RETURN(HloModuleProto sanitized_proto,\n+                        HloModule::RemapInstructionIds(proto.hlo_module()));\n+    return CreateModuleFromProto(sanitized_proto, debug_options);\n+  }\n   return CreateModuleFromProto(proto.hlo_module(), debug_options);\n }\n "
        },
        {
            "sha": "fa6263448f344bf95888d95f30d65fa653ff1b53",
            "filename": "third_party/xla/xla/service/hlo_module_util.h",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_util.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -57,10 +57,13 @@ absl::StatusOr<std::unique_ptr<HloModule>> CreateModuleFromProto(\n     bool is_module_post_optimizations = false);\n \n // Reads the proto file in xla.HloProto format, creates and returns the\n-// HloModule.\n+// HloModule. If remap_instruction_ids is true, remaps instruction ids to start\n+// from 0 and increment sequentially. If false, the instruction ids are kept as\n+// they are in the proto.\n absl::StatusOr<std::unique_ptr<HloModule>> ReadModuleFromBinaryProtoFile(\n     absl::string_view filename,\n-    const DebugOptions& debug_options = DebugOptions::default_instance());\n+    const DebugOptions& debug_options = DebugOptions::default_instance(),\n+    bool remap_instruction_ids = true);\n \n // Reads the proto file in xla.HloModule format, creates and returns the\n // HloModule."
        },
        {
            "sha": "9e75345f44bdf16981bc8daf08ae34d89670a360",
            "filename": "third_party/xla/xla/service/instruction_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -527,7 +527,7 @@ class ReversePostOrderFusionQueue : public FusionQueue {\n bool MultiOutputFusionCreatesCycle(HloInstruction* producer,\n                                    HloInstruction* consumer,\n                                    const HloReachabilityMap& reachability) {\n-  absl::flat_hash_set<int> operands;\n+  absl::flat_hash_set<int64_t> operands;\n   auto insert = [&](const HloInstruction* operand) {\n     if (operand == producer) {\n       return false;\n@@ -562,7 +562,7 @@ bool MultiOutputFusionCreatesCycle(HloInstruction* producer,\n   std::vector<HloInstruction*> worklist = producer->users();\n   worklist.insert(worklist.end(), producer->control_successors().begin(),\n                   producer->control_successors().end());\n-  absl::flat_hash_set<int> visits;\n+  absl::flat_hash_set<int64_t> visits;\n   while (!worklist.empty()) {\n     const HloInstruction* user = worklist.back();\n     worklist.pop_back();"
        },
        {
            "sha": "ea54c67b09219df0ce080a5bf471b5822703c9cc",
            "filename": "third_party/xla/xla/service/spmd/shardy/shardy_xla_pass_test.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass_test.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -708,13 +708,13 @@ TEST_F(ShardyXLATest, WhileWithFreeVariables) {\n       ROOT %compare.24 = pred[] compare(s32[] %get-tuple-element.20, s32[] %get-tuple-element.21), direction=LT\n     }\n \n-    ENTRY %main.30 (Arg_0.1: f32[32,96], Arg_1.2: f32[32,96]) -> f32[32,96] {\n+    ENTRY %main.30 (Arg_0.1: f32[32,96], Arg_1.1: f32[32,96]) -> f32[32,96] {\n       %Arg_0.1 = f32[32,96]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n       %constant.3 = s32[] constant(0)\n       %constant.5 = s32[] constant(32)\n       %constant.4 = s32[] constant(1)\n-      %Arg_1.2 = f32[32,96]{1,0} parameter(1), sharding={devices=[2,1,2]<=[4] last_tile_dim_replicate}\n-      %tuple.6 = (f32[32,96]{1,0}, s32[], s32[], s32[], f32[32,96]{1,0}) tuple(f32[32,96]{1,0} %Arg_0.1, s32[] %constant.3, s32[] %constant.5, s32[] %constant.4, f32[32,96]{1,0} %Arg_1.2)\n+      %Arg_1.1 = f32[32,96]{1,0} parameter(1), sharding={devices=[2,1,2]<=[4] last_tile_dim_replicate}\n+      %tuple.6 = (f32[32,96]{1,0}, s32[], s32[], s32[], f32[32,96]{1,0}) tuple(f32[32,96]{1,0} %Arg_0.1, s32[] %constant.3, s32[] %constant.5, s32[] %constant.4, f32[32,96]{1,0} %Arg_1.1)\n       %while.25 = (f32[32,96]{1,0}, s32[], s32[], s32[], f32[32,96]{1,0}) while((f32[32,96]{1,0}, s32[], s32[], s32[], f32[32,96]{1,0}) %tuple.6), condition=%region_1.17, body=%region_0.7\n       %get-tuple-element.27 = s32[] get-tuple-element((f32[32,96]{1,0}, s32[], s32[], s32[], f32[32,96]{1,0}) %while.25), index=1\n       %get-tuple-element.26 = f32[32,96]{1,0} get-tuple-element((f32[32,96]{1,0}, s32[], s32[], s32[], f32[32,96]{1,0}) %while.25), index=0\n@@ -812,10 +812,10 @@ TEST_F(ShardyXLATest, TestUseTuplesTrue) {\n   const char* const hloString = R\"(\n     HloModule pjit_f, buffer_donor={ (1, {}) }, input_output_alias={ {}: (2, {}, must-alias) }, entry_computation_layout={(f32[8,16]{1,0:T(8,128)}, f32[16,32]{1,0:T(8,128)}, f32[8,32]{1,0:T(8,128)})->f32[8,32]{1,0:T(8,128)}}, allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8, frontend_attributes={xla.sdy.use_tuple_args=\"t\"}\n \n-    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.2: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n+    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.1: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n       %Arg_0.1 = f32[8,16]{1,0} parameter(0)\n-      %Arg_1.2 = f32[16,32]{1,0} parameter(1)\n-      %dot.4 = f32[8,32]{1,0} dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+      %Arg_1.1 = f32[16,32]{1,0} parameter(1)\n+      %dot.4 = f32[8,32]{1,0} dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n       %Arg_2.3 = f32[8,32]{1,0} parameter(2)\n       ROOT %add.5 = f32[8,32]{1,0} add(f32[8,32]{1,0} %dot.4, f32[8,32]{1,0} %Arg_2.3)\n     })\";\n@@ -838,10 +838,10 @@ TEST_F(ShardyXLATest, TestUseTuplesTrueNoSetLayout) {\n   const char* const hloString = R\"(\n     HloModule pjit_f, allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8, frontend_attributes={xla.sdy.use_tuple_args=\"t\"}\n \n-    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.2: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n+    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.1: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n       %Arg_0.1 = f32[8,16] parameter(0)\n-      %Arg_1.2 = f32[16,32] parameter(1)\n-      %dot.4 = f32[8,32] dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+      %Arg_1.1 = f32[16,32] parameter(1)\n+      %dot.4 = f32[8,32] dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n       %Arg_2.3 = f32[8,32] parameter(2)\n       ROOT %add.5 = f32[8,32] add(f32[8,32] %dot.4, f32[8,32] %Arg_2.3)\n     })\";\n@@ -870,10 +870,10 @@ TEST_F(ShardyXLATest, TestRunShardingPropagationFalseUseTuplesFalse) {\n   const char* const hloString = R\"(\n     HloModule pjit_f, buffer_donor={ (1, {}) }, input_output_alias={ {}: (2, {}, must-alias) }, entry_computation_layout={(f32[8,16]{1,0:T(8,128)}, f32[16,32]{1,0:T(8,128)}, f32[8,32]{1,0:T(8,128)})->f32[8,32]{1,0:T(8,128)}}, allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8\n \n-    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.2: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n+    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.1: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n       %Arg_0.1 = f32[8,16]{1,0} parameter(0)\n-      %Arg_1.2 = f32[16,32]{1,0} parameter(1)\n-      %dot.4 = f32[8,32]{1,0} dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+      %Arg_1.1 = f32[16,32]{1,0} parameter(1)\n+      %dot.4 = f32[8,32]{1,0} dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n       %Arg_2.3 = f32[8,32]{1,0} parameter(2)\n       ROOT %add.5 = f32[8,32]{1,0} add(f32[8,32]{1,0} %dot.4, f32[8,32]{1,0} %Arg_2.3)\n     })\";\n@@ -897,10 +897,10 @@ TEST_F(ShardyXLATest, TestRunShardingPropagationFalseUseTuplesTrue) {\n   const char* const hloString = R\"(\n     HloModule pjit_f, buffer_donor={ (1, {}) }, input_output_alias={ {}: (2, {}, must-alias) }, entry_computation_layout={(f32[8,16]{1,0:T(8,128)}, f32[16,32]{1,0:T(8,128)}, f32[8,32]{1,0:T(8,128)})->f32[8,32]{1,0:T(8,128)}}, allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8, frontend_attributes={xla.sdy.use_tuple_args=\"t\"}\n \n-    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.2: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n+    ENTRY %main.7 (Arg_0.1: f32[8,16], Arg_1.1: f32[16,32], Arg_2.3: f32[8,32]) -> f32[8,32] {\n       %Arg_0.1 = f32[8,16]{1,0} parameter(0)\n-      %Arg_1.2 = f32[16,32]{1,0} parameter(1)\n-      %dot.4 = f32[8,32]{1,0} dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+      %Arg_1.1 = f32[16,32]{1,0} parameter(1)\n+      %dot.4 = f32[8,32]{1,0} dot(f32[8,16]{1,0} %Arg_0.1, f32[16,32]{1,0} %Arg_1.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n       %Arg_2.3 = f32[8,32]{1,0} parameter(2)\n       ROOT %add.5 = f32[8,32]{1,0} add(f32[8,32]{1,0} %dot.4, f32[8,32]{1,0} %Arg_2.3)\n     })\";\n@@ -911,13 +911,13 @@ TEST_F(ShardyXLATest, TestRunShardingPropagationFalseUseTuplesTrue) {\n   // CHECK-SAME:    entry_computation_layout={((f32[8,16]{1,0:T(8,128)}, f32[16,32]{1,0:T(8,128)}, f32[8,32]{1,0:T(8,128)}))->f32[8,32]{1,0:T(8,128)}},\n   // CHECK-SAME:    allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8\n   //\n-  // CHECK:       ENTRY %main.0 (arg_tuple.1: (f32[8,16], f32[16,32], f32[8,32])) -> f32[8,32] {\n+  // CHECK:       ENTRY %main.1 (arg_tuple.1: (f32[8,16], f32[16,32], f32[8,32])) -> f32[8,32] {\n   // CHECK-NEXT:    %arg_tuple.1 = (f32[8,16], f32[16,32], f32[8,32]) parameter(0)\n-  // CHECK-NEXT:    %get-tuple-element.2 = f32[8,16] get-tuple-element(%arg_tuple.1), index=0\n-  // CHECK-NEXT:    %get-tuple-element.3 = f32[16,32] get-tuple-element(%arg_tuple.1), index=1\n-  // CHECK-NEXT:    %dot.5 = f32[8,32] dot(%get-tuple-element.2, %get-tuple-element.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-  // CHECK-NEXT:    %get-tuple-element.4 = f32[8,32] get-tuple-element(%arg_tuple.1), index=2\n-  // CHECK-NEXT:    ROOT %add.6 = f32[8,32] add(%dot.5, %get-tuple-element.4)\n+  // CHECK-NEXT:    %get-tuple-element.3 = f32[8,16] get-tuple-element(%arg_tuple.1), index=0\n+  // CHECK-NEXT:    %get-tuple-element.4 = f32[16,32] get-tuple-element(%arg_tuple.1), index=1\n+  // CHECK-NEXT:    %dot.1 = f32[8,32] dot(%get-tuple-element.3, %get-tuple-element.4), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  // CHECK-NEXT:    %get-tuple-element.5 = f32[8,32] get-tuple-element(%arg_tuple.1), index=2\n+  // CHECK-NEXT:    ROOT %add.1 = f32[8,32] add(%dot.1, %get-tuple-element.5)\n )\";\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n@@ -945,9 +945,9 @@ ENTRY %main.0 (Arg_0.0: s64[2]) -> s64[2] {\n }\n )\";\n   const char* const expected = R\"(\n-  // CHECK:               ENTRY %main.3 (Arg_0.1: s64[2]) -> s64[2] {\n+  // CHECK:               ENTRY %main.1 (Arg_0.1: s64[2]) -> s64[2] {\n   // CHECK-NEXT:            ROOT %Arg_0.1 = s64[2] parameter(0)\n-  // CHECK-NEXT{LITERAL}:   %custom-call.2 = () custom-call(%Arg_0.1), custom_call_target=\"xla_ffi_python_cpu_callback\",\n+  // CHECK-NEXT{LITERAL}:   %custom-call.1 = () custom-call(%Arg_0.1), custom_call_target=\"xla_ffi_python_cpu_callback\",\n   // CHECK-SAME{LITERAL}:   operand_layout_constraints={s64[2]{0}}, custom_call_has_side_effect=true, api_version=API_VERSION_TYPED_FFI,\n   // CHECK-SAME{LITERAL}:   sharding={{maximal device=0}}\n )\";\n@@ -989,11 +989,11 @@ TEST_F(ShardyXLATest, WhileShardingOnlyOnFreeVariables) {\n       ROOT %compare.24 = pred[] compare(s32[] %get-tuple-element.21, s32[] %constant.23), direction=LT\n     }\n \n-    ENTRY %main.28 (Arg_0.1: f32[32,96], Arg_1.2: f32[32,96]) -> f32[32,96] {\n+    ENTRY %main.28 (Arg_0.1: f32[32,96], Arg_1.1: f32[32,96]) -> f32[32,96] {\n       %Arg_0.1 = f32[32,96]{1,0} parameter(0)\n       %constant.3 = s32[] constant(0)\n-      %Arg_1.2 = f32[32,96]{1,0} parameter(1), sharding={devices=[4,1]<=[4]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@mesh, [{\\\"x\\\", ?}, {?}]>\"}\n-      %custom-call.4 = f32[32,96]{1,0} custom-call(f32[32,96]{1,0} %Arg_1.2), custom_call_target=\"Sharding\", sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>\"}\n+      %Arg_1.1 = f32[32,96]{1,0} parameter(1), sharding={devices=[4,1]<=[4]}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@mesh, [{\\\"x\\\", ?}, {?}]>\"}\n+      %custom-call.4 = f32[32,96]{1,0} custom-call(f32[32,96]{1,0} %Arg_1.1), custom_call_target=\"Sharding\", sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>\"}\n       %tuple.5 = (f32[32,96]{1,0}, s32[], f32[32,96]{1,0}) tuple(f32[32,96]{1,0} %Arg_0.1, s32[] %constant.3, f32[32,96]{1,0} %custom-call.4)\n       %while.25 = (f32[32,96]{1,0}, s32[], f32[32,96]{1,0}) while((f32[32,96]{1,0}, s32[], f32[32,96]{1,0}) %tuple.5), condition=%region_1.18, body=%region_0.6\n       ROOT %get-tuple-element.26 = f32[32,96]{1,0} get-tuple-element((f32[32,96]{1,0}, s32[], f32[32,96]{1,0}) %while.25), index=0"
        },
        {
            "sha": "20c288fea3f2767bc0638942285ae9e27d8c3ffe",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -188,7 +188,7 @@ class SpmdBuilder : public HloComputation::Builder {\n \n   // Map from the currently visiting (old) instruction to new instructions\n   // created during SPMD partitioning.\n-  HloInstructionMap<std::vector<HloInstruction*>> instructions_;\n+  HloInstructionMapInternal<std::vector<HloInstruction*>> instructions_;\n \n   // Maps from each created instruction to a set of dimensions that are from\n   // broadcasts or elementwise ops over broadcasts. This means elements along"
        },
        {
            "sha": "5a931a679394e9e08d3dfcd66163e945de958766",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4208b67d6cd7348f0db331087277809db697c27c/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=4208b67d6cd7348f0db331087277809db697c27c",
            "patch": "@@ -3037,9 +3037,9 @@ OpNameStringToOperandKindAndMode(std::string opstring) {\n // Struct describing the convolution, pointwise and reduction ops in the\n // graph.\n struct OpDescriptor {\n-  int uid;                        // The UID of the op.\n-  std::vector<int> operand_uids;  // The UIDs of the operands of the op that\n-                                  // are part of the graph.\n+  int64_t uid;                        // The UID of the op.\n+  std::vector<int64_t> operand_uids;  // The UIDs of the operands of the op that\n+                                      // are part of the graph.\n   OpMode mode;                    // The mode describing the op.\n   TensorKind operand_kind;        // The kind of a second operand.\n   TensorKind result_kind;         // The kind of the output.\n@@ -3054,13 +3054,13 @@ class OpGraph {\n  public:\n   OpGraph() = default;\n \n-  absl::Status AddOp(int uid, std::vector<int> operand_uids, OpMode mode,\n-                     TensorKind operand_kind, TensorKind result_kind,\n-                     dnn::DataType result_type) {\n+  absl::Status AddOp(int64_t uid, std::vector<int64_t> operand_uids,\n+                     OpMode mode, TensorKind operand_kind,\n+                     TensorKind result_kind, dnn::DataType result_type) {\n     ops_.emplace_back(OpDescriptor({uid, operand_uids, mode, operand_kind,\n                                     result_kind, result_type, false, -1}));\n     // If they exist, the operands are virtual.\n-    for (int operand_uid : operand_uids) {\n+    for (int64_t operand_uid : operand_uids) {\n       auto it = std::find_if(\n           ops_.begin(), ops_.end(),\n           [operand_uid](OpDescriptor op) { return op.uid == operand_uid; });\n@@ -3072,7 +3072,7 @@ class OpGraph {\n     return absl::OkStatus();\n   }\n \n-  absl::StatusOr<OpDescriptor> FindOpDescriptor(int uid) const {\n+  absl::StatusOr<OpDescriptor> FindOpDescriptor(int64_t uid) const {\n     auto it = std::find_if(ops_.begin(), ops_.end(),\n                            [uid](OpDescriptor op) { return op.uid == uid; });\n     if (it == ops_.end()) {\n@@ -3088,7 +3088,7 @@ class OpGraph {\n     return ops_[index];\n   }\n \n-  absl::Status SetSequenceIndex(int uid, int index) {\n+  absl::Status SetSequenceIndex(int64_t uid, int index) {\n     auto it = std::find_if(ops_.begin(), ops_.end(),\n                            [uid](OpDescriptor op) { return op.uid == uid; });\n     if (it == ops_.end()) {\n@@ -3137,15 +3137,15 @@ GetGenericCudnnOperationGraph(\n       dnn::DataType output_type;\n       std::string::size_type m = serialized_graph.find('[', pos);\n       std::string::size_type n = serialized_graph.find(']', pos);\n-      int uid = std::stoi(serialized_graph.substr(pos, m - pos - 1));\n+      int64_t uid = std::stol(serialized_graph.substr(pos, m - pos - 1));\n       std::string data_type_string = serialized_graph.substr(m + 1, n - m - 1);\n       m = serialized_graph.find('(', pos);\n       std::string op_string = serialized_graph.substr(n + 1, m - n - 1);\n-      std::vector<int> operands;\n+      std::vector<int64_t> operands;\n       std::string::size_type l = serialized_graph.find_first_of(\",)\", m + 1);\n       while (l > m + 1) {\n         operands.push_back(\n-            std::stoi(serialized_graph.substr(m + 1, l - m - 1)));\n+            std::stol(serialized_graph.substr(m + 1, l - m - 1)));\n         if (serialized_graph[l] == ')') {\n           break;\n         }\n@@ -3329,7 +3329,7 @@ GetGenericCudnnOperationGraph(\n     TF_ASSIGN_OR_RETURN(op_descriptor, op_graph.OpDescriptorAt(op_index));\n     std::vector<OpDescriptor> preceding_ops;\n     preceding_ops.reserve(op_descriptor.operand_uids.size());\n-    for (int operand_uid : op_descriptor.operand_uids) {\n+    for (int64_t operand_uid : op_descriptor.operand_uids) {\n       preceding_ops.emplace_back(\n           op_graph.FindOpDescriptor(operand_uid).value());\n     }"
        }
    ],
    "stats": {
        "total": 1389,
        "additions": 873,
        "deletions": 516
    }
}