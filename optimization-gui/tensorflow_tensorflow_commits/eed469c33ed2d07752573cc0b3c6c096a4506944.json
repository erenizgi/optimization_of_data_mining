{
    "author": "KanishAnand",
    "message": "Remove undefined support of tuple shardings from `NumTiles()` and `TiledDataRank()` methods. These methods should only be defined for non-tuple shardings, when sharding is tuple `tile_assignment_` is not populated therefore these would be returning undefined results.\n\nPiperOrigin-RevId: 845191404",
    "sha": "eed469c33ed2d07752573cc0b3c6c096a4506944",
    "files": [
        {
            "sha": "e958a1fc966bf888fca16c6266be1651566ee6f3",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eed469c33ed2d07752573cc0b3c6c096a4506944/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eed469c33ed2d07752573cc0b3c6c096a4506944/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc?ref=eed469c33ed2d07752573cc0b3c6c096a4506944",
            "patch": "@@ -1099,23 +1099,12 @@ int64_t HloSharding::TotalNumTiles() const {\n }\n \n int64_t HloSharding::NumTiles() const {\n-  if (IsTileMaximal()) {\n-    return 1;\n-  }\n-  CHECK(!IsManual());\n-  CHECK(!IsUnknown());\n-  return Product(absl::Span<const int64_t>(tile_assignment_.dimensions())\n-                     .subspan(0, TiledDataRank()));\n-}\n-\n-int64_t HloSharding::NumTilesLeaf() const {\n-  DCHECK(!IsTuple());\n   if (IsTileMaximalLeaf()) {\n     return 1;\n   }\n   CHECK(!IsManualLeaf() && !IsUnknownLeaf());\n   return Product(absl::Span<const int64_t>(tile_assignment_.dimensions())\n-                     .subspan(0, TiledDataRankLeaf()));\n+                     .subspan(0, TiledDataRank()));\n }\n \n int64_t HloSharding::NumTiles(absl::Span<const int64_t> dims) const {"
        },
        {
            "sha": "93a58a4cdfb4d6067271b43f64a9898d5adac633",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eed469c33ed2d07752573cc0b3c6c096a4506944/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eed469c33ed2d07752573cc0b3c6c096a4506944/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=eed469c33ed2d07752573cc0b3c6c096a4506944",
            "patch": "@@ -545,9 +545,8 @@ class HloSharding {\n   // Gets the total number of tiles including subgroups and partial replication.\n   int64_t TotalNumTiles() const;\n   // Gets the number of tiles. If it has partial replication, this will not\n-  // equal the device count.\n+  // equal the device count. This method is not defined for tuple shardings.\n   int64_t NumTiles() const;\n-  int64_t NumTilesLeaf() const;\n   // Like NumTiles() but considers only some specific dimensions passed as\n   // argument\n   int64_t NumTiles(absl::Span<const int64_t> dims) const;\n@@ -587,17 +586,8 @@ class HloSharding {\n   }\n \n   // Returns the data rank for tiled sharding. It doesn't include subgroup dims.\n+  // This method is not defined for tuple shardings.\n   int64_t TiledDataRank() const {\n-    CHECK(IsTiled());\n-    int64_t rank = tile_assignment_.num_dimensions();\n-    if (ReplicateOnLastTileDim()) {\n-      rank--;\n-    }\n-    rank -= subgroup_types_.size();\n-    return rank;\n-  }\n-  int64_t TiledDataRankLeaf() const {\n-    DCHECK(!IsTuple());\n     CHECK(IsTiledLeaf());\n     int64_t rank = tile_assignment_.num_dimensions();\n     if (ReplicateOnLastTileDim()) {"
        },
        {
            "sha": "a2ce251f7ab382e84877b440c2c6a03d5bc06ec9",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eed469c33ed2d07752573cc0b3c6c096a4506944/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eed469c33ed2d07752573cc0b3c6c096a4506944/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=eed469c33ed2d07752573cc0b3c6c096a4506944",
            "patch": "@@ -361,7 +361,7 @@ static bool IsLeafShardingMoreSpecific(const HloSharding& lhs,\n     return false;\n   }\n   if (!rhs.IsTileMaximalLeaf()) {\n-    return lhs.NumTilesLeaf() > rhs.NumTilesLeaf();\n+    return lhs.NumTiles() > rhs.NumTiles();\n   }\n   // If we are not replicated then only tiled (not tile maximal) shardings\n   // can improve us.\n@@ -2864,7 +2864,10 @@ std::optional<HloSharding> ReturnImprovedShardingImpl(\n   if (from.IsManual()) {\n     return std::nullopt;\n   }\n-  int64_t sharding_tiles = from.NumTiles();\n+  int64_t sharding_tiles;\n+  if (!from.IsTuple()) {\n+    sharding_tiles = from.NumTiles();\n+  }\n   if (MergeSharding(*to_improved, &from, may_combine_partial_sharding)) {\n     // Override existing tiled sharding only when the new sharding is compatible\n     // with the existing one. This avoids unexpected resharding when `sharding`"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 8,
        "deletions": 26
    }
}