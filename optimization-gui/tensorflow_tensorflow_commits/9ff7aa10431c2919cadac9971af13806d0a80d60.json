{
    "author": "Moerafaat",
    "message": "[XLA:GPU] Re-enable TMA for B200. Scaled-dot test broke because of accidental enablement of warp-specialization during the TMA roll-out.\n\nPiperOrigin-RevId: 837578776",
    "sha": "9ff7aa10431c2919cadac9971af13806d0a80d60",
    "files": [
        {
            "sha": "2ff38f4db936927e400e301a6a3d5d460a5be758",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ff7aa10431c2919cadac9971af13806d0a80d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ff7aa10431c2919cadac9971af13806d0a80d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=9ff7aa10431c2919cadac9971af13806d0a80d60",
            "patch": "@@ -126,7 +126,9 @@ std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n \n   // TODO(b/449668102): Currently only supporting warp specialization on\n   // Blackwell+. Potentially extend support to Hopper.\n-  if (!compute_capability.IsAtLeastBlackwell()) {\n+  if (!debug_options_\n+           .xla_gpu_experimental_enable_triton_warp_specialization() ||\n+      !compute_capability.IsAtLeastBlackwell()) {\n     return tma_parameterized_configs;\n   }\n   std::vector<TritonGemmConfig> warp_specialized_configs;"
        },
        {
            "sha": "26f1a4723c88206dc934a87f1546ac7a8cf4d70c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ff7aa10431c2919cadac9971af13806d0a80d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ff7aa10431c2919cadac9971af13806d0a80d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=9ff7aa10431c2919cadac9971af13806d0a80d60",
            "patch": "@@ -1808,6 +1808,41 @@ TEST_F(GemmFusionAutotunerTest, TmaRunCorrectlyForDotsOfBroadcasts) {\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n }\n \n+// TODO(b/449668102): Remove this test once warp specialization is enabled by\n+// default.\n+TEST_F(GemmFusionAutotunerTest, WarpSpecializationIsOffByDefault) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Not supported on ROCm.\";\n+  }\n+\n+  std::unique_ptr<VerifiedHloModule> module = ParseAndReturnVerifiedModule(R\"(\n+    ENTRY e {\n+      p0 = f32[64,64] parameter(0)\n+      p1 = f32[64,64] parameter(1)\n+      ROOT r = f32[64,64] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    })\")\n+                                                  .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const std::vector<TritonGemmConfig> configs,\n+      GetPossibleMatmulAutotuneTritonConfigs(\n+          *Cast<HloDotInstruction>(\n+              module->entry_computation()->root_instruction()),\n+          GetCudaComputeCapability(), GetToolkitVersion(),\n+          GetDebugOptionsForTest(), &mlir_context_));\n+\n+  std::set<TritonGemmConfig> configs_set(configs.begin(), configs.end());\n+\n+  auto any_ws_allowed = [](const std::vector<TritonGemmConfig>& configs) {\n+    return std::any_of(configs.begin(), configs.end(),\n+                       [](const TritonGemmConfig& config) {\n+                         return config.is_warp_specialization_allowed;\n+                       });\n+  };\n+  EXPECT_FALSE(any_ws_allowed(configs));\n+}\n+\n TEST_F(GemmFusionAutotunerTest, ReadsOverrideFile) {\n   if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";"
        },
        {
            "sha": "095e0a3c550265d4054b1ed81eb0ce91f2595336",
            "filename": "third_party/xla/xla/stream_executor/gpu/tma_metadata.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ff7aa10431c2919cadac9971af13806d0a80d60/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ff7aa10431c2919cadac9971af13806d0a80d60/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc?ref=9ff7aa10431c2919cadac9971af13806d0a80d60",
            "patch": "@@ -465,7 +465,7 @@ bool IsTmaAvailableForDevice(\n     const stream_executor::DeviceDescription& device_info) {\n   if (auto* cuda_cc =\n           device_info.gpu_compute_capability().cuda_compute_capability()) {\n-    return cuda_cc->IsHopper();\n+    return cuda_cc->IsAtLeastHopper();\n   }\n   return false;\n }"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 39,
        "deletions": 2
    }
}