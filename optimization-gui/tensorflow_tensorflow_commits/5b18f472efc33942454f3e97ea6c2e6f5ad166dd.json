{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 819010361",
    "sha": "5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
    "files": [
        {
            "sha": "625307a0e288e2338c704c033d7c6900109323bf",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/async_importer.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 15,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -150,7 +150,9 @@ absl::StatusOr<mlir::Operation*> ImportOldStyleAsyncDone(\n   assert(operands.size() == 1 &&\n          \"*-done ops must take only a single async_bundle operand\");\n   auto async_start = operands[0].getDefiningOp<mlir::mhlo::AsyncStartOp>();\n-  if (!async_start) return InvalidArgument(\"*-start requires *-done as input\");\n+  if (!async_start) {\n+    return InvalidArgument(\"*-start requires *-done as input\");\n+  }\n   attributes.push_back(builder->getNamedAttr(\n       \"called_computation\",\n       mlir::FlatSymbolRefAttr::get(builder->getContext(),\n@@ -167,13 +169,11 @@ absl::StatusOr<mlir::Operation*> ImportOldStyleAsyncDone(\n     auto op = builder->create<mlir::mhlo::AsyncDoneOp>(loc, result_type,\n                                                        operands, attributes);\n     return {op};\n-  } else {\n-    if (useBundleResult) result_type = async_bundle.getTypes()[1];\n-    auto op = builder->create<mlir::mhlo::AsyncDoneOp>(\n-        loc, Untuple(result_type), operands, attributes);\n-    return CreateTupleFromOpResults(builder, loc, op.getOperation(),\n-                                    result_type);\n   }\n+  if (useBundleResult) result_type = async_bundle.getTypes()[1];\n+  auto op = builder->create<mlir::mhlo::AsyncDoneOp>(loc, Untuple(result_type),\n+                                                     operands, attributes);\n+  return CreateTupleFromOpResults(builder, loc, op.getOperation(), result_type);\n }\n \n }  // namespace\n@@ -213,8 +213,9 @@ absl::StatusOr<mlir::Operation*> ImportSend(\n     // `send-done` ops to use the new-style async API, we need to reorder the\n     // arguments to be in (args, token, sync flag) order.\n     auto result_types = mlir::cast<mlir::TupleType>(result_type).getTypes();\n-    if (result_types.size() != 3)\n+    if (result_types.size() != 3) {\n       return InvalidArgument(\"send should return a 3-tuple\");\n+    }\n     auto async_arg_type = mlir::TupleType::get(\n         builder->getContext(), {result_types[0], result_types[2]});\n     auto async_bundled_tuple = mlir::TupleType::get(\n@@ -276,8 +277,9 @@ absl::StatusOr<mlir::Operation*> ImportRecv(\n   // Currently only consolidates async recv with result, 0-result recv uses old\n   // style, unclear if this support is needed.\n   auto result_types = llvm::cast<mlir::TupleType>(result_type).getTypes();\n-  if (result_types.size() != 3)\n+  if (result_types.size() != 3) {\n     return InvalidArgument(\"recv should return a 3-tuple\");\n+  }\n \n   bool isPipelined =\n       instruction->users().front()->opcode() != HloOpcode::kRecvDone;\n@@ -351,14 +353,17 @@ absl::StatusOr<mlir::Operation*> ImportAllGatherStart(\n       builder->getI64IntegerAttr(all_gather_start->all_gather_dimension())));\n   attributes.push_back(\n       ConvertReplicaGroups(all_gather_start->replica_groups(), builder));\n-  if (all_gather_start->channel_id().has_value())\n+  if (all_gather_start->channel_id().has_value()) {\n     attributes.push_back(stablehlo::ConvertChannelHandle(\n         all_gather_start->channel_id().value(), builder));\n-  if (all_gather_start->use_global_device_ids())\n+  }\n+  if (all_gather_start->use_global_device_ids()) {\n     attributes.push_back(ConvertUseGlobalDeviceIds(builder));\n-  if (all_gather_start->operands().size() > 1)\n+  }\n+  if (all_gather_start->operands().size() > 1) {\n     return InvalidArgument(\n         \"Async tuple all-gather is not supported in StableHLO\");\n+  }\n \n   if (!llvm::isa<mlir::TupleType>(result_type)) {\n     // Async AllGather's output type is bundle<input_type,output_type>\n@@ -384,14 +389,17 @@ absl::StatusOr<mlir::Operation*> ImportAllReduceStart(\n   auto all_reduce_start = Cast<HloAllReduceInstruction>(instruction);\n   attributes.push_back(\n       ConvertReplicaGroups(all_reduce_start->replica_groups(), builder));\n-  if (all_reduce_start->channel_id().has_value())\n+  if (all_reduce_start->channel_id().has_value()) {\n     attributes.push_back(stablehlo::ConvertChannelHandle(\n         all_reduce_start->channel_id().value(), builder));\n-  if (all_reduce_start->use_global_device_ids())\n+  }\n+  if (all_reduce_start->use_global_device_ids()) {\n     attributes.push_back(ConvertUseGlobalDeviceIds(builder));\n-  if (all_reduce_start->operands().size() > 1)\n+  }\n+  if (all_reduce_start->operands().size() > 1) {\n     return InvalidArgument(\n         \"Async tuple all-reduce is not supported in StableHLO\");\n+  }\n \n   if (!llvm::isa<mlir::TupleType>(result_type)) {\n     // Async AllReduce's output type is bundle<input_type,output_type>"
        },
        {
            "sha": "8b68c112604104417186c4cc7fc68c557aaa2995",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/attribute_importer.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 11,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fattribute_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fattribute_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fattribute_importer.cc?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -98,7 +98,9 @@ mlir::NamedAttribute ConvertChannelHandle(const ChannelHandle& channel,\n mlir::NamedAttribute ConvertChannelHandle(std::optional<int64_t> channel_id,\n                                           mlir::Builder* builder) {\n   ChannelHandle channel_handle;\n-  if (channel_id) channel_handle.set_handle(*channel_id);\n+  if (channel_id) {\n+    channel_handle.set_handle(*channel_id);\n+  }\n   return stablehlo::ConvertChannelHandle(channel_handle, builder);\n }\n \n@@ -256,7 +258,9 @@ mlir::ArrayAttr ConvertOutputOperandAliasing(\n \n mlir::ArrayAttr ConvertPrecisionConfig(const PrecisionConfig* config,\n                                        mlir::Builder* builder) {\n-  if (!config) return {};\n+  if (!config) {\n+    return {};\n+  }\n \n   // TODO(b/129709049) The HLO text format elides this in the all DEFAULT\n   // case and the parser sticks it in. Maybe we should too.\n@@ -297,7 +301,9 @@ mlir::stablehlo::ResultAccuracyAttr ConvertResultAccuracy(\n \n mlir::ArrayAttr ConvertPrecisionConfig(const PrecisionConfig* config,\n                                        mlir::Builder* builder) {\n-  if (!config) return {};\n+  if (!config) {\n+    return {};\n+  }\n \n   // TODO(b/129709049) The HLO text format elides this in the all DEFAULT\n   // case and the parser sticks it in. Maybe we should too.\n@@ -425,9 +431,10 @@ absl::StatusOr<mlir::mhlo::CustomCallApiVersion> ConvertCustomCallApiVersion(\n                       stablehlo::ConvertCustomCallApiVersion(api_version));\n   auto mhlo_api_version = mlir::mhlo::symbolizeCustomCallApiVersion(\n       mlir::stablehlo::stringifyCustomCallApiVersion(stablehlo_api_version));\n-  if (!mhlo_api_version.has_value())\n+  if (!mhlo_api_version.has_value()) {\n     return InvalidArgument(\"Unknown CustomCallApiVersion enum value #%d\",\n                            api_version);\n+  }\n   return mhlo_api_version.value();\n }\n \n@@ -442,7 +449,9 @@ mlir::NamedAttribute ConvertChannelHandle(const ChannelHandle& channel,\n mlir::NamedAttribute ConvertChannelHandle(std::optional<int64_t> channel_id,\n                                           mlir::Builder* builder) {\n   ChannelHandle channel_handle;\n-  if (channel_id) channel_handle.set_handle(*channel_id);\n+  if (channel_id) {\n+    channel_handle.set_handle(*channel_id);\n+  }\n   return ConvertChannelHandle(channel_handle, builder);\n }\n \n@@ -461,8 +470,9 @@ mlir::NamedAttribute ConvertReplicaGroups(\n   std::vector<int64_t> attr(num_groups * group_size, -1);\n   for (int i = 0; i < num_groups; ++i) {\n     int index = i * group_size;\n-    for (const int64_t& id : replica_groups[i].replica_ids())\n+    for (const int64_t& id : replica_groups[i].replica_ids()) {\n       attr[index++] = id;\n+    }\n   }\n   auto type = mlir::RankedTensorType::get({num_groups, group_size},\n                                           builder->getIntegerType(64));\n@@ -492,9 +502,10 @@ absl::StatusOr<mlir::ArrayAttr> ExtractLayoutsFromShapes(\n     const absl::Span<const Shape> shapes_with_layouts, mlir::Builder* builder) {\n   std::vector<mlir::Attribute> layouts;\n   for (auto& shape_and_layout : shapes_with_layouts) {\n-    if (shape_and_layout.IsTuple())\n+    if (shape_and_layout.IsTuple()) {\n       return Unimplemented(\n           \"Layout support for nested tuples is not implemented.\");\n+    }\n     // XLA can have invalid layout for certain values (such as token types).\n     // These are imported as empty layout in MHLO.\n     if (!shape_and_layout.IsArray()) {\n@@ -511,23 +522,28 @@ absl::StatusOr<mlir::ArrayAttr> ExtractLayoutsFromShapes(\n     }\n \n     const xla::Layout& xla_layout = shape_and_layout.layout();\n-    if (!xla_layout.tiles().empty())\n+    if (!xla_layout.tiles().empty()) {\n       return Unimplemented(\"Tiled layout is not supported yet\");\n-    if (xla_layout.memory_space() != xla::Layout::kDefaultMemorySpace)\n+    }\n+    if (xla_layout.memory_space() != xla::Layout::kDefaultMemorySpace) {\n       return Unimplemented(\n           \"Layout support for non-default memory space is not yet implemented\");\n+    }\n \n     llvm::SmallVector<int64_t> layout;\n-    for (int64_t dim_index : xla_layout.minor_to_major())\n+    for (int64_t dim_index : xla_layout.minor_to_major()) {\n       layout.push_back(dim_index);\n+    }\n     layouts.push_back(builder->getIndexTensorAttr(layout));\n   }\n   return builder->getArrayAttr(layouts);\n }\n \n absl::StatusOr<mlir::ArrayAttr> ExtractLayoutsFromTuple(\n     const Shape shape, mlir::Builder* builder) {\n-  if (!shape.IsTuple()) return InvalidArgument(\"Expected shape to be Tuple\");\n+  if (!shape.IsTuple()) {\n+    return InvalidArgument(\"Expected shape to be Tuple\");\n+  }\n   return ExtractLayoutsFromShapes(shape.tuple_shapes(), builder);\n }\n "
        },
        {
            "sha": "09f82597e2ac3ccfbbf9e49eaee2efe6850167c4",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/attribute_importer.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fattribute_importer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fattribute_importer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fattribute_importer.h?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -134,12 +134,12 @@ mlir::NamedAttribute ConvertUseGlobalDeviceIds(mlir::Builder* builder);\n // Extracts layouts from shapes and converts it into layout attributes (array of\n // rank-1 index tensors). Returns an error if any of the shapes is a tuple.\n absl::StatusOr<mlir::ArrayAttr> ExtractLayoutsFromShapes(\n-    const absl::Span<const Shape> shapes_with_layouts, mlir::Builder* builder);\n+    absl::Span<const Shape> shapes_with_layouts, mlir::Builder* builder);\n \n // Extracts the layouts of each element from a tuple shape and returns them as\n // an array of rank-1 index tensors. Returns an error in presence of nested\n // tuple shapes.\n-absl::StatusOr<mlir::ArrayAttr> ExtractLayoutsFromTuple(const xla::Shape shape,\n+absl::StatusOr<mlir::ArrayAttr> ExtractLayoutsFromTuple(xla::Shape shape,\n                                                         mlir::Builder* builder);\n \n }  // namespace xla"
        },
        {
            "sha": "f71b9561b49551e2b416d99488c5c1328b3d5a00",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/custom_call_importer.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fcustom_call_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fcustom_call_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fcustom_call_importer.cc?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -151,11 +151,10 @@ mlir::Type getQuantizedType(mlir::DictionaryAttr& backend_config) {\n     return mlir::quant::UniformQuantizedPerAxisType::get(\n         is_signed, storage_type, expressed_type, scales, zero_points,\n         quantization_dimension, storage_min, storage_max);\n-  } else {\n-    return mlir::quant::UniformQuantizedType::get(\n-        is_signed, storage_type, expressed_type, scales[0], zero_points[0],\n-        storage_min, storage_max);\n   }\n+  return mlir::quant::UniformQuantizedType::get(\n+      is_signed, storage_type, expressed_type, scales[0], zero_points[0],\n+      storage_min, storage_max);\n }\n \n absl::StatusOr<mlir::Operation*> ImportCustomCallAsOp("
        },
        {
            "sha": "057385cc2c11252aefd153a761b528aeda228bb9",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/hlo_function_importer.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 33,
            "changes": 94,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -232,15 +232,17 @@ absl::StatusOr<mlir::Value> createConstantZeroLike(mlir::Value operand,\n \n   LLVM_DEBUG(llvm::dbgs() << \"CreateConstantZeroLike: \" << operand << \", \"\n                           << type << '\\n');\n-  if (type.hasStaticShape())\n+  if (type.hasStaticShape()) {\n     return builder\n         ->create<mlir::stablehlo::ConstantOp>(loc, builder->getZeroAttr(type))\n         ->getResult(0);\n+  }\n \n   // Note: Currently this only supports a single bounded dimension.\n-  if (!mlir::hlo::hasSingleBoundedDimension(type))\n+  if (!mlir::hlo::hasSingleBoundedDimension(type)) {\n     return Internal(\n         \"Currently HLO to MHLO only supports a single bounded dimension.\");\n+  }\n \n   auto bounded_dim = std::distance(type.getShape().begin(),\n                                    llvm::find_if(type.getShape(), [](auto dim) {\n@@ -286,17 +288,23 @@ void HloFunctionImporter::ReplaceBlockArgumentsWithImplicitOperands(\n \n static bool IsNestedTupleInData(Type type) {\n   auto tuple_type = mlir::dyn_cast<mlir::TupleType>(type);\n-  if (!tuple_type) return false;\n+  if (!tuple_type) {\n+    return false;\n+  }\n \n   assert(llvm::isa<mlir::stablehlo::TokenType>(tuple_type.getType(1)) &&\n          \"Infeed: Non token type\");\n   auto data_type = tuple_type.getType(0);\n \n   auto data_tuple_type = mlir::dyn_cast<mlir::TupleType>(data_type);\n-  if (!data_tuple_type) return false;\n+  if (!data_tuple_type) {\n+    return false;\n+  }\n \n   for (auto child_type : data_tuple_type.getTypes()) {\n-    if (mlir::isa<mlir::TupleType>(child_type)) return true;\n+    if (mlir::isa<mlir::TupleType>(child_type)) {\n+      return true;\n+    }\n   }\n \n   return false;\n@@ -682,9 +690,10 @@ absl::StatusOr<Value> HloFunctionImporter::ImportInstructions(\n     mlir::SymbolTable& symbol_table, mlir::OpBuilder* builder,\n     bool flatten_computation_args_result) {\n   mlir::Block* block = builder->getBlock();\n-  if (block == nullptr)\n+  if (block == nullptr) {\n     return InvalidArgument(\n         \"ImportInstructions requires a valid block in the builder\");\n+  }\n \n   HloFunctionImporter importer(symbol_table, {}, builder,\n                                flatten_computation_args_result);\n@@ -697,9 +706,10 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstruction(\n     mlir::SymbolTable& symbol_table, mlir::OpBuilder* builder,\n     bool flatten_computation_args_result, DynamicShapeHandlingMode mode) {\n   mlir::Block* block = builder->getBlock();\n-  if (block == nullptr)\n+  if (block == nullptr) {\n     return InvalidArgument(\n         \"ImportInstructions requires a valid block in the builder\");\n+  }\n \n   HloFunctionImporter importer(symbol_table, {}, builder,\n                                flatten_computation_args_result);\n@@ -759,7 +769,9 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n \n       const Literal& literal = constant->literal();\n       auto attr = CreateDenseElementsAttrFromLiteral(literal, *builder_);\n-      if (!attr.ok()) return attr.status();\n+      if (!attr.ok()) {\n+        return attr.status();\n+      }\n       mlir::Operation* new_operation =\n           func_builder->create<mlir::stablehlo::ConstantOp>(loc, attr.value());\n       for (auto attr : attributes) {\n@@ -800,7 +812,8 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n             ->create<mlir::mhlo::AsyncStartOp>(loc, bundle_result_type,\n                                                operands, attributes)\n             .getOperation();\n-      } else if (instruction->opcode() == HloOpcode::kAsyncUpdate) {\n+      }\n+      if (instruction->opcode() == HloOpcode::kAsyncUpdate) {\n         auto bundle_result_type = mlir::mhlo::AsyncBundleType::get(\n             context_, llvm::cast<mlir::TupleType>(result_type).getTypes());\n         // XLA Feature -- MHLO Only\n@@ -848,7 +861,8 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n                       .getOperation();\n \n         return CreateTupleFromOpResults(func_builder, loc, op, result_type);\n-      } else if (instruction->opcode() == HloOpcode::kBatchNormInference) {\n+      }\n+      if (instruction->opcode() == HloOpcode::kBatchNormInference) {\n         return func_builder\n             ->create<mlir::stablehlo::BatchNormInferenceOp>(\n                 loc, result_type, operands, attributes)\n@@ -1053,9 +1067,10 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n       auto collective_broadcast = Cast<HloChannelInstruction>(instruction);\n       attributes.push_back(ConvertReplicaGroups(\n           collective_broadcast->replica_groups(), builder_));\n-      if (collective_broadcast->channel_id().has_value())\n+      if (collective_broadcast->channel_id().has_value()) {\n         attributes.push_back(stablehlo::ConvertChannelHandle(\n             collective_broadcast->channel_id().value(), builder_));\n+      }\n       return func_builder\n           ->create<mlir::stablehlo::CollectiveBroadcastOp>(loc, result_type,\n                                                            operands, attributes)\n@@ -1066,9 +1081,10 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n       auto collective_permute = Cast<HloChannelInstruction>(instruction);\n       attributes.push_back(ConvertSourceTargetPairs(\n           collective_permute->source_target_pairs(), builder_));\n-      if (collective_permute->channel_id().has_value())\n+      if (collective_permute->channel_id().has_value()) {\n         attributes.push_back(stablehlo::ConvertChannelHandle(\n             collective_permute->channel_id().value(), builder_));\n+      }\n       return func_builder\n           ->create<mlir::stablehlo::CollectivePermuteOp>(loc, result_type,\n                                                          operands, attributes)\n@@ -1143,9 +1159,10 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n         } else {\n           mlir::Attribute attr =\n               mlir::parseAttribute(raw_backend_config, builder_->getContext());\n-          if (!mlir::isa<mlir::DictionaryAttr>(attr))\n+          if (!mlir::isa<mlir::DictionaryAttr>(attr)) {\n             return Internal(\n                 \"Couldn't parse backend config into a dictionary attribute\");\n+          }\n \n           attributes.push_back(builder_->getNamedAttr(\"backend_config\", attr));\n         }\n@@ -1157,7 +1174,9 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n       if (custom_call->HasLiteral()) {\n         const Literal& literal = custom_call->literal();\n         auto attr = CreateDenseElementsAttrFromLiteral(literal, *builder_);\n-        if (!attr.ok()) return attr.status();\n+        if (!attr.ok()) {\n+          return attr.status();\n+        }\n         attributes.push_back(\n             builder_->getNamedAttr(\"mhlo.literal\", attr.value()));\n       }\n@@ -1219,8 +1238,9 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n       attributes.push_back(ConvertComparisonDirection(compare->direction()));\n       auto default_type = Comparison::DefaultComparisonType(\n           compare->operand(0)->shape().element_type());\n-      if (compare->type() != default_type)\n+      if (compare->type() != default_type) {\n         attributes.push_back(ConvertComparisonType(compare->type()));\n+      }\n       return func_builder\n           ->create<mlir::stablehlo::CompareOp>(loc, result_type, operands,\n                                                attributes)\n@@ -1551,11 +1571,13 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n           builder_->getI64IntegerAttr(all_gather->all_gather_dimension())));\n       attributes.push_back(\n           ConvertReplicaGroups(all_gather->replica_groups(), builder_));\n-      if (all_gather->channel_id().has_value())\n+      if (all_gather->channel_id().has_value()) {\n         attributes.push_back(stablehlo::ConvertChannelHandle(\n             all_gather->channel_id().value(), builder_));\n-      if (all_gather->use_global_device_ids())\n+      }\n+      if (all_gather->use_global_device_ids()) {\n         attributes.push_back(ConvertUseGlobalDeviceIds(builder_));\n+      }\n       auto all_gather_op = func_builder->create<mlir::stablehlo::AllGatherOp>(\n           loc, result_types, operands, attributes);\n       if (result_tuple_ty) {\n@@ -1582,11 +1604,13 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n \n       attributes.push_back(\n           ConvertReplicaGroups(all_reduce->replica_groups(), builder_));\n-      if (all_reduce->channel_id().has_value())\n+      if (all_reduce->channel_id().has_value()) {\n         attributes.push_back(stablehlo::ConvertChannelHandle(\n             all_reduce->channel_id().value(), builder_));\n-      if (all_reduce->use_global_device_ids())\n+      }\n+      if (all_reduce->use_global_device_ids()) {\n         attributes.push_back(ConvertUseGlobalDeviceIds(builder_));\n+      }\n       auto all_reduce_op = func_builder->create<mlir::stablehlo::AllReduceOp>(\n           loc, result_types, operands, attributes);\n       TF_RETURN_IF_ERROR(ImportAsRegion(*all_reduce->to_apply(),\n@@ -1851,11 +1875,13 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n           builder_->getI64IntegerAttr(reduce_scatter->scatter_dimension())));\n       attributes.push_back(\n           ConvertReplicaGroups(reduce_scatter->replica_groups(), builder_));\n-      if (reduce_scatter->channel_id().has_value())\n+      if (reduce_scatter->channel_id().has_value()) {\n         attributes.push_back(stablehlo::ConvertChannelHandle(\n             reduce_scatter->channel_id().value(), builder_));\n-      if (reduce_scatter->use_global_device_ids())\n+      }\n+      if (reduce_scatter->use_global_device_ids()) {\n         attributes.push_back(ConvertUseGlobalDeviceIds(builder_));\n+      }\n       auto reduce_scatter_op =\n           func_builder->create<mlir::stablehlo::ReduceScatterOp>(\n               loc, result_type, operands, attributes);\n@@ -2012,12 +2038,11 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n             ->create<mlir::stablehlo::OrOp>(loc, result_type, operands,\n                                             attributes)\n             .getOperation();\n-      } else {\n-        return func_builder\n-            ->create<mlir::stablehlo::AddOp>(loc, result_type, operands,\n-                                             attributes)\n-            .getOperation();\n       }\n+      return func_builder\n+          ->create<mlir::stablehlo::AddOp>(loc, result_type, operands,\n+                                           attributes)\n+          .getOperation();\n     }\n     case HloOpcode::kAfterAll: {\n       // HLO AfterAll ops without any token input are used to create a token.\n@@ -2027,12 +2052,11 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n             ->create<mlir::stablehlo::CreateTokenOp>(loc, result_type, operands,\n                                                      attributes)\n             .getOperation();\n-      } else {\n-        return func_builder\n-            ->create<mlir::stablehlo::AfterAllOp>(loc, result_type, operands,\n-                                                  attributes)\n-            .getOperation();\n       }\n+      return func_builder\n+          ->create<mlir::stablehlo::AfterAllOp>(loc, result_type, operands,\n+                                                attributes)\n+          .getOperation();\n     }\n \n     case HloOpcode::kConvert: {\n@@ -2379,7 +2403,9 @@ mlir::DenseIntElementsAttr HloFunctionImporter::ConvertDimensions(\n     absl::Span<const int64_t> op_dimensions) {\n   llvm::SmallVector<APInt, 8> dimensions;\n   dimensions.reserve(op_dimensions.size());\n-  for (auto value : op_dimensions) dimensions.emplace_back(APInt(64, value));\n+  for (auto value : op_dimensions) {\n+    dimensions.emplace_back(APInt(64, value));\n+  }\n \n   return DenseIntElementsAttr::get(\n       RankedTensorType::get(dimensions.size(), builder_->getIntegerType(64)),\n@@ -2517,7 +2543,9 @@ mlir::Attribute ConvertSharding(const HloSharding& sharding,\n mlir::Attribute ConvertSharding(const OpSharding& sharding,\n                                 mlir::Builder* builder) {\n   auto hlo_sharding = HloSharding::FromProto(sharding);\n-  if (!hlo_sharding.ok()) return {};\n+  if (!hlo_sharding.ok()) {\n+    return {};\n+  }\n   return ConvertSharding(hlo_sharding.value(), builder);\n }\n "
        },
        {
            "sha": "2f65724b80aaae3c5b61b6a00737aef6fb5167a1",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/hlo_utils.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.cc?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -139,13 +139,17 @@ absl::StatusOr<mlir::MemRefType> ConvertTensorShapeToMemRefType(\n     const Shape& shape, mlir::Builder builder) {\n   auto element_type_or =\n       ConvertPrimitiveTypeToMlirType(shape.element_type(), builder);\n-  if (!element_type_or.ok()) return element_type_or.status();\n+  if (!element_type_or.ok()) {\n+    return element_type_or.status();\n+  }\n \n   using mlir::MemRefType;\n   auto dimensions = shape.dimensions();\n   llvm::SmallVector<int64_t, 4> array(dimensions.begin(), dimensions.end());\n   auto permutation_or = GetPermutationIfAvailable(shape, builder);\n-  if (!permutation_or.ok()) return permutation_or.status();\n+  if (!permutation_or.ok()) {\n+    return permutation_or.status();\n+  }\n   return MemRefType::get(array, element_type_or.value(),\n                          permutation_or.value());\n }\n@@ -194,9 +198,10 @@ mlir::Value CreateTupleValue(mlir::OpBuilder* func_builder, mlir::Location loc,\n   }\n \n   llvm::SmallVector<mlir::Value> flatten_sub_values;\n-  for (auto child_type : tuple_type.getTypes())\n+  for (auto child_type : tuple_type.getTypes()) {\n     flatten_sub_values.push_back(\n         CreateTupleValue(func_builder, loc, flatten_values, child_type));\n+  }\n \n   return func_builder->create<mlir::stablehlo::TupleOp>(loc, flatten_sub_values)\n       .getResult();\n@@ -206,7 +211,9 @@ mlir::Operation* CreateTupleFromOpResults(mlir::OpBuilder* func_builder,\n                                           mlir::Location loc,\n                                           mlir::Operation* op,\n                                           mlir::Type type) {\n-  if (!mlir::isa<mlir::TupleType>(type)) return op;\n+  if (!mlir::isa<mlir::TupleType>(type)) {\n+    return op;\n+  }\n \n   mlir::ValueRange flattened_results_ref(op->getResults());\n   auto result ="
        },
        {
            "sha": "b58e082cb42dae86984b9ab9b3032d4feca54aad",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/hlo_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.h?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -53,7 +53,7 @@ absl::StatusOr<mlir::DenseElementsAttr> CreateDenseElementsAttrFromLiteral(\n // Creates an DenseIntElementsAttr using the elements of the vector and the\n // optional shape.\n mlir::DenseIntElementsAttr CreateDenseIntElementsAttrFromVector(\n-    const llvm::ArrayRef<int64_t> vector, mlir::Builder builder,\n+    llvm::ArrayRef<int64_t> vector, mlir::Builder builder,\n     llvm::ArrayRef<int64_t> shape = {});\n \n // Converts the given XLA shape for tensors to the template MLIR type.\n@@ -62,7 +62,9 @@ static absl::StatusOr<TypeT> ConvertTensorShapeToType(const Shape& xla_ty,\n                                                       mlir::Builder builder) {\n   auto element_type_or =\n       ConvertPrimitiveTypeToMlirType(xla_ty.element_type(), builder);\n-  if (!element_type_or.ok()) return element_type_or.status();\n+  if (!element_type_or.ok()) {\n+    return element_type_or.status();\n+  }\n \n   bool is_bounded_dynamic = false;\n   int64_t rank = xla_ty.dimensions().size();"
        },
        {
            "sha": "47b06d7f3ba22e570eec4634e7958923c6ed3e63",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/module_attributes_importer.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 12,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fmodule_attributes_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5b18f472efc33942454f3e97ea6c2e6f5ad166dd/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fmodule_attributes_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fmodule_attributes_importer.cc?ref=5b18f472efc33942454f3e97ea6c2e6f5ad166dd",
            "patch": "@@ -75,17 +75,19 @@ mlir::ArrayAttr ConvertCrossProgramPrefetches(\n                                     param_map[index] = arg_index++;\n                                   });\n     }\n-    for (const auto& [parameter, index, alt_memory_offset] : prefetches)\n+    for (const auto& [parameter, index, alt_memory_offset] : prefetches) {\n       shapes.push_back(mlir::mhlo::CrossProgramPrefetchAttr::get(\n           builder->getContext(),\n           original_param_index_to_flattened_arg_index[parameter][index],\n           /*indices=*/{}, alt_memory_offset));\n+    }\n   } else {\n-    for (const auto& [parameter, index, alt_memory_offset] : prefetches)\n+    for (const auto& [parameter, index, alt_memory_offset] : prefetches) {\n       shapes.push_back(mlir::mhlo::CrossProgramPrefetchAttr::get(\n           builder->getContext(), parameter,\n           llvm::ArrayRef<int64_t>(index.data(), index.size()),\n           alt_memory_offset));\n+    }\n   }\n \n   return mlir::ArrayAttr::get(builder->getContext(), shapes);\n@@ -236,12 +238,14 @@ void ImportFrontendAttributes(const HloModule& hlo_module,\n                               mlir::ModuleOp module, mlir::Builder builder) {\n   if (!hlo_module.frontend_attributes().map().empty()) {\n     llvm::SmallVector<mlir::NamedAttribute, 4> frontend_attributes;\n-    for (const auto& [k, v] : hlo_module.frontend_attributes().map())\n+    for (const auto& [k, v] : hlo_module.frontend_attributes().map()) {\n       frontend_attributes.push_back(\n           builder.getNamedAttr(k, builder.getStringAttr(v)));\n-    if (!frontend_attributes.empty())\n+    }\n+    if (!frontend_attributes.empty()) {\n       module->setAttr(xla::kMhloFrontendAttributes,\n                       builder.getDictionaryAttr(frontend_attributes));\n+    }\n   }\n }\n \n@@ -279,10 +283,11 @@ void ImportNumReplicas(const HloModule& hlo_module, mlir::ModuleOp module,\n \n void ImportSpmdOutputSharding(const xla::HloModule& hlo_module,\n                               mlir::ModuleOp module, mlir::Builder builder) {\n-  if (hlo_module.has_spmd_output_sharding())\n+  if (hlo_module.has_spmd_output_sharding()) {\n     module->setAttr(\n         xla::kMhloSpmdOutputSharding,\n         ConvertSharding(hlo_module.spmd_output_sharding(), &builder));\n+  }\n }\n \n void ImportSpmdParametersShardings(const HloModule& hlo_module,\n@@ -294,10 +299,12 @@ void ImportSpmdParametersShardings(const HloModule& hlo_module,\n     parameter_shardings.reserve(hlo_module.spmd_parameters_shardings().size());\n     for (const auto& root_sharding : hlo_module.spmd_parameters_shardings()) {\n       llvm::ArrayRef<HloSharding> shardings = root_sharding;\n-      if (root_sharding.IsTuple() && flatten_computation_args_result)\n+      if (root_sharding.IsTuple() && flatten_computation_args_result) {\n         shardings = root_sharding.tuple_elements();\n-      for (const auto& sharding : shardings)\n+      }\n+      for (const auto& sharding : shardings) {\n         parameter_shardings.push_back(ConvertSharding(sharding, &builder));\n+      }\n     }\n     module->setAttr(xla::kMhloSpmdParametersShardings,\n                     builder.getArrayAttr(parameter_shardings));\n@@ -319,7 +326,9 @@ mlir::DictionaryAttr AppendAutoLayoutModeAttribute(mlir::Builder builder,\n   llvm::SmallVector<mlir::NamedAttribute> attrs;\n   if (dict) {\n     for (auto attr : dict.getValue()) {\n-      if (attr.getName() != xla::kMhloLayoutMode) attrs.push_back(attr);\n+      if (attr.getName() != xla::kMhloLayoutMode) {\n+        attrs.push_back(attr);\n+      }\n     }\n   }\n   attrs.push_back(builder.getNamedAttr(xla::kMhloLayoutMode,\n@@ -342,9 +351,12 @@ void ImportParameterLayoutModes(mlir::func::FuncOp main,\n   CHECK_EQ(parameter_shapes.size(), main.getNumArguments());\n   for (size_t i = 0; i < main.getNumArguments(); ++i) {\n     const Shape& shape = *parameter_shapes[i];\n-    if (shape.IsTuple() || (shape.IsArray() && shape.dimensions().size() == 0))\n+    if (shape.IsTuple() || (shape.IsArray() && shape.dimensions().empty())) {\n+      continue;\n+    }\n+    if (LayoutUtil::HasAnyLayout(*parameter_shapes[i])) {\n       continue;\n-    if (LayoutUtil::HasAnyLayout(*parameter_shapes[i])) continue;\n+    }\n     main.setArgAttrs(\n         i, AppendAutoLayoutModeAttribute(builder, main.getArgAttrDict(i)));\n   }\n@@ -362,9 +374,12 @@ void ImportResultLayoutModes(mlir::func::FuncOp main,\n   CHECK_EQ(result_shapes.size(), main.getNumResults());\n   for (size_t i = 0; i < main.getNumResults(); ++i) {\n     const Shape& shape = *result_shapes[i];\n-    if (shape.IsTuple() || (shape.IsArray() && shape.dimensions().size() == 0))\n+    if (shape.IsTuple() || (shape.IsArray() && shape.dimensions().empty())) {\n       continue;\n-    if (LayoutUtil::HasAnyLayout(shape)) continue;\n+    }\n+    if (LayoutUtil::HasAnyLayout(shape)) {\n+      continue;\n+    }\n     main.setResultAttrs(\n         i, AppendAutoLayoutModeAttribute(builder, main.getResultAttrDict(i)));\n   }"
        }
    ],
    "stats": {
        "total": 241,
        "additions": 158,
        "deletions": 83
    }
}