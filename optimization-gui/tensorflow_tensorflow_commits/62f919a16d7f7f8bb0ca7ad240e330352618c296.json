{
    "author": "akuegel",
    "message": "Update documentation to remove deprecated --xla_gpu_graph_level flag.\n\nPiperOrigin-RevId: 805785151",
    "sha": "62f919a16d7f7f8bb0ca7ad240e330352618c296",
    "files": [
        {
            "sha": "65a534147db8c6dfd82794af8b3d735ca41d407f",
            "filename": "third_party/xla/docs/flags_guidance.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62f919a16d7f7f8bb0ca7ad240e330352618c296/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62f919a16d7f7f8bb0ca7ad240e330352618c296/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fflags_guidance.md?ref=62f919a16d7f7f8bb0ca7ad240e330352618c296",
            "patch": "@@ -81,7 +81,8 @@ Flag                                            | Type                 | Notes\n | :---- | :---- | :----- |\n | `xla_gpu_enable_latency_hiding_scheduler` | Boolean (true/false) |This flag enables latency hiding schedulers to overlap asynchronous communication with computation efficiently. The default value is False. |\n | `xla_gpu_enable_triton_gemm` | Boolean (true/false) | Use Triton-based matrix multiplication. |\n-| `xla_gpu_graph_level` | Flag (0-3) | The legacy flag for setting GPU graph level. Use xla_gpu_enable_command_buffer in new use cases. 0 = off; 1 = capture fusions and memcpys; 2 = capture gemms; 3 = capture convolutions. |\n+| `xla_gpu_enable_command_buffer` | List of CommandBufferCmdType | Which kind of\n+commands should be captured in command buffers. |\n | `xla_gpu_all_reduce_combine_threshold_bytes` | Integer (bytes) | These flags tune when to combine multiple small AllGather / ReduceScatter / AllReduce into one big AllGather / ReduceScatter / AllReduce to reduce time spent on cross-device communication. For example, for the AllGather / ReduceScatter thresholds on a Transformer-based workload, consider tuning them high enough so as to combine at least a Transformer Layerâ€™s weight AllGather / ReduceScatter. By default, the combine_threshold_bytes is set to 256. |\n | `xla_gpu_all_gather_combine_threshold_bytes` | Integer (bytes) | See xla_gpu_all_reduce_combine_threshold_bytes above. |\n | `xla_gpu_reduce_scatter_combine_threshold_bytes` | Integer (bytes) | See xla_gpu_all_reduce_combine_threshold_bytes above. |"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}