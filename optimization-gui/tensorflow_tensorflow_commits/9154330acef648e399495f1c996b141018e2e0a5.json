{
    "author": "hyeontaek",
    "message": "[IFRT] Introduce `ShardingSpec` and subclasses\n\n`ShardingSpec` and its subclasses are a device-agnostic alternative to `Sharding` and its subclasses. Instead of using a concrete device or device list, it only tracks the number of devices in a partitioning scheme. This makes it easy to serialize/deserialize it when a device assignment may change (e.g., model export/import roundtrip) and when carrying the partitioning information to a non-IFRT-client context where concrete IFRT devices are undefined (e.g., non-client internals of transports).\n\n`Sharding` and subclasses untouched in this CL. They will eventually be migrated to use `ShardingSpec` and subclasses internally, essentially becoming a tuple of (`ShardingSpecRef`, `DeviceListRef`, and `MemoryKind`), plus a few helper methods.\n\nPiperOrigin-RevId: 850740873",
    "sha": "9154330acef648e399495f1c996b141018e2e0a5",
    "files": [
        {
            "sha": "28c87ae6526e1f0b09c76b67504259db258763c8",
            "filename": "third_party/xla/xla/python/ifrt/BUILD",
            "status": "modified",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -40,6 +40,7 @@ cc_library(\n         \"executable_serdes.cc\",\n         \"shape.cc\",\n         \"sharding.cc\",\n+        \"sharding_spec.cc\",\n         \"topology.cc\",\n         \"tuple.cc\",\n         \"value.cc\",\n@@ -64,6 +65,7 @@ cc_library(\n         \"executable_serdes.h\",\n         \"shape.h\",\n         \"sharding.h\",\n+        \"sharding_spec.h\",\n         \"topology.h\",\n         \"tuple.h\",\n         \"value.h\",\n@@ -88,6 +90,7 @@ cc_library(\n         \":serdes_version\",\n         \":shape_proto_cc\",\n         \":sharding_proto_cc\",\n+        \":sharding_spec_proto_cc\",\n         \":user_context\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n@@ -402,6 +405,21 @@ xla_cc_test(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"sharding_spec_test\",\n+    size = \"small\",\n+    srcs = [\"sharding_spec_test.cc\"],\n+    deps = [\n+        \":ifrt\",\n+        \"//xla/python/ifrt/ir:sharding_param\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/hash:hash_testing\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"test_util\",\n     testonly = True,\n@@ -763,6 +781,48 @@ cc_library(\n     alwayslink = True,\n )\n \n+cc_library(\n+    name = \"sharding_spec_serdes\",\n+    srcs = [\"sharding_spec_serdes.cc\"],\n+    compatible_with = get_compatible_with_portable(),\n+    visibility = internal_visibility([\n+        \":friends\",\n+        \":internal\",\n+        \":users\",\n+    ]),\n+    deps = [\n+        \":ifrt\",\n+        \":serdes\",\n+        \":serdes_version\",\n+        \":sharding_spec_serdes_proto_cc\",\n+        \"//xla/python/ifrt/ir:sharding_param\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//llvm:Support\",\n+    ],\n+    alwayslink = True,\n+)\n+\n+xla_cc_test(\n+    name = \"sharding_spec_serdes_test\",\n+    srcs = [\"sharding_spec_serdes_test.cc\"],\n+    deps = [\n+        \":ifrt\",\n+        \":serdes\",\n+        \":serdes_proto_cc\",\n+        \":serdes_test_util\",\n+        \":serdes_version\",\n+        \":sharding_spec_serdes\",\n+        \"//xla/python/ifrt/ir:sharding_param\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"sharding_serdes_test\",\n     srcs = [\"sharding_serdes_test.cc\"],\n@@ -993,6 +1053,31 @@ tf_proto_library(\n     ]),\n )\n \n+tf_proto_library(\n+    name = \"sharding_spec_proto\",\n+    srcs = [\"sharding_spec.proto\"],\n+    protodeps = [\":serdes_proto\"],\n+    visibility = internal_visibility([\n+        \":friends\",\n+        \":internal\",\n+        \":users\",\n+    ]),\n+)\n+\n+tf_proto_library(\n+    name = \"sharding_spec_serdes_proto\",\n+    srcs = [\"sharding_spec_serdes.proto\"],\n+    protodeps = [\n+        \":shape_proto\",\n+        \"//xla/python/ifrt/ir:sharding_param_proto\",\n+    ],\n+    visibility = internal_visibility([\n+        \":friends\",\n+        \":internal\",\n+        \":users\",\n+    ]),\n+)\n+\n cc_library(\n     name = \"plugin_program\",\n     srcs = [\"plugin_program.cc\"],"
        },
        {
            "sha": "5fe18fcded6d7bffc6ced47fb2010ce7fdb2e69c",
            "filename": "third_party/xla/xla/python/ifrt/sharding_spec.cc",
            "status": "added",
            "additions": 641,
            "deletions": 0,
            "changes": 641,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,641 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <optional>\n+#include <ostream>\n+#include <string>\n+#include <utility>\n+#include <variant>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/hash/hash.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/python/ifrt/index.h\"\n+#include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/ir/sharding_param.h\"\n+#include \"xla/python/ifrt/serdes.h\"\n+#include \"xla/python/ifrt/serdes_version.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding_spec.pb.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+\n+namespace {\n+\n+// Returns if `sharding_param` indicates a fully replicated sharding.\n+bool ComputeIsFullyReplicated(const ShardingParam& sharding_param) {\n+  return llvm::all_of(sharding_param.dim_shards(),\n+                      [](auto shards) { return shards == 1; });\n+}\n+\n+// Iterates the major-to-minor Cartesian product of a Span of containers of the\n+// same type.\n+//\n+// For example, for {1, 2, 3} x {4, 5}, it iterates in the order of\n+//   {1, 4}, {1, 5}, {2, 4}, {2, 5}, {3, 4}, {3, 5}\n+// The values are copied into the result vectors.\n+template <typename ContainerT>\n+class MajorToMinorIter {\n+ public:\n+  using IteratorT = typename ContainerT::const_iterator;\n+  using ValueT = typename ContainerT::value_type;\n+\n+  // Returns the iterator at the begin of the Cartesian product.\n+  static MajorToMinorIter<ContainerT> cbegin(\n+      absl::Span<const ContainerT> containers) {\n+    std::vector<IteratorT> iters;\n+    iters.reserve(containers.size());\n+    for (const ContainerT& container : containers) {\n+      iters.push_back(container.cbegin());\n+    }\n+    return MajorToMinorIter(containers, std::move(iters));\n+  }\n+\n+  // Returns the vector of values at the iteration point.\n+  std::vector<ValueT> operator*() const {\n+    std::vector<ValueT> result;\n+    result.reserve(iters_.size());\n+    for (const auto& iter : iters_) {\n+      result.push_back(*iter);\n+    }\n+    return result;\n+  }\n+\n+  // Moves to the next.\n+  void operator++() {\n+    for (int i = iters_.size() - 1; i >= 0; --i) {\n+      ++iters_[i];\n+      if (iters_[i] != containers_[i].end()) {\n+        break;\n+      }\n+      if (i != 0) {\n+        // Carry over.\n+        iters_[i] = containers_[i].begin();\n+      }\n+    }\n+  }\n+\n+  // Returns whether the iterator has reached the end.\n+  // Note: Due to the implementation of ++, not all iters_ is end().\n+  bool IsEnd() const {\n+    return iters_.empty() || iters_[0] == containers_[0].end();\n+  }\n+\n+ private:\n+  MajorToMinorIter(absl::Span<const ContainerT> containers,\n+                   std::vector<IteratorT> iters)\n+      : containers_(containers), iters_(iters) {\n+    DCHECK_EQ(iters.size(), containers.size());\n+  }\n+\n+  absl::Span<const ContainerT> containers_;\n+  std::vector<IteratorT> iters_;\n+};\n+\n+// Returns the indices of the tiles.\n+//\n+// For example, when `dim_shards` is {2, 3}, the result is\n+//   {0, 0}, {0, 1}, {0, 2}, {1, 0}, {1, 1}, {1, 2}\n+std::vector<Index> GetTileIndices(absl::Span<const int64_t> dim_shards) {\n+  std::vector<std::vector<int64_t>> indices;\n+  indices.reserve(dim_shards.size());\n+  for (const int64_t dim_shard : dim_shards) {\n+    std::vector<int64_t> index(dim_shard);\n+    absl::c_iota(index, 0);\n+    indices.push_back(std::move(index));\n+  }\n+\n+  std::vector<Index> result;\n+  int64_t shard_count =\n+      absl::c_accumulate(dim_shards, 1, std::multiplies<int64_t>());\n+  result.reserve(shard_count);\n+  for (auto iter = MajorToMinorIter<std::vector<int64_t>>::cbegin(indices);\n+       !iter.IsEnd(); ++iter) {\n+    result.push_back(Index(*iter));\n+  }\n+  return result;\n+}\n+\n+}  // namespace\n+\n+char ShardingSpec::ID = 0;\n+char SingleDeviceShardingSpec::ID = 0;\n+char OpaqueShardingSpec::ID = 0;\n+char ConcreteShardingSpec::ID = 0;\n+char ConcreteEvenShardingSpec::ID = 0;\n+char ShardingParamShardingSpec::ID = 0;\n+\n+ShardingSpec::ShardingSpec(int num_shards, bool is_fully_replicated)\n+    : num_shards_(num_shards), is_fully_replicated_(is_fully_replicated) {}\n+\n+bool ShardingSpec::operator==(const ShardingSpec& other) const {\n+  if (this == &other) {\n+    return true;\n+  }\n+  return num_shards_ == other.num_shards_ && HasSamePartitioning(other);\n+}\n+\n+absl::StatusOr<ShardingSpecRef> ShardingSpec::FromProto(\n+    const ShardingSpecProto& sharding_spec_proto) {\n+  return Deserialize<ShardingSpec>(\n+      sharding_spec_proto.serialized_sharding_spec(), /*options=*/nullptr);\n+}\n+\n+absl::Status ShardingSpec::ToProto(ShardingSpecProto& sharding_spec_proto,\n+                                   SerDesVersion version) const {\n+  // `ShardingSpecProto` does not store its own version. It delegates the\n+  // details to SerDes of the `ShardingSpec` subclasses.\n+  auto options = std::make_unique<SerializeOptions>(version);\n+  return Serialize(*this, std::move(options),\n+                   *sharding_spec_proto.mutable_serialized_sharding_spec());\n+}\n+\n+std::ostream& operator<<(std::ostream& os, const ShardingSpec& sharding_spec) {\n+  return os << sharding_spec.DebugString();\n+}\n+\n+std::unique_ptr<SingleDeviceShardingSpec> SingleDeviceShardingSpec::Create() {\n+  return std::unique_ptr<SingleDeviceShardingSpec>(\n+      new SingleDeviceShardingSpec());\n+}\n+\n+SingleDeviceShardingSpec::SingleDeviceShardingSpec()\n+    : llvm::RTTIExtends<SingleDeviceShardingSpec, ShardingSpec>(\n+          /*num_shards=*/1, /*is_fully_replicated=*/true) {}\n+\n+absl::StatusOr<Shape> SingleDeviceShardingSpec::GetShardShape(\n+    const Shape& shape) const {\n+  return shape;\n+}\n+\n+bool SingleDeviceShardingSpec::HasSamePartitioning(\n+    const ShardingSpec& other) const {\n+  if (this == &other) {\n+    return true;\n+  }\n+  return llvm::isa<SingleDeviceShardingSpec>(&other);\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>>\n+SingleDeviceShardingSpec::Disassemble(const Shape& shape) const {\n+  return std::vector<std::pair<Shape, ShardingSpecRef>>{\n+      {shape, SingleDeviceShardingSpec::Create()}};\n+}\n+\n+absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+SingleDeviceShardingSpec::Disassemble(const DynamicShape& dynamic_shape) const {\n+  return std::vector<std::pair<DynamicShape, ShardingSpecRef>>{\n+      {dynamic_shape, SingleDeviceShardingSpec::Create()}};\n+}\n+\n+absl::StatusOr<std::vector<IndexDomain>> SingleDeviceShardingSpec::IndexDomains(\n+    const Shape& shape) const {\n+  return std::vector<IndexDomain>{IndexDomain(shape)};\n+}\n+\n+std::string SingleDeviceShardingSpec::DebugString() const {\n+  return \"SingleDeviceShardingSpec()\";\n+}\n+\n+void SingleDeviceShardingSpec::Hash(absl::HashState state) const {\n+  absl::HashState::combine(std::move(state), num_shards_);\n+}\n+\n+std::unique_ptr<OpaqueShardingSpec> OpaqueShardingSpec::Create(int num_shards) {\n+  return std::unique_ptr<OpaqueShardingSpec>(\n+      new OpaqueShardingSpec(num_shards));\n+}\n+\n+OpaqueShardingSpec::OpaqueShardingSpec(int num_shards)\n+    : llvm::RTTIExtends<OpaqueShardingSpec, ShardingSpec>(\n+          num_shards, /*is_fully_replicated=*/false) {}\n+\n+absl::StatusOr<Shape> OpaqueShardingSpec::GetShardShape(\n+    const Shape& shape) const {\n+  return InvalidArgument(\n+      \"OpaqueShardingSpec does not have shard shape information\");\n+}\n+\n+bool OpaqueShardingSpec::HasSamePartitioning(const ShardingSpec& other) const {\n+  if (this == &other) {\n+    return true;\n+  }\n+  // If the objects are not the same, we cannot tell whether the two\n+  // OpaqueShardingSpecs are using the same logical partitioning.\n+  return this == &other;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>>\n+OpaqueShardingSpec::Disassemble(const Shape& shape) const {\n+  return InvalidArgument(\n+      \"OpaqueShardingSpec does not have shard shape information\");\n+}\n+\n+absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+OpaqueShardingSpec::Disassemble(const DynamicShape& dynamic_shape) const {\n+  return InvalidArgument(\n+      \"OpaqueShardingSpec does not have shard shape information\");\n+}\n+\n+absl::StatusOr<std::vector<IndexDomain>> OpaqueShardingSpec::IndexDomains(\n+    const Shape& shape) const {\n+  return InvalidArgument(\n+      \"OpaqueShardingSpec does not have index domain information\");\n+}\n+\n+std::string OpaqueShardingSpec::DebugString() const {\n+  return absl::StrFormat(\"OpaqueShardingSpec(num_shards: %d)\", num_shards_);\n+}\n+\n+void OpaqueShardingSpec::Hash(absl::HashState state) const {\n+  absl::HashState::combine(std::move(state), num_shards_);\n+}\n+\n+std::unique_ptr<ConcreteShardingSpec> ConcreteShardingSpec::Create(\n+    Shape shape, std::vector<Shape> shard_shapes,\n+    std::optional<std::vector<xla::ifrt::IndexDomain>> index_domains) {\n+  int num_shards = shard_shapes.size();\n+  return std::unique_ptr<ConcreteShardingSpec>(new ConcreteShardingSpec(\n+      num_shards, std::move(shape), std::move(shard_shapes),\n+      std::move(index_domains)));\n+}\n+\n+std::unique_ptr<ConcreteShardingSpec> ConcreteShardingSpec::Create(\n+    DynamicShape dynamic_shape,\n+    std::vector<DynamicShape> shard_dynamic_shapes) {\n+  int num_shards = shard_dynamic_shapes.size();\n+  return std::unique_ptr<ConcreteShardingSpec>(new ConcreteShardingSpec(\n+      num_shards, std::move(dynamic_shape), std::move(shard_dynamic_shapes)));\n+}\n+\n+ConcreteShardingSpec::ConcreteShardingSpec(\n+    int num_shards, Shape shape, std::vector<Shape> shard_shapes,\n+    std::optional<std::vector<xla::ifrt::IndexDomain>> index_domains)\n+    : llvm::RTTIExtends<ConcreteShardingSpec, ShardingSpec>(\n+          num_shards, /*is_fully_replicated=*/false),\n+      shape_(std::move(shape)),\n+      shard_shapes_(std::move(shard_shapes)),\n+      index_domains_(std::move(index_domains)) {\n+  // If all per-shard shapes are the same, cache this shape for\n+  // `GetShardShape()`. Ideally, users should have used\n+  // `ConcreteEvenShardingSpec` for such a case, but there are existing use\n+  // cases that instantiate `ConcreteShardingSpec` from a list of per-shard\n+  // shapes without checking for identical per-shard shapes.\n+  const auto& static_shard_shapes = std::get<std::vector<Shape>>(shard_shapes_);\n+  bool identical = true;\n+  for (int i = 1; i < static_shard_shapes.size(); ++i) {\n+    if (static_shard_shapes[i] != static_shard_shapes[0]) {\n+      identical = false;\n+      break;\n+    }\n+  }\n+  if (identical && !static_shard_shapes.empty()) {\n+    shard_shape_ = static_shard_shapes[0];\n+  }\n+}\n+\n+ConcreteShardingSpec::ConcreteShardingSpec(\n+    int num_shards, DynamicShape dynamic_shape,\n+    std::vector<DynamicShape> shard_dynamic_shapes)\n+    : llvm::RTTIExtends<ConcreteShardingSpec, ShardingSpec>(\n+          num_shards, /*is_fully_replicated=*/false),\n+      shape_(std::move(dynamic_shape)),\n+      shard_shapes_(std::move(shard_dynamic_shapes)) {}\n+\n+absl::StatusOr<Shape> ConcreteShardingSpec::GetShardShape(\n+    const Shape& shape) const {\n+  if (shard_shape_.has_value()) {\n+    return *shard_shape_;\n+  }\n+  return InvalidArgument(\n+      \"ConcreteShardingSpec does not have a fixed shard shape\");\n+}\n+\n+bool ConcreteShardingSpec::HasSamePartitioning(\n+    const ShardingSpec& other) const {\n+  if (this == &other) {\n+    return true;\n+  }\n+  const auto* other_concrete_sharding_spec =\n+      llvm::dyn_cast<ConcreteShardingSpec>(&other);\n+  if (!other_concrete_sharding_spec) {\n+    return false;\n+  }\n+  return shape_ == other_concrete_sharding_spec->shape_ &&\n+         shard_shapes_ == other_concrete_sharding_spec->shard_shapes_;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>>\n+ConcreteShardingSpec::Disassemble(const Shape& shape) const {\n+  if (!has_static_shape()) {\n+    return InvalidArgument(\n+        \"ConcreteShardingSpec holds dynamic shape, but was asked \"\n+        \"to disassemble static shape %s\",\n+        shape.DebugString());\n+  }\n+  if (shape != std::get<Shape>(shape_)) {\n+    return InvalidArgument(\n+        \"ConcreteShardingSpec can only disassemble shape %s, but was asked \"\n+        \"to disassemble shape %s\",\n+        std::get<Shape>(shape_).DebugString(), shape.DebugString());\n+  }\n+  const std::vector<Shape>& shard_shapes =\n+      std::get<std::vector<Shape>>(shard_shapes_);\n+  std::vector<std::pair<Shape, ShardingSpecRef>> result;\n+  result.reserve(shard_shapes.size());\n+  for (const auto& shard_shape : shard_shapes) {\n+    result.push_back({shard_shape, SingleDeviceShardingSpec::Create()});\n+  }\n+  return result;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+ConcreteShardingSpec::Disassemble(const DynamicShape& dynamic_shape) const {\n+  if (!has_dynamic_shape()) {\n+    return InvalidArgument(\n+        \"ConcreteShardingSpec holds static shape, but was asked \"\n+        \"to disassemble dynamic shape %s\",\n+        dynamic_shape.DebugString());\n+  }\n+  if (dynamic_shape != std::get<DynamicShape>(shape_)) {\n+    return InvalidArgument(\n+        \"ConcreteShardingSpec can only disassemble dynamic shape %s, but was \"\n+        \"asked to disassemble dynamic shape %s\",\n+        std::get<DynamicShape>(shape_).DebugString(),\n+        dynamic_shape.DebugString());\n+  }\n+  const std::vector<DynamicShape>& shard_dynamic_shapes =\n+      std::get<std::vector<DynamicShape>>(shard_shapes_);\n+  std::vector<std::pair<DynamicShape, ShardingSpecRef>> result;\n+  result.reserve(shard_dynamic_shapes.size());\n+  for (const auto& shard_dynamic_shape : shard_dynamic_shapes) {\n+    result.push_back({shard_dynamic_shape, SingleDeviceShardingSpec::Create()});\n+  }\n+  return result;\n+}\n+\n+absl::StatusOr<std::vector<IndexDomain>> ConcreteShardingSpec::IndexDomains(\n+    const Shape& shape) const {\n+  if (!index_domains_.has_value()) {\n+    return InvalidArgument(\n+        \"ConcreteShardingSpec does not have index domain information\");\n+  }\n+  return *index_domains_;\n+}\n+\n+std::string ConcreteShardingSpec::DebugString() const {\n+  return std::visit(\n+      [this](const auto& shape, const auto& shard_shapes) {\n+        return absl::StrFormat(\n+            \"ConcreteShardingSpec(num_shards: %d, shape: %s, \"\n+            \"shard_shapes: [%s], index_domains: %s)\",\n+            num_shards_, shape.DebugString(),\n+            absl::StrJoin(shard_shapes, \",\",\n+                          [](std::string* out, const auto& shard_shape) {\n+                            absl::StrAppend(out, shard_shape.DebugString());\n+                          }),\n+            index_domains_.has_value()\n+                ? absl::StrCat(\"[\", absl::StrJoin(*index_domains_, \",\"), \"]\")\n+                : \"<nullopt>\");\n+      },\n+      shape_, shard_shapes_);\n+}\n+\n+void ConcreteShardingSpec::Hash(absl::HashState state) const {\n+  absl::HashState::combine(std::move(state), num_shards_, shape_,\n+                           shard_shapes_);\n+}\n+\n+std::unique_ptr<ConcreteEvenShardingSpec> ConcreteEvenShardingSpec::Create(\n+    int num_shards, Shape shape, Shape shard_shape, bool is_fully_replicated) {\n+  return std::unique_ptr<ConcreteEvenShardingSpec>(new ConcreteEvenShardingSpec(\n+      num_shards, std::move(shape), std::move(shard_shape),\n+      is_fully_replicated));\n+}\n+\n+ConcreteEvenShardingSpec::ConcreteEvenShardingSpec(int num_shards, Shape shape,\n+                                                   Shape shard_shape,\n+                                                   bool is_fully_replicated)\n+    : llvm::RTTIExtends<ConcreteEvenShardingSpec, ShardingSpec>(\n+          num_shards, is_fully_replicated),\n+      shape_(std::move(shape)),\n+      shard_shape_(std::move(shard_shape)) {}\n+\n+absl::StatusOr<Shape> ConcreteEvenShardingSpec::GetShardShape(\n+    const Shape& shape) const {\n+  if (shape != shape_) {\n+    return InvalidArgument(\n+        \"ConcreteEvenShardingSpec has a shard shape for shape %s, but was \"\n+        \"asked to get a shard shape for shape %s\",\n+        shape_.DebugString(), shape.DebugString());\n+  }\n+  return shard_shape_;\n+}\n+\n+bool ConcreteEvenShardingSpec::HasSamePartitioning(\n+    const ShardingSpec& other) const {\n+  if (this == &other) {\n+    return true;\n+  }\n+  const auto* other_concrete_even_sharding_spec =\n+      llvm::dyn_cast<ConcreteEvenShardingSpec>(&other);\n+  if (!other_concrete_even_sharding_spec) {\n+    return false;\n+  }\n+  return num_shards_ == other_concrete_even_sharding_spec->num_shards_ &&\n+         shape_ == other_concrete_even_sharding_spec->shape_ &&\n+         shard_shape_ == other_concrete_even_sharding_spec->shard_shape_ &&\n+         is_fully_replicated_ ==\n+             other_concrete_even_sharding_spec->is_fully_replicated_;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>>\n+ConcreteEvenShardingSpec::Disassemble(const Shape& shape) const {\n+  if (shape != shape_) {\n+    return InvalidArgument(\n+        \"ConcreteEvenShardingSpec can only disassemble shape %s, but was \"\n+        \"asked to disassemble shape %s\",\n+        shape_.DebugString(), shape.DebugString());\n+  }\n+  std::vector<std::pair<Shape, ShardingSpecRef>> result;\n+  result.reserve(num_shards_);\n+  for (int i = 0; i < num_shards_; ++i) {\n+    result.push_back({shard_shape_, SingleDeviceShardingSpec::Create()});\n+  }\n+  return result;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+ConcreteEvenShardingSpec::Disassemble(const DynamicShape& dynamic_shape) const {\n+  return InvalidArgument(\n+      \"ConcreteEvenShardingSpec can only disassemble static shape, but was \"\n+      \"asked to disassemble dynamic shape %s\",\n+      dynamic_shape.DebugString());\n+}\n+\n+absl::StatusOr<std::vector<IndexDomain>> ConcreteEvenShardingSpec::IndexDomains(\n+    const Shape& shape) const {\n+  return InvalidArgument(\n+      \"ConcreteEvenShardingSpec does not have index domain information\");\n+}\n+\n+std::string ConcreteEvenShardingSpec::DebugString() const {\n+  return absl::StrFormat(\n+      \"ConcreteEvenShardingSpec(num_shards: %d, shape: %s, \"\n+      \"shard_shape: %s, is_fully_replicated: %s)\",\n+      num_shards_, shape_.DebugString(), shard_shape_.DebugString(),\n+      is_fully_replicated_ ? \"true\" : \"false\");\n+}\n+\n+void ConcreteEvenShardingSpec::Hash(absl::HashState state) const {\n+  absl::HashState::combine(std::move(state), num_shards_, is_fully_replicated_,\n+                           shape_, shard_shape_);\n+}\n+\n+std::unique_ptr<ShardingParamShardingSpec> ShardingParamShardingSpec::Create(\n+    ShardingParam sharding_param) {\n+  int num_shards = sharding_param.NumDevices();\n+  return std::unique_ptr<ShardingParamShardingSpec>(\n+      new ShardingParamShardingSpec(num_shards, std::move(sharding_param)));\n+}\n+\n+ShardingParamShardingSpec::ShardingParamShardingSpec(\n+    int num_shards, ShardingParam sharding_param)\n+    : llvm::RTTIExtends<ShardingParamShardingSpec, ShardingSpec>(\n+          num_shards, ComputeIsFullyReplicated(sharding_param)),\n+      sharding_param_(std::move(sharding_param)) {}\n+\n+absl::StatusOr<Shape> ShardingParamShardingSpec::GetShardShape(\n+    const Shape& shape) const {\n+  if (shape.dims().size() != sharding_param_.dim_shards().size()) {\n+    return InvalidArgument(\n+        \"Numbers of dimensions don't match. From Shape %d vs from \"\n+        \"ShardingParam %d\",\n+        shape.dims().size(), sharding_param_.dim_shards().size());\n+  }\n+  std::vector<int64_t> dims;\n+  dims.reserve(shape.dims().size());\n+  for (const auto [dim, dim_shards] :\n+       llvm::zip(shape.dims(), sharding_param_.dim_shards())) {\n+    if (dim % dim_shards != 0) {\n+      return InvalidArgument(\n+          \"Uneven shard is not supported. dim: %d, dim_shards: %d\", dim,\n+          dim_shards);\n+    }\n+    dims.push_back(dim / dim_shards);\n+  }\n+  return Shape(dims);\n+}\n+\n+bool ShardingParamShardingSpec::HasSamePartitioning(\n+    const ShardingSpec& other) const {\n+  if (this == &other) {\n+    return true;\n+  }\n+  const auto* other_sharding_param_sharding_spec =\n+      llvm::dyn_cast<ShardingParamShardingSpec>(&other);\n+  if (!other_sharding_param_sharding_spec) {\n+    return false;\n+  }\n+  return sharding_param_ == other_sharding_param_sharding_spec->sharding_param_;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>>\n+ShardingParamShardingSpec::Disassemble(const Shape& shape) const {\n+  TF_ASSIGN_OR_RETURN(Shape local_shape, GetShardShape(shape));\n+  std::vector<std::pair<Shape, ShardingSpecRef>> result;\n+  result.reserve(num_shards_);\n+  for (int i = 0; i < num_shards_; ++i) {\n+    result.push_back({local_shape, SingleDeviceShardingSpec::Create()});\n+  }\n+  return result;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+ShardingParamShardingSpec::Disassemble(\n+    const DynamicShape& dynamic_shape) const {\n+  return InvalidArgument(\n+      \"ShardingParamShardingSpec can only disassemble static shape, but was \"\n+      \"asked to disassemble dynamic shape %s\",\n+      dynamic_shape.DebugString());\n+}\n+\n+absl::StatusOr<std::vector<IndexDomain>>\n+ShardingParamShardingSpec::IndexDomains(const Shape& shape) const {\n+  // Calculate the origins of tiles, ignoring device assignments.\n+  TF_ASSIGN_OR_RETURN(Shape local_shape, GetShardShape(shape));\n+  std::vector<Index> tile_indices =\n+      GetTileIndices(sharding_param_.dim_shards());\n+  std::vector<Index> origins;\n+  origins.reserve(tile_indices.size());\n+  for (const Index& tile_index : tile_indices) {\n+    origins.push_back(tile_index * local_shape.dims());\n+  }\n+\n+  // Calculate the device assignments.\n+  // `origins[i]` should go to `device_list[i]`.\n+  static constexpr int kInvalidIndex = -1;\n+  llvm::SmallVector<int, 4> device_list;\n+  sharding_param_.minor_to_major().ToDeviceList(device_list);\n+  std::vector<int> device_to_index(device_list.size(), kInvalidIndex);\n+  for (int i = 0; i < device_list.size(); ++i) {\n+    device_to_index[device_list[i]] = i;\n+  }\n+\n+  // Replication is the minor axis in `device_list`.\n+  DCHECK_EQ(device_to_index.size() % origins.size(), 0);\n+  int replication = device_to_index.size() / origins.size();\n+\n+  DCHECK_EQ(device_to_index.size(), num_shards_);\n+  std::vector<IndexDomain> result;\n+  result.reserve(num_shards_);\n+  for (int i = 0; i < device_to_index.size(); ++i) {\n+    int index = device_to_index[i];\n+    DCHECK_NE(index, kInvalidIndex);\n+    result.push_back(IndexDomain(origins[index / replication], local_shape));\n+  }\n+  return result;\n+}\n+\n+std::string ShardingParamShardingSpec::DebugString() const {\n+  return absl::StrFormat(\"ShardingParamShardingSpec(num_shards: %d, %s)\",\n+                         num_shards_, sharding_param_.DebugString());\n+}\n+\n+void ShardingParamShardingSpec::Hash(absl::HashState state) const {\n+  absl::HashState::combine(std::move(state), num_shards_, is_fully_replicated_,\n+                           sharding_param_);\n+}\n+\n+}  // namespace ifrt\n+}  // namespace xla"
        },
        {
            "sha": "799bc0a39c407eb8b62e5e274b5c3d121826fb14",
            "filename": "third_party/xla/xla/python/ifrt/sharding_spec.h",
            "status": "added",
            "additions": 435,
            "deletions": 0,
            "changes": 435,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.h?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,435 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PYTHON_IFRT_SHARDING_SPEC_H_\n+#define XLA_PYTHON_IFRT_SHARDING_SPEC_H_\n+\n+#include <memory>\n+#include <optional>\n+#include <ostream>\n+#include <string>\n+#include <utility>\n+#include <variant>\n+#include <vector>\n+\n+#include \"absl/base/nullability.h\"\n+#include \"absl/hash/hash.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/ir/sharding_param.h\"\n+#include \"xla/python/ifrt/serdes.h\"\n+#include \"xla/python/ifrt/serdes_default_version_accessor.h\"\n+#include \"xla/python/ifrt/serdes_version.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding_spec.pb.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+\n+class ShardingSpec;\n+\n+using ShardingSpecRef = absl_nonnull std::shared_ptr<const ShardingSpec>;\n+\n+// `ShardingSpec` represents partitioning of a logical array into a certain\n+// ordered list of shards.\n+//\n+// Shards may contain duplicate or exclusive data, depending on the partitioning\n+// scheme. One well-known case of \"fully replicated\" sharding spec indicates\n+// that every shard contains the entire data of the logical array.\n+//\n+// Shards are order-sensitive. The ordering of individual shards in a sharding\n+// spec must be respected, in particular when the list of the shards is used in\n+// parallel to another order-sensitive list, such as `DeviceList`. For example,\n+// the first shard from `ShardingSpec` would be placed on the first device from\n+// `DeviceList`, the second shard from `ShardingSpec` would be placed on the\n+// second device from `DeviceList`, and so on.\n+//\n+// Depending on the type of a sharding spec, the partitioning of the sharding\n+// spec is applicable only to a particular array shape, but that of some others\n+// may be applicable to a broad set of shapes.\n+class ShardingSpec : public llvm::RTTIExtends<ShardingSpec, Serializable> {\n+ public:\n+  // Returns the number of shards.\n+  int num_shards() const { return num_shards_; }\n+\n+  // Returns if this sharding spec is fully replicated. A fully replicated\n+  // sharding spec means that the logical shape and shard shapes are identical\n+  // (`GetShardShape(shape) == shape`), and every shard of the array contains\n+  // the entire data of the logical array.\n+  bool IsFullyReplicated() const { return is_fully_replicated_; }\n+\n+  // Returns if this sharding spec is equal to `other`.\n+  bool operator==(const ShardingSpec& other) const;\n+  bool operator!=(const ShardingSpec& other) const { return !(*this == other); }\n+\n+  // Returns a shard shape if the sharding spec always has the equal shape for\n+  // all shards. Returns an error if the sharding spec may not have a single\n+  // shard shape, or `shape` is not a valid shape for this sharding spec.\n+  virtual absl::StatusOr<Shape> GetShardShape(const Shape& shape) const = 0;\n+\n+  // Returns if this sharding spec has the same logical partitioning as `other`.\n+  // By the same logical partitioning, we mean that `ShardingSpec` type is the\n+  // same, and the partitioning scheme within the sharding spec is equivalent.\n+  // It does not need to check if `Disassemble()` would return the same result.\n+  virtual bool HasSamePartitioning(const ShardingSpec& other) const = 0;\n+\n+  // Breaks a shape up into per-device shapes and sharding specs. See\n+  // Array::DisassembleIntoSingleDeviceArrays(). It may return an error if\n+  // disassembly is unsupported.\n+  virtual absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>>\n+  Disassemble(const Shape& shape) const = 0;\n+\n+  // Variant of `Disassemble` that takes a dynamic shape.\n+  virtual absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+  Disassemble(const DynamicShape& dynamic_shape) const = 0;\n+\n+  // Maps each shard to an `IndexDomain` over `shape`. The result is a list of\n+  // `index_domain_i` such that `array[index_domain_i] = disassembled_array_i`.\n+  // Note that multiple shards may map onto equal `IndexDomain`. For instance, a\n+  // fully replicated sharding spec would return a vector of\n+  // `[IndexDomain(shape)] * num_shards()`.\n+  virtual absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const = 0;\n+\n+  template <typename H>\n+  friend H AbslHashValue(H h, const ShardingSpec& value) {\n+    value.Hash(absl::HashState::Create(&h));\n+    return std::move(h);\n+  }\n+\n+  // Deserializes `ShardingSpecProto` into `ShardingSpec`.\n+  // Note that `ShardingSpec` serialization uses `SerDes` to handle an open set\n+  // of `ShardingSpec` subclasses. See `serdes.h`.\n+  static absl::StatusOr<ShardingSpecRef> FromProto(\n+      const ShardingSpecProto& sharding_spec_proto);\n+\n+  // Converts `Sharding` into a protobuf.\n+  absl::Status ToProto(\n+      ShardingSpecProto& sharding_spec_proto,\n+      SerDesVersion version = SerDesDefaultVersionAccessor::Get()) const;\n+\n+  // Serializes `ShardingSpec` into `ShardingSpecProto`.\n+  // Note that `ShardingSpec` serialization uses `SerDes` to handle an open set\n+  // of `ShardingSpec` subclasses. See `serdes.h`.\n+  absl::StatusOr<ShardingSpecProto> ToProto(\n+      SerDesVersion version = SerDesDefaultVersionAccessor::Get()) const {\n+    ShardingSpecProto proto;\n+    TF_RETURN_IF_ERROR(ToProto(proto, version));\n+    return proto;\n+  }\n+\n+  // TODO(hyeontaek): Remove this method in favor of AbslStringify.\n+  virtual std::string DebugString() const = 0;\n+\n+  template <typename Sink>\n+  friend void AbslStringify(Sink& sink, const ShardingSpec& sharding_spec) {\n+    sink.Append(sharding_spec.DebugString());\n+  }\n+\n+  // TODO(hyeontaek): Remove this template definition. In theory,\n+  // `std::shared_ptr<>` is responsible for defining it. Consider introducing a\n+  // `std::shared_ptr<>` version of `RCReferenceWrapper` to own this template\n+  // definition.\n+  template <class Sink>\n+  friend void AbslStringify(\n+      Sink& sink, std::shared_ptr<const ShardingSpec>& sharding_spec) {\n+    if (sharding_spec == nullptr) {\n+      sink.Append(\"<nullptr>\");\n+    } else {\n+      sink.Append(sharding_spec->DebugString());\n+    }\n+  }\n+\n+  static char ID;  // NOLINT\n+\n+ protected:\n+  ShardingSpec(int num_shards, bool is_fully_replicated);\n+\n+  virtual void Hash(absl::HashState state) const = 0;\n+\n+  int num_shards_;\n+  bool is_fully_replicated_;\n+};\n+\n+std::ostream& operator<<(std::ostream& os, const ShardingSpec& sharding_spec);\n+\n+// TODO(hyeontaek): Move the subclasses of `ShardingSpec` to a seperate file,\n+// making this sharding_spec.{h,cc} only define interface and common functions.\n+\n+// Single-device sharding spec.\n+//\n+// TODO(hyeontaek): `SingleDeviceShardingSpec` tends to be created or consumed\n+// in a large quantity. It may be useful for performance optimization to\n+// special-case this sharding type rather than expressing it as a general\n+// `ShardingSpec`.\n+class SingleDeviceShardingSpec final\n+    : public llvm::RTTIExtends<SingleDeviceShardingSpec, ShardingSpec> {\n+ public:\n+  // Creates a single-device sharding spec.\n+  static std::unique_ptr<SingleDeviceShardingSpec> Create();\n+\n+  // ShardingSpec implementation.\n+\n+  ~SingleDeviceShardingSpec() override = default;\n+\n+  absl::StatusOr<Shape> GetShardShape(const Shape& shape) const override;\n+\n+  bool HasSamePartitioning(const ShardingSpec& other) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>> Disassemble(\n+      const Shape& shape) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+  Disassemble(const DynamicShape& dynamic_shape) const override;\n+\n+  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const override;\n+\n+  std::string DebugString() const override;\n+\n+  static char ID;  // NOLINT\n+\n+ private:\n+  SingleDeviceShardingSpec();\n+\n+  void Hash(absl::HashState state) const override;\n+};\n+\n+// Opaque sharding spec that does not define a fixed semantics for conversion\n+// between a logical shape and per-device shapes, and device placements.\n+class OpaqueShardingSpec\n+    : public llvm::RTTIExtends<OpaqueShardingSpec, ShardingSpec> {\n+ public:\n+  // Creates an opaque sharding spec. `Disassemble()` will fail.\n+  static std::unique_ptr<OpaqueShardingSpec> Create(int num_shards);\n+\n+  // ShardingSpec implementation.\n+\n+  ~OpaqueShardingSpec() override = default;\n+\n+  absl::StatusOr<Shape> GetShardShape(const Shape& shape) const override;\n+\n+  bool HasSamePartitioning(const ShardingSpec& other) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>> Disassemble(\n+      const Shape& shape) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+  Disassemble(const DynamicShape& dynamic_shape) const override;\n+\n+  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const override;\n+\n+  std::string DebugString() const override;\n+\n+  static char ID;  // NOLINT\n+\n+ private:\n+  explicit OpaqueShardingSpec(int num_shards);\n+\n+  void Hash(absl::HashState state) const override;\n+};\n+\n+// Opaque sharding spec that does not define a fixed semantics for conversion\n+// between a logical shape and shard shapes, and device placements. It can\n+// disassemble a certain shape into shard shapes that may not be identical. It\n+// is advised to use `ConcreteEvenShardingSpec` if all shard shapes are\n+// identical.\n+class ConcreteShardingSpec\n+    : public llvm::RTTIExtends<ConcreteShardingSpec, ShardingSpec> {\n+ public:\n+  // Creates a concrete sharding spec that may contain non-identical shard\n+  // shapes.\n+  static std::unique_ptr<ConcreteShardingSpec> Create(\n+      Shape shape, std::vector<Shape> shard_shapes,\n+      std::optional<std::vector<xla::ifrt::IndexDomain>> index_domains =\n+          std::nullopt);\n+\n+  // Creates a concrete sharding spec that may contain non-identical shard\n+  // dynamic shapes.\n+  static std::unique_ptr<ConcreteShardingSpec> Create(\n+      DynamicShape dynamic_shape,\n+      std::vector<DynamicShape> shard_dynamic_shapes);\n+\n+  bool has_dynamic_shape() const {\n+    DCHECK(this);\n+    return std::holds_alternative<DynamicShape>(shape_) &&\n+           std::holds_alternative<std::vector<DynamicShape>>(shard_shapes_);\n+  }\n+\n+  bool has_static_shape() const {\n+    DCHECK(this);\n+    return std::holds_alternative<Shape>(shape_) &&\n+           std::holds_alternative<std::vector<Shape>>(shard_shapes_);\n+  }\n+\n+  const Shape& shape() const {\n+    DCHECK(has_static_shape());\n+    return std::get<Shape>(shape_);\n+  }\n+\n+  const DynamicShape& dynamic_shape() const {\n+    DCHECK(has_dynamic_shape());\n+    return std::get<DynamicShape>(shape_);\n+  }\n+\n+  const std::vector<Shape>& shard_shapes() const {\n+    DCHECK(this);\n+    DCHECK(std::holds_alternative<std::vector<Shape>>(shard_shapes_));\n+    return std::get<std::vector<Shape>>(shard_shapes_);\n+  }\n+\n+  const std::vector<DynamicShape>& shard_dynamic_shapes() const {\n+    DCHECK(this);\n+    DCHECK(std::holds_alternative<std::vector<DynamicShape>>(shard_shapes_));\n+    return std::get<std::vector<DynamicShape>>(shard_shapes_);\n+  }\n+\n+  const std::optional<std::vector<xla::ifrt::IndexDomain>>& index_domains()\n+      const {\n+    return index_domains_;\n+  }\n+\n+  // ShardingSpec implementation.\n+\n+  ~ConcreteShardingSpec() override = default;\n+\n+  absl::StatusOr<Shape> GetShardShape(const Shape& shape) const override;\n+\n+  bool HasSamePartitioning(const ShardingSpec& other) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>> Disassemble(\n+      const Shape& shape) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+  Disassemble(const DynamicShape& dynamic_shape) const override;\n+\n+  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const override;\n+\n+  std::string DebugString() const override;\n+\n+  static char ID;  // NOLINT\n+\n+ private:\n+  ConcreteShardingSpec(\n+      int num_shards, Shape shape, std::vector<Shape> shard_shapes,\n+      std::optional<std::vector<xla::ifrt::IndexDomain>> index_domains);\n+\n+  ConcreteShardingSpec(int num_shards, DynamicShape dynamic_shape,\n+                       std::vector<DynamicShape> shard_dynamic_shapes);\n+\n+  void Hash(absl::HashState state) const override;\n+\n+  std::variant<Shape, DynamicShape> shape_;\n+  std::variant<std::vector<Shape>, std::vector<DynamicShape>> shard_shapes_;\n+  std::optional<Shape> shard_shape_;\n+  std::optional<std::vector<xla::ifrt::IndexDomain>> index_domains_;\n+};\n+\n+// Opaque sharding spec that does not define a fixed semantics for conversion\n+// between a logical shape and shard shapes, and device placements. It can\n+// disassemble a certain shape into shard shapes that are identical.\n+class ConcreteEvenShardingSpec\n+    : public llvm::RTTIExtends<ConcreteEvenShardingSpec, ShardingSpec> {\n+ public:\n+  // Creates a concrete even sharding spec.\n+  // TODO(hyeontaek): Remove the default value of `is_fully_replicated` once all\n+  // callers are updated to provide it explicitly.\n+  static std::unique_ptr<ConcreteEvenShardingSpec> Create(\n+      int num_shards, Shape shape, Shape shard_shape,\n+      bool is_fully_replicated = false);\n+\n+  Shape shape() const {\n+    DCHECK(this);\n+    return shape_;\n+  }\n+  const Shape& shard_shape() const {\n+    DCHECK(this);\n+    return shard_shape_;\n+  }\n+\n+  // ShardingSpec implementation.\n+\n+  ~ConcreteEvenShardingSpec() override = default;\n+\n+  absl::StatusOr<Shape> GetShardShape(const Shape& shape) const override;\n+\n+  bool HasSamePartitioning(const ShardingSpec& other) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>> Disassemble(\n+      const Shape& shape) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+  Disassemble(const DynamicShape& dynamic_shape) const override;\n+\n+  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const override;\n+\n+  std::string DebugString() const override;\n+\n+  static char ID;  // NOLINT\n+\n+ private:\n+  ConcreteEvenShardingSpec(int num_shards, Shape shape, Shape shard_shape,\n+                           bool is_fully_replicated);\n+\n+  void Hash(absl::HashState state) const override;\n+\n+  Shape shape_;\n+  Shape shard_shape_;\n+};\n+\n+// Sharding spec derived from an IR ShardingParam.\n+class ShardingParamShardingSpec\n+    : public llvm::RTTIExtends<ShardingParamShardingSpec, ShardingSpec> {\n+ public:\n+  static std::unique_ptr<ShardingParamShardingSpec> Create(\n+      ShardingParam sharding_param);\n+\n+  const ShardingParam& sharding_param() const { return sharding_param_; }\n+\n+  absl::StatusOr<Shape> GetShardShape(const Shape& shape) const override;\n+\n+  bool HasSamePartitioning(const ShardingSpec& other) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>> Disassemble(\n+      const Shape& shape) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+  Disassemble(const DynamicShape& dynamic_shape) const override;\n+\n+  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const override;\n+\n+  std::string DebugString() const override;\n+\n+  static char ID;  // NOLINT\n+\n+ private:\n+  ShardingParamShardingSpec(int num_shards, ShardingParam sharding_param);\n+\n+  void Hash(absl::HashState state) const override;\n+\n+  ShardingParam sharding_param_;\n+};\n+\n+}  // namespace ifrt\n+}  // namespace xla\n+\n+#endif  // XLA_PYTHON_IFRT_SHARDING_SPEC_H_"
        },
        {
            "sha": "ddba1935d34aaf7785851d92e8e1dc0b5814227f",
            "filename": "third_party/xla/xla/python/ifrt/sharding_spec.proto",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec.proto?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,29 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+syntax = \"proto3\";\n+\n+package xla.ifrt;\n+\n+import \"xla/python/ifrt/serdes.proto\";\n+\n+option java_multiple_files = true;\n+option java_outer_classname = \"ShardingSpec\";\n+\n+// Proto equivalent of C++ `ShardingSpec`. A suitable serializer and\n+// deserializer implementation must be registered.\n+message ShardingSpecProto {\n+  xla.ifrt.Serialized serialized_sharding_spec = 1;\n+}"
        },
        {
            "sha": "68c009854330c466b0b4f77f9ba3e1f6afcb72ea",
            "filename": "third_party/xla/xla/python/ifrt/sharding_spec_serdes.cc",
            "status": "added",
            "additions": 352,
            "deletions": 0,
            "changes": 352,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,352 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/python/ifrt/ir/sharding_param.h\"\n+#include \"xla/python/ifrt/serdes.h\"\n+#include \"xla/python/ifrt/serdes_version.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+#include \"xla/python/ifrt/sharding_spec_serdes.pb.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+\n+namespace {\n+\n+// Serialization/deserialization for `SingleDeviceShardingSpec`.\n+class SingleDeviceShardingSpecSerDes\n+    : public llvm::RTTIExtends<SingleDeviceShardingSpecSerDes, SerDes> {\n+ public:\n+  absl::string_view type_name() const override {\n+    return \"xla::ifrt::SingleDeviceShardingSpec\";\n+  }\n+\n+  absl::StatusOr<std::string> Serialize(\n+      const Serializable& serializable,\n+      std::unique_ptr<SerializeOptions> options) override {\n+    const SerDesVersion version = GetRequestedSerDesVersion(options.get());\n+    if (version.version_number() < SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version.version_number(),\n+                       \" for SingleDeviceShardingSpec serialization\"));\n+    }\n+    SingleDeviceShardingSpecProto proto;\n+    proto.set_version_number(SerDesVersionNumber(0).value());\n+    return proto.SerializeAsString();\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<Serializable>> Deserialize(\n+      const std::string& serialized,\n+      std::unique_ptr<DeserializeOptions> options) override {\n+    SingleDeviceShardingSpecProto proto;\n+    if (!proto.ParseFromString(serialized)) {\n+      return absl::InvalidArgumentError(\n+          \"Failed to parse serialized SingleDeviceShardingSpec\");\n+    }\n+    const SerDesVersionNumber version_number(proto.version_number());\n+    if (version_number != SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version_number,\n+                       \" for SingleDeviceShardingSpec deserialization\"));\n+    }\n+    return SingleDeviceShardingSpec::Create();\n+  }\n+\n+  static char ID;  // NOLINT\n+};\n+\n+// Serialization/deserialization for `OpaqueShardingSpec`.\n+class OpaqueShardingSpecSerDes\n+    : public llvm::RTTIExtends<OpaqueShardingSpecSerDes, SerDes> {\n+ public:\n+  absl::string_view type_name() const override {\n+    return \"xla::ifrt::OpaqueShardingSpec\";\n+  }\n+\n+  absl::StatusOr<std::string> Serialize(\n+      const Serializable& serializable,\n+      std::unique_ptr<SerializeOptions> options) override {\n+    const SerDesVersion version = GetRequestedSerDesVersion(options.get());\n+    if (version.version_number() < SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version.version_number(),\n+                       \" for OpaqueShardingSpec serialization\"));\n+    }\n+    const OpaqueShardingSpec& sharding_spec =\n+        llvm::cast<OpaqueShardingSpec>(serializable);\n+    OpaqueShardingSpecProto proto;\n+    proto.set_version_number(SerDesVersionNumber(0).value());\n+    proto.set_num_shards(sharding_spec.num_shards());\n+    return proto.SerializeAsString();\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<Serializable>> Deserialize(\n+      const std::string& serialized,\n+      std::unique_ptr<DeserializeOptions> options) override {\n+    OpaqueShardingSpecProto proto;\n+    if (!proto.ParseFromString(serialized)) {\n+      return absl::InvalidArgumentError(\n+          \"Failed to parse serialized OpaqueShardingSpec\");\n+    }\n+    const SerDesVersionNumber version_number(proto.version_number());\n+    if (version_number != SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version_number,\n+                       \" for OpaqueShardingSpec deserialization\"));\n+    }\n+    return OpaqueShardingSpec::Create(proto.num_shards());\n+  }\n+\n+  static char ID;  // NOLINT\n+};\n+\n+// Serialization/deserialization for `ConcreteShardingSpec`.\n+class ConcreteShardingSpecSerDes\n+    : public llvm::RTTIExtends<ConcreteShardingSpecSerDes, SerDes> {\n+ public:\n+  absl::string_view type_name() const override {\n+    return \"xla::ifrt::ConcreteShardingSpec\";\n+  }\n+\n+  absl::StatusOr<std::string> Serialize(\n+      const Serializable& serializable,\n+      std::unique_ptr<SerializeOptions> options) override {\n+    const SerDesVersion version = GetRequestedSerDesVersion(options.get());\n+    if (version.version_number() < SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version.version_number(),\n+                       \" for ConcreteShardingSpec serialization\"));\n+    }\n+    const ConcreteShardingSpec& sharding_spec =\n+        llvm::cast<ConcreteShardingSpec>(serializable);\n+    if (sharding_spec.index_domains().has_value()) {\n+      return absl::UnimplementedError(\n+          \"Index domains are not yet supported in ConcreteShardingSpec \"\n+          \"serialization\");\n+    }\n+    ConcreteShardingSpecProto proto;\n+    proto.set_version_number(SerDesVersionNumber(0).value());\n+    if (sharding_spec.has_static_shape()) {\n+      sharding_spec.shape().ToProto(*proto.mutable_shape(), version);\n+      for (const Shape& shape : sharding_spec.shard_shapes()) {\n+        *proto.add_shard_shapes() = shape.ToProto(version);\n+      }\n+    } else {\n+      sharding_spec.dynamic_shape().ToProto(*proto.mutable_dynamic_shape(),\n+                                            version);\n+      for (const DynamicShape& dynamic_shape :\n+           sharding_spec.shard_dynamic_shapes()) {\n+        dynamic_shape.ToProto(*proto.add_shard_dynamic_shapes(), version);\n+      }\n+    }\n+    return proto.SerializeAsString();\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<Serializable>> Deserialize(\n+      const std::string& serialized,\n+      std::unique_ptr<DeserializeOptions> options) override {\n+    ConcreteShardingSpecProto proto;\n+    if (!proto.ParseFromString(serialized)) {\n+      return absl::InvalidArgumentError(\n+          \"Failed to parse serialized ConcreteShardingSpec\");\n+    }\n+    const SerDesVersionNumber version_number(proto.version_number());\n+    if (version_number != SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version_number,\n+                       \" for ConcreteShardingSpec deserialization\"));\n+    }\n+    if (proto.has_shape()) {\n+      TF_ASSIGN_OR_RETURN(auto shape, Shape::FromProto(proto.shape()));\n+      std::vector<Shape> shard_shapes;\n+      shard_shapes.reserve(proto.shard_shapes_size());\n+      for (const auto& shard_shape_proto : proto.shard_shapes()) {\n+        TF_ASSIGN_OR_RETURN(auto shard_shape,\n+                            Shape::FromProto(shard_shape_proto));\n+        shard_shapes.push_back(std::move(shard_shape));\n+      }\n+      return ConcreteShardingSpec::Create(std::move(shape),\n+                                          std::move(shard_shapes));\n+    }\n+    if (!proto.has_dynamic_shape()) {\n+      return absl::InvalidArgumentError(\n+          \"ConcreteShardingSpec must have Shape or DynamicShape.\");\n+    }\n+    TF_ASSIGN_OR_RETURN(auto dynamic_shape,\n+                        DynamicShape::FromProto(proto.dynamic_shape()));\n+    std::vector<DynamicShape> shard_dynamic_shapes;\n+    shard_dynamic_shapes.reserve(proto.shard_dynamic_shapes_size());\n+    for (const auto& shard_dynamic_shape_proto : proto.shard_dynamic_shapes()) {\n+      TF_ASSIGN_OR_RETURN(auto dynamic_shape,\n+                          DynamicShape::FromProto(shard_dynamic_shape_proto));\n+      shard_dynamic_shapes.push_back(std::move(dynamic_shape));\n+    }\n+    return ConcreteShardingSpec::Create(std::move(dynamic_shape),\n+                                        std::move(shard_dynamic_shapes));\n+  }\n+\n+  static char ID;  // NOLINT\n+};\n+\n+// Serialization/deserialization for `ConcreteEvenShardingSpec`.\n+class ConcreteEvenShardingSpecSerDes\n+    : public llvm::RTTIExtends<ConcreteEvenShardingSpecSerDes, SerDes> {\n+ public:\n+  absl::string_view type_name() const override {\n+    return \"xla::ifrt::ConcreteEvenShardingSpec\";\n+  }\n+\n+  absl::StatusOr<std::string> Serialize(\n+      const Serializable& serializable,\n+      std::unique_ptr<SerializeOptions> options) override {\n+    const SerDesVersion version = GetRequestedSerDesVersion(options.get());\n+    if (version.version_number() < SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version.version_number(),\n+                       \" for ConcreteEvenShardingSpec serialization\"));\n+    }\n+    const ConcreteEvenShardingSpec& sharding_spec =\n+        llvm::cast<ConcreteEvenShardingSpec>(serializable);\n+    ConcreteEvenShardingSpecProto proto;\n+    proto.set_version_number(SerDesVersionNumber(0).value());\n+    proto.set_num_shards(sharding_spec.num_shards());\n+    sharding_spec.shape().ToProto(*proto.mutable_shape(), version);\n+    sharding_spec.shard_shape().ToProto(*proto.mutable_shard_shape(), version);\n+    proto.set_is_fully_replicated(sharding_spec.IsFullyReplicated());\n+    return proto.SerializeAsString();\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<Serializable>> Deserialize(\n+      const std::string& serialized,\n+      std::unique_ptr<DeserializeOptions> options) override {\n+    ConcreteEvenShardingSpecProto proto;\n+    if (!proto.ParseFromString(serialized)) {\n+      return absl::InvalidArgumentError(\n+          \"Failed to parse serialized ConcreteEvenShardingSpec\");\n+    }\n+    const SerDesVersionNumber version_number(proto.version_number());\n+    if (version_number != SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version_number,\n+                       \" for ConcreteEvenShardingSpec deserialization\"));\n+    }\n+    TF_ASSIGN_OR_RETURN(auto shape, Shape::FromProto(proto.shape()));\n+    TF_ASSIGN_OR_RETURN(auto shard_shape,\n+                        Shape::FromProto(proto.shard_shape()));\n+    return ConcreteEvenShardingSpec::Create(\n+        proto.num_shards(), std::move(shape), std::move(shard_shape),\n+        proto.is_fully_replicated());\n+  }\n+\n+  static char ID;  // NOLINT\n+};\n+\n+class ShardingParamShardingSpecSerDes\n+    : public llvm::RTTIExtends<ShardingParamShardingSpecSerDes, SerDes> {\n+ public:\n+  absl::string_view type_name() const override {\n+    return \"xla::ifrt::ShardingParamShardingSpec\";\n+  }\n+\n+  absl::StatusOr<std::string> Serialize(\n+      const Serializable& serializable,\n+      std::unique_ptr<SerializeOptions> options) override {\n+    const SerDesVersion version = GetRequestedSerDesVersion(options.get());\n+    if (version.version_number() < SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version.version_number(),\n+                       \" for ShardingParamShardingSpec serialization\"));\n+    }\n+    const ShardingParamShardingSpec& sharding_spec =\n+        llvm::cast<ShardingParamShardingSpec>(serializable);\n+    ShardingParamShardingSpecProto proto;\n+    proto.set_version_number(SerDesVersionNumber(0).value());\n+    TF_RETURN_IF_ERROR(sharding_spec.sharding_param().ToProto(\n+        *proto.mutable_sharding_param(), version));\n+    return proto.SerializeAsString();\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<Serializable>> Deserialize(\n+      const std::string& serialized,\n+      std::unique_ptr<DeserializeOptions> options) override {\n+    ShardingParamShardingSpecProto proto;\n+    if (!proto.ParseFromString(serialized)) {\n+      return absl::InvalidArgumentError(\n+          \"Failed to parse serialized ShardingParamShardingSpec\");\n+    }\n+    const SerDesVersionNumber version_number(proto.version_number());\n+    if (version_number != SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version_number,\n+                       \" for ShardingParamShardingSpec deserialization\"));\n+    }\n+    TF_ASSIGN_OR_RETURN(ShardingParam sharding_param,\n+                        ShardingParam::FromProto(proto.sharding_param()));\n+    return ShardingParamShardingSpec::Create(std::move(sharding_param));\n+  }\n+\n+  static char ID;  // NOLINT\n+};\n+\n+[[maybe_unused]] char SingleDeviceShardingSpecSerDes::ID = 0;   // NOLINT\n+[[maybe_unused]] char OpaqueShardingSpecSerDes::ID = 0;         // NOLINT\n+[[maybe_unused]] char ConcreteShardingSpecSerDes::ID = 0;       // NOLINT\n+[[maybe_unused]] char ConcreteEvenShardingSpecSerDes::ID = 0;   // NOLINT\n+[[maybe_unused]] char ShardingParamShardingSpecSerDes::ID = 0;  // NOLINT\n+\n+// clang-format off\n+bool register_single_device_sharding_spec_serdes = ([]{\n+  RegisterSerDes<SingleDeviceShardingSpec>(\n+      std::make_unique<SingleDeviceShardingSpecSerDes>());\n+}(), true);\n+\n+bool register_opaque_sharding_spec_serdes = ([]{\n+  RegisterSerDes<OpaqueShardingSpec>(\n+      std::make_unique<OpaqueShardingSpecSerDes>());\n+}(), true);\n+\n+bool register_concrete_sharding_spec_serdes = ([]{\n+  RegisterSerDes<ConcreteShardingSpec>(\n+      std::make_unique<ConcreteShardingSpecSerDes>());\n+}(), true);\n+\n+bool register_concrete_even_sharding_spec_serdes = ([]{\n+  RegisterSerDes<ConcreteEvenShardingSpec>(\n+      std::make_unique<ConcreteEvenShardingSpecSerDes>());\n+}(), true);\n+\n+bool register_sharding_param_sharding_spec_serdes = ([]{\n+  RegisterSerDes<ShardingParamShardingSpec>(\n+      std::make_unique<ShardingParamShardingSpecSerDes>());\n+}(), true);\n+// clang-format on\n+\n+}  // namespace\n+\n+}  // namespace ifrt\n+}  // namespace xla"
        },
        {
            "sha": "8eefd14b10195289b3cdaa5f8c8f02ec6a2cf0ba",
            "filename": "third_party/xla/xla/python/ifrt/sharding_spec_serdes.proto",
            "status": "added",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes.proto?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,65 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+syntax = \"proto3\";\n+\n+package xla.ifrt;\n+\n+import \"xla/python/ifrt/ir/sharding_param.proto\";\n+import \"xla/python/ifrt/shape.proto\";\n+\n+option java_multiple_files = true;\n+option java_outer_classname = \"ShardingSpecSerDes\";\n+\n+// Proto equivalent of C++ `SingleDeviceShardingSpec`.\n+message SingleDeviceShardingSpecProto {\n+  int32 version_number = 1;\n+}\n+\n+// Proto equivalent of C++ `OpaqueShardingSpec`.\n+message OpaqueShardingSpecProto {\n+  int32 version_number = 1;\n+\n+  int32 num_shards = 2;\n+}\n+\n+// Proto equivalent of C++ `ConcreteShardingSpec`.\n+message ConcreteShardingSpecProto {\n+  int32 version_number = 1;\n+\n+  oneof shape_or_dynamic_shape {\n+    ShapeProto shape = 2;\n+    DynamicShapeProto dynamic_shape = 3;\n+  }\n+  repeated ShapeProto shard_shapes = 4;\n+  repeated DynamicShapeProto shard_dynamic_shapes = 5;\n+}\n+\n+// Proto equivalent of C++ `ConcreteEvenShardingSpec`.\n+message ConcreteEvenShardingSpecProto {\n+  int32 version_number = 1;\n+\n+  int32 num_shards = 2;\n+  ShapeProto shape = 3;\n+  ShapeProto shard_shape = 4;\n+  bool is_fully_replicated = 5;\n+}\n+\n+// Proto equivalent of C++ `ShardingParamShardingSpec`.\n+message ShardingParamShardingSpecProto {\n+  int32 version_number = 1;\n+\n+  ShardingParamProto sharding_param = 2;\n+}"
        },
        {
            "sha": "4272bc321f5c50e89ed9270ea86772da47dac004",
            "filename": "third_party/xla/xla/python/ifrt/sharding_spec_serdes_test.cc",
            "status": "added",
            "additions": 167,
            "deletions": 0,
            "changes": 167,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_serdes_test.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,167 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/python/ifrt/ir/sharding_param.h\"\n+#include \"xla/python/ifrt/serdes.h\"\n+#include \"xla/python/ifrt/serdes.pb.h\"\n+#include \"xla/python/ifrt/serdes_test_util.h\"\n+#include \"xla/python/ifrt/serdes_version.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ::testing::ElementsAreArray;\n+\n+using ShardingSpecSerDesTestParam = std::tuple<SerDesVersion, int>;\n+\n+class ShardingSpecSerDesTest\n+    : public testing::TestWithParam<ShardingSpecSerDesTestParam> {\n+ public:\n+  SerDesVersion version() const { return std::get<0>(GetParam()); }\n+  int num_shards() const { return std::get<1>(GetParam()); }\n+};\n+\n+TEST_P(ShardingSpecSerDesTest, SingleDeviceShardingSpecRoundTrip) {\n+  auto sharding_spec = SingleDeviceShardingSpec::Create();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto serialized,\n+      Serialize(*sharding_spec, std::make_unique<SerializeOptions>(version())));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto out_sharding_spec,\n+      Deserialize<SingleDeviceShardingSpec>(serialized, /*options=*/nullptr));\n+}\n+\n+TEST_P(ShardingSpecSerDesTest, OpaqueShardingSpecRoundTrip) {\n+  auto sharding_spec = OpaqueShardingSpec::Create(num_shards());\n+\n+  auto options = std::make_unique<SerializeOptions>(version());\n+  TF_ASSERT_OK_AND_ASSIGN(auto serialized,\n+                          Serialize(*sharding_spec, std::move(options)));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto out_sharding_spec,\n+      Deserialize<OpaqueShardingSpec>(serialized, /*options=*/nullptr));\n+\n+  EXPECT_THAT(out_sharding_spec->num_shards(), sharding_spec->num_shards());\n+}\n+\n+TEST_P(ShardingSpecSerDesTest, ConcreteShardingSpecRoundTrip) {\n+  std::vector<Shape> shard_shapes(num_shards(), Shape({10, 20}));\n+  auto sharding_spec =\n+      ConcreteShardingSpec::Create(/*shape=*/Shape({10 * num_shards(), 20}),\n+                                   /*shard_shapes=*/shard_shapes);\n+\n+  auto options = std::make_unique<SerializeOptions>(version());\n+  TF_ASSERT_OK_AND_ASSIGN(auto serialized,\n+                          Serialize(*sharding_spec, std::move(options)));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto out_sharding_spec,\n+      Deserialize<ConcreteShardingSpec>(serialized, /*options=*/nullptr));\n+\n+  EXPECT_THAT(out_sharding_spec->num_shards(), sharding_spec->num_shards());\n+  EXPECT_THAT(out_sharding_spec->shape(), sharding_spec->shape());\n+  EXPECT_THAT(out_sharding_spec->shard_shapes(),\n+              ElementsAreArray(sharding_spec->shard_shapes()));\n+}\n+\n+TEST_P(ShardingSpecSerDesTest, ConcreteShardingSpecWithDynamicShapeRoundTrip) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape shard_dynamic_shape,\n+      DynamicShape::Create(Shape({10, 20}),\n+                           BoundedDynamicShapeTag({false, true})));\n+  std::vector<DynamicShape> shard_dynamic_shapes(num_shards(),\n+                                                 shard_dynamic_shape);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape dynamic_shape,\n+      DynamicShape::Create(Shape({10 * num_shards(), 20}),\n+                           BoundedDynamicShapeTag({false, true})));\n+  auto sharding_spec = ConcreteShardingSpec::Create(\n+      /*dynamic_shape=*/dynamic_shape,\n+      /*shard_dynamic_shapes=*/shard_dynamic_shapes);\n+\n+  auto options = std::make_unique<SerializeOptions>(version());\n+  TF_ASSERT_OK_AND_ASSIGN(auto serialized,\n+                          Serialize(*sharding_spec, std::move(options)));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto out_sharding_spec,\n+      Deserialize<ConcreteShardingSpec>(serialized, /*options=*/nullptr));\n+\n+  EXPECT_THAT(out_sharding_spec->num_shards(), sharding_spec->num_shards());\n+  EXPECT_THAT(out_sharding_spec->dynamic_shape(),\n+              sharding_spec->dynamic_shape());\n+  EXPECT_THAT(out_sharding_spec->shard_dynamic_shapes(),\n+              ElementsAreArray(sharding_spec->shard_dynamic_shapes()));\n+}\n+\n+TEST_P(ShardingSpecSerDesTest, ConcreteEvenShardingSpecRoundTrip) {\n+  auto sharding_spec = ConcreteEvenShardingSpec::Create(\n+      num_shards(),\n+      /*shape=*/Shape({10 * num_shards(), 20}),\n+      /*shard_shape=*/Shape({10, 20}), /*is_fully_replicated=*/false);\n+\n+  auto options = std::make_unique<SerializeOptions>(version());\n+  TF_ASSERT_OK_AND_ASSIGN(auto serialized,\n+                          Serialize(*sharding_spec, std::move(options)));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto out_sharding_spec,\n+      Deserialize<ConcreteEvenShardingSpec>(serialized, /*options=*/nullptr));\n+\n+  EXPECT_THAT(out_sharding_spec->num_shards(), sharding_spec->num_shards());\n+  EXPECT_THAT(out_sharding_spec->shape(), sharding_spec->shape());\n+  EXPECT_THAT(out_sharding_spec->shard_shape(), sharding_spec->shard_shape());\n+  EXPECT_THAT(out_sharding_spec->IsFullyReplicated(),\n+              sharding_spec->IsFullyReplicated());\n+}\n+\n+TEST_P(ShardingSpecSerDesTest, ShardingParamShardingSpecRoundTrip) {\n+  auto sharding_spec = ShardingParamShardingSpec::Create(\n+      ShardingParam({num_shards(), 1}, {{0}, {num_shards()}}));\n+\n+  auto options = std::make_unique<SerializeOptions>(version());\n+  TF_ASSERT_OK_AND_ASSIGN(auto serialized,\n+                          Serialize(*sharding_spec, std::move(options)));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto out_sharding_spec,\n+      Deserialize<ShardingParamShardingSpec>(serialized, /*options=*/nullptr));\n+\n+  EXPECT_THAT(out_sharding_spec->num_shards(), sharding_spec->num_shards());\n+  EXPECT_THAT(out_sharding_spec->sharding_param(),\n+              sharding_spec->sharding_param());\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    SerDesVersion_NumShards, ShardingSpecSerDesTest,\n+    testing::Combine(testing::ValuesIn(test_util::AllSupportedSerDesVersions()),\n+                     testing::Values(2, 4)));\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla"
        },
        {
            "sha": "c000bcbf7a4e33c93fe04504b657c09eb319c9fe",
            "filename": "third_party/xla/xla/python/ifrt/sharding_spec_test.cc",
            "status": "added",
            "additions": 657,
            "deletions": 0,
            "changes": 657,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding_spec_test.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,657 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/hash/hash_testing.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"xla/python/ifrt/index.h\"\n+#include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/ir/sharding_param.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ::testing::ElementsAre;\n+using ::testing::ElementsAreArray;\n+using ::testing::HasSubstr;\n+using ::testing::SizeIs;\n+\n+struct ShardingSpecTestParam {\n+  int num_shards;\n+};\n+\n+class ShardingSpecTest : public testing::TestWithParam<ShardingSpecTestParam> {\n+ public:\n+  int num_shards() const { return GetParam().num_shards; }\n+};\n+\n+class SingleDeviceShardingSpecTest : public ShardingSpecTest {};\n+class OpaqueShardingSpecTest : public ShardingSpecTest {};\n+class ConcreteShardingSpecTest : public ShardingSpecTest {};\n+class ConcreteEvenShardingSpecTest : public ShardingSpecTest {};\n+class ShardingParamShardingSpecTest : public ShardingSpecTest {};\n+\n+TEST_P(SingleDeviceShardingSpecTest, IsFullyReplicated) {\n+  ShardingSpecRef sharding = SingleDeviceShardingSpec::Create();\n+  EXPECT_TRUE(sharding->IsFullyReplicated());\n+}\n+\n+TEST_P(SingleDeviceShardingSpecTest, GetShardShape) {\n+  ShardingSpecRef sharding = SingleDeviceShardingSpec::Create();\n+  EXPECT_THAT(sharding->GetShardShape(Shape({10, 20})),\n+              absl_testing::IsOkAndHolds(Shape({10, 20})));\n+}\n+\n+TEST_P(SingleDeviceShardingSpecTest, HasSamePartitioning) {\n+  ShardingSpecRef sharding0 = SingleDeviceShardingSpec::Create();\n+\n+  EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding0));\n+  {\n+    ShardingSpecRef sharding1 = SingleDeviceShardingSpec::Create();\n+    EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+}\n+\n+TEST_P(SingleDeviceShardingSpecTest, IndexDomains) {\n+  ShardingSpecRef sharding = SingleDeviceShardingSpec::Create();\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, sharding->IndexDomains(shape));\n+  EXPECT_THAT(index_domains, ElementsAre(IndexDomain(shape)));\n+}\n+\n+TEST_P(SingleDeviceShardingSpecTest, Disassemble) {\n+  ShardingSpecRef sharding = SingleDeviceShardingSpec::Create();\n+\n+  {  // Disassemble static shape.\n+    Shape shape({10, 20});\n+    TF_ASSERT_OK_AND_ASSIGN(auto disassembled, sharding->Disassemble(shape));\n+    ASSERT_THAT(disassembled, SizeIs(1));\n+    const auto& [result_shape, result_sharding] = disassembled[0];\n+    EXPECT_EQ(shape, result_shape);\n+    EXPECT_EQ(*result_sharding, *sharding);\n+  }\n+  {  // Disassemble dynamic shape.\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        DynamicShape dynamic_shape,\n+        DynamicShape::Create(Shape({10, 20}),\n+                             BoundedDynamicShapeTag({true, true})));\n+    TF_ASSERT_OK_AND_ASSIGN(auto disassembled,\n+                            sharding->Disassemble(dynamic_shape));\n+    ASSERT_THAT(disassembled, SizeIs(1));\n+    const auto& [result_shape, result_sharding] = disassembled[0];\n+    EXPECT_EQ(dynamic_shape, result_shape);\n+    EXPECT_EQ(*result_sharding, *sharding);\n+  }\n+}\n+\n+TEST_P(OpaqueShardingSpecTest, IsFullyReplicated) {\n+  ShardingSpecRef sharding = OpaqueShardingSpec::Create(2);\n+  EXPECT_FALSE(sharding->IsFullyReplicated());\n+}\n+\n+TEST_P(OpaqueShardingSpecTest, GetShardShape) {\n+  ShardingSpecRef sharding = OpaqueShardingSpec::Create(num_shards());\n+  EXPECT_THAT(\n+      sharding->GetShardShape(Shape({10, 20})),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"OpaqueShardingSpec does not have shard shape information\")));\n+}\n+\n+TEST_P(OpaqueShardingSpecTest, HasSamePartitioning) {\n+  ShardingSpecRef sharding0 = OpaqueShardingSpec::Create(2);\n+\n+  EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding0));\n+  // OpaqueShardingSpec::HasSamePartitioning currently only returns true for the\n+  // same object.\n+  ShardingSpecRef sharding1 = OpaqueShardingSpec::Create(2);\n+  EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+}\n+\n+TEST_P(OpaqueShardingSpecTest, FailedToDisassemble) {\n+  ShardingSpecRef sharding = OpaqueShardingSpec::Create(num_shards());\n+\n+  EXPECT_THAT(\n+      sharding->Disassemble(Shape({30})),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"OpaqueShardingSpec does not have shard shape information\")));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape dynamic_shape,\n+      DynamicShape::Create(Shape({30}), BoundedDynamicShapeTag({true})));\n+  EXPECT_THAT(\n+      sharding->Disassemble(dynamic_shape),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"OpaqueShardingSpec does not have shard shape information\")));\n+}\n+\n+TEST_P(OpaqueShardingSpecTest, IndexDomainsFails) {\n+  ShardingSpecRef sharding = OpaqueShardingSpec::Create(num_shards());\n+\n+  EXPECT_THAT(\n+      sharding->IndexDomains(Shape({30})),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"OpaqueShardingSpec does not have index domain information\")));\n+}\n+\n+TEST_P(OpaqueShardingSpecTest, Hash) {\n+  EXPECT_TRUE(absl::VerifyTypeImplementsAbslHashCorrectly({\n+      *OpaqueShardingSpec::Create(2),\n+      *OpaqueShardingSpec::Create(3),\n+  }));\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, IsFullyReplicated) {\n+  std::vector<Shape> shard_shapes{Shape({10}), Shape({20})};\n+  ShardingSpecRef sharding =\n+      ConcreteShardingSpec::Create(Shape({30}), shard_shapes);\n+  EXPECT_FALSE(sharding->IsFullyReplicated());\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, GetShardShapeSuccess) {\n+  Shape shard_shape({30});\n+  std::vector<Shape> shard_shapes(2, shard_shape);\n+  ShardingSpecRef sharding =\n+      ConcreteShardingSpec::Create(Shape({30}), shard_shapes);\n+  EXPECT_THAT(sharding->GetShardShape(Shape({30})),\n+              absl_testing::IsOkAndHolds(shard_shape));\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, GetShardShapeFailure) {\n+  std::vector<Shape> shard_shapes{Shape({10}), Shape({20})};\n+  ShardingSpecRef sharding =\n+      ConcreteShardingSpec::Create(Shape({30}), shard_shapes);\n+  EXPECT_THAT(\n+      sharding->GetShardShape(Shape({30})),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\"ConcreteShardingSpec does not have a fixed shard shape\")));\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, HasSamePartitioning) {\n+  std::vector<Shape> shard_shapes0{Shape({10}), Shape({20})};\n+  ShardingSpecRef sharding0 =\n+      ConcreteShardingSpec::Create(Shape({30}), shard_shapes0);\n+\n+  EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding0));\n+  {\n+    std::vector<Shape> shard_shapes1{Shape({10}), Shape({20})};\n+    ShardingSpecRef sharding1 =\n+        ConcreteShardingSpec::Create(Shape({30}), shard_shapes1);\n+    EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Different number of shards.\n+  {\n+    std::vector<Shape> shard_shapes1{Shape({10}), Shape({20}), Shape({30})};\n+    ShardingSpecRef sharding1 =\n+        ConcreteShardingSpec::Create(Shape({60}), shard_shapes1);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Difference shape.\n+  {\n+    std::vector<Shape> shard_shapes1{Shape({10}), Shape({20})};\n+    ShardingSpecRef sharding1 =\n+        ConcreteShardingSpec::Create(Shape({40}), shard_shapes1);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Different shard shapes.\n+  {\n+    std::vector<Shape> shard_shapes1{Shape({10000}), Shape({20})};\n+    ShardingSpecRef sharding1 =\n+        ConcreteShardingSpec::Create(Shape({30}), shard_shapes1);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, Disassemble) {\n+  std::vector<Shape> shard_shapes{Shape({3}), Shape({7}), Shape({3}),\n+                                  Shape({7})};\n+  ShardingSpecRef sharding =\n+      ConcreteShardingSpec::Create(Shape({20}), shard_shapes);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled,\n+                          sharding->Disassemble(Shape({20})));\n+  ASSERT_THAT(disassembled, SizeIs(4));\n+  for (int i = 0; i < 4; ++i) {\n+    const auto& [shape, result_sharding] = disassembled[i];\n+    EXPECT_EQ(shape, shard_shapes[i]);\n+    EXPECT_EQ(*result_sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, DisassembleDynamicShape) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape dynamic_shape,\n+      DynamicShape::Create(Shape({20}), BoundedDynamicShapeTag({true})));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape shard_dynamic_shape0,\n+      DynamicShape::Create(Shape({3}), BoundedDynamicShapeTag({true})));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape shard_dynamic_shape1,\n+      DynamicShape::Create(Shape({7}), BoundedDynamicShapeTag({true})));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape shard_dynamic_shape2,\n+      DynamicShape::Create(Shape({3}), BoundedDynamicShapeTag({true})));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape shard_dynamic_shape3,\n+      DynamicShape::Create(Shape({7}), BoundedDynamicShapeTag({true})));\n+  std::vector<DynamicShape> shard_dynamic_shapes{\n+      std::move(shard_dynamic_shape0),\n+      std::move(shard_dynamic_shape1),\n+      std::move(shard_dynamic_shape2),\n+      std::move(shard_dynamic_shape3),\n+  };\n+  auto sharding =\n+      ConcreteShardingSpec::Create(dynamic_shape, shard_dynamic_shapes);\n+  EXPECT_THAT(sharding->Disassemble(Shape({20})),\n+              absl_testing::StatusIs(\n+                  tsl::error::INVALID_ARGUMENT,\n+                  HasSubstr(\"ConcreteShardingSpec holds dynamic shape\")));\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(auto disassembled,\n+                            sharding->Disassemble(DynamicShape(dynamic_shape)));\n+    ASSERT_THAT(disassembled, SizeIs(4));\n+    for (int i = 0; i < 4; ++i) {\n+      const auto& [result_dynamic_shape, result_sharding] = disassembled[i];\n+      EXPECT_EQ(result_dynamic_shape, shard_dynamic_shapes[i]);\n+      EXPECT_EQ(*result_sharding, *SingleDeviceShardingSpec::Create());\n+    }\n+  }\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, DisassembleFailsForUnexpectedShape) {\n+  std::vector<Shape> shard_shapes{Shape({10}), Shape({20})};\n+  ShardingSpecRef sharding =\n+      ConcreteShardingSpec::Create(Shape({30}), shard_shapes);\n+\n+  EXPECT_THAT(sharding->Disassemble(Shape({40})),\n+              absl_testing::StatusIs(\n+                  tsl::error::INVALID_ARGUMENT,\n+                  HasSubstr(\"ConcreteShardingSpec can only disassemble\")));\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, IndexDomains) {\n+  std::vector<Shape> shard_shapes = {Shape({1}), Shape({2})};\n+  std::vector<IndexDomain> index_domains{\n+      IndexDomain(Index({0}), Shape({1})),\n+      IndexDomain(Index({1}), Shape({2})),\n+  };\n+  ShardingSpecRef sharding =\n+      ConcreteShardingSpec::Create(Shape({15}), shard_shapes, index_domains);\n+\n+  EXPECT_THAT(sharding->IndexDomains(Shape({15})),\n+              absl_testing::IsOkAndHolds(ElementsAreArray(index_domains)));\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, IndexDomainsMissing) {\n+  std::vector<Shape> shard_shapes{Shape({10}), Shape({20})};\n+  ShardingSpecRef sharding =\n+      ConcreteShardingSpec::Create(Shape({30}), shard_shapes);\n+\n+  EXPECT_THAT(\n+      sharding->IndexDomains(Shape({30})),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"ConcreteShardingSpec does not have index domain information\")));\n+}\n+\n+TEST_P(ConcreteShardingSpecTest, Hash) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto dynamic_shape,\n+      DynamicShape::Create(Shape({30}), BoundedDynamicShapeTag({true})));\n+  EXPECT_TRUE(absl::VerifyTypeImplementsAbslHashCorrectly({\n+      *ConcreteShardingSpec::Create(Shape({30}), {Shape({10}), Shape({20})}),\n+      *ConcreteShardingSpec::Create(dynamic_shape,\n+                                    {dynamic_shape, dynamic_shape}),\n+  }));\n+}\n+\n+TEST_P(ConcreteEvenShardingSpecTest, IsFullyReplicated) {\n+  {\n+    // Fully replicated.\n+    ShardingSpecRef sharding = ConcreteEvenShardingSpec::Create(\n+        /*num_shards=*/2, Shape({30}), Shape({15}),\n+        /*is_fully_replicated=*/true);\n+    EXPECT_TRUE(sharding->IsFullyReplicated());\n+  }\n+  {\n+    // Not fully replicated.\n+    ShardingSpecRef sharding = ConcreteEvenShardingSpec::Create(\n+        /*num_shards=*/2, Shape({30}), Shape({15}),\n+        /*is_fully_replicated=*/false);\n+    EXPECT_FALSE(sharding->IsFullyReplicated());\n+  }\n+}\n+\n+TEST_P(ConcreteEvenShardingSpecTest, GetShardShape) {\n+  ShardingSpecRef sharding = ConcreteEvenShardingSpec::Create(\n+      /*num_shards=*/2, Shape({30}), Shape({15}),\n+      /*is_fully_replicated=*/true);\n+  EXPECT_THAT(sharding->GetShardShape(Shape({30})),\n+              absl_testing::IsOkAndHolds(Shape({15})));\n+  EXPECT_THAT(\n+      sharding->GetShardShape(Shape({45})),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"ConcreteEvenShardingSpec has a shard shape for shape [30], \"\n+              \"but was asked to get a shard shape for shape [45]\")));\n+}\n+\n+TEST_P(ConcreteEvenShardingSpecTest, HasSamePartitioning) {\n+  ShardingSpecRef sharding0 = ConcreteEvenShardingSpec::Create(\n+      /*num_shards=*/2, Shape({30}), Shape({15}),\n+      /*is_fully_replicated=*/true);\n+\n+  EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding0));\n+  {\n+    ShardingSpecRef sharding1 = ConcreteEvenShardingSpec::Create(\n+        /*num_shards=*/2, Shape({30}), Shape({15}),\n+        /*is_fully_replicated=*/true);\n+    EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Different number of shards.\n+  {\n+    ShardingSpecRef sharding1 = ConcreteEvenShardingSpec::Create(\n+        /*num_shards=*/3, Shape({30}), Shape({15}),\n+        /*is_fully_replicated=*/true);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Difference shape.\n+  {\n+    ShardingSpecRef sharding1 = ConcreteEvenShardingSpec::Create(\n+        /*num_shards=*/2, Shape({45}), Shape({15}),\n+        /*is_fully_replicated=*/true);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Different shard shape.\n+  {\n+    ShardingSpecRef sharding1 = ConcreteEvenShardingSpec::Create(\n+        /*num_shards=*/2, Shape({30}), Shape({10}),\n+        /*is_fully_replicated=*/true);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Different is_fully_replicated.\n+  {\n+    ShardingSpecRef sharding1 = ConcreteEvenShardingSpec::Create(\n+        /*num_shards=*/2, Shape({30}), Shape({15}),\n+        /*is_fully_replicated=*/false);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+}\n+\n+TEST_P(ConcreteEvenShardingSpecTest, Disassemble) {\n+  ShardingSpecRef sharding =\n+      ConcreteEvenShardingSpec::Create(num_shards(), Shape({30}), Shape({5}),\n+                                       /*is_fully_replicated=*/false);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled,\n+                          sharding->Disassemble(Shape({30})));\n+  ASSERT_THAT(disassembled, SizeIs(num_shards()));\n+  for (int i = 0; i < num_shards(); ++i) {\n+    const auto& [shape, result_sharding] = disassembled[i];\n+    EXPECT_EQ(shape, Shape({5}));\n+    EXPECT_EQ(*result_sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_P(ConcreteEvenShardingSpecTest, DisassembleFailsForUnexpectedShape) {\n+  ShardingSpecRef sharding = ConcreteEvenShardingSpec::Create(\n+      /*num_shards=*/2, Shape({30}), Shape({15}),\n+      /*is_fully_replicated=*/false);\n+\n+  EXPECT_THAT(sharding->Disassemble(Shape({40})),\n+              absl_testing::StatusIs(\n+                  tsl::error::INVALID_ARGUMENT,\n+                  HasSubstr(\"ConcreteEvenShardingSpec can only disassemble\")));\n+}\n+\n+TEST_P(ConcreteEvenShardingSpecTest, IndexDomainsFails) {\n+  ShardingSpecRef sharding = ConcreteEvenShardingSpec::Create(\n+      /*num_shards=*/2, Shape({30}), Shape({5}),\n+      /*is_fully_replicated=*/false);\n+\n+  EXPECT_THAT(\n+      sharding->IndexDomains(Shape({30})),\n+      absl_testing::StatusIs(tsl::error::INVALID_ARGUMENT,\n+                             HasSubstr(\"ConcreteEvenShardingSpec does not have \"\n+                                       \"index domain information\")));\n+}\n+\n+TEST_P(ConcreteEvenShardingSpecTest, Hash) {\n+  EXPECT_TRUE(absl::VerifyTypeImplementsAbslHashCorrectly({\n+      *ConcreteEvenShardingSpec::Create(\n+          /*num_shards=*/2, Shape({30}), Shape({30}),\n+          /*is_fully_replicated=*/true),\n+      *ConcreteEvenShardingSpec::Create(/*num_shards=*/2, Shape({30}),\n+                                        Shape({15}),\n+                                        /*is_fully_replicated=*/false),\n+      *ConcreteEvenShardingSpec::Create(\n+          /*num_shards=*/3, Shape({30}), Shape({10}),\n+          /*is_fully_replicated=*/false),\n+  }));\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, IsFullyReplicated) {\n+  {\n+    // Fully replicated.\n+    ShardingParam param{/*dim_shards=*/{1, 1},\n+                        {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+    ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+    EXPECT_TRUE(param_sharding->IsFullyReplicated());\n+  }\n+  {\n+    // Not fully replicated.\n+    ShardingParam param{/*dim_shards=*/{1, 6},\n+                        {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+    ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+    EXPECT_FALSE(param_sharding->IsFullyReplicated());\n+  }\n+  {\n+    // Not fully replicated.\n+    ShardingParam param{/*dim_shards=*/{2, 3},\n+                        {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+    ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+    EXPECT_FALSE(param_sharding->IsFullyReplicated());\n+  }\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, GetShardShape) {\n+  ShardingParam param{/*dim_shards=*/{2, 3},\n+                      {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+  ShardingSpecRef sharding = ShardingParamShardingSpec::Create(param);\n+  EXPECT_THAT(sharding->GetShardShape(Shape({6, 6})),\n+              absl_testing::IsOkAndHolds(Shape({3, 2})));\n+  EXPECT_THAT(sharding->GetShardShape(Shape({6, 6, 6})),\n+              absl_testing::StatusIs(\n+                  tsl::error::INVALID_ARGUMENT,\n+                  HasSubstr(\"Numbers of dimensions don't match. From \"\n+                            \"Shape 3 vs from ShardingParam 2\")));\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, HasSamePartitioning) {\n+  ShardingParam param0{/*dim_shards=*/{2, 3},\n+                       {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+  ShardingSpecRef sharding0 = ShardingParamShardingSpec::Create(param0);\n+\n+  EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding0));\n+  {\n+    ShardingParam param1{/*dim_shards=*/{2, 3},\n+                         {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+    ShardingSpecRef sharding1 = ShardingParamShardingSpec::Create(param1);\n+    EXPECT_TRUE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Different number of shards.\n+  {\n+    ShardingParam param1{/*dim_shards=*/{3, 1},\n+                         {/*permutation=*/{1, 0}, /*axis_sizes=*/{1, 3}}};\n+    ShardingSpecRef sharding1 = ShardingParamShardingSpec::Create(param1);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+  // Different sharding param.\n+  {\n+    ShardingParam param1{/*dim_shards=*/{3, 2},\n+                         {/*permutation=*/{0, 1}, /*axis_sizes=*/{3, 2}}};\n+    ShardingSpecRef sharding1 = ShardingParamShardingSpec::Create(param1);\n+    EXPECT_FALSE(sharding0->HasSamePartitioning(*sharding1));\n+  }\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, Disassemble) {\n+  ShardingParam param{/*dim_shards=*/{2, 3},\n+                      {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+  ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(auto disassembled,\n+                            param_sharding->Disassemble(Shape({6, 6})));\n+    ASSERT_THAT(disassembled, SizeIs(6));\n+    for (int i = 0; i < 6; ++i) {\n+      const auto& [shape, sharding] = disassembled[i];\n+      EXPECT_EQ(shape, Shape({3, 2}));\n+      EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+    }\n+  }\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, DisassembleFailsWhenRankNotMatch) {\n+  ShardingParam param{/*dim_shards=*/{2, 3},\n+                      {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+  ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+\n+  EXPECT_THAT(param_sharding->Disassemble(Shape({6, 6, 6})),\n+              absl_testing::StatusIs(\n+                  tsl::error::INVALID_ARGUMENT,\n+                  HasSubstr(\"Numbers of dimensions don't match. From \"\n+                            \"Shape 3 vs from ShardingParam 2\")));\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, DisassembleFailsForUnevenSharding) {\n+  ShardingParam param{/*dim_shards=*/{2, 3},\n+                      {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+  ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+\n+  EXPECT_THAT(\n+      param_sharding->Disassemble(Shape({7, 6})),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\"Uneven shard is not supported. dim: 7, dim_shards: 2\")));\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, IndexDomain) {\n+  ShardingParam param{/*dim_shards=*/{2, 3},\n+                      {/*permutation=*/{0, 1}, /*axis_sizes=*/{2, 3}}};\n+  ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(auto index_domains,\n+                            param_sharding->IndexDomains(Shape({6, 6})));\n+    EXPECT_THAT(index_domains,\n+                ElementsAre(IndexDomain(Index({0, 0}), Shape({3, 2})),\n+                            IndexDomain(Index({0, 2}), Shape({3, 2})),\n+                            IndexDomain(Index({0, 4}), Shape({3, 2})),\n+                            IndexDomain(Index({3, 0}), Shape({3, 2})),\n+                            IndexDomain(Index({3, 2}), Shape({3, 2})),\n+                            IndexDomain(Index({3, 4}), Shape({3, 2}))));\n+  }\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, IndexDomainWithPermutation) {\n+  ShardingParam param{/*dim_shards=*/{2, 3},\n+                      {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}};\n+  ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(auto index_domains,\n+                            param_sharding->IndexDomains(Shape({6, 6})));\n+    EXPECT_THAT(index_domains,\n+                ElementsAre(IndexDomain(Index({0, 0}), Shape({3, 2})),\n+                            IndexDomain(Index({0, 4}), Shape({3, 2})),\n+                            IndexDomain(Index({3, 2}), Shape({3, 2})),\n+                            IndexDomain(Index({0, 2}), Shape({3, 2})),\n+                            IndexDomain(Index({3, 0}), Shape({3, 2})),\n+                            IndexDomain(Index({3, 4}), Shape({3, 2}))));\n+  }\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, IndexDomainWithReplication) {\n+  ShardingParam param{/*dim_shards=*/{2, 1},\n+                      {/*permutation=*/{0, 1}, /*axis_sizes=*/{2, 3}}};\n+  ShardingSpecRef param_sharding = ShardingParamShardingSpec::Create(param);\n+\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(auto index_domains,\n+                            param_sharding->IndexDomains(Shape({6, 6})));\n+    EXPECT_THAT(index_domains,\n+                ElementsAre(IndexDomain(Index({0, 0}), Shape({3, 6})),\n+                            IndexDomain(Index({0, 0}), Shape({3, 6})),\n+                            IndexDomain(Index({0, 0}), Shape({3, 6})),\n+                            IndexDomain(Index({3, 0}), Shape({3, 6})),\n+                            IndexDomain(Index({3, 0}), Shape({3, 6})),\n+                            IndexDomain(Index({3, 0}), Shape({3, 6}))));\n+  }\n+}\n+\n+TEST_P(ShardingParamShardingSpecTest, Hash) {\n+  EXPECT_TRUE(absl::VerifyTypeImplementsAbslHashCorrectly({\n+      *ShardingParamShardingSpec::Create(\n+          ShardingParam{/*dim_shards=*/{2, 3},\n+                        {/*permutation=*/{1, 0}, /*axis_sizes=*/{3, 2}}}),\n+      *ShardingParamShardingSpec::Create(\n+          ShardingParam{/*dim_shards=*/{3, 2},\n+                        {/*permutation=*/{0, 1}, /*axis_sizes=*/{3, 2}}}),\n+  }));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(NumShards, SingleDeviceShardingSpecTest,\n+                         testing::Values(ShardingSpecTestParam{\n+                             /*num_shards=*/6}));\n+INSTANTIATE_TEST_SUITE_P(NumShards, OpaqueShardingSpecTest,\n+                         testing::Values(ShardingSpecTestParam{\n+                             /*num_shards=*/6}));\n+INSTANTIATE_TEST_SUITE_P(NumShards, ConcreteShardingSpecTest,\n+                         testing::Values(ShardingSpecTestParam{\n+                             /*num_shards=*/6}));\n+INSTANTIATE_TEST_SUITE_P(NumShards, ConcreteEvenShardingSpecTest,\n+                         testing::Values(ShardingSpecTestParam{\n+                             /*num_shards=*/6}));\n+INSTANTIATE_TEST_SUITE_P(NumShards, ShardingParamShardingSpecTest,\n+                         testing::Values(ShardingSpecTestParam{\n+                             /*num_shards=*/6}));\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla"
        },
        {
            "sha": "af16ec4135c62af8774e232ad9177e2b3f337d8c",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/BUILD",
            "status": "modified",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -28,10 +28,12 @@ cc_library(\n     srcs = [\n         \"xla_compiler.cc\",\n         \"xla_sharding.cc\",\n+        \"xla_sharding_spec.cc\",\n     ],\n     hdrs = [\n         \"xla_compiler.h\",\n         \"xla_sharding.h\",\n+        \"xla_sharding_spec.h\",\n     ],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n@@ -84,6 +86,14 @@ tf_proto_library(\n     ],\n )\n \n+tf_proto_library(\n+    name = \"xla_sharding_spec_proto\",\n+    srcs = [\"xla_sharding_spec.proto\"],\n+    protodeps = [\n+        \"//xla:xla_data_proto\",\n+    ],\n+)\n+\n cc_library(\n     name = \"xla_sharding_serdes\",\n     srcs = [\"xla_sharding_serdes.cc\"],\n@@ -106,6 +116,26 @@ cc_library(\n     alwayslink = True,\n )\n \n+cc_library(\n+    name = \"xla_sharding_spec_serdes\",\n+    srcs = [\"xla_sharding_spec_serdes.cc\"],\n+    compatible_with = get_compatible_with_portable(),\n+    deps = [\n+        \":xla_ifrt\",\n+        \":xla_sharding_spec_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/python/ifrt:serdes\",\n+        \"//xla/python/ifrt:serdes_version\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//llvm:Support\",\n+    ],\n+    alwayslink = True,\n+)\n+\n xla_cc_test(\n     name = \"xla_sharding_serdes_test\",\n     srcs = [\"xla_sharding_serdes_test.cc\"],\n@@ -125,6 +155,23 @@ xla_cc_test(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"xla_sharding_spec_serdes_test\",\n+    srcs = [\"xla_sharding_spec_serdes_test.cc\"],\n+    deps = [\n+        \":xla_ifrt\",\n+        \":xla_sharding_spec_serdes\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/python/ifrt\",\n+        \"//xla/python/ifrt:serdes\",\n+        \"//xla/python/ifrt:serdes_test_util\",\n+        \"//xla/python/ifrt:serdes_version\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//llvm:Support\",\n+    ],\n+)\n+\n cc_library(\n     name = \"xla_executable_version\",\n     srcs = [\"xla_executable_version.cc\"],\n@@ -241,6 +288,25 @@ xla_cc_test(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"xla_sharding_spec_test\",\n+    size = \"small\",\n+    srcs = [\"xla_sharding_spec_test.cc\"],\n+    deps = [\n+        \":xla_ifrt\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:tile_assignment\",\n+        \"//xla/python/ifrt\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/hash:hash_testing\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n # TODO(hyeontaek): Move this target out of pjrt_ifrt.\n cc_library(\n     name = \"reshard_impl_test_lib\","
        },
        {
            "sha": "4224d8eea13375b5b3e2e0c966c03e2a87c5de05",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding_spec.cc",
            "status": "added",
            "additions": 295,
            "deletions": 0,
            "changes": 295,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,295 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/pjrt_ifrt/xla_sharding_spec.h\"\n+\n+#include <atomic>\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/base/optimization.h\"\n+#include \"absl/hash/hash.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/python/ifrt/index.h\"\n+#include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+\n+char XlaCompatibleShardingSpec::ID = 0;  // NOLINT\n+char HloShardingSpec::ID = 0;            // NOLINT\n+\n+namespace {\n+\n+// Generates IndexDomains for an HloShardingSpec, using XLA HloSharding APIs.\n+// Note that this is O(N^2) where N is the number of devices (shards).\n+std::vector<IndexDomain> IndexDomainsSlowPath(\n+    const xla::HloSharding& hlo_sharding, int num_shards, const Shape& shape) {\n+  // Only shape dimensions are used.\n+  auto xla_shape = xla::ShapeUtil::MakeShapeWithDescendingLayout(\n+      xla::PrimitiveType::S32, shape.dims());\n+  if (num_shards > 8) {\n+    LOG_FIRST_N(WARNING, 1) << \"Taking a slow path for \"\n+                               \"HloShardingSpec::IndexDomains(). This will not \"\n+                               \"scale for a large number of devices.\";\n+  }\n+\n+  std::vector<IndexDomain> result;\n+  result.reserve(num_shards);\n+\n+  Index::Elements origin(shape.dims().size());\n+  Shape::Dimensions shard_shape(shape.dims().size());\n+  for (int device_idx = 0; device_idx < num_shards; ++device_idx) {\n+    auto tile_offset = hlo_sharding.TileOffsetForDevice(xla_shape, device_idx);\n+    auto tile_limit = hlo_sharding.TileLimitForDevice(xla_shape, device_idx);\n+    for (int i = 0; i < shape.dims().size(); ++i) {\n+      origin[i] = tile_offset[i];\n+      shard_shape[i] = tile_limit[i] - tile_offset[i];\n+    }\n+    result.push_back(IndexDomain(Index(origin), Shape(shard_shape)));\n+  }\n+  return result;\n+}\n+\n+}  // namespace\n+\n+std::unique_ptr<HloShardingSpec> HloShardingSpec::Create(\n+    int num_shards, xla::HloSharding xla_hlo_sharding) {\n+  if (!xla_hlo_sharding.IsReplicated() && !xla_hlo_sharding.IsUnreduced() &&\n+      xla_hlo_sharding.IsTiled()) {\n+    CHECK_EQ(num_shards, xla_hlo_sharding.num_devices())\n+        << \"`num_shards` and `xla_hlo_sharding`'s `num_devices` does not \"\n+           \"match: \"\n+        << num_shards << \" vs. \" << xla_hlo_sharding.num_devices();\n+  }\n+  return std::unique_ptr<HloShardingSpec>(\n+      new HloShardingSpec(num_shards, std::move(xla_hlo_sharding)));\n+}\n+\n+HloShardingSpec::HloShardingSpec(int num_shards,\n+                                 xla::HloSharding xla_hlo_sharding)\n+    : llvm::RTTIExtends<HloShardingSpec, XlaCompatibleShardingSpec>(\n+          num_shards, /*is_fully_replicated=*/false),\n+      xla_hlo_sharding_(std::move(xla_hlo_sharding)) {\n+  is_fully_replicated_ =\n+      xla_hlo_sharding_.IsReplicated() ||\n+      ((xla_hlo_sharding_.IsTiled() || xla_hlo_sharding_.IsTileMaximal()) &&\n+       num_shards_ == 1);\n+}\n+\n+absl::StatusOr<Shape> HloShardingSpec::GetShardShape(const Shape& shape) const {\n+  if (xla_hlo_sharding_.IsTileMaximal() || xla_hlo_sharding_.IsManual() ||\n+      xla_hlo_sharding_.IsUnreduced() || xla_hlo_sharding_.IsUnknown()) {\n+    return shape;\n+  }\n+  if (shape.dims().size() != xla_hlo_sharding_.TiledDataRank()) {\n+    return InvalidArgument(\n+        \"Numbers of dimensions don't match. From Shape %d vs from \"\n+        \"HloSharding %d\",\n+        shape.dims().size(), xla_hlo_sharding_.TiledDataRank());\n+  }\n+  const absl::Span<const int64_t> sharding_dims =\n+      xla_hlo_sharding_.dimensions();\n+  Shape::Dimensions tile_shape;\n+  tile_shape.reserve(shape.dims().size());\n+  for (int64_t i = 0; i < shape.dims().size(); ++i) {\n+    tile_shape.push_back(xla::CeilOfRatio(shape.dims()[i], sharding_dims[i]));\n+  }\n+  return Shape(std::move(tile_shape));\n+}\n+\n+bool HloShardingSpec::HasSamePartitioning(const ShardingSpec& other) const {\n+  if (this == &other) {\n+    return true;\n+  }\n+  if (num_shards() != other.num_shards()) {\n+    return false;\n+  }\n+  const auto* other_hlo_sharding_spec = llvm::dyn_cast<HloShardingSpec>(&other);\n+  if (!other_hlo_sharding_spec) {\n+    return false;\n+  }\n+  return xla_hlo_sharding_ == other_hlo_sharding_spec->xla_hlo_sharding_;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>>\n+HloShardingSpec::Disassemble(const Shape& shape) const {\n+  bool is_even_sharding = false;\n+  if (xla_hlo_sharding_.IsReplicated() || xla_hlo_sharding_.IsTileMaximal() ||\n+      xla_hlo_sharding_.IsUnreduced()) {\n+    is_even_sharding = true;\n+  } else if (xla_hlo_sharding_.IsTiled()) {\n+    const int64_t tiled_data_rank = xla_hlo_sharding_.TiledDataRank();\n+    if (shape.dims().size() != tiled_data_rank) {\n+      return absl::InvalidArgumentError(absl::StrFormat(\n+          \"shape must have %d dimensions, but has %d dimensions: \"\n+          \"shape=%s, sharding=%s\",\n+          tiled_data_rank, shape.dims().size(), shape.DebugString(),\n+          xla_hlo_sharding_.ToString()));\n+    }\n+\n+    is_even_sharding = true;\n+    for (int i = 0; i < tiled_data_rank; ++i) {\n+      if (shape.dims()[i] % xla_hlo_sharding_.dimension(i) != 0) {\n+        is_even_sharding = false;\n+        break;\n+      }\n+    }\n+  } else if (xla_hlo_sharding_.IsManual()) {\n+    // By convention, MANUAL sharding has the same global/shard shapes.\n+    is_even_sharding = true;\n+  }\n+\n+  if (is_even_sharding) {\n+    TF_ASSIGN_OR_RETURN(Shape shard_shape, GetShardShape(shape));\n+    std::vector<std::pair<Shape, ShardingSpecRef>> result;\n+    result.reserve(num_shards_);\n+    for (int i = 0; i < num_shards_; ++i) {\n+      result.push_back({\n+          shard_shape,\n+          SingleDeviceShardingSpec::Create(),\n+      });\n+    }\n+    return result;\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(std::vector<IndexDomain> index_domains,\n+                      IndexDomains(shape));\n+  CHECK_EQ(index_domains.size(), num_shards_);\n+  std::vector<std::pair<Shape, ShardingSpecRef>> result;\n+  result.reserve(num_shards_);\n+  for (int i = 0; i < index_domains.size(); ++i) {\n+    result.push_back({\n+        index_domains[i].shape(),\n+        SingleDeviceShardingSpec::Create(),\n+    });\n+  }\n+  return result;\n+}\n+\n+absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+HloShardingSpec::Disassemble(const DynamicShape& dynamic_shape) const {\n+  return InvalidArgument(\n+      \"HloShardingSpec can only disassemble static shape, but was asked \"\n+      \"to disassemble dynamic shape %s\",\n+      dynamic_shape.DebugString());\n+}\n+\n+absl::StatusOr<std::vector<IndexDomain>> HloShardingSpec::IndexDomains(\n+    const Shape& shape) const {\n+  std::vector<IndexDomain> result;\n+\n+  if (xla_hlo_sharding_.IsManual()) {\n+    return absl::InvalidArgumentError(\n+        \"Manual sharding does not support IndexDomains\");\n+  }\n+  if (xla_hlo_sharding_.IsUnreduced()) {\n+    return absl::InvalidArgumentError(\n+        \"Unreduced sharding does not support IndexDomains\");\n+  }\n+  if (xla_hlo_sharding_.IsReplicated() || xla_hlo_sharding_.IsTileMaximal()) {\n+    // Fast path for a fully replicated or maximal sharding.\n+    IndexDomain element(shape);\n+    result.resize(/*count=*/num_shards_, /*value=*/element);\n+    return result;\n+  }\n+  if (!xla_hlo_sharding_.IsTiled()) {\n+    return IndexDomainsSlowPath(xla_hlo_sharding_, num_shards_, shape);\n+  }\n+  for (const xla::OpSharding::Type subgroup_type :\n+       xla_hlo_sharding_.subgroup_types()) {\n+    if (subgroup_type != xla::OpSharding::REPLICATED) {\n+      return IndexDomainsSlowPath(xla_hlo_sharding_, num_shards_, shape);\n+    }\n+  }\n+\n+  const int64_t tiled_data_rank = xla_hlo_sharding_.TiledDataRank();\n+  if (shape.dims().size() != tiled_data_rank) {\n+    return absl::InvalidArgumentError(\n+        absl::StrFormat(\"shape must have %d dimensions, but has %d dimensions: \"\n+                        \"shape=%s, sharding=%s\",\n+                        tiled_data_rank, shape.dims().size(),\n+                        shape.DebugString(), xla_hlo_sharding_.ToString()));\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(Shape tile_shape, GetShardShape(shape));\n+\n+  const absl::Span<const int64_t> shape_dims = shape.dims();\n+  std::vector<std::optional<IndexDomain>> all(num_shards_);\n+  TF_RETURN_IF_ERROR(xla_hlo_sharding_.EachTile(\n+      shape_dims, [shape_dims, &all](int device_index,\n+                                     absl::Span<const int64_t> tile_offset,\n+                                     absl::Span<const int64_t> tile_limit) {\n+        Shape::Dimensions tile_shape;\n+        tile_shape.reserve(shape_dims.size());\n+        for (int i = 0; i < shape_dims.size(); ++i) {\n+          tile_shape.push_back(tile_limit[i] - tile_offset[i]);\n+        }\n+        all[device_index] =\n+            IndexDomain(Index(tile_offset), Shape(std::move(tile_shape)));\n+      }));\n+\n+  result.reserve(num_shards_);\n+  for (int device_idx = 0; device_idx < num_shards_; ++device_idx) {\n+    result.push_back(*std::move(all[device_idx]));\n+  }\n+\n+  return result;\n+}\n+\n+std::string HloShardingSpec::DebugString() const {\n+  return absl::StrFormat(\"HloShardingSpec(num_shards: %d, hlo_sharding: %s)\",\n+                         num_shards_, xla_hlo_sharding_.ToString());\n+}\n+\n+void HloShardingSpec::Hash(absl::HashState state) const {\n+  uint64_t hash = hash_.load(std::memory_order_relaxed);\n+  if (hash == kUnsetHash) {\n+    hash = absl::HashOf(num_shards_, xla_hlo_sharding_);\n+    if (ABSL_PREDICT_FALSE(hash == kUnsetHash)) {\n+      ++hash;\n+    }\n+    hash_.store(hash, std::memory_order_relaxed);\n+  }\n+  absl::HashState::combine(std::move(state), hash);\n+}\n+\n+std::vector<IndexDomain> TEST_HloShardingSpecIndexDomainsSlowPath(\n+    const HloShardingSpec& sharding_spec, const Shape& shape) {\n+  return IndexDomainsSlowPath(sharding_spec.xla_hlo_sharding(),\n+                              sharding_spec.num_shards(), shape);\n+}\n+\n+}  // namespace ifrt\n+}  // namespace xla"
        },
        {
            "sha": "8047a218893e5e0db97d6578045d001201d992dc",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding_spec.h",
            "status": "added",
            "additions": 102,
            "deletions": 0,
            "changes": 102,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.h?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,102 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PYTHON_PJRT_IFRT_XLA_SHARDING_SPEC_H_\n+#define XLA_PYTHON_PJRT_IFRT_XLA_SHARDING_SPEC_H_\n+\n+#include <atomic>\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/hash/hash.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+\n+// XLA-compatible sharding spec types.\n+class XlaCompatibleShardingSpec\n+    : public llvm::RTTIExtends<XlaCompatibleShardingSpec, ShardingSpec> {\n+ public:\n+  using llvm::RTTIExtends<XlaCompatibleShardingSpec, ShardingSpec>::RTTIExtends;\n+\n+  static char ID;  // NOLINT\n+};\n+\n+// XLA `HloSharding` wrapper. `HloSharding` is the main sharding representation\n+// in XLA. This class holds an `HloSharding` to be used with IFRT as a\n+// `ShardingSpec`.\n+class HloShardingSpec final\n+    : public llvm::RTTIExtends<HloShardingSpec, XlaCompatibleShardingSpec> {\n+ public:\n+  // Creates an `HloShardingSpec` wrapper.\n+  static std::unique_ptr<HloShardingSpec> Create(\n+      int num_shards, xla::HloSharding xla_hlo_sharding);\n+\n+  // Returns the wrapped XLA `HloSharding`.\n+  const xla::HloSharding& xla_hlo_sharding() const { return xla_hlo_sharding_; }\n+\n+  // ShardingSpec implementation.\n+\n+  ~HloShardingSpec() override = default;\n+\n+  absl::StatusOr<Shape> GetShardShape(const Shape& shape) const override;\n+\n+  bool HasSamePartitioning(const ShardingSpec& other) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingSpecRef>>> Disassemble(\n+      const Shape& shape) const override;\n+\n+  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingSpecRef>>>\n+  Disassemble(const DynamicShape& dynamic_shape) const override;\n+\n+  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const override;\n+\n+  std::string DebugString() const override;\n+\n+  static char ID;  // NOLINT\n+\n+ private:\n+  HloShardingSpec(int num_shards, xla::HloSharding xla_hlo_sharding);\n+\n+  void Hash(absl::HashState state) const override;\n+\n+  xla::HloSharding xla_hlo_sharding_;\n+\n+  // Cached hash. 0 indicates the hash needs to be computed and cached.\n+  // May be written multiple times with the same non-zero value.\n+  static constexpr uint64_t kUnsetHash = 0;\n+  mutable std::atomic<uint64_t> hash_ = kUnsetHash;\n+};\n+\n+// Test only: returns `HloShardingSpec::IndexDomains()`, using\n+// `xla::HloSharding` APIs internally.\n+std::vector<IndexDomain> TEST_HloShardingSpecIndexDomainsSlowPath(\n+    const HloShardingSpec& sharding_spec, const Shape& shape);\n+\n+}  // namespace ifrt\n+}  // namespace xla\n+\n+#endif  // XLA_PYTHON_PJRT_IFRT_XLA_SHARDING_SPEC_H_"
        },
        {
            "sha": "85f6e9069f71a60625639676530a523a23c6691f",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding_spec.proto",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec.proto?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,31 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+syntax = \"proto3\";\n+\n+package xla.ifrt;\n+\n+import \"xla/xla_data.proto\";\n+\n+option java_multiple_files = true;\n+option java_outer_classname = \"XlaShardingSpec\";\n+\n+// Wire format for `HloShardingSpec`.\n+message HloShardingSpecProto {\n+  int32 version_number = 1;\n+\n+  int32 num_shards = 2;\n+  xla.OpSharding xla_op_sharding = 3;\n+}"
        },
        {
            "sha": "39b9de5e436faccdad9853ffe3490060c21e2f0e",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding_spec_serdes.cc",
            "status": "added",
            "additions": 102,
            "deletions": 0,
            "changes": 102,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_serdes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_serdes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_serdes.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,102 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/python/ifrt/serdes.h\"\n+#include \"xla/python/ifrt/serdes_version.h\"\n+#include \"xla/python/pjrt_ifrt/xla_sharding_spec.h\"\n+#include \"xla/python/pjrt_ifrt/xla_sharding_spec.pb.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+\n+namespace {\n+\n+// Serialization/deserialization for `HloShardingSpec`.\n+class HloShardingSpecSerDes\n+    : public llvm::RTTIExtends<HloShardingSpecSerDes, SerDes> {\n+ public:\n+  absl::string_view type_name() const override {\n+    return \"xla::ifrt::HloShardingSpec\";\n+  }\n+\n+  absl::StatusOr<std::string> Serialize(\n+      const Serializable& serializable,\n+      std::unique_ptr<SerializeOptions> options) override {\n+    const SerDesVersion version = GetRequestedSerDesVersion(options.get());\n+    if (version.version_number() < SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version.version_number(),\n+                       \" for HloShardingSpec serialization\"));\n+    }\n+\n+    const HloShardingSpec& sharding_spec =\n+        llvm::cast<HloShardingSpec>(serializable);\n+    HloShardingSpecProto proto;\n+    proto.set_version_number(SerDesVersionNumber(0).value());\n+    *proto.mutable_xla_op_sharding() =\n+        sharding_spec.xla_hlo_sharding().ToProto();\n+    proto.set_num_shards(sharding_spec.num_shards());\n+\n+    return proto.SerializeAsString();\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<Serializable>> Deserialize(\n+      const std::string& serialized,\n+      std::unique_ptr<DeserializeOptions> options) override {\n+    HloShardingSpecProto proto;\n+    if (!proto.ParseFromString(serialized)) {\n+      return absl::InvalidArgumentError(\n+          \"Failed to parse serialized HloShardingSpec\");\n+    }\n+    const SerDesVersionNumber version_number(proto.version_number());\n+    if (version_number != SerDesVersionNumber(0)) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Unsupported \", version_number,\n+                       \" for HloShardingSpec deserialization\"));\n+    }\n+    TF_ASSIGN_OR_RETURN(auto xla_hlo_sharding,\n+                        xla::HloSharding::FromProto(proto.xla_op_sharding()));\n+\n+    return HloShardingSpec::Create(proto.num_shards(),\n+                                   std::move(xla_hlo_sharding));\n+  }\n+\n+  static char ID;  // NOLINT\n+};\n+\n+[[maybe_unused]] char HloShardingSpecSerDes::ID = 0;  // NOLINT\n+\n+// clang-format off\n+bool register_hlo_sharding_spec_serdes = ([] {\n+  RegisterSerDes<HloShardingSpec>(\n+      std::make_unique<HloShardingSpecSerDes>());\n+}(), true);\n+// clang-format on\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla"
        },
        {
            "sha": "d3a64185b10d6f44fe26910c1ecdad555cd06db5",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding_spec_serdes_test.cc",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_serdes_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_serdes_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_serdes_test.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,70 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <tuple>\n+\n+#include <gtest/gtest.h>\n+#include \"llvm/Support/Casting.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/python/ifrt/serdes.h\"\n+#include \"xla/python/ifrt/serdes_test_util.h\"\n+#include \"xla/python/ifrt/serdes_version.h\"\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+#include \"xla/python/pjrt_ifrt/xla_sharding_spec.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ShardingSpecSerDesTestParam = std::tuple<SerDesVersion, int>;\n+\n+class ShardingSpecSerDesTest\n+    : public testing::TestWithParam<ShardingSpecSerDesTestParam> {\n+ public:\n+  SerDesVersion version() const { return std::get<0>(GetParam()); }\n+  int num_shards() const { return std::get<1>(GetParam()); }\n+};\n+\n+TEST_P(ShardingSpecSerDesTest, HloShardingSpecRoundTrip) {\n+  ASSERT_EQ(num_shards() % 2, 0);\n+  auto xla_hlo_sharding =\n+      xla::HloSharding::Tile(xla::TileAssignment({2, num_shards() / 2}));\n+  auto sharding_spec = HloShardingSpec::Create(num_shards(), xla_hlo_sharding);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto serialized,\n+      Serialize(*sharding_spec, std::make_unique<SerializeOptions>(version())));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto deserialized,\n+      Deserialize<ShardingSpec>(serialized, /*options=*/nullptr));\n+\n+  const auto* deserialized_spec =\n+      llvm::dyn_cast<HloShardingSpec>(deserialized.get());\n+  ASSERT_NE(deserialized_spec, nullptr);\n+  EXPECT_EQ(deserialized_spec->num_shards(), num_shards());\n+  EXPECT_EQ(deserialized_spec->xla_hlo_sharding(), xla_hlo_sharding);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    SerDesVersion_NumShards, ShardingSpecSerDesTest,\n+    testing::Combine(testing::ValuesIn(test_util::AllSupportedSerDesVersions()),\n+                     testing::Values(2, 4)));\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla"
        },
        {
            "sha": "7eafd7210d4c0a45e32680a700bec9860835ca81",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding_spec_test.cc",
            "status": "added",
            "additions": 521,
            "deletions": 0,
            "changes": 521,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9154330acef648e399495f1c996b141018e2e0a5/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_spec_test.cc?ref=9154330acef648e399495f1c996b141018e2e0a5",
            "patch": "@@ -0,0 +1,521 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/pjrt_ifrt/xla_sharding_spec.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <utility>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/hash/hash_testing.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/hlo/ir/tile_assignment.h\"\n+#include \"xla/python/ifrt/index.h\"\n+#include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding_spec.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ::testing::ElementsAre;\n+using ::testing::ElementsAreArray;\n+using ::testing::HasSubstr;\n+using ::testing::SizeIs;\n+\n+class HloShardingSpecTest : public testing::Test {};\n+\n+TEST_F(HloShardingSpecTest, CreateWithNumShardsMismatch) {\n+  auto xla_hlo_sharding = xla::HloSharding::IotaTile({2, 3});\n+  EXPECT_DEATH(\n+      HloShardingSpec::Create(/*num_shards=*/5, xla_hlo_sharding),\n+      \"`num_shards` and `xla_hlo_sharding`'s `num_devices` does not match\");\n+}\n+\n+TEST_F(HloShardingSpecTest, IsFullyReplicated) {\n+  int num_shards = 6;\n+  {\n+    // Fully replicated HloSharding is fully replicated.\n+    auto xla_hlo_sharding = xla::HloSharding::Replicate();\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_TRUE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Single-tile HloSharding is fully replicated.\n+    int num_shards = 1;\n+    auto xla_hlo_sharding = xla::HloSharding::IotaTile({1, 1});\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_TRUE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Multi-tile HloSharding with last_dim_replicate where all replices are on\n+    // the last tile dimension is fully replicated.\n+    auto xla_hlo_sharding = xla::HloSharding::PartialTile(\n+        xla::TileAssignment(xla::IotaTileAssignment::Create({1, 6})));\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_TRUE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Multi-tile HloSharding with last_dim_replicate where not all replices are\n+    // on the last tile dimension is not fully replicated.\n+    auto xla_hlo_sharding = xla::HloSharding::PartialTile(\n+        xla::TileAssignment(xla::IotaTileAssignment::Create({2, 3})));\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_FALSE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Multi-tile HloSharding with no last_dim_replicate is not fully\n+    // replicated.\n+    auto xla_hlo_sharding = xla::HloSharding::IotaTile({1, 6});\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_FALSE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Maximal HloSharding with a single device is fully replicated.\n+    int num_shards = 1;\n+    auto xla_hlo_sharding = xla::HloSharding::AssignDevice(/*device_id=*/0);\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_TRUE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Maximal HloSharding with more than one device is not fully replicated.\n+    auto xla_hlo_sharding = xla::HloSharding::AssignDevice(/*device_id=*/0);\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_FALSE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Manual HloSharding is not fully replicated.\n+    auto xla_hlo_sharding = xla::HloSharding::Manual();\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_FALSE(spec->IsFullyReplicated());\n+  }\n+  {\n+    // Unknown HloSharding is not fully replicated.\n+    auto xla_hlo_sharding = xla::HloSharding::Unknown();\n+    std::shared_ptr<const HloShardingSpec> spec =\n+        HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+    EXPECT_FALSE(spec->IsFullyReplicated());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, GetShardShape) {\n+  int num_shards = 6;\n+  auto xla_hlo_sharding = xla::HloSharding::IotaTile({2, 3});\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+  EXPECT_THAT(spec->GetShardShape(Shape({6, 6})),\n+              absl_testing::IsOkAndHolds(Shape({3, 2})));\n+  EXPECT_THAT(spec->GetShardShape(Shape({6, 6, 6})),\n+              absl_testing::StatusIs(\n+                  tsl::error::INVALID_ARGUMENT,\n+                  HasSubstr(\"Numbers of dimensions don't match. From \"\n+                            \"Shape 3 vs from HloSharding 2\")));\n+}\n+\n+TEST_F(HloShardingSpecTest, HasSamePartitioning) {\n+  int num_shards0 = 6;\n+  auto xla_hlo_sharding0 = xla::HloSharding::IotaTile({2, 3});\n+  std::shared_ptr<const HloShardingSpec> spec0 =\n+      HloShardingSpec::Create(num_shards0, xla_hlo_sharding0);\n+\n+  EXPECT_TRUE(spec0->HasSamePartitioning(*spec0));\n+  {\n+    // Different number of shards.\n+    int num_shards1 = 3;\n+    auto xla_hlo_sharding1 = xla::HloSharding::IotaTile({3, 1});\n+    std::shared_ptr<const HloShardingSpec> spec1 =\n+        HloShardingSpec::Create(num_shards1, xla_hlo_sharding1);\n+    EXPECT_FALSE(spec0->HasSamePartitioning(*spec1));\n+  }\n+  // Different HloSharding.\n+  {\n+    auto xla_hlo_sharding1 = xla::HloSharding::IotaTile({3, 2});\n+    std::shared_ptr<const HloShardingSpec> spec1 =\n+        HloShardingSpec::Create(num_shards0, xla_hlo_sharding1);\n+    EXPECT_FALSE(spec0->HasSamePartitioning(*spec1));\n+  }\n+\n+  // Replicated sharding with different numbers of devices.\n+  {\n+    int num_shards1 = 3;\n+    std::shared_ptr<const HloShardingSpec> spec0 =\n+        HloShardingSpec::Create(num_shards0, xla::HloSharding::Replicate());\n+    std::shared_ptr<const HloShardingSpec> spec1 =\n+        HloShardingSpec::Create(num_shards1, xla::HloSharding::Replicate());\n+    EXPECT_FALSE(spec0->HasSamePartitioning(*spec1));\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithReplication) {\n+  int num_shards = 6;\n+  // Fully replicated.\n+  auto xla_hlo_sharding = xla::HloSharding::Replicate();\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, spec->IndexDomains(shape));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAre(IndexDomain(shape), IndexDomain(shape), IndexDomain(shape),\n+                  IndexDomain(shape), IndexDomain(shape), IndexDomain(shape)));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAreArray(TEST_HloShardingSpecIndexDomainsSlowPath(*spec, shape)));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleWithReplication) {\n+  int num_shards = 6;\n+  // Fully replicated.\n+  auto xla_hlo_sharding = xla::HloSharding::Replicate();\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled, spec->Disassemble(shape));\n+  ASSERT_THAT(disassembled, SizeIs(6));\n+  for (int i = 0; i < 6; ++i) {\n+    const auto& [shape, sharding] = disassembled[i];\n+    EXPECT_EQ(shape, Shape({10, 20}));\n+    EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithTile) {\n+  int num_shards = 6;\n+  // 6-way sharded along axis 0, 1-way sharded along axis 1.\n+  auto xla_hlo_sharding = xla::HloSharding::Tile(xla::TileAssignment({6, 1}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({12, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, spec->IndexDomains(shape));\n+  EXPECT_THAT(index_domains,\n+              ElementsAre(IndexDomain(Index({0, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({2, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({4, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({6, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({8, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({10, 0}), Shape({2, 20}))));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAreArray(TEST_HloShardingSpecIndexDomainsSlowPath(*spec, shape)));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleWithTile) {\n+  int num_shards = 6;\n+  // 6-way sharded along axis 0, 1-way sharded along axis 1.\n+  auto xla_hlo_sharding = xla::HloSharding::Tile(xla::TileAssignment({6, 1}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({12, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled, spec->Disassemble(shape));\n+  ASSERT_THAT(disassembled, SizeIs(6));\n+  for (int i = 0; i < 6; ++i) {\n+    const auto& [shape, sharding] = disassembled[i];\n+    EXPECT_EQ(shape, Shape({2, 20}));\n+    EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithUnevenTile) {\n+  int num_shards = 6;\n+  // 6-way sharded along axis 0, 1-way sharded along axis 1.\n+  auto xla_hlo_sharding = xla::HloSharding::Tile(xla::TileAssignment({6, 1}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({11, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, spec->IndexDomains(shape));\n+  EXPECT_THAT(index_domains,\n+              ElementsAre(IndexDomain(Index({0, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({2, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({4, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({6, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({8, 0}), Shape({2, 20})),\n+                          IndexDomain(Index({10, 0}), Shape({1, 20}))));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAreArray(TEST_HloShardingSpecIndexDomainsSlowPath(*spec, shape)));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleWithUnevenTile) {\n+  int num_shards = 6;\n+  // 6-way sharded along axis 0, 1-way sharded along axis 1.\n+  auto xla_hlo_sharding = xla::HloSharding::Tile(xla::TileAssignment({6, 1}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({11, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled, spec->Disassemble(shape));\n+  ASSERT_THAT(disassembled, SizeIs(6));\n+  for (int i = 0; i < 6; ++i) {\n+    const auto& [shape, sharding] = disassembled[i];\n+    if (i < 5) {\n+      EXPECT_EQ(shape, Shape({2, 20}));\n+    } else {\n+      EXPECT_EQ(shape, Shape({1, 20}));\n+    }\n+    EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithPartialTile) {\n+  int num_shards = 6;\n+  // 2-way sharded along axis 0, 1-way sharded along axis 1, each shard\n+  // replicated by 3 times.\n+  auto xla_hlo_sharding =\n+      xla::HloSharding::PartialTile(xla::TileAssignment({2, 1, 3}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, spec->IndexDomains(shape));\n+  EXPECT_THAT(index_domains,\n+              ElementsAre(IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20}))));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAreArray(TEST_HloShardingSpecIndexDomainsSlowPath(*spec, shape)));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleWithPartialTile) {\n+  int num_shards = 6;\n+  // 2-way sharded along axis 0, 1-way sharded along axis 1, each shard\n+  // replicated by 3 times.\n+  auto xla_hlo_sharding =\n+      xla::HloSharding::PartialTile(xla::TileAssignment({2, 1, 3}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled, spec->Disassemble(shape));\n+  ASSERT_THAT(disassembled, SizeIs(6));\n+  for (int i = 0; i < 6; ++i) {\n+    const auto& [shape, sharding] = disassembled[i];\n+    EXPECT_EQ(shape, Shape({5, 20}));\n+    EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithSubgroupReplicated) {\n+  int num_shards = 6;\n+  // 2-way sharded along axis 0, 1-way sharded along axis 1, each shard\n+  // replicated by 3 times.\n+  auto xla_hlo_sharding = xla::HloSharding::Subgroup(\n+      xla::TileAssignment({2, 1, 3}), {xla::OpSharding::REPLICATED});\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, spec->IndexDomains(shape));\n+  EXPECT_THAT(index_domains,\n+              ElementsAre(IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20}))));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAreArray(TEST_HloShardingSpecIndexDomainsSlowPath(*spec, shape)));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleWithSubgroupReplicated) {\n+  int num_shards = 6;\n+  // 2-way sharded along axis 0, 1-way sharded along axis 1, each shard\n+  // replicated by 3 times.\n+  auto xla_hlo_sharding = xla::HloSharding::Subgroup(\n+      xla::TileAssignment({2, 1, 3}), {xla::OpSharding::REPLICATED});\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled, spec->Disassemble(shape));\n+  ASSERT_THAT(disassembled, SizeIs(6));\n+  for (int i = 0; i < 6; ++i) {\n+    const auto& [shape, sharding] = disassembled[i];\n+    EXPECT_EQ(shape, Shape({5, 20}));\n+    EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithSubgroupMaximalSlowPath) {\n+  int num_shards = 6;\n+  // 2-way sharded along axis 0, 1-way sharded along axis 1, each shard\n+  // maximal-replicated by 3 times, device#0 in each replication is maximal.\n+  auto xla_hlo_sharding = xla::HloSharding::Subgroup(\n+      xla::TileAssignment({2, 1, 3}), {xla::OpSharding::MAXIMAL});\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, spec->IndexDomains(shape));\n+  EXPECT_THAT(index_domains,\n+              ElementsAre(IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({0, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20})),\n+                          IndexDomain(Index({5, 0}), Shape({5, 20}))));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAreArray(TEST_HloShardingSpecIndexDomainsSlowPath(*spec, shape)));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleWithSubgroupMaximalSlowPath) {\n+  int num_shards = 6;\n+  // 2-way sharded along axis 0, 1-way sharded along axis 1, each shard\n+  // maximal-replicated by 3 times, device#0 in each replication is maximal.\n+  auto xla_hlo_sharding = xla::HloSharding::Subgroup(\n+      xla::TileAssignment({2, 1, 3}), {xla::OpSharding::MAXIMAL});\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled, spec->Disassemble(shape));\n+  ASSERT_THAT(disassembled, SizeIs(6));\n+  for (int i = 0; i < 6; ++i) {\n+    const auto& [shape, sharding] = disassembled[i];\n+    EXPECT_EQ(shape, Shape({5, 20}));\n+    EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithTileTranspose) {\n+  int num_shards = 4;\n+  auto xla_hlo_sharding =\n+      xla::HloSharding::IotaTile(/*tile_assignment_dims=*/{2, 2},\n+                                 /*reshape_dims=*/{2, 2},\n+                                 /*transpose_perm=*/{1, 0});\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+  Shape shape({4, 4});\n+  TF_ASSERT_OK_AND_ASSIGN(auto index_domains, spec->IndexDomains(shape));\n+  EXPECT_THAT(\n+      index_domains,\n+      ElementsAreArray(TEST_HloShardingSpecIndexDomainsSlowPath(*spec, shape)));\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithUnreduced) {\n+  int num_shards = 6;\n+  auto xla_hlo_sharding = xla::HloSharding::Unreduced();\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  EXPECT_THAT(\n+      spec->IndexDomains(shape).status(),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\"Unreduced sharding does not support IndexDomains\")));\n+}\n+\n+TEST_F(HloShardingSpecTest, IndexDomainsWithManual) {\n+  int num_shards = 6;\n+  auto xla_hlo_sharding = xla::HloSharding::Manual();\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  EXPECT_THAT(spec->IndexDomains(shape).status(),\n+              absl_testing::StatusIs(\n+                  tsl::error::INVALID_ARGUMENT,\n+                  HasSubstr(\"Manual sharding does not support IndexDomains\")));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleWithManual) {\n+  int num_shards = 6;\n+  auto xla_hlo_sharding = xla::HloSharding::Manual();\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10, 20});\n+  TF_ASSERT_OK_AND_ASSIGN(auto disassembled, spec->Disassemble(shape));\n+  ASSERT_THAT(disassembled, SizeIs(6));\n+  for (int i = 0; i < 6; ++i) {\n+    const auto& [shape, sharding] = disassembled[i];\n+    EXPECT_EQ(shape, Shape({10, 20}));\n+    EXPECT_EQ(*sharding, *SingleDeviceShardingSpec::Create());\n+  }\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleFailsWithMismatchingShapeDimsSize) {\n+  int num_shards = 2;\n+  // 2-way sharded along axis 0, 1-way sharded along axis 1.\n+  auto xla_hlo_sharding = xla::HloSharding::Tile(xla::TileAssignment({2, 1}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  Shape shape({10});\n+  EXPECT_THAT(\n+      spec->Disassemble(shape),\n+      absl_testing::StatusIs(\n+          tsl::error::INVALID_ARGUMENT,\n+          HasSubstr(\"shape must have 2 dimensions, but has 1 dimensions\")));\n+}\n+\n+TEST_F(HloShardingSpecTest, DisassembleFailsWithDynamicShape) {\n+  int num_shards = 2;\n+  auto xla_hlo_sharding =\n+      xla::HloSharding::Tile(xla::TileAssignment(absl::Span<const int64_t>{2}));\n+  std::shared_ptr<const HloShardingSpec> spec =\n+      HloShardingSpec::Create(num_shards, xla_hlo_sharding);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DynamicShape dynamic_shape,\n+      DynamicShape::Create(Shape({10}), BoundedDynamicShapeTag({true})));\n+  EXPECT_THAT(\n+      spec->Disassemble(dynamic_shape),\n+      absl_testing::StatusIs(tsl::error::INVALID_ARGUMENT,\n+                             HasSubstr(\"can only disassemble static shape\")));\n+}\n+\n+TEST_F(HloShardingSpecTest, Hash) {\n+  EXPECT_TRUE(absl::VerifyTypeImplementsAbslHashCorrectly({\n+      HloShardingSpec::Create(6, xla::HloSharding::Replicate()),\n+      HloShardingSpec::Create(1, xla::HloSharding::Replicate()),\n+      HloShardingSpec::Create(6,\n+                              xla::HloSharding::AssignDevice(/*device_id=*/0)),\n+      HloShardingSpec::Create(\n+          6, xla::HloSharding::PartialTile(\n+                 xla::TileAssignment(xla::IotaTileAssignment::Create({2, 3})))),\n+  }));\n+}\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 3618,
        "additions": 3618,
        "deletions": 0
    }
}