{
    "author": "GleasonK",
    "message": "[StableHLO->HLO] Add pass to sanitize unregistered attributes on lowering.\n\nPiperOrigin-RevId: 806420245",
    "sha": "6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
    "files": [
        {
            "sha": "ae2dcfb2e44c358616a5e43251840f393c7442ca",
            "filename": "third_party/xla/xla/hlo/translate/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -134,9 +134,3 @@ cc_library(\n         \"@stablehlo//:stablehlo_passes_optimization\",\n     ],\n )\n-\n-cc_library(\n-    name = \"attributes\",\n-    hdrs = [\"attributes.h\"],\n-    compatible_with = get_compatible_with_portable(),\n-)"
        },
        {
            "sha": "574e84831fbf10939f4ac199396559378d36d1c5",
            "filename": "third_party/xla/xla/hlo/translate/attributes.h",
            "status": "removed",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07c6380860b1264d2c9347fb8747daa1f59bd90e/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fattributes.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07c6380860b1264d2c9347fb8747daa1f59bd90e/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fattributes.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fattributes.h?ref=07c6380860b1264d2c9347fb8747daa1f59bd90e",
            "patch": "@@ -1,24 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_HLO_TRANSLATE_ATTRIBUTES_H_\n-#define XLA_HLO_TRANSLATE_ATTRIBUTES_H_\n-\n-constexpr char kFrontendAttributesAttr[] = \"mhlo.frontend_attributes\";\n-constexpr char kShardingAttr[] = \"mhlo.sharding\";\n-constexpr char kParameterReplicationAttr[] = \"mhlo.parameter_replication\";\n-constexpr char kOriginalValueAttr[] = \"mhlo.original_value\";\n-\n-#endif  // XLA_HLO_TRANSLATE_ATTRIBUTES_H_"
        },
        {
            "sha": "f1d2fd7b569bb0d207da9e9fde0f864fc9a95101",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2FBUILD?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -42,6 +42,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/mlir_hlo\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"//xla/tsl/platform:errors\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n@@ -104,8 +105,8 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/translate:attributes\",\n         \"//xla/mlir_hlo\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -224,7 +225,7 @@ cc_library(\n         \"stack_location_utils\",\n         \":hlo_utils\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/translate:attributes\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Support\",\n     ],\n@@ -243,6 +244,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/mlir_hlo\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"//xla/service:computation_layout\",\n         \"//xla/service:hlo_module_config\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "100461ac46b60c259a2b4ea3f40aed246fff4a26",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/async_importer.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/hlo/translate/hlo_to_mhlo/attribute_importer.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/hlo_utils.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -52,9 +53,6 @@ namespace xla {\n \n namespace {\n \n-constexpr char kFrontendAttributesAttr[] = \"mhlo.frontend_attributes\";\n-constexpr char kShardingAttr[] = \"mhlo.sharding\";\n-\n // ============\n // Imports an old-style async start op. E.g. an HLO all-gather-start\n // instruction is imported as an async-start associated with an all-gather\n@@ -103,8 +101,8 @@ absl::StatusOr<mlir::Operation*> ImportOldStyleAsyncStart(\n   async_attributes.push_back(builder->getNamedAttr(\n       \"called_computation\",\n       mlir::FlatSymbolRefAttr::get(builder->getContext(), function.getName())));\n-  async_attributes.push_back(builder->getNamedAttr(\n-      \"execution_thread\", builder->getStringAttr(\"main\")));\n+  async_attributes.push_back(\n+      builder->getNamedAttr(kExecutionThread, builder->getStringAttr(\"main\")));\n \n   // Attach the frontend_attributes and sharding attributes to the async op\n   // instead of the sync op. First, semantically sharding attributes cannot be\n@@ -114,8 +112,8 @@ absl::StatusOr<mlir::Operation*> ImportOldStyleAsyncStart(\n   // the `mhlo.async_start` ops, so attaching them to the sync op will make them\n   // disappear during StableHLO/MHLO to HLO lowering.\n   for (auto it = attributes.begin(); it != attributes.end();) {\n-    if (it->getName() == kShardingAttr ||\n-        it->getName() == kFrontendAttributesAttr) {\n+    if (it->getName() == xla::kMhloSharding ||\n+        it->getName() == xla::kMhloFrontendAttributes) {\n       async_attributes.push_back(*it);\n       it = attributes.erase(it);\n     } else {\n@@ -134,7 +132,7 @@ absl::StatusOr<mlir::Operation*> ImportOldStyleAsyncStart(\n   async_builder.create<mlir::func::ReturnOp>(loc, sync_operation->getResults());\n   TF_RETURN_IF_ERROR(mutate_op(sync_operation));\n \n-  function->setAttr(\"execution_thread\", builder->getStringAttr(\"main\"));\n+  function->setAttr(kExecutionThread, builder->getStringAttr(\"main\"));\n \n   auto bundle_result_type =\n       mlir::mhlo::AsyncBundleType::get(context, result_types);\n@@ -245,7 +243,7 @@ absl::StatusOr<mlir::Operation*> ImportSend(\n       // to grab a slice of the sharding. All shardings are maximal, so we\n       // just need 1 of them.\n       send->setAttr(\n-          kShardingAttr,\n+          xla::kMhloSharding,\n           mlir::StringAttr::get(\n               builder->getContext(),\n               HloSharding::FromProto(sharding.ToProto().tuple_shardings()[0])\n@@ -317,7 +315,7 @@ absl::StatusOr<mlir::Operation*> ImportRecv(\n         OpSharding sharding_proto = sharding.ToProto();\n         auto* tuple_shardings = sharding_proto.mutable_tuple_shardings();\n         tuple_shardings->DeleteSubrange(1, 1);\n-        recv->setAttr(kShardingAttr,\n+        recv->setAttr(xla::kMhloSharding,\n                       mlir::StringAttr::get(\n                           builder->getContext(),\n                           HloSharding::FromProto(sharding_proto)->ToString()));"
        },
        {
            "sha": "51d2a6ee818f4c7453eff9192d17a24b69ba4555",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/hlo_function_importer.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -69,7 +69,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/ir/hlo_sharding_metadata.h\"\n-#include \"xla/hlo/translate/attributes.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/async_importer.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/attribute_importer.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/custom_call_importer.h\"\n@@ -78,6 +77,7 @@ limitations under the License.\n #include \"xla/layout.h\"\n #include \"xla/literal.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/protobuf_util.h\"\n #include \"xla/service/hlo.pb.h\"\n@@ -440,7 +440,7 @@ absl::StatusOr<FuncOp> HloFunctionImporter::ImportAsFunc(\n       for (int i = 0; i < leaf_count; ++i) {\n         if (!flattened_shardings.empty()) {\n           function.setArgAttr(\n-              arg_index, kShardingAttr,\n+              arg_index, xla::kMhloSharding,\n               ConvertSharding(flattened_shardings[i], builder_));\n         }\n         if (frontend_attributes) {\n@@ -449,12 +449,12 @@ absl::StatusOr<FuncOp> HloFunctionImporter::ImportAsFunc(\n                 \"A tuple parameter that is being flattened shouldn't have \"\n                 \"frontend attributes\");\n           }\n-          function.setArgAttr(arg_index, kFrontendAttributesAttr,\n+          function.setArgAttr(arg_index, xla::kMhloFrontendAttributes,\n                               frontend_attributes);\n         }\n         if (parameter->parameter_replicated_at_leaf_buffers() &&\n             parameter->parameter_replicated_at_leaf_buffers()->at(i)) {\n-          function.setArgAttr(arg_index, kParameterReplicationAttr,\n+          function.setArgAttr(arg_index, xla::kMhloParameterReplication,\n                               builder_->getBoolArrayAttr({true}));\n         }\n         // NOTE: since we are flattening args, all arguments will share the same\n@@ -465,12 +465,12 @@ absl::StatusOr<FuncOp> HloFunctionImporter::ImportAsFunc(\n       }\n     } else {\n       if (parameter->has_sharding()) {\n-        function.setArgAttr(arg_index, kShardingAttr,\n+        function.setArgAttr(arg_index, xla::kMhloSharding,\n                             ConvertSharding(parameter->sharding(), builder_));\n       }\n       if (frontend_attributes) {\n         function.setArgAttr(\n-            arg_index, kFrontendAttributesAttr,\n+            arg_index, xla::kMhloFrontendAttributes,\n             GetFrontendAttributes(*builder_, parameter->frontend_attributes()));\n       }\n       if (parameter->parameter_replicated_at_leaf_buffers().has_value()) {\n@@ -483,7 +483,7 @@ absl::StatusOr<FuncOp> HloFunctionImporter::ImportAsFunc(\n         }\n         if (nontrival) {\n           function.setArgAttr(\n-              arg_index, kParameterReplicationAttr,\n+              arg_index, xla::kMhloParameterReplication,\n               builder_->getBoolArrayAttr(replicated_at_leaf_buffers));\n         }\n       }\n@@ -497,7 +497,7 @@ absl::StatusOr<FuncOp> HloFunctionImporter::ImportAsFunc(\n   bool is_token = computation.root_instruction()->shape().IsToken();\n   if (is_token && computation.root_instruction()->has_sharding()) {\n     function.setResultAttr(\n-        0, kShardingAttr,\n+        0, xla::kMhloSharding,\n         ConvertSharding(computation.root_instruction()->sharding(), builder_));\n   }\n   if (!is_token && computation.root_instruction()->has_sharding()) {\n@@ -512,13 +512,13 @@ absl::StatusOr<FuncOp> HloFunctionImporter::ImportAsFunc(\n     }\n     for (const auto& [ret_index, ret_sharding] :\n          llvm::enumerate(ret_shardings)) {\n-      function.setResultAttr(ret_index, kShardingAttr,\n+      function.setResultAttr(ret_index, xla::kMhloSharding,\n                              ConvertSharding(ret_sharding, builder_));\n     }\n   }\n   if (computation.execution_thread() != \"main\") {\n     function->setAttr(\n-        \"execution_thread\",\n+        kExecutionThread,\n         builder_->getStringAttr(ToStringRef(computation.execution_thread())));\n   }\n \n@@ -713,7 +713,8 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n   llvm::SmallVector<NamedAttribute, 10> attributes;\n   if (instruction->has_sharding()) {\n     attributes.push_back(builder_->getNamedAttr(\n-        kShardingAttr, ConvertSharding(instruction->sharding(), builder_)));\n+        xla::kMhloSharding,\n+        ConvertSharding(instruction->sharding(), builder_)));\n   }\n \n   llvm::SmallVector<NamedAttribute, 4> frontend_attributes;\n@@ -726,7 +727,7 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n   if (!frontend_attributes.empty()) {\n     frontend_attributes_index = attributes.size();\n     attributes.push_back(builder_->getNamedAttr(\n-        kFrontendAttributesAttr,\n+        xla::kMhloFrontendAttributes,\n         builder_->getDictionaryAttr(frontend_attributes)));\n   }\n \n@@ -772,7 +773,7 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n       auto execution_thread = ToStringRef(async_op->async_execution_thread());\n       attributes.push_back(builder_->getNamedAttr(\n           \"execution_thread\", builder_->getStringAttr(execution_thread)));\n-      function->setAttr(\"execution_thread\",\n+      function->setAttr(xla::kExecutionThread,\n                         builder_->getStringAttr(execution_thread));\n \n       if (instruction->opcode() == HloOpcode::kAsyncStart) {\n@@ -955,7 +956,7 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n           attributes.erase(attributes.begin() + frontend_attributes_index);\n         } else {\n           attributes[frontend_attributes_index] = builder_->getNamedAttr(\n-              kFrontendAttributesAttr,\n+              xla::kMhloFrontendAttributes,\n               builder_->getDictionaryAttr(fe_attrs_without_composite_attrs));\n         }\n \n@@ -998,11 +999,11 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n           if (frontend_attributes.size() == 1) {\n             frontend_attributes_index = attributes.size();\n             attributes.push_back(builder_->getNamedAttr(\n-                kFrontendAttributesAttr,\n+                xla::kMhloFrontendAttributes,\n                 builder_->getDictionaryAttr(frontend_attributes)));\n           } else {\n             attributes[frontend_attributes_index] = builder_->getNamedAttr(\n-                kFrontendAttributesAttr,\n+                xla::kMhloFrontendAttributes,\n                 builder_->getDictionaryAttr(frontend_attributes));\n           }\n         }\n@@ -1740,9 +1741,9 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n               {rng_op->operand(0)->shape(), instruction->shape()});\n           HloSharding tuple_sharding = HloSharding::Tuple(\n               tuple_shape, {HloSharding::Replicate(), instruction->sharding()});\n-          CHECK_EQ(attributes.front().getName().str(), kShardingAttr);\n+          CHECK_EQ(attributes.front().getName().str(), xla::kMhloSharding);\n           attributes.front() = builder_->getNamedAttr(\n-              kShardingAttr, ConvertSharding(tuple_sharding, builder_));\n+              xla::kMhloSharding, ConvertSharding(tuple_sharding, builder_));\n         }\n       }\n       CHECK_EQ(flattened_ret_types.size(), 2);\n@@ -2250,7 +2251,7 @@ absl::StatusOr<mlir::Operation*> HloFunctionImporter::ImportInstructionImpl(\n }\n \n void SetXlaShape(mlir::Operation* op, const Shape& shape) {\n-  op->setAttr(\"xla_shape\",\n+  op->setAttr(xla::kXlaShape,\n               mlir::Builder(op->getContext())\n                   .getStringAttr(shape.ToString(/*print_layout=*/true)));\n }"
        },
        {
            "sha": "f7cad3db07bacf7fb596925c57cd7ca260a86573",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/location_importer.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Flocation_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Flocation_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Flocation_importer.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/translate/attributes.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/hlo_utils.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/stack_location_utils.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n \n namespace mlir {\n namespace hlo {\n@@ -36,9 +36,10 @@ mlir::Location GenerateInstructionLocation(\n   auto fuse_original_value_if_present = [&](mlir::Location loc) {\n     auto original_value = instruction->original_value();\n     if (original_value) {\n-      return b.getFusedLoc({loc, mlir::NameLoc::get(b.getStringAttr(\n-                                     std::string(kOriginalValueAttr) + \"={\" +\n-                                     original_value->ToString() + \"}\"))});\n+      return b.getFusedLoc(\n+          {loc, mlir::NameLoc::get(\n+                    b.getStringAttr(std::string(xla::kMhloOriginalValueAttr) +\n+                                    \"={\" + original_value->ToString() + \"}\"))});\n     }\n     return loc;\n   };"
        },
        {
            "sha": "8d9700036412ac616a0a366fbb58220e762070b2",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/module_attributes_importer.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 41,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fmodule_attributes_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fmodule_attributes_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fmodule_attributes_importer.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/layout.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n #include \"xla/service/computation_layout.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/shape.h\"\n@@ -52,23 +53,8 @@ limitations under the License.\n namespace xla {\n namespace {\n \n-constexpr char kCrossProgramPrefetches[] = \"mhlo.cross_program_prefetches\";\n-constexpr char kEntryComputationParameterLayouts[] =\n-    \"mhlo.xla_entry_computation_parameter_layouts\";\n-constexpr char kEntryComputationParameterTiles[] =\n-    \"mhlo.xla_entry_computation_parameter_tiles\";\n-constexpr char kEntryComputationResultLayout[] =\n-    \"mhlo.xla_entry_computation_result_layout\";\n-constexpr char kEntryComputationResultTiles[] =\n-    \"mhlo.xla_entry_computation_result_tiles\";\n-constexpr char kFrontendAttributes[] = \"mhlo.frontend_attributes\";\n-constexpr char kInputOutputAlias[] = \"mhlo.input_output_alias\";\n-constexpr char kIsDynamic[] = \"mhlo.is_dynamic\";\n-constexpr char kNumPartitions[] = \"mhlo.num_partitions\";\n-constexpr char kNumReplicas[] = \"mhlo.num_replicas\";\n-constexpr char kSpmdOutputSharding[] = \"mhlo.spmd_output_sharding\";\n-constexpr char kSpmdParametersShardings[] = \"mhlo.spmd_parameters_shardings\";\n-constexpr char kUseAutoSpmdPartitioning[] = \"mhlo.use_auto_spmd_partitioning\";\n+// All constants must be registered in:\n+//   xla/mlir_hlo/utils/unregistered_attributes.h\n \n mlir::ArrayAttr ConvertCrossProgramPrefetches(\n     const absl::Span<const HloModule::CrossProgramPrefetchInfo> prefetches,\n@@ -122,9 +108,9 @@ void ImportEntryComputationParameterLayoutAndTiles(\n             parameter_tiles.push_back(layout_attrs.second);\n           });\n     }\n-    module->setAttr(kEntryComputationParameterLayouts,\n+    module->setAttr(xla::kMhloXlaEntryComputationParameterLayouts,\n                     builder.getArrayAttr({parameter_layouts}));\n-    module->setAttr(kEntryComputationParameterTiles,\n+    module->setAttr(xla::kMhloXlaEntryComputationParameterTiles,\n                     builder.getArrayAttr({parameter_tiles}));\n     return;\n   }\n@@ -151,9 +137,9 @@ void ImportEntryComputationParameterLayoutAndTiles(\n       parameter_tiles.push_back(layout_attrs.second);\n     }\n   }\n-  module->setAttr(kEntryComputationParameterLayouts,\n+  module->setAttr(xla::kMhloXlaEntryComputationParameterLayouts,\n                   builder.getArrayAttr({parameter_layouts}));\n-  module->setAttr(kEntryComputationParameterTiles,\n+  module->setAttr(xla::kMhloXlaEntryComputationParameterTiles,\n                   builder.getArrayAttr({parameter_tiles}));\n }\n \n@@ -172,9 +158,9 @@ void ImportEntryComputationResultLayoutAndTiles(\n           result_layouts.push_back(layout_attrs.first);\n           result_tiles.push_back(layout_attrs.second);\n         });\n-    module->setAttr(kEntryComputationResultLayout,\n+    module->setAttr(xla::kMhloXlaEntryComputationResultLayout,\n                     builder.getArrayAttr(result_layouts));\n-    module->setAttr(kEntryComputationResultTiles,\n+    module->setAttr(xla::kMhloXlaEntryComputationResultTiles,\n                     builder.getArrayAttr(result_tiles));\n     return;\n   }\n@@ -188,19 +174,19 @@ void ImportEntryComputationResultLayoutAndTiles(\n       result_tiles.push_back(layout_attrs.second);\n     }\n     module->setAttr(\n-        kEntryComputationResultLayout,\n+        xla::kMhloXlaEntryComputationResultLayout,\n         builder.getArrayAttr({builder.getArrayAttr(result_layouts)}));\n-    module->setAttr(kEntryComputationResultTiles,\n+    module->setAttr(xla::kMhloXlaEntryComputationResultTiles,\n                     builder.getArrayAttr({builder.getArrayAttr(result_tiles)}));\n     return;\n   }\n \n   std::pair<mlir::Attribute, mlir::ArrayAttr> layout_attrs =\n       GetLayoutAttribute(builder, computation_layout.result_layout().shape(),\n                          computation_layout.result_layout().layout());\n-  module->setAttr(kEntryComputationResultLayout,\n+  module->setAttr(xla::kMhloXlaEntryComputationResultLayout,\n                   builder.getArrayAttr({layout_attrs.first}));\n-  module->setAttr(kEntryComputationResultTiles,\n+  module->setAttr(xla::kMhloXlaEntryComputationResultTiles,\n                   builder.getArrayAttr({layout_attrs.second}));\n }\n \n@@ -211,7 +197,7 @@ void ImportCrossProgramPrefetches(const HloModule& hlo_module,\n                                   bool flatten_computation_args_result,\n                                   mlir::Builder builder) {\n   module->setAttr(\n-      kCrossProgramPrefetches,\n+      xla::kMhloCrossProgramPrefetches,\n       ConvertCrossProgramPrefetches(hlo_module.CrossProgramPrefetches(),\n                                     *hlo_module.entry_computation(), &builder,\n                                     flatten_computation_args_result));\n@@ -254,29 +240,30 @@ void ImportFrontendAttributes(const HloModule& hlo_module,\n       frontend_attributes.push_back(\n           builder.getNamedAttr(k, builder.getStringAttr(v)));\n     if (!frontend_attributes.empty())\n-      module->setAttr(kFrontendAttributes,\n+      module->setAttr(xla::kMhloFrontendAttributes,\n                       builder.getDictionaryAttr(frontend_attributes));\n   }\n }\n \n void ImportInputOutputAlias(const xla::HloModule& hlo_module,\n                             mlir::ModuleOp module, mlir::Builder builder) {\n-  module->setAttr(kInputOutputAlias,\n+  module->setAttr(xla::kMhloInputOutputAlias,\n                   ConvertInputOutputAlias(\n                       hlo_module.input_output_alias_config(), &builder));\n }\n \n void ImportIsDynamic(const xla::HloModule& hlo_module, mlir::ModuleOp module,\n                      mlir::Builder builder) {\n-  module->setAttr(kIsDynamic, mlir::BoolAttr::get(builder.getContext(),\n-                                                  hlo_module.is_dynamic()));\n+  module->setAttr(\n+      xla::kMhloIsDynamic,\n+      mlir::BoolAttr::get(builder.getContext(), hlo_module.is_dynamic()));\n }\n \n void ImportNumPartitions(const xla::HloModule& hlo_module,\n                          mlir::ModuleOp module, mlir::Builder builder) {\n   const auto& config = hlo_module.config();\n   if (config.num_partitions() != 1) {\n-    module->setAttr(kNumPartitions,\n+    module->setAttr(xla::kMhloNumPartitions,\n                     builder.getI32IntegerAttr(config.num_partitions()));\n   }\n }\n@@ -285,7 +272,7 @@ void ImportNumReplicas(const HloModule& hlo_module, mlir::ModuleOp module,\n                        mlir::Builder builder) {\n   const auto& config = hlo_module.config();\n   if (config.replica_count() != 1) {\n-    module->setAttr(kNumReplicas,\n+    module->setAttr(xla::kMhloNumReplicas,\n                     builder.getI32IntegerAttr(config.replica_count()));\n   }\n }\n@@ -294,7 +281,7 @@ void ImportSpmdOutputSharding(const xla::HloModule& hlo_module,\n                               mlir::ModuleOp module, mlir::Builder builder) {\n   if (hlo_module.has_spmd_output_sharding())\n     module->setAttr(\n-        kSpmdOutputSharding,\n+        xla::kMhloSpmdOutputSharding,\n         ConvertSharding(hlo_module.spmd_output_sharding(), &builder));\n }\n \n@@ -312,15 +299,15 @@ void ImportSpmdParametersShardings(const HloModule& hlo_module,\n       for (const auto& sharding : shardings)\n         parameter_shardings.push_back(ConvertSharding(sharding, &builder));\n     }\n-    module->setAttr(kSpmdParametersShardings,\n+    module->setAttr(xla::kMhloSpmdParametersShardings,\n                     builder.getArrayAttr(parameter_shardings));\n   }\n }\n \n void ImportUseAutoSpmdPartitioning(const HloModule& hlo_module,\n                                    mlir::ModuleOp module,\n                                    mlir::Builder builder) {\n-  module->setAttr(kUseAutoSpmdPartitioning,\n+  module->setAttr(xla::kMhloUseAutoSpmdPartitioning,\n                   mlir::BoolAttr::get(builder.getContext(),\n                                       hlo_module.use_auto_spmd_partitioning()));\n }\n@@ -329,15 +316,14 @@ namespace {\n \n mlir::DictionaryAttr AppendAutoLayoutModeAttribute(mlir::Builder builder,\n                                                    mlir::DictionaryAttr dict) {\n-  constexpr llvm::StringRef kLayoutMode = \"mhlo.layout_mode\";\n   llvm::SmallVector<mlir::NamedAttribute> attrs;\n   if (dict) {\n     for (auto attr : dict.getValue()) {\n-      if (attr.getName() != kLayoutMode) attrs.push_back(attr);\n+      if (attr.getName() != xla::kMhloLayoutMode) attrs.push_back(attr);\n     }\n   }\n-  attrs.push_back(\n-      builder.getNamedAttr(kLayoutMode, builder.getStringAttr(\"auto\")));\n+  attrs.push_back(builder.getNamedAttr(xla::kMhloLayoutMode,\n+                                       builder.getStringAttr(\"auto\")));\n   return builder.getDictionaryAttr(attrs);\n }\n "
        },
        {
            "sha": "537a3d8cb1b3b64baf94ffbdd052407bc0e407db",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -118,8 +118,8 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/hlo/translate:attributes\",\n         \"//xla/hlo/translate/hlo_to_mhlo:hlo_utils\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Support\",\n@@ -132,6 +132,7 @@ cc_library(\n     hdrs = [\"module_attributes_exporter.h\"],\n     deps = [\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/tsl/platform:errors\",\n@@ -192,13 +193,13 @@ cc_library(\n         \"//xla/hlo/builder/lib:slicing\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/hlo/translate:attributes\",\n         \"//xla/hlo/translate/hlo_to_mhlo:hlo_utils\",\n         \"//xla/mlir/utils:error_util\",\n         \"//xla/mlir/utils:type_util\",\n         \"//xla/mlir_hlo\",\n         \"//xla/mlir_hlo:mhlo_passes\",\n         \"//xla/mlir_hlo:stablehlo_extension_passes\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:computation_layout\",\n         \"//xla/service:hlo_module_config\",\n@@ -298,6 +299,7 @@ cc_library(\n         \"//xla/hlo/builder:xla_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/translate:register\",\n+        \"//xla/mlir_hlo:unregistered_attributes\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:hlo_proto_util\","
        },
        {
            "sha": "76a33ce127bcb6a688bcef74e47e3e5b954d03c6",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/location_exporter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Flocation_exporter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Flocation_exporter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Flocation_exporter.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -30,9 +30,9 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/hlo/ir/hlo_original_value.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n-#include \"xla/hlo/translate/attributes.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/hlo_utils.h\"\n #include \"xla/hlo/translate/mhlo_to_hlo/stack_frame_index_builder.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace mlir {\n@@ -51,7 +51,7 @@ static std::string GetNameFromLocImpl(Location loc) {\n       // in functions where the op's name is first.\n       auto name = name_loc.getName().strref().split('@').first;\n       // Skip if the name is for op type.\n-      if (name.starts_with(kOriginalValueAttr)) {\n+      if (name.starts_with(xla::kMhloOriginalValueAttr)) {\n         continue;\n       }\n       if (name.ends_with(\":\")) {\n@@ -85,7 +85,7 @@ static std::string GetOpTypeFromLoc(Location loc) {\n       // Add name in NameLoc. For NameLoc we also account for names due to ops\n       // in functions where the op's name is first.\n       auto op_type = name_loc.getName().strref().split('@').first;\n-      if (op_type.starts_with(kOriginalValueAttr)) {\n+      if (op_type.starts_with(xla::kMhloOriginalValueAttr)) {\n         continue;\n       }\n       if (op_type.ends_with(\":\")) {\n@@ -118,7 +118,7 @@ static std::shared_ptr<xla::OriginalValue> GetOriginalValueFromLoc(\n \n     if (auto name_loc = mlir::dyn_cast<NameLoc>(curr_loc)) {\n       auto original_value = name_loc.getName().strref().split('@').first;\n-      if (!original_value.starts_with(kOriginalValueAttr)) {\n+      if (!original_value.starts_with(xla::kMhloOriginalValueAttr)) {\n         continue;\n       }\n       loc_original_value = original_value.split('=').second;"
        },
        {
            "sha": "6e3bc57fbdb3128f11fca19cdfa2b3b86a5cd69b",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 74,
            "changes": 119,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -85,7 +85,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n-#include \"xla/hlo/translate/attributes.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/hlo_utils.h\"\n #include \"xla/hlo/translate/mhlo_to_hlo/attribute_exporter.h\"\n #include \"xla/hlo/translate/mhlo_to_hlo/layout_util.h\"\n@@ -102,6 +101,7 @@ limitations under the License.\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n #include \"xla/mlir_hlo/mhlo/transforms/passes.h\"\n #include \"xla/mlir_hlo/stablehlo_ext/transforms/passes.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/hlo.pb.h\"\n@@ -128,12 +128,8 @@ using ::tsl::uint32;\n using ::tsl::uint64;\n using ::tsl::uint8;\n \n-// Boolean attribute.\n-constexpr char kJaxBufferDonor[] = \"jax.buffer_donor\";\n-\n-// BitcastOp lowering strings.\n-constexpr char kResultLayout[] = \"result_layout\";\n-constexpr char kSourceLayout[] = \"source_layout\";\n+// All Module level and Function level attributes must be included in:\n+//   xla/mlir_hlo/utils/unregistered_attributes.h\n \n // CustomCallOp lowering strings.\n constexpr char kAggregateToTopk[] = \"aggregate_to_topk\";\n@@ -152,38 +148,12 @@ constexpr char kReductionInputSizeOverride[] = \"reduction_input_size_override\";\n constexpr char kReplicaGroups[] = \"replica_groups\";\n constexpr char kTopK[] = \"top_k\";\n \n-// MHLO attributes. Module level attributes require namespacing.\n-constexpr char kMhloCrossProgramPrefetches[] = \"mhlo.cross_program_prefetches\";\n-constexpr char kMhloInputOutputAlias[] = \"mhlo.input_output_alias\";\n-constexpr char kMhloIsDynamic[] = \"mhlo.is_dynamic\";\n-constexpr char kMhloLiteral[] = \"mhlo.literal\";\n-constexpr char kMhloReplication[] = \"mhlo.is_same_data_across_replicas\";\n-constexpr char kMhloSpmdOutputSharding[] = \"mhlo.spmd_output_sharding\";\n-constexpr char kMhloSpmdParametersShardings[] =\n-    \"mhlo.spmd_parameters_shardings\";\n-constexpr char kMhloUseAutoSpmdPartitioning[] =\n-    \"mhlo.use_auto_spmd_partitioning\";\n-constexpr char kMhloXlaEntryComputationParameterLayouts[] =\n-    \"mhlo.xla_entry_computation_parameter_layouts\";\n-constexpr char kMhloXlaEntryComputationParameterTiles[] =\n-    \"mhlo.xla_entry_computation_parameter_tiles\";\n-constexpr char kMhloXlaEntryComputationResultLayout[] =\n-    \"mhlo.xla_entry_computation_result_layout\";\n-constexpr char kMhloXlaEntryComputationResultTiles[] =\n-    \"mhlo.xla_entry_computation_result_tiles\";\n-\n // Miscellaneous string literals.\n constexpr char kArgEmptyTuple[] = \"arg_empty_tuple\";\n constexpr char kArgPrefix[] = \"Arg_\";\n constexpr char kArgTuple[] = \"arg_tuple\";\n-constexpr char kDefaultLayoutAttrName[] = \"xla_shape\";\n-constexpr char kExecutionThread[] = \"execution_thread\";\n-// Array attribute. Same shape as infeed result, but contains a\n-// minor_to_major array for every tensor.\n-constexpr char kLayout[] = \"layout\";\n constexpr char kMain[] = \"main\";\n constexpr char kRegionPrefix[] = \"region_\";\n-constexpr char kTfAliasingOutput[] = \"tf.aliasing_output\";\n \n // Passes through everything except for unique_ptr, on which it calls get().\n // This exists to allow the generated code to call XLA functions that take a raw\n@@ -386,9 +356,8 @@ static xla::TriangularSolveOptions::Transpose Convert_transpose_a(\n       .value();\n }\n \n-static xla::Layout ExtractLayout(\n-    mlir::Operation* op, int rank,\n-    llvm::StringRef attr_name = kDefaultLayoutAttrName) {\n+static xla::Layout ExtractLayout(mlir::Operation* op, int rank,\n+                                 llvm::StringRef attr_name = xla::kXlaShape) {\n   if (auto attr = op->getAttrOfType<mlir::DenseIntElementsAttr>(attr_name)) {\n     llvm::SmallVector<int64_t, 4> minor_to_major;\n     DCHECK_EQ(rank, attr.size());\n@@ -404,7 +373,7 @@ static xla::Layout ExtractLayout(\n // Returns a failure or a valid XLA shape corresponding to the given op's\n // results.\n static mlir::FailureOr<xla::Shape> ExtractXlaShape(mlir::Operation* op) {\n-  if (auto attr = op->getAttrOfType<mlir::StringAttr>(kDefaultLayoutAttrName)) {\n+  if (auto attr = op->getAttrOfType<mlir::StringAttr>(xla::kXlaShape)) {\n     return *xla::ParseShape(\n         absl::string_view(attr.getValue().data(), attr.getValue().size()));\n   } else {\n@@ -895,7 +864,7 @@ static xla::ResultAccuracy Convert_result_accuracy(\n // returns std::nullopt.\n static std::optional<xla::OpSharding> CreateOpShardingFromAttribute(\n     mlir::Operation* op) {\n-  auto shardingAttr = op->getAttrOfType<mlir::StringAttr>(kShardingAttr);\n+  auto shardingAttr = op->getAttrOfType<mlir::StringAttr>(xla::kMhloSharding);\n   if (!shardingAttr) {\n     return std::nullopt;\n   }\n@@ -927,7 +896,7 @@ static xla::FrontendAttributes CreateXlaFrontendAttributesFromOp(\n     mlir::Operation* op) {\n   xla::FrontendAttributes frontend_attributes;\n   auto frontend_attributes_dict =\n-      op->getAttrOfType<mlir::DictionaryAttr>(kFrontendAttributesAttr);\n+      op->getAttrOfType<mlir::DictionaryAttr>(xla::kMhloFrontendAttributes);\n   if (!frontend_attributes_dict) return frontend_attributes;\n   CreateFrontendAttributes(frontend_attributes_dict, frontend_attributes);\n   return frontend_attributes;\n@@ -939,7 +908,7 @@ static void ExtractFrontendAttributesFromFunction(\n   fe_attrs->resize(function.getNumArguments(), std::nullopt);\n   for (int i = 0, end = function.getNumArguments(); i < end; ++i)\n     if (auto fe_attr = function.getArgAttrOfType<mlir::DictionaryAttr>(\n-            i, kFrontendAttributesAttr)) {\n+            i, xla::kMhloFrontendAttributes)) {\n       xla::FrontendAttributes frontend_attributes;\n       CreateFrontendAttributes(fe_attr, frontend_attributes);\n       (*fe_attrs)[i] = frontend_attributes;\n@@ -968,8 +937,8 @@ static void ExtractShardingsFromFunction(\n           module, xla::sdy::kMeshesRoundTripAttr);\n \n   for (int i = 0, end = function.getNumArguments(); i < end; ++i) {\n-    if (auto sharding =\n-            function.getArgAttrOfType<mlir::StringAttr>(i, kShardingAttr)) {\n+    if (auto sharding = function.getArgAttrOfType<mlir::StringAttr>(\n+            i, xla::kMhloSharding)) {\n       (*arg_shardings)[i] = xla::ConvertSharding(sharding.getValue());\n       // Due to limitations with accurately getting OpShardings from manual\n       // computation bodies with Shardy shardings, only extract OpShardings from\n@@ -988,8 +957,8 @@ static void ExtractShardingsFromFunction(\n   ret_shardings->resize(function.getNumResults(),\n                         std::optional<xla::OpSharding>());\n   for (int i = 0, end = function.getNumResults(); i < end; ++i) {\n-    if (auto sharding =\n-            function.getResultAttrOfType<mlir::StringAttr>(i, kShardingAttr)) {\n+    if (auto sharding = function.getResultAttrOfType<mlir::StringAttr>(\n+            i, xla::kMhloSharding)) {\n       (*ret_shardings)[i] = xla::ConvertSharding(sharding.getValue());\n     } else if (is_entry_function) {\n       if (auto sharding = xla::ExtractShardyResultShardingFromFrontendAttrs(\n@@ -2575,8 +2544,8 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n       auto name = attr.getName();\n       return name == kCallTargetName || name == kBackendConfig ||\n              name == kApiVersion || name == kCalledComputations ||\n-             name == kHasSideEffect || name == kShardingAttr ||\n-             name == kFrontendAttributesAttr;\n+             name == kHasSideEffect || name == xla::kMhloSharding ||\n+             name == xla::kMhloFrontendAttributes;\n     };\n     for (const auto& attr : op->getAttrs()) {\n       if (!isSupportedAttrName(attr))\n@@ -2634,7 +2603,7 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n \n   absl::StatusOr<xla::Literal> literal;\n   const xla::Literal* literal_ptr = nullptr;\n-  auto literal_attr = op->getAttrOfType<DenseElementsAttr>(kMhloLiteral);\n+  auto literal_attr = op->getAttrOfType<DenseElementsAttr>(xla::kMhloLiteral);\n   if (literal_attr) {\n     literal = mhlo::CreateLiteralFromAttribute(literal_attr, {});\n     if (!literal.ok()) return failure();\n@@ -4293,8 +4262,8 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n       auto name = attr.getName();\n       return name == kCallTargetName || name == kBackendConfig ||\n              name == kApiVersion || name == kCalledComputations ||\n-             name == kHasSideEffect || name == kShardingAttr ||\n-             name == kFrontendAttributesAttr;\n+             name == kHasSideEffect || name == xla::kMhloSharding ||\n+             name == xla::kMhloFrontendAttributes;\n     };\n     for (const auto& attr : op->getAttrs()) {\n       if (!isSupportedAttrName(attr))\n@@ -4344,7 +4313,7 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n \n   absl::StatusOr<xla::Literal> literal;\n   const xla::Literal* literal_ptr = nullptr;\n-  auto literal_attr = op->getAttrOfType<DenseElementsAttr>(kMhloLiteral);\n+  auto literal_attr = op->getAttrOfType<DenseElementsAttr>(xla::kMhloLiteral);\n   if (literal_attr) {\n     literal = mhlo::CreateLiteralFromAttribute(literal_attr, {});\n     if (!literal.ok()) return failure();\n@@ -5181,11 +5150,11 @@ LogicalResult ExportXlaOp(BitcastOp op, OpLoweringContext ctx) {\n         xla::internal::XlaBuilderFriend::GetInstruction(operand);\n     xla::LayoutProto result_layout =\n         ExtractLayout(op, bitcast_proto->shape().dimensions_size(),\n-                      kResultLayout)\n+                      xla::kBitcastResultLayout)\n             .ToProto();\n     xla::LayoutProto source_layout =\n         ExtractLayout(op, operand_proto->shape().dimensions_size(),\n-                      kSourceLayout)\n+                      xla::kBitcastSourceLayout)\n             .ToProto();\n     xla::gpu::BitcastBackendConfig bitcast_config;\n     *bitcast_config.mutable_source_layout() = source_layout;\n@@ -5579,7 +5548,8 @@ LogicalResult ConvertToHloModule::LowerConstant(\n LogicalResult ConvertToHloModule::LowerInfeed(\n     mlir::Operation* inst, xla::XlaBuilder* builder,\n     ConvertToHloModule::ValueLoweringMap* value_lowering) {\n-  mlir::ArrayAttr layout = inst->getAttrOfType<mlir::ArrayAttr>(kLayout);\n+  mlir::ArrayAttr layout =\n+      inst->getAttrOfType<mlir::ArrayAttr>(xla::kInfeedLayout);\n   if (!layout) return success();\n \n   // We propagate layout to the following three ops:\n@@ -5881,14 +5851,14 @@ LogicalResult ConvertToHloModule::RunOnFunction(mlir::func::FuncOp f) {\n     bool any_arg_replicated = false;\n     entry_args_same_across_replicas.reserve(f.getNumArguments());\n     for (int64_t i = 0; i < f.getNumArguments(); ++i) {\n-      auto attr = f.getArgAttrOfType<mlir::BoolAttr>(i, kMhloReplication);\n+      auto attr = f.getArgAttrOfType<mlir::BoolAttr>(i, xla::kMhloReplication);\n       entry_args_same_across_replicas.push_back(attr != nullptr &&\n                                                 attr.getValue());\n       any_arg_replicated |= entry_args_same_across_replicas.back();\n       // Pass the alias info to the builder so that it will build the alias info\n       // into the resulting HloModule.\n       auto buffer_donor =\n-          f.getArgAttrOfType<mlir::BoolAttr>(i, kJaxBufferDonor);\n+          f.getArgAttrOfType<mlir::BoolAttr>(i, xla::kJaxBufferDonor);\n       if (buffer_donor) {\n         if (options_.use_tuple_args) {\n           builder->AddBufferDonor(/*param_number=*/0, /*param_index=*/{i});\n@@ -5897,7 +5867,7 @@ LogicalResult ConvertToHloModule::RunOnFunction(mlir::func::FuncOp f) {\n         }\n       }\n       auto aliasing_output =\n-          f.getArgAttrOfType<mlir::IntegerAttr>(i, kTfAliasingOutput);\n+          f.getArgAttrOfType<mlir::IntegerAttr>(i, xla::kTfAliasingOutput);\n       if (!aliasing_output) continue;\n       xla::ShapeIndex output_index;\n       if ((options_.return_tuple && entry_function) || f.getNumResults() != 1) {\n@@ -5933,7 +5903,7 @@ LogicalResult ConvertToHloModule::RunOnFunction(mlir::func::FuncOp f) {\n     return failure();\n   }\n   if (auto execution_thread =\n-          f->getAttrOfType<mlir::StringAttr>(kExecutionThread)) {\n+          f->getAttrOfType<mlir::StringAttr>(xla::kExecutionThread)) {\n     absl::Status status = xla::internal::XlaBuilderFriend::SetExecutionThread(\n         &module_builder_, computation, execution_thread.str());\n     if (!status.ok()) {\n@@ -5942,8 +5912,8 @@ LogicalResult ConvertToHloModule::RunOnFunction(mlir::func::FuncOp f) {\n   }\n   absl::flat_hash_map<int, std::vector<bool>> parameter_replication;\n   for (int i = 0; i < f.getNumArguments(); ++i) {\n-    if (auto pr =\n-            f.getArgAttrOfType<mlir::ArrayAttr>(i, kParameterReplicationAttr)) {\n+    if (auto pr = f.getArgAttrOfType<mlir::ArrayAttr>(\n+            i, xla::kMhloParameterReplication)) {\n       auto& replicated_at_leaf_buffers = parameter_replication[i];\n       for (auto b : pr.getValue()) {\n         replicated_at_leaf_buffers.push_back(\n@@ -5957,7 +5927,7 @@ LogicalResult ConvertToHloModule::RunOnFunction(mlir::func::FuncOp f) {\n       auto& replicated_at_leaf_buffers = parameter_replication[0];\n       for (int i = 0; i < f.getNumArguments(); ++i) {\n         if (auto pr = f.getArgAttrOfType<mlir::ArrayAttr>(\n-                i, kParameterReplicationAttr)) {\n+                i, xla::kMhloParameterReplication)) {\n           for (auto b : pr.getValue()) {\n             replicated_at_leaf_buffers.push_back(\n                 cast<mlir::BoolAttr>(b).getValue());\n@@ -6341,69 +6311,70 @@ absl::Status ConvertMlirHloToHlo(mlir::ModuleOp module,\n                       converter.ConsumeMainProto());\n   StringRef module_name = module.getName() ? *module.getName() : kMain;\n   hlo_module.set_name(module_name.str());\n-  if (auto cross_program_prefetches =\n-          module->getAttrOfType<mlir::ArrayAttr>(kMhloCrossProgramPrefetches)) {\n+  if (auto cross_program_prefetches = module->getAttrOfType<mlir::ArrayAttr>(\n+          xla::kMhloCrossProgramPrefetches)) {\n     for (const auto& prefetch :\n          Convert_cross_program_prefetches(cross_program_prefetches)) {\n       *hlo_module.add_cross_program_prefetches() = std::move(prefetch);\n     }\n   }\n-  if (auto is_dynamic = module->getAttrOfType<mlir::BoolAttr>(kMhloIsDynamic)) {\n+  if (auto is_dynamic =\n+          module->getAttrOfType<mlir::BoolAttr>(xla::kMhloIsDynamic)) {\n     hlo_module.set_is_dynamic(is_dynamic.getValue());\n   }\n   if (auto frontend_attributes =\n-          module->getAttrOfType<DictionaryAttr>(kFrontendAttributesAttr)) {\n+          module->getAttrOfType<DictionaryAttr>(xla::kMhloFrontendAttributes)) {\n     CreateFrontendAttributes(frontend_attributes,\n                              *hlo_module.mutable_frontend_attributes());\n   }\n-  if (auto use_auto_spmd_partitioning =\n-          module->getAttrOfType<mlir::BoolAttr>(kMhloUseAutoSpmdPartitioning)) {\n+  if (auto use_auto_spmd_partitioning = module->getAttrOfType<mlir::BoolAttr>(\n+          xla::kMhloUseAutoSpmdPartitioning)) {\n     hlo_module.set_use_auto_spmd_partitioning(\n         use_auto_spmd_partitioning.getValue());\n   }\n-  if (auto spmd_output_sharding =\n-          module->getAttrOfType<mlir::StringAttr>(kMhloSpmdOutputSharding)) {\n+  if (auto spmd_output_sharding = module->getAttrOfType<mlir::StringAttr>(\n+          xla::kMhloSpmdOutputSharding)) {\n     *hlo_module.mutable_spmd_output_sharding() =\n         *xla::ConvertSharding(spmd_output_sharding.getValue());\n   }\n   if (auto input_output_alias =\n-          module->getAttrOfType<mlir::ArrayAttr>(kMhloInputOutputAlias)) {\n+          module->getAttrOfType<mlir::ArrayAttr>(xla::kMhloInputOutputAlias)) {\n     if (std::optional<xla::HloInputOutputAliasProto> input_output_alias_proto =\n             xla::ConvertInputOutputAlias(input_output_alias.getValue())) {\n       *hlo_module.mutable_input_output_alias() = *input_output_alias_proto;\n     }\n   }\n   if (auto spmd_parameters_sharding = module->getAttrOfType<mlir::ArrayAttr>(\n-          kMhloSpmdParametersShardings)) {\n+          xla::kMhloSpmdParametersShardings)) {\n     for (const auto& sharding : spmd_parameters_sharding.getValue()) {\n       *hlo_module.add_spmd_parameters_shardings() = *xla::ConvertSharding(\n           mlir::cast<mlir::StringAttr>(sharding).getValue());\n     }\n   }\n   if (auto xla_entry_computation_parameter_layout =\n           module->getAttrOfType<mlir::ArrayAttr>(\n-              kMhloXlaEntryComputationParameterLayouts)) {\n+              xla::kMhloXlaEntryComputationParameterLayouts)) {\n     auto status = mhlo::ExportModuleEntryComputationParameterLayouts(\n         xla_entry_computation_parameter_layout, hlo_module);\n     if (!status.ok()) return status;\n   }\n   if (auto xla_entry_computation_parameter_tiles =\n           module->getAttrOfType<mlir::ArrayAttr>(\n-              kMhloXlaEntryComputationParameterTiles)) {\n+              xla::kMhloXlaEntryComputationParameterTiles)) {\n     auto status = mhlo::ExportModuleEntryComputationParameterTiles(\n         xla_entry_computation_parameter_tiles, hlo_module);\n     if (!status.ok()) return status;\n   }\n   if (auto xla_entry_computation_result_layout =\n           module->getAttrOfType<mlir::ArrayAttr>(\n-              kMhloXlaEntryComputationResultLayout)) {\n+              xla::kMhloXlaEntryComputationResultLayout)) {\n     auto status = mhlo::ExportModuleEntryComputationResultLayout(\n         xla_entry_computation_result_layout, hlo_module);\n     if (!status.ok()) return status;\n   }\n   if (auto xla_entry_computation_result_tiles =\n           module->getAttrOfType<mlir::ArrayAttr>(\n-              kMhloXlaEntryComputationResultTiles)) {\n+              xla::kMhloXlaEntryComputationResultTiles)) {\n     auto status = mhlo::ExportModuleEntryComputationResultTiles(\n         xla_entry_computation_result_tiles, hlo_module);\n     if (!status.ok()) return status;"
        },
        {
            "sha": "1ed60353b2d429708015a9f352caf5116b63342b",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/module_attributes_exporter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmodule_attributes_exporter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmodule_attributes_exporter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmodule_attributes_exporter.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/Support/LLVM.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -39,8 +40,8 @@ namespace mlir {\n namespace mhlo {\n namespace {\n \n-constexpr char kMhloNumPartitions[] = \"mhlo.num_partitions\";\n-constexpr char kMhloNumReplicas[] = \"mhlo.num_replicas\";\n+// All module level attribute strings must be registered in\n+//   `xla/mlir_hlo/utils/unregistered_attributes.h`.\n \n std::vector<int64_t> ConvertDenseIntAttr(DenseIntElementsAttr attr) {\n   auto values = attr.getValues<int64_t>();\n@@ -149,11 +150,11 @@ absl::StatusOr<xla::HloComputationProto*> FindEntryComputation(\n \n void ExportHloModuleConfig(xla::HloModuleConfig& config, ModuleOp module) {\n   if (auto num_partitions =\n-          module->getAttrOfType<IntegerAttr>(kMhloNumPartitions)) {\n+          module->getAttrOfType<IntegerAttr>(xla::kMhloNumPartitions)) {\n     config.set_num_partitions(num_partitions.getInt());\n   }\n   if (auto num_replicas =\n-          module->getAttrOfType<IntegerAttr>(kMhloNumReplicas)) {\n+          module->getAttrOfType<IntegerAttr>(xla::kMhloNumReplicas)) {\n     config.set_replica_count(num_replicas.getInt());\n   }\n }"
        },
        {
            "sha": "6d7023245298b46609ad638ee6eaa1e4974579f6",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/translate.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftranslate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftranslate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftranslate.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -45,6 +45,7 @@ limitations under the License.\n #include \"xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.h\"\n #include \"xla/hlo/translate/mhlo_to_hlo/type_to_shape.h\"\n #include \"xla/hlo/translate/register.h\"\n+#include \"xla/mlir_hlo/utils/unregistered_attributes.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_proto_util.h\"\n@@ -53,8 +54,6 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n-constexpr char kParameterReplicationAttr[] = \"mhlo.parameter_replication\";\n-\n namespace xla {\n \n mlir::LogicalResult MlirHloToHloTranslateFunction(mlir::ModuleOp module,\n@@ -127,17 +126,19 @@ absl::Status ConvertMlirHloToHloViaBuilder(\n     computation.mutable_proto()->mutable_computations(0)->set_execution_thread(\n         execution_thread.str());\n   }\n-  for (int i = 0; i < main.getNumArguments(); ++i)\n+  for (int i = 0; i < main.getNumArguments(); ++i) {\n     if (auto pr = main.getArgAttrOfType<mlir::ArrayAttr>(\n-            i, kParameterReplicationAttr))\n-      for (auto b : pr.getValue())\n+            i, xla::kMhloParameterReplication)) {\n+      for (auto b : pr.getValue()) {\n         computation.mutable_proto()\n             ->mutable_computations(0)\n             ->mutable_instructions(i)\n             ->mutable_parameter_replication()\n             ->add_replicated_at_leaf_buffers(\n                 mlir::cast<mlir::BoolAttr>(b).getValue());\n-\n+      }\n+    }\n+  }\n   auto hlo_module = computation.proto();\n   mlir::StringRef module_name = module.getName() ? *module.getName() : \"main\";\n   hlo_module.set_name(module_name.str());"
        },
        {
            "sha": "f63514c26ad02a8a04e6f949977c6e9c4ee5c46c",
            "filename": "third_party/xla/xla/hlo/translate/stablehlo.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fstablehlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fstablehlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fstablehlo.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -76,6 +76,8 @@ absl::Status StablehloToMhlo(mlir::ModuleOp module, bool run_canonicalizer) {\n   pm.addPass(mlir::mhlo::createStablehloLegalizeToHloPass());\n   if (run_canonicalizer) {\n     pm.addNestedPass<mlir::func::FuncOp>(mlir::createCanonicalizerPass());\n+    pm.addPass(mlir::stablehlo_ext::\n+                   createStablehloSanitizeDiscardableAttributesPass());\n   }\n   // In order to export to XLA, we must sink constants to control flow\n   // regions, since XLA uses functional control flow."
        },
        {
            "sha": "908abdc05da3695b287b51a0fe7a055aea2f7844",
            "filename": "third_party/xla/xla/mlir_hlo/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -273,6 +273,7 @@ cc_library(\n         \":hlo_ops_inc_gen\",\n         \":hlo_ops_pattern_inc_gen\",\n         \":hlo_ops_typedefs_inc_gen\",\n+        \":unregistered_attributes\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:Analysis\",\n         \"@llvm-project//mlir:ArithDialect\",\n@@ -303,6 +304,14 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"unregistered_attributes\",\n+    srcs = [\"utils/unregistered_attributes.cc\"],\n+    hdrs = [\"utils/unregistered_attributes.h\"],\n+    strip_include_prefix = \".\",\n+    deps = [],\n+)\n+\n cc_library(\n     name = \"hlo_dialect_registration\",\n     srcs = [\"mhlo/IR/init.cc\"],\n@@ -1029,6 +1038,7 @@ cc_library(\n         \"stablehlo_ext/transforms/stablehlo_legalize_quant_composite.cpp\",\n         \"stablehlo_ext/transforms/stablehlo_prepare_for_hlo_export.cpp\",\n         \"stablehlo_ext/transforms/stablehlo_refine_shapes.cpp\",\n+        \"stablehlo_ext/transforms/stablehlo_sanitize_unregistered_attributes.cpp\",\n         \"stablehlo_ext/transforms/symbolic_shape_optimization.cpp\",\n     ],\n     hdrs = [\n@@ -1043,6 +1053,7 @@ cc_library(\n         \":stablehlo_extension_base\",\n         \":stablehlo_extension_ops\",\n         \":stablehlo_extension_pass_inc_gen\",\n+        \":unregistered_attributes\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:AsmParser\","
        },
        {
            "sha": "97af491a65a184705059a771114309d7e51c2091",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -89,6 +89,7 @@ limitations under the License.\n #include \"stablehlo/dialect/TypeInference.h\"\n #include \"utils/convert_op_folder.h\"\n #include \"utils/hlo_utils.h\"\n+#include \"utils/unregistered_attributes.h\"\n \n namespace mlir {\n #include \"hlo_patterns.cc.inc\"\n@@ -7631,7 +7632,7 @@ LogicalResult MhloDialect::verifyOperationAttribute(Operation* op,\n              << \"attribute \" << attr.getName()\n              << \" can only be used on function-like operations\";\n   }\n-  if (attr.getName() == \"mhlo.cross_program_prefetches\") {\n+  if (attr.getName() == xla::kMhloCrossProgramPrefetches) {\n     auto arrayAttr = dyn_cast<ArrayAttr>(attr.getValue());\n     if (!arrayAttr)\n       return op->emitOpError() << \"cross_program_prefetches must be an array\";\n@@ -7648,7 +7649,7 @@ LogicalResult MhloDialect::verifyOperationAttribute(Operation* op,\n       if (failed(res)) return res;\n     }\n   }\n-  if (attr.getName() == \"mhlo.spmd_parameters_sharding\") {\n+  if (attr.getName() == xla::kMhloSpmdParametersShardings) {\n     auto arrayAttr = dyn_cast<ArrayAttr>(attr.getValue());\n     if (!arrayAttr)\n       return op->emitOpError() << \"spmd_parameters_sharding: must be an array\";"
        },
        {
            "sha": "d98fbbcae234d18dfb4522e15e3407560c83084c",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2FCMakeLists.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2FCMakeLists.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2FCMakeLists.txt?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -21,6 +21,7 @@ add_mlir_dialect_library(StablehloExtensionPasses\n   chlo_recompose_ops.cpp\n   chlo_preserve_high_level_ops.cpp\n   stablehlo_canonicalize_dynamism.cpp\n+  stablehlo_sanitize_unregistered_attributes.cpp\n   stablehlo_refine_shapes.cpp\n   sdy_refine_shapes.cpp\n "
        },
        {
            "sha": "6ea1a375f339e42db7e1ceba236255f60be3346c",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/passes.td",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fpasses.td?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -208,6 +208,20 @@ def SinkConstantsToControlFlowPass : Pass<\"stablehlo-ext-sink-constants-to-contr\n   }];\n }\n \n+def StablehloSanitizeDiscardableAttributesPass : Pass<\"stablehlo-ext-sanitize-discardable-attributes\", \"ModuleOp\"> {\n+  let summary = \"Removes discardable attributes that are unrecognized in XLA\";\n+  let description = [{\n+    This pass removes discardable attributes that are unrecognized in XLA.\n+    Only specific attributes are supported in HLO, and many are silently\n+    dropped. This pass makes that behavior more explicit and allows users to\n+    more clearly represent the HLO that will be generated.\n+\n+    This is especilly useful in fingerprinting XLA MLIR programs, today if\n+    discardable attributes change, the fingerprint will change, even though the\n+    resulting HLO is the same.\n+  }];\n+}\n+\n def SymbolicShapeOptimizationPass : Pass<\"stablehlo-ext-symbolic-shape-optimization\", \"func::FuncOp\"> {\n   let summary = \"Analyzes shapes and performs shape-related optimizations\";\n   let description = [{"
        },
        {
            "sha": "8ef4b18904fea3cb7dd1d48f6d8d57772d995a95",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_sanitize_unregistered_attributes.cpp",
            "status": "added",
            "additions": 188,
            "deletions": 0,
            "changes": 188,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_sanitize_unregistered_attributes.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_sanitize_unregistered_attributes.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_sanitize_unregistered_attributes.cpp?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -0,0 +1,188 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+   Copyright 2023 The StableHLO Authors.\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <functional>\n+#include <utility>\n+\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/Support/Debug.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"stablehlo_ext/transforms/passes.h\"  // NOLINT: Used in passes.h.inc\n+#include \"utils/unregistered_attributes.h\"\n+\n+#define DEBUG_TYPE \"stablehlo-ext-passes\"\n+\n+namespace mlir {\n+namespace stablehlo_ext {\n+\n+#define GEN_PASS_DEF_STABLEHLOSANITIZEDISCARDABLEATTRIBUTESPASS\n+#include \"stablehlo_ext/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+// To be extra safe we error if we encounter SDY discardable attributes.\n+// We should run Shardy Export pass prior to sanitizing, which converts\n+// most SDY attributes into mhlo.frontend_attributes.\n+// NOLINTNEXTLINE(llvm-prefer-static-over-anonymous-namespace)\n+LogicalResult verifyNoSdyAttributes(Operation* op, NamedAttribute attr) {\n+  auto name = attr.getName();\n+  if (name.getValue().starts_with(\"sdy.\")) {\n+    return op->emitError(\"SDY attribute encountered: \")\n+           << name\n+           << \". Run SDY export pass prior to sanitizing unregistered \"\n+              \"attributes.\";\n+  }\n+  return success();\n+}\n+\n+struct SanitizeDiscardableFuncAttributes\n+    : public OpRewritePattern<func::FuncOp> {\n+  using OpRewritePattern<func::FuncOp>::OpRewritePattern;\n+\n+  LogicalResult rewriteArgAttrs(\n+      func::FuncOp funcOp, PatternRewriter& rewriter, ArrayAttr argAttrs,\n+      const std::function<void(int32_t, DictionaryAttr)>& updateAttrsFn) const {\n+    if (!argAttrs) {\n+      return rewriter.notifyMatchFailure(\n+          funcOp, \"no discardable attributes or func attrs found\");\n+    }\n+\n+    bool changed = false;\n+    for (auto [idx, argAttr] : llvm::enumerate(argAttrs)) {\n+      auto dict = mlir::dyn_cast<DictionaryAttr>(argAttr);\n+      if (!dict)\n+        return rewriter.notifyMatchFailure(funcOp, \"expected dict attr\");\n+\n+      SmallVector<NamedAttribute> sanitizedDictAttrs;\n+      for (NamedAttribute attr : dict) {\n+        if (failed(verifyNoSdyAttributes(funcOp, attr))) return failure();\n+        auto name = attr.getName();\n+        mlir::StringRef nameRef(name.data(), name.size());\n+        if (xla::IsKnownDiscardableFuncAttribute(nameRef)) {\n+          sanitizedDictAttrs.push_back(attr);\n+        } else {\n+          LLVM_DEBUG(llvm::dbgs()\n+                     << \"Removing discardable attribute: \" << name << \"\\n\");\n+        }\n+      }\n+      if (sanitizedDictAttrs.size() != dict.size()) {\n+        updateAttrsFn(\n+            idx, DictionaryAttr::get(funcOp.getContext(), sanitizedDictAttrs));\n+        changed = true;\n+      }\n+    }\n+    return success(/*success=*/changed);\n+  }\n+\n+  LogicalResult matchAndRewrite(func::FuncOp funcOp,\n+                                PatternRewriter& rewriter) const override {\n+    for (auto attr : funcOp->getDiscardableAttrs()) {\n+      if (failed(verifyNoSdyAttributes(funcOp, attr))) return failure();\n+\n+      auto name = attr.getName();\n+      if (!xla::IsKnownDiscardableFuncAttribute({name.data(), name.size()})) {\n+        LLVM_DEBUG(llvm::dbgs()\n+                   << \"Removing discardable func attribute: \" << name << \"\\n\");\n+        funcOp->removeDiscardableAttr(name);\n+        return success();\n+      }\n+    }\n+\n+    bool changedArgs =\n+        succeeded(rewriteArgAttrs(funcOp, rewriter, funcOp.getAllArgAttrs(),\n+                                  [&](int32_t idx, DictionaryAttr dict) {\n+                                    funcOp.setArgAttrs(idx, dict);\n+                                  }));\n+    bool changedResults =\n+        succeeded(rewriteArgAttrs(funcOp, rewriter, funcOp.getAllResultAttrs(),\n+                                  [&](int32_t idx, DictionaryAttr dict) {\n+                                    funcOp.setResultAttrs(idx, dict);\n+                                  }));\n+    if (changedArgs || changedResults) return success();\n+    return rewriter.notifyMatchFailure(funcOp,\n+                                       \"no discardable attributes found\");\n+  }\n+};\n+\n+struct SanitizeDiscardableOpAttributes final : RewritePattern {\n+  explicit SanitizeDiscardableOpAttributes(MLIRContext* context)\n+      : RewritePattern(MatchAnyOpTypeTag(), /*benefit=*/1, context) {}\n+\n+  LogicalResult matchAndRewrite(Operation* op,\n+                                PatternRewriter& rewriter) const override {\n+    if (mlir::isa<func::FuncOp>(op))\n+      return rewriter.notifyMatchFailure(op, \"skipping func op\");\n+\n+    for (auto attr : op->getDiscardableAttrs()) {\n+      if (failed(verifyNoSdyAttributes(op, attr))) return failure();\n+      auto name = attr.getName();\n+      if (!xla::IsKnownDiscardableOpAttribute({name.data(), name.size()})) {\n+        LLVM_DEBUG(llvm::dbgs()\n+                   << \"Removing discardable op attribute: \" << name << \"\\n\");\n+        op->removeDiscardableAttr(name);\n+        return success();\n+      }\n+    }\n+    return rewriter.notifyMatchFailure(op, \"no unregistered attributes found\");\n+  }\n+};\n+\n+struct StablehloSanitizeDiscardableAttributesPass\n+    : public impl::StablehloSanitizeDiscardableAttributesPassBase<\n+          StablehloSanitizeDiscardableAttributesPass> {\n+  using StablehloSanitizeDiscardableAttributesPassBase::\n+      StablehloSanitizeDiscardableAttributesPassBase;\n+\n+  void runOnOperation() override {\n+    // Remove discardable attributes from module.\n+    ModuleOp module = getOperation();\n+    for (auto attr : module->getDiscardableAttrs()) {\n+      if (failed(verifyNoSdyAttributes(module, attr)))\n+        return signalPassFailure();\n+      auto name = attr.getName();\n+      if (!xla::IsKnownDiscardableModuleAttribute({name.data(), name.size()})) {\n+        LLVM_DEBUG(llvm::dbgs() << \"Removing discardable module attribute: \"\n+                                << name << \"\\n\");\n+        module->removeDiscardableAttr(name);\n+      }\n+    }\n+\n+    PatternRewriter rewriter(&getContext());\n+    RewritePatternSet patterns(&getContext());\n+    patterns.add<SanitizeDiscardableFuncAttributes>(&getContext());\n+    patterns.add<SanitizeDiscardableOpAttributes>(&getContext());\n+    GreedyRewriteConfig config;\n+    config.enableConstantCSE(false);\n+    config.enableFolding(false);\n+    if (failed(applyPatternsGreedily(module, std::move(patterns), config))) {\n+      module->emitError(\"Failed to sanitize discardable attributes.\");\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+}  // namespace stablehlo_ext\n+}  // namespace mlir"
        },
        {
            "sha": "2290b9fad8eb21942bc4b1a7e32eeaa655f29843",
            "filename": "third_party/xla/xla/mlir_hlo/tests/stablehlo_ext/stablehlo_sanitize_unregistered_attributes.mlir",
            "status": "added",
            "additions": 173,
            "deletions": 0,
            "changes": 173,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fstablehlo_sanitize_unregistered_attributes.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fstablehlo_sanitize_unregistered_attributes.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fstablehlo_sanitize_unregistered_attributes.mlir?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -0,0 +1,173 @@\n+// RUN: mlir-hlo-opt --stablehlo-ext-sanitize-discardable-attributes --split-input-file --verify-diagnostics %s | FileCheck %s\n+\n+// -----\n+\n+// CHECK-LABEL: module @module_known_attr\n+// CHECK-SAME: mhlo.frontend_attributes\n+// CHECK-SAME: mhlo.input_output_alias\n+// CHECK-SAME: mhlo.is_dynamic\n+// CHECK-SAME: mhlo.is_same_data_across_replicas\n+// CHECK-SAME: mhlo.num_partitions\n+// CHECK-SAME: mhlo.num_replicas\n+// CHECK-SAME: mhlo.spmd_output_sharding\n+// CHECK-SAME: mhlo.spmd_parameters_shardings\n+// CHECK-SAME: mhlo.use_auto_spmd_partitioning\n+// CHECK-SAME: mhlo.xla_entry_computation_parameter_layouts\n+// CHECK-SAME: mhlo.xla_entry_computation_parameter_tiles\n+// CHECK-SAME: mhlo.xla_entry_computation_result_layout\n+// CHECK-SAME: mhlo.xla_entry_computation_result_tiles\n+module @module_known_attr attributes {\n+  mhlo.frontend_attributes = \"\",\n+  mhlo.input_output_alias = \"\",\n+  mhlo.is_dynamic = \"\",\n+  mhlo.is_same_data_across_replicas = \"\",\n+  mhlo.num_partitions = \"\",\n+  mhlo.num_replicas = \"\",\n+  mhlo.spmd_output_sharding = \"\",\n+  mhlo.spmd_parameters_shardings = [\"\"],\n+  mhlo.use_auto_spmd_partitioning = \"\",\n+  mhlo.xla_entry_computation_parameter_layouts = \"\",\n+  mhlo.xla_entry_computation_parameter_tiles = \"\",\n+  mhlo.xla_entry_computation_result_layout = \"\",\n+  mhlo.xla_entry_computation_result_tiles = \"\"\n+} {\n+  func.func @main(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {\n+    return %arg0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: module @module_unknown_attr\n+// CHECK-NOT: xla_tpu_user_reserved_hbm_bytes\n+module @module_unknown_attr attributes {xla.xla_tpu_user_reserved_hbm_bytes = 0 : i64} {\n+  func.func @func_unknown_attr(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {\n+    return %arg0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @func_known_attr\n+// CHECK-SAME: execution_thread\n+// CHECK-SAME: mhlo.frontend_attributes\n+func.func @func_known_attr(%arg0: tensor<2x2xi32>)\n+    -> tensor<2x2xi32> attributes {\n+      execution_thread = \"host\",\n+      mhlo.frontend_attributes = \"\"\n+    } {\n+  return %arg0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @func_unknown_attr\n+// CHECK-NOT: xla_tpu_user_reserved_hbm_bytes\n+func.func @func_unknown_attr(%arg0: tensor<2x2xi32>)\n+    -> tensor<2x2xi32> attributes {xla_tpu_user_reserved_hbm_bytes = 0 : i64} {\n+  return %arg0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @func_arg_known_attr\n+// CHECK-SAME: jax.buffer_donor\n+// CHECK-SAME: mhlo.frontend_attributes\n+// CHECK-SAME: mhlo.layout_mode\n+// CHECK-SAME: mhlo.parameter_replication\n+// CHECK-SAME: mhlo.sharding\n+// CHECK-SAME: tf.aliasing_output\n+func.func @func_arg_known_attr(%arg0: tensor<2x2xi32> {\n+    jax.buffer_donor = true,\n+    mhlo.frontend_attributes = \"\",\n+    mhlo.layout_mode = \"{1,0}\",\n+    mhlo.parameter_replication = [true],\n+    mhlo.sharding = \"\",\n+    tf.aliasing_output = 0 : i32\n+  }) -> tensor<2x2xi32> {\n+  return %arg0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @func_arg_unknown_attr\n+// CHECK-NOT: mhlo.unknown_attr\n+func.func @func_arg_unknown_attr(%arg0: tensor<2x2xi32> {mhlo.unknown_attr = \"\"}) -> tensor<2x2xi32> {\n+  return %arg0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @func_result_unknown_attr\n+// CHECK-NOT: mhlo.unknown_attr\n+func.func @func_result_unknown_attr(%arg0: tensor<2x2xi32>) -> (tensor<2x2xi32> {mhlo.unknown_attr = \"\"}) {\n+  return %arg0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @op_known_attr\n+// CHECK-NEXT: stablehlo.add\n+// CHECK-SAME: layout = \"\"\n+// CHECK-SAME: mhlo.frontend_attributes\n+// CHECK-SAME: mhlo.literal\n+// CHECK-SAME: mhlo.original_value\n+// CHECK-SAME: mhlo.sharding\n+// CHECK-SAME: result_layout\n+// CHECK-SAME: source_layout\n+// CHECK-SAME: xla_shape\n+func.func @op_known_attr(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {\n+  %0 = stablehlo.add %arg0, %arg0 {\n+    layout = \"\",\n+    mhlo.frontend_attributes = \"\",\n+    mhlo.literal = \"\",\n+    mhlo.original_value = \"\",\n+    mhlo.sharding = \"mhlo.sharding\",\n+    result_layout = \"\",\n+    source_layout = \"\",\n+    xla_shape = \"\"\n+  } : tensor<2x2xi32>\n+  return %0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @op_unknown_attr\n+// CHECK-NOT: xla.unknown_attr\n+func.func @op_unknown_attr(%arg0: tensor<2x2xi32>)\n+    -> tensor<2x2xi32> attributes {xla_tpu_user_reserved_hbm_bytes = 0 : i64} {\n+  %0 = stablehlo.add %arg0, %arg0 {xla.unknown_attr = \"\"} : tensor<2x2xi32>\n+  return %0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// expected-error@+1 {{SDY attribute encountered: \"sdy.somthing\". Run SDY export pass prior to sanitizing unregistered attributes.}}\n+module @sdy_module_attr_error attributes {sdy.somthing = \"\"} {\n+  func.func @main(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {\n+    return %arg0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// expected-error@+1 {{SDY attribute encountered: \"sdy.somthing\". Run SDY export pass prior to sanitizing unregistered attributes.}}\n+func.func @sdy_func_attr_error(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> attributes {sdy.somthing = \"\"} {\n+  return %arg0 : tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+// expected-error@+1 {{SDY attribute encountered: \"sdy.somthing\". Run SDY export pass prior to sanitizing unregistered attributes.}}\n+func.func @sdy_func_arg_attr_error(%arg0: tensor<2x2xi32> {sdy.somthing = \"\"}) -> tensor<2x2xi32> {\n+  return %arg0 : tensor<2x2xi32>\n+}\n+\n+\n+// -----\n+\n+func.func @sdy_op_attr_error(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {\n+  // expected-error @+1 {{SDY attribute encountered: \"sdy.somthing\". Run SDY export pass prior to sanitizing unregistered attributes.}}\n+  %0 = stablehlo.add %arg0, %arg0 {sdy.somthing = \"\"} : tensor<2x2xi32>\n+  return %0 : tensor<2x2xi32>\n+}"
        },
        {
            "sha": "d85a8a6db1fcebfaf64feb05c638eff7aa342c6d",
            "filename": "third_party/xla/xla/mlir_hlo/utils/unregistered_attributes.cc",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Funregistered_attributes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Funregistered_attributes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Funregistered_attributes.cc?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -0,0 +1,75 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"utils/unregistered_attributes.h\"\n+\n+#include <algorithm>\n+#include <array>\n+#include <string_view>\n+\n+namespace xla {\n+\n+bool IsKnownDiscardableModuleAttribute(std::string_view attr_name) {\n+  {\n+    static constexpr std::array<std::string_view, 14>\n+        kKnownDiscardableAttributes{\n+            xla::kMhloCrossProgramPrefetches,\n+            xla::kMhloFrontendAttributes,\n+            xla::kMhloInputOutputAlias,\n+            xla::kMhloIsDynamic,\n+            xla::kMhloNumPartitions,\n+            xla::kMhloNumReplicas,\n+            xla::kMhloReplication,\n+            xla::kMhloSpmdOutputSharding,\n+            xla::kMhloSpmdParametersShardings,\n+            xla::kMhloUseAutoSpmdPartitioning,\n+            xla::kMhloXlaEntryComputationParameterLayouts,\n+            xla::kMhloXlaEntryComputationParameterTiles,\n+            xla::kMhloXlaEntryComputationResultLayout,\n+            xla::kMhloXlaEntryComputationResultTiles,\n+        };\n+\n+    return std::find(kKnownDiscardableAttributes.begin(),\n+                     kKnownDiscardableAttributes.end(),\n+                     attr_name) != kKnownDiscardableAttributes.end();\n+  }\n+}\n+bool IsKnownDiscardableFuncAttribute(std::string_view attr_name) {\n+  static constexpr std::array<std::string_view, 8> kKnownDiscardableAttributes{\n+      xla::kExecutionThread,        xla::kJaxBufferDonor,\n+      xla::kMhloFrontendAttributes, xla::kMhloLayoutMode,\n+      xla::kMhloMemoryKind,         xla::kMhloParameterReplication,\n+      xla::kMhloSharding,           xla::kTfAliasingOutput,\n+  };\n+\n+  return std::find(kKnownDiscardableAttributes.begin(),\n+                   kKnownDiscardableAttributes.end(),\n+                   attr_name) != kKnownDiscardableAttributes.end();\n+}\n+\n+bool IsKnownDiscardableOpAttribute(std::string_view attr_name) {\n+  static constexpr std::array<std::string_view, 8> kKnownDiscardableAttributes{\n+      xla::kBitcastResultLayout, xla::kBitcastSourceLayout,\n+      xla::kInfeedLayout,        xla::kMhloFrontendAttributes,\n+      xla::kMhloLiteral,         xla::kMhloOriginalValueAttr,\n+      xla::kMhloSharding,        xla::kXlaShape,\n+  };\n+\n+  return std::find(kKnownDiscardableAttributes.begin(),\n+                   kKnownDiscardableAttributes.end(),\n+                   attr_name) != kKnownDiscardableAttributes.end();\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "5ab6bfb51713ccf8c0ac66d40ea55393aee1dc76",
            "filename": "third_party/xla/xla/mlir_hlo/utils/unregistered_attributes.h",
            "status": "added",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Funregistered_attributes.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a9d3b2d5a56548e091a0abaa7b90a33250e69ac/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Funregistered_attributes.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Funregistered_attributes.h?ref=6a9d3b2d5a56548e091a0abaa7b90a33250e69ac",
            "patch": "@@ -0,0 +1,81 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_MLIR_HLO_UTILS_UNREGISTERED_ATTRIBUTES_H_\n+#define XLA_MLIR_HLO_UTILS_UNREGISTERED_ATTRIBUTES_H_\n+\n+#include <string_view>\n+\n+namespace xla {\n+\n+// This file captures all discardable attributes that XLA supports.\n+// Attributes not in this list will be dropped when exporting to StableHLO.\n+\n+// Module level attributes require namespacing.\n+inline constexpr char kMhloCrossProgramPrefetches[] =\n+    \"mhlo.cross_program_prefetches\";\n+inline constexpr char kMhloInputOutputAlias[] = \"mhlo.input_output_alias\";\n+inline constexpr char kMhloIsDynamic[] = \"mhlo.is_dynamic\";\n+inline constexpr char kMhloLiteral[] = \"mhlo.literal\";\n+inline constexpr char kMhloReplication[] = \"mhlo.is_same_data_across_replicas\";\n+inline constexpr char kMhloSpmdOutputSharding[] = \"mhlo.spmd_output_sharding\";\n+inline constexpr char kMhloSpmdParametersShardings[] =\n+    \"mhlo.spmd_parameters_shardings\";\n+inline constexpr char kMhloUseAutoSpmdPartitioning[] =\n+    \"mhlo.use_auto_spmd_partitioning\";\n+inline constexpr char kMhloXlaEntryComputationParameterLayouts[] =\n+    \"mhlo.xla_entry_computation_parameter_layouts\";\n+inline constexpr char kMhloXlaEntryComputationParameterTiles[] =\n+    \"mhlo.xla_entry_computation_parameter_tiles\";\n+inline constexpr char kMhloXlaEntryComputationResultLayout[] =\n+    \"mhlo.xla_entry_computation_result_layout\";\n+inline constexpr char kMhloXlaEntryComputationResultTiles[] =\n+    \"mhlo.xla_entry_computation_result_tiles\";\n+inline constexpr char kMhloNumPartitions[] = \"mhlo.num_partitions\";\n+inline constexpr char kMhloNumReplicas[] = \"mhlo.num_replicas\";\n+\n+// Function attributes.\n+inline constexpr char kExecutionThread[] = \"execution_thread\";\n+inline constexpr char kJaxBufferDonor[] = \"jax.buffer_donor\";\n+inline constexpr char kMhloMemoryKind[] = \"mhlo.memory_kind\";\n+inline constexpr char kTfAliasingOutput[] = \"tf.aliasing_output\";\n+inline constexpr char kMhloParameterReplication[] =\n+    \"mhlo.parameter_replication\";\n+\n+// Op / Argument attributes.\n+inline constexpr char kMhloFrontendAttributes[] = \"mhlo.frontend_attributes\";\n+inline constexpr char kMhloLayoutMode[] = \"mhlo.layout_mode\";\n+inline constexpr char kMhloSharding[] = \"mhlo.sharding\";\n+inline constexpr char kInfeedLayout[] = \"layout\";\n+inline constexpr char kXlaShape[] = \"xla_shape\";\n+inline constexpr char kMhloOriginalValueAttr[] = \"mhlo.original_value\";\n+inline constexpr char kBitcastResultLayout[] = \"result_layout\";\n+inline constexpr char kBitcastSourceLayout[] = \"source_layout\";\n+\n+// Returns true if the given attribute name is a known XLA discardable module\n+// attribute.\n+bool IsKnownDiscardableModuleAttribute(std::string_view attr_name);\n+\n+// Returns true if the given attribute name is a known XLA discardable function\n+// attribute.\n+bool IsKnownDiscardableFuncAttribute(std::string_view attr_name);\n+\n+// Returns true if the given attribute name is a known XLA discardable op\n+// attribute, this applies to all ops except for module and func ops.\n+bool IsKnownDiscardableOpAttribute(std::string_view attr_name);\n+\n+}  // namespace xla\n+\n+#endif  // XLA_MLIR_HLO_UTILS_UNREGISTERED_ATTRIBUTES_H_"
        }
    ],
    "stats": {
        "total": 875,
        "additions": 677,
        "deletions": 198
    }
}