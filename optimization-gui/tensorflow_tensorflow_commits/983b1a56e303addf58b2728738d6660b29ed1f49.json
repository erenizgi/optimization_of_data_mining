{
    "author": "draganmladjenovic",
    "message": "PR #26704: [ROCm] Embeded device lib\n\nImported from GitHub PR https://github.com/openxla/xla/pull/26704\n\nCopybara import of the project:\n\n--\n984e7166ab165700e176deb45a032043d09f6e2b by Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>:\n\n[ROCm] Introduce xla_gpu_use_embeded_device_lib to use bundled bitcode files\n\nAlso trim bitcode file list to ockl.bc and ocml.bc only.\n\nMerging this change closes #26704\n\nPiperOrigin-RevId: 816159553",
    "sha": "983b1a56e303addf58b2728738d6660b29ed1f49",
    "files": [
        {
            "sha": "925c9d48df3df246ca78a0ff31f1e67add4aefef",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -121,7 +121,6 @@ limitations under the License.\n #include \"xla/service/dump.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/service/gpu/llvm_gpu_backend/amdgpu_backend.h\"\n #include \"xla/service/gpu/llvm_gpu_backend/nvptx_libdevice_path.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/model/symbolic_tile_analysis.h\"\n@@ -139,7 +138,6 @@ limitations under the License.\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/rocm_rocdl_path.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n@@ -2314,8 +2312,7 @@ std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n     return nvptx::LibDevicePath(\n         hlo_config.debug_options().xla_gpu_cuda_data_dir());\n   }\n-  return amdgpu::LibDevicePath(\n-      device_info.rocm_compute_capability().gcn_arch_name(), tsl::RocdlRoot());\n+  return \"\";\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "501602b5db28c44ea5bd84d2c6c8634813f5e62a",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -2170,6 +2170,11 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       \"Threshold until which elemental dot emitter is preferred for GEMMs \"\n       \"(minimum combined number of elements of both matrices \"\n       \"in non-batch dimensions to be considered for a rewrite).\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_use_embeded_device_lib\",\n+      bool_setter_for(&DebugOptions::set_xla_gpu_use_embeded_device_lib),\n+      debug_options->xla_gpu_use_embeded_device_lib(),\n+      \"Whether to use embeded bitcode library in codegen.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_use_memcpy_local_p2p\",\n       bool_setter_for(&DebugOptions::set_xla_gpu_use_memcpy_local_p2p),"
        },
        {
            "sha": "cdedeafa4ee132fd1cbbb50fcaec9e93fb955eec",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/BUILD",
            "status": "modified",
            "additions": 80,
            "deletions": 3,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -1,4 +1,5 @@\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"//xla:py_strict.bzl\", \"py_strict_binary\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\n     \"//xla/tsl:tsl.bzl\",\n@@ -120,11 +121,58 @@ cc_library(\n     ],\n )\n \n+py_strict_binary(\n+    name = \"generate_amdgpu_device_lib_data_tool\",\n+    srcs = [\"generate_amdgpu_device_lib_data_tool.py\"],\n+)\n+\n+genrule(\n+    name = \"generate_amdgpu_device_lib_data\",\n+    srcs = [\n+        \"@rocm_device_libs//:ockl\",\n+        \"@rocm_device_libs//:ocml\",\n+    ],\n+    outs = [\"amdgpu_device_lib_data.h\"],\n+    cmd = \"$(location {}) --llvm_link_bin $(location {}) $(SRCS) -o $@ --cpp_identifier=kAMDGPUDeviceLibData\".format(\n+        \":generate_amdgpu_device_lib_data_tool\",\n+        \"@llvm-project//llvm:llvm-link\",\n+    ),\n+    tags = if_google([\n+        # Embedding libdevice is not supported in the Google-internal build.\n+        \"manual\",\n+        \"notap\",\n+        \"nobuilder\",\n+    ]),\n+    tools = [\n+        \":generate_amdgpu_device_lib_data_tool\",\n+        \"@llvm-project//llvm:llvm-link\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"amdgpu_device_lib_data\",\n+    hdrs = [\n+        \":generate_amdgpu_device_lib_data\",\n+    ],\n+    tags = if_google([\n+        # Embedding libdevice is not supported in the Google-internal build.\n+        \"manual\",\n+        \"notap\",\n+        \"nobuilder\",\n+    ]),\n+    deps = [\n+        \"@llvm-project//llvm:Support\",\n+    ],\n+)\n+\n cc_library(\n     name = \"amdgpu_backend\",\n     srcs = [\"amdgpu_backend.cc\"],\n     hdrs = [\"amdgpu_backend.h\"],\n-    local_defines = if_oss([\"HAS_SUPPORT_FOR_LLD_AS_A_LIBRARY=1\"]),\n+    local_defines = if_oss([\n+        \"HAS_SUPPORT_FOR_LLD_AS_A_LIBRARY=1\",\n+        \"HAS_SUPPORT_FOR_EMBEDDED_LIB_DEVICE=1\",\n+    ]),\n     deps = [\n         \":llvm_gpu_backend\",\n         \":load_ir_module\",\n@@ -140,11 +188,13 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util:env_var\",\n         \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/synchronization\",\n         \"@llvm-project//llvm:AMDGPUAsmParser\",  # buildcleaner: keep\n         \"@llvm-project//llvm:Analysis\",\n         \"@llvm-project//llvm:BitReader\",\n@@ -159,13 +209,13 @@ cc_library(\n         \"@llvm-project//llvm:Scalar\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:Target\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:random\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n     ] + if_oss([\n         # keep sorted\n-        \"@com_google_absl//absl/base:core_headers\",\n-        \"@com_google_absl//absl/synchronization\",\n+        \":amdgpu_device_lib_data\",\n         \"@llvm-project//lld:Common\",\n         \"@llvm-project//lld:ELF\",  # buildcleaner: keep\n     ]),\n@@ -232,6 +282,33 @@ xla_cc_test(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"amdgpu_bitcode_link_test\",\n+    size = \"small\",\n+    srcs = [\"amdgpu_bitcode_link_test.cc\"],\n+    data = [\n+        \"tests_data/amdgpu.ll\",\n+    ],\n+    tags = if_google([\n+        # Embedded libdevice is required for this test, but not supported in the Google-internal build.\n+        \"notap\",\n+        \"manual\",\n+        \"nobuilder\",\n+    ]) + [\n+        \"gpu\",\n+        \"rocm-only\",\n+    ],\n+    deps = [\n+        \":amdgpu_backend\",\n+        \":load_ir_module\",\n+        \"//xla/tsl/platform:rocm_rocdl_path\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@local_tsl//tsl/platform:path\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"load_ir_module_test\",\n     size = \"small\","
        },
        {
            "sha": "b9fa8a83cad1eadd0862c9e106d9e057d3a91de4",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/amdgpu_backend.cc",
            "status": "modified",
            "additions": 138,
            "deletions": 69,
            "changes": 207,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <functional>\n #include <ios>\n #include <memory>\n-#include <mutex>  // NOLINT\n #include <optional>\n #include <string>\n #include <system_error>  // NOLINT\n@@ -30,6 +29,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/call_once.h\"\n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -38,7 +38,9 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringSet.h\"\n #include \"llvm/Analysis/CGSCCPassManager.h\"\n #include \"llvm/Analysis/LazyCallGraph.h\"\n #include \"llvm/Analysis/LoopAnalysisManager.h\"\n@@ -47,6 +49,9 @@ limitations under the License.\n #include \"llvm/Bitcode/BitcodeReader.h\"\n #include \"llvm/Bitcode/BitcodeWriter.h\"\n #include \"llvm/CodeGen/CommandFlags.h\"\n+#include \"llvm/IR/Constants.h\"\n+#include \"llvm/IR/DerivedTypes.h\"\n+#include \"llvm/IR/GlobalVariable.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n #include \"llvm/IR/Metadata.h\"\n@@ -59,12 +64,14 @@ limitations under the License.\n #include \"llvm/PassRegistry.h\"\n #include \"llvm/Passes/PassBuilder.h\"\n #include \"llvm/Passes/StandardInstrumentations.h\"\n+#include \"llvm/Support/Alignment.h\"\n #include \"llvm/Support/CodeGen.h\"\n #include \"llvm/Support/FileSystem.h\"\n #include \"llvm/Support/Program.h\"\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"llvm/Target/TargetMachine.h\"\n+#include \"llvm/TargetParser/TargetParser.h\"\n #include \"llvm/Transforms/IPO/AlwaysInliner.h\"\n #include \"llvm/Transforms/IPO/Internalize.h\"\n #include \"llvm/Transforms/Scalar.h\"\n@@ -89,45 +96,33 @@ limitations under the License.\n #include <array>\n \n #include \"absl/base/const_init.h\"\n-#include \"absl/synchronization/mutex.h\"\n #include \"lld/Common/Driver.h\"\n LLD_HAS_DRIVER(elf)\n #endif\n \n+#ifdef HAS_SUPPORT_FOR_EMBEDDED_LIB_DEVICE\n+#include \"xla/service/gpu/llvm_gpu_backend/amdgpu_device_lib_data.h\"\n+#else\n+constexpr const char* kAMDGPUDeviceLibData = \"\";\n+#endif\n+\n namespace xla {\n namespace gpu {\n namespace {\n \n // Inline threshold value to use in LLVM AMDGPU backend.\n const int kAMDGPUInlineThreshold = 0x100000;\n+const int32_t kAMDGPUAbiVersion = 500;\n \n // Gets the ROCm-Device-Libs filenames for a particular AMDGPU version.\n-std::vector<std::string> GetROCDLPaths(std::string gcn_arch_name,\n-                                       const std::string& rocdl_dir_path) {\n-  // AMDGPU version-neutral bitcodes.\n-  static std::vector<std::string>* rocdl_filenames =\n-      new std::vector<std::string>(\n-          {\"opencl.bc\", \"ocml.bc\", \"ockl.bc\", \"oclc_finite_only_off.bc\",\n-           \"oclc_daz_opt_off.bc\", \"oclc_correctly_rounded_sqrt_on.bc\",\n-           \"oclc_unsafe_math_off.bc\", \"oclc_wavefrontsize64_on.bc\",\n-           \"oclc_abi_version_500.bc\"});\n-\n+std::vector<std::string> GetROCDLPaths(const std::string& rocdl_dir_path) {\n   // Construct full path to ROCDL bitcode libraries.\n   std::vector<std::string> result;\n-  result.reserve(rocdl_filenames->size() + 1);\n-  for (auto& filename : *rocdl_filenames) {\n-    result.push_back(tsl::io::JoinPath(rocdl_dir_path, filename));\n+  result.reserve(2);\n+  for (absl::string_view filename : {\"ocml.bc\", \"ockl.bc\"}) {\n+    result.emplace_back(tsl::io::JoinPath(rocdl_dir_path, filename));\n   }\n \n-  // Add AMDGPU version-specific bitcodes.\n-  std::vector<std::string> tokens = absl::StrSplit(gcn_arch_name, ':');\n-  std::string amdgpu_version = gcn_arch_name;\n-  if (!tokens.empty() && tokens[0].size() >= 3) {\n-    amdgpu_version = tokens[0].substr(3);\n-  }\n-  result.push_back(tsl::io::JoinPath(\n-      rocdl_dir_path,\n-      absl::StrCat(\"oclc_isa_version_\", amdgpu_version, \".bc\")));\n   return result;\n }\n \n@@ -140,10 +135,10 @@ struct HsacoCacheEntry {\n \n struct HsacoCache {\n  protected:\n-  std::vector<HsacoCacheEntry> cache;\n-  std::mutex m_mutex;\n-  int request_count = 0;\n-  int hit_count = 0;\n+  std::vector<HsacoCacheEntry> cache ABSL_GUARDED_BY(mutex);\n+  absl::Mutex mutex;\n+  int request_count ABSL_GUARDED_BY(mutex) = 0;\n+  int hit_count ABSL_GUARDED_BY(mutex) = 0;\n \n  public:\n   static bool Find(const std::string& ir, uint64_t& hash,\n@@ -156,29 +151,38 @@ static HsacoCache g_hsacoCache;  // NOLINT: static/global vars forbidden\n \n bool HsacoCache::Find(const std::string& ir, uint64_t& hash,\n                       const std::string& gfx, std::vector<uint8_t>& hsaco) {\n-  std::lock_guard<std::mutex> lg(g_hsacoCache.m_mutex);\n+  absl::MutexLock lock(g_hsacoCache.mutex);\n   hash = std::hash<std::string>{}(ir);\n   bool hit = false;\n   for (auto& x : g_hsacoCache.cache) {\n-    if (x.hash != hash) continue;\n-    if (x.gfx != gfx) continue;\n-    if (x.ir != ir) continue;\n+    if (x.hash != hash) {\n+      continue;\n+    }\n+    if (x.gfx != gfx) {\n+      continue;\n+    }\n+    if (x.ir != ir) {\n+      continue;\n+    }\n     hsaco = x.hsaco;\n     hit = true;\n     break;\n   }\n   g_hsacoCache.request_count++;\n-  if (hit) g_hsacoCache.hit_count++;\n-  if (!(g_hsacoCache.request_count % 50))\n+  if (hit) {\n+    g_hsacoCache.hit_count++;\n+  }\n+  if (!(g_hsacoCache.request_count % 50)) {\n     VLOG(1) << \"HSACO cache: \" << g_hsacoCache.request_count << \" requests, \"\n             << g_hsacoCache.hit_count << \" hits\";\n+  }\n   return hit;\n }\n \n void HsacoCache::Add(const std::string& ir, uint64_t hash,\n                      const std::string& gfx,\n                      const std::vector<uint8_t>& hsaco) {\n-  std::lock_guard<std::mutex> lg(g_hsacoCache.m_mutex);\n+  absl::MutexLock lock(g_hsacoCache.mutex);\n   g_hsacoCache.cache.resize(g_hsacoCache.cache.size() + 1);\n   g_hsacoCache.cache.back().ir = ir;\n   g_hsacoCache.cache.back().hash = hash;\n@@ -330,18 +334,6 @@ absl::StatusOr<std::vector<uint8_t>> EmitModuleToHsaco(\n   return hsaco;\n }\n \n-// Links ROCm-Device-Libs into the given module if the module needs it.\n-absl::Status LinkROCDLIfNecessary(llvm::Module* module,\n-                                  std::string gcn_arch_name,\n-                                  const std::string& rocdl_dir_path) {\n-  if (!CouldNeedDeviceBitcode(*module)) {\n-    return absl::OkStatus();\n-  }\n-\n-  return LinkWithBitcodeVector(module,\n-                               GetROCDLPaths(gcn_arch_name, rocdl_dir_path));\n-}\n-\n absl::Status AMDGPUTargetModuleLinker(\n     llvm::Module* module, se::GpuComputeCapability gpu_version,\n     const DebugOptions& debug_options,\n@@ -354,19 +346,18 @@ absl::Status AMDGPUTargetModuleLinker(\n     return xla::Internal(\"Incompatible compute capability was specified.\");\n   }\n \n-  std::string gcn_arch_name = compute_capability->gcn_arch_name();\n   TF_RETURN_IF_ERROR(\n-      LinkROCDLIfNecessary(module, gcn_arch_name, device_bitcode_dir_path));\n+      amdgpu::LinkROCDLIfNecessary(module, compute_capability->gfx_version(),\n+                                   debug_options, device_bitcode_dir_path));\n \n   // If ftz is enabled, set it as an attribute on every function in the module.\n   if (debug_options.xla_gpu_ftz()) {\n     for (llvm::Function& fn : *module) {\n       fn.addFnAttr(\"denormal-fp-math-f32\", \"preserve-sign\");\n     }\n   }\n-  const int32_t kAbiVersion = 500;\n   module->addModuleFlag(llvm::Module::Error, \"amdhsa_code_object_version\",\n-                        kAbiVersion);\n+                        kAMDGPUAbiVersion);\n \n   return absl::OkStatus();\n }\n@@ -385,12 +376,17 @@ std::string MapGCNArchNameTokenToFeatureStr(const std::string& token,\n                                             const std::string& gfx) {\n   if (token == \"sramecc+\") {\n     return \"+sramecc\";\n-  } else if (token == \"sramecc-\") {\n-    if (gfx == \"gfx90a\" || gfx == \"gfx942\") return \"\";\n+  }\n+  if (token == \"sramecc-\") {\n+    if (gfx == \"gfx90a\" || gfx == \"gfx942\") {\n+      return \"\";\n+    }\n     return \"-sramecc\";\n-  } else if (token == \"xnack+\") {\n+  }\n+  if (token == \"xnack+\") {\n     return \"+xnack\";\n-  } else if (token == \"xnack-\") {\n+  }\n+  if (token == \"xnack-\") {\n     return \"-xnack\";\n   }\n   return \"\";\n@@ -405,7 +401,9 @@ std::pair<std::string, std::string> GetFeatureStrFromGCNArchName(\n   // feature str, based on the underlying GPU HW to get max performance.\n   std::vector<std::string> tokens = absl::StrSplit(gcn_arch_name, ':');\n   std::vector<std::string> mapped_tokens;\n-  if (!tokens.empty()) gfx = tokens[0];\n+  if (!tokens.empty()) {\n+    gfx = tokens[0];\n+  }\n   for (auto it = tokens.begin(); it != tokens.end(); it++) {\n     // Skip the first token, that is the gfxNNN str\n     // The rest of the tokens are the feature/targetid strings\n@@ -476,6 +474,84 @@ void AMDGPUBackendInit(const DebugOptions& debug_options,\n \n namespace amdgpu {\n \n+// Links ROCm-Device-Libs into the given module if the module needs it.\n+absl::Status LinkROCDLIfNecessary(llvm::Module* module,\n+                                  const std::string& gfx_version,\n+                                  const DebugOptions& debug_options,\n+                                  const std::string& rocdl_dir_path) {\n+  if (!CouldNeedDeviceBitcode(*module)) {\n+    return absl::OkStatus();\n+  }\n+\n+  auto addControlVariable = [&](llvm::StringRef name, uint32_t value,\n+                                uint32_t bitwidth = 8) {\n+    if (module->getNamedGlobal(name)) {\n+      return;\n+    }\n+    llvm::IntegerType* type =\n+        llvm::IntegerType::getIntNTy(module->getContext(), bitwidth);\n+    llvm::GlobalVariable* control_variable = new llvm::GlobalVariable(\n+        *module, type, /*isConstant=*/true,\n+        llvm::GlobalValue::LinkageTypes::LinkOnceODRLinkage,\n+        llvm::ConstantInt::get(type, value), name, /*before=*/nullptr,\n+        /*threadLocalMode=*/llvm::GlobalValue::ThreadLocalMode::NotThreadLocal,\n+        /*addressSpace=*/4);\n+    control_variable->setVisibility(\n+        llvm::GlobalValue::VisibilityTypes::ProtectedVisibility);\n+    control_variable->setAlignment(llvm::MaybeAlign(bitwidth / 8));\n+    control_variable->setUnnamedAddr(llvm::GlobalValue::UnnamedAddr::Local);\n+  };\n+\n+  addControlVariable(\"__oclc_finite_only_opt\", false);\n+  // TODO(rocm): Maybe check ftz for this one\n+  addControlVariable(\"__oclc_daz_opt\", false);\n+  addControlVariable(\"__oclc_correctly_rounded_sqrt32\", true);\n+  addControlVariable(\"__oclc_unsafe_math_opt\", false);\n+\n+  auto [major, minor, stepping] = llvm::AMDGPU::getIsaVersion(gfx_version);\n+\n+  CHECK(major != 0) << \"Could not parse gfx_version.\";\n+\n+  // TODO(rocm): Not great, not terrible\n+  addControlVariable(\"__oclc_wavefrontsize64\", major == 9);\n+  addControlVariable(\"__oclc_ISA_version\",\n+                     1000 * major + 100 * stepping + minor, 32);\n+  addControlVariable(\"__oclc_ABI_version\", kAMDGPUAbiVersion, 32);\n+\n+  if (debug_options.xla_gpu_use_embeded_device_lib()) {\n+    llvm::Linker linker(*module);\n+    auto device_lib = llvm::getLazyBitcodeModule(\n+        {kAMDGPUDeviceLibData, \"device_lib\"}, module->getContext());\n+    if (!device_lib) {\n+      return absl::InternalError(\"Error loading embeded device lib.\");\n+    }\n+    if (linker.linkInModule(\n+            std::move(*device_lib), llvm::Linker::Flags::LinkOnlyNeeded,\n+            [](llvm::Module& M, const llvm::StringSet<>& GVS) {\n+              internalizeModule(M, [&GVS](const llvm::GlobalValue& GV) {\n+                return !GV.hasName() || (GVS.count(GV.getName()) == 0);\n+              });\n+            })) {\n+      return absl::InternalError(\"Error linking embeded device lib.\");\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  TF_RETURN_IF_ERROR(\n+      LinkWithBitcodeVector(module, GetROCDLPaths(rocdl_dir_path)));\n+\n+  // Sanitize stray metadata from the bitcode files\n+  if (auto* opencl_version = module->getNamedMetadata(\"opencl.ocl.version\")) {\n+    module->eraseNamedMetadata(opencl_version);\n+  }\n+\n+  if (auto* ident = module->getNamedMetadata(\"llvm.ident\")) {\n+    module->eraseNamedMetadata(ident);\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n std::vector<std::string> GetAMDGPUBackendOptions(\n     const DebugOptions& debug_options) {\n   std::vector<std::string> backend_llvm_opts;\n@@ -491,17 +567,6 @@ std::vector<std::string> GetAMDGPUBackendOptions(\n   return backend_llvm_opts;\n }\n \n-std::string LibDevicePath(std::string gcn_arch_name,\n-                          const std::string& rocdl_dir_path) {\n-  auto libdevice_dir_paths = GetROCDLPaths(gcn_arch_name, rocdl_dir_path);\n-  for (auto libdevice_dir_path : libdevice_dir_paths) {\n-    if (libdevice_dir_path.find(\"ocml.bc\")) {\n-      return libdevice_dir_path;\n-    }\n-  }\n-  return \"\";\n-}\n-\n absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(\n     llvm::Module* module, se::GpuComputeCapability gpu_version,\n     const DebugOptions& debug_options,\n@@ -524,11 +589,15 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(\n   // the code is the same (but verify that they are what we expect).\n   if (str.size() >= 13 && str.substr(0, 13) == \"; ModuleID = \") {\n     auto pos = str.find('\\n');\n-    if (pos != std::string::npos) str = str.substr(pos + 1);\n+    if (pos != std::string::npos) {\n+      str = str.substr(pos + 1);\n+    }\n   }\n   if (str.size() >= 18 && str.substr(0, 18) == \"source_filename = \") {\n     auto pos = str.find('\\n');\n-    if (pos != std::string::npos) str = str.substr(pos + 1);\n+    if (pos != std::string::npos) {\n+      str = str.substr(pos + 1);\n+    }\n   }\n   str += module_config_cache_key;\n   {"
        },
        {
            "sha": "007bbbeb90cbffa2c328587811c5ec3572985a77",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/amdgpu_backend.h",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.h?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -27,9 +27,11 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n \n namespace xla::gpu::amdgpu {\n-// Get path to libdevice file.\n-std::string LibDevicePath(std::string gcn_arch_name,\n-                          const std::string& rocdl_dir_path);\n+// Links ROCm-Device-Libs into the given module if the module needs it.\n+absl::Status LinkROCDLIfNecessary(llvm::Module* module,\n+                                  const std::string& gfx_version,\n+                                  const DebugOptions& debug_options,\n+                                  const std::string& rocdl_dir_path);\n // Compiles the argument module and returns it with LLVM AMDGPU backend.\n // rocdl_dir_path is the parent directory of ROCm-Device-Libs bitcode libraries.\n // The contents of the module may be changed."
        },
        {
            "sha": "9d10f5e182d55c232d405c77ce56fcb20a4209b1",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/amdgpu_bitcode_link_test.cc",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_bitcode_link_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_bitcode_link_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_bitcode_link_test.cc?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -0,0 +1,70 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <string>\n+\n+#include <gtest/gtest.h>\n+#include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/amdgpu_backend.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/load_ir_module.h\"\n+#include \"xla/tsl/platform/rocm_rocdl_path.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"tsl/platform/path.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+std::string TestIRFile() {\n+  return tsl::io::JoinPath(tsl::testing::XlaSrcRoot(), \"service\", \"gpu\",\n+                           \"llvm_gpu_backend\", \"tests_data\", \"amdgpu.ll\");\n+}\n+\n+bool HasUndefinedFunctions(const llvm::Module& M) {\n+  for (const llvm::Function& F : M) {\n+    if (F.isDeclaration() && !F.isIntrinsic()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+TEST(BitcodeLinkTest, TestLinkEmbeded) {\n+  llvm::LLVMContext llvm_context;\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_use_embeded_device_lib(true);\n+  auto module = LoadIRModule(TestIRFile(), &llvm_context);\n+  ASSERT_TRUE(HasUndefinedFunctions(*module));\n+  auto status = amdgpu::LinkROCDLIfNecessary(module.get(), \"gfx1200\",\n+                                             debug_options, \"<empty>\");\n+  ASSERT_TRUE(status.ok());\n+  ASSERT_FALSE(HasUndefinedFunctions(*module));\n+}\n+\n+TEST(BitcodeLinkTest, TestLinkFromInstallation) {\n+  llvm::LLVMContext llvm_context;\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_use_embeded_device_lib(false);\n+  auto module = LoadIRModule(TestIRFile(), &llvm_context);\n+  ASSERT_TRUE(HasUndefinedFunctions(*module));\n+  auto status = amdgpu::LinkROCDLIfNecessary(module.get(), \"gfx1200\",\n+                                             debug_options, tsl::RocdlRoot());\n+  ASSERT_TRUE(status.ok());\n+  ASSERT_FALSE(HasUndefinedFunctions(*module));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "15758215ba283550c81b1513f2259735d4059107",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/generate_amdgpu_device_lib_data_tool.py",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fgenerate_amdgpu_device_lib_data_tool.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fgenerate_amdgpu_device_lib_data_tool.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fgenerate_amdgpu_device_lib_data_tool.py?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -0,0 +1,84 @@\n+# Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tool to generate C++ headers from LLVM bitcode files.\n+\n+This tool links multiple LLVM bitcode files using llvm-link and then converts\n+the resulting bitcode into a C++ header file containing a byte array and an\n+llvm::StringRef.\n+\"\"\"\n+\n+import argparse\n+import itertools\n+import subprocess\n+\n+\n+def main():\n+  parser = argparse.ArgumentParser()\n+  parser.add_argument(\n+      \"--llvm_link_bin\", required=True, help=\"Path to the llvm-link binary\"\n+  )\n+  parser.add_argument(\n+      \"-o\", \"--output\", required=True, help=\"Output filename for the C++ header\"\n+  )\n+  parser.add_argument(\n+      \"input_files\", nargs=\"+\", help=\"Variable number of input filenames\"\n+  )\n+  parser.add_argument(\n+      \"--cpp_namespace\",\n+      default=\"\",\n+      help=\"Namespace to be used when generating data\",\n+  )\n+  parser.add_argument(\n+      \"--cpp_identifier\",\n+      required=True,\n+      help=\"Identifier to be used to refer to data\",\n+  )\n+\n+  args = parser.parse_args()\n+  llvm_link_bin = args.llvm_link_bin\n+  output_filename = args.output\n+  input_filenames = args.input_files\n+  cpp_namespace = args.cpp_namespace\n+  cpp_identifier = args.cpp_identifier\n+\n+  result = subprocess.run(\n+      [llvm_link_bin, \"-f\", \"-o\", \"-\", \"/dev/null\"]\n+      + list(\n+          itertools.chain.from_iterable(\n+              (\"--override\", f) for f in input_filenames\n+          )\n+      ),\n+      capture_output=True,\n+      check=True,\n+  )\n+\n+  llvm_output = result.stdout\n+  data_string = \"\".join(\"\\\\x{:02x}\".format(byte) for byte in llvm_output)\n+\n+  with open(output_filename, \"w\") as output_file:\n+    output_file.write(f\"\"\"\\\n+#pragma once\n+\n+#include \"llvm/ADT/StringRef.h\"\n+\n+namespace {cpp_namespace} {{\n+  inline const char kRaw_{cpp_identifier}[] = \"{data_string}\";\n+  constexpr llvm::StringRef {cpp_identifier}{{kRaw_{cpp_identifier}, sizeof(kRaw_{cpp_identifier})}};\n+}} // namespace {cpp_namespace}\n+\"\"\")\n+\n+\n+if __name__ == \"__main__\":\n+  main()"
        },
        {
            "sha": "a183d4531e40bf09018b498efd1301ee0475698d",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/tests_data/amdgpu.ll",
            "status": "added",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu.ll",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu.ll",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Ftests_data%2Famdgpu.ll?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -0,0 +1,65 @@\n+; ModuleID = 'example.hip'\n+source_filename = \"example.hip\"\n+target datalayout = \"e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-p7:160:256:256:32-p8:128:128:128:48-p9:192:256:256:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7:8:9\"\n+target triple = \"amdgcn-amd-amdhsa\"\n+\n+@__hip_cuid_dbc6fc0be16cd677 = addrspace(1) global i8 0\n+@llvm.compiler.used = appending addrspace(1) global [1 x ptr] [ptr addrspacecast (ptr addrspace(1) @__hip_cuid_dbc6fc0be16cd677 to ptr)], section \"llvm.metadata\"\n+\n+; Function Attrs: convergent mustprogress nofree norecurse nounwind willreturn memory(read, argmem: readwrite)\n+define protected amdgpu_kernel void @_Z3fooPfS_i(ptr addrspace(1) noundef writeonly captures(none) %dst.coerce, ptr addrspace(1) noundef readonly captures(none) %src.coerce, i32 noundef %limit) local_unnamed_addr #0 {\n+entry:\n+  %call.i12 = tail call i64 @__ockl_get_group_id(i32 noundef 0) #3\n+  %conv.i = trunc i64 %call.i12 to i32\n+  %call.i = tail call i64 @__ockl_get_local_size(i32 noundef 0) #3\n+  %conv.i13 = trunc i64 %call.i to i32\n+  %mul = mul i32 %conv.i13, %conv.i\n+  %call.i14 = tail call i64 @__ockl_get_local_id(i32 noundef 0) #3\n+  %conv.i15 = trunc i64 %call.i14 to i32\n+  %add = add i32 %mul, %conv.i15\n+  %cmp.not = icmp slt i32 %add, %limit\n+  br i1 %cmp.not, label %if.end, label %cleanup\n+\n+if.end:                                           ; preds = %entry\n+  %idxprom = sext i32 %add to i64\n+  %arrayidx7 = getelementptr inbounds float, ptr addrspace(1) %dst.coerce, i64 %idxprom\n+  %arrayidx = getelementptr inbounds float, ptr addrspace(1) %src.coerce, i64 %idxprom\n+  %0 = load float, ptr addrspace(1) %arrayidx, align 4, !tbaa !5\n+  %call5 = tail call contract float @__ocml_exp_f32(float noundef %0) #4\n+  store float %call5, ptr addrspace(1) %arrayidx7, align 4, !tbaa !5\n+  br label %cleanup\n+\n+cleanup:                                          ; preds = %entry, %if.end\n+  ret void\n+}\n+\n+; Function Attrs: convergent mustprogress nofree nounwind willreturn memory(read)\n+declare hidden float @__ocml_exp_f32(float noundef) local_unnamed_addr #1\n+\n+; Function Attrs: convergent mustprogress nofree nounwind willreturn memory(none)\n+declare hidden i64 @__ockl_get_group_id(i32 noundef) local_unnamed_addr #2\n+\n+; Function Attrs: convergent mustprogress nofree nounwind willreturn memory(none)\n+declare hidden i64 @__ockl_get_local_size(i32 noundef) local_unnamed_addr #2\n+\n+; Function Attrs: convergent mustprogress nofree nounwind willreturn memory(none)\n+declare hidden i64 @__ockl_get_local_id(i32 noundef) local_unnamed_addr #2\n+\n+attributes #0 = { convergent mustprogress nofree norecurse nounwind willreturn memory(read, argmem: readwrite) \"amdgpu-flat-work-group-size\"=\"1,1024\" \"amdgpu-waves-per-eu\"=\"8,16\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"gfx1200\" \"target-features\"=\"+16-bit-insts,+atomic-buffer-global-pk-add-f16-insts,+atomic-buffer-pk-add-bf16-inst,+atomic-ds-pk-add-16-insts,+atomic-fadd-rtn-insts,+atomic-flat-pk-add-16-insts,+atomic-fmin-fmax-global-f32,+atomic-global-pk-add-bf16-inst,+ci-insts,+dl-insts,+dot10-insts,+dot11-insts,+dot12-insts,+dot7-insts,+dot8-insts,+dot9-insts,+dpp,+fp8-conversion-insts,+gfx10-3-insts,+gfx10-insts,+gfx11-insts,+gfx12-insts,+gfx8-insts,+gfx9-insts,+wavefrontsize32\" \"uniform-work-group-size\"=\"true\" }\n+attributes #1 = { convergent mustprogress nofree nounwind willreturn memory(read) \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"gfx1200\" \"target-features\"=\"+16-bit-insts,+atomic-buffer-global-pk-add-f16-insts,+atomic-buffer-pk-add-bf16-inst,+atomic-ds-pk-add-16-insts,+atomic-fadd-rtn-insts,+atomic-flat-pk-add-16-insts,+atomic-fmin-fmax-global-f32,+atomic-global-pk-add-bf16-inst,+ci-insts,+dl-insts,+dot10-insts,+dot11-insts,+dot12-insts,+dot7-insts,+dot8-insts,+dot9-insts,+dpp,+fp8-conversion-insts,+gfx10-3-insts,+gfx10-insts,+gfx11-insts,+gfx12-insts,+gfx8-insts,+gfx9-insts,+wavefrontsize32\" \"uniform-work-group-size\"=\"false\" }\n+attributes #2 = { convergent mustprogress nofree nounwind willreturn memory(none) \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"gfx1200\" \"target-features\"=\"+16-bit-insts,+atomic-buffer-global-pk-add-f16-insts,+atomic-buffer-pk-add-bf16-inst,+atomic-ds-pk-add-16-insts,+atomic-fadd-rtn-insts,+atomic-flat-pk-add-16-insts,+atomic-fmin-fmax-global-f32,+atomic-global-pk-add-bf16-inst,+ci-insts,+dl-insts,+dot10-insts,+dot11-insts,+dot12-insts,+dot7-insts,+dot8-insts,+dot9-insts,+dpp,+fp8-conversion-insts,+gfx10-3-insts,+gfx10-insts,+gfx11-insts,+gfx12-insts,+gfx8-insts,+gfx9-insts,+wavefrontsize32\" \"uniform-work-group-size\"=\"false\" }\n+attributes #3 = { convergent nounwind willreturn memory(none) }\n+attributes #4 = { convergent nounwind willreturn memory(read) }\n+\n+!llvm.module.flags = !{!0, !1, !2, !3}\n+!llvm.ident = !{!4}\n+\n+!0 = !{i32 1, !\"amdhsa_code_object_version\", i32 600}\n+!1 = !{i32 1, !\"amdgpu_printf_kind\", !\"hostcall\"}\n+!2 = !{i32 1, !\"wchar_size\", i32 4}\n+!3 = !{i32 8, !\"PIC Level\", i32 2}\n+!4 = !{!\"clang version 22.0.0git\"}\n+!5 = !{!6, !6, i64 0}\n+!6 = !{!\"float\", !7, i64 0}\n+!7 = !{!\"omnipotent char\", !8, i64 0}\n+!8 = !{!\"Simple C++ TBAA\"}"
        },
        {
            "sha": "9db76193debf16360a67ab285bca902ae2d5d6c0",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/983b1a56e303addf58b2728738d6660b29ed1f49/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=983b1a56e303addf58b2728738d6660b29ed1f49",
            "patch": "@@ -919,6 +919,9 @@ message DebugOptions {\n   // ragged-all-to-all operations.\n   optional bool xla_gpu_unsupported_use_ragged_all_to_all_one_shot_kernel = 375;\n \n+  // Use embeded device library in codegen\n+  optional bool xla_gpu_use_embeded_device_lib = 420;\n+\n   // Use lld as a library for the linking step\n   optional bool xla_gpu_use_inprocess_lld = 389;\n \n@@ -1355,7 +1358,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 420\n+  // Next id: 421\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 532,
        "additions": 452,
        "deletions": 80
    }
}