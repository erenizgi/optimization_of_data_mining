{
    "author": "akuegel",
    "message": "[XLA:GPU] Delete dead autotuning code.\n\nThe calling code has been removed previously.\n\nPiperOrigin-RevId: 810315225",
    "sha": "d12f301c39c0652d81f909910c57ae2db9e9dd0b",
    "files": [
        {
            "sha": "3d61f55ac9425d831c3336512e5e2e28ed43e61b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d12f301c39c0652d81f909910c57ae2db9e9dd0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d12f301c39c0652d81f909910c57ae2db9e9dd0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=d12f301c39c0652d81f909910c57ae2db9e9dd0b",
            "patch": "@@ -131,7 +131,6 @@ cc_library(\n         \"//xla/service/gpu/autotuning:redzone_buffers\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n-        \"//xla/service/gpu/transforms:priority_fusion\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\","
        },
        {
            "sha": "0bd3993dfd2efcafe184e4061911d6012fc40a93",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d12f301c39c0652d81f909910c57ae2db9e9dd0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d12f301c39c0652d81f909910c57ae2db9e9dd0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc?ref=d12f301c39c0652d81f909910c57ae2db9e9dd0b",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n-#include \"xla/service/gpu/transforms/priority_fusion.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -125,49 +124,6 @@ CublasBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   return configs;\n }\n \n-namespace {\n-HloCostAnalysis::Options PriorityFusionOptions() {\n-  // The real pointer size is set in GpuCompiler. In HloCostAnalysis, the\n-  // pointer size is used only to determine the size of tuple types. We\n-  // shouldn't have any tuples in the autotuned module, so it's safe to use\n-  // the default value here, instead of piping the real value.\n-  HloCostAnalysis::Options options;\n-  options.count_multiple_input_accesses = true;\n-  return options;\n-}\n-}  // namespace\n-\n-absl::StatusOr<std::unique_ptr<HloModule>> RewriteToCublasCustomCall(\n-    std::unique_ptr<HloModule> hlo_module,\n-    const se::DeviceDescription& gpu_device_info) {\n-  HloInstruction* dot = hlo_query::GetFirstInstructionWithOpcode(\n-      *hlo_module->entry_computation(), HloOpcode::kDot);\n-  // Substitute algorithms, which are not supported by cuBLAS for the check, but\n-  // don't use cuBlas in the end. This assumes that the substituting algorithm\n-  // has result which are close enough for the check in this file.\n-  if (dot->precision_config().algorithm() ==\n-      PrecisionConfig::ALG_DOT_TF32_TF32_F32_X3) {\n-    dot->mutable_precision_config()->set_algorithm(\n-        PrecisionConfig::ALG_DOT_F32_F32_F32);\n-  }\n-\n-  for (GemmRewriterOptions::DType dtype :\n-       {GemmRewriterOptions::DType::kFp8Only,\n-        GemmRewriterOptions::DType::kNonFp8Only}) {\n-    GemmRewriter gemm_rewriter(gpu_device_info.cuda_compute_capability(),\n-                               gpu_device_info.runtime_version(),\n-                               GemmRewriterOptions{dtype});\n-    DotAlgorithmRewriter dot_algorithm_rewriter;\n-    PriorityFusion fusion_pass(\n-        /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions());\n-    TF_RETURN_IF_ERROR(dot_algorithm_rewriter.Run(hlo_module.get()).status());\n-    TF_RETURN_IF_ERROR(gemm_rewriter.Run(hlo_module.get()).status());\n-    TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module.get()).status());\n-  }\n-\n-  return hlo_module;\n-}\n-\n absl::StatusOr<std::unique_ptr<BackendConfig>> CublasBackend::GetDefaultConfig(\n     const HloInstruction& instr) {\n   if (!IsLegacyCublasMatmul(instr)) {"
        }
    ],
    "stats": {
        "total": 45,
        "additions": 0,
        "deletions": 45
    }
}