{
    "author": "ezhulenev",
    "message": "[xla:cpu:xnn] Delete thread pool heuristic for XNN fusions\n\nPiperOrigin-RevId: 797101864",
    "sha": "d1ae2d07a3f0a5a753a84f504e9f86c82d875f38",
    "files": [
        {
            "sha": "bc1409c1aa70e34294e89c1b9051126198213146",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h?ref=d1ae2d07a3f0a5a753a84f504e9f86c82d875f38",
            "patch": "@@ -57,7 +57,7 @@ class XnnFusionThunk : public Thunk {\n   ~XnnFusionThunk() override;\n \n   struct Options {\n-    // Pass xnn_scheduler_t constructed from the intra-op threadpool to the\n+    // Pass XnnThreadpool constructed from the intra-op threadpool to the\n     // XNNPACK runtime to allow XNNPACK to parallelize the execution.\n     bool use_threadpool = true;\n   };"
        },
        {
            "sha": "70489f092b791c250ca1f944347e874e765fb76d",
            "filename": "third_party/xla/xla/backends/cpu/xnn_support.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc?ref=d1ae2d07a3f0a5a753a84f504e9f86c82d875f38",
            "patch": "@@ -49,51 +49,6 @@ limitations under the License.\n \n namespace xla::cpu {\n \n-// Thresholds for when to use thread pool for XNNPACK fusions for different\n-// HLOs. These numbers picked up randomly and need benchmarks to tune.\n-static constexpr int64_t kDotThreshold = 10 * 1000;\n-static constexpr int64_t kDefaultThreshold = 100 * 1000;\n-\n-static int64_t MaxElementsCount(const Shape& shape) {\n-  int64_t ret = 0;\n-  ShapeUtil::ForEachSubshape(\n-      shape, [&](const Shape& shape, const ShapeIndex& index) {\n-        ret = std::max(ret, ShapeUtil::ElementsIn(shape));\n-      });\n-  return ret;\n-}\n-\n-// We rely on a very simple heuristic to determine if thread pool is beneficial\n-// for XNNPACK fusions. We assume that if the HLO produces a large result (or\n-// has large operands), thread pool will be beneficial for running operation in\n-// parallel. For small operations, thread pool overheads are higher than the\n-// actual computation.\n-static int64_t MaxElementsCount(const HloInstruction* hlo,\n-                                bool include_operands = true) {\n-  int64_t ret = MaxElementsCount(hlo->shape());\n-  if (include_operands) {\n-    for (auto* operand : hlo->operands()) {\n-      ret = std::max(ret, MaxElementsCount(operand->shape()));\n-    }\n-  }\n-  return ret;\n-}\n-\n-bool XnnShouldUseThreadPool(const HloInstruction* hlo) {\n-  switch (hlo->opcode()) {\n-    case HloOpcode::kDot:\n-      return MaxElementsCount(hlo) > kDotThreshold;\n-    default:\n-      return MaxElementsCount(hlo) > kDefaultThreshold;\n-  }\n-}\n-\n-bool XnnShouldUseThreadPool(const HloComputation* computation) {\n-  return absl::c_any_of(\n-      computation->instructions(),\n-      [](const HloInstruction* hlo) { return XnnShouldUseThreadPool(hlo); });\n-}\n-\n bool AreDtypesSupported(const Shape& lhs_shape, const Shape& rhs_shape,\n                         const Shape& out_shape,\n                         const TargetMachineFeatures* cpu_features) {"
        },
        {
            "sha": "2e39e9430e3eb0e28f4723a2f183d92fc33012bb",
            "filename": "third_party/xla/xla/backends/cpu/xnn_support.h",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.h?ref=d1ae2d07a3f0a5a753a84f504e9f86c82d875f38",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/cpu/codegen/target_machine_features.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/shape.h\"\n@@ -31,12 +30,6 @@ namespace xla::cpu {\n \n inline constexpr absl::string_view kXnnFusionKind = \"__xnn_fusion\";\n \n-// Returns true if XNNPACK should use thread pool to execute given HLO\n-// instruction or computation. We rely on simple heuristics to determine if\n-// thread pool is beneficial.\n-bool XnnShouldUseThreadPool(const HloInstruction* hlo);\n-bool XnnShouldUseThreadPool(const HloComputation* computation);\n-\n // Returns true if the dot operation is supported by XNNPACK. Returns an error\n // if the dot operation shape is invalid.\n absl::StatusOr<bool> IsDotSupportedByXnn("
        },
        {
            "sha": "e90da1fd6f939b88cd1fd26ef2f2621e5ce0b273",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1ae2d07a3f0a5a753a84f504e9f86c82d875f38/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=d1ae2d07a3f0a5a753a84f504e9f86c82d875f38",
            "patch": "@@ -1169,10 +1169,9 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitDotThunk(\n       }\n \n       if (use_xnn) {\n-        XnnDotThunk::Options options = {XnnShouldUseThreadPool(instruction)};\n         bool capture_rhs = HloPredicateIsOp<HloOpcode::kParameter>(rhs);\n         return ThunkSequence::Of<XnnDotThunk>(\n-            std::move(options), ThunkInfo(instruction), dnums, lhs_slice,\n+            XnnDotThunk::Options{}, ThunkInfo(instruction), dnums, lhs_slice,\n             lhs->shape(), rhs_slice, rhs->shape(), out_slice,\n             instruction->shape(), capture_rhs);\n       } else {\n@@ -1539,9 +1538,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitXnnFusionThunk(\n   // Construct XNNPACK subgraph builder from the fusion computation.\n   TF_ASSIGN_OR_RETURN(auto builder, EmitXnnFusionBuilder(computation));\n \n-  XnnFusionThunk::Options options = {XnnShouldUseThreadPool(computation)};\n   return ThunkSequence::Of<XnnFusionThunk>(\n-      std::move(options), ThunkInfo(instruction), std::move(arguments),\n+      XnnFusionThunk::Options{}, ThunkInfo(instruction), std::move(arguments),\n       std::move(results),\n       [b = std::move(builder)](auto, auto) mutable { return b(); });\n }"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 3,
        "deletions": 57
    }
}