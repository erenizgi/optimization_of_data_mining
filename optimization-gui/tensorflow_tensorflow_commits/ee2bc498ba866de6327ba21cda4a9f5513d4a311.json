{
    "author": "pifon2a",
    "message": "[XLA:GPU] Move Compiler::GpuTargetConfig to target_config.h.\n\nPiperOrigin-RevId: 849693973",
    "sha": "ee2bc498ba866de6327ba21cda4a9f5513d4a311",
    "files": [
        {
            "sha": "cdb70dfb461ccfce601942579159c0fa7cb7df15",
            "filename": "third_party/xla/xla/backends/gpu/target_config/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2FBUILD?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -2,6 +2,8 @@ load(\"@bazel_skylib//:bzl_library.bzl\", \"bzl_library\")\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\"//xla/backends/gpu/target_config:build_defs.bzl\", \"embed_files\")\n+load(\"//xla/tsl:tsl.bzl\", \"if_google\")\n+load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -23,16 +25,23 @@ exports_files(glob([\n embed_files(\n     name = \"embed_gpu_specs\",\n     srcs = glob([\"specs/*.txtpb\"]),\n+    compatible_with = get_compatible_with_portable() + if_google([\"//buildenv/target:libtpu\"]),\n     cpp_namespace = \"xla::gpu\",\n )\n \n cc_library(\n     name = \"target_config\",\n     srcs = [\"target_config.cc\"],\n     hdrs = [\"target_config.h\"],\n+    compatible_with = get_compatible_with_portable() + if_google([\"//buildenv/target:libtpu\"]),\n     deps = [\n         \":embed_gpu_specs\",\n+        \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n+        \"//xla/stream_executor:dnn\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:semantic_version\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -50,6 +59,7 @@ xla_cc_test(\n         \"//xla/tsl/platform:status_matchers\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_protobuf//:protobuf\",\n     ],\n )\n \n@@ -59,7 +69,6 @@ bzl_library(\n     visibility = [\"//visibility:private\"],\n     deps = [\n         \"//xla/tsl:package_groups_bzl\",\n-        \"//xla/tsl:tsl_default_bzl\",\n         \"//xla/tsl/platform:rules_cc_bzl\",\n     ],\n )"
        },
        {
            "sha": "a5d3192bf56c3270fb6f66e38a813eec7f6d0637",
            "filename": "third_party/xla/xla/backends/gpu/target_config/build_defs.bzl",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Fbuild_defs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Fbuild_defs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Fbuild_defs.bzl?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -1,12 +1,11 @@\n \"\"\"Contains embed_files build rule.\"\"\"\n \n load(\"//xla/tsl:package_groups.bzl\", \"DEFAULT_LOAD_VISIBILITY\")\n-load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n \n visibility(DEFAULT_LOAD_VISIBILITY)\n \n-def embed_files(name, srcs, cpp_namespace = \"\", **kwargs):\n+def embed_files(name, srcs, cpp_namespace = \"\", compatible_with = None, **kwargs):\n     \"\"\"Compiles srcs into a cc_library with functions returning embedded file data.\n \n     Example:\n@@ -25,6 +24,7 @@ def embed_files(name, srcs, cpp_namespace = \"\", **kwargs):\n         name: name for the generated cc_library target\n         srcs: files to embed\n         cpp_namespace: If set, the generated code will be wrapped in this namespace\n+        compatible_with: The `compatible_with` attribute to pass to the generated targets.\n         **kwargs: keyword arguments passed onto the generated cc_library() rule.\n     \"\"\"\n \n@@ -101,12 +101,13 @@ def embed_files(name, srcs, cpp_namespace = \"\", **kwargs):\n             namespace_open = namespace_open,\n             namespace_close = namespace_close,\n         ),\n-        compatible_with = get_compatible_with_portable(),\n+        compatible_with = compatible_with,\n     )\n \n     cc_library(\n         name = name,\n         srcs = [name + \".cc\"],\n         hdrs = [name + \".h\"],\n+        compatible_with = compatible_with,\n         **kwargs\n     )"
        },
        {
            "sha": "f52c704f2e74ef7dc740dc5543d77711271997f1",
            "filename": "third_party/xla/xla/backends/gpu/target_config/target_config.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config.cc?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -24,9 +24,15 @@ limitations under the License.\n #include \"google/protobuf/text_format.h\"\n #include \"xla/backends/gpu/target_config/embed_gpu_specs.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/dnn.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n+\n+namespace se = ::stream_executor;\n+\n namespace {\n \n absl::StatusOr<absl::string_view> GetEmbeddedGpuTargetConfigData(\n@@ -76,4 +82,60 @@ absl::StatusOr<stream_executor::GpuTargetConfigProto> GetGpuTargetConfig(\n   return config;\n }\n \n+GpuTargetConfig::GpuTargetConfig(se::StreamExecutor* s)\n+    : device_description(s->GetDeviceDescription()),\n+      platform_name(s->GetPlatform()->Name()),\n+      device_description_str(s->GetDeviceDescription().name()) {\n+  se::dnn::DnnSupport* dnn = s->AsDnn();\n+  if (dnn != nullptr) {\n+    absl::StatusOr<se::dnn::VersionInfo> dnn_version = dnn->GetVersion();\n+    if (dnn_version.ok()) {\n+      dnn_version_info = *dnn_version;\n+    }\n+  }\n+}\n+\n+bool GpuTargetConfig::operator==(const GpuTargetConfig& other) const {\n+  return platform_name == other.platform_name &&\n+         dnn_version_info == other.dnn_version_info &&\n+         device_description_str == other.device_description_str &&\n+         device_description == other.device_description;\n+}\n+\n+absl::StatusOr<GpuTargetConfig> GpuTargetConfig::FromProto(\n+    const se::GpuTargetConfigProto& proto) {\n+  GpuTargetConfig target_config;\n+  TF_ASSIGN_OR_RETURN(\n+      target_config.device_description,\n+      se::DeviceDescription::FromProto(proto.gpu_device_info()));\n+  target_config.platform_name = proto.platform_name();\n+  target_config.dnn_version_info =\n+      se::dnn::VersionInfo(proto.dnn_version_info());\n+  target_config.device_description_str = proto.device_description_str();\n+  se::SemanticVersion runtime_version(proto.runtime_version().major(),\n+                                      proto.runtime_version().minor(),\n+                                      proto.runtime_version().patch());\n+  target_config.device_description.set_runtime_version(runtime_version);\n+  se::SemanticVersion dnn_version(\n+      static_cast<unsigned>(proto.dnn_version_info().major()),\n+      static_cast<unsigned>(proto.dnn_version_info().minor()),\n+      static_cast<unsigned>(proto.dnn_version_info().patch()));\n+  target_config.device_description.set_dnn_version(dnn_version);\n+  return target_config;\n+}\n+\n+se::GpuTargetConfigProto GpuTargetConfig::ToProto() const {\n+  se::GpuTargetConfigProto proto;\n+  *proto.mutable_gpu_device_info() = device_description.ToGpuProto();\n+  proto.set_platform_name(platform_name);\n+  *proto.mutable_dnn_version_info() = dnn_version_info.ToProto();\n+  se::RuntimeVersionProto runtime_version_proto;\n+  runtime_version_proto.set_major(device_description.runtime_version().major());\n+  runtime_version_proto.set_minor(device_description.runtime_version().minor());\n+  runtime_version_proto.set_patch(device_description.runtime_version().patch());\n+  *proto.mutable_runtime_version() = runtime_version_proto;\n+  proto.set_device_description_str(device_description_str);\n+  return proto;\n+}\n+\n }  // namespace xla::gpu"
        },
        {
            "sha": "73d65290a701ee62a37ef6ec48368b9844d72614",
            "filename": "third_party/xla/xla/backends/gpu/target_config/target_config.h",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config.h?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -19,7 +19,10 @@ limitations under the License.\n #include <string>\n \n #include \"absl/status/statusor.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/dnn.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla::gpu {\n \n@@ -37,6 +40,28 @@ enum class GpuModel {\n   V100,\n };\n \n+// Description of a target device for compilation.\n+struct GpuTargetConfig {\n+  explicit GpuTargetConfig(stream_executor::StreamExecutor* s);\n+\n+  static absl::StatusOr<GpuTargetConfig> FromProto(\n+      const stream_executor::GpuTargetConfigProto& proto);\n+\n+  stream_executor::GpuTargetConfigProto ToProto() const;\n+\n+  bool operator==(const GpuTargetConfig& other) const;\n+\n+  std::string ToString() { return ToProto().DebugString(); }\n+\n+  stream_executor::DeviceDescription device_description;\n+  std::string platform_name;\n+  stream_executor::dnn::VersionInfo dnn_version_info;\n+  std::string device_description_str;\n+\n+ private:\n+  GpuTargetConfig() = default;\n+};\n+\n // Returns the GpuTargetConfigProto for the given GPU model.\n absl::StatusOr<stream_executor::GpuTargetConfigProto> GetGpuTargetConfig(\n     GpuModel gpu_model);"
        },
        {
            "sha": "8ff48534585b158e864ad6ab0d0de7a0f2687665",
            "filename": "third_party/xla/xla/backends/gpu/target_config/target_config_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Ftarget_config%2Ftarget_config_test.cc?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"google/protobuf/text_format.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n \n@@ -72,5 +73,22 @@ INSTANTIATE_TEST_SUITE_P(\n     [](const ::testing::TestParamInfo<GetGpuTargetConfigTest::ParamType>&\n            info) { return info.param.test_name; });\n \n+TEST(TargetConfigTest, CompareEqualFromSameProto) {\n+  stream_executor::GpuTargetConfigProto config_proto;\n+  ASSERT_TRUE(google::protobuf::TextFormat::ParseFromString(\n+      R\"pb(\n+        platform_name: \"platform\"\n+        dnn_version_info { major: 2 }\n+        runtime_version { major: 12 }\n+        gpu_device_info { threads_per_block_limit: 5 }\n+        device_description_str: \"foo\"\n+      )pb\",\n+      &config_proto));\n+\n+  ASSERT_OK_AND_ASSIGN(auto config1, GpuTargetConfig::FromProto(config_proto));\n+  ASSERT_OK_AND_ASSIGN(auto config2, GpuTargetConfig::FromProto(config_proto));\n+  EXPECT_THAT(config1, ::testing::Eq(config2));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "d789f7ae1ac8a337827e0eba08ba156458bb036d",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -183,7 +183,7 @@ GetTargetConfigForDevices(absl::Span<PjRtDevice* const> devices) {\n         tensorflow::down_cast<const PjRtStreamExecutorDevice*>(device)\n             ->local_device_state();\n     if (local_device_state != nullptr) {\n-      return xla::Compiler::GpuTargetConfig(local_device_state->executor())\n+      return xla::gpu::GpuTargetConfig(local_device_state->executor())\n           .ToProto();\n     }\n   }"
        },
        {
            "sha": "e5f810eca7beab8d9c54294504797fd4e48d98d8",
            "filename": "third_party/xla/xla/pjrt/pjrt_executable.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -112,7 +112,7 @@ struct CompileOptions {\n       std::vector<std::pair<std::string, OptionOverride>>;\n   EnvironmentOptionOverrides env_option_overrides;\n \n-  std::optional<xla::Compiler::GpuTargetConfig> gpu_target_config;\n+  std::optional<xla::gpu::GpuTargetConfig> gpu_target_config;\n \n   // Allow to modify the input MLIR / XLA program.\n   // This is used to run passes on the MLIR parameter without having to clone it"
        },
        {
            "sha": "80ac4f304fe159789218b08edfc8829eff4db80a",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -1618,16 +1618,13 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/backends/cpu:target_machine_options\",\n+        \"//xla/backends/gpu/target_config\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/stream_executor:device_address_allocator\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:platform\",\n-        \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "de4bdb3f43d3c7dffd2294c5e628e9f6d92c3508",
            "filename": "third_party/xla/xla/service/compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 61,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n \n #include <functional>\n #include <memory>\n-#include <string>\n #include <utility>\n #include <vector>\n \n@@ -29,74 +28,14 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/service/metrics_hook_interface.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/platform.h\"\n-#include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n \n /* static */ absl::Mutex Compiler::platform_compiler_mutex_(absl::kConstInit);\n \n-Compiler::GpuTargetConfig::GpuTargetConfig(se::StreamExecutor* s)\n-    : device_description(s->GetDeviceDescription()),\n-      platform_name(s->GetPlatform()->Name()),\n-      device_description_str(s->GetDeviceDescription().name()) {\n-  se::dnn::DnnSupport* dnn = s->AsDnn();\n-  if (dnn != nullptr) {\n-    absl::StatusOr<se::dnn::VersionInfo> dnn_version = dnn->GetVersion();\n-    if (dnn_version.ok()) {\n-      dnn_version_info = *dnn_version;\n-    }\n-  }\n-}\n-\n-absl::StatusOr<Compiler::GpuTargetConfig> Compiler::GpuTargetConfig::FromProto(\n-    const se::GpuTargetConfigProto& proto) {\n-  GpuTargetConfig target_config;\n-  TF_ASSIGN_OR_RETURN(\n-      target_config.device_description,\n-      stream_executor::DeviceDescription::FromProto(proto.gpu_device_info()));\n-  target_config.platform_name = proto.platform_name();\n-  target_config.dnn_version_info =\n-      se::dnn::VersionInfo(proto.dnn_version_info());\n-  target_config.device_description_str = proto.device_description_str();\n-  se::SemanticVersion runtime_version(proto.runtime_version().major(),\n-                                      proto.runtime_version().minor(),\n-                                      proto.runtime_version().patch());\n-  target_config.device_description.set_runtime_version(runtime_version);\n-  se::SemanticVersion dnn_version(\n-      static_cast<unsigned>(proto.dnn_version_info().major()),\n-      static_cast<unsigned>(proto.dnn_version_info().minor()),\n-      static_cast<unsigned>(proto.dnn_version_info().patch()));\n-  target_config.device_description.set_dnn_version(dnn_version);\n-  return target_config;\n-}\n-\n-se::GpuTargetConfigProto Compiler::GpuTargetConfig::ToProto() const {\n-  stream_executor::GpuTargetConfigProto proto;\n-  *proto.mutable_gpu_device_info() = device_description.ToGpuProto();\n-  proto.set_platform_name(platform_name);\n-  *proto.mutable_dnn_version_info() = dnn_version_info.ToProto();\n-  se::RuntimeVersionProto runtime_version_proto;\n-  runtime_version_proto.set_major(device_description.runtime_version().major());\n-  runtime_version_proto.set_minor(device_description.runtime_version().minor());\n-  runtime_version_proto.set_patch(device_description.runtime_version().patch());\n-  *proto.mutable_runtime_version() = runtime_version_proto;\n-  proto.set_device_description_str(device_description_str);\n-  return proto;\n-}\n-\n-bool Compiler::GpuTargetConfig::operator==(const GpuTargetConfig& other) const {\n-  return platform_name == other.platform_name &&\n-         dnn_version_info == other.dnn_version_info &&\n-         device_description_str == other.device_description_str &&\n-         device_description == other.device_description;\n-}\n-\n std::vector<std::unique_ptr<tsl::protobuf::Message>>\n Compiler::ComputeBackendConfigs(const HloInstruction& hlo,\n                                 se::StreamExecutor* executor) const {"
        },
        {
            "sha": "865237a1eff7fd7cc3c3e719e3d7a127c029fe50",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 5,
            "deletions": 26,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"google/protobuf/message.h\"\n #include \"xla/backends/cpu/target_machine_options.h\"\n+#include \"xla/backends/gpu/target_config/target_config.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -50,8 +51,6 @@ limitations under the License.\n #include \"xla/service/metrics_hook_interface.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_address_allocator.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n@@ -103,26 +102,7 @@ class AotCompilationMetadata {\n class Compiler {\n  public:\n   // Description of a target device for compilation.\n-  struct GpuTargetConfig {\n-    explicit GpuTargetConfig(se::StreamExecutor* s);\n-\n-    static absl::StatusOr<GpuTargetConfig> FromProto(\n-        const se::GpuTargetConfigProto& proto);\n-\n-    se::GpuTargetConfigProto ToProto() const;\n-\n-    bool operator==(const GpuTargetConfig& other) const;\n-\n-    std::string ToString() { return ToProto().DebugString(); }\n-\n-    se::DeviceDescription device_description;\n-    std::string platform_name;\n-    se::dnn::VersionInfo dnn_version_info;\n-    std::string device_description_str;\n-\n-   private:\n-    GpuTargetConfig() = default;\n-  };\n+  using GpuTargetConfig = ::xla::gpu::GpuTargetConfig;\n \n   // Description of a target CPU for compilation.\n   struct CpuTargetConfig {\n@@ -460,11 +440,10 @@ class AotCompilationOptions {\n     sanitize_abilists_dataflow_ = abilists;\n   }\n \n-  const std::optional<Compiler::GpuTargetConfig>& gpu_target_config() const {\n+  const std::optional<gpu::GpuTargetConfig>& gpu_target_config() const {\n     return gpu_target_config_;\n   }\n-  void set_gpu_target_config(\n-      const Compiler::GpuTargetConfig& gpu_target_config) {\n+  void set_gpu_target_config(const gpu::GpuTargetConfig& gpu_target_config) {\n     gpu_target_config_ = gpu_target_config;\n   }\n \n@@ -498,7 +477,7 @@ class AotCompilationOptions {\n   bool sanitize_dataflow_ = false;\n   std::vector<std::string> sanitize_abilists_dataflow_;\n   // Contains target-specific information required by AOT compilation.\n-  std::optional<Compiler::GpuTargetConfig> gpu_target_config_;\n+  std::optional<gpu::GpuTargetConfig> gpu_target_config_;\n   EarlyExitPoint early_exit_point_ = EarlyExitPoint::kNone;\n };\n "
        },
        {
            "sha": "943bf434e96558f58583bb6715ad2e07cbe1387c",
            "filename": "third_party/xla/xla/service/compiler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler_test.cc?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -86,25 +86,5 @@ TEST(TargetConfigTest, ProtoConstructorFillsAllFields) {\n          \"validated!\";\n }\n \n-TEST(TargetConfigTest, CompareEqualFromSameProto) {\n-  stream_executor::GpuTargetConfigProto config_proto;\n-  ASSERT_TRUE(tsl::protobuf::TextFormat::ParseFromString(\n-      R\"pb(\n-        platform_name: \"platform\"\n-        dnn_version_info { major: 2 }\n-        runtime_version { major: 12 }\n-        gpu_device_info { threads_per_block_limit: 5 }\n-        device_description_str: \"foo\"\n-      )pb\",\n-      &config_proto));\n-\n-  ASSERT_OK_AND_ASSIGN(auto config1,\n-                       Compiler::GpuTargetConfig::FromProto(config_proto));\n-  ASSERT_OK_AND_ASSIGN(auto config2,\n-                       Compiler::GpuTargetConfig::FromProto(config_proto));\n-\n-  EXPECT_THAT(config1, ::testing::Eq(config2));\n-}\n-\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "c546696f7549364a8d6d2efbf3ae59d4c1d84e03",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -1615,6 +1615,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/backends/gpu/runtime:thunk_proto_cc\",\n+        \"//xla/backends/gpu/target_config\",\n         \"//xla/core/host_offloading:hlo_host_device_type_call_wrapper\",\n         \"//xla/core/host_offloading:host_compute_asyncifier\",\n         \"//xla/hlo/analysis:alias_info\","
        },
        {
            "sha": "01d37fd0a07d0415f1888a29810aec73c3a28c14",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -68,6 +68,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/backends/gpu/target_config/target_config.h\"\n #include \"xla/core/host_offloading/hlo_host_device_type_call_wrapper.h\"\n #include \"xla/core/host_offloading/host_compute_asyncifier.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n@@ -369,7 +370,7 @@ MaybeOwningThreadPool CreateMaybeOwningThreadPool(\n \n DeviceOrDevicelessConfig GetDeviceConfig(\n     se::StreamExecutor* stream_exec, const GpuCompiler::CompileOptions& options,\n-    const Compiler::GpuTargetConfig& gpu_target_config) {\n+    const GpuTargetConfig& gpu_target_config) {\n   if (stream_exec) {\n     return DeviceOrDevicelessConfig{\n         DeviceConfig{stream_exec, options.device_allocator}};\n@@ -474,7 +475,7 @@ absl::Status RunPreSPMDPartitionerPasses(HloModule* hlo_module) {\n }\n \n absl::Status RunSPMDPasses(\n-    HloModule* hlo_module, const Compiler::GpuTargetConfig& gpu_target_config,\n+    HloModule* hlo_module, const GpuTargetConfig& gpu_target_config,\n     const AliasInfo* alias_info,\n     const AlgebraicSimplifierOptions& layout_insensitive_algsimp_opts,\n     int64_t max_windowed_einsum_iteration) {\n@@ -559,7 +560,7 @@ bool BackendConfigDeviceTypeIsHost(HloInstruction* instr) {\n }  // namespace\n \n absl::Status RunOptimizationPasses(\n-    HloModule* hlo_module, const Compiler::GpuTargetConfig& gpu_target_config,\n+    HloModule* hlo_module, const GpuTargetConfig& gpu_target_config,\n     const AlgebraicSimplifierOptions& layout_insensitive_algsimp_opts,\n     absl::string_view platform_name, bool enable_sort_rewriter) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n@@ -1121,7 +1122,7 @@ absl::Status RunLayoutAssignmentPasses(\n }\n \n absl::Status RunFusionPasses(HloModule* hlo_module,\n-                             const Compiler::GpuTargetConfig& gpu_target_config,\n+                             const GpuTargetConfig& gpu_target_config,\n                              tsl::thread::ThreadPool* thread_pool,\n                              HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n                              const GpuAliasInfo* alias_info,\n@@ -1251,7 +1252,7 @@ absl::Status RunPostFusionPasses(\n absl::Status RunPostFusionSimplificationPasses(\n     HloModule* hlo_module, const AlgebraicSimplifierOptions& algsimp_options,\n     se::GpuComputeCapability gpu_version,\n-    const Compiler::GpuTargetConfig& gpu_target_config) {\n+    const GpuTargetConfig& gpu_target_config) {\n   HloPassPipeline pipeline(\"post-fusion-simplification-pipeline optimization\");\n   pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n \n@@ -1281,8 +1282,7 @@ absl::Status RunPostFusionSimplificationPasses(\n absl::Status RunPostFusionVerificationPasses(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n     const GpuCompiler::CompileOptions& options,\n-    const Compiler::GpuTargetConfig& gpu_target_config,\n-    mlir::MLIRContext* mlir_context) {\n+    const GpuTargetConfig& gpu_target_config, mlir::MLIRContext* mlir_context) {\n   HloPassPipeline pipeline(\"post-fusion-verification-pipeline optimization\");\n \n   if (hlo_module->config()\n@@ -1935,10 +1935,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n \n // Returns the TargetConfig, either from the module debug options, or from the\n // CompilationOptions, or if both of those are absent, from the attached GPU.\n-/*static*/ absl::StatusOr<Compiler::GpuTargetConfig>\n-GpuCompiler::GetTargetConfig(const Compiler::CompileOptions& options,\n-                             const DebugOptions& debug_opts,\n-                             se::StreamExecutor* executor) {\n+/*static*/ absl::StatusOr<GpuTargetConfig> GpuCompiler::GetTargetConfig(\n+    const Compiler::CompileOptions& options, const DebugOptions& debug_opts,\n+    se::StreamExecutor* executor) {\n   if (options.gpu_target_config.has_value()) {\n     return *options.gpu_target_config;\n   }\n@@ -1954,11 +1953,10 @@ GpuCompiler::GetTargetConfig(const Compiler::CompileOptions& options,\n           \"Failed to parse GpuTargetConfigProto\");\n     }\n \n-    return Compiler::GpuTargetConfig::FromProto(gpu_target_config_proto);\n+    return GpuTargetConfig::FromProto(gpu_target_config_proto);\n   }\n   if (executor) {\n-    Compiler::GpuTargetConfig target_config =\n-        Compiler::GpuTargetConfig{executor};\n+    GpuTargetConfig target_config = GpuTargetConfig{executor};\n     int64_t device_memory_size =\n         target_config.device_description.device_memory_size();\n     // Checking for device_memory_size == -1 is how we detect that we are"
        },
        {
            "sha": "ee862c33826cae83c505e0219c760d14f5476cfb",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2bc498ba866de6327ba21cda4a9f5513d4a311/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=ee2bc498ba866de6327ba21cda4a9f5513d4a311",
            "patch": "@@ -105,7 +105,7 @@ class GpuCompiler : public LLVMCompiler {\n \n   int64_t GetPointerSize() const { return pointer_size_; }\n \n-  static absl::StatusOr<Compiler::GpuTargetConfig> GetTargetConfig(\n+  static absl::StatusOr<GpuTargetConfig> GetTargetConfig(\n       const Compiler::CompileOptions& options, const DebugOptions& debug_opts,\n       se::StreamExecutor* executor);\n \n@@ -173,8 +173,7 @@ class GpuCompiler : public LLVMCompiler {\n       HloPassPipeline* pipeline, const se::GpuComputeCapability& gpu_version,\n       const CompileOptions& options, HloModule* hlo_module,\n       AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n-      se::StreamExecutor* stream_exec,\n-      const Compiler::GpuTargetConfig* target_config) {\n+      se::StreamExecutor* stream_exec, const GpuTargetConfig* target_config) {\n     return absl::OkStatus();\n   }\n \n@@ -193,7 +192,7 @@ class GpuCompiler : public LLVMCompiler {\n       HloPassPipeline* pipeline, HloModule* hlo_module,\n       const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n       stream_executor::StreamExecutor* stream_executor,\n-      const Compiler::GpuTargetConfig* target_config,\n+      const GpuTargetConfig* target_config,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n     return absl::OkStatus();\n   }"
        }
    ],
    "stats": {
        "total": 278,
        "additions": 143,
        "deletions": 135
    }
}