{
    "author": "WillFroom",
    "message": "[XLA][XTile] Create initial shared tiled dialect + ops.\n\nPiperOrigin-RevId: 819273024",
    "sha": "263a774f70a2e964570451f0614f70e8ab441095",
    "files": [
        {
            "sha": "ba55e927201244e1236b2e34b426ee5d9bc6923e",
            "filename": "third_party/xla/xla/codegen/tools/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -19,6 +19,7 @@ xla_cc_binary(\n         \"//xla/codegen/emitters/ir/tests:__subpackages__\",\n         \"//xla/codegen/emitters/tests:__pkg__\",\n         \"//xla/codegen/emitters/transforms/tests:__subpackages__\",\n+        \"//xla/codegen/xtile/ir/tests:__pkg__\",\n     ],\n     deps = [\n         \"//xla/backends/cpu/codegen/emitters/ir:xla_cpu\",\n@@ -29,6 +30,7 @@ xla_cc_binary(\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/emitters/transforms:pass_pipelines\",\n         \"//xla/codegen/emitters/transforms:passes\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/mlir_hlo\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"@llvm-project//llvm:Support\","
        },
        {
            "sha": "a2ad738faf4a11c387541d84f72142511ed685f9",
            "filename": "third_party/xla/xla/codegen/tools/emitters_opt.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -31,6 +31,8 @@ limitations under the License.\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Pass/PassOptions.h\"\n+#include \"mlir/Pass/PassRegistry.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n #include \"mlir/Transforms/Passes.h\"\n@@ -39,22 +41,24 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n #include \"xla/backends/gpu/codegen/emitters/ir/xla_gpu_ops.h\"\n #include \"xla/backends/gpu/codegen/emitters/transforms/passes.h\"\n-#include \"xla/codegen/emitters/ir/xla_ops.h\"\n+#include \"xla/codegen/emitters/ir/xla_dialect.h\"\n #include \"xla/codegen/emitters/transforms/pass_pipelines.h\"\n #include \"xla/codegen/emitters/transforms/passes.h\"\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n \n int main(int argc, char** argv) {\n   mlir::DialectRegistry registry;\n-  registry.insert<\n-      mlir::DLTIDialect, mlir::LLVM::LLVMDialect, mlir::NVVM::NVVMDialect,\n-      mlir::affine::AffineDialect, mlir::arith::ArithDialect,\n-      mlir::complex::ComplexDialect, mlir::func::FuncDialect,\n-      mlir::gpu::GPUDialect, mlir::math::MathDialect, mlir::mhlo::MhloDialect,\n-      mlir::mhlo::MhloDialect, mlir::scf::SCFDialect,\n-      mlir::tensor::TensorDialect, mlir::vector::VectorDialect, xla::XlaDialect,\n-      xla::cpu::XlaCpuDialect, xla::gpu::XlaGpuDialect>();\n+  registry.insert<mlir::DLTIDialect, mlir::LLVM::LLVMDialect,\n+                  mlir::NVVM::NVVMDialect, mlir::affine::AffineDialect,\n+                  mlir::arith::ArithDialect, mlir::complex::ComplexDialect,\n+                  mlir::func::FuncDialect, mlir::gpu::GPUDialect,\n+                  mlir::math::MathDialect, mlir::mhlo::MhloDialect,\n+                  mlir::mhlo::MhloDialect, mlir::scf::SCFDialect,\n+                  mlir::tensor::TensorDialect, mlir::vector::VectorDialect,\n+                  xla::XlaDialect, xla::cpu::XlaCpuDialect,\n+                  xla::gpu::XlaGpuDialect, xla::xtile::XTileDialect>();\n   mlir::func::registerAllExtensions(registry);\n   mlir::LLVM::registerInlinerInterface(registry);\n   mlir::registerCanonicalizerPass();"
        },
        {
            "sha": "b53709b6357d42f47187963d3f9a72bf2ceb5eb7",
            "filename": "third_party/xla/xla/codegen/xtile/ir/BUILD",
            "status": "added",
            "additions": 106,
            "deletions": 0,
            "changes": 106,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,106 @@\n+load(\"@llvm-project//mlir:tblgen.bzl\", \"gentbl_cc_library\", \"td_library\")\n+load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n+load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = [\":friends\"],\n+    licenses = [\"notice\"],\n+)\n+\n+package_group(\n+    name = \"friends\",\n+    includes = [\n+        \"//xla:friends\",\n+    ],\n+)\n+\n+td_library(\n+    name = \"xtile_td_files\",\n+    srcs = glob([\"*.td\"]),\n+    compatible_with = get_compatible_with_portable(),\n+    includes = [\".\"],\n+    deps = [\n+        \"@llvm-project//mlir:BuiltinDialectTdFiles\",\n+        \"@llvm-project//mlir:CallInterfacesTdFiles\",\n+        \"@llvm-project//mlir:ControlFlowInterfacesTdFiles\",\n+        \"@llvm-project//mlir:FunctionInterfacesTdFiles\",\n+        \"@llvm-project//mlir:InferTypeOpInterfaceTdFiles\",\n+        \"@llvm-project//mlir:OpBaseTdFiles\",\n+        \"@llvm-project//mlir:SideEffectInterfacesTdFiles\",\n+    ],\n+)\n+\n+gentbl_cc_library(\n+    name = \"xtile_dialect_inc_gen\",\n+    compatible_with = get_compatible_with_portable(),\n+    strip_include_prefix = \".\",\n+    tbl_outs = {\n+        \"xtile_dialect.h.inc\": [\"-gen-dialect-decls\"],\n+        \"xtile_dialect.cc.inc\": [\"-gen-dialect-defs\"],\n+    },\n+    tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n+    td_file = \"xtile_dialect.td\",\n+    deps = [\":xtile_td_files\"],\n+)\n+\n+gentbl_cc_library(\n+    name = \"xtile_ops_inc_gen\",\n+    compatible_with = get_compatible_with_portable(),\n+    strip_include_prefix = \".\",\n+    tbl_outs = {\n+        \"xtile_ops.h.inc\": [\"-gen-op-decls\"],\n+        \"xtile_ops.cc.inc\": [\"-gen-op-defs\"],\n+    },\n+    tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n+    td_file = \"xtile_ops.td\",\n+    deps = [\":xtile_td_files\"],\n+)\n+\n+gentbl_cc_library(\n+    name = \"xtile_attrs_inc_gen\",\n+    compatible_with = get_compatible_with_portable(),\n+    strip_include_prefix = \".\",\n+    tbl_outs = {\n+        \"xtile_attrs.h.inc\": [\"-gen-attrdef-decls\"],\n+        \"xtile_attrs.cc.inc\": [\"-gen-attrdef-defs\"],\n+    },\n+    tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n+    td_file = \"xtile_attrs.td\",\n+    deps = [\":xtile_td_files\"],\n+)\n+\n+cc_library(\n+    name = \"xtile\",\n+    srcs = [\n+        \"xtile_attrs.cc\",\n+        \"xtile_dialect.cc\",\n+        \"xtile_ops.cc\",\n+    ],\n+    hdrs = [\n+        \"xtile_attrs.h\",\n+        \"xtile_dialect.h\",\n+        \"xtile_ops.h\",\n+    ],\n+    deps = [\n+        \":xtile_attrs_inc_gen\",\n+        \":xtile_dialect_inc_gen\",\n+        \":xtile_ops_inc_gen\",\n+        \"//xla/hlo/analysis:indexing_analysis\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:BytecodeOpInterface\",\n+        \"@llvm-project//mlir:FuncDialect\",\n+        \"@llvm-project//mlir:FunctionInterfaces\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:InferTypeOpInterface\",\n+        \"@llvm-project//mlir:InliningUtils\",\n+        \"@llvm-project//mlir:MemRefDialect\",\n+        \"@llvm-project//mlir:SideEffectInterfaces\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@llvm-project//mlir:TensorDialect\",\n+        \"@stablehlo//:stablehlo_ops\",\n+    ],\n+)"
        },
        {
            "sha": "10228fcc460af821c5685541d937e47bb011a65e",
            "filename": "third_party/xla/xla/codegen/xtile/ir/tests/BUILD",
            "status": "added",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2FBUILD?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,16 @@\n+load(\"//xla:lit.bzl\", \"lit_test_suite\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    licenses = [\"notice\"],\n+)\n+\n+lit_test_suite(\n+    name = \"tests\",\n+    srcs = glob([\"*.mlir\"]),\n+    cfg = \"//xla:lit.cfg.py\",\n+    tools = [\n+        \"//xla/codegen/tools:emitters_opt\",\n+        \"@llvm-project//llvm:FileCheck\",\n+    ],\n+)"
        },
        {
            "sha": "c04c82dc43c87150c014c04f89508b60f01b7e40",
            "filename": "third_party/xla/xla/codegen/xtile/ir/tests/ops.mlir",
            "status": "added",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,101 @@\n+// RUN: emitters_opt %s --split-input-file --verify-roundtrip -verify-diagnostics\n+\n+xtile.entry_func @happy_path(%input: memref<1024x4xf32>, %output: memref<128x1024xf32>, %tile_id: index) {\n+  %tile = xtile.extract %input[%tile_id, %tile_id][10, 1][1, 1] : memref<1024x4xf32> -> tensor<10xf32>\n+  xtile.insert %tile into %output[%tile_id, %tile_id][10, 1][1, 1] : tensor<10xf32> -> memref<128x1024xf32>\n+  xtile.return\n+}\n+\n+// -----\n+\n+xtile.entry_func @with_attributes(\n+  %input: memref<1024xf32> {xla.some_attr = 1},\n+  %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:10, tiles_per_workgroup:5>} {\n+  xtile.return\n+}\n+\n+// -----\n+\n+// expected-error@+1 {{entry function arguments should be of the form (arg: memref..., tile_id: index)}}\n+xtile.entry_func @tile_id_at_start(%tile_id: index, %input: memref<1024xf32>, %output: memref<1024xf32>) {\n+  xtile.return\n+}\n+\n+// -----\n+\n+// expected-error@+1 {{entry function arguments should be of the form (arg: memref..., tile_id: index)}}\n+xtile.entry_func @too_many_tile_ids(%input: memref<1024xf32>, %id0: index, %id1: index) {\n+  xtile.return\n+}\n+\n+// -----\n+\n+func.func @incorrect_full_shape_extract(%arg: memref<1024xf32>) -> tensor<10xf32> {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{full tile shape size: 2 does not match rank of source: 1}}\n+  %tile = xtile.extract %arg[%offset][10, 1][1] : memref<1024xf32> -> tensor<10xf32>\n+  return %tile : tensor<10xf32>\n+}\n+\n+// -----\n+\n+func.func @incorrect_offset_count_extract(%arg: memref<1024xf32>) -> tensor<10xf32> {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{expected 1 offset operands, got 2}}\n+  %tile = xtile.extract %arg[%offset, %offset][10][1] : memref<1024xf32> -> tensor<10xf32>\n+  return %tile : tensor<10xf32>\n+}\n+\n+// -----\n+\n+func.func @incorrect_rank_reduction_extract(%arg: memref<16x1024xf32>) -> tensor<10xf32> {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{full tile shape: [16, 10] does not reduce to result shape: [10]}}\n+  %tile = xtile.extract %arg[%offset, %offset][16, 10][1, 1] : memref<16x1024xf32> -> tensor<10xf32>\n+  return %tile : tensor<10xf32>\n+}\n+\n+// -----\n+\n+func.func @type_mismatch_extract(%arg: memref<1024xf32>) -> tensor<10xf64> {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{result element type: 'f64' does not match element type of source: 'f32'}}\n+  %tile = xtile.extract %arg[%offset][10][1] : memref<1024xf32> -> tensor<10xf64>\n+  return %tile : tensor<10xf64>\n+}\n+\n+// -----\n+\n+func.func @incorrect_full_shape_insert(%src: tensor<24xf32>, %dst: memref<1024xf32>) {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{full tile shape size: 2 does not match rank of destination: 1}}\n+  xtile.insert %src into %dst[%offset][24, 1][1] : tensor<24xf32> -> memref<1024xf32>\n+  return\n+}\n+\n+// -----\n+\n+func.func @incorrect_offset_count_insert(%src: tensor<24xf32>, %dst: memref<1024xf32>) {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{expected 1 offset operands, got 2}}\n+  xtile.insert %src into %dst[%offset, %offset][24][1] : tensor<24xf32> -> memref<1024xf32>\n+  return\n+}\n+\n+// -----\n+\n+func.func @incorrect_rank_reduction_insert(%src: tensor<24xf32>, %dst: memref<16x1024xf32>) {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{full tile shape: [16, 24] does not reduce to source shape: [24]}}\n+  xtile.insert %src into %dst[%offset, %offset][16, 24][1, 1] : tensor<24xf32> -> memref<16x1024xf32>\n+  return\n+}\n+\n+// -----\n+\n+func.func @type_mismatch_insert(%src: tensor<24xf64>, %dst: memref<1024xf32>) {\n+  %offset = arith.constant 0 : index\n+  // expected-error@+1 {{destination element type: 'f32' does not match element type of source: 'f64'}}\n+  xtile.insert %src into %dst[%offset][24][1] : tensor<24xf64> -> memref<1024xf32>\n+  return\n+}"
        },
        {
            "sha": "0bd88b941ea1cc504589f3fc8eefd5b8d8931006",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_attrs.cc",
            "status": "added",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.cc?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,16 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/codegen/xtile/ir/xtile_attrs.h\""
        },
        {
            "sha": "191fc077142486cd5ef93313e7b39cbc36b107c2",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_attrs.h",
            "status": "added",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.h?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,24 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CODEGEN_XTILE_IR_XTILE_ATTRS_H_\n+#define XLA_CODEGEN_XTILE_IR_XTILE_ATTRS_H_\n+\n+#include \"mlir/IR/Attributes.h\"  // IWYU pragma: keep\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"  // IWYU pragma: keep\n+#define GET_ATTRDEF_CLASSES\n+#include \"xla/codegen/xtile/ir/xtile_attrs.h.inc\"  // IWYU pragma: export\n+\n+#endif  // XLA_CODEGEN_XTILE_IR_XTILE_ATTRS_H_"
        },
        {
            "sha": "511333e1ebbb851c12332c26b550f853f424c3ba",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_attrs.td",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.td?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,38 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CODEGEN_XTILE_IR_XTILE_ATTRS\n+#define XLA_CODEGEN_XTILE_IR_XTILE_ATTRS\n+\n+include \"mlir/IR/AttrTypeBase.td\"\n+include \"mlir/IR/EnumAttr.td\"\n+include \"xla/codegen/xtile/ir/xtile_dialect.td\"\n+\n+class XTile_Attr<string name, list<Trait> traits = []> :\n+      AttrDef<XTileDialect, name, traits> {\n+}\n+\n+def XTile_TilingInfoAttr : XTile_Attr<\"TilingInfo\"> {\n+  let summary = \"Contains auxillary information about how a program is tiled.\";\n+  let mnemonic = \"tiling_info\";\n+  let parameters = (ins \"int32_t\":$tile_count, \"int32_t\":$tiles_per_workgroup);\n+\n+  let assemblyFormat = [{\n+    `<` `tile_count` `:` $tile_count `,`\n+    `tiles_per_workgroup` `:` $tiles_per_workgroup `>`\n+  }];\n+}\n+\n+#endif // XLA_CODEGEN_XTILE_IR_XTILE_ATTRS"
        },
        {
            "sha": "c41a91e8ac9c5406b1c6b355ac6f74ecb7f3f546",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_dialect.cc",
            "status": "added",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.cc?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,72 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"\n+\n+#include \"llvm/ADT/TypeSwitch.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/DialectImplementation.h\"  // IWYU pragma: keep\n+#include \"mlir/Transforms/InliningUtils.h\"\n+#include \"xla/codegen/xtile/ir/xtile_attrs.h\"  // IWYU pragma: keep\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"  // IWYU pragma: keep\n+\n+// Include the auto-generated implementation file.\n+#include \"xla/codegen/xtile/ir/xtile_dialect.cc.inc\"\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"xla/codegen/xtile/ir/xtile_attrs.cc.inc\"\n+\n+namespace xla::xtile {\n+\n+namespace {\n+\n+struct XTileInlinerInterface final : public mlir::DialectInlinerInterface {\n+  using DialectInlinerInterface::DialectInlinerInterface;\n+  // We allow all callables to be inlined.\n+  bool isLegalToInline(mlir::Operation* call, mlir::Operation* callable,\n+                       bool wouldBeCloned) const override {\n+    return true;\n+  }\n+\n+  // We allow any op from the xla dialect to be inlined.\n+  bool isLegalToInline(mlir::Operation* op, mlir::Region* dest,\n+                       bool wouldBeCloned,\n+                       mlir::IRMapping& valueMapping) const override {\n+    return true;\n+  }\n+  // We allow any ops to be inlined into any region.\n+  bool isLegalToInline(mlir::Region* dest, mlir::Region* src,\n+                       bool wouldBeCloned,\n+                       mlir::IRMapping& valueMapping) const override {\n+    return true;\n+  }\n+};\n+\n+}  // namespace\n+\n+void XTileDialect::initialize() {\n+  addOperations<\n+#define GET_OP_LIST\n+#include \"xla/codegen/xtile/ir/xtile_ops.cc.inc\"\n+      >();\n+\n+  addAttributes<\n+#define GET_ATTRDEF_LIST\n+#include \"xla/codegen/xtile/ir/xtile_attrs.cc.inc\"\n+      >();\n+\n+  addInterfaces<XTileInlinerInterface>();\n+}\n+\n+}  // namespace xla::xtile"
        },
        {
            "sha": "1002f5e84d5e2d0a622b54274b8015b1c7a633ea",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_dialect.h",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.h?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,29 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CODEGEN_XTILE_IR_XTILE_DIALECT_H_\n+#define XLA_CODEGEN_XTILE_IR_XTILE_DIALECT_H_\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/Dialect.h\"  // IWYU pragma: keep\n+#include \"stablehlo/dialect/StablehloOps.h\"  // IWYU pragma: keep\n+\n+// Include the auto-generated header file.\n+\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h.inc\"  // IWYU pragma: export\n+\n+#endif  // XLA_CODEGEN_XTILE_IR_XTILE_DIALECT_H_"
        },
        {
            "sha": "409a78493347682ffbb59ec4ea85dd2dc11ad9ca",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_dialect.td",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_dialect.td?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,38 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CODEGEN_XTILE_IR_XTILE_DIALECT\n+#define XLA_CODEGEN_XTILE_IR_XTILE_DIALECT\n+\n+include \"mlir/IR/DialectBase.td\"\n+\n+def XTileDialect : Dialect {\n+  let name = \"xtile\";\n+  let description = [{\n+    This dialect contains ops required for lowering HLO to LLVM.\n+  }];\n+  let cppNamespace = \"::xla::xtile\";\n+\n+  let dependentDialects = [\n+    \"mlir::arith::ArithDialect\",\n+    \"mlir::memref::MemRefDialect\",\n+    \"mlir::tensor::TensorDialect\",\n+    \"mlir::stablehlo::StablehloDialect\",\n+  ];\n+\n+  let useDefaultAttributePrinterParser = 1;\n+}\n+\n+#endif // XLA_CODEGEN_XTILE_IR_XTILE_DIALECT"
        },
        {
            "sha": "e3ad8f8e3d605dadf0fa18521b5605d29ad055fb",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_ops.cc",
            "status": "added",
            "additions": 223,
            "deletions": 0,
            "changes": 223,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.cc?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,223 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+\n+#include <cassert>\n+#include <cstddef>\n+#include <cstdint>\n+#include <optional>\n+#include <string>\n+\n+#include \"absl/log/check.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"llvm/ADT/DenseSet.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/OpImplementation.h\"\n+#include \"mlir/IR/OperationSupport.h\"\n+#include \"mlir/IR/SymbolTable.h\"\n+#include \"mlir/IR/TypeRange.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Interfaces/CallInterfaces.h\"\n+#include \"mlir/Interfaces/FunctionImplementation.h\"\n+#include \"mlir/Support/LLVM.h\"\n+\n+#define GET_OP_CLASSES\n+#include \"xla/codegen/xtile/ir/xtile_ops.cc.inc\"\n+\n+namespace xla::xtile {\n+\n+// This is lifted from the func::FuncOp builder, modified to make the tile\n+// index implicit.\n+void EntryFuncOp::build(mlir::OpBuilder& builder, mlir::OperationState& state,\n+                        mlir::StringRef name,\n+                        mlir::ArrayRef<mlir::Type> memref_arg_types,\n+                        mlir::ArrayRef<mlir::NamedAttribute> attrs,\n+                        mlir::ArrayRef<mlir::DictionaryAttr> memref_arg_attrs) {\n+  state.addAttribute(mlir::SymbolTable::getSymbolAttrName(),\n+                     builder.getStringAttr(name));\n+  mlir::SmallVector<mlir::Type> arg_types(memref_arg_types.begin(),\n+                                          memref_arg_types.end());\n+  // Append the tile id index type.\n+  arg_types.push_back(builder.getIndexType());\n+  mlir::FunctionType function_type = builder.getFunctionType(arg_types,\n+                                                             /*results=*/{});\n+  state.addAttribute(getFunctionTypeAttrName(state.name),\n+                     mlir::TypeAttr::get(function_type));\n+  state.attributes.append(attrs.begin(), attrs.end());\n+  state.addRegion();\n+\n+  if (memref_arg_attrs.empty()) {\n+    return;\n+  }\n+\n+  assert(memref_arg_types.size() == memref_arg_attrs.size());\n+  // As the arg attrs passed relate to the memref arg types we need to also\n+  // append a tile id attr.\n+  llvm::SmallVector<mlir::DictionaryAttr> arg_attrs_with_tile_id(\n+      memref_arg_attrs.begin(), memref_arg_attrs.end());\n+  arg_attrs_with_tile_id.push_back(\n+      mlir::DictionaryAttr::get(builder.getContext(), {}));\n+  mlir::call_interface_impl::addArgAndResultAttrs(\n+      builder, state, arg_attrs_with_tile_id, /*resultAttrs=*/{},\n+      getArgAttrsAttrName(state.name), getResAttrsAttrName(state.name));\n+}\n+\n+mlir::ParseResult EntryFuncOp::parse(mlir::OpAsmParser& parser,\n+                                     mlir::OperationState& result) {\n+  auto buildFuncType =\n+      [](mlir::Builder& builder, mlir::ArrayRef<mlir::Type> argTypes,\n+         mlir::ArrayRef<mlir::Type> results,\n+         mlir::function_interface_impl::VariadicFlag,\n+         std::string&) { return builder.getFunctionType(argTypes, results); };\n+\n+  return mlir::function_interface_impl::parseFunctionOp(\n+      parser, result, /*allowVariadic=*/false,\n+      getFunctionTypeAttrName(result.name), buildFuncType,\n+      getArgAttrsAttrName(result.name), getResAttrsAttrName(result.name));\n+}\n+\n+void EntryFuncOp::print(mlir::OpAsmPrinter& printer) {\n+  mlir::function_interface_impl::printFunctionOp(\n+      printer, *this, /*isVariadic=*/false, getFunctionTypeAttrName(),\n+      getArgAttrsAttrName(), getResAttrsAttrName());\n+}\n+\n+mlir::LogicalResult EntryFuncOp::verify() {\n+  if (!getResultTypes().empty()) {\n+    return emitOpError() << \"entry function should not have any return values\";\n+  }\n+\n+  if (getArgumentTypes().empty()) {\n+    return emitOpError()\n+           << \"entry function must have at least the workgroup id\";\n+  }\n+\n+  // Deciphering the exact user error is non-trivial as they may have the\n+  // arguments in the wrong order or the incorrect number of workgroup ids etc,\n+  // so we just give a generic error message.\n+  constexpr absl::string_view argument_error =\n+      \"entry function arguments should be of the form (arg: memref..., \"\n+      \"tile_id: index)\";\n+\n+  for (mlir::Type arg_types : getArgumentTypes().drop_back()) {\n+    if (!mlir::isa<mlir::MemRefType>(arg_types)) {\n+      return emitOpError() << argument_error;\n+    }\n+  }\n+\n+  if (!mlir::isa<mlir::IndexType>(getArgumentTypes().back())) {\n+    return emitOpError() << argument_error;\n+  }\n+\n+  return mlir::success();\n+}\n+\n+llvm::SmallDenseSet<unsigned> ExtractTileOp::getReducedDimensions() {\n+  std::optional<llvm::SmallDenseSet<unsigned>> mask =\n+      mlir::computeRankReductionMask(getFullTileShape(), getType().getShape());\n+  // This should have already been verified.\n+  CHECK(mask.has_value());\n+  return *mask;\n+}\n+\n+// This is the function ODS expects you to implement\n+mlir::LogicalResult ExtractTileOp::verify() {\n+  mlir::MemRefType source_type = getSource().getType();\n+  int64_t source_rank = source_type.getRank();\n+  mlir::Type source_element_type = source_type.getElementType();\n+\n+  if (getFullTileShape().size() != source_rank) {\n+    return emitOpError() << \"full tile shape size: \"\n+                         << getFullTileShape().size()\n+                         << \" does not match rank of source: \" << source_rank;\n+  }\n+\n+  size_t offset_count = getOffsets().size();\n+  if (offset_count != source_rank) {\n+    return emitOpError() << \"expected \" << source_rank\n+                         << \" offset operands, got \" << offset_count;\n+  }\n+\n+  mlir::RankedTensorType result_type = getType();\n+  if (!mlir::computeRankReductionMask(getFullTileShape(),\n+                                      result_type.getShape())) {\n+    return emitOpError() << \"full tile shape: [\" << getFullTileShape()\n+                         << \"] does not reduce to result shape: [\"\n+                         << result_type.getShape() << \"]\";\n+  }\n+\n+  if (result_type.getElementType() != source_element_type) {\n+    return emitOpError() << \"result element type: \"\n+                         << result_type.getElementType()\n+                         << \" does not match element type of source: \"\n+                         << source_element_type;\n+  }\n+\n+  return mlir::success();\n+}\n+\n+llvm::SmallDenseSet<unsigned> InsertTileOp::getReducedDimensions() {\n+  std::optional<llvm::SmallDenseSet<unsigned>> mask =\n+      mlir::computeRankReductionMask(getFullTileShape(),\n+                                     getSource().getType().getShape());\n+  // This should have already been verified.\n+  CHECK(mask.has_value());\n+  return *mask;\n+}\n+\n+mlir::LogicalResult InsertTileOp::verify() {\n+  mlir::MemRefType destination_type = getDestination().getType();\n+  int64_t destination_rank = destination_type.getRank();\n+\n+  if (getFullTileShape().size() != destination_rank) {\n+    return emitOpError() << \"full tile shape size: \"\n+                         << getFullTileShape().size()\n+                         << \" does not match rank of destination: \"\n+                         << destination_rank;\n+  }\n+\n+  size_t offset_count = getOffsets().size();\n+  if (offset_count != destination_rank) {\n+    return emitOpError() << \"expected \" << destination_rank\n+                         << \" offset operands, got \" << offset_count;\n+  }\n+\n+  mlir::RankedTensorType source_type = getSource().getType();\n+  if (!mlir::computeRankReductionMask(getFullTileShape(),\n+                                      source_type.getShape())) {\n+    return emitOpError() << \"full tile shape: [\" << getFullTileShape()\n+                         << \"] does not reduce to source shape: [\"\n+                         << source_type.getShape() << \"]\";\n+  }\n+\n+  mlir::Type destination_element_type = destination_type.getElementType();\n+  mlir::Type source_element_type = source_type.getElementType();\n+  if (destination_element_type != source_element_type) {\n+    return emitOpError() << \"destination element type: \"\n+                         << destination_element_type\n+                         << \" does not match element type of source: \"\n+                         << source_element_type;\n+  }\n+\n+  return mlir::success();\n+}\n+\n+}  // namespace xla::xtile"
        },
        {
            "sha": "3dc6c2603a56dc9715bb42fa56dc5ec7eebf31f1",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_ops.h",
            "status": "added",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.h?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,42 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CODEGEN_XTILE_IR_XTILE_OPS_H_\n+#define XLA_CODEGEN_XTILE_IR_XTILE_OPS_H_\n+\n+#include <optional>\n+#include <utility>\n+\n+#include \"llvm/ADT/DenseMap.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/Bytecode/BytecodeOpInterface.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/Attributes.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/BuiltinTypes.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/Dialect.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/MLIRContext.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/OpDefinition.h\"  // IWYU pragma: keep\n+#include \"mlir/IR/OpImplementation.h\"  // IWYU pragma: keep\n+#include \"mlir/Interfaces/CallInterfaces.h\"  // IWYU pragma: keep\n+#include \"mlir/Interfaces/InferTypeOpInterface.h\"  // IWYU pragma: keep\n+#include \"mlir/Interfaces/SideEffectInterfaces.h\"  // IWYU pragma: keep\n+#include \"xla/codegen/xtile/ir/xtile_attrs.h\"  // IWYU pragma: keep\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"  // IWYU pragma: keep\n+#include \"xla/hlo/analysis/indexing_map.h\"  // IWYU pragma: keep\n+\n+#define GET_OP_CLASSES\n+#include \"xla/codegen/xtile/ir/xtile_ops.h.inc\"  // IWYU pragma: keep\n+\n+#endif  // XLA_CODEGEN_XTILE_IR_XTILE_OPS_H_"
        },
        {
            "sha": "636806865d2d5e97a9eaa3539ebd15036b8e434d",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_ops.td",
            "status": "added",
            "additions": 169,
            "deletions": 0,
            "changes": 169,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/263a774f70a2e964570451f0614f70e8ab441095/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td?ref=263a774f70a2e964570451f0614f70e8ab441095",
            "patch": "@@ -0,0 +1,169 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CODEGEN_XTILE_IR_XTILE_OPS\n+#define XLA_CODEGEN_XTILE_IR_XTILE_OPS\n+\n+include \"mlir/Interfaces/CallInterfaces.td\"\n+include \"mlir/Interfaces/ControlFlowInterfaces.td\"\n+include \"mlir/Interfaces/FunctionInterfaces.td\"\n+include \"mlir/Interfaces/InferTypeOpInterface.td\"\n+include \"mlir/Interfaces/SideEffectInterfaces.td\"\n+include \"mlir/IR/AttrTypeBase.td\"\n+include \"mlir/IR/OpAsmInterface.td\"\n+include \"mlir/IR/OpBase.td\"\n+include \"mlir/IR/SymbolInterfaces.td\"\n+include \"xla/codegen/xtile/ir/xtile_dialect.td\"\n+include \"xla/codegen/xtile/ir/xtile_attrs.td\"\n+\n+class XTile_Op<string mnemonic, list<Trait> traits = []> :\n+      Op<XTileDialect, mnemonic, traits> {\n+}\n+\n+\n+// Define your custom entry operation\n+def EntryFuncOp : XTile_Op<\"entry_func\", [\n+    Symbol,\n+    IsolatedFromAbove,\n+    FunctionOpInterface]>\n+{\n+  let summary = \"My custom entry function operation\";\n+\n+  let description = [{\n+    This operation defines a custom entry function that is the starting\n+    point for execution. It has a single-block region, takes a\n+    variadic list of memrefs and exactly one tile id index arguments,\n+    it has no return values.\n+  }];\n+\n+  let arguments = (ins SymbolNameAttr:$sym_name,\n+                       TypeAttrOf<FunctionType>:$function_type,\n+                       OptionalAttr<DictArrayAttr>:$arg_attrs,\n+                       OptionalAttr<XTile_TilingInfoAttr>:$tile_info,\n+                       OptionalAttr<DictArrayAttr>:$res_attrs\n+  );\n+\n+  // The entry function has no return values.\n+  let results = (outs);\n+\n+  let regions = (region SizedRegion<1>:$body);\n+\n+  let builders = [OpBuilder<(ins\n+    \"mlir::StringRef\":$name, \"mlir::ArrayRef<mlir::Type>\":$memref_arg_types,\n+    CArg<\"mlir::ArrayRef<mlir::NamedAttribute>\", \"{}\">:$attrs,\n+    CArg<\"mlir::ArrayRef<mlir::DictionaryAttr>\", \"{}\">:$memref_arg_attrs)\n+  >];\n+\n+  let extraClassDeclaration = [{\n+    //===------------------------------------------------------------------===//\n+    // Helper Methods\n+    //===------------------------------------------------------------------===//\n+    mlir::ValueRange getBufferArgs() {\n+      return getBody().getArguments().drop_back();\n+    }\n+\n+    mlir::Value getTileId() {\n+      return getBody().getArguments().back();\n+    }\n+\n+    //===------------------------------------------------------------------===//\n+    // FunctionOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the region on the current operation that is callable.\n+    mlir::Region *getCallableRegion() { return &getBody(); }\n+\n+    /// Returns the argument types of this function.\n+    mlir::ArrayRef<mlir::Type> getArgumentTypes() { return getFunctionType().getInputs(); }\n+\n+    /// Returns the result types of this function.\n+    mlir::ArrayRef<mlir::Type> getResultTypes() { return getFunctionType().getResults(); }\n+  }];\n+\n+  let hasCustomAssemblyFormat = 1;\n+  let hasVerifier = 1;\n+}\n+\n+def EntryFuncReturnOp : XTile_Op<\"return\", [Pure, HasParent<\"EntryFuncOp\">,\n+                                ReturnLike, Terminator]> {\n+  let summary = \"Terminates the entry function\";\n+\n+  let description = [{\n+    This operation terminates the entry function block. It has no operands\n+    and produces no results.\n+  }];\n+\n+  let arguments = (ins);\n+  let results = (outs);\n+\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+def ExtractTileOp : XTile_Op<\"extract\"> {\n+  let summary = \"Extract a tile from a memref.\";\n+  let description = [{\n+  }];\n+\n+  let arguments = (ins\n+    AnyMemRef:$source,\n+    Variadic<Index>:$offsets,\n+    DenseI64ArrayAttr:$full_tile_shape,\n+    DenseI64ArrayAttr:$strides\n+  );\n+\n+\n+  let results = (outs AnyRankedTensor:$result);\n+\n+  let assemblyFormat = [{\n+    $source `[` $offsets `]` $full_tile_shape $strides\n+    `:` type($source) `->` type($result) attr-dict\n+  }];\n+\n+  let extraClassDeclaration = [{\n+    llvm::SmallDenseSet<unsigned> getReducedDimensions();\n+  }];\n+\n+  let hasVerifier = 1;\n+}\n+\n+def InsertTileOp : XTile_Op<\"insert\"> {\n+  let summary = \"Insert a tile into a memref.\";\n+  let description = [{\n+  }];\n+\n+  let arguments = (ins\n+    AnyRankedTensor:$source,\n+    AnyMemRef:$destination,\n+    Variadic<Index>:$offsets,\n+    DenseI64ArrayAttr:$full_tile_shape,\n+    DenseI64ArrayAttr:$strides\n+  );\n+\n+  let results = (outs);\n+\n+  let assemblyFormat = [{\n+    $source `into` $destination `[` $offsets `]` $full_tile_shape $strides\n+    `:` type($source) `->` type($destination) attr-dict\n+  }];\n+\n+  let extraClassDeclaration = [{\n+    llvm::SmallDenseSet<unsigned> getReducedDimensions();\n+  }];\n+\n+  let hasVerifier = 1;\n+}\n+\n+#endif // XLA_CODEGEN_XTILE_IR_XTILE_OPS\n+"
        }
    ],
    "stats": {
        "total": 898,
        "additions": 889,
        "deletions": 9
    }
}