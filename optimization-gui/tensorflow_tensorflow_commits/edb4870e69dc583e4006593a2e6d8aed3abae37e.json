{
    "author": "derdrdirk",
    "message": "[Autotuner] Add MIOpen backend. The backend only returns a default algorithm.\n\nPiperOrigin-RevId: 828911283",
    "sha": "edb4870e69dc583e4006593a2e6d8aed3abae37e",
    "files": [
        {
            "sha": "01c8dbb0bd23bb6867c8b8bc9ad0e692a501f2ca",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 57,
            "deletions": 0,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=edb4870e69dc583e4006593a2e6d8aed3abae37e",
            "patch": "@@ -847,6 +847,30 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"miopen\",\n+    srcs = [\"miopen.cc\"],\n+    hdrs = [\"miopen.h\"],\n+    deps = [\n+        \":gpu_codegen_backend\",\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:cublas_cudnn\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/protobuf:dnn_proto_cc\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"legacy_cache_test\",\n     srcs = [\"legacy_cache_test.cc\"],\n@@ -920,3 +944,36 @@ xla_cc_binary(\n         \"//xla/stream_executor/rocm:all_runtime\",\n     ]),\n )\n+\n+xla_test(\n+    name = \"miopen_test\",\n+    srcs = [\"miopen_test.cc\"],\n+    backends = [\"gpu\"],\n+    tags = [\n+        \"rocm-only\",\n+    ],\n+    deps = [\n+        \":miopen\",\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/service/gpu:amdgpu_compiler\",\n+        \"//xla/service/gpu:amdgpu_compiler_impl\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/stream_executor:device_description_proto_cc\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/rocm:rocm_platform_id\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:status_matchers\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/protobuf:dnn_proto_cc\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)"
        },
        {
            "sha": "c4e0872244c09288dde3a85dbfeb6e5480eae082",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/miopen.cc",
            "status": "added",
            "additions": 157,
            "deletions": 0,
            "changes": 157,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.cc?ref=edb4870e69dc583e4006593a2e6d8aed3abae37e",
            "patch": "@@ -0,0 +1,157 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/miopen.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/protobuf/dnn.pb.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+using MIOpenBackendConfig = stream_executor::dnn::AlgorithmProto;\n+\n+namespace {\n+\n+// Replaces the instruction with a new instruction with the same name in the\n+// parent computation. The given instruction will be replaced by a tuple of the\n+// convolution result and the workspace size. A few following instructions will\n+// be added to the parent computation to extract the convolution result from the\n+// new tuple.\n+absl::Status ApplyConfigAndUpdateWorkspaceInOutputTuple(\n+    HloInstruction& instr, const MIOpenBackendConfig& config) {\n+  HloComputation* computation = instr.parent();\n+  std::vector<Shape> new_call_element_shapes;\n+  // Add the shapes of the outputs of the convolution.\n+  new_call_element_shapes.reserve(instr.shape().tuple_shapes().size() - 1);\n+  for (int i = 0; i < instr.shape().tuple_shapes().size() - 1; ++i) {\n+    new_call_element_shapes.emplace_back(instr.shape().tuple_shapes(i));\n+  }\n+  // The final element is the size of the workspace.\n+  int64_t workspace_size = config.workspace_size().value();\n+  new_call_element_shapes.emplace_back(\n+      ShapeUtil::MakeShape(U8, {workspace_size}));\n+  Shape new_call_shape = ShapeUtil::MakeTupleShape(new_call_element_shapes);\n+  HloInstruction* new_call = computation->AddInstruction(\n+      instr.CloneWithNewOperands(new_call_shape, instr.operands()));\n+  new_call->SetAndSanitizeName(instr.name());\n+\n+  TF_ASSIGN_OR_RETURN(GpuBackendConfig gpu_backend_config,\n+                      instr.backend_config<GpuBackendConfig>());\n+  CudnnConvBackendConfig* cudnn_conv_config =\n+      gpu_backend_config.mutable_cudnn_conv_backend_config();\n+  *cudnn_conv_config->mutable_algorithm() = config;\n+  TF_RETURN_IF_ERROR(new_call->set_backend_config(gpu_backend_config));\n+\n+  std::vector<HloInstruction*> new_tuple_elements;\n+  new_tuple_elements.reserve(new_call->shape().tuple_shapes().size() - 1);\n+  for (int i = 0; i < new_call->shape().tuple_shapes().size() - 1; ++i) {\n+    new_tuple_elements.emplace_back(\n+        computation->AddInstruction(HloInstruction::CreateGetTupleElement(\n+            new_call->shape().tuple_shapes(i), new_call, i)));\n+  }\n+  new_tuple_elements.emplace_back(computation->AddInstruction(\n+      HloInstruction::CreateConstant(LiteralUtil::CreateR1<uint8_t>({}))));\n+\n+  // Repackage new_call so it has the same shape as the original call, namely\n+  // (conv_result, u8[0]).\n+  HloInstruction* new_tuple = computation->AddInstruction(\n+      HloInstruction::CreateTuple(new_tuple_elements));\n+\n+  TF_RETURN_IF_ERROR(instr.parent()->ReplaceInstruction(&instr, new_tuple));\n+  return absl::OkStatus();\n+}\n+\n+absl::Status ApplyConfigToMIOpenCustomCall(HloInstruction& instr,\n+                                           const MIOpenBackendConfig& config) {\n+  if (config.has_workspace_size() && config.workspace_size().value() > 0) {\n+    return ApplyConfigAndUpdateWorkspaceInOutputTuple(instr, config);\n+  }\n+  TF_ASSIGN_OR_RETURN(GpuBackendConfig gpu_config,\n+                      instr.backend_config<GpuBackendConfig>());\n+  CudnnConvBackendConfig* cudnn_conv_config =\n+      gpu_config.mutable_cudnn_conv_backend_config();\n+  *cudnn_conv_config->mutable_algorithm() = config;\n+  TF_RETURN_IF_ERROR(instr.set_backend_config(std::move(gpu_config)));\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace\n+\n+bool MIOpenBackend::IsSupported(const HloInstruction& instr) {\n+  return IsCustomCallToDnnConvolution(instr);\n+}\n+\n+absl::StatusOr<std::unique_ptr<BackendConfig>> MIOpenBackend::GetDefaultConfig(\n+    const HloInstruction& instr) {\n+  if (IsSupported(instr)) {\n+    MIOpenBackendConfig config;\n+    config.set_algo_id(-1);\n+    auto any = std::make_unique<google::protobuf::Any>();\n+    any->PackFrom(config);\n+    return any;\n+  }\n+  return absl::InvalidArgumentError(\n+      \"MIOpen backend doesn't support getting a default config for this \"\n+      \"instruction.\");\n+}\n+\n+absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+MIOpenBackend::GetSupportedConfigs(const HloInstruction& instr) {\n+  if (IsSupported(instr)) {\n+    MIOpenBackendConfig config;\n+    config.set_algo_id(-1);\n+    auto any = std::make_unique<google::protobuf::Any>();\n+    any->PackFrom(config);\n+    std::vector<std::unique_ptr<BackendConfig>> configs;\n+    configs.push_back(std::move(any));\n+    return configs;\n+  }\n+  return std::vector<std::unique_ptr<BackendConfig>>();\n+}\n+\n+absl::Status MIOpenBackend::ApplyConfig(HloInstruction& instr,\n+                                        const BackendConfig& config) {\n+  MIOpenBackendConfig algorithm_config;\n+  if (!config.UnpackTo(&algorithm_config)) {\n+    return absl::InvalidArgumentError(\n+        \"Failed to unpack MIOpenBackendConfig from Any.\");\n+  }\n+  if (IsSupported(instr)) {\n+    return ApplyConfigToMIOpenCustomCall(instr, algorithm_config);\n+  }\n+  return absl::InvalidArgumentError(\n+      \"MIOpen backend doesn't support this instruction.\");\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "dd5f31c75cefbae7214e2b65508f7f71049b40d3",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/miopen.h",
            "status": "added",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen.h?ref=edb4870e69dc583e4006593a2e6d8aed3abae37e",
            "patch": "@@ -0,0 +1,59 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_AUTOTUNER_MIOPEN_H_\n+#define XLA_BACKENDS_GPU_AUTOTUNER_MIOPEN_H_\n+\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// A codegen backend for MIOpen.\n+class MIOpenBackend : public GpuCodegenBackend {\n+ public:\n+  explicit MIOpenBackend(stream_executor::StreamExecutor* stream_executor,\n+                         const DebugOptions* debug_options, Compiler* compiler,\n+                         const Compiler::TargetConfig* target_config)\n+      : GpuCodegenBackend(\"MIOpen\", debug_options, compiler, target_config,\n+                          stream_executor) {}\n+\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+  GetSupportedConfigs(const HloInstruction& instr) override;\n+\n+  absl::StatusOr<std::unique_ptr<BackendConfig>> GetDefaultConfig(\n+      const HloInstruction& instr) override;\n+\n+  absl::Status ApplyConfig(HloInstruction& instr,\n+                           const BackendConfig& config) override;\n+\n+ private:\n+  bool IsSupported(const HloInstruction& instr) override;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_AUTOTUNER_MIOPEN_H_"
        },
        {
            "sha": "19b0420753b9fee9c49f9cefd211f075a9e9b799",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/miopen_test.cc",
            "status": "added",
            "additions": 175,
            "deletions": 0,
            "changes": 175,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/edb4870e69dc583e4006593a2e6d8aed3abae37e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fmiopen_test.cc?ref=edb4870e69dc583e4006593a2e6d8aed3abae37e",
            "patch": "@@ -0,0 +1,175 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/miopen.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/statusor.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/amdgpu_compiler.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/rocm/rocm_platform_id.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/status_matchers.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/protobuf/dnn.pb.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+using MIOpenBackendConfig = stream_executor::dnn::AlgorithmProto;\n+\n+using ::testing::SizeIs;\n+using ::tsl::proto_testing::EqualsProto;\n+using ::tsl::testing::IsOkAndHolds;\n+\n+const char kMIOpenCustomCallHlo[] = R\"(\n+  HloModule module\n+\n+  ENTRY %main {\n+    %arg0 = f32[3,56,56,16]{2,1,0,3} parameter(0)\n+    %arg1 = f32[3,3,3,64]{2,1,0,3} parameter(1)\n+    %cudnn-conv = (f32[54,54,16,64]{1,0,3,2}, u8[0]{0})\n+      custom-call(%arg0, %arg1), custom_call_target=\"__cudnn$convForward\",\n+      window={size=3x3},\n+      dim_labels=f01b_i01o->01bf,\n+      backend_config={\n+        \"cudnn_conv_backend_config\":{\n+          \"activation_mode\":\"kNone\",\n+          \"conv_result_scale\":1,\n+          \"side_input_scale\":0,\n+          \"leakyrelu_alpha\":0\n+        },\n+      }\n+    ROOT %get-tuple-element = f32[54,54,16,64]{1,0,3,2} get-tuple-element(%cudnn-conv), index=0\n+  })\";\n+\n+class MIOpenBackendTest : public HloHardwareIndependentTestBase {\n+ protected:\n+  DebugOptions debug_options_;\n+  AMDGPUCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n+  MIOpenBackend backend_;\n+\n+  MIOpenBackendTest()\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(stream_executor_, &debug_options_, &compiler_,\n+                 &target_config_) {}\n+\n+  bool IsRocm() {\n+    return stream_executor_->GetPlatform()->id() == se::rocm::kROCmPlatformId;\n+  }\n+};\n+\n+TEST_F(MIOpenBackendTest, CanCreateMIOpenBackend) {\n+  ASSERT_NE(nullptr, &backend_);\n+}\n+\n+TEST_F(MIOpenBackendTest, GetSupportedConfigsFromMIOpenCustomCall) {\n+  if (!IsRocm()) {\n+    GTEST_SKIP() << \"Skipping test on non-ROCm platform\";\n+  }\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(kMIOpenCustomCallHlo));\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n+      backend_.GetSupportedConfigs(\n+          (*hlo_module->entry_computation()->root_instruction()->operand(0)));\n+  ASSERT_THAT(configs, IsOkAndHolds(SizeIs(1)));\n+  MIOpenBackendConfig algorithm_config;\n+  ASSERT_TRUE((*configs)[0]->UnpackTo(&algorithm_config));\n+  EXPECT_EQ(algorithm_config.algo_id(), -1);\n+}\n+\n+TEST_F(MIOpenBackendTest, GetDefaultConfigFromMIOpenCustomCall) {\n+  if (!IsRocm()) {\n+    GTEST_SKIP() << \"Skipping test on non-ROCm platform\";\n+  }\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(kMIOpenCustomCallHlo));\n+  absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n+      backend_.GetDefaultConfig(\n+          (*hlo_module->entry_computation()->root_instruction()->operand(0)));\n+  TF_ASSERT_OK(config);\n+  MIOpenBackendConfig algorithm_config;\n+  ASSERT_TRUE(config->get()->UnpackTo(&algorithm_config));\n+  EXPECT_EQ(algorithm_config.algo_id(), -1);\n+}\n+\n+TEST_F(MIOpenBackendTest, ApplyConfigToMIOpenCustomCall) {\n+  if (!IsRocm()) {\n+    GTEST_SKIP() << \"Skipping test on non-ROCm platform\";\n+  }\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(kMIOpenCustomCallHlo));\n+  MIOpenBackendConfig config;\n+  config.set_algo_id(1);\n+  HloInstruction* instr =\n+      hlo_module->entry_computation()->root_instruction()->mutable_operand(0);\n+  google::protobuf::Any any;\n+  any.PackFrom(config);\n+  TF_ASSERT_OK(backend_.ApplyConfig(*instr, any));\n+  TF_ASSERT_OK_AND_ASSIGN(GpuBackendConfig gpu_config,\n+                          instr->backend_config<GpuBackendConfig>());\n+  EXPECT_THAT(gpu_config.cudnn_conv_backend_config().algorithm(),\n+              EqualsProto(config));\n+}\n+\n+TEST_F(MIOpenBackendTest, ApplyConfigToMIOpenCustomCallWithWorkspace) {\n+  if (!IsRocm()) {\n+    GTEST_SKIP() << \"Skipping test on non-ROCm platform\";\n+  }\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(kMIOpenCustomCallHlo));\n+  MIOpenBackendConfig config;\n+  config.set_algo_id(1);\n+  config.mutable_workspace_size()->set_value(1024);\n+  HloInstruction* instr =\n+      hlo_module->entry_computation()->root_instruction()->mutable_operand(0);\n+  google::protobuf::Any any;\n+  any.PackFrom(config);\n+  TF_ASSERT_OK(backend_.ApplyConfig(*instr, any));\n+\n+  auto* replaced_instr =\n+      hlo_module->entry_computation()->GetInstructionWithName(\"cudnn-conv\");\n+\n+  TF_ASSERT_OK_AND_ASSIGN(GpuBackendConfig gpu_config,\n+                          replaced_instr->backend_config<GpuBackendConfig>());\n+  EXPECT_THAT(gpu_config.cudnn_conv_backend_config().algorithm(),\n+              EqualsProto(config));\n+  EXPECT_EQ(replaced_instr->shape().tuple_shapes(1).dimensions(0), 1024);\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 448,
        "additions": 448,
        "deletions": 0
    }
}