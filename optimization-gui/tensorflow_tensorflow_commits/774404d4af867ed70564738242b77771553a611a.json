{
    "author": "nvgrw",
    "message": "Rename num_replicas to num_devices in HloRunnerAgnosticTestBase.\n\nPiperOrigin-RevId: 843023763",
    "sha": "774404d4af867ed70564738242b77771553a611a",
    "files": [
        {
            "sha": "71f4c3b69df79997728bf6ca0d33587a2212409b",
            "filename": "third_party/xla/xla/tests/hlo_runner_agnostic_test_base.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 27,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/774404d4af867ed70564738242b77771553a611a/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/774404d4af867ed70564738242b77771553a611a/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.cc?ref=774404d4af867ed70564738242b77771553a611a",
            "patch": "@@ -148,11 +148,10 @@ absl::StatusOr<Literal> HloRunnerAgnosticTestBase::Execute(\n absl::StatusOr<std::vector<Literal>>\n HloRunnerAgnosticTestBase::ExecuteReplicated(\n     std::unique_ptr<HloModule> module,\n-    const absl::Span<const Literal* const> arguments,\n-    const int64_t num_replicas, const bool use_threads,\n-    const bool run_hlo_passes) {\n+    const absl::Span<const Literal* const> arguments, const int64_t num_devices,\n+    const bool use_threads, const bool run_hlo_passes) {\n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_devices = num_replicas;\n+  options.num_devices = num_devices;\n   options.arguments = {arguments.begin(), arguments.end()};\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = use_threads;\n@@ -163,11 +162,11 @@ HloRunnerAgnosticTestBase::ExecuteReplicated(\n absl::StatusOr<std::vector<Literal>>\n HloRunnerAgnosticTestBase::ExecuteReplicated(\n     std::unique_ptr<HloModule> module,\n-    const absl::Span<const Literal* const> arguments,\n-    const int64_t num_replicas, DeviceAssignment* const device_assignment,\n-    const bool run_hlo_passes, const bool use_threads) {\n+    const absl::Span<const Literal* const> arguments, const int64_t num_devices,\n+    DeviceAssignment* const device_assignment, const bool run_hlo_passes,\n+    const bool use_threads) {\n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_devices = num_replicas;\n+  options.num_devices = num_devices;\n   options.arguments = {arguments.begin(), arguments.end()};\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = use_threads;\n@@ -181,10 +180,10 @@ HloRunnerAgnosticTestBase::ExecuteReplicated(\n     absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n     absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n     absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n-    const int64_t num_replicas, const bool run_hlo_passes,\n+    const int64_t num_devices, const bool run_hlo_passes,\n     DeviceAssignment* const device_assignment) {\n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_devices = num_replicas;\n+  options.num_devices = num_devices;\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = true;\n   return test_runner_->ExecuteReplicated(\n@@ -196,11 +195,10 @@ absl::StatusOr<std::vector<Literal>>\n HloRunnerAgnosticTestBase::ExecuteReplicated(\n     std::unique_ptr<HloModule> module,\n     const std::vector<std::vector<Literal*>> arguments,\n-    const int64_t num_replicas, const bool run_hlo_passes,\n+    const int64_t num_devices, const bool run_hlo_passes,\n     DeviceAssignment* const device_assignment) {\n-  CHECK(num_replicas > 0 && \"expect at least one replica\");\n-  CHECK(num_replicas == arguments.size() &&\n-        \"expect arguments for each replica\");\n+  CHECK(num_devices > 0 && \"expected at least one device\");\n+  CHECK(num_devices == arguments.size() && \"expect arguments for each device\");\n   int64_t argument_count = arguments.front().size();\n   TF_RETURN_IF_ERROR(PreprocessModuleForTestRunner(module.get()));\n   TF_ASSIGN_OR_RETURN(\n@@ -213,7 +211,7 @@ HloRunnerAgnosticTestBase::ExecuteReplicated(\n       [&](int64_t replica_idx, int64_t argument_idx) -> const Literal* {\n         return arguments[replica_idx][argument_idx];\n       },\n-      num_replicas, /*run_hlo_passes=*/run_hlo_passes,\n+      num_devices, /*run_hlo_passes=*/run_hlo_passes,\n       /*device_assignment=*/device_assignment);\n }\n \n@@ -313,15 +311,12 @@ HloRunnerAgnosticTestBase::RunAndCompareTwoModulesReplicated(\n     std::unique_ptr<HloModule> module_0, std::unique_ptr<HloModule> module_1,\n     const std::vector<Literal>& fake_arguments, const bool run_hlo_passes,\n     const bool use_threads, const std::optional<ErrorSpec>& error) {\n-  const HloRunnerInterface::ReplicatedExecuteOptions options{\n-      /*num_replicas=*/module_0->config().replica_count(),\n-      /*arguments=*/LiteralUtil::MakePointers(fake_arguments),\n-      /*infeed_values=*/{},\n-      /*infeed_steps=*/-1,\n-      /*outfeed_shape=*/{},\n-      /*outfeed_values=*/nullptr,\n-      /*run_hlo_passes=*/run_hlo_passes,\n-      /*use_threads=*/use_threads};\n+  HloRunnerInterface::ReplicatedExecuteOptions options;\n+  options.num_devices =\n+      module_0->config().replica_count() * module_0->config().num_partitions();\n+  options.arguments = LiteralUtil::MakePointers(fake_arguments);\n+  options.run_hlo_passes = run_hlo_passes;\n+  options.use_threads = use_threads;\n   return RunAndCompareTwoModulesReplicated(std::move(module_0),\n                                            std::move(module_1), options, error);\n }\n@@ -512,9 +507,10 @@ ::testing::AssertionResult HloRunnerAgnosticTestBase::Run(\n \n ::testing::AssertionResult HloRunnerAgnosticTestBase::RunReplicated(\n     const absl::string_view hlo_string, const bool run_hlo_passes,\n-    const int64_t num_replicas, const tsl::protobuf::Message* backend_config) {\n+    const int64_t num_devices, const tsl::protobuf::Message* backend_config) {\n   absl::StatusOr<std::unique_ptr<VerifiedHloModule>> module =\n-      ParseAndReturnVerifiedModule(hlo_string, num_replicas);\n+      ParseAndReturnVerifiedModule(hlo_string, /*num_replicas=*/num_devices,\n+                                   /*num_partitions=*/1);\n   if (!module.ok()) {\n     return ::testing::AssertionFailure()\n            << \"Error while parsing HLO text format: \"\n@@ -540,7 +536,7 @@ ::testing::AssertionResult HloRunnerAgnosticTestBase::RunReplicated(\n   }\n \n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_devices = num_replicas;\n+  options.num_devices = num_devices;\n   options.arguments = {fake_argument_ptrs.begin(), fake_argument_ptrs.end()};\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = true;"
        },
        {
            "sha": "ea3ffce16ff77c008aae78713a4baa0564dc4f65",
            "filename": "third_party/xla/xla/tests/hlo_runner_agnostic_test_base.h",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/774404d4af867ed70564738242b77771553a611a/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/774404d4af867ed70564738242b77771553a611a/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.h?ref=774404d4af867ed70564738242b77771553a611a",
            "patch": "@@ -159,36 +159,36 @@ class HloRunnerAgnosticTestBase : public HloHardwareIndependentTestBase {\n     return test_runner_->CreateExecutable(std::move(module), run_hlo_passes);\n   }\n \n-  // Executes the given module on multiple replicas.\n+  // Executes the given module on multiple devices.\n   //\n   // use_threads indicates whether this replicated computation will be executed\n-  // with a thread-per-replica, vs using an implicitly async call such as\n+  // with a thread-per-device, vs using an implicitly async call such as\n   // Executable::ExecuteOnStreams.\n   absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n       std::unique_ptr<HloModule> module,\n-      absl::Span<const Literal* const> arguments, int64_t num_replicas,\n+      absl::Span<const Literal* const> arguments, int64_t num_devices,\n       bool use_threads, bool run_hlo_passes = false);\n \n   // Same as above, but uses specified device assignment.\n   absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n       std::unique_ptr<HloModule> module,\n-      absl::Span<const Literal* const> arguments, int64_t num_replicas,\n+      absl::Span<const Literal* const> arguments, int64_t num_devices,\n       DeviceAssignment* device_assignment, bool run_hlo_passes,\n       bool use_threads);\n \n-  // Same as above, but allows passing different programs for replicas.\n+  // Same as above, but allows passing different programs for devices.\n   absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n       absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n       absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n       absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n-      int64_t num_replicas, bool run_hlo_passes,\n+      int64_t num_devices, bool run_hlo_passes,\n       DeviceAssignment* device_assignment = nullptr);\n \n   // Convenience function for above. Allows passing different inputs to\n-  // different replicas of the same program.\n+  // different devices of the same program.\n   absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n       std::unique_ptr<HloModule> module,\n-      std::vector<std::vector<Literal*>> arguments, int64_t num_replicas,\n+      std::vector<std::vector<Literal*>> arguments, int64_t num_devices,\n       bool run_hlo_passes, DeviceAssignment* device_assignment = nullptr);\n \n   // Executes an hlo module with fake inputs and checks that the execution is\n@@ -225,8 +225,8 @@ class HloRunnerAgnosticTestBase : public HloHardwareIndependentTestBase {\n       bool use_threads, const std::optional<ErrorSpec>& error);\n \n   // Parses the modules, and executes them based on `run_hlo_passes` and\n-  // `use_threads` flags. The replica count should be mentioned in the module\n-  // itself.\n+  // `use_threads` flags. The replica + partition count should be set in the\n+  // module itself.\n   ::testing::AssertionResult RunAndCompareTwoModulesReplicated(\n       absl::string_view module_0_str, absl::string_view module_1_str,\n       bool run_hlo_passes, bool use_threads,\n@@ -268,10 +268,10 @@ class HloRunnerAgnosticTestBase : public HloHardwareIndependentTestBase {\n       absl::Span<const Literal* const> arguments,\n       const std::optional<ErrorSpec>& error, bool run_hlo_passes = true);\n \n-  // Executes an hlo module with fake inputs on multiple replicas.\n+  // Executes an hlo module with fake inputs on multiple devices.\n   ::testing::AssertionResult RunReplicated(\n       absl::string_view hlo_string, bool run_hlo_passes = true,\n-      int64_t num_replicas = 1,\n+      int64_t num_devices = 1,\n       const tsl::protobuf::Message* backend_config = nullptr);\n \n   // If assert_determinism is true, the assertion will fail unless all runs"
        }
    ],
    "stats": {
        "total": 74,
        "additions": 35,
        "deletions": 39
    }
}