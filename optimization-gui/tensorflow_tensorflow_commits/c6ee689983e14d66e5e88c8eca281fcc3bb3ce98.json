{
    "author": "WillFroom",
    "message": "[XLA:CPU] Create op to extract workgroup id from a call frame.\n\nPiperOrigin-RevId: 811364529",
    "sha": "c6ee689983e14d66e5e88c8eca281fcc3bb3ce98",
    "files": [
        {
            "sha": "0ff3804e171ffe357d2ae6ce1fc2ab947f773325",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/ir/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2FBUILD?ref=c6ee689983e14d66e5e88c8eca281fcc3bb3ce98",
            "patch": "@@ -68,7 +68,10 @@ gentbl_cc_library(\n     },\n     tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n     td_file = \"xla_cpu_ops.td\",\n-    deps = [\":xla_cpu_td_files\"],\n+    deps = [\n+        \":xla_cpu_td_files\",\n+        \"//xla/codegen/emitters/ir:xla_td_files\",\n+    ],\n )\n \n cc_library("
        },
        {
            "sha": "a93446d9528d1ae6db35ce2adb05206af7409667",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/ir/tests/ops.mlir",
            "status": "modified",
            "additions": 7,
            "deletions": 14,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2Ftests%2Fops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2Ftests%2Fops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2Ftests%2Fops.mlir?ref=c6ee689983e14d66e5e88c8eca281fcc3bb3ce98",
            "patch": "@@ -1,27 +1,20 @@\n-// RUN: emitters_opt %s --split-input-file | FileCheck %s\n+// RUN: emitters_opt %s --split-input-file -verify-roundtrip\n \n func.func @load(%arg0: !xla_cpu.call_frame) -> tensor<32x32xf32> {\n   %0 = xla_cpu.load %arg0, 0 : tensor<32x32xf32>\n   return %0 : tensor<32x32xf32>\n }\n \n-// CHECK-LABEL: @load(\n-// CHECK:   %[[ARG0:.+]]: !xla_cpu.call_frame\n-// CHECK: ) -> tensor<32x32xf32> {\n-// CHECK:   %[[LOAD:.+]] = xla_cpu.load %[[ARG0]], 0 : tensor<32x32xf32>\n-// CHECK:   return %[[LOAD]] : tensor<32x32xf32>\n-// CHECK: }\n-\n // -----\n \n func.func @load(%arg0: !xla_cpu.call_frame) -> memref<64x32xf32> {\n   %0 = xla_cpu.load %arg0, 0 : memref<64x32xf32>\n   return %0 : memref<64x32xf32>\n }\n \n-// CHECK-LABEL: @load(\n-// CHECK:   %[[ARG0:.+]]: !xla_cpu.call_frame\n-// CHECK: ) -> memref<64x32xf32> {\n-// CHECK:   %[[LOAD:.+]] = xla_cpu.load %[[ARG0]], 0 : memref<64x32xf32>\n-// CHECK:   return %[[LOAD]] : memref<64x32xf32>\n-// CHECK: }\n+// -----\n+\n+func.func @extract_workgroup_id(%arg0: !xla_cpu.call_frame) -> index {\n+  %0 = xla_cpu.extract_workgroup_id %arg0, x\n+  return %0 : index\n+}"
        },
        {
            "sha": "78591ef6800b508b5f592092ac29946fce7be6a9",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/ir/xla_cpu_ops.td",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2Fxla_cpu_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2Fxla_cpu_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fir%2Fxla_cpu_ops.td?ref=c6ee689983e14d66e5e88c8eca281fcc3bb3ce98",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n include \"mlir/IR/OpBase.td\"\n include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_dialect.td\"\n include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_types.td\"\n+include \"xla/codegen/emitters/ir/xla_attrs.td\"\n \n class XLACPU_Op<string mnemonic, list<Trait> traits = []> :\n       Op<XlaCpuDialect, mnemonic, traits> {\n@@ -51,6 +52,33 @@ def XLACPU_LoadOp : XLACPU_Op<\"load\"> {\n   }];\n }\n \n+//===----------------------------------------------------------------------===//\n+// !xla_cpu.extract_workgroup_id\n+//===----------------------------------------------------------------------===//\n+\n+def XLACPU_ExtractWorkgroupIdOp : XLACPU_Op<\"extract_workgroup_id\"> {\n+  let summary = \"Extracts the workgroup id from the call frame\";\n+\n+  let description = [{\n+    Given a call frame, returns the workgroup id in the dimension provided.\n+\n+    ```mlir\n+    %0 = xla_cpu.extract_workgroup_id %call_frame, x\n+    %1 = xla_cpu.extract_workgroup_id %call_frame, y\n+    %2 = xla_cpu.extract_workgroup_id %call_frame, z\n+    ```\n+  }];\n+\n+  let arguments = (ins XLACPU_CallFrame:$call_frame,\n+                       WorkGroupDimensionAttr:$dimension);\n+\n+  let results = (outs Index);\n+\n+  let assemblyFormat = [{\n+    $call_frame `,` $dimension attr-dict\n+  }];\n+}\n+\n //===----------------------------------------------------------------------===//\n // !xla_cpu.success\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "37233801cf4bfa6c765377d0eab7a273eb380d76",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/tests/lower_to_llvm.mlir",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_to_llvm.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_to_llvm.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_to_llvm.mlir?ref=c6ee689983e14d66e5e88c8eca281fcc3bb3ce98",
            "patch": "@@ -34,6 +34,31 @@ func.func @output_not_error(%arg0: !xla_cpu.call_frame) -> index {\n \n // -----\n \n+func.func @extract_workgroup_id(%call_frame: !xla_cpu.call_frame) -> (index, index, index) {\n+  %id_x = xla_cpu.extract_workgroup_id %call_frame, x\n+  %id_y = xla_cpu.extract_workgroup_id %call_frame, y\n+  %id_z = xla_cpu.extract_workgroup_id %call_frame, z\n+  return %id_x, %id_y, %id_z : index, index, index\n+}\n+\n+// CHECK-LABEL: func.func @extract_workgroup_id(\n+// CHECK-SAME: %[[CALL_FRAME:.+]]: !xla_cpu.call_frame) -> (index, index, index) {\n+// CHECK: %[[CALL_FRAME_PTR:.+]] = builtin.unrealized_conversion_cast %[[CALL_FRAME]] : !xla_cpu.call_frame to !llvm.ptr\n+// CHECK: %[[WORKGROUP_GEP:.+]] = llvm.getelementptr inbounds %[[CALL_FRAME_PTR]][0, 1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<\"XLA_CPU_KernelCallFrame\", (ptr, ptr, i64, ptr)>\n+// CHECK: %[[WORKGROUP_PTR:.+]] = llvm.load %[[WORKGROUP_GEP]] : !llvm.ptr -> !llvm.ptr\n+// CHECK: %[[WORKGROUP_X_GEP:.+]] = llvm.getelementptr inbounds %[[WORKGROUP_PTR]][0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<\"kernel_dim3\", (i64, i64, i64)>\n+// CHECK: %[[WORKGROUP_X:.+]] = llvm.load %[[WORKGROUP_X_GEP]] invariant : !llvm.ptr -> i64\n+// CHECK: %[[WORKGROUP_X_IDX:.+]] = builtin.unrealized_conversion_cast %[[WORKGROUP_X]] : i64 to index\n+// CHECK: %[[WORKGROUP_Y_GEP:.+]] = llvm.getelementptr inbounds %[[WORKGROUP_PTR]][0, 1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<\"kernel_dim3\", (i64, i64, i64)>\n+// CHECK: %[[WORKGROUP_Y:.+]] = llvm.load %[[WORKGROUP_Y_GEP]] invariant : !llvm.ptr -> i64\n+// CHECK: %[[WORKGROUP_Y_IDX:.+]] = builtin.unrealized_conversion_cast %[[WORKGROUP_Y]] : i64 to index\n+// CHECK: %[[WORKGROUP_Z_GEP:.+]] = llvm.getelementptr inbounds %[[WORKGROUP_PTR]][0, 2] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<\"kernel_dim3\", (i64, i64, i64)>\n+// CHECK: %[[WORKGROUP_Z:.+]] = llvm.load %[[WORKGROUP_Z_GEP]] invariant : !llvm.ptr -> i64\n+// CHECK: %[[WORKGROUP_Z_IDX:.+]] = builtin.unrealized_conversion_cast %[[WORKGROUP_Z]] : i64 to index\n+// CHECK: return %[[WORKGROUP_X_IDX]], %[[WORKGROUP_Y_IDX]], %[[WORKGROUP_Z_IDX]] : index, index, index\n+\n+// -----\n+\n func.func private @wrap_entry(\n   %arg0: tensor<2xi32> {llvm.dereferenceable = 8 : index},\n   %arg1: tensor<21x12xi32> {llvm.dereferenceable = 1008 : index})"
        },
        {
            "sha": "f8b86895f3c98f8a70d837ed3d3ed754ef780cd8",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/xla_cpu_rewrite_patterns.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 52,
            "changes": 115,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fxla_cpu_rewrite_patterns.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c6ee689983e14d66e5e88c8eca281fcc3bb3ce98/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fxla_cpu_rewrite_patterns.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fxla_cpu_rewrite_patterns.cc?ref=c6ee689983e14d66e5e88c8eca281fcc3bb3ce98",
            "patch": "@@ -119,6 +119,61 @@ struct LowerLoadOp : public mlir::OpRewritePattern<LoadOp> {\n   }\n };\n \n+struct LowerExtractWorkgroupIdOp\n+    : public mlir::OpRewritePattern<ExtractWorkgroupIdOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      ExtractWorkgroupIdOp op, mlir::PatternRewriter& rewriter) const override {\n+    mlir::MLIRContext* context = rewriter.getContext();\n+    mlir::ImplicitLocOpBuilder builder(op.getLoc(), rewriter);\n+\n+    auto ptr_type = builder.getType<mlir::LLVM::LLVMPointerType>();\n+    auto kernel_call_frame = KernelCallFrameType(context);\n+    auto kernel_dim = KernelDim3Type(context);\n+    auto i64_ty = builder.getI64Type();\n+\n+    // Get a pointer to the `WorkGroupThread` struct.\n+    auto cast = builder\n+                    .create<mlir::UnrealizedConversionCastOp>(ptr_type,\n+                                                              op.getCallFrame())\n+                    .getResult(0);\n+    auto workgroup_gep = builder.create<mlir::LLVM::GEPOp>(\n+        ptr_type, kernel_call_frame, cast,\n+        mlir::ArrayRef<mlir::LLVM::GEPArg>{mlir::LLVM::GEPArg(0),\n+                                           mlir::LLVM::GEPArg(1)},\n+        mlir::LLVM::GEPNoWrapFlags::inbounds);\n+    auto workgroup_ptr =\n+        builder.create<mlir::LLVM::LoadOp>(ptr_type, workgroup_gep);\n+\n+    int32_t workgroup_dim_idx = static_cast<int32_t>(op.getDimension());\n+    auto workgroup_dim_gep = builder.create<mlir::LLVM::GEPOp>(\n+        ptr_type, kernel_dim, workgroup_ptr,\n+        mlir::ArrayRef<mlir::LLVM::GEPArg>{\n+            mlir::LLVM::GEPArg(0), mlir::LLVM::GEPArg(workgroup_dim_idx)},\n+        mlir::LLVM::GEPNoWrapFlags::inbounds);\n+    auto workgroup_dim_load =\n+        builder.create<mlir::LLVM::LoadOp>(i64_ty, workgroup_dim_gep);\n+    workgroup_dim_load.setInvariant(true);\n+\n+    mlir::Value workgroup_dim = workgroup_dim_load.getResult();\n+    auto index_ty = builder.getIntegerType(\n+        mlir::DataLayout::closest(builder.getInsertionBlock()->getParentOp())\n+            .getTypeSizeInBits(mlir::IndexType::get(context)));\n+    if (index_ty != i64_ty) {\n+      workgroup_dim = builder.create<mlir::LLVM::TruncOp>(\n+          index_ty, workgroup_dim, mlir::LLVM::IntegerOverflowFlags::nsw);\n+    }\n+    auto workgroup_dim_cast = builder.create<mlir::UnrealizedConversionCastOp>(\n+        mlir::IndexType::get(context), workgroup_dim);\n+\n+    rewriter.replaceOp(op, workgroup_dim_cast.getResult(0));\n+\n+    return mlir::success();\n+  }\n+};\n+\n struct LowerWorkGroupIdOp : public mlir::OpRewritePattern<WorkGroupIdOp> {\n  public:\n   using OpRewritePattern::OpRewritePattern;\n@@ -253,7 +308,12 @@ class WrapEntryWithCallFrame\n       }\n       call_args.push_back(load);\n     }\n-    call_args.append(GetWorkGroupIds(call_frame_arg, builder));\n+\n+    for (auto workgroup_id : {WorkGroupDimension::x, WorkGroupDimension::y,\n+                              WorkGroupDimension::z}) {\n+      call_args.push_back(builder.create<ExtractWorkgroupIdOp>(\n+          mlir::IndexType::get(context), call_frame_arg, workgroup_id));\n+    }\n \n     // Use func::call here rather than pure call to avoid the entry function\n     // being DCEd.\n@@ -300,56 +360,6 @@ class WrapEntryWithCallFrame\n     }\n   }\n \n-  static llvm::SmallVector<mlir::Value, 3> GetWorkGroupIds(\n-      mlir::Value call_frame, mlir::ImplicitLocOpBuilder& builder) {\n-    mlir::MLIRContext* context = builder.getContext();\n-    auto ptr = builder.getType<mlir::LLVM::LLVMPointerType>();\n-    auto kernel_call_frame = KernelCallFrameType(context);\n-    auto kernel_dim = KernelDim3Type(context);\n-    auto i64_ty = builder.getIntegerType(\n-        mlir::DataLayout::closest(builder.getInsertionBlock()->getParentOp())\n-            .getTypeSizeInBits(builder.getI64Type()));\n-\n-    // Get a pointer to the `WorkGroupThread` struct.\n-    auto cast =\n-        builder.create<mlir::UnrealizedConversionCastOp>(ptr, call_frame)\n-            .getResult(0);\n-    auto workgroup_gep = builder.create<mlir::LLVM::GEPOp>(\n-        ptr, kernel_call_frame, cast,\n-        mlir::ArrayRef<mlir::LLVM::GEPArg>{mlir::LLVM::GEPArg(0),\n-                                           mlir::LLVM::GEPArg(1)},\n-        mlir::LLVM::GEPNoWrapFlags::inbounds);\n-    auto workgroup_ptr = builder.create<mlir::LLVM::LoadOp>(ptr, workgroup_gep);\n-\n-    llvm::SmallVector<mlir::Value, 3> workgroup_ids;\n-    for (int32_t workgroup_dim_idx : {0, 1, 2}) {\n-      auto workgroup_dim_gep = builder.create<mlir::LLVM::GEPOp>(\n-          ptr, kernel_dim, workgroup_ptr,\n-          mlir::ArrayRef<mlir::LLVM::GEPArg>{\n-              mlir::LLVM::GEPArg(0), mlir::LLVM::GEPArg(workgroup_dim_idx)},\n-          mlir::LLVM::GEPNoWrapFlags::inbounds);\n-      auto workgroup_dim_load =\n-          builder.create<mlir::LLVM::LoadOp>(i64_ty, workgroup_dim_gep);\n-      workgroup_dim_load.setInvariant(true);\n-\n-      mlir::Value workgroup_dim = workgroup_dim_load.getResult();\n-      auto index_ty = builder.getIntegerType(\n-          mlir::DataLayout::closest(builder.getInsertionBlock()->getParentOp())\n-              .getTypeSizeInBits(mlir::IndexType::get(context)));\n-      if (index_ty != i64_ty) {\n-        workgroup_dim = builder.create<mlir::LLVM::TruncOp>(\n-            index_ty, workgroup_dim, mlir::LLVM::IntegerOverflowFlags::nsw);\n-      }\n-      auto workgroup_dim_cast =\n-          builder.create<mlir::UnrealizedConversionCastOp>(\n-              mlir::IndexType::get(context), workgroup_dim);\n-\n-      workgroup_ids.push_back(workgroup_dim_cast.getResult(0));\n-    }\n-\n-    return workgroup_ids;\n-  }\n-\n   static void SetKernelFunctionAttributes(mlir::Builder& builder,\n                                           mlir::func::FuncOp& func,\n                                           int32_t vector_width) {\n@@ -376,7 +386,8 @@ class WrapEntryWithCallFrame\n void PopulateXlaCpuConversionPatterns(mlir::RewritePatternSet& patterns,\n                                       int32_t vector_width) {\n   patterns.add<LowerLoadOp, LowerWorkGroupIdOp, LowerSuccessOp,\n-               RewriteFunctionSignatures>(patterns.getContext());\n+               RewriteFunctionSignatures, LowerExtractWorkgroupIdOp>(\n+      patterns.getContext());\n   patterns.add<WrapEntryWithCallFrame>(patterns.getContext(), vector_width);\n }\n "
        }
    ],
    "stats": {
        "total": 194,
        "additions": 127,
        "deletions": 67
    }
}