{
    "author": "sergey-kozub",
    "message": "PR #30847: [XLA:GPU] Support scaled dot in cuDNN fusion compiler\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30847\n\nüìù Summary of Changes\nLower scaled dot HLO op to cuDNN graph in the fusion compiler.\n\nüöÄ Kind of Contribution\n‚ú® New Feature (part of CL chain)\n\nCopybara import of the project:\n\n--\n9978cde77a646d222c4ce063e6088b0f886be9fb by Sergey Kozub <skozub@nvidia.com>:\n\nSupport scaled dot in cuDNN fusion compiler\n\nMerging this change closes #30847\n\nPiperOrigin-RevId: 804879631",
    "sha": "492ed08b337ffb202632f2763afc3dea55416ab9",
    "files": [
        {
            "sha": "fea26c3405e97a37d72e338b3cc29baaf1b5bc15",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/492ed08b337ffb202632f2763afc3dea55416ab9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/492ed08b337ffb202632f2763afc3dea55416ab9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=492ed08b337ffb202632f2763afc3dea55416ab9",
            "patch": "@@ -1127,6 +1127,70 @@ e {\n )\");\n }\n \n+TEST_F(CuDnnFusionFileCheckTest, BlockScaledDotLowering) {\n+  const std::string kHloText = R\"(\n+block_scaled_dot {\n+  %lhs = f8e4m3fn[256,128] parameter(0)\n+  %lhs_scale = f8e8m0fnu[256,4] parameter(1)\n+  %rhs = f8e4m3fn[384,128] parameter(2)\n+  %rhs_scale = f8e8m0fnu[384,4] parameter(3)\n+  ROOT %result = f32[256,384] scaled-dot(%lhs, %lhs_scale, %rhs, %rhs_scale),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY main {\n+  %lhs = f8e4m3fn[256,128] parameter(0)\n+  %lhs_scale = f8e8m0fnu[256,4] parameter(1)\n+  %rhs = f8e4m3fn[384,128] parameter(2)\n+  %rhs_scale = f8e8m0fnu[384,4] parameter(3)\n+  ROOT %result = f32[256,384] fusion(%lhs,%lhs_scale, %rhs, %rhs_scale),\n+      kind=kCustom, calls=block_scaled_dot,\n+      backend_config={\"fusion_backend_config\":{kind:\"__cudnn$fusion\"}}\n+})\";\n+  EXPECT_TRUE(*RunCuDnnFileCheck(kHloText, R\"(\n+CHECK: \"nodes\"\n+CHECK: {\n+CHECK: \"block_size\": [{{[[:space:]]*32[[:space:]]*}}]\n+CHECK: \"X\": \"lhs\"\n+CHECK: \"scale\": \"lhs_scale\"\n+CHECK: \"Y\": \"result_lhs_dq\"\n+CHECK: \"tag\": \"BLOCK_SCALE_DEQUANTIZE\"\n+CHECK: {\n+CHECK: \"block_size\": [{{[[:space:]]*32[[:space:]]*}}]\n+CHECK: \"X\": \"rhs\"\n+CHECK: \"scale\": \"rhs_scale\"\n+CHECK: \"Y\": \"result_rhs_dq\"\n+CHECK: \"tag\": \"BLOCK_SCALE_DEQUANTIZE\"\n+CHECK: {\n+CHECK: \"A\": \"result_lhs_dq\"\n+CHECK: \"B\": \"result_rhs_dq\"\n+CHECK: \"C\": \"result\"\n+CHECK: \"tag\": \"MATMUL\"\n+CHECK: \"tensors\"\n+CHECK: \"lhs\":\n+CHECK: \"dim\": [{{[[:space:]]*1,[[:space:]]*256,[[:space:]]*128[[:space:]]*}}]\n+CHECK: \"stride\": [{{[[:space:]]*1,[[:space:]]*128,[[:space:]]*1[[:space:]]*}}]\n+CHECK: \"lhs_scale\":\n+CHECK: \"dim\": [{{[[:space:]]*1,[[:space:]]*256,[[:space:]]*4[[:space:]]*}}]\n+CHECK: \"reordering_type\": \"F8_128x4\"\n+CHECK: \"stride\": [{{[[:space:]]*1,[[:space:]]*4,[[:space:]]*1[[:space:]]*}}]\n+CHECK: \"result\":\n+CHECK: \"dim\": [{{[[:space:]]*1,[[:space:]]*256,[[:space:]]*384[[:space:]]*}}]\n+CHECK: \"stride\": [{{[[:space:]]*1,[[:space:]]*384,[[:space:]]*1[[:space:]]*}}]\n+CHECK: \"result_lhs_dq\":\n+CHECK: \"is_virtual\": true\n+CHECK: \"result_rhs_dq\":\n+CHECK: \"is_virtual\": true\n+CHECK: \"rhs\":\n+CHECK: \"dim\": [{{[[:space:]]*1,[[:space:]]*128,[[:space:]]*384[[:space:]]*}}]\n+CHECK: \"stride\": [{{[[:space:]]*1,[[:space:]]*1,[[:space:]]*128[[:space:]]*}}]\n+CHECK: \"rhs_scale\":\n+CHECK: \"dim\": [{{[[:space:]]*1,[[:space:]]*4,[[:space:]]*384[[:space:]]*}}]\n+CHECK: \"reordering_type\": \"F8_128x4\"\n+CHECK: \"stride\": [{{[[:space:]]*1,[[:space:]]*1,[[:space:]]*4[[:space:]]*}}]\n+)\"));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "39a3d46edeeb9faf7f7f6f4ad2d2395d6fa12dc5",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fusion_compiler.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 10,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/492ed08b337ffb202632f2763afc3dea55416ab9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/492ed08b337ffb202632f2763afc3dea55416ab9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc?ref=492ed08b337ffb202632f2763afc3dea55416ab9",
            "patch": "@@ -157,6 +157,10 @@ inline std::optional<fe::DataType_t> ToCudnnDataType(const PrimitiveType type) {\n       return t::FP8_E5M2;\n     case PrimitiveType::F8E4M3FN:\n       return t::FP8_E4M3;\n+    case PrimitiveType::F8E8M0FNU:\n+      return t::FP8_E8M0;\n+    case PrimitiveType::F4E2M1FN:\n+      return t::FP4_E2M1;\n     default:\n       return std::nullopt;\n   }\n@@ -179,23 +183,26 @@ inline std::optional<fe::DataType_t> GetComputeDataType(\n // Extracts dimensions and strides from HLO tensors in the format expected by\n // cuDNN.\n class GemmDimensionAdapter {\n-  explicit GemmDimensionAdapter(const HloDotInstruction& dot,\n+  explicit GemmDimensionAdapter(const HloInstruction& dot,\n                                 TritonFusionAnalysis analysis)\n-      : analysis_(std::move(analysis)), dot_(dot) {};\n+      : analysis_(std::move(analysis)), dot_(dot) {}\n \n  public:\n   const TritonFusionAnalysis analysis_;\n \n   static absl::StatusOr<std::optional<GemmDimensionAdapter>> Create(\n       const HloComputation& computation) {\n+    const HloInstruction* maybe_scaled_dot =\n+        hlo_query::GetFirstInstructionWithOpcode(computation,\n+                                                 HloOpcode::kScaledDot);\n     const HloInstruction* maybe_dot =\n         hlo_query::GetFirstInstructionWithOpcode(computation, HloOpcode::kDot);\n-    if (maybe_dot == nullptr) {\n+    if (maybe_scaled_dot == nullptr && maybe_dot == nullptr) {\n       VLOG(3) << \"Not a GEMM fusion.\";\n       return std::nullopt;\n     }\n-    const HloDotInstruction* dot = DynCast<HloDotInstruction>(\n-        hlo_query::GetFirstInstructionWithOpcode(computation, HloOpcode::kDot));\n+    const HloInstruction* dot =\n+        maybe_dot != nullptr ? maybe_dot : maybe_scaled_dot;\n     if (absl::c_any_of(dot->precision_config().operand_precision(),\n                        [](int x) { return x != PrecisionConfig::DEFAULT; })) {\n       VLOG(3) << \"Non-default precision is not supported.\";\n@@ -222,6 +229,7 @@ class GemmDimensionAdapter {\n     int lhs_noncontracting_index = -1;\n     switch (scope) {\n       case TritonFusionAnalysis::Scope::LHS:\n+      case TritonFusionAnalysis::Scope::LHS_SCALE:\n         lhs_noncontracting_index =\n             GetNonContractingDims(dot_.operand(0)->shape(),\n                                   dims.lhs_batch_dimensions(),\n@@ -233,6 +241,7 @@ class GemmDimensionAdapter {\n             lhs_noncontracting_index, dims.lhs_contracting_dimensions(0)};\n         break;\n       case TritonFusionAnalysis::Scope::RHS:\n+      case TritonFusionAnalysis::Scope::RHS_SCALE:\n         dim_indices = {dims.rhs_batch_dimensions().empty()\n                            ? -1\n                            : dims.rhs_batch_dimensions(0),\n@@ -248,9 +257,6 @@ class GemmDimensionAdapter {\n                        lhs_noncontracting_index,\n                        dot_.shape().dimensions_size() - 1};\n         break;\n-      case TritonFusionAnalysis::Scope::LHS_SCALE:\n-      case TritonFusionAnalysis::Scope::RHS_SCALE:\n-        LOG(FATAL) << \"Unsupported scope.\";\n     }\n \n     Result result;\n@@ -350,7 +356,7 @@ class GemmDimensionAdapter {\n \n  private:\n   int64_t lhs_noncontracting_split_ = 1;\n-  const HloDotInstruction& dot_;\n+  const HloInstruction& dot_;\n };\n \n template <PrimitiveType XlaT, typename T>\n@@ -453,8 +459,16 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n     return true;\n   };\n   for (const TritonFusionAnalysis::Scope scope :\n-       {TritonFusionAnalysis::Scope::LHS, TritonFusionAnalysis::Scope::RHS,\n+       {TritonFusionAnalysis::Scope::LHS,\n+        TritonFusionAnalysis::Scope::LHS_SCALE,\n+        TritonFusionAnalysis::Scope::RHS,\n+        TritonFusionAnalysis::Scope::RHS_SCALE,\n         TritonFusionAnalysis::Scope::OUTPUT}) {\n+    if (!adapter->analysis_.is_scaled_dot() &&\n+        (scope == TritonFusionAnalysis::Scope::LHS_SCALE ||\n+         scope == TritonFusionAnalysis::Scope::RHS_SCALE)) {\n+      continue;\n+    }\n     for (const HloInstruction* parameter :\n          adapter->analysis_.ScopeParameters(scope)) {\n       const std::optional<GemmDimensionAdapter::Result> dims =\n@@ -575,6 +589,36 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n           graph.matmul(operand(0), operand(1),\n                        graph::Matmul_attributes().set_compute_data_type(\n                            compute_dtype.value()));\n+    } else if (HloPredicateIsOp<HloOpcode::kScaledDot>(hlo)) {\n+      const auto compute_dtype =\n+          GetComputeDataType(hlo->shape().element_type());\n+      if (!compute_dtype.has_value()) {\n+        return std::nullopt;\n+      }\n+      const auto& dimension_numbers = hlo->dot_dimension_numbers();\n+      std::array<std::shared_ptr<graph::Tensor_attributes>, 2> dot_operands;\n+      for (int i = 0; i < 2; ++i) {\n+        const Shape& input_shape = hlo->operand(i * 2)->shape();\n+        const Shape& scale_shape = hlo->operand(i * 2 + 1)->shape();\n+        int dim = i == 0 ? dimension_numbers.lhs_contracting_dimensions(0)\n+                         : dimension_numbers.rhs_contracting_dimensions(0);\n+        int block_size =\n+            input_shape.dimensions(dim) / scale_shape.dimensions(dim);\n+\n+        auto scale = operand(i * 2 + 1);\n+        scale->set_reordering_type(fe::TensorReordering_t::F8_128x4);\n+        auto dq_attrs = graph::Block_scale_dequantize_attributes()\n+                            .set_block_size(block_size)\n+                            .set_compute_data_type(fe::DataType_t::FLOAT);\n+        dot_operands[i] =\n+            graph.block_scale_dequantize(operand(i * 2), scale, dq_attrs);\n+        dot_operands[i]->set_name(\n+            absl::StrCat(hlo->name(), i == 0 ? \"_lhs\" : \"_rhs\", \"_dq\"));\n+      }\n+      hlo_to_cudnn[hlo] =\n+          graph.matmul(dot_operands[0], dot_operands[1],\n+                       graph::Matmul_attributes().set_compute_data_type(\n+                           compute_dtype.value()));\n     } else {\n       VLOG(3) << \"Unimplemented operation.\";\n       return std::nullopt;"
        }
    ],
    "stats": {
        "total": 128,
        "additions": 118,
        "deletions": 10
    }
}