{
    "author": "vwbaker",
    "message": "Create a NativeEmitter backend for the autotuner. This will allow us to autotune native emitters against other backends such as the BlockLevelEmitter.\n\nPiperOrigin-RevId: 798190268",
    "sha": "ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c",
    "files": [
        {
            "sha": "01bdff0ee007b954f2789ff237d6ed032e6d1acc",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c",
            "patch": "@@ -575,6 +575,56 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"native_emitter\",\n+    srcs = [\"native_emitter.cc\"],\n+    hdrs = [\"native_emitter.h\"],\n+    deps = [\n+        \":gpu_codegen_backend\",\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:nullability\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"native_emitter_test\",\n+    srcs = [\"native_emitter_test.cc\"],\n+    backends = [\n+        \"a100\",\n+        \"h100\",\n+        \"b200\",\n+    ],\n+    tags = [\n+        \"cuda-only\",\n+        \"no_mac\",\n+    ],\n+    deps = [\n+        \":native_emitter\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:nvptx_compiler\",\n+        \"//xla/service/gpu:nvptx_compiler_impl\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"factory_cuda\",\n     srcs = [\"factory_cuda.cc\"],"
        },
        {
            "sha": "68d4ee6392b8057670e3bc234d397749687de629",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.cc",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc?ref=ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c",
            "patch": "@@ -0,0 +1,84 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/native_emitter.h\"\n+\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+bool IsSupported(const HloInstruction& instr) {\n+  return instr.opcode() == HloOpcode::kFusion &&\n+         // TODO: b/440062644 - Support multi-output fusions.\n+         !Cast<HloFusionInstruction>(&instr)->IsMultiOutputFusion();\n+}\n+\n+absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+NativeEmitterBackend::GetSupportedConfigs(const HloInstruction& instr) {\n+  std::vector<std::unique_ptr<BackendConfig>> configs;\n+  if (!IsSupported(instr)) {\n+    return configs;\n+  }\n+  auto config = GetDefaultConfig(instr);\n+  if (config.ok()) {\n+    configs.push_back(std::move(config.value()));\n+  }\n+  return configs;\n+}\n+\n+absl::StatusOr<std::unique_ptr<BackendConfig>>\n+NativeEmitterBackend::GetDefaultConfig(const HloInstruction& instr) {\n+  NativeEmitterBackendConfig config;\n+  auto any = std::make_unique<google::protobuf::Any>();\n+  any->PackFrom(config);\n+  return any;\n+}\n+\n+absl::Status NativeEmitterBackend::ApplyConfig(HloInstruction& instr,\n+                                               const BackendConfig& config) {\n+  NativeEmitterBackendConfig native_emitter_fusion_config;\n+  if (!config.UnpackTo(&native_emitter_fusion_config)) {\n+    return absl::InvalidArgumentError(\n+        \"Invalid backend config type for NativeEmitterBackendConfig.\");\n+  }\n+  auto fusion_instr = Cast<HloFusionInstruction>(&instr);\n+  fusion_instr->set_fusion_kind(HloInstruction::FusionKind::kInput);\n+  TF_ASSIGN_OR_RETURN(GpuBackendConfig gpu_backend_config,\n+                      instr.backend_config<GpuBackendConfig>());\n+  *gpu_backend_config.mutable_native_emitter_backend_config() =\n+      native_emitter_fusion_config;\n+  TF_RETURN_IF_ERROR(fusion_instr->set_backend_config(gpu_backend_config));\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "2431df9196b96a990b6eff83d0fbc0df230183b7",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.h",
            "status": "added",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h?ref=ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c",
            "patch": "@@ -0,0 +1,64 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_AUTOTUNER_NATIVE_EMITTER_H_\n+#define XLA_BACKENDS_GPU_AUTOTUNER_NATIVE_EMITTER_H_\n+\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/base/nullability.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// Codegen backend for XLA's native fusion emitters.\n+//\n+// This backend enables us to autotune XLA's native emitters against other\n+// backends.\n+class NativeEmitterBackend : public GpuCodegenBackend {\n+ public:\n+  explicit NativeEmitterBackend(\n+      stream_executor::StreamExecutor* absl_nonnull stream_executor,\n+      const DebugOptions* absl_nonnull debug_options,\n+      Compiler* absl_nonnull compiler)\n+      : GpuCodegenBackend(\"NativeEmitter\", stream_executor, debug_options,\n+                          compiler) {}\n+\n+  // Returns all supported configurations for the given instruction.\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+  GetSupportedConfigs(const HloInstruction& instr) override;\n+\n+  // Returns a default configuration for the instruction.\n+  absl::StatusOr<std::unique_ptr<BackendConfig>> GetDefaultConfig(\n+      const HloInstruction& instr) override;\n+\n+  // Applies a given fusion config to the instruction.\n+  absl::Status ApplyConfig(HloInstruction& instr,\n+                           const BackendConfig& config) override;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_AUTOTUNER_NATIVE_EMITTER_H_"
        },
        {
            "sha": "79699e4aaab49f0e574d5a5f9938d752454208a7",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "added",
            "additions": 188,
            "deletions": 0,
            "changes": 188,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c",
            "patch": "@@ -0,0 +1,188 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/native_emitter.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/nvptx_compiler.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+const char kReductionFusionHlo[] = R\"(\n+HloModule m\n+\n+%func (lhs: f32[], rhs: f32[]) -> f32[] {\n+  %rhs = f32[] parameter(1)\n+  %lhs = f32[] parameter(0)\n+  ROOT %sum = f32[] add(%lhs, %rhs)\n+}\n+\n+%fused_reduce.clone (param_0: f32[32,4096,2048]) -> f32[32,2048] {\n+  %param_0 = f32[32,4096,2048]{2,1,0} parameter(0)\n+  %c0 = f32[] constant(0)\n+  ROOT %reduce = f32[32,2048]{1,0} reduce(%param_0, %c0), dimensions={1},\n+    to_apply=%func\n+}\n+\n+ENTRY %entry_computation (p0: f32[32,4096,2048]) -> f32[32,2048] {\n+  %p0 = f32[32,4096,2048]{2,1,0} parameter(0)\n+  ROOT %reduce_fusion = f32[32,2048]{1,0} fusion(%p0), kind=kCustom,\n+    calls=%fused_reduce.clone,\n+    backend_config={ \"fusion_backend_config\": {\n+      \"kind\":\"__triton\",\n+      \"block_level_fusion_config\":{\n+        \"num_warps\":\"8\",\"output_tiles\":[{\"sizes\":[\"1\",\"4\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false\n+      }\n+    }}\n+})\";\n+\n+const char kMultiOutputFusionHlo[] = R\"(\n+HloModule m\n+\n+%fused_add_and_sub (p0: f32[32,16], p1: f32[32,16]) -> (f32[32,16], f32[32,16]) {\n+  %p0 = f32[32,16]{1,0} parameter(0)\n+  %p1 = f32[32,16]{1,0} parameter(1)\n+  %add = f32[32,16]{1,0} add(%p0, %p1)\n+  %sub = f32[32,16]{1,0} subtract(%p0, %p1)\n+  ROOT %tuple = (f32[32,16]{1,0}, f32[32,16]{1,0}) tuple(%add, %sub)\n+}\n+\n+ENTRY %entry_computation (p0: f32[32,16], p1: f32[32,16]) -> (f32[32,16], f32[32,16]) {\n+  %p0 = f32[32,16]{1,0} parameter(0)\n+  %p1 = f32[32,16]{1,0} parameter(1)\n+  ROOT %reduce_fusion = (f32[32,16]{1,0}, f32[32,16]{1,0}) fusion(%p0, %p1), kind=kCustom,\n+    calls=%fused_add_and_sub,\n+    backend_config={ \"fusion_backend_config\": {\n+      \"kind\":\"__triton\",\n+      \"block_level_fusion_config\":{\n+        \"num_warps\":\"1\",\"output_tiles\":[{\"sizes\":[\"1\",\"4\"]}],\n+        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false\n+      }\n+    }}\n+})\";\n+\n+class NativeEmitterBackendTest : public HloHardwareIndependentTestBase {\n+ protected:\n+  NativeEmitterBackendTest()\n+      : backend_(PlatformUtil::GetDefaultPlatform()\n+                     .value()\n+                     ->ExecutorForDevice(0)\n+                     .value(),\n+                 &debug_options_, &compiler_) {}\n+\n+  DebugOptions debug_options_;\n+  NVPTXCompiler compiler_;\n+  NativeEmitterBackend backend_;\n+};\n+\n+TEST_F(NativeEmitterBackendTest, GetDefaultConfig) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n+                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n+  auto fusion = reduction_module->entry_computation()->root_instruction();\n+  // Call GetDefaultConfig on the fusion instruction.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n+                          backend_.GetDefaultConfig(*(fusion)));\n+  // Verify the returned config is a native emitter config.\n+  NativeEmitterBackendConfig native_emitter_config;\n+  ASSERT_TRUE(config->UnpackTo(&native_emitter_config));\n+}\n+\n+TEST_F(NativeEmitterBackendTest, GetSupportedConfigs) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n+                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n+  auto fusion = reduction_module->entry_computation()->root_instruction();\n+  // Call GetSupportedConfigs on the fusion instruction.\n+  TF_ASSERT_OK_AND_ASSIGN(std::vector<std::unique_ptr<BackendConfig>> configs,\n+                          backend_.GetSupportedConfigs(*(fusion)));\n+  // There should only be a single config for the native emitter backend.\n+  ASSERT_EQ(configs.size(), 1);\n+  // Verify the returned config is a native emitter config.\n+  NativeEmitterBackendConfig native_emitter_config;\n+  ASSERT_TRUE(configs[0]->UnpackTo(&native_emitter_config));\n+}\n+\n+TEST_F(NativeEmitterBackendTest,\n+       GetSupportedConfigsDoesNotSupportMultiOutputFusions) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kMultiOutputFusionHlo));\n+  auto fusion_instruction = module->entry_computation()->root_instruction();\n+  // Call GetSupportedConfigs on the fusion instruction.\n+  TF_ASSERT_OK_AND_ASSIGN(std::vector<std::unique_ptr<BackendConfig>> configs,\n+                          backend_.GetSupportedConfigs(*(fusion_instruction)));\n+  // GetSupportedConfigs should return an empty vector as it doesn't support\n+  // multi-output fusions.\n+  ASSERT_TRUE(configs.empty());\n+}\n+\n+TEST_F(NativeEmitterBackendTest, ApplyConfig) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n+                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n+  auto fusion = reduction_module->entry_computation()->root_instruction();\n+  // Call ApplyConfig on the fusion instruction.\n+  NativeEmitterBackendConfig native_emitter_config;\n+  BackendConfig config;\n+  config.PackFrom(native_emitter_config);\n+  ASSERT_THAT(backend_.ApplyConfig(*(fusion), config), absl_testing::IsOk());\n+  // Verify the fusion instruction is now a kInput fusion.\n+  ASSERT_EQ(fusion->fusion_kind(), HloInstruction::FusionKind::kInput);\n+  // Verify the fusion instruction has a native emitter backend config.\n+  ASSERT_TRUE(fusion->has_backend_config());\n+  TF_ASSERT_OK_AND_ASSIGN(GpuBackendConfig gpu_backend_config,\n+                          fusion->backend_config<GpuBackendConfig>());\n+  ASSERT_TRUE(gpu_backend_config.has_native_emitter_backend_config());\n+}\n+\n+TEST_F(NativeEmitterBackendTest, ApplyConfigFailsForUnsupportedConfig) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n+                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n+  auto fusion = reduction_module->entry_computation()->root_instruction();\n+  BlockLevelFusionConfig block_level_fusion_config;\n+  BackendConfig config;\n+  config.PackFrom(block_level_fusion_config);\n+  ASSERT_THAT(backend_.ApplyConfig(*(fusion), config),\n+              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+}\n+\n+TEST_F(NativeEmitterBackendTest, CompileForDefaultConfig) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n+                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n+  auto fusion = reduction_module->entry_computation()->root_instruction();\n+  // Call GetDefaultConfig on the fusion instruction.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n+                          backend_.GetDefaultConfig(*(fusion)));\n+  // Attempt to compile the fusion using the retrieved backend config.\n+  auto maybe_executable = backend_.Compile(*fusion, *config);\n+  // Verify that compilation succeeded and returned a valid executable.\n+  EXPECT_THAT(maybe_executable, absl_testing::IsOk());\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "aef6b4c18b222e3496d840ac4ad0ff4d04db64b7",
            "filename": "third_party/xla/xla/service/gpu/backend_configs.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto?ref=ed97e556991da0646d6b3f3d5ea3a6f3f7310f8c",
            "patch": "@@ -341,14 +341,16 @@ message BlockScaledDotBackendConfig {\n   int32 block_size = 1;\n }\n \n+message NativeEmitterBackendConfig {}\n+\n enum DeviceType {\n   DEVICE_TYPE_INVALID = 0;\n   DEVICE_TYPE_DEVICE = 1;\n   DEVICE_TYPE_HOST = 2;\n }\n \n // Generic backend config for XLA:GPU\n-// Next-Id: 15\n+// Next-Id: 16\n message GpuBackendConfig {\n   // Specifies which operation queue the current instruction will run on.\n   // A backend may have multiple operation queues to run instructions\n@@ -380,6 +382,8 @@ message GpuBackendConfig {\n     CustomCallBackendConfig custom_call_backend_config = 11;\n \n     BlockScaledDotBackendConfig block_scaled_dot_backend_config = 13;\n+\n+    NativeEmitterBackendConfig native_emitter_backend_config = 15;\n   }\n \n   // This attribute instructs the latency-hiding scheduler to"
        }
    ],
    "stats": {
        "total": 392,
        "additions": 391,
        "deletions": 1
    }
}