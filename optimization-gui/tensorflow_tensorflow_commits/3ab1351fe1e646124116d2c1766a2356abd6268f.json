{
    "author": "felixwqp",
    "message": "Add comments for `GPUCommunicationType`.\n\nPiperOrigin-RevId: 818705728",
    "sha": "3ab1351fe1e646124116d2c1766a2356abd6268f",
    "files": [
        {
            "sha": "62161101f5d7257665e069511180a5cec99bb0b1",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ab1351fe1e646124116d2c1766a2356abd6268f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ab1351fe1e646124116d2c1766a2356abd6268f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=3ab1351fe1e646124116d2c1766a2356abd6268f",
            "patch": "@@ -25,9 +25,16 @@ namespace xla {\n namespace gpu {\n \n enum class GPUCommunicationType {\n+  // The communication type could not be determined.\n   UNDEFINED = 0,\n+  // Communication involves devices from multiple hosts, and every host\n+  // involved in the communication pattern has all of its devices participating.\n   RAIL_ALIGNED = 1,\n+  // Communication involves devices from multiple hosts, but at least one of\n+  // the involved hosts has only a subset of its devices participating.\n   NON_RAIL_ALIGNED = 2,\n+  // All devices participating in the collective operation reside on the same\n+  // host machine.\n   SINGLE_HOST = 3\n };\n "
        }
    ],
    "stats": {
        "total": 7,
        "additions": 7,
        "deletions": 0
    }
}