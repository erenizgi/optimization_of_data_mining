{
    "author": "WillFroom",
    "message": "[XTile] Move ConvertElementwise0DTensorToScalarPass from triton_xla to xtile.\n\nThe default conversion from stablehlo to linalg leaves behind 0D tensor elementwise ops, so moving this to xtile so I can use it in the CPU lowering.\n\nPiperOrigin-RevId: 831269559",
    "sha": "2209da210b251c3298dc05c67cc8bd6be9c0fd23",
    "files": [
        {
            "sha": "5b3173a3e9b6b72ed0533b444dab9b7043ebe643",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -242,6 +242,7 @@ cc_library(\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n         \"//xla/codegen/tiling:tiled_hlo_schedule\",\n         \"//xla/codegen/xtile/ir:xtile\",\n+        \"//xla/codegen/xtile/ir/transforms:passes\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:hlo_module_config\","
        },
        {
            "sha": "ac4e11b064a94c7873ee55c3f9f6fe1572fb0c9e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -111,6 +111,7 @@ limitations under the License.\n #include \"xla/codegen/tiling/tiled_hlo_fusion_instruction.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/codegen/tiling/tiled_hlo_schedule.h\"\n+#include \"xla/codegen/xtile/ir/transforms/passes.h\"\n #include \"xla/codegen/xtile/ir/xtile_dialect.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n@@ -2256,8 +2257,7 @@ absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n     // The legacy emitter supports 0D tensors so we would get inconsistent\n     // results if we try to rewrite them.\n     if (fusion_kind != kTritonGemmFusionKind) {\n-      pm.addPass(\n-          mlir::triton::xla::CreateTritonXLAConvert0DTensorToScalarPass());\n+      pm.addPass(xtile::createConvertElementwise0DTensorToScalarPass());\n     }\n     pm.addPass(mlir::triton::xla::CreateTensorLowerToTritonPass());\n     pm.addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());"
        },
        {
            "sha": "f4fbaad9ad144031c129f51a47430c851abcf1ec",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -37,7 +37,6 @@ cc_library(\n         \"round_f32_to_tf32_for_tf32_dot_pass.cc\",\n         \"stablehlo_lower_to_triton.cc\",\n         \"tensor_lower_to_triton.cc\",\n-        \"triton_xla_convert_tensor_to_scalar_pass.cc\",\n         \"triton_xla_convert_unsupported_types.cc\",\n         \"triton_xla_extract_insert_to_triton_pass.cc\",\n         \"triton_xla_fold_transpose_pass.cc\","
        },
        {
            "sha": "e9d42fd476d57015c20125d184cf8df7075fee97",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -52,7 +52,6 @@ std::unique_ptr<mlir::Pass> CreateStableHLOLowerToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTensorLowerToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAMathToLibdevicePass(\n     absl::string_view libdevice_path, absl::string_view triple);\n-std::unique_ptr<mlir::Pass> CreateTritonXLAConvert0DTensorToScalarPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "a7b046f55a7df19aa295870c9024346d0c4c9893",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -246,12 +246,4 @@ def TritonXLAMathToLibdevicePass\n   ];\n }\n \n-def TritonXLAConvert0DTensorToScalarPass\n-    : Pass<\"triton-xla-convert-0d-tensor-to-scalar\", \"mlir::ModuleOp\"> {\n-  let summary = \"Lowers 0D tensors to scalars.\";\n-  let dependentDialects = [\n-    \"::xla::xtile::XTileDialect\",\n-  ];\n-}\n-\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "b873d063ed992225151940c17442a9f49c4a34db",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_convert_tensor_to_scalar_pass.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daf4d831954f1ec2cdeb5e352a11ebc79e36d0f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_tensor_to_scalar_pass.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daf4d831954f1ec2cdeb5e352a11ebc79e36d0f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_tensor_to_scalar_pass.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_tensor_to_scalar_pass.mlir?ref=daf4d831954f1ec2cdeb5e352a11ebc79e36d0f9",
            "patch": "@@ -1,29 +0,0 @@\n-// RUN: xla-opt %s -split-input-file -triton-xla-convert-0d-tensor-to-scalar \\\n-// RUN: | FileCheck %s\n-\n-func.func @addf(%arg0: tensor<f32>) -> tensor<f32> {\n-  // CHECK: arith.addf {{.*}} : f32\n-  %0 = arith.addf %arg0, %arg0 : tensor<f32>\n-  return %0 : tensor<f32>\n-}\n-\n-// -----\n-\n-func.func @addf() -> tensor<f32> {\n-  // CHECK: arith.constant 1.000000e+00 : f32\n-  %0 = arith.constant dense<1.0> : tensor<f32>\n-  return %0 : tensor<f32>\n-}\n-\n-// -----\n-\n-func.func @addf(%arg0: tensor<f32>) -> tensor<f32> {\n-  // CHECK: tt.extern_elementwise {{.*}}{libname = \"dev\", libpath = \"/path\",\n-  // CHECK-SAME: pure = true, symbol = \"sym\"} : (f32) -> f32\n-  %0 = tt.extern_elementwise %arg0\n-    {libname = \"dev\",\n-     libpath = \"/path\",\n-     pure = true,\n-     symbol = \"sym\"} : (tensor<f32>) -> tensor<f32> \n-  return %0 : tensor<f32>\n-}"
        },
        {
            "sha": "08926c228e160cbce95f33e1dcfd01f8a905c2c2",
            "filename": "third_party/xla/xla/codegen/xtile/ir/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2FBUILD?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -31,6 +31,7 @@ gentbl_cc_library(\n cc_library(\n     name = \"passes\",\n     srcs = [\n+        \"convert_elementwise_0d_tensor_to_scalar_pass.cc\",\n         \"verify_legal_xtile_ops.cc\",\n     ],\n     hdrs = [\"passes.h\"],\n@@ -39,6 +40,7 @@ cc_library(\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "1b2acb322875bca01cc75e9727e20973a9e1eae9",
            "filename": "third_party/xla/xla/codegen/xtile/ir/transforms/convert_elementwise_0d_tensor_to_scalar_pass.cc",
            "status": "renamed",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Fconvert_elementwise_0d_tensor_to_scalar_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Fconvert_elementwise_0d_tensor_to_scalar_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Fconvert_elementwise_0d_tensor_to_scalar_pass.cc?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -13,30 +13,29 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include <memory>\n #include <optional>\n #include <utility>\n \n+#include \"llvm/ADT/STLExtras.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Location.h\"\n #include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/ValueRange.h\"\n-#include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n-#include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n-#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+#include \"xla/codegen/xtile/ir/transforms/passes.h\"  // IWYU pragma: keep\n \n-namespace mlir::triton::xla {\n+namespace xla::xtile {\n \n-#define GEN_PASS_DEF_TRITONXLACONVERT0DTENSORTOSCALARPASS\n-#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+#define GEN_PASS_DEF_CONVERTELEMENTWISE0DTENSORTOSCALARPASS\n+#include \"xla/codegen/xtile/ir/transforms/passes.h.inc\"\n \n namespace {\n \n@@ -54,10 +53,16 @@ struct ElementwiseConverter\n       return rewriter.notifyMatchFailure(op, \"failed to convert type\");\n     }\n \n-    mlir::Operation* new_op = Operation::create(\n-        op->getLoc(), op->getName(), new_result_types, operands, op->getAttrs(),\n-        op->getPropertiesStorage(), op->getSuccessors(), op->getNumRegions());\n-    rewriter.replaceOp(op, rewriter.insert(new_op));\n+    mlir::IRMapping mapping;\n+    mapping.map(op->getOperands(), operands);\n+    mlir::Operation* new_op = rewriter.clone(*op, mapping);\n+\n+    for (auto [results, new_type] :\n+         llvm::zip(new_op->getResults(), new_result_types)) {\n+      results.setType(new_type);\n+    }\n+\n+    rewriter.replaceOp(op, new_op);\n     return mlir::success();\n   }\n };\n@@ -86,9 +91,9 @@ struct ConstantConversionPattern\n   }\n };\n \n-struct TritonXLAConvert0DTensorToScalarPass\n-    : public impl::TritonXLAConvert0DTensorToScalarPassBase<\n-          TritonXLAConvert0DTensorToScalarPass> {\n+struct ConvertElementwise0DTensorToScalarPass\n+    : public impl::ConvertElementwise0DTensorToScalarPassBase<\n+          ConvertElementwise0DTensorToScalarPass> {\n   void runOnOperation() override {\n     mlir::TypeConverter type_converter;\n     type_converter.addConversion([](mlir::Type type) { return type; });\n@@ -142,14 +147,11 @@ struct TritonXLAConvert0DTensorToScalarPass\n     if (mlir::failed(mlir::applyPartialConversion(getOperation(), target,\n                                                   std::move(patterns)))) {\n       signalPassFailure();\n+      return;\n     }\n   }\n };\n \n }  // namespace\n \n-std::unique_ptr<mlir::Pass> CreateTritonXLAConvert0DTensorToScalarPass() {\n-  return std::make_unique<TritonXLAConvert0DTensorToScalarPass>();\n-}\n-\n-}  // namespace mlir::triton::xla\n+}  // namespace xla::xtile",
            "previous_filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_convert_tensor_to_scalar_pass.cc"
        },
        {
            "sha": "a9a198318571dcaffdb4d3558b4747af28cb8b82",
            "filename": "third_party/xla/xla/codegen/xtile/ir/transforms/passes.td",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Fpasses.td?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -24,3 +24,12 @@ def VerifyLegalXTileOpsPass : Pass<\"xtile-verify-legal-ops\", \"mlir::ModuleOp\"> {\n   }];\n \n }\n+\n+def ConvertElementwise0DTensorToScalarPass\n+    : Pass<\"convert-elementwise-0d-tensor-to-scalar\"> {\n+  let summary = \"Lowers 0D tensors of elementwise ops to scalars.\";\n+\n+  let dependentDialects = [\n+    \"mlir::tensor::TensorDialect\",\n+  ];\n+}"
        },
        {
            "sha": "3892f343766d51c4c41b6ee2f33d9469090fa269",
            "filename": "third_party/xla/xla/codegen/xtile/ir/transforms/tests/convert_elementwise_0d_tensor_to_scalar_pass.mlir",
            "status": "added",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Ftests%2Fconvert_elementwise_0d_tensor_to_scalar_pass.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2209da210b251c3298dc05c67cc8bd6be9c0fd23/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Ftests%2Fconvert_elementwise_0d_tensor_to_scalar_pass.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Ftests%2Fconvert_elementwise_0d_tensor_to_scalar_pass.mlir?ref=2209da210b251c3298dc05c67cc8bd6be9c0fd23",
            "patch": "@@ -0,0 +1,25 @@\n+// RUN: emitters_opt %s \\\n+// RUN: -split-input-file -convert-elementwise-0d-tensor-to-scalar \\\n+// RUN: | FileCheck %s\n+\n+func.func @converts_0d_addf(%arg0: tensor<f32>) -> tensor<f32> {\n+  // CHECK: arith.addf {{.*}} : f32\n+  %0 = arith.addf %arg0, %arg0 : tensor<f32>\n+  return %0 : tensor<f32>\n+}\n+\n+// -----\n+\n+func.func @skips_1d_addf(%arg0: tensor<1xf32>) -> tensor<1xf32> {\n+  // CHECK: arith.addf {{.*}} : tensor<1xf32>\n+  %0 = arith.addf %arg0, %arg0 : tensor<1xf32>\n+  return %0 : tensor<1xf32>\n+}\n+\n+// -----\n+\n+func.func @converts_0d_constant() -> tensor<f32> {\n+  // CHECK: arith.constant 1.000000e+00 : f32\n+  %0 = arith.constant dense<1.0> : tensor<f32>\n+  return %0 : tensor<f32>\n+}"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 60,
        "deletions": 60
    }
}