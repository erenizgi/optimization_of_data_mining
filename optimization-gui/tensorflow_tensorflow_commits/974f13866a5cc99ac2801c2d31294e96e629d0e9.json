{
    "author": "GleasonK",
    "message": "[MHLO Sunset] Delete MHLO->Linalg/Arith passes, users migrated to StableHLO\n\nPiperOrigin-RevId: 814262987",
    "sha": "974f13866a5cc99ac2801c2d31294e96e629d0e9",
    "files": [
        {
            "sha": "0488dd9b3e90dc164767a4812fb0d3a5afc39949",
            "filename": "third_party/xla/xla/mlir_hlo/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 68,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD?ref=974f13866a5cc99ac2801c2d31294e96e629d0e9",
            "patch": "@@ -331,12 +331,10 @@ cc_library(\n         \"mhlo/transforms/chlo_legalize_to_hlo/chlo_legalize_to_hlo_pass.cc\",\n         \"mhlo/transforms/collapse_elementwise_map/collapse_elementwise_map.cc\",\n         \"mhlo/transforms/expand_hlo_tuples/expand_hlo_tuples.cc\",\n-        \"mhlo/transforms/hlo_legalize_to_arithmetic/hlo_legalize_to_arithmetic.cc\",\n         \"mhlo/transforms/hlo_legalize_to_memref/hlo_legalize_to_memref.cc\",\n         \"mhlo/transforms/hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo_pass.cc\",\n         \"mhlo/transforms/legalize_dot_to_dot_general/legalize_dot_to_dot_general.cc\",\n         \"mhlo/transforms/legalize_einsum_to_dot_general/legalize_einsum_to_dot_general.cc\",\n-        \"mhlo/transforms/legalize_to_linalg/legalize_to_linalg.cc\",\n         \"mhlo/transforms/legalize_torch_index_select_to_gather/legalize_torch_index_select_to_gather.cc\",\n         \"mhlo/transforms/legalize_trigonometric_to_approximation/legalize_trigonometric_to_approximation.cc\",\n         \"mhlo/transforms/materialize_broadcasts/materialize_broadcasts.cc\",\n@@ -354,19 +352,14 @@ cc_library(\n         \"mhlo/interfaces/bufferizable_op_interface_impl.h\",\n         \"mhlo/transforms/passes.h\",\n         \"mhlo/transforms/rewriters.h\",\n-        \"mhlo/utils/legalize_to_linalg_utils.h\",\n-        \"mhlo/utils/mhlo_rng_utils.h\",\n     ],\n     strip_include_prefix = \".\",\n     deps = [\n         \":chlo_legalize_to_hlo_inc_gen\",\n         \":hlo_legalize_to_stablehlo\",\n-        \":legalize_to_linalg_utils\",\n         \":map_chlo_to_hlo_op\",\n         \":map_mhlo_to_scalar_op\",\n         \":mhlo_pass_inc_gen\",\n-        \":mhlo_rng_utils\",\n-        \":mhlo_scatter_gather_utils\",\n         \":mlir_hlo\",\n         \":shape_component_analysis\",\n         \":stablehlo_legalize_to_hlo\",\n@@ -503,66 +496,6 @@ cc_library(\n     deps = [\"@llvm-project//llvm:Support\"],\n )\n \n-cc_library(\n-    name = \"legalize_to_linalg_utils\",\n-    srcs = [\"mhlo/utils/legalize_to_linalg_utils.cc\"],\n-    hdrs = [\"mhlo/utils/legalize_to_linalg_utils.h\"],\n-    strip_include_prefix = \".\",\n-    deps = [\n-        \":map_mhlo_to_scalar_op\",\n-        \":mlir_hlo\",\n-        \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:AffineDialect\",\n-        \"@llvm-project//mlir:BufferizationDialect\",\n-        \"@llvm-project//mlir:ComplexDialect\",\n-        \"@llvm-project//mlir:FuncDialect\",\n-        \"@llvm-project//mlir:IR\",\n-        \"@llvm-project//mlir:LinalgDialect\",\n-        \"@llvm-project//mlir:LinalgUtils\",\n-        \"@llvm-project//mlir:MathDialect\",\n-        \"@llvm-project//mlir:ShapeDialect\",\n-        \"@llvm-project//mlir:SparseTensorDialect\",\n-        \"@llvm-project//mlir:Support\",\n-        \"@llvm-project//mlir:TensorDialect\",\n-        \"@llvm-project//mlir:TensorUtils\",\n-        \"@llvm-project//mlir:TransformUtils\",\n-        \"@llvm-project//mlir:Transforms\",\n-        \"@stablehlo//:chlo_ops\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"mhlo_rng_utils\",\n-    srcs = [\"mhlo/utils/mhlo_rng_utils.cc\"],\n-    hdrs = [\"mhlo/utils/mhlo_rng_utils.h\"],\n-    strip_include_prefix = \".\",\n-    deps = [\n-        \":mlir_hlo\",\n-        \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:ArithDialect\",\n-        \"@llvm-project//mlir:DialectUtils\",\n-        \"@llvm-project//mlir:IR\",\n-        \"@llvm-project//mlir:LinalgDialect\",\n-        \"@llvm-project//mlir:MathDialect\",\n-        \"@llvm-project//mlir:Support\",\n-        \"@llvm-project//mlir:TensorDialect\",\n-        \"@llvm-project//mlir:Transforms\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"mhlo_scatter_gather_utils\",\n-    srcs = [\"mhlo/utils/mhlo_scatter_gather_utils.cc\"],\n-    hdrs = [\"mhlo/utils/mhlo_scatter_gather_utils.h\"],\n-    strip_include_prefix = \".\",\n-    deps = [\n-        \":mlir_hlo\",\n-        \"@llvm-project//mlir:DialectUtils\",\n-        \"@llvm-project//mlir:Support\",\n-        \"@llvm-project//mlir:TensorDialect\",\n-    ],\n-)\n-\n cc_library(\n     name = \"unfuse_batch_norm\",\n     srcs = [\"mhlo/transforms/unfuse_batch_norm/unfuse_batch_norm.cc\"],\n@@ -790,7 +723,6 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:LLVMCommonConversion\",\n         \"@llvm-project//mlir:LLVMDialect\",\n-        \"@llvm-project//mlir:LinalgTransforms\",\n         \"@llvm-project//mlir:MathToLLVM\",\n         \"@llvm-project//mlir:MemRefDialect\",\n         \"@llvm-project//mlir:MemRefToLLVM\","
        },
        {
            "sha": "4e8c6dfd495a2d8c5d465a911180461b2f9008f2",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2FCMakeLists.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2FCMakeLists.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2FCMakeLists.txt?ref=974f13866a5cc99ac2801c2d31294e96e629d0e9",
            "patch": "@@ -89,27 +89,6 @@ add_mlir_library(MhloToMemrefConversion\n   MLIRTransformUtils\n )\n \n-add_mlir_library(MhloToArithmeticConversion\n-  hlo_legalize_to_arithmetic/hlo_legalize_to_arithmetic.cc\n-\n-  DEPENDS\n-  MLIRhlo_opsIncGen\n-  MLIRMhloPassIncGen\n-\n-  LINK_COMPONENTS\n-  Core\n-\n-  LINK_LIBS PUBLIC\n-  MhloDialect\n-  MhloTypeConversion\n-  MLIRIR\n-  MLIRPass\n-  MLIRMathDialect\n-  MLIRSCFDialect\n-  MLIRTransforms\n-  MLIRTransformUtils\n-)\n-\n add_mlir_library(ChloPasses\n   chlo_legalize_to_hlo/chlo_legalize_to_hlo_pass.cc\n \n@@ -132,32 +111,6 @@ add_mlir_library(ChloPasses\n   MLIRTransformUtils\n )\n \n-add_mlir_library(MhloToLinalg\n-  legalize_to_linalg/legalize_to_linalg.cc\n-\n-  DEPENDS\n-  MLIRhlo_opsIncGen\n-  MLIRMhloPassIncGen\n-\n-  LINK_COMPONENTS\n-  Core\n-\n-  LINK_LIBS PUBLIC\n-  HloToLinalgUtils\n-  MhloDialect\n-  MhloRngUtils\n-  MhloToArithmeticConversion\n-  MhloTypeConversion\n-  MLIRBufferizationDialect\n-  MLIRComplexDialect\n-  MLIRIR\n-  MLIRLinalgTransforms\n-  MLIRLinalgUtils\n-  MLIRPass\n-  MLIRRewrite\n-  MLIRTransformUtils\n-)\n-\n add_mlir_library(MhloToStablehlo\n   hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo.cc\n   hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo_pass.cc"
        },
        {
            "sha": "5edc5bd808aa26c23e00b4508b59606b05d64534",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_arithmetic/hlo_legalize_to_arithmetic.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 258,
            "changes": 258,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_arithmetic%2Fhlo_legalize_to_arithmetic.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_arithmetic%2Fhlo_legalize_to_arithmetic.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_arithmetic%2Fhlo_legalize_to_arithmetic.cc?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,258 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-// This file implements logic for lowering HLO dialect to Arithmetic dialect.\n-\n-#include <memory>\n-#include <optional>\n-#include <utility>\n-\n-#include \"mhlo/IR/hlo_ops.h\"\n-#include \"mhlo/transforms/map_mhlo_to_scalar_op.h\"\n-#include \"mhlo/transforms/passes.h\"\n-#include \"mhlo/transforms/rewriters.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/BuiltinDialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Support/LLVM.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-\n-namespace mlir {\n-namespace mhlo {\n-\n-#define GEN_PASS_DEF_HLOLEGALIZETOARITHMETICPASS\n-#include \"mhlo/transforms/mhlo_passes.h.inc\"\n-\n-namespace {\n-\n-struct RngGetAndUpdateStatePattern\n-    : public OpConversionPattern<mhlo::XlaRngGetAndUpdateStateOp> {\n-  using OpConversionPattern<\n-      mhlo::XlaRngGetAndUpdateStateOp>::OpConversionPattern;\n-\n-  LogicalResult matchAndRewrite(\n-      mhlo::XlaRngGetAndUpdateStateOp op,\n-      XlaRngGetAndUpdateStateOpAdaptor adaptor,\n-      ConversionPatternRewriter& rewriter) const final {\n-    // Get various type related information\n-    auto loc = op->getLoc();\n-\n-    const auto globalName = rewriter.getStringAttr(\"rng_state\");\n-    constexpr auto initialSeed = 0x7012395ull;\n-    auto seedType = rewriter.getIntegerType(128);\n-    auto memrefType = MemRefType::get({}, seedType);\n-\n-    auto resultType = op.getType();\n-    auto wordSize = resultType.getElementType().getIntOrFloatBitWidth();\n-    auto smallerIntType = rewriter.getIntegerType(wordSize);\n-    auto numElements = resultType.getNumElements();\n-\n-    // Get or define the global variable\n-    auto* globalOp = mlir::SymbolTable::lookupNearestSymbolFrom(op, globalName);\n-    if (!globalOp) {\n-      auto* parent = mlir::SymbolTable::getNearestSymbolTable(op);\n-      OpBuilder::InsertionGuard g(rewriter);\n-      rewriter.setInsertionPointToStart(&parent->getRegions().front().front());\n-\n-      const auto priv = rewriter.getStringAttr(\"private\");\n-      auto initialValue = mlir::DenseElementsAttr::get(\n-          mlir::RankedTensorType::get({}, seedType),\n-          rewriter.getIntegerAttr(seedType, initialSeed));\n-      globalOp = rewriter.create<memref::GlobalOp>(\n-          loc, globalName, priv, memrefType, initialValue, /*constant=*/false,\n-          /*alignment=*/IntegerAttr());\n-    }\n-    assert(isa<memref::GlobalOp>(globalOp) &&\n-           \"rng_state was defined somewhere else, not as a global op\");\n-\n-    // Get and update\n-    Value rngState =\n-        rewriter.create<memref::GetGlobalOp>(loc, memrefType, globalName);\n-    Value oldVal = rewriter.create<memref::LoadOp>(loc, rngState);\n-    Value delta = rewriter.create<arith::ConstantOp>(\n-        loc, rewriter.getIntegerAttr(seedType,\n-                                     static_cast<int64_t>(adaptor.getDelta())));\n-    Value newVal = rewriter.create<arith::AddIOp>(loc, oldVal, delta);\n-    (void)rewriter.create<memref::StoreOp>(loc, newVal, rngState);\n-\n-    // Create the proper return type by packing the old seed into a tensor\n-    SmallVector<Value> pieces;\n-    for (int i = (numElements - 1) * wordSize; i >= 0; i -= wordSize) {\n-      Value shiftDistance = rewriter.create<arith::ConstantOp>(\n-          loc, rewriter.getIntegerAttr(seedType, i));\n-      pieces.push_back(rewriter.create<arith::TruncIOp>(\n-          loc, smallerIntType,\n-          rewriter.create<arith::ShRUIOp>(loc, oldVal, shiftDistance)));\n-    }\n-\n-    // Obtain a tensor with the correct shape and bit widths but the incorrect\n-    // integer signedness, then cast the tensor to the correct signedness to\n-    // ensure that unrealized casts will successfully lower later.\n-    Value resultTensor = rewriter.create<tensor::FromElementsOp>(\n-        loc, mlir::RankedTensorType::get(resultType.getShape(), smallerIntType),\n-        pieces);\n-    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(op, resultType,\n-                                                            resultTensor);\n-    return success();\n-  }\n-};\n-\n-template <typename OpTy>\n-struct ScalarHloToArithmeticPattern : public OpConversionPattern<OpTy> {\n-  ScalarHloToArithmeticPattern(\n-      TypeConverter& typeConverter, MLIRContext* context,\n-      llvm::function_ref<bool(Operation*)> filterFn = nullptr,\n-      PatternBenefit benefit = 1)\n-      : OpConversionPattern<OpTy>(typeConverter, context, benefit),\n-        filterFn(filterFn) {}\n-\n-  LogicalResult matchAndRewrite(\n-      OpTy op, typename OpTy::Adaptor adaptor,\n-      ConversionPatternRewriter& rewriter) const final {\n-    if (filterFn && !filterFn(op)) return failure();\n-\n-    auto isScalar = [&](Value v) {\n-      return mlir::cast<ShapedType>(v.getType()).getRank() == 0;\n-    };\n-\n-    if (!llvm::all_of(adaptor.getOperands(), isScalar))\n-      return rewriter.notifyMatchFailure(op, \"All operands must be scalar.\");\n-\n-    auto loc = op.getLoc();\n-\n-    std::optional<ShapedType> resultTy;\n-    resultTy = mlir::dyn_cast<ShapedType>(\n-        this->typeConverter->convertType(op->getResultTypes().front()));\n-\n-    SmallVector<Value> operands;\n-    for (auto operand : adaptor.getOperands()) {\n-      operands.push_back(\n-          rewriter.create<tensor::ExtractOp>(loc, operand, ValueRange()));\n-    }\n-    Value scalarResult = mhlo::MhloOpToStdScalarOp::mapOp(\n-        op, resultTy->getElementType(), operands, /*attributes=*/{}, &rewriter);\n-    if (!scalarResult) return failure();\n-    rewriter.replaceOpWithNewOp<tensor::FromElementsOp>(op, *resultTy,\n-                                                        scalarResult);\n-    return success();\n-  }\n-\n- private:\n-  llvm::function_ref<bool(Operation*)> filterFn;\n-};\n-\n-struct HloLegalizeToArithmeticPass\n-    : public impl::HloLegalizeToArithmeticPassBase<\n-          HloLegalizeToArithmeticPass> {\n-  void getDependentDialects(DialectRegistry& registry) const override {\n-    registry.insert<arith::ArithDialect, memref::MemRefDialect,\n-                    tensor::TensorDialect>();\n-  }\n-\n- public:\n-  void runOnOperation() override {\n-    auto& context = getContext();\n-    RewritePatternSet patterns(&context);\n-    ConversionTarget target(context);\n-\n-    populateHloToArithmeticConversionPatterns(&patterns);\n-\n-    target.addIllegalOp<XlaRngGetAndUpdateStateOp>();\n-    target.addLegalDialect<arith::ArithDialect, BuiltinDialect,\n-                           memref::MemRefDialect, tensor::TensorDialect>();\n-\n-    auto module = getOperation();\n-    if (failed(applyPartialConversion(module, target, std::move(patterns))))\n-      signalPassFailure();\n-  }\n-};\n-\n-}  // namespace\n-\n-void populateHloToArithmeticConversionPatterns(RewritePatternSet* patterns) {\n-  patterns->add<RngGetAndUpdateStatePattern>(patterns->getContext());\n-}\n-\n-void populateScalarHloToArithmeticConversionPatterns(\n-    MLIRContext* context, TypeConverter& typeConverter,\n-    RewritePatternSet* patterns,\n-    llvm::function_ref<bool(Operation*)> filterFn) {\n-  // clang-format off\n-  patterns->add<\n-      ScalarHloToArithmeticPattern<mhlo::AbsOp>,\n-      ScalarHloToArithmeticPattern<mhlo::AcosOp>,\n-      ScalarHloToArithmeticPattern<mhlo::AcoshOp>,\n-      ScalarHloToArithmeticPattern<mhlo::AddOp>,\n-      ScalarHloToArithmeticPattern<mhlo::AndOp>,\n-      ScalarHloToArithmeticPattern<mhlo::Atan2Op>,\n-      ScalarHloToArithmeticPattern<mhlo::AtanhOp>,\n-      ScalarHloToArithmeticPattern<mhlo::BitcastConvertOp>,\n-      ScalarHloToArithmeticPattern<mhlo::CbrtOp>,\n-      ScalarHloToArithmeticPattern<mhlo::CeilOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ClampOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ClzOp>,\n-      ScalarHloToArithmeticPattern<mhlo::CompareOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ComplexOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ConvertOp>,\n-      ScalarHloToArithmeticPattern<mhlo::CopyOp>,\n-      ScalarHloToArithmeticPattern<mhlo::CoshOp>,\n-      ScalarHloToArithmeticPattern<mhlo::CosineOp>,\n-      ScalarHloToArithmeticPattern<mhlo::DivOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ErfOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ExpOp>,\n-      ScalarHloToArithmeticPattern<mhlo::Expm1Op>,\n-      ScalarHloToArithmeticPattern<mhlo::FloorOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ImagOp>,\n-      ScalarHloToArithmeticPattern<mhlo::IsFiniteOp>,\n-      ScalarHloToArithmeticPattern<mhlo::Log1pOp>,\n-      ScalarHloToArithmeticPattern<mhlo::LogOp>,\n-      ScalarHloToArithmeticPattern<mhlo::LogisticOp>,\n-      ScalarHloToArithmeticPattern<mhlo::MaxOp>,\n-      ScalarHloToArithmeticPattern<mhlo::MinOp>,\n-      ScalarHloToArithmeticPattern<mhlo::MulOp>,\n-      ScalarHloToArithmeticPattern<mhlo::NegOp>,\n-      ScalarHloToArithmeticPattern<mhlo::NotOp>,\n-      ScalarHloToArithmeticPattern<mhlo::OrOp>,\n-      ScalarHloToArithmeticPattern<mhlo::PopulationCountOp>,\n-      ScalarHloToArithmeticPattern<mhlo::PowOp>,\n-      ScalarHloToArithmeticPattern<mhlo::RealOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ReducePrecisionOp>,\n-      ScalarHloToArithmeticPattern<mhlo::RemOp>,\n-      ScalarHloToArithmeticPattern<mhlo::RoundNearestEvenOp>,\n-      ScalarHloToArithmeticPattern<mhlo::RoundOp>,\n-      ScalarHloToArithmeticPattern<mhlo::RsqrtOp>,\n-      ScalarHloToArithmeticPattern<mhlo::SelectOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ShiftLeftOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ShiftRightArithmeticOp>,\n-      ScalarHloToArithmeticPattern<mhlo::ShiftRightLogicalOp>,\n-      ScalarHloToArithmeticPattern<mhlo::SignOp>,\n-      ScalarHloToArithmeticPattern<mhlo::SineOp>,\n-      ScalarHloToArithmeticPattern<mhlo::SqrtOp>,\n-      ScalarHloToArithmeticPattern<mhlo::SubtractOp>,\n-      ScalarHloToArithmeticPattern<mhlo::TanOp>,\n-      ScalarHloToArithmeticPattern<mhlo::TanhOp>,\n-      ScalarHloToArithmeticPattern<mhlo::XorOp>\n-  >(typeConverter, context, filterFn);\n-  // clang-format on\n-}\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createLegalizeToArithmeticPass() {\n-  return std::make_unique<HloLegalizeToArithmeticPass>();\n-}\n-\n-}  // namespace mhlo\n-}  // namespace mlir"
        },
        {
            "sha": "5c417b2201100b15e78e9b877302601459893503",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/legalize_to_linalg/legalize_to_linalg.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 4682,
            "changes": 4682,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_to_linalg%2Flegalize_to_linalg.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_to_linalg%2Flegalize_to_linalg.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_to_linalg%2Flegalize_to_linalg.cc?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8"
        },
        {
            "sha": "88e19cb649b52cc8dd1a29b9a10b82521543b34e",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/mhlo_passes.td",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_passes.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_passes.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_passes.td?ref=974f13866a5cc99ac2801c2d31294e96e629d0e9",
            "patch": "@@ -49,11 +49,6 @@ def ChloLegalizeToHloPass : Pass<\"chlo-legalize-to-hlo\", \"func::FuncOp\"> {\n   ];\n }\n \n-def HloLegalizeToArithmeticPass :Pass<\"hlo-legalize-to-arithmetic\", \"ModuleOp\"> {\n-  let summary = \"Legalize from HLO dialect to arithmetic dialect.\";\n-  let constructor = \"createLegalizeToArithmeticPass()\";\n-}\n-\n def LegalizeDotToDotGeneralPass : Pass<\"mhlo-legalize-dot-to-dot-general\", \"func::FuncOp\"> {\n   let summary = \"Legalizes dot ops to dot_general ops.\";\n   let constructor = \"createLegalizeDotToDotGeneralPass()\";\n@@ -75,15 +70,6 @@ def LegalizeTanhToApproximationPass : Pass<\"mhlo-legalize-trigonometric-to-appro\n   let constructor = \"createLegalizeTrigonometricToApproximationPass()\";\n }\n \n-def HloLegalizeToLinalgPass : Pass<\"hlo-legalize-to-linalg\", \"func::FuncOp\"> {\n-  let summary = \"Legalize from HLO dialect to Linalg dialect.\";\n-  let constructor = \"createLegalizeHloToLinalgPass()\";\n-  let options = [Option<\"enablePrimitiveOps\", \"enable-primitive-ops\", \"bool\",\n-                        /*default=*/\"false\",\n-                        \"Lower to primitive Linalg ops (map, reduce and \"\n-                        \"transpose) when possible, instead of linalg.generic\">];\n-}\n-\n def TestMaterializeBroadcastsPass : Pass<\"mhlo-test-materialize-broadcasts\", \"func::FuncOp\"> {\n   let summary = \"Test pass for materializing 'broadcast_dimensions' attributes.\";\n   let constructor = \"createTestMaterializeBroadcastsPass()\";"
        },
        {
            "sha": "630e71e9b5d6136a4d8c4e0afe1c5d49b07ded90",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/passes.h",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fpasses.h?ref=974f13866a5cc99ac2801c2d31294e96e629d0e9",
            "patch": "@@ -44,13 +44,6 @@ ChloLegalizeToHighLevelMhloPassOptions getDefaultChloToHighLevelMhloOptions();\n /// Returns options for the ChloLegalizeToHighLevelMhloPass for the GPU backend.\n ChloLegalizeToHighLevelMhloPassOptions getGpuChloToHighLevelMhloOptions();\n \n-/// Lowers from HLO dialect to Arithmetic dialect.\n-std::unique_ptr<OperationPass<ModuleOp>> createLegalizeToArithmeticPass();\n-\n-/// Lowers from HLO dialect to Linalg dialect.\n-std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeHloToLinalgPass(\n-    bool enablePrimitiveOps = false);\n-\n // Sinks constants implicitly captured in control flow regions. This is\n // necessary to export to XLA.\n std::unique_ptr<OperationPass<func::FuncOp>>"
        },
        {
            "sha": "be078677983c87d273a285525082c31048804325",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2FCMakeLists.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2FCMakeLists.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2FCMakeLists.txt?ref=974f13866a5cc99ac2801c2d31294e96e629d0e9",
            "patch": "@@ -18,39 +18,6 @@ include_directories(BEFORE\n     ${CMAKE_CURRENT_BINARY_DIR}\n     ${CMAKE_CURRENT_SOURCE_DIR})\n \n-add_mlir_library(MhloRngUtils\n-  mhlo_rng_utils.cc\n-\n-  DEPENDS\n-  MLIRhlo_opsIncGen\n-\n-  LINK_COMPONENTS\n-  Core\n-\n-  LINK_LIBS PUBLIC\n-  MhloDialect\n-  MLIRArithDialect\n-  MLIRIR\n-  MLIRLinalgUtils\n-  MLIRMathDialect\n-  MLIRRewrite\n-  MLIRSupport\n-  MLIRTensorDialect\n-  MLIRTransforms\n-)\n-\n-add_mlir_library(MhloScatterUtils\n-  mhlo_scatter_gather_utils.cc\n-\n-  DEPENDS\n-  MLIRhlo_opsIncGen\n-\n-  LINK_COMPONENTS\n-  Core\n-\n-  LINK_LIBS PUBLIC\n-  MhloDialect\n-)\n \n add_mlir_library(MhloTypeConversion\n   type_conversion.cc\n@@ -67,24 +34,3 @@ add_mlir_library(MhloTypeConversion\n   StablehloOps\n )\n \n-add_mlir_library(HloToLinalgUtils\n-  legalize_to_linalg_utils.cc\n-\n-  DEPENDS\n-  MLIRhlo_opsIncGen\n-  MLIRMhloPassIncGen\n-\n-  LINK_COMPONENTS\n-  Core\n-\n-  LINK_LIBS PUBLIC\n-  MhloDialect\n-  MhloTypeConversion\n-  MLIRBufferizationDialect\n-  MLIRComplexDialect\n-  MLIRIR\n-  MLIRLinalgUtils\n-  MLIRPass\n-  MLIRRewrite\n-  MLIRTransformUtils\n-)"
        },
        {
            "sha": "33ff55eeaf53fdd454299a8a3acd5d18d38264e9",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/legalize_to_linalg_utils.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 146,
            "changes": 146,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Flegalize_to_linalg_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Flegalize_to_linalg_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Flegalize_to_linalg_utils.cc?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,146 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-// This file implements utilities for lowering CHLO/HLO dialect to Linalg\n-// dialect.\n-\n-#include \"mhlo/utils/legalize_to_linalg_utils.h\"\n-\n-#include <algorithm>\n-#include <numeric>\n-#include <string>\n-#include <utility>\n-\n-#include \"mlir/Dialect/Bufferization/IR/Bufferization.h\"\n-#include \"mlir/Dialect/SparseTensor/IR/SparseTensor.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/Support/LLVM.h\"\n-#include \"stablehlo/dialect/ChloOps.h\"\n-\n-namespace mlir {\n-namespace mhlo {\n-namespace {\n-\n-bool hasIntegralShapeType(Operation* op) {\n-  auto stp = mlir::dyn_cast<ShapedType>(op->getOperand(0).getType());\n-  return stp && stp.getElementType().isIntOrIndex();\n-}\n-\n-}  // namespace\n-\n-SmallVector<utils::IteratorType, 3> getParallelAndReductionIterators(\n-    unsigned nLoops, unsigned nReduction) {\n-  SmallVector<utils::IteratorType, 3> res(nLoops - nReduction,\n-                                          utils::IteratorType::parallel);\n-  res.append(nReduction, utils::IteratorType::reduction);\n-  return res;\n-}\n-\n-SmallVector<utils::IteratorType, 3> getNParallelLoopsAttrs(\n-    unsigned nParallelLoops) {\n-  return getParallelAndReductionIterators(nParallelLoops, 0);\n-}\n-\n-Value getEmptySparseTensor(OpBuilder& b, Location loc, ShapedType type,\n-                           ArrayRef<Value> dynSizes) {\n-  return b.create<bufferization::AllocTensorOp>(\n-      loc, mlir::cast<TensorType>(type), dynSizes,\n-      /*copy=*/Value(),\n-      /*memory_space=*/IntegerAttr());\n-}\n-\n-Value getEmptyTensor(OpBuilder& b, Location loc, ShapedType type,\n-                     ArrayRef<Value> dynSizes) {\n-  return b.create<tensor::EmptyOp>(\n-      loc, type.getShape(), type.getElementType(), dynSizes,\n-      mlir::cast<RankedTensorType>(type).getEncoding());\n-}\n-\n-Value getEmptyTensorFor(OpBuilder& b, Location loc, ShapedType resultType,\n-                        Operation* op, ValueRange operands) {\n-  bool isSparse = sparse_tensor::getSparseTensorEncoding(resultType) != nullptr;\n-  // Collect the sizes for a ranked tensor to be passed as parameter to a\n-  // new tensor initialization operation. This operation only needs the\n-  // dynamic sizes.\n-  SmallVector<Value> sizes;\n-  if (resultType.hasRank() && !resultType.hasStaticShape()) {\n-    // Ask the op for its output shape.\n-    auto shapeSource = cast<InferShapedTypeOpInterface>(op);\n-    SmallVector<Value, 1> reifiedShapes;\n-    (void)shapeSource.reifyReturnTypeShapes(b, operands, reifiedShapes);\n-    assert(reifiedShapes.size() == 1 && \"Expected one reified result\");\n-    // Construct sizes for the required dimensions.\n-    for (const auto& en : llvm::enumerate(resultType.getShape())) {\n-      if (en.value() != ShapedType::kDynamic) continue;\n-      sizes.push_back(b.create<tensor::ExtractOp>(\n-          loc, reifiedShapes[0],\n-          ValueRange{b.create<arith::ConstantIndexOp>(loc, en.index())}));\n-    }\n-  }\n-  return isSparse ? getEmptySparseTensor(b, loc, resultType, sizes)\n-                  : getEmptyTensor(b, loc, resultType, sizes);\n-}\n-\n-Value preSparsify(Operation* op, llvm::SmallVector<Value, 2>& values, Type rtp,\n-                  OpBuilder* b) {\n-  // Apply for semi-ring operations that lower to elaborate code\n-  // (any sign-op, or an integral abs-op).\n-  // TODO(peiming, ajcbik): these all can potentially be optimized by applying\n-  // value transform on sparse_tenosr.value memref\n-  if (isa<mhlo::SignOp>(op) || isa<mhlo::NegOp>(op) ||\n-      (isa<mhlo::AbsOp>(op) && hasIntegralShapeType(op)) ||\n-      isa<chlo::AsinOp>(op) || isa<chlo::AsinhOp>(op) ||\n-      isa<chlo::AtanOp>(op) || isa<chlo::AtanhOp>(op) ||\n-      isa<chlo::BesselI1eOp>(op) || isa<chlo::SinhOp>(op) ||\n-      isa<chlo::TanOp>(op)) {\n-    if (!sparse_tensor::getSparseTensorEncoding(op->getResult(0).getType()) &&\n-        !sparse_tensor::getSparseTensorEncoding(op->getOperand(0).getType()))\n-      return Value();\n-    Location loc = op->getLoc();\n-    auto semiring = b->create<sparse_tensor::UnaryOp>(loc, rtp, values[0]);\n-    Type itp = values[0].getType();\n-    Block* present = b->createBlock(&semiring.getPresentRegion(), {}, itp, loc);\n-    b->setInsertionPointToStart(&semiring.getPresentRegion().front());\n-    values[0] = present->getArgument(0);\n-    return semiring;\n-  }\n-  return Value();\n-}\n-\n-Value postSparsify(Operation* op, Value semiring, Value result, OpBuilder* b) {\n-  if (semiring) {\n-    b->create<sparse_tensor::YieldOp>(op->getLoc(), result);\n-    b->setInsertionPointAfter(semiring.getDefiningOp());\n-    return semiring;\n-  }\n-  return result;\n-}\n-\n-bool allOperandsAreScalarTensors(Operation* op) {\n-  return llvm::all_of(op->getOperands(), [](Value operand) {\n-    auto operandTy = mlir::dyn_cast<ShapedType>(operand.getType());\n-    return operandTy && operandTy.getRank() == 0;\n-  });\n-}\n-\n-bool isInBodyOfLinalgOps(Operation* op) {\n-  auto* parentOp = op->getParentRegion()->getParentOp();\n-  return parentOp->getDialect() ==\n-         parentOp->getContext()->getLoadedDialect<linalg::LinalgDialect>();\n-}\n-\n-}  // namespace mhlo\n-\n-}  // namespace mlir"
        },
        {
            "sha": "85a68db333353e23007a5b1d197a944924953cff",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/legalize_to_linalg_utils.h",
            "status": "removed",
            "additions": 0,
            "deletions": 189,
            "changes": 189,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Flegalize_to_linalg_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Flegalize_to_linalg_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Flegalize_to_linalg_utils.h?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,189 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-// This file supports the lowering of CHLO/HLO dialect to Linalg dialect.\n-\n-#ifndef MLIR_HLO_DIALECT_MHLO_TRANSFORMS_LEGALIZE_TO_LINALG_UTILS_H_\n-#define MLIR_HLO_DIALECT_MHLO_TRANSFORMS_LEGALIZE_TO_LINALG_UTILS_H_\n-\n-#include <algorithm>\n-#include <numeric>\n-#include <optional>\n-#include <string>\n-#include <utility>\n-\n-#include \"llvm/ADT/STLExtras.h\"\n-#include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/ADT/StringSet.h\"\n-#include \"mhlo/transforms/map_mhlo_to_scalar_op.h\"\n-#include \"mlir/Dialect/Linalg/IR/Linalg.h\"\n-#include \"mlir/Dialect/Linalg/Utils/Utils.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/AffineExpr.h\"\n-#include \"mlir/IR/Attributes.h\"\n-#include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinTypes.h\"\n-#include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"mlir/IR/Operation.h\"\n-#include \"mlir/IR/OperationSupport.h\"\n-#include \"mlir/IR/TypeUtilities.h\"\n-#include \"mlir/Support/LLVM.h\"\n-#include \"mlir/Support/LogicalResult.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-\n-namespace mlir {\n-namespace mhlo {\n-\n-/// Returns an ArrayAttr that contains `nLoops` attributes. All the attributes\n-/// are \"parallel\" except the last `nReduction` elements, where are \"reduction\"\n-/// attributes.\n-SmallVector<utils::IteratorType, 3> getParallelAndReductionIterators(\n-    unsigned nLoops, unsigned nReduction);\n-\n-/// Returns an ArrayAttr that contains `nParallelLoops` \"parallel\" attributes.\n-SmallVector<utils::IteratorType, 3> getNParallelLoopsAttrs(\n-    unsigned nParallelLoops);\n-\n-/// Generates an init sparse tensor.\n-Value getEmptySparseTensor(OpBuilder& b, Location loc, ShapedType type,\n-                           ArrayRef<Value> dynSizes);\n-\n-/// Generates a tensor.empty op.\n-Value getEmptyTensor(OpBuilder& b, Location loc, ShapedType type,\n-                     ArrayRef<Value> dynSizes);\n-\n-/// Generates an empty tensor for the result of the operation, which could be a\n-/// dense tensor or a sparse tensor.\n-Value getEmptyTensorFor(OpBuilder& b, Location loc, ShapedType resultType,\n-                        Operation* op, ValueRange operands);\n-\n-/// Sparsifies a (block of) operation(s) that cannot be handled directly\n-/// by the sparse compiler but has well-known semi-ring semantics.\n-///\n-/// This yields something of the following form:\n-///\n-///   %result = sparse_tensor.unary %values[0]\n-///     present={\n-///       ^bb1(%val):\n-///         ... codegen proceeds here using %val ....\n-///         sparse_tensor.yield\n-///     }\n-///     absent={}\n-///   linalg.yield %result\n-Value preSparsify(Operation* op, llvm::SmallVector<Value, 2>& values, Type rtp,\n-                  OpBuilder* b);\n-\n-/// Finalizes sparse semi-ring construction.\n-Value postSparsify(Operation* op, Value semiring, Value result, OpBuilder* b);\n-\n-/// Returns true if all operands are tensors with rank 0.\n-bool allOperandsAreScalarTensors(Operation* op);\n-\n-/// Returns true if parent op is linalg.\n-bool isInBodyOfLinalgOps(Operation* op);\n-\n-/// Converts a HLO operation to a linalg.generic op that contains the\n-/// corresponding scalar operations.\n-template <typename OpTy>\n-class PointwiseToLinalgConverter : public OpConversionPattern<OpTy> {\n- public:\n-  using OpConversionPattern<OpTy>::OpConversionPattern;\n-\n-  LogicalResult matchAndRewrite(\n-      OpTy op, typename OpTy::Adaptor adaptor,\n-      ConversionPatternRewriter& rewriter) const final {\n-    auto loc = op.getLoc();\n-    // Find maximum rank / number of loops.\n-    auto getRank = [](Value v) {\n-      return mlir::cast<ShapedType>(v.getType()).getRank();\n-    };\n-    auto isScalar = [&](Value v) { return getRank(v) == 0; };\n-    auto it = llvm::find_if_not(adaptor.getOperands(), isScalar);\n-    Value maxRankArg =\n-        it != adaptor.getOperands().end() ? *it : adaptor.getOperands().front();\n-    int64_t nloops = getRank(maxRankArg);\n-\n-    // Apply only if all operands are scalar or have the same rank. Some ops,\n-    // like `mhlo.select`, support implicit broadcasting of scalars.\n-    if (!llvm::all_of(adaptor.getOperands(), [&](Value v) {\n-          int64_t r = getRank(v);\n-          return r == 0 || r == nloops;\n-        })) {\n-      return rewriter.notifyMatchFailure(\n-          op, \"Operands must be os same rank or scalar.\");\n-    }\n-\n-    // Find result type, if on tensors.\n-    std::optional<ShapedType> resultTy;\n-    resultTy = mlir::dyn_cast<ShapedType>(\n-        this->typeConverter->convertType(op->getResultTypes().front()));\n-\n-    // Check result type compatibility.\n-    if (!resultTy || !resultTy->hasRank() || resultTy->getRank() != nloops ||\n-        !(resultTy->getElementType().isSignlessIntOrFloat() ||\n-          isa<ComplexType>(resultTy->getElementType()))) {\n-      return rewriter.notifyMatchFailure(\n-          op, \"mismatched operand/result types or iterator count\");\n-    }\n-\n-    if (allOperandsAreScalarTensors(op) && isInBodyOfLinalgOps(op))\n-      return failure();\n-\n-    // Find input/output values and types.\n-    ValueRange inputs = adaptor.getOperands();\n-    Value output =\n-        getEmptyTensorFor(rewriter, loc, *resultTy, op, adaptor.getOperands());\n-\n-    // Create indexing maps.\n-    AffineMap scalarMap = AffineMap::get(nloops, 0, rewriter.getContext());\n-    AffineMap idMap = rewriter.getMultiDimIdentityMap(nloops);\n-    SmallVector<AffineMap, 4> maps;\n-    for (Value v : inputs) maps.push_back(isScalar(v) ? scalarMap : idMap);\n-    maps.push_back(idMap);\n-\n-    // Build `linalg.generic` op.\n-    bool failed = false;\n-    auto linalgOp = rewriter.create<linalg::GenericOp>(\n-        loc, resultTy ? *resultTy : TypeRange{}, inputs, output, maps,\n-        getNParallelLoopsAttrs(nloops),\n-        [&](OpBuilder& nestedBuilder, Location /*nested_loc*/,\n-            ValueRange args) {\n-          Type innerResultTy = getElementTypeOrSelf(output);\n-          auto argvec = llvm::to_vector<2>(args.take_front(inputs.size()));\n-          auto semiring = preSparsify(op, argvec, innerResultTy, &rewriter);\n-          Value innerResult = mhlo::MhloOpToStdScalarOp::mapOp(\n-              op, innerResultTy, argvec, /*attributes=*/{}, &rewriter);\n-          if (innerResult == nullptr) {\n-            failed = true;\n-          } else {\n-            innerResult = postSparsify(op, semiring, innerResult, &rewriter);\n-            nestedBuilder.create<linalg::YieldOp>(loc, innerResult);\n-          }\n-        },\n-        linalg::getPrunedAttributeList(op));\n-    if (failed) return failure();\n-\n-    rewriter.replaceOp(op, linalgOp->getResults());\n-    return success();\n-  }\n-};\n-\n-}  // namespace mhlo\n-\n-}  // namespace mlir\n-\n-#endif  // MLIR_HLO_DIALECT_MHLO_TRANSFORMS_LEGALIZE_TO_LINALG_UTILS_H_"
        },
        {
            "sha": "066c2b494b41c86f39180e60d1173d18469fdb18",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/mhlo_rng_utils.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 762,
            "changes": 762,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_rng_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_rng_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_rng_utils.cc?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,762 +0,0 @@\n-/* Copyright 2023 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"mhlo/utils/mhlo_rng_utils.h\"\n-\n-#include <algorithm>\n-#include <array>\n-#include <cstdint>\n-#include <utility>\n-\n-#include \"llvm/ADT/ArrayRef.h\"\n-#include \"llvm/ADT/SmallVector.h\"\n-#include \"mhlo/IR/hlo_ops.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/Linalg/IR/Linalg.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/Dialect/Utils/ReshapeOpsUtils.h\"\n-#include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n-#include \"mlir/IR/BuiltinTypes.h\"\n-#include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/Types.h\"\n-#include \"mlir/Support/LLVM.h\"\n-\n-namespace mlir {\n-namespace mhlo {\n-namespace {\n-\n-class ArithOp {\n- public:\n-  ArithOp(OpBuilder b, Location l, Value v) : builder(b), loc(l), value(v) {}\n-\n-  explicit operator Value() { return value; }\n-  Value val() { return value; }\n-\n-  ArithOp constantI(int64_t value, int64_t bits) {\n-    Value val = arith::ConstantOp::create(\n-        builder, loc,\n-        builder.getIntegerAttr(builder.getIntegerType(bits), value));\n-    return ArithOp(builder, loc, val);\n-  }\n-\n-  ArithOp extendUI(int32_t bits) {\n-    Value ext = arith::ExtUIOp::create(builder, loc,\n-                                       builder.getIntegerType(bits), value);\n-    return ArithOp(builder, loc, ext);\n-  }\n-\n-  ArithOp truncI(int64_t bits) {\n-    if (value.getType().getIntOrFloatBitWidth() == bits) return *this;\n-    Value trunc = arith::TruncIOp::create(builder, loc,\n-                                          builder.getIntegerType(bits), value);\n-    return ArithOp(builder, loc, trunc);\n-  }\n-\n-  ArithOp linalgIndex(int32_t index) {\n-    Value val = linalg::IndexOp::create(builder, loc, index);\n-    return ArithOp(builder, loc, val);\n-  }\n-\n-  ArithOp indexCast(int32_t bitwidth) {\n-    if (mlir::isa<IntegerType>(value.getType())) {\n-      Value cast = arith::IndexCastOp::create(builder, loc,\n-                                              builder.getIndexType(), value);\n-      return ArithOp(builder, loc, cast);\n-    }\n-\n-    Value cast = arith::IndexCastOp::create(\n-        builder, loc, builder.getIntegerType(bitwidth), value);\n-    return ArithOp(builder, loc, cast);\n-  }\n-\n-  ArithOp rotateLeft(int32_t rotation) {\n-    int32_t bits = value.getType().getIntOrFloatBitWidth();\n-    ArithOp cLeft = constantI(rotation, bits);\n-    ArithOp cRight = constantI(bits - rotation, bits);\n-    ArithOp rLeft = (*this << cLeft);\n-    ArithOp rRight = (*this >> cRight);\n-    return rLeft | rRight;\n-  }\n-\n-  ArithOp operator+(ArithOp &rhs) {\n-    Value res = arith::AddIOp::create(builder, loc, value, rhs.value);\n-    return ArithOp(builder, loc, res);\n-  }\n-\n-  ArithOp operator*(ArithOp &rhs) {\n-    Value res = arith::MulIOp::create(builder, loc, value, rhs.value);\n-    return ArithOp(builder, loc, res);\n-  }\n-\n-  ArithOp operator|(ArithOp &rhs) {\n-    Value res = arith::OrIOp::create(builder, loc, value, rhs.value);\n-    return ArithOp(builder, loc, res);\n-  }\n-\n-  ArithOp operator^(ArithOp &rhs) {\n-    Value res = arith::XOrIOp::create(builder, loc, value, rhs.value);\n-    return ArithOp(builder, loc, res);\n-  }\n-\n-  ArithOp operator<<(ArithOp &rhs) {\n-    Value shl = arith::ShLIOp::create(builder, loc, value, rhs.value);\n-    return ArithOp(builder, loc, shl);\n-  }\n-\n-  ArithOp operator>>(ArithOp &rhs) {\n-    Value shr = arith::ShRUIOp::create(builder, loc, value, rhs.value);\n-    return ArithOp(builder, loc, shr);\n-  }\n-\n- private:\n-  OpBuilder builder;\n-  Location loc;\n-  Value value;\n-};\n-\n-std::pair<ArithOp, ArithOp> splitI64(ArithOp i64) {\n-  auto low = i64.truncI(32);\n-  auto c32 = i64.constantI(/*value=*/32, /*bits=*/64);\n-  auto high = (i64 >> c32).truncI(32);\n-  return {low, high};\n-}\n-\n-ArithOp fuseI32s(ArithOp low, ArithOp high) {\n-  auto c32 = high.constantI(/*value=*/32, /*bits=*/64);\n-  high = high.extendUI(64) << c32;\n-  low = low.extendUI(64);\n-  return low | high;\n-}\n-\n-// Implements the ThreeFry counter-based PRNG algorithm.\n-// Salmon et al. SC 2011. Parallel random numbers: as easy as 1, 2, 3.\n-// http://www.thesalmons.org/john/random123/papers/random123sc11.pdf\n-std::pair<ArithOp, ArithOp> runThreeFry2xi32(ArithOp key0, ArithOp key1,\n-                                             ArithOp initialState) {\n-  ArithOp index = initialState.linalgIndex(0);\n-  index = index.indexCast(64);\n-  index = index + initialState;\n-\n-  // Split into the 2xi32 used for threefry.\n-  std::pair<ArithOp, ArithOp> input = splitI64(index);\n-  ArithOp input0 = input.first;\n-  ArithOp input1 = input.second;\n-\n-  // Magic number and rotation distances specified by the Threefry2x32\n-  // algorithm.\n-  llvm::SmallVector<int32_t, 8> rotations = {13, 15, 26, 6, 17, 29, 16, 24};\n-  ArithOp magic = key0.constantI(/*value=*/0x1bd11bda, /*bits=*/32);\n-\n-  ArithOp key2 = magic ^ key0 ^ key1;\n-  std::array<ArithOp, 3> ks{key0, key1, key2};\n-  std::array<ArithOp, 2> x{input0 + key0, input1 + key1};\n-\n-  // Performs a single round of the Threefry2x32 algorithm, with a rotation\n-  // amount 'rotation'.\n-  for (int i = 0; i < 5; ++i) {\n-    int32_t rot = (4 * i) % rotations.size();\n-    int32_t k1 = (i + 1) % ks.size();\n-    int32_t k2 = (i + 2) % ks.size();\n-\n-    for (int j = 0; j < 4; ++j) {\n-      x[0] = x[0] + x[1];\n-      x[1] = x[1].rotateLeft(rotations[rot + j]);\n-      x[1] = x[0] ^ x[1];\n-    }\n-\n-    ArithOp c = x[0].constantI(/*value=*/i + 1, /*bits=*/32);\n-    x[0] = x[0] + ks[k1];\n-    x[1] = x[1] + ks[k2];\n-    x[1] = x[1] + c;\n-  }\n-\n-  return std::pair<ArithOp, ArithOp>(x[0], x[1]);\n-}\n-\n-// Extract and potentially reconstruct the i32 key-pair as necessary.\n-std::pair<Value, Value> extractKey32(OpBuilder &builder, Location loc,\n-                                     Value store) {\n-  ShapedType storeTy = mlir::cast<ShapedType>(store.getType());\n-  if (storeTy.getRank() != 1) return {nullptr, nullptr};\n-\n-  Type storeETy = storeTy.getElementType();\n-  auto i32Ty = builder.getIntegerType(32);\n-  auto i64Ty = builder.getIntegerType(64);\n-\n-  if (storeTy.getDimSize(0) == 4 && storeETy.isInteger(32)) {\n-    Value idx0 = arith::ConstantIndexOp::create(builder, loc, 0);\n-    Value idx1 = arith::ConstantIndexOp::create(builder, loc, 1);\n-    Value key0 = tensor::ExtractOp::create(builder, loc, store, idx0);\n-    Value key1 = tensor::ExtractOp::create(builder, loc, store, idx1);\n-    key0 = arith::BitcastOp::create(builder, loc, i32Ty, key0);\n-    key1 = arith::BitcastOp::create(builder, loc, i32Ty, key1);\n-    return {key0, key1};\n-  }\n-\n-  if (storeTy.getDimSize(0) == 2 && storeETy.isInteger(64)) {\n-    Value idx1 = arith::ConstantIndexOp::create(builder, loc, 0);\n-    Value state = tensor::ExtractOp::create(builder, loc, store, idx1);\n-    Value cast = arith::BitcastOp::create(builder, loc, i64Ty, state);\n-    auto pair = splitI64(ArithOp(builder, loc, cast));\n-    return std::pair<Value, Value>(pair.first, pair.second);\n-  }\n-\n-  return {nullptr, nullptr};\n-}\n-\n-// Extract and potentially reconstruct the i64 state as necessary.\n-Value extractState64(OpBuilder &builder, Location loc, Value store) {\n-  ShapedType storeTy = mlir::cast<ShapedType>(store.getType());\n-  if (storeTy.getRank() != 1) return nullptr;\n-\n-  Type storeETy = storeTy.getElementType();\n-  auto i64Ty = builder.getIntegerType(64);\n-\n-  if (storeTy.getDimSize(0) == 2 && storeETy.isInteger(64)) {\n-    Value idx1 = arith::ConstantIndexOp::create(builder, loc, 1);\n-    Value state = tensor::ExtractOp::create(builder, loc, store, idx1);\n-    Value cast = arith::BitcastOp::create(builder, loc, i64Ty, state);\n-    return cast;\n-  }\n-\n-  if (storeTy.getDimSize(0) == 4 && storeETy.isInteger(32)) {\n-    Value idx2 = arith::ConstantIndexOp::create(builder, loc, 2);\n-    Value idx3 = arith::ConstantIndexOp::create(builder, loc, 3);\n-\n-    Value low = tensor::ExtractOp::create(builder, loc, store, idx2);\n-    Value high = tensor::ExtractOp::create(builder, loc, store, idx3);\n-\n-    ArithOp i64 =\n-        fuseI32s(ArithOp(builder, loc, high), ArithOp(builder, loc, low));\n-    return arith::BitcastOp::create(builder, loc, i64Ty, i64.val());\n-  }\n-\n-  return nullptr;\n-}\n-\n-Value setState64(OpBuilder &b, Location loc, Value store, Value state) {\n-  ShapedType storeTy = mlir::cast<ShapedType>(store.getType());\n-  if (storeTy.getRank() != 1) return nullptr;\n-\n-  Type storeETy = storeTy.getElementType();\n-\n-  if (storeTy.getDimSize(0) == 2 && storeETy.isInteger(64)) {\n-    state = arith::BitcastOp::create(b, loc, storeETy, state);\n-    Value idx1 = arith::ConstantIndexOp::create(b, loc, 1);\n-    return tensor::InsertOp::create(b, loc, storeTy, state, store,\n-                                    ValueRange{idx1});\n-  }\n-\n-  if (storeTy.getDimSize(0) == 4 && storeETy.isInteger(32)) {\n-    Value idx2 = arith::ConstantIndexOp::create(b, loc, 2);\n-    Value idx3 = arith::ConstantIndexOp::create(b, loc, 3);\n-    std::pair<ArithOp, ArithOp> states = splitI64(ArithOp(b, loc, state));\n-    Value state0 =\n-        arith::BitcastOp::create(b, loc, storeETy, states.first.val());\n-    Value state1 =\n-        arith::BitcastOp::create(b, loc, storeETy, states.second.val());\n-    Value insert0 = tensor::InsertOp::create(b, loc, storeTy, state0, store,\n-                                             ValueRange{idx2});\n-    Value insert1 = tensor::InsertOp::create(b, loc, storeTy, state1, insert0,\n-                                             ValueRange{idx3});\n-    return insert1;\n-  }\n-\n-  return nullptr;\n-}\n-\n-Value reshapeToTarget(OpBuilder &builder, Location loc, ShapedType destTy,\n-                      Value src) {\n-  auto srcTy = mlir::cast<ShapedType>(src.getType());\n-  // Expand out to the target shape.\n-\n-  auto reassociationIndices =\n-      getReassociationIndicesForCollapse(destTy.getShape(), srcTy.getShape());\n-  if (reassociationIndices.has_value()) {\n-    src = tensor::ExpandShapeOp::create(builder, loc, destTy, src,\n-                                        reassociationIndices.value());\n-  }\n-\n-  // It is also possible our target is Rank-0, then we would\n-  // need to collapse.\n-  reassociationIndices =\n-      getReassociationIndicesForCollapse(srcTy.getShape(), destTy.getShape());\n-  if (reassociationIndices.has_value()) {\n-    src = tensor::CollapseShapeOp::create(builder, loc, destTy, src,\n-                                          reassociationIndices.value());\n-  }\n-\n-  return src;\n-}\n-\n-// Compute the shape for computing three fry.\n-std::pair<ShapedType, int64_t> threeFry32Shape(ShapedType resultTy) {\n-  if (resultTy.getRank() == 0) {\n-    return {resultTy, 0};\n-  }\n-\n-  auto shape = resultTy.getShape();\n-  uint64_t halfDim =\n-      std::max_element(shape.begin(), shape.end()) - shape.begin();\n-\n-  for (int i = 0, s = shape.size(); i < s; i++) {\n-    if (shape[i] & 0x1) continue;\n-    halfDim = i;\n-    break;\n-  }\n-\n-  llvm::SmallVector<int64_t> newShape(shape);\n-  newShape[halfDim] = (newShape[halfDim] + 1) / 2;\n-  if (halfDim == (newShape.size() - 1)) {\n-    newShape.push_back(1);\n-  }\n-\n-  return {RankedTensorType::get(newShape, resultTy.getElementType()), halfDim};\n-}\n-\n-// This implementation generates a 32-bit tensor of ThreeFry random numbers.\n-// It matches the XLA implementation bit-exact and includes an inefficient\n-// method of concatenating / slicing the pairs of generated numbers.\n-//\n-// We should consider dropping the complex slicing and simply generating\n-// 2x the values, then downcast to a 32-bit. It substantially simplifies\n-// the computation and avoids the concat / slice behavior.\n-LogicalResult generateLinalgThreeFry32(OpBuilder &builder, Location loc,\n-                                       ShapedType resultTy, Value &store,\n-                                       Value &result) {\n-  Type resultETy = resultTy.getElementType();\n-\n-  // Extract the stateful values as an i64 and increment the state ahead.\n-  Value initialState = extractState64(builder, loc, store);\n-  if (!initialState) return failure();\n-\n-  std::pair<Value, Value> keys = extractKey32(builder, loc, store);\n-  if (!keys.first || !keys.second) return failure();\n-\n-  ArithOp key0(builder, loc, keys.first);\n-  ArithOp key1(builder, loc, keys.second);\n-\n-  // Compute the intermediate type we use to compute three fry values, including\n-  // the dimension that was halved.\n-  auto pair = threeFry32Shape(resultTy);\n-  ShapedType intermediateType = pair.first;\n-  int64_t halfDim = pair.second;\n-  int64_t count = intermediateType.getNumElements();\n-\n-  // Compute the number of random i64s generated and increment state.\n-  Value countVal =\n-      arith::ConstantOp::create(builder, loc, builder.getI64IntegerAttr(count));\n-  Value newState = arith::AddIOp::create(builder, loc, initialState, countVal);\n-\n-  // Generate a 1D tensor with for the random values.\n-  Value destLeft = tensor::EmptyOp::create(\n-      builder, loc, ArrayRef<int64_t>({count}), resultETy);\n-  Value destRight = tensor::EmptyOp::create(\n-      builder, loc, ArrayRef<int64_t>({count}), resultETy);\n-\n-  ShapedType destTy = mlir::cast<ShapedType>(destLeft.getType());\n-\n-  SmallVector<AffineMap> indexingMaps(2, builder.getMultiDimIdentityMap(1));\n-  SmallVector<utils::IteratorType> iterators(1, utils::IteratorType::parallel);\n-\n-  linalg::GenericOp generic = linalg::GenericOp::create(\n-      builder, loc, TypeRange{destTy, destTy},\n-      /*inputs=*/ValueRange(),\n-      /*outputs=*/ValueRange{destLeft, destRight},\n-      /*indexingMaps=*/indexingMaps, iterators,\n-      [&](OpBuilder &b, Location nestedLoc, ValueRange) {\n-        // Grab three fry results and write to each array.\n-        auto split =\n-            runThreeFry2xi32(key0, key1, ArithOp(b, nestedLoc, initialState));\n-        auto first = split.first.truncI(resultETy.getIntOrFloatBitWidth());\n-        auto second = split.second.truncI(resultETy.getIntOrFloatBitWidth());\n-        b.create<linalg::YieldOp>(loc, ValueRange{first.val(), second.val()});\n-      });\n-\n-  if (resultTy.getNumElements() == 1) {\n-    result = reshapeToTarget(builder, loc, resultTy, generic.getResult(0));\n-    store = setState64(builder, loc, store, newState);\n-    return success();\n-  }\n-\n-  // Reshape to the target size and concatenate on the dimension following the\n-  // half dimension.\n-  Value random0 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(0));\n-  Value random1 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(1));\n-  Value concatenate =\n-      mhlo::ConcatenateOp::create(builder, loc, ValueRange{random0, random1},\n-                                  builder.getI64IntegerAttr(halfDim + 1));\n-\n-  // Collapse the concat dimension back into the parent.\n-  llvm::SmallVector<int64_t> collapseShape(resultTy.getShape());\n-  collapseShape[halfDim] =\n-      collapseShape[halfDim] + (collapseShape[halfDim] & 1);\n-  Value reshape = mhlo::ReshapeOp::create(\n-      builder, loc, resultTy.clone(collapseShape), concatenate);\n-\n-  // Slice to only the required results.\n-  llvm::SmallVector<int64_t> offset(resultTy.getRank(), 0);\n-  llvm::SmallVector<int64_t> stride(resultTy.getRank(), 1);\n-  Value slice = mhlo::SliceOp::create(\n-      builder, loc, resultTy, reshape, builder.getI64TensorAttr(offset),\n-      builder.getI64TensorAttr(resultTy.getShape()),\n-      builder.getI64TensorAttr(stride));\n-\n-  // Set the new tensor values.\n-  store = setState64(builder, loc, store, newState);\n-  result = slice;\n-\n-  return success();\n-}\n-\n-LogicalResult generateLinalgThreeFry64(OpBuilder &builder, Location loc,\n-                                       ShapedType resultTy, Value &store,\n-                                       Value &result) {\n-  Type resultETy = resultTy.getElementType();\n-  int64_t count = resultTy.getNumElements();\n-\n-  // Extract the stateful values as an i64 and increment the state ahead.\n-  Value initialState = extractState64(builder, loc, store);\n-  if (!initialState) return failure();\n-\n-  std::pair<Value, Value> keys = extractKey32(builder, loc, store);\n-  if (!keys.first || !keys.second) return failure();\n-\n-  ArithOp key0(builder, loc, keys.first);\n-  ArithOp key1(builder, loc, keys.second);\n-\n-  // Compute the number of random i64s generated and increment state.\n-  Value countVal =\n-      arith::ConstantOp::create(builder, loc, builder.getI64IntegerAttr(count));\n-  Value newState = arith::AddIOp::create(builder, loc, initialState, countVal);\n-\n-  // Generate a 1D tensor with for the random values.\n-  Value dest = tensor::EmptyOp::create(builder, loc, ArrayRef<int64_t>({count}),\n-                                       resultETy);\n-  ShapedType destTy = mlir::cast<ShapedType>(dest.getType());\n-\n-  SmallVector<AffineMap> indexingMaps(1, builder.getMultiDimIdentityMap(1));\n-  SmallVector<utils::IteratorType> iterators(1, utils::IteratorType::parallel);\n-\n-  auto random = linalg::GenericOp::create(\n-      builder, loc, destTy, /*inputs=*/ValueRange(),\n-      /*outputs=*/ValueRange{dest},\n-      /*indexingMaps=*/indexingMaps, iterators,\n-      [&](OpBuilder &b, Location nestedLoc, ValueRange) {\n-        // Generate three fry results, fuse, and return an\n-        // i64.\n-        auto split =\n-            runThreeFry2xi32(key0, key1, ArithOp(b, nestedLoc, initialState));\n-        Value result = fuseI32s(split.first, split.second).val();\n-        b.create<linalg::YieldOp>(nestedLoc, result);\n-      });\n-\n-  store = setState64(builder, loc, store, newState);\n-  result = reshapeToTarget(builder, loc, resultTy, random.getResult(0));\n-  return success();\n-}\n-\n-using PhiloxKey = std::pair<ArithOp, ArithOp>;\n-using PhiloxState = std::array<ArithOp, 4>;\n-\n-// Computes high and low words from multiplying 32 bit integers.\n-// Per the paper, mulhi and mullo of the same arguments can be computed\n-// Simultaneously in a single instruction on x86 architectures.\n-std::pair<ArithOp, ArithOp> multiplyHilo(ArithOp counter, ArithOp key) {\n-  counter = counter.extendUI(64);\n-  key = key.extendUI(64);\n-  ArithOp product = counter * key;\n-  ArithOp ci64 = counter.constantI(/*value=*/32, /*bits=*/64);\n-  ArithOp hi = product >> ci64;\n-  hi = hi.truncI(32);\n-  product = product.truncI(32);\n-  return std::pair<ArithOp, ArithOp>{hi, product};\n-}\n-\n-PhiloxState philoxRound(PhiloxState x, PhiloxKey key) {\n-  // These are philox specific constants.\n-  ArithOp m0 = x[0].constantI(0xD2511F53, 32);\n-  ArithOp m1 = x[2].constantI(0xCD9E8D57, 32);\n-  std::pair<ArithOp, ArithOp> p0 = multiplyHilo(x[0], m0);\n-  std::pair<ArithOp, ArithOp> p1 = multiplyHilo(x[2], m1);\n-\n-  PhiloxState state = {p1.first ^ x[1] ^ key.first, p1.second,\n-                       p0.first ^ x[3] ^ key.second, p0.second};\n-  return state;\n-}\n-\n-PhiloxKey raiseKey(PhiloxKey key) {\n-  // These are philox specific constants.\n-  ArithOp w0 = key.first.constantI(0x9E3779B9, 32);\n-  ArithOp w1 = key.first.constantI(0xBB67AE85, 32);\n-  return PhiloxKey{key.first + w0, key.second + w1};\n-}\n-\n-// Implements the Philox 4x32 counter-based PRNG algorithm.\n-// The Philox PRNG has been proposed in:\n-// Salmon et al. SC 2011. Parallel random numbers: as easy as 1, 2, 3.\n-// http://www.thesalmons.org/john/random123/papers/random123sc11.pdf\n-std::array<ArithOp, 4> runPhilox4x32(PhiloxKey key, ArithOp state) {\n-  ArithOp index = state.linalgIndex(0);\n-  index = index.indexCast(64);\n-  index = index + state;\n-\n-  // Split into the 2xi32 used for threefry.\n-  std::pair<ArithOp, ArithOp> input = splitI64(index);\n-  ArithOp input0 = input.first;\n-  ArithOp input1 = input.second;\n-\n-  // We initialize the state as such to match the XLA implementation.\n-  PhiloxState state4 = {input0, input1, key.first, key.second};\n-\n-  // We perform 10 rounds to match the XLA implementation.\n-  static const int kNumRounds = 10;\n-  for (int round = 0; round < kNumRounds; ++round, key = raiseKey(key)) {\n-    state4 = philoxRound(state4, key);\n-  }\n-  return state4;\n-}\n-\n-// Generates an array of primitive type U32 with the given shape containing\n-// random bits generated by the Philox algorithm. Returns the array and the new\n-// state of the random number generator.\n-LogicalResult generateLinalgPhilox32(OpBuilder &builder, Location loc,\n-                                     ShapedType resultTy, Value &store,\n-                                     Value &result) {\n-  Type resultETy = resultTy.getElementType();\n-\n-  Value initialState = extractState64(builder, loc, store);\n-  if (!initialState) return failure();\n-\n-  std::pair<Value, Value> keys = extractKey32(builder, loc, store);\n-  if (!keys.first || !keys.second) return failure();\n-\n-  int64_t numElements = resultTy.getNumElements();\n-  int64_t count = (numElements + 3) / 4;\n-  ShapedType intermediateType =\n-      RankedTensorType::get({count, 1}, resultTy.getElementType());\n-  int64_t concatDim = 1;\n-\n-  // Compute the number of random i64s generated and increment state.\n-  Value countVal =\n-      arith::ConstantOp::create(builder, loc, builder.getI64IntegerAttr(count));\n-  Value newState = arith::AddIOp::create(builder, loc, initialState, countVal);\n-\n-  // set up four outputs\n-  Value dest0 = tensor::EmptyOp::create(builder, loc,\n-                                        ArrayRef<int64_t>({count}), resultETy);\n-  Value dest1 = tensor::EmptyOp::create(builder, loc,\n-                                        ArrayRef<int64_t>({count}), resultETy);\n-  Value dest2 = tensor::EmptyOp::create(builder, loc,\n-                                        ArrayRef<int64_t>({count}), resultETy);\n-  Value dest3 = tensor::EmptyOp::create(builder, loc,\n-                                        ArrayRef<int64_t>({count}), resultETy);\n-\n-  ShapedType destTy = mlir::cast<ShapedType>(dest0.getType());\n-\n-  SmallVector<AffineMap> indexingMaps(4, builder.getMultiDimIdentityMap(1));\n-  SmallVector<utils::IteratorType> iterators(1, utils::IteratorType::parallel);\n-\n-  linalg::GenericOp generic = linalg::GenericOp::create(\n-      builder, loc, TypeRange{destTy, destTy, destTy, destTy},\n-      /*inputs=*/ValueRange(),\n-      /*outputs=*/ValueRange{dest0, dest1, dest2, dest3},\n-      /*indexingMaps=*/indexingMaps, iterators,\n-      [&](OpBuilder &b, Location nestedLoc, ValueRange) {\n-        auto output =\n-            runPhilox4x32(PhiloxKey{ArithOp(b, nestedLoc, keys.first),\n-                                    ArithOp(b, nestedLoc, keys.second)},\n-                          ArithOp(b, nestedLoc, initialState));\n-        auto out0 = output[0].truncI(resultETy.getIntOrFloatBitWidth());\n-        auto out1 = output[1].truncI(resultETy.getIntOrFloatBitWidth());\n-        auto out2 = output[2].truncI(resultETy.getIntOrFloatBitWidth());\n-        auto out3 = output[3].truncI(resultETy.getIntOrFloatBitWidth());\n-        b.create<linalg::YieldOp>(\n-            loc, ValueRange{out0.val(), out1.val(), out2.val(), out3.val()});\n-      });\n-\n-  if (resultTy.getNumElements() == 1) {\n-    result = reshapeToTarget(builder, loc, resultTy, generic.getResult(0));\n-    store = setState64(builder, loc, store, newState);\n-    return success();\n-  }\n-\n-  Value r0 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(0));\n-  Value r1 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(1));\n-  Value r2 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(2));\n-  Value r3 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(3));\n-\n-  Value concatenate =\n-      mhlo::ConcatenateOp::create(builder, loc, ValueRange{r0, r1, r2, r3},\n-                                  builder.getI64IntegerAttr(concatDim));\n-\n-  // Collapse the concat dimension back into the parent.\n-  llvm::SmallVector<int64_t> collapseShape(intermediateType.getShape());\n-  collapseShape[0] = collapseShape[0] * 4;\n-  Value reshapeIntermediate = mhlo::ReshapeOp::create(\n-      builder, loc, resultTy.clone(collapseShape), concatenate);\n-\n-  // Slice to only the required results.\n-  collapseShape[0] = resultTy.getNumElements();\n-\n-  llvm::SmallVector<int64_t> offset(resultTy.getRank(), 0);\n-  llvm::SmallVector<int64_t> stride(resultTy.getRank(), 1);\n-  Value slice = mhlo::SliceOp::create(\n-      builder, loc, intermediateType.clone(collapseShape), reshapeIntermediate,\n-      builder.getI64TensorAttr(offset), builder.getI64TensorAttr(collapseShape),\n-      builder.getI64TensorAttr(stride));\n-  Value reshapeResult = mhlo::ReshapeOp::create(builder, loc, resultTy, slice);\n-\n-  // Set the new tensor values.\n-  store = setState64(builder, loc, store, newState);\n-  result = reshapeResult;\n-\n-  return success();\n-}\n-\n-LogicalResult generateLinalgPhilox64(OpBuilder &builder, Location loc,\n-                                     ShapedType resultTy, Value &store,\n-                                     Value &result) {\n-  Type resultETy = resultTy.getElementType();\n-\n-  Value initialState = extractState64(builder, loc, store);\n-  if (!initialState) return failure();\n-\n-  std::pair<Value, Value> keys = extractKey32(builder, loc, store);\n-  if (!keys.first || !keys.second) return failure();\n-\n-  int64_t numElements = resultTy.getNumElements();\n-  int64_t count = (numElements + 1) / 2;\n-  ShapedType intermediateType =\n-      RankedTensorType::get({count, 1}, resultTy.getElementType());\n-  int64_t concatDim = 1;\n-\n-  // Compute the number of random i64s generated and increment state.\n-  Value countVal =\n-      arith::ConstantOp::create(builder, loc, builder.getI64IntegerAttr(count));\n-  Value newState = arith::AddIOp::create(builder, loc, initialState, countVal);\n-\n-  // set up four outputs\n-  Value dest0 = tensor::EmptyOp::create(builder, loc,\n-                                        ArrayRef<int64_t>({count}), resultETy);\n-  Value dest1 = tensor::EmptyOp::create(builder, loc,\n-                                        ArrayRef<int64_t>({count}), resultETy);\n-  ShapedType destTy = mlir::cast<ShapedType>(dest0.getType());\n-\n-  SmallVector<AffineMap> indexingMaps(2, builder.getMultiDimIdentityMap(1));\n-  SmallVector<utils::IteratorType> iterators(1, utils::IteratorType::parallel);\n-\n-  linalg::GenericOp generic = linalg::GenericOp::create(\n-      builder, loc, TypeRange{destTy, destTy},\n-      /*inputs=*/ValueRange(),\n-      /*outputs=*/ValueRange{dest0, dest1},\n-      /*indexingMaps=*/indexingMaps, iterators,\n-      [&](OpBuilder &b, Location nestedLoc, ValueRange) {\n-        auto output =\n-            runPhilox4x32(PhiloxKey{ArithOp(b, nestedLoc, keys.first),\n-                                    ArithOp(b, nestedLoc, keys.second)},\n-                          ArithOp(b, nestedLoc, initialState));\n-        auto out0 = output[0];\n-        auto out1 = output[1];\n-        auto out2 = output[2];\n-        auto out3 = output[3];\n-        Value result1 = fuseI32s(out0, out1).val();\n-        Value result2 = fuseI32s(out2, out3).val();\n-        b.create<linalg::YieldOp>(loc, ValueRange{result1, result2});\n-      });\n-\n-  if (resultTy.getNumElements() == 1) {\n-    result = reshapeToTarget(builder, loc, resultTy, generic.getResult(0));\n-    store = setState64(builder, loc, store, newState);\n-    return success();\n-  }\n-\n-  Value r0 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(0));\n-  Value r1 =\n-      reshapeToTarget(builder, loc, intermediateType, generic.getResult(1));\n-  Value concatenate = mhlo::ConcatenateOp::create(\n-      builder, loc, ValueRange{r0, r1}, builder.getI64IntegerAttr(concatDim));\n-\n-  // Collapse the concat dimension back into the parent.\n-  llvm::SmallVector<int64_t> collapseShape(intermediateType.getShape());\n-  collapseShape[0] = collapseShape[0] * 2;\n-  Value reshapeIntermediate = mhlo::ReshapeOp::create(\n-      builder, loc, resultTy.clone(collapseShape), concatenate);\n-\n-  // Slice to only the required results.\n-  collapseShape[0] = resultTy.getNumElements();\n-\n-  llvm::SmallVector<int64_t> offset(resultTy.getRank(), 0);\n-  llvm::SmallVector<int64_t> stride(resultTy.getRank(), 1);\n-  Value slice = mhlo::SliceOp::create(\n-      builder, loc, intermediateType.clone(collapseShape), reshapeIntermediate,\n-      builder.getI64TensorAttr(offset), builder.getI64TensorAttr(collapseShape),\n-      builder.getI64TensorAttr(stride));\n-  Value reshapeResult = mhlo::ReshapeOp::create(builder, loc, resultTy, slice);\n-\n-  // Set the new tensor values.\n-  store = setState64(builder, loc, store, newState);\n-  result = reshapeResult;\n-\n-  return success();\n-}\n-\n-}  // namespace\n-\n-LogicalResult generateLinalgThreeFry(OpBuilder &builder, Location loc,\n-                                     ShapedType resultTy, Value &state,\n-                                     Value &result) {\n-  Type eTy = resultTy.getElementType();\n-  if (eTy.getIntOrFloatBitWidth() == 64) {\n-    return generateLinalgThreeFry64(builder, loc, resultTy, state, result);\n-  }\n-\n-  if (eTy.getIntOrFloatBitWidth() == 32) {\n-    return generateLinalgThreeFry32(builder, loc, resultTy, state, result);\n-  }\n-\n-  if (eTy.getIntOrFloatBitWidth() == 16)\n-    return generateLinalgThreeFry32(builder, loc, resultTy, state, result);\n-  {}\n-\n-  return failure();\n-}\n-\n-LogicalResult generateLinalgPhilox(OpBuilder &builder, Location loc,\n-                                   ShapedType resultTy, Value &state,\n-                                   Value &result) {\n-  Type eTy = resultTy.getElementType();\n-  if (eTy.getIntOrFloatBitWidth() == 64) {\n-    return generateLinalgPhilox64(builder, loc, resultTy, state, result);\n-  }\n-\n-  // The 32 bit implementation trancates to result eTy.\n-  if (eTy.getIntOrFloatBitWidth() == 32 || eTy.getIntOrFloatBitWidth() == 16) {\n-    return generateLinalgPhilox32(builder, loc, resultTy, state, result);\n-  }\n-\n-  return failure();\n-}\n-\n-}  // namespace mhlo\n-}  // namespace mlir"
        },
        {
            "sha": "03cd2bb9302a2d41f062f383dd73428f17562da4",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/mhlo_rng_utils.h",
            "status": "removed",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_rng_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_rng_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_rng_utils.h?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,36 +0,0 @@\n-/* Copyright 2023 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef MLIR_HLO_MHLO_UTILS_MHLO_RNG_UTILS_H_\n-#define MLIR_HLO_MHLO_UTILS_MHLO_RNG_UTILS_H_\n-\n-#include \"mhlo/IR/hlo_ops.h\"\n-#include \"mlir/IR/Value.h\"\n-\n-namespace mlir {\n-namespace mhlo {\n-\n-LogicalResult generateLinalgThreeFry(OpBuilder& builder, Location loc,\n-                                     ShapedType resultTy, Value& state,\n-                                     Value& result);\n-\n-LogicalResult generateLinalgPhilox(OpBuilder& builder, Location loc,\n-                                   ShapedType resultTy, Value& state,\n-                                   Value& result);\n-\n-}  // namespace mhlo\n-}  // namespace mlir\n-\n-#endif  // MLIR_HLO_MHLO_UTILS_MHLO_RNG_UTILS_H_"
        },
        {
            "sha": "d70c06ddf65f143788a9546c7e858b5d26b3c89f",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/mhlo_scatter_gather_utils.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 139,
            "changes": 139,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_scatter_gather_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_scatter_gather_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_scatter_gather_utils.cc?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,139 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-// This file implements utilities for the canonicalization of ScatterOp and\n-// GatherOp.\n-\n-#include \"mhlo/utils/mhlo_scatter_gather_utils.h\"\n-\n-#include <utility>\n-\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/Dialect/Utils/ReshapeOpsUtils.h\"\n-#include \"mlir/Support/LLVM.h\"\n-\n-namespace mlir {\n-namespace mhlo {\n-\n-template <typename R>\n-static bool isSeq(R&& range, int64_t start, int64_t size) {\n-  return llvm::equal(range, llvm::seq<int64_t>(start, start + size));\n-}\n-\n-static SmallVector<int64_t> getInversePermutation(\n-    llvm::ArrayRef<int64_t> permutation) {\n-  SmallVector<int64_t> inversePermutation(permutation.size());\n-  for (size_t i = 0, e = permutation.size(); i < e; ++i)\n-    inversePermutation[permutation[i]] = i;\n-  return inversePermutation;\n-}\n-\n-bool isCanonicalScatter(ScatterOp scatterOp) {\n-  if (llvm::any_of(scatterOp.getOperandTypes(), [](Type operandType) {\n-        return !mlir::isa<RankedTensorType>(operandType);\n-      }))\n-    return false;\n-\n-  ScatterDimensionNumbersAttr dimsAttrs =\n-      scatterOp.getScatterDimensionNumbers();\n-  auto indicesType =\n-      mlir::cast<RankedTensorType>(scatterOp.getScatterIndices().getType());\n-  auto operandType =\n-      mlir::cast<RankedTensorType>(scatterOp.getOperands().front().getType());\n-\n-  return indicesType.getRank() == 2 && dimsAttrs.getIndexVectorDim() == 1 &&\n-         dimsAttrs.getInsertedWindowDims().empty() &&\n-         isSeq(dimsAttrs.getUpdateWindowDims(), 1, operandType.getRank()) &&\n-         isSeq(dimsAttrs.getScatterDimsToOperandDims(), 0,\n-               indicesType.getDimSize(1));\n-}\n-\n-bool isCanonicalGather(GatherOp gatherOp) {\n-  const auto& startIndiceShape = gatherOp.getStartIndices().getType();\n-  const auto& dims = gatherOp.getDimensionNumbers();\n-\n-  return startIndiceShape.getRank() == 2 && dims.getIndexVectorDim() == 1 &&\n-         isSeq(dims.getStartIndexMap(), 0, dims.getStartIndexMap().size()) &&\n-         dims.getCollapsedSliceDims().empty() &&\n-         isSeq(dims.getOffsetDims(), 1, dims.getOffsetDims().size());\n-}\n-\n-// Creates a permutation that shuffles dimensions of `operands` to match the\n-// order in the index vector.\n-\n-std::pair<SmallVector<int64_t>, SmallVector<int64_t>>\n-makeOperandStartIndexPermutations(ArrayRef<int64_t> dimMap, int operandRank) {\n-  SmallVector<int64_t> permutation{dimMap};\n-  permutation.reserve(operandRank);\n-  for (int i = 0; i < operandRank; ++i) {\n-    if (!llvm::is_contained(dimMap, i)) permutation.push_back(i);\n-  }\n-  return {permutation, getInversePermutation(permutation)};\n-}\n-\n-Value insertDegenerateDimensions(OpBuilder& b, Location loc, Value tensor,\n-                                 ArrayRef<int64_t> dimsToInsert) {\n-  assert(llvm::is_sorted(dimsToInsert) && \"dimsToInsert must be sorted\");\n-  if (dimsToInsert.empty()) return tensor;\n-  TensorType type = mlir::cast<TensorType>(tensor.getType());\n-  SmallVector<int64_t> newShape{type.getShape()};\n-  for (int64_t dim : dimsToInsert) newShape.insert(newShape.begin() + dim, 1);\n-  auto newType = RankedTensorType::get(newShape, type.getElementType());\n-\n-  return b\n-      .create<tensor::ExpandShapeOp>(\n-          loc, newType, tensor,\n-          *getReassociationIndicesForReshape(type, newType))\n-      .getResult();\n-}\n-\n-// Checks if the indexVectorDim is equal to the rank of `indices`. In that\n-// case add the trailing 1 dimension. If indexVectorDim is not the innermost\n-// dimension, insert transpose to make it so.\n-static Value ensureIndexVectorDimPosition(OpBuilder& b, Location loc,\n-                                          Value indices,\n-                                          int64_t indexVectorDim) {\n-  int64_t indicesRank = mlir::cast<TensorType>(indices.getType()).getRank();\n-  if (indexVectorDim == indicesRank - 1) return indices;\n-  if (indexVectorDim == indicesRank)\n-    return insertDegenerateDimensions(b, loc, indices, {indicesRank});\n-\n-  SmallVector<int64_t> permutation;\n-  for (int64_t i = 0; i < indicesRank; ++i)\n-    if (i != indexVectorDim) permutation.push_back(i);\n-  permutation.push_back(indexVectorDim);\n-  return b.create<TransposeOp>(loc, indices, b.getI64TensorAttr(permutation))\n-      .getResult();\n-}\n-\n-Value canonicalizeStartIndices(OpBuilder& b, Location loc, Value indices,\n-                               int64_t indexVectorDim) {\n-  indices = ensureIndexVectorDimPosition(b, loc, indices, indexVectorDim);\n-\n-  int64_t indicesRank = mlir::cast<TensorType>(indices.getType()).getRank();\n-\n-  if (indicesRank == 2) return indices;\n-  if (indicesRank == 1) return insertDegenerateDimensions(b, loc, indices, {0});\n-\n-  // Insert reshape to collapse all outer dimensions of `Indices`.\n-  SmallVector<ReassociationIndices> reassociation{\n-      llvm::to_vector<2>(llvm::seq<int64_t>(0, indicesRank - 1)),\n-      {indicesRank - 1}};\n-  return b.create<tensor::CollapseShapeOp>(loc, indices, reassociation)\n-      .getResult();\n-}\n-\n-}  // namespace mhlo\n-}  // namespace mlir"
        },
        {
            "sha": "2a4c5d7cb3183e4fefd3aa1e5dc2d90ef82dea00",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/mhlo_scatter_gather_utils.h",
            "status": "removed",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_scatter_gather_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_scatter_gather_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Fmhlo_scatter_gather_utils.h?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,73 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-// This file implements utilities for the canonicalization of ScatterOp and\n-// GatherOp.\n-\n-#ifndef MLIR_HLO_DIALECT_MHLO_TRANSFORMS_MHLO_SCATTER_GATHER_UTILS_H_\n-#define MLIR_HLO_DIALECT_MHLO_TRANSFORMS_MHLO_SCATTER_GATHER_UTILS_H_\n-\n-#include <utility>\n-\n-#include \"mhlo/IR/hlo_ops.h\"\n-\n-namespace mlir {\n-namespace mhlo {\n-\n-// Checks if the scatter has the following characteristics:\n-// - scatter_indices is a two-dimensional tensor\n-// - index_vector_dim is 1\n-// - inserted_window_dims is []\n-// - update_window_dims is [0, 1, ...]\n-// - scatter_dims_to_operand_dims is [0, 1, ...]\n-bool isCanonicalScatter(ScatterOp scatterOp);\n-\n-// Checks if the gather has the following characteristics:\n-// - start_indices is a two-dimensional tensor\n-// - index_vector_dim is 1\n-// - collapsed_slice_dims is []\n-// - offset_dims is [1, 2, ...]\n-// - start_index_map is [0, 1, ...]\n-bool isCanonicalGather(GatherOp gatherOp);\n-\n-// Expands the shape of `tensor`, inserting degenerate dimensions.\n-//\n-// For example tensor<10x4xf32> and dimsToInsert = {0, 2}\n-// will result in tensor<1x10x1x4xf32>.\n-Value insertDegenerateDimensions(OpBuilder& b, Location loc, Value tensor,\n-                                 ArrayRef<int64_t> dimsToInsert);\n-\n-// Given a map from index vector positions to dimension numbers, creates a\n-// permutation that when applied to the operand, let you replace the map with\n-// the identity permutation. Also returns its inverse. In gather, the map is\n-// called `start_index_map`. In scatter, it's `scatter_dims_to_operand_dims`.\n-std::pair<SmallVector<int64_t>, SmallVector<int64_t>>\n-makeOperandStartIndexPermutations(ArrayRef<int64_t> dimMap, int operandRank);\n-\n-// Insert transposes and reshapes to bring `indices` to the 2D shape, where\n-// the dim0 is the product of all dimensions that are not equal to\n-// `indexVectorDim` and dim1 is the index vector dim.\n-//\n-// Examples.\n-//\n-// [a, I, b] will be transposed to [a, b, I], then reshaped into [ab, I].\n-// [a, b] will be reshaped to [a, b, I(1)] and then reshaped into [ab, I(1)].\n-Value canonicalizeStartIndices(OpBuilder& b, Location loc, Value indices,\n-                               int64_t indexVectorDim);\n-\n-}  // namespace mhlo\n-}  // namespace mlir\n-\n-#endif  // MLIR_HLO_DIALECT_MHLO_TRANSFORMS_MHLO_SCATTER_GATHER_UTILS_H_"
        },
        {
            "sha": "e6478ec65dbe66080cd66061387da117354df2d8",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/type_conversion.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc?ref=974f13866a5cc99ac2801c2d31294e96e629d0e9",
            "patch": "@@ -81,26 +81,6 @@ Value materializeCastToIllegal(OpBuilder& builder, Type type,\n       ->getResult(0);\n }\n \n-Value scalarToTensor(OpBuilder& builder, Type type,\n-                                    ValueRange inputs, Location loc) {\n-  assert(inputs.size() == 1);\n-  if (mlir::isa<ShapedType>(inputs.front().getType())) {\n-    return Value();\n-  }\n-  Value result =\n-      tensor::FromElementsOp::create(\n-          builder, loc, RankedTensorType::get({}, inputs.front().getType()),\n-          inputs.front())\n-          .getResult();\n-  // Convert to a signed integer if necessary.\n-  Type elementType = mlir::getElementTypeOrSelf(type);\n-  if (elementType.isInteger() && !elementType.isSignlessInteger()) {\n-    result = UnrealizedConversionCastOp::create(builder, loc, type, result)\n-                 ->getResult(0);\n-  }\n-  return result;\n-}\n-\n // Flatten the given value ranges into a single vector of values.\n SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {\n   SmallVector<Value> result;\n@@ -162,11 +142,6 @@ RemoveSignTypeConverter::RemoveSignTypeConverter() {\n   addTargetMaterialization(materializeCastFromIllegal);\n }\n \n-LinalgTypeConverter::LinalgTypeConverter() : RemoveSignTypeConverter() {\n-  addSourceMaterialization(scalarToTensor);\n-  addTargetMaterialization(scalarToTensor);\n-}\n-\n }  // namespace mhlo\n \n namespace stablehlo {"
        },
        {
            "sha": "363b243c54a50d09820f024e4c2e5e75f47bf363",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/type_conversion.h",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/974f13866a5cc99ac2801c2d31294e96e629d0e9/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.h?ref=974f13866a5cc99ac2801c2d31294e96e629d0e9",
            "patch": "@@ -30,15 +30,6 @@ class RemoveSignTypeConverter : public TypeConverter {\n   RemoveSignTypeConverter();\n };\n \n-// Type converter which adds additional materializations (beyond signless)\n-// that are needed as part of the HloToLinalg conversion patterns.\n-// This is the type converter used by the test pass and is the sanctioned\n-// way to use the underlying patterns.\n-class LinalgTypeConverter : public RemoveSignTypeConverter {\n- public:\n-  LinalgTypeConverter();\n-};\n-\n }  // namespace mhlo\n \n namespace stablehlo {"
        },
        {
            "sha": "2a13bb9964207fc1f12ca0d43d1840ff6bbe767b",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-legalize-rng-to-linalg.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 510,
            "changes": 510,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-rng-to-linalg.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-rng-to-linalg.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-rng-to-linalg.mlir?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,510 +0,0 @@\n-// RUN: mlir-hlo-opt %s --hlo-legalize-to-linalg --split-input-file --canonicalize | \\\n-// RUN: FILECHECK_OPTS=\"\" FileCheck %s\n-\n-func.func @three_fry_i64(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi64>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<THREE_FRY>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi64>)\n-  return %output_state, %output : tensor<2xi64>, tensor<8xi64>\n-}\n-\n-// CHECK-LABEL: func.func @three_fry_i64(\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n-// CHECK-DAG: %[[VAL_1:.*]] = arith.constant 5 : i32\n-// CHECK-DAG: %[[VAL_2:.*]] = arith.constant 4 : i32\n-// CHECK-DAG: %[[VAL_3:.*]] = arith.constant 2 : i32\n-// CHECK-DAG: %[[VAL_4:.*]] = arith.constant 8 : i32\n-// CHECK-DAG: %[[VAL_5:.*]] = arith.constant 24 : i32\n-// CHECK-DAG: %[[VAL_6:.*]] = arith.constant 16 : i32\n-// CHECK-DAG: %[[VAL_7:.*]] = arith.constant 3 : i32\n-// CHECK-DAG: %[[VAL_8:.*]] = arith.constant 29 : i32\n-// CHECK-DAG: %[[VAL_9:.*]] = arith.constant 1 : i32\n-// CHECK-DAG: %[[VAL_10:.*]] = arith.constant 6 : i32\n-// CHECK-DAG: %[[VAL_11:.*]] = arith.constant 26 : i32\n-// CHECK-DAG: %[[VAL_12:.*]] = arith.constant 17 : i32\n-// CHECK-DAG: %[[VAL_13:.*]] = arith.constant 15 : i32\n-// CHECK-DAG: %[[VAL_14:.*]] = arith.constant 19 : i32\n-// CHECK-DAG: %[[VAL_15:.*]] = arith.constant 13 : i32\n-// CHECK-DAG: %[[VAL_16:.*]] = arith.constant 466688986 : i32\n-// CHECK-DAG: %[[VAL_17:.*]] = arith.constant 8 : i64\n-// CHECK-DAG: %[[VAL_18:.*]] = arith.constant 32 : i64\n-// CHECK-DAG: %[[VAL_19:.*]] = arith.constant 0 : index\n-// CHECK-DAG: %[[VAL_20:.*]] = arith.constant 1 : index\n-\n-// CHECK-DAG: %[[VAL_21:.*]] = tensor.extract %[[ARG0]]{{\\[}}%[[VAL_20]]] : tensor<2xi64>\n-// CHECK-DAG: %[[VAL_22:.*]] = tensor.extract %[[ARG0]]{{\\[}}%[[VAL_19]]] : tensor<2xi64>\n-// CHECK-DAG: %[[VAL_23:.*]] = arith.trunci %[[VAL_22]] : i64 to i32\n-// CHECK-DAG: %[[VAL_24:.*]] = arith.shrui %[[VAL_22]], %[[VAL_18]] : i64\n-// CHECK-DAG: %[[VAL_25:.*]] = arith.trunci %[[VAL_24]] : i64 to i32\n-// CHECK-DAG: %[[VAL_26:.*]] = arith.addi %[[VAL_21]], %[[VAL_17]] : i64\n-// CHECK-DAG: %[[VAL_27:.*]] = tensor.empty() : tensor<8xi64>\n-// CHECK-DAG: %[[VAL_28:.*]] = arith.xori %[[VAL_23]], %[[VAL_16]] : i32\n-// CHECK-DAG: %[[VAL_29:.*]] = arith.xori %[[VAL_28]], %[[VAL_25]] : i32\n-\n-// CHECK: %[[GENERIC:.*]] = linalg.generic\n-// CHECK-SAME: {indexing_maps = [#map], iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[VAL_27]] : tensor<8xi64>)\n-\n-// CHECK: ^bb0(%[[VAL_31:.*]]: i64):\n-\n-// CHECK-DAG:   %[[VAL_32:.*]] = linalg.index 0 : index\n-// CHECK-DAG:   %[[VAL_33:.*]] = arith.index_cast %[[VAL_32]] : index to i64\n-// CHECK-DAG:   %[[VAL_34:.*]] = arith.addi %[[VAL_33]], %[[VAL_21]] : i64\n-// CHECK-DAG:   %[[VAL_35:.*]] = arith.trunci %[[VAL_34]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_36:.*]] = arith.shrui %[[VAL_34]], %[[VAL_18]] : i64\n-// CHECK-DAG:   %[[VAL_37:.*]] = arith.trunci %[[VAL_36]] : i64 to i32\n-\n-// CHECK-DAG:   %[[VAL_38:.*]] = arith.addi %[[VAL_35]], %[[VAL_23]] : i32\n-// CHECK-DAG:   %[[VAL_39:.*]] = arith.addi %[[VAL_37]], %[[VAL_25]] : i32\n-\n-// CHECK-DAG:   %[[VAL_40:.*]] = arith.addi %[[VAL_38]], %[[VAL_39]] : i32\n-// CHECK-DAG:   %[[VAL_41:.*]] = arith.shli %[[VAL_39]], %[[VAL_15]] : i32\n-// CHECK-DAG:   %[[VAL_42:.*]] = arith.shrui %[[VAL_39]], %[[VAL_14]] : i32\n-// CHECK-DAG:   %[[VAL_43:.*]] = arith.ori %[[VAL_41]], %[[VAL_42]] : i32\n-// CHECK-DAG:   %[[VAL_44:.*]] = arith.xori %[[VAL_40]], %[[VAL_43]] : i32\n-\n-// CHECK-DAG:   %[[VAL_45:.*]] = arith.addi %[[VAL_40]], %[[VAL_44]] : i32\n-// CHECK-DAG:   %[[VAL_46:.*]] = arith.shli %[[VAL_44]], %[[VAL_13]] : i32\n-// CHECK-DAG:   %[[VAL_47:.*]] = arith.shrui %[[VAL_44]], %[[VAL_12]] : i32\n-// CHECK-DAG:   %[[VAL_48:.*]] = arith.ori %[[VAL_46]], %[[VAL_47]] : i32\n-// CHECK-DAG:   %[[VAL_49:.*]] = arith.xori %[[VAL_45]], %[[VAL_48]] : i32\n-\n-// CHECK-DAG:   %[[VAL_50:.*]] = arith.addi %[[VAL_45]], %[[VAL_49]] : i32\n-// CHECK-DAG:   %[[VAL_51:.*]] = arith.shli %[[VAL_49]], %[[VAL_11]] : i32\n-// CHECK-DAG:   %[[VAL_52:.*]] = arith.shrui %[[VAL_49]], %[[VAL_10]] : i32\n-// CHECK-DAG:   %[[VAL_53:.*]] = arith.ori %[[VAL_51]], %[[VAL_52]] : i32\n-// CHECK-DAG:   %[[VAL_54:.*]] = arith.xori %[[VAL_50]], %[[VAL_53]] : i32\n-\n-// CHECK-DAG:   %[[VAL_55:.*]] = arith.addi %[[VAL_50]], %[[VAL_54]] : i32\n-// CHECK-DAG:   %[[VAL_56:.*]] = arith.shli %[[VAL_54]], %[[VAL_10]] : i32\n-// CHECK-DAG:   %[[VAL_57:.*]] = arith.shrui %[[VAL_54]], %[[VAL_11]] : i32\n-// CHECK-DAG:   %[[VAL_58:.*]] = arith.ori %[[VAL_56]], %[[VAL_57]] : i32\n-// CHECK-DAG:   %[[VAL_59:.*]] = arith.xori %[[VAL_55]], %[[VAL_58]] : i32\n-\n-// CHECK-DAG:   %[[VAL_60:.*]] = arith.addi %[[VAL_55]], %[[VAL_25]] : i32\n-// CHECK-DAG:   %[[VAL_61:.*]] = arith.addi %[[VAL_59]], %[[VAL_29]] : i32\n-// CHECK-DAG:   %[[VAL_62:.*]] = arith.addi %[[VAL_61]], %[[VAL_9]] : i32\n-// CHECK-DAG:   %[[VAL_63:.*]] = arith.addi %[[VAL_60]], %[[VAL_62]] : i32\n-// CHECK-DAG:   %[[VAL_64:.*]] = arith.shli %[[VAL_62]], %[[VAL_12]] : i32\n-// CHECK-DAG:   %[[VAL_65:.*]] = arith.shrui %[[VAL_62]], %[[VAL_13]] : i32\n-// CHECK:   %[[VAL_66:.*]] = arith.ori %[[VAL_64]], %[[VAL_65]] : i32\n-\n-// CHECK:   linalg.yield %[[YIELDED:.*]] : i64\n-\n-// Set the updated state.\n-// CHECK: %[[VAL_159:.*]] = tensor.insert %[[VAL_26]] into %[[ARG0]]{{\\[}}%[[VAL_20]]] : tensor<2xi64>\n-\n-// CHECK: return %[[VAL_159]], %[[GENERIC:.*]] : tensor<2xi64>, tensor<8xi64>\n-\n-// -----\n-\n-func.func @three_fry_i32(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi32>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<THREE_FRY>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi32>)\n-  return %output_state, %output : tensor<2xi64>, tensor<8xi32>\n-}\n-\n-// CHECK-LABEL: func.func @three_fry_i32\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n- //CHECK-DAG: %[[C0:.+]] = arith.constant 0 : index\n- //CHECK-DAG: %[[C1:.+]] = arith.constant 1 : index\n-// CHECK-DAG: %[[C4:.+]] = arith.constant 4 : i64\n-\n-// Check we update state correctly:\n-// CHECK: %[[STATE:.+]] = tensor.extract %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: %[[NEWSTATE:.+]] = arith.addi %[[STATE]], %[[C4]] : i64\n-\n-// CHECK: %[[DEST0:.+]] = tensor.empty() : tensor<4xi32>\n-// CHECK: %[[DEST1:.+]] = tensor.empty() : tensor<4xi32>\n-// CHECK: %[[GENERIC:.+]]:2 = linalg.generic\n-// CHECK-SAME: indexing_maps = [#map, #map]\n-// CHECK-SAME: iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[DEST0]], %[[DEST1]] : tensor<4xi32>, tensor<4xi32>)\n-\n-// CHECK: %expanded = tensor.expand_shape %[[GENERIC]]#0\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi32> into tensor<4x1xi32>\n-\n-// CHECK: %expanded_1 = tensor.expand_shape %[[GENERIC]]#1\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi32> into tensor<4x1xi32>\n-\n-// CHECK: %[[EMPTY:.+]] = tensor.empty() : tensor<4x2xi32>\n-// CHECK: %[[CONCAT:.+]] = linalg.generic\n-// CHECK-SAME: outs(%[[EMPTY]] : tensor<4x2xi32>)\n-\n-// CHECK: %[[COLLAPSE:.+]] = tensor.collapse_shape %[[CONCAT]]\n-// CHECK-SAME{literal}: [[0, 1]] : tensor<4x2xi32> into tensor<8xi32>\n-// CHECK: %[[INSERTED:.+]] = tensor.insert %[[NEWSTATE]] into %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-\n-// CHECK: return %[[INSERTED]], %[[COLLAPSE]]\n-\n-\n-// -----\n-\n-func.func @three_fry_odd_i32(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<7x11xi32>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<THREE_FRY>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<7x11xi32>)\n-  return %output_state, %output : tensor<2xi64>, tensor<7x11xi32>\n-}\n-\n-\n-// CHECK-LABEL: func.func @three_fry_odd_i32\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n- //CHECK-DAG: %[[C0:.+]] = arith.constant 0 : index\n- //CHECK-DAG: %[[C1:.+]] = arith.constant 1 : index\n-// CHECK-DAG: %[[C42:.+]] = arith.constant 42 : i64\n-\n-// Check we update state correctly:\n-// CHECK: %[[STATE:.+]] = tensor.extract %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: %[[NEWSTATE:.+]] = arith.addi %[[STATE]], %[[C42]] : i64\n-\n-// CHECK: %[[DEST0:.+]] = tensor.empty() : tensor<42xi32>\n-// CHECK: %[[DEST1:.+]] = tensor.empty() : tensor<42xi32>\n-// CHECK: %[[GENERIC:.+]]:2 = linalg.generic \n-// CHECK-SAME: indexing_maps = [#map, #map]\n-// CHECK-SAME: iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[DEST0]], %[[DEST1]] : tensor<42xi32>, tensor<42xi32>)\n-\n-// CHECK: %expanded = tensor.expand_shape %[[GENERIC]]#0\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi32> into tensor<7x6x1xi32>\n-\n-// CHECK: %expanded_1 = tensor.expand_shape %[[GENERIC]]#1\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi32> into tensor<7x6x1xi32>\n-\n-// CHECK: %[[EMPTY:.+]] = tensor.empty() : tensor<7x6x2xi32>\n-// CHECK: %[[CONCAT:.+]] = linalg.generic\n-// CHECK-SAME: outs(%[[EMPTY]] : tensor<7x6x2xi32>)\n-\n-// CHECK: %[[COLLAPSE:.+]] = tensor.collapse_shape %10\n-// CHECK-SAME{literal}: [[0], [1, 2]] : tensor<7x6x2xi32> into tensor<7x12xi32>\n-\n-// CHECK: %[[SLICE:.+]] = tensor.extract_slice %[[COLLAPSE]][0, 0] [7, 11] [1, 1]\n-// CHECK: %[[INSERTED:.+]] = tensor.insert %[[NEWSTATE]] into %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: return %[[INSERTED]], %[[SLICE]] : tensor<2xi64>, tensor<7x11xi32>\n-\n-// -----\n-\n-func.func @three_fry_i16(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi16>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<THREE_FRY>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi16>)\n-  return %output_state, %output : tensor<2xi64>, tensor<8xi16>\n-}\n-\n-// CHECK-LABEL: func.func @three_fry_i16\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n- //CHECK-DAG: %[[C0:.+]] = arith.constant 0 : index\n- //CHECK-DAG: %[[C1:.+]] = arith.constant 1 : index\n-// CHECK-DAG: %[[C4:.+]] = arith.constant 4 : i64\n-\n-// Check we update state correctly:\n-// CHECK: %[[STATE:.+]] = tensor.extract %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: %[[NEWSTATE:.+]] = arith.addi %[[STATE]], %[[C4]] : i64\n-\n-// CHECK: %[[DEST0:.+]] = tensor.empty() : tensor<4xi16>\n-// CHECK: %[[DEST1:.+]] = tensor.empty() : tensor<4xi16>\n-// CHECK: %[[GENERIC:.+]]:2 = linalg.generic \n-// CHECK-SAME: indexing_maps = [#map, #map]\n-// CHECK-SAME: iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[DEST0]], %[[DEST1]] : tensor<4xi16>, tensor<4xi16>)\n-\n-// CHECK: %expanded = tensor.expand_shape %[[GENERIC]]#0\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi16> into tensor<4x1xi16>\n-\n-// CHECK: %expanded_1 = tensor.expand_shape %[[GENERIC]]#1\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi16> into tensor<4x1xi16>\n-\n-// CHECK: %[[EMPTY:.+]] = tensor.empty() : tensor<4x2xi16>\n-// CHECK: %[[CONCAT:.+]] = linalg.generic\n-// CHECK-SAME: outs(%[[EMPTY]] : tensor<4x2xi16>)\n-\n-// CHECK: %[[COLLAPSE:.+]] = tensor.collapse_shape %[[CONCAT]]\n-// CHECK-SAME{literal}: [[0, 1]] : tensor<4x2xi16> into tensor<8xi16>\n-// CHECK: %[[INSERTED:.+]] = tensor.insert %[[NEWSTATE]] into %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-\n-// CHECK: return %[[INSERTED]], %[[COLLAPSE]] : tensor<2xi64>, tensor<8xi16>\n-\n-// -----\n-\n-func.func @philox_i64(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi64>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<PHILOX>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi64>)\n-  return %output_state, %output : tensor<2xi64>, tensor<8xi64>\n-}\n-\n-\n-// CHECK-LABEL: func.func @philox_i64(\n-// CHECK-SAME: %[[VAL_0:.*]]: tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi64>) {\n-// CHECK-DAG: %[[VAL_1:.*]] = arith.constant -1767562579 : i32\n-// CHECK-DAG: %[[VAL_2:.*]] = arith.constant -1879881855 : i32\n-// CHECK-DAG: %[[VAL_3:.*]] = arith.constant -616729560 : i32\n-// CHECK-DAG: %[[VAL_4:.*]] = arith.constant -239350328 : i32\n-// CHECK-DAG: %[[VAL_5:.*]] = arith.constant 534103459 : i32\n-// CHECK-DAG: %[[VAL_6:.*]] = arith.constant 1401181199 : i32\n-// CHECK-DAG: %[[VAL_7:.*]] = arith.constant 1684936478 : i32\n-// CHECK-DAG: %[[VAL_8:.*]] = arith.constant -1253254570 : i32\n-// CHECK-DAG: %[[VAL_9:.*]] = arith.constant -1459197799 : i32\n-// CHECK-DAG: %[[VAL_10:.*]] = arith.constant 387276957 : i32\n-// CHECK-DAG: %[[VAL_11:.*]] = arith.constant -308364780 : i32\n-// CHECK-DAG: %[[VAL_12:.*]] = arith.constant 2027808484 : i32\n-// CHECK-DAG: %[[VAL_13:.*]] = arith.constant 842468239 : i32\n-// CHECK-DAG: %[[VAL_14:.*]] = arith.constant -626627285 : i32\n-// CHECK-DAG: %[[VAL_15:.*]] = arith.constant 1993301258 : i32\n-// CHECK-DAG: %[[VAL_16:.*]] = arith.constant 1013904242 : i32\n-// CHECK-DAG: %[[VAL_18:.*]] = arith.constant 3449720151 : i64\n-// CHECK-DAG: %[[VAL_17:.*]] = arith.constant 3528531795 : i64\n-// CHECK-DAG: %[[VAL_19:.*]] = arith.constant 1 : index\n-// CHECK-DAG: %[[VAL_20:.*]] = arith.constant -1150833019 : i32\n-// CHECK-DAG: %[[VAL_21:.*]] = arith.constant -1640531527 : i32\n-// CHECK-DAG: %[[VAL_22:.*]] = arith.constant 4 : i64\n-// CHECK-DAG: %[[VAL_23:.*]] = arith.constant 32 : i64\n-// CHECK-DAG: %[[VAL_24:.*]] = arith.constant 0 : index\n-// CHECK-DAG: %[[VAL_25:.*]] = tensor.extract %[[VAL_0]]{{\\[}}%[[VAL_19]]] : tensor<2xi64>\n-// CHECK-DAG: %[[VAL_26:.*]] = tensor.extract %[[VAL_0]]{{\\[}}%[[VAL_24]]] : tensor<2xi64>\n-// CHECK-DAG: %[[VAL_27:.*]] = arith.trunci %[[VAL_26]] : i64 to i32\n-// CHECK-DAG: %[[VAL_28:.*]] = arith.shrui %[[VAL_26]], %[[VAL_23]] : i64\n-// CHECK-DAG: %[[VAL_29:.*]] = arith.trunci %[[VAL_28]] : i64 to i32\n-// CHECK-DAG: %[[VAL_30:.*]] = arith.addi %[[VAL_25]], %[[VAL_22]] : i64\n-// CHECK-DAG: %[[VAL_31:.*]] = tensor.empty() : tensor<4xi64>\n-// CHECK-DAG: %[[VAL_32:.*]] = tensor.empty() : tensor<4xi64>\n-// CHECK-DAG: %[[VAL_33:.*]]:2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} outs(%[[VAL_31]], %[[VAL_32]] : tensor<4xi64>, tensor<4xi64>) {\n-// CHECK-DAG: ^bb0(%[[VAL_34:.*]]: i64, %[[VAL_35:.*]]: i64):\n-// CHECK-DAG:   %[[VAL_36:.*]] = linalg.index 0 : index\n-// CHECK-DAG:   %[[VAL_37:.*]] = arith.index_cast %[[VAL_36]] : index to i64\n-// CHECK-DAG:   %[[VAL_38:.*]] = arith.addi %[[VAL_37]], %[[VAL_25]] : i64\n-// CHECK-DAG:   %[[VAL_39:.*]] = arith.trunci %[[VAL_38]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_40:.*]] = arith.shrui %[[VAL_38]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_41:.*]] = arith.trunci %[[VAL_40]] : i64 to i32\n-\n-// CHECK-DAG:   %[[VAL_42:.*]] = arith.extui %[[VAL_39]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_43:.*]] = arith.muli %[[VAL_42]], %[[VAL_17]] : i64\n-// CHECK-DAG:   %[[VAL_44:.*]] = arith.shrui %[[VAL_43]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_45:.*]] = arith.trunci %[[VAL_44]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_46:.*]] = arith.trunci %[[VAL_43]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_47:.*]] = arith.extui %[[VAL_27]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_48:.*]] = arith.muli %[[VAL_47]], %[[VAL_18]] : i64\n-// CHECK-DAG:   %[[VAL_49:.*]] = arith.shrui %[[VAL_48]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_50:.*]] = arith.trunci %[[VAL_49]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_51:.*]] = arith.trunci %[[VAL_48]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_52:.*]] = arith.xori %[[VAL_50]], %[[VAL_41]] : i32\n-// CHECK-DAG:   %[[VAL_53:.*]] = arith.xori %[[VAL_52]], %[[VAL_27]] : i32\n-\n-// CHECK-DAG:   %[[VAL_54:.*]] = arith.addi %[[VAL_27]], %[[VAL_21]] : i32\n-// CHECK-DAG:   %[[VAL_55:.*]] = arith.addi %[[VAL_29]], %[[VAL_20]] : i32\n-// CHECK-DAG:   %[[VAL_56:.*]] = arith.extui %[[VAL_53]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_57:.*]] = arith.muli %[[VAL_56]], %[[VAL_17]] : i64\n-// CHECK-DAG:   %[[VAL_58:.*]] = arith.shrui %[[VAL_57]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_59:.*]] = arith.trunci %[[VAL_58]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_60:.*]] = arith.trunci %[[VAL_57]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_61:.*]] = arith.extui %[[VAL_45]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_62:.*]] = arith.muli %[[VAL_61]], %[[VAL_18]] : i64\n-// CHECK-DAG:   %[[VAL_63:.*]] = arith.shrui %[[VAL_62]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_64:.*]] = arith.trunci %[[VAL_63]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_65:.*]] = arith.trunci %[[VAL_62]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_66:.*]] = arith.xori %[[VAL_64]], %[[VAL_51]] : i32\n-// CHECK-DAG:   %[[VAL_67:.*]] = arith.xori %[[VAL_66]], %[[VAL_54]] : i32\n-// CHECK-DAG:   %[[VAL_68:.*]] = arith.xori %[[VAL_59]], %[[VAL_46]] : i32\n-// CHECK-DAG:   %[[VAL_69:.*]] = arith.xori %[[VAL_68]], %[[VAL_55]] : i32\n-\n-// CHECK-DAG:   %[[VAL_70:.*]] = arith.addi %[[VAL_27]], %[[VAL_16]] : i32\n-// CHECK-DAG:   %[[VAL_71:.*]] = arith.addi %[[VAL_29]], %[[VAL_15]] : i32\n-// CHECK-DAG:   %[[VAL_72:.*]] = arith.extui %[[VAL_67]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_73:.*]] = arith.muli %[[VAL_72]], %[[VAL_17]] : i64\n-// CHECK-DAG:   %[[VAL_74:.*]] = arith.shrui %[[VAL_73]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_75:.*]] = arith.trunci %[[VAL_74]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_76:.*]] = arith.trunci %[[VAL_73]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_77:.*]] = arith.extui %[[VAL_69]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_78:.*]] = arith.muli %[[VAL_77]], %[[VAL_18]] : i64\n-// CHECK-DAG:   %[[VAL_79:.*]] = arith.shrui %[[VAL_78]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_80:.*]] = arith.trunci %[[VAL_79]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_81:.*]] = arith.trunci %[[VAL_78]] : i64 to i32\n-// CHECK:   %[[VAL_82:.*]] = arith.xori %[[VAL_80]], %[[VAL_65]] : i32\n-// CHECK-DAG:   %[[VAL_83:.*]] = arith.xori %[[VAL_82]], %[[VAL_70]] : i32\n-// CHECK-DAG:   %[[VAL_84:.*]] = arith.xori %[[VAL_75]], %[[VAL_60]] : i32\n-// CHECK-DAG:   %[[VAL_85:.*]] = arith.xori %[[VAL_84]], %[[VAL_71]] : i32\n-\n-// CHECK-DAG:   %[[VAL_86:.*]] = arith.addi %[[VAL_27]], %[[VAL_14]] : i32\n-// CHECK-DAG:   %[[VAL_87:.*]] = arith.addi %[[VAL_29]], %[[VAL_13]] : i32\n-// CHECK-DAG:   %[[VAL_88:.*]] = arith.extui %[[VAL_83]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_89:.*]] = arith.muli %[[VAL_88]], %[[VAL_17]] : i64\n-// CHECK-DAG:   %[[VAL_90:.*]] = arith.shrui %[[VAL_89]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_91:.*]] = arith.trunci %[[VAL_90]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_92:.*]] = arith.trunci %[[VAL_89]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_93:.*]] = arith.extui %[[VAL_85]] : i32 to i64\n-// CHECK-DAG:   %[[VAL_94:.*]] = arith.muli %[[VAL_93]], %[[VAL_18]] : i64\n-// CHECK-DAG:   %[[VAL_95:.*]] = arith.shrui %[[VAL_94]], %[[VAL_23]] : i64\n-// CHECK-DAG:   %[[VAL_96:.*]] = arith.trunci %[[VAL_95]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_97:.*]] = arith.trunci %[[VAL_94]] : i64 to i32\n-// CHECK-DAG:   %[[VAL_98:.*]] = arith.xori %[[VAL_96]], %[[VAL_81]] : i32\n-// CHECK-DAG:   %[[VAL_99:.*]] = arith.xori %[[VAL_98]], %[[VAL_86]] : i32\n-// CHECK-DAG:   %[[VAL_100:.*]] = arith.xori %[[VAL_91]], %[[VAL_76]] : i32\n-// CHECK-DAG:   %[[VAL_101:.*]] = arith.xori %[[VAL_100]], %[[VAL_87]] : i32\n-\n-// CHECK: linalg.yield %[[YIELDED_1:.*]], %[[YIELDED_2:.*]] : i64, i64\n-// CHECK-DAG: %[[VAL_206:.*]] = tensor.expand_shape %[[VAL_207:.*]]#0 {{\\[\\[}}0, 1]] {{.*}} : tensor<4xi64> into tensor<4x1xi64>\n-// CHECK-DAG: %[[VAL_208:.*]] = tensor.expand_shape %[[VAL_207]]#1 {{\\[\\[}}0, 1]] {{.*}} : tensor<4xi64> into tensor<4x1xi64>\n-// CHECK-DAG: %[[VAL_209:.*]] = tensor.empty() : tensor<4x2xi64>\n-// CHECK-DAG: %[[VAL_213:.*]] = tensor.insert %[[VAL_30]] into %[[VAL_0]]{{\\[}}%[[VAL_19]]] : tensor<2xi64>\n-\n-// CHECK: return %[[VAL_213]], %[[GENERIC:.*]] : tensor<2xi64>, tensor<8xi64>\n-\n-\n-\n-// -----\n-\n-func.func @philox_i32(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi32>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<PHILOX>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi32>)\n-  return %output_state, %output : tensor<2xi64>, tensor<8xi32>\n-}\n-\n-// CHECK-LABEL: func.func @philox_i32\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n-// CHECK-DAG: %[[C0:.+]] = arith.constant 0 : index\n- //CHECK-DAG: %[[C1:.+]] = arith.constant 1 : index\n-// CHECK-DAG: %[[C2:.+]] = arith.constant 2 : i64\n-\n-// Check we update state correctly:\n-// CHECK: %[[STATE:.+]] = tensor.extract %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: %[[NEWSTATE:.+]] = arith.addi %[[STATE]], %[[C2]] : i64\n-\n-// CHECK: %[[DEST0:.+]] = tensor.empty() : tensor<2xi32>\n-// CHECK: %[[DEST1:.+]] = tensor.empty() : tensor<2xi32>\n-// CHECK: %[[DEST2:.+]] = tensor.empty() : tensor<2xi32>\n-// CHECK: %[[DEST3:.+]] = tensor.empty() : tensor<2xi32>\n-// CHECK: %[[GENERIC:.+]]:4 = linalg.generic\n-// CHECK-SAME: indexing_maps = [#map, #map, #map, #map]\n-// CHECK-SAME: iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[DEST0]], %[[DEST1]], %[[DEST2]], %[[DEST3]] : tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>)\n-\n-// CHECK: %[[CONCAT:.+]] = linalg.generic\n-\n-// CHECK: %[[COLLAPSE:.+]] = tensor.collapse_shape %[[CONCAT]]\n-// CHECK-SAME{literal}: [[0, 1]] : tensor<2x4xi32> into tensor<8xi32>\n-// CHECK: %[[INSERTED:.+]] = tensor.insert %[[NEWSTATE]] into %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-\n-// CHECK: return %[[INSERTED]], %[[COLLAPSE]]\n-\n-\n-// -----\n-\n-func.func @philox_i32_odd(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<7x11xi32>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<PHILOX>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<7x11xi32>)\n-  return %output_state, %output : tensor<2xi64>, tensor<7x11xi32>\n-}\n-\n-// CHECK-LABEL: func.func @philox_i32_odd\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n- //CHECK-DAG: %[[C0:.+]] = arith.constant 0 : index\n- //CHECK-DAG: %[[C1:.+]] = arith.constant 1 : index\n-// CHECK-DAG: %[[C20:.+]] = arith.constant 20 : i64\n-\n-// Check we update state correctly:\n-// CHECK: %[[STATE:.+]] = tensor.extract %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: %[[NEWSTATE:.+]] = arith.addi %[[STATE]], %[[C20]] : i64\n-\n-// CHECK: %[[DEST0:.+]] = tensor.empty() : tensor<20xi32>\n-// CHECK: %[[DEST1:.+]] = tensor.empty() : tensor<20xi32>\n-// CHECK: %[[DEST2:.+]] = tensor.empty() : tensor<20xi32>\n-// CHECK: %[[DEST3:.+]] = tensor.empty() : tensor<20xi32>\n-// CHECK: %[[GENERIC:.+]]:4 = linalg.generic\n-// CHECK-SAME: indexing_maps = [#map, #map, #map, #map]\n-// CHECK-SAME: iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[DEST0]], %[[DEST1]], %[[DEST2]], %[[DEST3]] : tensor<20xi32>, tensor<20xi32>, tensor<20xi32>, tensor<20xi32>)\n-\n-\n-// CHECK: %expanded = tensor.expand_shape %[[GENERIC]]#0\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi32> into tensor<4x1xi32>\n-\n-// CHECK: %expanded_1 = tensor.expand_shape %[[GENERIC]]#1\n-// CHECK-SAME{literal}: [[0, 1]] {{.*}} : tensor<4xi32> into tensor<4x1xi32>\n-\n-\n-// CHECK: %[[EMPTY:.+]] = tensor.empty() : tensor<20x4xi32>\n-// CHECK: %[[CONCAT:.+]] = linalg.generic\n-// CHECK-SAME: outs(%[[EMPTY]] : tensor<20x4xi32>)\n-\n-// CHECK: %[[COLLAPSE:.+]] = tensor.collapse_shape %[[CONCAT]]\n-\n-\n-// CHECK: %[[VAL_213:.*]] = tensor.expand_shape %[[COLLAPSE]] {{\\[\\[}}0, 1]] {{.*}} : tensor<80xi32> into tensor<80x1xi32>\n-// CHECK: %[[VAL_214:.*]] = tensor.extract_slice %[[VAL_213]][0, 0] [77, 1] [1, 1] : tensor<80x1xi32> to tensor<77x1xi32>\n-// CHECK: %[[VAL_215:.*]] = tensor.collapse_shape %[[VAL_214]] {{\\[\\[}}0, 1]] : tensor<77x1xi32> into tensor<77xi32>\n-// CHECK: %[[VAL_216:.*]] = tensor.expand_shape %[[VAL_215]] {{\\[\\[}}0, 1]] {{.*}} : tensor<77xi32> into tensor<7x11xi32>\n-// CHECK: %[[VAL_217:.*]] = tensor.insert %[[VAL_30]] into %[[VAL_0]]{{\\[}}%[[VAL_19]]] : tensor<2xi64>\n-// CHECK: return %[[VAL_217]], %[[VAL_216]] : tensor<2xi64>, tensor<7x11xi32>\n-\n-\n-// -----\n-\n-\n-func.func @philox_i64_odd(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<3x5xi64>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<PHILOX>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<3x5xi64>)\n-  return %output_state, %output : tensor<2xi64>, tensor<3x5xi64>\n-}\n-\n-// CHECK-LABEL: func.func @philox_i64_odd\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n- //CHECK-DAG: %[[C0:.+]] = arith.constant 0 : index\n- //CHECK-DAG: %[[C1:.+]] = arith.constant 1 : index\n-// CHECK-DAG: %[[C8:.+]] = arith.constant 8 : i64\n-\n-// Check we update state correctly:\n-// CHECK: %[[STATE:.+]] = tensor.extract %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: %[[NEWSTATE:.+]] = arith.addi %[[STATE]], %[[C8]] : i64\n-\n-// CHECK: %[[DEST2:.+]] = tensor.empty() : tensor<8xi64>\n-// CHECK: %[[DEST3:.+]] = tensor.empty() : tensor<8xi64>\n-// CHECK: %[[GENERIC:.+]]:2 = linalg.generic\n-// CHECK-SAME: indexing_maps = [#map, #map]\n-// CHECK-SAME: iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[DEST2]], %[[DEST3]] : tensor<8xi64>, tensor<8xi64>)\n-\n-// CHECK: %[[EMPTY:.+]] = tensor.empty() : tensor<8x2xi64>\n-// CHECK: %[[CONCAT:.+]] = linalg.generic\n-// CHECK-SAME: outs(%[[EMPTY]] : tensor<8x2xi64>)\n-\n-// CHECK-DAG: %[[COLLAPSE:.+]] = tensor.collapse_shape %[[CONCAT]] {{\\[\\[}}0, 1]] : tensor<8x2xi64> into tensor<16xi64>\n-\n-\n-// CHECK-DAG: %[[EXPANDED:.*]] = tensor.expand_shape %[[COLLAPSE]] {{\\[\\[}}0, 1]] {{.*}} : tensor<16xi64> into tensor<16x1xi64>\n-// CHECK-DAG: %[[SLICE:.*]] = tensor.extract_slice %[[EXPANDED]][0, 0] [15, 1] [1, 1] : tensor<16x1xi64> to tensor<15x1xi64>\n-// CHECK-DAG: %[[EXPAND_2:.*]] = tensor.collapse_shape %[[SLICE]] {{\\[\\[}}0, 1]] : tensor<15x1xi64> into tensor<15xi64>\n-// CHECK-DAG: %[[RESHAPE:.*]] = tensor.expand_shape %[[EXPAND_2]] {{\\[\\[}}0, 1]] {{.*}} : tensor<15xi64> into tensor<3x5xi64>\n-// CHECK-DAG: %[[INSERTED:.+]] = tensor.insert %[[NEWSTATE]] into %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: return %[[INSERTED]], %[[RESHAPE]]\n-\n-// -----\n-\n-func.func @philox_i16(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi16>) {\n-  %output_state, %output = \"mhlo.rng_bit_generator\"(%arg0) <{rng_algorithm = #mhlo.rng_algorithm<PHILOX>}> : (tensor<2xi64>) -> (tensor<2xi64>, tensor<8xi16>)\n-  return %output_state, %output : tensor<2xi64>, tensor<8xi16>\n-}\n-\n-// CHECK-LABEL: func.func @philox_i16\n-// CHECK-SAME:  %[[ARG0:.*]]: tensor<2xi64>\n-\n- //CHECK-DAG: %[[C0:.+]] = arith.constant 0 : index\n- //CHECK-DAG: %[[C1:.+]] = arith.constant 1 : index\n-// CHECK-DAG: %[[C2:.+]] = arith.constant 2 : i64\n-\n-// Check we update state correctly:\n-// CHECK: %[[STATE:.+]] = tensor.extract %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-// CHECK: %[[NEWSTATE:.+]] = arith.addi %[[STATE]], %[[C2]] : i64\n-\n-// CHECK: %[[DEST0:.+]] = tensor.empty() : tensor<2xi16>\n-// CHECK: %[[DEST1:.+]] = tensor.empty() : tensor<2xi16>\n-// CHECK: %[[DEST2:.+]] = tensor.empty() : tensor<2xi16>\n-// CHECK: %[[DEST3:.+]] = tensor.empty() : tensor<2xi16>\n-// CHECK: %[[GENERIC:.+]]:4 = linalg.generic\n-// CHECK-SAME: indexing_maps = [#map, #map, #map, #map]\n-// CHECK-SAME: iterator_types = [\"parallel\"]}\n-// CHECK-SAME: outs(%[[DEST0]], %[[DEST1]], %[[DEST2]], %[[DEST3]] : tensor<2xi16>, tensor<2xi16>, tensor<2xi16>, tensor<2xi16>)\n-\n-// CHECK: %[[EMPTY:.+]] = tensor.empty() : tensor<2x4xi16>\n-// CHECK: %[[CONCAT:.+]] = linalg.generic\n-// CHECK-SAME: outs(%[[EMPTY]] : tensor<2x4xi16>)\n-\n-// CHECK: %[[COLLAPSE:.+]] = tensor.collapse_shape %[[CONCAT]]\n-// CHECK-SAME{literal}: [[0, 1]] : tensor<2x4xi16> into tensor<8xi16>\n-// CHECK: %[[INSERTED:.+]] = tensor.insert %[[NEWSTATE]] into %[[ARG0]][%[[C1]]] : tensor<2xi64>\n-\n-// CHECK: return %[[INSERTED]], %[[COLLAPSE]]"
        },
        {
            "sha": "77c661b5eab80d7ce92263e9c3d8a371b49d6d01",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-legalize-to-arithmetic.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-arithmetic.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-arithmetic.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-arithmetic.mlir?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8",
            "patch": "@@ -1,28 +0,0 @@\n-// RUN: mlir-hlo-opt -hlo-legalize-to-arithmetic %s | FileCheck %s\n-\n-func.func @reshape_unsigned() -> tensor<2xui64> {\n-  %result = mhlo.xla.rng_get_and_update_state {delta = 1 : i64}\n-  func.return %result : tensor<2xui64>\n-}\n-\n-// CHECK: memref.global \"private\" @rng_state : memref<i128>\n-\n-// CHECK-LABEL:     func @reshape_unsigned\n-// CHECK: %[[GLOBAL:.*]] = memref.get_global @rng_state : memref<i128>\n-// CHECK: %[[OLD_SEED:.*]] = memref.load %[[GLOBAL]][] : memref<i128>\n-// CHECK: %[[DELTA:.*]] = arith.constant 1 : i128\n-// CHECK: %[[NEW_SEED:.*]] = arith.addi %[[OLD_SEED]], %[[DELTA]] : i128\n-// CHECK: memref.store %[[NEW_SEED]], %[[GLOBAL]][] : memref<i128>\n-// CHECK: %[[C64:.*]] = arith.constant 64 : i128\n-// CHECK: %[[UPPER_BITS:.*]] = arith.shrui %[[OLD_SEED]], %[[C64]] : i128\n-// CHECK: %[[UPPER_WORD:.*]] = arith.trunci %[[UPPER_BITS]] : i128 to i64\n-// CHECK: %[[C0:.*]] = arith.constant 0 : i128\n-// CHECK: %[[LOWER_BITS:.*]] = arith.shrui %[[OLD_SEED]], %[[C0]] : i128\n-// CHECK: %[[LOWER_WORD:.*]] = arith.trunci %[[LOWER_BITS]] : i128 to i64\n-\n-// CHECK: %[[PACKED:.*]] = tensor.from_elements %[[UPPER_WORD]],\n-// CHECK-SAME:  %[[LOWER_WORD]] : tensor<2xi64>\n-\n-// CHECK: %[[CASTED_RESULT:.*]] = builtin.unrealized_conversion_cast %[[PACKED]]\n-// CHECK-SAME: tensor<2xi64> to tensor<2xui64>\n-// CHECK: return %[[CASTED_RESULT]] : tensor<2xui64>"
        },
        {
            "sha": "8cfdb1a84eed1b71be85ce1633ded5dee14d2de9",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-legalize-to-linalg.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 6125,
            "changes": 6125,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-linalg.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-linalg.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-linalg.mlir?ref=e753ffc374a3f3779b37fa0ce1ed4e1cf944bfb8"
        }
    ],
    "stats": {
        "total": 13172,
        "additions": 0,
        "deletions": 13172
    }
}