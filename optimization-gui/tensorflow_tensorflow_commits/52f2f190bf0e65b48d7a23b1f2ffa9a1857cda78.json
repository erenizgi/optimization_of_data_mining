{
    "author": "ezhulenev",
    "message": "[xla:gpu] Document CollectiveMetadataThunk synchronization choice\n\nPiperOrigin-RevId: 840778207",
    "sha": "52f2f190bf0e65b48d7a23b1f2ffa9a1857cda78",
    "files": [
        {
            "sha": "d902021d887a222fb1d69beee0a394ca4cb17b40",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/52f2f190bf0e65b48d7a23b1f2ffa9a1857cda78/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/52f2f190bf0e65b48d7a23b1f2ffa9a1857cda78/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=52f2f190bf0e65b48d7a23b1f2ffa9a1857cda78",
            "patch": "@@ -220,15 +220,15 @@ absl::Status CollectiveMetadataThunk::Initialize(\n \n   std::vector<se::DeviceMemoryBase> parameters;\n   parameters.reserve(parameters_.size());\n-  for (const CollectiveMetadataThunk::Buffer& parameter : parameters_) {\n+  for (const Buffer& parameter : parameters_) {\n     parameters.push_back(\n         params.buffer_allocations->GetDeviceAddress(parameter.slice));\n   }\n   se::DeviceMemoryBase result_ptr =\n       params.buffer_allocations->GetDeviceAddress(result_);\n \n   TF_ASSIGN_OR_RETURN(void* multimem_address_space,\n-                      SetupMultimem(clique_key, params));\n+                      AllocateMultimem(clique_key, params));\n   return ConstructCollectiveMetadata(\n       std::move(parameters), params.stream, clique_key, multimem_address_space,\n       params.executor->device_ordinal(), result_ptr);\n@@ -239,7 +239,7 @@ absl::Status CollectiveMetadataThunk::ExecuteOnStream(\n   return absl::OkStatus();\n }\n \n-absl::StatusOr<void*> CollectiveMetadataThunk::SetupMultimem(\n+absl::StatusOr<void*> CollectiveMetadataThunk::AllocateMultimem(\n     const GpuCliqueKey& clique_key, const InitializeParams& params) {\n   se::DeviceMemoryBase memory_range;\n   for (const Buffer& parameter : parameters_) {"
        },
        {
            "sha": "5bfed238e205c435c7a31e06a37c5d6856afb60d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/52f2f190bf0e65b48d7a23b1f2ffa9a1857cda78/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/52f2f190bf0e65b48d7a23b1f2ffa9a1857cda78/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h?ref=52f2f190bf0e65b48d7a23b1f2ffa9a1857cda78",
            "patch": "@@ -52,6 +52,7 @@ class CollectiveMetadataThunk : public Thunk {\n         collective_config_(std::move(collective_config)),\n         parameters_(std::move(parameters)),\n         result_(result) {}\n+\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n \n@@ -71,14 +72,20 @@ class CollectiveMetadataThunk : public Thunk {\n       se::DeviceMemoryBase metadata, int64_t num_parameters,\n       int64_t num_devices, int64_t parameter_index);\n \n-  absl::StatusOr<void*> SetupMultimem(const GpuCliqueKey& clique_key,\n-                                      const InitializeParams& params);\n-\n  private:\n+  absl::StatusOr<void*> AllocateMultimem(const GpuCliqueKey& clique_key,\n+                                         const InitializeParams& params);\n+\n   const CollectiveConfig collective_config_;\n   std::vector<Buffer> parameters_;\n   BufferAllocation::Slice result_;\n \n+  // This is a collective multi-mem per stream executor allocated for the thunk\n+  // execution in the initialize stage. In theory multiple XLA executions can\n+  // run concurrently, and this map would lead to a data race, however XLA\n+  // programs with collective operations rely on locking cliques before the\n+  // execution starts, and we never get concurrent executions when collective\n+  // operations are present in the program.\n   absl::Mutex mutex_;\n   absl::flat_hash_map<se::StreamExecutor*, std::shared_ptr<CollectiveMultimem>>\n       collective_multimem_ ABSL_GUARDED_BY(mutex_);"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 13,
        "deletions": 6
    }
}