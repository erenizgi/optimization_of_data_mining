{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU] Add collective emitter to triton/ codegen.\n\nFor now only a naive block level config is emitted.\nAbility to emit triton IR for collectives would be added in a followup.\n\nPiperOrigin-RevId: 831304492",
    "sha": "51552274a6e2601dd7586280738dfdcd23769045",
    "files": [
        {
            "sha": "ef34e90f77bcc53785d8817d247fdb9030e72015",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=51552274a6e2601dd7586280738dfdcd23769045",
            "patch": "@@ -1183,6 +1183,49 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"collective_emitter\",\n+    srcs = [\"collective_emitter.cc\"],\n+    hdrs = [\"collective_emitter.h\"],\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla:types\",\n+        \"//xla:util\",\n+        \"//xla/backends/gpu/runtime:all_reduce\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:collective_ops_utils\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/gpu:all_reduce_kernel\",\n+        \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@llvm-project//llvm:Support\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"collective_emitter_test\",\n+    srcs = [\"collective_emitter_test.cc\"],\n+    deps = [\n+        \":collective_emitter\",\n+        \"//xla:shape_util\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/utils:hlo_query\",\n+        \"//xla/service:hlo_creation_utils\",\n+        \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"tma_utils_test\",\n     srcs = [\"tma_utils_test.cc\"],"
        },
        {
            "sha": "ac137732de64bc54dc50074c6e431c11c07a1c2d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=51552274a6e2601dd7586280738dfdcd23769045",
            "patch": "@@ -0,0 +1,134 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/codegen/triton/collective_emitter.h\"\n+\n+#include <cstdint>\n+#include <optional>\n+\n+#include \"absl/base/casts.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/Support/MathExtras.h\"\n+#include \"xla/backends/gpu/runtime/all_reduce.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::xla::se::gpu::AllReduceStrategy;\n+\n+bool CanAllReduceBeEmitted(const HloAllReduceInstruction* all_reduce,\n+                           ReductionKind reduction_kind, int64_t num_devices,\n+                           int64_t num_elements, PrimitiveType element_type,\n+                           AllReduceStrategy all_reduce_strategy) {\n+  if (!all_reduce->GetModule()\n+           ->config()\n+           .debug_options()\n+           .xla_gpu_unsupported_use_all_reduce_one_shot_kernel()) {\n+    return false;\n+  }\n+  // TODO(b/383125489): Support variadic all-reduce.\n+  if (all_reduce->operand_count() > 1) {\n+    return false;\n+  }\n+  const int64_t byte_size =\n+      num_elements * ShapeUtil::ByteSizeOfPrimitiveType(element_type);\n+  // TODO(b/457333991): Support twoShot for codegen.\n+  if (byte_size >\n+      GetMaxSupportedAllReduceSizeBytes(AllReduceStrategy::kOneShot)) {\n+    return false;\n+  }\n+  return IsAllReduceKernelSupported(num_devices, num_elements, element_type,\n+                                    reduction_kind, all_reduce_strategy);\n+}\n+\n+// The logic here is very naive and assumes a monotonic layout\n+// where only the last dimension is used as a tiling dimension.\n+absl::StatusOr<std::optional<BlockLevelFusionConfig>>\n+GetBlockLevelFusionConfigForAllReduce(\n+    const se::DeviceDescription& device_info,\n+    const HloAllReduceInstruction* all_reduce) {\n+  const std::optional<ReductionKind> reduction_kind =\n+      MatchReductionComputation(all_reduce->called_computations().front());\n+  if (!reduction_kind.has_value()) {\n+    return absl::InternalError(\n+        \"Reduction computation not found for all-reduce.\");\n+  }\n+  const int64_t num_devices = all_reduce->device_list().num_devices_per_group();\n+  const int64_t num_elements =\n+      ShapeUtil::ElementsIn(all_reduce->operand(0)->shape());\n+  const PrimitiveType element_type =\n+      all_reduce->operand(0)->shape().element_type();\n+  // NB: We do not codegen multimem kernels for now.\n+  const AllReduceStrategy all_reduce_strategy =\n+      GetAllReduceStrategy(num_elements, /*is_multimem_enabled=*/false);\n+  if (!CanAllReduceBeEmitted(all_reduce, reduction_kind.value(), num_devices,\n+                             num_elements, element_type, all_reduce_strategy)) {\n+    return std::nullopt;\n+  }\n+  const Shape& output_shape = all_reduce->shape();\n+  const LaunchDimensions launch_dims =\n+      AllReduceLaunchDimensions(num_elements, num_devices, all_reduce_strategy);\n+  BlockLevelFusionConfig block_level_config;\n+  block_level_config.set_num_warps(launch_dims.num_threads_per_block() /\n+                                   WarpSize(device_info));\n+  block_level_config.set_num_ctas(1);    // No block-level clustering.\n+  block_level_config.set_num_stages(1);  // No pipelining of loops.\n+  Tile* output_tile = block_level_config.add_output_tiles();\n+  const int64_t rank = output_shape.dimensions().size();\n+\n+  // Tile sizes are rolled up to power of 2 because this is what the triton\n+  // expects (and consequently the tiling infra).\n+  for (int i = 0; i < rank - 1; ++i) {\n+    output_tile->add_sizes(llvm::PowerOf2Ceil(output_shape.dimensions(i)));\n+  }\n+  // The last dimension is divided amongst blocks.\n+  if (rank > 0) {\n+    const int64_t tile_size =\n+        CeilOfRatio(output_shape.dimensions(rank - 1),\n+                    absl::implicit_cast<int64_t>(launch_dims.num_blocks()));\n+    output_tile->add_sizes(llvm::PowerOf2Ceil(tile_size));\n+  }\n+  return block_level_config;\n+}\n+}  // namespace\n+\n+absl::StatusOr<std::optional<BlockLevelFusionConfig>>\n+GetCollectiveBlockLevelFusionConfig(const se::DeviceDescription& device_info,\n+                                    const HloFusionInstruction* fusion_instr) {\n+  const HloInstruction* root = fusion_instr->fused_expression_root();\n+  switch (root->opcode()) {\n+    case HloOpcode::kAllReduceStart:\n+      return GetBlockLevelFusionConfigForAllReduce(\n+          device_info, Cast<HloAllReduceInstruction>(root));\n+    default:\n+      return std::nullopt;\n+  }\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "fb56e9947d295c9012783bc9d521519a0fc7ad2e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.h",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h?ref=51552274a6e2601dd7586280738dfdcd23769045",
            "patch": "@@ -0,0 +1,38 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_CODEGEN_TRITON_COLLECTIVE_EMITTER_H_\n+#define XLA_BACKENDS_GPU_CODEGEN_TRITON_COLLECTIVE_EMITTER_H_\n+\n+#include <optional>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/types.h\"  // IWYU pragma: keep\n+\n+namespace xla::gpu {\n+\n+// Returns the block level fusion config for the collective kernel.\n+// For now only all-reduce is supported.\n+// If an std::nullopt is returned, it implies that the collective kernel is\n+// not supported and cannot be emitted.\n+absl::StatusOr<std::optional<xla::gpu::BlockLevelFusionConfig>>\n+GetCollectiveBlockLevelFusionConfig(const se::DeviceDescription& device_info,\n+                                    const HloFusionInstruction* fusion_instr);\n+\n+}  // namespace xla::gpu\n+#endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_COLLECTIVE_EMITTER_H_"
        },
        {
            "sha": "93f26fcef841ac70399e9400fc879fd5db194fae",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter_test.cc",
            "status": "added",
            "additions": 153,
            "deletions": 0,
            "changes": 153,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc?ref=51552274a6e2601dd7586280738dfdcd23769045",
            "patch": "@@ -0,0 +1,153 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/codegen/triton/collective_emitter.h\"\n+\n+#include <memory>\n+#include <ostream>\n+#include <string>\n+#include <utility>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/hlo_creation_utils.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::testing::Optional;\n+using ::tsl::proto_testing::EqualsProto;\n+\n+struct ModuleWithFusion {\n+  std::unique_ptr<HloModule> module;\n+\n+  const HloFusionInstruction* FusionInstr() const {\n+    return Cast<HloFusionInstruction>(\n+        module->entry_computation()->root_instruction());\n+  }\n+};\n+\n+class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n+ public:\n+  CollectiveBlockLevelConfigTest()\n+      : device_info_{TestGpuDeviceInfo::RTXH100SXMDeviceInfo()} {}\n+\n+  absl::StatusOr<ModuleWithFusion> BuildModuleWithFusion(\n+      const Shape& shape) const {\n+    const std::string module_str = GetModuleStr(shape);\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n+                        ParseAndReturnVerifiedModule(module_str));\n+    const HloInstruction* instr = hlo_query::GetFirstInstructionWithOpcode(\n+        *module->entry_computation(), HloOpcode::kAllReduceStart);\n+    std::unique_ptr<HloModule> module_with_fusion =\n+        NewModuleWithFusion(instr, HloInstruction::FusionKind::kLoop);\n+    module_with_fusion->mutable_config()\n+        .mutable_debug_options()\n+        .set_xla_gpu_unsupported_use_all_reduce_one_shot_kernel(true);\n+    return ModuleWithFusion{std::move(module_with_fusion)};\n+  }\n+\n+ protected:\n+  static std::string GetModuleStr(const Shape& shape) {\n+    return absl::StrFormat(R\"(\n+      HloModule test\n+      apply_op {\n+        x = f32[] parameter(0)\n+        y = f32[] parameter(1)\n+        ROOT apply_op = f32[] add(x, y)\n+      }\n+\n+      ENTRY test_computation {\n+        param_0 = %1$s parameter(0)\n+        all-reduce-start = %1$s all-reduce-start(param_0), to_apply=apply_op, replica_groups={{0,1}}\n+        ROOT all-reduce-done = %1$s all-reduce-done(all-reduce-start)\n+      }\n+    )\",\n+                           shape.ToString());\n+  }\n+\n+  const se::DeviceDescription device_info_;\n+};\n+\n+struct AllReduceBlockLevelConfigTestCase {\n+  std::string test_name;\n+  Shape shape;\n+  std::string expected_proto;\n+\n+  // Teach gTest how to print the test case.\n+  [[maybe_unused]] friend void PrintTo(\n+      const AllReduceBlockLevelConfigTestCase& test_case, std::ostream* os) {\n+    *os << \"{test_name: \" << test_case.test_name\n+        << \" shape: \" << test_case.shape.ToString()\n+        << \" expected_proto: \" << test_case.expected_proto << \"}\";\n+  }\n+};\n+\n+class CollectiveEmitterParameterizedTest\n+    : public CollectiveBlockLevelConfigTest,\n+      public ::testing::WithParamInterface<AllReduceBlockLevelConfigTestCase> {\n+};\n+\n+TEST_P(CollectiveEmitterParameterizedTest, AllReduceBlockLevelConfig) {\n+  const auto& param = GetParam();\n+  TF_ASSERT_OK_AND_ASSIGN(const auto module_with_fusion,\n+                          BuildModuleWithFusion(param.shape));\n+  TF_ASSERT_OK_AND_ASSIGN(const auto block_level_config,\n+                          GetCollectiveBlockLevelFusionConfig(\n+                              device_info_, module_with_fusion.FusionInstr()));\n+  EXPECT_THAT(block_level_config, Optional(EqualsProto(param.expected_proto)));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    CollectiveEmitterParameterizedTestInstantiation,\n+    CollectiveEmitterParameterizedTest,\n+    ::testing::Values(AllReduceBlockLevelConfigTestCase{\n+                          /* .test_name = */ \"F32_65536\",\n+                          /* .shape = */ ShapeUtil::MakeShape(F32, {65536}),\n+                          /* .expected_proto = */ R\"pb(\n+                            num_warps: 16\n+                            num_ctas: 1\n+                            num_stages: 1\n+                            output_tiles { sizes: 4096 }\n+                          )pb\"},\n+                      AllReduceBlockLevelConfigTestCase{\n+                          /* .test_name= */ \"F32_200_100\",\n+                          /* .shape= */ ShapeUtil::MakeShape(F32, {200, 100}),\n+                          /* .expected_proto= */ R\"pb(\n+                            num_warps: 16\n+                            num_ctas: 1\n+                            num_stages: 1\n+                            output_tiles { sizes: 256 sizes: 16 }\n+                          )pb\"}),\n+    [](const ::testing::TestParamInfo<\n+        CollectiveEmitterParameterizedTest::ParamType>& info) {\n+      return info.param.test_name;\n+    });\n+}  // namespace\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "828978010d5cfde72d4623f5ac8c22e03df28a28",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 1,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc?ref=51552274a6e2601dd7586280738dfdcd23769045",
            "patch": "@@ -42,7 +42,10 @@ namespace xla::gpu {\n \n namespace {\n \n-using ::stream_executor::gpu::AllReduceStrategy;\n+using se::gpu::AllReduceStrategy;\n+static constexpr int64_t kMaxOneShotAllReduceSizeBytes = 256 * 1024;  // 256 KB\n+static constexpr int64_t kMaxTwoShotAllReduceSizeBytes =\n+    2 * 1024 * 1024;  // 2 MB\n \n template <typename T, ReductionKind kReductionKindV>\n class TagRegistry {\n@@ -159,6 +162,28 @@ bool IsElementReductionSupported(PrimitiveType element_type,\n \n }  // namespace\n \n+AllReduceStrategy GetAllReduceStrategy(int64_t input_size_bytes,\n+                                       bool is_multimem_enabled) {\n+  if (input_size_bytes > kMaxOneShotAllReduceSizeBytes) {\n+    return AllReduceStrategy::kTwoShot;\n+  }\n+  if (is_multimem_enabled) {\n+    return AllReduceStrategy::kMultimem;\n+  }\n+  return AllReduceStrategy::kOneShot;\n+}\n+\n+int64_t GetMaxSupportedAllReduceSizeBytes(AllReduceStrategy strategy) {\n+  switch (strategy) {\n+    case AllReduceStrategy::kOneShot:\n+      return kMaxOneShotAllReduceSizeBytes;\n+    case AllReduceStrategy::kTwoShot:\n+      return kMaxTwoShotAllReduceSizeBytes;\n+    case AllReduceStrategy::kMultimem:\n+      return kMaxTwoShotAllReduceSizeBytes;\n+  }\n+}\n+\n LaunchDimensions AllReduceLaunchDimensions(int64_t elements, int64_t num_ranks,\n                                            AllReduceStrategy strategy) {\n   int64_t threads_per_block;"
        },
        {
            "sha": "e2d5ae8bc1d1efd1d29e8fc96996eb218a8707ed",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.h?ref=51552274a6e2601dd7586280738dfdcd23769045",
            "patch": "@@ -30,6 +30,16 @@ limitations under the License.\n \n namespace xla::gpu {\n \n+// Returns the all-reduce strategy for the given input size.\n+// If `is_multimem_enabled` is true, then multimem strategies are also\n+// considered.\n+se::gpu::AllReduceStrategy GetAllReduceStrategy(int64_t input_size_bytes,\n+                                                bool is_multimem_enabled);\n+\n+// Returns the maximum supported all-reduce size in bytes for the given\n+// strategy.\n+int64_t GetMaxSupportedAllReduceSizeBytes(se::gpu::AllReduceStrategy strategy);\n+\n // Returns the launch dimensions for the all-reduce kernel.\n // The launch dimensions are determined by the number of elements and the\n // the all-reduce strategy."
        },
        {
            "sha": "7b51c14027151f6d933717b34db58de827c3ba63",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51552274a6e2601dd7586280738dfdcd23769045/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=51552274a6e2601dd7586280738dfdcd23769045",
            "patch": "@@ -87,28 +87,6 @@ absl::StatusOr<se::DeviceMemoryHandle> AllocateMemory(\n   return local_buffer_alloc;\n };\n \n-AllReduceStrategy GetAllReduceStrategy(int64_t input_size_bytes,\n-                                       bool is_multimem_enabled) {\n-  if (input_size_bytes > kMaxOneShotAllReduceSizeBytes) {\n-    return AllReduceStrategy::kTwoShot;\n-  }\n-  if (is_multimem_enabled) {\n-    return AllReduceStrategy::kMultimem;\n-  }\n-  return AllReduceStrategy::kOneShot;\n-}\n-\n-int64_t GetMaxSupportedAllReduceSizeBytes(AllReduceStrategy strategy) {\n-  switch (strategy) {\n-    case AllReduceStrategy::kOneShot:\n-      return kMaxOneShotAllReduceSizeBytes;\n-    case AllReduceStrategy::kTwoShot:\n-      return kMaxTwoShotAllReduceSizeBytes;\n-    case AllReduceStrategy::kMultimem:\n-      return kMaxTwoShotAllReduceSizeBytes;\n-  }\n-}\n-\n }  // namespace\n \n absl::StatusOr<bool> CollectiveKernelThunk::IsSupported("
        }
    ],
    "stats": {
        "total": 427,
        "additions": 404,
        "deletions": 23
    }
}