{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 845632463",
    "sha": "136038ca4cc2d9745271f40d01366eb7a90eb27f",
    "files": [
        {
            "sha": "ede2bed5eced15c1a8bc1ab46597f6e64f612f75",
            "filename": "tensorflow/core/common_runtime/gradients.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgradients.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgradients.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgradients.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -40,18 +40,18 @@ namespace tensorflow {\n static const char* const kGradientOp = \"SymbolicGradient\";\n static const char* const kNodeLabel = \"Func\";\n \n-string NodeOut::name() const {\n+std::string NodeOut::name() const {\n   if (index == 0) {\n     return node->name();\n   } else {\n-    return strings::StrCat(node->name(), \":\", index);\n+    return absl::StrCat(node->name(), \":\", index);\n   }\n }\n \n DataType NodeOut::dtype() const { return node->output_type(index); }\n \n struct NodeOutHash {\n-  uint64 operator()(const NodeOut& x) const {\n+  uint64_t operator()(const NodeOut& x) const {\n     return Hash64(reinterpret_cast<const char*>(&x.node), sizeof(Node*),\n                   x.index);\n   }\n@@ -334,7 +334,7 @@ NodeOut SymbolicGradientBuilder::SumGradients(const NodeOut& src) {\n   return {add, 0};\n }\n \n-static bool IsPrimitiveOpWithNoGrad(const string& func) {\n+static bool IsPrimitiveOpWithNoGrad(const std::string& func) {\n   gradient::Creator creator;\n   absl::Status s = gradient::GetOpGradientCreator(func, &creator);\n   return s.ok() && (creator == nullptr);"
        },
        {
            "sha": "6eb32e450e1dcfd315271563a052a8d6baf6400b",
            "filename": "tensorflow/core/common_runtime/gradients.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgradients.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgradients.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgradients.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -28,7 +28,7 @@ struct NodeOut {\n   int index;\n \n   // Returns the string name that represents the output of this node.\n-  string name() const;\n+  std::string name() const;\n   // Returns the data type of the output of this node.\n   DataType dtype() const;\n };"
        },
        {
            "sha": "5fb43daa1c0b8d5747a655c06c6e35f0cf8ce620",
            "filename": "tensorflow/core/common_runtime/graph_constructor.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -137,14 +137,14 @@ class GraphConstructor {\n     bool expect_device_spec;\n     bool propagate_device_spec;\n \n-    string prefix;\n+    std::string prefix;\n     bool uniquify_names;\n     bool uniquify_prefix;\n     std::map<TensorId, TensorId> input_map;\n     bool skip_mapped_nodes;\n-    std::vector<string> control_dependencies;\n+    std::vector<std::string> control_dependencies;\n     std::vector<TensorId> return_tensors;\n-    std::vector<string> return_nodes;\n+    std::vector<std::string> return_nodes;\n \n     // TODO(ashankar): This bool exists to separate out functionality required\n     // to make ImportGraphDef a close equivalent of Python's import_graph_def\n@@ -166,7 +166,7 @@ class GraphConstructor {\n     // value to the Node when they are missing from the NodeDef.\n     bool add_default_attributes = true;\n \n-    string default_device;\n+    std::string default_device;\n   };\n \n   typedef absl::Span<const NodeDef* const> NodeDefSlice;\n@@ -288,7 +288,7 @@ class GraphConstructor {\n \n   // Returns a unique version of `original_name`, or `original_name` if it's\n   // already unique in the graph.\n-  string FindUniqueName(absl::string_view original_name);\n+  std::string FindUniqueName(absl::string_view original_name);\n \n   // Decrement pending count for users of `processed` and add the ones that now\n   // have all of their pending inputs satisfied to `ready_`.\n@@ -321,7 +321,7 @@ class GraphConstructor {\n   const VersionDef original_versions_;\n \n   // A copy of opts_.prefix, possibly uniquified.\n-  string prefix_;\n+  std::string prefix_;\n \n   StackTracesMap traces_;\n \n@@ -364,7 +364,7 @@ class GraphConstructor {\n \n   // Imported node names that have been uniquified. The key is the original\n   // name, the value is the new unique name.\n-  gtl::FlatMap<string, string> uniquified_names_;\n+  gtl::FlatMap<std::string, std::string> uniquified_names_;\n \n   // Index of NodeDefs in node_defs_ with all inputs already converted. We use a\n   // (sorted) set so nodes are created in the order defined in the GraphDef.\n@@ -381,10 +381,10 @@ class GraphConstructor {\n   // Used in the conversion from node_defs_ to g_ to represent the ith input\n   // of a node.\n   struct InputInfo {\n-    explicit InputInfo(const string& node_name, Node* n, int i)\n+    explicit InputInfo(const std::string& node_name, Node* n, int i)\n         : name(node_name), node(n), index(i) {}\n     // Use string instead of StringPiece so we don't have to manage lifetime\n-    string name;\n+    std::string name;\n     Node* node;\n     int index;\n \n@@ -402,10 +402,10 @@ class GraphConstructor {\n   // Used in the conversion from node_defs_ to g_ to represent an edge from\n   // the node named 'name' to node 'n'.\n   struct EdgeInfo {\n-    explicit EdgeInfo(const string& name, int i1, Node* n, int i2)\n+    explicit EdgeInfo(const std::string& name, int i1, Node* n, int i2)\n         : src_name(name), src_index(i1), dst_node(n), dst_index(i2) {}\n     // Use string instead of StringPiece so we don't have to manage lifetime\n-    string src_name;\n+    std::string src_name;\n     int src_index;\n     Node* dst_node;\n     int dst_index;\n@@ -594,7 +594,7 @@ bool NodeNameInValues(const std::map<TensorId, TensorId>& input_map,\n   return false;\n }\n \n-bool NodeNameInValues(const std::vector<string>& control_dependencies,\n+bool NodeNameInValues(const std::vector<std::string>& control_dependencies,\n                       const absl::string_view& node_name) {\n   return std::find(control_dependencies.begin(), control_dependencies.end(),\n                    node_name) != control_dependencies.end();\n@@ -632,7 +632,7 @@ absl::Status GraphConstructor::EnsureNoNameCollisions() {\n   }\n   if (prefix_.empty() && opts_.importing && !opts_.uniquify_names) {\n     for (size_t i = 0; i < node_def_count(); ++i) {\n-      const string& name = get_node_def(i).name();\n+      const std::string& name = get_node_def(i).name();\n       if (NameExistsInGraph(name)) {\n         return errors::InvalidArgument(\"Node name '\", name,\n                                        \"' already exists in the Graph\");\n@@ -646,7 +646,7 @@ absl::Status GraphConstructor::EnsureNoNameCollisions() {\n                                      \"' would lead to invalid node names\");\n     }\n     if (NameExistsInGraph(prefix_no_slash) && opts_.uniquify_prefix) {\n-      prefix_ = strings::StrCat(FindUniqueName(prefix_no_slash), \"/\");\n+      prefix_ = absl::StrCat(FindUniqueName(prefix_no_slash), \"/\");\n     }\n   }\n   return absl::OkStatus();\n@@ -668,7 +668,7 @@ absl::Status GraphConstructor::ValidateInputMapAndControlDependencies() {\n                                      \"control edge and non-control edge\");\n     }\n   }\n-  for (const string& node : opts_.control_dependencies) {\n+  for (const std::string& node : opts_.control_dependencies) {\n     if (existing_nodes_.count(node) == 0) {\n       return errors::InvalidArgument(\n           \"node '\", node,\n@@ -727,7 +727,7 @@ absl::Status GraphConstructor::InitFromEdges() {\n   const int num_nodes = node_def_count();\n   pending_count_.reserve(num_nodes);\n   outputs_.resize(num_nodes);\n-  gtl::FlatSet<string> next_iteration_nodes;\n+  gtl::FlatSet<std::string> next_iteration_nodes;\n   for (int n = 0; n < node_def_count(); ++n) {\n     const NodeDef& node_def = get_node_def(n);\n     if (IsNextIteration(node_def)) {\n@@ -752,7 +752,7 @@ absl::Status GraphConstructor::InitFromEdges() {\n           num_control_edges++;\n         } else {\n           TensorId id(ParseTensorName(input_name));\n-          if (next_iteration_nodes.find(string(id.first)) !=\n+          if (next_iteration_nodes.find(std::string(id.first)) !=\n               next_iteration_nodes.end()) {\n             has_loop_back_edge = true;\n           }\n@@ -796,7 +796,7 @@ absl::Status GraphConstructor::ValidateColocationConstraints(\n     return absl::OkStatus();\n   const auto iter = node_def.attr().find(kColocationAttrName);\n   if (iter == node_def.attr().end()) return absl::OkStatus();\n-  for (const string& c : iter->second.list().s()) {\n+  for (const std::string& c : iter->second.list().s()) {\n     absl::string_view s(c);\n     if (absl::ConsumePrefix(&s, kColocationGroupPrefix) &&\n         gdef_nodes_.find(s) == gdef_nodes_.end()) {\n@@ -957,11 +957,11 @@ void GraphConstructor::AddControlDependencies(\n \n   // node_def either has no inputs or all remapped inputs, add the control\n   // dependencies\n-  for (const string& control_dep : opts_.control_dependencies) {\n-    string input = TensorId(control_dep, Graph::kControlSlot).ToString();\n+  for (const std::string& control_dep : opts_.control_dependencies) {\n+    std::string input = TensorId(control_dep, Graph::kControlSlot).ToString();\n     bool found = false;\n     for (int i = node_def->input_size() - 1; i >= 0; --i) {\n-      const string& node_input = node_def->input(i);\n+      const std::string& node_input = node_def->input(i);\n       if (node_input[0] != '^') {\n         // Control inputs are at the end. Break when we reach the non-control\n         // inputs.\n@@ -984,17 +984,17 @@ void GraphConstructor::AddControlDependencies(\n void GraphConstructor::AddPrefixToNodeDef(\n     const std::vector<bool>& input_already_exists, NodeDef* node_def) {\n   if (prefix_.empty()) return;\n-  node_def->set_name(strings::StrCat(prefix_, node_def->name()));\n+  node_def->set_name(absl::StrCat(prefix_, node_def->name()));\n   // Update names of input nodes\n   for (int i = 0; i < node_def->input_size(); ++i) {\n     // Skip remapped inputs (which already exist in g_ and are not being\n     // imported).\n     if (input_already_exists[i]) continue;\n     absl::string_view input(node_def->input(i));\n     if (absl::ConsumePrefix(&input, \"^\")) {\n-      node_def->set_input(i, strings::StrCat(\"^\", prefix_, input));\n+      node_def->set_input(i, absl::StrCat(\"^\", prefix_, input));\n     } else {\n-      node_def->set_input(i, strings::StrCat(prefix_, input));\n+      node_def->set_input(i, absl::StrCat(prefix_, input));\n     }\n   }\n   // Update names of colocation groups\n@@ -1004,7 +1004,7 @@ void GraphConstructor::AddPrefixToNodeDef(\n     for (int i = 0; i < list->s_size(); ++i) {\n       absl::string_view v(list->s(i));\n       if (absl::ConsumePrefix(&v, kColocationGroupPrefix)) {\n-        list->set_s(i, strings::StrCat(kColocationGroupPrefix, prefix_, v));\n+        list->set_s(i, absl::StrCat(kColocationGroupPrefix, prefix_, v));\n       }\n     }\n   }\n@@ -1013,7 +1013,7 @@ void GraphConstructor::AddPrefixToNodeDef(\n void GraphConstructor::UniquifyNames(\n     const std::vector<bool>& input_already_exists, NodeDef* node_def) {\n   if (NameExistsInGraph(node_def->name())) {\n-    string old_name = node_def->name();\n+    std::string old_name = node_def->name();\n     node_def->set_name(FindUniqueName(node_def->name()));\n     uniquified_names_[old_name] = node_def->name();\n     // Note that we don't have to update gdef_nodes_ or gdef_prefixes_ with\n@@ -1028,7 +1028,7 @@ void GraphConstructor::UniquifyNames(\n     // We require that UniquifyNames() is called on all NodeDefs in topological\n     // order. This guarantees that node_def's inputs will already be uniquified\n     // if necessary.\n-    auto iter = uniquified_names_.find(string(id.first));\n+    auto iter = uniquified_names_.find(std::string(id.first));\n     if (iter == uniquified_names_.end()) continue;\n     id.first = iter->second;\n     node_def->set_input(i, id.ToString());\n@@ -1039,18 +1039,18 @@ void GraphConstructor::UpdateUniquifiedColocationNames() {\n   for (const auto& pair : gdef_nodes_) {\n     Node* node = pair.second.node;\n     if (node == nullptr) continue;\n-    std::vector<string> coloc_values;\n+    std::vector<std::string> coloc_values;\n     if (!TryGetNodeAttr(node->attrs(), kColocationAttrName, &coloc_values))\n       continue;\n     bool updated = false;\n     for (size_t i = 0; i < coloc_values.size(); ++i) {\n       absl::string_view val(coloc_values[i]);\n       if (absl::ConsumePrefix(&val, kColocationGroupPrefix)) {\n-        auto name_pair = uniquified_names_.find(string(val));\n+        auto name_pair = uniquified_names_.find(std::string(val));\n         if (name_pair == uniquified_names_.end()) continue;\n         updated = true;\n         coloc_values[i] =\n-            strings::StrCat(kColocationGroupPrefix, name_pair->second);\n+            absl::StrCat(kColocationGroupPrefix, name_pair->second);\n       }\n     }\n     if (updated) {\n@@ -1071,13 +1071,13 @@ bool GraphConstructor::NameExistsInGraphDef(absl::string_view name) {\n   return false;\n }\n \n-string GraphConstructor::FindUniqueName(absl::string_view original_name) {\n-  string name(original_name);\n+std::string GraphConstructor::FindUniqueName(absl::string_view original_name) {\n+  std::string name(original_name);\n   int count = 0;\n   // Check that any generated names don't collide with imported NodeDefs (as\n   // well as nodes in g_).\n   while (NameExistsInGraph(name) || (count > 0 && NameExistsInGraphDef(name))) {\n-    name = strings::StrCat(original_name, \"_\", ++count);\n+    name = absl::StrCat(original_name, \"_\", ++count);\n   }\n   return name;\n }\n@@ -1280,7 +1280,7 @@ absl::Status GraphConstructor::Convert() {\n         return errors::InvalidArgument(out.str());\n       }\n \n-      inputs.emplace_back(string(tensor_id.node()), src_node, src_index);\n+      inputs.emplace_back(std::string(tensor_id.node()), src_node, src_index);\n     }\n \n     if (has_data_back_edge && !IsMerge(node_def)) {"
        },
        {
            "sha": "e527801ea9f426ed1e916fabf554790cb232950d",
            "filename": "tensorflow/core/common_runtime/graph_constructor.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -89,7 +89,7 @@ struct ImportGraphDefOptions {\n   // prefix=\"animals\" and GraphDef contains a node \"bunny\" then the node will be\n   // named \"animals/bunny\" in *g. Must not be already used as a node name or\n   // prefix in the graph.\n-  string prefix;\n+  std::string prefix;\n \n   // If true, imported node names will be modified if their name already exists\n   // in the graph. If false, conflicting names will be treated as an error. Note\n@@ -125,7 +125,7 @@ struct ImportGraphDefOptions {\n   // Note that to avoid creating many redundant control edges, ImportGraphDef()\n   // won't add control edges to nodes that will inherit the dependencies from\n   // other nodes in `gdef`.\n-  std::vector<string> control_dependencies;\n+  std::vector<std::string> control_dependencies;\n \n   // Tensors in `gdef` that will be returned via the ImportGraphDefResults\n   // output parameter of `ImportGraphDef()`. If this list is non-empty, the\n@@ -151,7 +151,7 @@ struct ImportGraphDefOptions {\n   // Unlike `return_tensors`, `input_map` has no effect on the nodes\n   // returned. `return_nodes` must be empty if `skip_mapped_nodes` is true.\n   // TODO(skyewm): make this work with `skip_mapped_nodes` if there's a need.\n-  std::vector<string> return_nodes;\n+  std::vector<std::string> return_nodes;\n \n   // If true, checks that all colocation constraints are nodes in the GraphDef.\n   bool validate_colocation_constraints = true;\n@@ -165,7 +165,7 @@ struct ImportGraphDefOptions {\n   // python API.\n \n   // Try to set default execution device for this grapth.\n-  string default_device;\n+  std::string default_device;\n \n   // If true, propagates a node's assigned device. By default the runtime\n   // will recompute the assigned device every time."
        },
        {
            "sha": "df0c63473b849dea10a17719ebf0f8de1ec6ec35",
            "filename": "tensorflow/core/common_runtime/graph_constructor_fuzz.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor_fuzz.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor_fuzz.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor_fuzz.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -63,10 +63,10 @@ void FuzzGraphEndToEndSimpleFixedInput(const GraphDef& graph_def) {\n   p1.scalar<float>()() = 1.0;\n   Tensor p2(DT_FLOAT, TensorShape({1}));\n   p2.scalar<float>()() = 2.0;\n-  std::vector<std::pair<string, Tensor>> inputs = {{\"Placeholder\", p1},\n-                                                   {\"Placeholder_1\", p2}};\n-  std::vector<string> output_names = {\"O_FUZZ\"};\n-  std::vector<string> target_names;\n+  std::vector<std::pair<std::string, Tensor>> inputs = {{\"Placeholder\", p1},\n+                                                        {\"Placeholder_1\", p2}};\n+  std::vector<std::string> output_names = {\"O_FUZZ\"};\n+  std::vector<std::string> target_names;\n   std::vector<Tensor> outputs;\n   status = sess->Run(inputs, output_names, target_names, &outputs);\n }\n@@ -93,22 +93,22 @@ void FuzzGraphEndToEndAllStatic(const GraphDef& graph_def) {\n     return;\n   }\n \n-  std::vector<std::pair<string, Tensor>> inputs = {};\n-  std::vector<string> output_names = {};\n-  std::vector<string> target_names = {};\n+  std::vector<std::pair<std::string, Tensor>> inputs = {};\n+  std::vector<std::string> output_names = {};\n+  std::vector<std::string> target_names = {};\n   std::vector<Tensor> outputs = {};\n   status = sess->Run(inputs, output_names, target_names, &outputs);\n }\n FUZZ_TEST(GraphDefFuzz, FuzzGraphEndToEndAllStatic);\n \n-Node* FindNode(const string& name, Graph* graph) {\n+Node* FindNode(const std::string& name, Graph* graph) {\n   for (Node* n : graph->nodes()) {\n     if (n->name() == name) return n;\n   }\n   return nullptr;\n }\n \n-bool HasNode(const string& name, Graph* graph) {\n+bool HasNode(const std::string& name, Graph* graph) {\n   return FindNode(name, graph) != nullptr;\n }\n \n@@ -399,10 +399,10 @@ void FuzzGraphEndToEndFDP(std::vector<uint8_t> data) {\n     input_tensors.push_back(input_tensor);\n   }\n \n-  std::vector<std::pair<string, Tensor>> inputs = {{\"N0\", input_tensors[0]},\n-                                                   {\"N1\", input_tensors[1]}};\n-  std::vector<string> output_names = {last_node};\n-  std::vector<string> target_names;\n+  std::vector<std::pair<std::string, Tensor>> inputs = {\n+      {\"N0\", input_tensors[0]}, {\"N1\", input_tensors[1]}};\n+  std::vector<std::string> output_names = {last_node};\n+  std::vector<std::string> target_names;\n   std::vector<Tensor> outputs;\n   s = sess->Run(inputs, output_names, target_names, &outputs);\n   if (!s.ok()) {"
        },
        {
            "sha": "036ee63a354f89e149cc04161d1e6da977d8e3f1",
            "filename": "tensorflow/core/common_runtime/graph_constructor_test.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 51,
            "changes": 107,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_constructor_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -53,22 +53,22 @@ class GraphConstructorTest : public ::testing::Test {\n  protected:\n   GraphConstructorTest() : graph_(OpRegistry::Global()) {}\n \n-  void Convert(const string& gdef_ascii) {\n+  void Convert(const std::string& gdef_ascii) {\n     CHECK(protobuf::TextFormat::ParseFromString(gdef_ascii, &gdef_));\n   }\n \n-  void ExpectError(const string& gdef_ascii,\n-                   const std::vector<string>& expected_error_strs,\n-                   string not_expected_error_str = \"\") {\n+  void ExpectError(const std::string& gdef_ascii,\n+                   const std::vector<std::string>& expected_error_strs,\n+                   std::string not_expected_error_str = \"\") {\n     // Used to verify that errors don't change graph\n-    const string original_graph_description = GraphDebugString();\n+    const std::string original_graph_description = GraphDebugString();\n \n     Convert(gdef_ascii);\n     GraphConstructorOptions opts;\n     absl::Status status = ConvertGraphDefToGraph(opts, gdef_, &graph_);\n     EXPECT_FALSE(status.ok());\n \n-    for (const string& error : expected_error_strs) {\n+    for (const std::string& error : expected_error_strs) {\n       EXPECT_TRUE(absl::StrContains(status.message(), error))\n           << \"Expected to find '\" << error << \"' in \" << status;\n     }\n@@ -82,33 +82,35 @@ class GraphConstructorTest : public ::testing::Test {\n     EXPECT_EQ(original_graph_description, GraphDebugString());\n   }\n \n-  void ExpectError(const string& gdef_ascii, const ImportGraphDefOptions& opts,\n-                   const std::vector<string>& expected_error_strs,\n+  void ExpectError(const std::string& gdef_ascii,\n+                   const ImportGraphDefOptions& opts,\n+                   const std::vector<std::string>& expected_error_strs,\n                    ShapeRefiner* refiner = nullptr,\n                    ImportGraphDefResults* results = nullptr) {\n     // Used to verify that errors don't change graph\n-    const string original_graph_description = GraphDebugString();\n+    const std::string original_graph_description = GraphDebugString();\n \n     Convert(gdef_ascii);\n     absl::Status status =\n         ImportGraphDef(opts, gdef_, &graph_, refiner, results);\n     EXPECT_FALSE(status.ok());\n \n-    for (const string& error : expected_error_strs) {\n+    for (const std::string& error : expected_error_strs) {\n       EXPECT_TRUE(absl::StrContains(status.message(), error))\n           << \"Expected to find '\" << error << \"' in \" << status;\n     }\n \n     EXPECT_EQ(original_graph_description, GraphDebugString());\n   }\n \n-  void ExpectOK(const string& gdef_ascii) {\n+  void ExpectOK(const std::string& gdef_ascii) {\n     Convert(gdef_ascii);\n     GraphConstructorOptions opts;\n     TF_CHECK_OK(ConvertGraphDefToGraph(opts, gdef_, &graph_));\n   }\n \n-  void ExpectOK(const string& gdef_ascii, const ImportGraphDefOptions& opts,\n+  void ExpectOK(const std::string& gdef_ascii,\n+                const ImportGraphDefOptions& opts,\n                 ShapeRefiner* refiner = nullptr,\n                 ImportGraphDefResults* results = nullptr) {\n     Convert(gdef_ascii);\n@@ -125,16 +127,17 @@ class GraphConstructorTest : public ::testing::Test {\n         << graph_.versions().producer();\n   }\n \n-  Node* FindNode(const string& name) {\n+  Node* FindNode(const std::string& name) {\n     for (Node* n : graph_.nodes()) {\n       if (n->name() == name) return n;\n     }\n     return nullptr;\n   }\n \n-  bool HasNode(const string& name) { return FindNode(name) != nullptr; }\n+  bool HasNode(const std::string& name) { return FindNode(name) != nullptr; }\n \n-  bool HasEdge(const string& src, int src_out, const string& dst, int dst_in) {\n+  bool HasEdge(const std::string& src, int src_out, const std::string& dst,\n+               int dst_in) {\n     for (const Edge* e : graph_.edges()) {\n       if (e->src()->name() == src && e->src_output() == src_out &&\n           e->dst()->name() == dst && e->dst_input() == dst_in) {\n@@ -144,11 +147,11 @@ class GraphConstructorTest : public ::testing::Test {\n     return false;\n   }\n \n-  bool HasControlEdge(const string& src, const string& dst) {\n+  bool HasControlEdge(const std::string& src, const std::string& dst) {\n     return HasEdge(src, Graph::kControlSlot, dst, Graph::kControlSlot);\n   }\n \n-  string ColocationGroup(const string& node) {\n+  std::string ColocationGroup(const std::string& node) {\n     Node* n = nullptr;\n     for (Node* ni : graph_.nodes()) {\n       if (ni->name() == node) {\n@@ -159,7 +162,7 @@ class GraphConstructorTest : public ::testing::Test {\n     if (n == nullptr) {\n       return \"\";\n     }\n-    std::vector<string> value;\n+    std::vector<std::string> value;\n     absl::Status s = GetNodeAttr(n->attrs(), kColocationAttrName, &value);\n     if (!s.ok()) {\n       return \"\";\n@@ -171,10 +174,11 @@ class GraphConstructorTest : public ::testing::Test {\n       return \"\";\n     }\n     absl::string_view loc(value[0]);\n-    return absl::ConsumePrefix(&loc, kColocationGroupPrefix) ? string(loc) : \"\";\n+    return absl::ConsumePrefix(&loc, kColocationGroupPrefix) ? std::string(loc)\n+                                                             : \"\";\n   }\n \n-  string GraphDebugString() const {\n+  std::string GraphDebugString() const {\n     return graph_.ToGraphDefDebug().DebugString();\n   }\n \n@@ -232,7 +236,7 @@ REGISTER_OP(\"RequiresCurrentGraphVersion\")\n \n TEST_F(GraphConstructorTest, InvalidNodeName) {\n   auto expect_invalid_name = [this](const char* name) {\n-    ExpectError(strings::StrCat(\"node { name: '\", name, \"' op: 'ABC' }\"),\n+    ExpectError(absl::StrCat(\"node { name: '\", name, \"' op: 'ABC' }\"),\n                 {\"Node name contains invalid characters\"});\n   };\n \n@@ -504,7 +508,7 @@ TEST_F(GraphConstructorTest, ImportGraphThatUsesConstantValueFromInsideLoop) {\n         f.write(str(tf.get_default_graph().as_graph_def()))\n \n   */\n-  const string pb_ascii = R\"EOF(\n+  const std::string pb_ascii = R\"EOF(\n node {\n   name: \"Const\"\n   op: \"Const\"\n@@ -862,7 +866,7 @@ TEST_F(GraphConstructorTest, NoForwardCompatError) {\n }\n \n TEST_F(GraphConstructorTest, LowVersion) {\n-  ExpectError(strings::StrCat(\"versions { producer: \", -1, \" }\"),\n+  ExpectError(absl::StrCat(\"versions { producer: \", -1, \" }\"),\n               {strings::StrCat(\"GraphDef producer version -1 below min \"\n                                \"producer \",\n                                TF_GRAPH_DEF_VERSION_MIN_PRODUCER,\n@@ -872,7 +876,7 @@ TEST_F(GraphConstructorTest, LowVersion) {\n \n TEST_F(GraphConstructorTest, HighVersion) {\n   const int version = TF_GRAPH_DEF_VERSION + 1;\n-  ExpectError(strings::StrCat(\"versions { min_consumer: \", version, \" }\"),\n+  ExpectError(absl::StrCat(\"versions { min_consumer: \", version, \" }\"),\n               {strings::StrCat(\"GraphDef min consumer version \", version,\n                                \" above current version \", TF_GRAPH_DEF_VERSION,\n                                \" for TensorFlow \", TF_VERSION_STRING,\n@@ -885,7 +889,7 @@ TEST_F(GraphConstructorTest, BadVersion) {\n   ExpectError(\n       strings::StrCat(\"versions { producer: \", version, \" bad_consumers: \", bad,\n                       \" }\"),\n-      {strings::StrCat(\n+      {absl::StrCat(\n           \"GraphDef disallows consumer version \", bad,\n           \".  Please upgrade TensorFlow: this version is likely buggy.\")});\n }\n@@ -932,8 +936,8 @@ TEST_F(GraphConstructorTest, Error_ControlEdgeBeforeRealInput) {\n TEST_F(GraphConstructorTest, ImportGraphDef) {\n   GraphDef def;\n   ImportGraphDefOptions opts;\n-  const string& source = graph_.FindNodeId(Graph::kSourceId)->name();\n-  const string& sink = graph_.FindNodeId(Graph::kSinkId)->name();\n+  const std::string& source = graph_.FindNodeId(Graph::kSourceId)->name();\n+  const std::string& sink = graph_.FindNodeId(Graph::kSinkId)->name();\n \n   // Importing an empty graph is fine.\n   absl::Status s = ImportGraphDef(opts, def, &graph_, nullptr);\n@@ -2447,8 +2451,8 @@ TEST_F(GraphConstructorTest, ImportGraphDef_ErrorsDoNoChangeTheGraph) {\n   TF_EXPECT_OK(\n       NodeDefBuilder(\"scope/A\", \"TestParams\").Finalize(def.add_node()));\n   ImportGraphDefOptions opts;\n-  const string& source = graph_.FindNodeId(Graph::kSourceId)->name();\n-  const string& sink = graph_.FindNodeId(Graph::kSinkId)->name();\n+  const std::string& source = graph_.FindNodeId(Graph::kSourceId)->name();\n+  const std::string& sink = graph_.FindNodeId(Graph::kSinkId)->name();\n \n   absl::Status s = ImportGraphDef(opts, def, &graph_, nullptr);\n   ASSERT_EQ(absl::OkStatus(), s) << s;\n@@ -2457,7 +2461,7 @@ TEST_F(GraphConstructorTest, ImportGraphDef_ErrorsDoNoChangeTheGraph) {\n   EXPECT_TRUE(HasControlEdge(source, \"scope/A\"));\n   EXPECT_TRUE(HasControlEdge(\"scope/A\", sink));\n   EXPECT_EQ(3, graph_.num_edges());\n-  const string original_graph_description = GraphDebugString();\n+  const std::string original_graph_description = GraphDebugString();\n \n #define EXPECT_IMPORT_FAILURE(graph_def, options, expected_err)       \\\n   do {                                                                \\\n@@ -2663,10 +2667,10 @@ TEST_F(GraphConstructorTest, ImportGraphDef_FunctionDefs) {\n   p1.scalar<float>()() = 1.0;\n   Tensor p2(DT_FLOAT, TensorShape({1}));\n   p2.scalar<float>()() = 2.0;\n-  std::vector<std::pair<string, Tensor>> inputs = {{\"Placeholder\", p1},\n-                                                   {\"Placeholder_1\", p2}};\n-  std::vector<string> output_names = {\"Foo_d03c39a3\"};\n-  std::vector<string> target_names;\n+  std::vector<std::pair<std::string, Tensor>> inputs = {{\"Placeholder\", p1},\n+                                                        {\"Placeholder_1\", p2}};\n+  std::vector<std::string> output_names = {\"Foo_d03c39a3\"};\n+  std::vector<std::string> target_names;\n   std::vector<Tensor> outputs;\n   TF_ASSERT_OK(sess->Run(inputs, output_names, target_names, &outputs));\n \n@@ -2756,10 +2760,10 @@ TEST_F(GraphConstructorTest, ImportGraphDef_NestedFunctionDefs) {\n   p1.scalar<float>()() = 1.0;\n   Tensor p2(DT_FLOAT, TensorShape({1}));\n   p2.scalar<float>()() = 2.0;\n-  std::vector<std::pair<string, Tensor>> inputs = {{\"Placeholder\", p1},\n-                                                   {\"Placeholder_1\", p2}};\n-  std::vector<string> output_names = {\"Outer_966fa13d\"};\n-  std::vector<string> target_names;\n+  std::vector<std::pair<std::string, Tensor>> inputs = {{\"Placeholder\", p1},\n+                                                        {\"Placeholder_1\", p2}};\n+  std::vector<std::string> output_names = {\"Outer_966fa13d\"};\n+  std::vector<std::string> target_names;\n   std::vector<Tensor> outputs;\n   s = sess->Run(inputs, output_names, target_names, &outputs);\n   ASSERT_TRUE(s.ok()) << s.message();\n@@ -2835,16 +2839,16 @@ TEST_F(GraphConstructorTest, CopyGraph) {\n // Confirms that graph def version in the graph reaches the shape inference\n // function.\n TEST_F(GraphConstructorTest, GraphDefVersionUsedForShapeInference) {\n-  string gdef_ascii = strings::StrCat(R\"EOF(\n+  std::string gdef_ascii = absl::StrCat(R\"EOF(\n       node{ name:\"A\" op:\"RequiresCurrentGraphVersion\" }\n       versions { producer: )EOF\",\n-                                      TF_GRAPH_DEF_VERSION - 1, \"}\");\n+                                        TF_GRAPH_DEF_VERSION - 1, \"}\");\n   ImportGraphDefOptions opts;\n   ExpectError(gdef_ascii, opts, {\"Wrong graph version for shape\"});\n-  gdef_ascii = strings::StrCat(R\"EOF(\n+  gdef_ascii = absl::StrCat(R\"EOF(\n       node{ name:\"A\" op:\"RequiresCurrentGraphVersion\" }\n       versions { producer: )EOF\",\n-                               TF_GRAPH_DEF_VERSION, \"}\");\n+                            TF_GRAPH_DEF_VERSION, \"}\");\n   ExpectOK(gdef_ascii, opts);\n }\n \n@@ -2887,7 +2891,7 @@ TEST_F(GraphConstructorTest, ImportGraphDefProvidedShapeRefinerVersions) {\n   ImportGraphDefOptions opts;\n   // A valid graph at producer version 20, but one\n   // that would not import if the graph_def_version were 21.\n-  string gdef_ascii;\n+  std::string gdef_ascii;\n #if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n   gdef_ascii = strings::StrCat(R\"EOF(\n node {\n@@ -2973,7 +2977,7 @@ versions {\n })EOF\");\n \n #else\n-  gdef_ascii = strings::StrCat(R\"EOF(\n+  gdef_ascii = R\"EOF(\n node {\n   name: \"Sum/input\"\n   op: \"Const\"\n@@ -3054,7 +3058,7 @@ node {\n }\n versions {\n   producer: 20\n-})EOF\");\n+})EOF\";\n #endif\n   // Create a shape refiner with the latest TF_GRAPH_DEF_VERSION.\n   // Importing the graphdef with an existing refiner should\n@@ -3098,7 +3102,7 @@ versions {\n })EOF\");\n \n #else\n-  gdef_ascii = strings::StrCat(R\"EOF(\n+  gdef_ascii = R\"EOF(\n node {\n   name: \"RandomConst\"\n   op: \"Const\"\n@@ -3128,7 +3132,7 @@ node {\n }\n versions {\n   producer: 21\n-})EOF\");\n+})EOF\";\n #endif\n \n   ExpectOK(gdef_ascii, opts, &refiner);\n@@ -3171,7 +3175,7 @@ versions {\n })EOF\");\n \n #else\n-  gdef_ascii = strings::StrCat(R\"EOF(\n+  gdef_ascii = R\"EOF(\n node {\n   name: \"RandomConst2\"\n   op: \"Const\"\n@@ -3201,7 +3205,7 @@ node {\n }\n versions {\n   producer: 17\n-})EOF\");\n+})EOF\";\n #endif\n   ExpectOK(gdef_ascii, opts, &refiner);\n \n@@ -3242,7 +3246,7 @@ TEST_F(GraphConstructorTest, ImportGraphDef_ValidateDefaultDevice) {\n   ImportGraphDefResults res;\n \n   TF_ASSERT_OK(ImportGraphDef(options, gdef, &graph_, nullptr, &res));\n-  std::map<string, string> node2dev;\n+  std::map<std::string, std::string> node2dev;\n   for (Node* n : graph_.nodes()) {\n     node2dev[n->name()] = n->requested_device();\n   }\n@@ -3253,7 +3257,8 @@ TEST_F(GraphConstructorTest, ImportGraphDef_ValidateDefaultDevice) {\n }\n \n TEST_F(GraphConstructorTest, ImportGraphDef_UnknownOps) {\n-  const string pb_ascii = \"node { name: 'op_from_contrib' op: 'OpFromContrib'}\";\n+  const std::string pb_ascii =\n+      \"node { name: 'op_from_contrib' op: 'OpFromContrib'}\";\n   // Try load twice to check for two parts of the error message. We cannot check\n   // for the whole thing in one go because the message includes the hostname.\n   ExpectError(pb_ascii, {\"Op type not registered 'OpFromContrib'\"});"
        },
        {
            "sha": "a3c1d024babae0d5fe7aa4bf2529c22a4f353985",
            "filename": "tensorflow/core/common_runtime/graph_execution_state.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_execution_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_execution_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_execution_state.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -64,7 +64,7 @@ limitations under the License.\n namespace tensorflow {\n \n namespace {\n-bool IsCollectiveV2(const string& op) {\n+bool IsCollectiveV2(const std::string& op) {\n   return op == \"CollectiveReduceV2\" || op == \"CollectiveGatherV2\" ||\n          op == \"CollectiveBcastRecvV2\" || op == \"CollectiveBcastSendV2\" ||\n          op == \"ColectiveReduceScatterV2\" || op == \"ColectiveAllToAllV2\";\n@@ -199,7 +199,7 @@ absl::Status GraphExecutionState::Extend(\n   *gdef.mutable_library() = flib_def_->ToProto();\n \n   // 2. Build an index of the new node names.\n-  std::unordered_set<string> new_names;\n+  std::unordered_set<std::string> new_names;\n   for (const NodeDef& node : extension_def.node()) {\n     new_names.insert(node.name());\n   }\n@@ -315,7 +315,7 @@ namespace {\n \n class TensorConnectionPruneRewrite : public subgraph::PruneRewrite {\n  public:\n-  TensorConnectionPruneRewrite(const string* endpoint_name,\n+  TensorConnectionPruneRewrite(const std::string* endpoint_name,\n                                NodeBuilder::NodeOut from_tensor)\n       : subgraph::PruneRewrite(endpoint_name, nullptr /* device_info */),\n         from_tensor_(std::move(from_tensor)) {}\n@@ -336,8 +336,8 @@ class TensorConnectionPruneRewrite : public subgraph::PruneRewrite {\n     TF_RETURN_IF_ERROR(s);\n \n     TF_RETURN_IF_ERROR(\n-        NodeBuilder(strings::StrCat(\"_identity_\", feed_tensor.node->name(), \"_\",\n-                                    feed_tensor.index),\n+        NodeBuilder(absl::StrCat(\"_identity_\", feed_tensor.node->name(), \"_\",\n+                                 feed_tensor.index),\n                     \"Identity\")\n             .Input(from_tensor_)\n             .Attr(\"T\",\n@@ -355,7 +355,7 @@ class TensorConnectionPruneRewrite : public subgraph::PruneRewrite {\n \n template <class Map>\n absl::Status LookupDevice(\n-    const DeviceSet& device_set, const string& tensor_name,\n+    const DeviceSet& device_set, const std::string& tensor_name,\n     const Map& tensor2device,\n     const tensorflow::DeviceAttributes** out_device_attrs) {\n   *out_device_attrs = nullptr;\n@@ -394,7 +394,7 @@ struct TensorAndDevice {\n \n // Tensors of some DataTypes cannot placed in device memory as feeds or\n // fetches. Validate against a allowlist of those known to work.\n-bool IsFeedAndFetchSupported(DataType dtype, const string& device_type) {\n+bool IsFeedAndFetchSupported(DataType dtype, const std::string& device_type) {\n   // The mechanism for supporting feeds of device-backed Tensors requires\n   // the _Arg kernel to be registered for the corresponding type (and that\n   // the input to the kernel be in device and not host memory).\n@@ -474,8 +474,8 @@ absl::Status ValidateFeedAndFetchDevices(\n absl::Status GetFeedShapeAndTypeFromAttribute(const NodeDef& node,\n                                               PartialTensorShape* shape,\n                                               DataType* type) {\n-  static const gtl::FlatSet<string>* const kHasExplicitShapeAttribute =\n-      CHECK_NOTNULL((new gtl::FlatSet<string>{\n+  static const gtl::FlatSet<std::string>* const kHasExplicitShapeAttribute =\n+      CHECK_NOTNULL((new gtl::FlatSet<std::string>{\n           \"Placeholder\", \"PlaceholderV2\", \"PlaceholderWithDefault\",\n           \"ParallelConcat\", \"ImmutableConst\", \"_ParallelConcatStart\",\n           \"InfeedDequeue\", \"OutfeedDequeue\", \"CollectiveBcastSend\",\n@@ -520,7 +520,7 @@ absl::Status GraphExecutionState::PruneGraph(\n     for (int i = 0; i < options.callable_options.feed_size(); ++i) {\n       // WARNING: feed MUST be a reference, since ArgFeedRewrite and\n       // tensors_and_devices holds on to its address.\n-      const string& feed = options.callable_options.feed(i);\n+      const std::string& feed = options.callable_options.feed(i);\n       const DeviceAttributes* device_info;\n       TF_RETURN_IF_ERROR(LookupDevice(*device_set_, feed,\n                                       options.callable_options.feed_devices(),\n@@ -540,7 +540,7 @@ absl::Status GraphExecutionState::PruneGraph(\n     for (int i = 0; i < options.callable_options.fetch_size(); ++i) {\n       // WARNING: fetch MUST be a reference, since RetvalFetchRewrite and\n       // tensors_and_devices holds on to its address.\n-      const string& fetch = options.callable_options.fetch(i);\n+      const std::string& fetch = options.callable_options.fetch(i);\n       const DeviceAttributes* device_info;\n       TF_RETURN_IF_ERROR(LookupDevice(*device_set_, fetch,\n                                       options.callable_options.fetch_devices(),\n@@ -561,11 +561,11 @@ absl::Status GraphExecutionState::PruneGraph(\n     }\n     const DeviceAttributes* device_info =\n         &device_set_->client_device()->attributes();\n-    for (const string& feed : options.callable_options.feed()) {\n+    for (const std::string& feed : options.callable_options.feed()) {\n       feed_rewrites.emplace_back(\n           new subgraph::RecvFeedRewrite(&feed, device_info));\n     }\n-    for (const string& fetch : options.callable_options.fetch()) {\n+    for (const std::string& fetch : options.callable_options.fetch()) {\n       fetch_rewrites.emplace_back(\n           new subgraph::SendFetchRewrite(&fetch, device_info));\n     }\n@@ -598,7 +598,7 @@ absl::Status GraphExecutionState::PruneGraph(\n         &tensor_connection.to_tensor(), {from_node, from_id.second}));\n   }\n \n-  std::vector<string> target_node_names(\n+  std::vector<std::string> target_node_names(\n       options.callable_options.target().begin(),\n       options.callable_options.target().end());\n   TF_RETURN_IF_ERROR(subgraph::RewriteGraphForExecution(\n@@ -699,7 +699,7 @@ absl::Status GraphExecutionState::OptimizeGraph(\n           options.callable_options.tensor_connection().empty())) {\n       std::vector<SafeTensorId> feeds;\n \n-      for (const string& feed : options.callable_options.feed()) {\n+      for (const std::string& feed : options.callable_options.feed()) {\n         feeds.emplace_back(ParseTensorName(feed));\n       }\n       for (const TensorConnection& tensor_connection :\n@@ -830,7 +830,7 @@ absl::Status GraphExecutionState::OptimizeGraph(\n     *optimized_flib = std::make_unique<FunctionLibraryDefinition>(*flib_def);\n \n     for (const FunctionDef& fdef : new_graph.library().function()) {\n-      const string& func_name = fdef.signature().name();\n+      const std::string& func_name = fdef.signature().name();\n \n       if ((*optimized_flib)->Contains(func_name)) {\n         VLOG(3) << \"Replace function: name=\" << func_name;\n@@ -864,7 +864,7 @@ absl::Status GraphExecutionState::OptimizeGraph(\n absl::Status GraphExecutionState::BuildGraph(\n     const BuildGraphOptions& options, std::unique_ptr<ClientGraph>* out) {\n   VLOG(1) << \"BuildGraph\";\n-  const uint64 start_time_usecs = Env::Default()->NowMicros();\n+  const uint64_t start_time_usecs = Env::Default()->NowMicros();\n   if (!graph_) {\n     // It is only valid to call this method directly when the original graph\n     // was created with the option `place_pruned_graph == false`.\n@@ -922,7 +922,7 @@ absl::Status GraphExecutionState::BuildGraph(\n     // nodes in the Graph and FunctionLibraryDefinition for collective ops and\n     // if found, initialize a collective_graph_key as a hash of the ordered set\n     // of instance keys.\n-    std::set<int32> instance_key_set;\n+    std::set<int32_t> instance_key_set;\n     bool has_collective_v2 = false;\n     for (Node* node : optimized_graph->nodes()) {\n       if (node->IsCollective()) {\n@@ -952,7 +952,7 @@ absl::Status GraphExecutionState::BuildGraph(\n       }\n     }\n     if (!instance_key_set.empty()) {\n-      uint64 hash = 0x8774aa605c729c72ULL;\n+      uint64_t hash = 0x8774aa605c729c72ULL;\n       for (int32_t instance_key : instance_key_set) {\n         hash = Hash64Combine(instance_key, hash);\n       }"
        },
        {
            "sha": "a718b57063f10d59b390eb5eacb5a69084c34e05",
            "filename": "tensorflow/core/common_runtime/graph_execution_state.h",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_execution_state.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_execution_state.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_execution_state.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -43,10 +43,10 @@ struct GraphExecutionStateOptions {\n   const DeviceSet* device_set = nullptr;\n   const SessionOptions* session_options = nullptr;\n   // Unique session identifier. Can be empty.\n-  string session_handle;\n+  std::string session_handle;\n   // A map from node name to device name, representing the unchangeable\n   // placement of stateful nodes.\n-  std::unordered_map<string, string> stateful_placements;\n+  std::unordered_map<std::string, std::string> stateful_placements;\n   // Whether to run Placer on the graph.\n   bool run_placer = true;\n \n@@ -166,7 +166,7 @@ class GraphExecutionState {\n   const FunctionLibraryDefinition& flib_def() const { return *flib_def_; }\n \n   // Returns the node with the given name, or null if it does not exist.\n-  const Node* get_node_by_name(const string& name) const {\n+  const Node* get_node_by_name(const std::string& name) const {\n     NodeNameToCostIdMap::const_iterator iter =\n         node_name_to_cost_id_map_.find(name);\n     if (iter != node_name_to_cost_id_map_.end()) {\n@@ -178,7 +178,7 @@ class GraphExecutionState {\n \n   // Returns the map of stateful placements as a map of\n   // node name to placement string.\n-  std::unordered_map<string, string> GetStatefulPlacements() const {\n+  std::unordered_map<std::string, std::string> GetStatefulPlacements() const {\n     return stateful_placements_;\n   }\n \n@@ -194,8 +194,9 @@ class GraphExecutionState {\n   // is true, such as \"params\" and \"queue\" nodes.  Once placed these\n   // nodes can not be moved to a different device.  Maps node names to\n   // device names.\n-  std::unordered_map<string, string> stateful_placements_;  // Immutable after\n-                                                            // ctor.\n+  std::unordered_map<std::string, std::string>\n+      stateful_placements_;  // Immutable after\n+                             // ctor.\n   void SaveStatefulNodes(Graph* graph);\n   void RestoreStatefulNodes(Graph* graph);\n \n@@ -215,7 +216,7 @@ class GraphExecutionState {\n   const DeviceSet* device_set_;            // Not owned\n   const SessionOptions* session_options_;  // Not owned\n   // Unique session identifier. Can be empty.\n-  string session_handle_;\n+  std::string session_handle_;\n \n   // Map from name to Node for the full graph in placed_.\n   NodeNameToCostIdMap node_name_to_cost_id_map_;"
        },
        {
            "sha": "746c080e4d3f669af1beb6cb337a1bd864af35d5",
            "filename": "tensorflow/core/common_runtime/graph_optimizer.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_optimizer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_optimizer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_optimizer.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -36,8 +36,8 @@ class GraphOptimizer {\n     // pass may replace a node with a different node of the same name that has a\n     // different number of outputs, or outputs with different known shapes.\n     // TODO(b/65453533) introduce a unique way to name nodes in a graph.\n-    std::unordered_map<string, std::vector<PartialTensorShape>>* shape_map =\n-        nullptr;\n+    std::unordered_map<std::string, std::vector<PartialTensorShape>>*\n+        shape_map = nullptr;\n \n     // If not null then only nodes for which cse_consider_fn returns true will\n     // be considered for CSE."
        },
        {
            "sha": "8379c126e22711aa3fdc0866e88cafb18da79fb2",
            "filename": "tensorflow/core/common_runtime/graph_runner.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -58,7 +58,7 @@ class SimpleRendezvous : public RendezvousInterface {\n     }\n \n     mutex_lock l(mu_);\n-    string edge_name(parsed.edge_name);\n+    std::string edge_name(parsed.edge_name);\n     if (table_.count(edge_name) > 0) {\n       return errors::Internal(\"Send of an already sent tensor\");\n     }\n@@ -71,7 +71,7 @@ class SimpleRendezvous : public RendezvousInterface {\n     Tensor tensor;\n     absl::Status status = absl::OkStatus();\n     {\n-      string key(parsed.edge_name);\n+      std::string key(parsed.edge_name);\n       mutex_lock l(mu_);\n       if (table_.count(key) <= 0) {\n         status = errors::Internal(\"Did not find key \", key);\n@@ -85,7 +85,7 @@ class SimpleRendezvous : public RendezvousInterface {\n   void StartAbort(const absl::Status& status) override {}\n \n  private:\n-  typedef std::unordered_map<string, Tensor> Table;\n+  typedef std::unordered_map<std::string, Tensor> Table;\n \n   mutex mu_;\n   Table table_ TF_GUARDED_BY(mu_);\n@@ -103,7 +103,7 @@ GraphRunner::~GraphRunner() {}\n absl::Status GraphRunner::Run(Graph* graph,\n                               FunctionLibraryRuntime* function_library,\n                               const NamedTensorList& inputs,\n-                              const std::vector<string>& output_names,\n+                              const std::vector<std::string>& output_names,\n                               std::vector<Tensor>* outputs) {\n   if (device_ == nullptr) {\n     return errors::NotFound(\"Cannot find a device for GraphRunner.\");\n@@ -130,12 +130,12 @@ absl::Status GraphRunner::Run(Graph* graph,\n   SimpleRendezvous rendez;\n \n   // Extract the input names and keys, and feed in the inputs.\n-  std::vector<string> input_names;\n+  std::vector<std::string> input_names;\n   for (const auto& in : inputs) {\n-    const string& tensor_name = in.first;\n+    const std::string& tensor_name = in.first;\n     input_names.emplace_back(tensor_name);\n-    string full_key = Rendezvous::CreateKey(\"/device:CPU:0\", 1, \"/device:CPU:1\",\n-                                            tensor_name, FrameAndIter(0, 0));\n+    std::string full_key = Rendezvous::CreateKey(\n+        \"/device:CPU:0\", 1, \"/device:CPU:1\", tensor_name, FrameAndIter(0, 0));\n     Rendezvous::ParsedKey parsed;\n     TF_RETURN_IF_ERROR(Rendezvous::ParseKey(full_key, &parsed));\n     TF_RETURN_IF_ERROR(rendez.Send(parsed, Rendezvous::Args(), in.second,\n@@ -194,7 +194,7 @@ absl::Status GraphRunner::Run(Graph* graph,\n \n   outputs->resize(output_names.size());\n   for (size_t i = 0; i < output_names.size(); ++i) {\n-    const string& output_key =\n+    const std::string& output_key =\n         Rendezvous::CreateKey(\"/device:CPU:0\", 1, \"/device:CPU:1\",\n                               output_names[i], FrameAndIter(0, 0));\n     Rendezvous::ParsedKey parsed;"
        },
        {
            "sha": "3f651727db59231460b6204aa92615cf86287e59",
            "filename": "tensorflow/core/common_runtime/graph_runner.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -58,10 +58,10 @@ class GraphRunner {\n   //\n   // REQUIRES: `graph`, `env`, and `outputs` are not nullptr.\n   // `function_library` may be nullptr.\n-  typedef std::vector<std::pair<string, Tensor>> NamedTensorList;\n+  typedef std::vector<std::pair<std::string, Tensor>> NamedTensorList;\n   absl::Status Run(Graph* graph, FunctionLibraryRuntime* function_library,\n                    const NamedTensorList& inputs,\n-                   const std::vector<string>& output_names,\n+                   const std::vector<std::string>& output_names,\n                    std::vector<Tensor>* outputs);\n \n  private:"
        },
        {
            "sha": "2d41bc455d532231a97da255a3cf18e1700cd7ce",
            "filename": "tensorflow/core/common_runtime/graph_runner_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_runner_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -64,8 +64,8 @@ TEST(GraphRunnerTest, DeepCopy) {\n   Tensor p2_data(DT_FLOAT, TensorShape({}));\n   p1_data.scalar<float>()() = 1.0f;\n   p2_data.scalar<float>()() = 2.0f;\n-  std::vector<std::pair<string, Tensor>> inputs = {{\"p1:0\", p1_data},\n-                                                   {\"p2:0\", p2_data}};\n+  std::vector<std::pair<std::string, Tensor>> inputs = {{\"p1:0\", p1_data},\n+                                                        {\"p2:0\", p2_data}};\n \n   // Create and destroy the GraphRunner, and ensure that the outputs are\n   // consumable beyond the lifetime of GraphRunner.\n@@ -102,8 +102,8 @@ TEST(GraphRunnerTest, FeedAndFetch) {\n   Tensor p2_data(DT_FLOAT, TensorShape({}));\n   p1_data.scalar<float>()() = 1.0f;\n   p2_data.scalar<float>()() = 2.0f;\n-  std::vector<std::pair<string, Tensor>> inputs = {{\"p1:0\", p1_data},\n-                                                   {\"p2:0\", p2_data}};\n+  std::vector<std::pair<std::string, Tensor>> inputs = {{\"p1:0\", p1_data},\n+                                                        {\"p2:0\", p2_data}};\n \n   GraphRunner graph_runner(Env::Default());\n   std::vector<Tensor> outputs;"
        },
        {
            "sha": "65359febf97937fcc03ac36ebdf1c37dbef7364b",
            "filename": "tensorflow/core/common_runtime/graph_view.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_view.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_view.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_view.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -40,12 +40,12 @@ limitations under the License.\n \n namespace tensorflow {\n \n-string NodeItem::DebugString() const {\n-  string ret = strings::StrCat(\"{name:'\", kernel->name(), \"' id:\", node_id);\n+std::string NodeItem::DebugString() const {\n+  std::string ret = absl::StrCat(\"{name:'\", kernel->name(), \"' id:\", node_id);\n   if (is_source) {\n-    strings::StrAppend(&ret, \" source}\");\n+    absl::StrAppend(&ret, \" source}\");\n   } else {\n-    strings::StrAppend(&ret, \" def:{\", SummarizeNodeDef(kernel->def()), \"}}\");\n+    absl::StrAppend(&ret, \" def:{\", SummarizeNodeDef(kernel->def()), \"}}\");\n   }\n   return ret;\n }\n@@ -67,7 +67,7 @@ GraphView::~GraphView() {\n }\n \n namespace {\n-typedef std::tuple<int32, int32> OutputAndControlEdges;\n+typedef std::tuple<int32_t, int32_t> OutputAndControlEdges;\n \n OutputAndControlEdges CountOutputEdges(const Node* n) {\n   DCHECK_LE(n->out_edges().size(), std::numeric_limits<int32_t>::max());\n@@ -102,8 +102,8 @@ size_t GraphView::NodeItemBytes(const Node* n) {\n             sizeof(ControlEdgeInfo)                // output_control_edges[...]\n       + num_outputs * sizeof(AllocatorAttributes)  // output_attr[...]\n       + num_outputs * sizeof(int)                  // forward_from[num_outputs]\n-      + num_inputs * sizeof(uint8)                 // input_type[num_inputs]\n-      + num_outputs * sizeof(uint8);               // output_type[num_outputs]\n+      + num_inputs * sizeof(uint8_t)               // input_type[num_inputs]\n+      + num_outputs * sizeof(uint8_t);             // output_type[num_outputs]\n   static constexpr size_t kItemAlignment = sizeof(NodeItem*);\n   static_assert(kItemAlignment % alignof(NodeItem) == 0,\n                 \"NodeItem must be aligned with kItemAlignment\");\n@@ -141,7 +141,7 @@ char* GraphView::InitializeNode(char* ptr, const Node* n) {\n   // values as \"int\" vs \"size_t\" in CHECK_LE.\n   CHECK_LE(static_cast<int64_t>(ptr - space_),\n            std::numeric_limits<uint32_t>::max());\n-  const uint32 offset = static_cast<uint32>(ptr - space_);\n+  const uint32_t offset = static_cast<uint32_t>(ptr - space_);\n   node_offsets_[id] = offset;\n   ptr += bytes;\n \n@@ -197,10 +197,10 @@ char* GraphView::InitializeNode(char* ptr, const Node* n) {\n   }\n \n   DCHECK_LT(DataType_MAX, 255);  // Must fit in uint8\n-  uint8* input_types = item->input_type_base();\n+  uint8_t* input_types = item->input_type_base();\n   item->is_any_input_ref_typed = false;\n   for (int i = 0; i < num_inputs; i++) {\n-    input_types[i] = static_cast<uint8>(n->input_type(i));\n+    input_types[i] = static_cast<uint8_t>(n->input_type(i));\n     DCHECK_EQ(item->input_type(i), n->input_type(i));\n     item->is_any_input_ref_typed |= IsRefType(n->input_type(i));\n   }\n@@ -215,9 +215,9 @@ char* GraphView::InitializeNode(char* ptr, const Node* n) {\n         GetNodeAttr(n->attrs(), \"_scoped_allocator\", &scoped_allocator_attrs);\n \n     int* forward_from = item->forward_from_base();\n-    uint8* output_types = item->output_type_base();\n+    uint8_t* output_types = item->output_type_base();\n     for (int i = 0; i < num_outputs; ++i) {\n-      output_types[i] = static_cast<uint8>(n->output_type(i));\n+      output_types[i] = static_cast<uint8_t>(n->output_type(i));\n       DCHECK_EQ(item->output_type(i), n->output_type(i));\n \n       forward_from[i] = OpKernelContext::Params::kNoReservation;\n@@ -264,7 +264,7 @@ absl::Status GraphView::Initialize(const Graph* g) {\n     total_bytes += NodeItemBytes(n);\n   }\n \n-  node_offsets_ = new uint32[num_nodes];\n+  node_offsets_ = new uint32_t[num_nodes];\n   for (int i = 0; i < num_nodes; i++) {\n     node_offsets_[i] = std::numeric_limits<uint32_t>::max();\n   }\n@@ -363,7 +363,7 @@ absl::Status InferAllocAttr(const Node* n, const Node* dst,\n   // Note that it's possible for *n to be a Recv and *dst to be a Send,\n   // so these two cases are not mutually exclusive.\n   if (IsRecv(n)) {\n-    string src_name;\n+    std::string src_name;\n     s = GetNodeAttr(n->attrs(), \"send_device\", &src_name);\n     if (!s.ok()) return s;\n     DeviceNameUtils::ParsedName parsed_src_name;\n@@ -388,7 +388,7 @@ absl::Status InferAllocAttr(const Node* n, const Node* dst,\n     }\n   }\n   if (IsSend(dst)) {\n-    string dst_name;\n+    std::string dst_name;\n     s = GetNodeAttr(dst->attrs(), \"recv_device\", &dst_name);\n     if (!s.ok()) return s;\n     DeviceNameUtils::ParsedName parsed_dst_name;"
        },
        {
            "sha": "32df420842d6578cb817e881b3a8abc36a298005",
            "filename": "tensorflow/core/common_runtime/graph_view.h",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_view.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_view.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgraph_view.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -102,10 +102,10 @@ struct NodeItem {\n   int input_start = 0;\n \n   // Number of output edges, excluding control edges.\n-  int32 num_output_edges;\n+  int32_t num_output_edges;\n \n   // Number of output control edges.\n-  int32 num_output_control_edges;\n+  int32_t num_output_control_edges;\n \n   // If non-null, contains an array of num_outputs bools, where the ith bool\n   // is true if and only if the ith output is consumed by another node.\n@@ -143,7 +143,7 @@ struct NodeItem {\n   // 0... for forward from that input.\n   const int* forward_from() const { return forward_from_base(); }\n \n-  string DebugString() const;\n+  std::string DebugString() const;\n \n  private:\n   friend class GraphView;\n@@ -185,18 +185,18 @@ struct NodeItem {\n                                       num_output_control_edges +\n                                   sizeof(AllocatorAttributes) * num_outputs);\n   }\n-  uint8* input_type_base() const {\n-    return reinterpret_cast<uint8*>(\n+  uint8_t* input_type_base() const {\n+    return reinterpret_cast<uint8_t*>(\n         var() + sizeof(EdgeInfo) * num_output_edges +\n         sizeof(ControlEdgeInfo) * num_output_control_edges +\n         sizeof(AllocatorAttributes) * num_outputs + sizeof(int) * num_outputs);\n   }\n-  uint8* output_type_base() const {\n-    return reinterpret_cast<uint8*>(\n+  uint8_t* output_type_base() const {\n+    return reinterpret_cast<uint8_t*>(\n         var() + sizeof(EdgeInfo) * num_output_edges +\n         sizeof(ControlEdgeInfo) * num_output_control_edges +\n         sizeof(AllocatorAttributes) * num_outputs + sizeof(int) * num_outputs +\n-        sizeof(uint8) * num_inputs);\n+        sizeof(uint8_t) * num_inputs);\n   }\n \n   NodeItem(const NodeItem&) = delete;\n@@ -220,7 +220,7 @@ class GraphView {\n   NodeItem* node(int32_t id) const {\n     DCHECK_GE(id, 0);\n     DCHECK_LT(id, num_nodes_);\n-    uint32 offset = node_offsets_[id];\n+    uint32_t offset = node_offsets_[id];\n     return ((offset == std::numeric_limits<uint32_t>::max())\n                 ? nullptr\n                 : reinterpret_cast<NodeItem*>(space_ + node_offsets_[id]));\n@@ -232,19 +232,19 @@ class GraphView {\n   const NodeItem& node_ref(int32_t id) const {\n     DCHECK_GE(id, 0);\n     DCHECK_LT(id, num_nodes_);\n-    uint32 offset = node_offsets_[id];\n+    uint32_t offset = node_offsets_[id];\n     DCHECK_NE(offset, std::numeric_limits<uint32_t>::max());\n     return *reinterpret_cast<NodeItem*>(space_ + node_offsets_[id]);\n   }\n \n-  int32 num_nodes() const { return num_nodes_; }\n+  int32_t num_nodes() const { return num_nodes_; }\n \n  private:\n   char* InitializeNode(char* ptr, const Node* n);\n   size_t NodeItemBytes(const Node* n);\n \n-  int32 num_nodes_ = 0;\n-  uint32* node_offsets_ = nullptr;  // array of size \"num_nodes_\"\n+  int32_t num_nodes_ = 0;\n+  uint32_t* node_offsets_ = nullptr;  // array of size \"num_nodes_\"\n   // node_offsets_[id] holds the byte offset for node w/ \"id\" in space_\n \n   char* space_;  // NodeItem objects are allocated here"
        },
        {
            "sha": "ebbdfde177da79aa7d5422020022e354cfba92cc",
            "filename": "tensorflow/core/common_runtime/hierarchical_tree_broadcaster.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fhierarchical_tree_broadcaster.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fhierarchical_tree_broadcaster.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fhierarchical_tree_broadcaster.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -43,8 +43,8 @@ namespace tensorflow {\n \n namespace {\n // Key to be used for BufRendezvous by Broadcaster.\n-string BroadcastBufKey(const string& exec_key, int subdiv, int src_rank,\n-                       int dst_rank) {\n+std::string BroadcastBufKey(const std::string& exec_key, int subdiv,\n+                            int src_rank, int dst_rank) {\n   if (READABLE_KEYS) {\n     return strings::StrCat(\"broadcast(\", exec_key, \"):subdiv(\", subdiv,\n                            \"):src(\", src_rank, \"):dst(\", dst_rank, \")\");\n@@ -81,13 +81,13 @@ absl::Status HierarchicalTreeBroadcaster::InitializeCollectiveParams(\n   CHECK_EQ(col_params->instance.type, BROADCAST_COLLECTIVE);\n   CHECK_EQ(col_params->instance.impl_details.collective_name,\n            \"HierarchicalTreeBroadcast\");\n-  const string& device_name =\n+  const std::string& device_name =\n       col_params->group.members[col_params->default_rank].device.name();\n   // Start by counting the devices in each task.\n   // Precondition: device_names must be sorted so that all devices in\n   // the same task are adjacent.\n   std::vector<int> dev_per_task;\n-  const string* prior_task_name = &col_params->group.members[0].task;\n+  const std::string* prior_task_name = &col_params->group.members[0].task;\n   int dev_count = 1;\n   for (int di = 1; di < col_params->group.group_size; ++di) {\n     if (col_params->group.members[di].task != *prior_task_name) {\n@@ -102,8 +102,8 @@ absl::Status HierarchicalTreeBroadcaster::InitializeCollectiveParams(\n   CHECK_EQ(col_params->group.num_tasks, dev_per_task.size());\n \n   if (VLOG_IS_ON(2)) {\n-    string dpt_buf;\n-    for (int dpt : dev_per_task) strings::StrAppend(&dpt_buf, dpt, \";\");\n+    std::string dpt_buf;\n+    for (int dpt : dev_per_task) absl::StrAppend(&dpt_buf, dpt, \";\");\n     VLOG(2) << \"HierarchicalTreeBroadcaster::InitializeCollectiveParams device=\"\n             << device_name << \" source_rank=\" << col_params->source_rank\n             << \" dev_per_task=\" << dpt_buf;\n@@ -302,9 +302,9 @@ void HierarchicalTreeBroadcaster::RunTree() {\n     if (-1 == my_rank) continue;\n     int source_rank = col_params_->instance.impl_details.subdiv_source_rank[si];\n     if (VLOG_IS_ON(1)) {\n-      string subdiv_buf;\n+      std::string subdiv_buf;\n       for (int r : col_params_->instance.impl_details.subdiv_permutations[si]) {\n-        strings::StrAppend(&subdiv_buf, r, \",\");\n+        absl::StrAppend(&subdiv_buf, r, \",\");\n       }\n       VLOG(1) << \"Running Broadcast tree device=\" << col_ctx_->device_name\n               << \" subdiv=\" << si << \" perm=\" << subdiv_buf\n@@ -318,7 +318,7 @@ void HierarchicalTreeBroadcaster::RunTree() {\n     if (my_rank >= 0 && my_rank != source_rank) {\n       // Begin by receiving the value.\n       tsl::profiler::TraceMe activity(\n-          [&] { return strings::StrCat(\"ReceiveValue:\", si); },\n+          [&] { return absl::StrCat(\"ReceiveValue:\", si); },\n           tsl::profiler::TraceMeLevel::kInfo);\n       int recv_from_rank = TreeRecvFrom(*col_params_, si);\n       absl::Notification note;\n@@ -334,7 +334,7 @@ void HierarchicalTreeBroadcaster::RunTree() {\n     // Then forward value to all descendent devices.\n     {\n       tsl::profiler::TraceMe activity(\n-          [&] { return strings::StrCat(\"ForwardValue:\", si); },\n+          [&] { return absl::StrCat(\"ForwardValue:\", si); },\n           tsl::profiler::TraceMeLevel::kInfo);\n       if (my_rank >= 0 && status_.ok()) {\n         std::vector<int> send_to_ranks;\n@@ -413,7 +413,7 @@ void HierarchicalTreeBroadcaster::DispatchSend(int subdiv, int dst_rank,\n   tsl::profiler::ScopedMemoryDebugAnnotation op_annotation(\n       col_params_->name, col_ctx_->step_id, \"dynamic\", src_tensor->dtype(),\n       [src_tensor]() { return src_tensor->shape().DebugString(); });\n-  string send_buf_key =\n+  std::string send_buf_key =\n       BroadcastBufKey(col_ctx_->exec_key, subdiv, src_rank, dst_rank);\n   int dst_idx =\n       col_params_->instance.impl_details.subdiv_permutations[subdiv][dst_rank];\n@@ -434,7 +434,7 @@ void HierarchicalTreeBroadcaster::DispatchSend(int subdiv, int dst_rank,\n void HierarchicalTreeBroadcaster::DispatchRecv(int subdiv, int src_rank,\n                                                int dst_rank, Tensor* dst_tensor,\n                                                const StatusCallback& done) {\n-  string recv_buf_key =\n+  std::string recv_buf_key =\n       BroadcastBufKey(col_ctx_->exec_key, subdiv, src_rank, dst_rank);\n   int src_idx =\n       col_params_->instance.impl_details.subdiv_permutations[subdiv][src_rank];"
        },
        {
            "sha": "408d8cb65b3682cfca697cce57cdbb52ba95b41a",
            "filename": "tensorflow/core/common_runtime/hierarchical_tree_broadcaster_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fhierarchical_tree_broadcaster_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fhierarchical_tree_broadcaster_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fhierarchical_tree_broadcaster_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -191,7 +191,7 @@ class HierarchicalTreeBroadcasterTest : public ::testing::Test {\n       if (!instances_[di]->status_.ok()) {\n         ASSERT_GT(fail_after, 0);\n         ASSERT_NE(instances_[di]->status_.message().find(\"Deliberate failure\"),\n-                  string::npos);\n+                  std::string::npos);\n         ++failure_count_;\n         continue;\n       }\n@@ -221,7 +221,7 @@ class HierarchicalTreeBroadcasterTest : public ::testing::Test {\n       // In the test we always broadcast from rank 0.\n       col_params_->is_source = (rank == 0);\n       col_params_->source_rank = 0;\n-      string dev_name = col_params_->group.members[rank].device.name();\n+      std::string dev_name = col_params_->group.members[rank].device.name();\n       TF_CHECK_OK(test_env_->device_mgr->LookupDevice(dev_name, &device_))\n           << \"Couldn't find device \" << dev_name\n           << \" existing devices: \" << test_env_->device_mgr->DebugString();\n@@ -356,10 +356,10 @@ TEST_F(HierarchicalTreeBroadcasterInitParamsTest,\n   cp->instance.impl_details.collective_name = \"HierarchicalTreeBroadcast\";\n   std::vector<int> dev_per_task = {4, 4, 6, 8};\n   for (int ti = 0; ti < cp->group.num_tasks; ti++) {\n-    string task_name = strings::StrCat(\"/job:worker/replica:0/task:\", ti);\n+    std::string task_name = absl::StrCat(\"/job:worker/replica:0/task:\", ti);\n     for (int di = 0; di < dev_per_task[ti]; di++) {\n       CollGroupMember member;\n-      member.device.set_name(strings::StrCat(task_name, \"/device:GPU:\", di));\n+      member.device.set_name(absl::StrCat(task_name, \"/device:GPU:\", di));\n       member.task = task_name;\n       cp->group.members.push_back(member);\n       cp->group.group_size++;"
        },
        {
            "sha": "64ded72c5e0d4ec54e2d6b7b12bb1a8929ef963c",
            "filename": "tensorflow/core/common_runtime/immutable_executor_state.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fimmutable_executor_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fimmutable_executor_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fimmutable_executor_state.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -68,7 +68,7 @@ void GetMaxPendingCounts(const Node* n, size_t* max_pending,\n }  // namespace\n \n ImmutableExecutorState::FrameInfo* ImmutableExecutorState::EnsureFrameInfo(\n-    const string& fname) {\n+    const std::string& fname) {\n   auto iter = frame_info_.find(fname);\n   if (iter != frame_info_.end()) {\n     return iter->second.get();\n@@ -110,8 +110,8 @@ absl::Status ImmutableExecutorState::Initialize(const Graph& graph) {\n       // TODO(mrry): Track whether control flow was present in the\n       // pre-partitioned graph, and enable the caller (e.g.\n       // `DirectSession`) to relax this constraint.\n-      string send_device;\n-      string recv_device;\n+      std::string send_device;\n+      std::string recv_device;\n       TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), \"send_device\", &send_device));\n       TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), \"recv_device\", &recv_device));\n       if (send_device != recv_device) {\n@@ -120,7 +120,7 @@ absl::Status ImmutableExecutorState::Initialize(const Graph& graph) {\n     }\n \n     const int id = n->id();\n-    const string& frame_name = cf_info.frame_names[id];\n+    const std::string& frame_name = cf_info.frame_names[id];\n     FrameInfo* frame_info = EnsureFrameInfo(frame_name);\n \n     NodeItem* item = gview_.node(id);\n@@ -162,7 +162,7 @@ absl::Status ImmutableExecutorState::Initialize(const Graph& graph) {\n           GetNodeAttr(n->attrs(), \"is_constant\", &is_constant_enter));\n       item->is_constant_enter = is_constant_enter;\n \n-      string frame_name;\n+      std::string frame_name;\n       TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), \"frame_name\", &frame_name));\n       FrameInfo* frame_info = frame_info_[frame_name].get();\n \n@@ -214,7 +214,7 @@ absl::Status ImmutableExecutorState::Initialize(const Graph& graph) {\n     // Initialize static information about the frames in the graph.\n     frame_info->nodes->push_back(item);\n     if (item->is_enter) {\n-      string enter_name;\n+      std::string enter_name;\n       TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), \"frame_name\", &enter_name));\n       EnsureFrameInfo(enter_name)->input_count++;\n     }\n@@ -291,7 +291,7 @@ absl::Status ImmutableExecutorState::BuildControlFlowInfo(\n   std::vector<bool> visited;\n   visited.resize(num_nodes);\n \n-  string frame_name;\n+  std::string frame_name;\n   std::deque<Node*> ready;\n \n   // Initialize with the root nodes.\n@@ -360,15 +360,15 @@ void ImmutableExecutorState::InitializePending(const Graph* graph,\n   }\n \n   if (!requires_control_flow_) {\n-    atomic_pending_counts_.reset(new std::atomic<int32>[gview_.num_nodes()]);\n+    atomic_pending_counts_.reset(new std::atomic<int32_t>[gview_.num_nodes()]);\n     std::fill(atomic_pending_counts_.get(),\n               atomic_pending_counts_.get() + gview_.num_nodes(), 0);\n   }\n \n   for (const Node* n : graph->nodes()) {\n     if (IsSink(n)) continue;\n     const int id = n->id();\n-    const string& name = cf_info.frame_names[id];\n+    const std::string& name = cf_info.frame_names[id];\n     size_t max_pending, max_dead;\n     GetMaxPendingCounts(n, &max_pending, &max_dead);\n     auto& counts = EnsureFrameInfo(name)->pending_counts;"
        },
        {
            "sha": "7e7437c5311d20b5d30d76ed1a9808e3c820f99e",
            "filename": "tensorflow/core/common_runtime/immutable_executor_state.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fimmutable_executor_state.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fimmutable_executor_state.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fimmutable_executor_state.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -42,7 +42,7 @@ class Graph;\n class ImmutableExecutorState {\n  public:\n   struct FrameInfo {\n-    explicit FrameInfo(string name)\n+    explicit FrameInfo(std::string name)\n         : name(std::move(name)),\n           input_count(0),\n           total_inputs(0),\n@@ -51,7 +51,7 @@ class ImmutableExecutorState {\n           parallel_iterations(-1) {}\n \n     // The name of the frame.\n-    string name;\n+    std::string name;\n \n     // The total number of inputs to a frame.\n     int input_count;\n@@ -71,7 +71,7 @@ class ImmutableExecutorState {\n     std::unique_ptr<std::vector<const NodeItem*>> nodes;\n \n     // The number of iterations of this frame that can execute concurrently.\n-    int32 parallel_iterations;\n+    int32_t parallel_iterations;\n   };\n \n   explicit ImmutableExecutorState(const LocalExecutorParams& p)\n@@ -109,24 +109,24 @@ class ImmutableExecutorState {\n   //\n   // REQUIRES: `!requires_control_flow_support && len(dest) ==\n   // graph_view().num_nodes()`.\n-  void copy_pending_counts(std::atomic<int32>* dest) const {\n+  void copy_pending_counts(std::atomic<int32_t>* dest) const {\n     DCHECK(!requires_control_flow_);\n     memcpy(dest, atomic_pending_counts_.get(),\n-           graph_view().num_nodes() * sizeof(std::atomic<int32>));\n+           graph_view().num_nodes() * sizeof(std::atomic<int32_t>));\n     std::atomic_thread_fence(std::memory_order_release);\n   }\n \n  private:\n   struct ControlFlowInfo {\n-    gtl::FlatSet<string> unique_frame_names;\n-    std::vector<string> frame_names;\n+    gtl::FlatSet<std::string> unique_frame_names;\n+    std::vector<std::string> frame_names;\n   };\n \n   static absl::Status BuildControlFlowInfo(const Graph* graph,\n                                            ControlFlowInfo* cf_info);\n   void InitializePending(const Graph* graph, const ControlFlowInfo& cf_info);\n \n-  FrameInfo* EnsureFrameInfo(const string& fname);\n+  FrameInfo* EnsureFrameInfo(const std::string& fname);\n \n   // Owned.\n   LocalExecutorParams params_;\n@@ -150,7 +150,7 @@ class ImmutableExecutorState {\n \n   // If `requires_control_flow_` is false, this points to an array of initial\n   // pending counts for the nodes in the graph, indexed by node ID.\n-  std::unique_ptr<std::atomic<int32>[]> atomic_pending_counts_;\n+  std::unique_ptr<std::atomic<int32_t>[]> atomic_pending_counts_;\n \n   // Shallow copies of the constant tensors used in the graph.\n   std::vector<Tensor> const_tensors_;"
        },
        {
            "sha": "a627e9e8aff9c9d61441faa2dc6c2cd7528c23ae",
            "filename": "tensorflow/core/common_runtime/inline_function_utils.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 38,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -70,19 +70,19 @@ struct Endpoint {\n   int index;\n \n   // Returns the string name represents this endpoint.\n-  string name() const {\n+  std::string name() const {\n     if (index == 0) {\n       return node->name();\n     } else {\n-      return strings::StrCat(node->name(), \":\", index);\n+      return absl::StrCat(node->name(), \":\", index);\n     }\n   }\n \n   DataType dtype() const { return node->output_type(index); }\n };\n \n struct EndpointHash {\n-  uint64 operator()(const Endpoint& x) const {\n+  uint64_t operator()(const Endpoint& x) const {\n     return Hash64(reinterpret_cast<const char*>(&x.node), sizeof(Node*),\n                   x.index);\n   }\n@@ -120,15 +120,15 @@ static Node* AddIdentity(absl::string_view name, Graph* g, Endpoint input) {\n   return ret;\n }\n \n-std::vector<string> InputDevices(const Node& caller) {\n-  std::vector<string> input_devices(caller.in_edges().size());\n-  std::vector<string> input_tensors(caller.in_edges().size());\n+std::vector<std::string> InputDevices(const Node& caller) {\n+  std::vector<std::string> input_devices(caller.in_edges().size());\n+  std::vector<std::string> input_tensors(caller.in_edges().size());\n \n   for (const Edge* edge : caller.in_edges()) {\n     if (edge->IsControlEdge()) continue;\n-    const string& input_device = edge->src()->has_assigned_device_name()\n-                                     ? edge->src()->assigned_device_name()\n-                                     : edge->src()->requested_device();\n+    const std::string& input_device = edge->src()->has_assigned_device_name()\n+                                          ? edge->src()->assigned_device_name()\n+                                          : edge->src()->requested_device();\n     input_devices[edge->dst_input()] = input_device;\n     input_tensors[edge->dst_input()] =\n         absl::StrCat(edge->src()->name(), \":\", edge->src_output());\n@@ -154,22 +154,24 @@ class DefaultFunctionBodyPlacer : public InlinedFunctionBodyPlacer {\n   explicit DefaultFunctionBodyPlacer(const Node& caller)\n       : input_devices_(InputDevices(caller)) {}\n \n-  absl::optional<string> InputNodeDevice(int input_index) const override {\n+  absl::optional<std::string> InputNodeDevice(int input_index) const override {\n     return input_devices_[input_index];\n   }\n-  absl::optional<string> OutputNodeDevice(int output_index) const override {\n+  absl::optional<std::string> OutputNodeDevice(\n+      int output_index) const override {\n     return absl::nullopt;\n   }\n   bool ColocateInputOutputIdentities() const override { return false; }\n-  absl::optional<string> ControlNodeDevice() const override {\n+  absl::optional<std::string> ControlNodeDevice() const override {\n     return absl::nullopt;\n   }\n-  absl::optional<string> BodyNodeDevice(const NodeDef& ndef) const override {\n+  absl::optional<std::string> BodyNodeDevice(\n+      const NodeDef& ndef) const override {\n     return absl::nullopt;\n   }\n \n  private:\n-  const std::vector<string> input_devices_;\n+  const std::vector<std::string> input_devices_;\n };\n \n // Place all nodes on the same device as caller node.\n@@ -178,22 +180,24 @@ class SingleDeviceFunctionBodyPlacer : public InlinedFunctionBodyPlacer {\n   explicit SingleDeviceFunctionBodyPlacer(const Node& caller)\n       : caller_device_(caller.def().device()) {}\n \n-  absl::optional<string> InputNodeDevice(int input_index) const override {\n+  absl::optional<std::string> InputNodeDevice(int input_index) const override {\n     return caller_device_;\n   }\n-  absl::optional<string> OutputNodeDevice(int output_index) const override {\n+  absl::optional<std::string> OutputNodeDevice(\n+      int output_index) const override {\n     return caller_device_;\n   }\n   bool ColocateInputOutputIdentities() const override { return false; }\n-  absl::optional<string> ControlNodeDevice() const override {\n+  absl::optional<std::string> ControlNodeDevice() const override {\n     return caller_device_;\n   }\n-  absl::optional<string> BodyNodeDevice(const NodeDef& ndef) const override {\n+  absl::optional<std::string> BodyNodeDevice(\n+      const NodeDef& ndef) const override {\n     return caller_device_;\n   }\n \n  private:\n-  const string caller_device_;\n+  const std::string caller_device_;\n };\n \n // Place input nodes on the same device as the corresponding caller input\n@@ -209,17 +213,19 @@ class MultiDeviceFunctionBodyPlacer : public InlinedFunctionBodyPlacer {\n         DeviceNameUtils::ParseFullName(caller_device_, &caller_parsed_device_);\n   }\n \n-  absl::optional<string> InputNodeDevice(int input_index) const override {\n+  absl::optional<std::string> InputNodeDevice(int input_index) const override {\n     return input_devices_[input_index];\n   }\n-  absl::optional<string> OutputNodeDevice(int output_index) const override {\n+  absl::optional<std::string> OutputNodeDevice(\n+      int output_index) const override {\n     return absl::nullopt;\n   }\n   bool ColocateInputOutputIdentities() const override { return true; }\n-  absl::optional<string> ControlNodeDevice() const override {\n+  absl::optional<std::string> ControlNodeDevice() const override {\n     return caller_device_;\n   }\n-  absl::optional<string> BodyNodeDevice(const NodeDef& ndef) const override {\n+  absl::optional<std::string> BodyNodeDevice(\n+      const NodeDef& ndef) const override {\n     // LINT.IfChange\n     // TODO(ezhulenev): If function would have been instantiated as a\n     // multi-device function and executed via FunctionLibraryRuntime, it could\n@@ -240,10 +246,10 @@ class MultiDeviceFunctionBodyPlacer : public InlinedFunctionBodyPlacer {\n   }\n \n  private:\n-  string caller_device_;\n+  std::string caller_device_;\n   bool has_parsed_caller_device_;\n   DeviceNameUtils::ParsedName caller_parsed_device_;\n-  std::vector<string> input_devices_;\n+  std::vector<std::string> input_devices_;\n };\n \n }  // namespace\n@@ -286,7 +292,7 @@ using OutputControlSrc = InlineFunctionBodyOptions::OutputControlSource;\n // Propagate the debug info of `nodes` in function `func` to the `target` node.\n // If the debug info of any node is missing, its node name and function name\n // is used.\n-void PropagateDebugInfoToNode(const string& func,\n+void PropagateDebugInfoToNode(const std::string& func,\n                               const std::vector<const Node*>& nodes,\n                               NodeDef* target) {\n   if (nodes.empty() || target->has_experimental_debug_info()) {\n@@ -306,10 +312,10 @@ void PropagateDebugInfoToNode(const string& func,\n }\n }  // namespace\n \n-string InlineFunctionBodyOptions::DebugString() const {\n+std::string InlineFunctionBodyOptions::DebugString() const {\n   const auto true_false = [](bool b) { return b ? \"true\" : \"false\"; };\n \n-  const auto keep_caller_node_str = [this]() -> string {\n+  const auto keep_caller_node_str = [this]() -> std::string {\n     switch (keep_caller_node) {\n       case KeepCallerNode::kDoNotKeep:\n         return \"DoNotKeep\";\n@@ -508,7 +514,7 @@ absl::Status InlineFunctionBody(const FunctionLibraryDefinition& flib_def,\n   // Add a NoOp node for function control inputs/outputs.\n   const auto no_op = [&](absl::string_view name) -> Node* {\n     Node* node = AddNoOp(absl::StrCat(caller->name(), \"/\", name), g);\n-    const absl::optional<string> device = placer->ControlNodeDevice();\n+    const absl::optional<std::string> device = placer->ControlNodeDevice();\n     if (device.has_value()) node->set_requested_device(*device);\n     return node;\n   };\n@@ -517,13 +523,13 @@ absl::Status InlineFunctionBody(const FunctionLibraryDefinition& flib_def,\n   const auto input_identity = [&](absl::string_view name, Endpoint input,\n                                   int index) -> Node* {\n     Node* node = AddIdentity(absl::StrCat(caller->name(), \"/\", name), g, input);\n-    const absl::optional<string> device = placer->InputNodeDevice(index);\n+    const absl::optional<std::string> device = placer->InputNodeDevice(index);\n     if (device.has_value()) node->set_requested_device(*device);\n     bool colocate_identity = placer->ColocateInputOutputIdentities();\n     if (colocate_identity) {\n       node->AddAttr(kColocationAttrName,\n-                    std::vector<string>{absl::StrCat(kColocationGroupPrefix,\n-                                                     input.node->name())});\n+                    std::vector<std::string>{absl::StrCat(\n+                        kColocationGroupPrefix, input.node->name())});\n     }\n     return node;\n   };\n@@ -532,13 +538,13 @@ absl::Status InlineFunctionBody(const FunctionLibraryDefinition& flib_def,\n   const auto output_identity = [&](absl::string_view name, Endpoint input,\n                                    int index) -> Node* {\n     Node* node = AddIdentity(absl::StrCat(caller->name(), \"/\", name), g, input);\n-    const absl::optional<string> device = placer->OutputNodeDevice(index);\n+    const absl::optional<std::string> device = placer->OutputNodeDevice(index);\n     if (device.has_value()) node->set_requested_device(*device);\n     bool colocate_identity = placer->ColocateInputOutputIdentities();\n     if (colocate_identity) {\n       node->AddAttr(kColocationAttrName,\n-                    std::vector<string>{absl::StrCat(kColocationGroupPrefix,\n-                                                     input.node->name())});\n+                    std::vector<std::string>{absl::StrCat(\n+                        kColocationGroupPrefix, input.node->name())});\n     }\n     return node;\n   };\n@@ -597,7 +603,7 @@ absl::Status InlineFunctionBody(const FunctionLibraryDefinition& flib_def,\n   //\n   // If 'x' is a node in fbody->graph and its copy in 'g' is 'y', we\n   // remember 'y' in node_map[x->id()].\n-  std::unordered_set<string> fn_nodes;\n+  std::unordered_set<std::string> fn_nodes;\n   for (Node* n : fbody->graph->op_nodes()) {\n     fn_nodes.insert(n->name());\n   }\n@@ -606,7 +612,7 @@ absl::Status InlineFunctionBody(const FunctionLibraryDefinition& flib_def,\n     NodeDef ndef = n->def();\n \n     // Maybe override requested node device assignment.\n-    const absl::optional<string> device = placer->BodyNodeDevice(ndef);\n+    const absl::optional<std::string> device = placer->BodyNodeDevice(ndef);\n     if (device.has_value()) ndef.set_device(*device);\n \n     // Add inlined function name to inlined node debug information.\n@@ -617,7 +623,7 @@ absl::Status InlineFunctionBody(const FunctionLibraryDefinition& flib_def,\n     //  1) to node name to avoid collisions\n     //  2) to frame name to avoid multiple LoopCond nodes in one frame\n     //  3) to colocation attribute\n-    const string prefix = strings::StrCat(caller->name(), \"/\");\n+    const std::string prefix = absl::StrCat(caller->name(), \"/\");\n     TF_RETURN_IF_ERROR(AddPrefixAndSuffixToNode(prefix, /*suffix=*/\"\", &ndef,\n                                                 options.uniquify_frame_names));\n "
        },
        {
            "sha": "7ffafe13e5df033e7e192d18c37564e6dede4954",
            "filename": "tensorflow/core/common_runtime/inline_function_utils.h",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -41,13 +41,16 @@ class InlinedFunctionBodyPlacer {\n  public:\n   virtual ~InlinedFunctionBodyPlacer() = default;\n \n-  virtual absl::optional<string> InputNodeDevice(int input_index) const = 0;\n-  virtual absl::optional<string> OutputNodeDevice(int output_index) const = 0;\n+  virtual absl::optional<std::string> InputNodeDevice(\n+      int input_index) const = 0;\n+  virtual absl::optional<std::string> OutputNodeDevice(\n+      int output_index) const = 0;\n   // Returns true if the added input/output identity nodes should be colocated\n   // with the corresponding input/output from the function body.\n   virtual bool ColocateInputOutputIdentities() const = 0;\n-  virtual absl::optional<string> ControlNodeDevice() const = 0;\n-  virtual absl::optional<string> BodyNodeDevice(const NodeDef& ndef) const = 0;\n+  virtual absl::optional<std::string> ControlNodeDevice() const = 0;\n+  virtual absl::optional<std::string> BodyNodeDevice(\n+      const NodeDef& ndef) const = 0;\n \n   // LINT.IfChange\n   // Place input nodes on the same device as the corresponding caller input\n@@ -72,7 +75,7 @@ class InlinedFunctionBodyPlacer {\n       const Graph&, const Node&)>;\n \n   struct Config {\n-    string name;\n+    std::string name;\n     Factory get;\n   };\n \n@@ -147,7 +150,7 @@ struct InlineFunctionBodyOptions {\n   bool uniquify_frame_names = true;\n \n   // A human-readable debug string for this options.\n-  string DebugString() const;\n+  std::string DebugString() const;\n };\n \n // Returns 'OkStatus()' iff the function '*fbody' can be inlined at 'node'"
        },
        {
            "sha": "1e20e6da535a165a275f121a1ac41b46e0232886",
            "filename": "tensorflow/core/common_runtime/inline_function_utils_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Finline_function_utils_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -50,7 +50,7 @@ TEST(InlineFunctionBody, ColocationConstraintPropagation) {\n           {{\"z\"},\n            \"AddV2\",\n            {\"x\", \"y\"},\n-           {{\"T\", DT_FLOAT}, {\"_class\", std::vector<string>({\"loc:@x\"})}}},\n+           {{\"T\", DT_FLOAT}, {\"_class\", std::vector<std::string>({\"loc:@x\"})}}},\n       });\n   TF_ASSERT_OK(flib_def.AddFunctionDef(fdef));\n \n@@ -98,7 +98,8 @@ TEST(InlineFunctionBody, ColocationConstraintPropagation) {\n           // Func/call/input/_0.\n           NDef(\"call/z\", \"AddV2\", {\"Func/call/input/_0\", \"Func/call/input/_1\"},\n                {{\"T\", DT_FLOAT},\n-                {\"_class\", std::vector<string>({\"loc:@Func/call/input/_0\"})}}),\n+                {\"_class\",\n+                 std::vector<std::string>({\"loc:@Func/call/input/_0\"})}}),\n           NDef(\"Func/call/output/_2\", \"Identity\", {\"call/z\"},\n                {{\"T\", DT_FLOAT}}),\n       },"
        },
        {
            "sha": "4edf42ff812b8de29530b8da6fa663a7c3939372",
            "filename": "tensorflow/core/common_runtime/input_colocation_exemption_registry.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finput_colocation_exemption_registry.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finput_colocation_exemption_registry.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Finput_colocation_exemption_registry.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -27,7 +27,7 @@ InputColocationExemptionRegistry* InputColocationExemptionRegistry::Global() {\n   return registry;\n }\n \n-void InputColocationExemptionRegistry::Register(const string& op) {\n+void InputColocationExemptionRegistry::Register(const std::string& op) {\n   auto it = ops_.find(op);\n   if (it != ops_.end()) {\n     LOG(WARNING) << \"Input colocation exemption for op: \" << op"
        },
        {
            "sha": "9e4bbc9e77f4af9096a829a498389c4382e0cb2f",
            "filename": "tensorflow/core/common_runtime/input_colocation_exemption_registry.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finput_colocation_exemption_registry.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finput_colocation_exemption_registry.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Finput_colocation_exemption_registry.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -40,20 +40,20 @@ class InputColocationExemptionRegistry {\n   static InputColocationExemptionRegistry* Global();\n \n   // Returns the set of ops exempt from the input colocation constraints.\n-  const gtl::FlatSet<string>& Get() { return ops_; }\n+  const gtl::FlatSet<std::string>& Get() { return ops_; }\n \n   // Registers an op to be excluded from the input colocation constraints.\n-  void Register(const string& op);\n+  void Register(const std::string& op);\n \n  private:\n-  gtl::FlatSet<string> ops_;\n+  gtl::FlatSet<std::string> ops_;\n };\n \n namespace input_colocation_exemption_registration {\n \n class InputColocationExemptionRegistration {\n  public:\n-  explicit InputColocationExemptionRegistration(const string& op) {\n+  explicit InputColocationExemptionRegistration(const std::string& op) {\n     InputColocationExemptionRegistry::Global()->Register(op);\n   }\n };"
        },
        {
            "sha": "816d3dcae487a92967e4c6cb334c5c489d41dc94",
            "filename": "tensorflow/core/common_runtime/inspecting_placer.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finspecting_placer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finspecting_placer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Finspecting_placer.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -34,21 +34,21 @@ limitations under the License.\n \n namespace tensorflow {\n \n-string IOColocationGroups::DebugString() const {\n-  std::unordered_map<int, std::vector<string>> group_members;\n+std::string IOColocationGroups::DebugString() const {\n+  std::unordered_map<int, std::vector<std::string>> group_members;\n   for (int arg_index = 0; arg_index < input_groups.size(); ++arg_index) {\n     int group_id = input_groups[arg_index];\n-    group_members[group_id].push_back(strings::StrCat(\"i:\", arg_index));\n+    group_members[group_id].push_back(absl::StrCat(\"i:\", arg_index));\n   }\n   for (int ret_index = 0; ret_index < output_groups.size(); ++ret_index) {\n     int group_id = output_groups[ret_index];\n-    group_members[group_id].push_back(strings::StrCat(\"o:\", ret_index));\n+    group_members[group_id].push_back(absl::StrCat(\"o:\", ret_index));\n   }\n \n-  std::vector<string> group_strings;\n+  std::vector<std::string> group_strings;\n   for (const auto& it : group_members) {\n     int group_id = it.first;\n-    const std::vector<string>& members = it.second;\n+    const std::vector<std::string>& members = it.second;\n     const PossibleDevices& devices = group_devices[group_id];\n     group_strings.push_back(strings::StrCat(\n         \"Group(\", group_id, \" members = [\", absl::StrJoin(members, \", \"),\n@@ -57,11 +57,11 @@ string IOColocationGroups::DebugString() const {\n         \"\\\" resource_device_name = \\\"\",\n         DeviceNameUtils::ParsedNameToString(devices.resource_device_name),\n         \"\\\" device_types = [\",\n-        absl::StrJoin(\n-            devices.device_types, \", \",\n-            [](string* out, const std::pair<DeviceType, int32>& type_and_pref) {\n-              out->append(DeviceTypeString(type_and_pref.first));\n-            }),\n+        absl::StrJoin(devices.device_types, \", \",\n+                      [](std::string* out,\n+                         const std::pair<DeviceType, int32_t>& type_and_pref) {\n+                        out->append(DeviceTypeString(type_and_pref.first));\n+                      }),\n         \"])\"));\n   }\n "
        },
        {
            "sha": "27e45dacadad8b6649e9c921d43830593174f34b",
            "filename": "tensorflow/core/common_runtime/inspecting_placer.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finspecting_placer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Finspecting_placer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Finspecting_placer.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -59,7 +59,7 @@ struct IOColocationGroups {\n   // group_devices[i] contains possible devices for group with id i.\n   std::vector<PossibleDevices> group_devices;\n \n-  string DebugString() const;\n+  std::string DebugString() const;\n };\n \n class InspectingPlacer {"
        },
        {
            "sha": "8e89b0bec2f6d9b012f024fcd9c3071969671389",
            "filename": "tensorflow/core/common_runtime/int32_fulltype.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fint32_fulltype.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fint32_fulltype.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fint32_fulltype.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -29,7 +29,7 @@ namespace tensorflow {\n class Int32FulltypePass {\n  public:\n   Int32FulltypePass() = default;\n-  explicit Int32FulltypePass(string debug_location)\n+  explicit Int32FulltypePass(std::string debug_location)\n       : debug_location_(debug_location) {}\n \n   // For each node in this graph that outputs int32 tensors, set full\n@@ -57,7 +57,7 @@ class Int32FulltypePass {\n \n  private:\n   // Location of where annotations were added for debug messages.\n-  string debug_location_;\n+  std::string debug_location_;\n };\n \n }  // namespace tensorflow"
        },
        {
            "sha": "ed8587667e9bccafc237d1b54dacc5854a81a7eb",
            "filename": "tensorflow/core/common_runtime/int32_fulltype_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fint32_fulltype_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fint32_fulltype_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fint32_fulltype_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -96,14 +96,14 @@ class Int32FulltypeTest : public ::testing::Test {\n   // Returns the node in \"graph\" with the given name.\n   //\n   // REQUIRES: \"graph\" was produced by the most recent call to BuildGraph.\n-  Node* GetNodeByName(const Graph& graph, const string& name) {\n+  Node* GetNodeByName(const Graph& graph, const std::string& name) {\n     const auto search = nodes_by_name_.find(name);\n     CHECK(search != nodes_by_name_.end()) << \"Unknown node name: \" << name;\n     return graph.FindNodeId(search->second);\n   }\n \n  protected:\n-  std::unordered_map<string, int> nodes_by_name_;\n+  std::unordered_map<std::string, int> nodes_by_name_;\n \n  private:\n   void RebuildNodeNameMap(const Graph& graph) {"
        },
        {
            "sha": "be10cd744f35f14c7b1273bf14c3cea10d57443b",
            "filename": "tensorflow/core/common_runtime/isolate_placer_inspection_required_ops_pass_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fisolate_placer_inspection_required_ops_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fisolate_placer_inspection_required_ops_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fisolate_placer_inspection_required_ops_pass_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -67,11 +67,11 @@ void RunPassAndCompare(const GraphDef& original,\n   GraphDef rewritten;\n   RunPass(original, &rewritten);\n \n-  std::vector<string> errors;\n+  std::vector<std::string> errors;\n   errors.push_back(absl::StrCat(\"Graphs did not match.\\n  Rewritten graph:\\n\",\n                                 SummarizeGraphDef(rewritten)));\n   for (const GraphDef& alternative : expected_alternatives) {\n-    string diff;\n+    std::string diff;\n     bool graphs_equal = EqualGraphDef(rewritten, alternative, &diff);\n     if (graphs_equal) {\n       return;"
        },
        {
            "sha": "78f2d2195053410f22e83c98c1ea2fb7cbe9cb01",
            "filename": "tensorflow/core/common_runtime/kernel_benchmark_testlib.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fkernel_benchmark_testlib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fkernel_benchmark_testlib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fkernel_benchmark_testlib.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -45,7 +45,7 @@ namespace tensorflow {\n namespace test {\n \n // TODO(hongm): Convert `g` and `init` to using std::unique_ptr.\n-Benchmark::Benchmark(const string& device, Graph* g,\n+Benchmark::Benchmark(const std::string& device, Graph* g,\n                      const SessionOptions* options, Graph* init,\n                      Rendezvous* rendez, const char* executor_type,\n                      bool old_benchmark_api) {\n@@ -61,7 +61,7 @@ Benchmark::Benchmark(const string& device, Graph* g,\n \n   CHECK(!old_benchmark_api) << \"Expected new API only\";\n \n-  string t = absl::AsciiStrToUpper(device);\n+  std::string t = absl::AsciiStrToUpper(device);\n   // Allow NewDevice to allocate a new threadpool with different number of\n   // threads for each new benchmark.\n   LocalDevice::set_use_global_threadpool(false);\n@@ -121,7 +121,8 @@ Benchmark::Benchmark(const string& device, Graph* g,\n   TF_CHECK_OK(NewExecutor(executor_type, params, *g, &exec_));\n }\n \n-Benchmark::Benchmark(const string& device, Graph* g, bool old_benchmark_api)\n+Benchmark::Benchmark(const std::string& device, Graph* g,\n+                     bool old_benchmark_api)\n     : Benchmark(device, g, nullptr, nullptr, nullptr, \"\", old_benchmark_api) {}\n \n Benchmark::~Benchmark() {\n@@ -141,14 +142,14 @@ void Benchmark::Run(benchmark::State& state) {\n   RunWithRendezvousArgs({}, {}, state);\n }\n \n-string GetRendezvousKey(const Node* node) {\n-  string send_device;\n+std::string GetRendezvousKey(const Node* node) {\n+  std::string send_device;\n   TF_CHECK_OK(GetNodeAttr(node->attrs(), \"send_device\", &send_device));\n-  string recv_device;\n+  std::string recv_device;\n   TF_CHECK_OK(GetNodeAttr(node->attrs(), \"recv_device\", &recv_device));\n-  string tensor_name;\n+  std::string tensor_name;\n   TF_CHECK_OK(GetNodeAttr(node->attrs(), \"tensor_name\", &tensor_name));\n-  uint64 send_device_incarnation;\n+  uint64_t send_device_incarnation;\n   TF_CHECK_OK(\n       GetNodeAttr(node->attrs(), \"send_device_incarnation\",\n                   reinterpret_cast<int64_t*>(&send_device_incarnation)));\n@@ -157,8 +158,8 @@ string GetRendezvousKey(const Node* node) {\n }\n \n void Benchmark::RunWithRendezvousArgs(\n-    const std::vector<std::pair<string, Tensor>>& inputs,\n-    const std::vector<string>& outputs, benchmark::State& state) {\n+    const std::vector<std::pair<std::string, Tensor>>& inputs,\n+    const std::vector<std::string>& outputs, benchmark::State& state) {\n   if (!device_ || state.max_iterations == 0) {\n     return;\n   }\n@@ -179,7 +180,7 @@ void Benchmark::RunWithRendezvousArgs(\n       TF_CHECK_OK(rendez_->Send(parsed, Rendezvous::Args(), p.second, false));\n     }\n     TF_CHECK_OK(exec_->Run(args));\n-    for (const string& key : outputs) {\n+    for (const std::string& key : outputs) {\n       Rendezvous::ParsedKey parsed;\n       TF_CHECK_OK(Rendezvous::ParseKey(key, &parsed));\n       TF_CHECK_OK(rendez_->Recv(parsed, Rendezvous::Args(), &unused, &is_dead));\n@@ -197,7 +198,7 @@ void Benchmark::RunWithRendezvousArgs(\n       TF_CHECK_OK(rendez_->Send(parsed, Rendezvous::Args(), p.second, false));\n     }\n     TF_CHECK_OK(exec_->Run(args));\n-    for (const string& key : outputs) {\n+    for (const std::string& key : outputs) {\n       Rendezvous::ParsedKey parsed;\n       TF_CHECK_OK(Rendezvous::ParseKey(key, &parsed));\n       TF_CHECK_OK(rendez_->Recv(parsed, Rendezvous::Args(), &unused, &is_dead));"
        },
        {
            "sha": "a0e5486b96c1201d239e3b77bd00a2ff36fc8c82",
            "filename": "tensorflow/core/common_runtime/kernel_benchmark_testlib.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fkernel_benchmark_testlib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fkernel_benchmark_testlib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fkernel_benchmark_testlib.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -48,20 +48,20 @@ class Benchmark {\n   //   * In the new API, the timer starts automatically at the first\n   //     iteration of the loop and stops after the last iteration.\n   // TODO(vyng) Remove this once we have migrated all code to newer API.\n-  Benchmark(const string& device, Graph* g,\n+  Benchmark(const std::string& device, Graph* g,\n             const SessionOptions* options = nullptr, Graph* init = nullptr,\n             Rendezvous* rendez = nullptr, const char* executor_type = \"\",\n             bool old_benchmark_api = false);\n \n-  Benchmark(const string& device, Graph* g, bool old_benchmark_api);\n+  Benchmark(const std::string& device, Graph* g, bool old_benchmark_api);\n \n   ~Benchmark();\n \n   void Run(benchmark::State& state);\n \n   void RunWithRendezvousArgs(\n-      const std::vector<std::pair<string, Tensor>>& inputs,\n-      const std::vector<string>& outputs, benchmark::State& state);\n+      const std::vector<std::pair<std::string, Tensor>>& inputs,\n+      const std::vector<std::string>& outputs, benchmark::State& state);\n \n  private:\n   thread::ThreadPool* pool_ = nullptr;  // Not owned.\n@@ -78,7 +78,7 @@ class Benchmark {\n };\n \n // Returns the rendezvous key associated with the given Send/Recv node.\n-string GetRendezvousKey(const Node* node);\n+std::string GetRendezvousKey(const Node* node);\n \n }  // end namespace test\n }  // end namespace tensorflow"
        },
        {
            "sha": "9997ff2a30c008e41c29c1e9a9e0f5beef6ba967",
            "filename": "tensorflow/core/common_runtime/local_device.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flocal_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flocal_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flocal_device.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -85,7 +85,7 @@ struct LocalDevice::EigenThreadPoolInfo {\n     thread_opts.numa_node = numa_node;\n     eigen_worker_threads_.num_threads = intra_op_parallelism_threads;\n     eigen_worker_threads_.workers = new thread::ThreadPool(\n-        options.env, thread_opts, strings::StrCat(\"numa_\", numa_node, \"_Eigen\"),\n+        options.env, thread_opts, absl::StrCat(\"numa_\", numa_node, \"_Eigen\"),\n         intra_op_parallelism_threads,\n         !options.config.experimental().disable_thread_spinning(),\n         /*allocator=*/nullptr);"
        },
        {
            "sha": "88c169bc4a80d37a6edbaf9495d5f7171f66dd4b",
            "filename": "tensorflow/core/common_runtime/lower_case_op.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_case_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_case_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_case_op.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -38,7 +38,7 @@ class CaseBuilder {\n  public:\n   // Create a CaseBuilder to create the lowered form of `case` with branch\n   // functions identified by `branch_fn_names` in the `graph`.\n-  CaseBuilder(Node* case_op, const std::vector<string>& branch_fn_names,\n+  CaseBuilder(Node* case_op, const std::vector<std::string>& branch_fn_names,\n               bool keep_node_fetchable, Graph* graph);\n \n   // Constructs the basic conditional control flow using switch and merge nodes.\n@@ -58,7 +58,7 @@ class CaseBuilder {\n  private:\n   // Returns unique name containing the name of the Case op being rewritten\n   // (name_), infix and a suffix to ensure it is unique within the graph.\n-  string NewName(const string& infix);\n+  std::string NewName(const std::string& infix);\n \n   // Adds input to both the then and else nodes from src:src_output.\n   absl::Status AddInput(Node* src, int src_output);\n@@ -88,15 +88,15 @@ class CaseBuilder {\n   // for the side effects.\n   Node* branch_executed_node_;\n   Graph* graph_;\n-  string name_;\n+  std::string name_;\n   bool keep_node_fetchable_;\n \n   NodeDebugInfo debug_info_;\n   std::vector<NodeBuilder> branch_call_builders_;\n };\n \n CaseBuilder::CaseBuilder(Node* case_op,\n-                         const std::vector<string>& branch_fn_names,\n+                         const std::vector<std::string>& branch_fn_names,\n                          bool keep_node_fetchable, Graph* graph)\n     : case_op_(case_op),\n       num_branches_(branch_fn_names.size()),\n@@ -106,7 +106,7 @@ CaseBuilder::CaseBuilder(Node* case_op,\n       debug_info_(*case_op_) {\n   branch_call_builders_.reserve(num_branches_);\n   for (int b = 0; b < num_branches_; b++) {\n-    branch_call_builders_.emplace_back(NewName(strings::StrCat(\"branch\", b)),\n+    branch_call_builders_.emplace_back(NewName(absl::StrCat(\"branch\", b)),\n                                        branch_fn_names[b], graph->op_registry(),\n                                        &debug_info_);\n     branch_call_builders_[b].Device(case_op_->requested_device());\n@@ -129,7 +129,7 @@ absl::Status CaseBuilder::CreatePivotNodes() {\n   control_predecessor_ = branch_index;\n   pivots_.resize(num_branches_, nullptr);\n   for (int b = 0; b < num_branches_; b++) {\n-    TF_RETURN_IF_ERROR(NodeBuilder(NewName(strings::StrCat(\"pivot_\", b)),\n+    TF_RETURN_IF_ERROR(NodeBuilder(NewName(absl::StrCat(\"pivot_\", b)),\n                                    \"Identity\", graph_->op_registry(),\n                                    &debug_info_)\n                            .Input(branch_index, b)\n@@ -139,8 +139,8 @@ absl::Status CaseBuilder::CreatePivotNodes() {\n   return absl::OkStatus();\n }\n \n-string CaseBuilder::NewName(const string& infix) {\n-  return graph_->NewName(strings::StrCat(name_, \"/\", infix));\n+std::string CaseBuilder::NewName(const std::string& infix) {\n+  return graph_->NewName(absl::StrCat(name_, \"/\", infix));\n }\n \n absl::Status CaseBuilder::AddInput(Node* src, int src_output) {\n@@ -276,7 +276,7 @@ absl::Status RewriteCaseNode(Node* n, Graph* g, bool keep_node_fetchable) {\n   }\n \n   int num_branches = branches_attr->list().func_size();\n-  std::vector<string> branch_fn_names;\n+  std::vector<std::string> branch_fn_names;\n   branch_fn_names.reserve(num_branches);\n   for (int b = 0; b < num_branches; b++) {\n     branch_fn_names.emplace_back(branches_attr->list().func(b).name());"
        },
        {
            "sha": "d460d761fc646d6a62892102de999e8e49364847",
            "filename": "tensorflow/core/common_runtime/lower_case_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_case_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_case_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_case_op_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -184,8 +184,8 @@ TEST(LowerCaseOpTest, BranchFunctionsWithoutOutputs) {\n   using FDH = ::tensorflow::FunctionDefHelper;\n \n   // Wrap AssignAddVariable + Const into a function.\n-  const auto assign_add = [](const string& fn_name, int v) {\n-    const Tensor tensor = test::AsScalar<int32>(v);\n+  const auto assign_add = [](const std::string& fn_name, int v) {\n+    const Tensor tensor = test::AsScalar<int32_t>(v);\n     return FDH::Create(\n         fn_name, {\"v: resource\"}, {}, {},\n         {"
        },
        {
            "sha": "3a2de9036df4336522aa403cb4979be6a86b3414",
            "filename": "tensorflow/core/common_runtime/lower_function_call_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_function_call_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_function_call_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_function_call_op_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -36,13 +36,13 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-AttrValue FuncAttr(const string& name) {\n+AttrValue FuncAttr(const std::string& name) {\n   AttrValue attr;\n   attr.mutable_func()->set_name(name);\n   return attr;\n }\n \n-AttrValue FuncAttr(const string& name, const DataType type) {\n+AttrValue FuncAttr(const std::string& name, const DataType type) {\n   AttrValue attr;\n   attr.mutable_func()->set_name(name);\n   (*attr.mutable_func()->mutable_attr())[\"T\"].set_type(type);"
        },
        {
            "sha": "a2c2b6986a5e8b677961c493376793012fd90c2e",
            "filename": "tensorflow/core/common_runtime/lower_functional_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_functional_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_functional_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_functional_ops.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -52,7 +52,7 @@ bool CheckBoolAttr(const Node* n, absl::string_view attr_name) {\n \n // Checks if string attribute is defined and it's not empty.\n bool CheckStringAttr(const Node* n, absl::string_view attr_name) {\n-  string match;\n+  std::string match;\n   bool found = TryGetNodeAttr(n->attrs(), attr_name, &match);\n   return found && !match.empty();\n }"
        },
        {
            "sha": "2d47ac5d70bd3c061fdbe5c472f0b02038f23d90",
            "filename": "tensorflow/core/common_runtime/lower_functional_ops_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_functional_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_functional_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_functional_ops_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -66,7 +66,7 @@ absl::Status Rewrite(std::unique_ptr<Graph>* graph) {\n \n // (counter:int32, pred:bool, x:int32) -> counter < N\n FunctionDef WhileWithIfCond(int32_t N) {\n-  const Tensor kN = test::AsScalar<int32>(N);\n+  const Tensor kN = test::AsScalar<int32_t>(N);\n   return FDH::Define(\n       // Name\n       \"WhileWithIfCond\",\n@@ -90,7 +90,7 @@ FunctionDef WhileWithIfBody() {\n   then_func.set_name(\"XTimesTwo\");\n   NameAttrList else_func;\n   else_func.set_name(\"XTimesFour\");\n-  const Tensor kOne = test::AsScalar<int32>(1);\n+  const Tensor kOne = test::AsScalar<int32_t>(1);\n   std::vector<DataType> input_types = {DT_INT32};\n   std::vector<DataType> output_types = {DT_INT32};\n   return FDH::Define("
        },
        {
            "sha": "01beef8fc2328d8698be62962e9a02611bfd9fd2",
            "filename": "tensorflow/core/common_runtime/lower_if_op.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_if_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_if_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_if_op.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -60,7 +60,7 @@ class CondBuilder {\n  private:\n   // Returns unique name containing the name of the If op being rewritten\n   // (name_), infix and a suffix to ensure it is unique within the graph.\n-  string NewName(const string& infix);\n+  std::string NewName(const std::string& infix);\n \n   // Adds input to both the then and else nodes from src:src_output.\n   absl::Status AddInput(Node* src, int src_output);\n@@ -102,7 +102,7 @@ class CondBuilder {\n   // executed for the side effects.\n   Node* branch_executed_node_;\n   Graph* graph_;\n-  string name_;\n+  std::string name_;\n   bool keep_node_fetchable_;\n \n   NodeDebugInfo debug_info_;\n@@ -172,8 +172,8 @@ absl::Status CondBuilder::CreatePivotNodes() {\n   return absl::OkStatus();\n }\n \n-string CondBuilder::NewName(const string& infix) {\n-  return graph_->NewName(strings::StrCat(name_, \"/\", infix));\n+std::string CondBuilder::NewName(const std::string& infix) {\n+  return graph_->NewName(absl::StrCat(name_, \"/\", infix));\n }\n \n absl::Status CondBuilder::AddInput(Node* src, int src_output) {"
        },
        {
            "sha": "68c55d27d164337b2a78393b7b01cb9bebc19ed7",
            "filename": "tensorflow/core/common_runtime/lower_if_op_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_if_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_if_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_if_op_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -35,7 +35,7 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-AttrValue FuncAttr(const string& name) {\n+AttrValue FuncAttr(const std::string& name) {\n   AttrValue attr;\n   attr.mutable_func()->set_name(name);\n   return attr;\n@@ -153,8 +153,8 @@ TEST(LowerIfOpTest, BranchFunctionsWithoutOutputs) {\n   using FDH = ::tensorflow::FunctionDefHelper;\n \n   // Wrap AssignAddVariable + Const into a function.\n-  const auto assign_add = [](const string& fn_name, int v) {\n-    const Tensor tensor = test::AsScalar<int32>(v);\n+  const auto assign_add = [](const std::string& fn_name, int v) {\n+    const Tensor tensor = test::AsScalar<int32_t>(v);\n     return FDH::Create(\n         fn_name, {\"v: resource\"}, {}, {},\n         {"
        },
        {
            "sha": "84f03444a9397217ff4ea5fc958fbfb5eeaa8d31",
            "filename": "tensorflow/core/common_runtime/lower_while_op.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_while_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_while_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_while_op.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -132,7 +132,7 @@ class LowerWhileHelper {\n \n   // Returns unique name containing the name of the While op being rewritten\n   // (name_), infix and a suffix to ensure it is unique within the graph.\n-  string NewName(const string& infix);\n+  std::string NewName(const std::string& infix);\n \n   // Returns true if the input at index is a resource and the same resource is\n   // returned as an output.\n@@ -156,7 +156,7 @@ class LowerWhileHelper {\n   Graph* graph_;\n   const FunctionLibraryDefinition* flib_def_;\n   // Name of the `while_op_`.\n-  string name_;\n+  std::string name_;\n   // Max number of parallel_iterations for the while loop.\n   const int parallel_iterations_;\n   bool keep_node_fetchable_;\n@@ -363,15 +363,15 @@ absl::Status LowerWhileHelper::CreateSwitchNodes() {\n     if (IsLoopCarriedResource(i)) {\n       continue;\n     }\n-    string op_name;\n+    std::string op_name;\n     {\n       const Node* input_node;\n       TF_RETURN_IF_ERROR(while_op_->input_node(i, &input_node));\n-      op_name = strings::StrCat(input_node->name(), \"_switch\");\n+      op_name = absl::StrCat(input_node->name(), \"_switch\");\n     }\n     Node* merge_node = merge_nodes_[op_input_output_to_lowered_node_[i]];\n     Node* switch_node;\n-    string op_type = \"Switch\";\n+    std::string op_type = \"Switch\";\n     if (IsRefType(merge_node->output_type(0))) {\n       op_type = \"RefSwitch\";\n     }\n@@ -413,7 +413,7 @@ absl::Status LowerWhileHelper::CreateBodyFuncCallNode() {\n   // node is not the first one to be ready? Can we speed that case up using some\n   // sort of multi-input Merge?\n   Node* body_control_node_;\n-  string op_type = \"Identity\";\n+  std::string op_type = \"Identity\";\n   if (IsRefType(switch_nodes_[0]->output_type(1))) {\n     op_type = \"RefIdentity\";\n   }\n@@ -569,8 +569,8 @@ absl::Status LowerWhileHelper::UpdateConsumers() {\n   return absl::OkStatus();\n }\n \n-string LowerWhileHelper::NewName(const string& infix) {\n-  return graph_->NewName(strings::StrCat(name_, \"/\", infix));\n+std::string LowerWhileHelper::NewName(const std::string& infix) {\n+  return graph_->NewName(absl::StrCat(name_, \"/\", infix));\n }\n \n bool LowerWhileHelper::IsLoopCarriedResource(int index) {"
        },
        {
            "sha": "eb19c84c04dd444ef804acbcf7456a5fc2009d51",
            "filename": "tensorflow/core/common_runtime/lower_while_op_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_while_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Flower_while_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Flower_while_op_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -253,7 +253,8 @@ TEST(LowerWhileOpTest, ForwardAssignedInputDevice) {\n   TF_CHECK_OK(NodeBuilder(\"placed_node\", \"Placeholder\")\n                   .Attr(\"dtype\", type)\n                   .Finalize(graph.get(), &placeholder));\n-  const string assigned_device_name = \"/job:localhost/replica:0/task:0/gpu:0\";\n+  const std::string assigned_device_name =\n+      \"/job:localhost/replica:0/task:0/gpu:0\";\n   placeholder->set_assigned_device_name(assigned_device_name);\n   Node* while_node;\n   std::vector<NodeBuilder::NodeOut> inputs({NodeBuilder::NodeOut(placeholder)});\n@@ -343,11 +344,11 @@ TEST(LowerWhileOpTest, ForwardRequestedInputDevice) {\n   TF_ASSERT_OK(graph->AddFunctionLibrary(f_lib_proto));\n   auto type = DT_FLOAT;\n   // We will place the loop var on the gpu:0.\n-  const string gpu_0_device = \"/job:localhost/replica:0/task:0/gpu:0\";\n+  const std::string gpu_0_device = \"/job:localhost/replica:0/task:0/gpu:0\";\n   // We will place loop's control input on the gpu:1.\n-  const string gpu_1_device = \"/job:localhost/replica:0/task:0/gpu:1\";\n+  const std::string gpu_1_device = \"/job:localhost/replica:0/task:0/gpu:1\";\n   // We will place While op on gpu:2.\n-  const string gpu_2_device = \"/job:localhost/replica:0/task:0/gpu:2\";\n+  const std::string gpu_2_device = \"/job:localhost/replica:0/task:0/gpu:2\";\n   Node* gpu_0_ph;\n   TF_CHECK_OK(NodeBuilder(\"placed_node\", \"Placeholder\")\n                   .Attr(\"dtype\", type)\n@@ -483,11 +484,11 @@ TEST(LowerWhileOpTest, ForwardColocationKeyAttribute) {\n   TF_ASSERT_OK(graph->AddFunctionLibrary(f_lib_proto));\n   auto type = DT_FLOAT;\n   // We will place the loop var on the gpu:0.\n-  const string gpu_0_device = \"/job:localhost/replica:0/task:0/gpu:0\";\n+  const std::string gpu_0_device = \"/job:localhost/replica:0/task:0/gpu:0\";\n   // We will place loop's control input on the gpu:1.\n-  const string gpu_1_device = \"/job:localhost/replica:0/task:0/gpu:1\";\n+  const std::string gpu_1_device = \"/job:localhost/replica:0/task:0/gpu:1\";\n   // We will place While op on gpu:2.\n-  const string gpu_2_device = \"/job:localhost/replica:0/task:0/gpu:2\";\n+  const std::string gpu_2_device = \"/job:localhost/replica:0/task:0/gpu:2\";\n   Node* gpu_0_ph;\n   AttrValue gpu_0_colocation_attr;\n   gpu_0_colocation_attr.mutable_list()->add_s(\"loc@:some_op_on_gpu_0_device\");"
        },
        {
            "sha": "216fdfd6d239c406c35c2ecc0c4a1e987f067011",
            "filename": "tensorflow/core/common_runtime/memory_types.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -34,14 +34,14 @@ struct Endpoint {\n };\n \n struct EndpointHash {\n-  uint32 operator()(const Endpoint& x) const {\n+  uint32_t operator()(const Endpoint& x) const {\n     return Hash32(reinterpret_cast<const char*>(&x.node_id), sizeof(int),\n                   x.output_index);\n   }\n };\n \n struct EndpointEq {\n-  uint32 operator()(const Endpoint& x, const Endpoint& y) const {\n+  uint32_t operator()(const Endpoint& x, const Endpoint& y) const {\n     return (x.node_id == y.node_id) && (x.output_index == y.output_index);\n   }\n };\n@@ -116,14 +116,14 @@ absl::Status ValidateMemoryTypes(const DeviceType& device_type,\n // within this process. That is sufficient because EnsureMemoryTypes\n // is only used on a TensorFlow graph that is gonna to be executed in\n // a single tf device (hence within a single process).\n-static string GetTensorName(const Edge* edge) {\n+static std::string GetTensorName(const Edge* edge) {\n   static std::atomic<int64_t> counter(0);\n-  return strings::StrCat(\"memtype_\", counter.fetch_add(1), \"_\",\n-                         edge->src()->name());\n+  return absl::StrCat(\"memtype_\", counter.fetch_add(1), \"_\",\n+                      edge->src()->name());\n }\n \n-static Node* Send(Graph* g, const string& tensor_name,\n-                  const string& device_name, bool host, const Edge* edge) {\n+static Node* Send(Graph* g, const std::string& tensor_name,\n+                  const std::string& device_name, bool host, const Edge* edge) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), host ? \"_HostSend\" : \"_Send\")\n                   .Input(edge->src(), edge->src_output())\n@@ -138,8 +138,8 @@ static Node* Send(Graph* g, const string& tensor_name,\n   return ret;\n }\n \n-static Node* Recv(Graph* g, const string& tensor_name,\n-                  const string& device_name, bool host, const Edge* edge) {\n+static Node* Recv(Graph* g, const std::string& tensor_name,\n+                  const std::string& device_name, bool host, const Edge* edge) {\n   Node* ret;\n   TF_CHECK_OK(\n       NodeBuilder(g->NewName(\"n\"), host ? \"_HostRecv\" : \"_Recv\")\n@@ -156,7 +156,7 @@ static Node* Recv(Graph* g, const string& tensor_name,\n }\n \n absl::Status EnsureMemoryTypes(const DeviceType& device_type,\n-                               const string& device_name, Graph* g) {\n+                               const std::string& device_name, Graph* g) {\n   struct Item {\n     const Edge* edge;\n     MemoryType sm;\n@@ -191,7 +191,7 @@ absl::Status EnsureMemoryTypes(const DeviceType& device_type,\n       Endpoint key{e->src()->id(), e->src_output()};\n       auto iter = recv_nodes.find(key);\n       if (iter == recv_nodes.end()) {\n-        const string tensor_name = GetTensorName(e);\n+        const std::string tensor_name = GetTensorName(e);\n         Node* send =\n             Send(g, tensor_name, device_name, (item.sm == HOST_MEMORY), e);\n         recv = Recv(g, tensor_name, device_name, (item.dm == HOST_MEMORY), e);"
        },
        {
            "sha": "bbadfe24e156c8e9e55bfeaa0e89dfe28aa5d074",
            "filename": "tensorflow/core/common_runtime/memory_types.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types.h?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -36,7 +36,7 @@ absl::Status ValidateMemoryTypes(const DeviceType& device_type, const Graph* g);\n // be OK). Otherwise, returns an error and '*g' may be in an\n // invalidate state and the caller should discard it.\n absl::Status EnsureMemoryTypes(const DeviceType& device_type,\n-                               const string& device_name, Graph* g);\n+                               const std::string& device_name, Graph* g);\n \n // Get the memory type for 'index'th output of node 'n' in graph 'g', when\n // running on 'device_type'."
        },
        {
            "sha": "0be9855767940699367f7c31aceb883adda8ba44",
            "filename": "tensorflow/core/common_runtime/memory_types_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/136038ca4cc2d9745271f40d01366eb7a90eb27f/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types_test.cc?ref=136038ca4cc2d9745271f40d01366eb7a90eb27f",
            "patch": "@@ -30,7 +30,7 @@ namespace tensorflow {\n TEST(MemoryTypeChecker, Int32OK) {\n   Graph* g = new Graph(OpRegistry::Global());\n   Tensor v(DT_INT32, {});\n-  v.scalar<int32>().setZero();\n+  v.scalar<int32_t>().setZero();\n   auto in0 = test::graph::Constant(g, v);\n   auto in1 = test::graph::Constant(g, v);\n   test::graph::Add(g, in0, in1);\n@@ -45,7 +45,7 @@ TEST(MemoryTypeChecker, Int32OK) {\n TEST(MemoryTypeChecker, Int32NotOk) {\n   Graph* g = new Graph(OpRegistry::Global());\n   Tensor v(DT_INT32, {});\n-  v.scalar<int32>().setZero();\n+  v.scalar<int32_t>().setZero();\n   auto x = test::graph::Constant(g, v);\n   test::graph::Cast(g, x, DT_FLOAT);\n   TF_EXPECT_OK(ValidateMemoryTypes(DEVICE_CPU, g));"
        }
    ],
    "stats": {
        "total": 720,
        "additions": 369,
        "deletions": 351
    }
}