{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Unpack vector writes of sub-byte types.\n\nThe default lowering of transfer_read/write of a sub-byte vector type packs the elements, the semantics we want are un-packed so we have to explicitly do this.\n\nNot the most efficient but works for now.\n\nPiperOrigin-RevId: 840256967",
    "sha": "c90d7abd36d230ce3ec80171428d4584876bde0c",
    "files": [
        {
            "sha": "b8ed2a789b02d438cc20ac183570915eb62585d6",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -247,6 +247,7 @@ cc_library(\n         \"@llvm-project//mlir:TensorTransforms\",\n         \"@llvm-project//mlir:ToLLVMIRTranslation\",\n         \"@llvm-project//mlir:Transforms\",\n+        \"@llvm-project//mlir:UBDialect\",\n         \"@llvm-project//mlir:UBToLLVM\",\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@llvm-project//mlir:VectorToLLVM\","
        },
        {
            "sha": "edbb6752c8bd4a9475896cfd28536570621d9c5a",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -74,6 +74,7 @@ limitations under the License.\n #include \"mlir/Dialect/SCF/Transforms/BufferizableOpInterfaceImpl.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/Dialect/Tensor/Transforms/BufferizableOpInterfaceImpl.h\"\n+#include \"mlir/Dialect/UB/IR/UBOps.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/Dialect/Vector/Transforms/BufferizableOpInterfaceImpl.h\"\n #include \"mlir/Dialect/Vector/Transforms/Passes.h\"\n@@ -351,6 +352,8 @@ static void AddTiledLoweringPasses(mlir::OpPassManager& pm, bool fast_min_max) {\n   pm.addPass(cpu::createLowerToLLVMPass());\n   pm.addPass(mlir::createConvertVectorToSCFPass(\n       mlir::VectorTransferToSCFOptions().enableFullUnroll(false)));\n+  pm.addPass(cpu::CreateUnpackSubByteVectorWritePass());\n+\n   mlir::ConvertVectorToLLVMPassOptions options;\n \n   // If the tile size is 16x16 this will generate the most efficient code for\n@@ -539,7 +542,8 @@ mlir::DialectRegistry FusionCompiler::CreateDialectRegistry(\n       mlir::scf::SCFDialect, mlir::LLVM::LLVMDialect,\n       mlir::tensor::TensorDialect, mlir::vector::VectorDialect, xla::XlaDialect,\n       xla::xtile::XTileDialect, mlir::stablehlo::StablehloDialect,\n-      mlir::linalg::LinalgDialect, mlir::memref::MemRefDialect>();\n+      mlir::linalg::LinalgDialect, mlir::memref::MemRefDialect,\n+      mlir::ub::UBDialect>();\n \n   mlir::LLVM::registerInlinerInterface(registry);\n   mlir::func::registerInlinerExtension(registry);"
        },
        {
            "sha": "ca12e486f22b8ef19e9856de5908cc74698bdd5f",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/tiled_kernel_test.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -63,6 +63,10 @@ def compare_kernel(\n   )\n \n   def get_input(spec: InputSpec):\n+    if dtype == np.bool:\n+      return (\n+          np.arange(np.prod(spec.shape), dtype=np.int8).reshape(spec.shape) % 2\n+      )\n     return np.arange(np.prod(spec.shape), dtype=dtype).reshape(spec.shape)\n \n   inputs = [get_input(spec) for spec in input_specs]\n@@ -474,6 +478,35 @@ def test_broadcast_in_dim_outer(self):\n         lambda input: np.transpose(np.broadcast_to(input, (32, 4))),\n     )\n \n+  def test_i1_reshape_transpose(self):\n+    ir = \"\"\"\n+      module @__compute_module_bitcast_copy_fusion {\n+        xtile.entry_func @bitcast_copy_fusion(\n+            %arg0: memref<2x3xi8>, %arg1: memref<2x3x1xi8>,\n+            %arg2: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count : 1, tiles_per_workgroup : 1>} {\n+          %c0 = arith.constant 0 : index\n+          %1 = xtile.extract %arg0[%c0, %c0] [2, 4] [1, 1] : memref<2x3xi8> -> tensor<2x4xi8>\n+          %cst = arith.constant dense<0> : tensor<2x4xi8>\n+          %2 = arith.cmpi ne, %1, %cst : tensor<2x4xi8>\n+          %3 = stablehlo.reshape %2 : (tensor<2x4xi1>) -> tensor<1x2x4xi1>\n+          %4 = stablehlo.transpose %3, dims = [1, 2, 0] : (tensor<1x2x4xi1>) -> tensor<2x4x1xi1>\n+          %5 = arith.extui %4 : tensor<2x4x1xi1> to tensor<2x4x1xi8>\n+          xtile.insert %5 into %arg1[%c0, %c0, %c0] [2, 4, 1] [1, 1, 1] : tensor<2x4x1xi8> -> memref<2x3x1xi8>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"bitcast_copy_fusion\",\n+        1,\n+        [InputSpec((2, 3))],\n+        (2, 3),\n+        np.bool,\n+        lambda input: input,\n+    )\n+\n \n if __name__ == \"__main__\":\n   absltest.main()"
        },
        {
            "sha": "d81c055796750836f5c23eff53ef7a61a6d335ee",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -54,6 +54,7 @@ cc_library(\n         \"memref_copy_to_loops.cc\",\n         \"shlo_to_vector.cc\",\n         \"tensor_ops_to_bufferizable.cc\",\n+        \"unpack_sub_byte_vector_write_pass.cc\",\n         \"vector_to_scalar_pass.cc\",\n     ],\n     hdrs = [\"passes.h\"],"
        },
        {
            "sha": "39071eeae125fd3f52c2651acd482643f6fbcee4",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -41,6 +41,7 @@ std::unique_ptr<mlir::Pass> CreateTensorOpsToBufferizablePass();\n std::unique_ptr<mlir::Pass> CreateMemrefCopyToLoopsPass();\n std::unique_ptr<mlir::Pass> CreateFuseElementwisePass();\n std::unique_ptr<mlir::Pass> CreateVectorToScalarPass();\n+std::unique_ptr<mlir::Pass> CreateUnpackSubByteVectorWritePass();\n \n #define GEN_PASS_REGISTRATION\n #include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\""
        },
        {
            "sha": "5f0cfaeb473f902751298fbcdb1bbecf3fa9a621",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -107,3 +107,21 @@ def VectorToScalarPass : Pass<\"xtile-cpu-vector-to-scalar\"> {\n     \"::mlir::vector::VectorDialect\",\n   ];\n }\n+\n+\n+def UnpackSubByteVectorWritePass : Pass<\"xtile-cpu-unpack-sub-byte-vector-write\"> {\n+  let summary = \"Unpacks sub-byte vector writes.\";\n+\n+  let description = [{\n+    The default lowering of vector.transfer_write of subbyte types reflects that\n+    of llvm vector store instructions, i.e it writes the packed data directly.\n+    The semantics we use in the CPU lowering is that it will unpack the data\n+    into a series of bytes and store them separately. This pass implements that\n+    by using unrolling the transfer_write into individual elements.\n+  }];\n+\n+  let dependentDialects = [\n+    \"::mlir::memref::MemRefDialect\",\n+    \"::mlir::scf::SCFDialect\",\n+  ];\n+}"
        },
        {
            "sha": "507bea82b6f5b9b420ae9e4ba956673ff054f44e",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/unpack_sub_byte_vector_write_pass.mlir",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Funpack_sub_byte_vector_write_pass.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Funpack_sub_byte_vector_write_pass.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Funpack_sub_byte_vector_write_pass.mlir?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -0,0 +1,84 @@\n+// RUN: fusion_compiler_opt %s \\\n+// RUN:   -xtile-cpu-unpack-sub-byte-vector-write -split-input-file \\\n+// RUN:   | FileCheck %s\n+\n+func.func @unpacks_1d_read(%arg0 : memref<8xi1>) -> vector<2xi1> {\n+  // CHECK-DAG: %[[MASK:.*]] = ub.poison\n+  // CHECK-DAG: %[[C2:.*]] = arith.constant 2\n+  // CHECK-DAG: %[[C3:.*]] = arith.constant 3\n+  // CHECK: %[[READ0:.*]] = vector.transfer_read %arg0[%[[C2]]], %[[MASK]]\n+  // CHECK: %[[EXTRACT0:.*]] = vector.extract %[[READ0]][0]\n+  // CHECK: %[[READ1:.*]] = vector.transfer_read %arg0[%[[C3]]], %[[MASK]]\n+  // CHECK: %[[EXTRACT1:.*]] = vector.extract %[[READ1]][0]\n+  // CHECK: %[[RESULT:.*]] = vector.from_elements %[[EXTRACT0]], %[[EXTRACT1]]\n+  %mask = ub.poison : i1\n+  %c2 = arith.constant 2 : index\n+  %result = vector.transfer_read %arg0[%c2], %mask : memref<8xi1>, vector<2xi1>\n+  // CHECK: return %[[RESULT]]\n+  return %result : vector<2xi1>\n+}\n+\n+//-----\n+\n+func.func @ignores_trivial_case_read(%arg0 : memref<8xi1>) -> vector<1xi1> {\n+  // CHECK: %[[MASK:.*]] = ub.poison\n+  %mask = ub.poison : i1\n+  // CHECK-NEXT: %[[C2:.*]] = arith.constant 2\n+  %c2 = arith.constant 2 : index\n+  // CHECK-NEXT: %[[RESULT:.*]] = vector.transfer_read %arg0[%[[C2]]], %[[MASK]]\n+  %result = vector.transfer_read %arg0[%c2], %mask : memref<8xi1>, vector<1xi1>\n+  // CHECK-NEXT: return %[[RESULT]]\n+  return %result : vector<1xi1>\n+}\n+\n+//-----\n+\n+func.func @ignores_non_sub_byte_type_read(%arg0 : memref<8xi8>) -> vector<2xi8> {\n+  // CHECK: %[[MASK:.*]] = ub.poison\n+  %mask = ub.poison : i8\n+  // CHECK-NEXT: %[[C2:.*]] = arith.constant 2\n+  %c2 = arith.constant 2 : index\n+  // CHECK-NEXT: %[[RESULT:.*]] = vector.transfer_read %arg0[%[[C2]]], %[[MASK]]\n+  %result = vector.transfer_read %arg0[%c2], %mask : memref<8xi8>, vector<2xi8>\n+  // CHECK-NEXT: return %[[RESULT]]\n+  return %result : vector<2xi8>\n+}\n+\n+//-----\n+\n+func.func @unpacks_1d_write(%arg0 : vector<2xi1>, %arg1 : memref<8xi1>) {\n+  // CHECK-DAG: %[[C2:.*]] = arith.constant 2\n+  // CHECK-DAG: %[[C3:.*]] = arith.constant 3\n+  // CHECK: %[[ELEMENT0:.*]] = vector.extract %arg0[0]\n+  // CHECK: %[[VECTOR0:.*]] = vector.from_elements %[[ELEMENT0]]\n+  // CHECK: vector.transfer_write %[[VECTOR0]], %arg1[%[[C2]]]\n+  // CHECK: %[[ELEMENT1:.*]] = vector.extract %arg0[1]\n+  // CHECK: %[[VECTOR1:.*]] = vector.from_elements %[[ELEMENT1]]\n+  // CHECK: vector.transfer_write %[[VECTOR1]], %arg1[%[[C3]]]\n+  %c2 = arith.constant 2 : index\n+  vector.transfer_write %arg0, %arg1[%c2] : vector<2xi1>, memref<8xi1>\n+  // CHECK-NEXT: return\n+  return\n+}\n+\n+//-----\n+\n+func.func @ignores_trivial_case_write(%arg0 : vector<1xi1>, %arg1 : memref<8xi1>) {\n+  // CHECK: %[[C2:.*]] = arith.constant 2\n+  %c2 = arith.constant 2 : index\n+  // CHECK-NEXT: vector.transfer_write %arg0, %arg1[%[[C2]]]\n+  vector.transfer_write %arg0, %arg1[%c2] : vector<1xi1>, memref<8xi1>\n+  // CHECK-NEXT: return\n+  return\n+}\n+\n+//-----\n+\n+func.func @ignores_non_sub_byte_type_write(%arg0 : vector<2xi8>, %arg1 : memref<8xi8>) {\n+  // CHECK: %[[C2:.*]] = arith.constant 2\n+  %c2 = arith.constant 2 : index\n+  // CHECK-NEXT: vector.transfer_write %arg0, %arg1[%[[C2]]]\n+  vector.transfer_write %arg0, %arg1[%c2] : vector<2xi8>, memref<8xi8>\n+  // CHECK-NEXT: return\n+  return\n+}"
        },
        {
            "sha": "a31d3774a814599835a7e628b8d25647d5048dc9",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/unpack_sub_byte_vector_write_pass.cc",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Funpack_sub_byte_vector_write_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Funpack_sub_byte_vector_write_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Funpack_sub_byte_vector_write_pass.cc?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -0,0 +1,183 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <cstdint>\n+#include <memory>\n+#include <utility>\n+\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/AffineExpr.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DEF_UNPACKSUBBYTEVECTORWRITEPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+mlir::Value Get1DVectorElement(mlir::OpBuilder& builder, mlir::Location loc,\n+                               mlir::TypedValue<mlir::VectorType> vector,\n+                               int64_t idx) {\n+  auto vector_1d_type =\n+      mlir::VectorType::get({1}, vector.getType().getElementType());\n+  mlir::Value element =\n+      mlir::vector::ExtractOp::create(builder, loc, vector, idx);\n+  return mlir::vector::FromElementsOp::create(builder, loc, vector_1d_type,\n+                                              element);\n+}\n+\n+mlir::ArrayAttr GetBoundsAttr(mlir::OpBuilder& builder) {\n+  // I'm not confident what the semantics of inbounds are with subbyte types so\n+  // lets be conservative and set them all to false, the canonicalization pass\n+  // will set it to true where possible.\n+  llvm::SmallVector<bool> in_bounds{false};\n+  return builder.getBoolArrayAttr(in_bounds);\n+}\n+\n+template <typename TransferOp>\n+mlir::LogicalResult CheckCanUnroll(TransferOp op,\n+                                   mlir::PatternRewriter& rewriter) {\n+  if (!mlir::isa<mlir::MemRefType>(op.getBase().getType())) {\n+    return rewriter.notifyMatchFailure(op, \"base is not a memref.\");\n+  }\n+\n+  auto vector_type = op.getVectorType();\n+\n+  if (!vector_type.getElementType().isIntOrIndexOrFloat() ||\n+      vector_type.getElementType().getIntOrFloatBitWidth() >= 8) {\n+    return rewriter.notifyMatchFailure(op,\n+                                       \"element type is not a sub-byte type.\");\n+  }\n+\n+  auto num_elements = vector_type.getNumElements();\n+  if (vector_type.getRank() != 1 || num_elements == 1) {\n+    return rewriter.notifyMatchFailure(op, \"vector is already trivial.\");\n+  }\n+\n+  return mlir::success();\n+}\n+\n+mlir::LogicalResult UnrollTransferRead(mlir::vector::TransferReadOp op,\n+                                       mlir::PatternRewriter& rewriter) {\n+  if (auto status = CheckCanUnroll(op, rewriter); mlir::failed(status)) {\n+    return status;\n+  }\n+\n+  auto vector_type = op.getVectorType();\n+  auto num_elements = vector_type.getNumElements();\n+\n+  auto in_bounds_attr = GetBoundsAttr(rewriter);\n+\n+  llvm::SmallVector<mlir::Value> elements;\n+  for (int64_t idx = 0; idx != num_elements; ++idx) {\n+    auto vector_1d_type =\n+        mlir::VectorType::get({1}, vector_type.getElementType());\n+\n+    mlir::Value mask = op.getMask() ? Get1DVectorElement(rewriter, op->getLoc(),\n+                                                         op.getMask(), idx)\n+                                    : nullptr;\n+\n+    llvm::SmallVector<mlir::Value> offsets = op.getIndices();\n+    offsets.back() = mlir::arith::AddIOp::create(\n+        rewriter, op->getLoc(), offsets.back(),\n+        mlir::arith::ConstantIndexOp::create(rewriter, op->getLoc(), idx));\n+\n+    mlir::Value element_vector = mlir::vector::TransferReadOp::create(\n+        rewriter, op->getLoc(), vector_1d_type, op.getBase(), offsets,\n+        op.getPermutationMapAttr(), op.getPadding(), mask, in_bounds_attr);\n+    mlir::Value element = mlir::vector::ExtractOp::create(\n+        rewriter, op->getLoc(), element_vector, 0);\n+    elements.push_back(element);\n+  }\n+\n+  rewriter.replaceOpWithNewOp<mlir::vector::FromElementsOp>(op, vector_type,\n+                                                            elements);\n+  return mlir::success();\n+}\n+\n+mlir::LogicalResult UnrollTransferWrite(mlir::vector::TransferWriteOp op,\n+                                        mlir::PatternRewriter& rewriter) {\n+  if (auto status = CheckCanUnroll(op, rewriter); mlir::failed(status)) {\n+    return status;\n+  }\n+\n+  auto num_elements = op.getVectorType().getNumElements();\n+\n+  auto in_bounds_attr = GetBoundsAttr(rewriter);\n+  for (int64_t idx = 0; idx != num_elements; ++idx) {\n+    mlir::Value element =\n+        Get1DVectorElement(rewriter, op->getLoc(), op.getValueToStore(), idx);\n+    mlir::Value mask = op.getMask() ? Get1DVectorElement(rewriter, op->getLoc(),\n+                                                         op.getMask(), idx)\n+                                    : nullptr;\n+\n+    llvm::SmallVector<mlir::Value> offsets = op.getIndices();\n+    offsets.back() = mlir::arith::AddIOp::create(\n+        rewriter, op->getLoc(), offsets.back(),\n+        mlir::arith::ConstantIndexOp::create(rewriter, op->getLoc(), idx));\n+\n+    mlir::vector::TransferWriteOp::create(\n+        rewriter, op->getLoc(), mlir::TypeRange{}, element, op.getBase(),\n+        offsets, op.getPermutationMapAttr(), mask, in_bounds_attr);\n+  }\n+\n+  rewriter.eraseOp(op);\n+  return mlir::success();\n+}\n+\n+class UnpackSubByteVectorWritePass\n+    : public impl::UnpackSubByteVectorWritePassBase<\n+          UnpackSubByteVectorWritePass> {\n+ public:\n+  using UnpackSubByteVectorWritePassBase::UnpackSubByteVectorWritePassBase;\n+\n+  void runOnOperation() override {\n+    mlir::MLIRContext* context = &getContext();\n+    mlir::RewritePatternSet patterns(context);\n+    patterns.add(UnrollTransferRead);\n+    patterns.add(UnrollTransferWrite);\n+    if (mlir::failed(\n+            mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateUnpackSubByteVectorWritePass() {\n+  return std::make_unique<UnpackSubByteVectorWritePass>();\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "de986615da6c0e1b7c85709e4936a6b74c0e2790",
            "filename": "third_party/xla/xla/codegen/xtile/ir/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c90d7abd36d230ce3ec80171428d4584876bde0c/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD?ref=c90d7abd36d230ce3ec80171428d4584876bde0c",
            "patch": "@@ -105,6 +105,7 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:InferTypeOpInterface\",\n         \"@llvm-project//mlir:InliningUtils\",\n+        \"@llvm-project//mlir:LinalgDialect\",\n         \"@llvm-project//mlir:MemRefDialect\",\n         \"@llvm-project//mlir:SideEffectInterfaces\",\n         \"@llvm-project//mlir:Support\","
        }
    ],
    "stats": {
        "total": 328,
        "additions": 327,
        "deletions": 1
    }
}