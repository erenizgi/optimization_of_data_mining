{
    "author": "tensorflower-gardener",
    "message": "Keep 'Sharding' custom calls in auto-sharding when using Shardy.\n\nPiperOrigin-RevId: 798288482",
    "sha": "6abc3f276ebb5d9707407b03a7a8969e673e7636",
    "files": [
        {
            "sha": "f1c76862ac47e5f9f92b3f536f1d73b27d2f94cf",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc?ref=6abc3f276ebb5d9707407b03a7a8969e673e7636",
            "patch": "@@ -3559,7 +3559,8 @@ absl::StatusOr<bool> AutoShardingImplementation::RunAutoSharding(\n   TF_ASSIGN_OR_RETURN(\n       bool changed,\n       ProcessShardingInstruction(\n-          module, execution_threads, /*replace_sharding_with_copy=*/true,\n+          module, execution_threads,\n+          /*replace_sharding_with_copy=*/option_.replace_sharding_with_copy,\n           &unspecified_dims, /*saved_root_shardings=*/nullptr,\n           /*saved_parameter_shardings=*/nullptr,\n           /*instruction_to_shard_group_id=*/nullptr,\n@@ -3824,7 +3825,8 @@ absl::StatusOr<bool> AutoShardingImplementation::RunAutoSharding(\n       CHECK(instruction->has_sharding());\n       CHECK(instruction->sharding().IsManual());\n       CHECK(instruction->operand(0)->has_sharding());\n-      CHECK(!instruction->operand(0)->sharding().IsManual());\n+      CHECK(spmd::IsShardingCustomCall(instruction->operand(0)) ||\n+            !instruction->operand(0)->sharding().IsManual());\n     } else if (spmd::IsSPMDShardToFullShapeCustomCall(instruction)) {\n       CHECK(instruction->has_sharding());\n       CHECK(!instruction->sharding().IsManual());"
        },
        {
            "sha": "2db2beb0bece3ed68328d75dbf4f918f678c9bb8",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_option.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_option.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_option.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_option.h?ref=6abc3f276ebb5d9707407b03a7a8969e673e7636",
            "patch": "@@ -208,6 +208,11 @@ struct AutoShardingOption {\n   // ops in a principled manner.\n   bool insert_resharding_reshapes_for_non_dot_ops = false;\n \n+  // When folding the sharding attribute to its operand, if the module is\n+  // transformed to Shardy in later steps, we should not replace the sharding\n+  // custom call with copy.\n+  bool replace_sharding_with_copy = true;\n+\n   // The number of slices used\n   std::optional<int64_t> num_dcn_slices = std::nullopt;\n "
        },
        {
            "sha": "03333a03f75c41547a8f121d285ab7afe8144c8a",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_stablehlo_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_stablehlo_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_stablehlo_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_stablehlo_pass.cc?ref=6abc3f276ebb5d9707407b03a7a8969e673e7636",
            "patch": "@@ -130,6 +130,7 @@ class AutoShardingWrapperPass\n     option.device_mesh_shape = device_mesh_shape;\n     // Keep the mesh shape unchanged.\n     option.allow_mixed_mesh_shape = false;\n+    option.replace_sharding_with_copy = false;\n     // TODO(hanruobing): Add an option to control whether to keep the original\n     // sharding or not. The current behavior is to keep the original sharding.\n     // TODO(b/424109294): Figure out whether we need to pass backend-specific"
        },
        {
            "sha": "fcb6911eeff5168979c4bf840bc019137fe571cc",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_strategy.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.cc?ref=6abc3f276ebb5d9707407b03a7a8969e673e7636",
            "patch": "@@ -957,6 +957,8 @@ BuildStrategyAndCost(\n                 \"annotation.\");\n           }\n           generate_non_following_strategies(false);\n+        } else if (IsShardingCustomCall(ins)) {\n+          generate_non_following_strategies(false);\n         } else if (IsTopKCustomCall(ins)) {\n           generate_non_following_strategies(false, {0});\n         } else if (IsPartialReduceCustomCall(ins)) {"
        },
        {
            "sha": "14f7b738100c7e52be137afc1498cea875e6d178",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_util.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6abc3f276ebb5d9707407b03a7a8969e673e7636/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h?ref=6abc3f276ebb5d9707407b03a7a8969e673e7636",
            "patch": "@@ -63,6 +63,10 @@ inline bool IsSPMDShardToFullShapeCustomCall(const HloInstruction* ins) {\n   return ins->IsCustomCall(\"SPMDShardToFullShape\");\n }\n \n+inline bool IsShardingCustomCall(const HloInstruction* ins) {\n+  return ins->IsCustomCall(\"Sharding\");\n+}\n+\n inline std::pair<int, int> ParseMeshDims(const std::string& strategy_name) {\n   if (absl::StrContains(strategy_name, \"{0,1}\")) {\n     return {0, 1};"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 16,
        "deletions": 2
    }
}