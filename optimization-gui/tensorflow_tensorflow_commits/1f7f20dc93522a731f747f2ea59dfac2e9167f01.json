{
    "author": "vwbaker",
    "message": "Update autotuner cache to work with `--xla_gpu_experimental_enable_fusion_autotuner`\n\nSome tests in gpu_compiler_test.cc test the autotuner cache using this file. The change introduced by the flag adds another autotuner pass for transposes/reductions & therefore needs more entries in the file.\n\nPiperOrigin-RevId: 812798127",
    "sha": "1f7f20dc93522a731f747f2ea59dfac2e9167f01",
    "files": [
        {
            "sha": "173819ea69c449c1bb8ddd39cee6bc15680a2aaf",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f7f20dc93522a731f747f2ea59dfac2e9167f01/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f7f20dc93522a731f747f2ea59dfac2e9167f01/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=1f7f20dc93522a731f747f2ea59dfac2e9167f01",
            "patch": "@@ -168,3 +168,28 @@ results {\n     }\n   }\n }\n+results {\n+  device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 9.10.0\"\n+  hlo: \"{\\n  tmp_0 = f8e4m3fn[12288,4096]{0,1} parameter(0)\\n  tmp_1 = f8e4m3fn[4096,12288]{1,0} bitcast(f8e4m3fn[12288,4096]{0,1} tmp_0)\\n  ROOT tmp_2 = f8e4m3fn[12288,4096]{1,0} transpose(f8e4m3fn[4096,12288]{1,0} tmp_1), dimensions={1,0}\\n}\"\n+  result {\n+    other {\n+      name: \"BlockLevelEmitter\"\n+      config {\n+        type_url: \"type.googleapis.com/xla.gpu.BlockLevelFusionConfig\"\n+        value: \"\\020\\004\\032\\004\\n\\002@@ \\001(\\001\"\n+      }\n+    }\n+  }\n+}\n+results {\n+  device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 9.10.0\"\n+  hlo: \"{\\n  tmp_0 = bf16[3,32,1024,4,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[3,32768,4,1024]{3,2,1,0} bitcast(bf16[3,32,1024,4,1024]{4,3,2,1,0} tmp_0)\\n  tmp_2 = bf16[3,4,32768,1024]{3,2,1,0} transpose(bf16[3,32768,4,1024]{3,2,1,0} tmp_1), dimensions={0,2,1,3}\\n  tmp_3 = bf16[3,4,32,1024,1024]{4,3,2,1,0} bitcast(bf16[3,4,32768,1024]{3,2,1,0} tmp_2)\\n  tmp_4 = bf16[1,3,32,1024]{3,2,1,0} parameter(1)\\n  tmp_5 = bf16[3,32,1024]{2,1,0} bitcast(bf16[1,3,32,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[3,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[3,32,1024]{2,1,0} tmp_5), dimensions={0,2,3}\\n  tmp_7 = bf16[3,4,32,1024,1024]{4,3,2,1,0} add(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_3, bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_6)\\n  tmp_8 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_7), slice={[1:2], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_7), slice={[0:1], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_10 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[] constant({...})\\n  tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} broadcast(bf16[] tmp_11), dimensions={}\\n  tmp_13 = bf16[4,32,1024,1024]{3,2,1,0} multiply(bf16[4,32,1024,1024]{3,2,1,0} tmp_10, bf16[4,32,1024,1024]{3,2,1,0} tmp_12)\\n  tmp_14 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_13)\\n  tmp_15 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_14), dimensions={0,2,1}\\n  ROOT tmp_16 = (bf16[1,4,32,1024,1024]{4,3,2,1,0}, bf16[128,1024,1024]{2,1,0}) tuple(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_15)\\n}\"\n+  result {\n+    other {\n+      name: \"NativeEmitter\"\n+      config {\n+        type_url: \"type.googleapis.com/xla.gpu.NativeEmitterBackendConfig\"\n+      }\n+    }\n+  }\n+}"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 25,
        "deletions": 0
    }
}