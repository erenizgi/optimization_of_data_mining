{
    "author": "EusebioDM",
    "message": "Add proto (de)serialisation for `ConvolutionThunk`\n\nPiperOrigin-RevId: 818597563",
    "sha": "607bca2e9c65a2bbe993f5323a01e5b43cd39a4a",
    "files": [
        {
            "sha": "a0adae37e6d18e47aeba7cd96502ec97ace49c83",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=607bca2e9c65a2bbe993f5323a01e5b43cd39a4a",
            "patch": "@@ -511,6 +511,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:gpu_conv_runner\",\n+        \"//xla/service/gpu:gpu_conv_runner_proto_cc\",\n         \"//xla/service/gpu:stream_executor_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n@@ -535,6 +536,21 @@ cc_library(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"convolution_thunk_test\",\n+    srcs = [\"convolution_thunk_test.cc\"],\n+    deps = [\n+        \":convolution_thunk\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"copy_thunk\",\n     srcs = [\"copy_thunk.cc\"],\n@@ -2295,6 +2311,7 @@ tf_proto_library(\n         \"//xla:xla_data_proto\",\n         \"//xla/service:buffer_assignment_proto\",\n         \"//xla/service/gpu:backend_configs\",\n+        \"//xla/service/gpu:gpu_conv_runner_proto\",\n         \"//xla/service/gpu:gpu_norm_runner_proto\",\n         \"//xla/service/gpu:launch_dimensions_proto\",\n         \"//xla/stream_executor:launch_dim_proto\",\n@@ -2322,6 +2339,7 @@ cc_library(\n     hdrs = [\"thunk_proto_deserialization.h\"],\n     deps = [\n         \":conditional_thunk\",\n+        \":convolution_thunk\",\n         \":copy_thunk\",\n         \":cudnn_thunk\",\n         \":gemm_thunk\","
        },
        {
            "sha": "1b3694948b1bb782e359b5bdd348d21b97376839",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 5,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=607bca2e9c65a2bbe993f5323a01e5b43cd39a4a",
            "patch": "@@ -46,6 +46,7 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n+using buffer_assignment::BufferAllocationSliceProto;\n \n absl::StatusOr<std::unique_ptr<ConvolutionThunk>> ConvolutionThunk::Create(\n     ThunkInfo thunk_info, GpuConvDescriptor descriptor,\n@@ -57,21 +58,20 @@ absl::StatusOr<std::unique_ptr<ConvolutionThunk>> ConvolutionThunk::Create(\n \n   // Can't use std::make_unique because the constructor is private.\n   return absl::WrapUnique(new ConvolutionThunk(\n-      thunk_info, std::move(config), std::move(operand_slices),\n-      std::move(result_slices), scratch_slice));\n+      thunk_info, std::move(descriptor), std::move(config),\n+      std::move(operand_slices), std::move(result_slices), scratch_slice));\n }\n \n-// TODO: b/431980836 - Store the descriptor once when adding the\n-// (de)serialization methods.\n ConvolutionThunk::ConvolutionThunk(\n-    ThunkInfo thunk_info, GpuConvConfig config,\n+    ThunkInfo thunk_info, GpuConvDescriptor descriptor, GpuConvConfig config,\n     std::vector<BufferAllocation::Slice> operand_slices,\n     std::vector<BufferAllocation::Slice> result_slices,\n     BufferAllocation::Slice scratch_slice)\n     : Thunk(Kind::kConvolution, thunk_info),\n       operand_buffers_(std::move(operand_slices)),\n       result_buffers_(std::move(result_slices)),\n       scratch_buffer_(scratch_slice),\n+      descriptor_(std::move(descriptor)),\n       config_(std::move(config)) {}\n \n GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(\n@@ -149,6 +149,57 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n   return absl::OkStatus();\n }\n \n+absl::StatusOr<std::unique_ptr<ConvolutionThunk>> ConvolutionThunk::FromProto(\n+    ThunkInfo thunk_info, const ConvolutionThunkProto& proto,\n+    absl::Span<const BufferAllocation> buffer_allocations) {\n+  TF_ASSIGN_OR_RETURN(GpuConvDescriptor descriptor,\n+                      GpuConvDescriptor::FromProto(proto.conv_descriptor()));\n+\n+  std::vector<BufferAllocation::Slice> operand_slices;\n+  operand_slices.reserve(proto.operand_buffers_size());\n+  for (const BufferAllocationSliceProto& slice_proto :\n+       proto.operand_buffers()) {\n+    TF_ASSIGN_OR_RETURN(\n+        operand_slices.emplace_back(),\n+        BufferAllocation::Slice::FromProto(slice_proto, buffer_allocations));\n+  }\n+\n+  std::vector<BufferAllocation::Slice> result_slices;\n+  result_slices.reserve(proto.result_buffers_size());\n+  for (const BufferAllocationSliceProto& slice_proto : proto.result_buffers()) {\n+    TF_ASSIGN_OR_RETURN(\n+        result_slices.emplace_back(),\n+        BufferAllocation::Slice::FromProto(slice_proto, buffer_allocations));\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice scratch_slice,\n+                      BufferAllocation::Slice::FromProto(proto.scratch_buffer(),\n+                                                         buffer_allocations));\n+\n+  return Create(std::move(thunk_info), std::move(descriptor),\n+                std::move(operand_slices), std::move(result_slices),\n+                scratch_slice);\n+}\n+\n+absl::StatusOr<ThunkProto> ConvolutionThunk::ToProto() const {\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+\n+  ConvolutionThunkProto* conv_proto = proto.mutable_convolution_thunk();\n+  *conv_proto->mutable_conv_descriptor() = descriptor_.ToProto();\n+\n+  for (const BufferAllocation::Slice& slice : operand_buffers_) {\n+    TF_ASSIGN_OR_RETURN(*conv_proto->add_operand_buffers(), slice.ToProto());\n+  }\n+  for (const BufferAllocation::Slice& slice : result_buffers_) {\n+    TF_ASSIGN_OR_RETURN(*conv_proto->add_result_buffers(), slice.ToProto());\n+  }\n+  TF_ASSIGN_OR_RETURN(*conv_proto->mutable_scratch_buffer(),\n+                      scratch_buffer_.ToProto());\n+\n+  return proto;\n+}\n+\n ConvolutionReorderThunk::ConvolutionReorderThunk(\n     ThunkInfo thunk_info, absl::Span<int64_t> filter_nchw,\n     absl::InlinedVector<BufferAllocation::Slice, 2> operand_slices,"
        },
        {
            "sha": "23fbab53d8969fa6a3e69c85786eb599a044cf22",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.h",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h?ref=607bca2e9c65a2bbe993f5323a01e5b43cd39a4a",
            "patch": "@@ -56,8 +56,15 @@ class ConvolutionThunk : public Thunk {\n \n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n \n+  static absl::StatusOr<std::unique_ptr<ConvolutionThunk>> FromProto(\n+      ThunkInfo thunk_info, const ConvolutionThunkProto& proto,\n+      absl::Span<const BufferAllocation> buffer_allocations);\n+\n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n  private:\n-  ConvolutionThunk(ThunkInfo thunk_info, GpuConvConfig config,\n+  ConvolutionThunk(ThunkInfo thunk_info, GpuConvDescriptor descriptor,\n+                   GpuConvConfig config,\n                    std::vector<BufferAllocation::Slice> operand_slices,\n                    std::vector<BufferAllocation::Slice> result_slices,\n                    BufferAllocation::Slice scratch_slice);\n@@ -68,6 +75,10 @@ class ConvolutionThunk : public Thunk {\n   GenericConvRunner& GetOrCreateRunner(const stream_executor::Stream* stream,\n                                        bool* runner_created);\n \n+  // Technically this is only needed during initialization to create the\n+  // GpuConvConfig, but the actual GpuConvConfig is hard to serialize. So we\n+  // keep the descriptor around for serialization purposes.\n+  const GpuConvDescriptor descriptor_;\n   // Convolution config\n   const GpuConvConfig config_;\n   absl::Mutex mu_;"
        },
        {
            "sha": "6ebd309f06e7af3bf6ca4dd9d805aa4e6215afe6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk_test.cc",
            "status": "added",
            "additions": 135,
            "deletions": 0,
            "changes": 135,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk_test.cc?ref=607bca2e9c65a2bbe993f5323a01e5b43cd39a4a",
            "patch": "@@ -0,0 +1,135 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::tsl::proto_testing::EqualsProto;\n+using ::tsl::proto_testing::ParseTextProtoOrDie;\n+\n+TEST(ConvolutionThunkTest, ProtoRoundTrip) {\n+  auto proto = ParseTextProtoOrDie<ThunkProto>(R\"pb(\n+    thunk_info {\n+      profile_annotation: \"conv_thunk_profile\"\n+      execution_stream_id: 0\n+    }\n+    convolution_thunk {\n+      conv_descriptor {\n+        kind: FORWARD\n+        backend_config {\n+          conv_result_scale: 1\n+          activation_mode: 0\n+          side_input_scale: 0\n+          leakyrelu_alpha: 0\n+        }\n+        window {\n+          dimensions {\n+            size: 1\n+            stride: 1\n+            padding_low: 0\n+            padding_high: 0\n+            window_dilation: 1\n+            base_dilation: 1\n+            window_reversal: false\n+          }\n+          dimensions {\n+            size: 1\n+            stride: 1\n+            padding_low: 0\n+            padding_high: 0\n+            window_dilation: 1\n+            base_dilation: 1\n+            window_reversal: false\n+          }\n+        }\n+        operand0_shape {\n+          element_type: F32\n+          dimensions: [ 1, 1, 1, 1 ]\n+          layout {\n+            minor_to_major: [ 3, 2, 1, 0 ]\n+            tail_padding_alignment_in_elements: 1\n+          }\n+          is_dynamic_dimension: [ false, false, false, false ]\n+        }\n+        operand1_shape {\n+          element_type: F32\n+          dimensions: [ 1, 1, 1, 1 ]\n+          layout {\n+            minor_to_major: [ 3, 2, 1, 0 ]\n+            tail_padding_alignment_in_elements: 1\n+          }\n+          is_dynamic_dimension: [ false, false, false, false ]\n+        }\n+        result_shape {\n+          element_type: F32\n+          dimensions: [ 1, 1, 1, 1 ]\n+          layout {\n+            minor_to_major: [ 3, 2, 1, 0 ]\n+            tail_padding_alignment_in_elements: 1\n+          }\n+          is_dynamic_dimension: [ false, false, false, false ]\n+        }\n+        scratch_size: 1024\n+        dnums {\n+          input_batch_dimension: 0\n+          input_feature_dimension: 1\n+          input_spatial_dimensions: [ 2, 3 ]\n+          kernel_input_feature_dimension: 1\n+          kernel_output_feature_dimension: 0\n+          kernel_spatial_dimensions: [ 2, 3 ]\n+          output_batch_dimension: 0\n+          output_feature_dimension: 1\n+          output_spatial_dimensions: [ 2, 3 ]\n+        }\n+      }\n+      operand_buffers { offset: 0 size: 4 buffer_allocation_index: 0 }\n+      operand_buffers { offset: 0 size: 4 buffer_allocation_index: 1 }\n+      result_buffers { offset: 0 size: 4 buffer_allocation_index: 2 }\n+      scratch_buffer { offset: 0 size: 1024 buffer_allocation_index: 3 }\n+    }\n+  )pb\");\n+\n+  std::vector<BufferAllocation> buffer_allocations;\n+  buffer_allocations.emplace_back(/*index=*/0, /*size=*/4, /*color=*/0);\n+  buffer_allocations.emplace_back(/*index=*/1, /*size=*/4, /*color=*/0);\n+  buffer_allocations.emplace_back(/*index=*/2, /*size=*/4, /*color=*/0);\n+  buffer_allocations.emplace_back(/*index=*/3, /*size=*/1024, /*color=*/0);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Thunk::ThunkInfo thunk_info,\n+                          Thunk::ThunkInfo::FromProto(proto.thunk_info()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<ConvolutionThunk> thunk,\n+      ConvolutionThunk::FromProto(thunk_info, proto.convolution_thunk(),\n+                                  buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n+  EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "3c0d6c8685c1c92817b5804c0652902e931788df",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=607bca2e9c65a2bbe993f5323a01e5b43cd39a4a",
            "patch": "@@ -20,6 +20,7 @@ package xla.gpu;\n import \"xla/backends/gpu/runtime/dynamic_slice_thunk.proto\";\n import \"xla/service/buffer_assignment.proto\";\n import \"xla/service/gpu/backend_configs.proto\";\n+import \"xla/service/gpu/gpu_conv_runner.proto\";\n import \"xla/service/gpu/gpu_norm_runner.proto\";\n import \"xla/service/gpu/launch_dimensions.proto\";\n import \"xla/stream_executor/gpu/gpu_blas_lt.proto\";\n@@ -198,6 +199,13 @@ message NormThunkProto {\n   xla.buffer_assignment.BufferAllocationSliceProto scratch = 11;\n }\n \n+message ConvolutionThunkProto {\n+  GpuConvDescriptorProto conv_descriptor = 1;\n+  repeated xla.buffer_assignment.BufferAllocationSliceProto operand_buffers = 2;\n+  repeated xla.buffer_assignment.BufferAllocationSliceProto result_buffers = 3;\n+  xla.buffer_assignment.BufferAllocationSliceProto scratch_buffer = 4;\n+}\n+\n message ThunkProto {\n   ThunkInfoProto thunk_info = 1;\n \n@@ -225,6 +233,7 @@ message ThunkProto {\n     CublasLtMatmulThunkProto cublas_lt_matmul_thunk = 22;\n     OutfeedThunkProto outfeed_thunk = 23;\n     NormThunkProto norm_thunk = 24;\n+    ConvolutionThunkProto convolution_thunk = 25;\n   }\n }\n "
        },
        {
            "sha": "0f74b9b5dc84a44ac15ba0fe9d677be467ccb069",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/607bca2e9c65a2bbe993f5323a01e5b43cd39a4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc?ref=607bca2e9c65a2bbe993f5323a01e5b43cd39a4a",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"google/protobuf/descriptor.h\"\n #include \"google/protobuf/message.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n+#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n@@ -148,7 +149,10 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProto(\n     case ThunkProto::kNormThunk:\n       return NormThunk::FromProto(std::move(thunk_info),\n                                   thunk_proto.norm_thunk(), buffer_allocations);\n-\n+    case ThunkProto::kConvolutionThunk:\n+      return ConvolutionThunk::FromProto(std::move(thunk_info),\n+                                         thunk_proto.convolution_thunk(),\n+                                         buffer_allocations);\n     default:\n       std::optional<absl::string_view> unsupported_thunk_type =\n           GetStoredThunkTypeName(thunk_proto);"
        }
    ],
    "stats": {
        "total": 242,
        "additions": 235,
        "deletions": 7
    }
}