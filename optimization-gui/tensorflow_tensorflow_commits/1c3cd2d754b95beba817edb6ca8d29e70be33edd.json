{
    "author": "metaflow",
    "message": "[XLA:GPU] use reserved \"field name\"\n\nIt is a best practice that adds a compile time check to not reuse names\nof deleted fields.\n\nPiperOrigin-RevId: 836538803",
    "sha": "1c3cd2d754b95beba817edb6ca8d29e70be33edd",
    "files": [
        {
            "sha": "b8356fe3111a0c580534fee1b49f3e063f292744",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 107,
            "deletions": 123,
            "changes": 230,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c3cd2d754b95beba817edb6ca8d29e70be33edd/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c3cd2d754b95beba817edb6ca8d29e70be33edd/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=1c3cd2d754b95beba817edb6ca8d29e70be33edd",
            "patch": "@@ -160,8 +160,6 @@ message DebugOptions {\n   optional bool xla_unsupported_crash_on_hlo_pass_silent_hlo_change = 380;\n   // go/keep-sorted end\n \n-  reserved 346;  // xla_experimental_exec_time_optimization_effort\n-\n   //--------------------------------------------------------------------------//\n   // XLA:CPU options.\n   //--------------------------------------------------------------------------//\n@@ -292,19 +290,13 @@ message DebugOptions {\n \n   // go/keep-sorted end\n \n-  reserved 298;  // Was xla_cpu_use_thunk_runtime\n-\n   //--------------------------------------------------------------------------//\n   // XLA:GPU options.\n   //--------------------------------------------------------------------------//\n   // clang-format off\n-  // go/keep-sorted start newline_separated=yes skip_lines=2 ignore_prefixes=[\"optional AutotuneCacheMode\",\"optional bool\",\"optional float\",\"optional int32\",\"optional int64\",\"optional LibNvJitLinkMode\",\"map<string, string>\",\"optional PGLEStrictnessLevel\",\"optional PipelineParallelismOptLevel\",\"repeated CollectiveOpType\",\"repeated CommandBufferCmdType\",\"repeated string\",\"optional ShapeChecks\",\"optional string\",\"optional WhileLoopUnrolling\",\"reserved\",\"repeated GenericTritonEmitterFeature\",\"optional CommandBufferSchedulingMode\"] // NOLINT\n+  // go/keep-sorted start newline_separated=yes skip_lines=2 ignore_prefixes=[\"optional AutotuneCacheMode\",\"optional bool\",\"optional float\",\"optional int32\",\"optional int64\",\"optional LibNvJitLinkMode\",\"map<string, string>\",\"optional PGLEStrictnessLevel\",\"optional PipelineParallelismOptLevel\",\"repeated CollectiveOpType\",\"repeated CommandBufferCmdType\",\"repeated string\",\"optional ShapeChecks\",\"optional string\",\"optional WhileLoopUnrolling\",\"repeated GenericTritonEmitterFeature\",\"optional CommandBufferSchedulingMode\"] // NOLINT\n   // clang-format on\n \n-  reserved 160;  // Was xla_gpu_enable_cudnn_frontend\n-\n-  reserved 352;  // xla_gpu_dump_hlo_unoptimized_snapshots\n-\n   // Command buffer scheduling mode.\n   // SERIALIZE: Serialize all commands in a command buffer.\n   // CONCURRENT: Identify concurrent across operator through data conflicts.\n@@ -327,7 +319,8 @@ message DebugOptions {\n     // Additionally, enable collective-permute cycle decomposer. This set of\n     // optimizations will lead to best overlap for trivial pipeline parallelism\n     // implementation.\n-    reserved 2;  // Was PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE_CYCLE_DECOMPOSER\n+    reserved \"PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE_CYCLE_DECOMPOSER\";\n+    reserved 2;\n   }\n \n   // Limits the thunk buffer debug instrumentation to specific thunks.\n@@ -970,15 +963,6 @@ message DebugOptions {\n \n   // go/keep-sorted end\n \n-  reserved 167;  // xla_gpu_redzone_scratch_max_megabytes\n-  reserved 266;  // xla_gpu_enable_triton_hopper\n-  reserved 276;  // xla_gpu_enable_nccl_per_stream_comms\n-  reserved 226;  // xla_gpu_triton_gemm_disable_reduced_precision_reduction\n-  reserved 385;  // xla_gpu_experimental_enable_dynamic_dot_search_space\n-  reserved \"xla_gpu_unsupported_enable_generic_triton_emitter_for_gemms\";\n-  reserved 367;\n-  reserved 423;  // xla_gpu_experimental_enable_checksum_tracing_on_thunks\n-\n   //--------------------------------------------------------------------------//\n   // XLA:TPU options.\n   //--------------------------------------------------------------------------//\n@@ -1027,9 +1011,6 @@ message DebugOptions {\n   // mode.\n   optional bool xla_cpu_multi_thread_eigen = 60;\n \n-  reserved 63;   // Was xla_gpu_disable_multi_streaming\n-  reserved 134;  // Was xla_gpu_use_random_streams\n-\n   // If true, in LLVM-based backends, emit !alias.scope metadata in\n   // generated IR.\n   optional bool xla_llvm_enable_alias_scope_metadata = 70;\n@@ -1045,8 +1026,6 @@ message DebugOptions {\n   // If true, a set of expensive LLVM optimization passes will not be run.\n   optional bool xla_llvm_disable_expensive_passes = 73;\n \n-  reserved 80;  // Was hlo_reduce_precision_options\n-\n   // This is used by ClientLibraryTestBase::ComputeAndCompare*. If true, the\n   // computation will run n! times with all permunations of layouts for the\n   // output shape in rank n. For example, with a 3D shape, all permutations of\n@@ -1063,16 +1042,9 @@ message DebugOptions {\n   // HLO graph.\n   optional bool xla_hlo_graph_sharding_color = 92;\n \n-  reserved 93;  // Was xla_hlo_tfgraph_device_scopes\n-  reserved 94;  // Was xla_gpu_use_cudnn_batchnorm\n-\n   // Call oneDNN thunks for matmul and convolution fusions in the CPU backend.\n   optional bool xla_cpu_use_onednn = 97;\n \n-  reserved 177;  // Was xla_cpu_use_xla_runtime\n-  reserved 98;   // Was xla_gpu_max_kernel_unroll_factor\n-  reserved 207;  // Was xla_cpu_sparse_cuda_threads\n-\n   // Allows xla to increase the output precision of floating point operations\n   // and all floating-point conversions to be simplified, including those\n   // that affect the numerics. The `FloatNormalization` pass inserts many\n@@ -1090,10 +1062,6 @@ message DebugOptions {\n   // the host that run models in parallel across multiple devices.\n   optional int32 xla_force_host_platform_device_count = 102;\n \n-  reserved 171;  // Was xla_cpu_enable_mlir_lowering\n-  reserved 173;  // Was xla_gpu_enable_mlir_lowering\n-  reserved 179;  // Was xla_gpu_enable_softmax_fusion\n-\n   // Enable fast math with eigen in the HLO evaluator.\n   optional bool xla_hlo_evaluator_use_fast_path = 106;\n \n@@ -1191,42 +1159,24 @@ message DebugOptions {\n   // Whether to dump mlir using pretty print form.\n   optional bool xla_dump_full_hlo_config = 381;\n \n-  reserved 130;  // Was xla_gpu_deterministic_reductions\n-\n   // Debug options that trigger execution errors when NaN or Inf are detected.\n   optional bool xla_tpu_detect_nan = 135;\n   optional bool xla_tpu_detect_inf = 136;\n \n   // True if TraceMe annotations are enabled for XLA:CPU.\n   optional bool xla_cpu_enable_xprof_traceme = 137;\n \n-  reserved 141;  // was xla_gpu_asm_extra_flags\n-\n   // Per-heap size constraint. New heaps will be created if per-heap max size is\n   // reached.\n   optional int32 xla_multiheap_size_constraint_per_heap = 142;\n \n-  reserved 143;  // Was xla_detailed_logging_and_dumping\n-\n   // Enable detailed logging into vlog. If this is disabled, no\n   // compilation summary will be printed in the end of computation.\n   optional bool xla_detailed_logging = 252;\n \n   // Enable HLO dumping. If this is disabled, no HLO modules will be dumped.\n   optional bool xla_enable_dumping = 253;\n \n-  // Used to be xla_gpu_enable_async_all_reduce\n-  // xla_gpu_enable_async_collective_broadcast\n-  // xla_gpu_enable_async_collective_permute\n-  // xla_gpu_enable_async_all_gather\n-  // xla_gpu_enable_async_reduce_scatter\n-  // xla_gpu_enable_async_all_to_all\n-  // xla_gpu_enable_async_collectives\n-  reserved 152, 278, 183, 199, 200, 201, 238;\n-\n-  // Was xla_gpu_all_reduce_contiguous, xla_gpu_enable_all_reduce_splitter\n-  reserved 158, 299;\n-\n   // Whether to force inline before llvm module split to get a more balanced\n   // splits for parallel compilation.\n   optional bool xla_llvm_force_inline_before_split = 300;\n@@ -1239,40 +1189,15 @@ message DebugOptions {\n   // enables dumping in all pipelines.\n   optional string xla_dump_hlo_pipeline_re = 154;\n \n-  reserved 161;  // Was xla_gpu_bef_executable\n-  reserved 162;  // Was xla_gpu_bef_thunk\n-  reserved 169;  // Was xla_gpu_enable_xla_runtime_executable\n-  reserved 233;  // was xla_gpu_enable_gpu2_runtime\n-  reserved 234;  // was xla_gpu_enable_gpu2_hal\n-  reserved 202;  // Was xla_gpu_graph_num_runs_to_instantiate\n-  reserved 230;  // Was xla_gpu_graph_eviction_timeout_seconds\n-  reserved 168;  // Was xla_gpu_simplify_all_fp_conversions.\n-  reserved 172;  // Was xla_gpu_normalize_layouts.\n-  reserved 263;  // Was xla_gpu_enable_custom_fusions\n-  reserved 264;  // Was xla_gpu_enable_custom_fusions_re\n-\n   // Generate calls to Arm Compute Library in the CPU backend.\n   optional bool xla_cpu_use_acl = 174;\n \n   // By default, XLA:CPU will run fp16 dot/conv as fp32, as this is generally\n   // (much) faster on our hardware.  Set this flag to disable this behavior.\n   optional bool xla_cpu_strict_dot_conv_math = 175;\n \n-  reserved 402;  // Was xla_cpu_dump_unoptimized_hlo_snapshots\n-\n   optional bool xla_dump_latency_hiding_schedule = 182;\n \n-  reserved 184;  // Was xla_cpu_enable_mlir_tiling_and_fusion.\n-  reserved 192;  // Was xla_cpu_enable_mlir_fusion_outlining.\n-  reserved 191;  // Was xla_cpu_enable_experimental_deallocation.\n-  reserved 195;  // Was xla_cpu_enable_custom_matmul_tiling.\n-  reserved 196;  // Was xla_cpu_matmul_tiling_m_dim.\n-  reserved 197;  // Was xla_cpu_matmul_tiling_n_dim.\n-  reserved 198;  // Was xla_cpu_matmul_tiling_k_dim.\n-\n-  reserved 204;  // Was xla_gpu_lhs_enable_gpu_async_tracker.\n-  reserved 313;  // Was xla_gpu_run_post_layout_collective_pipeliner.\n-\n   enum PartitioningAlgorithm {\n     PARTITIONING_ALGORITHM_NOOP = 0;\n     PARTITIONING_ALGORITHM_EXP0 = 1;\n@@ -1282,10 +1207,6 @@ message DebugOptions {\n   // The partitioning algorithm to be used in the PartitionAssignment pass.\n   optional PartitioningAlgorithm xla_partitioning_algorithm = 187;\n \n-  reserved 211;  // Was xla_gpu_enable_dot_strength_reduction\n-  reserved 220;  // Was xla_gpu_enable_triton_softmax_fusion\n-  reserved 286;  // Was xla_gpu_enable_triton_softmax_priority_fusion\n-\n   // Maximum number of buffers to print when debugging buffer assignment.\n   optional int64 xla_debug_buffer_assignment_show_max = 251;\n \n@@ -1309,26 +1230,17 @@ message DebugOptions {\n   // Whether to enable checks for Inf values in computations.\n   optional DetectionMode xla_gpu_detect_inf = 428;\n \n-  reserved 275;  // was xla_gpu_enable_mlir_emitters\n-  reserved 281;  // was xla_gpu_max_mlir_kernels\n-  reserved 282;  // was xla_gpu_skip_mlir_kernels\n-  reserved 303;  // was xla_gpu_mlir_emitter_level\n-\n   // If true, large constants will be printed out when dumping HLOs.\n   optional bool xla_dump_large_constants = 290;\n \n   // Base length to rewrite the reduce window to, no rewrite if set to 0.\n   optional int64 xla_reduce_window_rewrite_base_length = 293;\n \n-  reserved 302;  // was xla_use_shardy\n-\n   // The command buffer trace cache size, increasing the cache size may\n   // sometimes reduces the chances of doing command buffer tracing for\n   // updating command buffer instance.\n   optional int64 xla_cmd_buffer_trace_cache_size = 311;\n \n-  reserved 314;  // was legacy_command_buffer_custom_call_targets\n-\n   // This flag is used for controlling HLO dumping and NVTX marker. If turned\n   // on, both HLO dumping and NVTX marker will use syntactic sugar wrappers\n   // as op names, while the actual op names will be shown if turned off.\n@@ -1361,8 +1273,6 @@ message DebugOptions {\n   // TODO(b/355487968): Remove this option when validation complete.\n   optional bool xla_enable_command_buffers_during_profiling = 317;\n \n-  reserved 319;  // was xla_gpu_enable_libnvjitlink with boolean type.\n-\n   enum AutotuneCacheMode {\n     AUTOTUNE_CACHE_MODE_UNSPECIFIED = 0;\n \n@@ -1408,34 +1318,106 @@ message DebugOptions {\n   // interpretation of these values is left to the backend.\n   map<string, string> xla_backend_extra_options = 500;\n \n-  // Reserved tags were xla_hlo_dump_as_graphdef, xla_dump_to,\n-  // xla_gpu_use_horizontal_fusion,\n-  // xla_gpu_unsafe_fallback_to_driver_on_ptxas_error,\n-  // xla_gpu_simplify_scatters, xla_gpu_simplify_gathers\n-  // xla_gpu_enable_cuda_graphs\n-  // xla_gpu_allow_all_reduce_kernel\n-  // xla_gpu_enable_experimental_block_size\n-  // xla_gpu_graph_level\n-  // xla_gpu_single_wave_autotuning\n-  // xla_gpu_enable_persistent_temp_buffers\n-  // xla_gpu_enable_triton_gemm_int4\n-  // xla_gpu_experimental_enable_triton_i4_rewrites\n-  // xla_gpu_enable_priority_fusion\n-  // xla_gpu_experimental_enable_triton_softmax_priority_fusion\n-  // xla_gpu_pgle_accuracy_checker\n-  // xla_gpu_enable_heuristic_pass_configuration\n-  // xla_gpu_enable_dot_strength_reduction\n-  // xla_gpu_triton_fusion_level\n-  // xla_gpu_enable_bf16_3way_gemm\n-  // xla_gpu_enable_bf16_6way_gemm\n-  // xla_gpu_enable_cudnn_fmha\n-  // xla_gpu_unsupported_force_triton_gemm\n-  // xla_allow_get_default_platform\n-  // xla_gpu_ensure_minor_dot_contraction_dims\n-  // xla_gpu_unsafe_pipelined_loop_annotator\n-  // xla_gpu_unsupported_generic_triton_emitter_features\n-  reserved 5, 117, 133, 139, 176, 178, 180, 193, 214, 194, 221, 242, 206, 320,\n-      325, 326, 332, 361, 270, 229, 271, 279, 218, 369, 371, 249, 309, 398;\n+  reserved \"hlo_reduce_precision_options\";\n+  reserved \"legacy_command_buffer_custom_call_targets\";\n+  reserved \"xla_allow_get_default_platform\";\n+  reserved \"xla_cpu_dump_unoptimized_hlo_snapshots\";\n+  reserved \"xla_cpu_enable_custom_matmul_tiling\";\n+  reserved \"xla_cpu_enable_experimental_deallocation\";\n+  reserved \"xla_cpu_enable_mlir_fusion_outlining\";\n+  reserved \"xla_cpu_enable_mlir_lowering\";\n+  reserved \"xla_cpu_enable_mlir_tiling_and_fusion\";\n+  reserved \"xla_cpu_matmul_tiling_k_dim\";\n+  reserved \"xla_cpu_matmul_tiling_m_dim\";\n+  reserved \"xla_cpu_matmul_tiling_n_dim\";\n+  reserved \"xla_cpu_sparse_cuda_threads\";\n+  reserved \"xla_cpu_use_thunk_runtime\";\n+  reserved \"xla_cpu_use_xla_runtime\";\n+  reserved \"xla_detailed_logging_and_dumping\";\n+  reserved \"xla_dump_ir\";\n+  reserved \"xla_experimental_exec_time_optimization_effort\";\n+  reserved \"xla_gpu_all_reduce_contiguous\";\n+  reserved \"xla_gpu_allow_all_reduce_kernel\";\n+  reserved \"xla_gpu_asm_extra_flags\";\n+  reserved \"xla_gpu_bef_executable\";\n+  reserved \"xla_gpu_bef_thunk\";\n+  reserved \"xla_gpu_deterministic_reductions\";\n+  reserved \"xla_gpu_disable_multi_streaming\";\n+  reserved \"xla_gpu_dump_hlo_unoptimized_snapshots\";\n+  reserved \"xla_gpu_enable_all_reduce_splitter\";\n+  reserved \"xla_gpu_enable_async_all_gather\";\n+  reserved \"xla_gpu_enable_async_all_reduce\";\n+  reserved \"xla_gpu_enable_async_all_to_all\";\n+  reserved \"xla_gpu_enable_async_collective_broadcast\";\n+  reserved \"xla_gpu_enable_async_collective_permute\";\n+  reserved \"xla_gpu_enable_async_collectives\";\n+  reserved \"xla_gpu_enable_async_reduce_scatter\";\n+  reserved \"xla_gpu_enable_bf16_3way_gemm\";\n+  reserved \"xla_gpu_enable_bf16_6way_gemm\";\n+  reserved \"xla_gpu_enable_cuda_graphs\";\n+  reserved \"xla_gpu_enable_cudnn_fmha\";\n+  reserved \"xla_gpu_enable_cudnn_frontend\";\n+  reserved \"xla_gpu_enable_custom_fusions_re\";\n+  reserved \"xla_gpu_enable_custom_fusions\";\n+  reserved \"xla_gpu_enable_dot_strength_reduction\";\n+  reserved \"xla_gpu_enable_experimental_block_size\";\n+  reserved \"xla_gpu_enable_gpu2_hal\";\n+  reserved \"xla_gpu_enable_gpu2_runtime\";\n+  reserved \"xla_gpu_enable_heuristic_pass_configuration\";\n+  reserved \"xla_gpu_enable_libnvjitlink\";\n+  reserved \"xla_gpu_enable_mlir_emitters\";\n+  reserved \"xla_gpu_enable_mlir_lowering\";\n+  reserved \"xla_gpu_enable_nccl_per_stream_comms\";\n+  reserved \"xla_gpu_enable_persistent_temp_buffers\";\n+  reserved \"xla_gpu_enable_pgle_accuracy_checker\";\n+  reserved \"xla_gpu_enable_priority_fusion\";\n+  reserved \"xla_gpu_enable_softmax_fusion\";\n+  reserved \"xla_gpu_enable_triton_gemm_int4\";\n+  reserved \"xla_gpu_enable_triton_hopper\";\n+  reserved \"xla_gpu_enable_triton_softmax_fusion\";\n+  reserved \"xla_gpu_enable_triton_softmax_priority_fusion\";\n+  reserved \"xla_gpu_enable_xla_runtime_executable\";\n+  reserved \"xla_gpu_ensure_minor_dot_contraction_dims\";\n+  reserved \"xla_gpu_experimental_enable_dynamic_dot_search_space\";\n+  reserved \"xla_gpu_experimental_enable_nan_counter_on_thunks\";\n+  reserved \"xla_gpu_experimental_enable_triton_i4_rewrites\";\n+  reserved \"xla_gpu_experimental_enable_triton_softmax_priority_fusion\";\n+  reserved \"xla_gpu_graph_eviction_timeout_seconds\";\n+  reserved \"xla_gpu_graph_level\";\n+  reserved \"xla_gpu_graph_num_runs_to_instantiate\";\n+  reserved \"xla_gpu_lhs_enable_gpu_async_tracker\";\n+  reserved \"xla_gpu_max_kernel_unroll_factor\";\n+  reserved \"xla_gpu_max_mlir_kernels\";\n+  reserved \"xla_gpu_mlir_emitter_level\";\n+  reserved \"xla_gpu_normalize_layouts\";\n+  reserved \"xla_gpu_redzone_scratch_max_megabytes\";\n+  reserved \"xla_gpu_run_post_layout_collective_pipeliner\";\n+  reserved \"xla_gpu_simplify_all_fp_conversions\";\n+  reserved \"xla_gpu_simplify_gathers\";\n+  reserved \"xla_gpu_simplify_scatters\";\n+  reserved \"xla_gpu_single_wave_autotuning\";\n+  reserved \"xla_gpu_skip_mlir_kernels\";\n+  reserved \"xla_gpu_triton_fusion_level\";\n+  reserved \"xla_gpu_triton_gemm_disable_reduced_precision_reduction\";\n+  reserved \"xla_gpu_unsafe_fallback_to_driver_on_ptxas_error\";\n+  reserved \"xla_gpu_unsafe_pipelined_loop_annotator\";\n+  reserved \"xla_gpu_unsupported_enable_generic_triton_emitter_for_gemms\";\n+  reserved \"xla_gpu_unsupported_force_triton_gemm\";\n+  reserved \"xla_gpu_unsupported_generic_triton_emitter_features\";\n+  reserved \"xla_gpu_use_cudnn_batchnorm\";\n+  reserved \"xla_gpu_use_horizontal_fusion\";\n+  reserved \"xla_gpu_use_random_streams\";\n+  reserved \"xla_hlo_dump_as_graphdef\";\n+  reserved \"xla_hlo_tfgraph_device_scopes\";\n+  reserved \"xla_use_shardy\";\n+\n+  reserved 5, 63, 80, 93, 94, 98, 117, 130, 133, 134, 139, 141, 143, 152, 158,\n+      160, 161, 162, 167, 168, 169, 171, 172, 173, 176, 177, 178, 179, 180, 183,\n+      184, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206,\n+      207, 211, 214, 218, 220, 221, 226, 229, 230, 233, 234, 238, 242, 249, 263,\n+      264, 266, 270, 271, 275, 276, 278, 279, 281, 282, 286, 298, 299, 302, 303,\n+      309, 313, 314, 319, 320, 325, 326, 332, 346, 352, 361, 367, 369, 371, 385,\n+      398, 402, 423;\n }\n \n // Contains flags which affects the GPU compilation result.\n@@ -1564,7 +1546,8 @@ message ExecutionOptions {\n   // works on TPU.\n   bool deduplicate_hlo = 12;\n \n-  reserved 13;  // Was broadcast_replicated_parameters_via_collectives\n+  reserved \"broadcast_replicated_parameters_via_collectives\";\n+  reserved 13;\n \n   // Allows sharding propagation to propagate to the parameters. This changes\n   // the input shape of the computation (which is undesirable), but it can be\n@@ -1676,7 +1659,8 @@ message HloModuleConfigProto {\n   repeated uint64 memory_space_assignment_config = 23;\n   repeated BoolList phase_ordering_config = 24;\n   int32 phase_index = 25;\n-  reserved 26;  // Was flag_config\n+  reserved \"flag_config\";\n+  reserved 26;\n   repeated bool allow_spmd_sharding_propagation_to_parameters = 33;\n   repeated bool allow_spmd_sharding_propagation_to_output = 27;\n   map<string, int64> analysis_allowance_map = 28;"
        }
    ],
    "stats": {
        "total": 230,
        "additions": 107,
        "deletions": 123
    }
}