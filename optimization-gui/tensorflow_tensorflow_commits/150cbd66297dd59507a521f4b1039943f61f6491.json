{
    "author": "beckerhe",
    "message": "Expose ProgramShape from Executable even when HloModule is not available\n\n- This makes `Executable::result_shape()` a virtual function and overrides it in `GpuExecutable`.\n- With that change `GpuExecutable::output_shape()` is superfluous and will get replaced by `GpuExecutable::result_shape()`\n- In addition a new virtual function `Executable::compute_computation_layout()` gets introduced which in its default implementation returns the compute ComputationLayout of the containing `HloModule`. `GpuExecutable` overwrites that with an implementation that doesn't require the `HloModule`.\n\nThis change works toward making `GpuExecutable` serializable.\n\nPiperOrigin-RevId: 800424865",
    "sha": "150cbd66297dd59507a521f4b1039943f61f6491",
    "files": [
        {
            "sha": "0e0b8dbc12925c8fc5c7b3e66329d56ebe022513",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -112,18 +112,14 @@ std::unique_ptr<GpuProfiler> GpuProfiler::Create(\n \n absl::StatusOr<std::unique_ptr<InputBuffers>> GpuProfiler::CreateInputBuffers(\n     const Executable* executable) {\n-  if (!executable->has_module()) {\n-    return absl::InvalidArgumentError(\n-        \"Cannot create input buffers, the executable does not have an \"\n-        \"attatched HloModule.\");\n-  }\n   TF_ASSIGN_OR_RETURN(\n       RedzoneBuffers buffers,\n-      RedzoneBuffers::FromComputation(\n-          *executable->module().entry_computation(), allocator_, stream_.get(),\n+      RedzoneBuffers::FromProgramShape(\n+          executable->compute_computation_layout().ComputeProgramShape(),\n           RedzoneBuffers::BuffersToCreate::kAllInputs,\n           options_.should_init_buffers,\n-          /*should_check_correctness=*/true, options_.redzone_padding_bytes));\n+          /*should_check_correctness=*/true, options_.redzone_padding_bytes,\n+          allocator_, stream_.get()));\n   auto gpu_buffers = std::make_unique<GpuInputBuffers>();\n   gpu_buffers->redzone_buffers = std::move(buffers);\n   return gpu_buffers;"
        },
        {
            "sha": "662a1831c09bf6372390a6fca597964b4a787c30",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -1924,7 +1924,7 @@ StreamExecutorGpuClient::RunAsync(\n   std::set<se::DeviceMemoryBase> buffers_in_result;\n \n   xla::ShapeTree<tsl::RCReference<RawSEDeviceMemory>> results(\n-      gpu_exec->output_shape());\n+      gpu_exec->result_shape());\n \n   for (auto& p : results) {\n     const ShapeIndex& index = p.first;\n@@ -1960,15 +1960,15 @@ StreamExecutorGpuClient::RunAsync(\n         input.is_donated = false;\n         continue;\n       } else if (!output_info.passthrough &&\n-                 !ShapeUtil::GetSubshape(gpu_exec->output_shape(), index)\n+                 !ShapeUtil::GetSubshape(gpu_exec->result_shape(), index)\n                       .IsTuple()) {\n         // The guard is above is not to insert copy-protection when aliasing\n         // pass-through params, as we do not need to write into the output\n         // buffer.\n         VLOG(3) << \"Using copy-protection: aliasing is specified, but the \"\n                    \"buffer is not donated; allocating a fresh buffer\";\n         int64_t allocation_size = ShapeUtil::ByteSizeOf(\n-            ShapeUtil::GetSubshape(gpu_exec->output_shape(), index));\n+            ShapeUtil::GetSubshape(gpu_exec->result_shape(), index));\n         absl::StatusOr<se::OwningDeviceMemory> allocated_buffer =\n             memory_allocator->Allocate(device_ordinal, allocation_size,\n                                        /*retry_on_failure=*/true,"
        },
        {
            "sha": "d4765249fa0a2dfc9ca252484fb1d9313f9f2172",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -2492,8 +2492,8 @@ PjRtStreamExecutorLoadedExecutable::PjRtStreamExecutorLoadedExecutable(\n   executables_.reserve(executables.size());\n   tsl::Fprint128 fingerprint = tsl::Fingerprint128(fingerprint_);\n   for (auto& executable : executables) {\n-    const auto& computation_layout =\n-        executable->executable()->module().entry_computation_layout();\n+    ComputationLayout computation_layout =\n+        executable->executable()->compute_computation_layout();\n     std::vector<Shape> parameter_shapes;\n     parameter_shapes.reserve(computation_layout.parameter_count());\n     for (int i = 0; i < computation_layout.parameter_count(); ++i) {"
        },
        {
            "sha": "6c09f2ccabf2891dd530c515f6a25361e8c40c75",
            "filename": "third_party/xla/xla/service/executable.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -370,11 +370,16 @@ class Executable {\n \n   // The shape (including layout) that results from this execution. This is the\n   // shape of the DeviceMemoryBase result value in ExecuteOnStream above.\n-  const Shape& result_shape() const {\n+  virtual Shape result_shape() const {\n     CHECK(hlo_module_ != nullptr);\n     return hlo_module_->config().entry_computation_layout().result_shape();\n   }\n \n+  virtual ComputationLayout compute_computation_layout() const {\n+    CHECK(hlo_module_ != nullptr);\n+    return hlo_module_->compute_computation_layout();\n+  }\n+\n   // Returns the size of the executable in bytes. Returns -1 if this query is\n   // not supported by the executable.\n   //"
        },
        {
            "sha": "a0be323d64922b3a44efcec8c9cead9da9354d93",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -614,6 +614,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:thunk_pass_pipeline\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service:computation_layout\",\n         \"//xla/service:dump\",\n         \"//xla/service:executable\",\n         \"//xla/service:hlo_value\",\n@@ -681,6 +682,7 @@ xla_cc_test(\n         \":gpu_executable\",\n         \":launch_dimensions\",\n         \"//xla:debug_options_flags\",\n+        \"//xla:shape_layout\",\n         \"//xla:shape_util\",\n         \"//xla/backends/gpu/runtime:copy_thunk\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\","
        },
        {
            "sha": "1b9f4b9d03cba48c5a4e5e307cadfde652a85f68",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -66,7 +66,6 @@ limitations under the License.\n #include \"xla/core/host_offloading/hlo_host_device_type_call_wrapper.h\"\n #include \"xla/core/host_offloading/host_compute_asyncifier.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n-#include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -514,9 +513,12 @@ GpuThunkAotCompilationResult::LoadExecutable(\n       std::move(ir_emitter_context.constants());\n   TF_ASSIGN_OR_RETURN(auto output_info,\n                       GetOutputInfo(*hlo_module, *buffer_assignment));\n-  const Shape& output_shape = hlo_module->result_shape();\n+  ProgramShape program_shape =\n+      hlo_module->entry_computation_layout().ComputeProgramShape();\n+  *program_shape.mutable_result() = hlo_module->result_shape();\n   DebugOptions debug_options = hlo_module->config().debug_options();\n   std::string hlo_module_name = hlo_module->name();\n+\n   {\n     tsl::profiler::TraceMe traceme(\"CreateGpuExecutable\");\n     std::unique_ptr<GpuAliasInfo> alias_info =\n@@ -531,7 +533,7 @@ GpuThunkAotCompilationResult::LoadExecutable(\n         /*constants=*/std::move(constants),\n         /*output_info=*/std::move(output_info),\n         /*module_name=*/std::move(hlo_module_name),\n-        /*output_shape=*/std::move(output_shape),\n+        /*program_shape=*/std::move(program_shape),\n         /*mlir_allocations=*/std::nullopt,\n         /*buffer_assignment=*/std::move(buffer_assignment),\n         /*alias_info=*/std::move(alias_info),\n@@ -2676,7 +2678,8 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n           /*constants=*/std::move(res.compile_module_results.constants),\n           /*output_info=*/std::move(res.compile_module_results.output_info),\n           /*module_name=*/std::move(res.compile_module_results.module_name),\n-          /*output_shape=*/std::move(res.compile_module_results.output_shape),\n+          /*program_shape=*/\n+          module->compute_computation_layout().ComputeProgramShape(),\n           /*mlir_allocations=*/\n           (res.compile_module_results.use_original_allocations\n                ? std::optional<std::vector<BufferAllocation>>()"
        },
        {
            "sha": "83f10eb4b48a3c94fe438b738943038b38668fdd",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -152,7 +152,7 @@ GpuExecutable::GpuExecutable(GpuExecutable::Params params)\n       thunks_(std::move(params.executable)),\n       execution_stream_ids_(GetExecutionStreamIds(*thunks_)),\n       module_name_(params.module_name),\n-      output_shape_(params.output_shape),\n+      program_shape_(params.program_shape),\n       allocations_(std::move(params.mlir_allocations)),\n       buffer_assignment_(std::move(params.buffer_assignment)),\n       alias_info_(std::move(params.alias_info)),\n@@ -744,8 +744,9 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n   const int device_ordinal = run_options->device_ordinal() != -1\n                                  ? run_options->device_ordinal()\n                                  : executor->device_ordinal();\n-  ExecutionOutput result(/*on_device_shape=*/output_shape_, memory_allocator,\n-                         device_ordinal, executor->device_ordinal());\n+  ExecutionOutput result(/*on_device_shape=*/program_shape_.result(),\n+                         memory_allocator, device_ordinal,\n+                         executor->device_ordinal());\n \n   TF_ASSIGN_OR_RETURN(\n       BufferAllocations buffer_allocations,\n@@ -824,14 +825,15 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n         // result, if the ExecutionOutput is not committed.\n         result.AddAliasedIndex(index);\n       } else if (!output_info.passthrough &&\n-                 !ShapeUtil::GetSubshape(output_shape_, index).IsTuple()) {\n+                 !ShapeUtil::GetSubshape(program_shape_.result(), index)\n+                      .IsTuple()) {\n         // The guard is above is not to insert copy-protection when aliasing\n         // pass-through params, as we do not need to write into the output\n         // buffer.\n         VLOG(3) << \"Using copy-protection: aliasing is specified, but the \"\n                    \"buffer is not donated; allocating a fresh buffer\";\n-        int64_t allocation_size =\n-            ShapeUtil::ByteSizeOf(ShapeUtil::GetSubshape(output_shape_, index));\n+        int64_t allocation_size = ShapeUtil::ByteSizeOf(\n+            ShapeUtil::GetSubshape(program_shape_.result(), index));\n         absl::StatusOr<se::OwningDeviceMemory> allocated_buffer =\n             memory_allocator->Allocate(device_ordinal, allocation_size,\n                                        /*retry_on_failure=*/true,"
        },
        {
            "sha": "291b2d5fc8b7b5859b2dc1a824d279692f52f80c",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/computation_layout.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n@@ -101,7 +102,7 @@ class GpuExecutable : public Executable {\n     std::vector<ConstantInfo> constants;\n     absl::flat_hash_map<ShapeIndex, OutputInfo> output_info;\n     std::string module_name;\n-    xla::Shape output_shape;\n+    ProgramShape program_shape;\n     std::optional<std::vector<BufferAllocation>> mlir_allocations;\n     std::unique_ptr<const BufferAssignment> buffer_assignment;\n     std::unique_ptr<GpuAliasInfo> alias_info;\n@@ -121,12 +122,16 @@ class GpuExecutable : public Executable {\n \n   const std::string& module_name() const { return module_name_; }\n \n-  const xla::Shape& output_shape() const { return output_shape_; }\n+  xla::Shape result_shape() const override { return program_shape_.result(); }\n \n   const absl::flat_hash_map<ShapeIndex, OutputInfo>& output_info() const {\n     return output_info_;\n   }\n \n+  ComputationLayout compute_computation_layout() const override {\n+    return ComputationLayout(program_shape_, /*ignore_layouts=*/false);\n+  }\n+\n   // This should be called before ExecuteOnStream.\n   void set_ir_module_string(const std::string& ir_module_string) {\n     ir_module_string_ = ir_module_string;\n@@ -265,7 +270,7 @@ class GpuExecutable : public Executable {\n \n   std::string module_name_;\n \n-  xla::Shape output_shape_;\n+  ProgramShape program_shape_;\n \n   // The allocations_ object contains allocations that **may** be used to\n   // provide information for allocating memory for every output/temp buffer."
        },
        {
            "sha": "7aab44489c7fe1c7582c709e0933929c1c3e257f",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/shape_layout.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -155,5 +156,23 @@ TEST(GpuExecutableTest, RunThunkPasses) {\n   EXPECT_EQ(dump_files.size(), 1);\n }\n \n+TEST(GpuExecutableTest, ComputeComputationLayout) {\n+  GpuExecutable::Params params;\n+  params.module_name = \"test_module\";\n+  params.program_shape.AddParameter(ShapeUtil::MakeShape(F32, {1, 2, 3}), \"p0\");\n+  params.program_shape.AddParameter(ShapeUtil::MakeShape(U8, {1}), \"p1\");\n+  *params.program_shape.mutable_result() = ShapeUtil::MakeShape(F64, {2});\n+  params.executable =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable,\n+                          GpuExecutable::Create(std::move(params)));\n+  EXPECT_THAT(executable->compute_computation_layout().parameter_layouts(),\n+              ElementsAre(ShapeLayout(ShapeUtil::MakeShape(F32, {1, 2, 3})),\n+                          ShapeLayout(ShapeUtil::MakeShape(U8, {1}))));\n+  EXPECT_EQ(executable->compute_computation_layout().result_layout(),\n+            ShapeLayout(ShapeUtil::MakeShape(F64, {2})));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "179184c96b187ec60d4f6c750a0eab230c3be937",
            "filename": "third_party/xla/xla/service/hlo_runner.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/150cbd66297dd59507a521f4b1039943f61f6491/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc?ref=150cbd66297dd59507a521f4b1039943f61f6491",
            "patch": "@@ -246,11 +246,10 @@ HloRunner::ExecuteWithExecutable(OpaqueExecutable* executable,\n                                  int64_t num_repeats) {\n   TF_ASSIGN_OR_RETURN(HloRunnerExecutable* const hlo_runner_executable,\n                       HloRunnerExecutable::TryUnwrap(*this, executable));\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<ScopedShapedBuffer> argument_buffers,\n-      TransferLiteralsToDevice(arguments, &hlo_runner_executable->executable()\n-                                               ->module()\n-                                               .entry_computation_layout()));\n+  const ComputationLayout computation_layout =\n+      hlo_runner_executable->executable()->compute_computation_layout();\n+  TF_ASSIGN_OR_RETURN(std::vector<ScopedShapedBuffer> argument_buffers,\n+                      TransferLiteralsToDevice(arguments, &computation_layout));\n \n   std::vector<absl::StatusOr<Literal>> results;\n   results.reserve(num_repeats);\n@@ -272,11 +271,10 @@ absl::StatusOr<Literal> HloRunner::ExecuteWithExecutableAndProfile(\n     ExecutionProfile* profile) {\n   TF_ASSIGN_OR_RETURN(HloRunnerExecutable* const hlo_runner_executable,\n                       HloRunnerExecutable::TryUnwrap(*this, executable));\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<ScopedShapedBuffer> argument_buffers,\n-      TransferLiteralsToDevice(arguments, &hlo_runner_executable->executable()\n-                                               ->module()\n-                                               .entry_computation_layout()));\n+  const ComputationLayout computation_layout =\n+      hlo_runner_executable->executable()->compute_computation_layout();\n+  TF_ASSIGN_OR_RETURN(std::vector<ScopedShapedBuffer> argument_buffers,\n+                      TransferLiteralsToDevice(arguments, &computation_layout));\n   TF_ASSIGN_OR_RETURN(ExecutionOutput result,\n                       ExecuteWithDeviceBuffers(\n                           /*executable=*/hlo_runner_executable,"
        }
    ],
    "stats": {
        "total": 104,
        "additions": 67,
        "deletions": 37
    }
}