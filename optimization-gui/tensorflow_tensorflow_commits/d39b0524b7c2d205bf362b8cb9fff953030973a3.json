{
    "author": "metaflow",
    "message": "[XLA:GPU] add xla_enable_scoped_logging_timers debug option\n\nIt will control whether we start timers in the backend instead of\nis_autotuning_compilation that is being depricated.\n\nI have opted for positive (\"..enabled..\") option for readability even though it will not\nbe set true by proto by default.\n\nPiperOrigin-RevId: 836995959",
    "sha": "d39b0524b7c2d205bf362b8cb9fff953030973a3",
    "files": [
        {
            "sha": "da7e7b744d2b924954ad1fe44eb024a1ee5363e8",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=d39b0524b7c2d205bf362b8cb9fff953030973a3",
            "patch": "@@ -106,6 +106,7 @@ class GpuCodegenBackend : public CodegenBackend {\n     debug_options.set_xla_gpu_async_dot(false);\n     debug_options.set_xla_embed_ir_in_executable(false);\n     debug_options.set_xla_gpu_kernel_cache_file(\"\");\n+    debug_options.set_xla_enable_scoped_logging_timers(false);\n     if (force_allow_register_spills) {\n       debug_options.set_xla_gpu_filter_kernels_spilling_registers_on_autotuning(\n           false);"
        },
        {
            "sha": "d242de871a04149859bda69f0a517885e0a69997",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=d39b0524b7c2d205bf362b8cb9fff953030973a3",
            "patch": "@@ -455,6 +455,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_experimental_enable_heuristic_collective_combining(true);\n   opts.set_xla_unsupported_crash_on_hlo_pass_silent_hlo_change(false);\n   opts.set_xla_disable_automatic_host_compute_offload(false);\n+  opts.set_xla_enable_scoped_logging_timers(true);\n   opts.set_xla_unsupported_crash_on_hlo_pass_noop_change(false);\n   opts.set_xla_gpu_experimental_enable_split_k_rewrite(false);\n   opts.set_xla_gpu_experimental_enable_triton_tma(true);\n@@ -2608,6 +2609,11 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       debug_options->xla_disable_automatic_host_compute_offload(),\n       \"Return an error if HostOffloader would have automatically offloaded some\"\n       \" compute to the host.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_enable_scoped_logging_timers\",\n+      bool_setter_for(&DebugOptions::set_xla_enable_scoped_logging_timers),\n+      debug_options->xla_enable_scoped_logging_timers(),\n+      \"Do not run scoped logging timers (only supported in some places).\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_experimental_matmul_perf_table_path\",\n       string_setter_for("
        },
        {
            "sha": "7fbdc995bbff6828e54ce78c8df0b0c0f00ee4ea",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=d39b0524b7c2d205bf362b8cb9fff953030973a3",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/attributes.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -184,6 +185,7 @@ class Compiler {\n         const HloModule& module)>\n         layout_canonicalization_callback = {};\n \n+    ABSL_DEPRECATED(\"This field is being deprecated, please do not rely on it.\")\n     bool is_autotuning_compilation = false;\n \n     // AOT device description. If provided, used instead of querying the device"
        },
        {
            "sha": "3ece795912077c35dcb4df5c6b4aaf3bc638ee50",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=d39b0524b7c2d205bf362b8cb9fff953030973a3",
            "patch": "@@ -299,7 +299,7 @@ AMDGPUCompiler::CompileTargetBinary(\n     // parallelized compilation of LLVM modules.\n     XLA_SCOPED_LOGGING_TIMER_IF(\n         \"AMDGPUCompiler::CompileTargetBinary - CompileToHsaco\",\n-        !options.is_autotuning_compilation);\n+        module_config.debug_options().xla_enable_scoped_logging_timers());\n     TF_ASSIGN_OR_RETURN(\n         hsaco, amdgpu::CompileToHsaco(\n                    llvm_module, device_description.gpu_compute_capability(),"
        },
        {
            "sha": "123b99b0da27fce013eb3d44548adbe482525fd2",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 17,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=d39b0524b7c2d205bf362b8cb9fff953030973a3",
            "patch": "@@ -1870,7 +1870,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   // We dump the post-optimization HLO in RunBackend so no need to dump it here.\n   XLA_SCOPED_LOGGING_TIMER_IF(\n       absl::StrCat(\"GpuCompiler::RunHloPasses for \", module->name()),\n-      !options.is_autotuning_compilation);\n+      debug_opts.xla_enable_scoped_logging_timers());\n   uint64_t start_usecs = tsl::Env::Default()->NowMicros();\n   tsl::profiler::TraceMe activity(\n       [&] { return absl::StrCat(\"HLO Transforms:\", module->name()); },\n@@ -2039,14 +2039,15 @@ GpuCompiler::CompileSingleModule(\n     const HloModule* debug_module, llvm::Module* llvm_module, bool relocatable,\n     const CompileOptions& options, std::optional<int> shard_number) {\n   tsl::profiler::TraceMe traceme(\"CompileSingleModule\");\n+  const DebugOptions& debug_options = module_config.debug_options();\n   {\n     // This may print multiple lines per HLO compilation because of the\n     // parallelized compilation of LLVM modules.\n     XLA_SCOPED_LOGGING_TIMER_IF(\n         absl::StrCat(\n             \"GpuCompiler::RunBackend - Running LLVM verifier for \",\n             (debug_module != nullptr ? debug_module->name() : \"(unknown)\")),\n-        VLOG_IS_ON(4) && !options.is_autotuning_compilation);\n+        VLOG_IS_ON(4) && debug_options.xla_enable_scoped_logging_timers());\n \n     llvm_module->getContext().setDiagnosticHandlerCallBack(\n         NullDiagnosticHandler, nullptr);\n@@ -2072,7 +2073,7 @@ GpuCompiler::CompileSingleModule(\n                           relocatable, debug_module, options, shard_number));\n \n   const bool should_dump = DumpingEnabledForHloModule(\n-      debug_module ? debug_module->name() : \"\", module_config.debug_options());\n+      debug_module ? debug_module->name() : \"\", debug_options);\n \n   if (should_dump) {\n     if (debug_module) {\n@@ -2518,24 +2519,13 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n     DumpToFileInDirOrStdout(*module, \"\", \"gpu_target_config.pbtxt\", textproto);\n   }\n \n-  if (!options.is_autotuning_compilation) {\n-    VLOG(1) << \"Starting to compile HLO module \" << module->name();\n-  }\n-\n   XLA_SCOPED_LOGGING_TIMER_IF(\n       absl::StrCat(\"GpuCompiler::RunBackend for \", module->name()),\n-      !options.is_autotuning_compilation);\n+      debug_opts.xla_enable_scoped_logging_timers());\n   std::string slow_compilation_msg =\n       absl::StrCat(\"Compiling module \", module->name(), \" for GPU\");\n   auto slow_compile_alarm = SlowCompilationAlarm(slow_compilation_msg);\n \n-  if (options.is_autotuning_compilation) {\n-    if (module->config().debug_options().xla_embed_ir_in_executable()) {\n-      LOG(WARNING) << \"Doing autotuning compilations with \"\n-                      \"xla_embed_ir_in_executable wastes memory!\";\n-    }\n-  }\n-\n   llvm::LLVMContext llvm_context;\n   const se::DeviceDescription& gpu_device_info =\n       gpu_target_config.device_description;\n@@ -2568,8 +2558,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n \n   // The module is being moved into the GpuExecutable below and we need to\n   // read a few config values from the module, before it becomes invalid.\n-  bool embed_ir_in_executable =\n-      module->config().debug_options().xla_embed_ir_in_executable();\n+  bool embed_ir_in_executable = debug_opts.xla_embed_ir_in_executable();\n \n   tsl::profiler::ScopedAnnotation annotation([&] {\n     return absl::StrFormat(\"XlaCreateGpuExecutable:#module=%s#\","
        },
        {
            "sha": "cd7fec178cb739d5d2ad8e551353ee87d8ffec85",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=d39b0524b7c2d205bf362b8cb9fff953030973a3",
            "patch": "@@ -636,7 +636,7 @@ NVPTXCompiler::CompileTargetBinary(\n   } else {\n     selected_module = llvm_module;\n   }\n-\n+  const DebugOptions& debug_options = module_config.debug_options();\n   std::string ptx;\n   if (!(debug_module &&\n         MaybeLoadPtxFromFile(module_config, debug_module, &ptx))) {\n@@ -646,20 +646,21 @@ NVPTXCompiler::CompileTargetBinary(\n         absl::StrCat(\n             \"NVPTXCompiler::CompileTargetBinary - CompileToPtx for \",\n             (debug_module != nullptr ? debug_module->name() : \"(unknown\")),\n-        !options.is_autotuning_compilation);\n+        debug_options.xla_enable_scoped_logging_timers());\n     uint64_t start_usecs = tsl::Env::Default()->NowMicros();\n+\n     TF_ASSIGN_OR_RETURN(\n         ptx, nvptx::CompileToPtx(selected_module,\n                                  device_description.gpu_compute_capability(),\n-                                 module_config.debug_options()));\n+                                 debug_options));\n \n     uint64_t end_usecs = tsl::Env::Default()->NowMicros();\n     // This won't record values for calls that error out (because if they error\n     // out we have no way of telling how far through the process we got).\n     RecordLlvmPassesAndLlvmToPtxDuration(end_usecs - start_usecs);\n \n     if (DumpingEnabledForHloModule(debug_module ? debug_module->name() : \"\",\n-                                   module_config.debug_options())) {\n+                                   debug_options)) {\n       if (debug_module) {\n         DumpToFileInDirOrStdout(*debug_module, \"\",\n                                 shard_number.has_value()\n@@ -697,7 +698,7 @@ NVPTXCompiler::CompileTargetBinary(\n   XLA_SCOPED_LOGGING_TIMER_IF(\n       absl::StrCat(\"NVPTXCompiler::CompileTargetBinary - PtxToCubin for \",\n                    module_name),\n-      !options.is_autotuning_compilation);\n+      debug_options.xla_enable_scoped_logging_timers());\n   tsl::profiler::ScopedAnnotation annotation([&] {\n     return absl::StrFormat(\"XlaCompileGpuAsm:#module=%s#\", module_name);\n   });"
        },
        {
            "sha": "6b6325408ccfaf23a2599e423d5472b94ba139b1",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d39b0524b7c2d205bf362b8cb9fff953030973a3/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=d39b0524b7c2d205bf362b8cb9fff953030973a3",
            "patch": "@@ -148,6 +148,10 @@ message DebugOptions {\n   // Return an error if HostOffloader would have automatically offloaded some\n   // compute to the host.\n   optional bool xla_disable_automatic_host_compute_offload = 408;\n+  // Setting xla_enable_scoped_logging_timers to false will disable some of the\n+  // timers (not all places support this). This is useful during autotuning\n+  // compilation, where we want to avoid the overhead of the timers.\n+  optional bool xla_enable_scoped_logging_timers = 436;\n   // Perform hash-based cycle detection in fixed-point loops.\n   optional bool xla_hlo_pass_fix_detect_cycles = 370;\n   // Keep shardings after SPMD.\n@@ -1325,7 +1329,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 436\n+  // Next id: 437\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 51,
        "additions": 27,
        "deletions": 24
    }
}