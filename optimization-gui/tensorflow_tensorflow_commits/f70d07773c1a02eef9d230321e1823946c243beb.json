{
    "author": "mrguenther",
    "message": "Make CaseOp folder support complex and token return types\n\nPiperOrigin-RevId: 834433805",
    "sha": "f70d07773c1a02eef9d230321e1823946c243beb",
    "files": [
        {
            "sha": "d5f42e92a6049e8025f0c7a05c3dd97b915650bc",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 191,
            "deletions": 0,
            "changes": 191,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f70d07773c1a02eef9d230321e1823946c243beb/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f70d07773c1a02eef9d230321e1823946c243beb/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=f70d07773c1a02eef9d230321e1823946c243beb",
            "patch": "@@ -254,6 +254,68 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n +  return %0 : !stablehlo.token\n +}\n +\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n+@@ -47,8 +47,8 @@\n+ ////////\n+ // CaseOp\n+ \n+-// CHECK-LABEL: func.func @case_fold_constant_branch_index\n+-func.func @case_fold_constant_branch_index(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {\n++// CHECK-LABEL: func.func @case_fold_constant_branch_index_int_result\n++func.func @case_fold_constant_branch_index_int_result(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {\n+   // CHECK-NEXT: {{(^ *|func\\.)}}return %arg1\n+   // CHECK-NOT:  stablehlo.case\n+   %branch_index = stablehlo.constant dense<1> : tensor<i32>\n+@@ -60,6 +60,47 @@\n+     stablehlo.return %arg2 : tensor<i32>\n+   }) : (tensor<i32>) -> tensor<i32>\n+   func.return %result: tensor<i32>\n++}\n++\n++// -----\n++\n++// CHECK-LABEL: func.func @case_fold_constant_branch_index_complex_result\n++func.func @case_fold_constant_branch_index_complex_result(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>, %arg2: tensor<complex<f32>>) -> tensor<complex<f32>> {\n++  // CHECK-NEXT: {{(^ *|func\\.)}}return %arg1\n++  // CHECK-NOT:  stablehlo.case\n++  %branch_index = stablehlo.constant dense<1> : tensor<i32>\n++  %result = \"stablehlo.case\"(%branch_index) ({\n++    stablehlo.return %arg0 : tensor<complex<f32>>\n++  }, {\n++    stablehlo.return %arg1 : tensor<complex<f32>>\n++  }, {\n++    stablehlo.return %arg2 : tensor<complex<f32>>\n++  }) : (tensor<i32>) -> tensor<complex<f32>>\n++  func.return %result: tensor<complex<f32>>\n++}\n++\n++// -----\n++\n++// CHECK-LABEL: func.func @case_fold_inline_call_tf_function\n++func.func @case_fold_inline_call_tf_function(%arg0: !stablehlo.token {jax.token = true}, %arg1: tensor<16xi32>, %arg2: tensor<16xi64>) -> (!stablehlo.token {jax.token = true}, tensor<16xi32> {jax.result_info = \"result\"}) {\n++  // CHECK: [[RESULT_TOKEN:%.+]] = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2)\n++  // CHECK: [[UNUSED_TOKEN:%.+]] = {{\"?}}stablehlo.case{{\"?}}(\n++  // CHECK: return [[RESULT_TOKEN]], %arg1\n++  %c = stablehlo.constant dense<1> : tensor<i32>\n++  %c_0 = stablehlo.constant dense<0> : tensor<i32>\n++  %0 = \"stablehlo.case\"(%c_0) ({\n++    stablehlo.return %c_0 : tensor<i32>\n++  }, {\n++    stablehlo.return %c : tensor<i32>\n++  }) : (tensor<i32>) -> tensor<i32>\n++  %1 = \"stablehlo.case\"(%0) ({\n++    %2 = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_index = 0 : i64, has_token_input_output = true}} : (!stablehlo.token, tensor<16xi32>, tensor<16xi64>) -> !stablehlo.token\n++    stablehlo.return %2 : !stablehlo.token\n++  }, {\n++    %2 = stablehlo.custom_call @tf.call_tf_function(%arg0, %arg1, %arg2) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_index = 1 : i64, has_token_input_output = true}} : (!stablehlo.token, tensor<16xi32>, tensor<16xi64>) -> !stablehlo.token\n++    stablehlo.return %2 : !stablehlo.token\n++  }) : (tensor<i32>) -> !stablehlo.token\n++  return %1, %arg1 : !stablehlo.token, tensor<16xi32>\n+ }\n+ \n+ // -----\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n@@ -514,6 +576,135 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stabl\n  \n  // Apply numpy broadcasting to the given operands, returning an error if any\n  // operands are not broadcastable.\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n+@@ -14,6 +14,7 @@\n+ \n+ #include <cassert>\n+ #include <cmath>\n++#include <complex>\n+ #include <cstddef>\n+ #include <cstdint>\n+ #include <functional>\n+@@ -38,6 +39,7 @@\n+ #include \"mlir/Dialect/CommonFolders.h\"\n+ #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+ #include \"mlir/Dialect/Utils/IndexingUtils.h\"\n++#include \"mlir/IR/Builders.h\"\n+ #include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n+ #include \"mlir/IR/BuiltinAttributes.h\"\n+ #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+@@ -82,6 +84,71 @@\n+                 /*isUnsigned=*/!isSigned);\n+ }\n+ \n++class LazyPlaceholderValue {\n++ public:\n++  static FailureOr<LazyPlaceholderValue> preparePlaceholderFor(\n++      PatternRewriter& rewriter, Value likeValue) {\n++    Type valueType = likeValue.getType();\n++\n++    // If `getZeroAttr(valueType)` returns a valid attribute, simply wrap the\n++    // result in a `stablehlo.constant` op.\n++    if (TypedAttr placeholderAttr = rewriter.getZeroAttr(valueType)) {\n++      return LazyPlaceholderValue([&rewriter, placeholderAttr](Location loc) {\n++        return ConstantOp::create(rewriter, loc, placeholderAttr);\n++      });\n++    }\n++\n++    // `getZeroAttr` doesn't support complex types, so we handle that case here.\n++    if (auto shapedType = dyn_cast<ShapedType>(valueType)) {\n++      if (auto complexElementType =\n++              dyn_cast<ComplexType>(shapedType.getElementType())) {\n++        if (!isa<FloatType>(complexElementType.getElementType()))\n++          return rewriter.notifyMatchFailure(\n++              likeValue.getLoc(),\n++              \"unexpected real component type for complex element type\");\n++        auto realImagComponentFloatType =\n++            cast<FloatType>(complexElementType.getElementType());\n++        APFloat apFloatZero(0.0);\n++        bool losesInfo;\n++        apFloatZero.convert(realImagComponentFloatType.getFloatSemantics(),\n++                            llvm::RoundingMode::NearestTiesToEven, &losesInfo);\n++        std::complex<APFloat> complexZeroScalar(apFloatZero, apFloatZero);\n++        auto complexZeroSplat =\n++            SplatElementsAttr::get(shapedType, complexZeroScalar);\n++        return LazyPlaceholderValue(\n++            [&rewriter, complexZeroSplat](Location loc) {\n++              return ConstantOp::create(rewriter, loc, complexZeroSplat);\n++            });\n++      }\n++    }\n++\n++    // If `valueType` is a token type, use `stablehlo.after_all` with no\n++    // arguments to create a placeholder token.\n++    if (isa<TokenType>(valueType)) {\n++      return LazyPlaceholderValue([&rewriter](Location loc) {  //\n++        return AfterAllOp::create(rewriter, loc, {});\n++      });\n++    }\n++\n++    // TODO: Support quantized and buffer types.\n++\n++    return rewriter.notifyMatchFailure(\n++        likeValue.getLoc(), \"unable to create placeholder value for type\");\n++  }\n++\n++  Value createAt(Location loc) const {\n++    if (!lazyInitializer)\n++      llvm::report_fatal_error(\"No lazy initializer for this value type.\");\n++    return lazyInitializer(loc);\n++  }\n++\n++ private:\n++  LazyPlaceholderValue(std::function<Value(Location)> lazyInitializer)\n++      : lazyInitializer(std::move(lazyInitializer)) {}\n++\n++  std::function<Value(Location)> lazyInitializer;\n++};\n++\n+ LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,\n+                                         Operation* op, ShapedType resultType) {\n+   if (!resultType.hasStaticShape())\n+@@ -737,18 +804,14 @@\n+     Operation* terminator = blockToInline->getTerminator();\n+     ValueRange results = terminator->getOperands();\n+ \n+-    // TODO: Add support for complex, quantized, and token return types.\n+-    // Currently, this pattern only supports int and float return types. We'll\n+-    // need a more general equivalent of `getZeroAttr` to support other types.\n+-    SmallVector<TypedAttr> placeholderAttrs;\n++    SmallVector<LazyPlaceholderValue> lazyPlaceholderResults;\n+     for (auto result : op.getResults()) {\n+-      TypedAttr placeholderAttr = rewriter.getZeroAttr(result.getType());\n+-      if (!placeholderAttr)\n+-        return rewriter.notifyMatchFailure(\n+-            op,\n+-            \"The case op's return type isn't currently supported by this \"\n+-            \"optimization pattern.\");\n+-      placeholderAttrs.push_back(placeholderAttr);\n++      auto placeholder =\n++          LazyPlaceholderValue::preparePlaceholderFor(rewriter, result);\n++\n++      if (failed(placeholder)) return failure();\n++\n++      lazyPlaceholderResults.push_back(std::move(placeholder.value()));\n+     }\n+ \n+     // Inline the active branch of the `case` op.\n+@@ -763,9 +826,9 @@\n+     Block& noopBlock = region.emplaceBlock();\n+     SmallVector<Value> placeholderResults;\n+     rewriter.setInsertionPointToEnd(&noopBlock);\n+-    for (auto placeholderAttr : placeholderAttrs) {\n++    for (const auto& lazyPlaceholderResult : lazyPlaceholderResults) {\n+       placeholderResults.push_back(\n+-          ConstantOp::create(rewriter, region.getLoc(), placeholderAttr));\n++          lazyPlaceholderResult.createAt(region.getLoc()));\n+     }\n+     stablehlo::ReturnOp::create(rewriter, region.getLoc(), placeholderResults);\n+ \n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td"
        }
    ],
    "stats": {
        "total": 191,
        "additions": 191,
        "deletions": 0
    }
}