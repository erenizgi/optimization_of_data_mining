{
    "author": "jcai19",
    "message": "[XLA][Numerics][HLO Value Tracking] Create an original value for compiler-inserted tuples during fusion\n\nThis also removes the code that handles original values in MergeFusionInstructionIntoMultiOutput, as it eventually calls into HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation to create a tuple result and the corresponding original value.\n\nPiperOrigin-RevId: 817944313",
    "sha": "05715ab5d2add87bae67e7763f723ce2d5893f0b",
    "files": [
        {
            "sha": "8c9b3e9ab0b8282d53987cb1cf96a76ef82d6c93",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 72,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc?ref=05715ab5d2add87bae67e7763f723ce2d5893f0b",
            "patch": "@@ -1960,68 +1960,6 @@ HloInstruction* HloCallableInstruction::AppendInstructionIntoCalledComputation(\n                                                         add_output);\n }\n \n-namespace {\n-\n-// Returns the original value that can be used for multi output fusion. The\n-// placeholder is just to provide a matching tuple tree of empty original arrays\n-// for the fusion logic in `SetOriginalValueOnFusedInstruction` to correctly\n-// populate the fused original value.\n-std::shared_ptr<OriginalValue> GetOriginalValueOrPlaceholderForFusion(\n-    HloInstruction* inst) {\n-  if (inst->original_value()) {\n-    if (!inst->original_value()->IsCompatibleWith(inst->shape())) {\n-      LOG(ERROR) << \"Instruction '\" << inst->name()\n-                 << \"' has original value incompatible with its \"\n-                    \"shape.\\nOriginal value: \"\n-                 << inst->original_value()->ToString()\n-                 << \"\\nShape: \" << inst->shape().ToString();\n-      // Return nullptr to bail out of original value tracking.\n-      return nullptr;\n-    }\n-    return inst->original_value();\n-  }\n-  return std::make_shared<OriginalValue>(inst->shape());\n-}\n-\n-// Sets the original value on the final (aka remaining) instruction after\n-// fusion. This function assumes the final instruction to have a fused\n-// shape. This fused shape should be a tuple containing elements from\n-// `first_fused_ov` and `second_fused_ov`.\n-void SetOriginalValueOnFusedInstruction(\n-    HloInstruction* final_instr, std::shared_ptr<OriginalValue> first_fused_ov,\n-    std::shared_ptr<OriginalValue> second_fused_ov) {\n-  if (!first_fused_ov || !second_fused_ov) {\n-    return;\n-  }\n-  if (first_fused_ov->is_synthetic_call() ||\n-      second_fused_ov->is_synthetic_call() ||\n-      (first_fused_ov->IsEmpty() && second_fused_ov->IsEmpty())) {\n-    // Synthetic calls are generated by optimization passes and usually they\n-    // should be inlined immediately. If somehow this multi output pass needs to\n-    // fuse synthetic calls, we just ignore the original value because it's not\n-    // clear how to fuse them.\n-    final_instr->set_original_value(nullptr);\n-    return;\n-  }\n-\n-  std::vector<std::optional<OriginalArray>> new_leaves;\n-  for (const auto& [index, value] : first_fused_ov->original_arrays()) {\n-    new_leaves.push_back(value);\n-  }\n-  for (const auto& [index, value] : second_fused_ov->original_arrays()) {\n-    new_leaves.push_back(value);\n-  }\n-\n-  auto new_ov = std::make_shared<OriginalValue>(final_instr->shape());\n-  int64_t leaf_index = 0;\n-  for (auto& [index, value] : new_ov->mutable_original_arrays()) {\n-    CHECK_LT(leaf_index, new_leaves.size());\n-    value = new_leaves[leaf_index++];\n-  }\n-  final_instr->set_original_value(new_ov);\n-}\n-}  // namespace\n-\n HloInstruction*\n HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     HloInstruction* instruction_to_append, bool add_output) {\n@@ -2151,9 +2089,6 @@ HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     }\n     // If this is already a multioutput instruction, expand the root tuple\n     // by 1.\n-    auto second_fused_ov =\n-        GetOriginalValueOrPlaceholderForFusion(instruction_to_append);\n-    auto first_fused_ov = GetOriginalValueOrPlaceholderForFusion(this);\n     HloInstruction::InstructionVector tuple_elements;\n     bool newly_created_tuple_instr = false;\n     if (root->opcode() == HloOpcode::kTuple) {\n@@ -2171,14 +2106,21 @@ HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     }\n     HloInstruction* new_root = called_computation()->AddInstruction(\n         HloInstruction::CreateTuple(tuple_elements));\n+    new_root->set_original_value(\n+        xla::OriginalValue::CreateFromInstruction(new_root));\n \n     // No need to create an original value for a new root with added outputs\n     // as the original value is saved in the get-tuple-element instructions\n     // that use it.\n     called_computation()->set_root_instruction(new_root,\n                                                /*accept_different_shape=*/true);\n+\n+    // Update the shape of the fusion instruction to the shape of the new root.\n     *mutable_shape() = new_root->shape();\n-    SetOriginalValueOnFusedInstruction(this, first_fused_ov, second_fused_ov);\n+    // Update the original value of the fusion instruction to the original value\n+    // of the new root.\n+    set_original_value(new_root->original_value());\n+\n     // The instruction might have an existing sharding, which will no longer\n     // be valid after we change the shape. So clear the sharding.\n     clear_sharding();\n@@ -2446,9 +2388,6 @@ void HloFusionInstruction::MergeFusionInstruction(\n \n void HloFusionInstruction::MergeFusionInstructionIntoMultiOutput(\n     HloFusionInstruction* instruction_to_merge) {\n-  auto first_fused_ov = GetOriginalValueOrPlaceholderForFusion(this);\n-  auto second_fused_ov =\n-      GetOriginalValueOrPlaceholderForFusion(instruction_to_merge);\n   // Add all non-parameter fused instructions to 'unfused_instructions' to be\n   // merged into 'this'. `old_to_new' maps the instructions in the fused node\n   // to the disassembled fusion instructions.\n@@ -2550,9 +2489,6 @@ void HloFusionInstruction::MergeFusionInstructionIntoMultiOutput(\n     }\n     TF_CHECK_OK(instruction->parent()->RemoveInstruction(instruction));\n   }\n-  if (this->IsMultiOutputFusion()) {\n-    SetOriginalValueOnFusedInstruction(this, first_fused_ov, second_fused_ov);\n-  }\n }\n \n HloComputation* HloFusionInstruction::fused_instructions_computation() const {"
        },
        {
            "sha": "9ca6b693639a7f53511af2dff9d8feec5e392ac7",
            "filename": "third_party/xla/xla/hlo/ir/hlo_original_value.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_original_value.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_original_value.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_original_value.cc?ref=05715ab5d2add87bae67e7763f723ce2d5893f0b",
            "patch": "@@ -219,12 +219,14 @@ std::shared_ptr<OriginalValue> OriginalValue::CreateFromInstruction(\n   if (instruction->opcode() == HloOpcode::kTuple) {\n     auto original_value = std::make_shared<OriginalValue>(\n         TupleTree<std::optional<OriginalArray>>(instruction->shape()));\n+    bool has_original_value = false;\n     for (int64_t i = 0; i < instruction->operand_count(); ++i) {\n       const HloInstruction* operand = instruction->operand(i);\n       auto op_original_value = operand->original_value();\n       if (!op_original_value || op_original_value->is_synthetic_call()) {\n-        return nullptr;\n+        continue;\n       }\n+      has_original_value = true;\n       const auto& op_tree = op_original_value->tree();\n       op_tree.ForEachElement([&](const ShapeIndex& index,\n                                  const std::optional<OriginalArray>& value) {\n@@ -233,7 +235,7 @@ std::shared_ptr<OriginalValue> OriginalValue::CreateFromInstruction(\n         *original_value->mutable_tree()->mutable_element(dest_index) = value;\n       });\n     }\n-    return original_value;\n+    return has_original_value ? original_value : nullptr;\n   }\n \n   // Default case: create a new tree with leaves pointing to this instruction."
        },
        {
            "sha": "2cd2dc855f9e531d41de9e647287c5e485f14629",
            "filename": "third_party/xla/xla/hlo/ir/hlo_original_value_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_original_value_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_original_value_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_original_value_test.cc?ref=05715ab5d2add87bae67e7763f723ce2d5893f0b",
            "patch": "@@ -291,7 +291,7 @@ ENTRY main {\n   EXPECT_EQ(p0->original_value()->ToString(), \"({\\\"p0\\\" {0}}, {\\\"p0\\\" {1}})\");\n }\n \n-TEST_F(OriginalValueHloTest, CreateFromInstructionTupleWithSynthetic) {\n+TEST_F(OriginalValueHloTest, CreateFromInstructionTupleWithSyntheticElement) {\n   const char* hlo_string = R\"(\n HloModule test\n \n@@ -312,7 +312,8 @@ ENTRY main {\n       std::make_shared<OriginalValue>(OriginalValue::SyntheticCall()));\n   tuple->set_original_value(OriginalValue::CreateFromInstruction(tuple));\n \n-  EXPECT_EQ(tuple->original_value(), nullptr);\n+  ASSERT_NE(tuple->original_value(), nullptr);\n+  EXPECT_EQ(tuple->original_value()->ToString(), \"({\\\"p0\\\"}, {})\");\n }\n \n TEST_F(OriginalValueHloTest, CopyOriginalValue) {"
        },
        {
            "sha": "4a03a505efc363420152633259f4ae15d80ef201",
            "filename": "third_party/xla/xla/service/instruction_fusion_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion_test.cc?ref=05715ab5d2add87bae67e7763f723ce2d5893f0b",
            "patch": "@@ -137,8 +137,12 @@ TEST_F(InstructionFusionTest, FuseInstructionsWithOriginalValue) {\n   EXPECT_THAT(fusion->fused_expression_root(),\n               op::Subtract(op::Add(), op::Parameter()))\n       << module->ToString();\n+  absl::string_view expected_origin = \"{\\\"sub\\\"}\";\n   ASSERT_NE(fusion->original_value(), nullptr);\n-  EXPECT_EQ(fusion->original_value()->ToString(), \"{\\\"sub\\\"}\");\n+  EXPECT_EQ(fusion->original_value()->ToString(), expected_origin);\n+  ASSERT_NE(fusion->fused_expression_root()->original_value(), nullptr);\n+  ASSERT_EQ(fusion->fused_expression_root()->original_value()->ToString(),\n+            expected_origin);\n }\n \n TEST_F(InstructionFusionTest,\n@@ -158,11 +162,15 @@ TEST_F(InstructionFusionTest,\n   HloInstruction* fusion = InstructionFusionForTesting().FuseIntoMultiOutput(\n       abs, tanh, module->entry_computation());\n \n+  absl::string_view expected_original_value = \"({\\\"tanh\\\"}, {\\\"abs\\\"})\";\n   ASSERT_THAT(fusion, op::Fusion()) << module->ToString();\n   EXPECT_THAT(fusion->fused_expression_root(), op::Tuple(op::Tanh(), op::Abs()))\n       << module->ToString();\n+  ASSERT_NE(fusion->fused_expression_root()->original_value(), nullptr);\n+  ASSERT_EQ(fusion->fused_expression_root()->original_value()->ToString(),\n+            expected_original_value);\n   ASSERT_NE(fusion->original_value(), nullptr);\n-  EXPECT_EQ(fusion->original_value()->ToString(), \"({\\\"tanh\\\"}, {\\\"abs\\\"})\");\n+  EXPECT_EQ(fusion->original_value()->ToString(), expected_original_value);\n }\n \n TEST_F(InstructionFusionTest, AvoidDuplicationIfNotAllFusible) {"
        },
        {
            "sha": "6fcd882a6baa27749eebd58cc286aa0a45b11ee4",
            "filename": "third_party/xla/xla/service/propagate_original_value_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fservice%2Fpropagate_original_value_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05715ab5d2add87bae67e7763f723ce2d5893f0b/third_party%2Fxla%2Fxla%2Fservice%2Fpropagate_original_value_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fpropagate_original_value_test.cc?ref=05715ab5d2add87bae67e7763f723ce2d5893f0b",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/algebraic_simplifier.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/call_inliner.h\"\n-#include \"xla/service/instruction_fusion.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -80,48 +79,6 @@ ENTRY %main (param: s32[2,8], param.1: s32[8,8]) -> s32[2,8] {\n   EXPECT_NE(new_root->original_value(), nullptr);\n }\n \n-TEST_F(PropagateOriginalValueTest, InstructionFusion) {\n-  constexpr absl::string_view hlo_string = R\"(\n-HloModule test, entry_computation_layout={(s32[]{:T(256)})->u32[2]{0:T(256)}}\n-\n-ENTRY test {\n-  Arg_0 = s32[]{:T(256)} parameter(0), origin={{\"Arg_0\"}}, metadata={op_name=\"seed\"}\n-  constant = s32[]{:T(256)} constant(32), origin={{\"constant\"}}\n-  shift-right-logical = s32[]{:T(256)} shift-right-logical(Arg_0, constant), origin={{\"shift-right-logical\"}}\n-  convert = u32[]{:T(256)} convert(shift-right-logical), origin={{\"convert\"}}\n-  bitcast = u32[1]{0:T(256)} bitcast(convert), origin={{\"reshape\"}}\n-  constant.1 = u32[]{:T(256)} constant(0)\n-  pad = u32[2]{0:T(256)} pad(bitcast, constant.1), padding=0_1\n-  convert.1 = u32[]{:T(256)} convert(Arg_0), origin={{\"convert.1\"}}\n-  bitcast.1 = u32[1]{0:T(256)} bitcast(convert.1), origin={{\"reshape.1\"}}\n-  pad.1 = u32[2]{0:T(256)} pad(bitcast.1, constant.1), padding=1_0\n-  ROOT add = u32[2]{0:T(256)} add(pad, pad.1), origin={{\"concatenate\"}}\n-}\n-  )\";\n-\n-  RunAndFilecheckHloRewrite(\n-      hlo_string,\n-      InstructionFusion(InstructionFusion::IsExpensive, /*may_duplicate=*/true),\n-      R\"(\n-CHECK: %fused_computation\n-CHECK:   %[[PARAM:.*]] = s32[]{:T(256)} parameter(0)\n-CHECK:   %[[CONSTANT:.*]] = s32[]{:T(256)} constant(32), origin={{[{]}}{\"constant\"}}\n-CHECK:   %[[SHIFT:.*]] = s32[]{:T(256)} shift-right-logical(%[[PARAM]], %[[CONSTANT]]), origin={{[{]}}{\"shift-right-logical\"}\n-CHECK:   %[[CONVERT:.*]] = u32[]{:T(256)} convert(%[[SHIFT]]), origin={{[{]}}{\"convert\"}\n-CHECK:   %[[BITCAST:.*]] = u32[1]{0:T(256)} bitcast(%[[CONVERT]]), origin={{[{]}}{\"reshape\"}\n-CHECK:   %[[CONSTANT1:.*]] = u32[]{:T(256)} constant(0)\n-CHECK:   %[[PAD:.*]] = u32[2]{0:T(256)} pad(%[[BITCAST]], %[[CONSTANT1]]), padding=0_1\n-CHECK:   %[[CONVERT1:.*]] = u32[]{:T(256)} convert(%[[PARAM]]), origin={{[{]}}{\"convert.1\"}\n-CHECK:   %[[BITCAST1:.*]] = u32[1]{0:T(256)} bitcast(%[[CONVERT1]]), origin={{[{]}}{\"reshape.1\"}\n-CHECK:   %[[PAD1:.*]] = u32[2]{0:T(256)} pad(%[[BITCAST1]], %[[CONSTANT1]]), padding=1_0\n-CHECK:   ROOT %[[ADD:.*]] = u32[2]{0:T(256)} add(%[[PAD]], %[[PAD1]]), origin={{[{]}}{\"concatenate\"}\n-\n-CHECK: ENTRY %test\n-CHECK:   %Arg_0 = s32[]{:T(256)} parameter(0), origin={{[{]}}{\"Arg_0\"}\n-CHECK:   ROOT %pad_add_fusion = u32[2]{0:T(256)} fusion(%Arg_0), kind=kLoop, calls=%fused_computation, origin={{[{]}}{\"concatenate\"}\n-)\");\n-}\n-\n TEST_F(PropagateOriginalValueTest, CallInlinerMultipleCallSites) {\n   const absl::string_view hlo_string = R\"(\n // CHECK-LABEL:test"
        }
    ],
    "stats": {
        "total": 146,
        "additions": 25,
        "deletions": 121
    }
}