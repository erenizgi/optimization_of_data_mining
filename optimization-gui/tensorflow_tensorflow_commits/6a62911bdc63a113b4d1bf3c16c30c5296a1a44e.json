{
    "author": "jcai19",
    "message": "[XLA][Numerics] Set original values properly when exporting stableHLO tuple results to HLO\n\nThis sets the correct original value for individual elements when exporting an tuple result.\n\nPiperOrigin-RevId: 810985823",
    "sha": "6a62911bdc63a113b4d1bf3c16c30c5296a1a44e",
    "files": [
        {
            "sha": "404b2a900a96d0f988181273c66d4aec57710715",
            "filename": "third_party/xla/xla/hlo/builder/xla_builder.h",
            "status": "modified",
            "additions": 20,
            "deletions": 6,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a62911bdc63a113b4d1bf3c16c30c5296a1a44e/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a62911bdc63a113b4d1bf3c16c30c5296a1a44e/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h?ref=6a62911bdc63a113b4d1bf3c16c30c5296a1a44e",
            "patch": "@@ -371,6 +371,11 @@ class XlaBuilder {\n   // Returns the OpSharding that will be attached to all instructions.\n   const std::optional<OpSharding>& sharding() const { return sharding_; }\n \n+  // Returns the OriginalValueProto that will be attached to all instructions.\n+  const std::optional<OriginalValueProto>& original_value() const {\n+    return original_value_;\n+  }\n+\n   // Sets the builder to a mode where it will die immediately when an error is\n   // encountered, rather than producing it in a deferred fashion when Build() is\n   // called (which is the default).\n@@ -1985,22 +1990,31 @@ class XlaScopedOriginalValueAssignment {\n   XlaScopedOriginalValueAssignment(\n       xla::XlaBuilder* builder,\n       std::optional<OriginalValueProto> original_value_proto)\n-      : builder_(builder) {\n-    if (original_value_proto.has_value()) {\n-      builder_->SetOriginalValue(original_value_proto.value());\n-    }\n+      : builder_(builder), prev_original_value_(builder->original_value()) {\n+    SetOriginalValue(original_value_proto);\n   }\n \n   XlaScopedOriginalValueAssignment(const XlaScopedOriginalValueAssignment&) =\n       delete;\n   XlaScopedOriginalValueAssignment& operator=(\n       const XlaScopedOriginalValueAssignment&) = delete;\n \n-  ~XlaScopedOriginalValueAssignment() { builder_->ClearOriginalValue(); }\n+  ~XlaScopedOriginalValueAssignment() {\n+    SetOriginalValue(prev_original_value_);\n+  }\n \n  private:\n+  void SetOriginalValue(\n+      const std::optional<OriginalValueProto>& original_value_proto) {\n+    if (original_value_proto.has_value()) {\n+      builder_->SetOriginalValue(original_value_proto.value());\n+    } else {\n+      builder_->ClearOriginalValue();\n+    }\n+  }\n+\n   xla::XlaBuilder* const builder_;\n-  std::optional<OpSharding> prev_sharding_;\n+  std::optional<OriginalValueProto> prev_original_value_;\n };\n \n // RAII-style object: save the current builder's frontend attributes, and merge"
        },
        {
            "sha": "9034858c0023442777a5f876e7766c53139446b7",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a62911bdc63a113b4d1bf3c16c30c5296a1a44e/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a62911bdc63a113b4d1bf3c16c30c5296a1a44e/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=6a62911bdc63a113b4d1bf3c16c30c5296a1a44e",
            "patch": "@@ -83,6 +83,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_original_value.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/translate/hlo_to_mhlo/hlo_utils.h\"\n@@ -1298,6 +1299,22 @@ void BuildGetTupleElementsForTupleResults(\n     mlir::Operation* op, xla::XlaOp tuple, xla::XlaBuilder* builder,\n     llvm::DenseMap<mlir::Value, xla::XlaOp>& values,\n     unsigned num_implicit_results = 0) {\n+  auto get_tuple_element_original_value_proto =\n+      [&builder](int64_t index) -> std::optional<xla::OriginalValueProto> {\n+    auto original_value_proto = builder->original_value();\n+    if (original_value_proto.has_value()) {\n+      auto original_value =\n+          xla::OriginalValue::FromProto(*original_value_proto);\n+      auto subtree = original_value->tree().Subtree({index});\n+      if (subtree.ok()) {\n+        auto element_original_value =\n+            std::make_shared<xla::OriginalValue>(std::move(subtree.value()));\n+        return element_original_value->ToProto();\n+      }\n+    }\n+    return std::nullopt;\n+  };\n+\n   const std::optional<xla::OpSharding>& sharding = builder->sharding();\n   if (sharding.has_value()) {\n     bool is_tuple_sharding = sharding->type() == xla::OpSharding::TUPLE;\n@@ -1309,11 +1326,19 @@ void BuildGetTupleElementsForTupleResults(\n       xla::XlaScopedShardingAssignment scoped_sharding(\n           builder,\n           is_tuple_sharding ? sharding->tuple_shardings(index) : sharding);\n+      // Set the original value for the get-tuple-element.\n+      xla::XlaScopedOriginalValueAssignment original_value(\n+          builder,\n+          get_tuple_element_original_value_proto(static_cast<int64_t>(index)));\n       values[result] = xla::GetTupleElement(tuple, index);\n     }\n   } else {\n     xla::XlaScopedShardingAssignment scoped_sharding(builder, std::nullopt);\n     for (auto [index, result] : llvm::enumerate(op->getResults())) {\n+      // Set the original value for the get-tuple-element.\n+      xla::XlaScopedOriginalValueAssignment original_value(\n+          builder,\n+          get_tuple_element_original_value_proto(static_cast<int64_t>(index)));\n       values[result] = xla::GetTupleElement(tuple, index);\n     }\n   }"
        },
        {
            "sha": "8280927ea5eb6edb6cfd5dbf46604a0d254114f3",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/location_to_original_value.mlir",
            "status": "modified",
            "additions": 37,
            "deletions": 20,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a62911bdc63a113b4d1bf3c16c30c5296a1a44e/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Flocation_to_original_value.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a62911bdc63a113b4d1bf3c16c30c5296a1a44e/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Flocation_to_original_value.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Flocation_to_original_value.mlir?ref=6a62911bdc63a113b4d1bf3c16c30c5296a1a44e",
            "patch": "@@ -1,6 +1,26 @@\n // RUN: xla-translate -split-input-file -mlir-hlo-to-hlo-text %s | FileCheck %s --dump-input=always --check-prefixes=CHECK\n \n \n+#loc1 = loc(\"x\")\n+#loc2 = loc(\"mhlo.original_value={{\\22x\\22}}\")\n+#loc3 = loc(\"y\")\n+#loc4 = loc(\"mhlo.original_value={{\\22y\\22}}\")\n+#loc5 = loc(\"add0\")\n+#loc6 = loc(\"Add_0\")\n+#loc7 = loc(\"mhlo.original_value={{\\22add1\\22}}\")\n+#loc8 = loc(\"Add_1\")\n+#loc9 = loc(\"source.txt\":17:0)\n+#loc10 = loc(\"mhlo.original_value={{\\22add2\\22}}\")\n+#loc11 = loc(fused[#loc1, #loc2])\n+#loc12 = loc(fused[#loc3, #loc4])\n+#loc13 = loc(fused[#loc6, #loc7])\n+#loc14 = loc(fused[#loc8, #loc9, #loc10])\n+#loc15 = loc(\"TopK_0\")\n+#loc16 = loc(\"mhlo.original_value={({\\22t\\22 {0}}, {\\22t\\22 {1}})}\")\n+#loc17 = loc(fused[#loc15, #loc16])\n+\n+module @Test {\n+\n // CHECK-LABEL: main\n // CHECK: %[[ARG0:.*]] = f32[4] parameter(0), origin={{[{][{]}}\"x\"{{[}][}]}}\n // CHECK: %[[ARG1:.*]] = f32[4] parameter(1), origin={{[{][{]}}\"y\"{{[}][}]}}\n@@ -9,27 +29,24 @@\n // CHECK-SAME: metadata={op_name=\"add0\"}\n // CHECK: %[[ADD1:.*]] = f32[4] add(%[[ADD0]], %[[ARG1]]), origin={{[{][{]}}\"add1\"{{[}][}]}}, metadata=\n // CHECK: ROOT %[[ADD2:.*]] = f32[4] add(%[[ADD1]], %[[ARG1]]), origin={{[{][{]}}\"add2\"{{[}][}]}}, metadata=\n-\n-#loc1 = loc(\"x\")\n-#loc2 = loc(\"mhlo.original_value={{\\22x\\22}}\")\n-#loc3 = loc(\"y\")\n-#loc4 = loc(\"mhlo.original_value={{\\22y\\22}}\")\n-#loc11 = loc(fused[#loc1, #loc2])\n-#loc12 = loc(fused[#loc3, #loc4])\n-module @Test attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {\n   func.func @main(%arg0: tensor<4xf32> loc(fused[#loc1, #loc2]), %arg1: tensor<4xf32> loc(fused[#loc3, #loc4])) -> tensor<4xf32> {\n     %0 = mhlo.add %arg0, %arg1 : tensor<4xf32> loc(#loc5)\n     %1 = mhlo.add %0, %arg1 : tensor<4xf32> loc(#loc13)\n     %2 = mhlo.add %1, %arg1 : tensor<4xf32> loc(#loc14)\n-    return %2 : tensor<4xf32> loc(#loc)\n-  } loc(#loc)\n-} loc(#loc)\n-#loc = loc(unknown)\n-#loc5 = loc(\"add0\")\n-#loc6 = loc(\"embedded_inference/Add_0\")\n-#loc7 = loc(\"mhlo.original_value={{\\22add1\\22}}\")\n-#loc8 = loc(\"embedded_inference/Add_1\")\n-#loc9 = loc(\"source.txt\":17:0)\n-#loc10 = loc(\"mhlo.original_value={{\\22add2\\22}}\")\n-#loc13 = loc(fused[#loc6, #loc7])\n-#loc14 = loc(fused[#loc8, #loc9, #loc10])\n\\ No newline at end of file\n+    return %2 : tensor<4xf32>\n+  }\n+\n+\n+// CHECK-LABEL: tuple_results\n+// CHECK:  %Arg_0.1 = f32[10] parameter(0)\n+// CHECK:  %[[TOPK:.*]] = (f32[8], s32[8]) topk(%Arg_0.1), k=8, largest=true, origin={({\"t\" {0}{{[}]}}, {\"t\" {1}{{[}]}})}\n+// CHECK:  ROOT %[[GTE0:.*]] = f32[8] get-tuple-element(%[[TOPK]]), index=0, origin={{[{][{]}}\"t\" {0}{{[}][}]}}\n+// CHECK:  %[[GTE1:.*]] = s32[8] get-tuple-element(%[[TOPK]]), index=1, origin={{[{][{]}}\"t\" {1}{{[}][}]}}\n+  func.func @tuple_results(%arg0: tensor<10xf32>) -> tensor<8xf32> {\n+    %0:2 = mhlo.topk(%arg0, k=8, largest=true) : tensor<10xf32> -> (tensor<8xf32>, tensor<8xi32>) loc(#loc17)\n+    return %0#0 : tensor<8xf32>\n+  }\n+\n+}\n+\n+"
        }
    ],
    "stats": {
        "total": 108,
        "additions": 82,
        "deletions": 26
    }
}