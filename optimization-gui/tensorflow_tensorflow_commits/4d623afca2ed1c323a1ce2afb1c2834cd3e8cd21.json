{
    "author": "WillFroom",
    "message": "[XLA][XTile] Make transpose folder work with xtile extract.\n\nPiperOrigin-RevId: 824439434",
    "sha": "4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
    "files": [
        {
            "sha": "c5d1154d9fff52481c67c1d19e99e78aa7e17082",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -34,8 +34,8 @@ void CreateTritonXlaPipeline(\n     const stream_executor::GpuComputeCapability& gpu_cc, bool rewrite_int4,\n     bool allow_tma) {\n   pm->addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n-  pm->addPass(mlir::triton::xla::CreateTritonXLALowerXTilePass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n+  pm->addPass(mlir::triton::xla::CreateTritonXLALowerXTilePass());\n \n   auto* cuda_cc = gpu_cc.cuda_compute_capability();\n   bool is_at_least_hopper = cuda_cc != nullptr && cuda_cc->IsAtLeastHopper();"
        },
        {
            "sha": "41b259074a00cffa1fac206b6739e75c72e1ce58",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -60,7 +60,8 @@ def TritonXLAFoldTransposePass : Pass<\"triton-xla-fold-transpose\", \"mlir::Module\n     This pass tries to remove transposes by folding them into loads.\n   }];\n   let dependentDialects = [\n-    \"::mlir::triton::xla::XlaTritonDialect\"\n+    \"::mlir::triton::xla::XlaTritonDialect\",\n+    \"::mlir::memref::MemRefDialect\",\n   ];\n   let constructor = \"CreateTritonXLAFoldTransposePass()\";\n }"
        },
        {
            "sha": "26cc8ea843a4e7b9cc16906e2f8b7a140ecde829",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_fold_transpose.mlir",
            "status": "modified",
            "additions": 19,
            "deletions": 10,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -1,16 +1,25 @@\n // RUN: xla-opt %s --triton-xla-fold-transpose | FileCheck %s\n \n-// CHECK-LABEL: func @fold_transpose_of_extract\n-func.func @fold_transpose_of_extract(%arg0: !tt.ptr<f32>, %arg1: i32) -> tensor<8x4xf32> {\n-  // CHECK: %[[EXTRACT:.*]] = triton_xla.extract from %arg0\n-  // CHECK-SAME: as memref<16x8x4xf32, #triton_xla.layout<[0, 2, 1]>>\n-  // CHECK-SAME: [0, 0, 0] [8, 1, 4] [1, 1, 1] : tensor<8x4xf32>\n-  %0 = triton_xla.extract from %arg0\n-    as memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>>\n-    [0, 0, 0] [4, 1, 8] [1, 1, 1] : tensor<4x8xf32>\n-  %1 = tt.trans %0 {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n+// CHECK-LABEL: func @push_transpose_of_extract_tile_to_memref\n+// CHECK-SAME: (%[[INPUT:.*]]: memref\n+// CHECK-SAME: , %[[OFFSET0:.*]]: index, %[[OFFSET1:.*]]: index, %[[OFFSET2:.*]]: index)\n+func.func @push_transpose_of_extract_tile_to_memref(\n+  %input: memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>>,\n+  %offset0: index, %offset1: index, %offset2: index)  ->  tensor<8x4xf32>\n+{\n+  // CHECK: %[[TRANSPOSE:.*]] = memref.transpose %[[INPUT]]\n+\n+  // CHECK-SAME: (d0, d1, d2) -> (d2, d1, d0)\n+  // CHECK-SAME: : memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>>\n+  // CHECK-SAME: to memref<16x8x4xf32, strided<[1, 64, 16]>>\n+  // CHECK: %[[EXTRACT:.*]] = xtile.extract %[[TRANSPOSE]]\n+  // CHECK-SAME: [%[[OFFSET2]], %[[OFFSET1]], %[[OFFSET0]]] [8, 1, 4] [1, 1, 1]\n+  // CHECK-SAME: : memref<16x8x4xf32, strided<[1, 64, 16]>> -> tensor<8x4xf32>\n+  %tile = xtile.extract %input[%offset0, %offset1, %offset2][4, 1, 8][1, 1, 1]\n+    : memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>> -> tensor<4x8xf32>\n+  %transposed = tt.trans %tile {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n   // CHECK: return %[[EXTRACT]] : tensor<8x4xf32>\n-  return %1 : tensor<8x4xf32>\n+  return %transposed : tensor<8x4xf32>\n }\n \n // CHECK-LABEL: func @push_transpose_up_through_broadcast"
        },
        {
            "sha": "940c5b91c0d33851d031a1063035f4816a2a7faa",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_lower_xtile.mlir",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -52,3 +52,17 @@ xtile.entry_func @scalar_insert_extract(%input: !memref_type,\n   xtile.insert %tile into %output[%tile_id][1][1] : tensor<f64> -> !memref_type\n   xtile.return\n }\n+\n+// -----\n+\n+// CHECK-LABEL: func.func @fold_transpose_into_ptr\n+// CHECK-SAME: (%[[ARG0:.*]]: memref<32x16xf64, #triton_xla.layout<[0, 1]>>)\n+func.func @fold_transpose_into_ptr(\n+    %arg0: memref<32x16xf64, #triton_xla.layout<[0, 1]>>) -> !tt.ptr<f64> {\n+  %transposed = memref.transpose %arg0 (d0, d1) -> (d1, d0)\n+    : memref<32x16xf64, #triton_xla.layout<[0, 1]>> to memref<16x32xf64>\n+  // CHECK: %[[PTR:.*]] = triton_xla.memref_to_ptr %[[ARG0]] from memref<32x16xf64, #triton_xla.layout<[0, 1]>> to <f64>\n+  %ptr = triton_xla.memref_to_ptr %transposed from memref<16x32xf64> to !tt.ptr<f64>\n+  // CHECK: return %[[PTR]] : !tt.ptr<f64>\n+  return %ptr : !tt.ptr<f64>\n+}"
        },
        {
            "sha": "40bf7793c34d409673a7918325248aa3477b7550",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_fold_transpose_pass.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 29,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -16,18 +16,18 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n-#include <optional>\n #include <type_traits>\n #include <utility>\n \n #include \"absl/algorithm/container.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n-#include \"llvm/ADT/DenseSet.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/ErrorHandling.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/OperationSupport.h\"\n@@ -38,8 +38,8 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/util.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n@@ -58,20 +58,17 @@ namespace {\n   return guard;\n }\n \n-LogicalResult FoldTransposeOfExtract(TransOp op, PatternRewriter& rewriter) {\n-  auto extract = op.getSrc().getDefiningOp<ExtractOp>();\n+// Push the transpose up through the extract tile, this will then be folded into\n+// MemrefToPtr at the lowering stage.\n+LogicalResult PushTransposeThroughExtractTile(TransOp op,\n+                                              PatternRewriter& rewriter) {\n+  auto extract = op.getSrc().getDefiningOp<::xla::xtile::ExtractTileOp>();\n   if (!extract) {\n     return rewriter.notifyMatchFailure(op, \"Transpose source is not extract.\");\n   }\n \n-  // Compute the dimensions dropped from the source.\n-  std::optional<llvm::SmallDenseSet<unsigned>> reduction_mask =\n-      computeRankReductionMask(extract.getStaticSizes(),\n-                               extract.getType().getShape());\n-  if (!reduction_mask) {\n-    return rewriter.notifyMatchFailure(op, \"Unsupported rank reduction.\");\n-  }\n-  SmallVector<unsigned> reduced_dims = to_vector(*reduction_mask);\n+  SmallVector<unsigned> reduced_dims =\n+      to_vector(extract.getReducedDimensions());\n   absl::c_sort(reduced_dims);\n \n   // Compute the set of not-reduced dimensions.\n@@ -88,8 +85,8 @@ LogicalResult FoldTransposeOfExtract(TransOp op, PatternRewriter& rewriter) {\n   }\n \n   // Compute the permutation of source dimensions.\n-  size_t src_rank = extract.getSrcShape().size();\n-  SmallVector<int32_t> permutation;\n+  size_t src_rank = extract.getSource().getType().getRank();\n+  SmallVector<int64_t> permutation;\n   permutation.reserve(src_rank);\n   for (auto [src_dim, dst_dim] :\n        llvm::zip_equal(retained_dims, op.getOrder())) {\n@@ -111,24 +108,20 @@ LogicalResult FoldTransposeOfExtract(TransOp op, PatternRewriter& rewriter) {\n     return result;\n   };\n \n-  SmallVector<int32_t> inv_permutation(permutation.size());\n-  for (auto [i, dim] : llvm::enumerate(permutation)) {\n-    inv_permutation[dim] = i;\n-  }\n+  auto permutation_map = mlir::AffineMapAttr::get(\n+      mlir::AffineMap::getPermutationMap(permutation, rewriter.getContext()));\n+  // TODO(willfroom): Return a permutation layout (b/455478641).\n+  auto pushed_transpose = mlir::memref::TransposeOp::create(\n+      rewriter, extract.getLoc(), extract.getSource(), permutation_map);\n \n-  SmallVector<int64_t> layout;\n-  layout.reserve(extract.getSrcLayout().size());\n-  for (auto dim : extract.getSrcLayout()) {\n-    layout.push_back(inv_permutation[dim]);\n-  }\n+  rewriter.replaceOpWithNewOp<::xla::xtile::ExtractTileOp>(\n+      op, op.getType(), pushed_transpose, permute(extract.getOffsets()),\n+      permute(extract.getFullTileShape()), permute(extract.getStrides()));\n \n-  rewriter.replaceOpWithNewOp<ExtractOp>(\n-      op, op.getType(), extract.getSrc(), permute(extract.getMixedOffsets()),\n-      permute(extract.getStaticSizes()), permute(extract.getStaticStrides()),\n-      permute(extract.getSrcShape()), layout);\n   if (extract->use_empty()) {\n     rewriter.eraseOp(extract);\n   }\n+\n   return success();\n }\n \n@@ -332,7 +325,7 @@ class TritonXLAFoldTransposePass\n  private:\n   void runOnOperation() override {\n     RewritePatternSet patterns(&getContext());\n-    patterns.add(FoldTransposeOfExtract);\n+    patterns.add(PushTransposeThroughExtractTile);\n     patterns.add(PushTransposeUpIntoIf);\n     patterns.add(HoistTransposeUpFromIf, /*benefit=*/2);\n     patterns.add(PushTransposeUpThroughBroadcast);"
        },
        {
            "sha": "471942ad4e3e6a3ac61d132ecdb96c357a558cf5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_xtile_pass.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 9,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -136,14 +136,14 @@ MemrefToPtrOp CreateMemrefToPtr(mlir::OpBuilder& builder,\n class XTileEntryToTriton\n     : public mlir::OpRewritePattern<::xla::xtile::EntryFuncOp> {\n  public:\n-  XTileEntryToTriton(mlir::MLIRContext* context, mlir::ModuleOp& module)\n-      : OpRewritePattern(context), module_(module) {}\n+  using OpRewritePattern::OpRewritePattern;\n \n   mlir::LogicalResult matchAndRewrite(\n       ::xla::xtile::EntryFuncOp entry_op,\n       mlir::PatternRewriter& rewriter) const override {\n-    mlir::ImplicitLocOpBuilder builder(module_->getLoc(), module_);\n-    builder.setInsertionPointToStart(module_.getBody());\n+    mlir::ModuleOp module = entry_op->getParentOfType<mlir::ModuleOp>();\n+    mlir::ImplicitLocOpBuilder builder(module->getLoc(), module);\n+    builder.setInsertionPointToStart(module.getBody());\n \n     auto new_arg_types = GetPtrArgTypes(entry_op.getBufferArgs());\n     auto new_func_op = builder.create<mlir::func::FuncOp>(\n@@ -194,9 +194,6 @@ class XTileEntryToTriton\n     rewriter.eraseOp(entry_op);\n     return success();\n   }\n-\n- private:\n-  mlir::ModuleOp& module_;\n };\n \n // Rewrite a xtile extract to a triton_xla extract.\n@@ -283,6 +280,25 @@ class XTileInsertToTriton\n   }\n };\n \n+class FoldIntoMemrefToPtr : public mlir::OpRewritePattern<MemrefToPtrOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      MemrefToPtrOp op, mlir::PatternRewriter& rewriter) const override {\n+    // As a transpose doesn't add any offset we can simply fold it into the\n+    // memref_to_ptr.\n+    auto transpose = op.getSrc().getDefiningOp<mlir::memref::TransposeOp>();\n+    if (!transpose) {\n+      return mlir::failure();\n+    }\n+\n+    rewriter.replaceOpWithNewOp<MemrefToPtrOp>(op, op.getType(),\n+                                               transpose.getIn());\n+    return mlir::success();\n+  }\n+};\n+\n class TritonXLALowerXTilePass\n     : public impl::TritonXLALowerXTilePassBase<TritonXLALowerXTilePass> {\n  public:\n@@ -294,8 +310,8 @@ class TritonXLALowerXTilePass\n \n     mlir::RewritePatternSet patterns(context);\n \n-    patterns.add<XTileEntryToTriton>(context, module);\n-    patterns.add<XTileExtractToTriton, XTileInsertToTriton>(context);\n+    patterns.add<XTileEntryToTriton, XTileExtractToTriton, XTileInsertToTriton,\n+                 FoldIntoMemrefToPtr>(context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(module, std::move(patterns)))) {\n       signalPassFailure();"
        },
        {
            "sha": "345f7c2f9edf30719eb82fbeb504158fe1893a20",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -687,6 +687,7 @@ lit_test_suite_for_gpus(\n #         \"@llvm-project//mlir:FuncExtensions\",\n #         \"@llvm-project//mlir:LLVMIRTransforms\",\n #         \"@llvm-project//mlir:LLVMToLLVMIRTranslation\",\n+#         \"@llvm-project//mlir:MemRefDialect\",\n #         \"@llvm-project//mlir:MlirOptLib\",\n #         \"@llvm-project//mlir:Pass\",\n #         \"@llvm-project//mlir:RegisterAllExtensions\",  # buildcleaner: keep"
        },
        {
            "sha": "f03459ec5e891d703c5869a0847a185cd082d096",
            "filename": "third_party/xla/xla/service/gpu/tests/xla-opt.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc?ref=4d623afca2ed1c323a1ce2afb1c2834cd3e8cd21",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include \"mlir/Dialect/Func/Extensions/InlinerExtension.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/LLVMIR/Transforms/InlinerInterfaceImpl.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/InitAllExtensions.h\"\n #include \"mlir/Pass/PassOptions.h\"\n@@ -87,10 +88,10 @@ int main(int argc, char** argv) {\n   mlir::LLVM::registerInlinerInterface(registry);\n   mlir::func::registerInlinerExtension(registry);\n   registerTritonDialects(registry);  // This registers all passes as well.\n-  registry\n-      .insert<mlir::func::FuncDialect, mlir::tensor::TensorDialect,\n-              mlir::triton::xla::XlaTritonDialect, xla::XlaDialect,\n-              xla::xtile::XTileDialect, mlir::stablehlo::StablehloDialect>();\n+  registry.insert<mlir::func::FuncDialect, mlir::tensor::TensorDialect,\n+                  mlir::triton::xla::XlaTritonDialect, xla::XlaDialect,\n+                  xla::xtile::XTileDialect, mlir::stablehlo::StablehloDialect,\n+                  mlir::memref::MemRefDialect>();\n   mlir::triton::xla::registerTritonXlaTransformsPasses();\n   xla::emitters::registerTransformsPasses();\n   xla::gpu::registerGpuFusionTransformsPasses();"
        }
    ],
    "stats": {
        "total": 143,
        "additions": 89,
        "deletions": 54
    }
}