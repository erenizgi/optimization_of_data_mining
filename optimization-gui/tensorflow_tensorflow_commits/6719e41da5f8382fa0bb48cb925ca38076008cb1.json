{
    "author": "EusebioDM",
    "message": "Add function to convert `ConvolutionKind` proto to the c++ enum\n\nFor the `ConvolutionThunk` (de)serialisation we need to make the `GpuConvDescriptor` serializable, and for that we need `CudnnConvKind` too.\n\nA couple additional changes:\n\n* Renamed the existing c++ enum to proto enum to a (hopefully) more readable name.\n* Enforce that all c++ enums can be mapped to the proto version at compile time. I can't think of a case where we wouldn't want this, and with this change we can get rid of some non-ok Status invariants.\n\nPiperOrigin-RevId: 817676211",
    "sha": "6719e41da5f8382fa0bb48cb925ca38076008cb1",
    "files": [
        {
            "sha": "fe9f6fa3b564973bc0378679b84a874c444b3ea0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -267,8 +267,8 @@ absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n GetConvolutionCustomCallConfigs(const HloCustomCallInstruction* instr,\n                                 se::StreamExecutor* stream_executor) {\n   TF_ASSIGN_OR_RETURN(GpuConvConfig gpu_conv_config, GetGpuConvConfig(instr));\n-  TF_ASSIGN_OR_RETURN(se::dnn::ConvolutionKind conv_kind,\n-                      GetDNNConvKindFromCudnnConvKind(gpu_conv_config.kind));\n+  se::dnn::ConvolutionKind conv_kind =\n+      CudnnConvKindToProto(gpu_conv_config.kind);\n   TF_ASSIGN_OR_RETURN(\n       se::dnn::DataType input_type,\n       GetDNNDataTypeFromPrimitiveType(gpu_conv_config.input_type));"
        },
        {
            "sha": "e935daceb612ad5bd632f72c63bafe0b03d34043",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -116,9 +116,6 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n         GpuConvParams conv_params,\n         GetGpuConvParams(config_, operand_se_buffers, result_se_buffers));\n \n-    TF_ASSIGN_OR_RETURN(se::dnn::ConvolutionKind kind,\n-                        GetDNNConvKindFromCudnnConvKind(config_.kind));\n-\n     TF_ASSIGN_OR_RETURN(se::dnn::DataType input_type,\n                         GetDNNDataTypeFromPrimitiveType(config_.input_type));\n \n@@ -133,11 +130,11 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n     std::vector<se::dnn::ProfileResult> profile_results;\n     dnn->GetMIOpenConvolveAlgorithms(\n-        kind, input_type, output_type, params.stream, config_.input_descriptor,\n-        conv_params.input_buf, config_.filter_descriptor,\n-        conv_params.filter_buf, config_.output_descriptor,\n-        conv_params.output_buf, config_.conv_desc, &scratch_allocator,\n-        &profile_results);\n+        CudnnConvKindToProto(config_.kind), input_type, output_type,\n+        params.stream, config_.input_descriptor, conv_params.input_buf,\n+        config_.filter_descriptor, conv_params.filter_buf,\n+        config_.output_descriptor, conv_params.output_buf, config_.conv_desc,\n+        &scratch_allocator, &profile_results);\n   }\n \n   TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),"
        },
        {
            "sha": "315584c4d653b25837877aab680cac73ba0f1ce2",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -2602,10 +2602,13 @@ xla_cc_test(\n     name = \"stream_executor_util_test\",\n     srcs = [\"stream_executor_util_test.cc\"],\n     deps = [\n+        \":cublas_cudnn\",\n         \":stream_executor_util\",\n         \"//xla:autotuning_proto_cc\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/tsl/util/proto:proto_utils\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/time\",\n         \"@com_google_googletest//:gtest_main\","
        },
        {
            "sha": "dbe4817536b3d080c889e6ca1f7b345f6a732b78",
            "filename": "third_party/xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -156,9 +156,6 @@ absl::StatusOr<se::DeviceMemory<uint8_t>> ScratchAllocator::AllocateBytes(\n absl::StatusOr<std::vector<GenericConvRunner>> GetAlgorithms(\n     const GpuConvConfig& config, se::Stream* stream, bool use_fallback,\n     const se::NumericOptions& numeric_options) {\n-  TF_ASSIGN_OR_RETURN(se::dnn::ConvolutionKind kind,\n-                      GetDNNConvKindFromCudnnConvKind(config.kind));\n-\n   TF_ASSIGN_OR_RETURN(se::dnn::DataType input_type,\n                       GetDNNDataTypeFromPrimitiveType(config.input_type));\n \n@@ -172,6 +169,7 @@ absl::StatusOr<std::vector<GenericConvRunner>> GetAlgorithms(\n   if (dnn == nullptr) {\n     return absl::InvalidArgumentError(\"No DNN in stream executor.\");\n   }\n+  se::dnn::ConvolutionKind kind = CudnnConvKindToProto(config.kind);\n   switch (kind) {\n     default:\n       return Internal(\"Unknown ConvolutionKind %d\", kind);\n@@ -258,9 +256,6 @@ GetMIOpenAlgorithms(const HloCustomCallInstruction* instr,\n                     const se::NumericOptions& numeric_options) {\n   TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(instr));\n \n-  TF_ASSIGN_OR_RETURN(se::dnn::ConvolutionKind kind,\n-                      GetDNNConvKindFromCudnnConvKind(config.kind));\n-\n   TF_ASSIGN_OR_RETURN(se::dnn::DataType dtype,\n                       GetDNNDataTypeFromPrimitiveType(config.output_type));\n \n@@ -274,8 +269,9 @@ GetMIOpenAlgorithms(const HloCustomCallInstruction* instr,\n     return absl::InvalidArgumentError(\"No DNN in stream executor.\");\n   }\n   TF_RETURN_IF_ERROR(dnn->GetConvolveRunners(\n-      kind, dtype, dtype, stream, params.config->input_descriptor,\n-      params.input_buf, params.config->filter_descriptor, params.filter_buf,\n+      CudnnConvKindToProto(config.kind), dtype, dtype, stream,\n+      params.config->input_descriptor, params.input_buf,\n+      params.config->filter_descriptor, params.filter_buf,\n       params.config->output_descriptor, params.output_buf,\n       params.config->conv_desc,\n       /* use_fallback = */ false, scratch_allocator, numeric_options,"
        },
        {
            "sha": "282e27805332aba3f877fa9a132aed67e04b7a1b",
            "filename": "third_party/xla/xla/service/gpu/gpu_conv_runner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_conv_runner.cc?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -68,9 +68,6 @@ absl::Status RunGpuConvUnfused(const GpuConvParams& params, se::Stream* stream,\n                     params.config->conv_result_scale);\n   }\n \n-  TF_ASSIGN_OR_RETURN(se::dnn::ConvolutionKind kind,\n-                      GetDNNConvKindFromCudnnConvKind(params.config->kind));\n-\n   TF_ASSIGN_OR_RETURN(\n       se::dnn::DataType input_type,\n       GetDNNDataTypeFromPrimitiveType(params.config->input_type));\n@@ -87,7 +84,7 @@ absl::Status RunGpuConvUnfused(const GpuConvParams& params, se::Stream* stream,\n     lazy_runner = &*local_runner;\n   }\n \n-  se::dnn::ConvOp::Config config{kind,\n+  se::dnn::ConvOp::Config config{CudnnConvKindToProto(params.config->kind),\n                                  input_type,\n                                  output_type,\n                                  params.config->input_descriptor,\n@@ -113,9 +110,6 @@ absl::Status RunGpuConvGraph(const GpuConvParams& params, se::Stream* stream,\n                     params.config->conv_result_scale);\n   }\n \n-  TF_ASSIGN_OR_RETURN(se::dnn::ConvolutionKind kind,\n-                      GetDNNConvKindFromCudnnConvKind(params.config->kind));\n-\n   TF_ASSIGN_OR_RETURN(\n       se::dnn::DataType input_type,\n       GetDNNDataTypeFromPrimitiveType(params.config->input_type));\n@@ -132,7 +126,7 @@ absl::Status RunGpuConvGraph(const GpuConvParams& params, se::Stream* stream,\n     lazy_runner = &*local_runner;\n   }\n \n-  se::dnn::GraphConvOp::Config config{kind,\n+  se::dnn::GraphConvOp::Config config{CudnnConvKindToProto(params.config->kind),\n                                       input_type,\n                                       output_type,\n                                       params.config->input_descriptor,"
        },
        {
            "sha": "64164d527e64572f911138e156fa41f56c0cb0e9",
            "filename": "third_party/xla/xla/service/gpu/stream_executor_util.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -541,8 +541,7 @@ void InitializeBuffer(se::Stream* stream, PrimitiveType buffer_type,\n       buffer_type);\n }\n \n-absl::StatusOr<se::dnn::ConvolutionKind> GetDNNConvKindFromCudnnConvKind(\n-    CudnnConvKind kind) {\n+se::dnn::ConvolutionKind CudnnConvKindToProto(CudnnConvKind kind) {\n   switch (kind) {\n     case CudnnConvKind::kBackwardFilter:\n       return se::dnn::BACKWARD_FILTER;\n@@ -554,6 +553,23 @@ absl::StatusOr<se::dnn::ConvolutionKind> GetDNNConvKindFromCudnnConvKind(\n       return se::dnn::FORWARD_BIAS_ACTIVATION;\n     case CudnnConvKind::kForwardGraph:\n       return se::dnn::FORWARD_GRAPH;\n+      // No default case to ensure that all cases are handled at compile time.\n+  }\n+}\n+\n+absl::StatusOr<CudnnConvKind> CudnnConvKindFromProto(\n+    se::dnn::ConvolutionKind kind) {\n+  switch (kind) {\n+    case se::dnn::BACKWARD_FILTER:\n+      return CudnnConvKind::kBackwardFilter;\n+    case se::dnn::BACKWARD_DATA:\n+      return CudnnConvKind::kBackwardInput;\n+    case se::dnn::FORWARD:\n+      return CudnnConvKind::kForward;\n+    case se::dnn::FORWARD_BIAS_ACTIVATION:\n+      return CudnnConvKind::kForwardActivation;\n+    case se::dnn::FORWARD_GRAPH:\n+      return CudnnConvKind::kForwardGraph;\n     default:\n       break;\n   }"
        },
        {
            "sha": "1113d3bb5b5bc188adaff66f1aaf77d4fa6e82b0",
            "filename": "third_party/xla/xla/service/gpu/stream_executor_util.h",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.h?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -126,8 +126,13 @@ absl::Status ExecuteKernelOnStream(\n void InitializeBuffer(se::Stream* stream, PrimitiveType buffer_type,\n                       int64_t* rng_state, se::DeviceMemoryBase buffer);\n \n-absl::StatusOr<se::dnn::ConvolutionKind> GetDNNConvKindFromCudnnConvKind(\n-    CudnnConvKind kind);\n+// Converts the C++ enum `CudnnConvKind`, to the proto enum version\n+// `ConvolutionKind`.\n+se::dnn::ConvolutionKind CudnnConvKindToProto(CudnnConvKind kind);\n+\n+// Converts the proto enum `ConvolutionKind`, to the C++ enum `CudnnConvKind`.\n+absl::StatusOr<CudnnConvKind> CudnnConvKindFromProto(\n+    se::dnn::ConvolutionKind kind);\n \n absl::StatusOr<se::dnn::NormKind> GetDNNNormKindFromCudnnNormKind(\n     CudnnNormKind kind);"
        },
        {
            "sha": "34cd3b7067eef48adc06abb9afeeaec350866ded",
            "filename": "third_party/xla/xla/service/gpu/stream_executor_util_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6719e41da5f8382fa0bb48cb925ca38076008cb1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util_test.cc?ref=6719e41da5f8382fa0bb48cb925ca38076008cb1",
            "patch": "@@ -18,15 +18,22 @@ limitations under the License.\n #include <cstdint>\n #include <vector>\n \n+#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/time/time.h\"\n #include \"xla/autotuning.pb.h\"\n+#include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/tsl/util/proto/proto_utils.h\"\n \n namespace xla::gpu {\n namespace {\n+using ::absl_testing::IsOkAndHolds;\n+using ::absl_testing::StatusIs;\n+using ::stream_executor::dnn::ConvolutionKind;\n \n struct Result {\n   int64_t run_time_ns;\n@@ -74,6 +81,35 @@ TEST(StreamExecutorTest, PickBestResult) {\n   EXPECT_EQ(ATRToResult(atr.value()), Result({6000, 0}));\n }\n \n+TEST(StreamExecutorUtilTest, CudnnConvKindToProto) {\n+  EXPECT_EQ(CudnnConvKindToProto(CudnnConvKind::kBackwardFilter),\n+            ConvolutionKind::BACKWARD_FILTER);\n+  EXPECT_EQ(CudnnConvKindToProto(CudnnConvKind::kBackwardInput),\n+            ConvolutionKind::BACKWARD_DATA);\n+  EXPECT_EQ(CudnnConvKindToProto(CudnnConvKind::kForward),\n+            ConvolutionKind::FORWARD);\n+  EXPECT_EQ(CudnnConvKindToProto(CudnnConvKind::kForwardActivation),\n+            ConvolutionKind::FORWARD_BIAS_ACTIVATION);\n+  EXPECT_EQ(CudnnConvKindToProto(CudnnConvKind::kForwardGraph),\n+            ConvolutionKind::FORWARD_GRAPH);\n+}\n+\n+TEST(StreamExecutorUtilTest, CudnnConvKindFromProto) {\n+  EXPECT_THAT(CudnnConvKindFromProto(ConvolutionKind::BACKWARD_FILTER),\n+              IsOkAndHolds(CudnnConvKind::kBackwardFilter));\n+  EXPECT_THAT(CudnnConvKindFromProto(ConvolutionKind::BACKWARD_DATA),\n+              IsOkAndHolds(CudnnConvKind::kBackwardInput));\n+  EXPECT_THAT(CudnnConvKindFromProto(ConvolutionKind::FORWARD),\n+              IsOkAndHolds(CudnnConvKind::kForward));\n+  EXPECT_THAT(CudnnConvKindFromProto(ConvolutionKind::FORWARD_BIAS_ACTIVATION),\n+              IsOkAndHolds(CudnnConvKind::kForwardActivation));\n+  EXPECT_THAT(CudnnConvKindFromProto(ConvolutionKind::FORWARD_GRAPH),\n+              IsOkAndHolds(CudnnConvKind::kForwardGraph));\n+\n+  EXPECT_THAT(CudnnConvKindFromProto(ConvolutionKind::INVALID),\n+              StatusIs(absl::StatusCode::kInternal));\n+}\n+\n }  // namespace\n \n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 107,
        "additions": 77,
        "deletions": 30
    }
}