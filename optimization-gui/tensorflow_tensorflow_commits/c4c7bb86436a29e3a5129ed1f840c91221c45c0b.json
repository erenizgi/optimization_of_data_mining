{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 809679240",
    "sha": "c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
    "files": [
        {
            "sha": "bf16a1eee71399704362aa57f665bd7507adb735",
            "filename": "third_party/xla/xla/stream_executor/cuda/caching_compilation_provider.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcaching_compilation_provider.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcaching_compilation_provider.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcaching_compilation_provider.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -50,7 +50,7 @@ absl::StatusOr<Assembly> CachingCompilationProvider::Compile(\n     const CompilationOptions& options) const {\n   CacheKey cache_key{cc, std::string{ptx}, options};\n   {\n-    absl::MutexLock lock(&assembly_cache_mutex_);\n+    absl::MutexLock lock(assembly_cache_mutex_);\n     auto it = assembly_cache_.find(cache_key);\n     if (it != assembly_cache_.end()) {\n       // The iterator will get invalid during the `Await` call if the cache is\n@@ -71,7 +71,7 @@ absl::StatusOr<Assembly> CachingCompilationProvider::Compile(\n \n   absl::StatusOr<Assembly> assembly = delegate_->Compile(cc, ptx, options);\n   {\n-    absl::MutexLock lock(&assembly_cache_mutex_);\n+    absl::MutexLock lock(assembly_cache_mutex_);\n     assembly_cache_[cache_key] = assembly;\n   }\n   return assembly;\n@@ -83,7 +83,7 @@ CachingCompilationProvider::CompileToRelocatableModule(\n     const CompilationOptions& options) const {\n   CacheKey cache_key{cc, std::string{ptx}, options};\n   {\n-    absl::MutexLock lock(&relocatable_module_cache_mutex_);\n+    absl::MutexLock lock(relocatable_module_cache_mutex_);\n     auto it = relocatable_module_cache_.find(cache_key);\n     if (it != relocatable_module_cache_.end()) {\n       // The iterator will get invalid during the `Await` call if the cache is\n@@ -104,7 +104,7 @@ CachingCompilationProvider::CompileToRelocatableModule(\n   absl::StatusOr<RelocatableModule> relocatable_module =\n       delegate_->CompileToRelocatableModule(cc, ptx, options);\n   {\n-    absl::MutexLock lock(&relocatable_module_cache_mutex_);\n+    absl::MutexLock lock(relocatable_module_cache_mutex_);\n     relocatable_module_cache_[cache_key] = relocatable_module;\n   }\n   return relocatable_module;"
        },
        {
            "sha": "ae56303c25a8e16d513ff56fbdc800490e7f958c",
            "filename": "third_party/xla/xla/stream_executor/cuda/caching_compilation_provider_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcaching_compilation_provider_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcaching_compilation_provider_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcaching_compilation_provider_test.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -336,7 +336,7 @@ TEST(CachingCompilationProviderTest, CompilationInterlockWorks) {\n \n   EXPECT_CALL(*mock_compilation_provider, Compile)\n       .WillOnce([&]() {\n-        absl::MutexLock lock(&mutex);\n+        absl::MutexLock lock(mutex);\n         compilation_started = true;\n         mutex.Await(absl::Condition(&compilation_supposed_to_be_done));\n         return kAssembly;\n@@ -357,7 +357,7 @@ TEST(CachingCompilationProviderTest, CompilationInterlockWorks) {\n     {\n       // We wait for the other compilation to start, so that the cache is in\n       // pending state.\n-      absl::MutexLock lock(&mutex);\n+      absl::MutexLock lock(mutex);\n       mutex.Await(absl::Condition(&compilation_started));\n     }\n     // This call makes sure we mutate the cache while the other compilation is\n@@ -366,7 +366,7 @@ TEST(CachingCompilationProviderTest, CompilationInterlockWorks) {\n                     CudaComputeCapability{10, 0}, \"ptx2\", CompilationOptions()),\n                 absl_testing::IsOkAndHolds(kAssembly));\n     // Then we let the other compilation finish\n-    absl::MutexLock lock(&mutex);\n+    absl::MutexLock lock(mutex);\n     compilation_supposed_to_be_done = true;\n   });\n }\n@@ -382,7 +382,7 @@ TEST(CachingCompilationProviderTest,\n \n   EXPECT_CALL(*mock_compilation_provider, CompileToRelocatableModule)\n       .WillOnce([&]() {\n-        absl::MutexLock lock(&mutex);\n+        absl::MutexLock lock(mutex);\n         compilation_started = true;\n         mutex.Await(absl::Condition(&compilation_supposed_to_be_done));\n         return kModule;\n@@ -403,7 +403,7 @@ TEST(CachingCompilationProviderTest,\n     {\n       // We wait for the other compilation to start, so that the cache is in\n       // pending state.\n-      absl::MutexLock lock(&mutex);\n+      absl::MutexLock lock(mutex);\n       mutex.Await(absl::Condition(&compilation_started));\n     }\n     // This call makes sure we mutate the cache while the other compilation is\n@@ -412,7 +412,7 @@ TEST(CachingCompilationProviderTest,\n                     CudaComputeCapability{10, 0}, \"ptx2\", CompilationOptions()),\n                 absl_testing::IsOkAndHolds(kModule));\n     // Then we let the other compilation finish\n-    absl::MutexLock lock(&mutex);\n+    absl::MutexLock lock(mutex);\n     compilation_supposed_to_be_done = true;\n   });\n }"
        },
        {
            "sha": "986a2e6dedba082565ec186d4a107fb05bc02c31",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_asm_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_asm_compiler.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -77,7 +77,7 @@ absl::StatusOr<absl::Span<const uint8_t>> CompileGpuAsmOrGetCached(\n   static auto& ptx_cache ABSL_GUARDED_BY(ptx_cache_mutex) =\n       *new absl::flat_hash_map<PtxCacheKey, PtxCompilerResult>();\n \n-  absl::MutexLock lock(&ptx_cache_mutex);\n+  absl::MutexLock lock(ptx_cache_mutex);\n   PtxCacheKey cache_key{cc, ptx, compilation_options.ToTuple()};\n   auto it = ptx_cache.find(cache_key);\n   if (it == ptx_cache.end()) {"
        },
        {
            "sha": "1635641d202296884d7d5fadb0774557b6336e9f",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_blas.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -188,7 +188,7 @@ static const char *const kCublasNotInitializedExplanation =\n     \"not built with support for the GPU in your machine.\";\n \n bool CUDABlas::Init() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n \n   std::unique_ptr<ActivateContext> activation = parent_->Activate();\n   cublasStatus_t ret = cublasCreate(&blas_);\n@@ -244,7 +244,7 @@ bool CUDABlas::SetStream(Stream *stream) {\n }\n \n absl::StatusOr<bool> CUDABlas::IsMainStreamSet() const {\n-  absl::MutexLock lock{&mu_};\n+  absl::MutexLock lock{mu_};\n   CHECK(blas_ != nullptr);\n   CUstream handle{};\n   if (auto ret = cublasGetStream(blas_, &handle);\n@@ -368,7 +368,7 @@ absl::Status CUDABlas::DoBlasInternalImpl(FuncT cublas_func, Stream *stream,\n                                           bool pointer_mode_host,\n                                           cublasMath_t math_type,\n                                           Args... args) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n \n   CHECK(blas_ != nullptr);\n   if (!SetStream(stream)) {\n@@ -1383,7 +1383,7 @@ bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n }\n \n absl::Status CUDABlas::GetVersion(std::string *version) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n \n   int v;\n   auto status = cublasGetVersion(blas_, &v);"
        },
        {
            "sha": "8187babc600a18f5868d7ad8f67a996085f0cbd7",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_blas_lt.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas_lt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas_lt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas_lt.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -152,7 +152,7 @@ absl::StatusOr<cublasLtEpilogue_t> AsCublasLtEpilogue(\n absl::Status BlasLt::Init() {\n   cublasLtHandle_t blas_lt;\n   SE_CUBLAS_RETURN_IF_ERROR(cublasLtCreate(&blas_lt));\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   blas_lt_.reset(blas_lt);\n   return absl::OkStatus();\n }\n@@ -247,7 +247,7 @@ auto BlasLt::MatmulPlan::GetAlgorithms(const Stream* stream,\n   std::vector<cublasLtMatmulHeuristicResult_t> results(max_algorithm_count);\n   {\n     auto blas_lt = static_cast<BlasLt*>(gpu::BlasLt::Get(stream));\n-    absl::MutexLock lock(&blas_lt->mu_);\n+    absl::MutexLock lock(blas_lt->mu_);\n     TF_RET_CHECK(blas_lt->blas_lt_ != nullptr);\n \n     cublasLtMatmulPreference_t cu_preference;\n@@ -388,7 +388,7 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n \n   auto palgo = std::any_cast<cublasLtMatmulAlgo_t>(&algorithm_->opaque_algo);\n   {\n-    absl::MutexLock lock(&blas_lt->mu_);\n+    absl::MutexLock lock(blas_lt->mu_);\n     TF_RET_CHECK(blas_lt->blas_lt_ != nullptr);\n     // We must set the bias and aux pointers while holding the mutex, to avoid a\n     // potential race condition from multiple threads sharing the same plan."
        },
        {
            "sha": "526f6b4cc1fab6664962782c32c68b946cd5c2e2",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -263,7 +263,7 @@ class CudnnAccess {\n   }\n \n   ~CudnnAccess() {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     cudnnDestroy(handle_);\n \n     if (compilation_handle_) {\n@@ -311,7 +311,7 @@ class CudnnAccess {\n   void NotifyStreamDestroyed(Stream* stream) {\n     CUstream cu_stream =\n         absl::bit_cast<CUstream>(stream->platform_specific_handle().stream);\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     if (current_stream_ && cu_stream == *current_stream_) {\n       current_stream_.reset();\n     }"
        },
        {
            "sha": "cdee6c762958cc684762b7fbef9581ad6617e905",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -821,7 +821,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n   const std::string& kernel_name = spec.kernel_name();\n \n   if (spec.has_cuda_cubin_in_memory()) {\n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     const char* cubin = reinterpret_cast<const char*>(\n         spec.cuda_cubin_in_memory()->cubin_bytes.data());\n     TF_ASSIGN_OR_RETURN(ModuleHandle module_handle, LoadModuleFromCuBin(cubin));\n@@ -842,7 +842,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n                  << \"] Loader spec has no ptx for kernel \" << kernel_name;\n     }\n \n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     TF_ASSIGN_OR_RETURN(ModuleHandle module_handle, LoadModuleFromPtx(ptx));\n     kernel_to_gpu_binary_[cuda_kernel.get()] = module_handle;\n \n@@ -874,7 +874,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n \n   {\n     // Keep track of loaded kernels.\n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     loaded_kernels_.insert(cuda_kernel.get());\n   }\n \n@@ -928,7 +928,7 @@ void CudaExecutor::UnloadKernel(const Kernel* kernel) {\n   VLOG(3) << \"[\" << device_ordinal() << \"] Unloading kernel \" << kernel << \" : \"\n           << kernel->name();\n \n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   loaded_kernels_.erase(kernel);\n \n   auto gpu_binary_it = kernel_to_gpu_binary_.find(kernel);\n@@ -950,22 +950,22 @@ absl::StatusOr<ModuleHandle> CudaExecutor::LoadModule(\n   // We store the pointer to the GPU binary (PTX or CUBIN) as\n   // ModuleHandle::id().\n   if (spec.has_cuda_cubin_in_memory()) {\n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     return LoadModuleFromCuBin(\n         reinterpret_cast<const char*>(spec.cuda_cubin_in_memory().data()));\n   } else if (spec.has_cuda_ptx_in_memory()) {\n     if (!spec.cuda_ptx_in_memory()) {\n       return absl::InternalError(\"PTX not found in spec\");\n     }\n \n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     return LoadModuleFromPtx(spec.cuda_ptx_in_memory());\n   }\n   return absl::InternalError(\"No method of loading CUDA module provided\");\n }\n \n bool CudaExecutor::UnloadModule(ModuleHandle module_handle) {\n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   return UnloadGpuBinary(module_handle);\n }\n \n@@ -993,7 +993,7 @@ int fpus_per_core(int cc_major, int cc_minor) {\n absl::StatusOr<std::shared_ptr<DeviceMemoryBase>>\n CudaExecutor::CreateOrShareConstant(Stream* stream,\n                                     absl::Span<const uint8_t> content) {\n-  absl::MutexLock lock{&shared_constants_mu_};\n+  absl::MutexLock lock{shared_constants_mu_};\n   // We assume all constants are uniquely identified by this hash. In the\n   // (highly unlikely) event of a hash collision, the program will likely crash\n   // (because the cached constant that will be returned by mistake is unlikely\n@@ -1163,17 +1163,17 @@ absl::Status CudaExecutor::SynchronousMemcpy(void* host_dst,\n \n void CudaExecutor::DeallocateStream(Stream* stream) {\n   {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     if (dnn_ != nullptr) {\n       dnn_->NotifyStreamDestroyed(stream);\n     }\n   }\n-  absl::MutexLock l(&alive_gpu_streams_mu_);\n+  absl::MutexLock l(alive_gpu_streams_mu_);\n   alive_gpu_streams_.erase(stream->platform_specific_handle().stream);\n }\n \n blas::BlasSupport* CudaExecutor::AsBlas() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (blas_ != nullptr) {\n     return blas_.get();\n   }\n@@ -1193,7 +1193,7 @@ blas::BlasSupport* CudaExecutor::AsBlas() {\n }\n \n dnn::DnnSupport* CudaExecutor::AsDnn() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (dnn_ != nullptr) {\n     return dnn_.get();\n   }\n@@ -1214,7 +1214,7 @@ dnn::DnnSupport* CudaExecutor::AsDnn() {\n }\n \n fft::FftSupport* CudaExecutor::AsFft() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (fft_ != nullptr) {\n     return fft_.get();\n   }\n@@ -1266,7 +1266,7 @@ absl::StatusOr<DeviceMemoryBase> CudaExecutor::GetSymbol(\n   CHECK(static_cast<bool>(module_handle));\n \n   {  // give limited scope to MutexLock\n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     auto it = gpu_binary_to_module_.find(module_handle);\n     CHECK(it != gpu_binary_to_module_.end());\n \n@@ -1307,7 +1307,7 @@ absl::StatusOr<std::unique_ptr<Event>> CudaExecutor::CreateEvent() {\n absl::StatusOr<std::unique_ptr<Stream>> CudaExecutor::CreateStream(\n     std::optional<std::variant<StreamPriority, int>> priority) {\n   TF_ASSIGN_OR_RETURN(auto stream, CudaStream::Create(this, priority));\n-  absl::MutexLock l(&alive_gpu_streams_mu_);\n+  absl::MutexLock l(alive_gpu_streams_mu_);\n   alive_gpu_streams_[stream->stream_handle()] = stream.get();\n   return std::move(stream);\n }\n@@ -1504,7 +1504,7 @@ absl::StatusOr<MemoryType> CudaExecutor::GetPointerMemorySpace(\n \n absl::StatusOr<const CudaKernel*> CudaExecutor::GetCudaKernel(\n     const Kernel* kernel) {\n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   auto it = loaded_kernels_.find(kernel);\n   if (it == loaded_kernels_.end()) {\n     return absl::NotFoundError(\"Kernel not loaded in this executor.\");"
        },
        {
            "sha": "aacf74a11660307787cf9a168cd0c2b5ea97c39d",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -114,7 +114,7 @@ class CudaExecutor : public GpuExecutor {\n   absl::StatusOr<MemoryType> GetPointerMemorySpace(const void* ptr) override;\n \n   Stream* FindAllocatedStream(void* gpu_stream) override {\n-    absl::MutexLock lock(&alive_gpu_streams_mu_);\n+    absl::MutexLock lock(alive_gpu_streams_mu_);\n     auto it = alive_gpu_streams_.find(gpu_stream);\n     if (it == alive_gpu_streams_.end()) {\n       return nullptr;"
        },
        {
            "sha": "edfaca09491a53b47337918501bb920b0da81da7",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_stream.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -260,7 +260,7 @@ CudaStream::~CudaStream() {\n \n absl::Status CudaStream::BlockHostUntilDone() {\n   TF_RETURN_IF_ERROR(SynchronizeStream(executor_, stream_handle_));\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   mutex_.Await(absl::Condition(&no_pending_host_callbacks_));\n   return absl::OkStatus();\n }\n@@ -339,7 +339,7 @@ absl::Status CudaStream::DoHostCallbackWithStatus(\n         // callback gets executed before we increase the counter on the main\n         // thread.\n         if (num_pending_host_callbacks == 0) {\n-          absl::MutexLock lock(&mutex_);\n+          absl::MutexLock lock(mutex_);\n           no_pending_host_callbacks_ = num_pending_host_callbacks_ <= 0;\n         }\n       });\n@@ -350,7 +350,7 @@ absl::Status CudaStream::DoHostCallbackWithStatus(\n   if (num_pending_host_callbacks == 1) {\n     // num_pending_host_callbacks == 1 means we had no pending host callbacks\n     // before this one.\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     no_pending_host_callbacks_ = num_pending_host_callbacks_ <= 0;\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "7209e565dd9d3ed27c3cf53a64f230e08da82700",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_stream_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream_test.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -282,26 +282,26 @@ TEST_F(CudaStreamTest, WaitForOtherStream) {\n   // - stream2 waits for stream1 to be done.\n   // - Afterwards stream2 invokes the host callback.\n   EXPECT_THAT(stream1->DoHostCallback([&]() {\n-    absl::MutexLock lock(&mutex);\n+    absl::MutexLock lock(mutex);\n     execution_order.push_back(ExecutionStage::kBeforeWaitForEvent);\n   }),\n               absl_testing::IsOk());\n   EXPECT_THAT(stream1->WaitFor(&event), absl_testing::IsOk());\n   EXPECT_THAT(stream1->DoHostCallback([&]() {\n-    absl::MutexLock lock(&mutex);\n+    absl::MutexLock lock(mutex);\n     execution_order.push_back(ExecutionStage::kAfterWaitForEvent);\n   }),\n               absl_testing::IsOk());\n   EXPECT_THAT(stream2->WaitFor(stream1.get()), absl_testing::IsOk());\n   EXPECT_THAT(stream2->DoHostCallback([&]() {\n-    absl::MutexLock lock(&mutex);\n+    absl::MutexLock lock(mutex);\n     execution_order.push_back(ExecutionStage::kAfterWaitForStream);\n   }),\n               absl_testing::IsOk());\n \n   EXPECT_THAT(stream1->RecordEvent(&event), absl_testing::IsOk());\n   EXPECT_THAT(stream2->BlockHostUntilDone(), absl_testing::IsOk());\n-  absl::MutexLock lock(&mutex);\n+  absl::MutexLock lock(mutex);\n   EXPECT_THAT(execution_order,\n               ElementsAre(ExecutionStage::kBeforeWaitForEvent,\n                           ExecutionStage::kAfterWaitForEvent,"
        },
        {
            "sha": "8b8a38ddd36c0cb1a612686c0bbbc9e0f1d26e17",
            "filename": "third_party/xla/xla/stream_executor/cuda/subprocess_compilation.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4c7bb86436a29e3a5129ed1f840c91221c45c0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc?ref=c4c7bb86436a29e3a5129ed1f840c91221c45c0b",
            "patch": "@@ -123,7 +123,7 @@ absl::StatusOr<SemanticVersion> GetToolVersion(absl::string_view tool_path) {\n       new absl::flat_hash_map<std::string, absl::StatusOr<SemanticVersion>>\n           ABSL_GUARDED_BY(mutex);\n \n-  absl::MutexLock lock(&mutex);\n+  absl::MutexLock lock(mutex);\n   auto it = cache->find(tool_path);\n   if (it != cache->end()) {\n     return it->second;\n@@ -233,7 +233,7 @@ static void LogPtxasTooOld(const std::string& ptxas_path, int cc_major,\n   static absl::Mutex* const mutex = new absl::Mutex;\n   static AlreadyLoggedSetTy* const already_logged = new AlreadyLoggedSetTy;\n \n-  absl::MutexLock lock(mutex);\n+  absl::MutexLock lock(*mutex);\n \n   if (already_logged->insert(std::make_tuple(ptxas_path, cc_major, cc_minor))\n           .second) {"
        }
    ],
    "stats": {
        "total": 92,
        "additions": 46,
        "deletions": 46
    }
}