{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Set the init value of the reductions tests to the neutral value.\n\nWe were previously testing incorrect behavior as the init value of a reduce must be the neutral value of the reduce function, I am going to take advantage of this requirement so I need to update the tests first.\n\nPiperOrigin-RevId: 830437169",
    "sha": "a819c35e82964a758211bb0aac213c95dccce77f",
    "files": [
        {
            "sha": "4e1e1241602c60cb4691bff0633113d336285b67",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/tiled_kernel_test.py",
            "status": "modified",
            "additions": 29,
            "deletions": 25,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a819c35e82964a758211bb0aac213c95dccce77f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a819c35e82964a758211bb0aac213c95dccce77f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py?ref=a819c35e82964a758211bb0aac213c95dccce77f",
            "patch": "@@ -26,6 +26,13 @@\n create_literal = testlib_utilities.create_literal_from_np\n \n \n+class InputSpec:\n+\n+  def __init__(self, shape: tuple[int, ...], input_value):\n+    self.shape = shape\n+    self.input_value = input_value\n+\n+\n def get_random_array(shape: tuple[int, ...], dtype: np.dtype) -> np.ndarray:\n   rng = np.random.default_rng()\n   return rng.uniform(low=-5, high=5, size=shape).astype(dtype)\n@@ -35,12 +42,11 @@ def compare_kernel(\n     ir: str,\n     kernel_name: str,\n     num_workgroups: int,\n-    input_shapes: Iterable[tuple[int, ...]],\n+    input_specs: Iterable[InputSpec],\n     output_shape: tuple[int, ...],\n     dtype,\n     expected_output: Callable[[np.ndarray, ...], np.ndarray],\n     maxulp: Optional[int] = None,\n-    random_inputs: bool = False,\n ) -> None:\n   mlir_emitter = cpu_testlib.MlirTestKernelEmitter(\n       ir, kernel_name, (num_workgroups, 1, 1)\n@@ -54,12 +60,12 @@ def compare_kernel(\n \n   # Simply use a all-ones arrays as inputs to make it easy to debug the kernel\n   # unless random inputs are requested.\n-  def get_input(shape):\n-    if random_inputs:\n-      return get_random_array(shape, dtype)\n-    return np.ones(shape=shape, dtype=dtype)\n+  def get_input(spec: InputSpec):\n+    if spec.input_value is None:\n+      return get_random_array(spec.shape, dtype)\n+    return np.full(shape=spec.shape, fill_value=spec.input_value, dtype=dtype)\n \n-  inputs = [get_input(shape) for shape in input_shapes]\n+  inputs = [get_input(spec) for spec in input_specs]\n \n   input_tensors = [create_literal(input) for input in inputs]\n   # Use a random array as the output to ensure all values are written to.\n@@ -99,7 +105,7 @@ def test_slice(self):\n         ir,\n         \"tiled_slice\",\n         1,\n-        [(5, 5)],\n+        [InputSpec((5, 5), 1)],\n         (5, 5),\n         np.float32,\n         lambda arg: arg.transpose(),\n@@ -123,7 +129,7 @@ def test_strided(self):\n         ir,\n         \"tiled_slice\",\n         1,\n-        [(64, 64)],\n+        [InputSpec((64, 64), 1)],\n         (4, 32),\n         np.float32,\n         lambda arg: arg[::21, ::2],\n@@ -150,7 +156,7 @@ def test_transpose(self):\n         ir,\n         \"tiled_transpose\",\n         8,\n-        [(4096, 4096)],\n+        [InputSpec((4096, 4096), 1)],\n         (4096, 4096),\n         np.float32,\n         lambda arg: arg.transpose(),\n@@ -179,7 +185,7 @@ def test_add_tranpose(self):\n         ir,\n         \"add_tranpose\",\n         8,\n-        [(4096, 4096)],\n+        [InputSpec((4096, 4096), 1)],\n         (4096, 4096),\n         np.float32,\n         lambda arg: arg + arg.transpose(),\n@@ -207,7 +213,7 @@ def test_dot_single_tile(self):\n         ir,\n         \"dot_single_tile\",\n         1,\n-        [(8, 16), (16, 8)],\n+        [InputSpec((8, 16), 1), InputSpec((16, 8), 1)],\n         (8, 8),\n         np.float32,\n         lambda lhs, rhs: lhs @ rhs,\n@@ -236,7 +242,7 @@ def test_dot_scalar_output(self):\n         ir,\n         \"test_dot_scalar_output\",\n         1,\n-        [(8, 16), (16, 8)],\n+        [InputSpec((8, 16), 1), InputSpec((16, 8), 1)],\n         (),\n         np.float32,\n         lambda lhs, rhs: np.tensordot(lhs, rhs, axes=[[1, 0], [0, 1]]),\n@@ -269,7 +275,7 @@ def test_dot_fusion_single_tile(self):\n         ir,\n         \"dot_fusion_single_tile\",\n         1,\n-        [(8, 16), (8, 16), (16, 1)],\n+        [InputSpec((8, 16), 1), InputSpec((8, 16), 1), InputSpec((16, 1), 1)],\n         (8, 1),\n         np.float32,\n         lambda lhs_0, lhs_1, rhs: np.tanh((lhs_0 + lhs_1) @ rhs),\n@@ -306,7 +312,7 @@ def test_reduction_add_inner(self):\n         ir,\n         \"reduction_add_inner\",\n         4,\n-        [(1024, 32), (1,)],\n+        [InputSpec((1024, 32), 1), InputSpec((1,), 0)],\n         (1024,),\n         np.int32,\n         lambda input, init: np.sum(input, axis=1) + init,\n@@ -342,10 +348,10 @@ def test_reduction_add_outer(self):\n         ir,\n         \"reduction_add_outer\",\n         4,\n-        [(1024, 32), (1,)],\n+        [InputSpec((1024, 32), 1), InputSpec((1,), 0)],\n         (32,),\n         np.float32,\n-        lambda input, init: np.sum(input, axis=0) + init,\n+        lambda input, init: np.sum(input, axis=0),\n     )\n \n   def test_reduction_middle(self):\n@@ -375,10 +381,10 @@ def test_reduction_middle(self):\n         ir,\n         \"reduction_add_middle\",\n         1,\n-        [(8, 4, 2), (1,)],\n+        [InputSpec((8, 4, 2), 1), InputSpec((1,), 0)],\n         (8, 2),\n         np.float32,\n-        lambda input, init: np.sum(input, axis=1) + init,\n+        lambda input, init: np.sum(input, axis=1),\n     )\n \n   def test_reduction_outer_inner(self):\n@@ -408,10 +414,10 @@ def test_reduction_outer_inner(self):\n         ir,\n         \"reduction_add_outer_inner\",\n         1,\n-        [(8, 4, 2), (1,)],\n+        [InputSpec((8, 4, 2), 1), InputSpec((1,), 0)],\n         (4,),\n         np.float32,\n-        lambda input, init: np.sum(input, axis=(0, 2)) + init,\n+        lambda input, init: np.sum(input, axis=(0, 2)),\n     )\n \n   def test_broadcast_in_dim_inner(self):\n@@ -433,11 +439,10 @@ def test_broadcast_in_dim_inner(self):\n         ir,\n         \"broadcast_in_dim_inner\",\n         1,\n-        [(4,)],\n+        [InputSpec((4,), None)],\n         (32, 4),\n         np.float32,\n         lambda input: np.broadcast_to(input, (32, 4)),\n-        random_inputs=True,\n     )\n \n   def test_broadcast_in_dim_outer(self):\n@@ -459,11 +464,10 @@ def test_broadcast_in_dim_outer(self):\n         ir,\n         \"broadcast_in_dim_outer\",\n         1,\n-        [(4,)],\n+        [InputSpec((4,), None)],\n         (4, 32),\n         np.float32,\n         lambda input: np.transpose(np.broadcast_to(input, (32, 4))),\n-        random_inputs=True,\n     )\n \n "
        }
    ],
    "stats": {
        "total": 54,
        "additions": 29,
        "deletions": 25
    }
}