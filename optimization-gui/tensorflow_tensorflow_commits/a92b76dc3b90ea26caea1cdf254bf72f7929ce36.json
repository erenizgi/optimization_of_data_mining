{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Introduce RaggedAllToAllMultiHostDecomposer pass.\n\nThis pass decomposes a multi-host ragged-all-to-all operation into intra-host and inter-host collective operations. The idea is to minimize intra-host traffic and use efficient fused inter-host kernel if fast interconnect (like NVLink) is available.\n\nPiperOrigin-RevId: 814399737",
    "sha": "a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
    "files": [
        {
            "sha": "56178dc5c6022f7048008bd6a604b5a4bfa4ea06",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -2395,6 +2395,26 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       debug_options->xla_gpu_unsupported_enable_ragged_all_to_all_decomposer(),\n       \"Internal: Enable the RaggedAllToAllDecomposer, an experimental pass \"\n       \"that rewrites ragged-all-to-all as a dense all-to-all operation.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer\",\n+      bool_setter_for(\n+          &DebugOptions::\n+              set_xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer),  // NOLINT\n+      debug_options\n+          ->xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer(),  // NOLINT\n+      \"Internal: Enable the RaggedAllToAllMultiHostDecomposer, an experimental \"\n+      \"pass to decompose ragged-all-to-all operation in intra-host and \"\n+      \"inter-host parts.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_unsupported_override_fast_interconnect_slice_size\",\n+      int64_setter_for(\n+          &DebugOptions::\n+              set_xla_gpu_unsupported_override_fast_interconnect_slice_size),\n+      debug_options\n+          ->xla_gpu_unsupported_override_fast_interconnect_slice_size(),\n+      \"Internal: Override the number of devices in the fast interconnect \"\n+      \"domain. Default is 0, which means the number of devices is not \"\n+      \"overridden.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_unsupported_use_all_reduce_one_shot_kernel\",\n       bool_setter_for("
        },
        {
            "sha": "302c28efef58c1d752b4e3561db39f95d7b2dd6a",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -1654,6 +1654,7 @@ cc_library(\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/service/gpu/transforms:ragged_all_to_all_canonicalizer\",\n         \"//xla/service/gpu/transforms:ragged_all_to_all_decomposer\",\n+        \"//xla/service/gpu/transforms:ragged_all_to_all_multi_host_decomposer\",\n         \"//xla/service/gpu/transforms:reduce_scatter_creator\",\n         \"//xla/service/gpu/transforms:reduction_degenerate_dim_remover\",\n         \"//xla/service/gpu/transforms:reduction_dimension_grouper\","
        },
        {
            "sha": "9d82464079675ec6be40c748a17333cbd3f1d307",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 2,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -240,6 +240,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n #include \"xla/service/gpu/transforms/ragged_all_to_all_canonicalizer.h\"\n #include \"xla/service/gpu/transforms/ragged_all_to_all_decomposer.h\"\n+#include \"xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.h\"\n #include \"xla/service/gpu/transforms/reduce_scatter_creator.h\"\n #include \"xla/service/gpu/transforms/reduction_degenerate_dim_remover.h\"\n #include \"xla/service/gpu/transforms/reduction_dimension_grouper.h\"\n@@ -947,7 +948,7 @@ absl::Status RunOptimizationPasses(\n }\n \n absl::Status RunCollectiveOptimizationPasses(\n-    HloModule* hlo_module,\n+    HloModule* hlo_module, const GpuCompiler::CompileOptions& options,\n     const AlgebraicSimplifierOptions& layout_insensitive_algsimp_opts,\n     se::GpuComputeCapability gpu_version, int num_visible_devices_per_process,\n     int64_t pointer_size) {\n@@ -967,6 +968,26 @@ absl::Status RunCollectiveOptimizationPasses(\n   if (debug_options.xla_gpu_unsupported_enable_ragged_all_to_all_decomposer()) {\n     collectives_pipeline.AddPass<RaggedAllToAllDecomposer>();\n   }\n+\n+  int64_t fast_interconnect_slice_size = [&]() {\n+    if (debug_options\n+            .xla_gpu_unsupported_override_fast_interconnect_slice_size() > 0) {\n+      return debug_options\n+          .xla_gpu_unsupported_override_fast_interconnect_slice_size();\n+    }\n+    return options.slice_size;\n+  }();\n+\n+  // `fast_interconnect_slice_size` can be 0 if CompileOptions were not set up\n+  // by the runner and the override flag is not set. In this case, we should not\n+  // run the RaggedAllToAllMultiHostDecomposer.\n+  if (debug_options\n+          .xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer() &&  // NOLINT\n+      fast_interconnect_slice_size > 0) {\n+    collectives_pipeline.AddPass<RaggedAllToAllMultiHostDecomposer>(\n+        fast_interconnect_slice_size);\n+  }\n+\n   collectives_pipeline.AddPass<AllReduceSimplifier>();\n   collectives_pipeline.AddPass<AllReduceFolder>();\n   collectives_pipeline.AddPass<AllReduceSplitter>();\n@@ -1540,7 +1561,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   se::GpuComputeCapability gpu_version =\n       device_description.gpu_compute_capability();\n   TF_RETURN_IF_ERROR(RunCollectiveOptimizationPasses(\n-      hlo_module, layout_insensitive_algsimp_opts, gpu_version,\n+      hlo_module, options, layout_insensitive_algsimp_opts, gpu_version,\n       platform->VisibleDeviceCount(), pointer_size_));\n \n   // Run target-specific HLO optimization passes for convolution"
        },
        {
            "sha": "e31daae0d246bc71436bea00425b98eea812fe52",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -3170,6 +3170,46 @@ xla_cc_test(\n     ],\n )\n \n+cc_library(\n+    name = \"ragged_all_to_all_multi_host_decomposer\",\n+    srcs = [\"ragged_all_to_all_multi_host_decomposer.cc\"],\n+    hdrs = [\"ragged_all_to_all_multi_host_decomposer.h\"],\n+    deps = [\n+        \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"ragged_all_to_all_multi_host_decomposer_test\",\n+    srcs = [\"ragged_all_to_all_multi_host_decomposer_test.cc\"],\n+    deps = [\n+        \":ragged_all_to_all_multi_host_decomposer\",\n+        \"//xla/hlo/testlib:filecheck\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n+        \"//xla/service:hlo_cse\",\n+        \"//xla/tests:test_utils\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"ragged_all_to_all_canonicalizer\",\n     srcs = [\"ragged_all_to_all_canonicalizer.cc\"],"
        },
        {
            "sha": "b38f836288ddbbccdde3f692b0ad7837301aebb0",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.cc",
            "status": "added",
            "additions": 255,
            "deletions": 0,
            "changes": 255,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -0,0 +1,255 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.h\"\n+\n+#include <cstdint>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/collective_device_list.h\"\n+#include \"xla/hlo/ir/dfs_hlo_visitor.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// Exchanges the metadata between the hosts and computes the intra-host\n+// metadata.\n+//\n+// If `correct_offsets` is true, the offsets are corrected to account for the\n+// number of input rows in the combined ragged tensor. It's needed for\n+// `input_offsets`.\n+HloInstruction* GetIntraHostMetadata(\n+    HloRaggedAllToAllInstruction* ragged_all_to_all,\n+    HloInstruction* metadata_operand, HloComputation* computation,\n+    const std::vector<ReplicaGroup>& replica_groups,\n+    int64_t num_updates_per_replica, int64_t fast_interconnect_slice_size,\n+    int64_t num_hosts, bool correct_offsets) {\n+  Shape new_metadata_shape = ShapeUtil::MakeShape(\n+      metadata_operand->shape().element_type(),\n+      {num_hosts, fast_interconnect_slice_size, num_updates_per_replica});\n+\n+  Shape new_metadata_transposed_shape = ShapeUtil::MakeShape(\n+      metadata_operand->shape().element_type(),\n+      {fast_interconnect_slice_size, num_hosts, num_updates_per_replica});\n+\n+  HloInstruction* new_input_offsets = computation->AddInstruction(\n+      HloInstruction::CreateReshape(new_metadata_shape, metadata_operand));\n+\n+  HloInstruction* new_local_metadata =\n+      computation->AddInstruction(HloInstruction::CreateAllToAll(\n+          /*shape=*/new_metadata_shape,\n+          /*operands=*/{new_input_offsets},\n+          /*device_list=*/CollectiveDeviceList(replica_groups),\n+          /*constrain_layout=*/false,\n+          /*channel_id=*/ragged_all_to_all->channel_id(),\n+          /*split_dimension=*/0));\n+\n+  if (correct_offsets) {\n+    HloInstruction* iota =\n+        computation->AddInstruction(HloInstruction::CreateIota(\n+            /*shape=*/new_metadata_shape,\n+            /*iota_dimension=*/0));\n+\n+    int64_t num_input_rows =\n+        ragged_all_to_all->operand(0)->shape().dimensions(0);\n+\n+    HloInstruction* num_input_rows_constant =\n+        computation->AddInstruction(HloInstruction::CreateConstant(\n+            LiteralUtil::CreateR0<int64_t>(num_input_rows)));\n+\n+    HloInstruction* num_input_rows_constant_broadcast =\n+        computation->AddInstruction(HloInstruction::CreateBroadcast(\n+            /*shape=*/new_metadata_shape, num_input_rows_constant,\n+            /*broadcast_dimensions=*/{}));\n+\n+    HloInstruction* input_offsets_offset =\n+        computation->AddInstruction(HloInstruction::CreateBinary(\n+            /*shape=*/new_metadata_shape, HloOpcode::kMultiply,\n+            /*lhs=*/iota, /*rhs=*/num_input_rows_constant_broadcast));\n+\n+    new_local_metadata =\n+        computation->AddInstruction(HloInstruction::CreateBinary(\n+            /*shape=*/new_metadata_shape, HloOpcode::kAdd,\n+            /*lhs=*/new_local_metadata,\n+            /*rhs=*/input_offsets_offset));\n+  }\n+\n+  HloInstruction* new_local_metadata_transposed =\n+      computation->AddInstruction(HloInstruction::CreateTranspose(\n+          /*shape=*/new_metadata_transposed_shape,\n+          /*operand=*/new_local_metadata,\n+          /*dimensions=*/{1, 0, 2}));\n+\n+  HloInstruction* intra_host_metadata =\n+      computation->AddInstruction(HloInstruction::CreateReshape(\n+          metadata_operand->shape(), new_local_metadata_transposed));\n+\n+  return intra_host_metadata;\n+}\n+\n+absl::StatusOr<bool> DecomposeRaggedAllToAll(\n+    HloInstruction* hlo, HloComputation* computation, HloModule* module,\n+    int64_t fast_interconnect_slice_size) {\n+  auto* ragged_all_to_all = Cast<HloRaggedAllToAllInstruction>(hlo);\n+\n+  auto replica_groups = ragged_all_to_all->replica_groups();\n+\n+  // TODO(b/445380264): Support multiple replica groups.\n+  if (replica_groups.size() > 1) {\n+    return false;\n+  }\n+\n+  // Replica groups can be empty in collective instruction. Empty replica groups\n+  // mean that all devices are participating in the collective. This semantics\n+  // is hard to handle in an HLO pass, because we don't have enough information\n+  // about the topology, so it's easier to skip this case. Note that this is not\n+  // a concert for production models, because Jax fills replica groups for all\n+  // collectives.\n+  if (replica_groups.empty()) {\n+    return false;\n+  }\n+\n+  const auto& replica_ids = replica_groups[0].replica_ids();\n+\n+  for (int i = 0; i < replica_ids.size(); ++i) {\n+    if (i != replica_ids[i]) {\n+      return false;\n+    }\n+  }\n+\n+  HloInstruction* input_offsets = ragged_all_to_all->mutable_operand(2);\n+\n+  int64_t num_updates_per_replica =\n+      input_offsets->shape().dimensions(0) / replica_ids.size();\n+\n+  int64_t num_participating_devices = 0;\n+  for (auto& replica_group : replica_groups) {\n+    num_participating_devices += replica_group.replica_ids_size();\n+  }\n+\n+  int64_t num_hosts =\n+      CeilOfRatio(num_participating_devices, fast_interconnect_slice_size);\n+\n+  // All participating devices are in the same fast interconnect slice.\n+  if (num_hosts == 1) {\n+    return false;\n+  }\n+\n+  // TODO(b/445380264): Support more than 2 hosts.\n+  if (num_hosts != 2) {\n+    return false;\n+  }\n+\n+  std::vector<ReplicaGroup> inter_host_replica_groups(\n+      fast_interconnect_slice_size);\n+  std::vector<ReplicaGroup> intra_host_replica_groups(num_hosts);\n+\n+  for (int i = 0; i < fast_interconnect_slice_size; ++i) {\n+    inter_host_replica_groups[i].add_replica_ids(i);\n+    inter_host_replica_groups[i].add_replica_ids(fast_interconnect_slice_size +\n+                                                 i);\n+\n+    intra_host_replica_groups[0].add_replica_ids(i);\n+    intra_host_replica_groups[1].add_replica_ids(fast_interconnect_slice_size +\n+                                                 i);\n+  }\n+\n+  std::vector<HloInstruction*> intra_host_metadata;\n+\n+  HloInstruction* input_operand = ragged_all_to_all->mutable_operand(0);\n+\n+  Shape new_input_shape = input_operand->shape();\n+  new_input_shape.set_dimensions(\n+      0, num_hosts * input_operand->shape().dimensions(0));\n+\n+  HloInstruction* all_gather_input =\n+      computation->AddInstruction(HloInstruction::CreateAllGather(\n+          /*shape=*/new_input_shape,\n+          /*operands=*/{ragged_all_to_all->mutable_operand(0)},\n+          /*all_gather_dimension=*/0,\n+          /*device_list=*/CollectiveDeviceList(inter_host_replica_groups),\n+          /*constrain_layout=*/false,\n+          /*channel_id=*/ragged_all_to_all->channel_id(),\n+          /*use_global_device_ids=*/false));\n+\n+  for (int i = 2; i < 6; ++i) {\n+    intra_host_metadata.push_back(GetIntraHostMetadata(\n+        ragged_all_to_all, ragged_all_to_all->mutable_operand(i), computation,\n+        inter_host_replica_groups, num_updates_per_replica,\n+        fast_interconnect_slice_size, num_hosts, /*correct_offsets=*/i == 2));\n+  }\n+\n+  HloInstruction* new_ragged_all_to_all =\n+      computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n+          /*shape=*/ragged_all_to_all->shape(),\n+          /*operands=*/\n+          {all_gather_input, ragged_all_to_all->mutable_operand(1),\n+           intra_host_metadata[0], intra_host_metadata[1],\n+           intra_host_metadata[2], intra_host_metadata[3]},\n+          /*replica_groups=*/intra_host_replica_groups,\n+          /*channel_id=*/ragged_all_to_all->channel_id()));\n+\n+  TF_RETURN_IF_ERROR(\n+      computation->ReplaceInstruction(hlo, new_ragged_all_to_all));\n+\n+  return true;\n+}\n+\n+absl::StatusOr<bool> RaggedAllToAllMultiHostDecomposer::Run(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  bool changed = false;\n+\n+  for (auto computation : module->computations(execution_threads)) {\n+    for (auto hlo : computation->MakeInstructionPostOrder()) {\n+      if (HloPredicateIsNotOp<HloOpcode::kRaggedAllToAll>(hlo)) {\n+        continue;\n+      }\n+\n+      if (hlo->operand(2)->shape().element_type() != S64) {\n+        return absl::InvalidArgumentError(\n+            \"RaggedAllToAllDecomposer only supports S64 offsets. Was \"\n+            \"`ragged-all-to-all-canonicalizer` pass executed?\");\n+      }\n+\n+      TF_ASSIGN_OR_RETURN(\n+          bool result, DecomposeRaggedAllToAll(hlo, computation, module,\n+                                               fast_interconnect_slice_size_));\n+      changed |= result;\n+    }\n+  }\n+  return changed;\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "011b5749ebe199eeda20d3f0031e1f073bdb04b9",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.h",
            "status": "added",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.h?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -0,0 +1,51 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_GPU_TRANSFORMS_RAGGED_ALL_TO_ALL_MULTI_HOST_DECOMPOSER_H_\n+#define XLA_SERVICE_GPU_TRANSFORMS_RAGGED_ALL_TO_ALL_MULTI_HOST_DECOMPOSER_H_\n+\n+#include <cstdint>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// Rewrites a `ragged-all-to-all` into inter-host and intra-host parts.\n+class RaggedAllToAllMultiHostDecomposer : public HloModulePass {\n+ public:\n+  explicit RaggedAllToAllMultiHostDecomposer(int fast_interconnect_slice_size)\n+      : fast_interconnect_slice_size_(fast_interconnect_slice_size) {}\n+\n+  absl::string_view name() const override {\n+    return \"ragged-all-to-all-multi-host-decomposer\";\n+  }\n+\n+  absl::StatusOr<bool> Run(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+\n+ private:\n+  int64_t fast_interconnect_slice_size_;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_SERVICE_GPU_TRANSFORMS_RAGGED_ALL_TO_ALL_MULTI_HOST_DECOMPOSER_H_"
        },
        {
            "sha": "648953665eefdb4e4b06a68c2de63d6dfc1bcca6",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer_test.cc",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -0,0 +1,166 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.h\"\n+\n+#include <memory>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"xla/hlo/testlib/filecheck.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n+#include \"xla/service/hlo_cse.h\"\n+#include \"xla/tests/test_utils.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+using RaggedAllToAllDecomposerTest = HloHardwareIndependentTestBase;\n+\n+TEST_F(RaggedAllToAllDecomposerTest, SimpleRaggedAllToAllIsSupported) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY main {\n+  input = bf16[128] parameter(0)\n+  output = bf16[256] parameter(1)\n+  input_offsets = s64[16] parameter(2)\n+  send_sizes = s64[16] parameter(3)\n+  output_offsets = s64[16] parameter(4)\n+  recv_sizes = s64[16] parameter(5)\n+  ROOT ra2a = bf16[256] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), \n+    replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}}\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/8);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+\n+  EXPECT_TRUE(changed);\n+  TF_EXPECT_OK(VerifyHloModule(module.get(), true, true));\n+  TF_EXPECT_OK(HloDCE().Run(module.get()));\n+  TF_EXPECT_OK(HloCSE(true).Run(module.get()));\n+\n+  LOG(ERROR) << module->ToString();\n+\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    // CHECK: all-gather{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n+    // CHECK-COUNT-4: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n+    // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}{{[}]}}\n+  )\"));\n+}\n+\n+TEST_F(RaggedAllToAllDecomposerTest, SingleHostRaggedAllToAllIsNotDecomposed) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY main {\n+    input = bf16[128] parameter(0)\n+    output = bf16[256] parameter(1)\n+    input_offsets = s64[8] parameter(2)\n+    send_sizes = s64[8] parameter(3)\n+    output_offsets = s64[8] parameter(4)\n+    recv_sizes = s64[8] parameter(5)\n+    ROOT ra2a = bf16[256] ragged-all-to-all(input, output, input_offsets,\n+      send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1,2,3,4,5,6,7}}\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/8);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(RaggedAllToAllDecomposerTest, MultipleReplicaGroupsAreNotSupported) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY main {\n+    input = bf16[128] parameter(0)\n+    output = bf16[256] parameter(1)\n+    input_offsets = s64[8] parameter(2)\n+    send_sizes = s64[8] parameter(3)\n+    output_offsets = s64[8] parameter(4)\n+    recv_sizes = s64[8] parameter(5)\n+    ROOT ra2a = bf16[256] ragged-all-to-all(input, output, input_offsets,\n+      send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/4);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(RaggedAllToAllDecomposerTest, OnlyDecompositionForTwoHostsIsSupported) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY main {\n+  input = bf16[128] parameter(0)\n+  output = bf16[256] parameter(1)\n+  input_offsets = s64[16] parameter(2)\n+  send_sizes = s64[16] parameter(3)\n+  output_offsets = s64[16] parameter(4)\n+  recv_sizes = s64[16] parameter(5)\n+  ROOT ra2a = bf16[256] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes),\n+    replica_groups=[1,16]<=[16]\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/4);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(RaggedAllToAllDecomposerTest, EmptyReplicaGroupsAreNotSupported) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY main {\n+  input = bf16[128] parameter(0)\n+  output = bf16[256] parameter(1)\n+  input_offsets = s64[16] parameter(2)\n+  send_sizes = s64[16] parameter(3)\n+  output_offsets = s64[16] parameter(4)\n+  recv_sizes = s64[16] parameter(5)\n+  ROOT ra2a = bf16[256] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes),\n+    replica_groups={}\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/4);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+  EXPECT_FALSE(changed);\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "85d71c8fdce7f9b3490bcbddc1c6951d61b2bb0f",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 140,
            "deletions": 20,
            "changes": 160,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -2649,14 +2649,12 @@ enum class RaggedAllToAllImplType {\n   kOneShot,\n };\n \n-class RaggedAllToAllTest : public CollectiveOpsWithFlagsBase,\n-                           public ::testing::WithParamInterface<\n-                               std::tuple<bool, RaggedAllToAllImplType>> {\n+class RaggedAllToAllTestBase : public CollectiveOpsWithFlagsBase {\n  public:\n-  RaggedAllToAllTest()\n+  RaggedAllToAllTestBase(bool enable_async, RaggedAllToAllImplType impl_type)\n       : CollectiveOpsWithFlagsBase(\n-            std::get<0>(GetParam()),\n-            std::get<1>(GetParam()) == RaggedAllToAllImplType::kMemcpy) {}\n+            enable_async, impl_type == RaggedAllToAllImplType::kMemcpy),\n+        impl_type_(impl_type) {}\n \n   // Creates random test data for a ragged-all-to-all.\n   //\n@@ -2813,9 +2811,9 @@ class RaggedAllToAllTest : public CollectiveOpsWithFlagsBase,\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions opts = CollectiveOpsWithFlagsBase::GetDebugOptionsForTest();\n     opts.set_xla_gpu_unsupported_enable_ragged_all_to_all_decomposer(\n-        std::get<1>(GetParam()) == RaggedAllToAllImplType::kDecomposer);\n+        impl_type_ == RaggedAllToAllImplType::kDecomposer);\n     opts.set_xla_gpu_unsupported_use_ragged_all_to_all_one_shot_kernel(\n-        std::get<1>(GetParam()) == RaggedAllToAllImplType::kOneShot);\n+        impl_type_ == RaggedAllToAllImplType::kOneShot);\n     return opts;\n   }\n \n@@ -2901,6 +2899,17 @@ class RaggedAllToAllTest : public CollectiveOpsWithFlagsBase,\n   std::vector<Literal> output_sizes_;\n \n   Literal output_init_;\n+\n+  RaggedAllToAllImplType impl_type_;\n+};\n+\n+class RaggedAllToAllTest : public RaggedAllToAllTestBase,\n+                           public ::testing::WithParamInterface<\n+                               std::tuple<bool, RaggedAllToAllImplType>> {\n+ public:\n+  RaggedAllToAllTest()\n+      : RaggedAllToAllTestBase(std::get<0>(GetParam()),\n+                               std::get<1>(GetParam())) {}\n };\n \n TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs) {\n@@ -3432,18 +3441,129 @@ std::string RaggedAllToAllImplTypeName(\n   }\n }\n \n-INSTANTIATE_TEST_SUITE_P(\n-    RaggedAllToAllTest, RaggedAllToAllTest,\n-    ::testing::Combine(::testing::Bool(),\n-                       ::testing::Values(RaggedAllToAllImplType::kNccl,\n-                                         RaggedAllToAllImplType::kMemcpy,\n-                                         RaggedAllToAllImplType::kDecomposer,\n-                                         RaggedAllToAllImplType::kOneShot)),\n-    [](const ::testing::TestParamInfo<std::tuple<bool, RaggedAllToAllImplType>>&\n-           info) {\n-      return absl::StrCat(GetAsyncTestName(std::get<0>(info.param)), \"_\",\n-                          RaggedAllToAllImplTypeName(std::get<1>(info.param)));\n-    });\n+class RaggedAllToAllMultiHostDecomposerTest : public RaggedAllToAllTestBase {\n+ public:\n+  RaggedAllToAllMultiHostDecomposerTest()\n+      : RaggedAllToAllTestBase(/*enable_async=*/false,\n+                               /*impl_type=*/RaggedAllToAllImplType::kOneShot) {\n+  }\n+\n+ protected:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options =\n+        RaggedAllToAllTestBase::GetDebugOptionsForTest();\n+    debug_options\n+        .set_xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer(\n+            true);\n+    return debug_options;\n+  }\n+};\n+\n+TEST_F(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[512,5,32] parameter(0)\n+    output = f32[512,5,32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[512,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes), \n+      replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 16;\n+  if (test_runner().device_count() < kNumReplicas * kNumPartitions) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+                 << \" devices (\" << test_runner().device_count()\n+                 << \" available)\";\n+  }\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  config.mutable_debug_options()\n+      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(1);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      HloTestBase::ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                                     /*num_replicas=*/kNumReplicas,\n+                                     /*run_hlo_passes=*/true,\n+                                     /*device_assignment=*/nullptr));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n+TEST_F(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[512,5,32] parameter(0)\n+    output = f32[512,5,32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[512,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes), \n+      replica_groups={{0,1,2,3,4,5,6,7}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 8;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 4;\n+  if (test_runner().device_count() < kNumReplicas * kNumPartitions) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+                 << \" devices (\" << test_runner().device_count()\n+                 << \" available)\";\n+  }\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  config.mutable_debug_options()\n+      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      HloTestBase::ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                                     /*num_replicas=*/kNumReplicas,\n+                                     /*run_hlo_passes=*/true,\n+                                     /*device_assignment=*/nullptr));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n \n TEST_F(CollectiveOpsTestE2E, MemcpyP2pWhileLoopCorrectness) {\n   absl::string_view hlo_string = R\"("
        },
        {
            "sha": "fa8b203d18a69dac1110071d58cf468b97a5a672",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a92b76dc3b90ea26caea1cdf254bf72f7929ce36/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=a92b76dc3b90ea26caea1cdf254bf72f7929ce36",
            "patch": "@@ -833,7 +833,7 @@ message DebugOptions {\n   optional ShapeChecks xla_gpu_shape_checks = 170;\n \n   // If true, shards the autotuning work between participating compiler\n-  // processes (typically in multi-host setups) and joins the results when\n+  // processes (typically multi-host setups) and joins the results when\n   // it's done.\n   optional bool xla_gpu_shard_autotuning = 304;\n \n@@ -876,6 +876,11 @@ message DebugOptions {\n   // Internal testing flag to switch RaggedAllToAllDecomposer on or off.\n   optional bool xla_gpu_unsupported_enable_ragged_all_to_all_decomposer = 350;\n \n+  // Internal testing flag to switch RaggedAllToAllMultiHostDecomposer on or\n+  // off.\n+  optional bool\n+      xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer = 415;\n+\n   // Internal debug/testing flag to switch Triton GEMM fusions on or off.\n   optional bool xla_gpu_unsupported_enable_triton_gemm = 322;\n \n@@ -891,6 +896,12 @@ message DebugOptions {\n   repeated GenericTritonEmitterFeature\n       xla_gpu_unsupported_generic_triton_emitter_features = 398;\n \n+  // Internal debug/testing flag to override the number of devices in the fast\n+  // interconnect domain. Default is 0, which means the number of devices is not\n+  // overridden.\n+  optional int64 xla_gpu_unsupported_override_fast_interconnect_slice_size =\n+      416;\n+\n   // Internal testing flag to enable one-shot kernel for single-host\n   // all-reduce operations.\n   optional bool xla_gpu_unsupported_use_all_reduce_one_shot_kernel = 387;\n@@ -1340,7 +1351,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 415\n+  // Next id: 417\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 733,
        "additions": 709,
        "deletions": 24
    }
}