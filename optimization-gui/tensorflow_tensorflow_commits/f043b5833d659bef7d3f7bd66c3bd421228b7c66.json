{
    "author": "tensorflower-gardener",
    "message": "add low_priority queue batch attr to mlrt batch_kernel\n\nPiperOrigin-RevId: 846412287",
    "sha": "f043b5833d659bef7d3f7bd66c3bd421228b7c66",
    "files": [
        {
            "sha": "442ef61d093a5bd2f37c0244e657626f74323a15",
            "filename": "tensorflow/core/tfrt/mlrt/kernel/batch_kernel.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f043b5833d659bef7d3f7bd66c3bd421228b7c66/tensorflow%2Fcore%2Ftfrt%2Fmlrt%2Fkernel%2Fbatch_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f043b5833d659bef7d3f7bd66c3bd421228b7c66/tensorflow%2Fcore%2Ftfrt%2Fmlrt%2Fkernel%2Fbatch_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fmlrt%2Fkernel%2Fbatch_kernel.cc?ref=f043b5833d659bef7d3f7bd66c3bd421228b7c66",
            "patch": "@@ -475,6 +475,43 @@ REGISTER_OP(kMlrtBatchFunctionName)\n     .Attr(\"container: string = ''\")\n     .Attr(\"shared_name: string = ''\")\n     .Attr(\"batching_queue: string = ''\")\n+    // A separate set of batch options for the low priority requests, which is\n+    // used for priority queue batching.\n+    .Attr(\"low_priority_max_batch_size: int = 0\")\n+    .Attr(\"low_priority_batch_timeout_micros: int = 0\")\n+    .Attr(\"low_priority_allowed_batch_sizes: list(int) = []\")\n+    .Attr(\"low_priority_max_enqueued_batches: int = 0\")\n+    // Policy that determines the mixed priority batching behavior when low\n+    // priority batch parameters are present.\n+    //\n+    // low_priority_padding_with_next_allowed_batch_size: If high priority\n+    // batches time out without reaching the max batch size, low priority inputs\n+    // pad the high priority batches up to the next allowed batch size. A low\n+    // priority only batch gets schedule only when the low priority input times\n+    // out or reaches the max batch size while there is no high priority input\n+    // waiting to be processed.\n+    // low_priority_padding_with_max_batch_size: Same as above but pad up to the\n+    // max batch size.\n+    // priority_isolation: High priority and low priority inputs never share the\n+    // same batch, i.e., no low priority input padding high priority batches.\n+    // Low priority inputs get scheduled only as part of low priority only\n+    // batches as described above.\n+    // priority_merge: High and low priority inputs are queued separately but\n+    // when a batch needs to be scheduled, the two queues are treated as one\n+    // merged flat list of inputs with high priority inputs at the front of the\n+    // list of tasks to use for the next batch. If all inputs are of the same\n+    // priority, the behavior is the same as disabling prioritization.\n+    .Attr(\n+        \"mixed_priority_policy: \"\n+        \"{'low_priority_padding_with_max_batch_size', \"\n+        \"'low_priority_padding_with_next_allowed_batch_size', \"\n+        \"'priority_isolation', 'priority_merge'} = \"\n+        \"'low_priority_padding_with_max_batch_size'\")\n+    // See the description of the batch_padding_policy attribute of\n+    // BatchFunction in core/ops/batch_ops.cc.\n+    .Attr(\n+        \"batch_padding_policy: \"\n+        \"{'PAD_UP', 'BATCH_DOWN', 'MINIMIZE_TPU_COST_PER_REQUEST'} = 'PAD_UP'\")\n     .Attr(\"Tin: list(type)\")\n     .Attr(\"Tcaptured: list(type) >= 0\")\n     .Attr(\"Tout: list(type)\")"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 37,
        "deletions": 0
    }
}