{
    "author": "khasanovaa",
    "message": "Add TMA support in LaunchCmd.\n\nPiperOrigin-RevId: 806188158",
    "sha": "26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
    "files": [
        {
            "sha": "eb7191132bf3b9c50640343a717fe135d4484411",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -105,6 +105,7 @@ cc_library(\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:trace_command_buffer_factory\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/lib/gtl:int_type\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n@@ -174,6 +175,7 @@ cc_library(\n         \":all_gather_thunk\",\n         \":all_reduce_thunk\",\n         \":all_to_all_thunk\",\n+        \":collective_thunk\",\n         \":command_buffer_cmd\",\n         \":conditional_thunk\",\n         \":copy_thunk\",\n@@ -840,25 +842,40 @@ cc_library(\n     ],\n )\n \n-xla_cc_test(\n+xla_test(\n     name = \"kernel_thunk_test\",\n     srcs = [\"kernel_thunk_test.cc\"],\n+    backends = [\"gpu\"],\n     deps = [\n+        \":command_buffer_cmd\",\n+        \":command_buffer_cmd_emitter\",\n+        \":command_buffer_thunk\",\n         \":kernel_thunk\",\n+        \":sequential_thunk\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service:executable\",\n+        \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/gpu:gpu_test_kernels\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n "
        },
        {
            "sha": "ad1702fb100f598ff9c3b60e52e54d0eef4b328c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 10,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -53,6 +53,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/annotation.h\"\n #include \"xla/backends/gpu/runtime/collective_broadcast_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -80,6 +81,7 @@ limitations under the License.\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -1001,16 +1003,18 @@ absl::StatusOr<const se::CommandBuffer::Command*> ComputationIdCmd::Record(\n // LaunchCmd\n //===----------------------------------------------------------------------===//\n \n-LaunchCmd::LaunchCmd(std::string kernel_name,\n-                     absl::Span<const BufferAllocation::Slice> args,\n-                     absl::Span<const MemoryAccess> args_access,\n-                     LaunchDimensions dims, int64_t shmem_bytes)\n+LaunchCmd::LaunchCmd(\n+    std::string kernel_name, absl::Span<const BufferAllocation::Slice> args,\n+    absl::Span<const MemoryAccess> args_access, LaunchDimensions dims,\n+    int64_t shmem_bytes,\n+    std::optional<stream_executor::gpu::TmaMetadata> tma_metadata)\n     : CommandBufferCmd(CommandBufferCmdType::kLaunchCmd),\n       kernel_name_(std::move(kernel_name)),\n       args_(args.begin(), args.end()),\n       args_access_(args_access.begin(), args_access.end()),\n       dims_(dims),\n-      shmem_bytes_(shmem_bytes) {}\n+      shmem_bytes_(shmem_bytes),\n+      tma_metadata_(std::move(tma_metadata)) {}\n \n absl::Status LaunchCmd::Initialize(const Thunk::InitializeParams& params,\n                                    StateManager& state) {\n@@ -1045,27 +1049,46 @@ absl::StatusOr<const se::CommandBuffer::Command*> LaunchCmd::Record(\n   VLOG(5) << \"LaunchCmd: kernel=\" << kernel_name_\n           << \"; shmem_bytes=\" << shmem_bytes_;\n \n+  se::StreamExecutor* executor = execute_params.stream->parent();\n   se::Kernel* kernel = [&] {\n     absl::MutexLock lock(&mutex_);\n-    return kernels_[execute_params.stream->parent()].get();\n+    return kernels_[executor].get();\n   }();\n \n   if (kernel == nullptr) {\n     return absl::InternalError(absl::StrCat(\n         \"Kernel not loaded on a command buffer executor: \", kernel_name_));\n   }\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> buffers;\n-  for (const BufferAllocation::Slice& arg : args_) {\n+  absl::InlinedVector<std::variant<se::DeviceMemoryBase, se::TensorMap>, 4>\n+      kernel_args_variant;\n+  stream_executor::gpu::TmaMetadata tma_metadata =\n+      tma_metadata_.value_or(se::gpu::TmaMetadata{});\n+  for (int idx = 0; idx < args_.size(); ++idx) {\n+    const BufferAllocation::Slice& arg = args_[idx];\n     se::DeviceMemoryBase buf =\n         execute_params.buffer_allocations->GetDeviceAddress(arg);\n     VLOG(5) << \"  Arg: \" << arg << \": \" << buf.opaque();\n-    buffers.push_back(buf);\n+\n+    if (auto it = tma_metadata.arg_index_to_tma_info.find(idx);\n+        it != tma_metadata.arg_index_to_tma_info.end()) {\n+      // TMA descriptor argument.\n+      stream_executor::gpu::TmaDescriptor tma_desc = it->second;\n+      TF_ASSIGN_OR_RETURN(se::TensorMap tensor_map,\n+                          executor->CreateTensorMap(tma_desc, buf.opaque()));\n+      VLOG(5) << \"  Using TensorMap for arg #\" << idx << \": \"\n+              << tma_desc.ToString();\n+      kernel_args_variant.push_back(std::move(tensor_map));\n+    } else {\n+      // Buffer argument.\n+      kernel_args_variant.push_back(buf);\n+    }\n   }\n \n   TF_ASSIGN_OR_RETURN(\n       auto kernel_args,\n-      se::PackKernelArgs<se::DeviceMemoryBase>(buffers, shmem_bytes_));\n+      se::PackKernelArgs(absl::MakeConstSpan(kernel_args_variant),\n+                         shmem_bytes_));\n \n   return Handle(\n       std::move(record_action),"
        },
        {
            "sha": "b4ec0591d5e7697e5d3717dbc28ce13958239b33",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -37,7 +37,6 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n@@ -61,6 +60,7 @@ limitations under the License.\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -654,7 +654,9 @@ class LaunchCmd : public CommandBufferCmd {\n   LaunchCmd(std::string kernel_name,\n             absl::Span<const BufferAllocation::Slice> args,\n             absl::Span<const BufferUse::MemoryAccess> args_access,\n-            LaunchDimensions dims, int64_t shmem_bytes);\n+            LaunchDimensions dims, int64_t shmem_bytes,\n+            std::optional<stream_executor::gpu::TmaMetadata> tma_metadata =\n+                std::nullopt);\n \n   absl::Status Initialize(const Thunk::InitializeParams& params,\n                           StateManager& state) override;\n@@ -672,6 +674,7 @@ class LaunchCmd : public CommandBufferCmd {\n   std::vector<BufferUse::MemoryAccess> args_access_;\n   LaunchDimensions dims_;\n   int64_t shmem_bytes_;\n+  std::optional<stream_executor::gpu::TmaMetadata> tma_metadata_;\n \n   // Command sequence can be recorded concurrently for multiple command buffers\n   // on different stream executors and we need to synchronize mutable state."
        },
        {
            "sha": "059935771a8b7fa7eed55799d2d313af13bdb1f6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/command_buffer_cmd_emitter.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <optional>\n #include <utility>\n@@ -30,6 +31,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_cmd.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n@@ -79,7 +81,7 @@ static auto ArgsAccess(const std::vector<bool>& written) {\n static absl::StatusOr<Command> Convert(const KernelThunk& thunk) {\n   return std::make_unique<LaunchCmd>(\n       thunk.kernel_name(), thunk.arguments(), ArgsAccess(thunk.written()),\n-      thunk.launch_dimensions(), thunk.shmem_bytes());\n+      thunk.launch_dimensions(), thunk.shmem_bytes(), thunk.tma_metadata());\n }\n \n static absl::StatusOr<Command> Convert(const CustomKernelThunk& thunk) {"
        },
        {
            "sha": "861ec8a6f9cf5fe57df2cfa2595c197a6593c594",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk_test.cc",
            "status": "modified",
            "additions": 174,
            "deletions": 0,
            "changes": 174,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n \n #include <array>\n+#include <cstddef>\n #include <memory>\n #include <optional>\n #include <string>\n@@ -24,19 +25,33 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/runtime/command_buffer_cmd.h\"\n+#include \"xla/backends/gpu/runtime/command_buffer_cmd_emitter.h\"\n+#include \"xla/backends/gpu/runtime/command_buffer_thunk.h\"\n+#include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/protobuf.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -247,5 +262,164 @@ TEST(KernelThunkTest, ToAndFromProto) {\n               ::testing::ElementsAre(slice0, slice1));\n   EXPECT_THAT(reconstructed_thunk->tma_metadata(), tma_metadata);\n }\n+\n+class KernelThunkTmaPTXTest : public ::testing::TestWithParam<bool> {\n+ public:\n+  absl::StatusOr<std::unique_ptr<KernelThunk>> GetTmaKernelThunk() {\n+    std::string tma_kernel_thunk = R\"pb(\n+      thunk_info { profile_annotation: \"tma_kernel\" execution_stream_id: 123 }\n+      kernel_thunk {\n+        args { size: 1048576 buffer_allocation_index: 0 }\n+        args { size: 1048576 offset: 1048576 }\n+        args { size: 4194304 offset: 2097152 }\n+        written: false\n+        written: false\n+        written: true\n+        kernel_name: \"tma_dot_kernel\"\n+        launch_dimensions {\n+          block_counts { coordinates { x: 4096 y: 1 z: 1 } }\n+          thread_counts_per_block { coordinates { x: 128 y: 1 z: 1 } }\n+        }\n+        shmem_bytes: 24600\n+        tma_metadata {\n+          arg_index_to_tma_info {\n+            key: 0\n+            value {\n+              element_size: 2\n+              global_dims: 512\n+              global_dims: 1024\n+              global_strides: 1024\n+              box_dims: 64\n+              box_dims: 16\n+              element_strides: 1\n+              element_strides: 1\n+              swizzle: SWIZZLE_BYTES128\n+              l2_promotion: L2_PROMOTION_BYTES128\n+            }\n+          }\n+          arg_index_to_tma_info {\n+            key: 1\n+            value {\n+              element_size: 2\n+              global_dims: 1024\n+              global_dims: 512\n+              global_strides: 2048\n+              box_dims: 16\n+              box_dims: 128\n+              element_strides: 1\n+              element_strides: 1\n+              swizzle: SWIZZLE_BYTES32\n+              l2_promotion: L2_PROMOTION_BYTES128\n+            }\n+          }\n+          arg_index_to_tma_info {\n+            key: 2\n+            value {\n+              element_size: 4\n+              global_dims: 1024\n+              global_dims: 1024\n+              global_strides: 4096\n+              box_dims: 16\n+              box_dims: 16\n+              element_strides: 1\n+              element_strides: 1\n+              swizzle: SWIZZLE_BYTES64\n+              l2_promotion: L2_PROMOTION_BYTES128\n+            }\n+          }\n+        }\n+      }\n+    )pb\";\n+\n+    ThunkProto tma_kernel_thunk_proto;\n+    tsl::protobuf::TextFormat::ParseFromString(tma_kernel_thunk,\n+                                               &tma_kernel_thunk_proto);\n+\n+    const size_t total_byte_size =\n+        tma_kernel_thunk_proto.kernel_thunk().args(0).size() +\n+        tma_kernel_thunk_proto.kernel_thunk().args(1).size() +\n+        tma_kernel_thunk_proto.kernel_thunk().args(2).size();\n+    allocations_.emplace_back(0, total_byte_size, 0);\n+\n+    return KernelThunk::FromProto(Thunk::ThunkInfo(),\n+                                  tma_kernel_thunk_proto.kernel_thunk(),\n+                                  allocations_);\n+  }\n+\n+  std::vector<BufferAllocation> allocations_;\n+};\n+\n+TEST_P(KernelThunkTmaPTXTest, TmaPTX) {\n+  TF_ASSERT_OK_AND_ASSIGN(se::Platform * platform,\n+                          se::PlatformManager::PlatformWithName(\"cuda\"));\n+  TF_ASSERT_OK_AND_ASSIGN(se::StreamExecutor * executor,\n+                          platform->ExecutorForDevice(0));\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n+                          executor->CreateStream());\n+  if (!stream_executor::gpu::IsTmaAvailableForDevice(\n+          executor->GetDeviceDescription())) {\n+    GTEST_SKIP() << \"TMA is not supported on this platform.\";\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<KernelThunk> kernel_thunk,\n+                          GetTmaKernelThunk());\n+\n+  Thunk::ExecutableSource executable_source;\n+  executable_source.text = stream_executor::gpu::GetTmaPtxKernelSpec()\n+                               .cuda_ptx_in_memory()\n+                               .value()\n+                               .ptx;\n+  Thunk::InitializeParams initialize_params;\n+  initialize_params.executor = executor;\n+  initialize_params.src = executable_source;\n+  initialize_params.stream = stream.get();\n+  BufferAllocations buffer_allocations(\n+      /*buffers=*/{executor->Allocate(allocations_[0].size())},\n+      /*device_ordinal=*/0,\n+      /*memory_allocator=*/nullptr);\n+\n+  initialize_params.buffer_allocations = &buffer_allocations;\n+\n+  ServiceExecutableRunOptions run_options;\n+  run_options.mutable_run_options()->set_stream(stream.get());\n+\n+  auto execute_params =\n+      Thunk::ExecuteParams::Create(run_options, buffer_allocations,\n+                                   stream.get(), nullptr, nullptr, nullptr, {});\n+\n+  const bool use_command_buffer = GetParam();\n+\n+  // We are checking both code paths for TMA kernels: through KernelThunk and\n+  // CommandBufferThunk.\n+  if (use_command_buffer) {\n+    ThunkSequence thunk_sequence;\n+    thunk_sequence.push_back(std::move(kernel_thunk));\n+\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        CommandBufferCmdExecutor cmds,\n+        ConvertToCommands(thunk_sequence, ConvertToCommandsOptions()));\n+    auto sequential_thunk = std::make_unique<SequentialThunk>(\n+        Thunk::ThunkInfo(), std::move(thunk_sequence));\n+\n+    auto cmd_buffer_thunk = std::make_unique<CommandBufferThunk>(\n+        std::move(cmds), Thunk::ThunkInfo(), std::move(sequential_thunk), true);\n+\n+    TF_ASSERT_OK(cmd_buffer_thunk->Initialize(initialize_params));\n+    TF_ASSERT_OK(cmd_buffer_thunk->ExecuteOnStream(execute_params));\n+  } else {\n+    TF_ASSERT_OK(kernel_thunk->Initialize(initialize_params));\n+    TF_ASSERT_OK(kernel_thunk->ExecuteOnStream(execute_params));\n+  }\n+\n+  TF_ASSERT_OK(stream->BlockHostUntilDone());\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(KernelThunkTmaPTXTestSuite, KernelThunkTmaPTXTest,\n+                         ::testing::Bool(),\n+                         [](const ::testing::TestParamInfo<bool>& info) {\n+                           return info.param ? \"in_command_buffer\"\n+                                             : \"in_kernel_thunk\";\n+                         });\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "af95cd1057a9d82c8c71c535469b7d0547cb0852",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -514,8 +514,6 @@ cc_library(\n     deps = [\n         \":gpu_kernel_registry\",\n         \":gpu_test_kernel_traits\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -573,7 +571,9 @@ xla_test(\n     deps = [\n         \":gpu_test_kernels\",\n         \":gpu_test_kernels_fatbin\",\n+        \":tma_metadata\",\n         \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:command_buffer\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_spec\",\n@@ -593,6 +593,7 @@ xla_test(\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n "
        },
        {
            "sha": "e5add3d5587a84eb9608acf1ee8ca774a572fdbc",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_kernel_test.cc",
            "status": "modified",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_test.cc?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -27,9 +27,11 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels_fatbin.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -42,13 +44,15 @@ limitations under the License.\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"tsl/platform/protobuf.h\"\n \n namespace stream_executor::gpu {\n namespace {\n \n using AddI32Kernel =\n     TypedKernelFactory<DeviceMemory<int32_t>, DeviceMemory<int32_t>,\n                        DeviceMemory<int32_t>>;\n+using TmaKernel = TypedKernelFactory<TensorMap, TensorMap, TensorMap>;\n \n class GpuKernelTest : public ::testing::Test {\n  public:\n@@ -140,5 +144,87 @@ TEST_F(GpuKernelTest, ArrayArgByValue) {\n \n   EXPECT_THAT(dst_host, ::testing::ElementsAreArray(storage));\n }\n+\n+TEST_F(GpuKernelTest, TmaLoadAndRunKernelFromPtx) {\n+  if (!IsTmaAvailableForDevice(executor_->GetDeviceDescription())) {\n+    GTEST_SKIP() << \"TMA is not supported on this platform.\";\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto stream, executor_->CreateStream());\n+  TF_ASSERT_OK_AND_ASSIGN(auto tma_kernel,\n+                          TmaKernel::Create(executor_, GetTmaPtxKernelSpec()));\n+\n+  auto get_tma_descriptor_from_proto =\n+      [](absl::string_view proto) -> absl::StatusOr<TmaDescriptor> {\n+    TmaDescriptorProto tma_descriptor_proto;\n+    tsl::protobuf::TextFormat::ParseFromString(proto, &tma_descriptor_proto);\n+    return TmaDescriptor::FromProto(tma_descriptor_proto);\n+  };\n+\n+  TF_ASSERT_OK_AND_ASSIGN(TmaDescriptor arg0_desc,\n+                          get_tma_descriptor_from_proto(\n+                              R\"pb(\n+                                element_size: 2\n+                                global_dims: 512\n+                                global_dims: 1024\n+                                global_strides: 1024\n+                                box_dims: 64\n+                                box_dims: 16\n+                                element_strides: 1\n+                                element_strides: 1\n+                                swizzle: SWIZZLE_BYTES128\n+                                l2_promotion: L2_PROMOTION_BYTES128\n+                              )pb\"));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(TmaDescriptor arg1_desc,\n+                          get_tma_descriptor_from_proto(\n+                              R\"pb(\n+                                element_size: 2\n+                                global_dims: 1024\n+                                global_dims: 512\n+                                global_strides: 2048\n+                                box_dims: 16\n+                                box_dims: 128\n+                                element_strides: 1\n+                                element_strides: 1\n+                                swizzle: SWIZZLE_BYTES32\n+                                l2_promotion: L2_PROMOTION_BYTES128\n+                              )pb\"));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(TmaDescriptor arg2_desc,\n+                          get_tma_descriptor_from_proto(\n+                              R\"pb(\n+                                element_size: 4\n+                                global_dims: 1024\n+                                global_dims: 1024\n+                                global_strides: 4096\n+                                box_dims: 16\n+                                box_dims: 16\n+                                element_strides: 1\n+                                element_strides: 1\n+                                swizzle: SWIZZLE_BYTES64\n+                                l2_promotion: L2_PROMOTION_BYTES128\n+                              )pb\"));\n+\n+  DeviceMemory<int16_t> mem0 = executor_->AllocateArray<int16_t>(512 * 1024);\n+  DeviceMemory<int16_t> mem1 = executor_->AllocateArray<int16_t>(1024 * 512);\n+  DeviceMemory<int32_t> mem2 = executor_->AllocateArray<int32_t>(1024 * 1024);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto tma0,\n+                          executor_->CreateTensorMap(arg0_desc, mem0.opaque()));\n+  TF_ASSERT_OK_AND_ASSIGN(auto tma1,\n+                          executor_->CreateTensorMap(arg1_desc, mem1.opaque()));\n+  TF_ASSERT_OK_AND_ASSIGN(auto tma2,\n+                          executor_->CreateTensorMap(arg2_desc, mem2.opaque()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<KernelArgs> packed_args,\n+      stream_executor::PackKernelArgs(\n+          absl::Span<const stream_executor::KernelArgument>({tma0, tma1, tma2}),\n+          tma_kernel->metadata()));\n+  TF_ASSERT_OK(\n+      tma_kernel->Launch(ThreadDim(), BlockDim(), stream.get(), *packed_args));\n+  TF_ASSERT_OK(stream->BlockHostUntilDone());\n+}\n+\n }  // namespace\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "cf73db5a5241c3c628407cc008fa9b5116a234cc",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_test_kernels.cc",
            "status": "modified",
            "additions": 617,
            "deletions": 0,
            "changes": 617,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.cc?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -111,4 +111,621 @@ KernelLoaderSpec GetAddI32PtxKernelSpec() {\n   return KernelLoaderSpec::CreateCudaPtxInMemorySpec(kAddI32KernelPtx, \"AddI32\",\n                                                      3);\n }\n+\n+KernelLoaderSpec GetTmaPtxKernelSpec() {\n+  // PTX kernel compiled from\n+  // https://github.com/jax-ml/jax/blob/739dbd3c52872e43098e28d3318b8f5f597b159d/tests/pallas/pallas_test.py#L547\n+  // test configuration:\n+  // m_1024_n_1024_k_512_dtype_float16_bm_128_bn_128_bk_32_gm_8\n+  // autotuner config:\n+  // --xla_gpu_override_gemm_autotuner='16 block_n: 16 block_k: 128 split_k: 1\n+  // num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true '\n+  static constexpr absl::string_view kTmaKernelPtx = R\"(\n+.version 8.2\n+.target sm_90a\n+.address_size 64\n+\n+    // .globl    tma_dot_kernel\n+.extern .shared .align 16 .b8 global_smem[24600];\n+\n+.visible .entry tma_dot_kernel(\n+    .param .align 64 .b8 tma_dot_kernel_param_0[128],\n+    .param .align 64 .b8 tma_dot_kernel_param_1[128],\n+    .param .align 64 .b8 tma_dot_kernel_param_2[128]\n+)\n+.reqntid 128, 1, 1\n+{\n+    .reg .pred     %p<46>;\n+    .reg .b32     %r<819>;\n+    .reg .b64     %rd<47>;\n+\n+    mov.b64     %rd16, tma_dot_kernel_param_0;\n+    mov.b64     %rd17, tma_dot_kernel_param_1;\n+    mov.b64     %rd18, tma_dot_kernel_param_2;\n+    cvta.param.u64     %rd15, %rd18;\n+    cvta.param.u64     %rd4, %rd17;\n+    cvta.param.u64     %rd3, %rd16;\n+    mov.u32     %r769, %ctaid.x;\n+    shr.u32     %r770, %r769, 2;\n+    and.b32     %r7, %r770, 536870896;\n+    shl.b32     %r771, %r769, 4;\n+    and.b32     %r10, %r771, 1008;\n+    mov.u32     %r772, %tid.x;\n+    setp.eq.b32     %p1, %r772, 0;\n+    mov.b64     %rd19, global_smem;\n+    cvt.u32.u64     %r768, %rd19;\n+    add.s32     %r4, %r768, 24576;\n+    // begin inline asm\n+    @%p1 mbarrier.init.shared::cta.b64 [%r4], 1;\n+    // end inline asm\n+    bar.sync     0;\n+    add.s32     %r13, %r768, 24584;\n+    // begin inline asm\n+    @%p1 mbarrier.init.shared::cta.b64 [%r13], 1;\n+    // end inline asm\n+    bar.sync     0;\n+    add.s32     %r22, %r768, 24592;\n+    // begin inline asm\n+    @%p1 mbarrier.init.shared::cta.b64 [%r22], 1;\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p1 mbarrier.arrive.expect_tx.shared.b64 _, [%r4], 8192;\n+    // end inline asm\n+    bar.sync     0;\n+    shr.u32     %r773, %r772, 5;\n+    shfl.sync.idx.b32     %r774, %r773, 0, 31, -1;\n+    elect.sync     %r775|%p29, -1;\n+    setp.lt.u32     %p30, %r772, 64;\n+    and.pred     %p5, %p30, %p29;\n+    and.b32     %r776, %r774, 1;\n+    shl.b32     %r777, %r776, 10;\n+    mul.wide.u32     %rd20, %r777, 2;\n+    add.s64     %rd21, %rd19, %rd20;\n+    shl.b32     %r6, %r776, 6;\n+    cvt.u32.u64     %r5, %rd21;\n+    // begin inline asm\n+    @%p5 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r5], [%rd3, {%r6, %r7}], [%r4];\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r778|%p31, -1;\n+    setp.lt.u32     %p32, %r772, 32;\n+    and.pred     %p6, %p32, %p31;\n+    add.s64     %rd22, %rd19, 12288;\n+    cvt.u32.u64     %r9, %rd22;\n+    mov.b32     %r11, 0;\n+    // begin inline asm\n+    @%p6 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r9], [%rd4, {%r10, %r11}], [%r4];\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p1 mbarrier.arrive.expect_tx.shared.b64 _, [%r13], 8192;\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r779|%p33, -1;\n+    and.pred     %p8, %p30, %p33;\n+    add.s64     %rd23, %rd19, 4096;\n+    add.s64     %rd24, %rd23, %rd20;\n+    or.b32     %r15, %r6, 128;\n+    cvt.u32.u64     %r14, %rd24;\n+    // begin inline asm\n+    @%p8 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r14], [%rd3, {%r15, %r7}], [%r13];\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r780|%p34, -1;\n+    and.pred     %p9, %p32, %p34;\n+    add.s32     %r393, %r768, 16384;\n+    mov.b32     %r20, 128;\n+    // begin inline asm\n+    @%p9 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r393], [%rd4, {%r10, %r20}], [%r13];\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p1 mbarrier.arrive.expect_tx.shared.b64 _, [%r22], 8192;\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r781|%p35, -1;\n+    and.pred     %p11, %p30, %p35;\n+    add.s64     %rd25, %rd19, 8192;\n+    add.s64     %rd26, %rd25, %rd20;\n+    or.b32     %r24, %r6, 256;\n+    cvt.u32.u64     %r23, %rd26;\n+    // begin inline asm\n+    @%p11 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r23], [%rd3, {%r24, %r7}], [%r22];\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r782|%p36, -1;\n+    and.pred     %p12, %p32, %p36;\n+    add.s32     %r576, %r768, 20480;\n+    mov.b32     %r29, 256;\n+    // begin inline asm\n+    @%p12 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r576], [%rd4, {%r10, %r29}], [%r22];\n+    // end inline asm\n+    shl.b32     %r783, %r772, 7;\n+    and.b32     %r784, %r783, 1920;\n+    shl.b32     %r785, %r772, 4;\n+    and.b32     %r786, %r785, 112;\n+    or.b32     %r787, %r784, %r786;\n+    and.b32     %r788, %r772, 16;\n+    xor.b32     %r789, %r787, %r788;\n+    cvt.u64.u32     %rd27, %r789;\n+    xor.b32     %r790, %r789, 32;\n+    cvt.u64.u32     %rd28, %r790;\n+    xor.b32     %r791, %r789, 64;\n+    cvt.u64.u32     %rd29, %r791;\n+    xor.b32     %r792, %r789, 96;\n+    cvt.u64.u32     %rd30, %r792;\n+    shl.b32     %r793, %r772, 5;\n+    and.b32     %r794, %r793, 864;\n+    bfe.s32     %r795, %r772, 2, 1;\n+    and.b32     %r796, %r795, 144;\n+    or.b32     %r797, %r796, %r794;\n+    shr.u32     %r798, %r772, 1;\n+    and.b32     %r799, %r798, 16;\n+    xor.b32     %r800, %r797, %r799;\n+    cvt.u64.u32     %rd31, %r800;\n+    add.s64     %rd32, %rd22, %rd31;\n+    bar.sync     0;\n+    // begin inline asm\n+    \n+{\n+    .reg .pred complete;\n+    waitLoop:\n+    mbarrier.try_wait.parity.shared.b64 complete, [%r4], %r11;\n+    @!complete bra.uni waitLoop;\n+}\n+\n+    // end inline asm\n+    add.s64     %rd33, %rd19, %rd27;\n+    cvt.u32.u64     %r37, %rd33;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r101, %r102, %r103, %r104}, [%r37];\n+    // end inline asm\n+    add.s32     %r591, %r37, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r157, %r158, %r159, %r160}, [%r591];\n+    // end inline asm\n+    add.s64     %rd34, %rd19, %rd28;\n+    cvt.u32.u64     %r47, %rd34;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r115, %r116, %r117, %r118}, [%r47];\n+    // end inline asm\n+    add.s32     %r601, %r47, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r171, %r172, %r173, %r174}, [%r601];\n+    // end inline asm\n+    add.s64     %rd35, %rd19, %rd29;\n+    cvt.u32.u64     %r57, %rd35;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r129, %r130, %r131, %r132}, [%r57];\n+    // end inline asm\n+    add.s32     %r611, %r57, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r185, %r186, %r187, %r188}, [%r611];\n+    // end inline asm\n+    add.s64     %rd36, %rd19, %rd30;\n+    cvt.u32.u64     %r67, %rd36;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r143, %r144, %r145, %r146}, [%r67];\n+    // end inline asm\n+    add.s32     %r621, %r67, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r199, %r200, %r201, %r202}, [%r621];\n+    // end inline asm\n+    cvt.u32.u64     %r77, %rd32;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r105, %r106, %r119, %r120}, [%r77];\n+    // end inline asm\n+    add.s32     %r631, %r77, 1024;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r133, %r134, %r147, %r148}, [%r631];\n+    // end inline asm\n+    add.s32     %r636, %r77, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r161, %r162, %r175, %r176}, [%r636];\n+    // end inline asm\n+    add.s32     %r641, %r77, 3072;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r189, %r190, %r203, %r204}, [%r641];\n+    // end inline asm\n+    mov.b32     %r107, %r11;\n+    mov.b32     %r108, %r11;\n+    mov.b32     %r109, %r11;\n+    mov.b32     %r110, %r11;\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r101, %r102, %r103, %r104 }, { %r105, %r106 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r115, %r116, %r117, %r118 }, { %r119, %r120 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r129, %r130, %r131, %r132 }, { %r133, %r134 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r143, %r144, %r145, %r146 }, { %r147, %r148 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r157, %r158, %r159, %r160 }, { %r161, %r162 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r171, %r172, %r173, %r174 }, { %r175, %r176 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r185, %r186, %r187, %r188 }, { %r189, %r190 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r199, %r200, %r201, %r202 }, { %r203, %r204 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p1 mbarrier.arrive.expect_tx.shared.b64 _, [%r4], 8192;\n+    // end inline asm\n+    // begin inline asm\n+    fence.proxy.async.shared::cta;\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r801|%p37, -1;\n+    and.pred     %p14, %p30, %p37;\n+    or.b32     %r207, %r6, 384;\n+    // begin inline asm\n+    @%p14 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r5], [%rd3, {%r207, %r7}], [%r4];\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r802|%p38, -1;\n+    and.pred     %p15, %p32, %p38;\n+    mov.b32     %r212, 384;\n+    // begin inline asm\n+    @%p15 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r9], [%rd4, {%r10, %r212}], [%r4];\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    \n+{\n+    .reg .pred complete;\n+    waitLoop:\n+    mbarrier.try_wait.parity.shared.b64 complete, [%r13], %r11;\n+    @!complete bra.uni waitLoop;\n+}\n+\n+    // end inline asm\n+    add.s64     %rd37, %rd23, %rd27;\n+    cvt.u32.u64     %r220, %rd37;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r284, %r285, %r286, %r287}, [%r220];\n+    // end inline asm\n+    add.s32     %r225, %r220, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r340, %r341, %r342, %r343}, [%r225];\n+    // end inline asm\n+    add.s64     %rd38, %rd23, %rd28;\n+    cvt.u32.u64     %r230, %rd38;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r298, %r299, %r300, %r301}, [%r230];\n+    // end inline asm\n+    add.s32     %r235, %r230, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r354, %r355, %r356, %r357}, [%r235];\n+    // end inline asm\n+    add.s64     %rd39, %rd23, %rd29;\n+    cvt.u32.u64     %r240, %rd39;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r312, %r313, %r314, %r315}, [%r240];\n+    // end inline asm\n+    add.s32     %r245, %r240, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r368, %r369, %r370, %r371}, [%r245];\n+    // end inline asm\n+    add.s64     %rd40, %rd23, %rd30;\n+    cvt.u32.u64     %r250, %rd40;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r326, %r327, %r328, %r329}, [%r250];\n+    // end inline asm\n+    add.s32     %r255, %r250, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r382, %r383, %r384, %r385}, [%r255];\n+    // end inline asm\n+    add.s32     %r260, %r77, 4096;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r288, %r289, %r302, %r303}, [%r260];\n+    // end inline asm\n+    add.s32     %r265, %r77, 5120;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r316, %r317, %r330, %r331}, [%r265];\n+    // end inline asm\n+    add.s32     %r270, %r77, 6144;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r344, %r345, %r358, %r359}, [%r270];\n+    // end inline asm\n+    add.s32     %r275, %r77, 7168;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r372, %r373, %r386, %r387}, [%r275];\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r284, %r285, %r286, %r287 }, { %r288, %r289 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r298, %r299, %r300, %r301 }, { %r302, %r303 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r312, %r313, %r314, %r315 }, { %r316, %r317 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r326, %r327, %r328, %r329 }, { %r330, %r331 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r340, %r341, %r342, %r343 }, { %r344, %r345 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r354, %r355, %r356, %r357 }, { %r358, %r359 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r368, %r369, %r370, %r371 }, { %r372, %r373 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r382, %r383, %r384, %r385 }, { %r386, %r387 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    bar.sync     0;\n+    mov.pred     %p16, 0;\n+    // begin inline asm\n+    @%p16 mbarrier.arrive.expect_tx.shared.b64 _, [%r13], 8192;\n+    // end inline asm\n+    // begin inline asm\n+    fence.proxy.async.shared::cta;\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r803|%p39, -1;\n+    or.b32     %r390, %r6, 512;\n+    add.s32     %r389, %r5, 4096;\n+    // begin inline asm\n+    @%p16 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r389], [%rd3, {%r390, %r7}], [%r13];\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r804|%p40, -1;\n+    mov.b32     %r395, 512;\n+    // begin inline asm\n+    @%p16 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r393], [%rd4, {%r10, %r395}], [%r13];\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    \n+{\n+    .reg .pred complete;\n+    waitLoop:\n+    mbarrier.try_wait.parity.shared.b64 complete, [%r22], %r11;\n+    @!complete bra.uni waitLoop;\n+}\n+\n+    // end inline asm\n+    add.s64     %rd41, %rd25, %rd27;\n+    cvt.u32.u64     %r403, %rd41;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r467, %r468, %r469, %r470}, [%r403];\n+    // end inline asm\n+    add.s32     %r408, %r403, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r523, %r524, %r525, %r526}, [%r408];\n+    // end inline asm\n+    add.s64     %rd42, %rd25, %rd28;\n+    cvt.u32.u64     %r413, %rd42;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r481, %r482, %r483, %r484}, [%r413];\n+    // end inline asm\n+    add.s32     %r418, %r413, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r537, %r538, %r539, %r540}, [%r418];\n+    // end inline asm\n+    add.s64     %rd43, %rd25, %rd29;\n+    cvt.u32.u64     %r423, %rd43;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r495, %r496, %r497, %r498}, [%r423];\n+    // end inline asm\n+    add.s32     %r428, %r423, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r551, %r552, %r553, %r554}, [%r428];\n+    // end inline asm\n+    add.s64     %rd44, %rd25, %rd30;\n+    cvt.u32.u64     %r433, %rd44;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r509, %r510, %r511, %r512}, [%r433];\n+    // end inline asm\n+    add.s32     %r438, %r433, 2048;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r565, %r566, %r567, %r568}, [%r438];\n+    // end inline asm\n+    add.s32     %r443, %r77, 8192;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r471, %r472, %r485, %r486}, [%r443];\n+    // end inline asm\n+    add.s32     %r448, %r77, 9216;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r499, %r500, %r513, %r514}, [%r448];\n+    // end inline asm\n+    add.s32     %r453, %r77, 10240;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r527, %r528, %r541, %r542}, [%r453];\n+    // end inline asm\n+    add.s32     %r458, %r77, 11264;\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r555, %r556, %r569, %r570}, [%r458];\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r467, %r468, %r469, %r470 }, { %r471, %r472 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r481, %r482, %r483, %r484 }, { %r485, %r486 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r495, %r496, %r497, %r498 }, { %r499, %r500 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r509, %r510, %r511, %r512 }, { %r513, %r514 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r523, %r524, %r525, %r526 }, { %r527, %r528 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r537, %r538, %r539, %r540 }, { %r541, %r542 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r551, %r552, %r553, %r554 }, { %r555, %r556 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r565, %r566, %r567, %r568 }, { %r569, %r570 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p16 mbarrier.arrive.expect_tx.shared.b64 _, [%r22], 8192;\n+    // end inline asm\n+    // begin inline asm\n+    fence.proxy.async.shared::cta;\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r805|%p41, -1;\n+    or.b32     %r573, %r6, 640;\n+    add.s32     %r572, %r5, 8192;\n+    // begin inline asm\n+    @%p16 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r572], [%rd3, {%r573, %r7}], [%r22];\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r806|%p42, -1;\n+    mov.b32     %r578, 640;\n+    // begin inline asm\n+    @%p16 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r576], [%rd4, {%r10, %r578}], [%r22];\n+    // end inline asm\n+    bar.sync     0;\n+    mov.b32     %r581, 1;\n+    // begin inline asm\n+    \n+{\n+    .reg .pred complete;\n+    waitLoop:\n+    mbarrier.try_wait.parity.shared.b64 complete, [%r4], %r581;\n+    @!complete bra.uni waitLoop;\n+}\n+\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r650, %r651, %r652, %r653}, [%r37];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r706, %r707, %r708, %r709}, [%r591];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r664, %r665, %r666, %r667}, [%r47];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r720, %r721, %r722, %r723}, [%r601];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r678, %r679, %r680, %r681}, [%r57];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r734, %r735, %r736, %r737}, [%r611];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r692, %r693, %r694, %r695}, [%r67];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r748, %r749, %r750, %r751}, [%r621];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r654, %r655, %r668, %r669}, [%r77];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r682, %r683, %r696, %r697}, [%r631];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r710, %r711, %r724, %r725}, [%r636];\n+    // end inline asm\n+    // begin inline asm\n+    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r738, %r739, %r752, %r753}, [%r641];\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r650, %r651, %r652, %r653 }, { %r654, %r655 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r664, %r665, %r666, %r667 }, { %r668, %r669 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r678, %r679, %r680, %r681 }, { %r682, %r683 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r692, %r693, %r694, %r695 }, { %r696, %r697 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r706, %r707, %r708, %r709 }, { %r710, %r711 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r720, %r721, %r722, %r723 }, { %r724, %r725 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r734, %r735, %r736, %r737 }, { %r738, %r739 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    // begin inline asm\n+    mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r107, %r108, %r109, %r110 }, { %r748, %r749, %r750, %r751 }, { %r752, %r753 }, { %r107, %r108, %r109, %r110 };\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p16 mbarrier.arrive.expect_tx.shared.b64 _, [%r4], 8192;\n+    // end inline asm\n+    // begin inline asm\n+    fence.proxy.async.shared::cta;\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r807|%p43, -1;\n+    or.b32     %r756, %r6, 768;\n+    // begin inline asm\n+    @%p16 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r5], [%rd3, {%r756, %r7}], [%r4];\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r808|%p44, -1;\n+    mov.b32     %r761, 768;\n+    // begin inline asm\n+    @%p16 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r9], [%rd4, {%r10, %r761}], [%r4];\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p1 mbarrier.inval.shared::cta.b64 [%r4];\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p1 mbarrier.inval.shared::cta.b64 [%r13];\n+    // end inline asm\n+    bar.sync     0;\n+    // begin inline asm\n+    @%p1 mbarrier.inval.shared::cta.b64 [%r22];\n+    // end inline asm\n+    and.b32     %r809, %r785, 448;\n+    shl.b32     %r810, %r772, 3;\n+    and.b32     %r811, %r810, 24;\n+    or.b32     %r812, %r809, %r811;\n+    shl.b32     %r813, %r772, 1;\n+    and.b32     %r814, %r813, 48;\n+    and.b32     %r815, %r772, 32;\n+    xor.b32     %r816, %r814, %r815;\n+    xor.b32     %r817, %r816, %r812;\n+    cvt.u64.u32     %rd45, %r817;\n+    add.s64     %rd46, %rd19, %rd45;\n+    st.shared.v2.b32     [%rd46], {%r107, %r108};\n+    st.shared.v2.b32     [%rd46+512], {%r109, %r110};\n+    // begin inline asm\n+    fence.proxy.async.shared::cta;\n+    // end inline asm\n+    bar.sync     0;\n+    elect.sync     %r818|%p45, -1;\n+    and.pred     %p28, %p32, %p45;\n+    // begin inline asm\n+    @%p28 cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%rd15, {%r10, %r7}], [%r768];\n+    // end inline asm\n+    cp.async.bulk.commit_group;\n+    cp.async.bulk.wait_group.read     0;\n+    bar.sync     0;\n+    ret;\n+\n+}\n+)\";\n+\n+  return KernelLoaderSpec::CreateCudaPtxInMemorySpec(kTmaKernelPtx,\n+                                                     \"tma_dot_kernel\", 3);\n+}  // NOLINT\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "ab16b7f3a02d2de20475e3770c0a5068a0cf4f64",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_test_kernels.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b8d3ad14b482d60af398cf2b7cbae9612cfb5d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.h?ref=26b8d3ad14b482d60af398cf2b7cbae9612cfb5d",
            "patch": "@@ -52,6 +52,9 @@ absl::StatusOr<KernelLoaderSpec> GetAddI32TestKernelSpec(\n // Returns a PTX kernel loader spec for the `AddI32` PTX kernel above.\n KernelLoaderSpec GetAddI32PtxKernelSpec();\n \n+// Returns TMA test kernel loaded from PTX.\n+KernelLoaderSpec GetTmaPtxKernelSpec();\n+\n }  // namespace stream_executor::gpu\n \n #endif  // XLA_STREAM_EXECUTOR_GPU_GPU_TEST_KERNELS_H_"
        }
    ],
    "stats": {
        "total": 958,
        "additions": 942,
        "deletions": 16
    }
}