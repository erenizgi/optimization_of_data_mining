{
    "author": "hhb",
    "message": "Improve the performance in `StreamExecutorGpuTopologyDescription::LogicalDeviceOfDefaultTypeForId`\n\nThis change introduces a `CreateDeviceDescription` helper function to generate a single device description. This allows `LogicalDeviceOfDefaultTypeForId` to directly create the description for a given device ID instead of creating and iterating through all possible device descriptions, improving lookup performance.\n\nPiperOrigin-RevId: 832105450",
    "sha": "ff993fcf5e36723d3086bebb4c9b701305a938ea",
    "files": [
        {
            "sha": "44d9dff1882690f0dcc9389bb6f21ba9c3e3b60b",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_topology_description.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 64,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff993fcf5e36723d3086bebb4c9b701305a938ea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff993fcf5e36723d3086bebb4c9b701305a938ea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.cc?ref=ff993fcf5e36723d3086bebb4c9b701305a938ea",
            "patch": "@@ -77,61 +77,65 @@ StreamExecutorGpuTopologyDescription::DeviceDescriptions() const {\n     return devices;\n   }\n   devices.reserve(gpu_topology_->number_of_devices());\n+  for (int device_id = 0; device_id < gpu_topology_->number_of_devices();\n+       ++device_id) {\n+    devices.push_back(CreateDeviceDescription(device_id));\n+  }\n+  return devices;\n+}\n+\n+std::unique_ptr<PjRtStreamExecutorDeviceDescription>\n+StreamExecutorGpuTopologyDescription::CreateDeviceDescription(\n+    int device_id) const {\n   // Instead of \"host\", we use \"process\", as it's more accurate and consistent\n   // with PjRt terminology. In a multi-process setting, a host can have multiple\n   // processes, e.g., one process per GPU.\n   const int32_t num_devices_per_process = gpu_topology_->num_devices_per_host();\n   const int32_t num_processes_per_partition =\n       gpu_topology_->num_hosts_per_partition();\n-  for (int device_id = 0; device_id < gpu_topology_->number_of_devices();\n-       ++device_id) {\n-    // The local_device_id, process_index and partition_index are inferred from\n-    // the global device id. It requires the global topology is symmetric:\n-    //  - all partitions have the same number of processes.\n-    //  - all processes have the same number of devices.\n-    //  - processes of the same partition are adjacent to each other.\n-    //\n-    // And it also requires the ids assignments follows the PjRt topology\n-    // exchange protocol in xla/pjrt/distributed/topology_util.cc:\n-    //  - ids are densely assigned and start from 0\n-    //  - from lower process index to higher process index\n-    //  - within the process, from lower device ordinal to higher device ordinal\n-    //\n-    // If the above requirements are not met, users should get the device\n-    // description by looking up individual device from PjRt client.\n-    const int local_device_id = num_devices_per_process == -1\n-                                    ? 0\n-                                    : (device_id % num_devices_per_process);\n-    const int process_index = num_devices_per_process == -1\n-                                  ? 0\n-                                  : (device_id / num_devices_per_process);\n-    const int process_index_in_partition =\n-        process_index == -1 ? 0 : (process_index % num_processes_per_partition);\n-    const int partition_index =\n-        num_processes_per_partition == -1\n-            ? 0\n-            : (process_index / num_processes_per_partition);\n-    auto description = std::make_unique<PjRtStreamExecutorDeviceDescription>(\n-        device_id, local_device_id, process_index, process_index_in_partition,\n-        partition_index, std::string(platform_version()));\n-    if (target_config_.has_value()) {\n-      std::string compute_capability = \"<unknown compute-capability>\";\n-      std::string gpu_vendor = \"<unknown gpu vendor>\";\n-      if (target_config_->gpu_device_info().has_cuda_compute_capability()) {\n-        const auto& cap =\n-            target_config_->gpu_device_info().cuda_compute_capability();\n-        compute_capability = absl::StrCat(cap.major(), \".\", cap.minor());\n-        gpu_vendor = \"NVIDIA Corporation\";\n-      }\n-\n-      StreamExecutorGpuTopologyDescription::SetupDeviceDescription(\n-          *description, gpu_vendor, compute_capability,\n-          target_config_->gpu_device_info().core_count(),\n-          target_config_->gpu_device_info().shared_memory_per_block_optin(), 0);\n+  // The local_device_id, process_index and partition_index are inferred from\n+  // the global device id. It requires the global topology is symmetric:\n+  //  - all partitions have the same number of processes.\n+  //  - all processes have the same number of devices.\n+  //  - processes of the same partition are adjacent to each other.\n+  //\n+  // And it also requires the ids assignments follows the PjRt topology\n+  // exchange protocol in xla/pjrt/distributed/topology_util.cc:\n+  //  - ids are densely assigned and start from 0\n+  //  - from lower process index to higher process index\n+  //  - within the process, from lower device ordinal to higher device ordinal\n+  //\n+  // If the above requirements are not met, users should get the device\n+  // description by looking up individual device from PjRt client.\n+  const int local_device_id =\n+      num_devices_per_process == -1 ? 0 : (device_id % num_devices_per_process);\n+  const int process_index =\n+      num_devices_per_process == -1 ? 0 : (device_id / num_devices_per_process);\n+  const int process_index_in_partition =\n+      process_index == -1 ? 0 : (process_index % num_processes_per_partition);\n+  const int partition_index =\n+      num_processes_per_partition == -1\n+          ? 0\n+          : (process_index / num_processes_per_partition);\n+  auto description = std::make_unique<PjRtStreamExecutorDeviceDescription>(\n+      device_id, local_device_id, process_index, process_index_in_partition,\n+      partition_index, std::string(platform_version()));\n+  if (target_config_.has_value()) {\n+    std::string compute_capability = \"<unknown compute-capability>\";\n+    std::string gpu_vendor = \"<unknown gpu vendor>\";\n+    if (target_config_->gpu_device_info().has_cuda_compute_capability()) {\n+      const auto& cap =\n+          target_config_->gpu_device_info().cuda_compute_capability();\n+      compute_capability = absl::StrCat(cap.major(), \".\", cap.minor());\n+      gpu_vendor = \"NVIDIA Corporation\";\n     }\n-    devices.push_back(std::move(description));\n+\n+    StreamExecutorGpuTopologyDescription::SetupDeviceDescription(\n+        *description, gpu_vendor, compute_capability,\n+        target_config_->gpu_device_info().core_count(),\n+        target_config_->gpu_device_info().shared_memory_per_block_optin(), 0);\n   }\n-  return devices;\n+  return description;\n }\n \n absl::StatusOr<std::string> StreamExecutorGpuTopologyDescription::Serialize()\n@@ -146,24 +150,20 @@ absl::StatusOr<std::string> StreamExecutorGpuTopologyDescription::Serialize()\n absl::StatusOr<std::pair<PjRtDeviceDimensions, int32_t>>\n StreamExecutorGpuTopologyDescription::LogicalDeviceOfDefaultTypeForId(\n     xla::PjRtGlobalDeviceId device_id) const {\n-  // TODO: b/435476605 - improve the lookup performance by adding a lookup api\n-  // in pjrt topology description.\n-  for (const auto& device_desc : DeviceDescriptions()) {\n-    if (device_desc->id() == device_id) {\n-      const auto& gpu_device_desc =\n-          tsl::down_cast<const xla::PjRtStreamExecutorDeviceDescription&>(\n-              *device_desc);\n-      const auto& coords = gpu_device_desc.coords();\n-      if (coords.size() != 3) {\n-        return absl::InvalidArgumentError(absl::StrCat(\n-            \"GPU topology must have 3 dimensions, but got \", coords.size()));\n-      }\n-      return std::make_pair(\n-          PjRtDeviceDimensions{coords[0], coords[1], coords[2]}, 0);\n-    }\n+  if (device_id.value() < 0 ||\n+      device_id.value() >= gpu_topology_->number_of_devices()) {\n+    return absl::InvalidArgumentError(\n+        absl::StrCat(\"Chip id \", device_id.value(), \" is out of range [0, \",\n+                     gpu_topology_->number_of_devices(), \")\"));\n+  }\n+  auto device_desc = CreateDeviceDescription(device_id.value());\n+  const auto& coords = device_desc->coords();\n+  if (coords.size() != 3) {\n+    return absl::InvalidArgumentError(absl::StrCat(\n+        \"GPU topology must have 3 dimensions, but got \", coords.size()));\n   }\n-  return absl::NotFoundError(absl::StrCat(\"Device id \", device_id.value(),\n-                                          \" not found in GPU topology.\"));\n+  return std::make_pair(PjRtDeviceDimensions{coords[0], coords[1], coords[2]},\n+                        0);\n }\n \n absl::StatusOr<Layout> StreamExecutorGpuTopologyDescription::GetDefaultLayout("
        },
        {
            "sha": "4479d776765e5d0e2635ada39d1cef7364f06983",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_topology_description.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff993fcf5e36723d3086bebb4c9b701305a938ea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff993fcf5e36723d3086bebb4c9b701305a938ea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.h?ref=ff993fcf5e36723d3086bebb4c9b701305a938ea",
            "patch": "@@ -125,6 +125,9 @@ class StreamExecutorGpuTopologyDescription : public PjRtTopologyDescription {\n   FromProto(const xla::PjRtTopologyDescriptionProto& proto);\n \n  private:\n+  std::unique_ptr<PjRtStreamExecutorDeviceDescription> CreateDeviceDescription(\n+      int device_id) const;\n+\n   const PjRtPlatformId platform_id_;\n   const std::string platform_name_;\n   std::shared_ptr<const GpuTopology> gpu_topology_;"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 67,
        "deletions": 64
    }
}