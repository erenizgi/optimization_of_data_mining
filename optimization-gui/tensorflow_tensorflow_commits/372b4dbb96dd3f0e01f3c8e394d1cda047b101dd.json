{
    "author": "tensorflower-gardener",
    "message": "Check if the operands are constant foldable before fusing `tfl.add` into `tfl.fully_connected`\n\nThe `FuseAddAndFullyConnected` optimization pattern fuses an `add` operation into the bias of a subsequent `fully_connected`. This change adds checks to ensure this fusion only happens if the `add`'s right-hand side, and the `fully_connected`'s filter and bias are constant. This prevents issues where the fused bias would not be a single constant vector, for example, when the filter comes from a dequantize operation.\n\nPiperOrigin-RevId: 807067198",
    "sha": "372b4dbb96dd3f0e01f3c8e394d1cda047b101dd",
    "files": [
        {
            "sha": "035f210a73a7f48c5b65c17070b2242550b32f17",
            "filename": "tensorflow/compiler/mlir/lite/tests/optimize.mlir",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/372b4dbb96dd3f0e01f3c8e394d1cda047b101dd/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/372b4dbb96dd3f0e01f3c8e394d1cda047b101dd/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir?ref=372b4dbb96dd3f0e01f3c8e394d1cda047b101dd",
            "patch": "@@ -531,6 +531,55 @@ func.func @doNotFuseAddIntoFollowingFullyConnected(%arg0: tensor<4x2xf32>, %arg1\n // CHECK: \"tfl.fully_connected\"\n }\n \n+// CHECK-LABEL: @doNotFuseAddIntoFollowingWeightOnlyQuantizedFullyConnected\n+func.func @doNotFuseAddIntoFollowingWeightOnlyQuantizedFullyConnected(%arg0: tensor<4x2xf32>) -> tensor<4x2xf32> {\n+  %cst = arith.constant dense<1.5> : tensor<2xf32>\n+  %0 = \"tfl.add\"(%arg0, %cst) {fused_activation_function = \"NONE\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  %1 = \"tfl.pseudo_qconst\"() <{qtype = tensor<2x2x!quant.uniform<i8:f32:0, {1.0, 1.1}>>, value = dense<1> : tensor<2x2xi8>}> : () -> tensor<2x2x!quant.uniform<i8:f32:0, {1.0, 1.1}>>\n+  %2 = \"tfl.dequantize\"(%1) : (tensor<2x2x!quant.uniform<i8:f32:0, {1.0, 1.1}>>) -> tensor<2x2xf32>\n+  %cst0 = arith.constant dense<2.0> : tensor<2xf32>\n+  %3 = \"tfl.fully_connected\"(%0, %2, %cst0) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  func.return %3 : tensor<4x2xf32>\n+\n+// CHECK-DAG: %[[ADD_CST:.*]] = arith.constant dense<1.500000e+00> : tensor<2xf32>\n+// CHECK-DAG: %[[ADD:.*]] = tfl.add(%arg0, %[[ADD_CST]]) <{fused_activation_function = \"NONE\"}> : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+// CHECK-DAG: %[[FILTER_QUANT:.*]] = \"tfl.pseudo_qconst\"() <{qtype = tensor<2x2x!quant.uniform<i8:f32:0, {1.000000e+00,1.100000e+00}>>, value = dense<1> : tensor<2x2xi8>}> : () -> tensor<2x2x!quant.uniform<i8:f32:0, {1.000000e+00,1.100000e+00}>>\n+// CHECK-DAG: %[[FILTER:.*]] = \"tfl.dequantize\"(%[[FILTER_QUANT]]) : (tensor<2x2x!quant.uniform<i8:f32:0, {1.000000e+00,1.100000e+00}>>) -> tensor<2x2xf32>\n+// CHECK-DAG: %[[BIAS:.*]] = arith.constant dense<2.000000e+00> : tensor<2xf32>\n+// CHECK-NEXT: %[[RESULT:.*]] = \"tfl.fully_connected\"(%[[ADD]], %[[FILTER]], %[[BIAS]]) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+// CHECK-NEXT: return %[[RESULT]] : tensor<4x2xf32>\n+}\n+\n+// CHECK-LABEL: @doNotFuseRhsNonConstAddIntoFollowingFullyConnected\n+func.func @doNotFuseRhsNonConstAddIntoFollowingFullyConnected(%arg0: tensor<4x2xf32>, %arg1: tensor<2xf32>) -> tensor<4x2xf32> {\n+  %0 = \"tfl.add\"(%arg0, %arg1) {fused_activation_function = \"NONE\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  %cst = arith.constant dense<1.5> : tensor<2x2xf32>\n+  %cst0 = arith.constant dense<2.0> : tensor<2xf32>\n+  %1 = \"tfl.fully_connected\"(%0, %cst, %cst0) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  func.return %1 : tensor<4x2xf32>\n+\n+// CHECK-DAG: %[[ADD:.*]] = tfl.add(%arg0, %arg1) <{fused_activation_function = \"NONE\"}> : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+// CHECK-DAG: %[[FILTER:.*]] = arith.constant dense<1.500000e+00> : tensor<2x2xf32>\n+// CHECK-DAG: %[[BIAS:.*]] = arith.constant dense<2.000000e+00> : tensor<2xf32>\n+// CHECK-NEXT: %[[RESULT:.*]] = \"tfl.fully_connected\"(%[[ADD]], %[[FILTER]], %[[BIAS]]) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+// CHECK-NEXT: return %[[RESULT]] : tensor<4x2xf32>\n+}\n+\n+// CHECK-LABEL: @doNotFuseAddIntoFollowingFullyConnectedWithNonConstBias\n+func.func @doNotFuseAddIntoFollowingFullyConnectedWithNonConstBias(%arg0: tensor<4x2xf32>, %arg1: tensor<2xf32>) -> tensor<4x2xf32> {\n+  %cst = arith.constant dense<1.5> : tensor<2xf32>\n+  %0 = \"tfl.add\"(%arg0, %cst) {fused_activation_function = \"NONE\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  %cst0 = arith.constant dense<1.5> : tensor<2x2xf32>\n+  %1 = \"tfl.fully_connected\"(%0, %cst0, %arg1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  func.return %1 : tensor<4x2xf32>\n+\n+// CHECK-DAG: %[[ADD_CST:.*]] = arith.constant dense<1.500000e+00> : tensor<2xf32>\n+// CHECK-DAG: %[[ADD:.*]] = tfl.add(%arg0, %[[ADD_CST]]) <{fused_activation_function = \"NONE\"}> : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+// CHECK-DAG: %[[FILTER:.*]] = arith.constant dense<1.500000e+00> : tensor<2x2xf32>\n+// CHECK-NEXT: %[[RESULT:.*]] = \"tfl.fully_connected\"(%[[ADD]], %[[FILTER]], %arg1) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+// CHECK-NEXT: return %[[RESULT]] : tensor<4x2xf32>\n+}\n+\n // CHECK-LABEL: @fuseMulIntoFollowingFullyConnected\n func.func @fuseMulIntoFollowingFullyConnected(%arg0: tensor<4x2xf32>) -> tensor<4x2xf32> {\n   %cst2 = arith.constant dense<1.5> : tensor<f32>"
        },
        {
            "sha": "e6caa86cb0ed8067d0b6674c0d73dc937e7e31cc",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_pass.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/372b4dbb96dd3f0e01f3c8e394d1cda047b101dd/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/372b4dbb96dd3f0e01f3c8e394d1cda047b101dd/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc?ref=372b4dbb96dd3f0e01f3c8e394d1cda047b101dd",
            "patch": "@@ -1456,6 +1456,21 @@ struct FuseAddAndFullyConnected\n         return failure();\n     }\n \n+    // Checks the constant requirements. Only apply this optimization if rhs,\n+    // filter, and bias are constant foldable. Otherwise, the generated FC bias\n+    // operand will not be folded to a single vector.\n+    if (!matchPattern(add_op.getRhs(), m_Constant())) {\n+      return failure();\n+    }\n+\n+    if (!matchPattern(fc_op.getFilter(), m_Constant())) {\n+      return failure();\n+    }\n+\n+    if (!matchPattern(old_bias, m_Constant())) {\n+      return failure();\n+    }\n+\n     auto new_bias = rewriter.create<TFL::FullyConnectedOp>(\n         fc_op.getLoc(), old_bias.getType(),\n         /*input=*/add_op.getRhs(),"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 64,
        "deletions": 0
    }
}