{
    "author": "nputikhin",
    "message": "[XLA:GPU] Forbid fusing power() with multiple users\n\nFusing power may result in op duplication, which normally does not affect performance much but there are cases where it is expensive.\n\nPiperOrigin-RevId: 804398340",
    "sha": "19a37b969e76fec3471a45b3e8e3c18cc131e967",
    "files": [
        {
            "sha": "12e67a2e066710c762155f9732332f3d466e2178",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a37b969e76fec3471a45b3e8e3c18cc131e967/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a37b969e76fec3471a45b3e8e3c18cc131e967/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=19a37b969e76fec3471a45b3e8e3c18cc131e967",
            "patch": "@@ -1277,6 +1277,32 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(gpu_version_).Run(module.get()).value());\n }\n \n+TEST_F(GemmFusionTest, FusionShouldNotDuplicatePowerOp) {\n+  // Elementwise operations with broadcast operands are usually fused, however\n+  // with multiple users it can result in executing the op twice.\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY e {\n+  p0 = f16[124,1024] parameter(0)\n+  constant1 = f16[] constant(2)\n+  broadcast1 = f16[124,1024] broadcast(constant1)\n+  pow = f16[124,1024] power(p0, broadcast1)\n+\n+  p1 = f16[1024,124] parameter(1)\n+  dot1 = f16[124,124] dot(pow, p1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+\n+  ROOT d = (f16[124,1024],f16[124,124]) tuple(pow, dot1)\n+})\")\n+                    .value();\n+  ASSERT_TRUE(GemmFusion(gpu_version_).Run(module.get()).value());\n+  MatchHloModule(*module, R\"(\n+; CHECK: power(\n+; CHECK-NOT: power(\n+)\");\n+}\n+\n TEST_F(GemmFusionTest, RaggedDotBecomesFusion) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n HloModule m"
        },
        {
            "sha": "437a2269739cf7d04c34bf408b17b9ae9332266d",
            "filename": "third_party/xla/xla/service/gpu/triton_tiling_propagation.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a37b969e76fec3471a45b3e8e3c18cc131e967/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a37b969e76fec3471a45b3e8e3c18cc131e967/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc?ref=19a37b969e76fec3471a45b3e8e3c18cc131e967",
            "patch": "@@ -1092,6 +1092,13 @@ GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n   if (hlo.opcode() == HloOpcode::kPad) {\n     return FusionDecision::Forbid(\"Pads are not fused yet.\");\n   }\n+  if (hlo.opcode() == HloOpcode::kPower && hlo.user_count() > 1) {\n+    // The check is placed specifically above the binary elementwise ops with\n+    // broadcast operands to prohibit fusing even with broadcast inputs.\n+    return FusionDecision::Forbid(\n+        \"Not fusing power with multiple users because it may result in \"\n+        \"expensive op duplication.\");\n+  }\n   if (auto decision =\n           legacy_triton::IsTritonSupportedInstruction(hlo, gpu_version);\n       !decision.CanFuse()) {"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 33,
        "deletions": 0
    }
}