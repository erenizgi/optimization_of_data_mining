{
    "author": "chsigg",
    "message": "Add more type compatibility checks for Triton GEMM support.\n\nEnforce that both LHS and RHS operands of a dot must have the same element type. Forbid dots where the input types are integral but the output type is floating-point, or vice-versa. Update tests to cover these new restrictions and refactor an existing test to directly check `CanTritonHandleGEMM`.\n\nPiperOrigin-RevId: 827817466",
    "sha": "838b4193102344b2d838315bb6337dd5924e5cb2",
    "files": [
        {
            "sha": "13fe12ea518b265b77ccd1415be40b3e4227ea82",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/838b4193102344b2d838315bb6337dd5924e5cb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/838b4193102344b2d838315bb6337dd5924e5cb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc?ref=838b4193102344b2d838315bb6337dd5924e5cb2",
            "patch": "@@ -249,13 +249,17 @@ bool IsDotAlgorithmSupportedByTriton(\n CodegenDecision AreDotInputAndOutputTypesSupportedAndCompatible(\n     const HloDotInstruction& dot, const se::GpuComputeCapability& gpu_version) {\n   auto output_type = dot.shape().element_type();\n-  auto lhs_type = dot.operand(0)->shape().element_type();\n-  auto rhs_type = dot.operand(1)->shape().element_type();\n-\n   if (!IsTritonSupportedDotOutputType(output_type, gpu_version)) {\n     return CodegenDecision::Forbid(\"Unsupported output data type for Dot op.\");\n   }\n \n+  auto lhs_type = dot.operand(0)->shape().element_type();\n+  auto rhs_type = dot.operand(1)->shape().element_type();\n+  if (lhs_type != rhs_type && !(primitive_util::IsF8Type(lhs_type) &&\n+                                primitive_util::IsF8Type(rhs_type))) {\n+    return CodegenDecision::Forbid(\"Non-fp8 input types must be the same.\");\n+  }\n+\n   if (!IsTritonSupportedDataType(lhs_type, gpu_version) ||\n       !IsTritonSupportedDataType(rhs_type, gpu_version)) {\n     return CodegenDecision::Forbid(\"Unsupported input data type for Dot op.\");\n@@ -268,6 +272,12 @@ CodegenDecision AreDotInputAndOutputTypesSupportedAndCompatible(\n         \"Currently, S32 output is only supported for 8-bit integral inputs.\");\n   }\n \n+  if (primitive_util::IsIntegralType(lhs_type) !=\n+      primitive_util::IsIntegralType(output_type)) {\n+    return CodegenDecision::Forbid(\n+        \"Dots between integer and floating-point types are not supported.\");\n+  }\n+\n   return CodegenDecision::Allow();\n }\n "
        },
        {
            "sha": "c2f0a4774c6e68a4872a83c6867fbd9af61281f3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 19,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/838b4193102344b2d838315bb6337dd5924e5cb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/838b4193102344b2d838315bb6337dd5924e5cb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=838b4193102344b2d838315bb6337dd5924e5cb2",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n #include <string>\n #include <tuple>\n #include <utility>\n-#include <variant>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n@@ -270,15 +269,14 @@ INSTANTIATE_TEST_SUITE_P(\n     DynamicSliceTestParamToString);\n \n TEST_F(TritonSupportTestBase,\n-       UnsupportedDotOutputTypeFailsGracefullyWithTriton) {\n+       UnsupportedDotOutputTypeFailsCanTritonHandleGEMM) {\n   const std::string kHloTest = R\"(\n triton_computation {\n   parameter_0 = f32[92,11]{1,0} parameter(0)\n   parameter_1 = f32[11,63]{1,0} parameter(1)\n   ROOT dot = pred[92,63]{1,0} dot(parameter_0, parameter_1),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n-\n ENTRY e {\n   parameter_0 = f32[92,11]{1,0} parameter(0)\n   parameter_1 = f32[11,63]{1,0} parameter(1)\n@@ -293,23 +291,71 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n                           ParseTemplateAndGetInstruction(\n                               kHloTest, /*data_type=*/{}, HloOpcode::kDot));\n-  const se::DeviceDescription dev_info =\n-      TestGpuDeviceInfo::RTXA6000DeviceInfo(GetComputeCapability());\n-  EXPECT_THAT(legacy_triton::IsTritonSupportedInstruction(\n-                  ti.Instruction(), GetComputeCapability())\n-                  .Explain(),\n-              ::testing::HasSubstr(\"Unsupported output data type for Dot op.\"));\n-  BlockLevelParameters block_level_parameters;\n-  block_level_parameters.num_ctas = 1;\n-  block_level_parameters.num_stages = 4;\n-  block_level_parameters.num_warps = 8;\n   EXPECT_THAT(\n-      TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n-                    dev_info, block_level_parameters, &llvm_module_,\n-                    symbolic_expr_context_),\n-      absl_testing::StatusIs(\n-          absl::StatusCode::kInternal,\n-          ::testing::HasSubstr(\"Failed to verify Triton module for fusion\")));\n+      legacy_triton::CanTritonHandleGEMM(\n+          *Cast<HloDotInstruction>(&ti.Instruction()), GetComputeCapability())\n+          .Explain(),\n+      ::testing::HasSubstr(\"Unsupported output data type for Dot op.\"));\n+}\n+\n+TEST_F(TritonSupportTestBase, UnsupportedIntFloatDotFailsCanTritonHandleGEMM) {\n+  const std::string kHloTest = R\"(\n+triton_computation {\n+  parameter_0 = s8[92,11]{1,0} parameter(0)\n+  parameter_1 = s8[11,63]{1,0} parameter(1)\n+  ROOT dot = f32[92,63]{1,0} dot(parameter_0, parameter_1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+ENTRY e {\n+  parameter_0 = s8[92,11]{1,0} parameter(0)\n+  parameter_1 = s8[11,63]{1,0} parameter(1)\n+  ROOT triton_op = f32[92,63]{1,0} fusion(parameter_0, parameter_1), kind=kCustom,\n+    calls=triton_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n+      triton_gemm_config:\n+        {\"block_m\":16,\"block_n\":32,\"block_k\":512,\n+         \"split_k\":1,\"num_stages\":4,\"num_warps\":8,\n+         \"num_ctas\":1}}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n+                          ParseTemplateAndGetInstruction(\n+                              kHloTest, /*data_type=*/{}, HloOpcode::kDot));\n+  EXPECT_THAT(\n+      legacy_triton::CanTritonHandleGEMM(\n+          *Cast<HloDotInstruction>(&ti.Instruction()), GetComputeCapability())\n+          .Explain(),\n+      ::testing::HasSubstr(\"Dots between integer and floating-point \"\n+                           \"types are not supported.\"));\n+}\n+\n+TEST_F(TritonSupportTestBase,\n+       UnsupportedDifferentOperandTypesDotFailsCanTritonHandleGEMM) {\n+  const std::string kHloTest = R\"(\n+triton_computation {\n+  parameter_0 = f16[92,11]{1,0} parameter(0)\n+  parameter_1 = f32[11,63]{1,0} parameter(1)\n+  ROOT dot = f32[92,63]{1,0} dot(parameter_0, parameter_1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+ENTRY e {\n+  parameter_0 = f16[92,11]{1,0} parameter(0)\n+  parameter_1 = f32[11,63]{1,0} parameter(1)\n+  ROOT triton_op = f32[92,63]{1,0} fusion(parameter_0, parameter_1), kind=kCustom,\n+    calls=triton_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n+      triton_gemm_config:\n+        {\"block_m\":16,\"block_n\":32,\"block_k\":512,\n+         \"split_k\":1,\"num_stages\":4,\"num_warps\":8,\n+         \"num_ctas\":1}}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n+                          ParseTemplateAndGetInstruction(\n+                              kHloTest, /*data_type=*/{}, HloOpcode::kDot));\n+  EXPECT_THAT(\n+      legacy_triton::CanTritonHandleGEMM(\n+          *Cast<HloDotInstruction>(&ti.Instruction()), GetComputeCapability())\n+          .Explain(),\n+      ::testing::HasSubstr(\"input types must be the same\"));\n }\n \n TEST_F(TritonSupportTestBase,"
        },
        {
            "sha": "a701b59f4b6742a9fd560fc08f125b99c8eba45d",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/838b4193102344b2d838315bb6337dd5924e5cb2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/838b4193102344b2d838315bb6337dd5924e5cb2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=838b4193102344b2d838315bb6337dd5924e5cb2",
            "patch": "@@ -144,9 +144,10 @@ ENTRY e {\n   r1 = s8[8,32,8] reshape(p0)\n   t1 = s8[32,8,8] transpose(r1), dimensions={1,0,2}\n   r0 = s8[32,64] reshape(t1)\n+  c1 = f16[32,64] convert(r0)\n   p1 = s8[32,32] parameter(1)\n   c0 = f16[32,32] convert(p1)\n-  ROOT d = f16[64,32] dot(r0, c0),\n+  ROOT d = f16[64,32] dot(c1, c0),\n     lhs_contracting_dims={0}, rhs_contracting_dims={1}\n })\")\n                     .value();\n@@ -1398,7 +1399,7 @@ ENTRY e {\n TEST_F(SmallDotGemmFusionTest, Int4DotIsRewritten) {\n   constexpr auto kInt4Dot = R\"(\n     ENTRY e {\n-      p0 = s8[16,16] parameter(0)\n+      p0 = bf16[16,16] parameter(0)\n       p1 = s4[16,16] parameter(1)\n       p1c = bf16[16,16] convert(p1)\n       ROOT dot = bf16[16,16] dot(p0, p1c),"
        }
    ],
    "stats": {
        "total": 105,
        "additions": 81,
        "deletions": 24
    }
}