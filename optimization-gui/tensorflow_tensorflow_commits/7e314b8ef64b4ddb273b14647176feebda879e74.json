{
    "author": "GleasonK",
    "message": "[StableHLO Optim] Raise fold limit, disable iota folding\n\nPiperOrigin-RevId: 812235661",
    "sha": "7e314b8ef64b4ddb273b14647176feebda879e74",
    "files": [
        {
            "sha": "997fa25974146e2a278637b30122abf971a707dd",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 87,
            "deletions": 16,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7e314b8ef64b4ddb273b14647176feebda879e74/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7e314b8ef64b4ddb273b14647176feebda879e74/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=7e314b8ef64b4ddb273b14647176feebda879e74",
            "patch": "@@ -258,10 +258,50 @@ diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -601,6 +601,37 @@\n- // -----\n+@@ -529,28 +529,15 @@\n+ // IotaOp\n  \n- ////////\n+ // CHECK-LABEL: func @eval_iota\n+-func.func @eval_iota() -> (tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {\n+-  // CHECK-NOT: stablehlo.iota\n+-  // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<\n+-  // CHECK-SAME: {{\\[\\[}}[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],\n+-  // CHECK-SAME: {{\\[}}[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],\n+-  // CHECK-SAME: {{\\[}}[2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2]]]> : tensor<3x4x5xi32>\n+-\n+-  // CHECK: [[RESULT1:%.*]] = stablehlo.constant dense<\n+-  // CHECK-SAME: {{\\[\\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],\n+-  // CHECK-SAME: {{\\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],\n+-  // CHECK-SAME: {{\\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]]]> : tensor<3x4x5xi32>\n+-\n+-  // CHECK: [[RESULT2:%.*]] = stablehlo.constant dense<\n+-  // CHECK-SAME: {{\\[\\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],\n+-  // CHECK-SAME: {{\\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],\n+-  // CHECk-SAME: {{\\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]]> : tensor<3x4x5xi32>\n+-\n++func.func @eval_iota() -> (tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {\n++  // CHECK:      [[RESULT0:%.*]] = stablehlo.constant dense<0> : tensor<1xi32>\n++  // CHECK-NEXT: [[RESULT1:%.*]] = stablehlo.iota dim = 1 : tensor<3x4x5xi32>\n++  // CHECK-NEXT: [[RESULT2:%.*]] = stablehlo.iota dim = 2 : tensor<3x4x5xi32>\n+   // CHECK: return [[RESULT0]], [[RESULT1]], [[RESULT2]]\n+-  %0 = stablehlo.iota dim = 0 : tensor<3x4x5xi32>\n++  %0 = stablehlo.iota dim = 0 : tensor<1xi32>\n+   %1 = stablehlo.iota dim = 1 : tensor<3x4x5xi32>\n+   %2 = stablehlo.iota dim = 2 : tensor<3x4x5xi32>\n+-  func.return %0, %1, %2 : tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>\n++  func.return %0, %1, %2 : tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>\n+ }\n+ \n+ // -----\n+@@ -596,6 +583,37 @@\n+   // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\\[\\[1, 2\\], \\[3, 4\\]\\]}}> : tensor<2x2xi32>\n+   // CHECK-NEXT: return [[CST1]], [[CST2]]\n+   return %0, %1 : tensor<1xi32>, tensor<2x2xi32>\n++}\n++\n++// -----\n++\n++////////\n +// SliceOp / DynamicSliceOp\n +\n +// CHECK-LABEL: @slice_fold\n@@ -288,15 +328,10 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n +  // CHECK: %[[RESULT:.*]] = stablehlo.constant dense<256> : tensor<1x1xi32>\n +  // CHECK: return %[[RESULT]]\n +  return %1 : tensor<1x1xi32>\n-+}\n-+\n-+// -----\n-+\n-+////////\n- // ConvertOp\n+ }\n  \n- // CHECK-LABEL: func @eval_convert_f32_to_i64\n-@@ -712,18 +743,412 @@\n+ // -----\n+@@ -712,18 +730,412 @@\n  // -----\n  \n  ////////\n@@ -717,7 +752,7 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n  \n  ////////\n  // SetDimensionSizeOp\n-@@ -748,6 +1173,19 @@\n+@@ -748,6 +1160,19 @@\n    // CHECK-NEXT: return [[RESULT0]]\n    %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<10xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<10>>\n    return %0 : tensor<?xf32, #stablehlo.bounds<10>>\n@@ -900,6 +935,27 @@ diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stable\n    populateForBroadcastingBinaryOp<ConvertRankedDynamicBroadcastBinaryOp>(\n        context, patterns, 5);\n    patterns->add<ConvertConstantLikeOp, ConvertSelectOp>(context);\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.td b/stablehlo/stablehlo/transforms/optimization/Passes.td\n+--- stablehlo/stablehlo/transforms/optimization/Passes.td\n++++ stablehlo/stablehlo/transforms/optimization/Passes.td\n+@@ -23,14 +23,14 @@\n+          \"explicit MLIR `MemoryEffects`. Notably, this means `func.call` ops \"\n+          \"will be assumed pure.\">,\n+   Option<\"foldOpElementLimit\", \"fold-op-element-limit\", \"int64_t\",\n+-         /*default=*/\"1\",\n++         /*default=*/\"65536\",\n+          \"Folding an op into a constant can sometimes come at the cost of \"\n+          \"memory overhead. (This occurs if the op's inputs are reused, meaning \"\n+          \"that they can't be deleted after the op is folded to a constant, or \"\n+-         \"when folding operations like `iota` whose outputs take up more \"\n++         \"when folding operations like `concat` whose outputs take up more \"\n+          \"memory than their inputs.) In such cases, this config option sets an \"\n+          \"upper limit on how many elements an op's result may have before the \"\n+-         \"op is no longer folded.\">,\n++         \"op is no longer folded. Splat folds are exempt from this limit.\">,\n+   Option<\"optimizeFloat\", \"optimize-float\", \"bool\", /*default=*/\"true\",\n+          \"Allow float optimizations that, though mathematically equivalent, \"\n+          \"may result in slightly different quantization of floating-point \"\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n@@ -1446,7 +1502,22 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n  };\n  \n  struct FoldIotaOpPattern : public FoldOpRewritePattern<IotaOp> {\n-@@ -1316,13 +1636,9 @@\n+@@ -1216,6 +1536,14 @@\n+       rewriter.replaceOpWithNewOp<ConstantOp>(\n+           op, DenseIntElementsAttr::get(resultType, values));\n+       return success();\n++    }\n++\n++    // TODO: Support more iota folding, but doing so currently causes OOMs,\n++    // so this pattern needs to be enabled more carefully.\n++    if (outputSize != 1) {\n++      return rewriter.notifyMatchFailure(\n++          op, \"expected output size to be 1, but got: \" +\n++                  std::to_string(outputSize));\n+     }\n+ \n+     int64_t sequences = 1;\n+@@ -1316,13 +1644,9 @@\n  \n      for (auto [inputValue, bodyArg] :\n           llvm::zip_equal(op.getOperands(), body.getArguments())) {\n@@ -1463,7 +1534,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n          return rewriter.notifyMatchFailure(op,\n                                             \"Input must be a splat constant.\");\n  \n-@@ -1332,7 +1648,7 @@\n+@@ -1332,7 +1656,7 @@\n              op, \"Could not get the shape of the body argument.\");\n  \n        bodyArgConstantAttrs.push_back(DenseElementsAttr::get(\n@@ -1472,7 +1543,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n      }\n  \n      for (BlockArgument bodyArg : body.getArguments()) {\n-@@ -1570,11 +1886,25 @@\n+@@ -1570,11 +1894,25 @@\n      PatternBenefit benefit) {\n    populateStablehloShapeFolderPatterns(context, patterns, options, benefit);\n  \n@@ -1499,7 +1570,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n                  FoldTransposeOpPattern,               //\n                  FoldWhileOpIfDeadAndPresumedPure,     //\n                  FoldWhileOpPattern,                   //\n-@@ -1605,6 +1935,7 @@\n+@@ -1605,6 +1943,7 @@\n    patterns->add<FoldConcatenateOpPattern>(context, options, benefit);\n    patterns->add<FoldConvertOpPattern>(context, options, benefit);\n    patterns->add<FoldDivOpPattern>(context, options, benefit);"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 87,
        "deletions": 16
    }
}