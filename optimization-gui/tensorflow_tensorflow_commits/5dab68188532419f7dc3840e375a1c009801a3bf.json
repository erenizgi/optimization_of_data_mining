{
    "author": "majiddadashi",
    "message": "Fuse L2 normalization patterns using Mul and a keep_dims Reshape.\n\nThis adds new patterns to the TFLite converter to fuse more forms of L2 normalization. These patterns handle models where L2 normalization is constructed using operations that are functionally equivalent to the standard form but use different ops.\n\nThe new patterns fuse subgraphs that:\n\nUse Mul(x, x) instead of Square(x).\nUse a Reshape op to add a trailing dimension of size 1 after the Sum reduction, which emulates the keep_dims=true behavior. A new C++ helper IsL2NormalizationKeepDimsReshape is added to check for this specific case.\nInclude an Add or Maximum op with a small constant for numerical stability.\nNew tests are added to verify these fusion patterns work correctly.\n\nPiperOrigin-RevId: 798338534",
    "sha": "5dab68188532419f7dc3840e375a1c009801a3bf",
    "files": [
        {
            "sha": "ffce22b25161e82e6163d60fc107685ca56d8cee",
            "filename": "tensorflow/compiler/mlir/lite/tests/optimize.mlir",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5dab68188532419f7dc3840e375a1c009801a3bf/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5dab68188532419f7dc3840e375a1c009801a3bf/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir?ref=5dab68188532419f7dc3840e375a1c009801a3bf",
            "patch": "@@ -1881,6 +1881,52 @@ func.func @InvalidL2NormalizePattern(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>)\n   // CHECK: return %3\n }\n \n+// CHECK-LABEL: @L2NormalizePattern4_Mul\n+func.func @L2NormalizePattern4_Mul(%arg0: tensor<1x2xf32>) -> tensor<1x2xf32> {\n+  %cst = arith.constant dense<[1]> : tensor<1xi32>\n+  %cst_shape = arith.constant dense<[1, 1]> : tensor<2xi32>\n+  %0 = \"tfl.mul\"(%arg0, %arg0) {fused_activation_function = \"NONE\"} : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<1x2xf32>\n+  %1 = \"tfl.sum\"(%0, %cst) {keep_dims = false} : (tensor<1x2xf32>, tensor<1xi32>) -> tensor<1xf32>\n+  %2 = \"tfl.rsqrt\"(%1) : (tensor<1xf32>) -> tensor<1xf32>\n+  %3 = \"tfl.reshape\"(%2, %cst_shape) : (tensor<1xf32>, tensor<2xi32>) -> tensor<1x1xf32>\n+  %4 = \"tfl.mul\"(%arg0, %3) {fused_activation_function = \"NONE\"} : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>\n+  func.return %4: tensor<1x2xf32>\n+  // CHECK: %[[RES:[0-9].*]] = \"tfl.l2_normalization\"([[INPUT:%.*]]) <{fused_activation_function = \"NONE\"}> : (tensor<1x2xf32>) -> tensor<1x2xf32>\n+  // CHECK: return %[[RES]]\n+}\n+\n+// CHECK-LABEL: @L2NormalizePattern5_Mul\n+func.func @L2NormalizePattern5_Mul(%arg0: tensor<1x2xf32>) -> tensor<1x2xf32> {\n+  %cst = arith.constant dense<[1]> : tensor<1xi32>\n+  %cst_1 = arith.constant dense<[1.0e-4]> : tensor<1xf32>\n+  %cst_shape = arith.constant dense<[1, 1]> : tensor<2xi32>\n+  %0 = \"tfl.mul\"(%arg0, %arg0) {fused_activation_function = \"NONE\"} : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<1x2xf32>\n+  %1 = \"tfl.sum\"(%0, %cst) {keep_dims = false} : (tensor<1x2xf32>, tensor<1xi32>) -> tensor<1xf32>\n+  %2 = \"tfl.rsqrt\"(%1) : (tensor<1xf32>) -> tensor<1xf32>\n+  %3 = \"tfl.add\"(%2, %cst_1) {fused_activation_function = \"NONE\"} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>\n+  %4 = \"tfl.reshape\"(%3, %cst_shape) : (tensor<1xf32>, tensor<2xi32>) -> tensor<1x1xf32>\n+  %5 = \"tfl.mul\"(%arg0, %4) {fused_activation_function = \"NONE\"} : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>\n+  func.return %5: tensor<1x2xf32>\n+  // CHECK: %[[RES:[0-9].*]] = \"tfl.l2_normalization\"([[INPUT:%.*]]) <{fused_activation_function = \"NONE\"}> : (tensor<1x2xf32>) -> tensor<1x2xf32>\n+  // CHECK: return %[[RES]]\n+}\n+\n+// CHECK-LABEL: @L2NormalizePattern6_Mul\n+func.func @L2NormalizePattern6_Mul(%arg0: tensor<1x2xf32>) -> tensor<1x2xf32> {\n+  %cst = arith.constant dense<[1]> : tensor<1xi32>\n+  %cst_1 = arith.constant dense<[1.0e-4]> : tensor<1xf32>\n+  %cst_shape = arith.constant dense<[1, 1]> : tensor<2xi32>\n+  %0 = \"tfl.mul\"(%arg0, %arg0) {fused_activation_function = \"NONE\"} : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<1x2xf32>\n+  %1 = \"tfl.sum\"(%0, %cst) {keep_dims = false} : (tensor<1x2xf32>, tensor<1xi32>) -> tensor<1xf32>\n+  %2 = \"tfl.rsqrt\"(%1) : (tensor<1xf32>) -> tensor<1xf32>\n+  %3 = \"tfl.maximum\"(%2, %cst_1) : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>\n+  %4 = \"tfl.reshape\"(%3, %cst_shape) : (tensor<1xf32>, tensor<2xi32>) -> tensor<1x1xf32>\n+  %5 = \"tfl.mul\"(%arg0, %4) {fused_activation_function = \"NONE\"} : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>\n+  func.return %5: tensor<1x2xf32>\n+  // CHECK: %[[RES:[0-9].*]] = \"tfl.l2_normalization\"([[INPUT:%.*]]) <{fused_activation_function = \"NONE\"}> : (tensor<1x2xf32>) -> tensor<1x2xf32>\n+  // CHECK: return %[[RES]]\n+}\n+\n // CHECK-LABEL: @InvalidL2NormalizePattern2\n // Epsilon in the add must be < 1e-3\n func.func @InvalidL2NormalizePattern2(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> tensor<2xf32> {"
        },
        {
            "sha": "0fa79202b63fbd6bd1925be8c371627ad3c7ea14",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_pass.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5dab68188532419f7dc3840e375a1c009801a3bf/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5dab68188532419f7dc3840e375a1c009801a3bf/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc?ref=5dab68188532419f7dc3840e375a1c009801a3bf",
            "patch": "@@ -121,6 +121,31 @@ bool L2NormalizeReduceAxis(Value sq_op, DenseElementsAttr axis) {\n   return true;\n }\n \n+// Checks if a ReshapeOp is equivalent to a `keep_dims=true` reduction by\n+// adding a trailing dimension of size 1. In the L2 normalization pattern, a\n+// `Sum` op reduces along the last axis, and this reshape is used to add back\n+// the reduced dimension to keep the original rank. This is used in declarative\n+// patterns to fuse L2 normalization operations.\n+bool IsL2NormalizationKeepDimsReshape(Value reshape_output) {\n+  auto producer = reshape_output.getDefiningOp<TFL::ReshapeOp>();\n+  if (!producer) {\n+    return false;\n+  }\n+\n+  auto input_type = mlir::dyn_cast<ShapedType>(producer.getInput().getType());\n+  auto output_type = mlir::dyn_cast<ShapedType>(reshape_output.getType());\n+  if (!input_type || !output_type || !input_type.hasRank() ||\n+      !output_type.hasRank()) {\n+    return false;\n+  }\n+\n+  const auto input_shape = input_type.getShape();\n+  const auto output_shape = output_type.getShape();\n+\n+  return output_shape.size() == input_shape.size() + 1 &&\n+         output_shape.back() == 1 && output_shape.drop_back() == input_shape;\n+}\n+\n // Is rankx2xi32 padding array \"balanced\"\n // i.e. 0 <= [d][1] - [d][0] <= 1 for all spatial dims d (and 0 elsewhere).\n template <typename T>"
        },
        {
            "sha": "3e9cc005dafe014c96a9ce62180132c2d719465b",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td",
            "status": "modified",
            "additions": 74,
            "deletions": 3,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5dab68188532419f7dc3840e375a1c009801a3bf/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_patterns.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5dab68188532419f7dc3840e375a1c009801a3bf/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_patterns.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_patterns.td?ref=5dab68188532419f7dc3840e375a1c009801a3bf",
            "patch": "@@ -389,14 +389,17 @@ def ConstAPFloatNegLargestOrNegInfinity : Constraint<CPred<\n def L2NormValidReduceIndex : Constraint<CPred<\n   \"L2NormalizeReduceAxis($0, llvm::cast<DenseElementsAttr>($1))\">>;\n \n+// Checks if the ReshapeOp adds a trailing dimension of size 1.\n+def IsL2NormalizationKeepDimsReshape\n+    : Constraint<CPred<\"IsL2NormalizationKeepDimsReshape($0)\">>;\n+\n // Currently L2Normalization doesn't support activation function\n // in TFLite.\n // TODO(karimnosseir): Add constraints that the kernel code assumes.\n // constraint on axis and depth.\n multiclass L2NormalizePatterns<Op FirstOp, Op SecondOp> {\n-  // This pattern constructs L2NormalizationOp from\n-  // Mul->Rsqrt->Sum->Square Or\n-  // Div->sqrt->Sum->Square\n+  // Fuses `(x * rsqrt(sum(square(x))))` or `(x / sqrt(sum(square(x))))`\n+  // into a L2NormalizationOp.\n   def L2NormalizePattern1#FirstOp#SecondOp : Pat<\n                   (FirstOp $x,\n                      (SecondOp\n@@ -438,6 +441,74 @@ multiclass L2NormalizePatterns<Op FirstOp, Op SecondOp> {\n            [(L2NormValidReduceIndex $sq_op, $axis),\n             (ConstDoubleValueLessThan<\"1e-3\"> $epsilon)]>;\n \n+  // Fuses L2 norm with Mul(x,x) instead of Square(x) and a reshape that\n+  // emulates keep_dims.\n+  def L2NormalizePattern4#FirstOp#SecondOp : Pat<\n+      (FirstOp $x,\n+          (TFL_ReshapeOp:$reshape_op\n+              (SecondOp\n+                  (TFL_SumOp\n+                      (TFL_MulOp:$mul_op $x, $x, TFL_AF_None),\n+                      (Arith_ConstantOp I32ElementsAttr:$axis),\n+                      ConstBoolAttrFalse\n+                  )\n+              ),\n+              (Arith_ConstantOp $shape)\n+          ),\n+          TFL_AF_None\n+      ),\n+      (TFL_L2NormalizationOp $x, TFL_AF_None),\n+      [(L2NormValidReduceIndex $mul_op, $axis), \n+       (IsL2NormalizationKeepDimsReshape $reshape_op)]\n+  >;\n+\n+  // Fuses L2 norm with Mul(x,x), a reshape, and a small constant add for\n+  // numerical stability.\n+  def L2NormalizePattern5#FirstOp#SecondOp : Pat<\n+      (FirstOp $x,\n+          (TFL_ReshapeOp:$reshape_op\n+              (TFL_AddOp\n+                  (SecondOp\n+                    (TFL_SumOp\n+                        (TFL_MulOp:$mul_op $x, $x, TFL_AF_None),\n+                        (Arith_ConstantOp I32ElementsAttr:$axis),\n+                        ConstBoolAttrFalse\n+                    )\n+                  ), (Arith_ConstantOp $epsilon), TFL_AF_None\n+              ),\n+              (Arith_ConstantOp $shape)\n+          ),\n+          TFL_AF_None\n+      ),\n+      (TFL_L2NormalizationOp $x, TFL_AF_None),\n+      [(L2NormValidReduceIndex $mul_op, $axis), \n+       (IsL2NormalizationKeepDimsReshape $reshape_op),\n+       (ConstDoubleValueLessThan<\"1e-3\"> $epsilon)]\n+  >;\n+\n+  // Fuses L2 norm with Mul(x,x), a reshape, and a small constant maximum for\n+  // numerical stability.\n+  def L2NormalizePattern6#FirstOp#SecondOp : Pat<\n+      (FirstOp $x,\n+          (TFL_ReshapeOp:$reshape_op\n+              (TFL_MaximumOp\n+                  (SecondOp\n+                    (TFL_SumOp\n+                        (TFL_MulOp:$mul_op $x, $x, TFL_AF_None),\n+                        (Arith_ConstantOp I32ElementsAttr:$axis),\n+                        ConstBoolAttrFalse\n+                    )\n+                  ), (Arith_ConstantOp $epsilon)\n+              ),\n+              (Arith_ConstantOp $shape)\n+          ),\n+          TFL_AF_None\n+      ),\n+      (TFL_L2NormalizationOp $x, TFL_AF_None),\n+      [(L2NormValidReduceIndex $mul_op, $axis), \n+       (IsL2NormalizationKeepDimsReshape $reshape_op),\n+       (ConstDoubleValueLessThan<\"1e-3\"> $epsilon)]\n+  >;\n }\n \n foreach L2NormalizePairs = [[TFL_MulOp, TFL_RsqrtOp], [TFL_DivOp, TFL_SqrtOp]]"
        }
    ],
    "stats": {
        "total": 148,
        "additions": 145,
        "deletions": 3
    }
}