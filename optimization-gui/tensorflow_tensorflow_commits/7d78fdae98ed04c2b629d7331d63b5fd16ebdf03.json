{
    "author": "basioli-k",
    "message": "[XLA:GPU][codegen] Add AllReduce checks to triton support checks\n\nPiperOrigin-RevId: 842693983",
    "sha": "7d78fdae98ed04c2b629d7331d63b5fd16ebdf03",
    "files": [
        {
            "sha": "66d7d22fd27cc9e48f278372a194adf314db821c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=7d78fdae98ed04c2b629d7331d63b5fd16ebdf03",
            "patch": "@@ -190,8 +190,11 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, mlir::Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n-  const int64_t num_elements =\n-      ShapeUtil::ElementsIn(computation->root_instruction()->shape());\n+  const HloInstruction* root_instruction = computation->root_instruction();\n+  if (root_instruction->opcode() == HloOpcode::kAllReduceDone) {\n+    root_instruction = root_instruction->operand(0);\n+  }\n+  const int64_t num_elements = ShapeUtil::ElementsIn(root_instruction->shape());\n   const TiledHloInstruction* tiled_input_hlo = tiled_hlo_reduce.operand(0);\n   TensorValue input_tile = values[tiled_input_hlo];\n \n@@ -279,6 +282,10 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n   }\n \n   // 2. Synchronization phase: Wait for all ranks to complete the scatter.\n+  if (all_reduce.device_list().replica_groups().empty()) {\n+    return Internal(\n+        \"Triton emitting AllReduce without replica groups is not supported.\");\n+  }\n   int64_t world_size = all_reduce.device_list().num_devices_per_group();\n   mtx::BlockBarrierOp::create(b, signal_buffers, device_rank, signal_value,\n                               b.getI32IntegerAttr(world_size));\n@@ -451,6 +458,9 @@ absl::StatusOr<TensorValue> EmitCollective(\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   const HloComputation* computation = fusion->fused_instructions_computation();\n   const HloInstruction* root = computation->root_instruction();\n+  if (root->opcode() == HloOpcode::kAllReduceDone) {\n+    root = root->operand(0);\n+  }\n   switch (root->opcode()) {\n     case HloOpcode::kAllReduceStart:\n       return EmitAllReduce("
        },
        {
            "sha": "a344c472fe143ab42ddf24f0e2a27cf104fadd22",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=7d78fdae98ed04c2b629d7331d63b5fd16ebdf03",
            "patch": "@@ -1191,6 +1191,10 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n                           values);\n   }\n \n+  if (hlo->opcode() == HloOpcode::kAllReduceDone) {\n+    return values[tiled_hlo.operand(0)];\n+  }\n+\n   if (hlo->IsElementwise()) {\n     std::vector<Value> operands;\n     operands.reserve(hlo->operands().size());"
        },
        {
            "sha": "2a0a85b0916a2e54a3e53bfcf2840e3eb5e233fe",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=7d78fdae98ed04c2b629d7331d63b5fd16ebdf03",
            "patch": "@@ -292,6 +292,31 @@ CodegenDecision CanTritonHandleReduce(\n       \"Reduction is not a row-reduction of a single operand.\");\n }\n \n+CodegenDecision IsTritonSupportedAllReduce(\n+    const HloAllReduceInstruction& all_reduce,\n+    const se::GpuComputeCapability& gpu_version) {\n+  if (all_reduce.replica_groups().empty()) {\n+    return CodegenDecision::Forbid(\"All-reduce does not have replica groups.\");\n+  }\n+  if (all_reduce.shape().element_type() == PrimitiveType::F8E4M3FN ||\n+      all_reduce.shape().element_type() == PrimitiveType::F8E5M2 ||\n+      all_reduce.shape().element_type() == PrimitiveType::S4) {\n+    return CodegenDecision::Forbid(\n+        \"S4, F8E4M3FN and F8E5M2 are not supported for all-reduces.\");\n+  }\n+\n+  bool is_triton_supported_all_reduce_computation = absl::c_all_of(\n+      all_reduce.to_apply()->instructions(), [&](const HloInstruction* instr) {\n+        return IsTritonSupportedInstructionImpl(*instr, gpu_version).CanFuse();\n+      });\n+  if (!is_triton_supported_all_reduce_computation) {\n+    return CodegenDecision::Forbid(\n+        \"Unsupported all-reduce computation by Triton.\");\n+  }\n+\n+  return CodegenDecision::Allow();\n+}\n+\n bool IsInTritonNestedGemmFusion(const HloInstruction& hlo) {\n   if (!hlo.parent()->IsFusionComputation()) {\n     return false;\n@@ -682,6 +707,12 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n     case HloOpcode::kFusion:\n       return IsTritonSupportedFusion(*Cast<HloFusionInstruction>(&instr),\n                                      gpu_version);\n+    case HloOpcode::kAllReduceStart:\n+      return IsTritonSupportedAllReduce(*Cast<HloAllReduceInstruction>(&instr),\n+                                        gpu_version);\n+    case HloOpcode::kAllReduceDone:\n+      return IsTritonSupportedAllReduce(\n+          *Cast<HloAllReduceInstruction>(instr.operand(0)), gpu_version);\n     default:\n       // Not all instructions have a special handling.\n       break;"
        },
        {
            "sha": "7ffb158965fd413d40558eca8d82a86ec73479ba",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 1,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=7d78fdae98ed04c2b629d7331d63b5fd16ebdf03",
            "patch": "@@ -1172,7 +1172,7 @@ ENTRY triton_computation {\n }\n \n TEST_P(CollectiveTest,\n-       UnsupportedAllReduceStartAndDoneFailGracefullyWithTriton) {\n+       IsTritonSupportedAllReduceStartAndDoneWithNoReplicaGroups) {\n   // 'all-reduce-start' and 'all-reduce-done' need to be tested together, since\n   // the HLO verifier relies on one directly consuming the other.\n   auto [data_type, cc] = GetParam();\n@@ -1201,6 +1201,36 @@ ENTRY triton_computation {\n   RunSupportTest(std::move(ti_done), /*output_tile_sizes=*/{2, 2}, cc);\n }\n \n+TEST_P(CollectiveTest,\n+       IsTritonSupportedAllReduceStartAndDoneWithReplicaGroups) {\n+  // 'all-reduce-start' and 'all-reduce-done' need to be tested together, since\n+  // the HLO verifier relies on one directly consuming the other.\n+  auto [data_type, cc] = GetParam();\n+  const std::string kHloTestTemplate = R\"(\n+apply_op {\n+  x = $0[] parameter(0)\n+  y = $0[] parameter(1)\n+  ROOT apply_op = $0[] add(x, y)\n+}\n+\n+ENTRY triton_computation {\n+  input = $0[128,32] parameter(0)\n+  all-reduce-start = $0[128,32] all-reduce-start(input), replica_groups={{0,1}},\n+      to_apply=apply_op\n+  ROOT all-reduce-done = $0[128,32] all-reduce-done(all-reduce-start)\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      TestedInstruction ti_start,\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type,\n+                                     HloOpcode::kAllReduceStart));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      TestedInstruction ti_done,\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type,\n+                                     HloOpcode::kAllReduceDone));\n+  RunSupportTest(std::move(ti_start), /*output_tile_sizes=*/{2, 2}, cc);\n+  RunSupportTest(std::move(ti_done), /*output_tile_sizes=*/{2, 2}, cc);\n+}\n+\n TEST_P(CollectiveTest, UnsupportedAllToAllFailsGracefullyWithTriton) {\n   auto [data_type, cc] = GetParam();\n   const std::string kHloTestTemplate = R\"("
        },
        {
            "sha": "e95c64815e783f3328a0292a0fd0615d60392b21",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=7d78fdae98ed04c2b629d7331d63b5fd16ebdf03",
            "patch": "@@ -300,6 +300,10 @@ std::string TritonSupportTestDeviceToString(\n \n namespace {\n \n+bool IsCollectiveFusion(const HloFusionInstruction& fusion) {\n+  return fusion.fused_expression_root()->opcode() == HloOpcode::kAllReduceDone;\n+}\n+\n // This function does nothing if the input module already has an entry\n // computation whose root is a fusion. Otherwise, creates a new entry\n // computation whose root is a fusion instruction that calls the original entry\n@@ -327,7 +331,9 @@ absl::Status ConvertEntryToTritonFusion(HloModule* module) {\n \n   gpu::GpuBackendConfig gpu_config;\n   gpu_config.mutable_fusion_backend_config()->set_kind(\n-      kTritonNestedGemmFusionKind);\n+      IsCollectiveFusion(*xla::Cast<HloFusionInstruction>(fusion))\n+          ? kTritonCollectiveFusionKind\n+          : kTritonNestedGemmFusionKind);\n   TF_RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n \n   auto new_entry ="
        },
        {
            "sha": "8f4e2ec440ae3f46882d23c4df20b9c4cfdb5037",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7d78fdae98ed04c2b629d7331d63b5fd16ebdf03/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc?ref=7d78fdae98ed04c2b629d7331d63b5fd16ebdf03",
            "patch": "@@ -1729,7 +1729,8 @@ HloInstructionIndexing ComputeOutputToInputIndexing(const HloInstruction* instr,\n       // b/65689298.\n       instr->opcode() == HloOpcode::kMap ||\n       // For a single device, all-reduce is an elementwise op.\n-      instr->opcode() == HloOpcode::kAllReduceStart) {\n+      instr->opcode() == HloOpcode::kAllReduceStart ||\n+      instr->opcode() == HloOpcode::kAllReduceDone) {\n     return ComputeOutputToInputCwiseOpIndexing(instr, mlir_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {\n@@ -1815,7 +1816,8 @@ HloInstructionIndexing ComputeInputToOutputIndexing(const HloInstruction* instr,\n       // b/65689298.\n       instr->opcode() == HloOpcode::kMap ||\n       // For a single device, all-reduce has 1:1 output to input mapping.\n-      instr->opcode() == HloOpcode::kAllReduceStart) {\n+      instr->opcode() == HloOpcode::kAllReduceStart ||\n+      instr->opcode() == HloOpcode::kAllReduceDone) {\n     return ComputeInputToOutputCwiseOpIndexing(instr, mlir_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {"
        }
    ],
    "stats": {
        "total": 95,
        "additions": 89,
        "deletions": 6
    }
}