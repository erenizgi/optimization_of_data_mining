{
    "author": "beckerhe",
    "message": "Remove forgotten ROCM version checks from NcclCollectives\n\nThe ROCm code path doesn't go through NcclCollectives anymore. Therefore these checks are obsolete.\n\nPiperOrigin-RevId: 846226180",
    "sha": "b64c84f2c3b2e5c577509d28f711777ee5d28b2c",
    "files": [
        {
            "sha": "d4990d32193f1028486603def249148644ff5850",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_collectives.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b64c84f2c3b2e5c577509d28f711777ee5d28b2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b64c84f2c3b2e5c577509d28f711777ee5d28b2c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc?ref=b64c84f2c3b2e5c577509d28f711777ee5d28b2c",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/collectives/nccl_collectives.h\"\n \n+#include <atomic>\n #include <cstdint>\n #include <cstdlib>\n #include <functional>\n@@ -104,9 +105,7 @@ static absl::StatusOr<ncclConfig_t> AsNcclConfig(\n     const se::StreamExecutor* stream_executor) {\n   ncclConfig_t comm_config = NCCL_CONFIG_INITIALIZER;\n   comm_config.blocking = config.blocking_communicators ? 1 : 0;\n-#if !defined(TENSORFLOW_USE_ROCM) || TF_ROCM_VERSION > 50700\n   comm_config.splitShare = config.split_share;\n-#endif\n   int nccl_version;\n   XLA_NCCL_RETURN_IF_ERROR(ncclGetVersion(&nccl_version));\n   if (config.max_nchannels > 0) {\n@@ -231,7 +230,6 @@ NcclCollectives::SplitCommunicatorsWithCancel(\n   const auto& gpu_config =\n       tsl::down_cast<const GpuCollectives::Config&>(config);\n \n-#if !defined(TENSORFLOW_USE_ROCM) || TF_ROCM_VERSION >= 60000\n   auto make_comm = [&](int i) -> absl::StatusOr<ncclComm_t> {\n     auto* device = tsl::down_cast<GpuCollectives::Device*>(ranks[i].device);\n     TF_RET_CHECK(device != nullptr);\n@@ -268,11 +266,6 @@ NcclCollectives::SplitCommunicatorsWithCancel(\n   }  // pool's destructor blocks until all scheduled work is done.\n   TF_RETURN_IF_ERROR(status);\n   return split_comms;\n-#else\n-  return absl::UnimplementedError(\n-      absl::StrFormat(\"%s:%d: NCCL operation ncclCommSplit not implemented\",\n-                      __FILE__, __LINE__));\n-#endif  // !defined(TENSORFLOW_USE_ROCM) || TF_ROCM_VERSION >= 60000\n }\n \n static absl::StatusOr<xla::gpu::GpuCollectives*> GetNvshmemCollectives() {"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 1,
        "deletions": 8
    }
}