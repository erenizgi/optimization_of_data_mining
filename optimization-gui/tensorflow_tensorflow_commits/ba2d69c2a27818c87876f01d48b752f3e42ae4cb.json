{
    "author": "tensorflower-gardener",
    "message": "Merge pull request #93951 from Sqvid:tf-bump-3.7-fix\n\nPiperOrigin-RevId: 798331790",
    "sha": "ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
    "files": [
        {
            "sha": "2f3b1ed4e761e92d4d9a9f4a025f04c8040ffd4d",
            "filename": ".bazelrc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/.bazelrc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/.bazelrc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.bazelrc?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -277,17 +277,14 @@ build:mkl_threadpool --define=tensorflow_mkldnn_contraction_kernel=0\n build:mkl_threadpool --define=build_with_mkl_opensource=true\n build:mkl_threadpool -c opt\n \n-# Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).\n-build:mkl_aarch64 --define=build_with_mkl_aarch64=true\n-build:mkl_aarch64 --define=build_with_openmp=true\n-build:mkl_aarch64 --define=build_with_acl=true\n-build:mkl_aarch64 -c opt\n-\n # Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).\n # with Eigen threadpool support\n build:mkl_aarch64_threadpool --define=build_with_mkl_aarch64=true\n build:mkl_aarch64_threadpool -c opt\n \n+# This is an alias for the mkl_aarch64_threadpool build.\n+build:mkl_aarch64 --config=mkl_aarch64_threadpool\n+\n # Default CUDA, CUDNN and NVSHMEM versions.\n build:cuda_version --repo_env=HERMETIC_CUDA_VERSION=\"12.5.1\"\n build:cuda_version --repo_env=HERMETIC_CUDNN_VERSION=\"9.3.0\""
        },
        {
            "sha": "87019a4cbabc0677ad401b74c45dd9661012b726",
            "filename": "tensorflow/tensorflow.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/tensorflow%2Ftensorflow.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/tensorflow%2Ftensorflow.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftensorflow.bzl?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -21,7 +21,6 @@ load(\n     \"if_mkl\",\n     \"if_mkl_ml\",\n     \"if_mkldnn_aarch64_acl\",\n-    \"if_mkldnn_aarch64_acl_openmp\",\n     \"if_mkldnn_openmp\",\n     \"onednn_v3_define\",\n )\n@@ -467,7 +466,6 @@ def tf_copts(\n         if_mkldnn_openmp([\"-DENABLE_ONEDNN_OPENMP\"]) +\n         onednn_v3_define() +\n         if_mkldnn_aarch64_acl([\"-DDNNL_AARCH64_USE_ACL=1\"]) +\n-        if_mkldnn_aarch64_acl_openmp([\"-DENABLE_ONEDNN_OPENMP\"]) +\n         if_zendnn([\"-DAMD_ZENDNN\"]) +\n         if_enable_acl([\"-DXLA_CPU_USE_ACL=1\", \"-fexceptions\"]) +\n         if_llvm_aarch32_available([\"-DTF_LLVM_AARCH32_AVAILABLE=1\"]) +"
        },
        {
            "sha": "d2e8cbce304add1170087d038070713837ea52a9",
            "filename": "tensorflow/workspace2.bzl",
            "status": "modified",
            "additions": 10,
            "deletions": 17,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/tensorflow%2Fworkspace2.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/tensorflow%2Fworkspace2.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fworkspace2.bzl?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -238,33 +238,26 @@ def _tf_repositories():\n         name = \"mkl_dnn_acl_compatible\",\n         build_file = \"@local_xla//third_party/mkl_dnn:mkldnn_acl.BUILD\",\n         patch_file = [\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_threadcap.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_reorder.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_thread_local_scheduler.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_fp32_bf16_reorder.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_bf16_capability_detection_for_ubuntu20.04.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_indirect_conv.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_allow_blocked_weight_format_for_matmul_primitive.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_fix_segfault_during_postop_execute.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_add_bf16_platform_support_check.patch\",\n-            \"@local_xla//third_party/mkl_dnn:onednn_acl_add_sbgemm_matmul_primitive_definition.patch\",\n+            \"@local_xla//third_party/mkl_dnn:onednn_acl_lock_fixed_format_matmul.patch\",\n+            \"@local_xla//third_party/mkl_dnn:onednn_acl_threadpool_default_max.patch\",\n         ],\n-        sha256 = \"2f76b407ef8893cca71340f88cd800019a1f14f8ac1bbdbb89a84be1370b52e3\",\n-        strip_prefix = \"oneDNN-3.2.1\",\n-        urls = tf_mirror_urls(\"https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.2.1.tar.gz\"),\n+        sha256 = \"5792cbc07764c6e25c459ff68efb5cfcd7f4a0ba66dca6a4a2c681cd7a644596\",\n+        strip_prefix = \"oneDNN-3.7\",\n+        urls = tf_mirror_urls(\"https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.7.zip\"),\n     )\n \n     tf_http_archive(\n         name = \"compute_library\",\n         patch_file = [\n+            \"@local_xla//third_party/compute_library:acl_gemm_scheduling_heuristic.patch\",\n+            \"@local_xla//third_party/compute_library:acl_stateless_gemm_workspace.patch\",\n             \"@local_xla//third_party/compute_library:compute_library.patch\",\n-            \"@local_xla//third_party/compute_library:acl_thread_local_scheduler.patch\",\n             \"@local_xla//third_party/compute_library:exclude_omp_scheduler.patch\",\n             \"@local_xla//third_party/compute_library:include_string.patch\",\n         ],\n-        sha256 = \"c4ca329a78da380163b2d86e91ba728349b6f0ee97d66e260a694ef37f0b0d93\",\n-        strip_prefix = \"ComputeLibrary-23.05.1\",\n-        urls = tf_mirror_urls(\"https://github.com/ARM-software/ComputeLibrary/archive/v23.05.1.tar.gz\"),\n+        sha256 = \"8273f68cd0bb17e9231a11a6618d245eb6d623884ae681c00e7a4eabca2dad42\",\n+        strip_prefix = \"ComputeLibrary-24.12\",\n+        urls = tf_mirror_urls(\"https://github.com/ARM-software/ComputeLibrary/archive/refs/tags/v24.12.tar.gz\"),\n     )\n \n     tf_http_archive("
        },
        {
            "sha": "679a9d5fbfe2cf3f8e5039aaee87dcf22d21d580",
            "filename": "third_party/xla/tensorflow.bazelrc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Ftensorflow.bazelrc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Ftensorflow.bazelrc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftensorflow.bazelrc?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -172,17 +172,14 @@ build:mkl_threadpool --define=tensorflow_mkldnn_contraction_kernel=0\n build:mkl_threadpool --define=build_with_mkl_opensource=true\n build:mkl_threadpool -c opt\n \n-# Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).\n-build:mkl_aarch64 --define=build_with_mkl_aarch64=true\n-build:mkl_aarch64 --define=build_with_openmp=true\n-build:mkl_aarch64 --define=build_with_acl=true\n-build:mkl_aarch64 -c opt\n-\n # Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).\n # with Eigen threadpool support\n build:mkl_aarch64_threadpool --define=build_with_mkl_aarch64=true\n build:mkl_aarch64_threadpool -c opt\n \n+# This is an alias for the mkl_aarch64_threadpool build.\n+build:mkl_aarch64 --config=mkl_aarch64_threadpool\n+\n # Default CUDA, CUDNN and NVSHMEM versions.\n build:cuda_version --repo_env=HERMETIC_CUDA_VERSION=\"12.9.1\"\n build:cuda_version --repo_env=HERMETIC_CUDNN_VERSION=\"9.8.0\""
        },
        {
            "sha": "cfd54bfdc6a1865d1db518f1dff57ad96b1acc33",
            "filename": "third_party/xla/third_party/compute_library/acl_gemm_scheduling_heuristic.patch",
            "status": "added",
            "additions": 145,
            "deletions": 0,
            "changes": 145,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_gemm_scheduling_heuristic.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_gemm_scheduling_heuristic.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_gemm_scheduling_heuristic.patch?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -0,0 +1,145 @@\n+# Copyright 2025 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+diff --git a/src/cpu/kernels/assembly/arm_gemm.hpp b/src/cpu/kernels/assembly/arm_gemm.hpp\n+index cbc8be416..e65bc00a0 100644\n+--- a/src/cpu/kernels/assembly/arm_gemm.hpp\n++++ b/src/cpu/kernels/assembly/arm_gemm.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2018-2022, 2024 Arm Limited.\n++ * Copyright (c) 2018-2022, 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -44,9 +44,7 @@ enum class GemmMethod\n+     GEMM_NATIVE,\n+     GEMM_HYBRID,\n+     GEMM_INTERLEAVED,\n+-    GEMM_INTERLEAVED_2D,\n+     QUANTIZE_WRAPPER,\n+-    QUANTIZE_WRAPPER_2D,\n+     GEMM_HYBRID_QUANTIZED\n+ };\n+ \n+diff --git a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n+index fc106140f..ec2039207 100644\n+--- a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n++++ b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2018-2024 Arm Limited.\n++ * Copyright (c) 2018-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -137,34 +137,6 @@ Params extract_parameters(const ITensorInfo *a, const ITensorInfo *b, const ITen\n+     return p;\n+ }\n+ \n+-IScheduler::Hints scheduling_hint_heuristic(arm_gemm::GemmMethod method, DataType data_type)\n+-{\n+-    // Schedule assembly kernel\n+-    const int         granule_threshold = 200;\n+-    IScheduler::Hints scheduling_hint   = IScheduler::Hints(Window::DimX);\n+-    if (method == arm_gemm::GemmMethod::GEMM_INTERLEAVED && data_type == DataType::F32)\n+-    {\n+-        scheduling_hint = IScheduler::Hints(Window::DimX, IScheduler::StrategyHint::DYNAMIC, granule_threshold);\n+-    }\n+-    else if (method == arm_gemm::GemmMethod::GEMM_INTERLEAVED_2D &&\n+-             (data_type == DataType::F32 || data_type == DataType::F16 || data_type == DataType::U8 ||\n+-              data_type == DataType::S8))\n+-    {\n+-        //GEMM_INTERLEAVED supports 2D parallelism, IScheduler::split_dimensions_all signals to parallelise over all window dimensions\n+-        scheduling_hint =\n+-            IScheduler::Hints(IScheduler::split_dimensions_all, IScheduler::StrategyHint::STATIC, granule_threshold);\n+-    }\n+-    else if (method == arm_gemm::GemmMethod::QUANTIZE_WRAPPER_2D &&\n+-             (data_type == DataType::QASYMM8 || data_type == DataType::QASYMM8_SIGNED))\n+-    {\n+-        //special case for QASYMM8 to support 2D parallelism, scheduler here may be tweaked differently compared to FP32 case\n+-        scheduling_hint =\n+-            IScheduler::Hints(IScheduler::split_dimensions_all, IScheduler::StrategyHint::STATIC, granule_threshold);\n+-    }\n+-\n+-    return scheduling_hint;\n+-}\n+-\n+ /** Fallback in case ACL doesn't have a function */\n+ template <typename TypeInput, typename TypeWeight, typename TypeOutput, class OutputStage = arm_gemm::Nothing>\n+ class Fallback : public CpuGemmAssemblyDispatch::IFallback\n+@@ -300,8 +272,6 @@ private:\n+     bool _is_prepared{false};\n+     /** GEMM meta-data */\n+     AsmGemmInfo _gemm_info{};\n+-    /** GEMM kernel description */\n+-    arm_gemm::KernelDescription _kernel_info{};\n+     /** Per channel quantization shifts */\n+     std::vector<int32_t> _shifts{};\n+     std::vector<int32_t> right_shifts{};\n+@@ -760,7 +730,20 @@ void Fallback<TypeInput, TypeWeight, TypeOutput, OutputStage>::run(ITensorPack &\n+         }\n+     }\n+ \n+-    const auto scheduling_hint = scheduling_hint_heuristic(_kernel_info.method, d->info()->data_type());\n++    // The scheduling_hint needs to be compatible with the window exposed by arm_gemm\n++    // The default case is when we split among the X dimension\n++    IScheduler::Hints scheduling_hint = IScheduler::Hints(Window::DimX);\n++    // If arm_gemm exposes a 2D window, perform 2D scheduling\n++    if (_optimised_kernel->window().num_iterations(Window::DimY) > 1 &&\n++        _optimised_kernel->window().num_iterations(Window::DimX) > 1)\n++    {\n++        scheduling_hint = IScheduler::Hints(IScheduler::split_dimensions_all);\n++    }\n++    // Split among Y\n++    else if (_optimised_kernel->window().num_iterations(Window::DimY) > 1)\n++    {\n++        scheduling_hint = IScheduler::Hints(Window::DimY);\n++    }\n+ \n+     // Set workspace if needed and reset number of threads as buffer manager gets re-created with max_threads\n+     CpuAuxTensorHandler workspace(offset_int_vec(AsmGemmWorkspace), _workspace_info, tensors, false);\n+diff --git a/src/runtime/IScheduler.cpp b/src/runtime/IScheduler.cpp\n+index 2dd87310a..9fa815fbd 100644\n+--- a/src/runtime/IScheduler.cpp\n++++ b/src/runtime/IScheduler.cpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2016-2024 Arm Limited.\n++ * Copyright (c) 2016-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -69,9 +69,20 @@ void IScheduler::schedule_common(ICPPKernel *kernel, const Hints &hints, const W\n+         const std::size_t m = max_window.num_iterations(Window::DimX);\n+         const std::size_t n = max_window.num_iterations(Window::DimY);\n+ \n++        const unsigned int num_iterations = m * n;\n++        const unsigned int num_threads    = std::min(num_iterations, this->num_threads());\n++\n+         //in c++17 this can be swapped for   auto [ m_threads, n_threads ] = split_2d(...\n+         unsigned m_threads, n_threads;\n+-        std::tie(m_threads, n_threads) = scheduler_utils::split_2d(this->num_threads(), m, n);\n++        std::tie(m_threads, n_threads) = scheduler_utils::split_2d(num_threads, m, n);\n++\n++        // Clamp m_threads and n_threads if not all threads have work to do\n++        unsigned int max_parallelism = std::min<unsigned int>(m, m_threads) * std::min<unsigned int>(n, n_threads);\n++        if (max_parallelism < num_threads)\n++        {\n++            m_threads = std::min<unsigned int>(m, m_threads);\n++            n_threads = std::min<unsigned int>(n, n_threads);\n++        }\n+ \n+         std::vector<IScheduler::Workload> workloads;\n+         for (unsigned int ni = 0; ni != n_threads; ++ni)"
        },
        {
            "sha": "4abc4380b32c1e8340ea0ee123cc735154ad8ed1",
            "filename": "third_party/xla/third_party/compute_library/acl_stateless_gemm_workspace.patch",
            "status": "added",
            "additions": 1463,
            "deletions": 0,
            "changes": 1463,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_stateless_gemm_workspace.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_stateless_gemm_workspace.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_stateless_gemm_workspace.patch?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -0,0 +1,1463 @@\n+# Copyright 2025 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+diff --git a/arm_compute/runtime/experimental/low_level/CpuGemmAssemblyDispatch.h b/arm_compute/runtime/experimental/low_level/CpuGemmAssemblyDispatch.h\n+index 759ff120e..dbbc663c5 100644\n+--- a/arm_compute/runtime/experimental/low_level/CpuGemmAssemblyDispatch.h\n++++ b/arm_compute/runtime/experimental/low_level/CpuGemmAssemblyDispatch.h\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2018-2024 Arm Limited.\n++ * Copyright (c) 2018-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -26,7 +26,6 @@\n+ #define ACL_ARM_COMPUTE_RUNTIME_EXPERIMENTAL_LOW_LEVEL_CPUGEMMASSEMBLYDISPATCH_H\n+ \n+ #include \"arm_compute/core/ITensorPack.h\"\n+-#include \"arm_compute/core/TensorInfo.h\"\n+ #include \"arm_compute/function_info/GEMMInfo.h\"\n+ #include \"arm_compute/runtime/IOperator.h\"\n+ \n+@@ -150,6 +149,11 @@ public:\n+                                const GEMMInfo            &gemm_info = GEMMInfo());\n+ \n+     /** Indicates whether or not there is a implementation for the configured GEMM\n++     *\n++     * @deprecated All fixed-format kernels are now stateless.\n++     * For now this function will always return true, but it will be removed\n++     * completely in a future release.\n++     *\n+      * @return a bool: true if the implementation is stateless; false if not.\n+      */\n+     bool has_stateless_impl() const;\n+diff --git a/src/core/NEON/kernels/arm_gemm/.clang-format b/src/core/NEON/kernels/arm_gemm/.clang-format\n+new file mode 100644\n+index 000000000..bd90a154e\n+--- /dev/null\n++++ b/src/core/NEON/kernels/arm_gemm/.clang-format\n+@@ -0,0 +1,25 @@\n++# Copyright (c) 2025 Arm Limited.\n++#\n++# SPDX-License-Identifier: MIT\n++#\n++# Permission is hereby granted, free of charge, to any person obtaining a copy\n++# of this software and associated documentation files (the \"Software\"), to\n++# deal in the Software without restriction, including without limitation the\n++# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n++# sell copies of the Software, and to permit persons to whom the Software is\n++# furnished to do so, subject to the following conditions:\n++#\n++# The above copyright notice and this permission notice shall be included in all\n++# copies or substantial portions of the Software.\n++#\n++# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n++# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n++# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n++# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n++# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n++# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n++# SOFTWARE.\n++---\n++# Disabling clang-format for this directory to reduce the diff-noise when\n++# porting changes from arm_gemm\n++DisableFormat: true\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_hybrid.hpp b/src/core/NEON/kernels/arm_gemm/gemm_hybrid.hpp\n+index ec8731994..15cca5c79 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_hybrid.hpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_hybrid.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2017-2021, 2024 Arm Limited.\n++ * Copyright (c) 2017-2021, 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -30,7 +30,6 @@\n+ #include \"bias_adder.hpp\"\n+ #include \"ndrange.hpp\"\n+ #include \"performance_parameters.hpp\"\n+-#include \"transform.hpp\"\n+ #include \"utils.hpp\"\n+ \n+ #ifdef CYCLE_PROFILING\n+@@ -145,8 +144,8 @@ public:\n+         return true;\n+     }\n+ \n+-    // Stateless execute\n+-    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &, int, GemmArrays<To, To, Tr>& g_array) override {\n++    // Common execution logic.\n++    void execute_common(const ndcoord_t &work_range, const ndcoord_t &, int, GemmArrays<To, To, Tr>& g_arrays) {\n+ #ifdef CYCLE_PROFILING\n+         profiler prof;\n+ #endif\n+@@ -190,27 +189,33 @@ public:\n+                 auto p = prof.ScopedProfiler(PROFILE_KERNEL, (unsigned long)(m_end - m_start) * kern_k * roundup(nmax-n0, strategy::out_width()));\n+ #endif\n+ \n+-                strat.kernel(g_array._Aptr + (multi * g_array._A_multi_stride) + (batch * g_array._A_batch_stride) + (m_start * g_array._lda) + k0, g_array._lda,\n++                strat.kernel(g_arrays._Aptr + (multi * g_arrays._A_multi_stride) + (batch * g_arrays._A_batch_stride) + (m_start * g_arrays._lda) + k0, g_arrays._lda,\n+                              b_panel,\n+-                             g_array._Cptr + (multi * g_array._C_multi_stride) + (batch * g_array._C_batch_stride) + (m_start * g_array._ldc) + n0, g_array._ldc,\n++                             g_arrays._Cptr + (multi * g_arrays._C_multi_stride) + (batch * g_arrays._C_batch_stride) + (m_start * g_arrays._ldc) + n0, g_arrays._ldc,\n+                              (m_end - m_start), (nmax - n0), kmax-k0,\n+-                             (strategy::supports_bias() && first_pass && g_array._bias) ? g_array._bias + (multi * g_array._bias_multi_stride) + n0 : nullptr,\n++                             (strategy::supports_bias() && first_pass && g_arrays._bias) ? g_arrays._bias + (multi * g_arrays._bias_multi_stride) + n0 : nullptr,\n+                              last_pass ? _act : Activation(), !first_pass);\n+ \n+                 // Add bias externally if needed\n+-                if (!strategy::supports_bias() && g_array._bias && first_pass) {\n+-                    bias_adder(g_array._Cptr + (multi * g_array._C_multi_stride) + (batch * g_array._C_batch_stride) + (m_start * g_array._ldc) + n0, g_array._ldc,\n+-                               g_array._bias + (multi * g_array._bias_multi_stride) + n0,\n++                if (!strategy::supports_bias() && g_arrays._bias && first_pass) {\n++                    bias_adder(g_arrays._Cptr + (multi * g_arrays._C_multi_stride) + (batch * g_arrays._C_batch_stride) + (m_start * g_arrays._ldc) + n0, g_arrays._ldc,\n++                               g_arrays._bias + (multi * g_arrays._bias_multi_stride) + n0,\n+                                (m_end - m_start), (nmax - n0));\n+                 }\n+ \n+             } while (p.next_dim1());\n+         }\n++\n++    }\n++\n++    // Stateless execute\n++    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid, GemmArrays<To, To, Tr> &g_arrays) override {\n++        return execute_common(work_range, thread_locator, threadid, g_arrays);\n+     }\n+ \n+     // Execute\n+     void execute(const ndcoord_t &work_range, const ndcoord_t & thread_locator, int threadid) override {\n+-        execute_stateless(work_range, thread_locator, threadid, this->_gemm_array);\n++        execute_common(work_range, thread_locator, threadid, this->_gemm_arrays);\n+     }\n+ \n+     // Interface implementation - pretransposed\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_hybrid_indirect.hpp b/src/core/NEON/kernels/arm_gemm/gemm_hybrid_indirect.hpp\n+index 4d11f042e..9f4b6da33 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_hybrid_indirect.hpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_hybrid_indirect.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2017-2024 Arm Limited.\n++ * Copyright (c) 2017-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -31,12 +31,10 @@\n+ #include <cassert>\n+ \n+ #include \"arm_gemm.hpp\"\n+-#include \"bias_adder.hpp\"\n+ #include \"convolver.hpp\"\n+ #include \"kernel_weight_format.hpp\"\n+ #include \"ndrange.hpp\"\n+ #include \"performance_parameters.hpp\"\n+-#include \"transform.hpp\"\n+ #include \"utils.hpp\"\n+ \n+ #ifdef CYCLE_PROFILING\n+@@ -423,8 +421,8 @@ public:\n+         return true;\n+     }\n+ \n+-    // Stateless execute\n+-    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &, int, GemmArrays<To, Tw, Tr>& g_array) override {\n++    // Common execution logic.\n++    void execute_common(const ndcoord_t &work_range, const ndcoord_t &, int, GemmArrays<To, Tw, Tr>& g_arrays) {\n+ #ifdef CYCLE_PROFILING\n+         profiler prof;\n+ #endif\n+@@ -452,7 +450,6 @@ public:\n+         /* Make sure we've been set up correctly. */\n+         assert(FixedFormat || _B_transposed);\n+         static_assert(std::is_same<To, Tloi>::value, \"gemm_native: Operand types must be the same.\");\n+-//        static_assert(std::is_same<Tr, Tri>::value, \"gemm_native: Result types must be the same.\");\n+ \n+         /* For now, each work item implies all the K for a given output\n+          * pixel (so we don't need to synchronize access to the output\n+@@ -504,9 +501,9 @@ public:\n+ \n+                 const Troi *b_panel;\n+                 if (FixedFormat) {\n+-                    b_panel = reinterpret_cast<const Troi *>(g_array._Bptr) +\n+-                               (multi * g_array._B_multi_stride) +\n+-                               ((n0 / stripe_width<strategy, FixedFormat>::get()) * g_array._ldb) +\n++                    b_panel = reinterpret_cast<const Troi *>(g_arrays._Bptr) +\n++                               (multi * g_arrays._B_multi_stride) +\n++                               ((n0 / stripe_width<strategy, FixedFormat>::get()) * g_arrays._ldb) +\n+                                (k0 * stripe_width<strategy, FixedFormat>::get());\n+                 } else {\n+                     b_panel = _B_transposed +\n+@@ -515,7 +512,7 @@ public:\n+                                (n0 * kern_k);\n+                 }\n+ \n+-                IndirectOutputArg<Tr> out_arg(g_array._Cptr + (multi * g_array._C_multi_stride) + (batch * g_array._C_batch_stride) + (m_start * g_array._ldc) + n0, g_array._ldc);\n++                IndirectOutputArg<Tr> out_arg(g_arrays._Cptr + (multi * g_arrays._C_multi_stride) + (batch * g_arrays._C_batch_stride) + (m_start * g_arrays._ldc) + n0, g_arrays._ldc);\n+ \n+ #ifdef CYCLE_PROFILING\n+                 auto p = prof.ScopedProfiler(PROFILE_KERNEL, (unsigned long)(m_end - m_start) * kern_k * roundup(nmax-n0, strategy::out_width()));\n+@@ -527,14 +524,14 @@ public:\n+ #endif\n+                                  strat, sections, string_lengths.data(),\n+                                  IndirectInputArg<To>(_indirect_buf + (multi * _args._nbatches * _args._Ksections) + (batch * _args._Ksections) + first_section, m_start, first_offset),\n+-                                 (m_end - m_start), (nmax - n0), kern_k, b_panel, g_array._ldb, out_arg,\n+-                                 (g_array._bias && first_pass) ? g_array._bias + (multi * g_array._bias_multi_stride) + n0 : nullptr,\n++                                 (m_end - m_start), (nmax - n0), kern_k, b_panel, g_arrays._ldb, out_arg,\n++                                 (g_arrays._bias && first_pass) ? g_arrays._bias + (multi * g_arrays._bias_multi_stride) + n0 : nullptr,\n+                                  last_pass ? _args._act : Activation(),\n+                                  !first_pass || _args._accumulate,\n+                                  // Quantization parameters\n+                                  _os, _col_bias+(multi * _args._Nsize), n0);\n+                 } else if (_convolver) {\n+-                    auto conv_cols = _convolver->process_columns(g_array._Aptr + (multi * g_array._A_multi_stride) + (batch * g_array._A_batch_stride), g_array._lda, k0, kmax, _rounded_Ksize);\n++                    auto conv_cols = _convolver->process_columns(g_arrays._Aptr + (multi * g_arrays._A_multi_stride) + (batch * g_arrays._A_batch_stride), g_arrays._lda, k0, kmax, _rounded_Ksize);\n+ \n+                     unsigned int pos=0;\n+                     auto conv_rows = conv_cols.process_rows(m_start, m_end - m_start);\n+@@ -560,8 +557,8 @@ public:\n+ #endif\n+                                  strat, sections, string_lengths.data(),\n+                                  IndirectInputArg<To>(in_row_strings.data(), 0, first_offset),\n+-                                 (m_end - m_start), (nmax - n0), kern_k, b_panel, g_array._ldb, out_arg,\n+-                                 (g_array._bias && first_pass) ? g_array._bias + (multi * g_array._bias_multi_stride) + n0 : nullptr,\n++                                 (m_end - m_start), (nmax - n0), kern_k, b_panel, g_arrays._ldb, out_arg,\n++                                 (g_arrays._bias && first_pass) ? g_arrays._bias + (multi * g_arrays._bias_multi_stride) + n0 : nullptr,\n+                                  last_pass ? _args._act : Activation(),\n+                                  !first_pass || _args._accumulate,\n+                                  // Quantization parameters\n+@@ -575,9 +572,9 @@ public:\n+                                  prof,\n+ #endif\n+                                  strat, 1, &len,\n+-                                 IndirectInputArg<To>(g_array._Aptr + (multi * g_array._A_multi_stride) + (batch * g_array._A_batch_stride) + m_start * g_array._lda + k0, g_array._lda),\n+-                                 (m_end - m_start), (nmax - n0), kern_k, b_panel, g_array._ldb, out_arg,\n+-                                 (g_array._bias && first_pass) ? g_array._bias + (multi * g_array._bias_multi_stride) + n0 : nullptr,\n++                                 IndirectInputArg<To>(g_arrays._Aptr + (multi * g_arrays._A_multi_stride) + (batch * g_arrays._A_batch_stride) + m_start * g_arrays._lda + k0, g_arrays._lda),\n++                                 (m_end - m_start), (nmax - n0), kern_k, b_panel, g_arrays._ldb, out_arg,\n++                                 (g_arrays._bias && first_pass) ? g_arrays._bias + (multi * g_arrays._bias_multi_stride) + n0 : nullptr,\n+                                  last_pass ? _args._act : Activation(),\n+                                  !first_pass || _args._accumulate,\n+                                  // Quantization parameters\n+@@ -587,9 +584,14 @@ public:\n+         }\n+     }\n+ \n++    // Stateless execute\n++    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid, GemmArrays<To, Tw, Tr>& g_arrays) override {\n++        return execute_common(work_range, thread_locator, threadid, g_arrays);\n++    }\n++\n+     // Execute\n+     void execute(const ndcoord_t &work_range, const ndcoord_t & thread_locator, int threadid) override {\n+-        execute_stateless(work_range, thread_locator, threadid, this->_gemm_array);\n++        execute_common(work_range, thread_locator, threadid, this->_gemm_arrays);\n+     }\n+ \n+     // Interface implementation - pretransposed\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_hybrid_quantized.hpp b/src/core/NEON/kernels/arm_gemm/gemm_hybrid_quantized.hpp\n+index 073012e5a..1993f7d4d 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_hybrid_quantized.hpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_hybrid_quantized.hpp\n+@@ -31,9 +31,6 @@\n+ #include \"ndrange.hpp\"\n+ #include \"utils.hpp\"\n+ \n+-#include \"mergeresults.hpp\"\n+-#include \"transform.hpp\"\n+-\n+ #ifdef CYCLE_PROFILING\n+ #include \"profiler.hpp\"\n+ #endif\n+@@ -70,8 +67,6 @@ class GemmHybridQuantized : public GemmCommon<To, To, Tr> {\n+     int32_t *row_bias = nullptr;\n+     int32_t *col_bias = nullptr;\n+ \n+-    void *working_space = nullptr;\n+-\n+     unsigned int _nthreads;\n+ \n+     unsigned int get_col_sum_size() const {\n+@@ -171,20 +166,17 @@ public:\n+         return true;\n+     }\n+ \n+-    // Stateless execute\n+-    // TODO: Make this actually stateless. This still uses the stateful\n+-    // execution data because it requires a workspace which would also need to\n+-    // be handled statelessly.\n+-    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &, int threadid, GemmArrays<To, To, Tr> &) override {\n+-        auto& g_array = this->_gemm_array;\n++    // Common execution logic.\n++    void execute_common(const ndcoord_t &work_range, const ndcoord_t &, int threadid, GemmArrays<To, To, Tr> &g_arrays) {\n+ #ifdef CYCLE_PROFILING\n+         profiler prof;\n+ #endif\n+         strategy strat(_ci);\n+ \n+-        uintptr_t working_int = reinterpret_cast<uintptr_t>(working_space);\n++        void *working_space = g_arrays._workspace;\n++        auto working_int = reinterpret_cast<uintptr_t>(working_space);\n+ \n+-        Tri *result_buffer = reinterpret_cast<Tri *>(working_int + (threadid * strategy::out_height() * _Nsize * sizeof(Tri)));\n++        auto *result_buffer = reinterpret_cast<Tri *>(working_int + (threadid * strategy::out_height() * _Nsize * sizeof(Tri)));\n+ \n+         /* Make sure we've been set up correctly. */\n+         assert(_B_transposed);\n+@@ -222,7 +214,7 @@ public:\n+ #ifdef CYCLE_PROFILING\n+                     auto p = prof.ScopedProfiler(PROFILE_KERNEL, (m_end - m_start) * kern_k * roundup(nmax-n0, strategy::out_width()));\n+ #endif\n+-                    strat.kernel(g_array._Aptr + (multi * g_array._A_multi_stride) + (batch * g_array._A_batch_stride) + (m_start * g_array._lda) + k0, g_array._lda,\n++                    strat.kernel(g_arrays._Aptr + (multi * g_arrays._A_multi_stride) + (batch * g_arrays._A_batch_stride) + (m_start * g_arrays._lda) + k0, g_arrays._lda,\n+                                  b_panel,\n+                                  result_buffer, (nmax-n0),\n+                                  (m_end - m_start), (nmax - n0), kern_k,\n+@@ -234,7 +226,7 @@ public:\n+                     auto p = prof.ScopedProfiler(PROFILE_ROWSUMS, (m_end - m_start) * _Ksize);\n+ #endif\n+                     compute_row_sums(_qp, _Ksize, (m_end - m_start),\n+-                                     g_array._Aptr + (multi * g_array._A_multi_stride) + (batch * g_array._A_batch_stride) + (m_start * g_array._lda), g_array._lda,\n++                                     g_arrays._Aptr + (multi * g_arrays._A_multi_stride) + (batch * g_arrays._A_batch_stride) + (m_start * g_arrays._lda), g_arrays._lda,\n+                                      local_row_sums);\n+                 }\n+ \n+@@ -244,16 +236,21 @@ public:\n+ #endif\n+ \n+                     requantize_block_32(_qp, (nmax - n0), (m_end - m_start), result_buffer, (nmax - n0),\n+-                                        g_array._Cptr + (multi * g_array._C_multi_stride) + (batch * g_array._C_batch_stride) + (m_start * g_array._ldc) + n0, g_array._ldc,\n++                                        g_arrays._Cptr + (multi * g_arrays._C_multi_stride) + (batch * g_arrays._C_batch_stride) + (m_start * g_arrays._ldc) + n0, g_arrays._ldc,\n+                                         local_row_sums, col_bias + (multi * _Nsize) + n0, n0);\n+                 }\n+             } while (p.next_dim0());\n+         }\n+     }\n+ \n++    // Stateless execute\n++    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid, GemmArrays<To, To, Tr> &g_arrays) override {\n++        return execute_common(work_range, thread_locator, threadid, g_arrays);\n++    }\n++\n+     // Execute\n+     void execute(const ndcoord_t &work_range, const ndcoord_t & thread_locator, int threadid) override {\n+-        execute_stateless(work_range, thread_locator, threadid, this->_gemm_array);\n++        execute_common(work_range, thread_locator, threadid, this->_gemm_arrays);\n+     }\n+ \n+     // Working space needed for intermediate result buffers.\n+@@ -262,7 +259,7 @@ public:\n+     }\n+ \n+     void set_working_space(void *buffer) override {\n+-        working_space = buffer;\n++        this->_gemm_arrays._workspace = buffer;\n+     }\n+ \n+     // Interface implementation - pretransposed\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp b/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp\n+index 6e1ea6589..5da2ed352 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2017-2024 Arm Limited.\n++ * Copyright (c) 2017-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -31,10 +31,8 @@\n+ #include \"convolver.hpp\"\n+ #include \"kernel_traits.hpp\"\n+ #include \"kernel_weight_format.hpp\"\n+-#include \"mergeresults.hpp\"\n+ #include \"performance_parameters.hpp\"\n+ #include \"quantized.hpp\"\n+-#include \"transform.hpp\"\n+ #include \"utils.hpp\"\n+ \n+ #ifdef CYCLE_PROFILING\n+@@ -418,6 +416,11 @@ struct get_kernel_weight_format<strategy, true, Tro> {\n+     }\n+ };\n+ \n++// Calculate the offset needed if the address is not a multiple of cache-line length\n++inline size_t get_cache_align_offset(uintptr_t addr) {\n++    return (addr & 0x3F) ? 0x40 - (addr & 0x3F) : 0;\n++}\n++\n+ } // anonymous namespace\n+ \n+ template<typename strategy, typename Tlo, typename Tro, typename Tr, typename OutputStage=Nothing, bool MergeStep=true, bool FixedFormat=false, bool ForceThreadColumns=false, bool ForceFloatAccumulate=false>\n+@@ -455,9 +458,6 @@ class GemmInterleaved : public GemmCommon<Tlo, Tro, Tr> {\n+ \n+     /* Working space, pretransposed buffer, buffer manager */\n+     const Troi *_B_transposed=nullptr;\n+-    void *_working_space=nullptr;\n+-\n+-    Tab *_accumulation_buffer=nullptr;\n+ \n+     /* Output stage */\n+     OutputStage  _os;\n+@@ -600,10 +600,25 @@ class GemmInterleaved : public GemmCommon<Tlo, Tro, Tr> {\n+         return num_buffers * size_per_buffer;\n+     }\n+ \n++    // Set up accumulation buffer\n++    Tab *get_accumulation_buffer_offset(void *working_space) const {\n++        Tab *accumulation_buffer = nullptr;\n++\n++        auto working_space_addr = reinterpret_cast<uintptr_t>(working_space);\n++\n++        if (get_accumulation_buffer_size() > 0) {\n++            auto acc_buff_addr = working_space_addr + get_a_working_size() + (get_c_working_size() * _maxthreads);\n++            acc_buff_addr += get_cache_align_offset(acc_buff_addr);\n++            accumulation_buffer = reinterpret_cast<Tab *>(acc_buff_addr);\n++        }\n++\n++        return accumulation_buffer;\n++    }\n++\n+     // Get pointer into accumulation buffer\n+-    Tab *get_accumulation_buffer(unsigned int M, unsigned int N, unsigned int batch, unsigned int multi) const {\n++    Tab *get_accumulation_buffer(Tab *accumulation_buffer, unsigned int M, unsigned int N, unsigned int batch, unsigned int multi) const {\n+         // Don't do anything if there's no buffer.\n+-        if (_accumulation_buffer == nullptr) {\n++        if (accumulation_buffer == nullptr) {\n+             return nullptr;\n+         }\n+ \n+@@ -623,7 +638,7 @@ class GemmInterleaved : public GemmCommon<Tlo, Tro, Tr> {\n+ \n+         size_t buffer_index = multi * buffers_per_multi + batch * buffers_per_batch + row * buffer_cols + col;\n+ \n+-        return _accumulation_buffer + (buffer_index * size_per_buffer);\n++        return accumulation_buffer + (buffer_index * size_per_buffer);\n+     }\n+ \n+     int32_t row_sum_multiplier() const {\n+@@ -806,7 +821,7 @@ public:\n+                       _Ksections(args._Ksections), _Ktotal(get_ktotal(args)),\n+                       _rounded_Ksize(roundup(_Ksize, strategy::k_unroll())),\n+                       _nbatches(args._nbatches), _nmulti(args._nmulti), _thread_columns(is_thread_columns(args)),\n+-                      _act(args._act), _accumulate(args._accumulate),  _maxthreads(args._maxthreads), _nthreads(args._maxthreads),\n++                      _act(args._act), _accumulate(args._accumulate), _maxthreads(args._maxthreads), _nthreads(args._maxthreads),\n+                       _k_block(get_k_block_size(args)), _x_block(get_x_block_size(args)), _Mround(roundup(args._Msize, strategy::out_height())),\n+                       _os() { }\n+ \n+@@ -832,27 +847,25 @@ public:\n+         _nthreads = std::min(nthreads, _maxthreads);\n+     }\n+ \n+-    // Stateless execute\n+-    // TODO: Make this actually stateless. This still uses the stateful\n+-    // execution data because it requires a workspace which would also need to\n+-    // be handled statelessly.\n+-    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &, int threadid, GemmArrays<Tlo, Tro, Tr> &) override {\n+-        auto& g_array = this->_gemm_array;\n++    // Common execution logic.\n++    void execute_common(const ndcoord_t &work_range, const ndcoord_t &, int threadid, GemmArrays<Tlo, Tro, Tr> &g_arrays) {\n+ #ifdef CYCLE_PROFILING\n+         profiler prof;\n+ #endif\n+ \n+-        /* Make sure we've been set up correctly. */\n++        void *working_space = g_arrays._workspace;\n++\n++        // Make sure we've been set up correctly.\n+         assert(FixedFormat || _B_transposed);\n+-        assert(_working_space);\n+-        int8_t *working_space_bytes = reinterpret_cast<int8_t *>(_working_space);\n+-\n+-        /* Align if needed */\n+-        intptr_t working_space_v = reinterpret_cast<intptr_t>(_working_space);\n+-        if (working_space_v & 0x3f) {\n+-            intptr_t alignment_offset = 0x40 - (working_space_v & 0x3f);\n+-            working_space_bytes += alignment_offset;\n+-        }\n++        assert(working_space);\n++\n++        // Align if needed.\n++        auto *working_space_bytes = reinterpret_cast<int8_t *>(working_space);\n++        auto working_space_addr = reinterpret_cast<uintptr_t>(working_space);\n++        working_space_bytes += get_cache_align_offset(working_space_addr);\n++        working_space = reinterpret_cast<void *>(working_space_bytes);\n++\n++        auto *accumulation_buffer = get_accumulation_buffer_offset(working_space);\n+ \n+         strategy strat(_ci);\n+ \n+@@ -890,8 +903,8 @@ public:\n+                     unsigned int kern_k = roundup(kmax - k0, strategy::k_unroll());\n+ \n+                     const Troi *b_ptr = FixedFormat ?\n+-                        reinterpret_cast<const Troi *>(g_array._Bptr) + (multi * g_array._B_multi_stride) +\n+-                                                     ((start_x / get_stripe_width<strategy, FixedFormat>::get()) * g_array._ldb) +\n++                        reinterpret_cast<const Troi *>(g_arrays._Bptr) + (multi * g_arrays._B_multi_stride) +\n++                                                     ((start_x / get_stripe_width<strategy, FixedFormat>::get()) * g_arrays._ldb) +\n+                                                      (k0 * get_stripe_width<strategy, FixedFormat>::get()) :\n+                         _B_transposed + (rounded_width * _Ktotal * multi) + (k0 * rounded_width) + (start_x * kern_k);\n+ \n+@@ -916,19 +929,19 @@ public:\n+                                                              _rounded_Ksize, start_row, end_row, k0, kmax, row_sum_multiplier());\n+                             } else if (_convolver) {\n+                                 transforms.PrepareA_convolution(a_panel,\n+-                                                                g_array._Aptr + (batch * g_array._A_batch_stride) + (multi * g_array._A_multi_stride),\n+-                                                                g_array._lda, *_convolver, _rounded_Ksize, start_row, end_row, k0, kmax, row_sum_multiplier());\n++                                                                g_arrays._Aptr + (batch * g_arrays._A_batch_stride) + (multi * g_arrays._A_multi_stride),\n++                                                                g_arrays._lda, *_convolver, _rounded_Ksize, start_row, end_row, k0, kmax, row_sum_multiplier());\n+                             } else {\n+                                 transforms.PrepareA(a_panel,\n+-                                                    g_array._Aptr + (batch * g_array._A_batch_stride) + (multi * g_array._A_multi_stride),\n+-                                                    g_array._lda, start_row, end_row, k0, std::min(kmax, _Ksize), row_sum_multiplier());\n++                                                    g_arrays._Aptr + (batch * g_arrays._A_batch_stride) + (multi * g_arrays._A_multi_stride),\n++                                                    g_arrays._lda, start_row, end_row, k0, std::min(kmax, _Ksize), row_sum_multiplier());\n+                             }\n+                         }\n+ \n+-                        Tr *result_ptr = g_array._Cptr + (batch * g_array._C_batch_stride) + (multi * g_array._C_multi_stride);\n++                        Tr *result_ptr = g_arrays._Cptr + (batch * g_arrays._C_batch_stride) + (multi * g_arrays._C_multi_stride);\n+ \n+                         // If we are using an accumulation buffer and this isn't the last pass, don't pass a result pointer.\n+-                        if (_accumulation_buffer && !last_pass) {\n++                        if (accumulation_buffer && !last_pass) {\n+                             result_ptr = nullptr;\n+                         }\n+ \n+@@ -938,19 +951,19 @@ public:\n+                             prof,\n+                         #endif\n+                             // Strategy and panel pointers\n+-                            strat, a_panel, b_ptr, g_array._ldb, c_panel,\n++                            strat, a_panel, b_ptr, g_arrays._ldb, c_panel,\n+                             // Result buffer pointers\n+-                            result_ptr, g_array._ldc,\n++                            result_ptr, g_arrays._ldc,\n+                             // K size, and M/N ranges\n+                             kern_k, start_row, end_row, start_x, end_x,\n+                             // Only do bias on the first pass\n+-                            ((bias_pass && g_array._bias) ? g_array._bias + (multi * g_array._bias_multi_stride) : nullptr),\n++                            ((bias_pass && g_arrays._bias) ? g_arrays._bias + (multi * g_arrays._bias_multi_stride) : nullptr),\n+                             // Only do activation on the last pass, and accumulation on any non-first pass.\n+                             (last_pass ? _act : Activation()), (!first_pass || _accumulate),\n+                             // Pass in quantization parameters for requantizing kernels (others will ignore)\n+                             _os, col_bias + (multi * _Nsize),\n+                             // Accumulation buffer\n+-                            get_accumulation_buffer(start_row, start_x, batch, multi));\n++                            get_accumulation_buffer(accumulation_buffer, start_row, start_x, batch, multi));\n+ \n+                         /* Increment to the next block */\n+                         start_row += strategy::out_height();\n+@@ -1013,12 +1026,12 @@ public:\n+                                                       _rounded_Ksize, first_m, last_m, current.k0(), current.kmax(), row_sum_multiplier());\n+                         } else if (_convolver) {\n+                             transforms.PrepareA_convolution(a_panel + ((batch * _Mround + first_m) * get_total_k_depth()),\n+-                                                      g_array._Aptr + (batch * g_array._A_batch_stride) + (current.multi() * g_array._A_multi_stride),\n+-                                                      g_array._lda, *_convolver, _rounded_Ksize, first_m, last_m, current.k0(), current.kmax(), row_sum_multiplier());\n++                                                      g_arrays._Aptr + (batch * g_arrays._A_batch_stride) + (current.multi() * g_arrays._A_multi_stride),\n++                                                      g_arrays._lda, *_convolver, _rounded_Ksize, first_m, last_m, current.k0(), current.kmax(), row_sum_multiplier());\n+                         } else {\n+                             transforms.PrepareA(a_panel + ((batch * _Mround + first_m) * get_total_k_depth()),\n+-                                                      g_array._Aptr + (batch * g_array._A_batch_stride) + (current.multi() * g_array._A_multi_stride),\n+-                                                      g_array._lda, first_m, last_m, current.k0(), std::min(_Ksize, current.kmax()), row_sum_multiplier());\n++                                                      g_arrays._Aptr + (batch * g_arrays._A_batch_stride) + (current.multi() * g_arrays._A_multi_stride),\n++                                                      g_arrays._lda, first_m, last_m, current.k0(), std::min(_Ksize, current.kmax()), row_sum_multiplier());\n+                         }\n+                     }\n+ \n+@@ -1038,8 +1051,8 @@ public:\n+ \n+                 // For FixedFormat cases, figure out the B pointer.  The loop below moves through batches and vertically through the output so this will be the same throughout.\n+                 if (FixedFormat) {\n+-                    b_panel = reinterpret_cast<const Troi *>(g_array._Bptr) + (current.multi() * g_array._B_multi_stride) +\n+-                                                                           ((current.x0() / get_stripe_width<strategy, FixedFormat>::get()) * g_array._ldb) +\n++                    b_panel = reinterpret_cast<const Troi *>(g_arrays._Bptr) + (current.multi() * g_arrays._B_multi_stride) +\n++                                                                           ((current.x0() / get_stripe_width<strategy, FixedFormat>::get()) * g_arrays._ldb) +\n+                                                                            (current.k0() * get_stripe_width<strategy, FixedFormat>::get());\n+                 }\n+ \n+@@ -1061,7 +1074,7 @@ public:\n+ \n+                     // But in the case where we have an accumulation buffer, we can't do that after all, unless\n+                     // there is no N blocking.\n+-                    if (_accumulation_buffer && ((current.x0() != 0) || (current.xmax() < _Nsize))) {\n++                    if (accumulation_buffer && ((current.x0() != 0) || (current.xmax() < _Nsize))) {\n+                         m_step = strategy::out_height();\n+                     }\n+ \n+@@ -1075,11 +1088,11 @@ public:\n+                         const bool bias_pass = (std::is_same<OutputStage, DequantizeFloat>::value && !MergeStep) ? last_pass : first_pass;\n+ \n+                         // Pointer to appropriate part of result array.\n+-                        Tr *result_ptr = g_array._Cptr + (batch * g_array._C_batch_stride) + (current.multi() * g_array._C_multi_stride);\n++                        Tr *result_ptr = g_arrays._Cptr + (batch * g_arrays._C_batch_stride) + (current.multi() * g_arrays._C_multi_stride);\n+ \n+                         // If we are using an accumulation buffer, we don't pass the result buffer to ask the kernel\n+                         // to write things into the accumulation buffer instead, except on the last pass.\n+-                        if (_accumulation_buffer && !last_pass) {\n++                        if (accumulation_buffer && !last_pass) {\n+                             result_ptr = nullptr;\n+                         }\n+ \n+@@ -1089,19 +1102,19 @@ public:\n+                             prof,\n+                         #endif\n+                             // Strategy and panel pointers\n+-                            strat, a_ptr, b_panel, g_array._ldb, c_panel,\n++                            strat, a_ptr, b_panel, g_arrays._ldb, c_panel,\n+                             // Result buffer pointers\n+-                            result_ptr, g_array._ldc,\n++                            result_ptr, g_arrays._ldc,\n+                             // K size, and M/N ranges\n+                             kern_k, y, ymax, current.x0(), current.xmax(),\n+                             // Only do bias on the first pass\n+-                            ((bias_pass && g_array._bias) ? g_array._bias + (current.multi() * g_array._bias_multi_stride) : nullptr),\n++                            ((bias_pass && g_arrays._bias) ? g_arrays._bias + (current.multi() * g_arrays._bias_multi_stride) : nullptr),\n+                             // Only do activation on the last pass, and accumulation on any non-first pass.\n+                             (last_pass ? _act : Activation()), (!first_pass || _accumulate),\n+                             // Pass in quantization parameters for requantizing kernels (others will ignore)\n+                             _os, col_bias + (current.multi() * _Nsize),\n+                             // Accumulation buffer\n+-                            get_accumulation_buffer(y, current.x0(), batch, current.multi()) );\n++                            get_accumulation_buffer(accumulation_buffer, y, current.x0(), batch, current.multi()) );\n+ \n+                         a_ptr += (strategy::out_height() * a_panel_stride);\n+                     }\n+@@ -1114,9 +1127,14 @@ public:\n+         }\n+     }\n+ \n++    // Stateless execute\n++    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid, GemmArrays<Tlo, Tro, Tr> &g_arrays) override {\n++        return execute_common(work_range, thread_locator, threadid, g_arrays);\n++    }\n++\n+     // Execute\n+     void execute(const ndcoord_t &work_range, const ndcoord_t & thread_locator, int threadid) override {\n+-        execute_stateless(work_range, thread_locator, threadid, this->_gemm_array);\n++        execute_common(work_range, thread_locator, threadid, this->_gemm_arrays);\n+     }\n+ \n+     // Interface implementation - working space\n+@@ -1131,32 +1149,12 @@ public:\n+ \n+     void set_working_space(void *working_space) override {\n+         // Make sure everything ends up cache line aligned\n+-        int8_t *working_space_bytes = reinterpret_cast<int8_t *>(working_space);\n+-        intptr_t working_space_int = reinterpret_cast<intptr_t>(working_space);\n+-\n+-        size_t diff=0;\n+-\n+-        if (working_space_int & 0x3F) {\n+-            diff = 0x40 - (working_space_int & 0x3F);\n+-        }\n+-\n+-        working_space_bytes += diff;\n+-        working_space_int += diff;\n++        auto *working_space_bytes = reinterpret_cast<int8_t *>(working_space);\n++        auto  working_space_addr = reinterpret_cast<uintptr_t>(working_space);\n++        working_space_bytes += get_cache_align_offset(working_space_addr);\n+ \n+         // Pretransposed case: just set internal pointer to parameter value.\n+-        _working_space = reinterpret_cast<void *>(working_space_bytes);\n+-\n+-        // Set up accumulation buffer\n+-        if (get_accumulation_buffer_size() > 0) {\n+-            intptr_t acc_buff_int = working_space_int + get_a_working_size() + (get_c_working_size() * _maxthreads);\n+-            // Make sure the accumulation buffer is aligned (needed if the other blocks are not a multiple of cache line length)\n+-            if (acc_buff_int & 0x3F) {\n+-                acc_buff_int += (0x40 - (acc_buff_int & 0x3F));\n+-            }\n+-            _accumulation_buffer = reinterpret_cast<Tab *>(acc_buff_int);\n+-        } else {\n+-            _accumulation_buffer = nullptr;\n+-        }\n++        this->_gemm_arrays._workspace = reinterpret_cast<void *>(working_space_bytes);\n+     }\n+ \n+     // Interface implementation - pretransposed\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_q8_mixed.cpp b/src/core/NEON/kernels/arm_gemm/gemm_q8_mixed.cpp\n+index a48244cb3..1c33f56a1 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_q8_mixed.cpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_q8_mixed.cpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2024 Arm Limited.\n++ * Copyright (c) 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -39,10 +39,8 @@\n+ #endif // ARM_COMPUTE_ENABLE_SVE\n+ \n+ #include \"gemm_hybrid_indirect.hpp\"\n+-#include \"gemm_hybrid_quantized.hpp\"\n++#include \"gemm_implementation.hpp\"\n+ #include \"gemm_interleaved.hpp\"\n+-#include \"gemv_pretransposed.hpp\"\n+-#include \"quantize_wrapper.hpp\"\n+ #include \"utils.hpp\"\n+ \n+ namespace arm_gemm {\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_qint8.cpp b/src/core/NEON/kernels/arm_gemm/gemm_qint8.cpp\n+index 18008e713..eb4c947b9 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_qint8.cpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_qint8.cpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2019-2020, 2022-2024 Arm Limited.\n++ * Copyright (c) 2019-2020, 2022-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -58,9 +58,9 @@\n+ \n+ #include \"gemm_hybrid_indirect.hpp\"\n+ #include \"gemm_hybrid_quantized.hpp\"\n++#include \"gemm_implementation.hpp\"\n+ #include \"gemm_interleaved.hpp\"\n+ #include \"gemv_pretransposed.hpp\"\n+-#include \"quantize_wrapper.hpp\"\n+ #include \"utils.hpp\"\n+ \n+ namespace arm_gemm {\n+@@ -241,13 +241,6 @@ GemmImplementation<int8_t, int8_t, int8_t, Requantize32>::with_estimate(\n+     [](const GemmArgs &args, const Requantize32 &) { return GemmInterleavedQuantized<cls_a64_gemm_s8_4x4, int8_t, int8_t, int8_t>::estimate_cycles<int8_t>(args); },\n+     [](const GemmArgs &args, const Requantize32 &qp) { return new GemmInterleavedQuantized<cls_a64_gemm_s8_4x4, int8_t, int8_t, int8_t>(args, qp); }\n+ ),\n+-{\n+-    GemmMethod::QUANTIZE_WRAPPER,\n+-    \"quantized_wrapper\",\n+-    [](const GemmArgs &args, const Requantize32 &) { return !args._indirect_input; },\n+-    [](const GemmArgs &, const Requantize32 &) { return false; },\n+-    [](const GemmArgs &args, const Requantize32 &qp) { return new QuantizeWrapper<int8_t, int8_t, int32_t>(args, qp); }\n+-},\n+ {\n+     GemmMethod::DEFAULT,\n+     \"\",\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_quint8.cpp b/src/core/NEON/kernels/arm_gemm/gemm_quint8.cpp\n+index 7c182b677..c6ee7bff1 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_quint8.cpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_quint8.cpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2019-2020, 2022-2024 Arm Limited.\n++ * Copyright (c) 2019-2020, 2022-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -54,9 +54,9 @@\n+ \n+ #include \"gemm_hybrid_indirect.hpp\"\n+ #include \"gemm_hybrid_quantized.hpp\"\n++#include \"gemm_implementation.hpp\"\n+ #include \"gemm_interleaved.hpp\"\n+ #include \"gemv_pretransposed.hpp\"\n+-#include \"quantize_wrapper.hpp\"\n+ \n+ namespace arm_gemm {\n+ \n+@@ -209,13 +209,6 @@ GemmImplementation<uint8_t, uint8_t, uint8_t, Requantize32>::with_estimate(\n+     [](const GemmArgs &args, const Requantize32 &) { return GemmInterleavedQuantized<cls_a64_gemm_u8_4x4, uint8_t, uint8_t, uint8_t>::estimate_cycles<uint8_t>(args); },\n+     [](const GemmArgs &args, const Requantize32 &qp) { return new GemmInterleavedQuantized<cls_a64_gemm_u8_4x4, uint8_t, uint8_t, uint8_t>(args, qp); }\n+ ),\n+-{\n+-    GemmMethod::QUANTIZE_WRAPPER,\n+-    \"quantized_wrapper\",\n+-    [](const GemmArgs &args, const Requantize32 &) { return !args._indirect_input; },\n+-    [](const GemmArgs &, const Requantize32 &) { return false; },\n+-    [](const GemmArgs &args, const Requantize32 &qp) { return new QuantizeWrapper<uint8_t, uint8_t, uint32_t>(args, qp); }\n+-},\n+ {\n+     GemmMethod::DEFAULT,\n+     \"\",\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemm_u8s8fp32.cpp b/src/core/NEON/kernels/arm_gemm/gemm_u8s8fp32.cpp\n+index 606b422b0..d90245f9a 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemm_u8s8fp32.cpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemm_u8s8fp32.cpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2024 Arm Limited.\n++ * Copyright (c) 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -40,9 +40,9 @@\n+ \n+ #include \"gemm_hybrid_indirect.hpp\"\n+ #include \"gemm_hybrid_quantized.hpp\"\n++#include \"gemm_implementation.hpp\"\n+ #include \"gemm_interleaved.hpp\"\n+ #include \"gemv_pretransposed.hpp\"\n+-#include \"quantize_wrapper.hpp\"\n+ #include \"utils.hpp\"\n+ \n+ namespace arm_gemm {\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemv_batched.hpp b/src/core/NEON/kernels/arm_gemm/gemv_batched.hpp\n+index 0ba7b7870..15941252f 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemv_batched.hpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemv_batched.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2017-2021, 2024 Arm Limited.\n++ * Copyright (c) 2017-2021, 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -64,15 +64,12 @@ public:\n+         _subgemm->set_nthreads(nthreads);\n+     }\n+ \n+-    // TODO: Make this actually stateless. This still uses the stateful\n+-    // execution data because it requires a workspace which would also need to\n+-    // be handled statelessly.\n+     void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid, GemmArrays<To, To, Tr> &) override {\n+         _subgemm->execute(work_range, thread_locator, threadid);\n+     }\n+ \n+     void execute(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid) override {\n+-        execute_stateless(work_range, thread_locator, threadid, this->_gemm_array);\n++        execute_stateless(work_range, thread_locator, threadid, this->_gemm_arrays);\n+     }\n+ \n+     size_t get_working_size() const override {\n+diff --git a/src/core/NEON/kernels/arm_gemm/gemv_pretransposed.hpp b/src/core/NEON/kernels/arm_gemm/gemv_pretransposed.hpp\n+index 08c419252..fb94dd9c3 100644\n+--- a/src/core/NEON/kernels/arm_gemm/gemv_pretransposed.hpp\n++++ b/src/core/NEON/kernels/arm_gemm/gemv_pretransposed.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2017-2022, 2024 Arm Limited.\n++ * Copyright (c) 2017-2022, 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -26,9 +26,6 @@\n+ #include <stdio.h>\n+ \n+ #include \"arm_gemm.hpp\"\n+-#include \"bias_adder.hpp\"\n+-#include \"mergeresults.hpp\"\n+-#include \"transform.hpp\"\n+ \n+ #ifdef CYCLE_PROFILING\n+ #include \"profiler.hpp\"\n+@@ -139,8 +136,8 @@ public:\n+         return { iceildiv(_args._Nsize, strategy::out_width()) * _args._nmulti };\n+     }\n+ \n+-    // Use the stateless interface to execute the GEMV.\n+-    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &, int, GemmArrays<To, To, Tr>& g_array) override {\n++    // Common execution logic.\n++    void execute_common(const ndcoord_t &work_range, const ndcoord_t &, int, GemmArrays<To, To, Tr>& g_arrays) {\n+ #ifdef CYCLE_PROFILING\n+         profiler prof;\n+ #endif\n+@@ -175,11 +172,11 @@ public:\n+ #ifdef CYCLE_PROFILING\n+                     auto p = prof.ScopedProfiler(PROFILE_KERNEL, (kmax-k0) * (nmax-n));\n+ #endif\n+-                    run_gemv_kernel<OutputStage>::run(strat, g_array._Aptr + (multi * g_array._A_multi_stride) + k0,\n++                    run_gemv_kernel<OutputStage>::run(strat, g_arrays._Aptr + (multi * g_arrays._A_multi_stride) + k0,\n+                                  _B_pretransposed + (multi * _buffer_per_multi) + (n * roundup(_args._Ksize, strategy::k_unroll())) + (k0 * strategy::out_width()),\n+-                                 g_array._Cptr + (multi * g_array._C_multi_stride) + n,\n++                                 g_arrays._Cptr + (multi * g_arrays._C_multi_stride) + n,\n+                                  (nmax - n), (kmax-k0),\n+-                                 g_array._bias ? g_array._bias + (multi * g_array._bias_multi_stride) + n : nullptr,\n++                                 g_arrays._bias ? g_arrays._bias + (multi * g_arrays._bias_multi_stride) + n : nullptr,\n+                                  _args._act, (k0 != 0) || _args._accumulate,\n+                                  _os, col_bias, n + (_args._Nsize * multi));\n+                 }\n+@@ -187,9 +184,14 @@ public:\n+         }\n+     }\n+ \n++    // Stateless execute\n++    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid, GemmArrays<To, To, Tr> &g_arrays) override {\n++        return execute_common(work_range, thread_locator, threadid, g_arrays);\n++    }\n++\n+     // Actually execute the GEMV.\n+     void execute(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid) override {\n+-        execute_stateless(work_range, thread_locator, threadid, this->_gemm_array);\n++        execute_common(work_range, thread_locator, threadid, this->_gemm_arrays);\n+     }\n+ \n+     /* Pretransposed interface implementation */\n+diff --git a/src/core/NEON/kernels/arm_gemm/quantize_wrapper.hpp b/src/core/NEON/kernels/arm_gemm/quantize_wrapper.hpp\n+deleted file mode 100644\n+index 90604e941..000000000\n+--- a/src/core/NEON/kernels/arm_gemm/quantize_wrapper.hpp\n++++ /dev/null\n+@@ -1,247 +0,0 @@\n+-/*\n+- * Copyright (c) 2019-2021, 2024 Arm Limited.\n+- *\n+- * SPDX-License-Identifier: MIT\n+- *\n+- * Permission is hereby granted, free of charge, to any person obtaining a copy\n+- * of this software and associated documentation files (the \"Software\"), to\n+- * deal in the Software without restriction, including without limitation the\n+- * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n+- * sell copies of the Software, and to permit persons to whom the Software is\n+- * furnished to do so, subject to the following conditions:\n+- *\n+- * The above copyright notice and this permission notice shall be included in all\n+- * copies or substantial portions of the Software.\n+- *\n+- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+- * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+- * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+- * SOFTWARE.\n+- */\n+-\n+-#pragma once\n+-\n+-#include \"arm_gemm.hpp\"\n+-\n+-#include \"barrier.hpp\"\n+-#include \"gemm_implementation.hpp\"\n+-#include \"quantized.hpp\"\n+-\n+-namespace arm_gemm {\n+-\n+-/* Quantized wrapper - do an integer GEMM and wrap around the quantization. */\n+-\n+-template<typename To, typename Tr, typename Tgemm>\n+-class QuantizeWrapper : public GemmCommon<To, To, Tr> {\n+-private:\n+-    UniqueGemmCommon<To, To, Tgemm>  _subgemm = nullptr;\n+-    int32_t                     *_row_sums = nullptr;\n+-    int32_t                     *_col_sums = nullptr;\n+-    Requantize32                 _params;\n+-    GemmArgs                     _args;\n+-    barrier                      _barrier;\n+-\n+-    void *working_space = nullptr;\n+-    bool  arrays_set = false;\n+-\n+-    /* We need a subgemm which outputs the 32-bit intermediates - how much space is needed for that? */\n+-    size_t subgemm_output_size() const {\n+-        return (_args._Msize * _args._Nsize * _args._nbatches * _args._nmulti * sizeof(int32_t));\n+-    }\n+-\n+-    size_t col_sum_size() const {\n+-        return (_args._Nsize * _args._nmulti * sizeof(int32_t));\n+-    }\n+-\n+-    size_t row_sum_size() const {\n+-        return (_args._Msize * _args._nbatches * _args._nmulti * sizeof(int32_t));\n+-    }\n+-\n+-    /* Local working space: We need space for the subgemm output (above) and\n+-     * the row sums.  */\n+-    size_t local_working_size() const {\n+-        return subgemm_output_size() + row_sum_size();\n+-    }\n+-\n+-    void set_child_arrays() {\n+-        if (working_space == nullptr || arrays_set == false)\n+-            return;\n+-\n+-        auto& g_array = this->_gemm_array;\n+-        /* Use the first part of our working space for the subgemm result, pass the operand details straight through. */\n+-        _subgemm->set_arrays(g_array._Aptr, g_array._lda, g_array._A_batch_stride, g_array._A_multi_stride,\n+-                             g_array._Bptr, g_array._ldb,                          g_array._B_multi_stride,\n+-                             reinterpret_cast<Tgemm *>(working_space), _args._Nsize, (_args._Nsize * _args._Msize), (_args._Nsize * _args._Msize * _args._nbatches),\n+-                             nullptr, 0);\n+-    }\n+-\n+-    void col_sums_pretransposed(const To *B, const int ldb, const int B_multi_stride) {\n+-        for (unsigned int multi=0; multi<_args._nmulti; multi++) {\n+-            compute_col_sums(_params, _args._Nsize, _args._Ksize, B + (multi * B_multi_stride), ldb, _col_sums + (multi * _args._Nsize), _args._Ksize, multi, 0);\n+-        }\n+-    }\n+-\n+-    void requantize_runtime(unsigned int threadid) {\n+-        unsigned int first_row = (threadid * _args._Msize) / _args._maxthreads;\n+-        unsigned int last_row = ((threadid+1) * _args._Msize) / _args._maxthreads;\n+-        auto& g_array = this->_gemm_array;\n+-\n+-        for (unsigned int multi=0; multi<_args._nmulti; multi++) {\n+-            for (unsigned int batch=0; batch<_args._nbatches; batch++) {\n+-                /* Compute row sums now */\n+-                compute_row_sums(_params, _args._Ksize, (last_row - first_row), g_array._Aptr + (multi * g_array._A_multi_stride) +\n+-                                    (batch * g_array._A_batch_stride) + (first_row * g_array._lda), g_array._lda, _row_sums +\n+-                                    (multi * _args._nbatches * _args._Msize) + (batch * _args._Msize) + first_row);\n+-                // If we don't care about negative values, call the version of this function that doesn't correct before shifting.\n+-                // 'c_offset' represents zero, so if the lowest possible quantized output value is the same or more than that we will not output negative numbers.\n+-                requantize_block_32(_params, _args._Nsize, (last_row - first_row), reinterpret_cast<Tgemm *>(working_space) +\n+-                                        (multi * (_args._Msize * _args._Nsize * _args._nbatches)) + (batch * (_args._Msize * _args._Nsize)) +\n+-                                        (first_row * _args._Nsize), _args._Nsize, g_array._Cptr + (multi * g_array._C_multi_stride) +\n+-                                        (batch * g_array._C_batch_stride) + (first_row * g_array._ldc), g_array._ldc, _row_sums +\n+-                                        (multi * _args._nbatches * _args._Msize) + (batch * _args._Msize) + first_row, _col_sums +\n+-                                        (multi * _args._Nsize), 0);\n+-            }\n+-        }\n+-    }\n+-\n+-\n+-public:\n+-    QuantizeWrapper(const QuantizeWrapper &) = delete;\n+-    QuantizeWrapper operator=(const QuantizeWrapper &) = delete;\n+-\n+-    QuantizeWrapper(const GemmArgs &args, const Requantize32 &qp) : _params(qp), _args(args), _barrier(args._maxthreads) {\n+-        GemmArgs newargs = GemmArgs(args._ci, args._Msize, args._Nsize, args._Ksize, args._Ksections, args._nbatches, args._nmulti, args._indirect_input, Activation(), args._maxthreads);\n+-        _subgemm = gemm<To, To, Tgemm>(newargs);\n+-\n+-        if (_subgemm == nullptr) {\n+-            return;\n+-        }\n+-    }\n+-\n+-    void set_arrays(const To *A, const int lda, const int A_batch_stride, const int A_multi_stride,\n+-                    const To *B, const int ldb, const int B_multi_stride,\n+-                          Tr *C, const int ldc, const int C_batch_stride, const int C_multi_stride,\n+-                    const Tr *bias, const int bias_multi_stride) override {\n+-        GemmCommon<To, To, Tr>::set_arrays(A, lda, A_batch_stride, A_multi_stride, B, ldb, B_multi_stride, C, ldc, C_batch_stride, C_multi_stride, bias, bias_multi_stride);\n+-\n+-        arrays_set = true;\n+-        set_child_arrays();\n+-    }\n+-\n+-    ndrange_t get_window_size() const override {\n+-        return { _subgemm->get_window_size() };\n+-    }\n+-\n+-    void set_nthreads(int nthreads) override {\n+-        _subgemm->set_nthreads(nthreads);\n+-        _barrier.set_nthreads(nthreads);\n+-        _args._maxthreads = nthreads;\n+-    }\n+-\n+-    // TODO: Make this actually stateless. This still uses the stateful\n+-    // execution data because it requires a workspace which would also need to\n+-    // be handled statelessly.\n+-    void execute_stateless(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid, GemmArrays<To, To, Tr> &) override {\n+-        _subgemm->execute(work_range, thread_locator, threadid);\n+-\n+-        _barrier.arrive_and_wait();\n+-\n+-        requantize_runtime(threadid);\n+-    }\n+-\n+-    void execute(const ndcoord_t &work_range, const ndcoord_t &thread_locator, int threadid) override {\n+-        execute_stateless(work_range, thread_locator, threadid, this->_gemm_array);\n+-    }\n+-\n+-    size_t get_working_size() const override {\n+-        return _subgemm->get_working_size() + local_working_size();\n+-    }\n+-\n+-    // Space arrangement:\n+-\n+-    // ptr\n+-    // V\n+-    // | subgemm output | row_sums | subgemm working space |\n+-    void set_working_space(void *space) override {\n+-        uintptr_t space_int = reinterpret_cast<uintptr_t>(space);\n+-\n+-        working_space = space;\n+-        _subgemm->set_working_space(reinterpret_cast<void *>(space_int + local_working_size()));\n+-\n+-        _row_sums = reinterpret_cast<int32_t *>(space_int + subgemm_output_size());\n+-\n+-        set_child_arrays();\n+-    }\n+-\n+-    bool B_is_pretransposed() const override {\n+-        /* We clear this flag if the subgemm isn't pretransposed, so just return its value */\n+-        return _subgemm->B_is_pretransposed();\n+-    }\n+-\n+-    bool B_pretranspose_required() const override {\n+-        return _subgemm->B_pretranspose_required();\n+-    }\n+-\n+-    size_t get_B_pretransposed_array_size() const override {\n+-        return _subgemm->get_B_pretransposed_array_size() + col_sum_size();\n+-    }\n+-\n+-    void requantize_bias(void *in_buffer, const To *B, const int ldb, const int B_multi_stride) override {\n+-        _col_sums = reinterpret_cast<int32_t *>(in_buffer);\n+-        col_sums_pretransposed(B, ldb, B_multi_stride);\n+-    }\n+-\n+-    void pretranspose_B_array(void *buffer, const To *B, const int ldb, const int B_multi_stride, bool transposed) override {\n+-        assert(!transposed);\n+-\n+-        uintptr_t buffer_int = reinterpret_cast<uintptr_t>(buffer);\n+-        _subgemm->pretranspose_B_array(reinterpret_cast<void *>(buffer_int + col_sum_size()), B, ldb, B_multi_stride, transposed);\n+-\n+-        requantize_bias(buffer, B, ldb, B_multi_stride);\n+-    }\n+-\n+-    void set_pretransposed_B_data(void *buffer) override {\n+-        uintptr_t buffer_int = reinterpret_cast<uintptr_t>(buffer);\n+-        _subgemm->set_pretransposed_B_data(reinterpret_cast<void *>(buffer_int + col_sum_size()));\n+-        _col_sums = reinterpret_cast<int32_t *>(buffer);\n+-    }\n+-\n+-    void set_quantized_bias(const int32_t *bias, size_t bias_multi_stride) override {\n+-        _params.bias = bias;\n+-        _params.bias_multi_stride = bias_multi_stride;\n+-    }\n+-\n+-    GemmConfig get_config() override {\n+-        GemmConfig c = _subgemm->get_config();\n+-\n+-        std::string n = \"quantize_wrapper[\";\n+-        n.append(c.filter);\n+-        n.append(\"]\");\n+-\n+-        c.method = GemmMethod::QUANTIZE_WRAPPER;\n+-        c.filter = n;\n+-\n+-        return c;\n+-    }\n+-\n+-    void update_quantization_parameters(const Requantize32 &re) override {\n+-        _params.bias = re.bias;\n+-        _params.a_offset = re.a_offset;\n+-        _params.b_offset = re.b_offset;\n+-        _params.c_offset = re.c_offset;\n+-        _params.per_layer_left_shift = re.per_layer_left_shift;\n+-        _params.per_layer_right_shift = re.per_layer_right_shift;\n+-        _params.per_layer_mul = re.per_layer_mul;\n+-        _params.per_channel_requant = re.per_channel_requant;\n+-        _params.per_channel_left_shifts = re.per_channel_left_shifts;\n+-        _params.per_channel_right_shifts = re.per_channel_right_shifts;\n+-        _params.per_channel_muls = re.per_channel_muls;\n+-        _params.minval = re.minval;\n+-        _params.maxval = re.maxval;\n+-    }\n+-};\n+-\n+-} // namespace arm_gemm\n+diff --git a/src/cpu/kernels/assembly/.clang-format b/src/cpu/kernels/assembly/.clang-format\n+new file mode 100644\n+index 000000000..bd90a154e\n+--- /dev/null\n++++ b/src/cpu/kernels/assembly/.clang-format\n+@@ -0,0 +1,25 @@\n++# Copyright (c) 2025 Arm Limited.\n++#\n++# SPDX-License-Identifier: MIT\n++#\n++# Permission is hereby granted, free of charge, to any person obtaining a copy\n++# of this software and associated documentation files (the \"Software\"), to\n++# deal in the Software without restriction, including without limitation the\n++# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n++# sell copies of the Software, and to permit persons to whom the Software is\n++# furnished to do so, subject to the following conditions:\n++#\n++# The above copyright notice and this permission notice shall be included in all\n++# copies or substantial portions of the Software.\n++#\n++# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n++# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n++# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n++# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n++# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n++# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n++# SOFTWARE.\n++---\n++# Disabling clang-format for this directory to reduce the diff-noise when\n++# porting changes from arm_gemm\n++DisableFormat: true\n+diff --git a/src/cpu/kernels/assembly/CpuGemmAssemblyWrapperKernel.h b/src/cpu/kernels/assembly/CpuGemmAssemblyWrapperKernel.h\n+index c3a1799e1..219d77a57 100644\n+--- a/src/cpu/kernels/assembly/CpuGemmAssemblyWrapperKernel.h\n++++ b/src/cpu/kernels/assembly/CpuGemmAssemblyWrapperKernel.h\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2018-2022, 2024 Arm Limited.\n++ * Copyright (c) 2018-2022, 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -104,10 +104,11 @@ public:\n+         ARM_COMPUTE_ERROR_ON_NULLPTR((reinterpret_cast<void *>(_kernel)));\n+         ARM_COMPUTE_ERROR_ON_UNCONFIGURED_KERNEL(this);\n+ \n+-        const auto *Aptr = reinterpret_cast<const TypeInput *>(tensors.get_tensor(ACL_SRC_0)->buffer());\n+-        const auto *Bptr = reinterpret_cast<const TypeWeight *>(tensors.get_tensor(ACL_SRC_1)->buffer());\n+-        const auto *bias = reinterpret_cast<const TypeOutput *>(tensors.get_tensor(ACL_SRC_2)->buffer());\n+-        auto       *Cptr = reinterpret_cast<TypeOutput *>(tensors.get_tensor(ACL_DST)->buffer());\n++        const auto *Aptr      = reinterpret_cast<const TypeInput *>(tensors.get_tensor(ACL_SRC_0)->buffer());\n++        const auto *Bptr      = reinterpret_cast<const TypeWeight *>(tensors.get_tensor(ACL_SRC_1)->buffer());\n++        const auto *bias      = reinterpret_cast<const TypeOutput *>(tensors.get_tensor(ACL_SRC_2)->buffer());\n++        void       *workspace = tensors.get_tensor(ACL_SRC_3)->buffer();\n++        auto       *Cptr      = reinterpret_cast<TypeOutput *>(tensors.get_tensor(ACL_DST)->buffer());\n+ \n+         ARM_COMPUTE_ERROR_ON_NULLPTR(Aptr, Cptr);\n+ \n+@@ -119,6 +120,7 @@ public:\n+         ga._Bptr = Bptr;\n+         ga._bias = bias;\n+         ga._Cptr = Cptr;\n++        ga.set_working_space(workspace);\n+ \n+         auto win = arm_gemm::to_ndcoord(window);\n+ \n+diff --git a/src/cpu/kernels/assembly/arm_gemm.hpp b/src/cpu/kernels/assembly/arm_gemm.hpp\n+index e65bc00a0..ea8566652 100644\n+--- a/src/cpu/kernels/assembly/arm_gemm.hpp\n++++ b/src/cpu/kernels/assembly/arm_gemm.hpp\n+@@ -44,7 +44,6 @@ enum class GemmMethod\n+     GEMM_NATIVE,\n+     GEMM_HYBRID,\n+     GEMM_INTERLEAVED,\n+-    QUANTIZE_WRAPPER,\n+     GEMM_HYBRID_QUANTIZED\n+ };\n+ \n+diff --git a/src/cpu/kernels/assembly/gemm_arrays.hpp b/src/cpu/kernels/assembly/gemm_arrays.hpp\n+index 2d4f7e1a0..0b4d79462 100644\n+--- a/src/cpu/kernels/assembly/gemm_arrays.hpp\n++++ b/src/cpu/kernels/assembly/gemm_arrays.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2024 Arm Limited.\n++ * Copyright (c) 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -52,6 +52,8 @@ struct IGemmArrays\n+                                     const void *bias,\n+                                     const int   bias_multi_stride) = 0; /* no row or batch stride needed */\n+ \n++    virtual void set_working_space(void *workspace) = 0;\n++\n+     virtual ~IGemmArrays() = default;\n+ };\n+ \n+@@ -71,6 +73,7 @@ struct GemmArrays : public IGemmArrays\n+     int       _C_multi_stride    = 0;\n+     const Tr *_bias              = nullptr;\n+     int       _bias_multi_stride = 0;\n++    void     *_workspace         = nullptr;\n+ \n+     GemmArrays() = default;\n+ \n+@@ -159,6 +162,11 @@ struct GemmArrays : public IGemmArrays\n+                    B_multi_stride, static_cast<Tr *>(C), ldc, C_batch_stride, C_multi_stride,\n+                    static_cast<const Tr *>(bias), bias_multi_stride);\n+     }\n++\n++    void set_working_space(void *workspace) override\n++    {\n++        _workspace = workspace;\n++    }\n+ };\n+ } // namespace arm_gemm\n+ \n+diff --git a/src/cpu/kernels/assembly/gemm_common.hpp b/src/cpu/kernels/assembly/gemm_common.hpp\n+index ce1873a49..c26c7a59b 100644\n+--- a/src/cpu/kernels/assembly/gemm_common.hpp\n++++ b/src/cpu/kernels/assembly/gemm_common.hpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2017-2021,2023-2024 Arm Limited.\n++ * Copyright (c) 2017-2021,2023-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -201,21 +201,21 @@ template <typename To, typename Tw, typename Tr>\n+ class GemmCommon : public IGemmCommon\n+ {\n+ protected:\n+-    GemmArrays<To, Tw, Tr> _gemm_array{};\n++    GemmArrays<To, Tw, Tr> _gemm_arrays{};\n+ \n+ public:\n+-    /* Pass in the pointers to the arrays to be operated on and their\n+-     * strides (templated version with appropriate types). */\n+     void set_gemm_arrays(GemmArrays<To, Tw, Tr> &ga)\n+     {\n+-        _gemm_array = ga;\n++        _gemm_arrays = ga;\n+     }\n+ \n+     const GemmArrays<To, Tw, Tr> &get_gemm_arrays() const\n+     {\n+-        return _gemm_array;\n++        return _gemm_arrays;\n+     }\n+ \n++    /* Pass in the pointers to the arrays to be operated on and their\n++     * strides (templated version with appropriate types). */\n+     virtual void set_arrays(const To                                     *A,\n+                             const int                                     lda,\n+                             const int                                     A_batch_stride,\n+@@ -230,8 +230,8 @@ public:\n+                             const Tr                                     *bias,\n+                             /* no row or batch stride needed */ const int bias_multi_stride)\n+     {\n+-        _gemm_array.set_arrays(A, lda, A_batch_stride, A_multi_stride, B, ldb, B_multi_stride, C, ldc, C_batch_stride,\n+-                               C_multi_stride, bias, bias_multi_stride);\n++        _gemm_arrays.set_arrays(A, lda, A_batch_stride, A_multi_stride, B, ldb, B_multi_stride, C, ldc, C_batch_stride,\n++                                C_multi_stride, bias, bias_multi_stride);\n+     }\n+ \n+     /* Implementation of the void * overload which casts its arguments to the appropriate type. */\n+diff --git a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n+index ec2039207..eaceff52e 100644\n+--- a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n++++ b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n+@@ -29,7 +29,6 @@\n+ \n+ #include \"src/core/CPP/Validate.h\"\n+ #include \"src/core/helpers/MemoryHelpers.h\"\n+-#include \"src/core/NEON/kernels/arm_gemm/utils.hpp\"\n+ #include \"src/core/utils/AssemblyUtils.h\"\n+ #include \"src/cpu/kernels/assembly/arm_gemm.hpp\"\n+ #include \"src/cpu/kernels/assembly/CpuGemmAssemblyWrapperKernel.h\"\n+@@ -231,11 +230,6 @@ public:\n+         _is_prepared = is_prepared;\n+     }\n+ \n+-    bool has_stateless_impl() const override\n+-    {\n+-        return _gemm_kernel_asm->get_working_size() == 0;\n+-    }\n+-\n+ private:\n+     enum AuxTensorIdx\n+     {\n+@@ -806,8 +800,11 @@ void Fallback<TypeInput, TypeWeight, TypeOutput, OutputStage>::run(ITensorPack &\n+     out_tensor.allocator()->init(*(d->info()));\n+     out_tensor.allocator()->import_memory(out_ptr);\n+ \n+-    ITensorPack gemm_pack{\n+-        {ACL_SRC_0, &in0_tensor}, {ACL_SRC_1, &in1_tensor}, {ACL_SRC_2, &bias_tensor}, {ACL_DST, &out_tensor}};\n++    ITensorPack gemm_pack{{ACL_SRC_0, &in0_tensor},\n++                          {ACL_SRC_1, &in1_tensor},\n++                          {ACL_SRC_2, &bias_tensor},\n++                          {ACL_SRC_3, workspace.get()},\n++                          {ACL_DST, &out_tensor}};\n+ \n+     // Set gemm parameters\n+     _gemm_kernel_asm->set_arrays(in0_ptr, lda, batch_stride_a, multi_stride_a, in1_ptr, ldb, multi_stride_b, out_ptr,\n+@@ -1037,11 +1034,6 @@ Status CpuGemmAssemblyDispatch::has_opt_impl(arm_compute::WeightFormat &expected\n+     return Status{};\n+ }\n+ \n+-bool CpuGemmAssemblyDispatch::has_stateless_impl() const\n+-{\n+-    return _arm_gemm->has_stateless_impl();\n+-}\n+-\n+ Status CpuGemmAssemblyDispatch::validate(\n+     const ITensorInfo *a, const ITensorInfo *b, const ITensorInfo *c, const ITensorInfo *d, const AsmGemmInfo &info)\n+ {\n+diff --git a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.h b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.h\n+index 84420f776..b45ce664b 100644\n+--- a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.h\n++++ b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.h\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2018-2024 Arm Limited.\n++ * Copyright (c) 2018-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -28,7 +28,6 @@\n+ \n+ #include \"src/core/common/Macros.h\"\n+ #include \"src/cpu/ICpuOperator.h\"\n+-#include \"src/cpu/kernels/assembly/arm_gemm.hpp\"\n+ \n+ namespace arm_compute\n+ {\n+@@ -93,7 +92,6 @@ public:\n+                                                                                 const bool,\n+                                                                                 const bool) = 0;\n+         virtual ~IFallback()                                                                = default;\n+-        virtual bool has_stateless_impl() const                                             = 0;\n+     };\n+ \n+ public:\n+@@ -172,17 +170,6 @@ public:\n+                                const ITensorInfo         *d,\n+                                const AsmGemmInfo         &info);\n+ \n+-    /** Checks if a stateless implementation is supported\n+-     *\n+-     * The arm_gemm kernels that have been made stateless so far are those that\n+-     * do not require any working space. Once all kernels have been made\n+-     * stateless we can deprecate it by always returning true, and eventually\n+-     * removing it completely\n+-     *\n+-     * @return True if stateless execution is supported else false\n+-     */\n+-    bool has_stateless_impl() const;\n+-\n+     /** Checks if activation is supported by the gemm assembly dispatcher\n+      *\n+      * @param[in] activation Activation to check\n+diff --git a/src/runtime/IScheduler.cpp b/src/runtime/IScheduler.cpp\n+index 9fa815fbd..d0d226d9f 100644\n+--- a/src/runtime/IScheduler.cpp\n++++ b/src/runtime/IScheduler.cpp\n+@@ -90,7 +90,7 @@ void IScheduler::schedule_common(ICPPKernel *kernel, const Hints &hints, const W\n+             for (unsigned int mi = 0; mi != m_threads; ++mi)\n+             {\n+                 workloads.push_back(\n+-                    [ni, mi, m_threads, n_threads, &max_window, &kernel](const ThreadInfo &info)\n++                    [ni, mi, m_threads, n_threads, &max_window, &kernel, &tensors](const ThreadInfo &info)\n+                     {\n+                         //narrow the window to our mi-ni workload\n+                         Window win = max_window.split_window(Window::DimX, mi, m_threads)\n+@@ -104,7 +104,14 @@ void IScheduler::schedule_common(ICPPKernel *kernel, const Hints &hints, const W\n+ \n+                         thread_locator.validate();\n+ \n+-                        kernel->run_nd(win, info, thread_locator);\n++                        if (tensors.empty())\n++                        {\n++                            kernel->run_nd(win, info, thread_locator);\n++                        }\n++                        else\n++                        {\n++                            kernel->run_op(tensors, win, info);\n++                        }\n+                     });\n+             }\n+         }\n+diff --git a/src/runtime/experimental/low_level/CpuGemmAssemblyDispatch.cpp b/src/runtime/experimental/low_level/CpuGemmAssemblyDispatch.cpp\n+index 6021d1330..93b65e31d 100644\n+--- a/src/runtime/experimental/low_level/CpuGemmAssemblyDispatch.cpp\n++++ b/src/runtime/experimental/low_level/CpuGemmAssemblyDispatch.cpp\n+@@ -1,5 +1,5 @@\n+ /*\n+- * Copyright (c) 2024 Arm Limited.\n++ * Copyright (c) 2024-2025 Arm Limited.\n+  *\n+  * SPDX-License-Identifier: MIT\n+  *\n+@@ -122,7 +122,7 @@ bool CpuGemmAssemblyDispatch::has_stateless_impl() const\n+ {\n+     ARM_COMPUTE_ERROR_ON_MSG(!is_configured(), \"calling has_stateless_impl() on unconfigured CpuGemmAssemblyDispatch\");\n+ \n+-    return _impl->cpu_gemm_assembly_dispatch->has_stateless_impl();\n++    return true;\n+ }\n+ \n+ bool CpuGemmAssemblyDispatch::is_activation_supported(const ActivationLayerInfo &activation)\n\\ No newline at end of file"
        },
        {
            "sha": "9ebf6b71fdb44a38fa54169c0ad956d861cbbe99",
            "filename": "third_party/xla/third_party/compute_library/acl_thread_local_scheduler.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_thread_local_scheduler.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_thread_local_scheduler.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Facl_thread_local_scheduler.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,98 +0,0 @@\n-diff --git a/arm_compute/runtime/Scheduler.h b/arm_compute/runtime/Scheduler.h\n-index 9e8add1f9..cf5e2bf4c 100644\n---- a/arm_compute/runtime/Scheduler.h\n-+++ b/arm_compute/runtime/Scheduler.h\n-@@ -75,7 +75,7 @@ public:\n-\n- private:\n-     static Type                        _scheduler_type;\n--    static std::shared_ptr<IScheduler> _custom_scheduler;\n-+    static thread_local std::shared_ptr<IScheduler> _custom_scheduler;\n-     static std::map<Type, std::unique_ptr<IScheduler>> _schedulers;\n-\n-     Scheduler();\n-diff --git a/src/cpu/operators/CpuDepthwiseConv2dAssemblyDispatch.cpp b/src/cpu/operators/CpuDepthwiseConv2dAssemblyDispatch.cpp\n-index a5b9eca56..d1ab19397 100644\n---- a/src/cpu/operators/CpuDepthwiseConv2dAssemblyDispatch.cpp\n-+++ b/src/cpu/operators/CpuDepthwiseConv2dAssemblyDispatch.cpp\n-@@ -60,8 +60,8 @@ void CpuDepthwiseConv2dAssemblyDispatch::configure(const ITensorInfo     *src,\n-                                                    const ConvolutionInfo &info)\n- {\n-     ARM_COMPUTE_LOG_PARAMS(src, weights, bias, dst, info);\n--    const CPUInfo     &ci          = NEScheduler::get().cpu_info();\n--    const unsigned int num_threads = NEScheduler::get().num_threads();\n-+    const CPUInfo     &ci          = CPUInfo::get();\n-+    const unsigned int num_threads = CPUInfo::get().get_cpu_num();\n-     _pImpl->is_prepared            = false;\n-     _pImpl->are_weights_const      = weights->are_values_constant();\n-\n-diff --git a/src/cpu/operators/CpuPool2d.cpp b/src/cpu/operators/CpuPool2d.cpp\n-index 722cd36ee..03aef1632 100644\n---- a/src/cpu/operators/CpuPool2d.cpp\n-+++ b/src/cpu/operators/CpuPool2d.cpp\n-@@ -66,8 +66,8 @@ void CpuPool2d::configure(ITensorInfo *src, ITensorInfo *dst, const PoolingLayer\n-\n-     if(run_optimised)\n-     {\n--        const CPUInfo     &ci          = NEScheduler::get().cpu_info();\n--        const unsigned int num_threads = NEScheduler::get().num_threads();\n-+        const CPUInfo     &ci          = CPUInfo::get();\n-+        const unsigned int num_threads = CPUInfo::get().get_cpu_num();\n-\n-         auto pooling_wrapper = std::make_unique<kernels::CpuPool2dAssemblyWrapperKernel>();\n-         ARM_COMPUTE_ERROR_ON(pooling_wrapper == nullptr);\n-diff --git a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n- *******************************************************************************\n- Copyright 2023 Arm Limited and affiliates.\n- SPDX-License-Identifier: Apache-2.0\n-\n- Licensed under the Apache License, Version 2.0 (the \"License\");\n- you may not use this file except in compliance with the License.\n- You may obtain a copy of the License at\n-\n-     http://www.apache.org/licenses/LICENSE-2.0\n-\n- Unless required by applicable law or agreed to in writing, software\n- distributed under the License is distributed on an \"AS IS\" BASIS,\n- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- See the License for the specific language governing permissions and\n- limitations under the License.\n- *******************************************************************************\n-index 9c8563140..f7771945a 100644\n---- a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n-+++ b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp\n-@@ -623,8 +623,8 @@ void create_arm_gemm(std::unique_ptr<CpuGemmAssemblyDispatch::IFallback> &arm_ge\n-                      arm_gemm::Activation activation, const AsmGemmInfo &info)\n- {\n-     Params         p           = extract_parameters(a, b, d, info);\n--    const CPUInfo &ci          = NEScheduler::get().cpu_info();\n--    unsigned int   num_threads = NEScheduler::get().num_threads();\n-+    const CPUInfo &ci          = CPUInfo::get();\n-+    unsigned int   num_threads = CPUInfo::get().get_cpu_num();\n-\n-     arm_gemm::GemmConfig cfg;\n-     cfg.weight_format = assembly_utils::map_to_arm_gemm_weight_format(info.weight_format);\n-@@ -696,8 +696,8 @@ Status CpuGemmAssemblyDispatch::has_opt_impl(arm_compute::WeightFormat &expected\n-     ARM_COMPUTE_UNUSED(c);\n-     arm_gemm::Activation act         = assembly_utils::map_to_arm_gemm_activation(info.activation_info);\n-     Params               p           = extract_parameters(a, b, d, info);\n--    const CPUInfo       &ci          = NEScheduler::get().cpu_info();\n--    unsigned int         num_threads = NEScheduler::get().num_threads();\n-+    const CPUInfo       &ci          = CPUInfo::get();\n-+    unsigned int         num_threads = CPUInfo::get().get_cpu_num();\n-     arm_gemm::GemmConfig cfg;\n-     cfg.weight_format                           = assembly_utils::map_to_arm_gemm_weight_format(info.weight_format);\n-     arm_gemm::WeightFormat arm_gemm_expected_wf = assembly_utils::map_to_arm_gemm_weight_format(expected_weight_format);\n-diff --git a/src/runtime/Scheduler.cpp b/src/runtime/Scheduler.cpp\n-index 0713b9a2a..f15ac2e22 100644\n---- a/src/runtime/Scheduler.cpp\n-+++ b/src/runtime/Scheduler.cpp\n-@@ -47,7 +47,7 @@ Scheduler::Type Scheduler::_scheduler_type = Scheduler::Type::CPP;\n- Scheduler::Type Scheduler::_scheduler_type = Scheduler::Type::ST;\n- #endif /* ARM_COMPUTE_*_SCHEDULER */\n-\n--std::shared_ptr<IScheduler> Scheduler::_custom_scheduler = nullptr;\n-+thread_local std::shared_ptr<IScheduler> Scheduler::_custom_scheduler = nullptr;\n-\n- namespace\n- {"
        },
        {
            "sha": "5f03fdcc59d7bc4cf7681f523c02a302e451d222",
            "filename": "third_party/xla/third_party/compute_library/exclude_omp_scheduler.patch",
            "status": "modified",
            "additions": 21,
            "deletions": 7,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Fexclude_omp_scheduler.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Fexclude_omp_scheduler.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fcompute_library%2Fexclude_omp_scheduler.patch?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -1,23 +1,37 @@\n+# Copyright 2025 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n diff --git a/src/BUILD.bazel b/src/BUILD.bazel\n-index bf71e534e2..22377f1a32 100644\n+index 547c98576..a31301230 100644\n --- a/src/BUILD.bazel\n +++ b/src/BUILD.bazel\n-@@ -971,7 +971,6 @@ filegroup(\n+@@ -1029,7 +1029,6 @@ filegroup(\n  \t\"runtime/NEON/functions/NETranspose.cpp\",\n  \t\"runtime/NEON/functions/NEUnstack.cpp\",\n  \t\"runtime/NEON/functions/NEWinogradConvolutionLayer.cpp\",\n -\t\"runtime/OMP/OMPScheduler.cpp\",\n  \t\"runtime/OffsetLifetimeManager.cpp\",\n  \t\"runtime/OffsetMemoryPool.cpp\",\n  \t\"runtime/OperatorTensor.cpp\",\n-@@ -984,6 +983,10 @@ filegroup(\n- \t\"runtime/Tensor.cpp\",\n- \t\"runtime/TensorAllocator.cpp\",\n- \t\"runtime/Utils.cpp\"]  +\n+@@ -1058,6 +1057,10 @@ filegroup(\n+ \t\"runtime/experimental/operators/CpuSub.cpp\",\n+ \t\"runtime/experimental/operators/CpuTranspose.cpp\",\n+ \t\"runtime/experimental/operators/CpuWinogradConv2d.cpp\"]  +\n +    select({\n +        \"//:openmp_flag\": [\"runtime/OMP/OMPScheduler.cpp\"],\n +        \"//conditions:default\": [],\n +    }) +\n      glob([\"**/*.h\",\n      \"**/*.hpp\",\n-     \"**/*.inl\"]),\n+     \"**/*.inl\"]),\n\\ No newline at end of file"
        },
        {
            "sha": "651f0a4189010d29f532946049de8633e70127e9",
            "filename": "third_party/xla/third_party/mkl_dnn/mkldnn_acl.BUILD",
            "status": "modified",
            "additions": 26,
            "deletions": 60,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fmkldnn_acl.BUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fmkldnn_acl.BUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fmkldnn_acl.BUILD?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -9,13 +9,6 @@ _DNNL_COPTS_THREADPOOL = [\n     \"-UUSE_CBLAS\",\n ]\n \n-_DNNL_COPTS_OMP = [\n-    \"-fopenmp\",\n-    \"-fexceptions\",\n-    \"-UUSE_MKL\",\n-    \"-UUSE_CBLAS\",\n-]\n-\n _DNNL_RUNTIME_THREADPOOL = {\n     \"#cmakedefine DNNL_CPU_THREADING_RUNTIME DNNL_RUNTIME_${DNNL_CPU_THREADING_RUNTIME}\": \"#define DNNL_CPU_THREADING_RUNTIME DNNL_RUNTIME_THREADPOOL\",\n     \"#cmakedefine DNNL_CPU_RUNTIME DNNL_RUNTIME_${DNNL_CPU_RUNTIME}\": \"#define DNNL_CPU_RUNTIME DNNL_RUNTIME_THREADPOOL\",\n@@ -63,61 +56,24 @@ _DNNL_RUNTIME_THREADPOOL = {\n     \"#cmakedefine01 BUILD_XEHPG\": \"#define BUILD_XEHPG 0\",\n     \"#cmakedefine01 BUILD_XEHPC\": \"#define BUILD_XEHPC 0\",\n     \"#cmakedefine01 BUILD_XEHP\": \"#define BUILD_XEHP 0\",\n-}\n-\n-_DNNL_RUNTIME_OMP = {\n-    \"#cmakedefine DNNL_CPU_THREADING_RUNTIME DNNL_RUNTIME_${DNNL_CPU_THREADING_RUNTIME}\": \"#define DNNL_CPU_THREADING_RUNTIME DNNL_RUNTIME_OMP\",\n-    \"#cmakedefine DNNL_CPU_RUNTIME DNNL_RUNTIME_${DNNL_CPU_RUNTIME}\": \"#define DNNL_CPU_RUNTIME DNNL_RUNTIME_OMP\",\n-    \"#cmakedefine DNNL_GPU_RUNTIME DNNL_RUNTIME_${DNNL_GPU_RUNTIME}\": \"#define DNNL_GPU_RUNTIME DNNL_RUNTIME_NONE\",\n-    \"#cmakedefine DNNL_USE_RT_OBJECTS_IN_PRIMITIVE_CACHE\": \"#undef DNNL_USE_RT_OBJECTS_IN_PRIMITIVE_CACHE\",\n-    \"#cmakedefine DNNL_WITH_SYCL\": \"#undef DNNL_WITH_SYCL\",\n-    \"#cmakedefine DNNL_WITH_LEVEL_ZERO\": \"#undef DNNL_WITH_LEVEL_ZERO\",\n-    \"#cmakedefine DNNL_SYCL_CUDA\": \"#undef DNNL_SYCL_CUDA\",\n-    \"#cmakedefine DNNL_SYCL_HIP\": \"#undef DNNL_SYCL_HIP\",\n-    \"#cmakedefine DNNL_ENABLE_STACK_CHECKER\": \"#undef DNNL_ENABLE_STACK_CHECKER\",\n-    \"#cmakedefine DNNL_EXPERIMENTAL\": \"#undef DNNL_EXPERIMENTAL\",\n-    \"#cmakedefine ONEDNN_BUILD_GRAPH\": \"#define ONEDNN_BUILD_GRAPH\",\n-    \"#cmakedefine01 BUILD_TRAINING\": \"#define BUILD_TRAINING 1\",\n-    \"#cmakedefine01 BUILD_INFERENCE\": \"#define BUILD_INFERENCE 0\",\n-    \"#cmakedefine01 BUILD_PRIMITIVE_ALL\": \"#define BUILD_PRIMITIVE_ALL 1\",\n-    \"#cmakedefine01 BUILD_BATCH_NORMALIZATION\": \"#define BUILD_BATCH_NORMALIZATION 0\",\n-    \"#cmakedefine01 BUILD_BINARY\": \"#define BUILD_BINARY 0\",\n-    \"#cmakedefine01 BUILD_CONCAT\": \"#define BUILD_CONCAT 0\",\n-    \"#cmakedefine01 BUILD_CONVOLUTION\": \"#define BUILD_CONVOLUTION 0\",\n-    \"#cmakedefine01 BUILD_DECONVOLUTION\": \"#define BUILD_DECONVOLUTION 0\",\n-    \"#cmakedefine01 BUILD_ELTWISE\": \"#define BUILD_ELTWISE 0\",\n-    \"#cmakedefine01 BUILD_INNER_PRODUCT\": \"#define BUILD_INNER_PRODUCT 0\",\n-    \"#cmakedefine01 BUILD_LAYER_NORMALIZATION\": \"#define BUILD_LAYER_NORMALIZATION 0\",\n-    \"#cmakedefine01 BUILD_LRN\": \"#define BUILD_LRN 0\",\n-    \"#cmakedefine01 BUILD_MATMUL\": \"#define BUILD_MATMUL 0\",\n-    \"#cmakedefine01 BUILD_POOLING\": \"#define BUILD_POOLING 0\",\n-    \"#cmakedefine01 BUILD_PRELU\": \"#define BUILD_PRELU 0\",\n-    \"#cmakedefine01 BUILD_REDUCTION\": \"#define BUILD_REDUCTION 0\",\n-    \"#cmakedefine01 BUILD_REORDER\": \"#define BUILD_REORDER 0\",\n-    \"#cmakedefine01 BUILD_RESAMPLING\": \"#define BUILD_RESAMPLING 0\",\n-    \"#cmakedefine01 BUILD_RNN\": \"#define BUILD_RNN 0\",\n-    \"#cmakedefine01 BUILD_SHUFFLE\": \"#define BUILD_SHUFFLE 0\",\n-    \"#cmakedefine01 BUILD_SOFTMAX\": \"#define BUILD_SOFTMAX 0\",\n-    \"#cmakedefine01 BUILD_SUM\": \"#define BUILD_SUM 0\",\n-    \"#cmakedefine01 BUILD_PRIMITIVE_CPU_ISA_ALL\": \"#define BUILD_PRIMITIVE_CPU_ISA_ALL 0\",\n-    \"#cmakedefine01 BUILD_SSE41\": \"#define BUILD_SSE41 0\",\n-    \"#cmakedefine01 BUILD_AVX2\": \"#define BUILD_AVX2 0\",\n-    \"#cmakedefine01 BUILD_AVX512\": \"#define BUILD_AVX512 0\",\n-    \"#cmakedefine01 BUILD_AMX\": \"#define BUILD_AMX 0\",\n-    \"#cmakedefine01 BUILD_PRIMITIVE_GPU_ISA_ALL\": \"#define BUILD_PRIMITIVE_GPU_ISA_ALL 0\",\n-    \"#cmakedefine01 BUILD_GEN9\": \"#define BUILD_GEN9 0\",\n-    \"#cmakedefine01 BUILD_GEN11\": \"#define BUILD_GEN11 0\",\n-    \"#cmakedefine01 BUILD_XELP\": \"#define BUILD_XELP 0\",\n-    \"#cmakedefine01 BUILD_XEHPG\": \"#define BUILD_XEHPG 0\",\n-    \"#cmakedefine01 BUILD_XEHPC\": \"#define BUILD_XEHPC 0\",\n-    \"#cmakedefine01 BUILD_XEHP\": \"#define BUILD_XEHP 0\",\n+    \"#cmakedefine01 BUILD_GROUP_NORMALIZATION\": \"#define BUILD_GROUP_NORMALIZATION 0\",\n+    \"#cmakedefine01 BUILD_GEMM_KERNELS_ALL\": \"#define BUILD_GEMM_KERNELS_ALL 1\",\n+    \"#cmakedefine01 BUILD_GEMM_KERNELS_NONE\": \"#define BUILD_GEMM_KERNELS_NONE 0\",\n+    \"#cmakedefine01 BUILD_GEMM_SSE41\": \"#define BUILD_GEMM_SSE41 0\",\n+    \"#cmakedefine01 BUILD_GEMM_AVX2\": \"#define BUILD_GEMM_AVX2 0\",\n+    \"#cmakedefine01 BUILD_GEMM_AVX512\": \"#define BUILD_GEMM_AVX512 0\",\n+    \"#cmakedefine DNNL_GPU_VENDOR\": \"#define DNNL_GPU_VENDOR INTEL\",\n+    \"#cmakedefine DNNL_SYCL_GENERIC\": \"#undef DNNL_SYCL_GENERIC\",\n+    \"#cmakedefine DNNL_DISABLE_GPU_REF_KERNELS\": \"#undef DNNL_DISABLE_GPU_REF_KERNELS\",\n+    \"#cmakedefine01 BUILD_SDPA\": \"#define BUILD_SDPA 0\",\n+    \"#cmakedefine01 BUILD_XE2\": \"#define BUILD_XE2 0\",\n+    \"#cmakedefine01 BUILD_XE3\": \"#define BUILD_XE3 0\",\n }\n \n expand_template(\n     name = \"dnnl_config_h\",\n     out = \"include/oneapi/dnnl/dnnl_config.h\",\n     substitutions = select({\n-        \"@local_xla//xla/tsl/mkl:build_with_mkl_aarch64_openmp\": _DNNL_RUNTIME_OMP,\n         \"//conditions:default\": _DNNL_RUNTIME_THREADPOOL,\n     }),\n     template = \"include/oneapi/dnnl/dnnl_config.h.in\",\n@@ -128,13 +84,21 @@ expand_template(\n     out = \"include/oneapi/dnnl/dnnl_version.h\",\n     substitutions = {\n         \"@DNNL_VERSION_MAJOR@\": \"3\",\n-        \"@DNNL_VERSION_MINOR@\": \"2\",\n-        \"@DNNL_VERSION_PATCH@\": \"1\",\n-        \"@DNNL_VERSION_HASH@\": \"N/A\",\n+        \"@DNNL_VERSION_MINOR@\": \"7\",\n+        \"@DNNL_VERSION_PATCH@\": \"0\",\n     },\n     template = \"include/oneapi/dnnl/dnnl_version.h.in\",\n )\n \n+expand_template(\n+    name = \"dnnl_version_hash_h\",\n+    out = \"include/oneapi/dnnl/dnnl_version_hash.h\",\n+    substitutions = {\n+        \"@DNNL_VERSION_HASH@\": \"N/A\",\n+    },\n+    template = \"include/oneapi/dnnl/dnnl_version_hash.h.in\",\n+)\n+\n cc_library(\n     name = \"mkl_dnn_acl\",\n     srcs = glob(\n@@ -156,10 +120,11 @@ cc_library(\n         exclude = [\n             \"src/cpu/x64/**\",\n             \"src/cpu/rv64/**\",\n+            \"src/cpu/sycl/**\",\n+            \"src/xpu/**\",\n         ],\n     ),\n     copts = select({\n-        \"@local_xla//xla/tsl/mkl:build_with_mkl_aarch64_openmp\": _DNNL_COPTS_OMP,\n         \"//conditions:default\": _DNNL_COPTS_THREADPOOL,\n     }),\n     defines = [\"DNNL_AARCH64_USE_ACL=1\"],\n@@ -185,6 +150,7 @@ cc_library(\n     ) + [\n         \":dnnl_config_h\",\n         \":dnnl_version_h\",\n+        \":dnnl_version_hash_h\",\n     ],\n     visibility = [\"//visibility:public\"],\n     deps = ["
        },
        {
            "sha": "42dd262323b577da1954e3ae34723a6c7c07e1f2",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_add_bf16_platform_support_check.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_add_bf16_platform_support_check.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_add_bf16_platform_support_check.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_add_bf16_platform_support_check.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,31 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-diff --git a/src/cpu/platform.cpp b/src/cpu/platform.cpp\n-index 65b887ea21..eabdb827bd 100644\n---- a/src/cpu/platform.cpp\n-+++ b/src/cpu/platform.cpp\n-@@ -117,6 +117,8 @@ bool has_data_type_support(data_type_t data_type) {\n- #if defined(USE_CBLAS) && defined(BLAS_HAS_SBGEMM) && defined(__MMA__)\n-             return true;\n- #endif\n-+#elif DNNL_AARCH64_USE_ACL\n-+            return arm_compute::CPUInfo::get().has_bf16();\n- #else\n-             return false;\n- #endif\n--- \n-2.34.1\n-"
        },
        {
            "sha": "779608a68058d2a7d50b95f23c2e8eb4770d0989",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_add_sbgemm_matmul_primitive_definition.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_add_sbgemm_matmul_primitive_definition.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_add_sbgemm_matmul_primitive_definition.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_add_sbgemm_matmul_primitive_definition.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,44 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-diff --git a/src/cpu/aarch64/matmul/acl_matmul.hpp b/src/cpu/aarch64/matmul/acl_matmul.hpp\n-index ab13efb9b2..ec261e156d 100644\n---- a/src/cpu/aarch64/matmul/acl_matmul.hpp\n-+++ b/src/cpu/aarch64/matmul/acl_matmul.hpp\n-@@ -78,11 +78,21 @@ struct acl_matmul_t : public primitive_t {\n-                     = utils::everyone_is(data_type::f16, src_md()->data_type,\n-                               weights_md()->data_type, dst_md()->data_type)\n-                     && platform::has_data_type_support(data_type::f16);\n-+            const bool is_fp32_bf16_ok\n-+                    = (utils::everyone_is(data_type::f32, src_md()->data_type,\n-+                               dst_md()->data_type, desc()->accum_data_type)\n-+                            && platform::has_data_type_support(data_type::f32)\n-+                            && utils::everyone_is(\n-+                                    data_type::bf16, weights_md()->data_type)\n-+                            && platform::has_data_type_support(\n-+                                    data_type::bf16));\n-+\n-             const bool is_weights_md_format_ok\n-                     = utils::one_of(weights_format_kind_received,\n-                             format_kind::any, format_kind::blocked);\n-             bool ok = is_dense_data()\n--                    && utils::one_of(true, is_fp32_ok, is_fp16_ok)\n-+                    && utils::one_of(\n-+                            true, is_fp32_ok, is_fp16_ok, is_fp32_bf16_ok)\n-                     && !has_zero_dim_memory() && is_weights_md_format_ok\n-                     && set_default_formats()\n-                     && attr()->has_default_values(\n--- \n-2.34.1"
        },
        {
            "sha": "ec2cb97f5131ba75bf6bf55f9475279dbb400d63",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_allow_blocked_weight_format_for_matmul_primitive.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 100,
            "changes": 100,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_allow_blocked_weight_format_for_matmul_primitive.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_allow_blocked_weight_format_for_matmul_primitive.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_allow_blocked_weight_format_for_matmul_primitive.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,100 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-diff --git a/src/cpu/aarch64/matmul/acl_matmul.hpp b/src/cpu/aarch64/matmul/acl_matmul.hpp\n-index 451cc78d52..ab13efb9b2 100644\n---- a/src/cpu/aarch64/matmul/acl_matmul.hpp\n-+++ b/src/cpu/aarch64/matmul/acl_matmul.hpp\n-@@ -67,6 +67,8 @@ struct acl_matmul_t : public primitive_t {\n- \n-         status_t init(engine_t *engine) {\n-             using smask_t = primitive_attr_t::skip_mask_t;\n-+            const format_kind_t weights_format_kind_received\n-+                    = weights_md_.format_kind;\n-             const bool is_fp32_ok\n-                     = utils::everyone_is(data_type::f32, src_md()->data_type,\n-                               weights_md()->data_type, dst_md()->data_type,\n-@@ -76,18 +78,20 @@ struct acl_matmul_t : public primitive_t {\n-                     = utils::everyone_is(data_type::f16, src_md()->data_type,\n-                               weights_md()->data_type, dst_md()->data_type)\n-                     && platform::has_data_type_support(data_type::f16);\n-+            const bool is_weights_md_format_ok\n-+                    = utils::one_of(weights_format_kind_received,\n-+                            format_kind::any, format_kind::blocked);\n-             bool ok = is_dense_data()\n-                     && utils::one_of(true, is_fp32_ok, is_fp16_ok)\n--                    && !has_zero_dim_memory()\n--                    && weights_md_.format_kind == format_kind::any\n-+                    && !has_zero_dim_memory() && is_weights_md_format_ok\n-                     && set_default_formats()\n-                     && attr()->has_default_values(\n-                             smask_t::oscale | smask_t::post_ops)\n-                     && attr_oscale_ok() && !has_runtime_dims_or_strides();\n-             if (!ok) return status::unimplemented;\n- \n--            CHECK(acl_matmul_utils::init_conf_matmul(\n--                    amp_, src_md_, weights_md_, dst_md_, *desc(), *attr()));\n-+            CHECK(acl_matmul_utils::init_conf_matmul(amp_, src_md_, weights_md_,\n-+                    dst_md_, *desc(), *attr(), weights_format_kind_received));\n- \n-             arm_compute::ActivationLayerInfo act_info;\n-             CHECK(post_ops.init(engine, attr_.post_ops_, dst_md_, act_info));\n-diff --git a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp\n-index a314d96384..027f915a8a 100644\n---- a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp\n-+++ b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp\n-@@ -27,7 +27,8 @@ namespace acl_matmul_utils {\n- \n- status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,\n-         memory_desc_t &wei_md, memory_desc_t &dst_md, const matmul_desc_t &md,\n--        const primitive_attr_t &attr) {\n-+        const primitive_attr_t &attr,\n-+        format_kind_t weights_format_kind_received) {\n- \n-     const memory_desc_wrapper src_d(&src_md);\n-     const memory_desc_wrapper wei_d(&wei_md);\n-@@ -128,9 +129,16 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,\n-     for (dim_t i = K_dim - 1; i >= 0; --i)\n-         batch_dims.push_back(i);\n- \n-+    const memory_desc_t weights_md_received = wei_md;\n-     acl_utils::reorder_to_weight_format(amp.wei_tensor_info, wei_md,\n-             expected_weight_format, K_dim, N_dim, {}, batch_dims);\n- \n-+    ACL_CHECK_SUPPORT((weights_format_kind_received == format_kind::blocked)\n-+                    && !(dnnl_memory_desc_equal(&weights_md_received, &wei_md)),\n-+            \"specified blocked format not supported by ACL, use \"\n-+            \"format_kind_t::any to find a supported blocked format for \"\n-+            \"your platform\");\n-+\n-     return status::success;\n- }\n- \n-diff --git a/src/cpu/aarch64/matmul/acl_matmul_utils.hpp b/src/cpu/aarch64/matmul/acl_matmul_utils.hpp\n-index 67bb2e78eb..5ba4241abc 100644\n---- a/src/cpu/aarch64/matmul/acl_matmul_utils.hpp\n-+++ b/src/cpu/aarch64/matmul/acl_matmul_utils.hpp\n-@@ -52,7 +52,8 @@ namespace acl_matmul_utils {\n- \n- status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,\n-         memory_desc_t &wei_md, memory_desc_t &dst_md, const matmul_desc_t &md,\n--        const primitive_attr_t &attr);\n-+        const primitive_attr_t &attr,\n-+        format_kind_t weights_format_kind_received);\n- \n- } // namespace acl_matmul_utils\n- \n--- \n-2.34.1"
        },
        {
            "sha": "6d6f0c0eaabb133b2e58b3774dd96e53b043fd82",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_bf16_capability_detection_for_ubuntu20.04.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_bf16_capability_detection_for_ubuntu20.04.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_bf16_capability_detection_for_ubuntu20.04.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_bf16_capability_detection_for_ubuntu20.04.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,50 +0,0 @@\n-From 9a9430c7db870b78c6402d786a67921af4a66334 Mon Sep 17 00:00:00 2001\n-From: Kentaro Kawakami <kawakami.k@fujitsu.com>\n-Date: Fri, 26 May 2023 10:58:36 +0900\n-Subject: [PATCH] cpu: aarch64: xbyak_aarch64: BF16 capability detection for\n- Ubuntu 20.04\n-\n----\n- .../aarch64/xbyak_aarch64/src/util_impl_linux.h   | 15 ++++++++++++---\n- 1 file changed, 12 insertions(+), 3 deletions(-)\n-\n-diff --git a/src/cpu/aarch64/xbyak_aarch64/src/util_impl_linux.h b/src/cpu/aarch64/xbyak_aarch64/src/util_impl_linux.h\n-index 743843bae50..3db37e972d1 100644\n---- a/src/cpu/aarch64/xbyak_aarch64/src/util_impl_linux.h\n-+++ b/src/cpu/aarch64/xbyak_aarch64/src/util_impl_linux.h\n-@@ -39,6 +39,13 @@\n- #include <asm/hwcap.h>\n- #endif\n- \n-+/* Linux kernel used in Ubuntu 20.04 does not have HWCAP2_BF16 definition. */\n-+#ifdef AT_HWCAP2\n-+#ifndef HWCAP2_BF16\n-+#define HWCAP2_BF16 (1UL << 14)\n-+#endif\n-+#endif\n-+\n- namespace Xbyak_aarch64 {\n- namespace util {\n- #define XBYAK_AARCH64_ERROR_ fprintf(stderr, \"%s, %d, Error occurrs during read cache infomation.\\n\", __FILE__, __LINE__);\n-@@ -383,7 +390,7 @@ class CpuInfoLinux : public CpuInfo {\n-   }\n- \n-   void setHwCap() {\n--    unsigned long hwcap = getauxval(AT_HWCAP);\n-+    const unsigned long hwcap = getauxval(AT_HWCAP);\n-     if (hwcap & HWCAP_ATOMICS)\n-       type_ |= (Type)XBYAK_AARCH64_HWCAP_ATOMIC;\n- \n-@@ -391,8 +398,10 @@ class CpuInfoLinux : public CpuInfo {\n-       type_ |= (Type)XBYAK_AARCH64_HWCAP_FP;\n-     if (hwcap & HWCAP_ASIMD)\n-       type_ |= (Type)XBYAK_AARCH64_HWCAP_ADVSIMD;\n--#ifdef HWCAP2_BF16\n--    if (hwcap & HWCAP2_BF16)\n-+\n-+#ifdef AT_HWCAP2\n-+    const unsigned long hwcap2 = getauxval(AT_HWCAP2);\n-+    if (hwcap2 & HWCAP2_BF16)\n-       type_ |= (Type)XBYAK_AARCH64_HWCAP_BF16;\n- #endif\n- "
        },
        {
            "sha": "39f7e74345e08b9ba811b482e2b486ea5bb78f1d",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_fix_segfault_during_postop_execute.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_fix_segfault_during_postop_execute.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_fix_segfault_during_postop_execute.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_fix_segfault_during_postop_execute.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,96 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-diff --git a/src/cpu/aarch64/acl_post_ops.cpp b/src/cpu/aarch64/acl_post_ops.cpp\n-index ea4bb200ec..3eb53b81bd 100644\n---- a/src/cpu/aarch64/acl_post_ops.cpp\n-+++ b/src/cpu/aarch64/acl_post_ops.cpp\n-@@ -24,7 +24,7 @@ namespace aarch64 {\n- \n- status_t acl_post_ops_t::execute(const exec_ctx_t &ctx, void *src_orig) const {\n- \n--    int post_op_index = 0;\n-+    int post_op_index = post_op_start_index_;\n- \n-     // As these are post ops, this src will also be our dst. If we have a sum\n-     // post op, the src/dst will start off in a temporary, then change to\n-diff --git a/src/cpu/aarch64/acl_post_ops.hpp b/src/cpu/aarch64/acl_post_ops.hpp\n-index 7b59ad71d3..ceaa95b73a 100644\n---- a/src/cpu/aarch64/acl_post_ops.hpp\n-+++ b/src/cpu/aarch64/acl_post_ops.hpp\n-@@ -32,7 +32,9 @@ struct acl_post_ops_t {\n-     // init the acl_post_ops_t. Note that this function modifies the passed in\n-     // post ops by setting the preferred memory formats\n-     status_t init(engine_t *engine, post_ops_t &post_ops,\n--            const memory_desc_t &dst_md) {\n-+            const memory_desc_t &dst_md, int post_op_start_index = 0) {\n-+\n-+        post_op_start_index_ = post_op_start_index;\n- \n-         CHECK(post_ops.set_default_formats(&dst_md));\n-         dst_data_type = dst_md.data_type;\n-@@ -41,7 +43,7 @@ struct acl_post_ops_t {\n-         sum_index = -1;\n-         post_op_primitives = {};\n- \n--        for (int i = 0; i < post_ops.len(); i++) {\n-+        for (int i = post_op_start_index; i < post_ops.len(); i++) {\n-             auto &po = post_ops.entry_[i];\n- \n-             if (po.is_sum()) {\n-@@ -135,7 +137,8 @@ struct acl_post_ops_t {\n-     // formats\n-     status_t init(engine_t *engine, post_ops_t &base_post_ops,\n-             const memory_desc_t &dst_md,\n--            arm_compute::ActivationLayerInfo &act_info_to_fuse) {\n-+            arm_compute::ActivationLayerInfo &act_info_to_fuse,\n-+            int post_op_start_index = 0) {\n- \n-         CHECK(base_post_ops.set_default_formats(&dst_md));\n-         dst_data_type = dst_md.data_type;\n-@@ -149,18 +152,11 @@ struct acl_post_ops_t {\n-                     \"eltwise post op scale must be 1 (no scale)\");\n-             CHECK(acl_utils::convert_to_acl_act(first_po, act_info_to_fuse));\n- \n--            // Copy all but the first, because it has been fused\n--            post_ops_t post_ops;\n--            for (int idx = 1; idx < base_post_ops.len(); ++idx) {\n--                // Construct empty entry then copy, so that we can check for failure\n--                post_ops.entry_.emplace_back();\n--                post_ops.entry_.back().copy_from(base_post_ops.entry_[idx]);\n--            }\n--            return init(engine, post_ops, dst_md);\n--\n-+             // post_op_start_index + 1 to skip the fused eltwise\n-+              return init(engine, base_post_ops, dst_md, post_op_start_index + 1);\n-         } else {\n-             // Nothing to fuse, just copy all post ops\n--            return init(engine, base_post_ops, dst_md);\n-+            return init(engine, base_post_ops, dst_md, post_op_start_index);\n-         }\n-     }\n- \n-@@ -179,6 +175,9 @@ struct acl_post_ops_t {\n- private:\n-     // Index of the sum post op if there is one, < 0 means no sum\n-     int sum_index = -1;\n-+    // Index of the first post op this primitive executes. This is typically the\n-+    // number of post ops which were fused.\n-+    int post_op_start_index_ = 0;\n-     data_type_t dst_data_type;\n-     // Vector of primitives used to execute the post ops. They are constructed\n-     // in init to be either acl_binary_t (for sum, add, sub, div, mul, min and\n--- \n-2.34.1"
        },
        {
            "sha": "202902a1894a86f7e14e3b137893098c932e1494",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_fp32_bf16_reorder.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 111,
            "changes": 111,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_fp32_bf16_reorder.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_fp32_bf16_reorder.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_fp32_bf16_reorder.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,111 +0,0 @@\n- *******************************************************************************\n- Copyright 2023 Arm Limited and affiliates.\n- SPDX-License-Identifier: Apache-2.0\n-\n- Licensed under the Apache License, Version 2.0 (the \"License\");\n- you may not use this file except in compliance with the License.\n- You may obtain a copy of the License at\n-\n-     http://www.apache.org/licenses/LICENSE-2.0\n-\n- Unless required by applicable law or agreed to in writing, software\n- distributed under the License is distributed on an \"AS IS\" BASIS,\n- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- See the License for the specific language governing permissions and\n- limitations under the License.\n- *******************************************************************************\n-diff --git a/src/cpu/aarch64/cpu_isa_traits.hpp b/src/cpu/aarch64/cpu_isa_traits.hpp\n-index 4a43b24c5..1a5cfe590 100644\n---- a/src/cpu/aarch64/cpu_isa_traits.hpp\n-+++ b/src/cpu/aarch64/cpu_isa_traits.hpp\n-@@ -1,6 +1,7 @@\n- /*******************************************************************************\n- * Copyright 2018-2023 Intel Corporation\n- * Copyright 2020-2023 FUJITSU LIMITED\n-+* Copyright 2023 Arm Ltd. and affiliates\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n-@@ -211,10 +212,10 @@ static inline bool mayiuse_atomic() {\n-     return cpu().isAtomicSupported();\n- }\n- \n--inline bool isa_has_bf16(cpu_isa_t isa) {\n--    return false;\n-+static inline bool mayiuse_bf16() {\n-+    using namespace Xbyak_aarch64::util;\n-+    return cpu().isBf16Supported();\n- }\n--\n- } // namespace\n- \n- /* whatever is required to generate string literals... */\n-diff --git a/src/cpu/aarch64/jit_uni_reorder.cpp b/src/cpu/aarch64/jit_uni_reorder.cpp\n-index 6bd259ec2..5541bb702 100644\n---- a/src/cpu/aarch64/jit_uni_reorder.cpp\n-+++ b/src/cpu/aarch64/jit_uni_reorder.cpp\n-@@ -1,7 +1,7 @@\n- /*******************************************************************************\n- * Copyright 2018-2023 Intel Corporation\n- * Copyright 2020-2023 FUJITSU LIMITED\n--* Copyright 2022 Arm Ltd. and affiliates\n-+* Copyright 2022-2023 Arm Ltd. and affiliates\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n-@@ -163,11 +163,11 @@ struct jit_uni_reorder_kernel_f32_t : public kernel_t, public jit_generator {\n- \n-         bool ok = true && p.ndims > 0\n-                 && utils::one_of(p.itype, f32, s32, data_type::s8, u8)\n--                && utils::one_of(p.otype, f32, s32, data_type::s8, u8)\n-+                && utils::one_of(p.otype, f32, bf16, s32, data_type::s8, u8)\n-                 && utils::everyone_is(0, p.ioff, p.ooff) /* do we need this? */\n-                 && utils::one_of(p.beta, 0.f, 1.f) /* anything else? */\n--                && simple_impl_desc_init(p, nullptr)\n--                && prb_has_small_strides(p);\n-+                && simple_impl_desc_init(p, nullptr) && prb_has_small_strides(p)\n-+                && ((p.otype != bf16) || (p.itype == f32 && mayiuse_bf16()));\n- \n-         return ok;\n-     }\n-@@ -648,6 +648,9 @@ struct jit_uni_reorder_kernel_f32_t : public kernel_t, public jit_generator {\n-                         cvt_v_s32_u8(startIdx, regNum);\n-                     if (idt == data_type::s8) cvt_v_s8_u8(startIdx, regNum);\n-                     break;\n-+                case bf16:\n-+                    if (idt == f32) cvt_v_f32_bf16(startIdx, regNum);\n-+                    break;\n-                 default: assert(!\"unreachable\");\n-             }\n-         };\n-@@ -1677,6 +1680,10 @@ struct jit_uni_reorder_kernel_f32_t : public kernel_t, public jit_generator {\n-         UNROLL_INST(fcvtzs, VReg4S, tmp, tmp);\n-     }\n- \n-+    void cvt_v_f32_bf16(const size_t startIdx, const size_t regNum) {\n-+        UNROLL_INST2(bfcvtn, VReg4H(i), VReg4S(i));\n-+    }\n-+\n-     void cvt_z_s8_s32(const size_t startIdx, const size_t regNum) {\n-         cvt_z_b_s(startIdx, regNum);\n-         UNROLL_INST(sxtb, ZRegS, tmp, P_ALL_ONE / T_m, tmp);\n-diff --git a/src/cpu/reorder/cpu_reorder_regular_f32_bf16.cpp b/src/cpu/reorder/cpu_reorder_regular_f32_bf16.cpp\n-index ba5499ba9..d4e21d316 100644\n---- a/src/cpu/reorder/cpu_reorder_regular_f32_bf16.cpp\n-+++ b/src/cpu/reorder/cpu_reorder_regular_f32_bf16.cpp\n-@@ -1,5 +1,6 @@\n- /*******************************************************************************\n- * Copyright 2020-2022 Intel Corporation\n-+* Copyright 2023 Arm Ltd. and affiliates\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n-@@ -34,6 +35,8 @@ const impl_list_map_t &regular_f32_bf16_impl_list_map() {\n-             DNNL_NON_X64_ONLY(REG_SR_BIDIR(f32, any, bf16, nChw16c))\n-             DNNL_NON_X64_ONLY(REG_SR_BIDIR(f32, any, bf16, nCdhw16c))\n- \n-+            DNNL_AARCH64_ONLY(CPU_REORDER_INSTANCE(aarch64::jit_uni_reorder_t))\n-+\n-             DNNL_NON_X64_ONLY(REG_SR(f32, oihw, bf16, OIhw8i16o2i, fmt_order::keep))\n-             DNNL_NON_X64_ONLY(REG_SR(f32, goihw, bf16, gOIhw8i16o2i, fmt_order::keep))\n-             DNNL_NON_X64_ONLY(REG_SR(f32, oihw, bf16, OIhw8o16i2o, fmt_order::keep))"
        },
        {
            "sha": "217e668352de5c37c7decbc6af67ed5ea910c76f",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_indirect_conv.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_indirect_conv.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_indirect_conv.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_indirect_conv.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,31 +0,0 @@\n- *******************************************************************************\n- Copyright 2024 Arm Limited and affiliates.\n- SPDX-License-Identifier: Apache-2.0\n-\n- Licensed under the Apache License, Version 2.0 (the \"License\");\n- you may not use this file except in compliance with the License.\n- You may obtain a copy of the License at\n-\n-     http://www.apache.org/licenses/LICENSE-2.0\n-\n- Unless required by applicable law or agreed to in writing, software\n- distributed under the License is distributed on an \"AS IS\" BASIS,\n- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- See the License for the specific language governing permissions and\n- limitations under the License.\n- *******************************************************************************\n-diff --git a/src/cpu/aarch64/acl_convolution_utils.cpp b/src/cpu/aarch64/acl_convolution_utils.cpp\n-index f043fee4bc..0384cce757 100644\n---- a/src/cpu/aarch64/acl_convolution_utils.cpp\n-+++ b/src/cpu/aarch64/acl_convolution_utils.cpp\n-@@ -313,10 +313,6 @@ status_t init_conf_indirect_gemm(acl_conv_conf_t &acp, memory_desc_t &src_md,\n-\n-     CHECK(acl_init_conf(acp, src_md, weights_md, dst_md, bias_md, cd, attr));\n-\n--    // Indirect is slower than gemm for low thread counts, except for fast math\n--    if (dnnl_get_max_threads() < 28 && !acp.fast_math)\n--        return status::unimplemented;\n--\n-     // If we do not need to pad input channels for fast math mode then it would\n-     // be faster to run convolution with im2row instead of using indirect kernel\n-     int block_by = arm_compute::block_by(acp.weights_info.weight_format());"
        },
        {
            "sha": "d0e87414f8e9675de151792af752f03763b2ecf7",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_lock_fixed_format_matmul.patch",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_lock_fixed_format_matmul.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_lock_fixed_format_matmul.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_lock_fixed_format_matmul.patch?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -0,0 +1,33 @@\n+# Copyright 2025 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+diff --git a/src/cpu/aarch64/matmul/acl_matmul.cpp b/src/cpu/aarch64/matmul/acl_matmul.cpp\n+index 380a0a3843..61cd2af4b7 100644\n+--- a/src/cpu/aarch64/matmul/acl_matmul.cpp\n++++ b/src/cpu/aarch64/matmul/acl_matmul.cpp\n+@@ -178,11 +178,9 @@ status_t acl_matmul_t::execute_forward(const exec_ctx_t &ctx) const {\n+\n+     std::unique_lock<std::mutex> locker {mtx_, std::defer_lock};\n+\n+-    // Some of the underlying kernels used by ACL still require some state and\n+-    // are not safe to be called in parallel with different execution contexts.\n+-    // Eventually when all kernels are truly stateless, this guard can be\n+-    // removed.\n+-    if (!acl_obj_->asm_gemm.has_stateless_impl()) { locker.lock(); }\n++    // Non-fixed-format kernels in ACL hold shared state and are not safe to be\n++    // called in parallel with different execution contexts.\n++    if (!IsFixedFormat) { locker.lock(); }\n+\n+     bool is_transA = amp.is_transA;\n+     bool is_transB = amp.is_transB;\n\\ No newline at end of file"
        },
        {
            "sha": "5da6756c70a2753ce3990337e08206357fb096ac",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_reorder.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 371,
            "changes": 371,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_reorder.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_reorder.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_reorder.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,371 +0,0 @@\n- *******************************************************************************\n- Copyright 2023 Arm Limited and affiliates.\n- SPDX-License-Identifier: Apache-2.0\n-\n- Licensed under the Apache License, Version 2.0 (the \"License\");\n- you may not use this file except in compliance with the License.\n- You may obtain a copy of the License at\n-\n-     http://www.apache.org/licenses/LICENSE-2.0\n-\n- Unless required by applicable law or agreed to in writing, software\n- distributed under the License is distributed on an \"AS IS\" BASIS,\n- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- See the License for the specific language governing permissions and\n- limitations under the License.\n- *******************************************************************************\n-diff --git a/src/cpu/aarch64/acl_reorder.cpp b/src/cpu/aarch64/acl_reorder.cpp\n-new file mode 100644\n-index 000000000..061751b55\n---- /dev/null\n-+++ b/src/cpu/aarch64/acl_reorder.cpp\n-@@ -0,0 +1,52 @@\n-+/*******************************************************************************\n-+* Copyright 2023 Arm Ltd. and affiliates\n-+*\n-+* Licensed under the Apache License, Version 2.0 (the \"License\");\n-+* you may not use this file except in compliance with the License.\n-+* You may obtain a copy of the License at\n-+*\n-+*     http://www.apache.org/licenses/LICENSE-2.0\n-+*\n-+* Unless required by applicable law or agreed to in writing, software\n-+* distributed under the License is distributed on an \"AS IS\" BASIS,\n-+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-+* See the License for the specific language governing permissions and\n-+* limitations under the License.\n-+*******************************************************************************/\n-+\n-+#include \"cpu/aarch64/acl_reorder.hpp\"\n-+\n-+namespace dnnl {\n-+namespace impl {\n-+namespace cpu {\n-+namespace aarch64 {\n-+\n-+status_t acl_reorder_fwd_t::execute_forward(const exec_ctx_t &ctx) const {\n-+    // Lock here is needed because resource_mapper does not support\n-+    // concurrent multithreaded access.\n-+    std::lock_guard<std::mutex> _lock {this->mtx};\n-+\n-+    auto src = CTX_IN_MEM(const void *, DNNL_ARG_FROM);\n-+    auto dst = CTX_OUT_MEM(void *, DNNL_ARG_TO);\n-+\n-+    // Retrieve primitive resource and configured Compute Library objects\n-+    auto *acl_resource\n-+            = ctx.get_resource_mapper()->get<acl_reorder_resource_t>(this);\n-+\n-+    acl_reorder_obj_t &acl_obj = acl_resource->get_acl_obj();\n-+\n-+    acl_obj.src_tensor.allocator()->import_memory(const_cast<void *>(src));\n-+    acl_obj.dst_tensor.allocator()->import_memory(dst);\n-+\n-+    acl_obj.reorder.run();\n-+\n-+    acl_obj.src_tensor.allocator()->free();\n-+    acl_obj.dst_tensor.allocator()->free();\n-+\n-+    return status::success;\n-+}\n-+\n-+} // namespace aarch64\n-+} // namespace cpu\n-+} // namespace impl\n-+} // namespace dnnl\n-diff --git a/src/cpu/aarch64/acl_reorder.hpp b/src/cpu/aarch64/acl_reorder.hpp\n-new file mode 100644\n-index 0000000000..edbc38914d\n---- /dev/null\n-+++ b/src/cpu/aarch64/acl_reorder.hpp\n-@@ -0,0 +1,262 @@\n-+/*******************************************************************************\n-+* Copyright 2023 Arm Ltd. and affiliates\n-+*\n-+* Licensed under the Apache License, Version 2.0 (the \"License\");\n-+* you may not use this file except in compliance with the License.\n-+* You may obtain a copy of the License at\n-+*\n-+*     http://www.apache.org/licenses/LICENSE-2.0\n-+*\n-+* Unless required by applicable law or agreed to in writing, software\n-+* distributed under the License is distributed on an \"AS IS\" BASIS,\n-+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-+* See the License for the specific language governing permissions and\n-+* limitations under the License.\n-+*******************************************************************************/\n-+#ifndef CPU_AARCH64_ACL_REORDER_HPP\n-+#define CPU_AARCH64_ACL_REORDER_HPP\n-+\n-+#include \"cpu/aarch64/acl_utils.hpp\"\n-+#include \"cpu/reorder/cpu_reorder_pd.hpp\"\n-+#include \"arm_compute/core/Types.h\"\n-+#include \"common/utils.hpp\"\n-+\n-+namespace dnnl {\n-+namespace impl {\n-+namespace cpu {\n-+namespace aarch64 {\n-+\n-+struct acl_reorder_obj_t {\n-+    arm_compute::NEReorderLayer reorder;\n-+    arm_compute::Tensor src_tensor;\n-+    arm_compute::Tensor dst_tensor;\n-+    arm_compute::WeightFormat src_wf;\n-+    arm_compute::WeightFormat dst_wf;\n-+};\n-+\n-+struct acl_reorder_conf_t {\n-+    arm_compute::TensorInfo src_info;\n-+    arm_compute::TensorInfo dst_info;\n-+    arm_compute::WeightFormat src_wf;\n-+    arm_compute::WeightFormat dst_wf;\n-+};\n-+\n-+struct acl_reorder_resource_t : public resource_t {\n-+    acl_reorder_resource_t() : acl_obj_(utils::make_unique<acl_reorder_obj_t>()) {}\n-+\n-+    status_t configure(const acl_reorder_conf_t &app) {\n-+        if (!acl_obj_) return status::out_of_memory;\n-+\n-+        // Init Compute Library tensors based on info from descriptor\n-+        acl_obj_->src_tensor.allocator()->init(app.src_info);\n-+        acl_obj_->dst_tensor.allocator()->init(app.dst_info);\n-+\n-+        // clang-format off\n-+        acl_obj_->reorder.configure(\n-+            &acl_obj_->src_tensor,\n-+            &acl_obj_->dst_tensor,\n-+            app.src_wf,\n-+            app.dst_wf\n-+            );\n-+        // clang-format on\n-+\n-+        return status::success;\n-+    }\n-+\n-+    acl_reorder_obj_t &get_acl_obj() const { return *acl_obj_; }\n-+    DNNL_DISALLOW_COPY_AND_ASSIGN(acl_reorder_resource_t);\n-+\n-+private:\n-+    std::unique_ptr<acl_reorder_obj_t> acl_obj_;\n-+}; // acl_reorder_resource_t\n-+\n-+struct acl_reorder_fwd_t : public primitive_t {\n-+    using primitive_t::primitive_t;\n-+    struct pd_t : public cpu_reorder_pd_t {\n-+\n-+        using cpu_reorder_pd_t::cpu_reorder_pd_t;\n-+\n-+        DECLARE_COMMON_PD_T(\"acl\", acl_reorder_fwd_t);\n-+\n-+        static status_t create(reorder_pd_t **reorder_pd, engine_t *engine,\n-+                const primitive_attr_t *attr, engine_t *src_engine,\n-+                const memory_desc_t *src_md, engine_t *dst_engine,\n-+                const memory_desc_t *dst_md) {\n-+\n-+            using namespace acl_utils;\n-+            // using skip_mask_t = dnnl_primitive_attr::skip_mask_t;\n-+\n-+            bool ok = src_md->data_type\n-+                            == dst_md->data_type // ACL only supports matching src/dst data types\n-+                    && utils::one_of(src_md->data_type,\n-+                            data_type::f32) // Only supports f32 for now\n-+                    && attr->has_default_values();\n-+            if (!ok) return status::unimplemented;\n-+\n-+            int mask = -1;\n-+            bool is_set = false;\n-+            // CHECK(attr->scales_.get(DNNL_ARG_DST, &mask, &is_set));\n-+            const memory_desc_wrapper input_d(src_md);\n-+            if (input_d.has_runtime_dims_or_strides() && is_set && mask > 0)\n-+                return status::unimplemented;\n-+\n-+            // Create and check primitive descriptor\n-+            auto _pd = new pd_t(attr, src_engine->kind(), src_md,\n-+                    dst_engine->kind(), dst_md);\n-+            if (_pd == nullptr) return status::out_of_memory;\n-+            if (_pd->init(engine, src_engine, dst_engine) != status::success) {\n-+                delete _pd;\n-+                return status::unimplemented;\n-+            }\n-+\n-+            const memory_desc_wrapper src_d(*src_md);\n-+            const memory_desc_wrapper dst_d(*dst_md);\n-+\n-+            const int ndims = src_d.ndims();\n-+\n-+            auto src_tag = memory_desc_matches_one_of_tag(\n-+                            *src_md, format_tag::ba, format_tag::cdba);\n-+            ACL_CHECK_SUPPORT(\n-+                            utils::one_of(format_tag::undef, src_tag),\n-+                            \"\");\n-+\n-+            arm_compute::TensorShape acl_tensor_shape_in;\n-+            arm_compute::TensorShape acl_tensor_shape_out;\n-+            // Need even amount of dims in dim 0 for ACL kernel (eg mulitple of 8 rows when blocking by 8)\n-+            int dim_0_rounded_up;\n-+\n-+            // Switch for 2 or 4 dim tensors\n-+            switch(ndims)\n-+            {\n-+                // Currently for Ab4a and Ab8a\n-+                // No format_tag for these, have to deduce from stride\n-+                case 2:\n-+                    {\n-+                        if(dst_md->dims[0] == 1 || dst_md->dims[1] == 1){\n-+                            return status::unimplemented;\n-+                        }\n-+                        int dst_dim_1 = dst_md->dims[1];\n-+                        int dst_dim_0_stride = dst_md->format_desc.blocking.strides[0];\n-+                        int dst_dim_1_stride = dst_md->format_desc.blocking.strides[1];\n-+                        // Interleave of 4 or 8 that stride for dim 1\n-+                        if (dst_dim_1_stride != 4 && dst_dim_1_stride != 8){\n-+                            return status::unimplemented;\n-+                        }\n-+                        // Check to ensure it's a blocking transpose\n-+                        if (dst_dim_1 * dst_dim_1_stride != dst_dim_0_stride){\n-+                            return status::unimplemented;\n-+                        }\n-+                        if(dst_dim_1_stride == 4){\n-+                            // Set Dest WeightFormat\n-+                            _pd->app_.dst_wf = arm_compute::WeightFormat::OHWIo4;\n-+                            dim_0_rounded_up\n-+                                    = utils::rnd_up(src_md->dims[0], 4);\n-+                        } else {\n-+                            // Set Dest WeightFormat\n-+                            _pd->app_.dst_wf = arm_compute::WeightFormat::OHWIo8;\n-+                            dim_0_rounded_up\n-+                                    = utils::rnd_up(src_md->dims[0], 8);\n-+                        }\n-+                        acl_tensor_shape_in = arm_compute::TensorShape(src_md->dims[1], src_md->dims[0]);\n-+                        acl_tensor_shape_out = arm_compute::TensorShape(src_md->dims[1], dim_0_rounded_up);\n-+\n-+                        break;\n-+                    }\n-+                // Currently for Acdb4a and Acdb8a\n-+                case 4:\n-+                    { \n-+\n-+                        auto dst_tag = memory_desc_matches_one_of_tag(\n-+                            *dst_md, format_tag::Acdb4a, format_tag::Acdb8a);\n-+                        ACL_CHECK_SUPPORT(\n-+                            utils::one_of(format_tag::undef, dst_tag),\n-+                            \"\");\n-+                        if(dst_tag == format_tag::Acdb4a){\n-+                            // Set Dest WeightFormat\n-+                            _pd->app_.dst_wf = arm_compute::WeightFormat::OHWIo4;\n-+                            dim_0_rounded_up\n-+                                    = utils::rnd_up(src_md->dims[0], 4);\n-+                        }\n-+                        else{\n-+                            // Set Dest WeightFormat\n-+                            _pd->app_.dst_wf = arm_compute::WeightFormat::OHWIo8;\n-+                            dim_0_rounded_up\n-+                                    = utils::rnd_up(src_md->dims[0], 8);\n-+                        }\n-+                        // Currently only supporting AxBx1x1 cases\n-+                        if(dst_md->dims[2] != 1 || dst_md->dims[3] != 1){\n-+                            return status::unimplemented;\n-+                        }\n-+                        if(dst_md->dims[0] == 1 || dst_md->dims[1] == 1){\n-+                            return status::unimplemented;\n-+                        }\n-+                        acl_tensor_shape_in = arm_compute::TensorShape(src_md->dims[3], src_md->dims[2], src_md->dims[1], src_md->dims[0]);\n-+                        acl_tensor_shape_out = arm_compute::TensorShape(src_md->dims[3], src_md->dims[2], src_md->dims[1], dim_0_rounded_up);\n-+                        break;\n-+                    }\n-+                default:\n-+                    return status::unimplemented;\n-+            }\n-+\n-+            // Choose the data layout\n-+            // bool is_nspc = utils::one_of(src_tag, format_tag::nhwc);\n-+            const auto acl_layout = arm_compute::DataLayout::NCHW;\n-+\n-+            // Set Source WeightFormat\n-+            _pd->app_.src_wf = arm_compute::WeightFormat::OHWI;\n-+\n-+            // Create ACL tensor infos\n-+            const data_type_t data_type = src_d.data_type();\n-+            const arm_compute::DataType acl_data_t\n-+                    = acl_utils::get_acl_data_t(data_type);\n-+            _pd->app_.src_info = arm_compute::TensorInfo(\n-+                        acl_tensor_shape_in, 1, acl_data_t, acl_layout);\n-+            _pd->app_.dst_info = arm_compute::TensorInfo(\n-+                        acl_tensor_shape_out, 1, acl_data_t, acl_layout);\n-+\n-+            // Init scratch memory, not used so 0 in this implementation\n-+            _pd->init_scratchpad_md();\n-+\n-+            return safe_ptr_assign(*reorder_pd, _pd);\n-+        } // create \n-+\n-+        friend dnnl::impl::impl_list_item_t;\n-+        acl_reorder_conf_t app_;\n-+\n-+    }; // pd_t\n-+\n-+    acl_reorder_fwd_t(const pd_t *apd) : primitive_t(apd) {}\n-+\n-+    status_t create_resource(\n-+            engine_t *engine, resource_mapper_t &mapper) const override {\n-+        if (mapper.has_resource(this)) return status::success;\n-+\n-+        auto r = utils::make_unique<acl_reorder_resource_t>();\n-+        if (!r) return status::out_of_memory;\n-+\n-+        // Configure the resource based on information from primitive descriptor\n-+        CHECK(r->configure(pd()->app_));\n-+\n-+        mapper.add(this, std::move(r));\n-+        return status::success;\n-+    }\n-+\n-+    status_t execute(const exec_ctx_t &ctx) const override {\n-+        return execute_forward(ctx);\n-+    }\n-+\n-+private:\n-+    // To guard the const execute_forward, the mutex must be 'mutable'\n-+    mutable std::mutex mtx;\n-+    status_t execute_forward(const exec_ctx_t &ctx) const;\n-+    const pd_t *pd() const { return (const pd_t *)primitive_t::pd().get(); }\n-+\n-+\n-+}; // acl_reorder_fwd_t\n-+\n-+} // namespace aarch64\n-+} // namespace cpu\n-+} // namespace impl\n-+} // namespace dnnl\n-+\n-+#endif // CPU_AARCH64_ACL_REORDER_HPP\n-diff --git a/src/cpu/reorder/cpu_reorder_regular_f32_f32.cpp b/src/cpu/reorder/cpu_reorder_regular_f32_f32.cpp\n-index a4150b619..f4d6b4de3 100644\n---- a/src/cpu/reorder/cpu_reorder_regular_f32_f32.cpp\n-+++ b/src/cpu/reorder/cpu_reorder_regular_f32_f32.cpp\n-@@ -16,6 +16,7 @@\n- *******************************************************************************/\n- \n- #include \"cpu/reorder/cpu_reorder.hpp\"\n-+#include \"cpu/aarch64/acl_reorder.hpp\"\n- \n- namespace dnnl {\n- namespace impl {\n-@@ -28,6 +29,7 @@ const impl_list_map_t &regular_f32_f32_impl_list_map() {\n-         // f32 -> f32\n-         {{f32, f32, 0}, {\n-             REG_FAST_DIRECT_COPY_F32_F32\n-+            DNNL_AARCH64_ONLY(CPU_REORDER_INSTANCE(aarch64::acl_reorder_fwd_t))\n- \n-             DNNL_X64_ONLY(CPU_REORDER_INSTANCE(x64::brgemm_matmul_matrix_B_reorder_t))\n-             DNNL_X64_ONLY(CPU_REORDER_INSTANCE(x64::jit_blk_reorder_t))\n-@@ -69,6 +71,8 @@ const impl_list_map_t &regular_f32_f32_impl_list_map() {\n-             nullptr,\n-         }},\n-         {{f32, f32, 4}, {\n-+\n-+            DNNL_AARCH64_ONLY(CPU_REORDER_INSTANCE(aarch64::acl_reorder_fwd_t))\n-             CPU_REORDER_INSTANCE(rnn_weights_reorder_t<f32, f32>)\n- \n-             REG_FAST_DIRECT_COPY_F32_F32"
        },
        {
            "sha": "9583308396dd1df55b4636d386cbee4bd1620c61",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_thread_local_scheduler.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 97,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_thread_local_scheduler.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_thread_local_scheduler.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_thread_local_scheduler.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,97 +0,0 @@\n- *******************************************************************************\n- Copyright 2023 Arm Limited and affiliates.\n- SPDX-License-Identifier: Apache-2.0\n-\n- Licensed under the Apache License, Version 2.0 (the \"License\");\n- you may not use this file except in compliance with the License.\n- You may obtain a copy of the License at\n-\n-     http://www.apache.org/licenses/LICENSE-2.0\n-\n- Unless required by applicable law or agreed to in writing, software\n- distributed under the License is distributed on an \"AS IS\" BASIS,\n- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- See the License for the specific language governing permissions and\n- limitations under the License.\n- *******************************************************************************\n-diff --git a/src/cpu/aarch64/acl_thread.cpp b/src/cpu/aarch64/acl_thread.cpp\n-index fd2c76d01..bd7bed837 100644\n---- a/src/cpu/aarch64/acl_thread.cpp\n-+++ b/src/cpu/aarch64/acl_thread.cpp\n-@@ -55,14 +55,17 @@ void acl_set_benchmark_scheduler_default() {\n- #endif\n- \n- #if DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_THREADPOOL\n--void acl_set_tp_scheduler() {\n--    static std::once_flag flag_once;\n--    // Create threadpool scheduler\n--    std::shared_ptr<arm_compute::IScheduler> threadpool_scheduler\n--            = std::make_unique<ThreadpoolScheduler>();\n-+void acl_set_tp_scheduler(int intra_threads = 0) {\n-+    static thread_local std::once_flag flag_once;\n-     // set CUSTOM scheduler in ACL\n-     std::call_once(flag_once,\n--            [&]() { arm_compute::Scheduler::set(threadpool_scheduler); });\n-+            [&]() {\n-+                    // Create threadpool scheduler\n-+                    std::shared_ptr<arm_compute::IScheduler> threadpool_scheduler\n-+                        = std::make_unique<ThreadpoolScheduler>();\n-+                    threadpool_scheduler->set_num_threads(intra_threads);\n-+\n-+                    arm_compute::Scheduler::set(threadpool_scheduler); });\n- }\n- \n- void acl_set_threadpool_num_threads() {\n-@@ -102,14 +105,6 @@ void set_acl_threading() {\n-         acl_set_benchmark_scheduler_default();\n-     }\n- #endif\n--#if DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_THREADPOOL\n--    if (verbose_has_profile_externals()) {\n--        acl_set_tp_benchmark_scheduler();\n--    } else {\n--        acl_set_tp_scheduler();\n--    }\n--\n--#endif\n- }\n- \n- } // namespace acl_thread_utils\n-diff --git a/src/cpu/aarch64/acl_thread.hpp b/src/cpu/aarch64/acl_thread.hpp\n-index f073376e6..654a2aa5d 100644\n---- a/src/cpu/aarch64/acl_thread.hpp\n-+++ b/src/cpu/aarch64/acl_thread.hpp\n-@@ -40,7 +40,7 @@ void acl_set_benchmark_scheduler_default();\n- \n- #if DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_THREADPOOL\n- // Retrieve threadpool size during primitive execution and set ThreadpoolScheduler num_threads\n--void acl_set_tp_scheduler();\n-+void acl_set_tp_scheduler(int intra_threads);\n- void acl_set_threadpool_num_threads();\n- // Swap BenchmarkScheduler for custom scheduler builds (i.e. ThreadPoolScheduler) for DNNL_VERBOSE=profile,profile_externals\n- void acl_set_tp_benchmark_scheduler();\n-diff --git a/src/cpu/aarch64/acl_threadpool_scheduler.cpp b/src/cpu/aarch64/acl_threadpool_scheduler.cpp\n-index 439ca862e..6656c37a5 100644\n---- a/src/cpu/aarch64/acl_threadpool_scheduler.cpp\n-+++ b/src/cpu/aarch64/acl_threadpool_scheduler.cpp\n-@@ -102,8 +102,6 @@ void ThreadpoolScheduler::schedule_op(ICPPKernel *kernel, const Hints &hints,\n- void ThreadpoolScheduler::run_workloads(\n-         std::vector<arm_compute::IScheduler::Workload> &workloads) {\n- \n--    arm_compute::lock_guard<std::mutex> lock(this->_run_workloads_mutex);\n--\n-     const unsigned int num_threads\n-             = std::min(static_cast<unsigned int>(_num_threads),\n-                     static_cast<unsigned int>(workloads.size()));\n-diff --git a/src/cpu/cpu_engine.cpp b/src/cpu/cpu_engine.cpp\n-index 0bfec3871..7207b2b60 100644\n---- a/src/cpu/cpu_engine.cpp\n-+++ b/src/cpu/cpu_engine.cpp\n-@@ -47,6 +47,7 @@ status_t cpu_engine_t::create_stream(stream_t **stream, unsigned flags) {\n- #if DNNL_CPU_RUNTIME == DNNL_RUNTIME_THREADPOOL\n- status_t cpu_engine_t::create_stream(stream_t **stream,\n-         dnnl::threadpool_interop::threadpool_iface *threadpool) {\n-+    dnnl::impl::cpu::aarch64::acl_thread_utils::acl_set_tp_scheduler(threadpool->get_num_threads());\n-     return safe_ptr_assign<stream_t>(\n-             *stream, new cpu_stream_t(this, threadpool));\n- }"
        },
        {
            "sha": "3a33af153e917c7d262e192b6136bc800bf00464",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_threadcap.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_threadcap.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93b8b3ee3ed110fc2cdd46a9be5ec633e216781a/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_threadcap.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_threadcap.patch?ref=93b8b3ee3ed110fc2cdd46a9be5ec633e216781a",
            "patch": "@@ -1,43 +0,0 @@\n- *******************************************************************************\n- Copyright 2023 Arm Limited and affiliates.\n- SPDX-License-Identifier: Apache-2.0\n-\n- Licensed under the Apache License, Version 2.0 (the \"License\");\n- you may not use this file except in compliance with the License.\n- You may obtain a copy of the License at\n-\n-     http://www.apache.org/licenses/LICENSE-2.0\n-\n- Unless required by applicable law or agreed to in writing, software\n- distributed under the License is distributed on an \"AS IS\" BASIS,\n- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- See the License for the specific language governing permissions and\n- limitations under the License.\n- *******************************************************************************\n-diff --git a/src/cpu/aarch64/acl_thread.cpp b/src/cpu/aarch64/acl_thread.cpp\n-index fd2c76d01..2d7c76d48 100644\n---- a/src/cpu/aarch64/acl_thread.cpp\n-+++ b/src/cpu/aarch64/acl_thread.cpp\n-@@ -17,6 +17,8 @@\n- #include \"cpu/aarch64/acl_thread.hpp\"\n- #if DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_THREADPOOL\n- #include \"cpu/aarch64/acl_threadpool_scheduler.hpp\"\n-+#elif DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_OMP\n-+#include <thread>\n- #endif\n- #include \"cpu/aarch64/acl_benchmark_scheduler.hpp\"\n- \n-@@ -30,9 +32,10 @@ namespace acl_thread_utils {\n- #if DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_OMP\n- void acl_thread_bind() {\n-     static std::once_flag flag_once;\n--    // The threads in Compute Library are bound for the cores 0..max_threads-1\n--    // dnnl_get_max_threads() returns OMP_NUM_THREADS\n--    const int max_threads = dnnl_get_max_threads();\n-+    // Cap the number of threads to 90% of the total core count\n-+    // to ensure Compute Library doesn't use too much resource\n-+    int capped_threads = (int)std::floor(0.9*std::thread::hardware_concurrency());\n-+    const int max_threads = std::min(capped_threads, dnnl_get_max_threads());\n-     // arm_compute::Scheduler does not support concurrent access thus a\n-     // workaround here restricts it to only one call\n-     std::call_once(flag_once, [&]() {"
        },
        {
            "sha": "78dd2462094cc582ec24d6fc1c04dbc06fe8341e",
            "filename": "third_party/xla/third_party/mkl_dnn/onednn_acl_threadpool_default_max.patch",
            "status": "added",
            "additions": 178,
            "deletions": 0,
            "changes": 178,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_threadpool_default_max.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_threadpool_default_max.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fmkl_dnn%2Fonednn_acl_threadpool_default_max.patch?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -0,0 +1,178 @@\n+# Copyright 2025 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+diff --git a/src/cpu/aarch64/acl_thread.cpp b/src/cpu/aarch64/acl_thread.cpp\n+index 53175a05f9..89731cb356 100644\n+--- a/src/cpu/aarch64/acl_thread.cpp\n++++ b/src/cpu/aarch64/acl_thread.cpp\n+@@ -1,5 +1,5 @@\n+ /*******************************************************************************\n+-* Copyright 2022-2024 Arm Ltd. and affiliates\n++* Copyright 2022-2025 Arm Ltd. and affiliates\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+@@ -83,17 +83,20 @@ void acl_set_threadpool_num_threads() {\n+ }\n+ // Swap BenchmarkScheduler for custom scheduler builds (i.e. ThreadPoolScheduler)\n+ void acl_set_tp_benchmark_scheduler() {\n+-    static std::once_flag flag_once;\n+-    // Create threadpool scheduler\n+-    std::unique_ptr<arm_compute::IScheduler> threadpool_scheduler\n+-            = std::make_unique<ThreadpoolScheduler>();\n+-    arm_compute::IScheduler *_real_scheduler = nullptr;\n+-    _real_scheduler = threadpool_scheduler.release();\n+-    // Create benchmark scheduler and set TP as real scheduler\n+-    std::shared_ptr<arm_compute::IScheduler> benchmark_scheduler\n+-            = std::make_unique<BenchmarkScheduler>(*_real_scheduler);\n+-    std::call_once(flag_once,\n+-            [&]() { arm_compute::Scheduler::set(benchmark_scheduler); });\n++    static thread_local std::once_flag flag_once;\n++    std::call_once(flag_once, [&]() {\n++        // Create threadpool scheduler\n++        std::unique_ptr<arm_compute::IScheduler> threadpool_scheduler\n++                = std::make_unique<ThreadpoolScheduler>();\n++        arm_compute::IScheduler *_real_scheduler = nullptr;\n++        _real_scheduler = threadpool_scheduler.release();\n++\n++        // Create benchmark scheduler and set TP as real scheduler\n++        std::shared_ptr<arm_compute::IScheduler> benchmark_scheduler\n++                = std::make_unique<BenchmarkScheduler>(*_real_scheduler);\n++\n++        arm_compute::Scheduler::set(benchmark_scheduler);\n++    });\n+ }\n+ #endif\n+ \n+diff --git a/src/cpu/aarch64/acl_threadpool_scheduler.cpp b/src/cpu/aarch64/acl_threadpool_scheduler.cpp\n+index 30910398d9..34cf44b7e2 100644\n+--- a/src/cpu/aarch64/acl_threadpool_scheduler.cpp\n++++ b/src/cpu/aarch64/acl_threadpool_scheduler.cpp\n+@@ -1,5 +1,5 @@\n+ /*******************************************************************************\n+-* Copyright 2022-2024 Arm Ltd. and affiliates\n++* Copyright 2022-2025 Arm Ltd. and affiliates\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+@@ -18,24 +18,17 @@\n+ \n+ #if DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_THREADPOOL\n+ \n+-#include \"cpu/aarch64/acl_thread.hpp\"\n+-\n+ #include \"common/counting_barrier.hpp\"\n+ #include \"common/dnnl_thread.hpp\"\n++#include \"cpu/aarch64/acl_thread.hpp\"\n+ \n+ #include \"arm_compute/core/CPP/ICPPKernel.h\"\n+ #include \"arm_compute/core/Error.h\"\n+-#include \"arm_compute/core/Helpers.h\"\n+-#include \"arm_compute/core/Utils.h\"\n+ #include \"arm_compute/runtime/IScheduler.h\"\n+ \n+-// BARRIER\n+ #include <atomic>\n+ #include <cassert>\n+-#include <chrono>\n+ #include <mutex>\n+-#include <thread>\n+-#include <condition_variable>\n+ \n+ namespace dnnl {\n+ namespace impl {\n+@@ -51,7 +44,7 @@ public:\n+ \n+     /// Function to check the next element in the range if there is one.\n+     bool get_next(unsigned int &next) {\n+-        next = atomic_fetch_add_explicit(\n++        next = std::atomic_fetch_add_explicit(\n+                 &_atomic_counter, 1u, std::memory_order_relaxed);\n+         return next < _end;\n+     }\n+@@ -70,11 +63,8 @@ void process_workloads(std::vector<IScheduler::Workload> &workloads,\n+     } while (feeder.get_next(workload_index));\n+ }\n+ \n+-ThreadpoolScheduler::ThreadpoolScheduler() {\n+-    using namespace dnnl::impl::threadpool_utils;\n+-    // Set number of threads to one when threadpool is not available.\n+-    _num_threads = get_active_threadpool() == nullptr ? 1 : num_threads_hint();\n+-}\n++ThreadpoolScheduler::ThreadpoolScheduler()\n++    : _num_threads(dnnl_get_max_threads()) {}\n+ \n+ ThreadpoolScheduler::~ThreadpoolScheduler() = default;\n+ \n+@@ -83,8 +73,8 @@ unsigned int ThreadpoolScheduler::num_threads() const {\n+ }\n+ \n+ void ThreadpoolScheduler::set_num_threads(unsigned int num_threads) {\n+-    arm_compute::lock_guard<std::mutex> lock(this->_run_workloads_mutex);\n+-    _num_threads = num_threads == 0 ? num_threads_hint() : num_threads;\n++    std::lock_guard<std::mutex> lock(this->_mtx);\n++    _num_threads = num_threads == 0 ? dnnl_get_max_threads() : num_threads;\n+ }\n+ \n+ void ThreadpoolScheduler::schedule(ICPPKernel *kernel, const Hints &hints) {\n+@@ -104,7 +94,7 @@ void ThreadpoolScheduler::schedule_op(ICPPKernel *kernel, const Hints &hints,\n+ void ThreadpoolScheduler::run_workloads(\n+         std::vector<arm_compute::IScheduler::Workload> &workloads) {\n+ \n+-    arm_compute::lock_guard<std::mutex> lock(this->_run_workloads_mutex);\n++    std::lock_guard<std::mutex> lock(this->_mtx);\n+ \n+     const unsigned int num_threads\n+             = std::min(static_cast<unsigned int>(_num_threads),\n+diff --git a/src/cpu/aarch64/acl_threadpool_scheduler.hpp b/src/cpu/aarch64/acl_threadpool_scheduler.hpp\n+index e9ba21c803..384dfec1b9 100644\n+--- a/src/cpu/aarch64/acl_threadpool_scheduler.hpp\n++++ b/src/cpu/aarch64/acl_threadpool_scheduler.hpp\n+@@ -1,5 +1,5 @@\n+ /*******************************************************************************\n+-* Copyright 2022 Arm Ltd. and affiliates\n++* Copyright 2022, 2025 Arm Ltd. and affiliates\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+@@ -22,7 +22,8 @@\n+ #if DNNL_CPU_THREADING_RUNTIME == DNNL_RUNTIME_THREADPOOL\n+ \n+ #include \"arm_compute/runtime/IScheduler.h\"\n+-#include \"support/Mutex.h\"\n++\n++#include <mutex>\n+ \n+ namespace dnnl {\n+ namespace impl {\n+@@ -32,7 +33,7 @@ namespace aarch64 {\n+ class ThreadpoolScheduler final : public arm_compute::IScheduler {\n+ public:\n+     ThreadpoolScheduler();\n+-    ~ThreadpoolScheduler();\n++    ~ThreadpoolScheduler() override;\n+ \n+     /// Sets the number of threads the scheduler will use to run the kernels.\n+     void set_num_threads(unsigned int num_threads) override;\n+@@ -54,8 +55,8 @@ protected:\n+     void run_workloads(std::vector<Workload> &workloads) override;\n+ \n+ private:\n+-    uint _num_threads {};\n+-    arm_compute::Mutex _run_workloads_mutex {};\n++    unsigned int _num_threads {};\n++    std::mutex _mtx;\n+ };\n+ \n+ } // namespace aarch64\n\\ No newline at end of file"
        },
        {
            "sha": "56a109f11d22348091f2eb71781b2c4e289a78d4",
            "filename": "third_party/xla/workspace2.bzl",
            "status": "modified",
            "additions": 10,
            "deletions": 17,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fworkspace2.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fworkspace2.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fworkspace2.bzl?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -205,33 +205,26 @@ def _tf_repositories():\n         name = \"mkl_dnn_acl_compatible\",\n         build_file = \"//third_party/mkl_dnn:mkldnn_acl.BUILD\",\n         patch_file = [\n-            \"//third_party/mkl_dnn:onednn_acl_threadcap.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_reorder.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_thread_local_scheduler.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_fp32_bf16_reorder.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_bf16_capability_detection_for_ubuntu20.04.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_indirect_conv.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_allow_blocked_weight_format_for_matmul_primitive.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_fix_segfault_during_postop_execute.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_add_bf16_platform_support_check.patch\",\n-            \"//third_party/mkl_dnn:onednn_acl_add_sbgemm_matmul_primitive_definition.patch\",\n+            \"//third_party/mkl_dnn:onednn_acl_lock_fixed_format_matmul.patch\",\n+            \"//third_party/mkl_dnn:onednn_acl_threadpool_default_max.patch\",\n         ],\n-        sha256 = \"2f76b407ef8893cca71340f88cd800019a1f14f8ac1bbdbb89a84be1370b52e3\",\n-        strip_prefix = \"oneDNN-3.2.1\",\n-        urls = tf_mirror_urls(\"https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.2.1.tar.gz\"),\n+        sha256 = \"5792cbc07764c6e25c459ff68efb5cfcd7f4a0ba66dca6a4a2c681cd7a644596\",\n+        strip_prefix = \"oneDNN-3.7\",\n+        urls = tf_mirror_urls(\"https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.7.zip\"),\n     )\n \n     tf_http_archive(\n         name = \"compute_library\",\n         patch_file = [\n+            \"//third_party/compute_library:acl_gemm_scheduling_heuristic.patch\",\n+            \"//third_party/compute_library:acl_stateless_gemm_workspace.patch\",\n             \"//third_party/compute_library:compute_library.patch\",\n-            \"//third_party/compute_library:acl_thread_local_scheduler.patch\",\n             \"//third_party/compute_library:exclude_omp_scheduler.patch\",\n             \"//third_party/compute_library:include_string.patch\",\n         ],\n-        sha256 = \"c4ca329a78da380163b2d86e91ba728349b6f0ee97d66e260a694ef37f0b0d93\",\n-        strip_prefix = \"ComputeLibrary-23.05.1\",\n-        urls = tf_mirror_urls(\"https://github.com/ARM-software/ComputeLibrary/archive/v23.05.1.tar.gz\"),\n+        sha256 = \"8273f68cd0bb17e9231a11a6618d245eb6d623884ae681c00e7a4eabca2dad42\",\n+        strip_prefix = \"ComputeLibrary-24.12\",\n+        urls = tf_mirror_urls(\"https://github.com/ARM-software/ComputeLibrary/archive/refs/tags/v24.12.tar.gz\"),\n     )\n \n     tf_http_archive("
        },
        {
            "sha": "2d5eff14fba6cc0618c6eafa62584d27dbbe6e24",
            "filename": "third_party/xla/xla/tsl/mkl/BUILD.bazel",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2FBUILD.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2FBUILD.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2FBUILD.bazel?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -80,14 +80,6 @@ config_setting(\n     },\n )\n \n-config_setting(\n-    name = \"build_with_mkl_aarch64_openmp\",\n-    define_values = {\n-        \"build_with_mkl_aarch64\": \"true\",\n-        \"build_with_openmp\": \"true\",\n-    },\n-)\n-\n filegroup(\n     name = \"LICENSE\",\n     srcs = ["
        },
        {
            "sha": "da31ec543c868921736ef94ddb1d2cd7d8ce8b71",
            "filename": "third_party/xla/xla/tsl/mkl/build_defs.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2Fbuild_defs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2Fbuild_defs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2Fbuild_defs.bzl?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -8,7 +8,6 @@ if_enable_mkl is a conditional to check if building with MKL and MKL is enabled.\n if_mkldnn_openmp checks if we are building x86 backend with OpenMP.\n if_onednn_async checks if we are building x86 backend (only Intel) with experimental async runtime support.\n if_mkldnn_aarch64_acl checks if we are building with Arm Compute Library.\n-if_mkldnn_aarch64_acl_openmp checks if we are building ACL with OpenMP.\n \n mkl_repository is a repository rule for creating MKL repository rule that can\n be pointed to either a local folder, or download it from the internet.\n@@ -161,12 +160,6 @@ def if_mkldnn_aarch64_acl(if_true, if_false = []):\n         \"//conditions:default\": if_false,\n     })\n \n-def if_mkldnn_aarch64_acl_openmp(if_true, if_false = []):\n-    return select({\n-        \"@local_xla//xla/tsl/mkl:build_with_mkl_aarch64_openmp\": if_true,\n-        \"//conditions:default\": if_false,\n-    })\n-\n # Enable Graph API on linux x86_64 and aarch64 platform with mkl builds\n def if_graph_api(if_true, if_false = []):\n     \"\"\"Returns `if_true` if Graph API is used with oneDNN.\"\"\""
        },
        {
            "sha": "7290f26123c238a849a2595b8ac243c73efca6db",
            "filename": "third_party/xla/xla/tsl/tsl.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Ftsl.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Ftsl.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Ftsl.bzl?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -11,7 +11,6 @@ load(\n     \"if_enable_mkl\",\n     \"if_mkl\",\n     \"if_mkldnn_aarch64_acl\",\n-    \"if_mkldnn_aarch64_acl_openmp\",\n     \"if_mkldnn_openmp\",\n     \"if_onednn_async\",\n     \"onednn_v3_define\",\n@@ -345,7 +344,6 @@ def tsl_copts(\n         if_onednn_async([\"-DENABLE_ONEDNN_ASYNC\"]) +\n         onednn_v3_define() +\n         if_mkldnn_aarch64_acl([\"-DDNNL_AARCH64_USE_ACL=1\"]) +\n-        if_mkldnn_aarch64_acl_openmp([\"-DENABLE_ONEDNN_OPENMP\"]) +\n         if_enable_acl([\"-DXLA_CPU_USE_ACL=1\", \"-fexceptions\"]) +\n         if_android_arm([\"-mfpu=neon\", \"-fomit-frame-pointer\"]) +\n         if_linux_x86_64([\"-msse3\"]) +"
        },
        {
            "sha": "478f0fd7a58ab58629d1087b63778b408d12a5be",
            "filename": "third_party/xla/xla/tsl/util/onednn_threadpool.h",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Futil%2Fonednn_threadpool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba2d69c2a27818c87876f01d48b752f3e42ae4cb/third_party%2Fxla%2Fxla%2Ftsl%2Futil%2Fonednn_threadpool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Futil%2Fonednn_threadpool.h?ref=ba2d69c2a27818c87876f01d48b752f3e42ae4cb",
            "patch": "@@ -161,9 +161,7 @@ class OneDnnThreadPool : public threadpool_iface {\n   static void set_onednn_max_threads(int num_threads) {\n #if DNNL_VERSION_MAJOR >= 3 || \\\n     (DNNL_VERSION_MAJOR == 2 && DNNL_VERSION_MINOR >= 7)\n-#ifndef DNNL_AARCH64_USE_ACL\n     dnnl_threadpool_interop_set_max_concurrency(num_threads);\n-#endif  // DNNL_AARCH64_USE_ACL\n #endif  // DNNL_VERSION_MAJOR >= 3 ||\n         // (DNNL_VERSION_MAJOR == 2 && DNNL_VERSION_MINOR >= 7)\n   }"
        }
    ],
    "stats": {
        "total": 3098,
        "additions": 1892,
        "deletions": 1206
    }
}