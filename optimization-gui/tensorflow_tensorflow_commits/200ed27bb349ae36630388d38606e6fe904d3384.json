{
    "author": "felixwqp",
    "message": "Add CollectivePermute support to CollectiveInterpolator.  The interpolation model for collective-permute uses transfer size and communication pattern type as input key.\n\nThe interpolation is implemented as follows:\n- `CollectivePermuteCostModelType` which classifies communication patterns (e.g. one-way, two-way-all-mutual) is added to `ExactInterpolatorKey` for collective-permute instructions.\n- Interpolation for collective-permute uses only exact matching via `ExactInterpolator` based on transfer size, and does not use `FallbackInterpolator`. This is because cost of collective-permute is primarily dependent on bytes transferred and communication pattern, and not on number of devices in the same way as other collectives.\n\nPiperOrigin-RevId: 834846468",
    "sha": "200ed27bb349ae36630388d38606e6fe904d3384",
    "files": [
        {
            "sha": "43b1d6c0cae4074be716898efdb6255c4ffcde67",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=200ed27bb349ae36630388d38606e6fe904d3384",
            "patch": "@@ -954,6 +954,7 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/functional:overload\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n@@ -980,6 +981,7 @@ xla_cc_test(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:string_view\","
        },
        {
            "sha": "61ac03f237ff807bb99fe4b706e31416449243e2",
            "filename": "third_party/xla/xla/service/gpu/model/collective_interpolator.cc",
            "status": "modified",
            "additions": 213,
            "deletions": 28,
            "changes": 241,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.cc?ref=200ed27bb349ae36630388d38606e6fe904d3384",
            "patch": "@@ -21,9 +21,11 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <utility>\n+#include <variant>\n #include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/functional/overload.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -76,13 +78,24 @@ constexpr int64_t kMaxDefaultNumberOfParticipatingDevices = 8;\n \n constexpr int64_t kMinDefaultNumberOfParticipatingDevices = 2;\n \n+// Collective op specification info, for all-gather, all-reduce, reduce-scatter,\n+// etc.\n+struct CollectiveOpSpecInfo {\n+  CollectiveDeviceList collective_device_list;\n+  GPUCommunicationType collective_comm;\n+};\n+\n+// Collective permute op specification info, for collective-permute, etc.\n+struct PermuteOpSpecInfo {\n+  CollectivePermuteCostModelType permute_type;\n+};\n+\n struct InterpolationSpecification {\n   HloOpcode opcode;\n-  GPUCommunicationType comm;\n   int64_t num_devices;\n   int64_t transfer_size;\n-  CollectiveDeviceList device_list;\n   PrimitiveType data_type;\n+  std::variant<CollectiveOpSpecInfo, PermuteOpSpecInfo> collective_params;\n };\n \n // Returns number of participating devices in an input `device_list`. Supports\n@@ -106,26 +119,41 @@ absl::StatusOr<InterpolationSpecification> Spec(\n         \"Cannot construct module from profile: \", profile.DebugString()));\n   }\n   auto instr = module->entry_computation()->root_instruction();\n-  auto collective = Cast<HloCollectiveInstruction>(instr);\n \n+  if (instr->opcode() == HloOpcode::kCollectivePermute) {\n+    auto* cp = Cast<HloCollectivePermuteInstruction>(instr);\n+    GpuHloCostAnalysis analysis(GpuHloCostAnalysis::Options(), device_info);\n+    TF_RETURN_IF_ERROR(cp->Accept(&analysis));\n+    int64_t bytes_transferred = analysis.BytesTransferred(*cp);\n+    CollectivePermuteCostModelType permute_type =\n+        GetCollectivePermuteCostModelType(\n+            *cp, /*num_devices_per_partition=*/num_devices_per_host);\n+    VLOG(10) << \"Spec permute_type: \" << static_cast<int>(permute_type);\n+    return InterpolationSpecification{/*opcode=*/cp->opcode(),\n+                                      /*num_devices=*/2,\n+                                      /*transfer_size=*/bytes_transferred,\n+                                      /*data_type=*/cp->shape().element_type(),\n+                                      /*collective_params=*/\n+                                      PermuteOpSpecInfo{permute_type}};\n+  }\n+\n+  auto collective = Cast<HloCollectiveInstruction>(instr);\n   GpuHloCostAnalysis analysis(GpuHloCostAnalysis::Options(), device_info);\n   TF_RETURN_IF_ERROR(collective->Accept(&analysis));\n   int64_t bytes_transferred = analysis.BytesTransferred(*collective);\n-\n-  TF_ASSIGN_OR_RETURN(auto comm,\n+  TF_ASSIGN_OR_RETURN(GPUCommunicationType comm,\n                       CommunicationType(num_devices_per_host, *collective,\n                                         device_info.gpu_compute_capability()));\n   TF_ASSIGN_OR_RETURN(int num_devices,\n                       GetNumParticipatingDevices(collective->device_list()));\n \n   return InterpolationSpecification{\n       /*opcode=*/collective->opcode(),\n-      /*comm=*/comm,\n       /*num_devices=*/num_devices,\n       /*transfer_size=*/bytes_transferred,\n-      /*device_list=*/collective->device_list(),\n       /*data_type=*/collective->shape().element_type(),\n-  };\n+      /*collective_params=*/\n+      CollectiveOpSpecInfo{collective->device_list(), comm}};\n }\n \n std::unique_ptr<HloModule> AllReduceModule(\n@@ -296,6 +324,30 @@ std::unique_ptr<HloModule> AllToAllModule(\n   return module;\n }\n \n+std::unique_ptr<HloModule> CollectivePermuteModule(\n+    const HloInstructionProfile& profile) {\n+  HloModuleConfig config;\n+\n+  auto module = std::make_unique<HloModule>(\"m\", config);\n+  absl::StatusOr<Shape> shape = Shape::FromProto(profile.instruction().shape());\n+  if (!shape.ok()) {\n+    VLOG(1) << \"Cannot parse shape: \" << profile.DebugString();\n+    return nullptr;\n+  }\n+\n+  HloComputation::Builder entry_builder(\"entry\");\n+  HloInstruction* p0 = entry_builder.AddInstruction(\n+      HloInstruction::CreateParameter(0, *shape, \"p0\"));\n+  std::vector<std::pair<int64_t, int64_t>> pairs;\n+  for (const auto& pair : profile.instruction().source_target_pairs()) {\n+    pairs.push_back({pair.source(), pair.target()});\n+  }\n+  entry_builder.AddInstruction(HloInstruction::CreateCollectivePermute(\n+      *shape, p0, pairs, profile.instruction().channel_id()));\n+  module->AddEntryComputation(entry_builder.Build());\n+  return module;\n+}\n+\n std::optional<CollectiveDeviceList> CanonicalDeviceList(\n     const HloCollectiveInstruction& instr) {\n   if (instr.device_list().iota_replica_group_list().has_value()) {\n@@ -378,13 +430,24 @@ ConstructExactInterpolators(int num_devices_per_host,\n     TF_ASSIGN_OR_RETURN(InterpolationSpecification spec,\n                         Spec(num_devices_per_host, profile, device_info));\n     // Construct exact interpolators.\n-    CollectiveInterpolator::ExactInterpolatorKey exact_key{\n-        /*opcode=*/spec.opcode,\n-        /*device_list=*/spec.device_list,\n-        /*data_type=*/\n-        RequiresAccumulation(spec.opcode) ? std::make_optional(spec.data_type)\n-                                          : std::nullopt,\n-    };\n+    CollectiveInterpolator::ExactInterpolatorKey exact_key = std::visit(\n+        absl::Overload{[&](PermuteOpSpecInfo permute_op_info) {\n+                         return CollectiveInterpolator::ExactInterpolatorKey{\n+                             /*opcode=*/spec.opcode,\n+                             /*collective_params=*/permute_op_info.permute_type,\n+                             /*data_type=*/std::nullopt};\n+                       },\n+                       [&](const CollectiveOpSpecInfo& op_info) {\n+                         return CollectiveInterpolator::ExactInterpolatorKey{\n+                             /*opcode=*/spec.opcode,\n+                             /*collective_params=*/\n+                             op_info.collective_device_list,\n+                             /*data_type=*/\n+                             RequiresAccumulation(spec.opcode)\n+                                 ? std::make_optional(spec.data_type)\n+                                 : std::nullopt};\n+                       }},\n+        spec.collective_params);\n     auto exact_it = exact_interpolators->find(exact_key);\n     if (exact_it == exact_interpolators->end()) {\n       auto interpolator = std::make_unique<\n@@ -409,29 +472,72 @@ absl::StatusOr<std::unique_ptr<\n ConstructExactNNInterpolators(int num_devices_per_host,\n                               const HloInstructionProfileList& profiles,\n                               const se::DeviceDescription& device_info) {\n+  VLOG(10) << \"ConstructExactNNInterpolators called.\";\n   auto exact_interpolators = std::make_unique<\n       absl::flat_hash_map<CollectiveInterpolator::ExactInterpolatorKey,\n                           std::unique_ptr<InterpolatorBase<int64_t, 1>>>>();\n \n   for (auto& profile : profiles.entries()) {\n+    VLOG(10) << \"Processing profile: \" << profile.DebugString();\n     TF_ASSIGN_OR_RETURN(InterpolationSpecification spec,\n                         Spec(num_devices_per_host, profile, device_info));\n+    std::visit(absl::Overload{\n+                   [&](PermuteOpSpecInfo permute_op_info) {\n+                     VLOG(10) << \"  Spec: opcode=\" << spec.opcode\n+                              << \", num_devices=\" << spec.num_devices\n+                              << \", transfer_size=\" << spec.transfer_size\n+                              << \", data_type=\" << spec.data_type\n+                              << \", permute_type=\"\n+                              << static_cast<int>(permute_op_info.permute_type);\n+                   },\n+                   [&](const CollectiveOpSpecInfo& op_info) {\n+                     VLOG(10) << \"  Spec: opcode=\" << spec.opcode\n+                              << \", num_devices=\" << spec.num_devices\n+                              << \", transfer_size=\" << spec.transfer_size\n+                              << \", data_type=\" << spec.data_type\n+                              << \", permute_type=nullopt\";\n+                   }},\n+               spec.collective_params);\n+\n     // Construct exact interpolators.\n-    CollectiveInterpolator::ExactInterpolatorKey exact_key{\n-        /*opcode=*/spec.opcode,\n-        /*device_list=*/spec.device_list,\n-        /*data_type=*/spec.data_type,\n-    };\n+    CollectiveInterpolator::ExactInterpolatorKey exact_key = std::visit(\n+        absl::Overload{[&](PermuteOpSpecInfo permute_op_info) {\n+                         return CollectiveInterpolator::ExactInterpolatorKey{\n+                             /*opcode=*/spec.opcode,\n+                             /*collective_params=*/permute_op_info.permute_type,\n+                             /*data_type=*/std::nullopt};\n+                       },\n+                       [&](const CollectiveOpSpecInfo& op_info) {\n+                         return CollectiveInterpolator::ExactInterpolatorKey{\n+                             /*opcode=*/spec.opcode,\n+                             /*collective_params=*/\n+                             op_info.collective_device_list,\n+                             /*data_type=*/spec.data_type};\n+                       }},\n+        spec.collective_params);\n+    VLOG(10) << \"  Constructed exact_key: opcode=\" << exact_key.opcode\n+             << \", data_type=\"\n+             << (exact_key.data_type.has_value()\n+                     ? std::to_string(static_cast<int>(*exact_key.data_type))\n+                     : \"nullopt\");\n+\n     auto exact_it = exact_interpolators->find(exact_key);\n     if (exact_it == exact_interpolators->end()) {\n+      VLOG(10) << \"  Exact interpolator not found for key, creating new one.\";\n       auto interpolator =\n           std::make_unique<EuclideanNNInterpolator<int64_t, 1>>();\n       (*exact_interpolators)[exact_key] = std::move(interpolator);\n+    } else {\n+      VLOG(10) << \"  Exact interpolator found for key.\";\n     }\n     std::array<int64_t, 1> exact_point = {spec.transfer_size};\n+    VLOG(10) << \"  Adding point {\" << exact_point[0] << \"} with throughput: \"\n+             << profile.network_throughput_bytes_per_sec();\n     exact_interpolators->at(exact_key)->Add(\n         exact_point, profile.network_throughput_bytes_per_sec());\n   }\n+  VLOG(10) << \"ConstructExactNNInterpolators finished. Total keys: \"\n+           << exact_interpolators->size();\n   return exact_interpolators;\n }\n \n@@ -448,12 +554,26 @@ ConstructFallbackInterpolators(int num_devices_per_host,\n   for (auto& profile : profiles.entries()) {\n     TF_ASSIGN_OR_RETURN(InterpolationSpecification spec,\n                         Spec(num_devices_per_host, profile, device_info));\n+    std::optional<GPUCommunicationType> collective_comm;\n+    std::visit(absl::Overload{[&](PermuteOpSpecInfo permute_op_info) {},\n+                              [&](const CollectiveOpSpecInfo& op_info) {\n+                                collective_comm = op_info.collective_comm;\n+                              }},\n+               spec.collective_params);\n+\n+    if (!collective_comm.has_value()) {\n+      // Collective-permute uses exact interpolation only.\n+      continue;\n+    }\n+\n     CollectiveInterpolator::FallbackInterpolatorKey key{\n         /*opcode=*/spec.opcode,\n-        /*communication_type=*/spec.comm,\n+        /*communication_type=*/collective_comm.value(),\n     };\n     auto it = fallback_interpolators->find(key);\n     if (it == fallback_interpolators->end()) {\n+      // For fallback interpolators, we initialize a EuclideanComplement\n+      // Interpolator with default min/max context values.\n       auto interpolator =\n           std::make_unique<EuclideanComplementInterpolator<int64_t, 2>>(\n               /*next_context=*/std::array<int64_t, 2>{-1, -1},\n@@ -487,12 +607,26 @@ ConstructFallbackNNInterpolators(int num_devices_per_host,\n   for (auto& profile : profiles.entries()) {\n     TF_ASSIGN_OR_RETURN(InterpolationSpecification spec,\n                         Spec(num_devices_per_host, profile, device_info));\n+    std::optional<GPUCommunicationType> collective_comm;\n+    std::visit(absl::Overload{[&](PermuteOpSpecInfo permute_op_info) {},\n+                              [&](const CollectiveOpSpecInfo& op_info) {\n+                                collective_comm = op_info.collective_comm;\n+                              }},\n+               spec.collective_params);\n+\n+    if (!collective_comm.has_value()) {\n+      // Collective-permute uses exact interpolation only.\n+      continue;\n+    }\n+\n     CollectiveInterpolator::FallbackInterpolatorKey key{\n         /*opcode=*/spec.opcode,\n-        /*communication_type=*/spec.comm,\n+        /*communication_type=*/collective_comm.value(),\n     };\n     auto it = fallback_interpolators->find(key);\n     if (it == fallback_interpolators->end()) {\n+      // For fallback NN interpolators, we initialize a EuclideanNNInterpolator\n+      // without default min/max context values.\n       auto interpolator =\n           std::make_unique<EuclideanNNInterpolator<int64_t, 2>>();\n \n@@ -548,16 +682,49 @@ CollectiveInterpolator::Create(int num_devices_per_host,\n }\n \n absl::StatusOr<absl::Duration> CollectiveInterpolator::EstimatedRuntime(\n-    const HloCollectiveInstruction& instr) const {\n+    const HloInstruction& instr) const {\n   // Exact interpolation.\n   int64_t bytes_transferred =\n       GetBytesTransferred(instr, device_info_, analysis_);\n \n-  std::optional<CollectiveDeviceList> devices = CanonicalDeviceList(instr);\n+  if (instr.opcode() == HloOpcode::kCollectivePermute) {\n+    auto* cp = Cast<HloCollectivePermuteInstruction>(&instr);\n+    const CollectivePermuteCostModelType& permute_type =\n+        GetCollectivePermuteCostModelType(\n+            *cp, /*num_devices_per_partition=*/cp->GetModule()\n+                     ->config()\n+                     .num_partitions());\n+\n+    VLOG(10) << \"EstimatedRuntime permute_type: \"\n+             << static_cast<int>(permute_type)\n+             << \" for instr: \" << instr.ToString() << \" num_partitions:\"\n+             << cp->GetModule()->config().num_partitions();\n+    ExactInterpolatorKey exact_key{\n+        /*opcode=*/instr.opcode(),\n+        /*collective_params=*/permute_type,\n+        /*data_type=*/std::nullopt,\n+    };\n+    VLOG(10) << \"Checking exact interpolator for CollectivePermute. Opcode: \"\n+             << instr.opcode()\n+             << \", PermuteType: \" << static_cast<int>(permute_type)\n+             << \", BytesTransferred: \" << bytes_transferred;\n+    if (exact_interpolators_->contains(exact_key)) {\n+      VLOG(10) << \"Exact interpolator found for CollectivePermute.\";\n+      std::array<int64_t, 1> point({bytes_transferred});\n+      return absl::Seconds(1.0 * bytes_transferred /\n+                           exact_interpolators_->at(exact_key)->Eval(point));\n+    }\n+    VLOG(10) << \"Exact interpolator NOT found for CollectivePermute.\";\n+    return absl::NotFoundError(\n+        absl::StrCat(\"Cannot find key for instr: \", instr.ToString()));\n+  }\n+  auto* collective = Cast<HloCollectiveInstruction>(&instr);\n+  std::optional<CollectiveDeviceList> devices =\n+      CanonicalDeviceList(*collective);\n   if (devices.has_value()) {\n     ExactInterpolatorKey exact_key{\n         /*opcode=*/instr.opcode(),\n-        /*device_list=*/*devices,\n+        /*collective_params=*/*devices,\n         /*data_type=*/\n         RequiresAccumulation(instr.opcode())\n             ? std::make_optional(instr.shape().element_type())\n@@ -570,14 +737,30 @@ absl::StatusOr<absl::Duration> CollectiveInterpolator::EstimatedRuntime(\n                            exact_interpolators_->at(exact_key)->Eval(point));\n     }\n   }\n-  // Fallback interpolation.\n+  // No fallback needed for permute.\n+  if (instr.opcode() == HloOpcode::kCollectivePermute) {\n+    return absl::NotFoundError(\n+        absl::StrCat(\"Cannot find key for instr: \", instr.ToString()));\n+  }\n+  auto* channel_instr = Cast<HloChannelInstruction>(&instr);\n   TF_ASSIGN_OR_RETURN(auto comm,\n-                      CommunicationType(num_devices_per_host_, instr,\n+                      CommunicationType(num_devices_per_host_, *channel_instr,\n                                         device_info_.gpu_compute_capability()));\n   TF_ASSIGN_OR_RETURN(auto num_devices, GetReplicaGroupCountAndSize(&instr));\n   std::array<int64_t, 2> point({bytes_transferred, num_devices->second});\n+  HloOpcode opcode = instr.opcode();\n+  if (instr.opcode() == HloOpcode::kAllGatherStart) {\n+    opcode = HloOpcode::kAllGather;\n+  } else if (instr.opcode() == HloOpcode::kAllReduceStart) {\n+    opcode = HloOpcode::kAllReduce;\n+  } else if (instr.opcode() == HloOpcode::kAsyncStart) {\n+    if (instr.async_wrapped_opcode() == HloOpcode::kReduceScatter) {\n+      opcode = HloOpcode::kReduceScatter;\n+    }\n+  }\n+\n   CollectiveInterpolator::FallbackInterpolatorKey key{\n-      /*opcode=*/AsyncToSyncOpcode(instr),\n+      /*opcode=*/opcode,\n       /*communication_type=*/comm,\n   };\n   if (!fallback_interpolators_->contains(key)) {\n@@ -591,6 +774,8 @@ absl::StatusOr<absl::Duration> CollectiveInterpolator::EstimatedRuntime(\n /*static*/ std::unique_ptr<HloModule> CollectiveInterpolator::ConstructModule(\n     const HloInstructionProfile& profile) {\n   switch (*StringToHloOpcode(profile.instruction().opcode())) {\n+    case HloOpcode::kCollectivePermute:\n+      return CollectivePermuteModule(profile);\n     case HloOpcode::kAllReduce:\n     case HloOpcode::kAllReduceStart:\n       return AllReduceModule(profile);"
        },
        {
            "sha": "dfd254fc667f95432657456cb60f24afde76dadf",
            "filename": "third_party/xla/xla/service/gpu/model/collective_interpolator.h",
            "status": "modified",
            "additions": 42,
            "deletions": 12,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator.h?ref=200ed27bb349ae36630388d38606e6fe904d3384",
            "patch": "@@ -20,8 +20,10 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <utility>\n+#include <variant>\n \n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/functional/overload.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/time/time.h\"\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n@@ -56,23 +58,46 @@ class CollectiveInterpolator {\n \n   struct ExactInterpolatorKey {\n     HloOpcode opcode;\n-    CollectiveDeviceList device_list;\n+    std::variant<CollectiveDeviceList, CollectivePermuteCostModelType>\n+        collective_params;\n     std::optional<PrimitiveType> data_type;\n \n     template <typename H>\n     friend H AbslHashValue(H h, const ExactInterpolatorKey& key) {\n-      return H::combine(\n-          std::move(h), key.opcode,\n-          key.device_list.ToString(/*print_full_replica_group_list=*/true),\n-          key.data_type);\n+      h = H::combine(std::move(h), key.opcode, key.data_type);\n+      std::visit(\n+          absl::Overload{[&](CollectivePermuteCostModelType permute_type) {\n+                           h = H::combine(std::move(h), permute_type);\n+                         },\n+                         [&](const CollectiveDeviceList& device_list) {\n+                           h = H::combine(std::move(h),\n+                                          device_list.ToString(\n+                                              /*print_full_replica_group_list=*/\n+                                              true));\n+                         }},\n+          key.collective_params);\n+      return h;\n     }\n \n     bool operator==(const ExactInterpolatorKey& other) const {\n-      return opcode == other.opcode &&\n-             device_list.ToString(/*print_full_replica_group_list=*/true) ==\n-                 other.device_list.ToString(\n-                     /*print_full_replica_group_list=*/true) &&\n-             data_type == other.data_type;\n+      if (opcode != other.opcode ||\n+          collective_params.index() != other.collective_params.index()) {\n+        return false;\n+      }\n+      return std::visit(\n+          absl::Overload{\n+              [&](CollectivePermuteCostModelType permute_type) {\n+                return permute_type == std::get<CollectivePermuteCostModelType>(\n+                                           other.collective_params);\n+              },\n+              [&](const CollectiveDeviceList& device_list) {\n+                return device_list.ToString(\n+                           /*print_full_replica_group_list=*/true) ==\n+                       std::get<CollectiveDeviceList>(other.collective_params)\n+                           .ToString(\n+                               /*print_full_replica_group_list=*/true);\n+              }},\n+          collective_params);\n     }\n   };\n \n@@ -97,9 +122,10 @@ class CollectiveInterpolator {\n   static std::unique_ptr<HloModule> ConstructModule(\n       const HloInstructionProfile& profile);\n \n-  // Returns the estimated runtime for a supported `collective`.\n+  // Returns the estimated runtime for a supported `collective` or\n+  // `collective-permute`.\n   absl::StatusOr<absl::Duration> EstimatedRuntime(\n-      const HloCollectiveInstruction& instr) const;\n+      const HloInstruction& instr) const;\n \n  private:\n   explicit CollectiveInterpolator(\n@@ -114,6 +140,10 @@ class CollectiveInterpolator {\n         analysis_(analysis) {}\n \n   ExactInterpolatorMap exact_interpolators_;\n+  // Fallback interpolators are only necessary for collective with complex\n+  // dimensions, e.g. async all-reduce, reduce-scatter, etc. Collective-permute\n+  // doesn't need fallback interpolators because its\n+  // category is simple and exact interpolation can cover all cases.\n   FallbackInterpolatorMap fallback_interpolators_;\n   const se::DeviceDescription& device_info_;\n   int num_devices_per_host_;"
        },
        {
            "sha": "7187900168cf02d3147a8a510051ccec9bc624b3",
            "filename": "third_party/xla/xla/service/gpu/model/collective_interpolator_test.cc",
            "status": "modified",
            "additions": 109,
            "deletions": 0,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc?ref=200ed27bb349ae36630388d38606e6fe904d3384",
            "patch": "@@ -19,9 +19,11 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n+#include <utility>\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/strings/string_view.h\"\n@@ -1081,5 +1083,112 @@ INSTANTIATE_TEST_SUITE_P(\n       return info.param.test_name;\n     });\n \n+struct CollectivePermuteTestCase {\n+  std::string test_name;\n+  CollectivePermuteCostModelType permute_type;\n+  absl::Duration expected_duration;\n+};\n+\n+class CollectivePermuteInterpolationTest\n+    : public TestWithParam<CollectivePermuteTestCase> {\n+  void SetUp() override {\n+    HloInstructionProfileList profiles;\n+    for (auto& [permute_type, points] : collective_permute_profiles_) {\n+      for (auto& [tensor_size, throughput] : points) {\n+        HloInstructionProfile entry =\n+            CollectivePermuteInstruction(permute_type, tensor_size);\n+        entry.set_network_throughput_bytes_per_sec(throughput);\n+        *profiles.add_entries() = entry;\n+      }\n+    }\n+    device_info_ = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+    interpolator_ = *CollectiveInterpolator::Create(kNumGpusPerHost, profiles,\n+                                                    device_info_);\n+  }\n+\n+ protected:\n+  absl::StatusOr<absl::Duration> EstimateRuntime(\n+      CollectivePermuteCostModelType permute_type, int64_t tensor_size) {\n+    auto instr = CollectivePermuteInstruction(permute_type, tensor_size);\n+    auto module = CollectiveInterpolator::ConstructModule(instr);\n+    module->mutable_config().set_num_partitions(kNumGpusPerHost);\n+    auto* eval = module->entry_computation()->root_instruction();\n+    return interpolator_->EstimatedRuntime(*eval);\n+  }\n+\n+ private:\n+  // Creates a collective permute instruction with a given `permute_type` and\n+  // `tensor_size`.\n+  // The perf table only supports the intra-partition collective permutes,\n+  // including one-way, two-way all-mutual and two-way has non-mutual.\n+  HloInstructionProfile CollectivePermuteInstruction(\n+      CollectivePermuteCostModelType permute_type, int64_t tensor_size) {\n+    Shape shape = ShapeUtil::MakeShape(PrimitiveType::F32, {tensor_size / 4});\n+\n+    HloInstructionProfile profile;\n+    profile.mutable_instruction()->set_opcode(\n+        HloOpcodeString(HloOpcode::kCollectivePermute));\n+    *profile.mutable_instruction()->mutable_shape() = shape.ToProto();\n+    profile.mutable_instruction()->set_channel_id(1);\n+\n+    std::vector<std::pair<int64_t, int64_t>> pairs;\n+    if (permute_type == CollectivePermuteCostModelType::kIntraPartitionOneWay) {\n+      pairs = {{0, 2}, {1, 3}};\n+    } else if (permute_type ==\n+               CollectivePermuteCostModelType::kIntraPartitionTwoWayAllMutual) {\n+      pairs = {{0, 1}, {1, 0}, {2, 3}, {3, 2}};\n+    } else if (permute_type == CollectivePermuteCostModelType::\n+                                   kIntraPartitionTwoWayHasNonMutual) {\n+      pairs = {{0, 1}, {1, 2}, {2, 3}, {3, 0}};\n+    }\n+    for (const auto& pair : pairs) {\n+      auto* p = profile.mutable_instruction()->add_source_target_pairs();\n+      p->set_source(pair.first);\n+      p->set_target(pair.second);\n+    }\n+    return profile;\n+  }\n+\n+  se::DeviceDescription device_info_;\n+  std::unique_ptr<CollectiveInterpolator> interpolator_;\n+  // The collective permute testing profiles for the perf table.\n+  // format: {collectove_permute_type: {{tensor_size_1, throughput_1},\n+  //                                    {tensor_size_2, throughput_2},\n+  //                                    ...}}\n+  absl::flat_hash_map<CollectivePermuteCostModelType,\n+                      std::vector<std::pair<int64_t, int64_t>>>\n+      collective_permute_profiles_ = {\n+          {CollectivePermuteCostModelType::kIntraPartitionOneWay,\n+           {{512, 512}, {1024, 1024}, {2048, 1536}}},\n+          {CollectivePermuteCostModelType::kIntraPartitionTwoWayAllMutual,\n+           {{512, 1024}, {1024, 2048}, {2048, 3072}}},\n+          {CollectivePermuteCostModelType::kIntraPartitionTwoWayHasNonMutual,\n+           {{512, 1536}, {1024, 3072}, {2048, 4096}}}};\n+};\n+\n+TEST_P(CollectivePermuteInterpolationTest, InterpolatesCorrectly) {\n+  const auto& [_, permute_type, expected_duration] = GetParam();\n+  auto runtime = EstimateRuntime(permute_type, 1024);\n+  ASSERT_TRUE(runtime.ok());\n+  EXPECT_NEAR(absl::ToDoubleSeconds(*runtime),\n+              absl::ToDoubleSeconds(expected_duration), 1e-5);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    CollectivePermuteInterpolationTestInstantiation,\n+    CollectivePermuteInterpolationTest,\n+    ValuesIn<CollectivePermuteTestCase>({\n+        {\"OneWay\", CollectivePermuteCostModelType::kIntraPartitionOneWay,\n+         absl::Seconds(1)},\n+        {\"TwoWayAllMutual\",\n+         CollectivePermuteCostModelType::kIntraPartitionTwoWayAllMutual,\n+         absl::Milliseconds(500)},\n+        {\"TwoWayHasNonMutual\",\n+         CollectivePermuteCostModelType::kIntraPartitionTwoWayHasNonMutual,\n+         absl::Seconds(1.0 / 3.0)},\n+    }),\n+    [](const TestParamInfo<CollectivePermuteInterpolationTest::ParamType>&\n+           info) { return info.param.test_name; });\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "40b2d7a7e9fe333f06f5ea1ee46aae1d604bfa2d",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_hlo_cost_analysis.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis.cc?ref=200ed27bb349ae36630388d38606e6fe904d3384",
            "patch": "@@ -574,6 +574,20 @@ absl::Status GpuHloCostAnalysis::HandleAllToAll(const HloInstruction* hlo) {\n   return absl::OkStatus();\n }\n \n+absl::Status GpuHloCostAnalysis::HandleCollectivePermute(\n+    const HloInstruction* hlo) {\n+  current_properties_[kCollBytesTransferred] +=\n+      ShapeUtil::ByteSizeOf(hlo->operand(0)->shape());\n+  return absl::OkStatus();\n+}\n+\n+absl::Status GpuHloCostAnalysis::HandleCollectivePermuteStart(\n+    const HloInstruction* hlo) {\n+  current_properties_[kCollBytesTransferred] +=\n+      ShapeUtil::ByteSizeOf(hlo->operand(0)->shape());\n+  return absl::OkStatus();\n+}\n+\n absl::Status GpuHloCostAnalysis::HandleElementwiseOp(\n     const HloInstruction* hlo) {\n   current_properties_[kFlopsKey] = GetFlopsForElementwiseOp(hlo);"
        },
        {
            "sha": "98517c38f0eaeb70c615ecd1338c84cc474c48a9",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_hlo_cost_analysis.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis.h?ref=200ed27bb349ae36630388d38606e6fe904d3384",
            "patch": "@@ -79,6 +79,8 @@ class GpuHloCostAnalysis : public HloCostAnalysis {\n   absl::Status HandleAsyncStart(const HloInstruction* hlo) override;\n   absl::Status HandleReduceScatter(const HloInstruction* hlo) override;\n   absl::Status HandleAllToAll(const HloInstruction* hlo) override;\n+  absl::Status HandleCollectivePermute(const HloInstruction* hlo) override;\n+  absl::Status HandleCollectivePermuteStart(const HloInstruction* hlo) override;\n \n   // Estimate the total size of IR accounting for both duplication\n   // of producer code by consumer and the total number of basic blocks."
        },
        {
            "sha": "3f1afbca789f9371a314fce753946ae50123c908",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_hlo_cost_analysis_test.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/200ed27bb349ae36630388d38606e6fe904d3384/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_hlo_cost_analysis_test.cc?ref=200ed27bb349ae36630388d38606e6fe904d3384",
            "patch": "@@ -669,6 +669,40 @@ ENTRY entry_computation {\n   EXPECT_EQ(analysis_.flop_count(*reduce), 32 * 39 * 6);\n }\n \n+TEST_F(GpuHloCostAnalysisTest, CollectivePermute) {\n+  absl::string_view hlo_string = R\"(\n+HloModule m, num_partitions=2\n+\n+ENTRY entry {\n+  p0 = f32[4096] parameter(0)\n+  ROOT cp = f32[4096] collective-permute(p0), source_target_pairs={{0,1},{1,0}}\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  ASSERT_IS_OK(module->entry_computation()->Accept(&analysis_));\n+  const HloInstruction* cp = module->entry_computation()->root_instruction();\n+  EXPECT_EQ(analysis_.BytesTransferred(*cp), 4096 * 4);\n+}\n+\n+TEST_F(GpuHloCostAnalysisTest, CollectivePermuteStart) {\n+  absl::string_view hlo_string = R\"(\n+HloModule m, num_partitions=2\n+\n+ENTRY entry {\n+  p0 = f32[4096] parameter(0)\n+  cps = (f32[4096], f32[4096]) collective-permute-start(p0), source_target_pairs={{0,1},{1,0}}\n+  ROOT r = f32[4096] collective-permute-done(cps)\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  ASSERT_IS_OK(module->entry_computation()->Accept(&analysis_));\n+  const HloInstruction* cps =\n+      module->entry_computation()->root_instruction()->operand(0);\n+  EXPECT_EQ(analysis_.BytesTransferred(*cps), 4096 * 4);\n+}\n+\n TEST_F(GpuHloCostAnalysisTest, AsyncAllReduce) {\n   absl::string_view hlo_string = R\"(\n HloModule m"
        }
    ],
    "stats": {
        "total": 456,
        "additions": 416,
        "deletions": 40
    }
}